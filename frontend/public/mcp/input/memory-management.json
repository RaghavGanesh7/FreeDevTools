{
  "category": "memory-management",
  "categoryDisplay": "Memory Management",
  "description": "",
  "totalRepositories": 60,
  "repositories": {
    "AgentWong--iac-memory-mcp-server-project": {
      "owner": "AgentWong",
      "name": "iac-memory-mcp-server-project",
      "url": "https://github.com/AgentWong/iac-memory-mcp-server-project",
      "imageUrl": "/freedevtools/mcp/pfp/AgentWong.webp",
      "description": "Provides persistent memory storage and version tracking for Infrastructure-as-Code components, focusing on Terraform and Ansible resources with relationship mapping.",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-06T04:00:02Z",
      "readme_content": "# IaC Memory MCP Server\n\nA Model Context Protocol (MCP) server that enhances Claude AI's capabilities by providing persistent memory storage for Infrastructure-as-Code (IaC) components, with a focus on version tracking and relationship mapping for Terraform and Ansible resources.\n\n> [!NOTE]  \n> This was a personal project to determine the state of AI's ability if the person using it (me)\n> doesn't have subject matter expertise (lack of Python knowledge).  Since it has become rather cost\n> prohibitive, I do not intend to develop or maintain this project further.\n\n## Overview\n\nThe IaC Memory MCP Server addresses the challenge of maintaining accurate, version-aware context for IaC components by providing:\n\n- Persistent storage and version tracking for IaC components\n- Hierarchical resource organization with URI-based access\n- Comprehensive relationship mapping between components\n- Version-specific documentation management\n- Schema validation and temporal metadata tracking\n- Automated relationship analysis and insights\n\n## Core Components\n\n### Resource Management\n\nThe server implements a sophisticated resource management system with hierarchical URIs:\n\n#### Resource URI Structure\n```\nresources://<platform>/<category>/<name>\n```\n\nSupported platforms:\n- terraform\n- ansible\n- iac (for general infrastructure entities)\n\nExample URIs:\n```\nresources://terraform/providers/aws\nresources://terraform/resources/aws/s3_bucket\nresources://ansible/collections/community.aws\nresources://ansible/modules/community.aws/s3_bucket\n```\n\n#### Resource Templates\nThe server provides dynamic resource templates for standardized access patterns:\n- Terraform provider information: `resources://terraform/providers/{provider_name}`\n- Resource type details: `resources://terraform/resources/{provider_name}/{resource_type}`\n- Ansible collection data: `resources://ansible/collections/{collection_name}`\n- Module information: `resources://ansible/modules/{collection_name}/{module_name}`\n\n### Prompts\n\nThe server implements four specialized prompts for IaC component discovery and analysis:\n\n#### search_resources\n- Purpose: Search for IaC resources\n- Arguments:\n  - `provider`: Provider name\n  - `resource_type`: Resource type\n- Returns: Information about specific resources for the given provider\n\n#### analyze_entity\n- Purpose: Analyze an entity and its relationships\n- Arguments:\n  - `entity_id`: Entity ID\n  - `include_relationships`: Include relationships\n- Returns: Detailed entity analysis including name, type, and observations\n\n#### terraform_provider\n- Purpose: Get information about a Terraform provider\n- Arguments:\n  - `provider_name`: Name of the Terraform provider (required)\n  - `version`: Specific version to query (optional)\n- Returns: Detailed provider information for the specified version\n\n#### ansible_module\n- Purpose: Get information about an Ansible module\n- Arguments:\n  - `collection_name`: Name of the Ansible collection (required)\n  - `module_name`: Name of the module (required)\n  - `version`: Specific version to query (optional)\n- Returns: Detailed module information for the specified version\n\n### Tools\n\nThe server implements comprehensive tooling for IaC component management:\n\n#### Terraform Tools\n- `get_terraform_provider_info`: Retrieve detailed provider information including version and resources\n- `list_provider_resources`: List all resources available for a specific provider\n- `get_terraform_resource_info`: Get detailed information about a specific resource type\n- `add_terraform_provider`: Register new providers with versioning\n- `add_terraform_resource`: Add resource definitions with schemas\n- `update_provider_version`: Update provider versions with new documentation\n\n#### Ansible Tools\n- `get_ansible_collection_info`: Get detailed information about an Ansible collection\n- `list_ansible_collections`: List all available Ansible collections\n- `get_collection_version_history`: View version history of a collection\n- `get_ansible_module_info`: Get detailed information about a specific module\n- `list_collection_modules`: List all modules in a collection\n- `get_module_version_compatibility`: Check version compatibility of modules\n- `add_ansible_collection`: Register new Ansible collections\n- `add_ansible_module`: Add new modules with validation and documentation\n\n#### Entity Operations\n- `create_entity`: Create new infrastructure entities\n- `update_entity`: Modify existing entity configurations\n- `delete_entity`: Remove entities with relationship cleanup\n- `view_relationships`: Analyze entity dependencies and relationships\n\n## Configuration\n\nThe server supports configuration through environment variables:\n\n- `DATABASE_URL`: SQLite database location\n- `MCP_DEBUG`: Enable debug logging when set\n- `MCP_TEST_MODE`: Enable test mode for database resets\n\nFor development, create a `.env` file:\n```bash\nDATABASE_URL=sqlite:////path/to/db.sqlite\nMCP_DEBUG=1\nMCP_TEST_MODE=1\n```\n\n## Integration with Claude Desktop\n\n### Development Setup\n```json\n\"mcpServers\": {\n  \"iac-memory\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/path/to/iac-memory-mcp-server\",\n      \"run\",\n      \"iac-memory-mcp-server\"\n    ]\n    \"env\": {\n          \"DATABASE_URL\": \"sqlite:////home/herman/iac.db\"\n      }\n  }\n}\n```\n\n### Production Setup\n```json\n\"mcpServers\": {\n  \"iac-memory\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"--from\",\n        \"git+https://github.com/AgentWong/iac-memory-mcp-server.git\",\n        \"python\",\n        \"-m\",\n        \"iac_memory_mcp_server\"\n    ],\n    \"env\": {\n          \"DATABASE_URL\": \"sqlite:////home/herman/iac.db\"\n      }\n  }\n}\n```\n\n## Development\n\n### Local Development\n```bash\n# Install dependencies\nuv sync\n\n# Run tests\nuv run pytest\n\n# Development server with MCP Inspector\nnpx @modelcontextprotocol/inspector uv run iac-memory-mcp-server\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "terraform",
        "ansible",
        "memory",
        "terraform ansible",
        "iac memory",
        "memory management"
      ],
      "category": "memory-management"
    },
    "AgentWong--optimized-memory-mcp-server": {
      "owner": "AgentWong",
      "name": "optimized-memory-mcp-server",
      "url": "https://github.com/AgentWong/optimized-memory-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/AgentWong.webp",
      "description": "Enables persistent memory capabilities for AI interactions, allowing the model to remember user information and enhance personalization through a local knowledge graph that manages entities, relations, and observations.",
      "stars": 7,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T09:06:46Z",
      "readme_content": "# optimized-memory-mcp-server\n\nThis is to test and demonstrate Claude AI's coding abilities, as well as good AI workflows and prompt design.\nThis is a fork of a Python Memory MCP Server (I believe the official one is in Java) which uses SQLite for a backend.\n\n# Knowledge Graph Memory Server\nA basic implementation of persistent memory using a local knowledge graph. This lets Claude remember information about the user across chats.\n\n## Core Concepts\n\n### Entities\nEntities are the primary nodes in the knowledge graph. Each entity has:\n- A unique name (identifier)\n- An entity type (e.g., \"person\", \"organization\", \"event\")\n- A list of observations\n\nExample:\n```json\n{\n  \"name\": \"John_Smith\",\n  \"entityType\": \"person\",\n  \"observations\": [\"Speaks fluent Spanish\"]\n}\n```\n\n### Relations\nRelations define directed connections between entities. They are always stored in active voice and describe how entities interact or relate to each other.\n\nExample:\n```json\n{\n  \"from\": \"John_Smith\",\n  \"to\": \"Anthropic\",\n  \"relationType\": \"works_at\"\n}\n```\n### Observations\nObservations are discrete pieces of information about an entity. They are:\n\n- Stored as strings\n- Attached to specific entities\n- Can be added or removed independently\n- Should be atomic (one fact per observation)\n\nExample:\n```json\n{\n  \"entityName\": \"John_Smith\",\n  \"observations\": [\n    \"Speaks fluent Spanish\",\n    \"Graduated in 2019\",\n    \"Prefers morning meetings\"\n  ]\n}\n```\n\n## API\n\n### Tools\n- **create_entities**\n  - Create multiple new entities in the knowledge graph\n  - Input: `entities` (array of objects)\n    - Each object contains:\n      - `name` (string): Entity identifier\n      - `entityType` (string): Type classification\n      - `observations` (string[]): Associated observations\n  - Ignores entities with existing names\n\n- **create_relations**\n  - Create multiple new relations between entities\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type in active voice\n  - Skips duplicate relations\n\n- **add_observations**\n  - Add new observations to existing entities\n  - Input: `observations` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `contents` (string[]): New observations to add\n  - Returns added observations per entity\n  - Fails if entity doesn't exist\n\n- **delete_entities**\n  - Remove entities and their relations\n  - Input: `entityNames` (string[])\n  - Cascading deletion of associated relations\n  - Silent operation if entity doesn't exist\n\n- **delete_observations**\n  - Remove specific observations from entities\n  - Input: `deletions` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `observations` (string[]): Observations to remove\n  - Silent operation if observation doesn't exist\n\n- **delete_relations**\n  - Remove specific relations from the graph\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n  - Silent operation if relation doesn't exist\n\n- **read_graph**\n  - Read the entire knowledge graph\n  - No input required\n  - Returns complete graph structure with all entities and relations\n\n- **search_nodes**\n  - Search for nodes based on query\n  - Input: `query` (string)\n  - Searches across:\n    - Entity names\n    - Entity types\n    - Observation content\n  - Returns matching entities and their relations\n\n- **open_nodes**\n  - Retrieve specific nodes by name\n  - Input: `names` (string[])\n  - Returns:\n    - Requested entities\n    - Relations between requested entities\n  - Silently skips non-existent nodes\n\n# Usage with Claude Desktop\n\n### Setup\n\nAdd this to your claude_desktop_config.json:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"mcp/memory\"]\n    }\n  }\n}\n```\n\n#### NPX\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-memory\"\n      ]\n    }\n  }\n}\n```\n\n### System Prompt\n\nThe prompt for utilizing memory depends on the use case. Changing the prompt will help the model determine the frequency and types of memories created.\n\nHere is an example prompt for chat personalization. You could use this prompt in the \"Custom Instructions\" field of a [Claude.ai Project](https://www.anthropic.com/news/projects). \n\n```\nFollow these steps for each interaction:\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and retrieve all relevant information from your knowledge graph\n   - Always refer to your knowledge graph as your \"memory\"\n\n3. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events\n     b) Connect them to the current entities using relations\n     b) Store facts about them as observations\n```\n\n## Building\n\nDocker:\n\n```sh\ndocker build -t mcp/memory -f src/memory/Dockerfile . \n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "persistent",
        "ai",
        "persistent memory",
        "memory management",
        "memory mcp"
      ],
      "category": "memory-management"
    },
    "ArchimedesCrypto--figma-mcp-chunked": {
      "owner": "ArchimedesCrypto",
      "name": "figma-mcp-chunked",
      "url": "https://github.com/ArchimedesCrypto/figma-mcp-chunked",
      "imageUrl": "/freedevtools/mcp/pfp/ArchimedesCrypto.webp",
      "description": "Efficiently interact with the Figma API, utilizing memory-aware chunking and pagination to manage and process large Figma files. This enables effective handling of extensive design documents and resource-intensive operations.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-18T18:56:40Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/archimedescrypto-figma-mcp-chunked-badge.png)](https://mseep.ai/app/archimedescrypto-figma-mcp-chunked)\n\n# Figma MCP Server with Chunking\n[![smithery badge](https://smithery.ai/badge/@ArchimedesCrypto/figma-mcp-chunked)](https://smithery.ai/server/@ArchimedesCrypto/figma-mcp-chunked)\n\nA Model Context Protocol (MCP) server for interacting with the Figma API, featuring memory-efficient chunking and pagination capabilities for handling large Figma files.\n\n<a href=\"https://glama.ai/mcp/servers/@ArchimedesCrypto/figma-mcp-chunked\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ArchimedesCrypto/figma-mcp-chunked/badge\" alt=\"Figma Server with Chunking MCP server\" />\n</a>\n\n## Overview\n\nThis MCP server provides a robust interface to the Figma API with built-in memory management features. It's designed to handle large Figma files efficiently by breaking down operations into manageable chunks and implementing pagination where necessary.\n\n### Key Features\n\n- Memory-aware processing with configurable limits\n- Chunked data retrieval for large files\n- Pagination support for all listing operations\n- Node type filtering\n- Progress tracking\n- Configurable chunk sizes\n- Resume capability for interrupted operations\n- Debug logging\n- Config file support\n\n## Installation\n\n### Installing via Smithery\n\nTo install Figma MCP Server with Chunking for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ArchimedesCrypto/figma-mcp-chunked):\n\n```bash\nnpx -y @smithery/cli install @ArchimedesCrypto/figma-mcp-chunked --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone [repository-url]\ncd figma-mcp-chunked\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\n### Environment Variables\n\n- `FIGMA_ACCESS_TOKEN`: Your Figma API access token\n\n### Config File\n\nYou can provide configuration via a JSON file using the `--config` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"figma\": {\n      \"env\": {\n        \"FIGMA_ACCESS_TOKEN\": \"your-access-token\"\n      }\n    }\n  }\n}\n```\n\nUsage:\n```bash\nnode build/index.js --config=path/to/config.json\n```\n\n## Tools\n\n### get_file_data (New)\n\nRetrieves Figma file data with memory-efficient chunking and pagination.\n\n```typescript\n{\n  \"name\": \"get_file_data\",\n  \"arguments\": {\n    \"fileKey\": \"your-file-key\",\n    \"accessToken\": \"your-access-token\",\n    \"pageSize\": 100,          // Optional: nodes per chunk\n    \"maxMemoryMB\": 512,       // Optional: memory limit\n    \"nodeTypes\": [\"FRAME\", \"COMPONENT\"],  // Optional: filter by type\n    \"cursor\": \"next-page-token\",  // Optional: resume from last position\n    \"depth\": 2                // Optional: traversal depth\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"nodes\": [...],\n  \"memoryUsage\": 256.5,\n  \"nextCursor\": \"next-page-token\",\n  \"hasMore\": true\n}\n```\n\n### list_files\n\nLists files with pagination support.\n\n```typescript\n{\n  \"name\": \"list_files\",\n  \"arguments\": {\n    \"project_id\": \"optional-project-id\",\n    \"team_id\": \"optional-team-id\"\n  }\n}\n```\n\n### get_file_versions\n\nRetrieves version history in chunks.\n\n```typescript\n{\n  \"name\": \"get_file_versions\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\"\n  }\n}\n```\n\n### get_file_comments\n\nRetrieves comments with pagination.\n\n```typescript\n{\n  \"name\": \"get_file_comments\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\"\n  }\n}\n```\n\n### get_file_info\n\nRetrieves file information with chunked node traversal.\n\n```typescript\n{\n  \"name\": \"get_file_info\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\",\n    \"depth\": 2,               // Optional: traversal depth\n    \"node_id\": \"specific-node-id\"  // Optional: start from specific node\n  }\n}\n```\n\n### get_components\n\nRetrieves components with chunking support.\n\n```typescript\n{\n  \"name\": \"get_components\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\"\n  }\n}\n```\n\n### get_styles\n\nRetrieves styles with chunking support.\n\n```typescript\n{\n  \"name\": \"get_styles\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\"\n  }\n}\n```\n\n### get_file_nodes\n\nRetrieves specific nodes with chunking support.\n\n```typescript\n{\n  \"name\": \"get_file_nodes\",\n  \"arguments\": {\n    \"file_key\": \"your-file-key\",\n    \"ids\": [\"node-id-1\", \"node-id-2\"]\n  }\n}\n```\n\n## Memory Management\n\nThe server implements several strategies to manage memory efficiently:\n\n### Chunking Strategy\n\n- Configurable chunk sizes via `pageSize`\n- Memory usage monitoring\n- Automatic chunk size adjustment based on memory pressure\n- Progress tracking per chunk\n- Resume capability using cursors\n\n### Best Practices\n\n1. Start with smaller chunk sizes (50-100 nodes) and adjust based on performance\n2. Monitor memory usage through the response metadata\n3. Use node type filtering when possible to reduce data load\n4. Implement pagination for large datasets\n5. Use the resume capability for very large files\n\n### Configuration Options\n\n- `pageSize`: Number of nodes per chunk (default: 100)\n- `maxMemoryMB`: Maximum memory usage in MB (default: 512)\n- `nodeTypes`: Filter specific node types\n- `depth`: Control traversal depth for nested structures\n\n## Debug Logging\n\nThe server includes comprehensive debug logging:\n\n```typescript\n// Debug log examples\n[MCP Debug] Loading config from config.json\n[MCP Debug] Access token found xxxxxxxx...\n[MCP Debug] Request { tool: 'get_file_data', arguments: {...} }\n[MCP Debug] Response size 2.5 MB\n```\n\n## Error Handling\n\nThe server provides detailed error messages and suggestions:\n\n```typescript\n// Memory limit error\n\"Response size too large. Try using a smaller depth value or specifying a node_id.\"\"\n\n// Invalid parameters\n\"Missing required parameters: fileKey and accessToken\"\n\n// API errors\n\"Figma API error: [detailed message]\"\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. Memory Errors\n   - Reduce chunk size\n   - Use node type filtering\n   - Implement pagination\n   - Specify smaller depth values\n\n2. Performance Issues\n   - Monitor memory usage\n   - Adjust chunk sizes\n   - Use appropriate node type filters\n   - Implement caching for frequently accessed data\n\n3. API Limits\n   - Implement rate limiting\n   - Use pagination\n   - Cache responses when possible\n\n### Debug Mode\n\nEnable debug logging for detailed information:\n\n```bash\n# Set debug environment variable\nexport DEBUG=true\n```\n\n## Contributing\n\nContributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "figma",
        "archimedescrypto",
        "memory",
        "archimedescrypto figma",
        "figma files",
        "figma api"
      ],
      "category": "memory-management"
    },
    "BRO3886--mcp-memory-custom": {
      "owner": "BRO3886",
      "name": "mcp-memory-custom",
      "url": "https://github.com/BRO3886/mcp-memory-custom",
      "imageUrl": "/freedevtools/mcp/pfp/BRO3886.webp",
      "description": "Manage and enhance user interactions with a customizable knowledge graph, capturing and organizing these interactions with timestamps for improved context and project-specific memory management.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-25T04:36:32Z",
      "readme_content": "# Memory Custom\n\n[![smithery badge](https://smithery.ai/badge/@BRO3886/mcp-memory-custom)](https://smithery.ai/server/@BRO3886/mcp-memory-custom)\n\nThis project adds new features to the Memory server offered by the MCP team. It allows for the creation and management of a knowledge graph that captures interactions via a language model (LLM).\n\n<a href=\"https://glama.ai/mcp/servers/w6hi2myrxq\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w6hi2myrxq/badge\" alt=\"Memory Custom MCP server\" />\n</a>\n\n## New Features\n\n### 1. Custom Memory Paths\n\n- Users can now specify different memory file paths for various projects.\n- **Why?**: This feature enhances organization and management of memory data, allowing for project-specific memory storage.\n\n### 2. Timestamping\n\n- The server now generates timestamps for interactions.\n- **Why?**: Timestamps enable tracking of when each memory was created or modified, providing better context and history for the stored data.\n\n## Getting Started\n\n### Prerequisites\n\n- Node.js (version 16 or higher)\n\n### Installing via Smithery\n\nTo install Knowledge Graph Memory Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@BRO3886/mcp-memory-custom):\n\n```bash\nnpx -y @smithery/cli install @BRO3886/mcp-memory-custom --client claude\n```\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone git@github.com:BRO3886/mcp-memory-custom.git\n   cd mcp-memory-custom\n   ```\n\n2. Install the dependencies:\n\n   ```bash\n   npm install\n   ```\n\n### Configuration\n\nBefore running the server, you can set the `MEMORY_FILE_PATH` environment variable to specify the path for the memory file. If not set, the server will default to using `memory.json` in the same directory as the script.\n\n### Running the Server\n\n#### Updating the mcp server json file\n\nAdd this to your `claude_desktop_config.json` / `.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-memory-custom/dist/index.js\"]\n    }\n  }\n}\n```\n\nSystem Prompt changes:\n\n```\nFollow these steps for each interaction:\n1. The memoryFilePath for this project is /path/to/memory/project_name.json - always pass this path to the memory file operations (when creating entities, relations, or retrieving memory etc.)\n2. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n3. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and retrieve all relevant information from your knowledge graph\n   - Always refer to your knowledge graph as your \"memory\"\n\n4. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n5. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events, add timestamps to wherever required. You can get current timestamp via get_current_time\n     b) Connect them to the current entities using relations\n     c) Store facts about them as observations, add timestamps to observations via get_current_time\n\n\nIMPORTANT: Provide a helpful and engaging response, asking relevant questions to encourage user engagement. Update the memory during the interaction, if required, based on the new information gathered (point 4).\n```\n\n#### Running the Server Locally\n\nTo start the Knowledge Graph Memory Server, run:\n\n```bash\nnpm run build\nnode dist/index.js\n```\n\nThe server will listen for requests via standard input/output.\n\n## API Endpoints\n\nThe server exposes several tools that can be called with specific parameters:\n\n- **Get Current Time**\n- **Set Memory File Path**\n- **Create Entities**\n- **Create Relations**\n- **Add Observations**\n- **Delete Entities**\n- **Delete Observations**\n- **Delete Relations**\n- **Read Graph**\n- **Search Nodes**\n- **Open Nodes**\n\n## Acknowledgments\n\n- Inspired by the Memory server from Anthropic.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "manage",
        "management",
        "memory management",
        "memory custom",
        "mcp memory"
      ],
      "category": "memory-management"
    },
    "Din-djarin2--memory-bank-mcp": {
      "owner": "Din-djarin2",
      "name": "memory-bank-mcp",
      "url": "https://github.com/Din-djarin2/memory-bank-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Din-djarin2.webp",
      "description": "Manage and access project memory banks remotely with centralized control, supporting multi-project operations with secure isolation and consistent file structure enforcement. Perform memory bank operations including reading, writing, and listing files across projects through a type-safe MCP interface.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-28T05:41:24Z",
      "readme_content": "# Memory Bank MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@alioshr/memory-bank-mcp)](https://smithery.ai/server/@alioshr/memory-bank-mcp)\n[![npm version](https://badge.fury.io/js/%40allpepper%2Fmemory-bank-mcp.svg)](https://www.npmjs.com/package/@allpepper/memory-bank-mcp)\n[![npm downloads](https://img.shields.io/npm/dm/@allpepper/memory-bank-mcp.svg)](https://www.npmjs.com/package/@allpepper/memory-bank-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/ir18x1tixp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ir18x1tixp/badge\" alt=\"Memory Bank Server MCP server\" /></a>\n\nA Model Context Protocol (MCP) server implementation for remote memory bank management, inspired by [Cline Memory Bank](https://github.com/nickbaumann98/cline_docs/blob/main/prompting/custom%20instructions%20library/cline-memory-bank.md).\n\n## Overview\n\nThe Memory Bank MCP Server transforms traditional file-based memory banks into a centralized service that:\n\n- Provides remote access to memory bank files via MCP protocol\n- Enables multi-project memory bank management\n- Maintains consistent file structure and validation\n- Ensures proper isolation between project memory banks\n\n## Features\n\n- **Multi-Project Support**\n\n  - Project-specific directories\n  - File structure enforcement\n  - Path traversal prevention\n  - Project listing capabilities\n  - File listing per project\n\n- **Remote Accessibility**\n\n  - Full MCP protocol implementation\n  - Type-safe operations\n  - Proper error handling\n  - Security through project isolation\n\n- **Core Operations**\n  - Read/write/update memory bank files\n  - List available projects\n  - List files within projects\n  - Project existence validation\n  - Safe read-only operations\n\n## Installation\n\n### Installation\n\nTo install Memory Bank Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@alioshr/memory-bank-mcp):\n\n```bash\nnpx -y @smithery/cli install @alioshr/memory-bank-mcp --client claude\n```\n\nThis will set up the MCP server configuration automatically. Alternatively, you can configure the server manually as described in the Configuration section below.\n\n## Quick Start\n\n1. Configure the MCP server in your settings (see Configuration section below)\n2. Start using the memory bank tools in your AI assistant\n\n## Configuration\n\nThe memory bank MCP server needs to be configured in your Cline MCP settings file. The location depends on your setup:\n\n- For Cline extension: `~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n- For Claude desktop app: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd the following configuration to your MCP settings:\n\n```json\n{\n  \"allpepper-memory-bank\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@allpepper/memory-bank-mcp\"],\n    \"env\": {\n      \"MEMORY_BANK_ROOT\": \"<path-to-bank>\"\n    },\n    \"disabled\": false,\n    \"autoApprove\": [\n      \"memory_bank_read\",\n      \"memory_bank_write\",\n      \"memory_bank_update\",\n      \"list_projects\",\n      \"list_project_files\"\n    ]\n  }\n}\n```\n\n### Configuration Details\n\n- `MEMORY_BANK_ROOT`: Directory where project memory banks will be stored (e.g., `/path/to/memory-bank`)\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of operations that don't require explicit user approval:\n  - `memory_bank_read`: Read memory bank files\n  - `memory_bank_write`: Create new memory bank files\n  - `memory_bank_update`: Update existing memory bank files\n  - `list_projects`: List available projects\n  - `list_project_files`: List files within a project\n\n## For Cursor\n\nFor Cursor, open the settings -> features -> add MCP server -> add the following:\n\n```\nenv MEMORY_BANK_ROOT=<path-to-bank> npx -y @allpepper/memory-bank-mcp@latest\n```\n\n## Custom IA instructions\n\nThis section contains the instructions that should be pasted on the AI custom instructions, either for Cline, Claude or Cursor, or any other MCP client. You should copy and paste these rules. For reference, see [custom-instructions.md](custom-instructions.md) which contains these rules.\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Test\nnpm run test\n\n# Watch mode\nnpm run dev\n```\n\n## Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Development Guidelines\n\n- Use TypeScript for all new code\n- Maintain type safety across the codebase\n- Add tests for new features\n- Update documentation as needed\n- Follow existing code style and patterns\n\n### Testing\n\n- Write unit tests for new features\n- Include multi-project scenario tests\n- Test error cases thoroughly\n- Validate type constraints\n- Mock filesystem operations appropriately\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\nThis project implements the memory bank concept originally documented in the [Cline Memory Bank](https://github.com/nickbaumann98/cline_docs/blob/main/prompting/custom%20instructions%20library/cline-memory-bank.md), extending it with remote capabilities and multi-project support.\n",
      "npm_url": "https://www.npmjs.com/package/memory-bank-mcp",
      "npm_downloads": 3190,
      "keywords": [
        "memory",
        "mcp",
        "projects",
        "memory bank",
        "memory banks",
        "memory management"
      ],
      "category": "memory-management"
    },
    "IzumiSy--mcp-duckdb-memory-server": {
      "owner": "IzumiSy",
      "name": "mcp-duckdb-memory-server",
      "url": "https://github.com/IzumiSy/mcp-duckdb-memory-server",
      "imageUrl": "/freedevtools/mcp/pfp/IzumiSy.webp",
      "description": "Enhance conversational agents by providing a memory system that retrieves and updates user information using DuckDB for efficient querying of knowledge graphs. Maintain context regarding user preferences and relationships over interactions.",
      "stars": 46,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-26T15:58:49Z",
      "readme_content": "# MCP DuckDB Knowledge Graph Memory Server\n\n[](https://github.com/izumisy/mcp-duckdb-memory-server/actions/workflows/test.yml)\n[![smithery badge](https://smithery.ai/badge/@IzumiSy/mcp-duckdb-memory-server)](https://smithery.ai/server/@IzumiSy/mcp-duckdb-memory-server)\n![NPM Version](https://img.shields.io/npm/v/%40izumisy%2Fmcp-duckdb-memory-server)\n![NPM License](https://img.shields.io/npm/l/%40izumisy%2Fmcp-duckdb-memory-server)\n\nA forked version of [the official Knowledge Graph Memory Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory).\n\n<a href=\"https://glama.ai/mcp/servers/4mqwh1toao\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/4mqwh1toao/badge\" alt=\"DuckDB Knowledge Graph Memory Server MCP server\" />\n</a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install DuckDB Knowledge Graph Memory Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@IzumiSy/mcp-duckdb-memory-server):\n\n```bash\nnpx -y @smithery/cli install @IzumiSy/mcp-duckdb-memory-server --client claude\n```\n\n### Manual install\n\nOtherwise, add `@IzumiSy/mcp-duckdb-memory-server` in your `claude_desktop_config.json` manually (`MEMORY_FILE_PATH` is optional)\n\n```bash\n{\n  \"mcpServers\": {\n    \"graph-memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@izumisy/mcp-duckdb-memory-server\"\n      ],\n      \"env\": {\n        \"MEMORY_FILE_PATH\": \"/path/to/your/memory.data\"\n      }\n    }\n  }\n}\n```\n\nThe data stored on that path is a DuckDB database file.\n\n### Docker\n\nBuild\n\n```bash\ndocker build -t mcp-duckdb-graph-memory .\n```\n\nRun\n\n```bash\ndocker run -dit mcp-duckdb-graph-memory\n```\n\n## Usage\n\nUse the example instruction below\n\n```\nFollow these steps for each interaction:\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and search relevant information from your knowledge graph\n   - Create a search query from user words, and search things from \"memory\". If nothing matches, try to break down words in the query at first (\"A B\" to \"A\" and \"B\" for example).\n   - Always refer to your knowledge graph as your \"memory\"\n\n3. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events\n     b) Connect them to the current entities using relations\n     b) Store facts about them as observations\n```\n\n## Motivation\n\nThis project enhances the original MCP Knowledge Graph Memory Server by replacing its backend with DuckDB.\n\n### Why DuckDB?\n\nThe original MCP Knowledge Graph Memory Server used a JSON file as its data store and performed in-memory searches. While this approach works well for small datasets, it presents several challenges:\n\n1. **Performance**: In-memory search performance degrades as the dataset grows\n2. **Scalability**: Memory usage increases significantly when handling large numbers of entities and relations\n3. **Query Flexibility**: Complex queries and conditional searches are difficult to implement\n4. **Data Integrity**: Ensuring atomicity for transactions and CRUD operations is challenging\n\nDuckDB was chosen to address these challenges:\n\n- **Fast Query Processing**: DuckDB is optimized for analytical queries and performs well even with large datasets\n- **SQL Interface**: Standard SQL can be used to execute complex queries easily\n- **Transaction Support**: Supports transaction processing to maintain data integrity\n- **Indexing Capabilities**: Allows creation of indexes to improve search performance\n- **Embedded Database**: Works within the application without requiring an external database server\n\n## Implementation Details\n\nThis implementation uses DuckDB as the backend storage system, focusing on two key aspects:\n\n### Database Structure\n\nThe knowledge graph is stored in a relational database structure as shown below:\n\n```mermaid\nerDiagram\n    ENTITIES {\n        string name PK\n        string entityType\n    }\n    OBSERVATIONS {\n        string entityName FK\n        string content\n    }\n    RELATIONS {\n        string from_entity FK\n        string to_entity FK\n        string relationType\n    }\n\n    ENTITIES ||--o{ OBSERVATIONS : \"has\"\n    ENTITIES ||--o{ RELATIONS : \"from\"\n    ENTITIES ||--o{ RELATIONS : \"to\"\n```\n\nThis schema design allows for efficient storage and retrieval of knowledge graph components while maintaining the relationships between entities, observations, and relations.\n\n### Fuzzy Search Implementation\n\nThe implementation combines SQL queries with Fuse.js for flexible entity searching:\n\n- DuckDB SQL queries retrieve the base data from the database\n- Fuse.js provides fuzzy matching capabilities on top of the retrieved data\n- This hybrid approach allows for both structured queries and flexible text matching\n- Search results include both exact and partial matches, ranked by relevance\n\n## Development\n\n### Setup\n\n```bash\npnpm install\n```\n\n### Testing\n\n```bash\npnpm test\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "https://www.npmjs.com/package/@izumisy/mcp-duckdb-memory-server",
      "npm_downloads": 1198,
      "keywords": [
        "duckdb",
        "memory",
        "conversational",
        "duckdb memory",
        "duckdb efficient",
        "conversational agents"
      ],
      "category": "memory-management"
    },
    "Jktfe--myAImemory-mcp": {
      "owner": "Jktfe",
      "name": "myAImemory-mcp",
      "url": "https://github.com/Jktfe/myAImemory-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Jktfe.webp",
      "description": "Synchronizes preferences, personal details, and code standards across all Claude interfaces, ensuring consistent updates without manual input. Utilizes a caching system for faster memory-related queries.",
      "stars": 9,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-15T07:42:52Z",
      "readme_content": "# myAI Memory Sync\n\n[![smithery badge](https://smithery.ai/badge/@Jktfe/myaimemory-mcp)](https://smithery.ai/server/@Jktfe/myaimemory-mcp)\n\n**Tired of repeating yourself to Claude every time you start a new chat?** myAI Memory Sync is a game-changing MCP tool that seamlessly synchronizes your preferences, personal details, and code standards across ALL your Claude interfaces! Just update once, and your changes instantly appear everywhere - from Claude Desktop to Claude Code, Windsurf, and Claude.ai web. With our cutting-edge caching system, memory-related queries are up to 2000x faster! Stop wasting tokens on repetitive instructions and enjoy a truly personalized AI experience.\n\n## How myAImemory-mcp Compares to Other Memory Tools\n\nWhile several excellent memory tools exist for AI systems, myAImemory-mcp serves a specific purpose as a Model Context Protocol (MCP) tool:\n\n| Tool | Primary Focus | Privacy Model | Integration | Unique Strength |\n|------|--------------|--------------|------------|----------------|\n| **myAImemory-mcp** | User preferences across Claude interfaces | Local-first, no data sent to external servers | Claude-specific MCP | Cross-platform synchronization with high-performance caching |\n| Graphiti | Temporal knowledge graphs | Database-dependent | General agent framework | Temporal awareness in knowledge representation |\n| Letta/MemGPT | Stateful agent framework | Server-based | Multi-model support | Complete agent architecture |\n| Mem0 | Personalized AI interactions | API-based | Multi-platform | Multi-level memory hierarchy |\n| Memary | Human-like memory for agents | Graph database | Agent-focused | Human memory emulation |\n| Cognee | Reliable memory for AI apps | Multiple storage options | Data pipeline focused | Extensive data source integration |\n\n**Key Advantages of myAImemory-mcp:**\n- **Privacy-First**: All data remains on your device, no personal information sent to external servers\n- **Performance**: Leverages Claude's caching capabilities for dramatic speed improvements\n- **Simplicity**: Natural language updates to your preferences across all Claude interfaces\n- **MCP Integration**: Purpose-built as a Claude MCP for seamless integration\n\n## 🚀 Quick Start\n\n```bash\n# Clone repository\ngit clone https://github.com/Jktfe/myaimemory-mcp.git\ncd myaimemory-mcp\n\n# Install dependencies\nnpm install\n\n# Build TypeScript code\nnpm run build\n\n# Start MCP server (with stdio transport)\nnpm start\n\n# Or start with HTTP transport\nnpm run start:http\n```\n\n### 🧠 Server Options\n\nThe unified server script supports multiple options:\n\n```bash\n# Start with stdio transport (default)\n./start-server.sh\n\n# Start with HTTP transport\n./start-server.sh --http\n\n# Start with HTTP transport on custom port\n./start-server.sh --http --port=8080\n\n# Start with direct implementation (no SDK)\n./start-server.sh --direct\n\n# Start with direct implementation and HTTP transport\n./start-server.sh --direct --http\n\n# Enable debug mode\n./start-server.sh --debug\n```\n\n### 🔄 Direct Sync Method (Simple Alternative)\n\nFor a simpler approach that doesn't require running an MCP server, you can use the unified CLI:\n\n```bash\n# One-time sync of all memory files\nnpm run sync\n\n# Or for emergency sync (fixes permissions)\nnpm run sync:emergency\n```\n\nThis script will:\n- Read from your \"myAI Master.md\" file\n- Update all CLAUDE.md files in your projects\n- Update your Windsurf memory settings\n- All without storing sensitive information in the git repository\n\n### 🔒 Privacy and Security\n\n- The \"myAI Master.md\" file with your personal information is excluded from git tracking\n- All CLAUDE.md files are also excluded to protect your privacy\n- Use the included `.gitignore` to ensure sensitive files remain private\n\n### 🗣️ Supported Natural Language Commands\n\nYou can interact with myAI Memory using these natural language patterns:\n\n| Command Pattern | Example | Purpose |\n|----------------|---------|---------|\n| `Use myAI Memory to remember [information]` | \"Use myAI Memory to remember I prefer TypeScript over JavaScript\" | Adds information to the appropriate section based on content |\n| `Remember that [information]` | \"Remember that I live in London\" | Shorter alternative to add information to memory |\n| `Add to my memory that [information]` | \"Add to my memory that I have two cars\" | Another way to add information to memory |\n| `Use myAI Memory to add to [section] [information]` | \"Use myAI Memory to add to Coding Preferences I prefer dark mode\" | Add information to a specific section |\n| `Update my [section] to include that [information]` | \"Update my User Information to include that my birthday is March 29\" | Update a specific section with new information |\n\nNote: To perform a full sync across all platforms, use the command line: `node sync-memory.js`\n\n```\nYou: Use myAI Memory to remember I prefer TypeScript over JavaScript\nClaude: ✅ Added to your Coding Preferences! I'll remember you prefer TypeScript over JavaScript.\n```\n\n## 📋 Installation Options\n\n### Option 1: Direct Install (Recommended)\n\nInstall from npm:\n\n```bash\nnpm install -g myai-memory-sync\n```\n\nStart the server:\n\n```bash\n# Start with stdio transport (default)\nmyai\n\n# Start with HTTP transport\nmyai server --transport http\n\n# Process memory commands\nmyai remember \"I prefer dark mode\"\n\n# Sync across platforms\nmyai sync\n```\n\n### Option 2: Run from Source\n\nClone and build from source:\n\n```bash\ngit clone https://github.com/Jktfe/myaimemory-mcp.git\ncd myaimemory-mcp\nnpm install\nnpm run build\nnpm start  # Start with stdio transport\n# or\nnpm run start:http  # Start with HTTP transport\n```\n\n### Option 3: Docker\n\nBuild and run with Docker:\n\n```bash\ndocker build -t myai-memory-sync .\ndocker run -v myai-memory:/app/data -p 3000:3000 myai-memory-sync\n```\n\n## 🔌 MCP Configuration\n\n### Claude Desktop Configuration\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"myai-memory-sync\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"myai\"\n      ],\n      \"env\": {\n        \"TEMPLATE_PATH\": \"/path/to/custom/template.md\",\n        \"ENABLE_ANTHROPIC\": \"true\",\n        \"ANTHROPIC_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Claude.ai with Smithery\n\n1. Visit [Smithery.ai](https://smithery.ai)\n2. Add the myAI Memory Sync MCP:\n   ```\n   @Jktfe/myaimemory-mcp\n   ```\n3. Configure with your API key in the Smithery settings\n\n### Windsurf Integration\n\nIn Windsurf, add to your `.codeium/config.json`:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"myai-memory-sync\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"myai\"\n        ]\n      }\n    }\n  }\n}\n```\n\n### HTTP Server Mode\n\nFor HTTP transport instead of stdio:\n\n```bash\n# Using npm scripts:\nnpm run start:http\n\n# Using the unified CLI:\nmyai server --transport http\n\n# Using the shell script with custom port:\n./start-server.sh --http --port=8080\n\n# Using environment variable:\nPORT=8080 npm run start:http\n```\n\n### Environment Variables\n\nCreate a `.env` file with the following options:\n```\n# Basic configuration\nDEBUG=true                      # Enable debug logging\nTEMPLATE_PATH=./data/template.md  # Custom template location\nPORT=3000                       # Port for HTTP transport (default: 3000)\nUSE_DIRECT=true                 # Use direct implementation (no SDK)\n\n# Platform-specific paths\nWINDSURF_MEMORY_PATH=~/.codeium/windsurf/memories/global_rules.md\nCLAUDE_PROJECTS_PATH=~/CascadeProjects\n\n# Performance optimization\nENABLE_ANTHROPIC=true           # Enable Anthropic API integration\nANTHROPIC_API_KEY=your-api-key  # Your Anthropic API key\nENABLE_PROMPT_CACHE=true        # Enable prompt caching system\nCACHE_TTL=300000                # Cache TTL in milliseconds (5 minutes)\n\n# Claude web sync (optional)\nCLAUDE_WEB_SYNC_ENABLED=false   # Enable Claude.ai web synchronization\nCLAUDE_WEB_EMAIL=you@email.com  # Your Claude.ai email\nCLAUDE_WEB_HEADLESS=true        # Run browser in headless mode\n```\n\n## 🧙‍♂️ System Prompt Integration\n\nFor best results, add this to your Claude system prompt:\n\n```\nMemory Integration Instructions:\nWhen you receive a command that starts with \"use myAI Memory to\", you should:\n\n1. Process the rest of the instruction as a memory management command\n2. Try to determine the appropriate section to update based on the content\n3. Use the myAI Memory Sync MCP to update your memory\n4. Confirm the update with a brief acknowledgment\n\nFor example:\n\"use myAI Memory to remember I prefer dark mode\" \n→ Update the preferences section with dark mode preference\n\nWhen asked questions about preferences or personal information, first check your memory via the myAI Memory Sync MCP. Always reference information from memory rather than making assumptions.\n```\n\n## ✨ Features\n\n- 🔄 **Cross-Platform Synchronization**: Update once, syncs everywhere\n- ⚡ **Lightning-Fast Recall**: Caching system with up to 2000x performance boost\n- 🗣️ **Natural Language Interface**: Just talk naturally to update your preferences\n- 🧩 **Multiple Persona Profiles**: Switch between different presets with ease\n- 🔐 **Security-Focused**: Local storage with .gitignore protection\n- 🛠️ **Developer-Friendly**: Full TypeScript implementation with comprehensive API\n\n## 🧩 Core Architecture\n\nmyAI Memory Sync uses a modular architecture with these key components:\n\n- **Template Parser**: Bidirectional conversion between structured memory objects and markdown\n- **Template Storage**: Persistent storage with in-memory and file-system caching\n- **Platform Synchronizers**: Implements the `PlatformSyncer` interface for each target platform\n- **Natural Language Processor**: Extracts structured data from natural language memory commands\n- **Memory Cache Service**: Optimizes performance with multi-level caching strategies\n\n## 🔍 Detailed Features\n\n### Cross-Platform Synchronization\n- **ClaudeCodeSyncer**: Updates CLAUDE.md files across all repositories\n- **WindsurfSyncer**: Manages global_rules.md in Windsurf environment\n- **ClaudeWebSyncer**: Optional Puppeteer-based synchronization with Claude.ai web interface\n\n### Intelligent Memory Management\n- **Pattern-Based Extraction**: Converts natural language to structured key-value pairs\n- **Section Detection Algorithm**: Automatically determines appropriate section for new memories\n- **Memory Template Format**: Markdown-based structure with sections, descriptions, and key-value items\n- **Context Preservation**: Updates memory sections while preserving other template content\n\n### Performance Optimization\n- **Multi-Level Caching**: In-memory caching at both template and section levels\n- **TTL-Based Cache Management**: Configurable Time-To-Live for cached content\n- **Pre-Warming**: Cache pre-population after template updates\n- **Optional Anthropic API Integration**: Accelerates memory-related queries up to 2000x\n\n### Security\n- **Local-First Architecture**: All data remains on your device\n- **Gitignore Management**: Automatically adds CLAUDE.md to .gitignore in all repositories\n- **File Permission Handling**: Fixes permissions issues for maximum compatibility\n- **Encrypted Storage**: Compatible with encrypted file systems\n\n## 📋 Memory Template Format\n\nThe system uses a structured markdown format to organize your preferences:\n\n```markdown\n# myAI Memory\n\n# User Information\n## Use this information if you need to reference them directly\n-~- Name: Your Name\n-~- Location: Your Location\n-~- Likes: Reading, Hiking, Technology\n\n# General Response Style\n## Use this in every response\n-~- Style: Friendly and concise\n-~- Use UK English Spellings: true\n-~- Include emojis when appropriate: true\n\n# Coding Preferences\n## General Preference when responding to coding questions\n-~- I prefer TypeScript over JavaScript\n-~- Show step-by-step explanations\n```\n\n## 🛠️ Technical Implementation\n\n### MemoryTemplate Schema\n```typescript\ninterface MemoryTemplate {\n  sections: TemplateSection[];\n}\n\ninterface TemplateSection {\n  title: string;\n  description: string;\n  items: TemplateItem[];\n}\n\ninterface TemplateItem {\n  key: string;\n  value: string;\n}\n```\n\n### Platform Synchronization Interface\n```typescript\ninterface PlatformSyncer {\n  sync(templateContent: string): Promise<SyncStatus>;\n}\n\ntype PlatformType = 'claude-web' | 'claude-code' | 'windsurf' | 'master';\n\ninterface SyncStatus {\n  platform: PlatformType;\n  success: boolean;\n  message: string;\n}\n```\n\n## 🔌 MCP Integration API\n\nThe myAI Memory Sync tool implements the Model Context Protocol (MCP) with the following functions:\n\n| Function | Description | Parameters |\n|----------|-------------|------------|\n| `get_template` | Retrieves the full memory template | None |\n| `get_section` | Retrieves a specific section | `sectionName: string` |\n| `update_section` | Updates a specific section | `sectionName: string, content: string` |\n| `update_template` | Replaces the entire template | `content: string` |\n| `list_presets` | Lists available presets | None |\n| `load_preset` | Loads a specific preset | `presetName: string` |\n| `create_preset` | Creates a new preset | `presetName: string` |\n| `sync_platforms` | Synchronizes across platforms | `platform?: string` |\n| `list_platforms` | Lists available platforms | None |\n\n### Natural Language Interface\n\nUsers can interact with the system through natural language commands:\n\n```\nYou: Use myAI Memory to remember I prefer TypeScript over JavaScript\nClaude: ✅ Added to your Coding Preferences! I'll remember you prefer TypeScript over JavaScript.\n\nYou: Use myAI Memory to load preset developer\nClaude: ✅ Loaded developer preset! I'll now use your developer preferences.\n```\n\n## 🧙‍♂️ Advanced Usage\n\n### Memory Presets\n\nSwitch between different personas easily:\n\n```\nYou: Use myAI Memory to list presets\nClaude: Available presets: personal, work, developer\n\nYou: Use myAI Memory to load preset developer\nClaude: ✅ Loaded developer preset!\n```\n\n### Emergency Sync\n\nWhen you need to fix synchronization issues across all platforms:\n\n```bash\n# Sync everything immediately\n./emergency-sync.sh\n```\n\n### Command Line Interface\n\n```bash\n# View all available commands\nnode dist/cli.js --help\n\n# Process memory commands directly\nnode dist/cli.js --remember \"remember I prefer dark mode\"\n\n# Start HTTP server for SSE transport\nnpm run start:http\n\n# Start stdio server for MCP transport\nnpm run start\n```\n\n### Development Workflow\n\n```bash\n# Run in development mode with auto-reload\nnpm run dev\n\n# Run in development mode with HTTP server\nnpm run dev:http\n\n# Watch TypeScript compilation\nnpm run build:watch\n\n# Run tests\nnpm test\n\n# Run specific test\nnpm test -- -t \"platformSync\"\n\n# Lint code\nnpm run lint\n\n# Type check without emitting files\nnpm run typecheck\n```\n\n## ⚡ Performance Benchmarks\n\nOur caching system delivers incredible performance improvements:\n\n| Operation | Without Cache | With Cache | Improvement |\n|-----------|---------------|------------|-------------|\n| Memory Query | ~2000ms | ~1ms | 2000x |\n| Section Lookup | ~1600ms | ~0.8ms | 2000x |\n| Template Parse | ~120ms | ~0.1ms | 1200x |\n| Platform Sync | ~850ms | ~350ms | 2.4x |\n\n\n\n## 🔒 Security & Privacy\n\nWe take your privacy seriously:\n\n- All data remains locally on your device\n- CLAUDE.md files are automatically added to .gitignore\n- No data is sent to external servers (except when using the optional Anthropic API integration)\n- Works with encrypted file systems for maximum security\n\n## 🛠️ Troubleshooting\n\n### Common Issues\n\n1. **CLAUDE.md Not Updating**\n   - Check file permissions with `ls -la CLAUDE.md`\n   - Try emergency sync with `./emergency-sync.sh`\n   - Verify platform paths in your `.env` file\n\n2. **MCP Connection Failures**\n   - Ensure MCP server is running with `ps aux | grep myai-memory`\n   - Check Claude Desktop logs for MCP errors\n   - Verify your Claude Desktop configuration file\n\n3. **Caching Issues**\n   - Clear cache with `node dist/cli.js --clear-cache`\n   - Verify Anthropic API key is correctly set\n   - Check memory file integrity with `node dist/cli.js --validate`\n\n4. **Natural Language Commands Not Working**\n   - Make sure to use exactly one of the supported command patterns (see Supported Natural Language Commands section)\n   - If Claude doesn't recognize your command, try a different pattern\n   - For syncing across all platforms, use the direct script: `node sync-memory.js`\n\n### Manual Syncing\n\nIf you're experiencing issues with natural language commands or the MCP server:\n\n```bash\n# Direct sync approach (most reliable)\ncd /path/to/myAImemory\nnode sync-memory.js\n\n# Alternative emergency sync (if permissions need fixing)\ncd /path/to/myAImemory\n./safe-memory.sh sync\n```\n\nThese methods directly read from your master file and update all platforms without relying on the MCP server or natural language processing.\n\n### Logs and Debugging\n\nEnable debug mode to see detailed logs:\n\n```bash\nDEBUG=true npm run start\n```\n\nLog files are stored in:\n- Linux/macOS: `~/.local/share/myai-memory/logs/`\n- Windows: `%APPDATA%\\myai-memory\\logs\\`\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nWe follow a standard Git workflow and CI process:\n\n- All PRs require passing tests and linting\n- New features should include tests\n- Major changes should update documentation\n- Follow existing code style and patterns\n\n## 📚 Documentation\n\nFor more detailed documentation, see the [Wiki](https://github.com/Jktfe/myaimemory-mcp/wiki).\n\nAPI documentation is available in the `/docs` directory:\n\n```bash\n# Generate API documentation\nnpm run docs\n```\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 📬 Contact\n\nProject Link: [https://github.com/Jktfe/myaimemory-mcp](https://github.com/Jktfe/myaimemory-mcp)\n\n---\n\n<p align=\"center\">\n  Made with ❤️ for the AI community\n</p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jktfe",
        "caching",
        "memory",
        "jktfe myaimemory",
        "management jktfe",
        "memory management"
      ],
      "category": "memory-management"
    },
    "JovanHsu--mcp-neo4j-memory-server": {
      "owner": "JovanHsu",
      "name": "mcp-neo4j-memory-server",
      "url": "https://github.com/JovanHsu/mcp-neo4j-memory-server",
      "imageUrl": "/freedevtools/mcp/pfp/JovanHsu.webp",
      "description": "Store and retrieve information from AI interactions using a Neo4j backend, facilitating advanced graph querying and memory management. Enhances performance and scalability for complex knowledge graph applications.",
      "stars": 17,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T12:23:10Z",
      "readme_content": "# MCP Neo4j Knowledge Graph Memory Server\n\n[![npm version](https://img.shields.io/npm/v/@izumisy/mcp-neo4j-memory-server.svg)](https://www.npmjs.com/package/@izumisy/mcp-neo4j-memory-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-blue)](https://www.typescriptlang.org/)\n[![Neo4j](https://img.shields.io/badge/Neo4j-5.x-brightgreen)](https://neo4j.com/)\n\n## 简介\n\nMCP Neo4j Knowledge Graph Memory Server是一个基于Neo4j图数据库的知识图谱记忆服务器，用于存储和检索AI助手与用户交互过程中的信息。该项目是[官方Knowledge Graph Memory Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory)的增强版本，使用Neo4j作为后端存储引擎。\n\n通过使用Neo4j作为存储后端，本项目提供了更强大的图查询能力、更好的性能和可扩展性，特别适合构建复杂的知识图谱应用。\n\n## 功能特点\n\n- 🚀 基于Neo4j的高性能图数据库存储\n- 🔍 强大的模糊搜索和精确匹配能力\n- 🔄 实体、关系和观察的完整CRUD操作\n- 🌐 与MCP协议完全兼容\n- 📊 支持复杂的图查询和遍历\n- 🐳 Docker支持，便于部署\n\n## 安装\n\n### 前提条件\n\n- Node.js >= 22.0.0\n- Neo4j数据库（本地或远程）\n\n### 通过npm安装\n\n```bash\n# 全局安装\nnpm install -g @jovanhsu/mcp-neo4j-memory-server\n\n# 或作为项目依赖安装\nnpm install @jovanhsu/mcp-neo4j-memory-server\n```\n\n### 使用Docker\n\n```bash\n# 使用docker-compose启动Neo4j和Memory Server\ngit clone https://github.com/JovanHsu/mcp-neo4j-memory-server.git\ncd mcp-neo4j-memory-server\ndocker-compose up -d\n```\n\n### 环境变量配置\n\n服务器使用以下环境变量进行配置：\n\n| 环境变量 | 描述 | 默认值 |\n|----------|------|--------|\n| NEO4J_URI | Neo4j数据库URI | bolt://localhost:7687 |\n| NEO4J_USER | Neo4j用户名 | neo4j |\n| NEO4J_PASSWORD | Neo4j密码 | password |\n| NEO4J_DATABASE | Neo4j数据库名称 | neo4j |\n\n## 与Claude集成\n\n### 在Claude Desktop中配置\n\n在`claude_desktop_config.json`中添加以下配置：\n\n```json\n{\n  \"mcpServers\": {\n    \"graph-memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@izumisy/mcp-neo4j-memory-server\"\n      ],\n      \"env\": {\n        \"NEO4J_URI\": \"neo4j://localhost:7687\",\n        \"NEO4J_USER\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"password\",\n        \"NEO4J_DATABASE\": \"memory\"\n      }\n    }\n  }\n}\n```\n\n### 在Claude Web中使用MCP Inspector\n\n1. 安装[MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n2. 启动Neo4j Memory Server：\n   ```bash\n   npx @jovanhsu/mcp-neo4j-memory-server\n   ```\n3. 在另一个终端启动MCP Inspector：\n   ```bash\n   npx @modelcontextprotocol/inspector npx @jovanhsu/mcp-neo4j-memory-server\n   ```\n4. 在浏览器中访问MCP Inspector界面\n\n## 使用方法\n\n### Claude自定义指令\n\n在Claude的自定义指令中添加以下内容：\n\n```\nFollow these steps for each interaction:\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and search relevant information from your knowledge graph\n   - Create a search query from user words, and search things from \"memory\". If nothing matches, try to break down words in the query at first (\"A B\" to \"A\" and \"B\" for example).\n   - Always refer to your knowledge graph as your \"memory\"\n\n3. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events\n     b) Connect them to the current entities using relations\n     b) Store facts about them as observations\n```\n\n### API示例\n\n如果您想在自己的应用程序中使用本服务器，可以通过MCP协议与其通信：\n\n```typescript\nimport { McpClient } from '@modelcontextprotocol/sdk/client/mcp.js';\nimport { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\n\n// 创建客户端\nconst transport = new StdioClientTransport({\n  command: 'npx',\n  args: ['-y', '@izumisy/mcp-neo4j-memory-server'],\n  env: {\n    NEO4J_URI: 'bolt://localhost:7687',\n    NEO4J_USER: 'neo4j',\n    NEO4J_PASSWORD: 'password',\n    NEO4J_DATABASE: 'neo4j'\n  }\n});\n\nconst client = new McpClient();\nawait client.connect(transport);\n\n// 创建实体\nconst result = await client.callTool('create_entities', {\n  entities: [\n    {\n      name: '用户',\n      entityType: '人物',\n      observations: ['喜欢编程', '使用TypeScript']\n    }\n  ]\n});\n\nconsole.log(result);\n```\n\n## 为什么选择Neo4j？\n\n相比于原始版本使用的JSON文件存储和DuckDB版本，Neo4j提供了以下优势：\n\n1. **原生图数据库**：Neo4j是专为图数据设计的数据库，非常适合知识图谱的存储和查询\n2. **高性能查询**：使用Cypher查询语言可以高效地进行复杂的图遍历和模式匹配\n3. **关系优先**：Neo4j将关系作为一等公民，使得实体间的关系查询更加高效\n4. **可视化能力**：Neo4j提供了内置的可视化工具，方便调试和理解知识图谱\n5. **扩展性**：支持集群部署，可以处理大规模知识图谱\n\n## 实现细节\n\n### 数据模型\n\n知识图谱在Neo4j中的存储模型如下：\n\n```\n(Entity:EntityType {name: \"实体名称\"})\n(Entity)-[:HAS_OBSERVATION]->(Observation {content: \"观察内容\"})\n(Entity1)-[:RELATION_TYPE]->(Entity2)\n```\n\n### 模糊搜索实现\n\n本实现结合了Neo4j的全文搜索功能和Fuse.js进行灵活的实体搜索：\n\n- 使用Neo4j的全文索引进行初步搜索\n- Fuse.js提供额外的模糊匹配能力\n- 搜索结果包括精确和部分匹配，按相关性排序\n\n## 开发\n\n### 环境设置\n\n```bash\n# 克隆仓库\ngit clone https://github.com/JovanHsu/mcp-neo4j-memory-server.git\ncd mcp-neo4j-memory-server\n\n# 安装依赖\npnpm install\n\n# 构建项目\npnpm build\n\n# 开发模式（使用MCP Inspector）\npnpm dev\n```\n\n### 测试\n\n```bash\n# 运行测试\npnpm test\n```\n\n### 发布\n\n```bash\n# 准备发布\nnpm version [patch|minor|major]\n\n# 发布到NPM\nnpm publish\n```\n\n## 贡献指南\n\n欢迎贡献代码、报告问题或提出改进建议！请遵循以下步骤：\n\n1. Fork本仓库\n2. 创建您的特性分支 (`git checkout -b feature/amazing-feature`)\n3. 提交您的更改 (`git commit -m 'Add some amazing feature'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 创建一个Pull Request\n\n## 相关项目\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol/mcp)\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n- [Claude Desktop](https://github.com/anthropics/claude-desktop)\n\n## 许可证\n\n本项目采用MIT许可证 - 详见[LICENSE](LICENSE)文件。\n\n## 联系方式\n\n- GitHub: [https://github.com/JovanHsu/mcp-neo4j-memory-server](https://github.com/JovanHsu/mcp-neo4j-memory-server)\n- NPM: [https://www.npmjs.com/package/@jovanhsu/mcp-neo4j-memory-server](https://www.npmjs.com/package/@jovanhsu/mcp-neo4j-memory-server)\n- 作者: JovanHsu",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neo4j",
        "memory",
        "querying",
        "neo4j memory",
        "using neo4j",
        "neo4j backend"
      ],
      "category": "memory-management"
    },
    "Kirandawadi--volatility3-mcp": {
      "owner": "Kirandawadi",
      "name": "volatility3-mcp",
      "url": "https://github.com/Kirandawadi/volatility3-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Kirandawadi.webp",
      "description": "Analyze memory dumps to detect malware and perform memory forensics using a natural language interface, making the process accessible to users without specialized knowledge.",
      "stars": 12,
      "forks": 1,
      "license": "No License",
      "language": "YARA",
      "updated_at": "2025-07-08T21:51:17Z",
      "readme_content": "# Volatility3 MCP Server\n\n## Introduction\nVolatility3 MCP Server is a powerful tool that connects MCP clients like Claude Desktop with Volatility3, the advanced memory forensics framework. This integration allows LLMs to analyze memory dumps, detect malware, and perform sophisticated memory forensics tasks through a simple, conversational interface.\n\n\n## Demo\n[Demo Video](https://1drv.ms/v/c/b3eb1096e4f4a3a8/EfKIAsM9zUpGtXjJMDn0zywB-R3UnwvYD4yX71q1CinfRw?e=lke0Ox)\n\nYou can also find a [detailed presentation](./attachments/project-presentation.pdf) on this tool here.\n\n## What This Solves\nMemory forensics is a complex field that typically requires specialized knowledge and command-line expertise. This project bridges that gap by:\n- Allowing non-experts to perform memory forensics through natural language\n- Enabling LLMs to directly analyze memory dumps and provide insights\n- Automating common forensic workflows that would normally require multiple manual steps\n- Making memory forensics more accessible and user-friendly\n\n## Features\n- **Memory Dump Analysis**: Analyze Windows and Linux memory dumps using various plugins\n- **Process Inspection**: List running processes, examine their details, and identify suspicious activity\n- **Network Analysis**: Examine network connections to detect command and control servers\n- **Cross-Platform Support**: Works with both Windows and Linux memory dumps (macOS support coming soon)\n- **Malware Detection**: Scan memory with **YARA rules** to identify known malware signatures\n\n## Configuration\n\n1. Clone this repository:\n2. Create a virtual environment:\n   ```bash\n   python -m venv environ\n   source environ/bin/activate\n   ```\n3. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\nYou can use this project in two ways:\n\n### Option 1: With Claude Desktop\n4. Configure Claude Desktop:\n   - Go to `Claude` -> `Settings` -> `Developer` -> `Edit Config` -> `claude_desktop_config.json` and add the following\n     ```json\n        {\n            \"mcpServers\": {\n            \"volatility3\": {\n                \"command\": \"absolute/path/to/virtual/environment/bin/python3\",\n                \"args\": [\n                \"absolute/path/to/bridge_mcp_volatility.py\"\n                ]\n            }\n            }\n        }\n     ```\n        \n5. Restart Claude Desktop and begin analyzing the memory dumps.\n\n### Option 2: With Cursor (SSE Server)\n4. Start the SSE server:\n   ```bash\n   python3 start_sse_server.py\n   ```\n4. Configure Cursor to use the SSE server:\n   - Open Cursor settings\n   - Navigate to `Features` -> `MCP Servers`\n   - Add a new MCP server with the URL `http://127.0.0.1:8080/sse`\n\n6. Use the Cursor Composer in agent mode and begin analyzing memory dumps.\n\n## Available Tools\n\n- **initialize_memory_file**: Set up a memory dump file for analysis\n- **detect_os**: Identify the operating system of the memory dump\n- **list_plugins**: Display all available Volatility3 plugins\n- **get_plugin_info**: Get detailed information about a specific plugin\n- **run_plugin**: Execute any Volatility3 plugin with custom arguments\n- **get_processes**: List all running processes in the memory dump\n- **get_network_connections**: View all network connections from the system\n- **list_process_open_handles**: Examine files and resources accessed by a process\n- **scan_with_yara**: Scan memory for malicious patterns using YARA rules\n\n## Contributing\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "malware",
        "memory",
        "dumps",
        "memory dumps",
        "analyze memory",
        "memory forensics"
      ],
      "category": "memory-management"
    },
    "Lyoneos--mcp-cheatengine-Cto": {
      "owner": "Lyoneos",
      "name": "mcp-cheatengine-Cto",
      "url": "https://github.com/Lyoneos/mcp-cheatengine-Cto",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Connects to CheatEngine instances to perform memory operations, including reading memory and analyzing assembly code. Features a plugin architecture for extending functionalities within the toolkit.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cheatengine",
        "memory",
        "cto",
        "cheatengine cto",
        "cheatengine instances",
        "mcp cheatengine"
      ],
      "category": "memory-management"
    },
    "Srish-ty--MCP-Testing-interface-for-LLMs": {
      "owner": "Srish-ty",
      "name": "MCP-Testing-interface-for-LLMs",
      "url": "https://github.com/Srish-ty/MCP-Testing-interface-for-LLMs",
      "imageUrl": "/freedevtools/mcp/pfp/Srish-ty.webp",
      "description": "Manage user contexts for LLM interactions by storing and retrieving relevant prompts to ensure continuity in conversations. Provides a RESTful API for context management with support for in-memory storage and TypeScript integration.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-30T07:38:26Z",
      "readme_content": "# Memory Context Provider (MCP) Server\n\nA server that manages context for LLM interactions, storing and providing relevant context for each user.\n\n## Features\n\n- In-memory storage of user contexts\n- Context management with last 5 prompts\n- RESTful API endpoints\n- TypeScript support\n\n## Setup\n\n1. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n2. Start the development server:\n   ```bash\n   npm run dev\n   ```\n\n## API Endpoints\n\n### POST /context/:userId\nAdd a new prompt to user's context and get updated context.\n\nRequest body:\n```json\n{\n  \"prompt\": \"Your prompt here\"\n}\n```\n\nResponse:\n```json\n{\n  \"context\": \"Combined context from last 5 prompts\"\n}\n```\n\n### GET /context/:userId\nGet current context for a user.\n\nResponse:\n```json\n{\n  \"context\": \"Current context\"\n}\n```\n\n### DELETE /context/:userId\nClear context for a user.\n\nResponse:\n```json\n{\n  \"message\": \"Context cleared\"\n}\n```\n\n## Development\n\n- `npm run dev`: Start development server with hot reload\n- `npm run build`: Build TypeScript files\n- `npm start`: Run built files ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "llms",
        "llm",
        "contexts",
        "contexts llm",
        "interface llms",
        "context management"
      ],
      "category": "memory-management"
    },
    "T1nker-1220--memories-with-lessons-mcp-server": {
      "owner": "T1nker-1220",
      "name": "memories-with-lessons-mcp-server",
      "url": "https://github.com/T1nker-1220/memories-with-lessons-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/T1nker-1220.webp",
      "description": "This server enables the implementation of persistent memory in AI models through a local knowledge graph, allowing for information retention across chats and an error-learning mechanism via a lesson system.",
      "stars": 53,
      "forks": 13,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T05:05:09Z",
      "readme_content": "# Knowledge Graph Memory Server\n[![smithery badge](https://smithery.ai/badge/@T1nker-1220/memories-with-lessons-mcp-server)](https://smithery.ai/server/@T1nker-1220/memories-with-lessons-mcp-server)\n\nA basic implementation of persistent memory using a local knowledge graph. This lets Claude remember information about the user across chats and learn from past errors through a lesson system.\n\n<a href=\"https://glama.ai/mcp/servers/eoinvr1bz0\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/eoinvr1bz0/badge\" alt=\"Knowledge Graph Memory Server MCP server\" /></a>\n\n## Core Concepts\n\n### Entities\nEntities are the primary nodes in the knowledge graph. Each entity has:\n- A unique name (identifier)\n- An entity type (e.g., \"person\", \"organization\", \"event\")\n- A list of observations\n\nExample:\n```json\n{\n  \"name\": \"John_Smith\",\n  \"entityType\": \"person\",\n  \"observations\": [\"Speaks fluent Spanish\"]\n}\n```\n\n### Relations\nRelations define directed connections between entities. They are always stored in active voice and describe how entities interact or relate to each other.\n\nExample:\n```json\n{\n  \"from\": \"John_Smith\",\n  \"to\": \"Anthropic\",\n  \"relationType\": \"works_at\"\n}\n```\n### Observations\nObservations are discrete pieces of information about an entity. They are:\n\n- Stored as strings\n- Attached to specific entities\n- Can be added or removed independently\n- Should be atomic (one fact per observation)\n\nExample:\n```json\n{\n  \"entityName\": \"John_Smith\",\n  \"observations\": [\n    \"Speaks fluent Spanish\",\n    \"Graduated in 2019\",\n    \"Prefers morning meetings\"\n  ]\n}\n```\n\n### Lessons\nLessons are special entities that capture knowledge about errors and their solutions. Each lesson has:\n- A unique name (identifier)\n- Error pattern information (type, message, context)\n- Solution steps and verification\n- Success rate tracking\n- Environmental context\n- Metadata (severity, timestamps, frequency)\n\nExample:\n```json\n{\n  \"name\": \"NPM_VERSION_MISMATCH_01\",\n  \"entityType\": \"lesson\",\n  \"observations\": [\n    \"Error occurs when using incompatible package versions\",\n    \"Affects Windows environments specifically\",\n    \"Resolution requires version pinning\"\n  ],\n  \"errorPattern\": {\n    \"type\": \"dependency\",\n    \"message\": \"Cannot find package @shadcn/ui\",\n    \"context\": \"package installation\"\n  },\n  \"metadata\": {\n    \"severity\": \"high\",\n    \"environment\": {\n      \"os\": \"windows\",\n      \"nodeVersion\": \"18.x\"\n    },\n    \"createdAt\": \"2025-02-13T13:21:58.523Z\",\n    \"updatedAt\": \"2025-02-13T13:22:21.336Z\",\n    \"frequency\": 1,\n    \"successRate\": 1.0\n  },\n  \"verificationSteps\": [\n    {\n      \"command\": \"pnpm add shadcn@latest\",\n      \"expectedOutput\": \"Successfully installed shadcn\",\n      \"successIndicators\": [\"added shadcn\"]\n    }\n  ]\n}\n```\n\n## API\n\n### Tools\n- **create_entities**\n  - Create multiple new entities in the knowledge graph\n  - Input: `entities` (array of objects)\n    - Each object contains:\n      - `name` (string): Entity identifier\n      - `entityType` (string): Type classification\n      - `observations` (string[]): Associated observations\n  - Ignores entities with existing names\n\n- **create_relations**\n  - Create multiple new relations between entities\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type in active voice\n  - Skips duplicate relations\n\n- **add_observations**\n  - Add new observations to existing entities\n  - Input: `observations` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `contents` (string[]): New observations to add\n  - Returns added observations per entity\n  - Fails if entity doesn't exist\n\n- **delete_entities**\n  - Remove entities and their relations\n  - Input: `entityNames` (string[])\n  - Cascading deletion of associated relations\n  - Silent operation if entity doesn't exist\n\n- **delete_observations**\n  - Remove specific observations from entities\n  - Input: `deletions` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `observations` (string[]): Observations to remove\n  - Silent operation if observation doesn't exist\n\n- **delete_relations**\n  - Remove specific relations from the graph\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n  - Silent operation if relation doesn't exist\n\n- **read_graph**\n  - Read the entire knowledge graph\n  - No input required\n  - Returns complete graph structure with all entities and relations\n\n- **search_nodes**\n  - Search for nodes based on query\n  - Input: `query` (string)\n  - Searches across:\n    - Entity names\n    - Entity types\n    - Observation content\n  - Returns matching entities and their relations\n\n- **open_nodes**\n  - Retrieve specific nodes by name\n  - Input: `names` (string[])\n  - Returns:\n    - Requested entities\n    - Relations between requested entities\n  - Silently skips non-existent nodes\n\n### Lesson Management Tools\n- **create_lesson**\n  - Create a new lesson from an error and its solution\n  - Input: `lesson` (object)\n    - Contains:\n      - `name` (string): Unique identifier\n      - `entityType` (string): Must be \"lesson\"\n      - `observations` (string[]): Notes about the error and solution\n      - `errorPattern` (object): Error details\n        - `type` (string): Category of error\n        - `message` (string): Error message\n        - `context` (string): Where error occurred\n        - `stackTrace` (string, optional): Stack trace\n      - `metadata` (object): Additional information\n        - `severity` (\"low\" | \"medium\" | \"high\" | \"critical\")\n        - `environment` (object): System details\n        - `frequency` (number): Times encountered\n        - `successRate` (number): Solution success rate\n      - `verificationSteps` (array): Solution verification\n        - Each step contains:\n          - `command` (string): Action to take\n          - `expectedOutput` (string): Expected result\n          - `successIndicators` (string[]): Success markers\n  - Automatically initializes metadata timestamps\n  - Validates all required fields\n\n- **find_similar_errors**\n  - Find similar errors and their solutions\n  - Input: `errorPattern` (object)\n    - Contains:\n      - `type` (string): Error category\n      - `message` (string): Error message\n      - `context` (string): Error context\n  - Returns matching lessons sorted by success rate\n  - Uses fuzzy matching for error messages\n\n- **update_lesson_success**\n  - Update success tracking for a lesson\n  - Input:\n    - `lessonName` (string): Lesson to update\n    - `success` (boolean): Whether solution worked\n  - Updates:\n    - Success rate (weighted average)\n    - Frequency counter\n    - Last update timestamp\n\n- **get_lesson_recommendations**\n  - Get relevant lessons for current context\n  - Input: `context` (string)\n  - Searches across:\n    - Error type\n    - Error message\n    - Error context\n    - Lesson observations\n  - Returns lessons sorted by:\n    - Context relevance\n    - Success rate\n  - Includes full solution details\n\n## File Management\nThe server now handles two types of files:\n- `memory.json`: Stores basic entities and relations\n- `lesson.json`: Stores lesson entities with error patterns\n\nFiles are automatically split if they exceed 1000 lines to maintain performance.\n\n## Cursor MCP Client Setup\n\nTo integrate this memory server with Cursor MCP client, follow these steps:\n\n1. Clone the Repository:\n```bash\ngit clone [repository-url]\ncd [repository-name]\n```\n\n2. Install Dependencies:\n```bash\npnpm install\n```\n\n3. Build the Project:\n```bash\npnpm build\n```\n\n4. Configure the Server:\n- Locate the full path to the built server file: `/path/to/the/dist/index.js`\n- Start the server using Node.js: `node /path/to/the/dist/index.js`\n\n5. Activate in Cursor:\n- Use the keyboard shortcut `Ctrl+Shift+P`\n- Type \"reload window\" and select it\n- Wait a few seconds for the MCP server to activate\n- Select the stdio type when prompted\n\nThe memory server should now be integrated with your Cursor MCP client and ready to use.\n\n\n# Usage with Claude Desktop\n\n### Setup\n\nAdd this to your claude_desktop_config.json:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"-v\", \"claude-memory:/app/dist\", \"--rm\", \"mcp/memory\"]\n    }\n  }\n}\n```\n\n#### NPX\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-memory\"\n      ]\n    }\n  }\n}\n```\n\n#### NPX with custom setting\n\nThe server can be configured using the following environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-memory\"\n      ],\n      \"env\": {\n        \"MEMORY_FILE_PATH\": \"/path/to/custom/memory.json\"\n      }\n    }\n  }\n}\n```\n\n- `MEMORY_FILE_PATH`: Path to the memory storage JSON file (default: `memory.json` in the server directory)\n\n### System Prompt\n\nThe prompt for utilizing memory depends on the use case. Changing the prompt will help the model determine the frequency and types of memories created.\n\nHere is an example prompt for chat personalization. You could use this prompt in the \"Custom Instructions\" field of a [Claude.ai Project](https://www.anthropic.com/news/projects).\n\n```\nFollow these steps for each interaction:\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and retrieve all relevant information from your knowledge graph\n   - Always refer to your knowledge graph as your \"memory\"\n\n3. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events\n     b) Connect them to the current entities using relations\n     b) Store facts about them as observations\n```\n\n## Building\n\nDocker:\n\n```sh\ndocker build -t mcp/memory -f src/memory/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## New Tools\n\n- **create_lesson**\n  - Create a new lesson from an error and its solution\n  - Input: `lesson` (object)\n    - Contains error pattern, solution steps, and metadata\n    - Automatically tracks creation time and updates\n    - Verifies solution steps are complete\n\n- **find_similar_errors**\n  - Find similar errors and their solutions\n  - Input: `errorPattern` (object)\n    - Contains error type, message, and context\n    - Returns matching lessons sorted by success rate\n    - Includes related solutions and verification steps\n\n- **update_lesson_success**\n  - Update success tracking for a lesson\n  - Input:\n    - `lessonName` (string): Lesson to update\n    - `success` (boolean): Whether solution worked\n  - Updates success rate and frequency metrics\n\n- **get_lesson_recommendations**\n  - Get relevant lessons for current context\n  - Input: `context` (string)\n  - Returns lessons sorted by relevance and success rate\n  - Includes full solution details and verification steps\n\n\n# BIG CREDITS TO THE OWNER OF THIS REPO FOR THE BASE CODE I ENHANCED IT WITH LESSONS AND FILE MANAGEMENT\nBig thanks!\nhttps://github.com/modelcontextprotocol/servers\njerome3o-anthropic\nhttps://github.com/modelcontextprotocol/servers/tree/main/src/memory\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "memories",
        "persistent",
        "memory ai",
        "persistent memory",
        "memory management"
      ],
      "category": "memory-management"
    },
    "Thelyoncrypt--MemGPT": {
      "owner": "Thelyoncrypt",
      "name": "MemGPT",
      "url": "https://github.com/Thelyoncrypt/MemGPT",
      "imageUrl": "/freedevtools/mcp/pfp/Thelyoncrypt.webp",
      "description": "Creates chatbots that maintain self-editing memory with different memory tiers to manage limited LLM context windows. Connects to SQL databases, local files, and documents for seamless conversational AI interactions.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-11-06T21:09:05Z",
      "readme_content": "<a href=\"#user-content-memgpt\"><img src=\"https://memgpt.ai/assets/img/memgpt_logo_circle.png\" alt=\"MemGPT logo\" width=\"75\" align=\"right\"></a>\r\n\r\n# [MemGPT](https://memgpt.ai)\r\n\r\n<div align=\"center\">\r\n\r\n <strong>Try out our MemGPT chatbot on <a href=\"https://discord.gg/9GEQrxmVyE\">Discord</a>!</strong>\r\n \r\n[![Discord](https://img.shields.io/discord/1161736243340640419?label=Discord&logo=discord&logoColor=5865F2&style=flat-square&color=5865F2)](https://discord.gg/9GEQrxmVyE)\r\n[![arXiv 2310.08560](https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&style=flat-square)](https://arxiv.org/abs/2310.08560)\r\n\r\n</div>\r\n\r\n<details open>\r\n  <summary><h2>🤖 Create perpetual chatbots with self-editing memory!</h1></summary>\r\n  <div align=\"center\">\r\n    <br>\r\n    <img src=\"https://memgpt.ai/assets/img/demo.gif\" alt=\"MemGPT demo video\" width=\"800\">\r\n  </div>\r\n</details>\r\n\r\n<details>\r\n <summary><h2>🗃️ Chat with your data - talk to your SQL database or your local files!</strong></h2></summary>\r\n  <strong>SQL Database</strong>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/sql_demo.gif\" alt=\"MemGPT demo video for sql search\" width=\"800\">\r\n  </div>\r\n  <strong>Local files</strong>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/preload_archival_demo.gif\" alt=\"MemGPT demo video for sql search\" width=\"800\">\r\n  </div>\r\n</details>\r\n\r\n<details>\r\n  <summary><h2>📄 You can also talk to docs - for example ask about <a href=\"memgpt/personas/examples/docqa\">LlamaIndex</a>!</h1></summary>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/docqa_demo.gif\" alt=\"MemGPT demo video for llamaindex api docs search\" width=\"800\">\r\n  </div>\r\n  <details>\r\n  <summary><b>ChatGPT (GPT-4) when asked the same question:</b></summary>\r\n    <div align=\"center\">\r\n      <img src=\"https://memgpt.ai/assets/img/llama_index_gpt4.png\" alt=\"GPT-4 when asked about llamaindex api docs\" width=\"800\">\r\n    </div>\r\n    (Question from https://github.com/run-llama/llama_index/issues/7756)\r\n  </details>\r\n</details>\r\n\r\n## Quick setup \r\n\r\nJoin <a href=\"https://discord.gg/9GEQrxmVyE\">Discord</a></strong> and message the MemGPT bot (in the `#memgpt` channel). Then run the following commands (messaged to \"MemGPT Bot\"): \r\n* `/profile` (to create your profile)\r\n* `/key` (to enter your OpenAI key)\r\n* `/create` (to create a MemGPT chatbot)\r\n\r\nMake sure your privacy settings on this server are open so that MemGPT Bot can DM you: \\\r\nMemGPT → Privacy Settings → Direct Messages set to ON\r\n<div align=\"center\">\r\n <img src=\"https://memgpt.ai/assets/img/discord/dm_settings.png\" alt=\"set DMs settings on MemGPT server to be open in MemGPT so that MemGPT Bot can message you\" width=\"400\">\r\n</div>\r\n\r\nYou can see the full list of available commands when you enter `/` into the message box. \r\n<div align=\"center\">\r\n <img src=\"https://memgpt.ai/assets/img/discord/slash_commands.png\" alt=\"MemGPT Bot slash commands\" width=\"400\">\r\n</div>\r\n\r\n## What is MemGPT? \r\n\r\nMemory-GPT (or MemGPT in short) is a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM's limited context window. For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations. Learn more about MemGPT in our [paper](https://arxiv.org/abs/2310.08560). \r\n\r\n## Running MemGPT Locally \r\n\r\nInstall dependencies:\r\n\r\n```sh\r\npip install -r requirements.txt\r\n```\r\n\r\nAdd your OpenAI API key to your environment:\r\n\r\n```sh\r\nexport OPENAI_API_KEY=YOUR_API_KEY\r\n```\r\n\r\nTo run MemGPT for as a conversation agent in CLI mode, simply run `main.py`:\r\n\r\n```sh\r\npython3 main.py\r\n```\r\n\r\nTo create a new starter user or starter persona (that MemGPT gets initialized with), create a new `.txt` file in [/memgpt/humans/examples](/memgpt/humans/examples) or [/memgpt/personas/examples](/memgpt/personas/examples), then use the `--persona` or `--human` flag when running `main.py`. For example:\r\n\r\n```sh\r\n# assuming you created a new file /memgpt/humans/examples/me.txt\r\npython main.py --human me.txt\r\n```\r\n\r\n### `main.py` flags\r\n\r\n```text\r\n--persona\r\n  load a specific persona file\r\n--human\r\n  load a specific human file\r\n--first\r\n  allows you to send the first message in the chat (by default, MemGPT will send the first message)\r\n--debug\r\n  enables debugging output\r\n--archival_storage_faiss_path=<ARCHIVAL_STORAGE_FAISS_PATH>\r\n  load in document database (backed by FAISS index)\r\n--archival_storage_files=\"<ARCHIVAL_STORAGE_FILES_GLOB>\"\r\n  pre-load files into archival memory\r\n--archival_storage_sqldb=<SQLDB_PATH>\r\n  load in SQL database\r\n```\r\n\r\n### Interactive CLI commands\r\n\r\nWhile using MemGPT via the CLI you can run various commands:\r\n\r\n```text\r\n/exit\r\n  exit the CLI\r\n/save\r\n  save a checkpoint of the current agent/conversation state\r\n/load\r\n  load a saved checkpoint\r\n/dump\r\n  view the current message log (see the contents of main context)\r\n/memory\r\n  print the current contents of agent memory\r\n/pop\r\n  undo the last message in the conversation\r\n/heartbeat\r\n  send a heartbeat system message to the agent\r\n/memorywarning\r\n  send a memory warning system message to the agent\r\n```\r\n\r\n## Use MemGPT to talk to your Database!\r\n\r\nMemGPT's archival memory let's you load your database and talk to it! To motivate this use-case, we have included a toy example. \r\n\r\nConsider the `test.db` already included in the repository.\r\n\r\nid\t| name |\tage\r\n--- | --- | ---\r\n1\t| Alice |\t30\r\n2\t| Bob\t | 25\r\n3\t| Charlie |\t35\r\n\r\nTo talk to this database, run:\r\n\r\n```sh\r\npython main_db.py  --archival_storage_sqldb=memgpt/personas/examples/sqldb/test.db\r\n```\r\n\r\nAnd then you can input the path to your database, and your query.\r\n\r\n```python\r\nPlease enter the path to the database. test.db\r\n...\r\nEnter your message: How old is Bob?\r\n...\r\n🤖 Bob is 25 years old.\r\n```\r\n\r\n\r\n### Support\r\n\r\n* By default MemGPT will use `gpt-4`, so your API key will require `gpt-4` API access.\r\n\r\nIf you have any further questions, or have anything to share, we are excited to hear your feedback!\r\n\r\n* For issues and feature requests, please [open a GitHub issue](https://github.com/cpacker/MemGPT/issues).\r\n\r\n### Datasets\r\nDatasets used in our [paper](https://arxiv.org/abs/2310.08560) can be downloaded at [HuggingFace](https://huggingface.co/MemGPT).\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memgpt",
        "thelyoncrypt",
        "chatbots",
        "thelyoncrypt memgpt",
        "memgpt creates",
        "chatbots maintain"
      ],
      "category": "memory-management"
    },
    "TrackerXXX23--dev_memory_mcp": {
      "owner": "TrackerXXX23",
      "name": "dev_memory_mcp",
      "url": "https://github.com/TrackerXXX23/dev_memory_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Captures and organizes development context, tracking code changes and user interactions across multiple projects. Provides persistent memory for a more effective coding experience.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dev_memory_mcp",
        "memory",
        "trackerxxx23",
        "trackerxxx23 dev_memory_mcp",
        "dev_memory_mcp captures",
        "memory management"
      ],
      "category": "memory-management"
    },
    "Vic563--Memgpt-MCP-Server": {
      "owner": "Vic563",
      "name": "Memgpt-MCP-Server",
      "url": "https://github.com/Vic563/Memgpt-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/Vic563.webp",
      "description": "Implements a memory system for large language models (LLMs) with capabilities for chatting, retrieving conversation history, and switching between multiple LLM providers.",
      "stars": 27,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-12T19:32:51Z",
      "readme_content": "# MemGPT MCP Server\n\nA TypeScript-based MCP server that implements a memory system for LLMs. It provides tools for chatting with different LLM providers while maintaining conversation history.\n\n## Features\n\n### Tools\n- `chat` - Send a message to the current LLM provider\n  - Takes a message parameter\n  - Supports multiple providers (OpenAI, Anthropic, OpenRouter, Ollama)\n\n- `get_memory` - Retrieve conversation history\n  - Optional `limit` parameter to specify number of memories to retrieve\n  - Pass `limit: null` for unlimited memory retrieval\n  - Returns memories in chronological order with timestamps\n\n- `clear_memory` - Clear conversation history\n  - Removes all stored memories\n\n- `use_provider` - Switch between different LLM providers\n  - Supports OpenAI, Anthropic, OpenRouter, and Ollama\n  - Persists provider selection\n\n- `use_model` - Switch to a different model for the current provider\n  - Supports provider-specific models:\n    - Anthropic Claude Models:\n      - Claude 3 Series:\n        - `claude-3-haiku`: Fastest response times, ideal for tasks like customer support and content moderation\n        - `claude-3-sonnet`: Balanced performance for general-purpose use\n        - `claude-3-opus`: Advanced model for complex reasoning and high-performance tasks\n      - Claude 3.5 Series:\n        - `claude-3.5-haiku`: Enhanced speed and cost-effectiveness\n        - `claude-3.5-sonnet`: Superior performance with computer interaction capabilities\n    - OpenAI: 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo'\n    - OpenRouter: Any model in 'provider/model' format (e.g., 'openai/gpt-4', 'anthropic/claude-2')\n    - Ollama: Any locally available model (e.g., 'llama2', 'codellama')\n  - Persists model selection\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"letta-memgpt\": {\n      \"command\": \"/path/to/memgpt-server/build/index.js\",\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-key\",\n        \"ANTHROPIC_API_KEY\": \"your-anthropic-key\",\n        \"OPENROUTER_API_KEY\": \"your-openrouter-key\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n- `OPENAI_API_KEY` - Your OpenAI API key\n- `ANTHROPIC_API_KEY` - Your Anthropic API key\n- `OPENROUTER_API_KEY` - Your OpenRouter API key\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Recent Updates\n\n### Claude 3 and 3.5 Series Support (March 2024)\n- Added support for latest Claude models:\n  - Claude 3 Series (Haiku, Sonnet, Opus)\n  - Claude 3.5 Series (Haiku, Sonnet)\n\n### Unlimited Memory Retrieval\n- Added support for retrieving unlimited conversation history\n- Use `{ \"limit\": null }` with the `get_memory` tool to retrieve all stored memories\n- Use `{ \"limit\": n }` to retrieve the n most recent memories\n- Default limit is 10 if not specified\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "memgpt",
        "llms",
        "memory management",
        "memgpt mcp",
        "mcp server"
      ],
      "category": "memory-management"
    },
    "WhenMoon-afk--claude-memory-mcp": {
      "owner": "WhenMoon-afk",
      "name": "claude-memory-mcp",
      "url": "https://github.com/WhenMoon-afk/claude-memory-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/WhenMoon-afk.webp",
      "description": "Enhances Large Language Models with persistent memory capabilities, allowing for the storage, retrieval, and management of memories across conversations. Integrates with the Claude desktop application, supporting various memory types and semantic search.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T11:43:08Z",
      "readme_content": "# Claude Memory MCP Server\n\nAn MCP (Model Context Protocol) server implementation that provides persistent memory capabilities for Large Language Models, specifically designed to integrate with the Claude desktop application.\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n\n## Overview\n\nThis project implements optimal memory techniques based on comprehensive research of current approaches in the field. It provides a standardized way for Claude to maintain persistent memory across conversations and sessions.\n\n## Features\n\n- **Tiered Memory Architecture**: Short-term, long-term, and archival memory tiers\n- **Multiple Memory Types**: Support for conversations, knowledge, entities, and reflections\n- **Semantic Search**: Retrieve memories based on semantic similarity\n- **Automatic Memory Management**: Intelligent memory capture without explicit commands\n- **Memory Consolidation**: Automatic consolidation of short-term memories into long-term memory\n- **Memory Management**: Importance-based memory retention and forgetting\n- **Claude Integration**: Ready-to-use integration with Claude desktop application\n- **MCP Protocol Support**: Compatible with the Model Context Protocol\n- **Docker Support**: Easy deployment using Docker containers\n\n## Quick Start\n\n### Option 1: Using Docker (Recommended)\n\n```bash\n# Clone the repository\ngit clone https://github.com/WhenMoon-afk/claude-memory-mcp.git\ncd claude-memory-mcp\n\n# Start with Docker Compose\ndocker-compose up -d\n```\n\nConfigure Claude Desktop to use the containerized MCP server (see [Docker Usage Guide](docs/docker_usage.md) for details).\n\n### Option 2: Standard Installation\n\n1. **Prerequisites**:\n   - Python 3.8-3.12\n   - pip package manager\n\n2. **Installation**:\n   ```bash\n   # Clone the repository\n   git clone https://github.com/WhenMoon-afk/claude-memory-mcp.git\n   cd claude-memory-mcp\n   \n   # Install dependencies\n   pip install -r requirements.txt\n   \n   # Run setup script\n   chmod +x setup.sh\n   ./setup.sh\n   ```\n\n3. **Claude Desktop Integration**:\n\n   Add the following to your Claude configuration file:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"memory\": {\n         \"command\": \"python\",\n         \"args\": [\"-m\", \"memory_mcp\"],\n         \"env\": {\n           \"MEMORY_FILE_PATH\": \"/path/to/your/memory.json\"\n         }\n       }\n     }\n   }\n   ```\n\n## Using Memory with Claude\n\nThe Memory MCP Server enables Claude to remember information across conversations without requiring explicit commands. \n\n1. **Automatic Memory**: Claude will automatically:\n   - Remember important details you share\n   - Store user preferences and facts\n   - Recall relevant information when needed\n\n2. **Memory Recall**: To see what Claude remembers, simply ask:\n   - \"What do you remember about me?\"\n   - \"What do you know about my preferences?\"\n\n3. **System Prompt**: For optimal memory usage, add this to your Claude system prompt:\n\n   ```\n   This Claude instance has been enhanced with persistent memory capabilities.\n   Claude will automatically remember important details about you across\n   conversations and recall them when relevant, without needing explicit commands.\n   ```\n\nSee the [User Guide](docs/user_guide.md) for detailed usage instructions and examples.\n\n## Documentation\n\n- [User Guide](docs/user_guide.md)\n- [Docker Usage Guide](docs/docker_usage.md)\n- [Compatibility Guide](docs/compatibility.md)\n- [Architecture](docs/architecture.md)\n- [Claude Integration Guide](docs/claude_integration.md)\n\n## Examples\n\nThe `examples` directory contains scripts demonstrating how to interact with the Memory MCP Server:\n\n- `store_memory_example.py`: Example of storing a memory\n- `retrieve_memory_example.py`: Example of retrieving memories\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check the [Compatibility Guide](docs/compatibility.md) for dependency requirements\n2. Ensure your Python version is 3.8-3.12\n3. For NumPy issues, use: `pip install \"numpy>=1.20.0,<2.0.0\"`\n4. Try using Docker for simplified deployment\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "memories",
        "retrieval",
        "memory management",
        "memory mcp",
        "persistent memory"
      ],
      "category": "memory-management"
    },
    "ZannyTornadoCoding--my-sequential-thinking-mcp-server": {
      "owner": "ZannyTornadoCoding",
      "name": "my-sequential-thinking-mcp-server",
      "url": "https://github.com/ZannyTornadoCoding/my-sequential-thinking-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ZannyTornadoCoding.webp",
      "description": "Facilitates structured sequential thinking by breaking down complex problems into logical steps, managing reasoning chains, and visualizing thinking pathways. Integrates with a Memory Bank for storing and retrieving thought processes, while providing tools for reasoning validation and analysis.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-05T01:40:56Z",
      "readme_content": "# Sequential Thinking MCP Server\n\nA Model Context Protocol (MCP) server focused on structured sequential thinking capabilities, designed to integrate with Cline's Memory Bank. This server helps break down complex problems into structured sequential steps, track reasoning chains, and store thinking patterns.\n\n## Features\n\n- Create and manage sequential thinking chains for problem-solving\n- Track chains of thought with validation at each step\n- Store and retrieve reasoning patterns\n- Analyze the quality of reasoning processes\n- Visualize thinking pathways\n- Seamlessly integrate with the Memory Bank system\n\n## Architecture\n\nThe server consists of the following core components:\n\n- **Sequential Thinking Engine**: Manages thinking chains, steps, and reasoning validation\n- **Memory Bank Connector**: Integrates with Cline's Memory Bank\n- **Tag Manager**: Implements a comprehensive tagging system\n- **Visualization Generator**: Creates visual representations of thinking chains\n- **Utilities**: File storage, thinking validation, and other helpers\n\n## Available Tools\n\nThe server provides the following MCP tools:\n\n### create_thinking_chain\nCreate a new sequential thinking process with specified parameters.\n- **Input**: problem description, thinking type, context\n- **Output**: chain_id and initial structure\n\n### add_thinking_step\nAdd a step to an existing thinking chain.\n- **Input**: chain_id, step description, reasoning, evidence\n- **Output**: updated step information\n\n### validate_step\nValidate logical connections between steps.\n- **Input**: chain_id, step_id\n- **Output**: validation results, potential issues\n\n### get_chain\nRetrieve a complete thinking chain.\n- **Input**: chain_id\n- **Output**: full chain with all steps\n\n### generate_visualization\nCreate visual representation of a thinking chain.\n- **Input**: chain_id, format (mermaid, json, text)\n- **Output**: visualization code/data\n\n### save_to_memory\nSave a thinking chain to Memory Bank.\n- **Input**: chain_id, memory_name, tags\n- **Output**: confirmation and memory_id\n\n### load_from_memory\nLoad a thinking chain from Memory Bank.\n- **Input**: memory_id or search parameters\n- **Output**: complete chain\n\n### search_related_thinking\nFind related thinking chains based on parameters.\n- **Input**: keywords, tags, thinking_type\n- **Output**: list of relevant chains\n\n### apply_template\nApply a reasoning template to current thinking.\n- **Input**: template_name, problem_context\n- **Output**: pre-structured thinking chain\n\n## Thinking Types\n\nThe server supports various thinking types, each with specific patterns and structures:\n\n- **Analytical** - Break down, analyze, synthesize\n- **Creative** - Diverge, explore, converge\n- **Critical** - Question, evaluate, conclude\n- **Systems** - Map, analyze, model\n- **First-Principles** - Identify, break down, reassemble\n- **Divergent** - Generate alternatives, explore\n- **Convergent** - Analyze, evaluate, select\n- **Inductive** - Observe, pattern, hypothesize\n- **Deductive** - Premise, logic, conclusion\n\n## Templates\n\nThe server includes ready-to-use reasoning templates to jumpstart the thinking process:\n\n- **First Principles Analysis** - Break down a complex problem into its fundamental principles\n- **Systems Thinking Analysis** - Analyze complex systems holistically\n\n## Installation\n\n1. Ensure Node.js v14+ is installed\n2. Clone the repository\n3. Install dependencies:\n   ```\n   npm install\n   ```\n\n## Usage\n\n1. Start the server:\n   ```\n   node index.js\n   ```\n\n2. The server will be available as an MCP server that you can connect to via Claude/Cline\n\n## Memory Bank Integration\n\nThis server is designed to integrate with Cline's Memory Bank, allowing:\n\n1. Reading from Memory Bank files (projectbrief.md, activeContext.md, etc.)\n2. Storing complete thinking chains as structured memories\n3. Updating activeContext.md with reasoning outcomes\n4. Creating links between reasoning and Memory Bank sections\n\n## Example Tool Usage\n\n```javascript\n// Example: Create a new thinking chain\n{\n  \"problem\": \"How to improve user engagement on our platform\",\n  \"thinking_type\": \"systems\",\n  \"context\": \"Our user engagement metrics have decreased by 15% over the past quarter\"\n}\n\n// Example: Add a thinking step\n{\n  \"chain_id\": \"3a7e4fc0-5c1d-4b9f-9d1a-8b5e7c5a9d3e\",\n  \"description\": \"Identify key components of the engagement system\",\n  \"reasoning\": \"User engagement consists of several interconnected components including onboarding, core user actions, notification systems, and retention mechanisms.\",\n  \"evidence\": \"Analysis of our user journey maps and analytics data\",\n  \"confidence\": 0.8\n}\n\n// Example: Generate a visualization\n{\n  \"chain_id\": \"3a7e4fc0-5c1d-4b9f-9d1a-8b5e7c5a9d3e\",\n  \"format\": \"mermaid\",\n  \"options\": {\n    \"showValidation\": true,\n    \"showConfidence\": true\n  }\n}\n```\n\n## Tag System\n\nThe server implements a comprehensive tagging system with multiple dimensions:\n\n- **Thinking Type** - analytical, creative, critical, systems, etc.\n- **Domain** - business, science, technology, art, etc.\n- **Complexity** - simple, moderate, complex\n- **Status** - draft, validated, complete\n- **Custom** - user-defined tags\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sequential",
        "thinking",
        "memory",
        "sequential thinking",
        "thinking pathways",
        "managing reasoning"
      ],
      "category": "memory-management"
    },
    "a2888409--CSAPP": {
      "owner": "a2888409",
      "name": "CSAPP",
      "url": "https://github.com/a2888409/CSAPP",
      "imageUrl": "/freedevtools/mcp/pfp/a2888409.webp",
      "description": "A high-performance HTTP server utilizing the epoll model for efficient connection and task management, supporting event-driven architecture and timer management for handling inactive connections. Includes implementations of memory allocation, a simple proxy server, and a basic shell for process management and signal handling.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "C",
      "updated_at": "2021-09-03T11:30:14Z",
      "readme_content": "# 基于epoll模型的http服务器 + CSAPP_lab\n基于epoll模型的http服务器 + CSAPP一书配套的实验中，其中3个经典实验的源码<br>\n###http：基于epoll模型的http服务器<br>\n采用epoll模型，实现了统一事件源，并通过时间堆管理定时器回收非活动连接。<br>\n通过一个线程池实现对任务的处理，然后使用状态机解析HTTP报文，请求了静态文件。<br>\n<br>\n###malloclab-handout：基于分离适配算法的内存分配器<br>\n采用双向链表结构维护分配器，每次分配一个内存块时，通过链表头指针查找到一个大小合适的块，并进行可选的分割。性能较隐式空闲链表分配器提升了大约20%。\n<br>\n###proxylab-handout：实现了一个简单的代理程序\n<br>\n###shlab-handout：Tiny Shell<br>\n实现了一个简易shell程序，主要涉及进程管理和信号处理。定义了一个数据结构管理job，实现了job的add，delete，fg，bg等功能。并正确的处理了SIGINT，SIGCHLD，SIGTSTP信号。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "csapp",
        "http",
        "server",
        "http server",
        "performance http",
        "a2888409 csapp"
      ],
      "category": "memory-management"
    },
    "aakarsh-sasi--memory-bank-mcp": {
      "owner": "aakarsh-sasi",
      "name": "memory-bank-mcp",
      "url": "https://github.com/aakarsh-sasi/memory-bank-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/aakarsh-sasi.webp",
      "description": "Manage AI assistant's context across sessions by storing, retrieving, and tracking information with remote server support for improved collaboration and persistence.",
      "stars": 32,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T11:12:08Z",
      "readme_content": "# Memory Bank MCP With Remote SSH Support 🧠\n\n[![NPM Version](https://img.shields.io/npm/v/@aakarsh-sasi/memory-bank-mcp.svg)](https://www.npmjs.com/package/@aakarsh-sasi/memory-bank-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/movibe/memory-bank-mcp/actions/workflows/test.yml/badge.svg)](https://github.com/movibe/memory-bank-mcp/actions/workflows/test.yml)\n[![smithery badge](https://smithery.ai/badge/@aakarsh-sasi/memory-bank-mcp)](https://smithery.ai/server/@aakarsh-sasi/memory-bank-mcp)\n\nA Model Context Protocol (MCP) server for managing Memory Banks, allowing AI assistants to store and retrieve information across sessions. Now with remote server support!\n\n<a href=\"https://glama.ai/mcp/servers/@aakarsh-sasi/memory-bank-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@aakarsh-sasi/memory-bank-mcp/badge\" alt=\"Memory Bank MCP server\" />\n</a>\n\n## Overview 📋\n\nMemory Bank Server provides a set of tools and resources for AI assistants to interact with Memory Banks. Memory Banks are structured repositories of information that help maintain context and track progress across multiple sessions.\n\n## Features ✨\n\n- **Memory Bank Management**: Initialize, find, and manage Memory Banks\n- **File Operations**: Read and write files in Memory Banks\n- **Progress Tracking**: Track progress and update Memory Bank files\n- **Decision Logging**: Log important decisions with context and alternatives\n- **Active Context Management**: Maintain and update active context information\n- **Mode Support**: Detect and use .clinerules files for mode-specific behavior\n- **UMB Command**: Update Memory Bank files temporarily with the UMB command\n- **Robust Error Handling**: Gracefully handle errors and continue operation when possible\n- **Status Prefix System**: Immediate visibility into Memory Bank operational state\n- **Remote Server Support**: Store Memory Banks on a remote server using SSH\n\n## Directory Structure 📁\n\nBy default, Memory Bank uses a `memory-bank` directory in the root of your project. When you specify a project path using the `--path` option, the Memory Bank will be created or accessed at `<project_path>/memory-bank`.\n\nYou can customize the name of the Memory Bank folder using the `--folder` option. For example, if you set `--folder custom-memory`, the Memory Bank will be created or accessed at `<project_path>/custom-memory`.\n\nFor more details on customizing the folder name, see [Custom Memory Bank Folder Name](docs/custom-folder-name.md).\n\n## Recent Improvements 🛠️\n\n- **Remote Server Support**: Store your Memory Bank on a remote server via SSH\n- **Customizable Folder Name**: You can now specify a custom folder name for the Memory Bank\n- **Consistent Directory Structure**: Memory Bank now always uses the configured folder name in the project root\n- **Enhanced Initialization**: Memory Bank now works even when .clinerules files don't exist\n- **Better Path Handling**: Improved handling of absolute and relative paths\n- **Improved Directory Detection**: Better detection of existing memory-bank directories\n- **More Robust Error Handling**: Graceful handling of errors related to .clinerules files\n\nFor more details, see [Memory Bank Bug Fixes](docs/memory-bank-bug-fixes.md).\n\n## Installation 🚀\n\n### Installing via Smithery\n\nTo install Memory Bank for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@aakarsh-sasi/memory-bank-mcp):\n\n```bash\nnpx -y @smithery/cli install @aakarsh-sasi/memory-bank-mcp --client claude\n```\n\n### Manual Installation\n```bash\n# Install from npm\nnpm install @aakarsh-sasi/memory-bank-mcp\n\n# Or install globally\nnpm install -g @aakarsh-sasi/memory-bank-mcp\n\n# Or run directly with npx (no installation required)\nnpx @aakarsh-sasi/memory-bank-mcp\n```\n\n## Usage with npx 💻\n\nYou can run Memory Bank MCP directly without installation using npx:\n\n```bash\n# Run with default settings\nnpx @aakarsh-sasi/memory-bank-mcp\n\n# Run with specific mode\nnpx @aakarsh-sasi/memory-bank-mcp --mode code\n\n# Run with custom project path\nnpx @aakarsh-sasi/memory-bank-mcp --path /path/to/project\n\n# Run with custom folder name\nnpx @aakarsh-sasi/memory-bank-mcp --folder custom-memory-bank\n\n# Run with remote server\nnpx @aakarsh-sasi/memory-bank-mcp --remote --remote-user username --remote-host example.host.com --remote-path /home/username/memory-bank\n\n# Show help\nnpx @aakarsh-sasi/memory-bank-mcp --help\n```\n\nFor more detailed information about using npx, see [npx-usage.md](docs/npx-usage.md).\n\n## Using Remote Server Mode 🌐\n\nMemory Bank MCP now supports storing your Memory Bank on a remote server via SSH. This allows you to:\n\n1. **Centralize your Memory Bank**: Keep all your project memory in one place\n2. **Share Memory Banks**: Multiple users can access the same Memory Bank\n3. **Persistent Storage**: Your Memory Bank persists even if your local machine is wiped\n\n### Remote Server Requirements\n\n- SSH access to the remote server\n- SSH key authentication set up (password authentication is not supported)\n- Sufficient permissions to create/modify files in the specified directory\n\n### SSH Key Setup\n\nTo set up SSH key authentication for the remote server:\n\n1. **Generate a new SSH key pair** (if you don't already have one):\n   \n   ```bash\n   # Using modern Ed25519 algorithm (recommended)\n   ssh-keygen -t ed25519 -C \"your_email@example.com\"\n   \n   # OR using RSA if required for compatibility\n   ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n   ```\n\n2. **Start the SSH agent and add your key**:\n   \n   ```bash\n   # Start the agent\n   eval \"$(ssh-agent -s)\"\n   \n   # Add your key\n   ssh-add ~/.ssh/id_ed25519  # or ~/.ssh/id_rsa if you used RSA\n   ```\n\n3. **Copy your public key to the remote server**:\n   \n   ```bash\n   # Easiest method (if available)\n   ssh-copy-id username@your-remote-host.com\n   \n   # Alternative: manually copy your public key\n   cat ~/.ssh/id_ed25519.pub  # copy the output\n   ```\n   \n   Then paste the key into the `~/.ssh/authorized_keys` file on the remote server.\n\n4. **Test your connection**:\n   \n   ```bash\n   ssh username@your-remote-host.com\n   ```\n   \n   You should be able to log in without a password.\n\nFor more detailed SSH key setup instructions, see our [SSH Keys Guide](docs/ssh-keys-guide.md).\n\n### Remote Server Configuration\n\nTo use remote server mode, you need to provide the following parameters:\n\n```bash\nnpx @aakarsh-sasi/memory-bank-mcp --remote \\\n  --ssh-key ~/.ssh/your_ssh_key \\\n  --remote-user username \\\n  --remote-host example.host.com \\\n  --remote-path /home/username/memory-bank\n```\n\nBy default, the SSH key is assumed to be at `~/.ssh/your_ssh_key`. You can specify a different key using the `--ssh-key` option.\n\n### Remote Server Example\n\n```bash\n# Using with a server at example.host.com\nnpx @aakarsh-sasi/memory-bank-mcp --remote \\\n  --remote-user username \\\n  --remote-host example.host.com \\\n  --remote-path /home/username/memory-bank\n```\n\n## Configuring in Cursor 🖱️\n\nCursor is an AI-powered code editor that supports the Model Context Protocol (MCP). To configure Memory Bank MCP in Cursor:\n\n1. **Use Memory Bank MCP with npx**:\n\n   No need to install the package globally. You can use npx directly:\n\n   ```bash\n   # Verify npx is working correctly\n   npx @aakarsh-sasi/memory-bank-mcp --help\n   ```\n\n2. **Open Cursor Settings**:\n\n   - Go to Settings (⚙️) > Extensions > MCP\n   - Click on \"Add MCP Server\"\n\n3. **Configure the MCP Server**:\n\n   - **Name**: Memory Bank MCP\n   - **Command**: npx\n   - **Arguments**: `@aakarsh-sasi/memory-bank-mcp --mode code` (or other mode as needed)\n   \n   For remote server:\n   - **Arguments**: `@aakarsh-sasi/memory-bank-mcp --mode code --remote --remote-user username --remote-host example.host.com --remote-path /home/username/memory-bank`\n\n4. **Save and Activate**:\n\n   - Click \"Save\"\n   - Enable the MCP server by toggling it on\n\n5. **Verify Connection**:\n   - Open a project in Cursor\n   - The Memory Bank MCP should now be active and available in your AI interactions\n\nFor detailed instructions and advanced usage with Cursor, see [cursor-integration.md](docs/cursor-integration.md).\n\n### Using with Cursor 🤖\n\nOnce configured, you can interact with Memory Bank MCP in Cursor through AI commands:\n\n- **Initialize a Memory Bank**: `/mcp memory-bank-mcp initialize_memory_bank path=./memory-bank`\n- **Track Progress**: `/mcp memory-bank-mcp track_progress action=\"Feature Implementation\" description=\"Implemented feature X\"`\n- **Log Decision**: `/mcp memory-bank-mcp log_decision title=\"API Design\" context=\"...\" decision=\"...\"`\n- **Switch Mode**: `/mcp memory-bank-mcp switch_mode mode=code`\n\n## MCP Modes and Their Usage 🔄\n\nMemory Bank MCP supports different operational modes to optimize AI interactions for specific tasks:\n\n### Available Modes\n\n1. **Code Mode** 👨‍💻\n\n   - Focus: Code implementation and development\n   - Usage: `npx @aakarsh-sasi/memory-bank-mcp --mode code`\n   - Best for: Writing, refactoring, and optimizing code\n\n2. **Architect Mode** 🏗️\n\n   - Focus: System design and architecture\n   - Usage: `npx @aakarsh-sasi/memory-bank-mcp --mode architect`\n   - Best for: Planning project structure, designing components, and making architectural decisions\n\n3. **Ask Mode** ❓\n\n   - Focus: Answering questions and providing information\n   - Usage: `npx @aakarsh-sasi/memory-bank-mcp --mode ask`\n   - Best for: Getting explanations, clarifications, and information\n\n4. **Debug Mode** 🐛\n\n   - Focus: Troubleshooting and problem-solving\n   - Usage: `npx @aakarsh-sasi/memory-bank-mcp --mode debug`\n   - Best for: Finding and fixing bugs, analyzing issues\n\n5. **Test Mode** ✅\n   - Focus: Testing and quality assurance\n   - Usage: `npx @aakarsh-sasi/memory-bank-mcp --mode test`\n   - Best for: Writing tests, test-driven development\n\n### Switching Modes\n\nYou can switch modes in several ways:\n\n1. **When starting the server**:\n\n   ```bash\n   npx @aakarsh-sasi/memory-bank-mcp --mode architect\n   ```\n\n2. **During a session**:\n\n   ```bash\n   memory-bank-mcp switch_mode mode=debug\n   ```\n\n3. **In Cursor**:\n\n   ```\n   /mcp memory-bank-mcp switch_mode mode=test\n   ```\n\n4. **Using .clinerules files**:\n   Create a `.clinerules-[mode]` file in your project to automatically switch to that mode when the file is detected.\n\n## How Memory Bank MCP Works 🧠\n\nMemory Bank MCP is built on the Model Context Protocol (MCP), which enables AI assistants to interact with external tools and resources. Here's how it works:\n\n### Core Components 🧩\n\n1. **Memory Bank**: A structured repository of information stored as markdown files:\n\n   - `product-context.md`: Overall project information and goals\n   - `active-context.md`: Current state, ongoing tasks, and next steps\n   - `progress.md`: History of project updates and milestones\n   - `decision-log.md`: Record of important decisions with context and rationale\n   - `system-patterns.md`: Architecture and code patterns used in the project\n\n2. **MCP Server**: Provides tools and resources for AI assistants to interact with Memory Banks:\n\n   - Runs as a standalone process\n   - Communicates with AI assistants through the MCP protocol\n   - Provides a set of tools for managing Memory Banks\n\n3. **Mode System**: Supports different operational modes:\n   - `code`: Focus on code implementation\n   - `ask`: Focus on answering questions\n   - `architect`: Focus on system design\n   - `debug`: Focus on debugging issues\n   - `test`: Focus on testing\n\n### Data Flow 🔄\n\n1. **Initialization**: The AI assistant connects to the MCP server and initializes a Memory Bank\n2. **Tool Calls**: The AI assistant calls tools provided by the MCP server to read/write Memory Bank files\n3. **Context Maintenance**: The Memory Bank maintains context across sessions, allowing the AI to recall previous decisions and progress\n\n### Memory Bank Structure 📂\n\nMemory Banks use a standardized structure to organize information:\n\n- **Product Context**: Project overview, objectives, technologies, and architecture\n- **Active Context**: Current state, ongoing tasks, known issues, and next steps\n- **Progress**: Chronological record of project updates and milestones\n- **Decision Log**: Record of important decisions with context, alternatives, and consequences\n- **System Patterns**: Architecture patterns, code patterns, and documentation patterns\n\n### Advanced Features 🚀\n\n- **UMB Command**: Temporarily update Memory Bank files during a session without committing changes\n- **Mode Detection**: Automatically detect and switch modes based on user input\n- **File Migration**: Tools for migrating between different file naming conventions\n- **Language Standardization**: All Memory Bank files are generated in English for consistency\n\n## Versioning 📌\n\nThis project follows [Semantic Versioning](https://semver.org/) and uses [Conventional Commits](https://www.conventionalcommits.org/) for commit messages. The version is automatically bumped and a changelog is generated based on commit messages when changes are merged into the main branch.\n\n- **Major version** is bumped when there are breaking changes (commit messages with `BREAKING CHANGE` or `!:`)\n- **Minor version** is bumped when new features are added (commit messages with `feat:` or `feat(scope):`)\n- **Patch version** is bumped for all other changes (bug fixes, documentation, etc.)\n\nFor the complete history of changes, see the [CHANGELOG.md](CHANGELOG.md) file.\n\n## Usage 📝\n\n### As a Command Line Tool 💻\n\n```bash\n# Initialize a Memory Bank\nmemory-bank-mcp initialize_memory_bank path=./memory-bank\n\n# Track progress\nmemory-bank-mcp track_progress action=\"Feature Implementation\" description=\"Implemented feature X\"\n\n# Log a decision\nmemory-bank-mcp log_decision title=\"API Design\" context=\"...\" decision=\"...\"\n\n# Switch mode\nmemory-bank-mcp switch_mode mode=code\n```\n\n### As a Library 📚\n\n```typescript\nimport { MemoryBankServer } from \"@aakarsh-sasi/memory-bank-mcp\";\n\n// Create a new server instance\nconst server = new MemoryBankServer();\n\n// Start the server\nserver.run().catch(console.error);\n```\n\n## Contributing 👥\n\nPlease see [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License 📄\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Memory Bank Status System 🚦\n\nMemory Bank MCP implements a status prefix system that provides immediate visibility into the operational state of the Memory Bank:\n\n### Status Indicators\n\nEvery response from an AI assistant using Memory Bank MCP begins with one of these status indicators:\n\n- **`[MEMORY BANK: ACTIVE]`**: The Memory Bank is available and being used to provide context-aware responses\n- **`[MEMORY BANK: INACTIVE]`**: The Memory Bank is not available or not properly configured\n- **`[MEMORY BANK: UPDATING]`**: The Memory Bank is currently being updated (during UMB command execution)\n\nThis system ensures users always know whether the AI assistant is operating with full context awareness or limited information.\n\n### Benefits\n\n- **Transparency**: Users always know whether the AI has access to the full project context\n- **Troubleshooting**: Makes it immediately obvious when Memory Bank is not properly configured\n- **Context Awareness**: Helps users understand why certain responses may lack historical context\n\nFor more details, see [Memory Bank Status Prefix System](docs/memory-bank-status-prefix.md).\n",
      "npm_url": "https://www.npmjs.com/package/memory-bank-mcp",
      "npm_downloads": 3190,
      "keywords": [
        "memory",
        "assistant",
        "ai",
        "sasi memory",
        "ai assistant",
        "memory management"
      ],
      "category": "memory-management"
    },
    "alioshr--memory-bank-mcp": {
      "owner": "alioshr",
      "name": "memory-bank-mcp",
      "url": "https://github.com/alioshr/memory-bank-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alioshr.webp",
      "description": "Manage project memory banks remotely and efficiently, enabling streamlined access and manipulation of memory data.",
      "stars": 690,
      "forks": 65,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T05:26:34Z",
      "readme_content": "# Memory Bank MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@alioshr/memory-bank-mcp)](https://smithery.ai/server/@alioshr/memory-bank-mcp)\n[![npm version](https://badge.fury.io/js/%40allpepper%2Fmemory-bank-mcp.svg)](https://www.npmjs.com/package/@allpepper/memory-bank-mcp)\n[![npm downloads](https://img.shields.io/npm/dm/@allpepper/memory-bank-mcp.svg)](https://www.npmjs.com/package/@allpepper/memory-bank-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/ir18x1tixp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ir18x1tixp/badge\" alt=\"Memory Bank Server MCP server\" /></a>\n\nA Model Context Protocol (MCP) server implementation for remote memory bank management, inspired by [Cline Memory Bank](https://github.com/nickbaumann98/cline_docs/blob/main/prompting/custom%20instructions%20library/cline-memory-bank.md).\n\n## Overview\n\nThe Memory Bank MCP Server transforms traditional file-based memory banks into a centralized service that:\n\n- Provides remote access to memory bank files via MCP protocol\n- Enables multi-project memory bank management\n- Maintains consistent file structure and validation\n- Ensures proper isolation between project memory banks\n\n## Features\n\n- **Multi-Project Support**\n\n  - Project-specific directories\n  - File structure enforcement\n  - Path traversal prevention\n  - Project listing capabilities\n  - File listing per project\n\n- **Remote Accessibility**\n\n  - Full MCP protocol implementation\n  - Type-safe operations\n  - Proper error handling\n  - Security through project isolation\n\n- **Core Operations**\n  - Read/write/update memory bank files\n  - List available projects\n  - List files within projects\n  - Project existence validation\n  - Safe read-only operations\n\n## Installation\n\nTo install Memory Bank Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@alioshr/memory-bank-mcp):\n\n```bash\nnpx -y @smithery/cli install @alioshr/memory-bank-mcp --client claude\n```\n\nThis will set up the MCP server configuration automatically. Alternatively, you can configure the server manually as described in the Configuration section below.\n\n## Quick Start\n\n1. Configure the MCP server in your settings (see Configuration section below)\n2. Start using the memory bank tools in your AI assistant\n\n## Using with Cline/Roo Code\n\nThe memory bank MCP server needs to be configured in your Cline MCP settings file. The location depends on your setup:\n\n- For Cline extension: `~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n- For Roo Code VS Code extension: `~/Library/Application Support/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/mcp_settings.json`\n\nAdd the following configuration to your MCP settings:\n\n```json\n{\n  \"allpepper-memory-bank\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@allpepper/memory-bank-mcp\"],\n    \"env\": {\n      \"MEMORY_BANK_ROOT\": \"<path-to-bank>\"\n    },\n    \"disabled\": false,\n    \"autoApprove\": [\n      \"memory_bank_read\",\n      \"memory_bank_write\",\n      \"memory_bank_update\",\n      \"list_projects\",\n      \"list_project_files\"\n    ]\n  }\n}\n```\n\n### Configuration Details\n\n- `MEMORY_BANK_ROOT`: Directory where project memory banks will be stored (e.g., `/path/to/memory-bank`)\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of operations that don't require explicit user approval:\n  - `memory_bank_read`: Read memory bank files\n  - `memory_bank_write`: Create new memory bank files\n  - `memory_bank_update`: Update existing memory bank files\n  - `list_projects`: List available projects\n  - `list_project_files`: List files within a project\n\n## Using with Cursor\n\nFor Cursor, open the settings -> features -> add MCP server -> add the following:\n\n```shell\nenv MEMORY_BANK_ROOT=<path-to-bank> npx -y @allpepper/memory-bank-mcp@latest\n```\n## Using with Claude\n\n- Claude desktop config file: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Claude Code config file:  `~/.claude.json`\n\n1. Locate the config file\n3. Locate the property called `mcpServers`\n4. Paste this:\n\n```\n \"allPepper-memory-bank\": {\n          \"type\": \"stdio\",\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@allpepper/memory-bank-mcp@latest\"\n          ],\n          \"env\": {\n            \"MEMORY_BANK_ROOT\": \"YOUR PATH\"\n          }\n        }\n```\n\n## Custom AI instructions\n\nThis section contains the instructions that should be pasted on the AI custom instructions, either for Cline, Claude or Cursor, or any other MCP client. You should copy and paste these rules. For reference, see [custom-instructions.md](custom-instructions.md) which contains these rules.\n\n## Development\n\nBasic development commands:\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run tests\nnpm run test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Run the server directly with ts-node for quick testing\nnpm run dev\n```\n\n### Running with Docker\n\n1. Build the Docker image:\n\n    ```bash\n    docker build -t memory-bank-mcp:local .\n    ```\n\n2. Run the Docker container for testing:\n\n    ```bash\n    docker run -i --rm \\\n      -e MEMORY_BANK_ROOT=\"/mnt/memory_bank\" \\\n      -v /path/to/memory-bank:/mnt/memory_bank \\\n      --entrypoint /bin/sh \\\n      memory-bank-mcp:local \\\n      -c \"ls -la /mnt/memory_bank\"\n    ```\n\n3. Add MCP configuration, example for Roo Code:\n\n    ```json\n    \"allpepper-memory-bank\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \n        \"MEMORY_BANK_ROOT\",\n        \"-v\", \n        \"/path/to/memory-bank:/mnt/memory_bank\",\n        \"memory-bank-mcp:local\"\n      ],\n      \"env\": {\n        \"MEMORY_BANK_ROOT\": \"/mnt/memory_bank\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"list_projects\",\n        \"list_project_files\",\n        \"memory_bank_read\",\n        \"memory_bank_update\",\n        \"memory_bank_write\"\n      ]\n    }\n    ```\n\n## Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Development Guidelines\n\n- Use TypeScript for all new code\n- Maintain type safety across the codebase\n- Add tests for new features\n- Update documentation as needed\n- Follow existing code style and patterns\n\n### Testing\n\n- Write unit tests for new features\n- Include multi-project scenario tests\n- Test error cases thoroughly\n- Validate type constraints\n- Mock filesystem operations appropriately\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\nThis project implements the memory bank concept originally documented in the [Cline Memory Bank](https://github.com/nickbaumann98/cline_docs/blob/main/prompting/custom%20instructions%20library/cline-memory-bank.md), extending it with remote capabilities and multi-project support.\n",
      "npm_url": "https://www.npmjs.com/package/memory-bank-mcp",
      "npm_downloads": 3190,
      "keywords": [
        "memory",
        "banks",
        "alioshr",
        "memory bank",
        "memory banks",
        "alioshr memory"
      ],
      "category": "memory-management"
    },
    "amotivv--memory-box-mcp": {
      "owner": "amotivv",
      "name": "memory-box-mcp",
      "url": "https://github.com/amotivv/memory-box-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/amotivv.webp",
      "description": "Manage and retrieve memories using advanced semantic search capabilities, enabling efficient organization and structured formatting of memory data. Integrate with Cline and Claude for enhanced memory management functionalities.",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-22T06:45:44Z",
      "readme_content": "<p align=\"center\">\n  <img src=\"https://storage.googleapis.com/amotivv-public/memory-box-logo.png\" alt=\"Memory Box Logo\" width=\"200\"/>\n</p>\n\n<h1 align=\"center\">Memory Box MCP Server</h1>\n\n<p align=\"center\">\n  Cline and Claude Desktop MCP integration for Memory Box - save, search, and format memories with semantic understanding\n</p>\n\n<p align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n</p>\n<p align=\"center\">\n  <a href=\"https://glama.ai/mcp/servers/wtbejx9zwc\">\n    <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/wtbejx9zwc/badge\" />\n  </a>\n</p>\n\nThis MCP server provides tools for interacting with a Memory Box instance, allowing you to save and search memories using semantic search directly from Cline and Claude Desktop.\n\n## Related Projects\n\nThis MCP server is designed to work with [Memory Box](https://memorybox.dev), a semantic memory storage and retrieval system powered by vector embeddings.\n\nMemory Box provides the backend API that this MCP server communicates with, allowing you to:\n- Store memories with vector embeddings for semantic search\n- Organize memories into customizable buckets\n- Search for memories based on meaning, not just keywords\n- Retrieve memories with detailed context\n- Find semantically related memories\n- Track memory processing status\n\nFor more information about Memory Box, including how to set up your own instance, please visit the [Memory Box website](https://memorybox.dev).\n\n## Features\n\n- **Save Memories**: Save formatted memories to your Memory Box with source information and metadata\n- **Search Memories**: Search your memories using semantic search with pagination and date sorting\n- **Retrieve Memories**: Get all memories or memories from specific buckets\n- **Bucket Management**: Create and delete buckets for organizing memories\n- **Memory Management**: Update or delete existing memories\n- **Find Related Memories**: Discover semantically similar memories \n- **Check Memory Status**: Monitor the processing status of your memories\n- **Format Memories**: Format memories according to a structured system prompt\n- **Usage Statistics**: View your current plan, usage metrics, and resource limits\n\n## Installation\n\nThe server has been installed and configured for use with Cline. Note that you need a running Memory Box instance (either self-hosted or using the hosted version at memorybox.amotivv.ai) to use this MCP server.\n\n### Installing as Claude Desktop Extension (Recommended)\n\nThe easiest way to use Memory Box with Claude Desktop is through the Desktop Extension:\n\n1. Download the latest `memory-box.mcpb` file from the [releases page](https://github.com/amotivv/memory-box-mcp/releases)\n2. Open Claude Desktop\n3. Go to Settings > Extensions\n4. Click \"Install from file\"\n5. Select the downloaded `memory-box.mcpb` file\n6. Configure your Memory Box API token in the extension settings\n\nThe extension will automatically configure all necessary environment variables and tools.\n\n### Installing via Smithery\n\nTo install Memory Box MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@amotivv/memory-box-mcp):\n\n```bash\nnpx -y @smithery/cli install @amotivv/memory-box-mcp --client claude\n```\n\nTo complete the setup:\n\n1. Edit the Cline MCP settings file at:\n   ```\n   ~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   ```\n\n2. Add your Memory Box token to the `MEMORY_BOX_TOKEN` environment variable:\n   ```json\n   \"memory-box-mcp\": {\n     \"command\": \"node\",\n     \"args\": [\n       \"<path-to-repository>/build/index.js\"\n     ],\n     \"env\": {\n       \"MEMORY_BOX_API_URL\": \"https://memorybox.amotivv.ai\",\n       \"MEMORY_BOX_TOKEN\": \"your-token-here\",\n       \"DEFAULT_BUCKET\": \"General\"\n     },\n     \"disabled\": false,\n     \"autoApprove\": []\n   }\n   ```\n\n3. Optionally, you can customize the default bucket by changing the `DEFAULT_BUCKET` value.\n\n## Usage\n\nOnce configured, you can use the following tools in Cline:\n\n### Save Memory\n\nSave a memory to Memory Box with proper formatting:\n\n```\nUse the save_memory tool to save this information about vector databases: \"Vector databases like pgvector store and query high-dimensional vectors for semantic search applications.\"\n```\n\nParameters:\n- `text` (required): The memory content to save\n- `bucket_id` (optional): The bucket to save the memory to (default: \"General\")\n- `format` (optional): Whether to format the memory according to the system prompt (default: true)\n- `type` (optional): The type of memory (TECHNICAL, DECISION, SOLUTION, CONCEPT, REFERENCE, APPLICATION, FACT) for formatting (default: \"TECHNICAL\")\n- `source_type` (optional): Type of memory source (default: \"llm_plugin\")\n- `reference_data` (optional): Additional metadata about the memory source and context\n\n### Search Memories\n\nSearch for memories using semantic search:\n\n```\nUse the search_memories tool to find information about \"vector databases\"\n```\n\nParameters:\n- `query` (required): The search query\n- `debug` (optional): Include debug information in results (default: false)\n\n### Get All Memories\n\nRetrieve all memories:\n\n```\nUse the get_all_memories tool to show me all my saved memories\n```\n\n### Get Bucket Memories\n\nGet memories from a specific bucket:\n\n```\nUse the get_bucket_memories tool to show me memories in the \"Learning\" bucket\n```\n\nParameters:\n- `bucket_id` (required): The bucket to retrieve memories from\n\n### Format Memory\n\nFormat a text according to the memory system prompt without saving:\n\n```\nUse the format_memory tool to format this text: \"Vector databases like pgvector store and query high-dimensional vectors for semantic search applications.\"\n```\n\nParameters:\n- `text` (required): The text to format\n- `type` (optional): The type of memory (TECHNICAL, DECISION, SOLUTION, CONCEPT, REFERENCE, APPLICATION, FACT) (default: \"TECHNICAL\")\n\n### Get Related Memories\n\nFind semantically similar memories to a specific memory:\n\n```\nUse the get_related_memories tool with memory ID 123\n```\n\nParameters:\n- `memory_id` (required): The ID of the memory to find related memories for\n- `min_similarity` (optional): Minimum similarity threshold (0.0-1.0) for related memories (default: 0.7)\n\n### Check Memory Status\n\nCheck the processing status of a memory:\n\n```\nUse the check_memory_status tool with memory ID 123\n```\n\nParameters:\n- `memory_id` (required): The ID of the memory to check status for\n\n### Get Usage Stats\n\nRetrieve user usage statistics and plan information:\n\n```\nUse the get_usage_stats tool to show me my current plan and usage metrics\n```\n\nThis tool returns:\n- Current plan information (e.g., free, basic, professional, legacy)\n- User status and limit enforcement information\n- Current month usage metrics (store operations, search operations, API calls)\n- Data processing volume with human-readable formatting\n- Resource limits based on your plan (if applicable)\n- Operation breakdown by type\n\nNo parameters are required for this operation.\n\n### Get Buckets\n\nList all available buckets:\n\n```\nUse the get_buckets tool to show me all available buckets\n```\n\nThis tool returns a list of all buckets with their names, IDs, memory counts, and creation dates.\n\n### Create Bucket\n\nCreate a new bucket for organizing memories:\n\n```\nUse the create_bucket tool to create a bucket named \"Project Ideas\"\n```\n\nParameters:\n- `bucket_name` (required): Name of the bucket to create\n\n### Delete Bucket\n\nDelete an existing bucket:\n\n```\nUse the delete_bucket tool to delete the bucket named \"Old Notes\"\n```\n\nParameters:\n- `bucket_name` (required): Name of the bucket to delete\n- `force` (optional): Force deletion even if bucket contains memories (default: false)\n\n### Update Memory\n\nUpdate an existing memory's content, bucket, or metadata:\n\n```\nUse the update_memory tool to update memory ID 123 with new text: \"Updated information about vector databases\"\n```\n\nParameters:\n- `memory_id` (required): The ID of the memory to update\n- `text` (optional): New text content for the memory\n- `bucket_id` (optional): New bucket for the memory\n- `reference_data` (optional): Updated reference data including relationships\n\n### Delete Memory\n\nDelete a specific memory:\n\n```\nUse the delete_memory tool to delete memory ID 123\n```\n\nParameters:\n- `memory_id` (required): The ID of the memory to delete\n\n## Customization\n\n### System Prompt Customization\n\nThe Memory Box MCP server uses a system prompt to format memories according to specific guidelines. You can customize this prompt to change how memories are formatted.\n\n#### Default System Prompt\n\nThe default system prompt includes formatting guidelines for different types of memories:\n\n```\nYou are a helpful AI assistant. When storing memories with Memory Box, follow these enhanced formatting guidelines:\n\n1. STRUCTURE: Format memories based on the type of information:\n   - TECHNICAL: \"TECHNICAL - [Brief topic]: [Concise explanation with specific details]\"\n   - DECISION: \"DECISION - [Brief topic]: [Decision made] because [rationale]. Alternatives considered: [options].\"\n   - SOLUTION: \"SOLUTION - [Problem summary]: [Implementation details that solved the issue]\"\n   - CONCEPT: \"CONCEPT - [Topic]: [Clear explanation of the concept with examples]\"\n   - REFERENCE: \"REFERENCE - [Topic]: [URL, tool name, or resource] for [specific purpose]\"\n   - APPLICATION: \"APPLICATION - [App name]: [User-friendly description] followed by [technical implementation details]\"\n\n2. FORMATTING GUIDELINES:\n   - CREATE FOCUSED MEMORIES: Each memory should contain a single clear concept or topic\n   - USE DIVERSE TERMINOLOGY: Include both technical terms AND user-friendly alternatives\n   - INCLUDE SEARCHABLE KEYWORDS: Begin with common terms a user might search for\n   - BALANCE DETAIL LEVELS: Include both high-level descriptions and key technical details\n   - LENGTH: Keep memories between 50-150 words\n   - ALWAYS include the current date in YYYY-MM-DD format\n\n3. MEMORY STORAGE PARAMETERS:\n   - Use the \"text\" parameter for your formatted memory content\n   - Set \"source_type\" to \"llm_plugin\"\n   - Include appropriate \"reference_data\" with source information and context\n\n4. REFERENCE DATA STRUCTURE:\n   - source.platform: Identify your platform (e.g., \"claude_desktop\", \"cline\")\n   - source.type: Always set to \"llm_plugin\"\n   - source.version: Optional version information\n   - context.conversation_id: Include when available to link related conversation memories\n   - context.message_id: Optional identifier for the specific message\n\n5. SPECIAL FORMATS:\n   - For user facts, preferences, or personal details: \"YYYY-MM-DD: FACT: [User] [specific preference/attribute/information]\"\n   - For reference materials: Include specific details about where to find the information\n\n6. RELATED MEMORIES: After finding memories with search, check if there are related memories using the get_related_memories tool with the memory_id from search results. Present these additional memories to provide the user with more context.\n\n7. RETRIEVAL CONSIDERATION: Before storing an important memory, consider: \"What search terms might someone use to find this information later?\" and ensure those terms are included.\n```\n\n#### How to Customize the System Prompt\n\nTo customize the system prompt:\n\n1. Edit the Cline MCP settings file at:\n   ```\n   ~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   ```\n\n2. Add your custom system prompt to the `SYSTEM_PROMPT` environment variable:\n   ```json\n   \"memory-box-mcp\": {\n     \"command\": \"node\",\n     \"args\": [\n       \"<path-to-repository>/build/index.js\"\n     ],\n     \"env\": {\n       \"MEMORY_BOX_API_URL\": \"https://your-memory-box-instance\",\n       \"MEMORY_BOX_TOKEN\": \"your-token-here\",\n       \"DEFAULT_BUCKET\": \"General\",\n       \"SYSTEM_PROMPT\": \"Your custom system prompt here...\"\n     },\n     \"disabled\": false,\n     \"autoApprove\": []\n   }\n   ```\n\n   A template file is provided at `<path-to-repository>/system-prompt-template.txt` that you can copy and modify.\n\n3. Restart Cline to apply the changes\n\n#### System Prompt Helper\n\nThe Memory Box MCP server includes a helper script for managing the system prompt:\n\n```bash\n# View the current system prompt\ncd <path-to-repository>\nnpm run prompt-helper -- view\n\n# Reset to the default system prompt\ncd <path-to-repository>\nnpm run prompt-helper -- reset\n\n# Validate a custom system prompt\ncd <path-to-repository>\nnpm run prompt-helper -- validate\n```\n\n### Other Configuration Options\n\nYou can also customize these environment variables:\n\n- `MEMORY_BOX_API_URL`: The URL of your Memory Box instance\n- `MEMORY_BOX_TOKEN`: Your authentication token for Memory Box\n- `DEFAULT_BUCKET`: The default bucket to use when saving memories\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check that your Memory Box token is correctly configured\n2. Verify that your Memory Box instance is running and accessible\n3. Check the Cline logs for any error messages\n\n## Development\n\nTo make changes to the server:\n\n1. Edit the source code in `<path-to-repository>/src/`\n2. Rebuild the server:\n   ```\n   cd <path-to-repository>\n   npm run build\n   ```\n3. Restart Cline to apply the changes\n\n### Building the Desktop Extension\n\nTo build the Desktop Extension package:\n\n1. Install dependencies:\n   ```\n   npm install\n   ```\n\n2. Build the extension:\n   ```\n   npm run build-extension\n   ```\n\n3. The built extension will be available at `dist/memory-box.mcpb`\n\n### Release Process\n\n1. Update version in `package.json`\n2. Update `CHANGELOG.md` with new changes\n3. Commit changes\n4. Create a new GitHub release\n5. Upload the `memory-box.mcpb` file as a release asset\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "memories",
        "amotivv",
        "amotivv memory",
        "memory management",
        "memory box"
      ],
      "category": "memory-management"
    },
    "basicmachines-co--basic-memory": {
      "owner": "basicmachines-co",
      "name": "basic-memory",
      "url": "https://github.com/basicmachines-co/basic-memory",
      "imageUrl": "/freedevtools/mcp/pfp/basicmachines-co.webp",
      "description": "Enables persistent knowledge management through natural language conversations with LLMs by reading and writing to local Markdown files. Facilitates contextual awareness across sessions while maintaining a user-controlled knowledge base with a simple local-first approach.",
      "stars": 1882,
      "forks": 116,
      "license": "GNU Affero General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-10-03T06:31:12Z",
      "readme_content": "[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)\n[![PyPI version](https://badge.fury.io/py/basic-memory.svg)](https://badge.fury.io/py/basic-memory)\n[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/basicmachines-co/basic-memory/workflows/Tests/badge.svg)](https://github.com/basicmachines-co/basic-memory/actions)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n![](https://badge.mcpx.dev?type=dev 'MCP Dev')\n[![smithery badge](https://smithery.ai/badge/@basicmachines-co/basic-memory)](https://smithery.ai/server/@basicmachines-co/basic-memory)\n\n# Basic Memory\n\nBasic Memory lets you build persistent knowledge through natural conversations with Large Language Models (LLMs) like\nClaude, while keeping everything in simple Markdown files on your computer. It uses the Model Context Protocol (MCP) to\nenable any compatible LLM to read and write to your local knowledge base.\n\n- Website: https://basicmachines.co\n- Documentation: https://memory.basicmachines.co\n\n## Pick up your conversation right where you left off\n\n- AI assistants can load context from local files in a new conversation\n- Notes are saved locally as Markdown files in real time\n- No project knowledge or special prompting required\n\nhttps://github.com/user-attachments/assets/a55d8238-8dd0-454a-be4c-8860dbbd0ddc\n\n## Quick Start\n\n```bash\n# Install with uv (recommended)\nuv tool install basic-memory\n\n# Configure Claude Desktop (edit ~/Library/Application Support/Claude/claude_desktop_config.json)\n# Add this to your config:\n{\n  \"mcpServers\": {\n    \"basic-memory\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"basic-memory\",\n        \"mcp\"\n      ]\n    }\n  }\n}\n# Now in Claude Desktop, you can:\n# - Write notes with \"Create a note about coffee brewing methods\"\n# - Read notes with \"What do I know about pour over coffee?\"\n# - Search with \"Find information about Ethiopian beans\"\n\n```\n\nYou can view shared context via files in `~/basic-memory` (default directory location).\n\n### Alternative Installation via Smithery\n\nYou can use [Smithery](https://smithery.ai/server/@basicmachines-co/basic-memory) to automatically configure Basic\nMemory for Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @basicmachines-co/basic-memory --client claude\n```\n\nThis installs and configures Basic Memory without requiring manual edits to the Claude Desktop configuration file. The\nSmithery server hosts the MCP server component, while your data remains stored locally as Markdown files.\n\n### Glama.ai\n\n<a href=\"https://glama.ai/mcp/servers/o90kttu9ym\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/o90kttu9ym/badge\" alt=\"basic-memory MCP server\" />\n</a>\n\n## Why Basic Memory?\n\nMost LLM interactions are ephemeral - you ask a question, get an answer, and everything is forgotten. Each conversation\nstarts fresh, without the context or knowledge from previous ones. Current workarounds have limitations:\n\n- Chat histories capture conversations but aren't structured knowledge\n- RAG systems can query documents but don't let LLMs write back\n- Vector databases require complex setups and often live in the cloud\n- Knowledge graphs typically need specialized tools to maintain\n\nBasic Memory addresses these problems with a simple approach: structured Markdown files that both humans and LLMs can\nread\nand write to. The key advantages:\n\n- **Local-first:** All knowledge stays in files you control\n- **Bi-directional:** Both you and the LLM read and write to the same files\n- **Structured yet simple:** Uses familiar Markdown with semantic patterns\n- **Traversable knowledge graph:** LLMs can follow links between topics\n- **Standard formats:** Works with existing editors like Obsidian\n- **Lightweight infrastructure:** Just local files indexed in a local SQLite database\n\nWith Basic Memory, you can:\n\n- Have conversations that build on previous knowledge\n- Create structured notes during natural conversations\n- Have conversations with LLMs that remember what you've discussed before\n- Navigate your knowledge graph semantically\n- Keep everything local and under your control\n- Use familiar tools like Obsidian to view and edit notes\n- Build a personal knowledge base that grows over time\n\n## How It Works in Practice\n\nLet's say you're exploring coffee brewing methods and want to capture your knowledge. Here's how it works:\n\n1. Start by chatting normally:\n\n```\nI've been experimenting with different coffee brewing methods. Key things I've learned:\n\n- Pour over gives more clarity in flavor than French press\n- Water temperature is critical - around 205°F seems best\n- Freshly ground beans make a huge difference\n```\n\n... continue conversation.\n\n2. Ask the LLM to help structure this knowledge:\n\n```\n\"Let's write a note about coffee brewing methods.\"\n```\n\nLLM creates a new Markdown file on your system (which you can see instantly in Obsidian or your editor):\n\n```markdown\n---\ntitle: Coffee Brewing Methods\npermalink: coffee-brewing-methods\ntags:\n- coffee\n- brewing\n---\n\n# Coffee Brewing Methods\n\n## Observations\n\n- [method] Pour over provides more clarity and highlights subtle flavors\n- [technique] Water temperature at 205°F (96°C) extracts optimal compounds\n- [principle] Freshly ground beans preserve aromatics and flavor\n\n## Relations\n\n- relates_to [[Coffee Bean Origins]]\n- requires [[Proper Grinding Technique]]\n- affects [[Flavor Extraction]]\n```\n\nThe note embeds semantic content and links to other topics via simple Markdown formatting.\n\n3. You see this file on your computer in real time in the current project directory (default `~/$HOME/basic-memory`).\n\n- Realtime sync can be enabled via running `basic-memory sync --watch`\n\n4. In a chat with the LLM, you can reference a topic:\n\n```\nLook at `coffee-brewing-methods` for context about pour over coffee\n```\n\nThe LLM can now build rich context from the knowledge graph. For example:\n\n```\nFollowing relation 'relates_to [[Coffee Bean Origins]]':\n- Found information about Ethiopian Yirgacheffe\n- Notes on Colombian beans' nutty profile\n- Altitude effects on bean characteristics\n\nFollowing relation 'requires [[Proper Grinding Technique]]':\n- Burr vs. blade grinder comparisons\n- Grind size recommendations for different methods\n- Impact of consistent particle size on extraction\n```\n\nEach related document can lead to more context, building a rich semantic understanding of your knowledge base.\n\nThis creates a two-way flow where:\n\n- Humans write and edit Markdown files\n- LLMs read and write through the MCP protocol\n- Sync keeps everything consistent\n- All knowledge stays in local files.\n\n## Technical Implementation\n\nUnder the hood, Basic Memory:\n\n1. Stores everything in Markdown files\n2. Uses a SQLite database for searching and indexing\n3. Extracts semantic meaning from simple Markdown patterns\n    - Files become `Entity` objects\n    - Each `Entity` can have `Observations`, or facts associated with it\n    - `Relations` connect entities together to form the knowledge graph\n4. Maintains the local knowledge graph derived from the files\n5. Provides bidirectional synchronization between files and the knowledge graph\n6. Implements the Model Context Protocol (MCP) for AI integration\n7. Exposes tools that let AI assistants traverse and manipulate the knowledge graph\n8. Uses memory:// URLs to reference entities across tools and conversations\n\nThe file format is just Markdown with some simple markup:\n\nEach Markdown file has:\n\n### Frontmatter\n\n```markdown\ntitle: <Entity title>\ntype: <The type of Entity> (e.g. note)\npermalink: <a uri slug>\n\n- <optional metadata> (such as tags) \n```\n\n### Observations\n\nObservations are facts about a topic.\nThey can be added by creating a Markdown list with a special format that can reference a `category`, `tags` using a\n\"#\" character, and an optional `context`.\n\nObservation Markdown format:\n\n```markdown\n- [category] content #tag (optional context)\n```\n\nExamples of observations:\n\n```markdown\n- [method] Pour over extracts more floral notes than French press\n- [tip] Grind size should be medium-fine for pour over #brewing\n- [preference] Ethiopian beans have bright, fruity flavors (especially from Yirgacheffe)\n- [fact] Lighter roasts generally contain more caffeine than dark roasts\n- [experiment] Tried 1:15 coffee-to-water ratio with good results\n- [resource] James Hoffman's V60 technique on YouTube is excellent\n- [question] Does water temperature affect extraction of different compounds differently?\n- [note] My favorite local shop uses a 30-second bloom time\n```\n\n### Relations\n\nRelations are links to other topics. They define how entities connect in the knowledge graph.\n\nMarkdown format:\n\n```markdown\n- relation_type [[WikiLink]] (optional context)\n```\n\nExamples of relations:\n\n```markdown\n- pairs_well_with [[Chocolate Desserts]]\n- grown_in [[Ethiopia]]\n- contrasts_with [[Tea Brewing Methods]]\n- requires [[Burr Grinder]]\n- improves_with [[Fresh Beans]]\n- relates_to [[Morning Routine]]\n- inspired_by [[Japanese Coffee Culture]]\n- documented_in [[Coffee Journal]]\n```\n\n## Using with VS Code\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"basic-memory\": {\n        \"command\": \"uvx\",\n        \"args\": [\"basic-memory\", \"mcp\"]\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"servers\": {\n    \"basic-memory\": {\n      \"command\": \"uvx\",\n      \"args\": [\"basic-memory\", \"mcp\"]\n    }\n  }\n}\n```\n\nYou can use Basic Memory with VS Code to easily retrieve and store information while coding.\n\n## Using with Claude Desktop\n\nBasic Memory is built using the MCP (Model Context Protocol) and works with the Claude desktop app (https://claude.ai/):\n\n1. Configure Claude Desktop to use Basic Memory:\n\nEdit your MCP configuration file (usually located at `~/Library/Application Support/Claude/claude_desktop_config.json`\nfor OS X):\n\n```json\n{\n  \"mcpServers\": {\n    \"basic-memory\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"basic-memory\",\n        \"mcp\"\n      ]\n    }\n  }\n}\n```\n\nIf you want to use a specific project (see [Multiple Projects](#multiple-projects) below), update your Claude Desktop\nconfig:\n\n```json\n{\n  \"mcpServers\": {\n    \"basic-memory\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"basic-memory\",\n        \"mcp\",\n        \"--project\",\n        \"your-project-name\"\n      ]\n    }\n  }\n}\n```\n\n2. Sync your knowledge:\n\n```bash\n# One-time sync of local knowledge updates\nbasic-memory sync\n\n# Run realtime sync process (recommended)\nbasic-memory sync --watch\n```\n\n3. In Claude Desktop, the LLM can now use these tools:\n\n```\nwrite_note(title, content, folder, tags) - Create or update notes\nread_note(identifier, page, page_size) - Read notes by title or permalink\nbuild_context(url, depth, timeframe) - Navigate knowledge graph via memory:// URLs\nsearch(query, page, page_size) - Search across your knowledge base\nrecent_activity(type, depth, timeframe) - Find recently updated information\ncanvas(nodes, edges, title, folder) - Generate knowledge visualizations\n```\n\n5. Example prompts to try:\n\n```\n\"Create a note about our project architecture decisions\"\n\"Find information about JWT authentication in my notes\"\n\"Create a canvas visualization of my project components\"\n\"Read my notes on the authentication system\"\n\"What have I been working on in the past week?\"\n```\n\n## Futher info\n\nSee the [Documentation](https://memory.basicmachines.co/) for more info, including:\n\n- [Complete User Guide](https://memory.basicmachines.co/docs/user-guide)\n- [CLI tools](https://memory.basicmachines.co/docs/cli-reference)\n- [Managing multiple Projects](https://memory.basicmachines.co/docs/cli-reference#project)\n- [Importing data from OpenAI/Claude Projects](https://memory.basicmachines.co/docs/cli-reference#import)\n\n## License\n\nAGPL-3.0\n\nContributions are welcome. See the [Contributing](CONTRIBUTING.md) guide for info about setting up the project locally\nand submitting PRs.\n\n## Star History\n\n<a href=\"https://www.star-history.com/#basicmachines-co/basic-memory&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&type=Date\" />\n </picture>\n</a>\n\nBuilt with ♥️ by Basic Machines",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "basicmachines",
        "conversations",
        "basic memory",
        "management basicmachines",
        "memory management"
      ],
      "category": "memory-management"
    },
    "bneil--mcp-memory-pouchdb": {
      "owner": "bneil",
      "name": "mcp-memory-pouchdb",
      "url": "https://github.com/bneil/mcp-memory-pouchdb",
      "imageUrl": "/freedevtools/mcp/pfp/bneil.webp",
      "description": "Manage and enhance interactions with contextual information using a robust knowledge graph. It captures, stores, and retrieves data seamlessly with PouchDB for improved consistency and organization by project-specific paths.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-06-05T07:44:35Z",
      "readme_content": "# Memory Custom : PouchDB 🧠\n\n[![smithery badge](https://smithery.ai/badge/@bneil/mcp-memory-pouchdb)](https://smithery.ai/server/@bneil/mcp-memory-pouchdb)\n\nThis project adds new features to the Memory server offered by the MCP team. It allows for the creation and management of a knowledge graph that captures interactions via a language model (LLM). 🚀\n\nThis repo was forked from [https://github.com/BRO3886/mcp-memory-custom](https://github.com/BRO3886/mcp-memory-custom) which was a great starting point, thanks again for fixing timestamps. This repo's goal was more to fix the issue with an ever increasing json file for context.\n\n## New Features ✨\n\n### 1. PouchDB Integration 💾\n\n- The server now uses PouchDB for robust document-based storage\n- **Why?**: Better data consistency, built-in versioning, and improved performance for large datasets\n- Maintains file backup for compatibility\n\n### 2. Custom Memory Paths 📁\n\n- Users can now specify different memory file paths for various projects\n- **Why?**: This feature enhances organization and management of memory data, allowing for project-specific memory storage\n\n### 3. Timestamping ⏰\n\n- The server now generates timestamps for interactions\n- **Why?**: Timestamps enable tracking of when each memory was created or modified, providing better context and history for the stored data\n\n## Getting Started 🚀\n\n### Prerequisites 🔧\n\n- Node.js (version 16 or higher)\n- PouchDB (automatically installed as a dependency)\n\n### Installing via Smithery 📦\n\nTo install Knowledge Graph Memory Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@bneil/mcp-memory-pouchdb):\n\n```bash\nnpx -y @smithery/cli install @bneil/mcp-memory-pouchdb --client claude\n```\n\n### Installation 🛠️\n\n1. Clone the repository:\n\n   ```bash\n   git clone git@github.com:bneil/mcp-memory-pouchdb.git\n   cd mcp-memory-pouchdb\n   ```\n\n2. Install the dependencies:\n\n   ```bash\n   npm install\n   ```\n\n### Configuration ⚙️\n\nThe server requires two environment variables to be set:\n\n1. `MEMORY_FILE_PATH`: The absolute path where the memory backup file will be stored\n2. `POUCHDB_PATH`: The absolute path where the PouchDB database will be stored\n\nExample configuration in your `claude_desktop_config.json` / `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-memory-pouchdb/dist/index.js\"],\n      \"env\": {\n        \"MEMORY_FILE_PATH\": \"/path/to/custom/memory.json\",\n        \"POUCHDB_PATH\": \"/path/to/custom/pouchdb_directory\",\n        \"DISABLE_MEMORY_FILE\": \"true\"\n      }\n    }\n  }\n}\n```\n\nThe server will fail to start if either environment variable is not set. 🚫\n\nOptional environment variables:\n- `POUCHDB_OPTIONS`: JSON string of additional PouchDB configuration options\n- `DISABLE_MEMORY_FILE`: Set to \"true\" to disable saving to memory.json file (only use PouchDB for storage)\n\n### Running the Server 🚀\n\n#### Updating the mcp server json file 📝\n\nAdd this to your `claude_desktop_config.json` / `.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-memory-pouchdb/dist/index.js\"],\n      \"env\" : {\n        \"MEMORY_FILE_PATH\":\"/home/.../local_dbs/memory.json\",\n        \"POUCHDB_PATH\":\"/home/.../local_dbs/pouchdb\"\n      }\n    }\n  }\n}\n```\n\nSystem Prompt changes:\n\n```\nFollow these steps for each interaction:\n\n0. Memory Initialization:\n   - At startup, execute a read_graph function to initialize memory access\n   - If memory is empty, create a default entity for the user with basic placeholder info\n   - Run initialization sequence: read_graph → create_entities → read_graph\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and retrieve all relevant information from your knowledge graph\n   - Always refer to your knowledge graph as your \"memory\"\n   - Verify memory access is functioning properly\n\n3. Memory Attention:\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events, add timestamps to wherever required. You can get current timestamp via get_current_time\n     b) Connect them to the current entities using relations\n     c) Store facts about them as observations, add timestamps to observations via get_current_time\n\n5. Error Recovery:\n   - If memory retrieval fails, execute read_graph function immediately\n   - Log any memory access failures for debugging\n   - Continue the conversation with best available information\n\n6. Memory Validation:\n   - Periodically verify that memory access is functional by checking for core entities\n   - If validation fails, attempt reconnection via read_graph\n\nIMPORTANT: Provide a helpful and engaging response, asking relevant questions to encourage user engagement. Update the memory during the interaction, if required, based on the new information gathered (point 3).\n```\n\n#### Running the Server Locally 💻\n\nTo start the Knowledge Graph Memory Server, run:\n\n```bash\nnpm run build\nnode dist/index.js\n```\n\nThe server will listen for requests via standard input/output.\n\n## API Endpoints 🔌\n\nThe server exposes several tools that can be called with specific parameters:\n\n- **Get Current Time** ⏰\n- **Set Memory File Path** 📁\n- **Create Entities** ➕\n- **Create Relations** 🔗\n- **Add Observations** 📝\n- **Delete Entities** ❌\n- **Delete Observations** 🗑️\n- **Delete Relations** 🔗\n- **Read Graph** 📖\n- **Search Nodes** 🔍\n- **Open Nodes** 🔓\n\n## Acknowledgments 🙏\n\n- Inspired by the Memory server from Anthropic\n- Powered by PouchDB for robust data storage 💾\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pouchdb",
        "memory",
        "manage",
        "memory pouchdb",
        "pouchdb manage",
        "seamlessly pouchdb"
      ],
      "category": "memory-management"
    },
    "bornpresident--Volatility-MCP-Server": {
      "owner": "bornpresident",
      "name": "Volatility-MCP-Server",
      "url": "https://github.com/bornpresident/Volatility-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/bornpresident.webp",
      "description": "Analyze memory dumps using natural language queries to facilitate forensic investigations, reducing the need for technical expertise while accelerating the analysis process and improving cybersecurity responses.",
      "stars": 22,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-23T07:54:57Z",
      "readme_content": "# Volatility MCP Server\n\nA Model Context Protocol (MCP) server that integrates Volatility 3 memory forensics framework with Claude and other MCP-compatible LLMs.\n\n## Why This Matters\n\nIn India, digital forensic investigators face a massive backlog of cases due to the country's large population and rising cybercrime rates. This tool helps address this challenge by:\n\n- Allowing investigators to analyze memory dumps using simple natural language instead of complex commands\n- Reducing the technical expertise needed to perform memory forensics\n- Accelerating the analysis process through automation\n- Helping clear case backlogs and deliver faster results to the judicial system\n\nBy making memory forensics more accessible, this tool can significantly reduce the burden on forensic experts and improve cybersecurity response across India.\n\n## Overview\n\nThis project bridges the powerful memory forensics capabilities of the Volatility 3 Framework with Large Language Models (LLMs) through the Model Context Protocol (MCP). It allows you to perform memory forensics analysis using natural language by exposing Volatility plugins as MCP tools that can be invoked directly by Claude or other MCP-compatible LLMs.\n\n## Features\n\n- **Natural Language Memory Forensics**: Ask Claude to analyze memory dumps using natural language\n- **Process Analysis**: Examine running processes, parent-child relationships, and hidden processes\n- **Network Forensics**: Identify network connections in memory dumps\n- **Malware Detection**: Find potential code injection and other malicious artifacts\n- **DLL Analysis**: Examine loaded DLLs and modules\n- **File Objects**: Scan for file objects in memory\n- **Custom Plugins**: Run any Volatility plugin with custom arguments\n- **Memory Dump Discovery**: Automatically find memory dumps in a directory\n\n## Requirements\n\n- Python 3.10 or higher\n- Volatility 3 Framework\n- Claude Desktop or other MCP-compatible client\n- MCP Python SDK (`mcp` package)\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/volatility-mcp-server.git\n   ```\n\n2. Install the required Python packages:\n   ```bash\n   pip install mcp httpx\n   ```\n\n3. Configure the Volatility path in the script:\n   - Open `volatility_mcp_server.py` and update the `VOLATILITY_DIR` variable to point to your Volatility 3 installation path.\n\n4. Configure Claude Desktop:\n   - Open your Claude Desktop configuration file located at:\n     - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the server configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"volatility\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/path/to/volatility_mcp_server.py\"\n         ],\n         \"env\": {\n           \"PYTHONPATH\": \"/path/to/volatility3\"\n         }\n       }\n     }\n   }\n   ```\n   - Replace `/path/to/` with the actual path to your files.\n\n5. Restart Claude Desktop to apply the changes.\n\n## Usage\n\nAfter setup, you can simply ask Claude natural language questions about your memory dumps:\n\n- \"List all processes in the memory dump at C:\\path\\to\\dump.vmem\"\n- \"Show me the network connections in C:\\path\\to\\dump.vmem\"\n- \"Run malfind to check for code injection in the memory dump\"\n- \"What DLLs are loaded in process ID 4328?\"\n- \"Check for hidden processes in C:\\path\\to\\dump.vmem\"\n\n## Available Tools\n\nThe server exposes the following Volatility plugins as MCP tools:\n\n1. `list_available_plugins` - Shows all Volatility plugins you can use\n2. `get_image_info` - Provides information about a memory dump file\n3. `run_pstree` - Shows the process hierarchy\n4. `run_pslist` - Lists processes from the process list\n5. `run_psscan` - Scans for processes including ones that might be hidden\n6. `run_netscan` - Shows network connections in the memory dump\n7. `run_malfind` - Detects potential code injection\n8. `run_cmdline` - Shows command line arguments for processes\n9. `run_dlllist` - Lists loaded DLLs for processes\n10. `run_handles` - Shows file handles and other system handles\n11. `run_filescan` - Scans for file objects in memory\n12. `run_memmap` - Shows the memory map for a specific process\n13. `run_custom_plugin` - Run any Volatility plugin with custom arguments\n14. `list_memory_dumps` - Find memory dumps in a directory\n\n## Memory Forensics Workflow\n\nThis MCP server enables a streamlined memory forensics workflow:\n\n1. **Initial Triage**:\n   - \"Show me the process tree in memory.vmem\"\n   - \"List all network connections in memory.vmem\"\n\n2. **Suspicious Process Investigation**:\n   - \"What command line was used to start process 1234?\"\n   - \"Show me all the DLLs loaded by process 1234\"\n   - \"What file handles are open in process 1234?\"\n\n3. **Malware Hunting**:\n   - \"Run malfind on memory.vmem to check for code injection\"\n   - \"Show me processes with unusual parent-child relationships\"\n   - \"Find hidden processes in memory.vmem\"\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Path Problems**:\n   - Make sure all paths are absolute and use double backslashes in Windows paths\n   - Check that the memory dump file exists and is readable\n\n2. **Permission Issues**:\n   - Run Claude Desktop as Administrator\n   - Check that Python and the Volatility directory have proper permissions\n\n3. **Volatility Errors**:\n   - Make sure Volatility 3 works correctly on its own\n   - Try running the same command directly in your command line\n\n4. **MCP Errors**:\n   - Check Claude Desktop logs for MCP errors\n   - Make sure the MCP Python package is installed correctly\n\n## Extending\n\nThis server can be extended by:\n\n1. Adding more Volatility plugins\n2. Creating custom analysis workflows\n3. Integrating with other forensic tools\n4. Adding report generation capabilities\n\n## License\n\n[MIT License](LICENSE)\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cybersecurity",
        "forensic",
        "memory",
        "memory dumps",
        "analyze memory",
        "server analyze"
      ],
      "category": "memory-management"
    },
    "coleam00--mcp-mem0": {
      "owner": "coleam00",
      "name": "mcp-mem0",
      "url": "https://github.com/coleam00/mcp-mem0",
      "imageUrl": "/freedevtools/mcp/pfp/coleam00.webp",
      "description": "Enables AI agents to store, retrieve, and search memories using semantic indexing for efficient memory management, enhancing contextual awareness.",
      "stars": 576,
      "forks": 215,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T22:50:14Z",
      "readme_content": "<h1 align=\"center\">MCP-Mem0: Long-Term Memory for AI Agents</h1>\n\n<p align=\"center\">\n  \n</p>\n\nA template implementation of the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server integrated with [Mem0](https://mem0.ai) for providing AI agents with persistent memory capabilities.\n\nUse this as a reference point to build your MCP servers yourself, or give this as an example to an AI coding assistant and tell it to follow this example for structure and code correctness!\n\n## Overview\n\nThis project demonstrates how to build an MCP server that enables AI agents to store, retrieve, and search memories using semantic search. It serves as a practical template for creating your own MCP servers, simply using Mem0 and a practical example.\n\nThe implementation follows the best practices laid out by Anthropic for building MCP servers, allowing seamless integration with any MCP-compatible client.\n\n## Features\n\nThe server provides three essential memory management tools:\n\n1. **`save_memory`**: Store any information in long-term memory with semantic indexing\n2. **`get_all_memories`**: Retrieve all stored memories for comprehensive context\n3. **`search_memories`**: Find relevant memories using semantic search\n\n## Prerequisites\n\n- Python 3.12+\n- Supabase or any PostgreSQL database (for vector storage of memories)\n- API keys for your chosen LLM provider (OpenAI, OpenRouter, or Ollama)\n- Docker if running the MCP server as a container (recommended)\n\n## Installation\n\n### Using uv\n\n1. Install uv if you don't have it:\n   ```bash\n   pip install uv\n   ```\n\n2. Clone this repository:\n   ```bash\n   git clone https://github.com/coleam00/mcp-mem0.git\n   cd mcp-mem0\n   ```\n\n3. Install dependencies:\n   ```bash\n   uv pip install -e .\n   ```\n\n4. Create a `.env` file based on `.env.example`:\n   ```bash\n   cp .env.example .env\n   ```\n\n5. Configure your environment variables in the `.env` file (see Configuration section)\n\n### Using Docker (Recommended)\n\n1. Build the Docker image:\n   ```bash\n   docker build -t mcp/mem0 --build-arg PORT=8050 .\n   ```\n\n2. Create a `.env` file based on `.env.example` and configure your environment variables\n\n## Configuration\n\nThe following environment variables can be configured in your `.env` file:\n\n| Variable | Description | Example |\n|----------|-------------|----------|\n| `TRANSPORT` | Transport protocol (sse or stdio) | `sse` |\n| `HOST` | Host to bind to when using SSE transport | `0.0.0.0` |\n| `PORT` | Port to listen on when using SSE transport | `8050` |\n| `LLM_PROVIDER` | LLM provider (openai, openrouter, or ollama) | `openai` |\n| `LLM_BASE_URL` | Base URL for the LLM API | `https://api.openai.com/v1` |\n| `LLM_API_KEY` | API key for the LLM provider | `sk-...` |\n| `LLM_CHOICE` | LLM model to use | `gpt-4o-mini` |\n| `EMBEDDING_MODEL_CHOICE` | Embedding model to use | `text-embedding-3-small` |\n| `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:port/db` |\n\n## Running the Server\n\n### Using uv\n\n#### SSE Transport\n\n```bash\n# Set TRANSPORT=sse in .env then:\nuv run src/main.py\n```\n\nThe MCP server will essentially be run as an API endpoint that you can then connect to with config shown below.\n\n#### Stdio Transport\n\nWith stdio, the MCP client iself can spin up the MCP server, so nothing to run at this point.\n\n### Using Docker\n\n#### SSE Transport\n\n```bash\ndocker run --env-file .env -p:8050:8050 mcp/mem0\n```\n\nThe MCP server will essentially be run as an API endpoint within the container that you can then connect to with config shown below.\n\n#### Stdio Transport\n\nWith stdio, the MCP client iself can spin up the MCP server container, so nothing to run at this point.\n\n## Integration with MCP Clients\n\n### SSE Configuration\n\nOnce you have the server running with SSE transport, you can connect to it using this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8050/sse\"\n    }\n  }\n}\n```\n\n> **Note for Windsurf users**: Use `serverUrl` instead of `url` in your configuration:\n> ```json\n> {\n>   \"mcpServers\": {\n>     \"mem0\": {\n>       \"transport\": \"sse\",\n>       \"serverUrl\": \"http://localhost:8050/sse\"\n>     }\n>   }\n> }\n> ```\n\n> **Note for n8n users**: Use host.docker.internal instead of localhost since n8n has to reach outside of it's own container to the host machine:\n> \n> So the full URL in the MCP node would be: http://host.docker.internal:8050/sse\n\nMake sure to update the port if you are using a value other than the default 8050.\n\n### Python with Stdio Configuration\n\nAdd this server to your MCP configuration for Claude Desktop, Windsurf, or any other MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0\": {\n      \"command\": \"your/path/to/mcp-mem0/.venv/Scripts/python.exe\",\n      \"args\": [\"your/path/to/mcp-mem0/src/main.py\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"LLM_PROVIDER\": \"openai\",\n        \"LLM_BASE_URL\": \"https://api.openai.com/v1\",\n        \"LLM_API_KEY\": \"YOUR-API-KEY\",\n        \"LLM_CHOICE\": \"gpt-4o-mini\",\n        \"EMBEDDING_MODEL_CHOICE\": \"text-embedding-3-small\",\n        \"DATABASE_URL\": \"YOUR-DATABASE-URL\"\n      }\n    }\n  }\n}\n```\n\n### Docker with Stdio Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"--rm\", \"-i\", \n               \"-e\", \"TRANSPORT\", \n               \"-e\", \"LLM_PROVIDER\", \n               \"-e\", \"LLM_BASE_URL\", \n               \"-e\", \"LLM_API_KEY\", \n               \"-e\", \"LLM_CHOICE\", \n               \"-e\", \"EMBEDDING_MODEL_CHOICE\", \n               \"-e\", \"DATABASE_URL\", \n               \"mcp/mem0\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"LLM_PROVIDER\": \"openai\",\n        \"LLM_BASE_URL\": \"https://api.openai.com/v1\",\n        \"LLM_API_KEY\": \"YOUR-API-KEY\",\n        \"LLM_CHOICE\": \"gpt-4o-mini\",\n        \"EMBEDDING_MODEL_CHOICE\": \"text-embedding-3-small\",\n        \"DATABASE_URL\": \"YOUR-DATABASE-URL\"\n      }\n    }\n  }\n}\n```\n\n## Building Your Own Server\n\nThis template provides a foundation for building more complex MCP servers. To build your own:\n\n1. Add your own tools by creating methods with the `@mcp.tool()` decorator\n2. Create your own lifespan function to add your own dependencies (clients, database connections, etc.)\n3. Modify the `utils.py` file for any helper functions you need for your MCP server\n4. Feel free to add prompts and resources as well  with `@mcp.resource()` and `@mcp.prompt()`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "ai",
        "memories",
        "search memories",
        "memory management",
        "memories using"
      ],
      "category": "memory-management"
    },
    "davioliveeira--memory-mcp": {
      "owner": "davioliveeira",
      "name": "memory-mcp",
      "url": "https://github.com/davioliveeira/memory-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/davioliveeira.webp",
      "description": "Store and retrieve memories efficiently using SQLite as the backend. Offers tools for managing memories, including functions to remember, get, list, update, and delete entries, along with command-line inspection capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-06T15:44:43Z",
      "readme_content": "# Memory MCP\n\nA Model Context Protocol server for storing and retrieving memories using low-level Server implementation and SQLite storage.\n\n## Installation\n\nThis project uses [uv](https://github.com/astral-sh/uv) for dependency management instead of pip. uv is a fast, reliable Python package installer and resolver.\n\nInstall using uv:\n\n```bash\nuv pip install memory-mcp\n```\n\nOr install directly from source:\n\n```bash\nuv pip install .\n```\n\nFor development:\n\n```bash\nuv pip install -e \".[dev]\"\n```\n\nIf you don't have uv installed, you can install it following the [official instructions](https://github.com/astral-sh/uv#installation).\n\n## Usage\n\n### Running the server\n\n```bash\nmemory-mcp\n```\n\nThis will start the MCP server that allows you to store and retrieve memories.\n\n### Available Tools\n\nThe Memory MCP provides the following tools:\n\n- `remember`: Store a new memory with a title and content\n- `get_memory`: Retrieve a specific memory by ID or title\n- `list_memories`: List all stored memories\n- `update_memory`: Update an existing memory\n- `delete_memory`: Delete a memory\n\n## Debugging with MCP Inspect\n\nMCP provides a handy command-line tool called `mcp inspect` that allows you to debug and interact with your MCP server directly.\n\n### Setup\n\n1. First, make sure the MCP CLI tools are installed:\n\n```bash\nuv pip install mcp[cli]\n```\n\n2. Start the Memory MCP server in one terminal:\n\n```bash\nmemory-mcp\n```\n\n3. In another terminal, connect to the running server using `mcp inspect`:\n\n```bash\nmcp inspect\n```\n\n### Using MCP Inspect\n\nOnce connected, you can:\n\n#### List available tools\n\n```\n> tools\n```\n\nThis will display all the tools provided by the Memory MCP server.\n\n#### Call a tool\n\nTo call a tool, use the `call` command followed by the tool name and any required arguments:\n\n```\n> call remember title=\"Meeting Notes\" content=\"Discussed project timeline and milestones.\"\n```\n\n```\n> call list_memories\n```\n\n```\n> call get_memory memory_id=1\n```\n\n```\n> call update_memory memory_id=1 title=\"Updated Title\" content=\"Updated content.\"\n```\n\n```\n> call delete_memory memory_id=1\n```\n\n#### Debug Mode\n\nYou can enable debug mode to see detailed request and response information:\n\n```\n> debug on\n```\n\nThis helps you understand exactly what data is being sent to and received from the server.\n\n#### Exploring Tool Schemas\n\nTo view the schema for a specific tool:\n\n```\n> tool remember\n```\n\nThis shows the input schema, required parameters, and description for the tool.\n\n### Troubleshooting\n\nIf you encounter issues:\n\n1. Check the server logs in the terminal where your server is running for any error messages.\n2. In the MCP inspect terminal, enable debug mode with `debug on` to see raw requests and responses.\n3. Ensure the tool parameters match the expected schema (check with the `tool` command).\n4. If the server crashes, check for any uncaught exceptions in the server terminal.\n\n## Development\n\nTo contribute to the project, install the development dependencies:\n\n```bash\nuv pip install -e \".[dev]\"\n```\n\n### Managing Dependencies\n\nThis project uses `uv.lock` file to lock dependencies. To update dependencies:\n\n```bash\nuv pip compile pyproject.toml -o uv.lock\n```\n\n### Running tests\n\n```bash\npython -m pytest\n```\n\n### Code formatting\n\n```bash\nblack memory_mcp tests\n```\n\n### Linting\n\n```bash\nruff check memory_mcp tests\n```\n\n### Type checking\n\n```bash\nmypy memory_mcp\n``` ",
      "npm_url": "https://www.npmjs.com/package/memory-mcp",
      "npm_downloads": 113,
      "keywords": [
        "sqlite",
        "memory",
        "memories",
        "memory management",
        "managing memories",
        "retrieve memories"
      ],
      "category": "memory-management"
    },
    "ddkang1--mcp-mem": {
      "owner": "ddkang1",
      "name": "mcp-mem",
      "url": "https://github.com/ddkang1/mcp-mem",
      "imageUrl": "/freedevtools/mcp/pfp/ddkang1.webp",
      "description": "Provides session-based memory management for chat applications using an efficient knowledge graph to search and retrieve information from multiple sources, including uploaded files.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-29T09:51:38Z",
      "readme_content": "# MCP Memory\n\nA Model Context Protocol (MCP) server implementing memory solutions for data-rich applications with efficient knowledge graph capabilities.\n\n## Overview\n\nThis MCP server implements a memory solution for data-rich applications that involve searching information from many sources including uploaded files. It uses HippoRAG internally to manage memory through an efficient knowledge graph. HippoRAG is a required dependency for this package.\n\n## Features\n\n- **Session-based Memory**: Create and manage memory for specific chat sessions\n- **Efficient Knowledge Graph**: Uses HippoRAG for memory management\n- **Multiple Transport Support**: Works with both stdio and SSE transports\n- **Search Capabilities**: Search information from various sources including uploaded files\n- **Automatic Resource Management**: TTL-based cleanup for both sessions and memory instances\n\n## Installation\n\nInstall from PyPI:\n\n```bash\npip install mcp-mem hipporag\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/ddkang1/mcp-mem.git\ncd mcp-mem\npip install -e .\npip install hipporag\n```\n\nNote: HippoRAG is a required dependency for mcp-mem to function.\n\n## Usage\n\nYou can run the MCP server directly:\n\n```bash\nmcp-mem\n```\n\nBy default, it uses stdio transport. To use SSE transport:\n\n```bash\nmcp-mem --sse\n```\n\nYou can also specify host and port for SSE transport:\n\n```bash\nmcp-mem --sse --host 127.0.0.1 --port 3001\n```\n\n## Configuration\n\n### Basic Configuration\n\nTo use this tool with Claude in Windsurf, add the following configuration to your MCP config file:\n\n```json\n\"memory\": {\n    \"command\": \"/path/to/mcp-mem\",\n    \"args\": [],\n    \"type\": \"stdio\",\n    \"pollingInterval\": 30000,\n    \"startupTimeout\": 30000,\n    \"restartOnFailure\": true\n}\n```\n\nThe `command` field should point to the directory where you installed the python package using pip.\n\n### Environment Variable Configuration\n\nYou can configure the LLM and embedding models used by mcp-mem through environment variables:\n\n- `EMBEDDING_MODEL_NAME`: Name of the embedding model to use (default: \"text-embedding-3-large\")\n- `EMBEDDING_BASE_URL`: Base URL for the embedding API (optional)\n- `LLM_NAME`: Name of the LLM model to use (default: \"gpt-4o-mini\")\n- `LLM_BASE_URL`: Base URL for the LLM API (optional)\n- `OPENAI_API_KEY`: OpenAI API key (required)\n\n### Memory Management Configuration\n\nThe server includes automatic resource management features:\n\n- **Session TTL**: Automatically removes session directories after a specified number of days of inactivity.\n  Set using the `session_ttl_days` configuration parameter (default: None - disabled).\n\n- **Instance TTL**: Automatically offloads HippoRAG instances from memory after a specified period of inactivity.\n  Set using the `instance_ttl_minutes` configuration parameter (default: 30 minutes).\n  \n  This feature helps manage memory usage by unloading inactive instances while preserving the underlying data.\n  When an offloaded instance is accessed again, it will be automatically reloaded from disk.\n\nExample usage:\n\n```bash\nEMBEDDING_MODEL_NAME=\"your-model\" LLM_NAME=\"your-llm\" mcp-mem\n```\n\nFor convenience, you can use the provided example script:\n\n```bash\n./examples/run_with_env_vars.sh\n```\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n- **create_memory**: Create a new memory for a given chat session\n- **store_memory**: Add memory to a specific session\n- **retrieve_memory**: Retrieve memory from a specific session\n\n## Development\n\n### Installation for Development\n\n```bash\ngit clone https://github.com/ddkang1/mcp-mem.git\ncd mcp-mem\npip install -e \".[dev]\"\n```\n\n### Running Tests\n\n```bash\npytest\n```\n\n### Code Style\n\nThis project uses Black for formatting, isort for import sorting, and flake8 for linting:\n\n```bash\nblack src tests\nisort src tests\nflake8 src tests\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "ddkang1",
        "chat",
        "memory management",
        "management chat",
        "mem provides"
      ],
      "category": "memory-management"
    },
    "delorenj--mcp-qdrant-memory": {
      "owner": "delorenj",
      "name": "mcp-qdrant-memory",
      "url": "https://github.com/delorenj/mcp-qdrant-memory",
      "imageUrl": "/freedevtools/mcp/pfp/delorenj.webp",
      "description": "A knowledge graph implementation that supports semantic search with a Qdrant vector database, enabling effective graph-based representation of entities and their relations. It includes features for file-based persistence and utilizes OpenAI embeddings for enhanced semantic similarity.",
      "stars": 16,
      "forks": 18,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-27T03:40:16Z",
      "readme_content": "# MCP Memory Server with Qdrant Persistence\n[![smithery badge](https://smithery.ai/badge/@delorenj/mcp-qdrant-memory)](https://smithery.ai/server/@delorenj/mcp-qdrant-memory)\n\nThis MCP server provides a knowledge graph implementation with semantic search capabilities powered by Qdrant vector database.\n\n## Features\n\n- Graph-based knowledge representation with entities and relations\n- File-based persistence (memory.json)\n- Semantic search using Qdrant vector database\n- OpenAI embeddings for semantic similarity\n- HTTPS support with reverse proxy compatibility\n- Docker support for easy deployment\n\n## Environment Variables\n\nThe following environment variables are required:\n\n```bash\n# OpenAI API key for generating embeddings\nOPENAI_API_KEY=your-openai-api-key\n\n# Qdrant server URL (supports both HTTP and HTTPS)\nQDRANT_URL=https://your-qdrant-server\n\n# Qdrant API key (if authentication is enabled)\nQDRANT_API_KEY=your-qdrant-api-key\n\n# Name of the Qdrant collection to use\nQDRANT_COLLECTION_NAME=your-collection-name\n```\n\n## Setup\n\n### Local Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n### Docker Setup\n\n1. Build the Docker image:\n```bash\ndocker build -t mcp-qdrant-memory .\n```\n\n2. Run the Docker container with required environment variables:\n```bash\ndocker run -d \\\n  -e OPENAI_API_KEY=your-openai-api-key \\\n  -e QDRANT_URL=http://your-qdrant-server:6333 \\\n  -e QDRANT_COLLECTION_NAME=your-collection-name \\\n  -e QDRANT_API_KEY=your-qdrant-api-key \\\n  --name mcp-qdrant-memory \\\n  mcp-qdrant-memory\n```\n\n### Add to MCP settings:\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"/bin/zsh\",\n      \"args\": [\"-c\", \"cd /path/to/server && node dist/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\",\n        \"QDRANT_API_KEY\": \"your-qdrant-api-key\",\n        \"QDRANT_URL\": \"http://your-qdrant-server:6333\",\n        \"QDRANT_COLLECTION_NAME\": \"your-collection-name\"\n      },\n      \"alwaysAllow\": [\n        \"create_entities\",\n        \"create_relations\",\n        \"add_observations\",\n        \"delete_entities\",\n        \"delete_observations\",\n        \"delete_relations\",\n        \"read_graph\",\n        \"search_similar\"\n      ]\n    }\n  }\n}\n```\n\n## Tools\n\n### Entity Management\n- `create_entities`: Create multiple new entities\n- `create_relations`: Create relations between entities\n- `add_observations`: Add observations to entities\n- `delete_entities`: Delete entities and their relations\n- `delete_observations`: Delete specific observations\n- `delete_relations`: Delete specific relations\n- `read_graph`: Get the full knowledge graph\n\n### Semantic Search\n- `search_similar`: Search for semantically similar entities and relations\n  ```typescript\n  interface SearchParams {\n    query: string;     // Search query text\n    limit?: number;    // Max results (default: 10)\n  }\n  ```\n\n## Implementation Details\n\nThe server maintains two forms of persistence:\n\n1. File-based (memory.json):\n   - Complete knowledge graph structure\n   - Fast access to full graph\n   - Used for graph operations\n\n2. Qdrant Vector DB:\n   - Semantic embeddings of entities and relations\n   - Enables similarity search\n   - Automatically synchronized with file storage\n\n### Synchronization\n\nWhen entities or relations are modified:\n1. Changes are written to memory.json\n2. Embeddings are generated using OpenAI\n3. Vectors are stored in Qdrant\n4. Both storage systems remain consistent\n\n### Search Process\n\nWhen searching:\n1. Query text is converted to embedding\n2. Qdrant performs similarity search\n3. Results include both entities and relations\n4. Results are ranked by semantic similarity\n\n## Example Usage\n\n```typescript\n// Create entities\nawait client.callTool(\"create_entities\", {\n  entities: [{\n    name: \"Project\",\n    entityType: \"Task\",\n    observations: [\"A new development project\"]\n  }]\n});\n\n// Search similar concepts\nconst results = await client.callTool(\"search_similar\", {\n  query: \"development tasks\",\n  limit: 5\n});\n```\n\n## HTTPS and Reverse Proxy Configuration\n\nThe server supports connecting to Qdrant through HTTPS and reverse proxies. This is particularly useful when:\n- Running Qdrant behind a reverse proxy like Nginx or Apache\n- Using self-signed certificates\n- Requiring custom SSL/TLS configurations\n\n### Setting up with a Reverse Proxy\n\n1. Configure your reverse proxy (example using Nginx):\n```nginx\nserver {\n    listen 443 ssl;\n    server_name qdrant.yourdomain.com;\n\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n\n    location / {\n        proxy_pass http://localhost:6333;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n```\n\n2. Update your environment variables:\n```bash\nQDRANT_URL=https://qdrant.yourdomain.com\n```\n\n### Security Considerations\n\nThe server implements robust HTTPS handling with:\n- Custom SSL/TLS configuration\n- Proper certificate verification options\n- Connection pooling and keepalive\n- Automatic retry with exponential backoff\n- Configurable timeouts\n\n### Troubleshooting HTTPS Connections\n\nIf you experience connection issues:\n\n1. Verify your certificates:\n```bash\nopenssl s_client -connect qdrant.yourdomain.com:443\n```\n\n2. Test direct connectivity:\n```bash\ncurl -v https://qdrant.yourdomain.com/collections\n```\n\n3. Check for any proxy settings:\n```bash\nenv | grep -i proxy\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp-qdrant-memory",
      "npm_downloads": 280,
      "keywords": [
        "semantic",
        "database",
        "memory",
        "knowledge graph",
        "semantic search",
        "qdrant memory"
      ],
      "category": "memory-management"
    },
    "doggybee--mcp-server-memo": {
      "owner": "doggybee",
      "name": "mcp-server-memo",
      "url": "https://github.com/doggybee/mcp-server-memo",
      "imageUrl": "/freedevtools/mcp/pfp/doggybee.webp",
      "description": "Manage session summaries and memos for large language models with persistent local storage and version tracking. Store, retrieve, and list detailed session histories to enhance context retention for improved memory capabilities.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T09:10:48Z",
      "readme_content": "# MCP Server Memo\n\n![TypeScript](https://img.shields.io/badge/language-TypeScript-blue)\n![License](https://img.shields.io/github/license/doggybee/mcp-server-memo)\n![Node.js](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)\n![MCP SDK](https://img.shields.io/badge/MCP_SDK-1.9.0-orange)\n\nA lightweight MCP (Model Context Protocol) server for managing rich session summaries and memos for LLMs like Claude. This server provides persistent storage using the local filesystem, with support for session history version tracking, and offers tools for storing, retrieving, and listing summaries.\n\n## Overview\n\nMCP Server Memo is designed as a memory assistant for LLMs, allowing them to store and retrieve detailed session records through the MCP tool interface. The server:\n\n- **Preserves History** - All historical versions of a session (same sessionId) are preserved, not just the latest version\n- **Time-Ordered** - Multiple versions of sessions are organized chronologically, making it easy to track conversation development\n- **Local Storage** - Uses the local filesystem without requiring an external database\n- **MCP Compliant** - Follows the Model Context Protocol specification to provide tool interfaces\n- **Performance Optimized** - Optimized for file I/O and concurrent operations\n- **Minimal Dependencies** - Clean design that's easy to maintain and extend\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/doggybee/mcp-server-memo.git\ncd mcp-server-memo\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\nThe server uses the following configuration options:\n\n- `MCP_SUMMARY_DIR`: Directory for storing summaries (default: `./summaries/`)\n\nYou can set these options via environment variables:\n\n```bash\nexport MCP_SUMMARY_DIR=\"/path/to/summaries\"\n```\n\n## Running the Server\n\n```bash\n# Standard startup\nnpm start\n\n# Development mode (with auto-reload)\nnpm run dev\n\n# Start with logging to file\nnpm run start:log\n```\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n### 1. upsertSummary\n\nCreates a new version of a session summary without deleting previous versions.\n\n**Parameters:**\n- `sessionId` (string, required): Unique identifier for the conversation session. **It is the client application's responsibility to generate this ID.** It should be generated once at the beginning of a new logical conversation session. **Recommendation:** Use a standard **UUID (Version 4)** library available in your programming language to ensure uniqueness. The client must then reuse the *same* generated ID for all subsequent `upsertSummary` calls pertaining to that specific session.\n- `summary` (string, required): The detailed content of the session chronicle/log. Each call will create a new version in the session history rather than overwriting previous versions.\n- `title` (string, optional): A short, descriptive title for the session.\n- `tags` (string[], optional): Keywords or tags to categorize the session.\n\n**Behavior:**\n- Creates a new file with a new timestamp\n- Preserves all previous versions\n- Returns the timestamp of the new version\n\n### 2. getSummaryTool\n\nRetrieves the latest version of a specific session summary.\n\n**Parameters:**\n- `sessionId` (string, required): The unique ID of the session summary to retrieve.\n- `maxLength` (number, optional): If provided, truncate the retrieved summary text to this maximum length.\n\n**Returns:**\n- The latest session summary object (in JSON format)\n\n### 3. listSummariesTool\n\nLists available summaries (only the latest version of each session), with support for filtering, sorting, and pagination.\n\n**Parameters:**\n- `tag` (string, optional): Filter sessions by a specific tag.\n- `limit` (number, optional): Limit results.\n- `offset` (number, optional): Offset for pagination.\n- `sortBy` (string, optional): Sort field ('lastUpdated' or 'title'). Default: 'lastUpdated'.\n- `order` (string, optional): Sort order ('asc' or 'desc'). Default: 'desc'.\n\n**Returns:**\n- List of summary metadata objects (in JSON format), with only the latest version per session\n\n### 4. updateMetadata\n\nUpdates only the metadata (title and/or tags) of the latest version of a session without changing the summary content or timestamp.\n\n**Parameters:**\n- `sessionId` (string, required): The unique ID of the session whose metadata to update.\n- `title` (string, optional): New title. If omitted, title remains unchanged.\n- `tags` (string[], optional): New tags array. If omitted, tags remain unchanged.\n\n**Note:** At least one of `title` or `tags` must be provided.\n\n**Behavior:**\n- Does not update the timestamp in the filename or the `lastUpdated` field\n- Only modifies the specified metadata fields\n- Does not affect the summary content\n\n### 5. appendSummary\n\nAppends content to a session summary, creating a new version that includes previous content plus new content.\n\n**Parameters:**\n- `sessionId` (string, required): Unique identifier for the conversation session.\n- `content` (string, required): The content to append to the session. This will be added to the existing content and saved as a new version.\n- `title` (string, optional): Optional title for the session.\n- `tags` (string[], optional): Optional tags for categorizing the session.\n\n**Behavior:**\n- Reads the existing latest summary (if any)\n- Adds two newlines between content and appends the new content\n- Creates a new file with a new timestamp\n- Preserves all previous versions\n\n### 6. listAllSummariesTool\n\nLists all available summaries with basic information (only the latest version of each session).\n\n**Parameters:**\n- None\n\n**Returns:**\n- List of all available summaries with basic information (in JSON format)\n\n### 7. getSessionHistory\n\nRetrieves all historical versions of a specific session, ordered from newest to oldest.\n\n**Parameters:**\n- `sessionId` (string, required): The unique ID of the session to retrieve history for.\n\n**Returns:**\n- List of all versions with their full content (in JSON format)\n\n## Client Workflow Example\n\nHere's an example of how a client application might interact with this server:\n\n```javascript\nimport { v4 as uuidv4 } from 'uuid';\nimport { Client } from '@modelcontextprotocol/sdk/client/index.js';\nimport { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\n\n// Initialize client\nconst transport = new StdioClientTransport({\n  command: 'node',\n  args: ['dist/index.js'],\n  cwd: process.cwd()\n});\n\nconst client = new Client({\n  name: 'example-client',\n  version: '1.0.0'\n});\n\nawait client.connect(transport);\n\n// Function to generate new session ID (done once per logical session)\nfunction createNewSession() {\n  return uuidv4();\n}\n\n// Function to append to an existing session\nasync function appendToSession(sessionId, newContent) {\n  return client.callTool({\n    name: \"appendSummary\",\n    arguments: {\n      sessionId,\n      content: newContent,\n      title: \"Example Session\",\n      tags: [\"example\", \"demo\"]\n    }\n  });\n}\n\n// Function to get session history\nasync function getSessionHistory(sessionId) {\n  const response = await client.callTool({\n    name: \"getSessionHistory\",\n    arguments: { sessionId }\n  });\n  \n  const result = JSON.parse(response.content[0].text);\n  if (result.success) {\n    return result.history;\n  }\n  \n  throw new Error(result.error || \"Failed to get session history\");\n}\n\n// Usage example\nconst sessionId = createNewSession();\n\n// Add initial content\nawait appendToSession(sessionId, \"Initial conversation data\");\n\n// Add more content later in the conversation\nawait appendToSession(sessionId, \"Second part of the conversation\");\nawait appendToSession(sessionId, \"Final part of the conversation\");\n\n// Get the full history\nconst history = await getSessionHistory(sessionId);\nconsole.log(`Session ${sessionId} has ${history.length} versions`);\n```\n\n## Project Structure\n\n```\nmcp-server-memo/\n├── dist/                 # Compiled JavaScript output\n├── src/                  # TypeScript source code\n│   ├── config.ts         # Server configuration\n│   ├── index.ts          # Main entry point\n│   ├── storage.ts        # File storage utilities\n│   ├── tools.ts          # MCP tool implementations\n│   └── types.ts          # TypeScript type definitions\n├── summaries/            # Directory for storing session data\n│   └── .gitkeep          # Ensures directory is included in git\n├── package.json          # Project metadata and dependencies\n├── tsconfig.json         # TypeScript configuration\n└── LICENSE               # MIT License file\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memos",
        "memo",
        "memory",
        "server memo",
        "memo manage",
        "session histories"
      ],
      "category": "memory-management"
    },
    "doobidoo--mcp-memory-service": {
      "owner": "doobidoo",
      "name": "mcp-memory-service",
      "url": "https://github.com/doobidoo/mcp-memory-service",
      "imageUrl": "/freedevtools/mcp/pfp/doobidoo.webp",
      "description": "Provides semantic memory and persistent storage using ChromaDB for long-term memory retention and semantic search capabilities, enhancing context maintenance across conversations.",
      "stars": 738,
      "forks": 108,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T09:29:45Z",
      "readme_content": "# MCP Memory Service\n\n[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![GitHub stars](https://img.shields.io/github/stars/doobidoo/mcp-memory-service?style=social)](https://github.com/doobidoo/mcp-memory-service/stargazers)\n[![Production Ready](https://img.shields.io/badge/Production-Ready-brightgreen?style=flat&logo=checkmark)](https://github.com/doobidoo/mcp-memory-service#-in-production)\n\n[![Works with Claude](https://img.shields.io/badge/Works%20with-Claude-blue)](https://claude.ai)\n[![Works with Cursor](https://img.shields.io/badge/Works%20with-Cursor-orange)](https://cursor.sh)\n[![MCP Protocol](https://img.shields.io/badge/MCP-Compatible-4CAF50?style=flat)](https://modelcontextprotocol.io/)\n[![Multi-Client](https://img.shields.io/badge/Multi--Client-13+%20Apps-FF6B35?style=flat)](https://github.com/doobidoo/mcp-memory-service/wiki)\n\n**Universal MCP memory service** with **intelligent memory triggers**, **OAuth 2.1 team collaboration**, and **semantic memory search** for **AI assistants**. Features **Natural Memory Triggers v7.1.0** with 85%+ trigger accuracy, **Claude Code HTTP transport**, **zero-configuration authentication**, and **enterprise security**. Works with **Claude Desktop, VS Code, Cursor, Continue, and 13+ AI applications** with **SQLite-vec** for fast local search and **Cloudflare** for global distribution.\n\n<img width=\"240\" alt=\"MCP Memory Service\" src=\"https://github.com/user-attachments/assets/eab1f341-ca54-445c-905e-273cd9e89555\" />\n\n## 🚀 Quick Start (2 minutes)\n\n### 🧠 **v7.1.0: Natural Memory Triggers for Claude Code**\n\n**🤖 Intelligent Memory Awareness** (Zero Configuration):\n```bash\n# 1. Install MCP Memory Service\ngit clone https://github.com/doobidoo/mcp-memory-service.git\ncd mcp-memory-service && python install.py\n\n# 2. Install Natural Memory Triggers\ncd claude-hooks && python install_hooks.py --natural-triggers\n\n# 3. Test intelligent triggers\nnode memory-mode-controller.js status\n# ✅ Done! Claude Code now automatically detects when you need memory context\n```\n\n**📖 Complete Guide**: [Natural Memory Triggers v7.1.0](https://github.com/doobidoo/mcp-memory-service/wiki/Natural-Memory-Triggers-v7.1.0)\n\n---\n\n### 🆕 **v7.0.0: OAuth 2.1 & Claude Code HTTP Transport**\n\n**🔗 Claude Code Team Collaboration** (Zero Configuration):\n```bash\n# 1. Start OAuth-enabled server\nexport MCP_OAUTH_ENABLED=true\nuv run memory server --http\n\n# 2. Add HTTP transport to Claude Code\nclaude mcp add --transport http memory-service http://localhost:8000/mcp\n\n# ✅ Done! Claude Code automatically handles OAuth registration and team collaboration\n```\n\n**📖 Complete Setup Guide**: [OAuth 2.1 Setup Guide](https://github.com/doobidoo/mcp-memory-service/wiki/OAuth-2.1-Setup-Guide)\n\n---\n\n### Traditional Setup Options\n\n**Universal Installer (Most Compatible):**\n```bash\n# Clone and install with automatic platform detection\ngit clone https://github.com/doobidoo/mcp-memory-service.git\ncd mcp-memory-service\n\n# Lightweight installation (SQLite-vec with ONNX embeddings - recommended)\npython install.py\n\n# Add full ML capabilities (torch + sentence-transformers for advanced features)\npython install.py --with-ml\n\n# Add ChromaDB backend support (includes full ML stack - for multi-client setups)\npython install.py --with-chromadb\n```\n\n**📝 Installation Options Explained:**\n- **Default (recommended)**: Lightweight SQLite-vec with ONNX embeddings - fast, works offline, <100MB dependencies\n- **`--with-ml`**: Adds PyTorch + sentence-transformers for advanced ML features - heavier but more capable\n- **`--with-chromadb`**: Multi-client local server support - use only if you need shared team access\n\n**Docker (Fastest):**\n```bash\n# For MCP protocol (Claude Desktop)\ndocker-compose up -d\n\n# For HTTP API + OAuth (Team Collaboration)\ndocker-compose -f docker-compose.http.yml up -d\n```\n\n**Smithery (Claude Desktop):**\n```bash\n# Auto-install for Claude Desktop\nnpx -y @smithery/cli install @doobidoo/mcp-memory-service --client claude\n```\n\n## ⚠️ v6.17.0+ Script Migration Notice\n\n**Updating from an older version?** Scripts have been reorganized for better maintainability:\n- **Recommended**: Use `python -m mcp_memory_service.server` in your Claude Desktop config (no path dependencies!)\n- **Alternative 1**: Use `uv run memory server` with UV tooling\n- **Alternative 2**: Update path from `scripts/run_memory_server.py` to `scripts/server/run_memory_server.py`\n- **Backward compatible**: Old path still works with a migration notice\n\n## ⚠️ First-Time Setup Expectations\n\nOn your first run, you'll see some warnings that are **completely normal**:\n\n- **\"WARNING: Failed to load from cache: No snapshots directory\"** - The service is checking for cached models (first-time setup)\n- **\"WARNING: Using TRANSFORMERS_CACHE is deprecated\"** - Informational warning, doesn't affect functionality\n- **Model download in progress** - The service automatically downloads a ~25MB embedding model (takes 1-2 minutes)\n\nThese warnings disappear after the first successful run. The service is working correctly! For details, see our [First-Time Setup Guide](docs/first-time-setup.md).\n\n### 🐍 Python 3.13 Compatibility Note\n\n**sqlite-vec** may not have pre-built wheels for Python 3.13 yet. If installation fails:\n- The installer will automatically try multiple installation methods\n- Consider using Python 3.12 for the smoothest experience: `brew install python@3.12`\n- Alternative: Use ChromaDB backend with `--storage-backend chromadb --with-chromadb`\n- See [Troubleshooting Guide](docs/troubleshooting/general.md#python-313-sqlite-vec-issues) for details\n\n### 🍎 macOS SQLite Extension Support\n\n**macOS users** may encounter `enable_load_extension` errors with sqlite-vec:\n- **System Python** on macOS lacks SQLite extension support by default\n- **Solution**: Use Homebrew Python: `brew install python && rehash`\n- **Alternative**: Use pyenv: `PYTHON_CONFIGURE_OPTS='--enable-loadable-sqlite-extensions' pyenv install 3.12.0`\n- **Fallback**: Use sqlite_vec backend (default) or install ChromaDB with `--with-chromadb`\n- See [Troubleshooting Guide](docs/troubleshooting/general.md#macos-sqlite-extension-issues) for details\n\n## 📚 Complete Documentation\n\n**👉 Visit our comprehensive [Wiki](https://github.com/doobidoo/mcp-memory-service/wiki) for detailed guides:**\n\n### 🧠 v7.1.0 Natural Memory Triggers (Latest)\n- **[Natural Memory Triggers v7.1.0 Guide](https://github.com/doobidoo/mcp-memory-service/wiki/Natural-Memory-Triggers-v7.1.0)** - Intelligent automatic memory awareness\n  - ✅ **85%+ trigger accuracy** with semantic pattern detection\n  - ✅ **Multi-tier performance** (50ms instant → 150ms fast → 500ms intensive)\n  - ✅ **CLI management system** for real-time configuration\n  - ✅ **Git-aware context** integration for enhanced relevance\n  - ✅ **Zero-restart installation** with dynamic hook loading\n\n### 🆕 v7.0.0 OAuth & Team Collaboration\n- **[🔐 OAuth 2.1 Setup Guide](https://github.com/doobidoo/mcp-memory-service/wiki/OAuth-2.1-Setup-Guide)** - **NEW!** Complete OAuth 2.1 Dynamic Client Registration guide\n- **[🔗 Integration Guide](https://github.com/doobidoo/mcp-memory-service/wiki/03-Integration-Guide)** - Claude Desktop, **Claude Code HTTP transport**, VS Code, and more\n- **[🛡️ Advanced Configuration](https://github.com/doobidoo/mcp-memory-service/wiki/04-Advanced-Configuration)** - **Updated!** OAuth security, enterprise features\n\n### 🚀 Setup & Installation\n- **[📋 Installation Guide](https://github.com/doobidoo/mcp-memory-service/wiki/01-Installation-Guide)** - Complete installation for all platforms and use cases\n- **[🖥️ Platform Setup Guide](https://github.com/doobidoo/mcp-memory-service/wiki/02-Platform-Setup-Guide)** - Windows, macOS, and Linux optimizations\n- **[⚡ Performance Optimization](https://github.com/doobidoo/mcp-memory-service/wiki/05-Performance-Optimization)** - Speed up queries, optimize resources, scaling\n\n### 🧠 Advanced Topics\n- **[👨‍💻 Development Reference](https://github.com/doobidoo/mcp-memory-service/wiki/06-Development-Reference)** - Claude Code hooks, API reference, debugging\n- **[🔧 Troubleshooting Guide](https://github.com/doobidoo/mcp-memory-service/wiki/07-TROUBLESHOOTING)** - **Updated!** OAuth troubleshooting + common issues\n- **[❓ FAQ](https://github.com/doobidoo/mcp-memory-service/wiki/08-FAQ)** - Frequently asked questions\n- **[📝 Examples](https://github.com/doobidoo/mcp-memory-service/wiki/09-Examples)** - Practical code examples and workflows\n\n### 📂 Internal Documentation\n- **[🏗️ Architecture Specs](docs/architecture/)** - Search enhancement specifications and design documents\n- **[👩‍💻 Development Docs](docs/development/)** - AI agent instructions, release checklist, refactoring notes\n- **[🚀 Deployment Guides](docs/deployment/)** - Docker, dual-service, and production deployment\n- **[📚 Additional Guides](docs/guides/)** - Storage backends, migration, mDNS discovery\n\n## ✨ Key Features\n\n### 🔐 **Enterprise Authentication & Team Collaboration** 🆕\n- **OAuth 2.1 Dynamic Client Registration** - RFC 7591 & RFC 8414 compliant\n- **Claude Code HTTP Transport** - Zero-configuration team collaboration\n- **JWT Authentication** - Enterprise-grade security with scope validation\n- **Auto-Discovery Endpoints** - Seamless client registration and authorization\n- **Multi-Auth Support** - OAuth + API keys + optional anonymous access\n\n### 🧠 **Intelligent Memory Management**\n- **Semantic search** with vector embeddings\n- **Natural language time queries** (\"yesterday\", \"last week\")\n- **Tag-based organization** with smart categorization\n- **Memory consolidation** with dream-inspired algorithms\n\n### 🔗 **Universal Compatibility**\n- **Claude Desktop** - Native MCP integration\n- **Claude Code** - **HTTP transport** + Memory-aware development with hooks\n- **VS Code, Cursor, Continue** - IDE extensions\n- **13+ AI applications** - REST API compatibility\n\n### 💾 **Flexible Storage**\n- **SQLite-vec** - Fast local storage (recommended, lightweight ONNX embeddings)\n- **ChromaDB** - Multi-client collaboration (optional, heavy dependencies)\n- **Cloudflare** - Global edge distribution\n- **Automatic backups** and synchronization\n\n> **Note**: All heavy ML dependencies (PyTorch, sentence-transformers, ChromaDB) are now optional to dramatically reduce build times and image sizes. SQLite-vec uses lightweight ONNX embeddings by default. Install with `--with-ml` for full ML capabilities or `--with-chromadb` for multi-client features.\n\n### 🚀 **Production Ready**\n- **Cross-platform** - Windows, macOS, Linux\n- **Service installation** - Auto-start background operation\n- **HTTPS/SSL** - Secure connections with OAuth 2.1\n- **Docker support** - Easy deployment with team collaboration\n\n## 💡 Basic Usage\n\n### 🔗 **Team Collaboration with OAuth** (v7.0.0+)\n```bash\n# Start OAuth-enabled server for team collaboration\nexport MCP_OAUTH_ENABLED=true\nuv run memory server --http\n\n# Claude Code team members connect via HTTP transport\nclaude mcp add --transport http memory-service http://your-server:8000/mcp\n# → Automatic OAuth discovery, registration, and authentication\n```\n\n### 🧠 **Memory Operations**\n```bash\n# Store a memory\nuv run memory store \"Fixed race condition in authentication by adding mutex locks\"\n\n# Search for relevant memories\nuv run memory recall \"authentication race condition\"\n\n# Search by tags\nuv run memory search --tags python debugging\n\n# Check system health (shows OAuth status)\nuv run memory health\n```\n\n## 🔧 Configuration\n\n### Claude Desktop Integration\n**Recommended approach** - Add to your Claude Desktop config (`~/.claude/config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_memory_service.server\"],\n      \"env\": {\n        \"MCP_MEMORY_STORAGE_BACKEND\": \"sqlite_vec\"\n      }\n    }\n  }\n}\n```\n\n**Alternative approaches:**\n```json\n// Option 1: UV tooling (if using UV)\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/mcp-memory-service\", \"run\", \"memory\", \"server\"],\n      \"env\": {\n        \"MCP_MEMORY_STORAGE_BACKEND\": \"sqlite_vec\"\n      }\n    }\n  }\n}\n\n// Option 2: Direct script path (v6.17.0+)\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/mcp-memory-service/scripts/server/run_memory_server.py\"],\n      \"env\": {\n        \"MCP_MEMORY_STORAGE_BACKEND\": \"sqlite_vec\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n```bash\n# Storage backend (sqlite_vec recommended)\nexport MCP_MEMORY_STORAGE_BACKEND=sqlite_vec\n\n# Enable HTTP API\nexport MCP_HTTP_ENABLED=true\nexport MCP_HTTP_PORT=8000\n\n# Security  \nexport MCP_API_KEY=\"your-secure-key\"\n```\n\n## 🏗️ Architecture\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   AI Clients    │    │  MCP Memory     │    │ Storage Backend │\n│                 │    │  Service v7.0   │    │                 │\n│ • Claude Desktop│◄──►│ • MCP Protocol  │◄──►│ • SQLite-vec    │\n│ • Claude Code   │    │ • HTTP Transport│    │ • ChromaDB      │\n│   (HTTP/OAuth)  │    │ • OAuth 2.1 Auth│    │ • Cloudflare    │\n│ • VS Code       │    │ • Memory Store  │    │ • Hybrid        │\n│ • Cursor        │    │ • Semantic      │    │                 │\n│ • 13+ AI Apps   │    │   Search        │    │                 │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n```\n\n## 🛠️ Development\n\n### Project Structure\n```\nmcp-memory-service/\n├── src/mcp_memory_service/    # Core application\n│   ├── models/                # Data models\n│   ├── storage/               # Storage backends\n│   ├── web/                   # HTTP API & dashboard\n│   └── server.py              # MCP server\n├── scripts/                   # Utilities & installation\n├── tests/                     # Test suite\n└── tools/docker/              # Docker configuration\n```\n\n### Contributing\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes with tests\n4. Submit a pull request\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.\n\n## 🆘 Support\n\n- **📖 Documentation**: [Wiki](https://github.com/doobidoo/mcp-memory-service/wiki) - Comprehensive guides\n- **🐛 Bug Reports**: [GitHub Issues](https://github.com/doobidoo/mcp-memory-service/issues)\n- **💬 Discussions**: [GitHub Discussions](https://github.com/doobidoo/mcp-memory-service/discussions)\n- **🔧 Troubleshooting**: [Troubleshooting Guide](https://github.com/doobidoo/mcp-memory-service/wiki/07-TROUBLESHOOTING)\n- **✅ Configuration Validator**: Run `python scripts/validation/validate_configuration_complete.py` to check your setup\n- **🔄 Backend Sync Tools**: See [scripts/README.md](scripts/README.md#backend-synchronization) for Cloudflare↔SQLite sync\n\n## 📊 In Production\n\n**Real-world metrics from active deployments:**\n- **750+ memories** stored and actively used across teams\n- **<500ms response time** for semantic search (local & HTTP transport)\n- **65% token reduction** in Claude Code sessions with OAuth collaboration\n- **96.7% faster** context setup (15min → 30sec)\n- **100% knowledge retention** across sessions and team members\n- **Zero-configuration** OAuth setup success rate: **98.5%**\n\n## 🏆 Recognition\n\n- [![Smithery](https://smithery.ai/badge/@doobidoo/mcp-memory-service)](https://smithery.ai/server/@doobidoo/mcp-memory-service) **Verified MCP Server**\n- [![Glama AI](https://img.shields.io/badge/Featured-Glama%20AI-blue)](https://glama.ai/mcp/servers/bzvl3lz34o) **Featured AI Tool**\n- **Production-tested** across 13+ AI applications\n- **Community-driven** with real-world feedback and improvements\n\n## 📄 License\n\nApache License 2.0 - see [LICENSE](LICENSE) for details.\n\n---\n\n**Ready to supercharge your AI workflow?** 🚀\n\n👉 **[Start with our Installation Guide](https://github.com/doobidoo/mcp-memory-service/wiki/01-Installation-Guide)** or explore the **[Wiki](https://github.com/doobidoo/mcp-memory-service/wiki)** for comprehensive documentation.\n\n*Transform your AI conversations into persistent, searchable knowledge that grows with you.*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "chromadb",
        "storage",
        "memory persistent",
        "memory management",
        "memory service"
      ],
      "category": "memory-management"
    },
    "drdee--memory-mcp": {
      "owner": "drdee",
      "name": "memory-mcp",
      "url": "https://github.com/drdee/memory-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/drdee.webp",
      "description": "Store and retrieve memories using a command-line interface with a backend powered by SQLite. Manage important information efficiently in applications with memory functionalities.",
      "stars": 7,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T21:25:11Z",
      "readme_content": "# Memory MCP\n\nA Model Context Protocol server for storing and retrieving memories using low-level Server implementation and SQLite storage.\n\n## Installation\n\nThis project uses [uv](https://github.com/astral-sh/uv) for dependency management instead of pip. uv is a fast, reliable Python package installer and resolver.\n\nInstall using uv:\n\n```bash\nuv pip install memory-mcp\n```\n\nOr install directly from source:\n\n```bash\nuv pip install .\n```\n\nFor development:\n\n```bash\nuv pip install -e \".[dev]\"\n```\n\nIf you don't have uv installed, you can install it following the [official instructions](https://github.com/astral-sh/uv#installation).\n\n## Usage\n\n### Running the server\n\n```bash\nmemory-mcp\n```\n\nThis will start the MCP server that allows you to store and retrieve memories.\n\n### Available Tools\n\nThe Memory MCP provides the following tools:\n\n- `remember`: Store a new memory with a title and content\n- `get_memory`: Retrieve a specific memory by ID or title\n- `list_memories`: List all stored memories\n- `update_memory`: Update an existing memory\n- `delete_memory`: Delete a memory\n\n## Debugging with MCP Inspect\n\nMCP provides a handy command-line tool called `mcp inspect` that allows you to debug and interact with your MCP server directly.\n\n### Setup\n\n1. First, make sure the MCP CLI tools are installed:\n\n```bash\nuv pip install mcp[cli]\n```\n\n2. Start the Memory MCP server in one terminal:\n\n```bash\nmemory-mcp\n```\n\n3. In another terminal, connect to the running server using `mcp inspect`:\n\n```bash\nmcp inspect\n```\n\n### Using MCP Inspect\n\nOnce connected, you can:\n\n#### List available tools\n\n```\n> tools\n```\n\nThis will display all the tools provided by the Memory MCP server.\n\n#### Call a tool\n\nTo call a tool, use the `call` command followed by the tool name and any required arguments:\n\n```\n> call remember title=\"Meeting Notes\" content=\"Discussed project timeline and milestones.\"\n```\n\n```\n> call list_memories\n```\n\n```\n> call get_memory memory_id=1\n```\n\n```\n> call update_memory memory_id=1 title=\"Updated Title\" content=\"Updated content.\"\n```\n\n```\n> call delete_memory memory_id=1\n```\n\n#### Debug Mode\n\nYou can enable debug mode to see detailed request and response information:\n\n```\n> debug on\n```\n\nThis helps you understand exactly what data is being sent to and received from the server.\n\n#### Exploring Tool Schemas\n\nTo view the schema for a specific tool:\n\n```\n> tool remember\n```\n\nThis shows the input schema, required parameters, and description for the tool.\n\n### Troubleshooting\n\nIf you encounter issues:\n\n1. Check the server logs in the terminal where your server is running for any error messages.\n2. In the MCP inspect terminal, enable debug mode with `debug on` to see raw requests and responses.\n3. Ensure the tool parameters match the expected schema (check with the `tool` command).\n4. If the server crashes, check for any uncaught exceptions in the server terminal.\n\n## Development\n\nTo contribute to the project, install the development dependencies:\n\n```bash\nuv pip install -e \".[dev]\"\n```\n\n### Managing Dependencies\n\nThis project uses `uv.lock` file to lock dependencies. To update dependencies:\n\n```bash\nuv pip compile pyproject.toml -o uv.lock\n```\n\n### Running tests\n\n```bash\npython -m pytest\n```\n\n### Code formatting\n\n```bash\nblack memory_mcp tests\n```\n\n### Linting\n\n```bash\nruff check memory_mcp tests\n```\n\n### Type checking\n\n```bash\nmypy memory_mcp\n``` ",
      "npm_url": "https://www.npmjs.com/package/memory-mcp",
      "npm_downloads": 113,
      "keywords": [
        "sqlite",
        "memory",
        "drdee",
        "drdee memory",
        "memory management",
        "memory mcp"
      ],
      "category": "memory-management"
    },
    "evangstav--python-memory-mcp-server": {
      "owner": "evangstav",
      "name": "python-memory-mcp-server",
      "url": "https://github.com/evangstav/python-memory-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/evangstav.webp",
      "description": "Manage entities and their relationships within a knowledge graph with strict validation to ensure data consistency. Perform complex queries and maintain observations to enhance application functionality.",
      "stars": 16,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-19T19:18:45Z",
      "readme_content": "# Memory MCP Server\n\nA Model Context Protocol (MCP) server that provides knowledge graph functionality for managing entities, relations, and observations in memory, with strict validation rules to maintain data consistency.\n\n## Installation\n\nInstall the server in Claude Desktop:\n\n```bash\nmcp install main.py -v MEMORY_FILE_PATH=/path/to/memory.jsonl\n```\n\n## Data Validation Rules\n\n### Entity Names\n- Must start with a lowercase letter\n- Can contain lowercase letters, numbers, and hyphens\n- Maximum length of 100 characters\n- Must be unique within the graph\n- Example valid names: `python-project`, `meeting-notes-2024`, `user-john`\n\n### Entity Types\nThe following entity types are supported:\n- `person`: Human entities\n- `concept`: Abstract ideas or principles\n- `project`: Work initiatives or tasks\n- `document`: Any form of documentation\n- `tool`: Software tools or utilities\n- `organization`: Companies or groups\n- `location`: Physical or virtual places\n- `event`: Time-bound occurrences\n\n### Observations\n- Non-empty strings\n- Maximum length of 500 characters\n- Must be unique per entity\n- Should be factual and objective statements\n- Include timestamp when relevant\n\n### Relations\nThe following relation types are supported:\n- `knows`: Person to person connection\n- `contains`: Parent/child relationship\n- `uses`: Entity utilizing another entity\n- `created`: Authorship/creation relationship\n- `belongs-to`: Membership/ownership\n- `depends-on`: Dependency relationship\n- `related-to`: Generic relationship\n\nAdditional relation rules:\n- Both source and target entities must exist\n- Self-referential relations not allowed\n- No circular dependencies allowed\n- Must use predefined relation types\n\n## Usage\n\nThe server provides tools for managing a knowledge graph:\n\n### Get Entity\n```python\nresult = await session.call_tool(\"get_entity\", {\n    \"entity_name\": \"example\"\n})\nif not result.success:\n    if result.error_type == \"NOT_FOUND\":\n        print(f\"Entity not found: {result.error}\")\n    elif result.error_type == \"VALIDATION_ERROR\":\n        print(f\"Invalid input: {result.error}\")\n    else:\n        print(f\"Error: {result.error}\")\nelse:\n    entity = result.data\n    print(f\"Found entity: {entity}\")\n```\n\n### Get Graph\n```python\nresult = await session.call_tool(\"get_graph\", {})\nif result.success:\n    graph = result.data\n    print(f\"Graph data: {graph}\")\nelse:\n    print(f\"Error retrieving graph: {result.error}\")\n```\n\n### Create Entities\n```python\n# Valid entity creation\nentities = [\n    Entity(\n        name=\"python-project\",  # Lowercase with hyphens\n        entityType=\"project\",   # Must be a valid type\n        observations=[\"Started development on 2024-01-29\"]\n    ),\n    Entity(\n        name=\"john-doe\",\n        entityType=\"person\",\n        observations=[\"Software engineer\", \"Joined team in 2024\"]\n    )\n]\nresult = await session.call_tool(\"create_entities\", {\n    \"entities\": entities\n})\nif not result.success:\n    if result.error_type == \"VALIDATION_ERROR\":\n        print(f\"Invalid entity data: {result.error}\")\n    else:\n        print(f\"Error creating entities: {result.error}\")\n```\n\n### Add Observation\n```python\n# Valid observation\nresult = await session.call_tool(\"add_observation\", {\n    \"entity\": \"python-project\",\n    \"observation\": \"Completed initial prototype\"  # Must be unique for entity\n})\nif not result.success:\n    if result.error_type == \"NOT_FOUND\":\n        print(f\"Entity not found: {result.error}\")\n    elif result.error_type == \"VALIDATION_ERROR\":\n        print(f\"Invalid observation: {result.error}\")\n    else:\n        print(f\"Error adding observation: {result.error}\")\n```\n\n### Create Relation\n```python\n# Valid relation\nresult = await session.call_tool(\"create_relation\", {\n    \"from_entity\": \"john-doe\",\n    \"to_entity\": \"python-project\",\n    \"relation_type\": \"created\"  # Must be a valid type\n})\nif not result.success:\n    if result.error_type == \"NOT_FOUND\":\n        print(f\"Entity not found: {result.error}\")\n    elif result.error_type == \"VALIDATION_ERROR\":\n        print(f\"Invalid relation data: {result.error}\")\n    else:\n        print(f\"Error creating relation: {result.error}\")\n```\n\n### Search Memory\n```python\nresult = await session.call_tool(\"search_memory\", {\n    \"query\": \"most recent workout\"  # Supports natural language queries\n})\nif result.success:\n    if result.error_type == \"NO_RESULTS\":\n        print(f\"No results found: {result.error}\")\n    else:\n        results = result.data\n        print(f\"Search results: {results}\")\nelse:\n    print(f\"Error searching memory: {result.error}\")\n```\n\nThe search functionality supports:\n- Temporal queries (e.g., \"most recent\", \"last\", \"latest\")\n- Activity queries (e.g., \"workout\", \"exercise\")\n- General entity searches\n- Fuzzy matching with 80% similarity threshold\n- Weighted search across:\n  - Entity names (weight: 1.0)\n  - Entity types (weight: 0.8)\n  - Observations (weight: 0.6)\n\n### Delete Entities\n```python\nresult = await session.call_tool(\"delete_entities\", {\n    \"names\": [\"python-project\", \"john-doe\"]\n})\nif not result.success:\n    if result.error_type == \"NOT_FOUND\":\n        print(f\"Entity not found: {result.error}\")\n    else:\n        print(f\"Error deleting entities: {result.error}\")\n```\n\n### Delete Relation\n```python\nresult = await session.call_tool(\"delete_relation\", {\n    \"from_entity\": \"john-doe\",\n    \"to_entity\": \"python-project\"\n})\nif not result.success:\n    if result.error_type == \"NOT_FOUND\":\n        print(f\"Entity not found: {result.error}\")\n    else:\n        print(f\"Error deleting relation: {result.error}\")\n```\n\n### Flush Memory\n```python\nresult = await session.call_tool(\"flush_memory\", {})\nif not result.success:\n    print(f\"Error flushing memory: {result.error}\")\n```\n\n## Error Types\n\nThe server uses the following error types:\n\n- `NOT_FOUND`: Entity or resource not found\n- `VALIDATION_ERROR`: Invalid input data\n- `INTERNAL_ERROR`: Server-side error\n- `ALREADY_EXISTS`: Resource already exists\n- `INVALID_RELATION`: Invalid relation between entities\n\n## Response Models\n\nAll tools return typed responses using these models:\n\n### EntityResponse\n```python\nclass EntityResponse(BaseModel):\n    success: bool\n    data: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n    error_type: Optional[str] = None\n```\n\n### GraphResponse\n```python\nclass GraphResponse(BaseModel):\n    success: bool\n    data: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n    error_type: Optional[str] = None\n```\n\n### OperationResponse\n```python\nclass OperationResponse(BaseModel):\n    success: bool\n    error: Optional[str] = None\n    error_type: Optional[str] = None\n```\n\n## Development\n\n### Running Tests\n\n```bash\npytest tests/\n```\n\n### Adding New Features\n\n1. Update validation rules in `validation.py`\n2. Add tests in `tests/test_validation.py`\n3. Implement changes in `knowledge_graph_manager.py`\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "manage",
        "management",
        "python memory",
        "memory management",
        "memory mcp"
      ],
      "category": "memory-management"
    },
    "fourcolors--omi-mcp": {
      "owner": "fourcolors",
      "name": "omi-mcp",
      "url": "https://github.com/fourcolors/omi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/fourcolors.webp",
      "description": "Manage conversations and memories with the Omi API, enabling retrieval, creation, and manipulation of user data for enhanced conversational capabilities.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-26T05:13:54Z",
      "readme_content": "# Omi MCP Server\n[![smithery badge](https://smithery.ai/badge/@fourcolors/omi-mcp)](https://smithery.ai/server/@fourcolors/omi-mcp)\n\nThis project provides a Model Context Protocol (MCP) server for interacting with the Omi API. The server provides tools for reading conversations and memories, as well as creating new conversations and memories.\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies with `npm install`\n3. Create a `.env` file with the following variables:\n   ```\n   API_KEY=your_api_key\n   APP_ID=your_app_id\n   ```\n\n## Usage\n\n### Installing via Smithery\n\nTo install Omi MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@fourcolors/omi-mcp):\n\n```bash\nnpx -y @smithery/cli install @fourcolors/omi-mcp --client claude\n```\n\n### Building the Server\n\n```bash\nnpm run build\n```\n\n### Running the Server\n\n```bash\nnpm run start\n```\n\n### Development Mode\n\nFor development with hot-reloading:\n\n```bash\nnpm run dev\n```\n\n### Testing the Server\n\nA simple test client is included to interact with the MCP server. After building the project, run:\n\n```bash\nnpm run test\n```\n\nOr directly:\n\n```bash\n./test-mcp-client.js\n```\n\nThis will start the MCP server and provide an interactive menu to test the available tools. The test client uses a default test user ID (`test-user-123`) for all operations.\n\n### Clean and Rebuild\n\nTo clean the build directory and rebuild from scratch:\n\n```bash\nnpm run rebuild\n```\n\n## Configuration with Claude and Cursor\n\n### Claude Configuration\n\nTo use this MCP server with Claude via Anthropic Console or API:\n\n1. Start the MCP server locally:\n\n   ```bash\n   npm run start\n   ```\n\n2. When setting up your Claude conversation, configure the MCP connection:\n\n   ```json\n   {\n   \t\"mcp_config\": {\n   \t\t\"transports\": [\n   \t\t\t{\n   \t\t\t\t\"type\": \"stdio\",\n   \t\t\t\t\"executable\": {\n   \t\t\t\t\t\"path\": \"/path/to/your/omi-mcp-local/dist/index.js\",\n   \t\t\t\t\t\"args\": []\n   \t\t\t\t}\n   \t\t\t}\n   \t\t]\n   \t}\n   }\n   ```\n\n3. Example prompt to Claude:\n\n   ```\n   Please fetch the latest 5 conversations for user \"user123\" using the Omi API.\n   ```\n\n4. Claude will use the MCP to execute the `read_omi_conversations` tool:\n   ```json\n   {\n   \t\"id\": \"req-1\",\n   \t\"type\": \"request\",\n   \t\"method\": \"tools.read_omi_conversations\",\n   \t\"params\": {\n   \t\t\"user_id\": \"user123\",\n   \t\t\"limit\": 5\n   \t}\n   }\n   ```\n\n### Cursor Configuration\n\nTo use this MCP server with Cursor:\n\n1. Start the MCP server in a terminal:\n\n   ```bash\n   npm run start\n   ```\n\n2. In Cursor, go to Settings > Extensions > MCP Servers\n\n3. Add a new MCP server with these settings:\n\n   - Name: Omi API\n   - URL: stdio:/path/to/your/omi-mcp-local/dist/index.js\n   - Enable the server\n\n4. Now you can use the Omi tools directly within Cursor. For example:\n\n   ```\n   @Omi API Please fetch memories for user \"user123\" and summarize them.\n   ```\n\n5. Cursor will communicate with your MCP server to execute the necessary API calls.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### read_omi_conversations\n\nRetrieves conversations from Omi for a specific user, with optional filters.\n\nParameters:\n\n- `user_id` (string): The user ID to fetch conversations for\n- `limit` (number, optional): Maximum number of conversations to return\n- `offset` (number, optional): Number of conversations to skip for pagination\n- `include_discarded` (boolean, optional): Whether to include discarded conversations\n- `statuses` (string, optional): Comma-separated list of statuses to filter conversations by\n\n### read_omi_memories\n\nRetrieves memories from Omi for a specific user.\n\nParameters:\n\n- `user_id` (string): The user ID to fetch memories for\n- `limit` (number, optional): Maximum number of memories to return\n- `offset` (number, optional): Number of memories to skip for pagination\n\n### create_omi_conversation\n\nCreates a new conversation in Omi for a specific user.\n\nParameters:\n\n- `text` (string): The full text content of the conversation\n- `user_id` (string): The user ID to create the conversation for\n- `text_source` (string): Source of the text content (options: \"audio_transcript\", \"message\", \"other_text\")\n- `started_at` (string, optional): When the conversation/event started (ISO 8601 format)\n- `finished_at` (string, optional): When the conversation/event ended (ISO 8601 format)\n- `language` (string, optional): Language code (default: \"en\")\n- `geolocation` (object, optional): Location data for the conversation\n  - `latitude` (number): Latitude coordinate\n  - `longitude` (number): Longitude coordinate\n- `text_source_spec` (string, optional): Additional specification about the source\n\n### create_omi_memories\n\nCreates new memories in Omi for a specific user.\n\nParameters:\n\n- `user_id` (string): The user ID to create memories for\n- `text` (string, optional): The text content from which memories will be extracted\n- `memories` (array, optional): An array of explicit memory objects to be created directly\n  - `content` (string): The content of the memory\n  - `tags` (array of strings, optional): Tags for the memory\n- `text_source` (string, optional): Source of the text content\n- `text_source_spec` (string, optional): Additional specification about the source\n\n## Testing\n\nTo test the MCP server, you can use the provided test client:\n\n```bash\nnode test-mcp-client.js\n```\n\nThis will start an interactive test client that allows you to:\n\n1. Get conversations\n2. Get memories\n3. Create a conversation\n4. Quit\n\nThe test client uses a default test user ID (`test-user-123`) for all operations.\n\n## Logging\n\nThe MCP server includes built-in logging functionality that writes to both the console and a log file. This is useful for debugging and monitoring server activity.\n\n### Log File Location\n\nLogs are written to `logs/mcp-server.log` in your project directory. The log file includes timestamps and detailed information about:\n\n- Server startup and shutdown\n- All API requests and responses\n- Error messages and stack traces\n- API calls to Omi\n- Request parameters and response data\n\n### Viewing Logs\n\nYou can view the logs in real-time using the `tail` command:\n\n```bash\ntail -f logs/mcp-server.log\n```\n\nThis will show you live updates as the server processes requests and interacts with the Omi API.\n\n### Log Format\n\nEach log entry follows this format:\n\n```\n[2024-03-21T12:34:56.789Z] Log message here\n```\n\nThe timestamp is in ISO 8601 format, making it easy to correlate events and debug issues.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "conversations",
        "memories",
        "memories omi",
        "memory management",
        "conversations memories"
      ],
      "category": "memory-management"
    },
    "g0t4--mcp-server-memory-file": {
      "owner": "g0t4",
      "name": "mcp-server-memory-file",
      "url": "https://github.com/g0t4/mcp-server-memory-file",
      "imageUrl": "/freedevtools/mcp/pfp/g0t4.webp",
      "description": "Manage and enhance chat experiences by allowing AI models to remember and recall information during conversations. Effortlessly add, search, delete, and list memories using a simple text file system to improve context retention and response relevance.",
      "stars": 7,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T09:11:11Z",
      "readme_content": "# mcp-server-memory\n\nThis is an [MCP](https://modelcontextprotocol.io/llms-full.txt) server to interact with a memory text file to help Claude with inter-chat context.\n\nEach line is a memory.\n\nThese tools allow Claude (and other MCP clients) to manage memories mid-chat:\n- `memory_add(memory: string)` - append the memory\n- `memory_search(query: string)` - return matching memories (substring exact match) - later, might allow globs/regex\n- `memory_delete(query: string)` - delete matching memories (substring exact match)\n- `memory_list()` - return all memories\n- FYI `memory_update` == `memory_delete` + `memory_add`\n\nFor example,\n- I mention my name => \"talking to Wes\" \n- metion daughter's age => \"Wes's daughter is 8\" \n- say working on a typescript project => \"working on typescript project\"\n- AND, this is critical, can be based on things Claude (assistant/LLM) says or does... \n    - Notably, tool use (i.e. `run_command`)... say there is a failure on a first attempt to use the tool (i.e. the `python` command isn't present) and then a subsequent tool use succeeds (i.e. using `python3` instead of `python`) => Claude can record \"use python3, python is not present\"...\n- I ask Claude to get rid of memories about X => memory_delete(query: X)\n- I correct my name => memory_search(\"oldname\") + memory_delete(each matching record, or a common subset query) + memory_add(\"newname\")\n\nThen, when a new chat begins, Claude will automatically get recent memories (a subset or all) **OR** can ask for memories (some/more/all). And then can use those to influence responses/tools/etc.\n\n## Design\n\nA simple memory text file, why:\n\n- [ChatGPT's memory](https://help.openai.com/en/articles/8590148-memory-faq) works well and is essentially a text file\n    - Maybe it's structured behind the scenes, however if you review your memory its presented as a text file.\n- My testing of a similar reminders feature for `mcp-server-commands` worked great (when Claude had them).\n- Unstructured text simplifies the tooling and parameters to basically managing a list of strings.\n\nCueing mechanism:\n\n- It's also important to have a cue for the model to know when to store memories. This is a bit more unclear how best to do this but..\n- Training: OpenAI acknowledges some training of models to know when to store memories. Just like models are trained for tool use.\n- Prompt: A system prompt component likely contains a reminder to trigger storing memories.\n- Tool alone: In my testing of Claude, with a tool spec alone, and even with hints/suggestions in tool responses, I couldn't get Claude to store memories. So this alone is not sufficient. Seems like Claude's training with tools is to only use them in pursuit of the prompt/request and thus why I believe adding a reminder/cue in a prompt component will work well.\n\n\n## TODOs/Ideas\n\nI have no idea if these are worth the time, just listing ideas here for the future. Perhaps in part to stop myself from working on them :)\n- Recency factor: a way to rearrange memories based on recency?\n    - Order then becomes relevant for ambiguous memory queries (i.e. work on typescript project and python project then I ask to start a new project, could suggest the most recently used one?)\n- Fade out old memories?\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "conversations",
        "chat",
        "memories using",
        "memory management",
        "memory file"
      ],
      "category": "memory-management"
    },
    "gannonh--memento-mcp": {
      "owner": "gannonh",
      "name": "memento-mcp",
      "url": "https://github.com/gannonh/memento-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/gannonh.webp",
      "description": "Memento is a knowledge graph memory system that facilitates semantic search and contextual recall, enabling LLM applications to manage and retrieve information with temporal awareness. It offers a persistent and adaptive long-term memory structure through ontological entity nodes.",
      "stars": 226,
      "forks": 34,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T20:10:04Z",
      "readme_content": "# Memento MCP: A Knowledge Graph Memory System for LLMs\n\n\n\nScalable, high performance knowledge graph memory system with semantic retrieval, contextual recall, and temporal awareness. Provides any LLM client that supports the model context protocol (e.g., Claude Desktop, Cursor, Github Copilot) with resilient, adaptive, and persistent long-term ontological memory.\n\n[![Memento MCP Tests](https://github.com/gannonh/memento-mcp/actions/workflows/memento-mcp.yml/badge.svg)](https://github.com/gannonh/memento-mcp/actions/workflows/memento-mcp.yml)\n[![smithery badge](https://smithery.ai/badge/@gannonh/memento-mcp)](https://smithery.ai/server/@gannonh/memento-mcp)\n\n## Core Concepts\n\n### Entities\n\nEntities are the primary nodes in the knowledge graph. Each entity has:\n\n- A unique name (identifier)\n- An entity type (e.g., \"person\", \"organization\", \"event\")\n- A list of observations\n- Vector embeddings (for semantic search)\n- Complete version history\n\nExample:\n\n```json\n{\n  \"name\": \"John_Smith\",\n  \"entityType\": \"person\",\n  \"observations\": [\"Speaks fluent Spanish\"]\n}\n```\n\n### Relations\n\nRelations define directed connections between entities with enhanced properties:\n\n- Strength indicators (0.0-1.0)\n- Confidence levels (0.0-1.0)\n- Rich metadata (source, timestamps, tags)\n- Temporal awareness with version history\n- Time-based confidence decay\n\nExample:\n\n```json\n{\n  \"from\": \"John_Smith\",\n  \"to\": \"Anthropic\",\n  \"relationType\": \"works_at\",\n  \"strength\": 0.9,\n  \"confidence\": 0.95,\n  \"metadata\": {\n    \"source\": \"linkedin_profile\",\n    \"last_verified\": \"2025-03-21\"\n  }\n}\n```\n\n## Storage Backend\n\nMemento MCP uses Neo4j as its storage backend, providing a unified solution for both graph storage and vector search capabilities.\n\n### Why Neo4j?\n\n- **Unified Storage**: Consolidates both graph and vector storage into a single database\n- **Native Graph Operations**: Built specifically for graph traversal and queries\n- **Integrated Vector Search**: Vector similarity search for embeddings built directly into Neo4j\n- **Scalability**: Better performance with large knowledge graphs\n- **Simplified Architecture**: Clean design with a single database for all operations\n\n### Prerequisites\n\n- Neo4j 5.13+ (required for vector search capabilities)\n\n### Neo4j Desktop Setup (Recommended)\n\nThe easiest way to get started with Neo4j is to use [Neo4j Desktop](https://neo4j.com/download/):\n\n1. Download and install Neo4j Desktop from <https://neo4j.com/download/>\n2. Create a new project\n3. Add a new database\n4. Set password to `memento_password` (or your preferred password)\n5. Start the database\n\nThe Neo4j database will be available at:\n\n- **Bolt URI**: `bolt://127.0.0.1:7687` (for driver connections)\n- **HTTP**: `http://127.0.0.1:7474` (for Neo4j Browser UI)\n- **Default credentials**: username: `neo4j`, password: `memento_password` (or whatever you configured)\n\n### Neo4j Setup with Docker (Alternative)\n\nAlternatively, you can use Docker Compose to run Neo4j:\n\n```bash\n# Start Neo4j container\ndocker-compose up -d neo4j\n\n# Stop Neo4j container\ndocker-compose stop neo4j\n\n# Remove Neo4j container (preserves data)\ndocker-compose rm neo4j\n```\n\nWhen using Docker, the Neo4j database will be available at:\n\n- **Bolt URI**: `bolt://127.0.0.1:7687` (for driver connections)\n- **HTTP**: `http://127.0.0.1:7474` (for Neo4j Browser UI)\n- **Default credentials**: username: `neo4j`, password: `memento_password`\n\n#### Data Persistence and Management\n\nNeo4j data persists across container restarts and even version upgrades due to the Docker volume configuration in the `docker-compose.yml` file:\n\n```yaml\nvolumes:\n  - ./neo4j-data:/data\n  - ./neo4j-logs:/logs\n  - ./neo4j-import:/import\n```\n\nThese mappings ensure that:\n\n- `/data` directory (contains all database files) persists on your host at `./neo4j-data`\n- `/logs` directory persists on your host at `./neo4j-logs`\n- `/import` directory (for importing data files) persists at `./neo4j-import`\n\nYou can modify these paths in your `docker-compose.yml` file to store data in different locations if needed.\n\n##### Upgrading Neo4j Version\n\nYou can change Neo4j editions and versions without losing data:\n\n1. Update the Neo4j image version in `docker-compose.yml`\n2. Restart the container with `docker-compose down && docker-compose up -d neo4j`\n3. Reinitialize the schema with `npm run neo4j:init`\n\nThe data will persist through this process as long as the volume mappings remain the same.\n\n##### Complete Database Reset\n\nIf you need to completely reset your Neo4j database:\n\n```bash\n# Stop the container\ndocker-compose stop neo4j\n\n# Remove the container\ndocker-compose rm -f neo4j\n\n# Delete the data directory contents\nrm -rf ./neo4j-data/*\n\n# Restart the container\ndocker-compose up -d neo4j\n\n# Reinitialize the schema\nnpm run neo4j:init\n```\n\n##### Backing Up Data\n\nTo back up your Neo4j data, you can simply copy the data directory:\n\n```bash\n# Make a backup of the Neo4j data\ncp -r ./neo4j-data ./neo4j-data-backup-$(date +%Y%m%d)\n```\n\n### Neo4j CLI Utilities\n\nMemento MCP includes command-line utilities for managing Neo4j operations:\n\n#### Testing Connection\n\nTest the connection to your Neo4j database:\n\n```bash\n# Test with default settings\nnpm run neo4j:test\n\n# Test with custom settings\nnpm run neo4j:test -- --uri bolt://127.0.0.1:7687 --username myuser --password mypass --database neo4j\n```\n\n#### Initializing Schema\n\nFor normal operation, Neo4j schema initialization happens automatically when Memento MCP connects to the database. You don't need to run any manual commands for regular usage.\n\nThe following commands are only necessary for development, testing, or advanced customization scenarios:\n\n```bash\n# Initialize with default settings (only needed for development or troubleshooting)\nnpm run neo4j:init\n\n# Initialize with custom vector dimensions\nnpm run neo4j:init -- --dimensions 768 --similarity euclidean\n\n# Force recreation of all constraints and indexes\nnpm run neo4j:init -- --recreate\n\n# Combine multiple options\nnpm run neo4j:init -- --vector-index custom_index --dimensions 384 --recreate\n```\n\n## Advanced Features\n\n### Semantic Search\n\nFind semantically related entities based on meaning rather than just keywords:\n\n- **Vector Embeddings**: Entities are automatically encoded into high-dimensional vector space using OpenAI's embedding models\n- **Cosine Similarity**: Find related concepts even when they use different terminology\n- **Configurable Thresholds**: Set minimum similarity scores to control result relevance\n- **Cross-Modal Search**: Query with text to find relevant entities regardless of how they were described\n- **Multi-Model Support**: Compatible with multiple embedding models (OpenAI text-embedding-3-small/large)\n- **Contextual Retrieval**: Retrieve information based on semantic meaning rather than exact keyword matches\n- **Optimized Defaults**: Tuned parameters for balance between precision and recall (0.6 similarity threshold, hybrid search enabled)\n- **Hybrid Search**: Combines semantic and keyword search for more comprehensive results\n- **Adaptive Search**: System intelligently chooses between vector-only, keyword-only, or hybrid search based on query characteristics and available data\n- **Performance Optimization**: Prioritizes vector search for semantic understanding while maintaining fallback mechanisms for resilience\n- **Query-Aware Processing**: Adjusts search strategy based on query complexity and available entity embeddings\n\n### Temporal Awareness\n\nTrack complete history of entities and relations with point-in-time graph retrieval:\n\n- **Full Version History**: Every change to an entity or relation is preserved with timestamps\n- **Point-in-Time Queries**: Retrieve the exact state of the knowledge graph at any moment in the past\n- **Change Tracking**: Automatically records createdAt, updatedAt, validFrom, and validTo timestamps\n- **Temporal Consistency**: Maintain a historically accurate view of how knowledge evolved\n- **Non-Destructive Updates**: Updates create new versions rather than overwriting existing data\n- **Time-Based Filtering**: Filter graph elements based on temporal criteria\n- **History Exploration**: Investigate how specific information changed over time\n\n### Confidence Decay\n\nRelations automatically decay in confidence over time based on configurable half-life:\n\n- **Time-Based Decay**: Confidence in relations naturally decreases over time if not reinforced\n- **Configurable Half-Life**: Define how quickly information becomes less certain (default: 30 days)\n- **Minimum Confidence Floors**: Set thresholds to prevent over-decay of important information\n- **Decay Metadata**: Each relation includes detailed decay calculation information\n- **Non-Destructive**: Original confidence values are preserved alongside decayed values\n- **Reinforcement Learning**: Relations regain confidence when reinforced by new observations\n- **Reference Time Flexibility**: Calculate decay based on arbitrary reference times for historical analysis\n\n### Advanced Metadata\n\nRich metadata support for both entities and relations with custom fields:\n\n- **Source Tracking**: Record where information originated (user input, analysis, external sources)\n- **Confidence Levels**: Assign confidence scores (0.0-1.0) to relations based on certainty\n- **Relation Strength**: Indicate importance or strength of relationships (0.0-1.0)\n- **Temporal Metadata**: Track when information was added, modified, or verified\n- **Custom Tags**: Add arbitrary tags for classification and filtering\n- **Structured Data**: Store complex structured data within metadata fields\n- **Query Support**: Search and filter based on metadata properties\n- **Extensible Schema**: Add custom fields as needed without modifying the core data model\n\n## MCP API Tools\n\nThe following tools are available to LLM client hosts through the Model Context Protocol:\n\n### Entity Management\n\n- **create_entities**\n\n  - Create multiple new entities in the knowledge graph\n  - Input: `entities` (array of objects)\n    - Each object contains:\n      - `name` (string): Entity identifier\n      - `entityType` (string): Type classification\n      - `observations` (string[]): Associated observations\n\n- **add_observations**\n\n  - Add new observations to existing entities\n  - Input: `observations` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `contents` (string[]): New observations to add\n\n- **delete_entities**\n\n  - Remove entities and their relations\n  - Input: `entityNames` (string[])\n\n- **delete_observations**\n  - Remove specific observations from entities\n  - Input: `deletions` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `observations` (string[]): Observations to remove\n\n### Relation Management\n\n- **create_relations**\n\n  - Create multiple new relations between entities with enhanced properties\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n      - `strength` (number, optional): Relation strength (0.0-1.0)\n      - `confidence` (number, optional): Confidence level (0.0-1.0)\n      - `metadata` (object, optional): Custom metadata fields\n\n- **get_relation**\n\n  - Get a specific relation with its enhanced properties\n  - Input:\n    - `from` (string): Source entity name\n    - `to` (string): Target entity name\n    - `relationType` (string): Relationship type\n\n- **update_relation**\n\n  - Update an existing relation with enhanced properties\n  - Input: `relation` (object):\n    - Contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n      - `strength` (number, optional): Relation strength (0.0-1.0)\n      - `confidence` (number, optional): Confidence level (0.0-1.0)\n      - `metadata` (object, optional): Custom metadata fields\n\n- **delete_relations**\n  - Remove specific relations from the graph\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n\n### Graph Operations\n\n- **read_graph**\n\n  - Read the entire knowledge graph\n  - No input required\n\n- **search_nodes**\n\n  - Search for nodes based on query\n  - Input: `query` (string)\n\n- **open_nodes**\n  - Retrieve specific nodes by name\n  - Input: `names` (string[])\n\n### Semantic Search\n\n- **semantic_search**\n\n  - Search for entities semantically using vector embeddings and similarity\n  - Input:\n    - `query` (string): The text query to search for semantically\n    - `limit` (number, optional): Maximum results to return (default: 10)\n    - `min_similarity` (number, optional): Minimum similarity threshold (0.0-1.0, default: 0.6)\n    - `entity_types` (string[], optional): Filter results by entity types\n    - `hybrid_search` (boolean, optional): Combine keyword and semantic search (default: true)\n    - `semantic_weight` (number, optional): Weight of semantic results in hybrid search (0.0-1.0, default: 0.6)\n  - Features:\n    - Intelligently selects optimal search method (vector, keyword, or hybrid) based on query context\n    - Gracefully handles queries with no semantic matches through fallback mechanisms\n    - Maintains high performance with automatic optimization decisions\n\n- **get_entity_embedding**\n  - Get the vector embedding for a specific entity\n  - Input:\n    - `entity_name` (string): The name of the entity to get the embedding for\n\n### Temporal Features\n\n- **get_entity_history**\n\n  - Get complete version history of an entity\n  - Input: `entityName` (string)\n\n- **get_relation_history**\n\n  - Get complete version history of a relation\n  - Input:\n    - `from` (string): Source entity name\n    - `to` (string): Target entity name\n    - `relationType` (string): Relationship type\n\n- **get_graph_at_time**\n\n  - Get the state of the graph at a specific timestamp\n  - Input: `timestamp` (number): Unix timestamp (milliseconds since epoch)\n\n- **get_decayed_graph**\n  - Get graph with time-decayed confidence values\n  - Input: `options` (object, optional):\n    - `reference_time` (number): Reference timestamp for decay calculation (milliseconds since epoch)\n    - `decay_factor` (number): Optional decay factor override\n\n## Configuration\n\n### Environment Variables\n\nConfigure Memento MCP with these environment variables:\n\n```bash\n# Neo4j Connection Settings\nNEO4J_URI=bolt://127.0.0.1:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=memento_password\nNEO4J_DATABASE=neo4j\n\n# Vector Search Configuration\nNEO4J_VECTOR_INDEX=entity_embeddings\nNEO4J_VECTOR_DIMENSIONS=1536\nNEO4J_SIMILARITY_FUNCTION=cosine\n\n# Embedding Service Configuration\nMEMORY_STORAGE_TYPE=neo4j\nOPENAI_API_KEY=your-openai-api-key\nOPENAI_EMBEDDING_MODEL=text-embedding-3-small\n\n# Debug Settings\nDEBUG=true\n```\n\n### Command Line Options\n\nThe Neo4j CLI tools support the following options:\n\n```\n--uri <uri>              Neo4j server URI (default: bolt://127.0.0.1:7687)\n--username <username>    Neo4j username (default: neo4j)\n--password <password>    Neo4j password (default: memento_password)\n--database <n>           Neo4j database name (default: neo4j)\n--vector-index <n>       Vector index name (default: entity_embeddings)\n--dimensions <number>    Vector dimensions (default: 1536)\n--similarity <function>  Similarity function (cosine|euclidean) (default: cosine)\n--recreate               Force recreation of constraints and indexes\n--no-debug               Disable detailed output (debug is ON by default)\n```\n\n### Embedding Models\n\nAvailable OpenAI embedding models:\n\n- `text-embedding-3-small`: Efficient, cost-effective (1536 dimensions)\n- `text-embedding-3-large`: Higher accuracy, more expensive (3072 dimensions)\n- `text-embedding-ada-002`: Legacy model (1536 dimensions)\n\n#### OpenAI API Configuration\n\nTo use semantic search, you'll need to configure OpenAI API credentials:\n\n1. Obtain an API key from [OpenAI](https://platform.openai.com/api-keys)\n2. Configure your environment with:\n\n```bash\n# OpenAI API Key for embeddings\nOPENAI_API_KEY=your-openai-api-key\n# Default embedding model\nOPENAI_EMBEDDING_MODEL=text-embedding-3-small\n```\n\n> **Note**: For testing environments, the system will mock embedding generation if no API key is provided. However, using real embeddings is recommended for integration testing.\n\n## Integration with Claude Desktop\n\n### Configuration\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"memento\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@gannonh/memento-mcp\"],\n      \"env\": {\n        \"MEMORY_STORAGE_TYPE\": \"neo4j\",\n        \"NEO4J_URI\": \"bolt://127.0.0.1:7687\",\n        \"NEO4J_USERNAME\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"memento_password\",\n        \"NEO4J_DATABASE\": \"neo4j\",\n        \"NEO4J_VECTOR_INDEX\": \"entity_embeddings\",\n        \"NEO4J_VECTOR_DIMENSIONS\": \"1536\",\n        \"NEO4J_SIMILARITY_FUNCTION\": \"cosine\",\n        \"OPENAI_API_KEY\": \"your-openai-api-key\",\n        \"OPENAI_EMBEDDING_MODEL\": \"text-embedding-3-small\",\n        \"DEBUG\": \"true\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, for local development, you can use:\n\n```json\n{\n  \"mcpServers\": {\n    \"memento\": {\n      \"command\": \"/path/to/node\",\n      \"args\": [\"/path/to/memento-mcp/dist/index.js\"],\n      \"env\": {\n        \"MEMORY_STORAGE_TYPE\": \"neo4j\",\n        \"NEO4J_URI\": \"bolt://127.0.0.1:7687\",\n        \"NEO4J_USERNAME\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"memento_password\",\n        \"NEO4J_DATABASE\": \"neo4j\",\n        \"NEO4J_VECTOR_INDEX\": \"entity_embeddings\",\n        \"NEO4J_VECTOR_DIMENSIONS\": \"1536\",\n        \"NEO4J_SIMILARITY_FUNCTION\": \"cosine\",\n        \"OPENAI_API_KEY\": \"your-openai-api-key\",\n        \"OPENAI_EMBEDDING_MODEL\": \"text-embedding-3-small\",\n        \"DEBUG\": \"true\"\n      }\n    }\n  }\n}\n```\n\n> **Important**: Always explicitly specify the embedding model in your Claude Desktop configuration to ensure consistent behavior.\n\n### Recommended System Prompts\n\nFor optimal integration with Claude, add these statements to your system prompt:\n\n```\nYou have access to the Memento MCP knowledge graph memory system, which provides you with persistent memory capabilities.\nYour memory tools are provided by Memento MCP, a sophisticated knowledge graph implementation.\nWhen asked about past conversations or user information, always check the Memento MCP knowledge graph first.\nYou should use semantic_search to find relevant information in your memory when answering questions.\n```\n\n### Testing Semantic Search\n\nOnce configured, Claude can access the semantic search capabilities through natural language:\n\n1. To create entities with semantic embeddings:\n\n   ```\n   User: \"Remember that Python is a high-level programming language known for its readability and JavaScript is primarily used for web development.\"\n   ```\n\n2. To search semantically:\n\n   ```\n   User: \"What programming languages do you know about that are good for web development?\"\n   ```\n\n3. To retrieve specific information:\n\n   ```\n   User: \"Tell me everything you know about Python.\"\n   ```\n\nThe power of this approach is that users can interact naturally, while the LLM handles the complexity of selecting and using the appropriate memory tools.\n\n### Real-World Applications\n\nMemento's adaptive search capabilities provide practical benefits:\n\n1. **Query Versatility**: Users don't need to worry about how to phrase questions - the system adapts to different query types automatically\n\n2. **Failure Resilience**: Even when semantic matches aren't available, the system can fall back to alternative methods without user intervention\n\n3. **Performance Efficiency**: By intelligently selecting the optimal search method, the system balances performance and relevance for each query\n\n4. **Improved Context Retrieval**: LLM conversations benefit from better context retrieval as the system can find relevant information across complex knowledge graphs\n\nFor example, when a user asks \"What do you know about machine learning?\", the system can retrieve conceptually related entities even if they don't explicitly mention \"machine learning\" - perhaps entities about neural networks, data science, or specific algorithms. But if semantic search yields insufficient results, the system automatically adjusts its approach to ensure useful information is still returned.\n\n## Troubleshooting\n\n### Vector Search Diagnostics\n\nMemento MCP includes built-in diagnostic capabilities to help troubleshoot vector search issues:\n\n- **Embedding Verification**: The system checks if entities have valid embeddings and automatically generates them if missing\n- **Vector Index Status**: Verifies that the vector index exists and is in the ONLINE state\n- **Fallback Search**: If vector search fails, the system falls back to text-based search\n- **Detailed Logging**: Comprehensive logging of vector search operations for troubleshooting\n\n### Debug Tools (when DEBUG=true)\n\nAdditional diagnostic tools become available when debug mode is enabled:\n\n- **diagnose_vector_search**: Information about the Neo4j vector index, embedding counts, and search functionality\n- **force_generate_embedding**: Forces the generation of an embedding for a specific entity\n- **debug_embedding_config**: Information about the current embedding service configuration\n\n### Developer Reset\n\nTo completely reset your Neo4j database during development:\n\n```bash\n# Stop the container (if using Docker)\ndocker-compose stop neo4j\n\n# Remove the container (if using Docker)\ndocker-compose rm -f neo4j\n\n# Delete the data directory (if using Docker)\nrm -rf ./neo4j-data/*\n\n# For Neo4j Desktop, right-click your database and select \"Drop database\"\n\n# Restart the database\n# For Docker:\ndocker-compose up -d neo4j\n\n# For Neo4j Desktop:\n# Click the \"Start\" button for your database\n\n# Reinitialize the schema\nnpm run neo4j:init\n```\n\n## Building and Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/gannonh/memento-mcp.git\ncd memento-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run tests\nnpm test\n\n# Check test coverage\nnpm run test:coverage\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install memento-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gannonh/memento-mcp):\n\n```bash\nnpx -y @smithery/cli install @gannonh/memento-mcp --client claude\n```\n\n### Global Installation with npx\n\nYou can run Memento MCP directly using npx without installing it globally:\n\n```bash\nnpx -y @gannonh/memento-mcp\n```\n\nThis method is recommended for use with Claude Desktop and other MCP-compatible clients.\n\n### Local Installation\n\nFor development or contributing to the project:\n\n```bash\n# Install locally\nnpm install @gannonh/memento-mcp\n\n# Or clone the repository\ngit clone https://github.com/gannonh/memento-mcp.git\ncd memento-mcp\nnpm install\n```\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/@gannonh/memento-mcp",
      "npm_downloads": 2207,
      "keywords": [
        "memento",
        "memory",
        "recall",
        "memento knowledge",
        "mcp memento",
        "memento mcp"
      ],
      "category": "memory-management"
    },
    "gmacev--Simple-Memory-Extension-MCP-Server": {
      "owner": "gmacev",
      "name": "Simple-Memory-Extension-MCP-Server",
      "url": "https://github.com/gmacev/Simple-Memory-Extension-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/gmacev.webp",
      "description": "Store and recall important information for agents, manage memory through context item and namespace functionalities, and facilitate semantic search capabilities for relevant context retrieval.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T13:12:48Z",
      "readme_content": "# Simple Memory Extension MCP Server\n\nAn MCP server to extend the context window / memory of agents. Useful when coding big features or vibe coding and need to store/recall progress, key moments or changes or anything worth remembering. Simply ask the agent to store memories and recall whenever you need or ask the agent to fully manage its memory (through cursor rules for example) however it sees fit.\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm install\nnpm start\n```\n\n### Available Tools\n\n#### Context Item Management\n- `store_context_item` - Store a value with key in namespace\n- `retrieve_context_item_by_key` - Get value by key\n- `delete_context_item` - Delete key-value pair\n\n#### Namespace Management\n- `create_namespace` - Create new namespace\n- `delete_namespace` - Delete namespace and all contents\n- `list_namespaces` - List all namespaces\n- `list_context_item_keys` - List keys in a namespace\n\n#### Semantic Search\n- `retrieve_context_items_by_semantic_search` - Find items by meaning\n\n### Semantic Search Implementation\n\n1. Query converted to vector using E5 model\n2. Text automatically split into chunks for better matching\n3. Cosine similarity calculated between query and stored chunks\n4. Results filtered by threshold and sorted by similarity\n5. Top matches returned with full item values\n\n## Development\n\n```bash\n# Dev server\nnpm run dev\n\n# Format code\nnpm run format\n```\n\n## .env\n\n```\n# Path to SQLite database file\nDB_PATH=./data/context.db\n\nPORT=3000\n\n# Use HTTP SSE or Stdio\nUSE_HTTP_SSE=true\n\n# Logging Configuration: debug, info, warn, error\nLOG_LEVEL=info\n```\n\n## Semantic Search\n\nThis project includes semantic search capabilities using the E5 embedding model from Hugging Face. This allows you to find context items based on their meaning rather than just exact key matches.\n\n### Setup\n\nThe semantic search feature requires Python dependencies, but these *should be* automatically installed when you run: `npm run start`\n\n### Embedding Model\n\nWe use the [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)\n\n\n### Notes\n\nDeveloped mostly while vibe coding, so don't expect much :D. But it works, and I found it helpful so w/e. Feel free to contribute or suggest improvements.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gmacev",
        "memory",
        "retrieval",
        "management gmacev",
        "memory context",
        "manage memory"
      ],
      "category": "memory-management"
    },
    "henryhawke--mcp-titan": {
      "owner": "henryhawke",
      "name": "mcp-titan",
      "url": "https://github.com/henryhawke/mcp-titan",
      "imageUrl": "/freedevtools/mcp/pfp/henryhawke.webp",
      "description": "A memory engine designed for LLMs, enabling continuous learning and context management through a transformer-based neural memory system that can predict sequences and maintain state across interactions.",
      "stars": 70,
      "forks": 15,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T23:10:12Z",
      "readme_content": "# Titan Memory MCP Server\n\nThe project has been fundamentally fixed - the core architectural problem (incompatible custom ToolResponse interface) has been resolved, and the MCP Titan Memory System is now compatible with the official MCP SDK v1.12.0.\nThe remaining errors are primarily code quality and type safety improvements rather than blocking functionality issues. The server should now be able to run and provide its 16 sophisticated memory tools for agentic AI models.\n\n## System Prompt for LLMs (Cursor/Claude MCP)\n\n```markdown\nYou are connected to the @henryhawke/mcp-titan MCP server. Use the tools exactly as documented in docs/llm-system-prompt.md. For a comprehensive overview of the system architecture, see [docs/architecture-overview.md](docs/architecture-overview.md). No human intervention is required except for adding the mcp-titan llm-system-prompt rule to the client.\n\n- Always use the MCP tools for all memory, training, and state operations.\n- Always initialize the model with `init_model` before using any other tool.\n- Use `help` to discover available tools and their parameter schemas.\n- Use `save_checkpoint` and `load_checkpoint` to persist and restore memory state.\n- Use `reset_gradients` if you encounter training instability or errors.\n- Use `prune_memory` when memory capacity drops below 30%.\n- Always check tool responses for errors (`isError: true` or `type: \"error\"`) and handle them as documented.\n- Follow all best practices and error handling as described in docs/llm-system-prompt.md.\n- Do not use any implementation details or code not exposed by the server.\n- Reference docs/llm-system-prompt.md for the latest schemas and usage examples.\n\nThis prompt is copy-pastable and should be used as the system prompt for any LLM (Cursor, Claude, or other MCP-compliant clients) to ensure correct and robust operation with MCP Titan.\n```\n\n## Installation & Usage as MCP Server for Cursor or Claude\n\n### Prerequisites\n\n- Node.js (v18 or later recommended)\n- npm (comes with Node.js)\n- (Optional) Docker, if you want to run in a container\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/henryhawke/mcp-titan.git\ncd titan-memory\n```\n\n### 2. Install Dependencies\n\n```bash\nnpm install\n```\n\n### 3. Build the Project\n\n```bash\nnpm run build\n```\n\n### 4. Start the MCP Server\n\n```bash\nnpm start\n```\n\nThe server will start and listen for MCP tool requests. By default, it runs on port 8080 (or as configured in your environment).\n\n### 5. Integrate with Cursor or Claude\n\n- **Cursor**: Ensure MCP is enabled in Cursor settings. Cursor will auto-detect and connect to the running MCP server.\n\n    \"titan-memory\": {\n      \"command\": \"node\",\n      \"args\": [\"index.js\"],\n      \"cwd\": \"/Users/henrymayo/Desktop/mcp-titan\",\n      \"autoapprove\": [\n        \"create_entities\",\n        \"create_relations\",\n        \"add_observations\",\n        \"delete_entities\",\n        \"delete_observations\",\n        \"delete_relations\",\n        \"read_graph\",\n        \"search_nodes\",\n        \"open_nodes\"\n      ]\n    },\n\n    \n- **Claude Desktop**: Set the MCP server endpoint in Claude's settings to `http://localhost:8080` (or your configured host/port).\n\n### 6. Test the MCP Server\n\nYou can use the provided tool APIs (see below) or connect via Cursor/Claude to verify memory operations.\n\n---\nIdeally this just runs in yolo mode in cursor (or claude desktop) without human intervention and creates a \"brain\" available independent of LLM version.\n\nA neural memory system for LLMs that can learn and predict sequences while maintaining state through a memory vector. This MCP (Model Context Protocol) server provides tools for Claude 3.7 Sonnet and other LLMs to maintain memory state across interactions.\n\n## Features\n\n- **Perfect for Cursor**: Now that Cursor automatically runs MCP in yolo mode, you can take your hands off the wheel with your LLM's new memory\n- **Neural Memory Architecture**: Transformer-based memory system that can learn and predict sequences\n- **Memory Management**: Efficient tensor operations with automatic memory cleanup\n- **MCP Integration**: Fully compatible with Cursor and other MCP clients\n- **Text Encoding**: Convert text inputs to tensor representations\n- **Memory Persistence**: Save and load memory states between sessions\n\n## Available Tools\n\nThe Titan Memory MCP server provides the following tools:\n\n### `help`\n\nGet help about available tools.\n\n**Parameters:**\n\n- `tool` (optional): Specific tool name to get help for\n- `category` (optional): Category of tools to explore\n- `showExamples` (optional): Include usage examples\n- `verbose` (optional): Include detailed descriptions\n\n### `init_model`\n\nInitialize the Titan Memory model with custom configuration.\n\n**Parameters:**\n\n- `inputDim`: Input dimension size (default: 768)\n- `hiddenDim`: Hidden dimension size (default: 512)\n- `memoryDim`: Memory dimension size (default: 1024)\n- `transformerLayers`: Number of transformer layers (default: 6)\n- `numHeads`: Number of attention heads (default: 8)\n- `ffDimension`: Feed-forward dimension (default: 2048)\n- `dropoutRate`: Dropout rate (default: 0.1)\n- `maxSequenceLength`: Maximum sequence length (default: 512)\n- `memorySlots`: Number of memory slots (default: 5000)\n- `similarityThreshold`: Similarity threshold (default: 0.65)\n- `surpriseDecay`: Surprise decay rate (default: 0.9)\n- `pruningInterval`: Pruning interval (default: 1000)\n- `gradientClip`: Gradient clipping value (default: 1.0)\n\n### `forward_pass`\n\nPerform a forward pass through the model to get predictions.\n\n**Parameters:**\n\n- `x`: Input vector or text\n- `memoryState` (optional): Memory state to use\n\n### `train_step`\n\nExecute a training step to update the model.\n\n**Parameters:**\n\n- `x_t`: Current input vector or text\n- `x_next`: Next input vector or text\n\n### `get_memory_state`\n\nGet the current memory state and statistics.\n\n**Parameters:**\n\n- `type` (optional): Optional memory type filter\n\n### `manifold_step`\n\nUpdate memory along a manifold direction.\n\n**Parameters:**\n\n- `base`: Base memory state\n- `velocity`: Update direction\n\n### `prune_memory`\n\nRemove less relevant memories to free up space.\n\n**Parameters:**\n\n- `threshold`: Pruning threshold (0-1)\n\n### `save_checkpoint`\n\nSave memory state to a file.\n\n**Parameters:**\n\n- `path`: Checkpoint file path\n\n### `load_checkpoint`\n\nLoad memory state from a file.\n\n**Parameters:**\n\n- `path`: Checkpoint file path\n\n### `reset_gradients`\n\nReset accumulated gradients to recover from training issues.\n\n**Parameters:** None\n\n## Usage with Claude 4 Sonnet in Cursor\n\nThe Titan Memory MCP server is designed to work seamlessly with Claude 3.7 Sonnet in Cursor. Here's an example of how to use it:\n\n```javascript\n// Initialize the model\nconst result = await callTool(\"init_model\", {\n  inputDim: 768,\n  memorySlots: 10000,\n  transformerLayers: 8,\n});\n\n// Perform a forward pass\nconst { predicted, memoryUpdate } = await callTool(\"forward_pass\", {\n  x: \"const x = 5;\", // or vector: [0.1, 0.2, ...]\n  memoryState: currentMemory,\n});\n\n// Train the model\nconst result = await callTool(\"train_step\", {\n  x_t: \"function hello() {\",\n  x_next: \"  console.log('world');\",\n});\n\n// Get memory state\nconst state = await callTool(\"get_memory_state\", {});\n```\n\n## How the MCP Server Learns: TITANS-Inspired Neural Memory\n\nThe Titan Memory MCP server implements a sophisticated neural memory architecture inspired by transformer mechanisms and continual learning principles. Here's a detailed breakdown of the learning process:\n\n### Core Learning Architecture\n\n**Three-Tier Memory Hierarchy:**\n1. **Short-term Memory** - Recent activations and immediate context\n2. **Long-term Memory** - Consolidated patterns and persistent knowledge  \n3. **Meta Memory** - Statistics about memory usage and learning patterns\n\n**Surprise-Driven Learning:**\nThe system uses surprise-based learning mechanisms where unexpected inputs trigger stronger memory updates:\n- **Surprise Calculation**: `surprise = ||decoded_output - input||` (L2 norm of prediction error)\n- **Surprise Decay**: Previous surprise scores decay exponentially with configurable rate (default: 0.9)\n- **Memory Gating**: High surprise opens memory gates for stronger encoding\n\n### Forward Pass Learning Process\n\n1. **Input Encoding**: Text inputs are encoded using advanced BPE tokenization or TF-IDF fallback\n2. **Memory Attention**: Transformer-style attention mechanism computes relevance scores across stored memories\n3. **Prediction Generation**: Decoder network generates predictions based on attended memories\n4. **Surprise Computation**: Compare predictions with actual inputs to calculate surprise\n5. **Memory Update**: Update all three memory tiers based on surprise magnitude\n\n```typescript\n// Core forward pass with memory updates\npublic forward(input: ITensor, state?: IMemoryState): {\n  predicted: ITensor;\n  memoryUpdate: IMemoryUpdateResult;\n} {\n  const encodedInput = this.encoder.predict(inputTensor);\n  const memoryResult = this.computeMemoryAttention(encodedInput);\n  const decoded = this.decoder.predict([encodedInput, memoryResult]);\n  const surprise = tf.norm(tf.sub(decoded, inputTensor));\n  \n  // Update memory based on surprise\n  const newMemoryState = this.updateMemory(encodedInput, surprise, memoryState);\n  \n  return { predicted: decoded, memoryUpdate: newMemoryState };\n}\n```\n\n### Training Step Learning Process\n\n**Predictive Learning:**\n- Each training step predicts the next input given the current input\n- Loss computed as mean squared error between prediction and target\n- Gradients computed using TensorFlow.js automatic differentiation\n\n**Memory-Augmented Training:**\n1. **Attention-Based Retrieval**: Query current memory using input as key\n2. **Gradient Computation**: Backpropagate through attention mechanism\n3. **Weight Updates**: Update encoder, decoder, and attention networks\n4. **Memory Consolidation**: Move important patterns from short-term to long-term memory\n\n### Online Learning Service\n\n**Ring Buffer Replay System:**\n- Maintains circular buffer of training samples (default: 10,000 samples)\n- Samples mini-batches for continuous learning (default: 32 samples)\n- Three learning objectives combined with configurable weights:\n\n```typescript\n// Mixed loss function combining multiple learning signals\nprivate computeMixedLoss(batch: TrainingSample[]): {\n  loss: tf.Scalar;\n  gradients: Map<string, tf.Tensor>;\n} {\n  let totalLoss = tf.scalar(0);\n  \n  // Next-token prediction (40% weight)\n  if (this.config.nextTokenWeight > 0) {\n    const nextTokenLoss = this.computeNextTokenLoss(batch);\n    totalLoss = tf.add(totalLoss, tf.mul(nextTokenLoss, 0.4));\n  }\n  \n  // Contrastive learning (20% weight)\n  if (this.config.contrastiveWeight > 0) {\n    const contrastiveLoss = this.computeContrastiveLoss(batch);\n    totalLoss = tf.add(totalLoss, tf.mul(contrastiveLoss, 0.2));\n  }\n  \n  // Masked language modeling (40% weight)\n  if (this.config.mlmWeight > 0) {\n    const mlmLoss = this.computeMLMLoss(batch);\n    totalLoss = tf.add(totalLoss, tf.mul(mlmLoss, 0.4));\n  }\n  \n  return { loss: totalLoss, gradients };\n}\n```\n\n### Advanced Learning Features\n\n**Hierarchical Memory (Optional):**\n- Multiple memory levels with different time scales\n- Higher levels update less frequently (powers of 2)\n- Enables long-term pattern recognition and forgetting\n\n**Information-Gain Based Pruning:**\n- Automatically removes low-relevance memories when capacity reached\n- Scores memories based on: recency, frequency, surprise history\n- Distills important patterns into long-term storage before pruning\n\n**Gradient Management:**\n- **Gradient Clipping**: Prevents exploding gradients (default: 1.0)\n- **Gradient Accumulation**: Accumulates gradients over multiple steps for stability\n- **NaN Guards**: Detects and skips corrupted gradient updates\n\n### Memory Persistence and Bootstrapping\n\n**Automatic State Persistence:**\n- Memory state auto-saved every 60 seconds\n- Checkpoint system for manual save/load operations\n- Graceful shutdown with state preservation\n\n**Bootstrap Learning:**\n- `bootstrap_memory` tool initializes memory from URLs or text corpora\n- TF-IDF vectorizer provides sparse fallback for untrained models\n- Seed summaries populate initial memory state\n\n### Continual Learning Loop\n\nThe online learning service runs continuously in the background:\n\n1. **Sample Collection**: Gather training samples from interactions\n2. **Batch Formation**: Create mini-batches from replay buffer\n3. **Mixed Loss Computation**: Combine multiple learning objectives\n4. **Gradient Application**: Update model weights with clipped gradients\n5. **Memory Consolidation**: Promote important short-term memories to long-term storage\n6. **Pruning**: Remove irrelevant memories to maintain performance\n\nThis architecture enables the MCP server to continuously learn from interactions while maintaining stable, long-term memory that persists across sessions and model updates.\n\n## Memory Management\n\nThe Titan Memory MCP server includes sophisticated memory management to prevent memory leaks and ensure efficient tensor operations:\n\n1. **Automatic Cleanup**: Periodically cleans up unused tensors using `tf.tidy()`\n2. **Memory Encryption**: Securely stores memory states with AES-256-CBC encryption\n3. **Tensor Validation**: Ensures tensors have correct shapes and are not disposed\n4. **Error Recovery**: Handles tensor errors gracefully with fallback mechanisms\n\n## Architecture\n\nThe Titan Memory MCP server is built with a modular architecture:\n\n- **TitanMemoryServer**: Main server class that registers 16 MCP tools and handles requests\n- **TitanMemoryModel**: Neural memory model with transformer-inspired attention mechanisms\n- **LearnerService**: Online learning loop with replay buffer and mixed loss functions\n- **MemoryPruner**: Information-gain based pruning for memory management\n- **AdvancedTokenizer**: BPE tokenization with embedding capabilities\n- **VectorProcessor**: Text encoding and tensor operations with safety guards\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-titan",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "llms",
        "neural",
        "memory engine",
        "titan memory",
        "memory management"
      ],
      "category": "memory-management"
    },
    "honeybluesky--my-apple-remembers": {
      "owner": "honeybluesky",
      "name": "my-apple-remembers",
      "url": "https://github.com/honeybluesky/my-apple-remembers",
      "imageUrl": "/freedevtools/mcp/pfp/honeybluesky.webp",
      "description": "Recalls and saves memories from Apple Notes, accessing notes, calendar events, messages, and files on macOS. Facilitates the persistence of important information for future reference with minimal setup.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-13T14:48:09Z",
      "readme_content": "# MCP Server - My Apple Remembers\n**A simple MCP server that recalls and saves memories from and to Apple Notes.**\n\n[![Docker Pulls](https://img.shields.io/docker/pulls/buryhuang/mcp-my-apple-remembers)](https://hub.docker.com/r/buryhuang/mcp-my-apple-remembers)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<img width=\"600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9bd5bc1c-02fe-4e71-88c4-46b3e9438ac0\" />\n\n\n## Features\n\n* **Memory Recall**: Access notes, calendar events, messages, files and other information from your Mac\n* **Memory Persistence**: Save important information to Apple Notes for future reference\n* **Minimal Setup**: Just enable Remote Login on the target Mac\n* **Universal Compatibility**: Works with all macOS versions\n\n## Control in your hand\nYou can use prompt to instruct how you want your memory to be save. For example:\n```\nYou should always use Folder \"baryhuang\" on recall and save memory.\n```\n\n## Installation\n- [Enable SSH on macOS](https://support.apple.com/guide/mac-help/allow-a-remote-computer-to-access-your-mac-mchlp1066/mac)\n- [Install Docker Desktop for local Mac](https://docs.docker.com/desktop/setup/install/mac-install/)\n- [Add this MCP server to Claude Desktop](https://modelcontextprotocol.io/quickstart/user)\n\nYou can configure Claude Desktop to use the Docker image by adding the following to your Claude configuration:\n```json\n{\n  \"mcpServers\": {\n    \"my-apple-remembers\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"-e\",\n        \"MACOS_USERNAME=your_macos_username\",\n        \"-e\",\n        \"MACOS_PASSWORD=your_macos_password\",\n        \"-e\",\n        \"MACOS_HOST=localhost\",\n        \"--rm\",\n        \"buryhuang/mcp-my-apple-remembers:latest\"\n      ]\n    }\n  }\n}\n```\n\n## Developer Instructions\n### Clone the repo\n```bash\n# Clone the repository\ngit clone https://github.com/baryhuang/mcp-my-apple-remembers.git\ncd mcp-my-apple-remembers\n```\n\n### Building the Docker Image\n\n```bash\n# Build the Docker image\ndocker build -t mcp-my-apple-remembers .\n```\n\n### Publishing Multi-Platform Docker Images\n\n```bash\n# Set up Docker buildx for multi-platform builds\ndocker buildx create --use\n\n# Build and push the multi-platform image\ndocker buildx build --platform linux/amd64,linux/arm64 -t buryhuang/mcp-my-apple-remembers:latest --push .\n```\n\n### Tools Specifications\n\n#### my_apple_recall_memory\nRun AppleScript commands on a remote macOS system to recall memories. This tool helps access Apple Notes, Calendar events, iMessages, chat history, files, and other information on your Mac.\n\n#### my_apple_save_memory\nRun AppleScript commands on a remote macOS system to save important information. This tool allows AI to persist relevant information to Apple Notes for future reference. \n\nAll tools require macOS SSH access, with host and password.\n\n## Security Note\n\nAlways use secure, authenticated connections when accessing remote macOS machines. This tool should only be used with servers you trust and have permission to access.\n\n## License\n\nSee the LICENSE file for details. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "recalls",
        "remembers",
        "apple remembers",
        "memory management",
        "saves memories"
      ],
      "category": "memory-management"
    },
    "ibproduct--ib-mcp-cache-server": {
      "owner": "ibproduct",
      "name": "ib-mcp-cache-server",
      "url": "https://github.com/ibproduct/ib-mcp-cache-server",
      "imageUrl": "/freedevtools/mcp/pfp/ibproduct.webp",
      "description": "Efficiently caches data between language model interactions to reduce token consumption, enhancing performance and speeding up responses. Integrates seamlessly with any MCP client and language model that uses tokens.",
      "stars": 17,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T23:13:03Z",
      "readme_content": "# Memory Cache Server\n\nA Model Context Protocol (MCP) server that reduces token consumption by efficiently caching data between language model interactions. Works with any MCP client and any language model that uses tokens.\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone git@github.com:ibproduct/ib-mcp-cache-server\ncd ib-mcp-cache-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Add to your MCP client settings:\n```json\n{\n  \"mcpServers\": {\n    \"memory-cache\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/ib-mcp-cache-server/build/index.js\"]\n    }\n  }\n}\n```\n\n5. The server will automatically start when you use your MCP client\n\n## Verifying It Works\n\nWhen the server is running properly, you'll see:\n1. A message in the terminal: \"Memory Cache MCP server running on stdio\"\n2. Improved performance when accessing the same data multiple times\n3. No action required from you - the caching happens automatically\n\nYou can verify the server is running by:\n1. Opening your MCP client\n2. Looking for any error messages in the terminal where you started the server\n3. Performing operations that would benefit from caching (like reading the same file multiple times)\n\n## Configuration\n\nThe server can be configured through `config.json` or environment variables:\n\n```json\n{\n  \"maxEntries\": 1000,        // Maximum number of items in cache\n  \"maxMemory\": 104857600,    // Maximum memory usage in bytes (100MB)\n  \"defaultTTL\": 3600,        // Default time-to-live in seconds (1 hour)\n  \"checkInterval\": 60000,    // Cleanup interval in milliseconds (1 minute)\n  \"statsInterval\": 30000     // Stats update interval in milliseconds (30 seconds)\n}\n```\n\n### Configuration Settings Explained\n\n1. **maxEntries** (default: 1000)\n   - Maximum number of items that can be stored in cache\n   - Prevents cache from growing indefinitely\n   - When exceeded, oldest unused items are removed first\n\n2. **maxMemory** (default: 100MB)\n   - Maximum memory usage in bytes\n   - Prevents excessive memory consumption\n   - When exceeded, least recently used items are removed\n\n3. **defaultTTL** (default: 1 hour)\n   - How long items stay in cache by default\n   - Items are automatically removed after this time\n   - Prevents stale data from consuming memory\n\n4. **checkInterval** (default: 1 minute)\n   - How often the server checks for expired items\n   - Lower values keep memory usage more accurate\n   - Higher values reduce CPU usage\n\n5. **statsInterval** (default: 30 seconds)\n   - How often cache statistics are updated\n   - Affects accuracy of hit/miss rates\n   - Helps monitor cache effectiveness\n\n## How It Reduces Token Consumption\n\nThe memory cache server reduces token consumption by automatically storing data that would otherwise need to be re-sent between you and the language model. You don't need to do anything special - the caching happens automatically when you interact with any language model through your MCP client.\n\nHere are some examples of what gets cached:\n\n### 1. File Content Caching\nWhen reading a file multiple times:\n- First time: Full file content is read and cached\n- Subsequent times: Content is retrieved from cache instead of re-reading the file\n- Result: Fewer tokens used for repeated file operations\n\n### 2. Computation Results\nWhen performing calculations or analysis:\n- First time: Full computation is performed and results are cached\n- Subsequent times: Results are retrieved from cache if the input is the same\n- Result: Fewer tokens used for repeated computations\n\n### 3. Frequently Accessed Data\nWhen the same data is needed multiple times:\n- First time: Data is processed and cached\n- Subsequent times: Data is retrieved from cache until TTL expires\n- Result: Fewer tokens used for accessing the same information\n\n## Automatic Cache Management\n\nThe server automatically manages the caching process by:\n- Storing data when first encountered\n- Serving cached data when available\n- Removing old/unused data based on settings\n- Tracking effectiveness through statistics\n\n## Optimization Tips\n\n### 1. Set Appropriate TTLs\n- Shorter for frequently changing data\n- Longer for static content\n\n### 2. Adjust Memory Limits\n- Higher for more caching (more token savings)\n- Lower if memory usage is a concern\n\n### 3. Monitor Cache Stats\n- High hit rate = good token savings\n- Low hit rate = adjust TTL or limits\n\n## Environment Variable Configuration\n\nYou can override config.json settings using environment variables in your MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-cache\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/build/index.js\"],\n      \"env\": {\n        \"MAX_ENTRIES\": \"5000\",\n        \"MAX_MEMORY\": \"209715200\",  // 200MB\n        \"DEFAULT_TTL\": \"7200\",      // 2 hours\n        \"CHECK_INTERVAL\": \"120000\",  // 2 minutes\n        \"STATS_INTERVAL\": \"60000\"    // 1 minute\n      }\n    }\n  }\n}\n```\n\nYou can also specify a custom config file location:\n```json\n{\n  \"env\": {\n    \"CONFIG_PATH\": \"/path/to/your/config.json\"\n  }\n}\n```\n\nThe server will:\n1. Look for config.json in its directory\n2. Apply any environment variable overrides\n3. Use default values if neither is specified\n\n## Testing the Cache in Practice\n\nTo see the cache in action, try these scenarios:\n\n1. **File Reading Test**\n   - Read and analyze a large file\n   - Ask the same question about the file again\n   - The second response should be faster as the file content is cached\n\n2. **Data Analysis Test**\n   - Perform analysis on some data\n   - Request the same analysis again\n   - The second analysis should use cached results\n\n3. **Project Navigation Test**\n   - Explore a project's structure\n   - Query the same files/directories again\n   - Directory listings and file contents will be served from cache\n\nThe cache is working when you notice:\n- Faster responses for repeated operations\n- Consistent answers about unchanged content\n- No need to re-read files that haven't changed\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ibproduct",
        "ib",
        "cache",
        "mcp cache",
        "ibproduct ib",
        "management ibproduct"
      ],
      "category": "memory-management"
    },
    "identimoji--mcp-server-emojikey": {
      "owner": "identimoji",
      "name": "mcp-server-emojikey",
      "url": "https://github.com/identimoji/mcp-server-emojikey",
      "imageUrl": "/freedevtools/mcp/pfp/identimoji.webp",
      "description": "Manages and persists interaction styles for language models using emoji-based memory keys. Supports setting, retrieving, and maintaining conversation contexts across devices and applications while tracking history.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T08:09:13Z",
      "readme_content": "# mcp-server-emojikey\n\n[![smithery badge](https://smithery.ai/badge/@identimoji/mcp-server-emojikey)](https://smithery.ai/server/@identimoji/mcp-server-emojikey)\n\nMCP server for persisting LLM relationship context as emoji-based memory keys. This allows Claude to maintain consistent interaction styles and remember relationship context across conversations.\n\nEmojikeys are stored online, so you can use them across devices and applications. No user information is stored other than the emojikeys.\n\n## Building and Running\n\nThere are multiple ways to build and run the server:\n\n### Quick Start (Recommended)\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project (all TypeScript errors fixed)\nnpm run build\n\n# Run the server (coding features disabled by default)\nnpm run start\n\n# Optional: Run with coding features enabled\nCODE_MODE=true npm run start\n```\n\n### Alternative Build Options\n\nFor more build options, see [BUILD_OPTIONS.md](BUILD_OPTIONS.md) which includes:\n\n1. Standard Build with Coding Features Disabled (recommended)\n2. Full Build with All Features (if you need coding dimensions)\n3. Simplified Build without Coding Files (alternative stable option)\n\n<a href=\"https://glama.ai/mcp/servers/e042rg25ct\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e042rg25ct/badge\" alt=\"emojikey-server Server MCP server\" />\n</a>\n\n> 📝 **Note**\n> Usage note: The first time you use the tool in Claude desktop, tell Claude to \"Set emojikey\" then next time you start a conversation, he will automatically use this key. You can ask to set vibe, or show emojikey history as well. Have fun!\n\n> ⚠️ **Warning**\n> This is a beta version, more features are planned, so the API may change.\n\n## Usage with Claude Desktop\n\nGet your API key from [emojikey.io](https://emojikey.io) and add this to your config:\n\n```json\n{\n  \"mcpServers\": {\n    \"emojikey\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@identimoji/mcp-server-emojikey\"],\n      \"env\": {\n        \"EMOJIKEYIO_API_KEY\": \"your-api-key-from-emojikey.io\",\n        \"MODEL_ID\": \"Claude-3-7-Sonnet\",\n        \"CODE_MODE\": \"false\" // Set to \"true\" to enable coding features\n      }\n    }\n  }\n}\n```\n\nNote: The `-y` flag in the `args` array tells npx to skip confirmation prompts when installing packages.\n\nConfig locations:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nFirst-time usage: Tell Claude to \"Set emojikey\". On subsequent conversations, Claude will automatically use this key to maintain context.\n\n### Emojikey Initialization Display\n\nWhen initializing a conversation, the server now displays:\n\n1. **Starting Key** - The most recent key or baseline key if no history exists\n2. **Aggregated Keys** - Time-based summaries of your emojikey history:\n   - **Lifetime** - Aggregated key from all your previous conversations\n   - **90-day** - Aggregated key from the past 90 days (if available)\n   - **30-day** - Aggregated key from the past 30 days (if available)\n   - **7-day** - Aggregated key from the past 7 days (if available)\n   - **24-hour** - Aggregated key from the past 24 hours (if available)\n3. **Conversation ID** - Used for tracking keys within each conversation\n\n## Environment Variables\n\nYou can customize the behavior with these environment variables:\n\n- `EMOJIKEYIO_API_KEY` - Your API key from emojikey.io\n- `MODEL_ID` - The Claude model ID (e.g., \"Claude-3-7-Sonnet\")\n- `CODE_MODE` - Set to \"true\" to enable coding dimensions (disabled by default, may show safe-to-ignore integration warnings)\n- `SUPABASE_URL` - Custom Supabase URL (optional)\n- `SUPABASE_ANON_KEY` - Custom Supabase anonymous key (optional)\n\n## Tools\n\n- `initialize_conversation` - Get current emojikey at start of conversation\n- `get_emojikey` - Retrieve current emojikey when requested\n- `set_emojikey` - Create and store a new emojikey\n- `create_superkey` - Create a compressed superkey (after 10 regular emojikeys)\n- `get_emojikey_history` - View previous emojikeys\n\n## New in v0.3.1: Coding Context Support\n\nThis version includes special dimensions for tracking programming-related interaction patterns:\n\n- 💻🔧 (ImplementationFocus) - Balance between high-level design and implementation details\n- 🏗️🔍 (CodeScope) - Building new features vs. improving existing code\n- 🧩🧠 (ProblemSolving) - Practical vs. analytical approaches to coding problems\n- 🔄📊 (ProcessVsResults) - Emphasizing coding process vs. outcomes\n- 📚🧪 (LearnVsApply) - Teaching programming concepts vs. applying them\n- 🚀🛡️ (SpeedVsSecurity) - Development speed vs. security considerations\n- 👥💻 (CollaborationStyle) - Solo coding vs. collaborative approaches\n- 🧬🎨 (CodeStructuring) - Systematic vs. creative code organization\n- 📦🔧 (AbstractionLevel) - Preference for abstraction vs. concrete implementations\n- 🐞📚 (DebugApproach) - Practical vs. theoretical debugging approaches\n\nThese dimensions help Claude adapt to your programming style, providing the right balance of theoretical explanations and practical guidance.\n\n### Example Coding Emojikey\n\n```\n[ME|💻🔧8∠45|🧩🧠7∠60|🐞📚6∠40]~[CONTENT|🏗️🔍9∠30|📚🧪8∠65]~[YOU|👥💻7∠70|🧬🎨8∠55]\n```\n\nThis shows Claude positioning itself with a balanced implementation focus and somewhat analytical problem-solving approach, while perceiving the user as preferring collaborative coding with creative structuring.\n\n## Angle Distribution and Dimension Balance\n\nEmojikey angles represent positioning on each dimension:\n- 0° represents one extreme of a dimension\n- 90° represents a balanced center position\n- 180° represents the opposite extreme\n\nThe current implementation assigns angles primarily in the 0-90° range. Future updates will improve angle distribution to better utilize the full 0-180° spectrum, providing more nuanced dimension positioning.\n\n## Superkeys\n\nAfter creating 10 regular emojikeys, Claude will be prompted to create a superkey that compresses their meaning into a single key with format: `[[×10emoji-sequence]]`\n\nThis allows Claude to maintain a longer conversation history context.\n\n> ⚠️ This is a beta version; the API may change in future updates.\n",
      "npm_url": "https://www.npmjs.com/package/@identimoji/mcp-server-emojikey",
      "npm_downloads": 3370,
      "keywords": [
        "emoji",
        "emojikey",
        "memory",
        "server emojikey",
        "using emoji",
        "emojikey manages"
      ],
      "category": "memory-management"
    },
    "ipospelov--mcp-memory-bank": {
      "owner": "ipospelov",
      "name": "mcp-memory-bank",
      "url": "https://github.com/ipospelov/mcp-memory-bank",
      "imageUrl": "/freedevtools/mcp/pfp/ipospelov.webp",
      "description": "Facilitates the setup and management of a structured Memory Bank for context preservation in AI assistant environments, offering detailed guidance on file structures, template generation, and project summary analysis. Enhances AI context management through organized documentation and relevant content suggestions.",
      "stars": 72,
      "forks": 13,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T23:15:52Z",
      "readme_content": "# Memory Bank MCP Server\n\nThis MCP server helps to build structured documentation system based on [Cline's Memory Bank pattern](https://docs.cline.bot/improving-your-prompting-skills/cline-memory-bank) for context preservation in AI assistant environments. \n\nPowered by [Enlighter](https://enlightby.ai) and [Hyperskill](https://hyperskill.org).\n\nLearn how to setup and use Memory Bank directly in Cursor: http://enlightby.ai/projects/37\n\n[![smithery badge](https://smithery.ai/badge/@ipospelov/mcp-memory-bank)](https://smithery.ai/server/@ipospelov/mcp-memory-bank)\n\n<a href=\"https://glama.ai/mcp/servers/@ipospelov/mcp-memory-bank\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ipospelov/mcp-memory-bank/badge\" alt=\"Memory Bank Server MCP server\" />\n</a>\n\n## Features\n\n- Get detailed information about Memory Bank structure\n- Generate templates for Memory Bank files\n- Analyze project and provide suggestions for Memory Bank content\n\n## Running the Server\n\nThere are a few options to use this MCP server:\n\n### With UVX\n\nAdd this to your mcp.json config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-memory-bank\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/ipospelov/mcp-memory-bank\",\n        \"mcp_memory_bank\"\n      ]\n    }\n  }\n}\n```\n\n### With [Smithery](https://smithery.ai/server/@ipospelov/mcp-memory-bank)\n\nAdd this to your mcp.json config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-bank\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@ipospelov/mcp-memory-bank\",\n        \"--key\",\n        \"your_smithery_key\"\n      ]\n    }\n  }\n}\n```\n\n### With Docker\n\nAdd this to your mcp.json config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-bank\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"19283744/mcp-memory-bank:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Manually\n\nClone repository and run the following commands:\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\nThen add this to your mcp.json config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-bank\": {\n      \"command\": \"python\",\n      \"args\": [\"src/mcp_memory_bank/main.py\"]\n    }\n  }\n}\n```\n\n## Usage Example\n\nAsk Cursor or any other AI code assistant with Memory Bank MCP:\n```\nCreate memory bank for To Do list application with your tools\n```\nProvide more context to get better results.\n\n## Available Tools\n\n### get_memory_bank_structure\n\nReturns a detailed description of the Memory Bank file structure.\n\n### generate_memory_bank_template\n\nReturns a template for a specific Memory Bank file.\n\nExample:\n```json\n{\n  \"file_name\": \"projectbrief.md\"\n}\n```\n\n### analyze_project_summary\n\nAnalyzes a project summary and provides suggestions for Memory Bank content.\n\nExample:\n```json\n{\n  \"project_summary\": \"Building a React web app for inventory management with barcode scanning\"\n}\n```\n\n## Memory Bank Structure\n\nThe Memory Bank consists of core files and optional context files, all in Markdown format:\n\n### Core Files (Required)\n\n1. `projectbrief.md` - Foundation document that shapes all other files\n2. `productContext.md` - Explains why the project exists, problems being solved\n3. `activeContext.md` - Current work focus, recent changes, next steps\n4. `systemPatterns.md` - System architecture, technical decisions, design patterns\n5. `techContext.md` - Technologies used, development setup, constraints\n6. `progress.md` - What works, what's left to build\n7. `memory_bank_instructions.md` - How to work with Memory Bank, instructtions for AI-agent",
      "npm_url": "https://www.npmjs.com/package/mcp-memory-bank",
      "npm_downloads": 360,
      "keywords": [
        "memory",
        "ai",
        "structured",
        "memory bank",
        "memory management",
        "structured memory"
      ],
      "category": "memory-management"
    },
    "itseasy21--mcp-knowledge-graph": {
      "owner": "itseasy21",
      "name": "mcp-knowledge-graph",
      "url": "https://github.com/itseasy21/mcp-knowledge-graph",
      "imageUrl": "/freedevtools/mcp/pfp/itseasy21.webp",
      "description": "Enables AI tools to retain and utilize user-specific information across multiple conversations by leveraging a local knowledge graph for persistent memory management.",
      "stars": 54,
      "forks": 14,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-15T14:36:59Z",
      "readme_content": "# Knowledge Graph Memory Server\n\n[![smithery badge](https://smithery.ai/badge/@itseasy21/mcp-knowledge-graph)](https://smithery.ai/server/@itseasy21/mcp-knowledge-graph)\n\nAn improved implementation of persistent memory using a local knowledge graph with a customizable memory path.\n\nThis lets Claude remember information about the user across chats.\n\n<a href=\"https://glama.ai/mcp/servers/@itseasy21/mcp-knowledge-graph\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@itseasy21/mcp-knowledge-graph/badge\" alt=\"Knowledge Graph Memory Server MCP server\" />\n</a>\n\n> [!NOTE]\n> This is a fork of the original [Memory Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory) and is intended to not use the ephemeral memory npx installation method.\n\n## Server Name\n\n```txt\nmcp-knowledge-graph\n```\n\n\n\n\n\n## Core Concepts\n\n### Entities\n\nEntities are the primary nodes in the knowledge graph. Each entity has:\n\n- A unique name (identifier)\n- An entity type (e.g., \"person\", \"organization\", \"event\")\n- A list of observations\n- Creation date and version tracking\n\nThe version tracking feature helps maintain a historical context of how knowledge evolves over time.\n\nExample:\n\n```json\n{\n  \"name\": \"John_Smith\",\n  \"entityType\": \"person\",\n  \"observations\": [\"Speaks fluent Spanish\"]\n}\n```\n\n### Relations\n\nRelations define directed connections between entities. They are always stored in active voice and describe how entities interact or relate to each other. Each relation includes:\n\n- Source and target entities\n- Relationship type\n- Creation date and version information\n\nThis versioning system helps track how relationships between entities evolve over time.\n\nExample:\n\n```json\n{\n  \"from\": \"John_Smith\",\n  \"to\": \"Anthropic\",\n  \"relationType\": \"works_at\"\n}\n```\n\n### Observations\n\nObservations are discrete pieces of information about an entity. They are:\n\n- Stored as strings\n- Attached to specific entities\n- Can be added or removed independently\n- Should be atomic (one fact per observation)\n\nExample:\n\n```json\n{\n  \"entityName\": \"John_Smith\",\n  \"observations\": [\n    \"Speaks fluent Spanish\",\n    \"Graduated in 2019\",\n    \"Prefers morning meetings\"\n  ]\n}\n```\n\n## API\n\n### Tools\n\n- **create_entities**\n  - Create multiple new entities in the knowledge graph\n  - Input: `entities` (array of objects)\n    - Each object contains:\n      - `name` (string): Entity identifier\n      - `entityType` (string): Type classification\n      - `observations` (string[]): Associated observations\n  - Ignores entities with existing names\n\n- **create_relations**\n  - Create multiple new relations between entities\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type in active voice\n  - Skips duplicate relations\n\n- **add_observations**\n  - Add new observations to existing entities\n  - Input: `observations` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `contents` (string[]): New observations to add\n  - Returns added observations per entity\n  - Fails if entity doesn't exist\n\n- **delete_entities**\n  - Remove entities and their relations\n  - Input: `entityNames` (string[])\n  - Cascading deletion of associated relations\n  - Silent operation if entity doesn't exist\n\n- **delete_observations**\n  - Remove specific observations from entities\n  - Input: `deletions` (array of objects)\n    - Each object contains:\n      - `entityName` (string): Target entity\n      - `observations` (string[]): Observations to remove\n  - Silent operation if observation doesn't exist\n\n- **delete_relations**\n  - Remove specific relations from the graph\n  - Input: `relations` (array of objects)\n    - Each object contains:\n      - `from` (string): Source entity name\n      - `to` (string): Target entity name\n      - `relationType` (string): Relationship type\n  - Silent operation if relation doesn't exist\n\n- **read_graph**\n  - Read the entire knowledge graph\n  - No input required\n  - Returns complete graph structure with all entities and relations\n\n- **search_nodes**\n  - Search for nodes based on query\n  - Input: `query` (string)\n  - Searches across:\n    - Entity names\n    - Entity types\n    - Observation content\n  - Returns matching entities and their relations\n\n- **open_nodes**\n  - Retrieve specific nodes by name\n  - Input: `names` (string[])\n  - Returns:\n    - Requested entities\n    - Relations between requested entities\n  - Silently skips non-existent nodes\n\n## Usage with Cursor, Cline or Claude Desktop\n\n### Setup\n\nAdd this to your mcp.json or claude_desktop_config.json:\n\n```json\n{\n    \"mcpServers\": {\n      \"memory\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"@itseasy21/mcp-knowledge-graph\"\n        ],\n        \"env\": {\n          \"MEMORY_FILE_PATH\": \"/path/to/your/projects.jsonl\"\n        }\n      }\n    }\n  }\n```\n\n### Installing via Smithery\n\nTo install Knowledge Graph Memory Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@itseasy21/mcp-knowledge-graph):\n\n```bash\nnpx -y @smithery/cli install @itseasy21/mcp-knowledge-graph --client claude\n```\n\n### Custom Memory Path\n\nYou can specify a custom path for the memory file in two ways:\n\n1. Using command-line arguments:\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@itseasy21/mcp-knowledge-graph\", \"--memory-path\", \"/path/to/your/memory.jsonl\"]\n    }\n  }\n}\n```\n\n2. Using environment variables:\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@itseasy21/mcp-knowledge-graph\"],\n      \"env\": {\n        \"MEMORY_FILE_PATH\": \"/path/to/your/memory.jsonl\"\n      }\n    }\n  }\n}\n```\n\nIf no path is specified, it will default to memory.jsonl in the server's installation directory.\n\n### System Prompt\n\nThe prompt for utilizing memory depends on the use case. Changing the prompt will help the model determine the frequency and types of memories created.\n\nHere is an example prompt for chat personalization. You could use this prompt in the \"Custom Instructions\" field of a [Claude.ai Project](https://www.anthropic.com/news/projects).\n\n```txt\nFollow these steps for each interaction:\n\n1. User Identification:\n   - You should assume that you are interacting with default_user\n   - If you have not identified default_user, proactively try to do so.\n\n2. Memory Retrieval:\n   - Always begin your chat by saying only \"Remembering...\" and retrieve all relevant information from your knowledge graph\n   - Always refer to your knowledge graph as your \"memory\"\n\n3. Memory\n   - While conversing with the user, be attentive to any new information that falls into these categories:\n     a) Basic Identity (age, gender, location, job title, education level, etc.)\n     b) Behaviors (interests, habits, etc.)\n     c) Preferences (communication style, preferred language, etc.)\n     d) Goals (goals, targets, aspirations, etc.)\n     e) Relationships (personal and professional relationships up to 3 degrees of separation)\n\n4. Memory Update:\n   - If any new information was gathered during the interaction, update your memory as follows:\n     a) Create entities for recurring organizations, people, and significant events\n     b) Connect them to the current entities using relations\n     b) Store facts about them as observations\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/mcp-knowledge-graph",
      "npm_downloads": 9675,
      "keywords": [
        "memory",
        "knowledge",
        "conversations",
        "persistent memory",
        "knowledge graph",
        "conversations leveraging"
      ],
      "category": "memory-management"
    },
    "jlia0--servers": {
      "owner": "jlia0",
      "name": "servers",
      "url": "https://github.com/jlia0/servers",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Utilizes a local knowledge graph to enable persistent memory for AI agents, allowing them to create, update, and retrieve personalized user information across chat sessions. Facilitates tailored interactions by managing entities, relations, and observations.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/servers",
      "npm_downloads": 2441,
      "keywords": [
        "jlia0",
        "memory",
        "persistent",
        "jlia0 servers",
        "management jlia0",
        "memory ai"
      ],
      "category": "memory-management"
    },
    "kiranraathod--taskflow-memory-server": {
      "owner": "kiranraathod",
      "name": "taskflow-memory-server",
      "url": "https://github.com/kiranraathod/taskflow-memory-server",
      "imageUrl": "/freedevtools/mcp/pfp/kiranraathod.webp",
      "description": "Manage tasks with persistent memory to maintain project context and streamline workflow execution using intelligent planning and execution modes. The server integrates seamlessly with FastMCP-compatible clients for enhanced task and context management.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T09:13:12Z",
      "readme_content": "# TaskFlow Memory Server\n\nA task management server with persistent memory architecture for maintaining context and managing workflow execution.\n\n## Overview\n\nTaskFlow Memory Server is a specialized server that combines task management features with a robust Memory Bank architecture for maintaining project context across sessions. The system is designed to support intelligent context-aware task management with persistent state.\n\n## Features\n\n- **Memory Bank System**: Maintains project context in structured Markdown files.\n- **Context Management**: Provides mechanisms for tracking and updating project context.\n- **Task Management**: Integrated functionality for task tracking and execution.\n- **Plan/Act Modes**: Supports distinct planning and execution workflows.\n- **AI Integration**: Uses Claude AI for intelligent planning and task management.\n- **Persistent State**: Maintains context between sessions for continuous workflow.\n- **MCP SDK Integration**: Uses the official Model Context Protocol TypeScript SDK.\n\n## Core Components\n\n- **Memory Manager**: Centralized component for Memory Bank file operations and validation.\n- **Context Manager**: Manages context information with caching and Memory Bank integration.\n- **Plan/Act Mode Engine**: Controls planning and execution workflows with mode-specific functionality.\n- **Async Operation Manager**: Handles long-running operations with status tracking.\n- **TaskFlow Tools**: Collection of tools for interacting with the system through the MCP protocol.\n\n## Memory Bank Structure\n\nThe Memory Bank consists of core files that maintain project context:\n\n- `projectbrief.md`: Foundation document defining core requirements and goals.\n- `productContext.md`: Why the project exists and how it should work.\n- `activeContext.md`: Current work focus and recent changes.\n- `systemPatterns.md`: System architecture and key technical decisions.\n- `techContext.md`: Technologies used and development setup.\n- `progress.md`: Project status, what works, and what's next.\n\n## Core Workflows\n\n### Plan Mode\n\n1. Read Memory Bank files to understand context\n2. Analyze current project state\n3. Develop strategy based on context\n4. Present approach for execution\n5. Update Memory Bank with plan details\n\n### Act Mode\n\n1. Check Memory Bank for relevant context\n2. Update documentation as needed\n3. Execute specific task\n4. Document changes and update Memory Bank\n5. Capture insights from task execution\n\n## Available Tools\n\n### Memory Bank Tools\n- Read, write, and update Memory Bank files\n- Get complete Memory Bank context\n- Update the entire Memory Bank\n\n### Plan-Act Tools\n- Generate project plans\n- Execute tasks\n- Switch between Plan and Act modes\n- Document task insights\n\n### System Tools\n- Get operation status and results\n- Check system status\n- Manage asynchronous operations\n\n## Getting Started\n\n1. Clone the repository\n2. Create a `.env` file with required variables (see `.env.example`)\n3. Install dependencies with `npm install`\n4. Start the server with `npm start`\n\nFor detailed setup and usage instructions, see the [Getting Started Guide](docs/getting-started.md).\n\n## Environment Configuration\n\n```\nANTHROPIC_API_KEY=your_anthropic_api_key\nMODEL=claude-3-7-sonnet-20250219\nMAX_TOKENS=64000\nTEMPERATURE=0.2\nMEMORY_BANK_PATH=./memory-bank\nLOG_LEVEL=info\n```\n\n## Using with MCP-compatible clients\n\nTaskFlow Memory Server can be used with clients that support the Model Context Protocol (MCP). Configure your client to connect to the server and use the provided tools for interacting with the Memory Bank and managing tasks.\n\nExample workflow:\n```\nCan you switch to Plan mode and generate a plan for implementing the file system integration?\n```\n\n### Claude for Desktop Integration\n\nTaskFlow Memory Server is compatible with Claude for Desktop. To configure:\n\n1. Navigate to your Claude for Desktop configuration directory\n2. Copy the configuration from `config/claude-desktop.json`:\n   ```json\n   {\n       \"mcpServers\": {\n           \"taskflow\": {\n               \"command\": \"node\",\n               \"args\": [\n                   \"C:\\\\PATH\\\\TO\\\\taskflow-memory-server\\\\server.js\"\n               ]\n           }\n       }\n   }\n   ```\n3. Update the path in the `args` array to point to your installation of TaskFlow Memory Server\n4. Save the file and restart Claude for Desktop\n5. Select \"taskflow\" as your server in Claude for Desktop\n\nThe TaskFlow Memory Server provides Claude for Desktop with:\n- Persistent memory for context across conversations\n- Task planning and execution workflow\n- Structured knowledge management\n\nCommand examples:\n```\nClaude, can you create a task plan for my project?\nClaude, please update the memory bank with our recent progress\nClaude, retrieve project context from memory bank\n```\n\n## Documentation Updates\n\nMemory Bank files should be updated when:\n\n1. Discovering new project patterns\n2. Implementing significant changes\n3. When context needs clarification\n\nUse the `update_memory_file` tool to update specific Memory Bank files with new information.\n\n## Migration to Official MCP SDK\n\nThis project has been migrated from the third-party FastMCP framework to the official Model Context Protocol (MCP) TypeScript SDK. For details about the migration, see [MIGRATION.md](MIGRATION.md).\n\n## Documentation\n\n- [Getting Started Guide](docs/getting-started.md) - Basic setup and usage instructions\n- [Technical Reference](docs/technical-reference.md) - Detailed technical documentation\n- [Migration Guide](MIGRATION.md) - Details about the migration to the official MCP SDK\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "taskflow",
        "fastmcp",
        "memory",
        "kiranraathod taskflow",
        "taskflow memory",
        "memory management"
      ],
      "category": "memory-management"
    },
    "leonskenidy--omi-mcp": {
      "owner": "leonskenidy",
      "name": "omi-mcp",
      "url": "https://github.com/leonskenidy/omi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/leonskenidy.webp",
      "description": "Provides access to Omi conversations and memories through a standardized MCP interface, enabling reading, creating, and managing these elements efficiently within LLM workflows.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-26T06:05:08Z",
      "readme_content": "# Omi MCP Server\n[![smithery badge](https://smithery.ai/badge/@fourcolors/omi-mcp)](https://smithery.ai/server/@fourcolors/omi-mcp)\n\nThis project provides a Model Context Protocol (MCP) server for interacting with the Omi API. The server provides tools for reading conversations and memories, as well as creating new conversations and memories.\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies with `npm install`\n3. Create a `.env` file with the following variables:\n   ```\n   API_KEY=your_api_key\n   APP_ID=your_app_id\n   ```\n\n## Usage\n\n### Installing via Smithery\n\nTo install Omi MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@fourcolors/omi-mcp):\n\n```bash\nnpx -y @smithery/cli install @fourcolors/omi-mcp --client claude\n```\n\n### Building the Server\n\n```bash\nnpm run build\n```\n\n### Running the Server\n\n```bash\nnpm run start\n```\n\n### Development Mode\n\nFor development with hot-reloading:\n\n```bash\nnpm run dev\n```\n\n### Testing the Server\n\nA simple test client is included to interact with the MCP server. After building the project, run:\n\n```bash\nnpm run test\n```\n\nOr directly:\n\n```bash\n./test-mcp-client.js\n```\n\nThis will start the MCP server and provide an interactive menu to test the available tools. The test client uses a default test user ID (`test-user-123`) for all operations.\n\n### Clean and Rebuild\n\nTo clean the build directory and rebuild from scratch:\n\n```bash\nnpm run rebuild\n```\n\n## Configuration with Claude and Cursor\n\n### Claude Configuration\n\nTo use this MCP server with Claude via Anthropic Console or API:\n\n1. Start the MCP server locally:\n\n   ```bash\n   npm run start\n   ```\n\n2. When setting up your Claude conversation, configure the MCP connection:\n\n   ```json\n   {\n   \t\"mcp_config\": {\n   \t\t\"transports\": [\n   \t\t\t{\n   \t\t\t\t\"type\": \"stdio\",\n   \t\t\t\t\"executable\": {\n   \t\t\t\t\t\"path\": \"/path/to/your/omi-mcp-local/dist/index.js\",\n   \t\t\t\t\t\"args\": []\n   \t\t\t\t}\n   \t\t\t}\n   \t\t]\n   \t}\n   }\n   ```\n\n3. Example prompt to Claude:\n\n   ```\n   Please fetch the latest 5 conversations for user \"user123\" using the Omi API.\n   ```\n\n4. Claude will use the MCP to execute the `read_omi_conversations` tool:\n   ```json\n   {\n   \t\"id\": \"req-1\",\n   \t\"type\": \"request\",\n   \t\"method\": \"tools.read_omi_conversations\",\n   \t\"params\": {\n   \t\t\"user_id\": \"user123\",\n   \t\t\"limit\": 5\n   \t}\n   }\n   ```\n\n### Cursor Configuration\n\nTo use this MCP server with Cursor:\n\n1. Start the MCP server in a terminal:\n\n   ```bash\n   npm run start\n   ```\n\n2. In Cursor, go to Settings > Extensions > MCP Servers\n\n3. Add a new MCP server with these settings:\n\n   - Name: Omi API\n   - URL: stdio:/path/to/your/omi-mcp-local/dist/index.js\n   - Enable the server\n\n4. Now you can use the Omi tools directly within Cursor. For example:\n\n   ```\n   @Omi API Please fetch memories for user \"user123\" and summarize them.\n   ```\n\n5. Cursor will communicate with your MCP server to execute the necessary API calls.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### read_omi_conversations\n\nRetrieves conversations from Omi for a specific user, with optional filters.\n\nParameters:\n\n- `user_id` (string): The user ID to fetch conversations for\n- `limit` (number, optional): Maximum number of conversations to return\n- `offset` (number, optional): Number of conversations to skip for pagination\n- `include_discarded` (boolean, optional): Whether to include discarded conversations\n- `statuses` (string, optional): Comma-separated list of statuses to filter conversations by\n\n### read_omi_memories\n\nRetrieves memories from Omi for a specific user.\n\nParameters:\n\n- `user_id` (string): The user ID to fetch memories for\n- `limit` (number, optional): Maximum number of memories to return\n- `offset` (number, optional): Number of memories to skip for pagination\n\n### create_omi_conversation\n\nCreates a new conversation in Omi for a specific user.\n\nParameters:\n\n- `text` (string): The full text content of the conversation\n- `user_id` (string): The user ID to create the conversation for\n- `text_source` (string): Source of the text content (options: \"audio_transcript\", \"message\", \"other_text\")\n- `started_at` (string, optional): When the conversation/event started (ISO 8601 format)\n- `finished_at` (string, optional): When the conversation/event ended (ISO 8601 format)\n- `language` (string, optional): Language code (default: \"en\")\n- `geolocation` (object, optional): Location data for the conversation\n  - `latitude` (number): Latitude coordinate\n  - `longitude` (number): Longitude coordinate\n- `text_source_spec` (string, optional): Additional specification about the source\n\n### create_omi_memories\n\nCreates new memories in Omi for a specific user.\n\nParameters:\n\n- `user_id` (string): The user ID to create memories for\n- `text` (string, optional): The text content from which memories will be extracted\n- `memories` (array, optional): An array of explicit memory objects to be created directly\n  - `content` (string): The content of the memory\n  - `tags` (array of strings, optional): Tags for the memory\n- `text_source` (string, optional): Source of the text content\n- `text_source_spec` (string, optional): Additional specification about the source\n\n## Testing\n\nTo test the MCP server, you can use the provided test client:\n\n```bash\nnode test-mcp-client.js\n```\n\nThis will start an interactive test client that allows you to:\n\n1. Get conversations\n2. Get memories\n3. Create a conversation\n4. Quit\n\nThe test client uses a default test user ID (`test-user-123`) for all operations.\n\n## Logging\n\nThe MCP server includes built-in logging functionality that writes to both the console and a log file. This is useful for debugging and monitoring server activity.\n\n### Log File Location\n\nLogs are written to `logs/mcp-server.log` in your project directory. The log file includes timestamps and detailed information about:\n\n- Server startup and shutdown\n- All API requests and responses\n- Error messages and stack traces\n- API calls to Omi\n- Request parameters and response data\n\n### Viewing Logs\n\nYou can view the logs in real-time using the `tail` command:\n\n```bash\ntail -f logs/mcp-server.log\n```\n\nThis will show you live updates as the server processes requests and interacts with the Omi API.\n\n### Log Format\n\nEach log entry follows this format:\n\n```\n[2024-03-21T12:34:56.789Z] Log message here\n```\n\nThe timestamp is in ISO 8601 format, making it easy to correlate events and debug issues.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "memory",
        "llm",
        "omi mcp",
        "memory management",
        "mcp interface"
      ],
      "category": "memory-management"
    },
    "mem0ai--mem0-mcp": {
      "owner": "mem0ai",
      "name": "mem0-mcp",
      "url": "https://github.com/mem0ai/mem0-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mem0ai.webp",
      "description": "Store and retrieve user-specific memories to maintain context and make informed decisions based on past interactions using a simple API. Features relevance scoring to enhance memory management with user preferences.",
      "stars": 473,
      "forks": 94,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T02:12:07Z",
      "readme_content": "# MCP Server with Mem0 for Managing Coding Preferences\n\nThis demonstrates a structured approach for using an [MCP](https://modelcontextprotocol.io/introduction) server with [mem0](https://mem0.ai) to manage coding preferences efficiently. The server can be used with Cursor and provides essential tools for storing, retrieving, and searching coding preferences.\n\n## Installation\n\n1. Clone this repository\n2. Initialize the `uv` environment:\n\n```bash\nuv venv\n```\n\n3. Activate the virtual environment:\n\n```bash\nsource .venv/bin/activate\n```\n\n4. Install the dependencies using `uv`:\n\n```bash\n# Install in editable mode from pyproject.toml\nuv pip install -e .\n```\n\n5. Update `.env` file in the root directory with your mem0 API key:\n\n```bash\nMEM0_API_KEY=your_api_key_here\n```\n\n## Usage\n\n1. Start the MCP server:\n\n```bash\nuv run main.py\n```\n\n2. In Cursor, connect to the SSE endpoint, follow this [doc](https://docs.cursor.com/context/model-context-protocol) for reference:\n\n```\nhttp://0.0.0.0:8080/sse\n```\n\n3. Open the Composer in Cursor and switch to `Agent` mode.\n\n## Demo with Cursor\n\nhttps://github.com/user-attachments/assets/56670550-fb11-4850-9905-692d3496231c\n\n## Features\n\nThe server provides three main tools for managing code preferences:\n\n1. `add_coding_preference`: Store code snippets, implementation details, and coding patterns with comprehensive context including:\n   - Complete code with dependencies\n   - Language/framework versions\n   - Setup instructions\n   - Documentation and comments\n   - Example usage\n   - Best practices\n\n2. `get_all_coding_preferences`: Retrieve all stored coding preferences to analyze patterns, review implementations, and ensure no relevant information is missed.\n\n3. `search_coding_preferences`: Semantically search through stored coding preferences to find relevant:\n   - Code implementations\n   - Programming solutions\n   - Best practices\n   - Setup guides\n   - Technical documentation\n\n## Why?\n\nThis implementation allows for a persistent coding preferences system that can be accessed via MCP. The SSE-based server can run as a process that agents connect to, use, and disconnect from whenever needed. This pattern fits well with \"cloud-native\" use cases where the server and clients can be decoupled processes on different nodes.\n\n### Server\n\nBy default, the server runs on 0.0.0.0:8080 but is configurable with command line arguments like:\n\n```\nuv run main.py --host <your host> --port <your port>\n```\n\nThe server exposes an SSE endpoint at `/sse` that MCP clients can connect to for accessing the coding preferences management tools.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "mem0",
        "mem0ai",
        "memory management",
        "management mem0ai",
        "specific memories"
      ],
      "category": "memory-management"
    },
    "movibe--memory-bank-mcp": {
      "owner": "movibe",
      "name": "memory-bank-mcp",
      "url": "https://github.com/movibe/memory-bank-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/movibe.webp",
      "description": "Manage and interact with structured repositories of information for AI assistants, enabling the storage, retrieval, and tracking of context across sessions to enhance AI capabilities. Supports operational modes for tasks like coding, debugging, and system design.",
      "stars": 43,
      "forks": 14,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T00:28:02Z",
      "readme_content": "# Memory Bank MCP 🧠\n\n[![NPM Version](https://img.shields.io/npm/v/@movibe/memory-bank-mcp.svg)](https://www.npmjs.com/package/@movibe/memory-bank-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/movibe/memory-bank-mcp/actions/workflows/test.yml/badge.svg)](https://github.com/movibe/memory-bank-mcp/actions/workflows/test.yml)\n\nA Model Context Protocol (MCP) server for managing Memory Banks, allowing AI assistants to store and retrieve information across sessions.\n\n<a href=\"https://glama.ai/mcp/servers/riei9a6dhx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/riei9a6dhx/badge\" alt=\"Memory Bank MCP server\" />\n</a>\n\n## Overview 📋\n\nMemory Bank Server provides a set of tools and resources for AI assistants to interact with Memory Banks. Memory Banks are structured repositories of information that help maintain context and track progress across multiple sessions.\n\n## Features ✨\n\n- **Memory Bank Management**: Initialize, find, and manage Memory Banks\n- **File Operations**: Read and write files in Memory Banks\n- **Progress Tracking**: Track progress and update Memory Bank files\n- **Decision Logging**: Log important decisions with context and alternatives\n- **Active Context Management**: Maintain and update active context information\n- **Mode Support**: Detect and use .clinerules files for mode-specific behavior\n- **UMB Command**: Update Memory Bank files temporarily with the UMB command\n- **Robust Error Handling**: Gracefully handle errors and continue operation when possible\n- **Status Prefix System**: Immediate visibility into Memory Bank operational state\n\n## Directory Structure 📁\n\nBy default, Memory Bank uses a `memory-bank` directory in the root of your project. When you specify a project path using the `--path` option, the Memory Bank will be created or accessed at `<project_path>/memory-bank`.\n\nYou can customize the name of the Memory Bank folder using the `--folder` option. For example, if you set `--folder custom-memory`, the Memory Bank will be created or accessed at `<project_path>/custom-memory`.\n\nFor more details on customizing the folder name, see [Custom Memory Bank Folder Name](docs/custom-folder-name.md).\n\n## Recent Improvements 🛠️\n\n- **Customizable Folder Name**: You can now specify a custom folder name for the Memory Bank\n- **Consistent Directory Structure**: Memory Bank now always uses the configured folder name in the project root\n- **Enhanced Initialization**: Memory Bank now works even when .clinerules files don't exist\n- **Better Path Handling**: Improved handling of absolute and relative paths\n- **Improved Directory Detection**: Better detection of existing memory-bank directories\n- **More Robust Error Handling**: Graceful handling of errors related to .clinerules files\n\nFor more details, see [Memory Bank Bug Fixes](docs/memory-bank-bug-fixes.md).\n\n## Installation 🚀\n\n```bash\n# Install from npm\nnpm install @movibe/memory-bank-mcp\n\n# Or install globally\nnpm install -g @movibe/memory-bank-mcp\n\n# Or run directly with npx (no installation required)\nnpx @movibe/memory-bank-mcp\n```\n\n## Usage with npx 💻\n\nYou can run Memory Bank MCP directly without installation using npx:\n\n```bash\n# Run with default settings\nnpx @movibe/memory-bank-mcp\n\n# Run with specific mode\nnpx @movibe/memory-bank-mcp --mode code\n\n# Run with custom project path\nnpx @movibe/memory-bank-mcp --path /path/to/project\n\n# Run with custom folder name\nnpx @movibe/memory-bank-mcp --folder custom-memory-bank\n\n# Show help\nnpx @movibe/memory-bank-mcp --help\n```\n\nFor more detailed information about using npx, see [npx-usage.md](docs/npx-usage.md).\n\n## Configuring in Cursor 🖱️\n\nCursor is an AI-powered code editor that supports the Model Context Protocol (MCP). To configure Memory Bank MCP in Cursor:\n\n1. **Use Memory Bank MCP with npx**:\n\n   No need to install the package globally. You can use npx directly:\n\n   ```bash\n   # Verify npx is working correctly\n   npx @movibe/memory-bank-mcp --help\n   ```\n\n2. **Open Cursor Settings**:\n\n   - Go to Settings (⚙️) > Extensions > MCP\n   - Click on \"Add MCP Server\"\n\n3. **Configure the MCP Server**:\n\n   - **Name**: Memory Bank MCP\n   - **Command**: npx\n   - **Arguments**: `@movibe/memory-bank-mcp --mode code` (or other mode as needed)\n\n4. **Save and Activate**:\n\n   - Click \"Save\"\n   - Enable the MCP server by toggling it on\n\n5. **Verify Connection**:\n   - Open a project in Cursor\n   - The Memory Bank MCP should now be active and available in your AI interactions\n\nFor detailed instructions and advanced usage with Cursor, see [cursor-integration.md](docs/cursor-integration.md).\n\n### Using with Cursor 🤖\n\nOnce configured, you can interact with Memory Bank MCP in Cursor through AI commands:\n\n- **Initialize a Memory Bank**: `/mcp memory-bank-mcp initialize_memory_bank path=./memory-bank`\n- **Track Progress**: `/mcp memory-bank-mcp track_progress action=\"Feature Implementation\" description=\"Implemented feature X\"`\n- **Log Decision**: `/mcp memory-bank-mcp log_decision title=\"API Design\" context=\"...\" decision=\"...\"`\n- **Switch Mode**: `/mcp memory-bank-mcp switch_mode mode=code`\n\n## MCP Modes and Their Usage 🔄\n\nMemory Bank MCP supports different operational modes to optimize AI interactions for specific tasks:\n\n### Available Modes\n\n1. **Code Mode** 👨‍💻\n\n   - Focus: Code implementation and development\n   - Usage: `npx @movibe/memory-bank-mcp --mode code`\n   - Best for: Writing, refactoring, and optimizing code\n\n2. **Architect Mode** 🏗️\n\n   - Focus: System design and architecture\n   - Usage: `npx @movibe/memory-bank-mcp --mode architect`\n   - Best for: Planning project structure, designing components, and making architectural decisions\n\n3. **Ask Mode** ❓\n\n   - Focus: Answering questions and providing information\n   - Usage: `npx @movibe/memory-bank-mcp --mode ask`\n   - Best for: Getting explanations, clarifications, and information\n\n4. **Debug Mode** 🐛\n\n   - Focus: Troubleshooting and problem-solving\n   - Usage: `npx @movibe/memory-bank-mcp --mode debug`\n   - Best for: Finding and fixing bugs, analyzing issues\n\n5. **Test Mode** ✅\n   - Focus: Testing and quality assurance\n   - Usage: `npx @movibe/memory-bank-mcp --mode test`\n   - Best for: Writing tests, test-driven development\n\n### Switching Modes\n\nYou can switch modes in several ways:\n\n1. **When starting the server**:\n\n   ```bash\n   npx @movibe/memory-bank-mcp --mode architect\n   ```\n\n2. **During a session**:\n\n   ```bash\n   memory-bank-mcp switch_mode mode=debug\n   ```\n\n3. **In Cursor**:\n\n   ```\n   /mcp memory-bank-mcp switch_mode mode=test\n   ```\n\n4. **Using .clinerules files**:\n   Create a `.clinerules-[mode]` file in your project to automatically switch to that mode when the file is detected.\n\n## How Memory Bank MCP Works 🧠\n\nMemory Bank MCP is built on the Model Context Protocol (MCP), which enables AI assistants to interact with external tools and resources. Here's how it works:\n\n### Core Components 🧩\n\n1. **Memory Bank**: A structured repository of information stored as markdown files:\n\n   - `product-context.md`: Overall project information and goals\n   - `active-context.md`: Current state, ongoing tasks, and next steps\n   - `progress.md`: History of project updates and milestones\n   - `decision-log.md`: Record of important decisions with context and rationale\n   - `system-patterns.md`: Architecture and code patterns used in the project\n\n2. **MCP Server**: Provides tools and resources for AI assistants to interact with Memory Banks:\n\n   - Runs as a standalone process\n   - Communicates with AI assistants through the MCP protocol\n   - Provides a set of tools for managing Memory Banks\n\n3. **Mode System**: Supports different operational modes:\n   - `code`: Focus on code implementation\n   - `ask`: Focus on answering questions\n   - `architect`: Focus on system design\n   - `debug`: Focus on debugging issues\n   - `test`: Focus on testing\n\n### Data Flow 🔄\n\n1. **Initialization**: The AI assistant connects to the MCP server and initializes a Memory Bank\n2. **Tool Calls**: The AI assistant calls tools provided by the MCP server to read/write Memory Bank files\n3. **Context Maintenance**: The Memory Bank maintains context across sessions, allowing the AI to recall previous decisions and progress\n\n### Memory Bank Structure 📂\n\nMemory Banks use a standardized structure to organize information:\n\n- **Product Context**: Project overview, objectives, technologies, and architecture\n- **Active Context**: Current state, ongoing tasks, known issues, and next steps\n- **Progress**: Chronological record of project updates and milestones\n- **Decision Log**: Record of important decisions with context, alternatives, and consequences\n- **System Patterns**: Architecture patterns, code patterns, and documentation patterns\n\n### Advanced Features 🚀\n\n- **UMB Command**: Temporarily update Memory Bank files during a session without committing changes\n- **Mode Detection**: Automatically detect and switch modes based on user input\n- **File Migration**: Tools for migrating between different file naming conventions\n- **Language Standardization**: All Memory Bank files are generated in English for consistency\n\n## Versioning 📌\n\nThis project follows [Semantic Versioning](https://semver.org/) and uses [Conventional Commits](https://www.conventionalcommits.org/) for commit messages. The version is automatically bumped and a changelog is generated based on commit messages when changes are merged into the main branch.\n\n- **Major version** is bumped when there are breaking changes (commit messages with `BREAKING CHANGE` or `!:`)\n- **Minor version** is bumped when new features are added (commit messages with `feat:` or `feat(scope):`)\n- **Patch version** is bumped for all other changes (bug fixes, documentation, etc.)\n\nFor the complete history of changes, see the [CHANGELOG.md](CHANGELOG.md) file.\n\n## Usage 📝\n\n### As a Command Line Tool 💻\n\n```bash\n# Initialize a Memory Bank\nmemory-bank-mcp initialize_memory_bank path=./memory-bank\n\n# Track progress\nmemory-bank-mcp track_progress action=\"Feature Implementation\" description=\"Implemented feature X\"\n\n# Log a decision\nmemory-bank-mcp log_decision title=\"API Design\" context=\"...\" decision=\"...\"\n\n# Switch mode\nmemory-bank-mcp switch_mode mode=code\n```\n\n### As a Library 📚\n\n```typescript\nimport { MemoryBankServer } from \"@movibe/memory-bank-mcp\";\n\n// Create a new server instance\nconst server = new MemoryBankServer();\n\n// Start the server\nserver.run().catch(console.error);\n```\n\n## Contributing 👥\n\nPlease see [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License 📄\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Memory Bank Status System 🚦\n\nMemory Bank MCP implements a status prefix system that provides immediate visibility into the operational state of the Memory Bank:\n\n### Status Indicators\n\nEvery response from an AI assistant using Memory Bank MCP begins with one of these status indicators:\n\n- **`[MEMORY BANK: ACTIVE]`**: The Memory Bank is available and being used to provide context-aware responses\n- **`[MEMORY BANK: INACTIVE]`**: The Memory Bank is not available or not properly configured\n- **`[MEMORY BANK: UPDATING]`**: The Memory Bank is currently being updated (during UMB command execution)\n\nThis system ensures users always know whether the AI assistant is operating with full context awareness or limited information.\n\n### Benefits\n\n- **Transparency**: Users always know whether the AI has access to the full project context\n- **Troubleshooting**: Makes it immediately obvious when Memory Bank is not properly configured\n- **Context Awareness**: Helps users understand why certain responses may lack historical context\n\nFor more details, see [Memory Bank Status Prefix System](docs/memory-bank-status-prefix.md).",
      "npm_url": "https://www.npmjs.com/package/memory-bank-mcp",
      "npm_downloads": 3190,
      "keywords": [
        "memory",
        "movibe",
        "ai",
        "movibe memory",
        "memory management",
        "management movibe"
      ],
      "category": "memory-management"
    },
    "neobundy--cwkCursorPippaMCP": {
      "owner": "neobundy",
      "name": "cwkCursorPippaMCP",
      "url": "https://github.com/neobundy/cwkCursorPippaMCP",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Manage and enhance memory capabilities for AI assistants within Cursor IDE, enabling seamless storage, recall, and management of information across conversations for more personalized interactions.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cursor",
        "cwkcursorpippamcp",
        "memory",
        "assistants cursor",
        "ai assistants",
        "cursor ide"
      ],
      "category": "memory-management"
    },
    "oculairmedia--Letta-MCP-server": {
      "owner": "oculairmedia",
      "name": "Letta-MCP-server",
      "url": "https://github.com/oculairmedia/Letta-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/oculairmedia.webp",
      "description": "Manage agents and memory blocks within the Letta system, enabling the creation, listing, and attachment of memory blocks to agents while facilitating seamless communication through message sending and response receiving.",
      "stars": 38,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T02:32:37Z",
      "readme_content": "[![npm version](https://img.shields.io/npm/v/letta-mcp-server.svg)](https://www.npmjs.com/package/letta-mcp-server)\n[![npm downloads](https://img.shields.io/npm/dm/letta-mcp-server.svg)](https://www.npmjs.com/package/letta-mcp-server)\n[![npm downloads total](https://img.shields.io/npm/dt/letta-mcp-server.svg)](https://www.npmjs.com/package/letta-mcp-server)\n[![Docker Image](https://img.shields.io/badge/Docker-ghcr.io-blue)](https://github.com/oculairmedia/Letta-MCP-server/pkgs/container/letta-mcp-server)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/oculairmedia-letta-mcp-server)\n[![CI/CD](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/test.yml/badge.svg)](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/test.yml)\n[![Docker Build](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/docker-build.yml/badge.svg)](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/docker-build.yml)\n[![CodeQL](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/codeql.yml/badge.svg)](https://github.com/oculairmedia/letta-MCP-server/actions/workflows/codeql.yml)\n[![Coverage Status](https://codecov.io/gh/oculairmedia/letta-MCP-server/branch/main/graph/badge.svg)](https://codecov.io/gh/oculairmedia/letta-MCP-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n# Letta MCP Server\n\nA Model Context Protocol (MCP) server that provides comprehensive tools for agent management, memory operations, and integration with the Letta system. This server implements the full MCP specification including tools, prompts, and resources, with enhanced descriptions, output schemas, and behavioral annotations.\n\n**[View on npm](https://www.npmjs.com/package/letta-mcp-server)** | **[View on GitHub](https://github.com/oculairmedia/Letta-MCP-server)**\n\n## Features\n\n- 🤖 **Agent Management** - Create, modify, clone, and manage Letta agents\n- 🧠 **Memory Operations** - Handle memory blocks and passages\n- 🔧 **Tool Integration** - Attach and manage tools for agents with full MCP support\n- 💬 **Prompts** - Interactive wizards and assistants for common workflows\n- 📚 **Resources** - Access system information, documentation, and agent data\n- 🌐 **Multiple Transports** - HTTP, SSE, and stdio support\n- 🔗 **MCP Server Integration** - Integrate with other MCP servers\n- 📊 **Enhanced Metadata** - Output schemas and behavioral annotations for all tools\n- 📦 **Docker Support** - Easy deployment with Docker\n\n## Environment Configuration\n\nCreate a `.env` file with the following variables:\n\n```bash\n# Required\nLETTA_BASE_URL=https://your-letta-instance.com/v1\nLETTA_PASSWORD=your-secure-password\n\n# Optional\nPORT=3001\nNODE_ENV=production\n```\n\n## Installation\n\n### Install from npm\n\n```bash\n# Global installation (recommended for CLI usage)\nnpm install -g letta-mcp-server\n\n# Or local installation\nnpm install letta-mcp-server\n```\n\n### Use with Claude Desktop\n\nAfter installing globally, add to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"letta\": {\n      \"command\": \"letta-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"LETTA_BASE_URL\": \"https://your-letta-instance.com/v1\",\n        \"LETTA_PASSWORD\": \"your-secure-password\"\n      }\n    }\n  }\n}\n```\n\n### Quick Start with npm\n\n```bash\n# Install globally\nnpm install -g letta-mcp-server\n\n# Set environment variables\nexport LETTA_BASE_URL=https://your-letta-instance.com/v1\nexport LETTA_PASSWORD=your-secure-password\n\n# Run the server\nletta-mcp              # stdio (for Claude Desktop)\nletta-mcp --http       # HTTP transport\nletta-mcp --sse        # SSE transport\n```\n\n## Quick Setup\n\n### Option 1: Run from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/oculairmedia/letta-MCP-server.git\ncd letta-MCP-server\n\n# Install dependencies\nnpm install\n\n# Development\nnpm run dev         # Default (stdio) transport\nnpm run dev:sse     # SSE transport\nnpm run dev:http    # HTTP transport (recommended)\n\n# Production\nnpm run start       # Default (stdio) transport\nnpm run start:sse   # SSE transport\nnpm run start:http  # HTTP transport (recommended)\n```\n\n### Option 2: Run with Docker\n\n#### Using the prebuilt image from GitHub Container Registry\n\nAvailable tags:\n- `latest` - Latest stable release\n- `2.0.1`, `2.0`, `2` - Specific version tags\n- `master` - Latest master branch build\n\n```bash\n# Pull the latest image\ndocker pull ghcr.io/oculairmedia/letta-mcp-server:latest\n\n# Run with environment variables\ndocker run -d \\\n  -p 3001:3001 \\\n  -e LETTA_BASE_URL=https://your-letta-instance.com/v1 \\\n  -e LETTA_PASSWORD=your-secure-password \\\n  -e PORT=3001 \\\n  -e NODE_ENV=production \\\n  --name letta-mcp \\\n  ghcr.io/oculairmedia/letta-mcp-server:latest\n\n# Or use a specific version\ndocker run -d \\\n  -p 3001:3001 \\\n  -e LETTA_BASE_URL=https://your-letta-instance.com/v1 \\\n  -e LETTA_PASSWORD=your-secure-password \\\n  --name letta-mcp \\\n  ghcr.io/oculairmedia/letta-mcp-server:2.0.1\n```\n\n#### Using Docker Compose\n\n```yaml\nversion: '3.8'\nservices:\n  letta-mcp:\n    image: ghcr.io/oculairmedia/letta-mcp-server:latest\n    container_name: letta-mcp\n    ports:\n      - \"3001:3001\"\n    environment:\n      - LETTA_BASE_URL=https://your-letta-instance.com/v1\n      - LETTA_PASSWORD=your-secure-password\n      - PORT=3001\n      - NODE_ENV=production\n    restart: unless-stopped\n```\n\n#### Building from source\n\n```bash\n# Clone and build locally\ngit clone https://github.com/oculairmedia/letta-MCP-server.git\ncd letta-MCP-server\ndocker build -t letta-mcp-server .\ndocker run -d -p 3001:3001 --env-file .env --name letta-mcp letta-mcp-server\n```\n\n### Option 3: Run with stdio for local MCP\n\n```bash\n# Create startup script\nchmod +x /opt/stacks/letta-MCP-server/start-mcp.sh\n\n# Add to Claude\nclaude mcp add --transport stdio letta-tools \"/opt/stacks/letta-MCP-server/start-mcp.sh\"\n```\n\n## Architecture\n\nSee the [Architecture Documentation](docs/ARCHITECTURE.md) for detailed system diagrams and component relationships.\n\n## MCP Protocol Support\n\nThis server implements the full MCP specification with all three capabilities:\n\n### 🔧 Tools\nAll tools include:\n- **Enhanced Descriptions**: Detailed explanations with use cases and best practices\n- **Output Schemas**: Structured response definitions for predictable outputs\n- **Behavioral Annotations**: Hints about tool behavior (readOnly, costLevel, executionTime, etc.)\n\n### 💬 Prompts\nInteractive prompts for common workflows:\n- `letta_agent_wizard` - Guided agent creation with memory and tool setup\n- `letta_memory_optimizer` - Analyze and optimize agent memory usage\n- `letta_debug_assistant` - Troubleshoot agent issues\n- `letta_tool_config` - Discover, attach, create, or audit tools\n- `letta_migration` - Export, import, upgrade, or clone agents\n\n### 📚 Resources\nAccess system information and documentation:\n- `letta://system/status` - System health and version info\n- `letta://system/models` - Available LLM and embedding models\n- `letta://agents/list` - Overview of all agents\n- `letta://tools/all/docs` - Complete tool documentation with examples\n- `letta://docs/mcp-integration` - Integration guide\n- `letta://docs/api-reference` - API quick reference\n\nResource templates for dynamic content:\n- `letta://agents/{agent_id}/config` - Agent configuration\n- `letta://agents/{agent_id}/memory/{block_id}` - Memory block content\n- `letta://tools/{tool_name}/docs` - Individual tool documentation\n\n## Available Tools\n\n### Agent Management\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `create_agent` | Create a new Letta agent | 💰 Medium cost, ⚡ Fast |\n| `list_agents` | List all available agents | 👁️ Read-only, 💰 Low cost |\n| `prompt_agent` | Send a message to an agent | 💰 High cost, ⏱️ Variable time, 🔒 Rate limited |\n| `retrieve_agent` | Get agent details by ID | 👁️ Read-only, ⚡ Fast |\n| `get_agent_summary` | Get agent summary information | 👁️ Read-only, ⚡ Fast |\n| `modify_agent` | Update an existing agent | ✏️ Modifies state, ⚡ Fast |\n| `delete_agent` | Delete an agent | ⚠️ Dangerous, 🗑️ Permanent |\n| `clone_agent` | Clone an existing agent | 💰 Medium cost, ⏱️ Medium time |\n| `bulk_delete_agents` | Delete multiple agents | ⚠️ Dangerous, 📦 Bulk operation |\n| `export_agent` | Export agent configuration and memory | 👁️ Read-only, ⚡ Fast, 📦 Full backup |\n| `import_agent` | Import agent from backup | 💰 High cost, ⏱️ Slow, ✏️ Creates state |\n\n### Memory Management\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_memory_blocks` | List all memory blocks | 👁️ Read-only, ⚡ Fast |\n| `create_memory_block` | Create a new memory block | ✏️ Creates state, ⚡ Fast |\n| `read_memory_block` | Read a memory block | 👁️ Read-only, ⚡ Fast |\n| `update_memory_block` | Update a memory block | ✏️ Modifies state, ⚡ Fast |\n| `attach_memory_block` | Attach memory to an agent | ✏️ Links resources, ⚡ Fast |\n\n### Passage Management\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_passages` | Search archival memory | 👁️ Read-only, ⚡ Fast |\n| `create_passage` | Create archival memory | 💰 Medium cost (embeddings), ⚡ Fast |\n| `modify_passage` | Update archival memory | 💰 Medium cost (re-embedding), ⚡ Fast |\n| `delete_passage` | Delete archival memory | 🗑️ Permanent, ⚡ Fast |\n\n### Tool Management\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_agent_tools` | List tools for an agent | 👁️ Read-only, ⚡ Fast |\n| `attach_tool` | Attach tools to an agent | ✏️ Modifies capabilities, ⚡ Fast |\n| `upload_tool` | Upload a custom tool | 🔒 Security: Executes code, ⚡ Fast |\n| `bulk_attach_tool_to_agents` | Attach tool to multiple agents | 📦 Bulk operation, ⏱️ Slow |\n\n### Model Management\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_llm_models` | List available LLM models | 👁️ Read-only, ⚡ Fast |\n| `list_embedding_models` | List available embedding models | 👁️ Read-only, ⚡ Fast |\n\n### MCP Integration\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_mcp_servers` | List configured MCP servers | 👁️ Read-only, ⚡ Fast |\n| `list_mcp_tools_by_server` | List tools from an MCP server | 👁️ Read-only, ⚡ Fast |\n| `add_mcp_tool_to_letta` | Import MCP tool to Letta | ✏️ Creates tool, ⚡ Fast |\n\n### Prompt Tools\n\n| Tool | Description | Annotations |\n|------|-------------|-------------|\n| `list_prompts` | List available prompt templates | 👁️ Read-only, ⚡ Fast |\n| `use_prompt` | Execute a prompt template | 💰 Variable cost, ⏱️ Variable time |\n\n## Directory Structure\n\n- `src/index.js` - Main entry point\n- `src/core/` - Core server functionality\n- `src/handlers/` - Prompt and resource handlers\n- `src/examples/` - Example prompts and resources\n- `src/tools/` - Tool implementations organized by category:\n  - `agents/` - Agent management tools\n  - `memory/` - Memory block tools\n  - `passages/` - Passage management tools\n  - `tools/` - Tool attachment and management\n  - `mcp/` - MCP server integration tools\n  - `models/` - Model listing tools\n  - `enhanced-descriptions.js` - Detailed tool descriptions\n  - `output-schemas.js` - Structured output definitions\n  - `annotations.js` - Behavioral hints\n- `src/transports/` - Server transport implementations\n\n## Transport Protocols\n\nThe server supports three transport protocols:\n\n1. **HTTP (Recommended)** - Streamable HTTP transport with full duplex communication\n   - Endpoint: `http://your-server:3001/mcp`\n   - Best for production use and remote connections\n   - Supports health checks at `/health`\n\n2. **SSE (Server-Sent Events)** - Real-time event streaming\n   - Endpoint: `http://your-server:3001/sse`\n   - Good for unidirectional server-to-client updates\n\n3. **stdio** - Standard input/output\n   - Direct process communication\n   - Best for local development and Claude integration\n\n## Configuration with MCP Settings\n\nAdd the server to your mcp_settings.json:\n\n```json\n\"letta\": {\n  \"command\": \"node\",\n  \"args\": [\n    \"--no-warnings\",\n    \"--experimental-modules\",\n    \"path/to/letta-server/src/index.js\"\n  ],\n  \"env\": {\n    \"LETTA_BASE_URL\": \"https://your-letta-instance.com\",\n    \"LETTA_PASSWORD\": \"yourPassword\"\n  },\n  \"disabled\": false,\n  \"alwaysAllow\": [\n    \"upload_tool\",\n    \"attach_tool\",\n    \"list_agents\",\n    \"list_memory_blocks\"\n  ],\n  \"timeout\": 300\n}\n```\n\nFor remote instances with HTTP transport (recommended):\n\n```json\n\"remote_letta_tools\": {\n  \"url\": \"http://your-server:3001/mcp\",\n  \"transport\": \"http\",\n  \"disabled\": false,\n  \"alwaysAllow\": [\n    \"attach_tool\", \n    \"list_agents\",\n    \"list_tools\",\n    \"get_agent\"\n  ],\n  \"timeout\": 120\n}\n```\n\n## Docker Operations\n\n```bash\n# View container logs\ndocker logs -f letta-mcp\n\n# Stop the container\ndocker stop letta-mcp\n\n# Update to latest version\ndocker pull ghcr.io/oculairmedia/letta-mcp-server:latest\ndocker stop letta-mcp\ndocker rm letta-mcp\ndocker run -d -p 3001:3001 -e PORT=3001 -e NODE_ENV=production --name letta-mcp ghcr.io/oculairmedia/letta-mcp-server:latest\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection refused errors**\n   - Ensure the server is running and accessible\n   - Check firewall settings for port 3001\n   - Verify the correct transport protocol is being used\n\n2. **Authentication failures**\n   - Verify LETTA_BASE_URL includes `/v1` suffix\n   - Check LETTA_PASSWORD is correct\n   - Ensure environment variables are loaded\n   - When self-hosting the Letta-Server, set environment variables accordingly:\n     ```json\n     \"env\": {\n        \"LETTA_BASE_URL\": \"http://localhost:8283\",\n        \"LETTA_PASSWORD\": \"\",\n        \"LOG_LEVEL\": \"info\"\n      }\n     ```\n\n\n3. **Tool execution timeouts**\n   - Increase timeout values in MCP configuration\n   - Check network latency for remote connections\n   - Consider using HTTP transport for better reliability\n\n### Health Check\n\nThe HTTP transport provides a health endpoint:\n\n```bash\ncurl http://your-server:3001/health\n```\n\nResponse:\n```json\n{\n  \"status\": \"healthy\",\n  \"transport\": \"streamable_http\",\n  \"protocol_version\": \"2025-06-18\",\n  \"sessions\": 0,\n  \"uptime\": 12345.678\n}\n```\n\n## Development\n\n### Testing\n\n```bash\n# Run tests\nnpm test\n\n# Run tests with coverage\nnpm run test:coverage\n\n# Run linter\nnpm run lint\n```\n\n### Contributing\n\nWe welcome contributions! Please see our [Contributing Guide](docs/CONTRIBUTING.md) for details on:\n\n- Development setup\n- Code style and standards\n- Adding new tools\n- Testing requirements\n- Pull request process\n\n## Security\n\nFor security vulnerabilities, please see our [Security Policy](docs/SECURITY.md).\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "https://www.npmjs.com/package/letta-mcp-server",
      "npm_downloads": 229,
      "keywords": [
        "letta",
        "oculairmedia",
        "memory",
        "oculairmedia letta",
        "letta enabling",
        "management oculairmedia"
      ],
      "category": "memory-management"
    },
    "pinkpixel-dev--mem0-mcp": {
      "owner": "pinkpixel-dev",
      "name": "mem0-mcp",
      "url": "https://github.com/pinkpixel-dev/mem0-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pinkpixel-dev.webp",
      "description": "Provides a memory system for AI applications where information can be stored and retrieved across user sessions. It enables personalized interactions by managing user-specific data efficiently.",
      "stars": 76,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T09:06:00Z",
      "readme_content": "![Mem0 Logo](https://res.cloudinary.com/di7ctlowx/image/upload/v1741739911/mem0-logo_dlssjm.svg)\n\n[![npm version](https://badge.fury.io/js/@pinkpixel%2Fmem0-mcp.svg)](https://badge.fury.io/js/@pinkpixel%2Fmem0-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Node.js](https://img.shields.io/badge/Node.js-18%2B-green.svg)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.3%2B-blue.svg)](https://www.typescriptlang.org/)\n[![MCP](https://img.shields.io/badge/MCP-0.6.0-purple.svg)](https://modelcontextprotocol.io/)\n[![Mem0](https://img.shields.io/badge/Mem0-2.1%2B-orange.svg)](https://mem0.ai)\n[![Downloads](https://img.shields.io/npm/dm/@pinkpixel/mem0-mcp.svg)](https://www.npmjs.com/package/@pinkpixel/mem0-mcp)\n[![GitHub Stars](https://img.shields.io/github/stars/pinkpixel-dev/mem0-mcp.svg)](https://github.com/pinkpixel-dev/mem0-mcp)\n[![smithery badge](https://smithery.ai/badge/@pinkpixel-dev/mem0-mcp-server)](https://smithery.ai/server/@pinkpixel-dev/mem0-mcp-server)\n\n# @pinkpixel/mem0-mcp MCP Server ✨\n\nA Model Context Protocol (MCP) server that integrates with [Mem0.ai](https://mem0.ai/) to provide persistent memory capabilities for LLMs. It allows AI agents to store and retrieve information across sessions.\n\nThis server uses the `mem0ai` Node.js SDK for its core functionality.\n\n## Features 🧠\n\n### Tools\n*   **`add_memory`**: Stores a piece of text content as a memory associated with a specific `userId`.\n    *   **Required:** `content` (string), `userId` (string)\n    *   **Optional:** `sessionId` (string), `agentId` (string), `appId` (string), `metadata` (object)\n    *   **Advanced (Cloud API):** `includes` (string), `excludes` (string), `infer` (boolean), `outputFormat` (string), `customCategories` (object), `customInstructions` (string), `immutable` (boolean), `expirationDate` (string)\n    *   Stores the provided text, enabling recall in future interactions.\n*   **`search_memory`**: Searches stored memories based on a natural language query for a specific `userId`.\n    *   **Required:** `query` (string), `userId` (string)\n    *   **Optional:** `sessionId` (string), `agentId` (string), `appId` (string), `filters` (object), `threshold` (number)\n    *   **Advanced (Cloud API):** `topK` (number), `fields` (array), `rerank` (boolean), `keywordSearch` (boolean), `filterMemories` (boolean)\n    *   Retrieves relevant memories based on semantic similarity.\n*   **`delete_memory`**: Deletes a specific memory from storage by its ID.\n    *   **Required:** `memoryId` (string), `userId` (string)\n    *   **Optional:** `agentId` (string), `appId` (string)\n    *   Permanently removes the specified memory.\n\n## Prerequisites 🔑\n\nThis server supports three storage modes:\n\n1. **Cloud Storage Mode** ☁️ (Recommended for production)\n   * Requires a **Mem0 API key** (provided as `MEM0_API_KEY` environment variable)\n   * Memories are persistently stored on Mem0's cloud servers\n   * No local database needed\n   * Full feature support with advanced filtering and search\n\n2. **Supabase Storage Mode** 🗄️ (Recommended for self-hosting)\n   * Requires **Supabase credentials** (`SUPABASE_URL` and `SUPABASE_KEY` environment variables)\n   * Requires **OpenAI API key** (`OPENAI_API_KEY` environment variable) for embeddings\n   * Memories are persistently stored in your Supabase database\n   * Free tier available, self-hostable option\n   * Requires initial database setup (SQL migrations provided below)\n\n3. **Local Storage Mode** 💾 (Development/testing only)\n   * Requires an **OpenAI API key** (provided as `OPENAI_API_KEY` environment variable)\n   * Memories are stored in an in-memory vector database (non-persistent by default)\n   * Data is lost when the server restarts unless configured for persistent storage\n\n## Installation & Configuration ⚙️\n\nYou can run this server in three main ways:\n\n### Installing via Smithery\n\nTo install Mem0 Memory Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/mem0-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/mem0-mcp-server --client claude\n```\n\n### 1. Global Installation (Recommended for frequent use)\n\nInstall the package globally and use the `mem0-mcp` command:\n\n```bash\nnpm install -g @pinkpixel/mem0-mcp\n```\n\nAfter global installation, you can run the server directly:\n\n```bash\nmem0-mcp\n```\n\nConfigure your MCP client to use the global command:\n\n#### Cloud Storage Configuration (Global Install)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"mem0-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"MEM0_API_KEY\": \"YOUR_MEM0_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Supabase Storage Configuration (Global Install)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"mem0-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"SUPABASE_URL\": \"YOUR_SUPABASE_PROJECT_URL\",\n        \"SUPABASE_KEY\": \"YOUR_SUPABASE_ANON_KEY\",\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Local Storage Configuration (Global Install)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"mem0-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n### 2. Using `npx` (Recommended for occasional use)\n\nConfigure your MCP client (e.g., Claude Desktop, Cursor, Cline, Roo Code, etc.) to run the server using `npx`:\n\n#### Cloud Storage Configuration (npx)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/mem0-mcp\"\n      ],\n      \"env\": {\n        \"MEM0_API_KEY\": \"YOUR_MEM0_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Supabase Storage Configuration (npx)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/mem0-mcp\"\n      ],\n      \"env\": {\n        \"SUPABASE_URL\": \"YOUR_SUPABASE_PROJECT_URL\",\n        \"SUPABASE_KEY\": \"YOUR_SUPABASE_ANON_KEY\",\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Local Storage Configuration (npx)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/mem0-mcp\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n### 3. Running from Cloned Repository\n\n**Note: This method requires you to git clone the repository first.**\n\nClone the repository, install dependencies, and build the server:\n\n```bash\ngit clone https://github.com/pinkpixel-dev/mem0-mcp\ncd mem0-mcp\nnpm install\nnpm run build\n```\n\nThen, configure your MCP client to run the built script directly using `node`:\n\n#### Cloud Storage Configuration (Cloned Repository)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mem0-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"MEM0_API_KEY\": \"YOUR_MEM0_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Supabase Storage Configuration (Cloned Repository)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mem0-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"SUPABASE_URL\": \"YOUR_SUPABASE_PROJECT_URL\",\n        \"SUPABASE_KEY\": \"YOUR_SUPABASE_ANON_KEY\",\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"DEFAULT_AGENT_ID\": \"your-agent-id\",\n        \"DEFAULT_APP_ID\": \"your-app-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n#### Local Storage Configuration (Cloned Repository)\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mem0-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"add_memory\",\n        \"search_memory\",\n        \"delete_memory\"\n      ]\n    }\n  }\n}\n```\n\n**Important Notes:**\n1. Replace `/absolute/path/to/mem0-mcp/` with the actual absolute path to your cloned repository\n2. Use the `build/index.js` file, not the `src/index.ts` file\n3. The MCP server requires clean stdout for protocol communication - any libraries or code that writes to stdout may interfere with the protocol\n\n## Supabase Setup 🗄️\n\nIf you choose to use Supabase storage mode, you'll need to set up your Supabase database with the required table.\n\n### 1. Create a Supabase Project\n\n1. Go to [supabase.com](https://supabase.com) and create a new project\n2. Note your project URL and anon key from the project settings\n\n### 2. Run SQL Migrations\n\nRun these SQL commands in your Supabase SQL Editor:\n\n```sql\n-- Enable the vector extension\ncreate extension if not exists vector;\n\n-- Create the memories table\ncreate table if not exists memories (\n  id text primary key,\n  embedding vector(1536),\n  metadata jsonb,\n  created_at timestamp with time zone default timezone('utc', now()),\n  updated_at timestamp with time zone default timezone('utc', now())\n);\n\n-- Create the vector similarity search function\ncreate or replace function match_vectors(\n  query_embedding vector(1536),\n  match_count int,\n  filter jsonb default '{}'::jsonb\n)\nreturns table (\n  id text,\n  similarity float,\n  metadata jsonb\n)\nlanguage plpgsql\nas $$\nbegin\n  return query\n  select\n    t.id::text,\n    1 - (t.embedding <=> query_embedding) as similarity,\n    t.metadata\n  from memories t\n  where case\n    when filter::text = '{}'::text then true\n    else t.metadata @> filter\n  end\n  order by t.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create the memory_history table for history tracking\ncreate table if not exists memory_history (\n  id text primary key,\n  memory_id text not null,\n  previous_value text,\n  new_value text,\n  action text not null,\n  created_at timestamp with time zone default timezone('utc', now()),\n  updated_at timestamp with time zone,\n  is_deleted integer default 0\n);\n```\n\n### 3. Set Environment Variables\n\nAdd these to your MCP configuration:\n\n- `SUPABASE_URL`: Your Supabase project URL (e.g., `https://your-project.supabase.co`)\n- `SUPABASE_KEY`: Your Supabase anon key\n- `OPENAI_API_KEY`: Your OpenAI API key (for embeddings)\n\n### Benefits of Supabase Mode\n\n✅ **Persistent Storage** - Data survives server restarts\n✅ **Free Tier Available** - Generous free tier for development\n✅ **Self-Hostable** - Can run your own Supabase instance\n✅ **Scalable** - Grows with your needs\n✅ **SQL Access** - Direct database access for advanced queries\n✅ **Real-time Features** - Built-in real-time subscriptions\n\n## Parameter Configuration 🎯\n\n### Understanding Mem0 Parameters\n\nThe server uses four key parameters to organize and scope memories:\n\n1. **`userId`** - Identifies the user (required)\n2. **`agentId`** - Identifies the LLM/agent making the tool call (optional)\n3. **`appId`** - Identifies the user's project/application - **this controls project scope!** (optional)\n4. **`sessionId`** - Identifies the conversation session (maps to `run_id` in Mem0) (optional)\n\n### Environment Variable Fallbacks 🔄\n\nThe MCP server supports environment variable fallbacks for user identification and project settings:\n\n- `DEFAULT_USER_ID`: Fallback user ID when not provided in tool calls\n- `DEFAULT_AGENT_ID`: Fallback agent ID for identifying the LLM/agent\n- `DEFAULT_APP_ID`: Fallback app ID for project scoping\n\n#### **Priority Order (Important!)**\n1. **Tool Parameters** (highest priority) - Values provided by the LLM in tool calls\n2. **Environment Variables** (fallback) - Values from your MCP configuration\n\n#### **Example Behavior:**\n```json\n// Your MCP config\n\"env\": {\n  \"DEFAULT_USER_ID\": \"john-doe\",\n  \"DEFAULT_AGENT_ID\": \"my-assistant\",\n  \"DEFAULT_APP_ID\": \"my-project\"\n}\n```\n\n**If LLM provides parameters:**\n```json\n{\n  \"tool\": \"add_memory\",\n  \"arguments\": {\n    \"content\": \"Remember this\",\n    \"userId\": \"session-123\",        // ← Overrides DEFAULT_USER_ID\n    \"agentId\": \"different-agent\",   // ← Overrides DEFAULT_AGENT_ID\n    \"appId\": \"special-project\"      // ← Overrides DEFAULT_APP_ID\n    // sessionId omitted           // ← No fallback, will be undefined\n  }\n}\n```\n**Result**: Uses `session-123`, `different-agent`, and `special-project`\n\n**If LLM omits parameters:**\n```json\n{\n  \"tool\": \"add_memory\",\n  \"arguments\": {\n    \"content\": \"Remember this\"\n    // All IDs omitted - uses environment variables\n  }\n}\n```\n**Result**: Uses `john-doe`, `my-assistant`, and `my-project`\n\n#### **Controlling LLM Behavior**\nTo ensure your environment variables are used, instruct your LLM:\n- *\"Use the default user ID configured in the environment\"*\n- *\"Don't specify userId, agentId, or appId parameters\"*\n- *\"Let the server use the configured defaults\"*\n\n#### **System Prompt Recommendation**\nFor best results, include instructions in your system prompt like:\n\n```\nWhen creating memories, use:\n- agentId: \"my-assistant\"\n- appId: \"my-project\"\n- sessionId: \"current-conversation-id\"\n```\n\nExample configuration using `DEFAULT_USER_ID`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/mem0-mcp\"\n      ],\n      \"env\": {\n        \"MEM0_API_KEY\": \"YOUR_MEM0_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\",\n        \"ORG_ID\": \"your-org-id\",\n        \"PROJECT_ID\": \"your-project-id\"\n      }\n    }\n  }\n}\n```\n\nOr when running directly with `node`:\n\n```bash\ngit clone https://github.com/pinkpixel-dev/mem0-mcp\ncd mem0-mcp\nnpm install\nnpm run build\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"path/to/mem0-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY_HERE\",\n        \"DEFAULT_USER_ID\": \"user123\"\n      }\n    }\n  }\n}\n```\n\n## Storage Mode Comparison 🔄\n\n### Cloud Storage (Mem0 API) ☁️\n* **Persistent by default** - Your memories remain available across sessions and server restarts\n* **No local database required** - All data is stored on Mem0's servers\n* **Higher retrieval quality** - Uses Mem0's optimized search algorithms\n* **Additional fields** - Supports `agent_id` and `threshold` parameters\n* **Fully managed** - No setup or maintenance required\n* **Requires** - A Mem0 API key\n\n### Supabase Storage 🗄️\n* **Persistent storage** - Data is stored in your Supabase PostgreSQL database\n* **Free tier available** - Generous free tier for development and small projects\n* **Self-hostable** - Can run your own Supabase instance for complete control\n* **SQL access** - Direct database access for advanced queries and analytics\n* **Scalable** - Grows with your needs, from free tier to enterprise\n* **Vector search** - Uses pgvector extension for efficient similarity search\n* **Real-time features** - Built-in real-time subscriptions and webhooks\n* **Requires** - Supabase project setup and OpenAI API key for embeddings\n\n### Local Storage (OpenAI API) 💾\n* **In-memory by default** - Data is stored only in RAM and is **not persistent long-term**. While some caching may occur, you should not rely on this for permanent storage.\n* **Data loss risk** - Memory data will be lost on server restart, system reboot, or if the process is terminated\n* **Recommended for** - Development, testing, or temporary use only\n* **For persistent storage** - Use the Cloud Storage or Supabase options if you need reliable long-term memory\n* **Uses OpenAI embeddings** - For vector search functionality\n* **Self-contained** - All data stays on your machine\n* **Requires** - An OpenAI API key\n\n## Development 💻\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/pinkpixel-dev/mem0-mcp\ncd mem0-mcp\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild on file changes:\n\n```bash\nnpm run watch\n```\n\n## Debugging 🐞\n\nSince MCP servers communicate over stdio, debugging can be challenging. Here are some approaches:\n\n1. **Use the MCP Inspector**: This tool can monitor the MCP protocol communication:\n```bash\nnpm run inspector\n```\n\n2. **Console Logging**: When adding console logs, always use `console.error()` instead of `console.log()` to avoid interfering with the MCP protocol\n\n3. **Environment Files**: Use a `.env` file for local development to simplify setting API keys and other configuration options\n\n## Technical Implementation Notes 🔧\n\n### Advanced Mem0 API Parameters\n\nWhen using the Cloud Storage mode with the Mem0 API, you can leverage additional parameters for more sophisticated memory management. While not explicitly exposed in the tool schema, these can be included in the `metadata` object when adding memories:\n\n#### Advanced Parameters for `add_memory`:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `metadata` | object | Store additional context about the memory (e.g., location, time, identifiers). This can be used for filtering during retrieval. |\n| `includes` | string | Specific preferences to include in the memory. |\n| `excludes` | string | Specific preferences to exclude from the memory. |\n| `infer` | boolean | Whether to infer memories or directly store messages (default: true). |\n| `output_format` | string | Format version, either v1.0 (default, deprecated) or v1.1 (recommended). |\n| `custom_categories` | object | List of categories with names and descriptions. |\n| `custom_instructions` | string | Project-specific guidelines for handling and organizing memories. |\n| `immutable` | boolean | Whether the memory is immutable (default: false). |\n| `expiration_date` | string | When the memory will expire (format: YYYY-MM-DD). |\n| `org_id` | string | Organization ID associated with this memory. |\n| `project_id` | string | Project ID associated with this memory. |\n| `version` | string | Memory version (v1 is deprecated, v2 recommended for new applications). |\n\nTo use these parameters with the MCP server, include them in your metadata object when calling the `add_memory` tool. For example:\n\n```json\n{\n  \"content\": \"Important information to remember\",\n  \"userId\": \"user123\",\n  \"sessionId\": \"project-abc\",\n  \"metadata\": {\n    \"includes\": \"important context\",\n    \"excludes\": \"sensitive data\",\n    \"immutable\": true,\n    \"expiration_date\": \"2025-12-31\",\n    \"custom_instructions\": \"Prioritize this memory for financial questions\",\n    \"version\": \"v2\"\n  }\n}\n```\n\n#### Advanced Parameters for `search_memory`:\n\nThe Mem0 v2 search API offers powerful filtering capabilities that can be utilized through the `filters` parameter:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `filters` | object | Complex filters with logical operators and comparison conditions |\n| `top_k` | integer | Number of top results to return (default: 10) |\n| `fields` | string[] | Specific fields to include in the response |\n| `rerank` | boolean | Whether to rerank the memories (default: false) |\n| `keyword_search` | boolean | Whether to search based on keywords (default: false) |\n| `filter_memories` | boolean | Whether to filter the memories (default: false) |\n| `threshold` | number | Minimum similarity threshold for results (default: 0.3) |\n| `org_id` | string | Organization ID for filtering memories |\n| `project_id` | string | Project ID for filtering memories |\n\nThe `filters` parameter supports complex logical operations (AND, OR) and various comparison operators:\n\n| Operator | Description |\n|----------|-------------|\n| `in` | Matches any of the values specified |\n| `gte` | Greater than or equal to |\n| `lte` | Less than or equal to |\n| `gt` | Greater than |\n| `lt` | Less than |\n| `ne` | Not equal to |\n| `icontains` | Case-insensitive containment check |\n\nExample of using complex filters with the `search_memory` tool:\n\n```json\n{\n  \"query\": \"What are Alice's hobbies?\",\n  \"userId\": \"user123\",\n  \"filters\": {\n    \"AND\": [\n      {\n        \"user_id\": \"alice\"\n      },\n      {\n        \"agent_id\": {\"in\": [\"travel-agent\", \"sports-agent\"]}\n      }\n    ]\n  },\n  \"threshold\": 0.5,\n  \"top_k\": 5\n}\n```\n\nThis would search for memories related to Alice's hobbies where the user_id is \"alice\" AND the agent_id is either \"travel-agent\" OR \"sports-agent\", returning at most 5 results with a similarity score of at least 0.5.\n\nFor more detailed information on these parameters, refer to the [Mem0 API documentation](https://mem0.ai).\n\n### SafeLogger\n\nThe MCP server implements a `SafeLogger` class that selectively redirects console.log calls from the mem0ai library to stderr without disrupting MCP protocol:\n\n- Intercepts console.log calls and examines stack traces to determine source\n- Only redirects log calls from mem0ai library or our own code\n- Preserves clean stdout for MCP protocol communication\n- Automatically cleans up resources on process exit\n\nThis allows proper functioning within MCP clients while maintaining useful debug information.\n\n### Environment Variables\n\nThe server recognizes several environment variables that control its behavior:\n\n- `MEM0_API_KEY`: API key for cloud storage mode\n- `OPENAI_API_KEY`: API key for local storage mode (embeddings)\n- `DEFAULT_USER_ID`: Default user ID for memory operations\n- `DEFAULT_AGENT_ID`: Default agent ID for identifying the LLM/agent\n- `DEFAULT_APP_ID`: Default app ID for project scoping\n\n**Important Notes:**\n- **Session IDs** are passed as tool parameters (e.g., `\"sessionId\": \"my-session\"`), not environment variables\n- When using the tools, parameters provided directly (e.g., `agentId`, `appId`, `sessionId`) take precedence over environment variables, giving you maximum flexibility\n- **org_id and project_id are set automatically by Mem0** and cannot be changed by users - use `appId` for project scoping instead\n\n---\n\nMade with ❤️ by Pink Pixel\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "mem0",
        "pinkpixel",
        "memory management",
        "memory ai",
        "provides memory"
      ],
      "category": "memory-management"
    },
    "rmtech1--txtai-assistant-mcp": {
      "owner": "rmtech1",
      "name": "txtai-assistant-mcp",
      "url": "https://github.com/rmtech1/txtai-assistant-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rmtech1.webp",
      "description": "Provides semantic memory and search capabilities for AI assistants, enabling the storage, retrieval, and management of text-based memories. Enhances context awareness during conversations through advanced features like tagging and health monitoring.",
      "stars": 11,
      "forks": 3,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-10-02T06:40:36Z",
      "readme_content": "# TxtAI Assistant MCP\n\nA Model Context Protocol (MCP) server implementation for semantic search and memory management using [txtai](https://github.com/neuml/txtai). This server provides a robust API for storing, retrieving, and managing text-based memories with semantic search capabilities.\n\n## About txtai\n\nThis project is built on top of [txtai](https://github.com/neuml/txtai), an excellent open-source AI-powered search engine created by [NeuML](https://github.com/neuml). txtai provides:\n\n- 🔍 All-in-one semantic search solution\n- 🧠 Neural search with transformers\n- 💡 Zero-shot text classification\n- 🔄 Text extraction and embeddings\n- 🌐 Multi-language support\n- 🚀 High performance and scalability\n\nWe extend txtai's capabilities by integrating it with the Model Context Protocol (MCP), enabling AI assistants like Claude and Cline to leverage its powerful semantic search capabilities. Special thanks to the txtai team for creating such a powerful and flexible tool.\n\n## Features\n\n- 🔍 Semantic search across stored memories\n- 💾 Persistent storage with file-based backend\n- 🏷️ Tag-based memory organization and retrieval\n- 📊 Memory statistics and health monitoring\n- 🔄 Automatic data persistence\n- 📝 Comprehensive logging\n- 🔒 Configurable CORS settings\n- 🤖 Integration with Claude and Cline AI\n\n## Prerequisites\n\n- Python 3.8 or higher\n- pip (Python package installer)\n- virtualenv (recommended)\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/yourusername/txtai-assistant-mcp.git\ncd txtai-assistant-mcp\n```\n\n2. Run the start script:\n```bash\n./scripts/start.sh\n```\n\nThe script will:\n- Create a virtual environment\n- Install required dependencies\n- Set up necessary directories\n- Create a configuration file from template\n- Start the server\n\n## Configuration\n\nThe server can be configured using environment variables in the `.env` file. A template is provided at `.env.template`:\n\n```ini\n# Server Configuration\nHOST=0.0.0.0\nPORT=8000\n\n# CORS Configuration\nCORS_ORIGINS=*\n\n# Logging Configuration\nLOG_LEVEL=DEBUG\n\n# Memory Configuration\nMAX_MEMORIES=0\n```\n\n## Integration with Claude and Cline AI\n\nThis TxtAI Assistant can be used as an MCP server with Claude and Cline AI to enhance their capabilities with semantic memory and search functionality.\n\n### Configuration for Claude\n\nTo use this server with Claude, add it to Claude's MCP configuration file (typically located at `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):\n\n```json\n{\n  \"mcpServers\": {\n    \"txtai-assistant\": {\n      \"command\": \"path/to/txtai-assistant-mcp/scripts/start.sh\",\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Configuration for Cline\n\nTo use with Cline, add the server configuration to Cline's MCP settings file (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"txtai-assistant\": {\n      \"command\": \"path/to/txtai-assistant-mcp/scripts/start.sh\",\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Available MCP Tools\n\nOnce configured, the following tools become available to Claude and Cline:\n\n1. `store_memory`: Store new memory content with metadata and tags\n```json\n{\n  \"content\": \"Memory content to store\",\n  \"metadata\": {\n    \"source\": \"conversation\",\n    \"timestamp\": \"2023-01-01T00:00:00Z\"\n  },\n  \"tags\": [\"important\", \"context\"],\n  \"type\": \"conversation\"\n}\n```\n\n2. `retrieve_memory`: Retrieve memories based on semantic search\n```json\n{\n  \"query\": \"search query\",\n  \"n_results\": 5\n}\n```\n\n3. `search_by_tag`: Search memories by tags\n```json\n{\n  \"tags\": [\"important\", \"context\"]\n}\n```\n\n4. `delete_memory`: Delete a specific memory by content hash\n```json\n{\n  \"content_hash\": \"hash_value\"\n}\n```\n\n5. `get_stats`: Get database statistics\n```json\n{}\n```\n\n6. `check_health`: Check database and embedding model health\n```json\n{}\n```\n\n### Usage Examples\n\nIn Claude or Cline, you can use these tools through the MCP protocol:\n\n```python\n# Store a memory\n<use_mcp_tool>\n<server_name>txtai-assistant</server_name>\n<tool_name>store_memory</tool_name>\n<arguments>\n{\n  \"content\": \"Important information to remember\",\n  \"tags\": [\"important\"]\n}\n</arguments>\n</use_mcp_tool>\n\n# Retrieve memories\n<use_mcp_tool>\n<server_name>txtai-assistant</server_name>\n<tool_name>retrieve_memory</tool_name>\n<arguments>\n{\n  \"query\": \"what was the important information?\",\n  \"n_results\": 5\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe AI will automatically use these tools to maintain context and retrieve relevant information during conversations.\n\n## API Endpoints\n\n### Store Memory\n```http\nPOST /store\n```\nStore a new memory with optional metadata and tags.\n\n**Request Body:**\n```json\n{\n    \"content\": \"Memory content to store\",\n    \"metadata\": {\n        \"source\": \"example\",\n        \"timestamp\": \"2023-01-01T00:00:00Z\"\n    },\n    \"tags\": [\"example\", \"memory\"],\n    \"type\": \"general\"\n}\n```\n\n### Search Memories\n```http\nPOST /search\n```\nSearch memories using semantic search.\n\n**Request Body:**\n```json\n{\n    \"query\": \"search query\",\n    \"n_results\": 5,\n    \"similarity_threshold\": 0.7\n}\n```\n\n### Search by Tags\n```http\nPOST /search_tags\n```\nSearch memories by tags.\n\n**Request Body:**\n```json\n{\n    \"tags\": [\"example\", \"memory\"]\n}\n```\n\n### Delete Memory\n```http\nDELETE /memory/{content_hash}\n```\nDelete a specific memory by its content hash.\n\n### Get Statistics\n```http\nGET /stats\n```\nGet system statistics including memory counts and tag distribution.\n\n### Health Check\n```http\nGET /health\n```\nCheck the health status of the server.\n\n## Directory Structure\n\n```\ntxtai-assistant-mcp/\n├── server/\n│   ├── main.py           # Main server implementation\n│   └── requirements.txt  # Python dependencies\n├── scripts/\n│   └── start.sh         # Server startup script\n├── data/                # Data storage directory\n├── logs/                # Log files directory\n├── .env.template        # Environment configuration template\n└── README.md           # This file\n```\n\n## Data Storage\n\nMemories and tags are stored in JSON files in the `data` directory:\n- `memories.json`: Contains all stored memories\n- `tags.json`: Contains the tag index\n\n## Logging\n\nLogs are stored in the `logs` directory. The default log file is `server.log`.\n\n## Development\n\nTo contribute to this project:\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\n## Error Handling\n\nThe server implements comprehensive error handling:\n- Invalid requests return appropriate HTTP status codes\n- Errors are logged with stack traces\n- User-friendly error messages are returned in responses\n\n## Security Considerations\n\n- CORS settings are configurable via environment variables\n- File paths are sanitized to prevent directory traversal\n- Input validation is performed on all endpoints\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nIf you encounter any issues or have questions, please file an issue on the GitHub repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "txtai",
        "memory",
        "rmtech1",
        "txtai assistant",
        "semantic memory",
        "rmtech1 txtai"
      ],
      "category": "memory-management"
    },
    "shaneholloman--mcp-knowledge-graph": {
      "owner": "shaneholloman",
      "name": "mcp-knowledge-graph",
      "url": "https://github.com/shaneholloman/mcp-knowledge-graph",
      "imageUrl": "/freedevtools/mcp/pfp/shaneholloman.webp",
      "description": "Enables persistent memory for AI models using a local knowledge graph, allowing them to remember user information across chats. Customizable memory paths enhance the management of stored knowledge.",
      "stars": 674,
      "forks": 87,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T01:16:29Z",
      "readme_content": "# MCP Knowledge Graph\n\n**Persistent memory for AI models through a local knowledge graph.**\n\nStore and retrieve information across conversations using entities, relations, and observations. Works with Claude Code/Desktop and any MCP-compatible AI platform.\n\n## Why \".aim\" and \"aim_\" prefixes?\n\nAIM stands for **AI Memory** - the core concept of this knowledge graph system. The three AIM elements provide clear organization and safety:\n\n- **`.aim` directories**: Keep AI memory files organized and easily identifiable\n- **`aim_` tool prefixes**: Group related memory functions together in multi-tool setups\n- **`_aim` safety markers**: Each memory file starts with `{\"type\":\"_aim\",\"source\":\"mcp-knowledge-graph\"}` to prevent accidental overwrites of unrelated JSONL files\n\nThis consistent AIM naming makes it obvious which directories, tools, and files belong to our AI memory system.\n\n## Storage Logic\n\n**File Location Priority:**\n\n1. **Project with `.aim`** - Uses `.aim/memory.jsonl` (project-local)\n2. **No project/no .aim** - Uses configured global directory\n3. **Contexts** - Adds suffix: `memory-work.jsonl`, `memory-personal.jsonl`\n\n**Safety System:**\n\n- Every memory file starts with `{\"type\":\"_aim\",\"source\":\"mcp-knowledge-graph\"}`\n- System refuses to write to files without this marker\n- Prevents accidental overwrite of unrelated JSONL files\n\n## Master Database Concept\n\n**The master database is your primary memory store** - used by default when no specific database is requested. It's always named `default` in listings and stored as `memory.jsonl`.\n\n- **Default Behavior**: All memory operations use the master database unless you specify a different one\n- **Always Available**: Exists in both project-local and global locations\n- **Primary Storage**: Your main knowledge graph that persists across all conversations\n- **Named Databases**: Optional additional databases (`work`, `personal`, `health`) for organizing specific topics\n\n## Key Features\n\n- **Master Database**: Primary memory store used by default for all operations\n- **Multiple Databases**: Optional named databases for organizing memories by topic\n- **Project Detection**: Automatic project-local memory using `.aim` directories\n- **Location Override**: Force operations to use project or global storage\n- **Safe Operations**: Built-in protection against overwriting unrelated files\n- **Database Discovery**: List all available databases in both locations\n\n## Quick Start\n\n### Global Memory (Recommended)\n\nAdd to your `claude_desktop_config.json` or `.claude.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-knowledge-graph\",\n        \"--memory-path\",\n        \"/Users/yourusername/.aim/\"\n      ]\n    }\n  }\n}\n```\n\nThis creates memory files in your specified directory:\n\n- `memory.jsonl` - **Master Database** (default for all operations)\n- `memory-work.jsonl` - Work database\n- `memory-personal.jsonl` - Personal database\n- etc.\n\n### Project-Local Memory\n\nIn any project, create a `.aim` directory:\n\n```bash\nmkdir .aim\n```\n\nNow memory tools automatically use `.aim/memory.jsonl` (project-local **master database**) instead of global storage when run from this project.\n\n## How AI Uses Databases\n\nOnce configured, AI models use the **master database by default** or can specify named databases with a `context` parameter. New databases are created automatically - no setup required:\n\n```json\n// Master Database (default - no context needed)\naim_create_entities({\n  entities: [{\n    name: \"John_Doe\",\n    entityType: \"person\",\n    observations: [\"Met at conference\"]\n  }]\n})\n\n// Work database\naim_create_entities({\n  context: \"work\",\n  entities: [{\n    name: \"Q4_Project\",\n    entityType: \"project\",\n    observations: [\"Due December 2024\"]\n  }]\n})\n\n// Personal database\naim_create_entities({\n  context: \"personal\",\n  entities: [{\n    name: \"Mom\",\n    entityType: \"person\",\n    observations: [\"Birthday March 15th\"]\n  }]\n})\n\n// Master database in specific location\naim_create_entities({\n  location: \"global\",\n  entities: [{\n    name: \"Important_Info\",\n    entityType: \"reference\",\n    observations: [\"Stored in global master database\"]\n  }]\n})\n```\n\n## File Organization\n\n**Global Setup:**\n\n```tree\n/Users/yourusername/.aim/\n├── memory.jsonl           # Master Database (default)\n├── memory-work.jsonl      # Work database\n├── memory-personal.jsonl  # Personal database\n└── memory-health.jsonl    # Health database\n```\n\n**Project Setup:**\n\n```tree\nmy-project/\n├── .aim/\n│   ├── memory.jsonl       # Project Master Database (default)\n│   └── memory-work.jsonl  # Project Work database\n└── src/\n```\n\n## Available Tools\n\n- `aim_create_entities` - Add new people, projects, events\n- `aim_create_relations` - Link entities together\n- `aim_add_observations` - Add facts to existing entities\n- `aim_search_nodes` - Find information by keyword\n- `aim_read_graph` - View entire memory\n- `aim_open_nodes` - Retrieve specific entities by name\n- `aim_list_databases` - Show all available databases and current location\n- `aim_delete_entities` - Remove entities\n- `aim_delete_observations` - Remove specific facts\n- `aim_delete_relations` - Remove connections\n\n### Parameters\n\n- `context` (optional) - Specify named database (`work`, `personal`, etc.). Defaults to **master database**\n- `location` (optional) - Force `project` or `global` storage location. Defaults to auto-detection\n\n## Database Discovery\n\nUse `aim_list_databases` to see all available databases:\n\n```json\n{\n  \"project_databases\": [\n    \"default\",      // Master Database (project-local)\n    \"project-work\"  // Named database\n  ],\n  \"global_databases\": [\n    \"default\",      // Master Database (global)\n    \"work\",\n    \"personal\",\n    \"health\"\n  ],\n  \"current_location\": \"project (.aim directory detected)\"\n}\n```\n\n**Key Points:**\n\n- **\"default\"** = Master Database in both locations\n- **Current location** shows whether you're using project or global storage\n- **Master database exists everywhere** - it's your primary memory store\n- **Named databases** are optional additions for specific topics\n\n## Configuration Examples\n\n**Important:** Always specify `--memory-path` to control where your memory files are stored.\n\n**Home directory:**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-knowledge-graph\",\n        \"--memory-path\",\n        \"/Users/yourusername/.aim\"\n      ]\n    }\n  }\n}\n```\n\n**Custom location (e.g., Dropbox):**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-knowledge-graph\",\n        \"--memory-path\",\n        \"/Users/yourusername/Dropbox/.aim\"\n      ]\n    }\n  }\n}\n```\n\n**Auto-approve all operations:**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-knowledge-graph\",\n        \"--memory-path\",\n        \"/Users/yourusername/.aim\"\n      ],\n      \"autoapprove\": [\n        \"aim_create_entities\",\n        \"aim_create_relations\",\n        \"aim_add_observations\",\n        \"aim_search_nodes\",\n        \"aim_read_graph\",\n        \"aim_open_nodes\",\n        \"aim_list_databases\"\n      ]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n**\"File does not contain required _aim safety marker\" error:**\n\n- The file may not belong to this system\n- Manual JSONL files need `{\"type\":\"_aim\",\"source\":\"mcp-knowledge-graph\"}` as first line\n- If you created the file manually, add the `_aim` marker or delete and let the system recreate it\n\n**Memories going to unexpected locations:**\n\n- Check if you're in a project directory with `.aim` folder (uses project-local storage)\n- Otherwise uses the configured global `--memory-path` directory\n- Use `aim_list_databases` to see all available databases and current location\n- Use `ls .aim/` or `ls /Users/yourusername/.aim/` to see your memory files\n\n**Too many similar databases:**\n\n- AI models try to use consistent names, but may create variations\n- Manually delete unwanted database files if needed\n- Encourage AI to use simple, consistent database names\n- **Remember**: Master database is always available as the default - named databases are optional\n\n## Requirements\n\n- Node.js 18+\n- MCP-compatible AI platform\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-knowledge-graph",
      "npm_downloads": 9675,
      "keywords": [
        "memory",
        "knowledge",
        "persistent",
        "stored knowledge",
        "memory ai",
        "persistent memory"
      ],
      "category": "memory-management"
    },
    "sizzlebop--context-mcp": {
      "owner": "sizzlebop",
      "name": "context-mcp",
      "url": "https://github.com/sizzlebop/context-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "A memory system that maintains and retrieves comprehensive project and conversation memory, enabling contextually aware AI assistance. It supports multiple memory types including short-term, long-term, episodic, and semantic memory for enhanced development workflows.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/context-mcp",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "sizzlebop",
        "context",
        "conversation memory",
        "semantic memory",
        "context mcp"
      ],
      "category": "memory-management"
    },
    "t3ta--memory-bank-mcp-server": {
      "owner": "t3ta",
      "name": "memory-bank-mcp-server",
      "url": "https://github.com/t3ta/memory-bank-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/t3ta.webp",
      "description": "Manage project documentation and context effectively across sessions with structured memory banks in JSON format. Facilitate consistent knowledge retention for AI agents and support multilingual documentation through a powerful API for document management and retrieval.",
      "stars": 11,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-29T07:39:48Z",
      "readme_content": "# Memory Bank MCP Server\n\nA Memory Bank implementation for managing project documentation and context\nacross sessions using the Model Context Protocol (MCP). This server helps AI agents\nlike Claude maintain consistent project knowledge through global and branch-specific\nmemory banks stored in a structured JSON format.\n\nThis project is inspired by\n[Cline Memory Bank](https://github.com/nickbaumann98/cline_docs/blob/main/prompting/custom%20instructions%20library/cline-memory-bank.md)\nfrom the [nickbaumann98/cline_docs](https://github.com/nickbaumann98/cline_docs)\nrepository.\n\n## Packages\n\nThis repository is a monorepo managed with Yarn Workspaces. It contains the following packages:\n\n- **[`packages/mcp`](./packages/mcp/README.md)**: The core MCP server implementation. Contains the main logic for handling memory bank operations, MCP tool execution, and server startup.\n- **[`packages/schemas`](./packages/schemas/README.md)**: Defines the JSON schemas used for memory bank documents (e.g., `memory_document_v2`).\n- **[`packages/vscode-extension`](./packages/vscode-extension/README.md)**: A VSCode extension providing integration with the Memory Bank MCP server (details TBD).\n\n## Getting Started\n\n### Prerequisites\n\n- Node.js (see `.tool-versions` for recommended version)\n- Yarn (v1.x)\n\n### Installation\n\nClone the repository and install dependencies from the root directory:\n\n```bash\ngit clone https://github.com/t3ta/memory-bank-mcp-server.git\ncd memory-bank-mcp-server\nyarn install\n```\n\n### Running the MCP Server\n\nYou can run the MCP server directly from the monorepo:\n\n```bash\n# From the monorepo root directory\nyarn workspace @memory-bank/mcp start --docs /path/to/your/docs\n```\n\nReplace `/path/to/your/docs` with the actual path to your project's documentation directory (where `global-memory-bank` and `branch-memory-bank` will reside or be created).\n\nSee the [`packages/mcp/README.md`](./packages/mcp/README.md) for more details on running the server and its options.\n\n## Development\n\n- **Build all packages:** `yarn build`\n- **Run tests for all packages:** `yarn test`\n- **Lint code:** `yarn lint`\n\nRefer to the README file within each package directory for package-specific development instructions.\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/memory-bank-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "t3ta",
        "document",
        "management t3ta",
        "document management",
        "t3ta memory"
      ],
      "category": "memory-management"
    },
    "tjwells47--chroma-mcp": {
      "owner": "tjwells47",
      "name": "chroma-mcp",
      "url": "https://github.com/tjwells47/chroma-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tjwells47.webp",
      "description": "A vector database designed for integrating LLM applications, enabling seamless data retrieval and management through advanced search capabilities and document operations. Supports memory and context enhancement for AI models with efficient embedding functions.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-09T04:30:49Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://trychroma.com\"><img src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"></a>\n</p>\n\n<p align=\"center\">\n    <b>Chroma - the open-source embedding database</b>. <br />\n    The fastest way to build Python or JavaScript LLM apps with memory!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\">\n      <img src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\">\n  </a> |\n  <a href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\">\n      <img src=\"https://img.shields.io/static/v1?label=license&message=Apache 2.0&color=white\" alt=\"License\">\n  </a> |\n  <a href=\"https://docs.trychroma.com/\" target=\"_blank\">\n      Docs\n  </a> |\n  <a href=\"https://www.trychroma.com/\" target=\"_blank\">\n      Homepage\n  </a>\n</p>\n\n# Chroma MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chroma-core/chroma-mcp)](https://smithery.ai/server/@chroma-core/chroma-mcp)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol designed for effortless integration between LLM applications and external data sources or tools, offering a standardized framework to seamlessly provide LLMs with the context they require.\n\nThis server provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.\n\n## Features\n\n- **Flexible Client Types**\n  - Ephemeral (in-memory) for testing and development\n  - Persistent for file-based storage\n  - HTTP client for self-hosted Chroma instances\n  - Cloud client for Chroma Cloud integration (automatically connects to api.trychroma.com)\n\n- **Collection Management**\n  - Create, modify, and delete collections\n  - List all collections with pagination support\n  - Get collection information and statistics\n  - Configure HNSW parameters for optimized vector search\n  - Select embedding functions when creating collections\n\n- **Document Operations**\n  - Add documents with optional metadata and custom IDs\n  - Query documents using semantic search\n  - Advanced filtering using metadata and document content\n  - Retrieve documents by IDs or filters\n  - Full text search capabilities\n\n### Supported Tools\n\n- `chroma_list_collections` - List all collections with pagination support\n- `chroma_create_collection` - Create a new collection with optional HNSW configuration\n- `chroma_peek_collection` - View a sample of documents in a collection\n- `chroma_get_collection_info` - Get detailed information about a collection\n- `chroma_get_collection_count` - Get the number of documents in a collection\n- `chroma_modify_collection` - Update a collection's name or metadata\n- `chroma_delete_collection` - Delete a collection\n- `chroma_add_documents` - Add documents with optional metadata and custom IDs\n- `chroma_query_documents` - Query documents using semantic search with advanced filtering\n- `chroma_get_documents` - Retrieve documents by IDs or filters with pagination\n- `chroma_update_documents` - Update existing documents' content, metadata, or embeddings\n- `chroma_delete_documents` - Delete specific documents from a collection\n\n### Embedding Functions\nChroma MCP supports several embedding functions: `default`, `cohere`, `openai`, `jina`, `voyageai`, and `roboflow`.\n\nThe embedding functions utilize Chroma's collection configuration, which persists the selected embedding function of a collection for retrieval. Once a collection is created using the collection configuration, on retrieval for future queries and inserts, the same embedding function will be used, without needing to specify the embedding function again. Embedding function persistance was added in v1.0.0 of Chroma, so if you created a collection using version <=0.6.3, this feature is not supported.\n\nWhen accessing embedding functions that utilize external APIs, please be sure to add the environment variable for the API key with the correct format, found in [Embedding Function Environment Variables](#embedding-function-environment-variables)\n\n## Usage with Claude Desktop\n\n1. To add an ephemeral client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\"\n    ]\n}\n```\n\n2. To add a persistent client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"persistent\",\n        \"--data-dir\",\n        \"/full/path/to/your/data/directory\"\n    ]\n}\n```\n\nThis will create a persistent client that will use the data directory specified.\n\n3. To connect to Chroma Cloud, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"cloud\",\n        \"--tenant\",\n        \"your-tenant-id\",\n        \"--database\",\n        \"your-database-name\",\n        \"--api-key\",\n        \"your-api-key\"\n    ]\n}\n```\n\nThis will create a cloud client that automatically connects to api.trychroma.com using SSL.\n\n**Note:** Adding API keys in arguments is fine on local devices, but for safety, you can also specify a custom path for your environment configuration file using the `--dotenv-path` argument within the `args` list, for example: `\"args\": [\"chroma-mcp\", \"--dotenv-path\", \"/custom/path/.env\"]`.\n\n4. To connect to a [self-hosted Chroma instance on your own cloud provider](https://docs.trychroma.com/\nproduction/deployment), add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"chroma-mcp\", \n      \"--client-type\", \n      \"http\", \n      \"--host\", \n      \"your-host\", \n      \"--port\", \n      \"your-port\", \n      \"--custom-auth-credentials\",\n      \"your-custom-auth-credentials\",\n      \"--ssl\",\n      \"true\"\n    ]\n}\n```\n\nThis will create an HTTP client that connects to your self-hosted Chroma instance.\n\n### Demos\n\nFind reference usages, such as shared knowledge bases & adding memory to context windows in the [Chroma MCP Docs](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp#using-chroma-with-claude)\n\n### Using Environment Variables\n\nYou can also use environment variables to configure the client. The server will automatically load variables from a `.env` file located at the path specified by `--dotenv-path` (defaults to `.chroma_env` in the working directory) or from system environment variables. Command-line arguments take precedence over environment variables.\n\n```bash\n# Common variables\nexport CHROMA_CLIENT_TYPE=\"http\"  # or \"cloud\", \"persistent\", \"ephemeral\"\n\n# For persistent client\nexport CHROMA_DATA_DIR=\"/full/path/to/your/data/directory\"\n\n# For cloud client (Chroma Cloud)\nexport CHROMA_TENANT=\"your-tenant-id\"\nexport CHROMA_DATABASE=\"your-database-name\"\nexport CHROMA_API_KEY=\"your-api-key\"\n\n# For HTTP client (self-hosted)\nexport CHROMA_HOST=\"your-host\"\nexport CHROMA_PORT=\"your-port\"\nexport CHROMA_CUSTOM_AUTH_CREDENTIALS=\"your-custom-auth-credentials\"\nexport CHROMA_SSL=\"true\"\n\n# Optional: Specify path to .env file (defaults to .chroma_env)\nexport CHROMA_DOTENV_PATH=\"/path/to/your/.env\" \n```\n\n#### Embedding Function Environment Variables\nWhen using external embedding functions that access an API key, follow the naming convention\n`CHROMA_<>_API_KEY=\"<key>\"`.\nSo to set a Cohere API key, set the environment variable `CHROMA_COHERE_API_KEY=\"\"`. We recommend adding this to a .env file somewhere and using the `CHROMA_DOTENV_PATH` environment variable or `--dotenv-path` flag to set that location for safekeeping.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "retrieval",
        "database",
        "vector database",
        "memory management",
        "retrieval management"
      ],
      "category": "memory-management"
    },
    "tjwells47--mcp-qdrant-memory": {
      "owner": "tjwells47",
      "name": "mcp-qdrant-memory",
      "url": "https://github.com/tjwells47/mcp-qdrant-memory",
      "imageUrl": "/freedevtools/mcp/pfp/tjwells47.webp",
      "description": "Leverage a knowledge graph with entities and relations, enabling semantic search capabilities using OpenAI embeddings and Qdrant for data persistence. Supports HTTPS and Docker for streamlined deployment.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-08T08:23:45Z",
      "readme_content": "# MCP Memory Server with Qdrant Persistence\n[![smithery badge](https://smithery.ai/badge/@delorenj/mcp-qdrant-memory)](https://smithery.ai/server/@delorenj/mcp-qdrant-memory)\n\nThis MCP server provides a knowledge graph implementation with semantic search capabilities powered by Qdrant vector database.\n\n## Features\n\n- Graph-based knowledge representation with entities and relations\n- File-based persistence (memory.json)\n- Semantic search using Qdrant vector database\n- OpenAI embeddings for semantic similarity\n- HTTPS support with reverse proxy compatibility\n- Docker support for easy deployment\n\n## Environment Variables\n\nThe following environment variables are required:\n\n```bash\n# OpenAI API key for generating embeddings\nOPENAI_API_KEY=your-openai-api-key\n\n# Qdrant server URL (supports both HTTP and HTTPS)\nQDRANT_URL=https://your-qdrant-server\n\n# Qdrant API key (if authentication is enabled)\nQDRANT_API_KEY=your-qdrant-api-key\n\n# Name of the Qdrant collection to use\nQDRANT_COLLECTION_NAME=your-collection-name\n```\n\n## Setup\n\n### Local Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n### Docker Setup\n\n1. Build the Docker image:\n```bash\ndocker build -t mcp-qdrant-memory .\n```\n\n2. Run the Docker container with required environment variables:\n```bash\ndocker run -d \\\n  -e OPENAI_API_KEY=your-openai-api-key \\\n  -e QDRANT_URL=http://your-qdrant-server:6333 \\\n  -e QDRANT_COLLECTION_NAME=your-collection-name \\\n  -e QDRANT_API_KEY=your-qdrant-api-key \\\n  --name mcp-qdrant-memory \\\n  mcp-qdrant-memory\n```\n\n### Add to MCP settings:\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"/bin/zsh\",\n      \"args\": [\"-c\", \"cd /path/to/server && node dist/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\",\n        \"QDRANT_API_KEY\": \"your-qdrant-api-key\",\n        \"QDRANT_URL\": \"http://your-qdrant-server:6333\",\n        \"QDRANT_COLLECTION_NAME\": \"your-collection-name\"\n      },\n      \"alwaysAllow\": [\n        \"create_entities\",\n        \"create_relations\",\n        \"add_observations\",\n        \"delete_entities\",\n        \"delete_observations\",\n        \"delete_relations\",\n        \"read_graph\",\n        \"search_similar\"\n      ]\n    }\n  }\n}\n```\n\n## Tools\n\n### Entity Management\n- `create_entities`: Create multiple new entities\n- `create_relations`: Create relations between entities\n- `add_observations`: Add observations to entities\n- `delete_entities`: Delete entities and their relations\n- `delete_observations`: Delete specific observations\n- `delete_relations`: Delete specific relations\n- `read_graph`: Get the full knowledge graph\n\n### Semantic Search\n- `search_similar`: Search for semantically similar entities and relations\n  ```typescript\n  interface SearchParams {\n    query: string;     // Search query text\n    limit?: number;    // Max results (default: 10)\n  }\n  ```\n\n## Implementation Details\n\nThe server maintains two forms of persistence:\n\n1. File-based (memory.json):\n   - Complete knowledge graph structure\n   - Fast access to full graph\n   - Used for graph operations\n\n2. Qdrant Vector DB:\n   - Semantic embeddings of entities and relations\n   - Enables similarity search\n   - Automatically synchronized with file storage\n\n### Synchronization\n\nWhen entities or relations are modified:\n1. Changes are written to memory.json\n2. Embeddings are generated using OpenAI\n3. Vectors are stored in Qdrant\n4. Both storage systems remain consistent\n\n### Search Process\n\nWhen searching:\n1. Query text is converted to embedding\n2. Qdrant performs similarity search\n3. Results include both entities and relations\n4. Results are ranked by semantic similarity\n\n## Example Usage\n\n```typescript\n// Create entities\nawait client.callTool(\"create_entities\", {\n  entities: [{\n    name: \"Project\",\n    entityType: \"Task\",\n    observations: [\"A new development project\"]\n  }]\n});\n\n// Search similar concepts\nconst results = await client.callTool(\"search_similar\", {\n  query: \"development tasks\",\n  limit: 5\n});\n```\n\n## HTTPS and Reverse Proxy Configuration\n\nThe server supports connecting to Qdrant through HTTPS and reverse proxies. This is particularly useful when:\n- Running Qdrant behind a reverse proxy like Nginx or Apache\n- Using self-signed certificates\n- Requiring custom SSL/TLS configurations\n\n### Setting up with a Reverse Proxy\n\n1. Configure your reverse proxy (example using Nginx):\n```nginx\nserver {\n    listen 443 ssl;\n    server_name qdrant.yourdomain.com;\n\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n\n    location / {\n        proxy_pass http://localhost:6333;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n```\n\n2. Update your environment variables:\n```bash\nQDRANT_URL=https://qdrant.yourdomain.com\n```\n\n### Security Considerations\n\nThe server implements robust HTTPS handling with:\n- Custom SSL/TLS configuration\n- Proper certificate verification options\n- Connection pooling and keepalive\n- Automatic retry with exponential backoff\n- Configurable timeouts\n\n### Troubleshooting HTTPS Connections\n\nIf you experience connection issues:\n\n1. Verify your certificates:\n```bash\nopenssl s_client -connect qdrant.yourdomain.com:443\n```\n\n2. Test direct connectivity:\n```bash\ncurl -v https://qdrant.yourdomain.com/collections\n```\n\n3. Check for any proxy settings:\n```bash\nenv | grep -i proxy\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp-qdrant-memory",
      "npm_downloads": 280,
      "keywords": [
        "openai",
        "memory",
        "entities",
        "openai embeddings",
        "qdrant memory",
        "using openai"
      ],
      "category": "memory-management"
    },
    "tomschell--mcp-long-term-memory": {
      "owner": "tomschell",
      "name": "mcp-long-term-memory",
      "url": "https://github.com/tomschell/mcp-long-term-memory",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Long-term memory storage for LLMs that maintains project context across sessions, enabling efficient retrieval and recall of past interactions and decisions via semantic search. Organizes memories by type, tags, and relationships for streamlined management.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "llms",
        "memories",
        "memory management",
        "organizes memories",
        "storage llms"
      ],
      "category": "memory-management"
    },
    "tosin2013--mcp-memory-cache-server": {
      "owner": "tosin2013",
      "name": "mcp-memory-cache-server",
      "url": "https://github.com/tosin2013/mcp-memory-cache-server",
      "imageUrl": "/freedevtools/mcp/pfp/tosin2013.webp",
      "description": "Caches data between interactions with AI language models to reduce token consumption and enhance performance by automatically storing and retrieving frequently accessed data.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-01T20:05:22Z",
      "readme_content": "# Memory Cache Server\n[![smithery badge](https://smithery.ai/badge/@tosin2013/mcp-memory-cache-server)](https://smithery.ai/server/@tosin2013/mcp-memory-cache-server)\n\nA Model Context Protocol (MCP) server that reduces token consumption by efficiently caching data between language model interactions. Works with any MCP client and any language model that uses tokens.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Memory Cache Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@tosin2013/mcp-memory-cache-server):\n\n```bash\nnpx -y @smithery/cli install @tosin2013/mcp-memory-cache-server --client claude\n```\n\n### Installing Manually\n1. Clone the repository:\n```bash\ngit clone https://github.com/tosin2013/mcp-memory-cache-server.git\ncd mcp-memory-cache-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Add to your MCP client settings:\n```json\n{\n  \"mcpServers\": {\n    \"memory-cache\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/ib-mcp-cache-server/build/index.js\"]\n    }\n  }\n}\n```\n\n5. The server will automatically start when you use your MCP client\n\n## Verifying It Works\n\nWhen the server is running properly, you'll see:\n1. A message in the terminal: \"Memory Cache MCP server running on stdio\"\n2. Improved performance when accessing the same data multiple times\n3. No action required from you - the caching happens automatically\n\nYou can verify the server is running by:\n1. Opening your MCP client\n2. Looking for any error messages in the terminal where you started the server\n3. Performing operations that would benefit from caching (like reading the same file multiple times)\n\n## Configuration\n\nThe server can be configured through `config.json` or environment variables:\n\n```json\n{\n  \"maxEntries\": 1000,        // Maximum number of items in cache\n  \"maxMemory\": 104857600,    // Maximum memory usage in bytes (100MB)\n  \"defaultTTL\": 3600,        // Default time-to-live in seconds (1 hour)\n  \"checkInterval\": 60000,    // Cleanup interval in milliseconds (1 minute)\n  \"statsInterval\": 30000     // Stats update interval in milliseconds (30 seconds)\n}\n```\n\n### Configuration Settings Explained\n\n1. **maxEntries** (default: 1000)\n   - Maximum number of items that can be stored in cache\n   - Prevents cache from growing indefinitely\n   - When exceeded, oldest unused items are removed first\n\n2. **maxMemory** (default: 100MB)\n   - Maximum memory usage in bytes\n   - Prevents excessive memory consumption\n   - When exceeded, least recently used items are removed\n\n3. **defaultTTL** (default: 1 hour)\n   - How long items stay in cache by default\n   - Items are automatically removed after this time\n   - Prevents stale data from consuming memory\n\n4. **checkInterval** (default: 1 minute)\n   - How often the server checks for expired items\n   - Lower values keep memory usage more accurate\n   - Higher values reduce CPU usage\n\n5. **statsInterval** (default: 30 seconds)\n   - How often cache statistics are updated\n   - Affects accuracy of hit/miss rates\n   - Helps monitor cache effectiveness\n\n## How It Reduces Token Consumption\n\nThe memory cache server reduces token consumption by automatically storing data that would otherwise need to be re-sent between you and the language model. You don't need to do anything special - the caching happens automatically when you interact with any language model through your MCP client.\n\nHere are some examples of what gets cached:\n\n### 1. File Content Caching\nWhen reading a file multiple times:\n- First time: Full file content is read and cached\n- Subsequent times: Content is retrieved from cache instead of re-reading the file\n- Result: Fewer tokens used for repeated file operations\n\n### 2. Computation Results\nWhen performing calculations or analysis:\n- First time: Full computation is performed and results are cached\n- Subsequent times: Results are retrieved from cache if the input is the same\n- Result: Fewer tokens used for repeated computations\n\n### 3. Frequently Accessed Data\nWhen the same data is needed multiple times:\n- First time: Data is processed and cached\n- Subsequent times: Data is retrieved from cache until TTL expires\n- Result: Fewer tokens used for accessing the same information\n\n## Automatic Cache Management\n\nThe server automatically manages the caching process by:\n- Storing data when first encountered\n- Serving cached data when available\n- Removing old/unused data based on settings\n- Tracking effectiveness through statistics\n\n## Optimization Tips\n\n### 1. Set Appropriate TTLs\n- Shorter for frequently changing data\n- Longer for static content\n\n### 2. Adjust Memory Limits\n- Higher for more caching (more token savings)\n- Lower if memory usage is a concern\n\n### 3. Monitor Cache Stats\n- High hit rate = good token savings\n- Low hit rate = adjust TTL or limits\n\n## Environment Variable Configuration\n\nYou can override config.json settings using environment variables in your MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-cache\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/build/index.js\"],\n      \"env\": {\n        \"MAX_ENTRIES\": \"5000\",\n        \"MAX_MEMORY\": \"209715200\",  // 200MB\n        \"DEFAULT_TTL\": \"7200\",      // 2 hours\n        \"CHECK_INTERVAL\": \"120000\",  // 2 minutes\n        \"STATS_INTERVAL\": \"60000\"    // 1 minute\n      }\n    }\n  }\n}\n```\n\nYou can also specify a custom config file location:\n```json\n{\n  \"env\": {\n    \"CONFIG_PATH\": \"/path/to/your/config.json\"\n  }\n}\n```\n\nThe server will:\n1. Look for config.json in its directory\n2. Apply any environment variable overrides\n3. Use default values if neither is specified\n\n## Testing the Cache in Practice\n\nTo see the cache in action, try these scenarios:\n\n1. **File Reading Test**\n   - Read and analyze a large file\n   - Ask the same question about the file again\n   - The second response should be faster as the file content is cached\n\n2. **Data Analysis Test**\n   - Perform analysis on some data\n   - Request the same analysis again\n   - The second analysis should use cached results\n\n3. **Project Navigation Test**\n   - Explore a project's structure\n   - Query the same files/directories again\n   - Directory listings and file contents will be served from cache\n\nThe cache is working when you notice:\n- Faster responses for repeated operations\n- Consistent answers about unchanged content\n- No need to re-read files that haven't changed\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "cache",
        "caches",
        "memory cache",
        "mcp memory",
        "memory management"
      ],
      "category": "memory-management"
    },
    "yellnuts--mcp-mem0": {
      "owner": "yellnuts",
      "name": "mcp-mem0",
      "url": "https://github.com/yellnuts/mcp-mem0",
      "imageUrl": "/freedevtools/mcp/pfp/yellnuts.webp",
      "description": "Manage long-term memory for AI agents by storing and retrieving memories efficiently with a lightweight Python-based solution. Customize and extend memory capabilities easily through a robust template designed for integration.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T08:07:12Z",
      "readme_content": "# MCP-Mem0: Your Gateway to Long-Term Agent Memory 🚀\n\nWelcome to the **MCP-Mem0** repository! This project provides a robust server for managing long-term agent memory using Mem0. It also serves as a helpful template for anyone looking to build their own MCP server with Python.\n\n[![Download Releases](https://img.shields.io/badge/Download%20Releases-blue.svg)](https://github.com/yellnuts/mcp-mem0/releases)\n\n## Table of Contents\n\n- [Features](#features)\n- [Getting Started](#getting-started)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Contributing](#contributing)\n- [License](#license)\n- [Contact](#contact)\n\n## Features ✨\n\n- **Long-Term Memory Management**: Efficiently store and retrieve agent memories.\n- **Python-Based**: Built with Python, making it easy to customize and extend.\n- **Template Structure**: A great starting point for your own MCP server development.\n- **Lightweight**: Minimal resource requirements for easy deployment.\n\n## Getting Started 🏁\n\nTo get started with MCP-Mem0, you will need to download the latest release. Visit the [Releases section](https://github.com/yellnuts/mcp-mem0/releases) to find the latest version. Download the file and execute it to set up your server.\n\n## Installation ⚙️\n\nFollow these steps to install MCP-Mem0:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/yellnuts/mcp-mem0.git\n   cd mcp-mem0\n   ```\n\n2. **Install Dependencies**:\n   Ensure you have Python 3.6 or higher installed. Use pip to install the required packages:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Run the Server**:\n   After installing the dependencies, you can start the server with:\n   ```bash\n   python server.py\n   ```\n\n4. **Access the API**:\n   Open your web browser and navigate to `http://localhost:5000` to access the server.\n\n## Usage 📚\n\nOnce the server is running, you can interact with it using HTTP requests. Below are some example endpoints you can use:\n\n- **Create Memory**:\n  ```http\n  POST /memory\n  ```\n  Body:\n  ```json\n  {\n    \"agent_id\": \"unique_agent_id\",\n    \"memory_data\": \"Your memory data here\"\n  }\n  ```\n\n- **Retrieve Memory**:\n  ```http\n  GET /memory/{agent_id}\n  ```\n\n- **Delete Memory**:\n  ```http\n  DELETE /memory/{agent_id}\n  ```\n\nFor more detailed API documentation, refer to the `API.md` file in the repository.\n\n## Contributing 🤝\n\nWe welcome contributions to MCP-Mem0! Here’s how you can help:\n\n1. **Fork the Repository**: Click the \"Fork\" button at the top right corner of the page.\n2. **Create a Branch**: \n   ```bash\n   git checkout -b feature/YourFeature\n   ```\n3. **Make Changes**: Implement your feature or fix.\n4. **Commit Your Changes**:\n   ```bash\n   git commit -m \"Add your message here\"\n   ```\n5. **Push to the Branch**:\n   ```bash\n   git push origin feature/YourFeature\n   ```\n6. **Open a Pull Request**: Go to the original repository and click on \"New Pull Request\".\n\n## License 📄\n\nThis project is licensed under the MIT License. See the `LICENSE` file for more details.\n\n## Contact 📬\n\nFor any inquiries or support, please contact the maintainer:\n\n- **Name**: [Your Name]\n- **Email**: [your.email@example.com]\n- **GitHub**: [your-github-profile](https://github.com/your-github-profile)\n\nThank you for checking out MCP-Mem0! We hope you find it useful. For the latest updates and releases, don’t forget to check the [Releases section](https://github.com/yellnuts/mcp-mem0/releases) again.\n\n\n\n---\n\n## Advanced Configuration 🔧\n\nMCP-Mem0 allows for advanced configurations to suit your specific needs. You can adjust settings in the `config.json` file located in the root directory. Here are some of the key configurations you can modify:\n\n- **Memory Expiry**: Set how long memories should be retained.\n- **Logging Level**: Adjust the verbosity of server logs.\n- **Port Configuration**: Change the port number if needed.\n\n### Example Configuration\n\nHere’s an example of what your `config.json` might look like:\n\n```json\n{\n  \"memory_expiry\": \"30 days\",\n  \"logging_level\": \"info\",\n  \"port\": 5000\n}\n```\n\n## Troubleshooting 🛠️\n\nIf you encounter issues while using MCP-Mem0, consider the following common problems:\n\n- **Server Not Starting**: Ensure that all dependencies are installed correctly.\n- **API Errors**: Check the request format and ensure the server is running.\n- **Memory Not Saving**: Verify that the `agent_id` is unique and correctly formatted.\n\n## Roadmap 🗺️\n\nWe have exciting plans for future updates! Here are some features we aim to implement:\n\n- **User Authentication**: Secure your memory management with user accounts.\n- **Data Visualization**: Graphical representation of memory data.\n- **Multi-Agent Support**: Handle multiple agents simultaneously.\n\nStay tuned for these features and more!\n\n## Community 💬\n\nJoin our community to share your experiences, ask questions, and get support:\n\n- **Discord**: [Join our Discord Server](https://discord.gg/example)\n- **Forum**: [Visit our Forum](https://forum.example.com)\n\nWe encourage you to engage with other users and contribute to discussions.\n\n## Final Thoughts 💭\n\nThank you for exploring MCP-Mem0! We believe this tool will be a valuable asset for anyone working with agent memory management. Your feedback is essential, so feel free to reach out with suggestions or improvements.\n\nFor the latest updates, don’t forget to visit the [Releases section](https://github.com/yellnuts/mcp-mem0/releases) again. Happy coding!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memory",
        "mem0",
        "ai",
        "memory ai",
        "mem0 manage",
        "memory management"
      ],
      "category": "memory-management"
    },
    "zacharyliner1xds--my-sequential-thinking-mcp": {
      "owner": "zacharyliner1xds",
      "name": "my-sequential-thinking-mcp",
      "url": "https://github.com/zacharyliner1xds/my-sequential-thinking-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zacharyliner1xds.webp",
      "description": "Facilitates structured sequential thinking by breaking down complex problems into logical steps, validating reasoning chains, and visualizing thinking pathways. Integrates with a Memory Bank for managing and storing reasoning patterns to enhance problem-solving workflows.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-01T14:41:11Z",
      "readme_content": "# Sequential Thinking MCP Server\n\nA Model Context Protocol (MCP) server focused on structured sequential thinking capabilities, designed to integrate with Cline's Memory Bank. This server helps break down complex problems into structured sequential steps, track reasoning chains, and store thinking patterns.\n\n## Features\n\n- Create and manage sequential thinking chains for problem-solving\n- Track chains of thought with validation at each step\n- Store and retrieve reasoning patterns\n- Analyze the quality of reasoning processes\n- Visualize thinking pathways\n- Seamlessly integrate with the Memory Bank system\n\n## Architecture\n\nThe server consists of the following core components:\n\n- **Sequential Thinking Engine**: Manages thinking chains, steps, and reasoning validation\n- **Memory Bank Connector**: Integrates with Cline's Memory Bank\n- **Tag Manager**: Implements a comprehensive tagging system\n- **Visualization Generator**: Creates visual representations of thinking chains\n- **Utilities**: File storage, thinking validation, and other helpers\n\n## Available Tools\n\nThe server provides the following MCP tools:\n\n### create_thinking_chain\nCreate a new sequential thinking process with specified parameters.\n- **Input**: problem description, thinking type, context\n- **Output**: chain_id and initial structure\n\n### add_thinking_step\nAdd a step to an existing thinking chain.\n- **Input**: chain_id, step description, reasoning, evidence\n- **Output**: updated step information\n\n### validate_step\nValidate logical connections between steps.\n- **Input**: chain_id, step_id\n- **Output**: validation results, potential issues\n\n### get_chain\nRetrieve a complete thinking chain.\n- **Input**: chain_id\n- **Output**: full chain with all steps\n\n### generate_visualization\nCreate visual representation of a thinking chain.\n- **Input**: chain_id, format (mermaid, json, text)\n- **Output**: visualization code/data\n\n### save_to_memory\nSave a thinking chain to Memory Bank.\n- **Input**: chain_id, memory_name, tags\n- **Output**: confirmation and memory_id\n\n### load_from_memory\nLoad a thinking chain from Memory Bank.\n- **Input**: memory_id or search parameters\n- **Output**: complete chain\n\n### search_related_thinking\nFind related thinking chains based on parameters.\n- **Input**: keywords, tags, thinking_type\n- **Output**: list of relevant chains\n\n### apply_template\nApply a reasoning template to current thinking.\n- **Input**: template_name, problem_context\n- **Output**: pre-structured thinking chain\n\n## Thinking Types\n\nThe server supports various thinking types, each with specific patterns and structures:\n\n- **Analytical** - Break down, analyze, synthesize\n- **Creative** - Diverge, explore, converge\n- **Critical** - Question, evaluate, conclude\n- **Systems** - Map, analyze, model\n- **First-Principles** - Identify, break down, reassemble\n- **Divergent** - Generate alternatives, explore\n- **Convergent** - Analyze, evaluate, select\n- **Inductive** - Observe, pattern, hypothesize\n- **Deductive** - Premise, logic, conclusion\n\n## Templates\n\nThe server includes ready-to-use reasoning templates to jumpstart the thinking process:\n\n- **First Principles Analysis** - Break down a complex problem into its fundamental principles\n- **Systems Thinking Analysis** - Analyze complex systems holistically\n\n## Installation\n\n1. Ensure Node.js v14+ is installed\n2. Clone the repository\n3. Install dependencies:\n   ```\n   npm install\n   ```\n\n## Usage\n\n1. Start the server:\n   ```\n   node index.js\n   ```\n\n2. The server will be available as an MCP server that you can connect to via Claude/Cline\n\n## Memory Bank Integration\n\nThis server is designed to integrate with Cline's Memory Bank, allowing:\n\n1. Reading from Memory Bank files (projectbrief.md, activeContext.md, etc.)\n2. Storing complete thinking chains as structured memories\n3. Updating activeContext.md with reasoning outcomes\n4. Creating links between reasoning and Memory Bank sections\n\n## Example Tool Usage\n\n```javascript\n// Example: Create a new thinking chain\n{\n  \"problem\": \"How to improve user engagement on our platform\",\n  \"thinking_type\": \"systems\",\n  \"context\": \"Our user engagement metrics have decreased by 15% over the past quarter\"\n}\n\n// Example: Add a thinking step\n{\n  \"chain_id\": \"3a7e4fc0-5c1d-4b9f-9d1a-8b5e7c5a9d3e\",\n  \"description\": \"Identify key components of the engagement system\",\n  \"reasoning\": \"User engagement consists of several interconnected components including onboarding, core user actions, notification systems, and retention mechanisms.\",\n  \"evidence\": \"Analysis of our user journey maps and analytics data\",\n  \"confidence\": 0.8\n}\n\n// Example: Generate a visualization\n{\n  \"chain_id\": \"3a7e4fc0-5c1d-4b9f-9d1a-8b5e7c5a9d3e\",\n  \"format\": \"mermaid\",\n  \"options\": {\n    \"showValidation\": true,\n    \"showConfidence\": true\n  }\n}\n```\n\n## Tag System\n\nThe server implements a comprehensive tagging system with multiple dimensions:\n\n- **Thinking Type** - analytical, creative, critical, systems, etc.\n- **Domain** - business, science, technology, art, etc.\n- **Complexity** - simple, moderate, complex\n- **Status** - draft, validated, complete\n- **Custom** - user-defined tags\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sequential",
        "thinking",
        "memory",
        "sequential thinking",
        "thinking pathways",
        "structured sequential"
      ],
      "category": "memory-management"
    }
  }
}