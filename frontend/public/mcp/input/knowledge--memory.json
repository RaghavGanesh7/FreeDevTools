{
  "category": "knowledge--memory",
  "categoryDisplay": "Knowledge & Memory",
  "description": "Persistent memory storage using knowledge graph structures. Enables AI models to maintain and query structured information across sessions.",
  "totalRepositories": 22,
  "repositories": {
    "0xshellming--mcp-summarizer": {
      "owner": "0xshellming",
      "name": "mcp-summarizer",
      "url": "https://github.com/0xshellming/mcp-summarizer",
      "imageUrl": "",
      "description": "AI Summarization MCP Server, Support for multiple content types: Plain text, Web pages, PDF documents, EPUB books, HTML content",
      "stars": 131,
      "forks": 20,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T16:15:57Z",
      "readme_content": "# MCP Content Summarizer Server\n\nA Model Context Protocol (MCP) server that provides intelligent summarization capabilities for various types of content using Google's Gemini 1.5 Pro model. This server can help you generate concise summaries while maintaining key information from different content formats.\n\n<a href=\"https://3min.top\"><img width=\"380\" height=\"200\" src=\"/public/imgs/section1_en.jpg\" alt=\"MCP Content Summarizer Server\" /></a>\n\n## Powered by 3MinTop\n\nThe summarization service is powered by [3MinTop](https://3min.top), an AI-powered reading tool that helps you understand a chapter's content in just three minutes. 3MinTop transforms complex content into clear summaries, making learning efficient and helping build lasting reading habits.\n\n## Features\n\n- Universal content summarization using Google's Gemini 1.5 Pro model\n- Support for multiple content types:\n  - Plain text\n  - Web pages\n  - PDF documents\n  - EPUB books\n  - HTML content\n- Customizable summary length\n- Multi-language support\n- Smart context preservation\n- Dynamic greeting resource for testing\n\n## Getting Started\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pnpm install\n   ```\n\n3. Build the project:\n   ```\n   pnpm run build\n   ```\n\n4. Start the server:\n   ```\n   pnpm start\n   ```\n\n## Development\n\n- Use `pnpm run dev` to start the TypeScript compiler in watch mode\n- Modify `src/index.ts` to customize server behavior or add new tools\n\n## Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```js\n{\n  \"mcpServers\": {\n    \"content-summarizer\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"{ABSOLUTE PATH TO FILE HERE}/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### summarize\n\nSummarizes content from various sources using the following parameters:\n- `content` (string | object): The input content to summarize. Can be:\n  - Text string\n  - URL for web pages\n  - Base64 encoded PDF\n  - EPUB file content\n- `type` (string): Content type (\"text\", \"url\", \"pdf\", \"epub\")\n- `maxLength` (number, optional): Maximum length of the summary in characters (default: 200)\n- `language` (string, optional): Target language for the summary (default: \"en\")\n- `focus` (string, optional): Specific aspect to focus on in the summary\n- `style` (string, optional): Summary style (\"concise\", \"detailed\", \"bullet-points\")\n\nExample usage:\n\n```typescript\n// Summarize a webpage\nconst result = await server.invoke(\"summarize\", {\n  content: \"https://example.com/article\",\n  type: \"url\",\n  maxLength: 300,\n  style: \"bullet-points\"\n});\n\n// Summarize a PDF document\nconst result = await server.invoke(\"summarize\", {\n  content: pdfBase64Content,\n  type: \"pdf\",\n  language: \"zh\",\n  style: \"detailed\"\n});\n```\n\n### greeting\n\nA dynamic resource that demonstrates basic MCP resource functionality:\n- URI format: `greeting://{name}`\n- Returns a greeting message with the provided name\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "apecloud--ApeRAG": {
      "owner": "apecloud",
      "name": "ApeRAG",
      "url": "https://github.com/apecloud/ApeRAG",
      "imageUrl": "",
      "description": "Production-ready RAG platform combining Graph RAG, vector search, and full-text search. Best choice for building your own Knowledge Graph and for Context Engineering",
      "stars": 830,
      "forks": 81,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T07:12:49Z",
      "readme_content": "# ApeRAG\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/apecloud/ApeRAG)](https://archestra.ai/mcp-catalog/apecloud__aperag)\n\n**üöÄ [Try ApeRAG Live Demo](https://rag.apecloud.com/)** - Experience the full platform capabilities with our hosted demo\n\n\n![HarryPotterKG2.png](docs%2Fimages%2FHarryPotterKG2.png)\n\n![chat2.png](docs%2Fimages%2Fchat2.png)\n\n\nApeRAG is a production-ready RAG (Retrieval-Augmented Generation) platform that combines Graph RAG, vector search, and full-text search with advanced AI agents. Build sophisticated AI applications with hybrid retrieval, multimodal document processing, intelligent agents, and enterprise-grade management features.\n\nApeRAG is the best choice for building your own Knowledge Graph, Context Engineering, and deploying intelligent AI agents that can autonomously search and reason across your knowledge base.\n\n[ÈòÖËØª‰∏≠ÊñáÊñáÊ°£](README-zh.md)\n\n- [Quick Start](#quick-start)\n- [Key Features](#key-features)\n- [Kubernetes Deployment (Recommended for Production)](#kubernetes-deployment-recommended-for-production)\n- [Development](./docs/development-guide.md)\n- [Build Docker Image](./docs/build-docker-image.md)\n- [Acknowledgments](#acknowledgments)\n- [License](#license)\n\n## Quick Start\n\n> Before installing ApeRAG, make sure your machine meets the following minimum system requirements:\n>\n> - CPU >= 2 Core\n> - RAM >= 4 GiB\n> - Docker & Docker Compose\n\nThe easiest way to start ApeRAG is through Docker Compose. Before running the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ngit clone https://github.com/apecloud/ApeRAG.git\ncd ApeRAG\ncp envs/env.template .env\ndocker-compose up -d --pull always\n```\n\nAfter running, you can access ApeRAG in your browser at:\n- **Web Interface**: http://localhost:3000/web/\n- **API Documentation**: http://localhost:8000/docs\n\n#### MCP (Model Context Protocol) Support\n\nApeRAG supports [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) integration, allowing AI assistants to interact with your knowledge base directly. After starting the services, configure your MCP client with:\n\n```json\n{\n  \"mcpServers\": {\n    \"aperag-mcp\": {\n      \"url\": \"https://rag.apecloud.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n**Important**: Replace `http://localhost:8000` with your actual ApeRAG API URL and `your-api-key-here` with a valid API key from your ApeRAG settings.\n\nThe MCP server provides:\n- **Collection browsing**: List and explore your knowledge collections\n- **Hybrid search**: Search using vector, full-text, and graph methods\n- **Intelligent querying**: Ask natural language questions about your documents\n\n#### Enhanced Document Parsing\n\nFor enhanced document parsing capabilities, ApeRAG supports an **advanced document parsing service** powered by MinerU, which provides superior parsing for complex documents, tables, and formulas. \n\n<details>\n<summary><strong>Enhanced Document Parsing Commands</strong></summary>\n\n```bash\n# Enable advanced document parsing service\nDOCRAY_HOST=http://aperag-docray:8639 docker compose --profile docray up -d\n\n# Enable advanced parsing with GPU acceleration \nDOCRAY_HOST=http://aperag-docray-gpu:8639 docker compose --profile docray-gpu up -d\n```\n\nOr use the Makefile shortcuts (requires [GNU Make](https://www.gnu.org/software/make/)):\n```bash\n# Enable advanced document parsing service\nmake compose-up WITH_DOCRAY=1\n\n# Enable advanced parsing with GPU acceleration (recommended)\nmake compose-up WITH_DOCRAY=1 WITH_GPU=1\n```\n\n</details>\n\n#### Development & Contributing\n\nFor developers interested in source code development, advanced configurations, or contributing to ApeRAG, please refer to our [Development Guide](./docs/development-guide.md) for detailed setup instructions.\n\n## Key Features\n\n**1. Advanced Index Types**:\nFive comprehensive index types for optimal retrieval: **Vector**, **Full-text**, **Graph**, **Summary**, and **Vision** - providing multi-dimensional document understanding and search capabilities.\n\n**2. Intelligent AI Agents**:\nBuilt-in AI agents with MCP (Model Context Protocol) tool support that can automatically identify relevant collections, search content intelligently, and provide web search capabilities for comprehensive question answering.\n\n**3. Enhanced Graph RAG with Entity Normalization**:\nDeeply modified LightRAG implementation with advanced entity normalization (entity merging) for cleaner knowledge graphs and improved relational understanding.\n\n**4. Multimodal Processing & Vision Support**:\nComplete multimodal document processing including vision capabilities for images, charts, and visual content analysis alongside traditional text processing.\n\n**5. Hybrid Retrieval Engine**:\nSophisticated retrieval system combining Graph RAG, vector search, full-text search, summary-based retrieval, and vision-based search for comprehensive document understanding.\n\n**6. MinerU Integration**:\nAdvanced document parsing service powered by MinerU technology, providing superior parsing for complex documents, tables, formulas, and scientific content with optional GPU acceleration.\n\n**7. Production-Grade Deployment**:\nFull Kubernetes support with Helm charts and KubeBlocks integration for simplified deployment of production-grade databases (PostgreSQL, Redis, Qdrant, Elasticsearch, Neo4j).\n\n**8. Enterprise Management**:\nBuilt-in audit logging, LLM model management, graph visualization, comprehensive document management interface, and agent workflow management.\n\n**9. MCP Integration**:\nFull support for Model Context Protocol (MCP), enabling seamless integration with AI assistants and tools for direct knowledge base access and intelligent querying.\n\n**10. Developer Friendly**:\nFastAPI backend, React frontend, async task processing with Celery, extensive testing, comprehensive development guides, and agent development framework for easy contribution and customization.\n\n## Kubernetes Deployment (Recommended for Production)\n\n> **Enterprise-grade deployment with high availability and scalability**\n\nDeploy ApeRAG to Kubernetes using our provided Helm chart. This approach offers high availability, scalability, and production-grade management capabilities.\n\n### Prerequisites\n\n*   [Kubernetes cluster](https://kubernetes.io/docs/setup/) (v1.20+)\n*   [`kubectl`](https://kubernetes.io/docs/tasks/tools/) configured and connected to your cluster\n*   [Helm v3+](https://helm.sh/docs/intro/install/) installed\n\n### Clone the Repository\n\nFirst, clone the ApeRAG repository to get the deployment files:\n\n```bash\ngit clone https://github.com/apecloud/ApeRAG.git\ncd ApeRAG\n```\n\n### Step 1: Deploy Database Services\n\nApeRAG requires PostgreSQL, Redis, Qdrant, and Elasticsearch. You have two options:\n\n**Option A: Use existing databases** - If you already have these databases running in your cluster, edit `deploy/aperag/values.yaml` to configure your database connection details, then skip to Step 2.\n\n**Option B: Deploy databases with KubeBlocks** - Use our automated database deployment (database connections are pre-configured):\n\n```bash\n# Navigate to database deployment scripts\ncd deploy/databases/\n\n# (Optional) Review configuration - defaults work for most cases\n# edit 00-config.sh\n\n# Install KubeBlocks and deploy databases\nbash ./01-prepare.sh          # Installs KubeBlocks\nbash ./02-install-database.sh # Deploys PostgreSQL, Redis, Qdrant, Elasticsearch\n\n# Monitor database deployment\nkubectl get pods -n default\n\n# Return to project root for Step 2\ncd ../../\n```\n\nWait for all database pods to be in `Running` status before proceeding.\n\n### Step 2: Deploy ApeRAG Application\n\n```bash\n# If you deployed databases with KubeBlocks in Step 1, database connections are pre-configured\n# If you're using existing databases, edit deploy/aperag/values.yaml with your connection details\n\n# Deploy ApeRAG\nhelm install aperag ./deploy/aperag --namespace default --create-namespace\n\n# Monitor ApeRAG deployment\nkubectl get pods -n default -l app.kubernetes.io/instance=aperag\n```\n\n### Configuration Options\n\n**Resource Requirements**: By default, includes [`doc-ray`](https://github.com/apecloud/doc-ray) service (requires 4+ CPU cores, 8GB+ RAM). To disable: set `docray.enabled: false` in `values.yaml`.\n\n**Advanced Settings**: Review `values.yaml` for additional configuration options including images, resources, and Ingress settings.\n\n### Access Your Deployment\n\nOnce deployed, access ApeRAG using port forwarding:\n\n```bash\n# Forward ports for quick access\nkubectl port-forward svc/aperag-frontend 3000:3000 -n default\nkubectl port-forward svc/aperag-api 8000:8000 -n default\n\n# Access in browser\n# Web Interface: http://localhost:3000\n# API Documentation: http://localhost:8000/docs\n```\n\nFor production environments, configure Ingress in `values.yaml` for external access.\n\n### Troubleshooting\n\n**Database Issues**: See `deploy/databases/README.md` for KubeBlocks management, credentials, and uninstall procedures.\n\n**Pod Status**: Check pod logs for any deployment issues:\n```bash\nkubectl logs -f deployment/aperag-api -n default\nkubectl logs -f deployment/aperag-frontend -n default\n```\n\n## Acknowledgments\n\nApeRAG integrates and builds upon several excellent open-source projects:\n\n### LightRAG\nThe graph-based knowledge retrieval capabilities in ApeRAG are powered by a deeply modified version of [LightRAG](https://github.com/HKUDS/LightRAG):\n- **Paper**: \"LightRAG: Simple and Fast Retrieval-Augmented Generation\" ([arXiv:2410.05779](https://arxiv.org/abs/2410.05779))\n- **Authors**: Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang\n- **License**: MIT License\n\nWe have extensively modified LightRAG to support production-grade concurrent processing, distributed task queues (Celery/Prefect), and stateless operations. See our [LightRAG modifications changelog](./aperag/graph/changelog.md) for details.\n\n## Community\n\n* [Discord](https://discord.gg/FsKpXukFuB)\n* [Feishu](docs%2Fimages%2Ffeishu-qr-code.png)\n\n<img src=\"docs/images/feishu-qr-code.png\" alt=\"Feishu\" width=\"150\"/>\n\n## Star History\n\n![star-history-2025922.png](docs%2Fimages%2Fstar-history-2025922.png)\n\n## License\n\nApeRAG is licensed under the Apache License 2.0. See the [LICENSE](./LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0
    },
    "chatmcp--mcp-server-chatsum": {
      "owner": "chatmcp",
      "name": "mcp-server-chatsum",
      "url": "https://github.com/chatmcp/mcp-server-chatsum",
      "imageUrl": "",
      "description": "Query and summarize your chat messages with AI prompts.",
      "stars": 1019,
      "forks": 100,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:31:58Z",
      "readme_content": "# mcp-server-chatsum\n\nThis MCP Server is used to summarize your chat messages.\n\n[‰∏≠ÊñáËØ¥Êòé](README_CN.md)\n\n![preview](./preview.png)\n\n> **Before you start**\n>\n> move to [chatbot](./chatbot) directory, follow the [README](./chatbot/README.md) to setup the chat database.\n>\n> start chatbot to save your chat messages.\n\n## Features\n\n### Resources\n\n### Tools\n\n- `query_chat_messages` - Query chat messages\n  - Query chat messages with given parameters\n  - Summarize chat messages based on the query prompt\n\n### Prompts\n\n## Development\n\n1. Set up environment variables:\n\ncreate `.env` file in the root directory, and set your chat database path.\n\n```txt\nCHAT_DB_PATH=path-to/chatbot/data/chat.db\n```\n\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\nBuild the server:\n\n```bash\npnpm build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-chatsum\": {\n      \"command\": \"path-to/bin/node\",\n      \"args\": [\"path-to/mcp-server-chatsum/build/index.js\"],\n      \"env\": {\n        \"CHAT_DB_PATH\": \"path-to/mcp-server-chatsum/chatbot/data/chat.db\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Community\n\n- [MCP Server Telegram](https://t.me/+N0gv4O9SXio2YWU1)\n- [MCP Server Discord](https://discord.gg/RsYPRrnyqg)\n\n## About the author\n\n- [idoubi](https://bento.me/idoubi)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "cameronrye--openzim-mcp": {
      "owner": "cameronrye",
      "name": "openzim-mcp",
      "url": "https://github.com/cameronrye/openzim-mcp",
      "imageUrl": "",
      "description": "Modern, secure MCP server for accessing ZIM format knowledge bases offline. Enables AI models to search and navigate Wikipedia, educational content, and other compressed knowledge archives with smart retrieval, caching, and comprehensive API.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T20:37:26Z",
      "readme_content": "# OpenZIM MCP Server\n\n<!-- Build and Quality Badges -->\n[![CI](https://github.com/cameronrye/openzim-mcp/workflows/CI/badge.svg)](https://github.com/cameronrye/openzim-mcp/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/cameronrye/openzim-mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/cameronrye/openzim-mcp)\n[![CodeQL](https://github.com/cameronrye/openzim-mcp/workflows/CodeQL%20Security%20Analysis/badge.svg)](https://github.com/cameronrye/openzim-mcp/actions/workflows/codeql.yml)\n[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=cameronrye_openzim-mcp&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=cameronrye_openzim-mcp)\n\n<!-- Package and Version Badges -->\n[![PyPI version](https://badge.fury.io/py/openzim-mcp.svg)](https://badge.fury.io/py/openzim-mcp)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/openzim-mcp)](https://pypi.org/project/openzim-mcp/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/openzim-mcp)](https://pypi.org/project/openzim-mcp/)\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/releases)\n\n<!-- Code Quality and Standards -->\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue)](https://mypy-lang.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<!-- Community and Contribution -->\n[![GitHub issues](https://img.shields.io/github/issues/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/issues)\n[![GitHub pull requests](https://img.shields.io/github/issues-pr/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/pulls)\n[![GitHub contributors](https://img.shields.io/github/contributors/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/graphs/contributors)\n[![GitHub stars](https://img.shields.io/github/stars/cameronrye/openzim-mcp?style=social)](https://github.com/cameronrye/openzim-mcp/stargazers)\n\n## üß† Built for LLM Intelligence\n\n**OpenZIM MCP transforms static ZIM archives into dynamic knowledge engines for Large Language Models.** Unlike basic file readers, this tool provides *intelligent, structured access* that LLMs need to effectively navigate and understand vast knowledge repositories.\n\nüöÄ **Why LLMs Love OpenZIM MCP:**\n- **Smart Navigation**: Browse by namespace (articles, metadata, media) instead of blind searching\n- **Context-Aware Discovery**: Get article structure, relationships, and metadata for deeper understanding\n- **Intelligent Search**: Advanced filtering, auto-complete suggestions, and relevance-ranked results\n- **Performance Optimized**: Cached operations and pagination prevent timeouts on massive archives\n- **Relationship Mapping**: Extract internal/external links to understand content connections\n\nWhether you're building a research assistant, knowledge chatbot, or content analysis system, OpenZIM MCP gives your LLM the structured access patterns it needs to unlock the full potential of offline knowledge archives. No more fumbling through raw text dumps! üéØ\n\n**OpenZIM MCP** is a modern, secure, and high-performance MCP (Model Context Protocol) server that enables AI models to access and search [ZIM format](https://en.wikipedia.org/wiki/ZIM_(file_format)) knowledge bases offline.\n\n[ZIM](https://en.wikipedia.org/wiki/ZIM_(file_format)) (Zeno IMproved) is an open file format developed by the [openZIM project](https://openzim.org/), designed specifically for offline storage and access to website content. The format supports high compression rates using Zstandard compression (default since 2021) and enables fast full-text searching, making it ideal for storing entire Wikipedia content and other large reference materials in relatively compact files. The openZIM project is sponsored by Wikimedia CH and supported by the Wikimedia Foundation, ensuring the format's continued development and adoption for offline knowledge access, especially in environments without reliable internet connectivity.\n\n## ‚ú® Features\n\n- üîí **Security First**: Comprehensive input validation and path traversal protection\n- ‚ö° **High Performance**: Intelligent caching and optimized ZIM file operations\n- üß† **Smart Retrieval**: Automatic fallback from direct access to search-based retrieval for reliable entry access\n- üß™ **Well Tested**: 90%+ test coverage with comprehensive test suite\n- üèóÔ∏è **Modern Architecture**: Modular design with dependency injection\n- üìù **Type Safe**: Full type annotations throughout the codebase\n- üîß **Configurable**: Flexible configuration with validation\n- üìä **Observable**: Structured logging and health monitoring\n\n## üöÄ Quick Start\n\n### Installation\n\n```bash\n# Install from PyPI (recommended)\npip install openzim-mcp\n```\n\n### Development Installation\n\nFor contributors and developers:\n\n```bash\n# Clone the repository\ngit clone https://github.com/cameronrye/openzim-mcp.git\ncd openzim-mcp\n\n# Install dependencies\nuv sync\n\n# Install development dependencies\nuv sync --dev\n```\n\n### Prepare ZIM Files\n\nDownload ZIM files (e.g., Wikipedia, Wiktionary, etc.) from the [Kiwix Library](https://browse.library.kiwix.org/) and place them in a directory:\n\n```bash\nmkdir ~/zim-files\n# Download ZIM files to ~/zim-files/\n```\n\n### Running the Server\n\n```bash\n# Using the console script (after pip install)\nopenzim-mcp /path/to/zim/files\n\n# Or using the module\npython -m openzim_mcp /path/to/zim/files\n\n# For development (from source)\nuv run python -m openzim_mcp /path/to/zim/files\n\n# Or using make (development)\nmake run ZIM_DIR=/path/to/zim/files\n```\n\n### MCP Configuration\n\nAdd to your MCP client configuration:\n\n```json\n{\n  \"openzim-mcp\": {\n    \"command\": \"openzim-mcp\",\n    \"args\": [\"/path/to/zim/files\"]\n  }\n}\n```\n\nAlternative configuration using Python module:\n\n```json\n{\n  \"openzim-mcp\": {\n    \"command\": \"python\",\n    \"args\": [\n      \"-m\",\n      \"openzim_mcp\",\n      \"/path/to/zim/files\"\n    ]\n  }\n}\n```\n\nFor development (from source):\n\n```json\n{\n  \"openzim-mcp\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/path/to/openzim-mcp\",\n      \"run\",\n      \"python\",\n      \"-m\",\n      \"openzim_mcp\",\n      \"/path/to/zim/files\"\n    ]\n  }\n}\n```\n\n## üõ†Ô∏è Development\n\n### Running Tests\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with coverage\nmake test-cov\n\n# Run specific test file\nuv run pytest tests/test_security.py -v\n\n# Run tests with ZIM test data (comprehensive testing)\nmake test-with-zim-data\n\n# Run integration tests only\nmake test-integration\n\n# Run tests that require ZIM test data\nmake test-requires-zim-data\n```\n\n### ZIM Test Data Integration\n\nOpenZIM MCP integrates with the official [zim-testing-suite](https://github.com/openzim/zim-testing-suite) for comprehensive testing with real ZIM files:\n\n```bash\n# Download essential test files (basic testing)\nmake download-test-data\n\n# Download all test files (comprehensive testing)\nmake download-test-data-all\n\n# List available test files\nmake list-test-data\n\n# Clean downloaded test data\nmake clean-test-data\n```\n\nThe test data includes:\n- **Basic files**: Small ZIM files for essential testing\n- **Real content**: Actual Wikipedia/Wikibooks content for integration testing\n- **Invalid files**: Malformed ZIM files for error handling testing\n- **Special cases**: Embedded content, split files, and edge cases\n\nTest files are automatically organized by category and priority level.\n\n### Code Quality\n\n```bash\n# Format code\nmake format\n\n# Run linting\nmake lint\n\n# Type checking\nmake type-check\n\n# Run all checks\nmake check\n```\n\n### Project Structure\n\n```text\nopenzim-mcp/\n‚îú‚îÄ‚îÄ openzim_mcp/             # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py        # Package initialization\n‚îÇ   ‚îú‚îÄ‚îÄ __main__.py        # Module entry point\n‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Main entry point\n‚îÇ   ‚îú‚îÄ‚îÄ server.py          # MCP server implementation\n‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ security.py        # Security and validation\n‚îÇ   ‚îú‚îÄ‚îÄ cache.py           # Caching functionality\n‚îÇ   ‚îú‚îÄ‚îÄ content_processor.py # Content processing\n‚îÇ   ‚îú‚îÄ‚îÄ zim_operations.py  # ZIM file operations\n‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py      # Custom exceptions\n‚îÇ   ‚îî‚îÄ‚îÄ constants.py       # Application constants\n‚îú‚îÄ‚îÄ tests/                 # Test suite\n‚îú‚îÄ‚îÄ pyproject.toml        # Project configuration\n‚îú‚îÄ‚îÄ Makefile              # Development commands\n‚îî‚îÄ‚îÄ README.md             # This file\n```\n\n---\n\n## üìö API Reference\n\n### Available Tools\n\n### list_zim_files - List all ZIM files in allowed directories\n\nNo parameters required.\n\n### search_zim_file - Search within ZIM file content\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `query` (string): Search query term\n\n**Optional parameters:**\n\n- `limit` (integer, default: 10): Maximum number of results to return\n- `offset` (integer, default: 0): Starting offset for results (for pagination)\n\n### get_zim_entry - Get detailed content of a specific entry in a ZIM file\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `entry_path` (string): Entry path, e.g., 'A/Some_Article'\n\n**Optional parameters:**\n\n- `max_content_length` (integer, default: 100000, minimum: 1000): Maximum length of returned content\n\n**Smart Retrieval Features:**\n\n- **Automatic Fallback**: If direct path access fails, automatically searches for the entry and uses the exact path found\n- **Path Mapping Cache**: Caches successful path mappings for improved performance on repeated access\n- **Enhanced Error Guidance**: Provides clear guidance when entries cannot be found, suggesting alternative approaches\n- **Transparent Operation**: Works seamlessly regardless of path encoding differences (spaces vs underscores, URL encoding, etc.)\n\n### get_zim_metadata - Get ZIM file metadata from M namespace entries\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n\n**Returns:**\nJSON string containing ZIM metadata including entry counts, archive information, and metadata entries like title, description, language, creator, etc.\n\n### get_main_page - Get the main page entry from W namespace\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n\n**Returns:**\nMain page content or information about the main page entry.\n\n### list_namespaces - List available namespaces and their entry counts\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n\n**Returns:**\nJSON string containing namespace information with entry counts, descriptions, and sample entries for each namespace (C, M, W, X, etc.).\n\n### browse_namespace - Browse entries in a specific namespace with pagination\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `namespace` (string): Namespace to browse (C, M, W, X, A, I, etc.)\n\n**Optional parameters:**\n\n- `limit` (integer, default: 50, range: 1-200): Maximum number of entries to return\n- `offset` (integer, default: 0): Starting offset for pagination\n\n**Returns:**\nJSON string containing namespace entries with titles, content previews, and pagination information.\n\n### search_with_filters - Search within ZIM file content with advanced filters\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `query` (string): Search query term\n\n**Optional parameters:**\n\n- `namespace` (string): Optional namespace filter (C, M, W, X, etc.)\n- `content_type` (string): Optional content type filter (text/html, text/plain, etc.)\n- `limit` (integer, default: 10, range: 1-100): Maximum number of results to return\n- `offset` (integer, default: 0): Starting offset for pagination\n\n**Returns:**\nFiltered search results with namespace and content type information.\n\n### get_search_suggestions - Get search suggestions and auto-complete\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `partial_query` (string): Partial search query (minimum 2 characters)\n\n**Optional parameters:**\n\n- `limit` (integer, default: 10, range: 1-50): Maximum number of suggestions to return\n\n**Returns:**\nJSON string containing search suggestions based on article titles and content.\n\n### get_article_structure - Extract article structure and metadata\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `entry_path` (string): Entry path, e.g., 'C/Some_Article'\n\n**Returns:**\nJSON string containing article structure including headings, sections, metadata, and word count.\n\n### extract_article_links - Extract internal and external links from an article\n\n**Required parameters:**\n\n- `zim_file_path` (string): Path to the ZIM file\n- `entry_path` (string): Entry path, e.g., 'C/Some_Article'\n\n**Returns:**\nJSON string containing categorized links (internal, external, media) with titles and metadata.\n\n---\n\n## Examples\n\n### Listing ZIM files\n\n```json\n{\n  \"name\": \"list_zim_files\"\n}\n```\n\nResponse:\n\n```plain\nFound 1 ZIM files in 1 directories:\n\n[\n  {\n    \"name\": \"wikipedia_en_100_2025-08.zim\",\n    \"path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"directory\": \"C:\\\\zim\",\n    \"size\": \"310.77 MB\",\n    \"modified\": \"2025-09-11T10:20:50.148427\"\n  }\n]\n```\n\n### Searching ZIM files\n\n```json\n{\n  \"name\": \"search_zim_file\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"query\": \"biology\",\n    \"limit\": 3\n  }\n}\n```\n\nResponse:\n\n```plain\nFound 51 matches for \"biology\", showing 1-3:\n\n## 1. Taxonomy (biology)\nPath: Taxonomy_(biology)\nSnippet: #  Taxonomy (biology) Part of a series on\n---\nEvolutionary biology\nDarwin's finches by John Gould\n\n  * Index\n  * Introduction\n  * [Main](Evolution \"Evolution\")\n  * Outline\n\n## 2. Protein\nPath: Protein\nSnippet: #  Protein A representation of the 3D structure of the protein myoglobin showing turquoise Œ±-helices. This protein was the first to have its structure solved by X-ray crystallography. Toward the right-center among the coils, a prosthetic group called a heme group (shown in gray) with a bound oxygen molecule (red).\n\n## 3. Ant\nPath: Ant\nSnippet: #  Ant Ants\nTemporal range: Late Aptian ‚Äì Present\n---\nFire ants\n[Scientific classification](Taxonomy_\\(biology\\) \"Taxonomy \\(biology\\)\")\nKingdom:  | [Animalia](Animal \"Animal\")\nPhylum:  | [Arthropoda](Arthropod \"Arthropod\")\nClass:  | [Insecta](Insect \"Insect\")\nOrder:  | Hymenoptera\nInfraorder:  | Aculeata\nSuperfamily:  |\nLatreille, 1809[1]\nFamily:  |\nLatreille, 1809\n```\n\n### Getting ZIM entries\n\n```json\n{\n  \"name\": \"get_zim_entry\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"entry_path\": \"Protein\"\n  }\n}\n```\n\nResponse:\n\n```plain\n# Protein\n\nPath: Protein\nType: text/html\n## Content\n\n#  Protein\n\nA representation of the 3D structure of the protein myoglobin showing turquoise Œ±-helices. This protein was the first to have its structure solved by X-ray crystallography. Toward the right-center among the coils, a prosthetic group called a heme group (shown in gray) with a bound oxygen molecule (red).\n\n**Proteins** are large biomolecules and macromolecules that comprise one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, providing structure to cells and organisms, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific 3D structure that determines its activity.\n\nA linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20‚Äì30 residues, are rarely considered to be proteins and are commonly called peptides.\n\n... [Content truncated, total of 56,202 characters, only showing first 1,500 characters] ...\n```\n\n### Smart Retrieval in Action\n\n**Example: Automatic path resolution**\n\n```json\n{\n  \"name\": \"get_zim_entry\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"entry_path\": \"A/Test Article\"\n  }\n}\n```\n\nResponse (showing smart retrieval working):\n\n```plain\n# Test Article\n\nRequested Path: A/Test Article\nActual Path: A/Test_Article\nType: text/html\n\n## Content\n\n# Test Article\n\nThis article demonstrates the smart retrieval system automatically handling\npath encoding differences. The system tried \"A/Test Article\" directly,\nthen automatically searched and found \"A/Test_Article\".\n\n... [Content continues] ...\n```\n\n### get_server_health - Get server health and statistics\n\nNo parameters required.\n\n**Returns:**\n\n- Server status and performance metrics\n- Cache statistics\n- Configuration information\n- Instance tracking information\n- Conflict detection results\n\n**Example Response:**\n\n```json\n{\n  \"status\": \"healthy\",\n  \"server_name\": \"openzim-mcp\",\n  \"allowed_directories\": 1,\n  \"cache\": {\n    \"enabled\": true,\n    \"size\": 1,\n    \"max_size\": 100,\n    \"ttl_seconds\": 3600\n  },\n  \"instance_tracking\": {\n    \"active_instances\": 1,\n    \"conflicts_detected\": 0\n  }\n}\n```\n\n### get_server_configuration - Get detailed server configuration\n\nNo parameters required.\n\n**Returns:**\nComprehensive server configuration including diagnostics, validation results, and conflict detection.\n\n**Example Response:**\n\n```json\n{\n  \"configuration\": {\n    \"server_name\": \"openzim-mcp\",\n    \"allowed_directories\": [\"/path/to/zim/files\"],\n    \"cache_enabled\": true,\n    \"config_hash\": \"abc123...\",\n    \"server_pid\": 12345\n  },\n  \"diagnostics\": {\n    \"validation_status\": \"healthy\",\n    \"conflicts_detected\": [],\n    \"warnings\": [],\n    \"recommendations\": []\n  }\n}\n```\n\n### diagnose_server_state - Comprehensive server diagnostics\n\nNo parameters required.\n\n**Returns:**\nDetailed diagnostic information including instance conflicts, configuration validation, file accessibility checks, and actionable recommendations.\n\n**Example Response:**\n\n```json\n{\n  \"status\": \"healthy\",\n  \"server_info\": {\n    \"pid\": 12345,\n    \"server_name\": \"openzim-mcp\",\n    \"config_hash\": \"abc123...\"\n  },\n  \"conflicts\": [],\n  \"issues\": [],\n  \"recommendations\": [\"Server appears to be running normally\"],\n  \"environment_checks\": {\n    \"directories_accessible\": true,\n    \"cache_functional\": true\n  }\n}\n```\n\n### resolve_server_conflicts - Identify and resolve server conflicts\n\nNo parameters required.\n\n**Returns:**\nResults of conflict resolution including cleanup actions and recommendations.\n\n**Example Response:**\n\n```json\n{\n  \"status\": \"success\",\n  \"cleanup_results\": {\n    \"stale_instances_removed\": 2\n  },\n  \"conflicts_found\": [],\n  \"actions_taken\": [\"Removed 2 stale instance files\"],\n  \"recommendations\": [\"No active conflicts detected\"]\n}\n```\n\n### Additional Search Examples\n\n**Computer-related search:**\n\n```json\n{\n  \"name\": \"search_zim_file\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"query\": \"computer\",\n    \"limit\": 2\n  }\n}\n```\n\nResponse:\n```plain\nFound 39 matches for \"computer\", showing 1-2:\n\n## 1. Video game\nPath: Video_game\nSnippet: #  Video game First-generation _Pong_ console at the Computerspielemuseum Berlin\n---\nPlatforms\n\n## 2. Protein\nPath: Protein\nSnippet: #  Protein A representation of the 3D structure of the protein myoglobin showing turquoise Œ±-helices. This protein was the first to have its structure solved by X-ray crystallography. Toward the right-center among the coils, a prosthetic group called a heme group (shown in gray) with a bound oxygen molecule (red).\n```\n\n**Getting detailed content:**\n\n```json\n{\n  \"name\": \"get_zim_entry\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"entry_path\": \"Evolution\",\n    \"max_content_length\": 1500\n  }\n}\n```\n\nResponse:\n```plain\n# Evolution\n\nPath: Evolution\nType: text/html\n## Content\n\n#  Evolution\n\nPart of the Biology series on\n---\n****\nMechanisms and processes\n\n  * Adaptation\n  * Genetic drift\n  * Gene flow\n  * History of life\n  * Maladaptation\n  * Mutation\n  * Natural selection\n  * Neutral theory\n  * Population genetics\n  * Speciation\n\n... [Content truncated, total of 110,237 characters, only showing first 1,500 characters] ...\n```\n\n### üéØ Advanced Knowledge Retrieval Examples\n\n**Getting ZIM metadata:**\n\n```json\n{\n  \"name\": \"get_zim_metadata\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\"\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"entry_count\": 100000,\n  \"all_entry_count\": 120000,\n  \"article_count\": 80000,\n  \"media_count\": 20000,\n  \"metadata_entries\": {\n    \"Title\": \"Wikipedia (English)\",\n    \"Description\": \"Wikipedia articles in English\",\n    \"Language\": \"eng\",\n    \"Creator\": \"Kiwix\",\n    \"Date\": \"2025-08-15\"\n  }\n}\n```\n\n**Browsing a namespace:**\n\n```json\n{\n  \"name\": \"browse_namespace\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"namespace\": \"C\",\n    \"limit\": 5,\n    \"offset\": 0\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"namespace\": \"C\",\n  \"total_in_namespace\": 80000,\n  \"offset\": 0,\n  \"limit\": 5,\n  \"returned_count\": 5,\n  \"has_more\": true,\n  \"entries\": [\n    {\n      \"path\": \"C/Biology\",\n      \"title\": \"Biology\",\n      \"content_type\": \"text/html\",\n      \"preview\": \"Biology is the scientific study of life...\"\n    }\n  ]\n}\n```\n\n**Filtered search:**\n\n```json\n{\n  \"name\": \"search_with_filters\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"query\": \"evolution\",\n    \"namespace\": \"C\",\n    \"content_type\": \"text/html\",\n    \"limit\": 3\n  }\n}\n```\n\n**Getting article structure:**\n\n```json\n{\n  \"name\": \"get_article_structure\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"entry_path\": \"C/Evolution\"\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"title\": \"Evolution\",\n  \"path\": \"C/Evolution\",\n  \"content_type\": \"text/html\",\n  \"headings\": [\n    {\"level\": 1, \"text\": \"Evolution\", \"id\": \"evolution\"},\n    {\"level\": 2, \"text\": \"History\", \"id\": \"history\"},\n    {\"level\": 2, \"text\": \"Mechanisms\", \"id\": \"mechanisms\"}\n  ],\n  \"sections\": [\n    {\n      \"title\": \"Evolution\",\n      \"level\": 1,\n      \"content_preview\": \"Evolution is the change in heritable traits...\",\n      \"word_count\": 150\n    }\n  ],\n  \"word_count\": 5000\n}\n```\n\n**Getting search suggestions:**\n\n```json\n{\n  \"name\": \"get_search_suggestions\",\n  \"arguments\": {\n    \"zim_file_path\": \"C:\\\\zim\\\\wikipedia_en_100_2025-08.zim\",\n    \"partial_query\": \"bio\",\n    \"limit\": 5\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"partial_query\": \"bio\",\n  \"suggestions\": [\n    {\"text\": \"Biology\", \"path\": \"C/Biology\", \"type\": \"title_start_match\"},\n    {\"text\": \"Biochemistry\", \"path\": \"C/Biochemistry\", \"type\": \"title_start_match\"},\n    {\"text\": \"Biodiversity\", \"path\": \"C/Biodiversity\", \"type\": \"title_start_match\"}\n  ],\n  \"count\": 3\n}\n```\n\n### üîß Server Management and Diagnostics Examples\n\n**Getting server health:**\n\n```json\n{\n  \"name\": \"get_server_health\"\n}\n```\n\nResponse:\n```json\n{\n  \"status\": \"healthy\",\n  \"server_name\": \"openzim-mcp\",\n  \"uptime_info\": {\n    \"process_id\": 12345,\n    \"started_at\": \"2025-09-14T10:30:00\"\n  },\n  \"cache_performance\": {\n    \"enabled\": true,\n    \"size\": 15,\n    \"max_size\": 100,\n    \"hit_rate\": 0.85\n  },\n  \"instance_tracking\": {\n    \"active_instances\": 1,\n    \"conflicts_detected\": 0\n  }\n}\n```\n\n**Diagnosing server state:**\n\n```json\n{\n  \"name\": \"diagnose_server_state\"\n}\n```\n\nResponse:\n```json\n{\n  \"status\": \"healthy\",\n  \"server_info\": {\n    \"pid\": 12345,\n    \"server_name\": \"openzim-mcp\",\n    \"config_hash\": \"abc123def456...\"\n  },\n  \"conflicts\": [],\n  \"issues\": [],\n  \"recommendations\": [\"Server appears to be running normally. No issues detected.\"],\n  \"environment_checks\": {\n    \"directories_accessible\": true,\n    \"cache_functional\": true,\n    \"zim_files_found\": 5\n  }\n}\n```\n\n**Resolving server conflicts:**\n\n```json\n{\n  \"name\": \"resolve_server_conflicts\"\n}\n```\n\nResponse:\n```json\n{\n  \"status\": \"success\",\n  \"cleanup_results\": {\n    \"stale_instances_removed\": 2,\n    \"files_cleaned\": [\"/home/user/.openzim_mcp_instances/server_99999.json\"]\n  },\n  \"conflicts_found\": [],\n  \"actions_taken\": [\"Removed 2 stale instance files\"],\n  \"recommendations\": [\"No active conflicts detected after cleanup\"]\n}\n```\n\n---\n\n## üéØ ZIM Entry Retrieval Best Practices\n\n### Smart Retrieval System\n\nOpenZIM MCP implements an intelligent entry retrieval system that automatically handles path encoding inconsistencies common in ZIM files:\n\n**How It Works:**\n1. **Direct Access First**: Attempts to retrieve the entry using the provided path exactly as given\n2. **Automatic Fallback**: If direct access fails, automatically searches for the entry using various search terms\n3. **Path Mapping Cache**: Caches successful path mappings to improve performance for repeated access\n4. **Enhanced Error Guidance**: Provides clear guidance when entries cannot be found\n\n**Benefits for LLM Users:**\n- **Transparent Operation**: No need to understand ZIM path encoding complexities\n- **Single Tool Call**: Eliminates the need for manual search-first methodology\n- **Reliable Results**: Consistent success across different path formats (spaces vs underscores, URL encoding, etc.)\n- **Performance Optimized**: Cached mappings improve repeated access speed\n\n**Example Scenarios Handled Automatically:**\n- `A/Test Article` ‚Üí `A/Test_Article` (space to underscore conversion)\n- `C/Caf√©` ‚Üí `C/Caf%C3%A9` (URL encoding differences)\n- `A/Some-Page` ‚Üí `A/Some_Page` (hyphen to underscore conversion)\n\n### Usage Recommendations\n\n**For Direct Entry Access:**\n```json\n{\n  \"name\": \"get_zim_entry\",\n  \"arguments\": {\n    \"zim_file_path\": \"/path/to/file.zim\",\n    \"entry_path\": \"A/Article_Name\"\n  }\n}\n```\n\n**When Entry Not Found:**\nThe system will automatically provide guidance:\n```\nEntry not found: 'A/Article_Name'.\nThe entry path may not exist in this ZIM file.\nTry using search_zim_file() to find available entries,\nor browse_namespace() to explore the file structure.\n```\n\n---\n\n## ‚ö†Ô∏è Important Notes and Limitations\n\n### Content Length Requirements\n- The `max_content_length` parameter for `get_zim_entry` must be at least 1000 characters\n- Content longer than the specified limit will be truncated with a note showing the total character count\n\n### Search Behavior\n- Search results may include articles that contain the search terms in various contexts\n- Results are ranked by relevance but may not always be directly related to the primary meaning of the search term\n- Search snippets provide a preview of the content but may not show the exact location where the search term appears\n\n### File Format Support\n- Currently supports ZIM files (Zeno IMproved format)\n- Tested with Wikipedia ZIM files (e.g., `wikipedia_en_100_2025-08.zim`)\n- File paths must be properly escaped in JSON (use `\\\\` for Windows paths)\n\n---\n\n## üîÑ Multi-Server Instance Management\n\nOpenZIM MCP includes advanced multi-server instance tracking and conflict detection to ensure reliable operation when multiple server instances are running.\n\n### Instance Tracking Features\n\n- **Automatic Instance Registration**: Each server instance is automatically registered with a unique process ID and configuration hash\n- **Conflict Detection**: Detects when multiple servers with different configurations are accessing the same directories\n- **Stale Instance Cleanup**: Automatically identifies and cleans up orphaned instance files from terminated processes\n- **Configuration Validation**: Ensures all server instances use compatible configurations\n\n### Conflict Types\n\n1. **Configuration Mismatch**: Multiple servers with different settings accessing the same directories\n2. **Multiple Instances**: Multiple servers running simultaneously (may cause confusion)\n3. **Stale Instances**: Orphaned instance files from terminated processes\n\n### Automatic Conflict Warnings\n\nOpenZIM MCP automatically includes conflict warnings in search results and file listings when issues are detected:\n\n```plain\nüîç **Server Conflict Detected**\n‚ö†Ô∏è Configuration mismatch with server PID 12345. Search results may be inconsistent.\nüí° Use 'resolve_server_conflicts()' to fix these issues.\n```\n\n### Best Practices\n\n- Use `diagnose_server_state()` regularly to check for conflicts\n- Run `resolve_server_conflicts()` to clean up stale instances\n- Ensure all server instances use the same configuration when accessing shared directories\n- Monitor server health with `get_server_health()` for instance tracking information\n\n---\n\n## üîß Configuration\n\nOpenZIM MCP supports configuration through environment variables with the `OPENZIM_MCP_` prefix:\n\n```bash\n# Cache configuration\nexport OPENZIM_MCP_CACHE__ENABLED=true\nexport OPENZIM_MCP_CACHE__MAX_SIZE=200\nexport OPENZIM_MCP_CACHE__TTL_SECONDS=7200\n\n# Content configuration\nexport OPENZIM_MCP_CONTENT__MAX_CONTENT_LENGTH=200000\nexport OPENZIM_MCP_CONTENT__SNIPPET_LENGTH=2000\nexport OPENZIM_MCP_CONTENT__DEFAULT_SEARCH_LIMIT=20\n\n# Logging configuration\nexport OPENZIM_MCP_LOGGING__LEVEL=DEBUG\nexport OPENZIM_MCP_LOGGING__FORMAT=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n# Server configuration\nexport OPENZIM_MCP_SERVER_NAME=my_openzim_mcp_server\n```\n\n### Configuration Options\n\n| Setting | Default | Description |\n|---------|---------|-------------|\n| `OPENZIM_MCP_CACHE__ENABLED` | `true` | Enable/disable caching |\n| `OPENZIM_MCP_CACHE__MAX_SIZE` | `100` | Maximum cache entries |\n| `OPENZIM_MCP_CACHE__TTL_SECONDS` | `3600` | Cache TTL in seconds |\n| `OPENZIM_MCP_CONTENT__MAX_CONTENT_LENGTH` | `100000` | Max content length |\n| `OPENZIM_MCP_CONTENT__SNIPPET_LENGTH` | `1000` | Max snippet length |\n| `OPENZIM_MCP_CONTENT__DEFAULT_SEARCH_LIMIT` | `10` | Default search result limit |\n| `OPENZIM_MCP_LOGGING__LEVEL` | `INFO` | Logging level |\n| `OPENZIM_MCP_LOGGING__FORMAT` | `%(asctime)s - %(name)s - %(levelname)s - %(message)s` | Log message format |\n| `OPENZIM_MCP_SERVER_NAME` | `openzim-mcp` | Server instance name |\n\n---\n\n## üîí Security Features\n\n- **Path Traversal Protection**: Secure path validation prevents access outside allowed directories\n- **Input Sanitization**: All user inputs are validated and sanitized\n- **Resource Management**: Proper cleanup of ZIM archive resources\n- **Error Handling**: Sanitized error messages prevent information disclosure\n- **Type Safety**: Full type annotations prevent type-related vulnerabilities\n\n---\n\n## üöÄ Performance Features\n\n- **Intelligent Caching**: LRU cache with TTL for frequently accessed content\n- **Resource Pooling**: Efficient ZIM archive management\n- **Optimized Content Processing**: Fast HTML to text conversion\n- **Lazy Loading**: Components initialized only when needed\n- **Memory Management**: Proper cleanup and resource management\n\n---\n\n## üß™ Testing\n\nThe project includes comprehensive testing with 90%+ coverage using both mock data and real ZIM files:\n\n### Test Categories\n\n- **Unit Tests**: Individual component testing with mocks\n- **Integration Tests**: End-to-end functionality testing with real ZIM files\n- **Security Tests**: Path traversal and input validation testing\n- **Performance Tests**: Cache and resource management testing\n- **Format Compatibility**: Testing with various ZIM file formats and versions\n- **Error Handling**: Testing with invalid and malformed ZIM files\n\n### Test Infrastructure\n\nOpenZIM MCP uses a hybrid testing approach:\n\n1. **Mock-based tests**: Fast unit tests using mocked libzim components\n2. **Real ZIM file tests**: Integration tests using official zim-testing-suite files\n3. **Automatic test data management**: Download and organize test files as needed\n\n### Test Data Sources\n\n- **Built-in test data**: Basic test files included in the repository\n- **zim-testing-suite integration**: Official test files from the OpenZIM project\n- **Environment variable support**: `ZIM_TEST_DATA_DIR` for custom test data locations\n\n```bash\n# Run tests with coverage report\nmake test-cov\n\n# View coverage report\nopen htmlcov/index.html\n\n# Run comprehensive tests with real ZIM files\nmake test-with-zim-data\n```\n\n### Test Markers\n\nTests are organized with pytest markers:\n\n- `@pytest.mark.requires_zim_data`: Tests requiring ZIM test data files\n- `@pytest.mark.integration`: Integration tests\n- `@pytest.mark.slow`: Long-running tests\n\n---\n\n## üìà Monitoring\n\nOpenZIM MCP provides built-in monitoring capabilities:\n\n- **Health Checks**: Server health and status monitoring\n- **Cache Metrics**: Cache hit rates and performance statistics\n- **Structured Logging**: JSON-formatted logs for easy parsing\n- **Error Tracking**: Comprehensive error logging and tracking\n\n---\n\n## üîÑ Versioning\n\nThis project uses [Semantic Versioning](https://semver.org/) with automated version management through [release-please](https://github.com/googleapis/release-please).\n\n### Automated Releases\n\nVersion bumps and releases are automated based on [Conventional Commits](https://www.conventionalcommits.org/):\n\n- **`feat:`** - New features (minor version bump)\n- **`fix:`** - Bug fixes (patch version bump)\n- **`feat!:`** or **`BREAKING CHANGE:`** - Breaking changes (major version bump)\n- **`perf:`** - Performance improvements (patch version bump)\n- **`docs:`**, **`style:`**, **`refactor:`**, **`test:`**, **`chore:`** - No version bump\n\n### Release Process\n\nThe project uses an **improved, consolidated release system** with automatic validation:\n\n1. **Automatic** (Recommended): Push conventional commits ‚Üí Release Please creates PR ‚Üí Merge PR ‚Üí Automatic release\n2. **Manual**: Use GitHub Actions UI for direct control over releases\n3. **Emergency**: Push tags directly for critical fixes\n\n**Key Features:**\n- ‚úÖ **Zero-touch releases** from main branch\n- ‚úÖ **Automatic version synchronization** validation\n- ‚úÖ **Comprehensive testing** before every release\n- ‚úÖ **Improved error handling** and rollback capabilities\n- ‚úÖ **Branch protection** prevents broken releases\n\nFor detailed instructions, see [Release Process Guide](docs/RELEASE_PROCESS_GUIDE.md).\n\n### Commit Message Format\n\n```\n<type>[optional scope]: <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n**Examples:**\n```bash\nfeat: add search suggestions endpoint\nfix: resolve path traversal vulnerability\nfeat!: change API response format\ndocs: update installation instructions\n```\n\n---\n\n## ü§ù Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests (`make check`)\n5. **Use conventional commit messages** (`git commit -m 'feat: add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Development Guidelines\n\n- Follow PEP 8 style guidelines\n- Add type hints to all functions\n- Write tests for new functionality\n- Update documentation as needed\n- **Use conventional commit messages** for automatic versioning\n- Ensure all tests pass before submitting\n\n---\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n## üôè Acknowledgments\n\n- [Kiwix](https://www.kiwix.org/) for the ZIM format and libzim library\n- [MCP](https://modelcontextprotocol.io/) for the Model Context Protocol\n- The open-source community for the excellent libraries used in this project\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "CheMiguel23--MemoryMesh": {
      "owner": "CheMiguel23",
      "name": "MemoryMesh",
      "url": "https://github.com/CheMiguel23/MemoryMesh",
      "imageUrl": "",
      "description": "Enhanced graph-based memory with a focus on AI role-play and story generation",
      "stars": 301,
      "forks": 39,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:31:59Z",
      "readme_content": "# MemoryMesh\n[![Release](https://img.shields.io/badge/Release-v0.2.8-blue.svg)](./CHANGELOG.md)\n[![smithery badge](https://smithery.ai/badge/memorymesh)](https://smithery.ai/server/memorymesh)\n![TypeScript](https://img.shields.io/badge/TypeScript-007ACC.svg?logo=typescript&logoColor=white)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Stars](https://img.shields.io/github/stars/CheMiguel23/MemoryMesh.svg?style=social)\n\nMemoryMesh is a knowledge graph server designed for AI models, with a focus on text-based RPGs and interactive storytelling. It helps AI maintain consistent, structured memory across conversations, enabling richer and more dynamic interactions.\n\n*The project is based on the [Knowledge Graph Memory Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory) from the MCP servers repository and retains its core functionality.*\n\n<a href=\"https://glama.ai/mcp/servers/kf6n6221pd\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kf6n6221pd/badge\" alt=\"MemoryMesh MCP server\" /></a>\n\n## IMPORTANT\nSince `v0.2.7` the default location of schemas was changed to `dist/data/schemas`.\nThis location is not expected to change in the future, but if you are updating from a previous version, make sure to move your schema files to the new location.\n\n## Quick Links\n\n*   [Installation](#installation)\n*   [Example](#example)\n*   [SchemaManager Guide Discussion](https://github.com/CheMiguel23/MemoryMesh/discussions/3)\n*   [MemoryViewer Guide Discussion](https://github.com/CheMiguel23/MemoryMesh/discussions/15)\n\n## Overview\n\nMemoryMesh is a local knowledge graph server that empowers you to build and manage structured information for AI models. While particularly well-suited for text-based RPGs, its adaptable design makes it useful for various applications, including social network simulations, organizational planning, or any scenario involving structured data.\n\n### Key Features\n\n*   **Dynamic Schema-Based Tools:** Define your data structure with schemas, and MemoryMesh automatically generates tools for adding, updating, and deleting data.\n*   **Intuitive Schema Design:** Create schemas that guide the AI in generating and connecting nodes, using required fields, enumerated types, and relationship definitions.\n*   **Metadata for AI Guidance:**  Use metadata to provide context and structure, helping the AI understand the meaning and relationships within your data.\n*   **Relationship Handling:** Define relationships within your schemas to encourage the AI to create connections (edges) between related data points (nodes).\n*   **Informative Feedback:**  Provides error feedback to the AI, enabling it to learn from mistakes and improve its interactions with the knowledge graph.\n*   **Event Support:** An event system tracks operations, providing insights into how the knowledge graph is being modified.\n\n#### Nodes\n\nNodes represent entities or concepts within the knowledge graph. Each node has:\n\n* `name`: A unique identifier.\n* `nodeType`: The type of the node (e.g., `npc`, `artifact`, `location`), defined by your schemas.\n* `metadata`: An array of strings providing descriptive details about the node.\n* `weight`: (Optional) A numerical value between 0 and 1 representing the strength of the relationship, defaulting to 1.\n\n**Example Node:**\n\n```json\n    {\n      \"name\": \"Aragorn\",\n      \"nodeType\": \"player_character\",\n      \"metadata\": [\n        \"Race: Human\",\n        \"Class: Ranger\",\n        \"Skills: Tracking, Swordsmanship\",\n        \"Affiliation: Fellowship of the Ring\"\n      ]\n    }\n```\n\n#### Edges\n\nEdges represent relationships between nodes. Each edge has:\n\n* `from`: The name of the source node.\n* `to`: The name of the target node.\n* `edgeType`: The type of relationship (e.g., `owns`, `located_in`).\n\n```json\n{\n  \"from\": \"Aragorn\",\n  \"to\": \"And√∫ril\",\n  \"edgeType\": \"owns\"\n}\n```\n\n#### Schemas\n\nSchemas are the heart of MemoryMesh. They define the structure of your data and drive the automatic generation of tools.\n\n##### Schema File Location\n\nPlace your schema files (`.schema.json`) in the `dist/data/schemas` directory of your built MemoryMesh project. MemoryMesh will automatically detect and process these files on startup.\n\n##### Schema Structure\n\nFile name: `[name].schema.json`. For example, for a schema defining an 'npc', the filename would be `add_npc.schema.json`.\n\n* `name` - Identifier for the schema and node type within the memory. **IMPORTANT**: The schema‚Äôs name *must* start with `add_` to be recognized.\n* `description` - Used as the description for the `add_<name>` tool, providing context for the AI. *(The `delete` and `update` tools have a generic description)*\n* `properties` - Each property includes its type, description, and additional constraints.\n    * `property`\n        * `type` - Supported values are `string` or `array`.\n        * `description` - Helps guide the AI on the entity‚Äôs purpose.\n        * `required` - Boolean. If `true`, the **AI is forced** to provide this property when creating a node.\n        * `enum` - An array of strings. If present, the **AI must choose** one of the given options.\n        * `relationship` - Defines a connection to another node. If a property is required and has a relationship, the **AI will always create** both the node and the corresponding edge.\n            * `edgeType` - Type of the relationship to be created.\n            * `description` - Helps guide the AI on the relationship‚Äôs purpose.\n* `additionalProperties` - Boolean. If `true`, allows the AI to add extra attributes beyond those defined as required or optional.\n\n##### Example Schema (add_npc.schema.json):\n\n```json\n{\n  \"name\": \"add_npc\",\n  \"description\": \"Schema for adding an NPC to the memory\" ,\n  \"properties\": {\n    \"name\": {\n      \"type\": \"string\",\n      \"description\": \"A unique identifier for the NPC\",\n      \"required\": true\n    },\n    \"race\": {\n      \"type\": \"string\",\n      \"description\": \"The species or race of the NPC\",\n      \"required\": true,\n      \"enum\": [\n        \"Human\",\n        \"Elf\",\n        \"Dwarf\",\n        \"Orc\",\n        \"Goblin\"\n      ]\n    },\n    \"currentLocation\": {\n      \"type\": \"string\",\n      \"description\": \"The current location of the NPC\",\n      \"required\": true,\n      \"relationship\": {\n        \"edgeType\": \"located_in\",\n        \"description\": \"The current location of the NPC\"\n      }\n    }\n  },\n  \"additionalProperties\": true\n}\n```\n\nBased on this schema, MemoryMesh automatically creates:\n* add_npc: To add new NPC nodes.\n* update_npc: To modify existing NPC nodes.\n* delete_npc: To remove NPC nodes.\n\nMemoryMesh includes 11 pre-built schemas designed for text-based RPGs, providing a ready-to-use foundation for game development.\n\n##### SchemaManager Tool\n\nMemoryMesh includes a [SchemaManager tool](https://github.com/CheMiguel23/MemoryMesh/blob/main/SchemaManager.html) to simplify schema creation and editing. It provides a visual interface, making it easy to define your data structures without writing JSON directly.\n\n<img width=\"370\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e8f0c808-2ff6-48da-ac7c-cf51aebde7b8\">\n\n### Dynamic Tools\n\nMemoryMesh simplifies interaction with your knowledge graph through **dynamic tools**. These tools are not manually coded but are **automatically generated** directly from your **schema definitions**. This means that when you define the structure of your data using schemas, MemoryMesh intelligently creates a set of tools tailored to work with that specific data structure.\n\n**Think of it like this:** You provide a blueprint (the schema), and MemoryMesh automatically constructs the necessary tools to build, modify, and remove elements based on that blueprint.\n\n#### How does it work behind the scenes?\n\nMemoryMesh has an intelligent system that reads your schema definitions. It analyzes the structure you've defined, including the properties of your entities and their relationships. Based on this analysis, it automatically creates a set of tools for each entity type:\n\n*   **`add_<entity>`:**  A tool for creating new instances of an entity.\n*   **`update_<entity>`:** A tool for modifying existing entities.\n*   **`delete_<entity>`:** A tool for removing entities.\n\nThese tools are then made available through a central hub within MemoryMesh, ensuring they can be easily accessed and used by any connected client or AI.\n\n**In essence, MemoryMesh's dynamic tool system provides a powerful and efficient way to manage your knowledge graph, freeing you to focus on the content and logic of your application rather than the underlying mechanics of data manipulation.**\n\n### Memory file\n\nBy default, data is stored in a JSON file in `dist/data/memory.json`.\n\n#### Memory Viewer\n\nThe Memory Viewer is a separate tool designed to help you visualize and inspect the contents of the knowledge graph managed by MemoryMesh. It provides a user-friendly interface for exploring nodes, edges, and their properties.\n\n##### Key Features:\n* Graph Visualization: View the knowledge graph as an interactive node-link diagram.\n* Node Inspection: Select nodes to see their nodeType, metadata, and connected edges.\n* Edge Exploration: Examine relationships between nodes, including edgeType and direction.\n* Search and Filtering: Quickly find specific nodes or filter them by type.\n* Table View: Allows you to easily find and inspect specific nodes and edges, or all of them at once.\n* Raw JSON View: Allows you to view the raw JSON data from the memory file.\n* Stats Panel: Provides key metrics and information about the knowledge graph: total nodes, total edges, node types, and edge types.\n* Search and Filter: Allows you to filter by node type or edge type and filter whether to show nodes, edges, or both.\n\n##### Accessing the Memory Viewer\nThe Memory Viewer is a standalone web application. [Memory Viewer discussion](https://github.com/CheMiguel23/MemoryMesh/discussions/15)\n\n##### Using the Memory Viewer\n* Select Memory File: In the Memory Viewer, click the \"Select Memory File\" button.\n* Choose File: Navigate to your MemoryMesh project directory and select the `memory.json` file (located in `dist/data/memory.json` by default).\n* Explore: The Memory Viewer will load and display the contents of your knowledge graph.\n\n## Memory Flow\n\n![image](https://github.com/user-attachments/assets/27519003-c1e6-448a-9fdb-cd0a0009f67d)\n\n## Prompt\n\nFor optimal results, use Claude's \"Projects\" feature with custom instructions. Here's an example of a prompt you can start with:\n\n```\nYou are a helpful AI assistant managing a knowledge graph for a text-based RPG. You have access to the following tools: add_npc, update_npc, delete_npc, add_location, update_location, delete_location, and other tools for managing the game world.\n\nWhen the user provides input, first process it using your available tools to update the knowledge graph. Then, respond in a way that is appropriate for a text-based RPG.\n```\n\nYou can also instruct the AI to perform specific actions directly in the chat.\n\nExperiment with different prompts to find what works best for your use case!\n\n### Example\n1. A [simple example](https://pastebin.com/0HvKg5FZ) with custom instructions.\n2. An example for the sake of example, with visualization _(NOT part of the functionality)_\n\n> Add a couple of cities, some npcs, couple locations around the city to explore, hide an artifact or two somewhere\n\n![image](https://github.com/user-attachments/assets/508d5903-2896-4665-a892-cdb7b81dfba6)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MemoryMesh for Claude Desktop automatically via [Smithery](https://smithery.ai/server/memorymesh):\n\n```bash\nnpx -y @smithery/cli install memorymesh --client claude\n```\n\n### Prerequisites\n\n*   **Node.js:** Version 18 or higher. You can download it from [nodejs.org](https://nodejs.org/).\n*   **npm:**  Usually included with Node.js.\n*   **Claude for Desktop:**  Make sure you have the latest version installed from [claude.ai/download](https://claude.ai/download).\n\n### Installation Steps\n\n1. **Clone the Repository:**\n\n    ```bash\n    git clone https://github.com/CheMiguel23/memorymesh.git\n    cd memorymesh\n    ```\n\n2. **Install Dependencies:**\n\n    ```bash\n    npm install\n    ```\n\n3. **Build the Project:**\n\n    ```bash\n    npm run build\n    ```\n   This command compiles the TypeScript code into JavaScript in the `dist` directory and copies sample schema and data files into it as well.\n\n4. **Verify File Copy (Optional):**\n\n    *   The build process should automatically copy the `data` folder to `dist`.\n    *   **Check** that `dist/data` exists and contains `.json` files. Also verify that `dist/data/schemas` exists and contains `.schema.json` files.\n\n5. **Configure Claude Desktop:**\n\n   Open your Claude Desktop configuration file:\n\n    * **macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`\n    * **Windows:** `%APPDATA%\\Claude\\claude_desktop_config.json`\n    * Add an entry for `memorymesh` to the `mcpServers` section. You can choose **one** of the following configuration options:\n\n    ```json\n    \"mcpServers\": {\n      \"memorymesh\": {\n        \"command\": \"node\", \n        \"args\": [\"/ABSOLUTE/PATH/TO/YOUR/PROJECT/memorymesh/dist/index.js\"]\n      }\n    }\n    ```\n\n    *   Replace `/ABSOLUTE/PATH/TO/YOUR/PROJECT/` with the **actual absolute path** to your `memorymesh` project directory.\n    *   **Example (macOS):**\n        ```json\n        \"command\": \"node\",\n        \"args\": [\"/Users/yourusername/Projects/memorymesh/dist/index.js\"]\n        ```\n    *   **Example (Windows):**\n        ```json\n        \"command\": \"node\",\n        \"args\": [\"C:\\\\Projects\\\\memorymesh\\\\dist\\\\index.js\"]\n        ```\n\n6. **Restart Claude Desktop:** Completely restart Claude Desktop for the changes to take effect.\n\n### Verify Installation\n\n1. Start Claude Desktop.\n2. Open a new chat.\n3. Look for the MCP plugin icon <img src=\"https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-plug-icon.svg\"/> in the top-right corner. If it's there, your configuration is likely correct.\n4. Click the <img src=\"https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-plug-icon.svg\"/> icon. You should see \"memorymesh\" in the list of connected servers.\n5. Click the <img src=\"https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg\"/> icon. If you see tools listed (e.g., `add_npc`, `update_npc`, etc.), your server is working and exposing tools correctly.\n\n### Updating\nBefore updates, make sure to back up your `dist/data` directory to avoid losing your memory data.\n\n### Troubleshooting\n\n*   **Server not appearing in Claude:**\n    *   Double-check the paths in your `claude_desktop_config.json`. Make sure they are absolute paths and correct.\n    *   Verify that the `dist` directory exists and contains the compiled JavaScript files, including `index.js`.\n    *   Check the Claude Desktop logs for errors:\n        *   **macOS:** `~/Library/Logs/Claude/mcp-server-memorymesh.log` (and `mcp.log`)\n        *   **Windows:** (Likely in a `Logs` folder under `%AppData%\\Claude`)\n\n*   **Tools not showing up:**\n    *   Make sure your `npm run build` command completed without errors.\n    *   Verify that your schema files are correctly placed in `dist/data/schemas` and follow the correct naming convention (`add_[entity].schema.json`).\n    *   Check your server's console output or logs for any errors during initialization.\n\n## Advanced Configuration\nMemoryMesh offers several ways to customize its behavior beyond the basic setup:\n\n### Variables\nYou can override default settings using in `/config/config.ts`\n* MEMORY_FILE: Specifies the path to the JSON file used for storing the knowledge graph data. (Default: `dist/data/memory.json`)\n* SCHEMAS_DIR: Path to schema files directory. (Default: `dist/data/schemas/memory.json`)\n\n## Limitations\n\n1. **Node Deletion:** The AI may be hesitant to delete nodes from the knowledge graph. Encourage it through prompts if needed.\n\n## Contribution\n\nContributions, feedback, and ideas are welcome!\nThis project is a personal exploration into integrating structured data with AI reasoning capabilities. Contributions, feedback, and ideas are welcome to push it further or inspire new projects.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "entanglr--zettelkasten-mcp": {
      "owner": "entanglr",
      "name": "zettelkasten-mcp",
      "url": "https://github.com/entanglr/zettelkasten-mcp",
      "imageUrl": "",
      "description": "A Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, and search atomic notes through Claude and other MCP-compatible clients.",
      "stars": 97,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T13:22:47Z",
      "readme_content": "# Zettelkasten MCP Server\n\nA Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, explore and synthesize atomic notes through Claude and other MCP-compatible clients.\n\n## What is Zettelkasten?\n\nThe Zettelkasten method is a knowledge management system developed by German sociologist Niklas Luhmann, who used it to produce over 70 books and hundreds of articles. It consists of three core principles:\n\n1. **Atomicity**: Each note contains exactly one idea, making it a discrete unit of knowledge\n2. **Connectivity**: Notes are linked together to create a network of knowledge, with meaningful relationships between ideas\n3. **Emergence**: As the network grows, new patterns and insights emerge that weren't obvious when the individual notes were created\n\nWhat makes the Zettelkasten approach powerful is how it enables exploration in multiple ways:\n\n- **Vertical exploration**: dive deeper into specific topics by following connections within a subject area.\n- **Horizontal exploration**: discover unexpected relationships between different fields by traversing links that cross domains.\n\nThis structure invites serendipitous discoveries as you follow trails of thought from note to note, all while keeping each piece of information easily accessible through its unique identifier. Luhmann called his system his \"second brain\" or \"communication partner\" - this digital implementation aims to provide similar benefits through modern technology.\n\n## Features\n\n- Create atomic notes with unique timestamp-based IDs\n- Link notes bidirectionally to build a knowledge graph\n- Tag notes for categorical organization\n- Search notes by content, tags, or links\n- Use markdown format for human readability and editing\n- Integrate with Claude through MCP for AI-assisted knowledge management\n- Dual storage architecture (see below)\n- Synchronous operation model for simplified architecture\n\n## Examples\n\n- Knowledge creation: [A small Zettelkasten knowledge network about the Zettelkasten method itself](https://github.com/entanglr/zettelkasten-mcp/discussions/5)\n\n## Note Types\n\nThe Zettelkasten MCP server supports different types of notes:\n\n|Type|Handle|Description|\n|---|---|---|\n|**Fleeting notes**|`fleeting`|Quick, temporary notes for capturing ideas|\n|**Literature notes**|`literature`|Notes from reading material|\n|**Permanent notes**|`permanent`|Well-formulated, evergreen notes|\n|**Structure notes**|`structure`|Index or outline notes that organize other notes|\n|**Hub notes**|`hub`|Entry points to the Zettelkasten on key topics|\n\n## Link Types\n\nThe Zettelkasten MCP server uses a comprehensive semantic linking system that creates meaningful connections between notes. Each link type represents a specific relationship, allowing for a rich, multi-dimensional knowledge graph.\n\n| Primary Link Type | Inverse Link Type | Relationship Description |\n|-------------------|-------------------|--------------------------|\n| `reference` | `reference` | Simple reference to related information (symmetric relationship) |\n| `extends` | `extended_by` | One note builds upon or develops concepts from another |\n| `refines` | `refined_by` | One note clarifies or improves upon another |\n| `contradicts` | `contradicted_by` | One note presents opposing views to another |\n| `questions` | `questioned_by` | One note poses questions about another |\n| `supports` | `supported_by` | One note provides evidence for another |\n| `related` | `related` | Generic relationship (symmetric relationship) |\n\n## Prompting\n\nTo ensure maximum effectiveness, we recommend using a system prompt (\"project instructions\"), project knowledge, and an appropriate chat prompt when asking the LLM to process information, or explore or synthesize your Zettelkasten notes. The `docs` directory in this repository contains the necessary files to get you started:\n\n### System prompts\n\nPick one:\n\n- [system-prompt.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/system/system-prompt.md)\n- [system-prompt-with-protocol.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/system/system-prompt-with-protocol.md)\n\n### Project knowledge\n\nFor end users:\n\n- [zettelkasten-methodology-technical.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/user/zettelkasten-methodology-technical.md)\n- [link-types-in-zettelkasten-mcp-server.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/user/link-types-in-zettelkasten-mcp-server.md)\n- (more info relevant to your project)\n\n### Chat Prompts\n\n- [chat-prompt-knowledge-creation.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/chat/chat-prompt-knowledge-creation.md)\n- [chat-prompt-knowledge-creation-batch.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/chat/chat-prompt-knowledge-creation-batch.md)\n- [chat-prompt-knowledge-exploration.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/chat/chat-prompt-knowledge-exploration.md)\n- [chat-prompt-knowledge-synthesis.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/chat/chat-prompt-knowledge-synthesis.md)\n\n### Project knowledge (dev)\n\nFor developers and contributors:\n\n- [Example - A simple MCP server.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/dev/Example%20-%20A%20simple%20MCP%20server%20that%20exposes%20a%20website%20fetching%20tool.md)\n- [MCP Python SDK-README.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/dev/MCP%20Python%20SDK-README.md)\n- [llms-full.txt](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/dev/llms-full.txt)\n\nNB: Optionally include the source code with a tool like [repomix](https://github.com/yamadashy/repomix).\n\n## Storage Architecture\n\nThis system uses a dual storage approach:\n\n1. **Markdown Files**: All notes are stored as human-readable Markdown files with YAML frontmatter for metadata. These files are the **source of truth** and can be:\n   - Edited directly in any text editor\n   - Placed under version control (Git, etc.)\n   - Backed up using standard file backup procedures\n   - Shared or transferred like any other text files\n\n2. **SQLite Database**: Functions as an indexing layer that:\n   - Facilitates efficient querying and search operations\n   - Enables Claude to quickly traverse the knowledge graph\n   - Maintains relationship information for faster link traversal\n   - Is automatically rebuilt from Markdown files when needed\n\nIf you edit Markdown files directly outside the system, you'll need to run the `zk_rebuild_index` tool to update the database. The database itself can be deleted at any time - it will be regenerated from your Markdown files.\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/entanglr/zettelkasten-mcp.git\ncd zettelkasten-mcp\n\n# Create a virtual environment with uv\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\nuv add \"mcp[cli]\"\n\n# Install dev dependencies\nuv sync --all-extras\n```\n\n## Configuration\n\nCreate a `.env` file in the project root by copying the example:\n\n```bash\ncp .env.example .env\n```\n\nThen edit the file to configure your connection parameters.\n\n## Usage\n\n### Starting the Server\n\n```bash\npython -m zettelkasten_mcp.main\n```\n\nOr with explicit configuration:\n\n```bash\npython -m zettelkasten_mcp.main --notes-dir ./data/notes --database-path ./data/db/zettelkasten.db\n```\n\n### Connecting to Claude Desktop\n\nAdd the following configuration to your Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"zettelkasten\": {\n      \"command\": \"/absolute/path/to/zettelkasten-mcp/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"zettelkasten_mcp.main\"\n      ],\n      \"env\": {\n        \"ZETTELKASTEN_NOTES_DIR\": \"/absolute/path/to/zettelkasten-mcp/data/notes\",\n        \"ZETTELKASTEN_DATABASE_PATH\": \"/absolute/path/to/zettelkasten-mcp/data/db/zettelkasten.db\",\n        \"ZETTELKASTEN_LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\n## Available MCP Tools\n\nAll tools have been prefixed with `zk_` for better organization:\n\n| Tool | Description |\n|---|---|\n| `zk_create_note` | Create a new note with a title, content, and optional tags |\n| `zk_get_note` | Retrieve a specific note by ID or title |\n| `zk_update_note` | Update an existing note's content or metadata |\n| `zk_delete_note` | Delete a note |\n| `zk_create_link` | Create links between notes |\n| `zk_remove_link` | Remove links between notes |\n| `zk_search_notes` | Search for notes by content, tags, or links |\n| `zk_get_linked_notes` | Find notes linked to a specific note |\n| `zk_get_all_tags` | List all tags in the system |\n| `zk_find_similar_notes` | Find notes similar to a given note |\n| `zk_find_central_notes` | Find notes with the most connections |\n| `zk_find_orphaned_notes` | Find notes with no connections |\n| `zk_list_notes_by_date` | List notes by creation/update date |\n| `zk_rebuild_index` | Rebuild the database index from Markdown files |\n\n## Project Structure\n\n```\nzettelkasten-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ zettelkasten_mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ models/       # Data models\n‚îÇ       ‚îú‚îÄ‚îÄ storage/      # Storage layer\n‚îÇ       ‚îú‚îÄ‚îÄ services/     # Business logic\n‚îÇ       ‚îî‚îÄ‚îÄ server/       # MCP server implementation\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ notes/            # Note storage (Markdown files)\n‚îÇ   ‚îî‚îÄ‚îÄ db/               # Database for indexing\n‚îú‚îÄ‚îÄ tests/                # Test suite\n‚îú‚îÄ‚îÄ .env.example          # Environment variable template\n‚îî‚îÄ‚îÄ README.md\n```\n\n## Tests\n\nComprehensive test suite for Zettelkasten MCP covering all layers of the application from models to the MCP server implementation.\n\n### How to Run the Tests\n\nFrom the project root directory, run:\n\n#### Using pytest directly\n```bash\npython -m pytest -v tests/\n```\n\n#### Using UV\n```bash\nuv run pytest -v tests/\n```\n\n#### With coverage report\n```bash\nuv run pytest --cov=zettelkasten_mcp --cov-report=term-missing tests/\n```\n\n#### Running a specific test file\n```bash\nuv run pytest -v tests/test_models.py\n```\n\n#### Running a specific test class\n```bash\nuv run pytest -v tests/test_models.py::TestNoteModel\n```\n\n#### Running a specific test function\n```bash\nuv run pytest -v tests/test_models.py::TestNoteModel::test_note_validation\n```\n\n### Tests Directory Structure\n\n```\ntests/\n‚îú‚îÄ‚îÄ conftest.py - Common fixtures for all tests\n‚îú‚îÄ‚îÄ test_integration.py - Integration tests for the entire system\n‚îú‚îÄ‚îÄ test_mcp_server.py - Tests for MCP server tools\n‚îú‚îÄ‚îÄ test_models.py - Tests for data models\n‚îú‚îÄ‚îÄ test_note_repository.py - Tests for note repository\n‚îú‚îÄ‚îÄ test_search_service.py - Tests for search service\n‚îú‚îÄ‚îÄ test_semantic_links.py - Tests for semantic linking\n‚îî‚îÄ‚îÄ test_zettel_service.py - Tests for zettel service\n```\n\n## Important Notice\n\n**‚ö†Ô∏è USE AT YOUR OWN RISK**: This software is experimental and provided as-is without warranty of any kind. While efforts have been made to ensure data integrity, it may contain bugs that could potentially lead to data loss or corruption. Always back up your notes regularly and use caution when testing with important information.\n\n## Credit Where Credit's Due\n\nThis MCP server was crafted with the assistance of Claude, who helped organize the atomic thoughts of this project into a coherent knowledge graph. Much like a good Zettelkasten system, Claude connected the dots between ideas that might otherwise have remained isolated. Unlike Luhmann's paper-based system, however, Claude didn't require 90,000 index cards to be effective.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "GistPad-MCP](https:--/github.com/lostintangent/gistpad-mcp) üìá üè† - Use GitHub Gists to manage and access your personal knowledge, daily notes, and reusable prompts. This acts as a companion to https://gistpad.dev and the [GistPad VS Code extension": {
      "owner": "GistPad-MCP](https:",
      "name": "/github.com/lostintangent/gistpad-mcp) üìá üè† - Use GitHub Gists to manage and access your personal knowledge, daily notes, and reusable prompts. This acts as a companion to https://gistpad.dev and the [GistPad VS Code extension",
      "url": "https://aka.ms/gistpad",
      "imageUrl": "",
      "description": "[graphlit-mcp-server](https://github.com/graphlit/graphlit-mcp-server) üìá ‚òÅÔ∏è - Ingest anything from Slack, Discord, websites, Google Drive, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf or Cline.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0
    },
    "hannesrudolph--mcp-ragdocs": {
      "owner": "hannesrudolph",
      "name": "mcp-ragdocs",
      "url": "https://github.com/hannesrudolph/mcp-ragdocs",
      "imageUrl": "",
      "description": "An MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context",
      "stars": 228,
      "forks": 27,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:27Z",
      "readme_content": "# RAG Documentation MCP Server\n\nAn MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context.\n\n<a href=\"https://glama.ai/mcp/servers/54hsrjhmq9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/54hsrjhmq9/badge\" alt=\"mcp-ragdocs MCP server\" /></a>\n\n## Features\n\n- Vector-based documentation search and retrieval\n- Support for multiple documentation sources\n- Semantic search capabilities\n- Automated documentation processing\n- Real-time context augmentation for LLMs\n\n## Tools\n\n### search_documentation\nSearch through stored documentation using natural language queries. Returns matching excerpts with context, ranked by relevance.\n\n**Inputs:**\n- `query` (string): The text to search for in the documentation. Can be a natural language query, specific terms, or code snippets.\n- `limit` (number, optional): Maximum number of results to return (1-20, default: 5). Higher limits provide more comprehensive results but may take longer to process.\n\n### list_sources\nList all documentation sources currently stored in the system. Returns a comprehensive list of all indexed documentation including source URLs, titles, and last update times. Use this to understand what documentation is available for searching or to verify if specific sources have been indexed.\n\n### extract_urls\nExtract and analyze all URLs from a given web page. This tool crawls the specified webpage, identifies all hyperlinks, and optionally adds them to the processing queue.\n\n**Inputs:**\n- `url` (string): The complete URL of the webpage to analyze (must include protocol, e.g., https://). The page must be publicly accessible.\n- `add_to_queue` (boolean, optional): If true, automatically add extracted URLs to the processing queue for later indexing. Use with caution on large sites to avoid excessive queuing.\n\n### remove_documentation\nRemove specific documentation sources from the system by their URLs. The removal is permanent and will affect future search results.\n\n**Inputs:**\n- `urls` (string[]): Array of URLs to remove from the database. Each URL must exactly match the URL used when the documentation was added.\n\n### list_queue\nList all URLs currently waiting in the documentation processing queue. Shows pending documentation sources that will be processed when run_queue is called. Use this to monitor queue status, verify URLs were added correctly, or check processing backlog.\n\n### run_queue\nProcess and index all URLs currently in the documentation queue. Each URL is processed sequentially, with proper error handling and retry logic. Progress updates are provided as processing occurs. Long-running operations will process until the queue is empty or an unrecoverable error occurs.\n\n### clear_queue\nRemove all pending URLs from the documentation processing queue. Use this to reset the queue when you want to start fresh, remove unwanted URLs, or cancel pending processing. This operation is immediate and permanent - URLs will need to be re-added if you want to process them later.\n\n## Usage\n\nThe RAG Documentation tool is designed for:\n\n- Enhancing AI responses with relevant documentation\n- Building documentation-aware AI assistants\n- Creating context-aware tooling for developers\n- Implementing semantic documentation search\n- Augmenting existing knowledge bases\n\n## Configuration\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@hannesrudolph/mcp-ragdocs\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"QDRANT_URL\": \"\",\n        \"QDRANT_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nYou'll need to provide values for the following environment variables:\n- `OPENAI_API_KEY`: Your OpenAI API key for embeddings generation\n- `QDRANT_URL`: URL of your Qdrant vector database instance\n- `QDRANT_API_KEY`: API key for authenticating with Qdrant\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Acknowledgments\n\nThis project is a fork of [qpd-v/mcp-ragdocs](https://github.com/qpd-v/mcp-ragdocs), originally developed by qpd-v. The original project provided the foundation for this implementation.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "jinzcdev--markmap-mcp-server": {
      "owner": "jinzcdev",
      "name": "markmap-mcp-server",
      "url": "https://github.com/jinzcdev/markmap-mcp-server",
      "imageUrl": "",
      "description": "An MCP server built on [markmap](https://github.com/markmap/markmap) that converts **Markdown** to interactive **mind maps**. Supports multi-format exports (PNG/JPG/SVG), live browser preview, one-click Markdown copy, and dynamic visualization features.",
      "stars": 117,
      "forks": 18,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T23:14:25Z",
      "readme_content": "# Markmap MCP Server\n\n![Sample Mindmap](./docs/markmap.svg)\n\n[![NPM Version](https://img.shields.io/npm/v/@jinzcdev/markmap-mcp-server.svg)](https://www.npmjs.com/package/@jinzcdev/markmap-mcp-server)\n[![GitHub License](https://img.shields.io/github/license/jinzcdev/markmap-mcp-server.svg)](LICENSE)\n[![Smithery Badge](https://smithery.ai/badge/@jinzcdev/markmap-mcp-server)](https://smithery.ai/server/@jinzcdev/markmap-mcp-server)\n[![‰∏≠ÊñáÊñáÊ°£](https://img.shields.io/badge/‰∏≠ÊñáÊñáÊ°£-ÁÇπÂáªÊü•Áúã-blue)](README_zh-CN.md)\n[![Stars](https://img.shields.io/github/stars/jinzcdev/markmap-mcp-server)](https://github.com/jinzcdev/markmap-mcp-server)\n\nMarkmap MCP Server is based on the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) that allows one-click conversion of Markdown text to interactive mind maps, built on the open source project [markmap](https://github.com/markmap/markmap). The generated mind maps support rich interactive operations and can be exported in various image formats.\n\n> üéâ **Explore More Mind Mapping Tools**\n>\n> Try [MarkXMind](https://github.com/jinzcdev/markxmind) - An online editor that creates complex mind maps using simple XMindMark syntax. It supports real-time preview, multi-format export (.xmind/.svg/.png), importing existing XMind files. [Try it now](https://markxmind.js.org/)!\n\n## Features\n\n- üå† **Markdown to Mind Map**: Convert Markdown text to interactive mind maps\n- üñºÔ∏è **Multi-format Export**: Support for exporting as PNG, JPG, and SVG images\n- üîÑ **Interactive Operations**: Support for zooming, expanding/collapsing nodes, and other interactive features\n- üìã **Markdown Copy**: One-click copy of the original Markdown content\n- üåê **Automatic Browser Preview**: Option to automatically open generated mind maps in the browser\n\n## Prerequisites\n\n1. Node.js (v20 or above)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markmap MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jinzcdev/markmap-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @jinzcdev/markmap-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Install from npm\nnpm install @jinzcdev/markmap-mcp-server -g\n\n# Basic run\nnpx -y @jinzcdev/markmap-mcp-server\n\n# Specify output directory\nnpx -y @jinzcdev/markmap-mcp-server --output /path/to/output/directory\n```\n\nAlternatively, you can clone the repository and run locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/jinzcdev/markmap-mcp-server.git\n\n# Navigate to the project directory\ncd markmap-mcp-server\n\n# Build project\nnpm install && npm run build\n\n# Run the server\nnode build/index.js\n```\n\n## Usage\n\nAdd the following configuration to your MCP client configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"markmap\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jinzcdev/markmap-mcp-server\"],\n      \"env\": {\n        \"MARKMAP_DIR\": \"/path/to/output/directory\"\n      }\n    }\n  }\n}\n```\n\n> [!TIP]\n>\n> The service supports the following environment variables:\n>\n> - `MARKMAP_DIR`: Specify the output directory for mind maps (optional, defaults to system temp directory)\n>\n> **Priority Note**:\n>\n> When both the `--output` command line argument and the `MARKMAP_DIR` environment variable are specified, the command line argument takes precedence.\n\n## Available Tools\n\n### markdown-to-mindmap\n\nConvert Markdown text into an interactive mind map.\n\n**Parameters:**\n\n- `markdown`: The Markdown content to convert (required string)\n- `open`: Whether to automatically open the generated mind map in the browser (optional boolean, default is false)\n\n**Return Value:**\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"JSON_DATA_OF_MINDMAP_FILEPATH\"\n    }\n  ]\n}\n```\n\n## License\n\nThis project is licensed under the [MIT](./LICENSE) License.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "kaliaboi--mcp-zotero": {
      "owner": "kaliaboi",
      "name": "mcp-zotero",
      "url": "https://github.com/kaliaboi/mcp-zotero",
      "imageUrl": "",
      "description": "A connector for LLMs to work with collections and sources on your Zotero Cloud",
      "stars": 135,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T12:49:43Z",
      "readme_content": "# MCP Zotero\n\n![NPM Version](https://img.shields.io/npm/v/mcp-zotero) [![smithery badge](https://smithery.ai/badge/mcp-zotero)](https://smithery.ai/server/mcp-zotero)\n\nA Model Context Protocol server for Zotero integration that allows Claude to interact with your Zotero library.\n\n<a href=\"https://glama.ai/mcp/servers/mjvu0xzzzz\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mjvu0xzzzz/badge\" alt=\"Zotero MCP server\" /></a>\n\n## Setup\n\n1. Get your Zotero credentials:\n\n   ```bash\n   # First, create an API key at https://www.zotero.org/settings/keys\n   # Then use it to get your user ID:\n   curl -H \"Zotero-API-Key: YOUR_API_KEY\" https://api.zotero.org/keys/current\n   ```\n\n   The response will look like:\n\n   ```json\n   {\n     \"userID\": 123456,\n     \"username\": \"your_username\",\n     \"access\": {\n       \"user\": {\n         \"library\": true,\n         \"files\": true,\n         \"notes\": true,\n         \"write\": true\n       }\n     }\n   }\n   ```\n\n   The `userID` value is what you need.\n\n2. Set environment variables:\n\n   ```bash\n   export ZOTERO_API_KEY=\"your-api-key\"\n   export ZOTERO_USER_ID=\"user-id-from-curl\"\n   ```\n\n3. Verify your credentials:\n\n   ```bash\n   # Test that your credentials work:\n   curl -H \"Zotero-API-Key: $ZOTERO_API_KEY\" \\\n        \"https://api.zotero.org/users/$ZOTERO_USER_ID/collections\"\n   ```\n\n   You should see your collections list in the response.\n\n4. Install and run:\n\n   ```bash\n   # Install globally (recommended)\n   npm install -g mcp-zotero\n   mcp-zotero\n\n   # Or run directly with npx\n   npx mcp-zotero\n   ```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"command\": \"mcp-zotero\",\n      \"env\": {\n        \"ZOTERO_API_KEY\": YOUR_API_KEY,\n        \"ZOTERO_USER_ID\": YOUR_USER_ID\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n- `get_collections`: List all collections in your library\n- `get_collection_items`: Get items in a specific collection\n- `get_item_details`: Get detailed information about a paper\n- `search_library`: Search your entire library\n- `get_recent`: Get recently added papers\n\n## Troubleshooting\n\nIf you encounter any issues:\n\n1. Verify your environment variables are set:\n\n   ```bash\n   echo $ZOTERO_API_KEY\n   echo $ZOTERO_USER_ID\n   ```\n\n2. Check the installation:\n\n   ```bash\n   npm list -g mcp-zotero\n   ```\n\n3. Try reinstalling:\n   ```bash\n   npm uninstall -g mcp-zotero\n   npm install -g mcp-zotero\n   ```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "mem0ai--mem0-mcp": {
      "owner": "mem0ai",
      "name": "mem0-mcp",
      "url": "https://github.com/mem0ai/mem0-mcp",
      "imageUrl": "",
      "description": "A Model Context Protocol server for Mem0 that helps manage coding preferences and patterns, providing tools for storing, retrieving and semantically handling code implementations, best practices and technical documentation in IDEs like Cursor and Windsurf",
      "stars": 473,
      "forks": 95,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:16Z",
      "readme_content": "# MCP Server with Mem0 for Managing Coding Preferences\n\nThis demonstrates a structured approach for using an [MCP](https://modelcontextprotocol.io/introduction) server with [mem0](https://mem0.ai) to manage coding preferences efficiently. The server can be used with Cursor and provides essential tools for storing, retrieving, and searching coding preferences.\n\n## Installation\n\n1. Clone this repository\n2. Initialize the `uv` environment:\n\n```bash\nuv venv\n```\n\n3. Activate the virtual environment:\n\n```bash\nsource .venv/bin/activate\n```\n\n4. Install the dependencies using `uv`:\n\n```bash\n# Install in editable mode from pyproject.toml\nuv pip install -e .\n```\n\n5. Update `.env` file in the root directory with your mem0 API key:\n\n```bash\nMEM0_API_KEY=your_api_key_here\n```\n\n## Usage\n\n1. Start the MCP server:\n\n```bash\nuv run main.py\n```\n\n2. In Cursor, connect to the SSE endpoint, follow this [doc](https://docs.cursor.com/context/model-context-protocol) for reference:\n\n```\nhttp://0.0.0.0:8080/sse\n```\n\n3. Open the Composer in Cursor and switch to `Agent` mode.\n\n## Demo with Cursor\n\nhttps://github.com/user-attachments/assets/56670550-fb11-4850-9905-692d3496231c\n\n## Features\n\nThe server provides three main tools for managing code preferences:\n\n1. `add_coding_preference`: Store code snippets, implementation details, and coding patterns with comprehensive context including:\n   - Complete code with dependencies\n   - Language/framework versions\n   - Setup instructions\n   - Documentation and comments\n   - Example usage\n   - Best practices\n\n2. `get_all_coding_preferences`: Retrieve all stored coding preferences to analyze patterns, review implementations, and ensure no relevant information is missed.\n\n3. `search_coding_preferences`: Semantically search through stored coding preferences to find relevant:\n   - Code implementations\n   - Programming solutions\n   - Best practices\n   - Setup guides\n   - Technical documentation\n\n## Why?\n\nThis implementation allows for a persistent coding preferences system that can be accessed via MCP. The SSE-based server can run as a process that agents connect to, use, and disconnect from whenever needed. This pattern fits well with \"cloud-native\" use cases where the server and clients can be decoupled processes on different nodes.\n\n### Server\n\nBy default, the server runs on 0.0.0.0:8080 but is configurable with command line arguments like:\n\n```\nuv run main.py --host <your host> --port <your port>\n```\n\nThe server exposes an SSE endpoint at `/sse` that MCP clients can connect to for accessing the coding preferences management tools.\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "modelcontextprotocol--server-memory": {
      "owner": "modelcontextprotocol",
      "name": "server-memory",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/memory",
      "imageUrl": "",
      "description": "Knowledge graph-based persistent memory system for maintaining context",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0
    },
    "MWGMorningwood--Central-Memory-MCP": {
      "owner": "MWGMorningwood",
      "name": "Central-Memory-MCP",
      "url": "https://github.com/MWGMorningwood/Central-Memory-MCP",
      "imageUrl": "",
      "description": "An Azure PaaS-hostable MCP server that provides a workspace-grounded knowledge graph for multiple developers using Azure Functions MCP triggers and Table storage.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-25T16:25:54Z",
      "readme_content": "# Central Memory MCP Server\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/MWGMorningwood/Central-Memory-MCP)](https://archestra.ai/mcp-catalog/mwgmorningwood__central-memory-mcp)\n\nModel Context Protocol (MCP) memory server built with Azure Functions and TypeScript, providing persistent knowledge graph storage for AI assistants in VS Code.  \nInspired by and forked from [`@modelcontextprotocol/server-memory`](https://github.com/modelcontextprotocol/servers/tree/main/src/memory)\n\n## üöÄ Quick Start\n\n```bash\nnpm install\nfunc start\n```\n\n### VS Code Integration\n\n1. **Install recommended extensions** from `.vscode/extensions.json`\n2. **MCP configuration** is ready in `.vscode/mcp.json`\n3. **Use `#memory-test` tools** in VS Code Copilot chat\n\n> **Note**: All MCP tools now use object parameters instead of JSON strings for better type safety and ease of use.\n\n### Test the Server\n\n```bash\n# Health check\ncurl http://localhost:7071/api/health\n\n# Use in VS Code Copilot with object parameters:\n# #memory-test_create_entities\n# #memory-test_read_graph\n# #memory-test_search_entities\n```\n\n### Example Usage in VS Code Copilot\n\n**Recommended workflow for best results:**\n\n```text\n1. First, check existing data:\n#memory-test_read_graph workspaceId=\"my-project\"\n\n2. Search for existing entities:\n#memory-test_search_entities workspaceId=\"my-project\" name=\"Alice\"\n\n3. Create entities (auto-updates existing ones):\n#memory-test_create_entities workspaceId=\"my-project\" entities={\"name\": \"Alice\", \"entityType\": \"Person\", \"observations\": [\"Software engineer\"]}\n\n4. Create relationships (auto-creates missing entities):\n#memory-test_create_relations workspaceId=\"my-project\" relations={\"from\": \"Alice\", \"to\": \"React Project\", \"relationType\": \"worksOn\"}\n\n5. Add observations (auto-creates entity if missing):\n#memory-test_add_observation workspaceId=\"my-project\" entityName=\"Alice\" observation=\"Leads the frontend team\" entityType=\"Person\"\n```\n\n**Key Features for Better LLM Usability:**\n- ‚úÖ Auto-creation of missing entities when adding observations or relations\n- ‚úÖ Helpful error messages with examples when validation fails  \n- ‚úÖ Workflow guidance to view graph first, then search, then create\n- ‚úÖ Clear parameter descriptions with expected formats\n- ‚úÖ Reduced friction - tools handle common edge cases automatically\n\n## üîß MCP Tools\n\n**Core Operations:**\n\n- `read_graph` - **RECOMMENDED FIRST STEP**: View the entire knowledge graph to understand existing data\n- `create_entities` - Create entities with auto-update of existing ones\n- `create_relations` - Create relationships with auto-creation of missing entities  \n- `search_entities` / `search_relations` - Search and verify existing data\n- `add_observation` - Add observations with auto-creation of missing entities\n- `update_entity` - Update entity observations and metadata\n- `delete_entity` - Remove entity and all its relations\n- `get_stats` - Get workspace statistics\n- `clear_memory` - Clear all workspace data\n\n**Recommended Workflow:**\n1. Use `read_graph` to understand existing data\n2. Use `search_entities` to check for existing entities\n3. Use `create_entities` to add new entities\n4. Use `create_relations` to connect entities\n5. Use `add_observation` to add new information\n\n**Advanced Features:**\n\n- `get_temporal_events` - Time-based activity tracking\n- `merge_entities` - Merge duplicate entities\n- `detect_duplicate_entities` - Find potential duplicates\n- `execute_batch_operations` - Batch multiple operations\n- `get_user_stats` - Get user-specific statistics\n- `search_relations_by_user` - Find relations by user\n\n## üèóÔ∏è Architecture\n\nBuilt with:\n\n- **Azure Functions v4** with TypeScript\n- **Azure Table Storage** for persistent data (via Azurite locally)\n- **Model Context Protocol (MCP)** for VS Code integration\n- **Workspace isolation** - each project gets separate storage\n\n## ÔøΩ Project Structure\n\n```text\nsrc/\n‚îú‚îÄ‚îÄ functions/         # Azure Functions endpoints\n‚îú‚îÄ‚îÄ services/          # Business logic (storage, entities, relations)\n‚îú‚îÄ‚îÄ types/             # TypeScript definitions\n‚îî‚îÄ‚îÄ index.ts           # Main entry point\n```\n\n## üìö Documentation\n\nFor detailed information, see the `.docs/` folder:\n\n- **[Architecture Guide](.docs/ARCHITECTURE.md)** - Technical design and patterns\n- **[API Reference](.docs/API.md)** - Complete endpoint documentation\n- **[Storage Guide](.docs/STORAGE.md)** - Storage configuration and workspace management\n- **[Deployment Guide](.docs/DEPLOYMENT.md)** - Production deployment options\n\n## üîí Production Notes\n\n- Uses Azure Table Storage with managed identity for security\n- Workspace isolation prevents data leakage between projects\n- Health endpoints for monitoring and container orchestration\n- Automatic fallback to local storage for development\n\n## üìù License\n\nMIT License - see LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "pi22by7--In-Memoria": {
      "owner": "pi22by7",
      "name": "In-Memoria",
      "url": "https://github.com/pi22by7/In-Memoria",
      "imageUrl": "",
      "description": "Persistent intelligence infrastructure for agentic development that gives AI coding assistants cumulative memory and pattern learning. Hybrid TypeScript/Rust implementation with local-first storage using SQLite + SurrealDB for semantic analysis and incremental codebase understanding.",
      "stars": 71,
      "forks": 8,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-10-03T01:57:32Z",
      "readme_content": "# In Memoria\n\n[![npm version](https://badge.fury.io/js/in-memoria.svg)](https://www.npmjs.com/package/in-memoria)\n[![npm downloads](https://img.shields.io/npm/dm/in-memoria.svg)](https://www.npmjs.com/package/in-memoria)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**Persistent memory and pattern learning for AI coding assistants via MCP.**\n\nAI coding tools suffer from complete **session amnesia**. Every conversation with Claude, Copilot, or Cursor starts from scratch, forcing you to re-explain your codebase architecture, patterns, and preferences repeatedly.\n\nIn Memoria solves this by building persistent intelligence about your code that AI assistants can access through the Model Context Protocol - giving them memory that persists across sessions.\n\n## The Problem\n\n```bash\n# What happens now\nYou: \"Refactor this function using our established patterns\"\nAI: \"What patterns? I don't know your codebase.\"\nYou: *explains architecture for the 50th time*\n\n# What should happen\nYou: \"Refactor this function using our established patterns\"\nAI: \"Based on your preference for functional composition and your naming conventions...\"\n```\n\nCurrent AI tools:\n\n- Re-analyze codebases every session (expensive)\n- Give generic suggestions that don't match your style\n- Have no memory of architectural decisions\n- Can't learn from corrections you've made\n\n## Quick Start\n\n```bash\n# Install and start the MCP server\nnpx in-memoria server\n\n# Connect from Claude Desktop (add to config)\n{\n  \"mcpServers\": {\n    \"in-memoria\": {\n      \"command\": \"npx\",\n      \"args\": [\"in-memoria\", \"server\"]\n    }\n  }\n}\n\n# Connect from Claude Code CLI\nclaude mcp add in-memoria -- npx in-memoria server\n\n# Learn from your codebase\nnpx in-memoria learn ./my-project\n\n# Native AST parsing for 12 programming languages\n# Supports: TypeScript, JavaScript, Python, Rust, Go, Java, C/C++, C#, Svelte, Vue, SQL\n# Intelligent filtering excludes build artifacts and dependencies\n```\n\n## How It Works\n\nIn Memoria runs as an MCP server that AI tools connect to. It provides 17 tools for codebase analysis and pattern learning.\n\n**Architecture:**\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    MCP    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    napi-rs    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  AI Tool (Claude)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  TypeScript Server   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ     Rust Core       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ  ‚Ä¢ AST Parser       ‚îÇ\n                                             ‚îÇ                          ‚îÇ  ‚Ä¢ Pattern Learner  ‚îÇ\n                                             ‚îÇ                          ‚îÇ  ‚Ä¢ Semantic Engine  ‚îÇ\n                                             ‚ñº                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                                   ‚îÇ SQLite + SurrealDB   ‚îÇ\n                                   ‚îÇ  (Local Storage)     ‚îÇ\n                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Core engines:**\n\n- **AST Parser** (Rust): Tree-sitter parsing for TypeScript, JavaScript, Python, Rust, Go, Java, C/C++, C#, Svelte, Vue, and SQL\n- **Pattern Learner** (Rust): Statistical analysis of naming conventions, function signatures, and architectural choices\n- **Semantic Engine** (Rust): Code relationship mapping and concept extraction with timeout protection\n- **TypeScript Layer**: MCP server, SQLite/SurrealDB operations, file watching\n- **Storage**: SQLite for structured patterns, SurrealDB for vector search and semantic queries\n\n## What It Learns\n\nIn Memoria builds statistical models from your actual code to understand your preferences:\n\n**Naming Patterns:**\n\n```typescript\n// Detects patterns like: useXxxData for API hooks, handleXxx for events\nconst useUserData = () => { ... }\nconst handleSubmit = () => { ... }\nconst formatUserName = (name: string) => { ... }\n\n// AI gets context: \"This developer uses camelCase, 'use' prefix for hooks,\n// 'handle' for events, 'format' for data transformation\"\n```\n\n**Architectural Choices:**\n\n```typescript\n// Learns you consistently use Result types instead of throwing\ntype ApiResult<T> =\n  | { success: true; data: T }\n  | { success: false; error: string };\n\n// AI suggests this pattern in new code instead of try/catch\n```\n\n**Code Structure Preferences:**\n\n```typescript\n// Detects your preference for functional composition\nconst processUser = pipe(validateUser, enrichUserData, saveUser);\n\n// vs object-oriented approaches you avoid\nclass UserProcessor { ... } // Rarely used in your codebase\n```\n\n**Project Organization:**\n\n```\nsrc/\n  components/     # UI components\n  services/      # Business logic\n  utils/         # Pure functions\n  types/         # Type definitions\n\n// AI learns your directory structure preferences\n```\n\n## MCP Tools for AI Assistants\n\nIn Memoria provides 17 tools that AI assistants can use to understand your codebase:\n\n**Getting Started:**\n\n- `get_learning_status` - Check what intelligence exists for a project\n- `auto_learn_if_needed` - Automatically learn from codebase if no intelligence exists\n- `quick_setup` - Initialize In Memoria for a new project\n\n**Code Analysis:**\n\n- `analyze_codebase` - Get architectural overview, complexity metrics, and language breakdown\n- `get_file_content` - Retrieve files with rich metadata and analysis\n- `get_project_structure` - Understand directory hierarchy and organization patterns\n- `search_codebase` - Semantic search that finds code by meaning, not just keywords\n\n**Pattern Intelligence:**\n\n- `get_pattern_recommendations` - Get coding suggestions that match your established style\n- `predict_coding_approach` - Predict how you'd solve similar problems based on your patterns\n- `get_developer_profile` - Access your learned coding preferences and decision patterns\n- `get_semantic_insights` - Discover code relationships and architectural concepts\n\n**Learning & Memory:**\n\n- `learn_codebase_intelligence` - Manually trigger analysis of a codebase\n- `contribute_insights` - Allow AI to add observations back to the knowledge base\n- `generate_documentation` - Create docs that understand your project's patterns\n\n**System Monitoring:**\n\n- `get_system_status` - Health check and component status\n- `get_intelligence_metrics` - Quality and completeness of learned patterns\n- `get_performance_status` - System performance and benchmarking\n\n## Implementation Details\n\n**Pattern Learning Algorithm:**\n\n1. Parse code into ASTs using tree-sitter\n2. Extract structural patterns (function signatures, class hierarchies, naming)\n3. Build frequency maps of developer choices\n4. Train classifier on decision patterns\n5. Generate predictions for new code contexts\n\n**Performance:**\n\n- **Smart file filtering** - Automatically excludes build artifacts, dependencies, and generated files\n- **Timeout protection** - Prevents analysis from hanging on complex files\n- **Fast analysis** - Optimized processing that skips `node_modules/`, `dist/`, `.next/`, and other non-source files\n- **File size limits** - Skips very large files to prevent memory issues\n- **Incremental analysis** - Only processes changed files in subsequent runs\n- **SQLite for structured data, SurrealDB embedded for semantic search and vectors**\n- **Cross-platform Rust binaries** (Windows, macOS, Linux)\n- **Built-in performance profiling** and memory leak detection\n- **Optimized for real-time file watching** without blocking development workflow\n\n## Team Usage\n\nIn Memoria works for individual developers and teams:\n\n**Individual:**\n\n- Learns your personal coding style\n- Remembers architectural decisions you've made\n- Provides context-aware suggestions\n\n**Team:**\n\n- Share `.in-memoria.db` files containing learned patterns across team members\n- Onboard new developers by providing pre-learned codebase intelligence\n- Ensure consistent AI suggestions team-wide through shared pattern recognition\n\n## Technical Comparison\n\n**vs GitHub Copilot's memory:**\n\n- Copilot: Basic fact storage, no pattern learning\n- In Memoria: Semantic analysis with prediction engine\n\n**vs Cursor's rules:**\n\n- Cursor: Static rules, manually defined\n- In Memoria: Dynamic learning from actual code\n\n**vs Custom RAG:**\n\n- RAG: Retrieves relevant code snippets\n- In Memoria: Understands coding patterns and predicts behavior\n\n## Build from Source\n\n```bash\ngit clone https://github.com/pi22by7/in-memoria\ncd in-memoria\nnpm install\nnpm run build\n```\n\n**Requirements:**\n\n- Node.js 18+\n- Rust 1.70+ (for building)\n- 2GB RAM minimum\n\n**Quality & Testing:**\n\n- 98.3% unit test pass rate (118/120 tests)\n- 100% MCP integration test coverage (23/23 tests)\n- Comprehensive server lifecycle testing\n- All Rust clippy warnings resolved\n- Zero memory leaks verified\n\n**Development:**\n\n```bash\nnpm run dev          # Start in development mode\nnpm test            # Run test suite\nnpm run build:rust  # Build Rust components\n```\n\n## Contributing\n\nThis is infrastructure for the AI development ecosystem. Contributions welcome:\n\n- **Language support** - Add tree-sitter parsers or extend file filtering\n- **Pattern learning improvements** - Enhance statistical analysis and concept extraction\n- **MCP tool additions** - New tools for AI assistant integration\n- **Performance optimizations** - Further speed improvements and memory usage reduction\n- **Timeout and reliability** - Additional safeguards for edge cases\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## FAQ\n\n**Q: Does this replace my AI coding assistant?**\nA: No, it enhances them. In Memoria provides memory and context that AI tools can use.\n\n**Q: What data is collected?**\nA: Everything stays local. No data is sent to external services.\n\n**Q: How accurate is pattern learning?**\nA: Pattern recognition accuracy improves with codebase size and consistency. Projects with established patterns and consistent style will see better pattern detection than smaller or inconsistent codebases. The system learns from frequency and repetition in your actual code.\n\n**Q: Performance impact?**\nA: Minimal. Runs in background with smart filtering that skips build artifacts and dependencies. Modern analysis engine with built-in safeguards for reliable operation.\n\n**Q: What file types are supported?**\nA: TypeScript, JavaScript, Python, Rust, Go, Java, C/C++, C#, Svelte, Vue, and SQL with native AST parsing. Build artifacts and dependencies are automatically excluded.\n\n**Q: What if analysis encounters issues?**\nA: Built-in reliability features handle edge cases gracefully. Large files and complex directories are processed efficiently with automatic fallbacks. Progress is shown during analysis.\n\n## License\n\nMIT - see [LICENSE](LICENSE)\n\n---\n\n**Try it:** `npx in-memoria server`\n\nGive your AI tools the memory they've been missing.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "pinecone-io--assistant-mcp": {
      "owner": "pinecone-io",
      "name": "assistant-mcp",
      "url": "https://github.com/pinecone-io/assistant-mcp",
      "imageUrl": "",
      "description": "Connects to your Pinecone Assistant and gives the agent context from its knowledge engine.",
      "stars": 37,
      "forks": 7,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-10-02T07:00:14Z",
      "readme_content": "# Pinecone Assistant MCP Server\n\nAn MCP server implementation for retrieving information from Pinecone Assistant.\n\n## Features\n\n- Retrieves information from Pinecone Assistant\n- Supports multiple results retrieval with a configurable number of results\n\n## Prerequisites\n\n- Docker installed on your system\n- Pinecone API key - obtain from the [Pinecone Console](https://app.pinecone.io)\n- Pinecone Assistant API host - after creating an Assistant (e.g. in Pinecone Console), you can find the host in the Assistant details page\n\n## Building with Docker\n\nTo build the Docker image:\n\n```sh\ndocker build -t pinecone/assistant-mcp .\n```\n\n## Running with Docker\n\nRun the server with your Pinecone API key:\n\n```sh\ndocker run -i --rm \\\n  -e PINECONE_API_KEY=<YOUR_PINECONE_API_KEY_HERE> \\\n  -e PINECONE_ASSISTANT_HOST=<YOUR_PINECONE_ASSISTANT_HOST_HERE> \\\n  pinecone/assistant-mcp\n```\n\n### Environment Variables\n\n- `PINECONE_API_KEY` (required): Your Pinecone API key\n- `PINECONE_ASSISTANT_HOST` (optional): Pinecone Assistant API host (default: https://prod-1-data.ke.pinecone.io)\n- `LOG_LEVEL` (optional): Logging level (default: info)\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"pinecone-assistant\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \n        \"PINECONE_API_KEY\", \n        \"-e\", \n        \"PINECONE_ASSISTANT_HOST\", \n        \"pinecone/assistant-mcp\"\n      ],\n      \"env\": {\n        \"PINECONE_API_KEY\": \"<YOUR_PINECONE_API_KEY_HERE>\",\n        \"PINECONE_ASSISTANT_HOST\": \"<YOUR_PINECONE_ASSISTANT_HOST_HERE>\"\n      }\n    }\n  }\n}\n```\n\n## Building from Source\n\nIf you prefer to build from source without Docker:\n\n1. Make sure you have Rust installed (https://rustup.rs/)\n2. Clone this repository\n3. Run `cargo build --release`\n4. The binary will be available at `target/release/assistant-mcp`\n\n### Testing with the inspector\n```sh\nexport PINECONE_API_KEY=<YOUR_PINECONE_API_KEY_HERE>\nexport PINECONE_ASSISTANT_HOST=<YOUR_PINECONE_ASSISTANT_HOST_HERE>\n# Run the inspector alone\nnpx @modelcontextprotocol/inspector cargo run\n# Or run with Docker directly through the inspector\nnpx @modelcontextprotocol/inspector -- docker run -i --rm -e PINECONE_API_KEY -e PINECONE_ASSISTANT_HOST pinecone/assistant-mcp\n```\n\n## License\n\nThis project is licensed under the terms specified in the LICENSE file.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "ragieai--mcp-server": {
      "owner": "ragieai",
      "name": "mcp-server",
      "url": "https://github.com/ragieai/ragie-mcp-server",
      "imageUrl": "",
      "description": "Retrieve context from your [Ragie](https://www.ragie.ai) (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0
    },
    "TechDocsStudio--biel-mcp": {
      "owner": "TechDocsStudio",
      "name": "biel-mcp",
      "url": "https://github.com/TechDocsStudio/biel-mcp",
      "imageUrl": "",
      "description": "Let AI tools like Cursor, VS Code, or Claude Desktop answer questions using your product docs. Biel.ai provides the RAG system and MCP server.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-07T10:21:02Z",
      "readme_content": "<div align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./logo-dark..jpg\" />\n    <img alt=\"Biel.ai\" src=\"./logo.jpg\" />\n  </picture>\n  <h1>Biel.ai MCP Server</h1>\n  <h3>Connect your IDE to your product docs</h3>\n</div>\n\n\nGive AI tools like Cursor, VS Code, and Claude Desktop access to your company's product knowledge through the [Biel.ai platform](https://biel.ai).\n\nBiel.ai provides a hosted Retrieval-Augmented Generation (RAG) layer that makes your documentation searchable and useful to AI tools. This enables smarter completions, accurate technical answers, and context-aware suggestions‚Äîdirectly in your IDE or chat environment.\n\n![Demo](./demo.png)\n\nWhen AI tools can read your product documentation, they become **significantly** more helpful‚Äîgenerating more accurate code completions, answering technical questions with context, and guiding developers with real-time product knowledge.\n\n\n> **Note:** Requires a Biel.ai account and project setup. **[Start your free 15-day trial](https://app.biel.ai/accounts/signup/)**.\n\n<h3><a href=\"https://docs.biel.ai/integrations/mcp-server?utm_source=github&utm_medium=referral&utm_campaign=readme\">See quickstart instructions ‚Üí</a></h3>\n\n## Getting started\n\n### 1. Get your MCP configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"biel-ai\": {\n      \"description\": \"Query your product's documentation, APIs, and knowledge base.\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.biel.ai/sse?project_slug=YOUR_PROJECT_SLUG&domain=https://your-docs-domain.com\"\n      ]\n    }\n  }\n}\n```\n\n**Required:** `project_slug` and `domain`  \n**Optional:** `api_key` (only needed for private projects)\n\n### 2. Add to your AI tool\n\n* **Cursor**: **Settings** ‚Üí **Tools & Integrations* ‚Üí **New MCP server**.\n* **Claude Desktop**: Edit `claude_desktop_config.json`  \n* **VS Code**: Install **MCP extension**.\n\n### 3. Start asking questions\n\n```\nCan you check in biel_ai what the auth headers are for the /users endpoint?\n```\n\n## Self-hosting (Optional)\n\nFor advanced users who prefer to run their own MCP server instance:\n\n### Local development\n```bash\n# Clone and run locally\ngit clone https://github.com/techdocsStudio/biel-mcp\ncd biel-mcp\npip install -r requirements.txt\npython biel_mcp_server.py\n```\n\n### Docker deployment\n```bash\n# Docker Compose (recommended)\ndocker-compose up -d --build\n\n# Or Docker directly\ndocker build -t biel-mcp .\ndocker run -d -p 7832:7832 biel-mcp\n```\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/techdocsStudio/biel-mcp/issues)\n- **Contact**: [support@biel.ai](mailto:support@biel.ai)\n- **Custom Demo**: [Book a demo](https://biel.ai/contact)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "topoteretes--cognee": {
      "owner": "topoteretes",
      "name": "cognee",
      "url": "https://github.com/topoteretes/cognee/tree/dev/cognee-mcp",
      "imageUrl": "",
      "description": "Memory manager for AI apps and Agents using various graph and vector stores and allowing ingestion from 30+ data sources",
      "stars": 7551,
      "forks": 672,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T11:20:09Z",
      "readme_content": "<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - Memory for AI Agents in 6 lines of code\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demo</a>\n  .\n  <a href=\"https://cognee.ai\">Learn more</a>\n  ¬∑\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Join Discord</a>\n  ¬∑\n  <a href=\"https://www.reddit.com/r/AIMemory/\">Join r/AIMemory</a>\n  .\n  <a href=\"https://docs.cognee.ai/\">Docs</a>\n  .\n  <a href=\"https://github.com/topoteretes/cognee-community\">cognee community repo</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n  <a href=\"https://github.com/sponsors/topoteretes\"><img src=\"https://img.shields.io/badge/Sponsor-‚ù§Ô∏è-ff69b4.svg\" alt=\"Sponsor\"></a>\n\n<p>\n  <a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\" style=\"display:inline-block; margin-right:10px;\">\n    <img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" width=\"250\" height=\"54\" />\n  </a>\n\n  <a href=\"https://trendshift.io/repositories/13955\" target=\"_blank\" style=\"display:inline-block;\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" width=\"250\" height=\"55\" />\n  </a>\n</p>\n\n\n\n\n\nBuild dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.\n\n  <p align=\"center\">\n  üåê Available Languages\n  :\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=de\">Deutsch</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=es\">Espa√±ol</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=fr\">fran√ßais</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ja\">Êó•Êú¨Ë™û</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ko\">ÌïúÍµ≠Ïñ¥</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=pt\">Portugu√™s</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ru\">–†—É—Å—Å–∫–∏–π</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=zh\">‰∏≠Êñá</a>\n  </p>\n\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"Why cognee?\" width=\"50%\" />\n</div>\n</div>\n\n\n\n## Get Started\n\nGet started quickly with a Google Colab  <a href=\"https://colab.research.google.com/drive/1jHbWVypDgCLwjE71GSXhRL3YxYhCZzG1?usp=sharing\">notebook</a> , <a href=\"https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&utm_medium=product-shared-content&utm_campaign=notebook&utm_content=78ffacb9-5832-4611-bb1a-560386068b30\">Deepnote notebook</a> or  <a href=\"https://github.com/topoteretes/cognee/tree/main/cognee-starter-kit\">starter repo</a>\n\n\n## About cognee\n\ncognee works locally and stores your data on your device.\nOur hosted solution is just our deployment of OSS cognee on Modal, with the goal of making development and productionization easier.\n\nSelf-hosted package:\n\n- Interconnects any kind of documents: past conversations, files, images, and audio transcriptions\n- Replaces RAG systems with a memory layer based on graphs and vectors\n- Reduces developer effort and cost, while increasing quality and precision\n- Provides Pythonic data pipelines that manage data ingestion from 30+ data sources\n- Is highly customizable with custom tasks, pipelines, and a set of built-in search endpoints\n\nHosted platform:\n- Includes a managed UI and a [hosted solution](https://www.cognee.ai)\n\n\n\n## Self-Hosted (Open Source)\n\n\n### üì¶ Installation\n\nYou can install Cognee using either **pip**, **poetry**, **uv** or any other python package manager.\n\nCognee supports Python 3.10 to 3.12\n\n#### With uv\n\n```bash\nuv pip install cognee\n```\n\nDetailed instructions can be found in our [docs](https://docs.cognee.ai/getting-started/installation#environment-configuration)\n\n### üíª Basic Usage\n\n#### Setup\n\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n\n```\n\nYou can also set the variables by creating .env file, using our <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">template.</a>\nTo use different LLM providers, for more info check out our <a href=\"https://docs.cognee.ai/setup-configuration/llm-providers\">documentation</a>\n\n\n#### Simple example\n\n\n\n##### Python\n\nThis script will run the default pipeline:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"Cognee turns documents into AI memory.\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Add memory algorithms to the graph\n    await cognee.memify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"What does cognee do?\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\nExample output:\n```\n  Cognee turns documents into AI memory.\n\n```\n##### Via CLI\n\nLet's get the basics covered\n\n```\ncognee-cli add \"Cognee turns documents into AI memory.\"\n\ncognee-cli cognify\n\ncognee-cli search \"What does cognee do?\"\ncognee-cli delete --all\n\n```\nor run\n```\ncognee-cli -ui\n```\n\n\n</div>\n\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [cogwit](https://www.cognee.ai)\n2. Add your API key to local UI and sync your data to Cogwit\n\n\n\n\n## Demos\n\n1. Cogwit Beta demo:\n\n[Cogwit Beta](https://github.com/user-attachments/assets/fa520cd2-2913-4246-a444-902ea5242cb0)\n\n2. Simple GraphRAG demo\n\n[Simple GraphRAG demo](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. cognee with Ollama\n\n[cognee with local models](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)\n\n\n## Contributing\nYour contributions are at the core of making this a true open source project. Any contributions you make are **greatly appreciated**. See [`CONTRIBUTING.md`](CONTRIBUTING.md) for more information.\n\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@misc{markovic2025optimizinginterfaceknowledgegraphs,\n      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning}, \n      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},\n      year={2025},\n      eprint={2505.24478},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2505.24478}, \n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "unibaseio--membase-mcp": {
      "owner": "unibaseio",
      "name": "membase-mcp",
      "url": "https://github.com/unibaseio/membase-mcp",
      "imageUrl": "",
      "description": "Save and query your agent memory in distributed way by Membase",
      "stars": 14,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T09:13:46Z",
      "readme_content": "# membase mcp server\n\n## Description\n\nMembase is the first decentralized memory layer for AI agents, powered by Unibase. It provides secure, persistent storage for conversation history, interaction records, and knowledge ‚Äî ensuring agent continuity, personalization, and traceability.\n\nThe Membase-MCP Server enables seamless integration with the Membase protocol, allowing agents to upload and retrieve memory from the Unibase DA network for decentralized, verifiable storage.\n\n## Functions\n\nMessages or memoiries can be visit at: <https://testnet.hub.membase.io/>\n\n- **get_conversation_id**: Get the current conversation id.\n- **switch_conversation**: Switch to a different conversation.\n- **save_message**: Save a message/memory into the current conversation.\n- **get_messages**: Get the last n messages from the current conversation.\n\n## Installation\n\n```shell\ngit clone https://github.com/unibaseio/membase-mcp.git\ncd membase-mcp\nuv run src/membase_mcp/server.py\n```\n\n## Environment variables\n\n- MEMBASE_ACCOUNT: your account to upload\n- MEMBASE_CONVERSATION_ID: your conversation id, should be unique, will preload its history\n- MEMBASE_ID: your instance id\n\n## Configuration on Claude/Windsurf/Cursor/Cline\n\n```json\n{\n  \"mcpServers\": {\n    \"membase\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/membase-mcp\",\n        \"run\", \n        \"src/membase_mcp/server.py\"\n      ],\n      \"env\": {\n        \"MEMBASE_ACCOUNT\": \"your account, 0x...\",\n        \"MEMBASE_CONVERSATION_ID\": \"your conversation id, should be unique\",\n        \"MEMBASE_ID\": \"your sub account, any string\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\ncall functions in llm chat\n\n- get conversation id and switch conversation\n\n![get conversation id and switch conversation](./asset/switch.png)\n\n- save message and get messages\n\n![save message and get messages](./asset/save.png)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "upstash--context7": {
      "owner": "upstash",
      "name": "context7",
      "url": "https://github.com/upstash/context7",
      "imageUrl": "",
      "description": "Up-to-date code documentation for LLMs and AI code editors.",
      "stars": 32510,
      "forks": 1611,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T12:53:54Z",
      "readme_content": "![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=white\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![ÁπÅÈ´î‰∏≠Êñá](https://img.shields.io/badge/docs-ÁπÅÈ´î‰∏≠Êñá-yellow)](./docs/README.zh-TW.md) [![ÁÆÄ‰Ωì‰∏≠Êñá](https://img.shields.io/badge/docs-ÁÆÄ‰Ωì‰∏≠Êñá-yellow)](./docs/README.zh-CN.md) [![Êó•Êú¨Ë™û](https://img.shields.io/badge/docs-Êó•Êú¨Ë™û-b7003a)](./docs/README.ja.md) [![ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú](https://img.shields.io/badge/docs-ÌïúÍµ≠Ïñ¥-green)](./docs/README.ko.md) [![Documentaci√≥n en Espa√±ol](https://img.shields.io/badge/docs-Espa√±ol-orange)](./docs/README.es.md) [![Documentation en Fran√ßais](https://img.shields.io/badge/docs-Fran√ßais-blue)](./docs/README.fr.md) [![Documenta√ß√£o em Portugu√™s (Brasil)](<https://img.shields.io/badge/docs-Portugu√™s%20(Brasil)-purple>)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ](https://img.shields.io/badge/docs-–†—É—Å—Å–∫–∏–π-darkblue)](./docs/README.ru.md) [![–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://img.shields.io/badge/docs-–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞-lightblue)](./docs/README.uk.md) [![T√ºrk√ße Dok√ºman](https://img.shields.io/badge/docs-T√ºrk√ße-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Ti·∫øng Vi·ªát](https://img.shields.io/badge/docs-Ti·∫øng%20Vi·ªát-red)](./docs/README.vi.md)\n\n## ‚ùå Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ‚ùå Code examples are outdated and based on year-old training data\n- ‚ùå Hallucinated APIs that don't even exist\n- ‚ùå Generic answers for old package versions\n\n## ‚úÖ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source ‚Äî and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1Ô∏è‚É£ Write your prompt naturally\n- 2Ô∏è‚É£ Tell the LLM to `use context7`\n- 3Ô∏è‚É£ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## üìö Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.\n\n## üõ†Ô∏è Installation\n\n### Requirements\n\n- Node.js >= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional) for higher rate limits and private repositories (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n> [!WARNING]\n> **SSE Protocol Deprecation Notice**\n>\n> The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n<details>\n<summary><b>Installing via Smithery</b></summary>\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client <CLIENT_NAME> --key <YOUR_SMITHERY_KEY>\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n> Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n</details>\n\n<details>\n<summary><b>Install in Amp</b></summary>\n\nRun this command in your terminal. See [Amp MCP docs](https://ampcode.com/manual#mcp) for more info.\n\n#### Without API Key (Basic Usage)\n\n```sh\namp mcp add context7 https://mcp.context7.com/mcp\n```\n\n#### With API Key (Higher Rate Limits & Private Repos)\n\n```sh\namp mcp add context7 --header \"CONTEXT7_API_KEY=YOUR_API_KEY\" https://mcp.context7.com/mcp\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"serverUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in VS Code</b></summary>\n\n[<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Context7%20MCP&color=0098FF\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[<img alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Context7%20MCP&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>\n<b>Install in Cline</b>\n</summary>\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5. Add context7 to `mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zed</b></summary>\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"source\": \"custom\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Augment Code</b></summary>\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n</details>\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Gemini CLI</b></summary>\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n</details>\n\n<details>\n<summary><b>Install in Claude Desktop</b></summary>\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Opencode</b></summary>\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in OpenAI Codex</b></summary>\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n‚ö†Ô∏è Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.\nIn that case, explicitly configure the MCP server with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\n‚ö†Ô∏è MacOS Notes\n\nOn MacOS, some users may encounter the same request timed out errors like Windows,\nit also can be solved tith the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"/Users/yourname/.nvm/versions/node/v22.14.0/bin/node\"  # Node.js full path\nargs = [\"/Users/yourname/.nvm/versions/node/v22.14.0/lib/node_modules/@upstash/context7-mcp/dist/index.js\",  \n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\nThis ensures Codex CLI works reliably on MacOS.\n\n</details>\n\n<details>\n\n<summary><b>Install in JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -> `Tools` -> `Junie` -> `MCP Settings`\n\n</details>\n\n<details>\n  \n<summary><b>Install in Kiro</b></summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n<details>\n<summary><b>Install in Trae</b></summary>\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Bun or Deno</b></summary>\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Docker</b></summary>\n\nIf you prefer to run the MCP server in a Docker container:\n\n1. **Build the Docker Image:**\n\n   First, create a `Dockerfile` in the project root (or anywhere you prefer):\n\n   <details>\n   <summary>Click to see Dockerfile content</summary>\n\n   ```Dockerfile\n   FROM node:18-alpine\n\n   WORKDIR /app\n\n   # Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   </details>\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"–°ontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n</details>\n\n<details>\n<summary><b>Install Using the Desktop Extension</b></summary>\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n</details>\n\n<details>\n<summary><b>Install in Windows</b></summary>\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Amazon Q Developer CLI</b></summary>\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Warp</b></summary>\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` > `AI` > `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n<details>\n\n<summary><b>Install in Copilot Coding Agent</b></summary>\n\n## Using Context7 with Copilot Coding Agent\n\nAdd the following configuration to the `mcp` section of your Copilot Coding Agent configuration file Repository->Settings->Copilot->Coding agent->MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in LM Studio</b></summary>\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) > `Install` > `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n</details>\n\n<details>\n<summary><b>Install in Visual Studio 2022</b></summary>\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\nAdd this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):\n\n```json\n{\n  \"inputs\": [],\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n</details>\n\n<details>\n<summary><b>Install in Crush</b></summary>\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in BoltAI</b></summary>\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n</details>\n\n<details>\n<summary><b>Install in Rovo Dev CLI</b></summary>\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zencoder</b></summary>\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n</details>\n\n<details>\n<summary><b>Install in Qodo Gen</b></summary>\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4. Add the following configuration:\n\n#### Qodo Gen Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Qodo Gen Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Perplexity Desktop</b></summary>\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` > `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n</details>\n\n## üî® Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## üõü Tips\n\n### Add a Rule\n\nIf you don‚Äôt want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings > Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you‚Äôll get Context7‚Äôs docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## üíª Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport <stdio|http>` ‚Äì Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port <number>` ‚Äì Port to listen on when using `http` transport (default `3000`).\n- `--api-key <key>` ‚Äì API key for authentication. You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).\n\nExample with HTTP transport and port 8080:\n\n```bash\nbun run dist/index.js --transport http --port 8080\n```\n\nAnother example with stdio transport:\n\n```bash\nbun run dist/index.js --transport stdio --api-key YOUR_API_KEY\n```\n\n<details>\n<summary><b>Local Configuration Example</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Testing with MCP Inspector</b></summary>\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n</details>\n\n## üö® Troubleshooting\n\n<details>\n<summary><b>Module Not Found Errors</b></summary>\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n</details>\n\n<details>\n<summary><b>ESM Resolution Issues</b></summary>\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>TLS/Certificate Issues</b></summary>\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>General MCP Client Errors</b></summary>\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n</details>\n\n## ‚ö†Ô∏è Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## ü§ù Connect with Us\n\nStay updated and join our community:\n\n- üì¢ Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- üåê Visit our [Website](https://context7.com)\n- üí¨ Join our [Discord Community](https://upstash.com/discord)\n\n## üì∫ Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline & RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In & Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ‚≠ê Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7&type=Date)](https://www.star-history.com/#upstash/context7&Date)\n\n## üìÑ License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "JamesANZ--memory-mcp": {
      "owner": "JamesANZ",
      "name": "memory-mcp",
      "url": "https://github.com/JamesANZ/memory-mcp",
      "imageUrl": "",
      "description": "An MCP server that stores and retrieves memories from multiple LLMs using MongoDB. Provides tools for saving, retrieving, adding, and clearing conversation memories with timestamps and LLM identification.",
      "stars": 6,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T22:15:16Z",
      "readme_content": "# Memory MCP\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/JamesANZ/memory-mcp)](https://archestra.ai/mcp-catalog/jamesanz__memory-mcp)\n\nA Model Context Protocol (MCP) server for logging and retrieving memories from LLM conversations with intelligent context window caching capabilities.\n\n## Features\n\n- **Save Memories**: Store memories from LLM conversations with timestamps and LLM identification\n- **Retrieve Memories**: Get all stored memories with detailed metadata\n- **Add Memories**: Append new memories without overwriting existing ones\n- **Clear Memories**: Remove all stored memories\n- **Context Window Caching**: Archive, retrieve, and summarize conversation context\n- **Relevance Scoring**: Automatically score archived content relevance to current context\n- **Tag-based Search**: Categorize and search context by tags\n- **Conversation Orchestration**: External system to manage context window caching\n- **MongoDB Storage**: Persistent storage using MongoDB database\n\n## Installation\n\n1. Install dependencies:\n\n```bash\nnpm install\n```\n\n2. Build the project:\n\n```bash\nnpm run build\n```\n\n## Configuration\n\nSet the MongoDB connection string via environment variable:\n\n```bash\nexport MONGODB_URI=\"mongodb://localhost:27017\"\n```\n\nDefault: `mongodb://localhost:27017`\n\n## Usage\n\n### Running the MCP Server\n\nStart the MCP server:\n\n```bash\nnpm start\n```\n\n### Running the Conversation Orchestrator Demo\n\nTry the interactive CLI demo:\n\n```bash\nnpm run cli\n```\n\nThe CLI demo allows you to:\n\n- Add messages to simulate conversation\n- See automatic archiving when context gets full\n- Trigger manual archiving and retrieval\n- Create summaries of archived content\n- Monitor conversation status and get recommendations\n\n### Basic Memory Tools\n\n1. **save-memories**: Save all memories to the database, overwriting existing ones\n   - `memories`: Array of memory strings to save\n   - `llm`: Name of the LLM (e.g., 'chatgpt', 'claude')\n   - `userId`: Optional user identifier\n\n2. **get-memories**: Retrieve all memories from the database\n   - No parameters required\n\n3. **add-memories**: Add new memories to the database without overwriting existing ones\n   - `memories`: Array of memory strings to add\n   - `llm`: Name of the LLM (e.g., 'chatgpt', 'claude')\n   - `userId`: Optional user identifier\n\n4. **clear-memories**: Clear all memories from the database\n   - No parameters required\n\n### Context Window Caching Tools\n\n5. **archive-context**: Archive context messages for a conversation with tags and metadata\n   - `conversationId`: Unique identifier for the conversation\n   - `contextMessages`: Array of context messages to archive\n   - `tags`: Tags for categorizing the archived content\n   - `llm`: Name of the LLM (e.g., 'chatgpt', 'claude')\n   - `userId`: Optional user identifier\n\n6. **retrieve-context**: Retrieve relevant archived context for a conversation\n   - `conversationId`: Unique identifier for the conversation\n   - `tags`: Optional tags to filter by\n   - `minRelevanceScore`: Minimum relevance score (0-1, default: 0.1)\n   - `limit`: Maximum number of items to return (default: 10)\n\n7. **score-relevance**: Score the relevance of archived context against current conversation context\n   - `conversationId`: Unique identifier for the conversation\n   - `currentContext`: Current conversation context to compare against\n   - `llm`: Name of the LLM (e.g., 'chatgpt', 'claude')\n\n8. **create-summary**: Create a summary of context items and link them to the summary\n   - `conversationId`: Unique identifier for the conversation\n   - `contextItems`: Context items to summarize\n   - `summaryText`: Human-provided summary text\n   - `llm`: Name of the LLM (e.g., 'chatgpt', 'claude')\n   - `userId`: Optional user identifier\n\n9. **get-conversation-summaries**: Get all summaries for a specific conversation\n   - `conversationId`: Unique identifier for the conversation\n\n10. **search-context-by-tags**: Search archived context and summaries by tags\n    - `tags`: Tags to search for\n\n### Example Usage in LLM\n\n#### Basic Memory Operations\n\n1. **Save all memories** (overwrites existing):\n\n   ```\n   User: \"Save all my memories from this conversation to the MCP server\"\n   LLM: [Uses save-memories tool with current conversation memories]\n   ```\n\n2. **Retrieve all memories**:\n   ```\n   User: \"Get all my memories from the MCP server\"\n   LLM: [Uses get-memories tool to retrieve stored memories]\n   ```\n\n#### Context Window Caching Workflow\n\n1. **Archive context when window gets full**:\n\n   ```\n   User: \"The conversation is getting long, archive the early parts\"\n   LLM: [Uses archive-context tool to store old messages with tags]\n   ```\n\n2. **Score relevance of archived content**:\n\n   ```\n   User: \"How relevant is the archived content to our current discussion?\"\n   LLM: [Uses score-relevance tool to evaluate archived content]\n   ```\n\n3. **Retrieve relevant archived context**:\n\n   ```\n   User: \"Bring back the relevant archived information\"\n   LLM: [Uses retrieve-context tool to get relevant archived content]\n   ```\n\n4. **Create summaries for long conversations**:\n   ```\n   User: \"Summarize the early parts of our conversation\"\n   LLM: [Uses create-summary tool to condense archived content]\n   ```\n\n## Conversation Orchestration System\n\nThe `ConversationOrchestrator` class provides automatic context window management:\n\n### Key Features\n\n- **Automatic Archiving**: Archives content when context usage reaches 80%\n- **Intelligent Retrieval**: Retrieves relevant content when usage drops below 30%\n- **Relevance Scoring**: Uses keyword overlap to score archived content relevance\n- **Smart Tagging**: Automatically generates tags based on content keywords\n- **Conversation State Management**: Tracks active conversations and their context\n- **Recommendations**: Provides suggestions for optimal context management\n\n### Usage Example\n\n```typescript\nimport { ConversationOrchestrator } from \"./orchestrator.js\";\n\nconst orchestrator = new ConversationOrchestrator(8000); // 8k word limit\n\n// Add a message (triggers automatic archiving/retrieval)\nconst result = await orchestrator.addMessage(\n  \"conversation-123\",\n  \"This is a new message in the conversation\",\n  \"claude\",\n);\n\n// Check if archiving is needed\nif (result.archiveDecision?.shouldArchive) {\n  await orchestrator.executeArchive(result.archiveDecision, result.state);\n}\n\n// Check if retrieval is needed\nif (result.retrievalDecision?.shouldRetrieve) {\n  await orchestrator.executeRetrieval(result.retrievalDecision, result.state);\n}\n```\n\n## Database Schema\n\n### Basic Memory Structure\n\n```typescript\ntype BasicMemory = {\n  _id: ObjectId;\n  memories: string[]; // Array of memory strings\n  timestamp: Date; // When memories were saved\n  llm: string; // LLM identifier (e.g., 'chatgpt', 'claude')\n  userId?: string; // Optional user identifier\n};\n```\n\n### Extended Memory Structure (Context Caching)\n\n```typescript\ntype ExtendedMemory = {\n  _id: ObjectId;\n  memories: string[]; // Array of memory strings\n  timestamp: Date; // When memories were saved\n  llm: string; // LLM identifier\n  userId?: string; // Optional user identifier\n  conversationId?: string; // Unique conversation identifier\n  contextType?: \"active\" | \"archived\" | \"summary\";\n  relevanceScore?: number; // 0-1 relevance score\n  tags?: string[]; // Categorization tags\n  parentContextId?: ObjectId; // Reference to original content for summaries\n  messageIndex?: number; // Order within conversation\n  wordCount?: number; // Size tracking\n  summaryText?: string; // Condensed version\n};\n```\n\n## Context Window Caching Workflow\n\nThe orchestration system automatically:\n\n1. **Monitors conversation length** and context usage\n2. **Archives content** when context usage reaches 80%\n3. **Scores relevance** of archived content against current context\n4. **Retrieves relevant content** when usage drops below 30%\n5. **Creates summaries** to condense very long conversations\n\n### Key Features\n\n- **Conversation Grouping**: All archived content is linked to specific conversation IDs\n- **Relevance Scoring**: Simple keyword overlap scoring (can be enhanced with semantic similarity)\n- **Tag-based Organization**: Categorize content for easy retrieval\n- **Summary Linking**: Preserve links between summaries and original content\n- **Backward Compatibility**: All existing memory functions work unchanged\n- **Automatic Management**: No manual intervention required for basic operations\n\n## Development\n\nTo run in development mode:\n\n```bash\nnpm run build\nnode build/index.js\n```\n\nTo run the CLI demo:\n\n```bash\nnpm run cli\n```\n\n## License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "JamesANZ--cross-llm-mcp": {
      "owner": "JamesANZ",
      "name": "cross-llm-mcp",
      "url": "https://github.com/JamesANZ/cross-llm-mcp",
      "imageUrl": "",
      "description": "An MCP server that enables cross-LLM communication and memory sharing, allowing different AI models to collaborate and share context across conversations.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T22:15:52Z",
      "readme_content": "# Cross-LLM MCP Server\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/JamesANZ/cross-llm-mcp)](https://archestra.ai/mcp-catalog/jamesanz__cross-llm-mcp)\n\nA Model Context Protocol (MCP) server that provides access to multiple Large Language Model (LLM) APIs including ChatGPT, Claude, and DeepSeek. This allows you to call different LLMs from within any MCP-compatible client and combine their responses.\n\n## Features\n\nThis MCP server offers five specialized tools for interacting with different LLM providers:\n\n### ü§ñ Individual LLM Tools\n\n#### `call-chatgpt`\n\nCall OpenAI's ChatGPT API with a prompt.\n\n**Input:**\n\n- `prompt` (string): The prompt to send to ChatGPT\n- `model` (optional, string): ChatGPT model to use (default: gpt-4)\n- `temperature` (optional, number): Temperature for response randomness (0-2, default: 0.7)\n- `max_tokens` (optional, number): Maximum tokens in response (default: 1000)\n\n**Output:**\n\n- ChatGPT response with model information and token usage statistics\n\n**Example:**\n\n```\nChatGPT Response\nModel: gpt-4\n\nHere's a comprehensive explanation of quantum computing...\n\n---\nUsage:\n- Prompt tokens: 15\n- Completion tokens: 245\n- Total tokens: 260\n```\n\n#### `call-claude`\n\nCall Anthropic's Claude API with a prompt.\n\n**Input:**\n\n- `prompt` (string): The prompt to send to Claude\n- `model` (optional, string): Claude model to use (default: claude-3-sonnet-20240229)\n- `temperature` (optional, number): Temperature for response randomness (0-1, default: 0.7)\n- `max_tokens` (optional, number): Maximum tokens in response (default: 1000)\n\n**Output:**\n\n- Claude response with model information and token usage statistics\n\n#### `call-deepseek`\n\nCall DeepSeek API with a prompt.\n\n**Input:**\n\n- `prompt` (string): The prompt to send to DeepSeek\n- `model` (optional, string): DeepSeek model to use (default: deepseek-chat)\n- `temperature` (optional, number): Temperature for response randomness (0-2, default: 0.7)\n- `max_tokens` (optional, number): Maximum tokens in response (default: 1000)\n\n**Output:**\n\n- DeepSeek response with model information and token usage statistics\n\n### üîÑ Combined Tools\n\n#### `call-all-llms`\n\nCall all available LLM APIs (ChatGPT, Claude, DeepSeek) with the same prompt and get combined responses.\n\n**Input:**\n\n- `prompt` (string): The prompt to send to all LLMs\n- `temperature` (optional, number): Temperature for response randomness (0-2, default: 0.7)\n- `max_tokens` (optional, number): Maximum tokens in response (default: 1000)\n\n**Output:**\n\n- Combined responses from all LLMs with individual model information and usage statistics\n- Summary of successful responses and total tokens used\n\n**Example:**\n\n```\nMulti-LLM Response\n\nPrompt: Explain quantum computing in simple terms\n\n---\n\n## CHATGPT\n\nModel: gpt-4\n\nQuantum computing is like having a super-powered computer...\n\n---\n\n## CLAUDE\n\nModel: claude-3-sonnet-20240229\n\nQuantum computing represents a fundamental shift...\n\n---\n\n## DEEPSEEK\n\nModel: deepseek-chat\n\nQuantum computing harnesses the principles of quantum mechanics...\n\n---\n\nSummary:\n- Successful responses: 3/3\n- Total tokens used: 1250\n```\n\n#### `call-llm`\n\nCall a specific LLM provider by name.\n\n**Input:**\n\n- `provider` (string): The LLM provider to call (\"chatgpt\", \"claude\", or \"deepseek\")\n- `prompt` (string): The prompt to send to the LLM\n- `model` (optional, string): Model to use (uses provider default if not specified)\n- `temperature` (optional, number): Temperature for response randomness (0-2, default: 0.7)\n- `max_tokens` (optional, number): Maximum tokens in response (default: 1000)\n\n**Output:**\n\n- Response from the specified LLM with model information and usage statistics\n\n## Installation\n\n1. Clone this repository:\n\n```bash\ngit clone <repository-url>\ncd cross-llm-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n## Getting API Keys\n\n### OpenAI/ChatGPT\n\n1. Visit [OpenAI Platform](https://platform.openai.com/api-keys)\n2. Sign up or log in to your account\n3. Create a new API key\n4. Add it to your `.env` file as `OPENAI_API_KEY`\n\n### Anthropic/Claude\n\n1. Visit [Anthropic Console](https://console.anthropic.com/)\n2. Sign up or log in to your account\n3. Create a new API key\n4. Add it to your `.env` file as `ANTHROPIC_API_KEY`\n\n### DeepSeek\n\n1. Visit [DeepSeek Platform](https://platform.deepseek.com/)\n2. Sign up or log in to your account\n3. Create a new API key\n4. Add it to your `.env` file as `DEEPSEEK_API_KEY`\n\n## Usage\n\n### Configuring Claude Desktop\n\nAdd the following configuration to your Claude Desktop MCP settings:\n\n```json\n{\n  \"cross-llm-mcp\": {\n    \"command\": \"node\",\n    \"args\": [\"/path/to/your/cross-llm-mcp/build/index.js\"],\n    \"cwd\": \"/path/to/your/cross-llm-mcp\",\n    \"env\": {\n      \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n      \"ANTHROPIC_API_KEY\": \"your_anthropic_api_key_here\",\n      \"DEEPSEEK_API_KEY\": \"your_deepseek_api_key_here\"\n    }\n  }\n}\n```\n\n**Replace the paths and API keys with your actual values:**\n\n- Update the `args` path to point to your `build/index.js` file\n- Update the `cwd` path to your project directory\n- Add your actual API keys to the `env` section\n\n### Running the Server\n\nThe server runs automatically when configured in Claude Desktop. You can also run it manually:\n\n```bash\nnpm start\n```\n\nThe server runs on stdio and can be connected to any MCP-compatible client.\n\n### Example Queries\n\nHere are some example queries you can make with this MCP server:\n\n#### Call ChatGPT\n\n```json\n{\n  \"tool\": \"call-chatgpt\",\n  \"arguments\": {\n    \"prompt\": \"Explain quantum computing in simple terms\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }\n}\n```\n\n#### Call Claude\n\n```json\n{\n  \"tool\": \"call-claude\",\n  \"arguments\": {\n    \"prompt\": \"What are the benefits of renewable energy?\",\n    \"model\": \"claude-3-sonnet-20240229\"\n  }\n}\n```\n\n#### Call All LLMs\n\n```json\n{\n  \"tool\": \"call-all-llms\",\n  \"arguments\": {\n    \"prompt\": \"Write a short poem about artificial intelligence\",\n    \"temperature\": 0.8\n  }\n}\n```\n\n#### Call Specific LLM\n\n```json\n{\n  \"tool\": \"call-llm\",\n  \"arguments\": {\n    \"provider\": \"deepseek\",\n    \"prompt\": \"Explain machine learning algorithms\",\n    \"max_tokens\": 800\n  }\n}\n```\n\n## Use Cases\n\n### 1. **Multi-Perspective Analysis**\n\nUse `call-all-llms` to get different perspectives on the same topic from multiple AI models.\n\n### 2. **Model Comparison**\n\nCompare responses from different LLMs to understand their strengths and weaknesses.\n\n### 3. **Redundancy and Reliability**\n\nIf one LLM is unavailable, you can still get responses from other providers.\n\n### 4. **Cost Optimization**\n\nChoose the most cost-effective LLM for your specific use case.\n\n### 5. **Quality Assurance**\n\nCross-reference responses from multiple models to validate information.\n\n## Configuration\n\n### Claude Desktop Setup\n\nThe recommended way to use this MCP server is through Claude Desktop with environment variables configured directly in the MCP settings:\n\n```json\n{\n  \"cross-llm-mcp\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"/Users/jamessangalli/Documents/projects/cross-llm-mcp/build/index.js\"\n    ],\n    \"cwd\": \"/Users/jamessangalli/Documents/projects/cross-llm-mcp\",\n    \"env\": {\n      \"OPENAI_API_KEY\": \"sk-proj-your-openai-key-here\",\n      \"ANTHROPIC_API_KEY\": \"sk-ant-your-anthropic-key-here\",\n      \"DEEPSEEK_API_KEY\": \"sk-your-deepseek-key-here\"\n    }\n  }\n}\n```\n\n### Environment Variables\n\nThe server reads the following environment variables:\n\n- `OPENAI_API_KEY`: Your OpenAI API key\n- `ANTHROPIC_API_KEY`: Your Anthropic API key\n- `DEEPSEEK_API_KEY`: Your DeepSeek API key\n- `DEFAULT_CHATGPT_MODEL`: Default ChatGPT model (default: gpt-4)\n- `DEFAULT_CLAUDE_MODEL`: Default Claude model (default: claude-3-sonnet-20240229)\n- `DEFAULT_DEEPSEEK_MODEL`: Default DeepSeek model (default: deepseek-chat)\n\n## API Endpoints\n\nThis MCP server uses the following API endpoints:\n\n- **OpenAI**: `https://api.openai.com/v1/chat/completions`\n- **Anthropic**: `https://api.anthropic.com/v1/messages`\n- **DeepSeek**: `https://api.deepseek.com/v1/chat/completions`\n\n## Error Handling\n\nThe server includes comprehensive error handling with detailed messages:\n\n### Missing API Key\n\n```\n**ChatGPT Error:** OpenAI API key not configured\n```\n\n### Invalid API Key\n\n```\n**Claude Error:** Claude API error: Invalid API key - please check your Anthropic API key\n```\n\n### Rate Limiting\n\n```\n**DeepSeek Error:** DeepSeek API error: Rate limit exceeded - please try again later\n```\n\n### Payment Issues\n\n```\n**ChatGPT Error:** ChatGPT API error: Payment required - please check your OpenAI billing\n```\n\n### Network Issues\n\n```\n**Claude Error:** Claude API error: Network timeout\n```\n\n### Supported Models\n\n#### ChatGPT Models\n\n- `gpt-4`\n- `gpt-4-turbo`\n- `gpt-3.5-turbo`\n- And other OpenAI models\n\n#### Claude Models\n\n- `claude-3-sonnet-20240229`\n- `claude-3-opus-20240229`\n- `claude-3-haiku-20240307`\n- And other Anthropic models\n\n#### DeepSeek Models\n\n- `deepseek-chat`\n- `deepseek-coder`\n- And other DeepSeek models\n\n## Project Structure\n\n```\ncross-llm-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Main MCP server with all 5 tools\n‚îÇ   ‚îú‚îÄ‚îÄ types.ts          # TypeScript type definitions\n‚îÇ   ‚îî‚îÄ‚îÄ llm-clients.ts    # LLM API client implementations\n‚îú‚îÄ‚îÄ build/                # Compiled JavaScript output\n‚îú‚îÄ‚îÄ env.example           # Environment variables template\n‚îú‚îÄ‚îÄ example-usage.md      # Detailed usage examples\n‚îú‚îÄ‚îÄ package.json          # Project dependencies and scripts\n‚îî‚îÄ‚îÄ README.md            # This file\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk` - MCP SDK for server implementation\n- `superagent` - HTTP client for API requests\n- `zod` - Schema validation for tool parameters\n\n## Development\n\n### Building the Project\n\n```bash\nnpm run build\n```\n\n### Adding New LLM Providers\n\nTo add a new LLM provider:\n\n1. Add the provider type to `src/types.ts`\n2. Implement the client in `src/llm-clients.ts`\n3. Add the tool to `src/index.ts`\n4. Update the `callAllLLMs` method to include the new provider\n\n## Troubleshooting\n\n### Common Issues\n\n**Server won't start**\n\n- Check that all dependencies are installed: `npm install`\n- Verify the build was successful: `npm run build`\n- Ensure the `.env` file exists and has valid API keys\n\n**API errors**\n\n- Verify your API keys are correct and active\n- Check your API usage limits and billing status\n- Ensure you're using supported model names\n\n**No responses**\n\n- Check that at least one API key is configured\n- Verify network connectivity\n- Look for error messages in the response\n\n### Debug Mode\n\nFor debugging, you can run the server directly:\n\n```bash\nnode build/index.js\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nIf you encounter any issues or have questions, please:\n\n1. Check the troubleshooting section above\n2. Review the error messages for specific guidance\n3. Ensure your API keys are properly configured\n4. Verify your network connectivity\n",
      "npm_url": "",
      "npm_downloads": 0
    }
  }
}