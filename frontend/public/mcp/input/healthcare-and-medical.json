{
  "category": "healthcare-and-medical",
  "categoryDisplay": "Healthcare and Medical",
  "description": "",
  "totalRepositories": 46,
  "repositories": {
    "1sustgmboab--nexonco-mcp": {
      "owner": "1sustgmboab",
      "name": "nexonco-mcp",
      "url": "https://github.com/1sustgmboab/nexonco-mcp",
      "imageUrl": "https://github.com/1sustgmboab.png",
      "description": "The NexonCo MCP Server is designed to help researchers and healthcare professionals analyze clinical evidence data for better treatment planning in precision medicine and oncology. It offers flexible search options and supports AI integration to enhance data insights.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T08:20:17Z",
      "readme_content": "# NexonCo MCP: Advanced Clinical Evidence Analysis Server üåü\n\n![NexonCo MCP](https://img.shields.io/badge/NexonCo_MCP-Server-brightgreen)\n\nWelcome to the **NexonCo MCP** repository! This project serves as an advanced MCP (Medical Care Platform) server designed for accessing and analyzing clinical evidence data. With flexible search options, it supports precision medicine and oncology research, making it a valuable tool for researchers and healthcare professionals.\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [API Documentation](#api-documentation)\n- [Contributing](#contributing)\n- [License](#license)\n- [Contact](#contact)\n- [Releases](#releases)\n\n## Introduction\n\nIn the rapidly evolving field of healthcare, access to reliable clinical evidence is crucial. The NexonCo MCP server provides a robust framework for researchers to query and analyze data efficiently. It is built with precision medicine in mind, allowing for tailored treatment plans based on individual patient data.\n\n## Features\n\n- **Flexible Search Options**: Users can search through a vast database of clinical evidence using various filters.\n- **User-Friendly Interface**: The platform offers an intuitive interface that simplifies data interaction.\n- **Integration with AI Tools**: Leverage AI capabilities to enhance data analysis and derive insights.\n- **Support for Oncology Research**: Special features tailored for oncology studies.\n- **Open Source**: Contribute to the project and help improve it for everyone.\n\n## Installation\n\nTo get started with the NexonCo MCP server, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/1sustgmboab/nexonco-mcp.git\n   ```\n\n2. **Navigate to the Directory**:\n   ```bash\n   cd nexonco-mcp\n   ```\n\n3. **Install Dependencies**:\n   Use your package manager to install the necessary dependencies. For example, if you're using npm:\n   ```bash\n   npm install\n   ```\n\n4. **Run the Server**:\n   After installing the dependencies, you can start the server:\n   ```bash\n   npm start\n   ```\n\n## Usage\n\nOnce the server is running, you can access it via your web browser. The main interface will guide you through the available features.\n\n### Searching for Clinical Evidence\n\n1. **Enter Search Terms**: Use the search bar to input keywords related to your research.\n2. **Apply Filters**: Narrow down results by applying various filters based on your requirements.\n3. **View Results**: Click on any entry to view detailed information about the clinical evidence.\n\n### Analyzing Data\n\n- Use built-in tools to analyze the retrieved data.\n- Generate reports based on your findings.\n\n## API Documentation\n\nThe NexonCo MCP server offers a comprehensive API for developers looking to integrate its features into their applications. \n\n### Endpoints\n\n- **GET /api/evidence**: Retrieve clinical evidence based on search parameters.\n- **POST /api/evidence**: Submit new clinical evidence for inclusion in the database.\n- **GET /api/reports**: Generate reports based on specified criteria.\n\nFor detailed API documentation, please refer to the [API Docs](https://github.com/1sustgmboab/nexonco-mcp/docs).\n\n## Contributing\n\nWe welcome contributions from the community! To contribute:\n\n1. Fork the repository.\n2. Create a new branch (`git checkout -b feature/YourFeature`).\n3. Make your changes.\n4. Commit your changes (`git commit -m 'Add some feature'`).\n5. Push to the branch (`git push origin feature/YourFeature`).\n6. Open a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](https://github.com/1sustgmboab/nexonco-mcp/LICENSE) file for details.\n\n## Contact\n\nFor any inquiries, please reach out via:\n\n- Email: contact@nexonco.com\n- GitHub Issues: [NexonCo MCP Issues](https://github.com/1sustgmboab/nexonco-mcp/issues)\n\n## Releases\n\nTo download the latest version of the NexonCo MCP server, visit our [Releases page](https://github.com/1sustgmboab/nexonco-mcp/releases). You can download and execute the files from there.\n\n---\n\nThank you for checking out the NexonCo MCP repository! We hope this tool helps you in your research endeavors. Your feedback and contributions are greatly appreciated!",
      "npm_url": "",
      "npm_downloads": 0
    },
    "acashmoney--bio-mcp": {
      "owner": "acashmoney",
      "name": "bio-mcp",
      "url": "https://github.com/acashmoney/bio-mcp",
      "imageUrl": "https://github.com/acashmoney.png",
      "description": "Enhances biomedical research through advanced protein structure analysis, enabling the examination of protein active sites and the discovery of disease-related proteins. It integrates with established protein databases for reliable insights and data access.",
      "stars": 17,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-27T01:43:39Z",
      "readme_content": "# BioMCP: Enabling agent-based biomedical R&D\n\n[![smithery badge](https://smithery.ai/badge/@acashmoney/bio-mcp)](https://smithery.ai/server/@acashmoney/bio-mcp)\n![BioMCP](cover.png)\n\n## Overview\n\nBioMCP is a Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server designed to enhance large language models with protein structure analysis capabilities. It provides tools for analyzing protein active sites and searching for disease-related proteins by interfacing with established protein databases. \n\nFuture work will be centered around enabling agents to utilize the BioMCP.\n\n## Features\n\n- **Active Site Analysis**: Examine the binding sites and functional residues of proteins using PDB IDs\n- **Disease-Protein Search**: Find protein structures associated with specific diseases or medical conditions\n- **Integrated Data Access**: Connect seamlessly with RCSB Protein Data Bank [(PDB)](https://www.rcsb.org/)\n\n## Technical Details\n\nBioMCP implements the Model Context Protocol, allowing language models to access specialized protein structure knowledge without requiring this information to be part of their training data. The server handles API connections, data formatting, and error handling to provide reliable protein structure insights.\n\n## API Endpoints\n\nBioMCP exposes two primary tools:\n\n1. `analyze-active-site`: Provides detailed information about protein binding sites using a PDB ID\n2. `search-disease-proteins`: Returns proteins related to specified diseases or medical conditions\n\n## Getting Started\n\n### Installing via Smithery\n\nTo install BioMCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@acashmoney/bio-mcp):\n\n```bash\nnpx -y @smithery/cli install @acashmoney/bio-mcp --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/acashmoney/bio-mcp.git\n\n# Install dependencies\nnpm install\n\n# Start the server\nnpm start\n```\n\n## Setup Instructions\n\n### Running the MCP Inspector\n\n1. Start the BioMCP server:\n   ```bash\n   npm start\n   ```\n\n2. In a separate terminal, install the MCP Inspector globally (if not already installed):\n   ```bash\n   npm install -g @anthropic-ai/mcp-inspector\n   ```\n\n3. Launch the MCP Inspector and connect to your local BioMCP server:\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n4. Use the inspector interface to test tools and view responses.\n\n### Using with Claude Desktop\n\n1. Build the BioMCP server:\n   ```bash\n   npm run build\n   ```\n\n2. Configure Claude Desktop to launch the MCP server:\n\n   a. Locate your Claude Desktop config.json file (typically in your user directory)\n   \n   b. Edit the config.json to include the BioMCP server build path. Example configuration:\n   ```json\n   {\n     \"globalShortcut\": \"\",\n     \"mcpServers\": {\n       \"bio-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n   \n   c. Replace `/path/to/your/build` with your actual path to the project directory.\n\n3. Restart Claude Desktop for the changes to take effect.\n\n4. You can now ask Claude questions that utilize the BioMCP tools:\n   - \"What are the key residues in the active site of PDB structure 6LU7?\"\n   - \"Find proteins related to Alzheimer's disease\"\n\n## Example Usage\n\nWhen integrated with a compatible language model, Bio-MCP enables queries like:\n\n- \"What are the key residues in the active site of PDB structure 6LU7?\"\n- \"Find proteins related to Alzheimer's disease\"\n\n## Requirements\n\n- Node.js 20.0.0 or higher\n- TypeScript 5.0+\n- Compatible MCP client implementation\n\n## Testing\n\nBioMCP includes a comprehensive testing suite with unit, integration, and end-to-end tests.\n\n### Running Tests\n\nRun all tests:\n```bash\nnpm test\n```\n\nRun specific test suites:\n```bash\n# Unit tests only\nnpm run test:unit\n\n# Integration tests only (API interactions)\nnpm run test:integration\n\n# End-to-end tests only\nnpm run test:e2e\n```\n\n### Linting\n\nCheck code quality:\n```bash\nnpm run lint\n```\n\nFix linting issues automatically:\n```bash\nnpm run lint:fix\n```\n\n## Roadmap\n\n- Expand level of detail for active site descriptions\n- Leverage 3-D coordinates\n- Tools for interfacing with literature\n- Tools for interfacing with computational biology models:\n  - RFdiffusion\n  - ProteinMPNN\n  - ColabFold\n  - Additional protein design and structure prediction tools\n- Agent-based research pipelines\n- Introduce client with protein visualization tools\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "ahmedjemaa-tech--medical-coding-reproducibility": {
      "owner": "ahmedjemaa-tech",
      "name": "medical-coding-reproducibility",
      "url": "https://github.com/ahmedjemaa-tech/medical-coding-reproducibility",
      "imageUrl": "https://github.com/ahmedjemaa-tech.png",
      "description": "Automates the process of assigning diagnosis and procedure codes from electronic health records. Utilizes advanced models to improve accuracy and efficiency in medical coding tasks, with tools and datasets from MIMIC-III and MIMIC-IV.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-03-29T10:21:50Z",
      "readme_content": "# ‚öïÔ∏èAutomated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study\n\nOfficial source code repository for the SIGIR 2023 paper [Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study](https://dl.acm.org/doi/10.1145/3539618.3591918)\n\n\n```bibtex\n@inproceedings{edinAutomatedMedicalCoding2023,\n  address = {Taipei, Taiwan},\n  title = {Automated {Medical} {Coding} on {MIMIC}-{III} and {MIMIC}-{IV}: {A} {Critical} {Review} and {Replicability} {Study}},\n  isbn = {978-1-4503-9408-6},\n  shorttitle = {Automated {Medical} {Coding} on {MIMIC}-{III} and {MIMIC}-{IV}},\n  doi = {10.1145/3539618.3591918},\n  booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},\n  publisher = {ACM Press},\n  author = {Edin, Joakim and Junge, Alexander and Havtorn, Jakob D. and Borgholt, Lasse and Maistro, Maria and Ruotsalo, Tuukka and Maal√∏e, Lars},\n  year = {2023}\n}\n```\n\n## Update\nWe released a new [paper](https://arxiv.org/pdf/2406.08958) and [repository](https://github.com/JoakimEdin/explainable-medical-coding/tree/main) for explainable medical coding. The new repository offers the following:\n- **Explainability**: Multiple feature attribution methods and metrics for multi-label classification. \n- **Implementation of a modified PLM-ICD**: We have fixed the problem of PLM-ICD occasionally collapsing during training.\n- **Huggingface Datasets**: we implemented MIMIC-III, IV, and MDACE as HuggingFace datasets.\n- **Inference code**: We provide code for inference without needing the training dataset.\nThe new repository no longer supports CNN, Bi-GRU, CAML, LAAT, and MultiResCNN.\n\nAlso, check out [my blog post](https://substack.com/home/post/p-145913061?source=queue) criticizing popular ideas in automated medical coding. I think it will be interesting for most researchers in the field\n\n## Introduction \nAutomatic medical coding is the task of automatically assigning diagnosis and procedure codes based on discharge summaries from electronic health records. This repository contains the code used in the paper Automated medical coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study. The repository contains code for training and evaluating medical coding models and new splits for MIMIC-III and the newly released MIMIC-IV. The following models have been implemented:\n\n| Model | Paper | Original Code |\n| ----- | ----- | ------------- |\n| CNN   |[Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100/) | [link](https://github.com/jamesmullenbach/caml-mimic) | \n| Bi-GRU|[Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100/) | [link](https://github.com/jamesmullenbach/caml-mimic) | \n|CAML   |[Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100/) | [link](https://github.com/jamesmullenbach/caml-mimic) | \n| MultiResCNN | [ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network](https://arxiv.org/pdf/1912.00862.pdf) | [link](https://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network) |\n| LAAT | [A Label Attention Model for ICD Coding from Clinical Text](https://arxiv.org/abs/2007.06351) | [link](https://github.com/aehrc/LAAT) |\n| PLM-ICD | [PLM-ICD: Automatic ICD Coding with Pretrained Language Models](https://aclanthology.org/2022.clinicalnlp-1.2/) | [link](https://github.com/MiuLab/PLM-ICD) |\n\nThe splits are found in `files/data`. The splits are described in the paper.\n\n## How to reproduce results\n### Setup Conda environment\n1. Create a conda environement `conda create -n coding python=3.10`\n2. Install the packages `pip install . -e`\n\n### Prepare MIMIC-III\nThis code has been developed on MIMIC-III v1.4. \n1. Download the MIMIC-III data into your preferred location `path/to/mimiciii`. Please note that you need to complete training to acces the data. The training is free, but takes a couple of hours.  - [link to data access](https://physionet.org/content/mimiciii/1.4/)\n2. Open the file `src/settings.py`\n3. Change the variable `DOWNLOAD_DIRECTORY_MIMICIII` to the path of your downloaded data `path/to/mimiciii`\n4. If you want to use the MIMIC-III full and MIMIC-III 50 from the [Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100/) you need to run `python prepare_data/prepare_mimiciii_mullenbach.py`\n5. If you want to use MIMIC-III clean from our paper you need to run `python prepare_data/prepare_mimiciii.py`\n\n### Prepare MIMIC-IV\nThis code has been developed on MIMIC-IV and MIMIC-IV v2.2. \n1. Download MIMIC-IV and MIMIC-IV-NOTE into your preferred location `path/to/mimiciv` and `path/to/mimiciv-note`. Please note that you need to complete training to acces the data. The training is free, but takes a couple of hours.  - [mimiciv](https://physionet.org/content/mimiciv/2.2/) and [mimiciv-note](https://physionet.org/content/mimic-iv-note/2.2/)\n2. Open the file `src/settings.py`\n3. Change the variable `DOWNLOAD_DIRECTORY_MIMICIV` to the path of your downloaded data `path/to/mimiciv`\n4. Change the variable `DOWNLOAD_DIRECTORY_MIMICIV_NOTE` to the path of your downloaded data `path/to/mimiciv-note`\n5. Run `python prepare_data/prepare_mimiciv.py`\n\n### Before running experiments\n1. Create a weights and biases account. It is possible to run the experiments without wandb.\n2. Download the [model checkpoints](https://drive.google.com/file/d/1hYeJhztAd-JbhqHojY7ZpLtkBcthD8AK/view?usp=share_link) and unzip it. Please note that these model weights can't be used commercially due to the MIMIC License.\n3. If you want to train PLM-ICD, you need to download [RoBERTa-base-PM-M3-Voc](https://dl.fbaipublicfiles.com/biolm/RoBERTa-base-PM-M3-Voc-hf.tar.gz), unzip it and change the `model_path` parameter in `configs/model/plm_icd.yaml` and `configs/text_transform\n/huggingface.yaml` to the path of the download. \n\n### Running experiments\n#### Training\nYou can run any experiment found in `configs/experiment`. Here are some examples:\n   * Train PLM-ICD on MIMIC-III clean on GPU 0: `python main.py experiment=mimiciii_clean/plm_icd gpu=0`\n   * Train CAML on MIMIC-III full on GPU 6: `python main.py experiment=mimiciii_full/caml gpu=6`\n   * Train LAAT on MIMIC-IV ICD-9 full on GPU 6: `python main.py experiment=mimiciv_icd9/laat gpu=6`\n   * Train LAAT on MIMIC-IV ICD-9 full on GPU 6 without weights and biases: `python main.py experiment=mimiciv_icd9/laat gpu=6 callbacks=no_wandb trainer.print_metrics=true`\n   \n#### Evaluation\nIf you just want to evaluate the models using the provided model_checkpoints you need to do set `trainer.epochs=0` and provide the path to the models checkpoint `load_model=path/to/model_checkpoint`. Make sure you the correct model-checkpoint with the correct configs.\n\nExample:\nEvaluate PLM-ICD on MIMIC-IV ICD-10 on GPU 1: `python main.py experiment=mimiciv_icd10/plm_icd gpu=1 load_model=path/to/model_checkpoints/mimiciv_icd10/plm_icd trainer.epochs=0`\n\n## Overview of the repository\n#### configs\nWe use [Hydra](https://hydra.cc/docs/intro/) for configurations. The condigs for every experiment is found in `configs/experiments`. Furthermore, the configuration for the sweeps are found in `configs/sweeps`. We used [Weights and Biases Sweeps](https://docs.wandb.ai/guides/sweeps) for most of our experiments.\n\n#### files\nThis is where the images and data is stored.\n\n#### notebooks\nThe directory only contains one notebook used for the code analysis. The notebook is not aimed to be used by others, but is included for others to validate our data analysis.\n\n#### prepare_data\nThe directory contains all the code for preparing the datasets and generating splits.\n\n#### reports\nThis is the code used to generate the plots and tables used in the paper. The code uses the Weights and Biases API to fetch the experiment results. The code is not usable by others, but was included for the possibility to validate our figures and tables.\n\n#### src\nThis is were the code for running the experiments is found.\n\n#### tests\nThe directory contains the unit tests\n\n## My setup\nI ran the experiments on one RTX 2080 Ti 11GB per experiment. I had 128 GB RAM on my machine.\n\n## ‚ö†Ô∏è Known issues \n* LAAT and PLM-ICD are unstable. The loss will sometimes diverge during training. The issue seems to be overflow in the softmax function in the label-wise attention. Using batch norm or layer norm before the softmax function might solve the issue. We did not try to fix the issue as we didn't want to change the original method during our reproducibility.\n* The code was only tested on a server with 128 GB RAM. A user with 32 GB RAM reported issues fitting MIMIC-IV into memory.\n* There is an error in the collate function in the Huggingface dataset. The attention mask is being padded with 1s instead of 0s. I have not fixed this issue because I want people to be able to reproduce the results from the paper.\n\n## Acknowledgement\nThank you Sotiris Lamprinidis for providing an efficient implementation of our multi-label stratification algorithm and some data preprocessing helper functions.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "bornpresident--Volatility-MCP-Server": {
      "owner": "bornpresident",
      "name": "Volatility-MCP-Server",
      "url": "https://github.com/bornpresident/Volatility-MCP-Server",
      "imageUrl": "https://github.com/bornpresident.png",
      "description": "Analyze memory dumps using natural language queries to facilitate forensic investigations, reducing the need for technical expertise while accelerating the analysis process and improving cybersecurity responses.",
      "stars": 22,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-23T07:54:57Z",
      "readme_content": "# Volatility MCP Server\n\nA Model Context Protocol (MCP) server that integrates Volatility 3 memory forensics framework with Claude and other MCP-compatible LLMs.\n\n## Why This Matters\n\nIn India, digital forensic investigators face a massive backlog of cases due to the country's large population and rising cybercrime rates. This tool helps address this challenge by:\n\n- Allowing investigators to analyze memory dumps using simple natural language instead of complex commands\n- Reducing the technical expertise needed to perform memory forensics\n- Accelerating the analysis process through automation\n- Helping clear case backlogs and deliver faster results to the judicial system\n\nBy making memory forensics more accessible, this tool can significantly reduce the burden on forensic experts and improve cybersecurity response across India.\n\n## Overview\n\nThis project bridges the powerful memory forensics capabilities of the Volatility 3 Framework with Large Language Models (LLMs) through the Model Context Protocol (MCP). It allows you to perform memory forensics analysis using natural language by exposing Volatility plugins as MCP tools that can be invoked directly by Claude or other MCP-compatible LLMs.\n\n## Features\n\n- **Natural Language Memory Forensics**: Ask Claude to analyze memory dumps using natural language\n- **Process Analysis**: Examine running processes, parent-child relationships, and hidden processes\n- **Network Forensics**: Identify network connections in memory dumps\n- **Malware Detection**: Find potential code injection and other malicious artifacts\n- **DLL Analysis**: Examine loaded DLLs and modules\n- **File Objects**: Scan for file objects in memory\n- **Custom Plugins**: Run any Volatility plugin with custom arguments\n- **Memory Dump Discovery**: Automatically find memory dumps in a directory\n\n## Requirements\n\n- Python 3.10 or higher\n- Volatility 3 Framework\n- Claude Desktop or other MCP-compatible client\n- MCP Python SDK (`mcp` package)\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/volatility-mcp-server.git\n   ```\n\n2. Install the required Python packages:\n   ```bash\n   pip install mcp httpx\n   ```\n\n3. Configure the Volatility path in the script:\n   - Open `volatility_mcp_server.py` and update the `VOLATILITY_DIR` variable to point to your Volatility 3 installation path.\n\n4. Configure Claude Desktop:\n   - Open your Claude Desktop configuration file located at:\n     - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the server configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"volatility\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/path/to/volatility_mcp_server.py\"\n         ],\n         \"env\": {\n           \"PYTHONPATH\": \"/path/to/volatility3\"\n         }\n       }\n     }\n   }\n   ```\n   - Replace `/path/to/` with the actual path to your files.\n\n5. Restart Claude Desktop to apply the changes.\n\n## Usage\n\nAfter setup, you can simply ask Claude natural language questions about your memory dumps:\n\n- \"List all processes in the memory dump at C:\\path\\to\\dump.vmem\"\n- \"Show me the network connections in C:\\path\\to\\dump.vmem\"\n- \"Run malfind to check for code injection in the memory dump\"\n- \"What DLLs are loaded in process ID 4328?\"\n- \"Check for hidden processes in C:\\path\\to\\dump.vmem\"\n\n## Available Tools\n\nThe server exposes the following Volatility plugins as MCP tools:\n\n1. `list_available_plugins` - Shows all Volatility plugins you can use\n2. `get_image_info` - Provides information about a memory dump file\n3. `run_pstree` - Shows the process hierarchy\n4. `run_pslist` - Lists processes from the process list\n5. `run_psscan` - Scans for processes including ones that might be hidden\n6. `run_netscan` - Shows network connections in the memory dump\n7. `run_malfind` - Detects potential code injection\n8. `run_cmdline` - Shows command line arguments for processes\n9. `run_dlllist` - Lists loaded DLLs for processes\n10. `run_handles` - Shows file handles and other system handles\n11. `run_filescan` - Scans for file objects in memory\n12. `run_memmap` - Shows the memory map for a specific process\n13. `run_custom_plugin` - Run any Volatility plugin with custom arguments\n14. `list_memory_dumps` - Find memory dumps in a directory\n\n## Memory Forensics Workflow\n\nThis MCP server enables a streamlined memory forensics workflow:\n\n1. **Initial Triage**:\n   - \"Show me the process tree in memory.vmem\"\n   - \"List all network connections in memory.vmem\"\n\n2. **Suspicious Process Investigation**:\n   - \"What command line was used to start process 1234?\"\n   - \"Show me all the DLLs loaded by process 1234\"\n   - \"What file handles are open in process 1234?\"\n\n3. **Malware Hunting**:\n   - \"Run malfind on memory.vmem to check for code injection\"\n   - \"Show me processes with unusual parent-child relationships\"\n   - \"Find hidden processes in memory.vmem\"\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Path Problems**:\n   - Make sure all paths are absolute and use double backslashes in Windows paths\n   - Check that the memory dump file exists and is readable\n\n2. **Permission Issues**:\n   - Run Claude Desktop as Administrator\n   - Check that Python and the Volatility directory have proper permissions\n\n3. **Volatility Errors**:\n   - Make sure Volatility 3 works correctly on its own\n   - Try running the same command directly in your command line\n\n4. **MCP Errors**:\n   - Check Claude Desktop logs for MCP errors\n   - Make sure the MCP Python package is installed correctly\n\n## Extending\n\nThis server can be extended by:\n\n1. Adding more Volatility plugins\n2. Creating custom analysis workflows\n3. Integrating with other forensic tools\n4. Adding report generation capabilities\n\n## License\n\n[MIT License](LICENSE)\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "ChristianHinge--dicom-mcp": {
      "owner": "ChristianHinge",
      "name": "dicom-mcp",
      "url": "https://github.com/ChristianHinge/dicom-mcp",
      "imageUrl": "https://github.com/ChristianHinge.png",
      "description": "Enables interaction with DICOM servers for querying patient information and analyzing medical imaging metadata. Extracts text from encapsulated PDF documents in DICOM format to enhance clinical report analysis and streamline medical data access.",
      "stars": 66,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T17:50:16Z",
      "readme_content": "# DICOM MCP Server for Medical Imaging Systems üè•\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n [![PyPI Version](https://img.shields.io/pypi/v/dicom-mcp.svg)](https://pypi.org/project/dicom-mcp/) [![PyPI Downloads](https://img.shields.io/pypi/dm/dicom-mcp.svg)](https://pypi.org/project/dicom-mcp/)  \n\nThe `dicom-mcp` server enables AI assistants to query, read, and move data on DICOM servers (PACS, VNA, etc.). \n\n<div align=\"center\">\n\nü§ù **[Contribute](#contributing)** ‚Ä¢\nüìù **[Report Bug](https://github.com/ChristianHinge/dicom-mcp/issues)**  ‚Ä¢\nüìù **[Blog Post 1](https://www.christianhinge.com/projects/dicom-mcp/)** \n\n</div>\n\n```text\n---------------------------------------------------------------------\nüßë‚Äç‚öïÔ∏è User: \"Any significant findings in John Doe's previous CT report?\"\n\nüß† LLM ‚Üí ‚öôÔ∏è Tools:\n   query_patients ‚Üí query_studies ‚Üí query_series ‚Üí extract_pdf_text_from_dicom\n\nüí¨ LLM Response: \"The report from 2025-03-26 mentions a history of splenomegaly (enlarged spleen)\"\n\nüßë‚Äç‚öïÔ∏è User: \"What's the volume of his spleen at the last scan and the scan today?\"\n\nüß† LLM ‚Üí ‚öôÔ∏è Tools:\n   (query_studies ‚Üí query_series ‚Üí move_series ‚Üí query_series ‚Üí extract_pdf_text_from_dicom) x2\n   (The move_series tool sends the latest CT to a DICOM segmentation node, which returns volume PDF report)\n\nüí¨ LLM Response: \"last year 2024-03-26: 412cm¬≥, today 2025-04-10: 350cm¬≥\"\n---------------------------------------------------------------------\n```\n\n\n## ‚ú® Core Capabilities\n\n`dicom-mcp` provides tools to:\n\n* **üîç Query Metadata**: Search for patients, studies, series, and instances using various criteria.\n* **üìÑ Read DICOM Reports (PDF)**: Retrieve DICOM instances containing encapsulated PDFs (e.g., clinical reports) and extract the text content.\n* **‚û°Ô∏è Send DICOM Images**: Send series or studies to other DICOM destinations, e.g. AI endpoints for image segmentation, classification, etc.\n* **‚öôÔ∏è Utilities**: Manage connections and understand query options.\n\n## üöÄ Quick Start\n### üì• Installation\nInstall using uv or pip:\n\n```bash\nuv tool install dicom-mcp\n```\nOr by cloning the repository:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/ChristianHinge/dicom-mcp\ncd dicom mcp\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[dev]\"\n```\n\n\n### ‚öôÔ∏è Configuration\n\n`dicom-mcp` requires a YAML configuration file (`config.yaml` or similar) defining DICOM nodes and calling AE titles. Adapt the configuration or keep as is for compatibility with the sample ORTHANC  Server.\n\n```yaml\nnodes:\n  main:\n    host: \"localhost\"\n    port: 4242 \n    ae_title: \"ORTHANC\"\n    description: \"Local Orthanc DICOM server\"\n\ncurrent_node: \"main\"\ncalling_aet: \"MCPSCU\" \n```\n> [!WARNING]\nDICOM-MCP is not meant for clinical use, and should not be connected with live hospital databases or databases with patient-sensitive data. Doing so could lead to both loss of patient data, and leakage of patient data onto the internet. DICOM-MCP can be used with locally hosted open-weight LLMs for complete data privacy. \n\n### (Optional) Sample ORTHANC server\nIf you don't have a DICOM server available, you can run a local ORTHANC server using Docker:\n\nClone the repository and install test dependencies `pip install -e \".[dev]`\n\n```bash\ncd tests\ndocker ocmpose up -d\ncd ..\npytest # uploads dummy pdf data to ORTHANC server\n```\nUI at [http://localhost:8042](http://localhost:8042)\n\n### üîå MCP Integration\n\nAdd to your client configuration (e.g. `claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"dicom\": {\n      \"command\": \"uv\",\n      \"args\": [\"tool\",\"dicom-mcp\", \"/path/to/your_config.yaml\"]\n    }\n  }\n}\n```\n\nFor development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/dicom-mcp\",\n                \"run\",\n                \"dicom-mcp\",\n                \"/path/to/your_config.yaml\"\n            ]\n        }\n    }\n}\n```\n\n\n## üõ†Ô∏è Tools Overview\n\n`dicom-mcp` provides four categories of tools for interaction with DICOM servers and DICOM data. \n\n### üîç Query Metadata\n\n* **`query_patients`**: Search for patients based on criteria like name, ID, or birth date.\n* **`query_studies`**: Find studies using patient ID, date, modality, description, accession number, or Study UID.\n* **`query_series`**: Locate series within a specific study using modality, series number/description, or Series UID.\n* **`query_instances`**: Find individual instances (images/objects) within a series using instance number or SOP Instance UID\n### üìÑ Read DICOM Reports (PDF)\n\n* **`extract_pdf_text_from_dicom`**: Retrieve a specific DICOM instance containing an encapsulated PDF and extract its text content.\n\n### ‚û°Ô∏è Send DICOM Images\n\n* **`move_series`**: Send a specific DICOM series to another configured DICOM node using C-MOVE.\n* **`move_study`**: Send an entire DICOM study to another configured DICOM node using C-MOVE.\n\n### ‚öôÔ∏è Utilities\n\n* **`list_dicom_nodes`**: Show the currently active DICOM node and list all configured nodes.\n* **`switch_dicom_node`**: Change the active DICOM node for subsequent operations.\n* **`verify_connection`**: Test the DICOM network connection to the currently active node using C-ECHO.\n* **`get_attribute_presets`**: List the available levels of detail (minimal, standard, extended) for metadata query results.<p>\n\n\n### Example interaction\nThe tools can be chained together to answer complex questions:\n\n\n<div align=\"center\">\n<img src=\"images/example.png\" alt=\"My Awesome Diagram\" width=\"700\">\n</div>\n\n\n## üìà Contributing\n### Running Tests\n\nTests require a running Orthanc DICOM server. You can use Docker:\n\n```bash\n# Navigate to the directory containing docker-compose.yml (e.g., tests/)\ncd tests\ndocker-compose up -d\n```\n\nRun tests using pytest:\n\n```bash\n# From the project root directory\npytest\n```\n\nStop the Orthanc container:\n\n```bash\ncd tests\ndocker-compose down\n```\n\n### Debugging\n\nUse the MCP Inspector for debugging the server communication:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run dicom-mcp /path/to/your_config.yaml --transport stdio\n```\n\n## üôè Acknowledgments\n\n* Built using [pynetdicom](https://github.com/pydicom/pynetdicom)\n* Uses [PyPDF2](https://pypi.org/project/PyPDF2/) for PDF text extraction\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "ctvidic--whoop-mcp-server": {
      "owner": "ctvidic",
      "name": "whoop-mcp-server",
      "url": "https://github.com/ctvidic/whoop-mcp-server",
      "imageUrl": "https://github.com/ctvidic.png",
      "description": "Access Whoop data for insights into cycles, recovery, strain, and workout metrics by querying the Whoop API. Retrieve data based on specific date ranges or calculate averages for strain over time.",
      "stars": 12,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-22T15:17:33Z",
      "readme_content": "# Whoop MCP Server\nPython Package License: MIT Python 3.12\n\nA Model Context Protocol (MCP) server that provides access to the Whoop API. It allows language models to query cycles, recovery, strain, and workout data from the Whoop API.\n\n## Available Tools\n\nThe server exposes the following tools:\n\n### Cycle Queries\n- `get_cycle_collection(start_date: str, end_date: str)`: Get cycle data for a specific date range\n- `get_latest_cycle()`: Get the most recent cycle data\n\n### Recovery and Strain\n- `get_recovery_data(start_date: str, end_date: str)`: Get recovery data for a specific date range\n- `get_strain_data(start_date: str, end_date: str)`: Get strain data for a specific date range\n- `get_average_strain(days: int = 7)`: Calculate average strain over specified number of days\n\n### Profile and Authentication\n- `get_profile()`: Get user profile information\n- `check_auth_status()`: Check authentication status with Whoop API\n\nDates should be provided in ISO format (YYYY-MM-DD).\n\n## Usage\n\nYou'll need Whoop credentials to use this server. The server uses email/password authentication with the Whoop API.\n\n### Claude for Desktop\n\nUpdate your `claude_desktop_config.json` (located in `~/Library/Application\\ Support/Claude/claude_desktop_config.json` on macOS and `%APPDATA%/Claude/claude_desktop_config.json` on Windows) to include the following:\n\n```json\n{\n    \"mcpServers\": {\n        \"Whoop\": {\n            \"command\": \"python\",\n            \"args\": [\"/path/to/whoop/src/whoop_server.py\"],\n            \"cwd\": \"/path/to/whoop\",\n            \"env\": {\n                \"WHOOP_EMAIL\": \"your.email@example.com\",\n                \"WHOOP_PASSWORD\": \"your_password\"\n            }\n        }\n    }\n}\n```\n\n### HTTP API Server\n\nThe project also includes an HTTP API server that exposes the same functionality over HTTP endpoints. To run it:\n\n```bash\n./run_whoop_server.sh\n```\n\n## Example Queries\n\nOnce connected, you can ask Claude questions like:\n\n- \"What's my recovery score for today?\"\n- \"Show me my strain data for the past week\"\n- \"What's my average strain over the last 7 days?\"\n- \"Get my latest cycle data\"\n\n## Error Handling\n\nThe server provides human-readable error messages for common issues:\n- Invalid date formats\n- API authentication errors\n- Network connectivity problems\n- Missing or invalid credentials\n\n## Project Structure\n\n```\nwhoop/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ whoop_server.py      # MCP server implementation\n‚îÇ   ‚îî‚îÄ‚îÄ whoop_http_server.py # HTTP API server implementation\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îî‚îÄ‚îÄ .env                 # Environment variables\n‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies\n‚îî‚îÄ‚îÄ run_whoop_server.sh     # Script to run HTTP server\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "DMontgomery40--mcp-local-server": {
      "owner": "DMontgomery40",
      "name": "mcp-local-server",
      "url": "https://github.com/DMontgomery40/mcp-local-server",
      "imageUrl": "https://github.com/DMontgomery40.png",
      "description": "Retrieve and analyze bird detection data with date and species filtering. Access audio recordings and generate detailed reports for enhanced birdwatching insights.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2024-12-15T21:16:17Z",
      "readme_content": "# BirdNet-Pi MCP Server\n\nA Python-based Model Context Protocol (MCP) server for BirdNet-Pi integration.\n\n## Features\n\n- Bird detection data retrieval with date and species filtering\n- Detection statistics and analysis\n- Audio recording access\n- Daily activity patterns\n- Report generation\n\n## Requirements\n\n- Python 3.8+\n- FastAPI\n- Uvicorn\n- Other dependencies listed in `requirements.txt`\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/YourUsername/mcp-server.git\ncd mcp-server\n```\n\n2. Create a virtual environment and activate it:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Set up your data directories:\n```bash\nmkdir -p data/audio data/reports\n```\n\n## Configuration\n\nThe server can be configured using environment variables:\n- `BIRDNET_DETECTIONS_FILE`: Path to detections JSON file (default: 'data/detections.json')\n- `BIRDNET_AUDIO_DIR`: Path to audio files directory (default: 'data/audio')\n- `BIRDNET_REPORT_DIR`: Path to reports directory (default: 'data/reports')\n\n## Running the Server\n\nStart the server:\n```bash\npython server.py\n```\n\nThe server will run on `http://localhost:8000`.\n\n## API Endpoints\n\n- `/functions` - List available functions (GET)\n- `/invoke` - Invoke a function (POST)\n\n### Available Functions\n\n1. `getBirdDetections`\n   - Get bird detections filtered by date range and species\n   - Parameters: startDate, endDate, species (optional)\n\n2. `getDetectionStats`\n   - Get detection statistics for a time period\n   - Parameters: period ('day', 'week', 'month', 'all'), minConfidence (optional)\n\n3. `getAudioRecording`\n   - Get audio recording for a detection\n   - Parameters: filename, format ('base64' or 'buffer')\n\n4. `getDailyActivity`\n   - Get bird activity patterns for a specific day\n   - Parameters: date, species (optional)\n\n5. `generateDetectionReport`\n   - Generate a report of detections\n   - Parameters: startDate, endDate, format ('html' or 'json')\n\n## Directory Structure\n\n```\nmcp-server/\n‚îú‚îÄ‚îÄ birdnet/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îú‚îÄ‚îÄ functions.py\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ audio/\n‚îÇ   ‚îî‚îÄ‚îÄ reports/\n‚îú‚îÄ‚îÄ server.py\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ README.md\n```",
      "npm_url": "",
      "npm_downloads": 0
    },
    "eka-care--eka_mcp_server": {
      "owner": "eka-care",
      "name": "eka_mcp_server",
      "url": "https://github.com/eka-care/eka_mcp_server",
      "imageUrl": "https://github.com/eka-care.png",
      "description": "Provides healthcare professionals with curated medical knowledge and drug information specific to India, enhancing AI responses by grounding them in verified medical data and treatment protocols. Facilitates access to extensive branded drug databases and treatment guidelines to improve clinical decision-making.",
      "stars": 19,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-11T13:27:27Z",
      "readme_content": "# Eka MCP Server\n[![License: MIT](https://img.shields.io/badge/license-MIT-C06524)](https://github.com/eka-care/eka_mcp_server/blob/main/LICENSE)\n[![PyPI - Version](https://img.shields.io/pypi/v/eka_mcp_server.svg)](https://pypi.org/project/eka_mcp_server)\n[![Downloads](https://static.pepy.tech/badge/eka_mcp_server/month)](https://pepy.tech/project/eka_mcp_server)\n\n## Overview\n\nEka Care's Model Context Protocol (MCP) server facilitates interaction with medical knowledge-bases specifically curated for the Indian healthcare context. While advanced models from Claude, OpenAI, and others can perform adequately in medical contexts, their responses often lack grounding in factual information and published references. Additionally, India faces a significant challenge with the absence of centralized repositories for branded medications in the public domain.\n\nThe Eka MCP Server addresses these challenges by providing structured access to curated knowledge-bases through specialized tools:\n\n* **Indian Branded Drug Search**: Enables lookup across 500,000+ branded drugs available in India, returning comprehensive metadata including generic composition and manufacturer information to enhance LLM responses.\n* **Indian Treatment Protocol Search**: Provides contextual access to over 180 treatment protocol documents published by authoritative Indian healthcare institutions such as ICMR and RSSDI.\n\n\nKey Benefits:\n* ü©∫ Medical Accuracy: Grounds AI responses in verified healthcare information\n* üîÑ Seamless Workflow: Provides critical information without requiring context switching\n* üõ°Ô∏è Reduced Hallucinations: Relies on curated medical data rather than AI's implicit general knowledge\n* üåê Open Ecosystem: Integrates with the growing MCP open standard\n\n# Get Started\n## Get your developer key from eka.care\n> [!NOTE]  \n> To obtain the `client-id`, and `client-token` reach out to us on ekaconnect@eka.care\n\n\n## Installation and Setup for Claude Desktop\n1. Install UV - https://docs.astral.sh/uv/getting-started/installation/#installation-methods\n2. Install Claude desktop application - https://claude.ai/download\n3. Locate the configuration file:\n   - **macOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n   \n   In case the file does not exist, create a new file named `claude_desktop_config.json` in the above directory.\n4. Modify/Create the configuration file with the following settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"eka-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"eka_mcp_server\",\n        \"--eka-api-host\",\n        \"https://api.eka.care\",\n        \"--client-id\",\n        \"<client_id>\",\n        \"--client-secret\",\n        \"<client_secret>\"\n      ]\n    }\n  }\n}\n```\n5. Replace the placeholder values:\n   - `<client_id>`: Your client ID\n   - `<client_secret>`: Your client secret\n\n## Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging experience, we recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx eka_mcp_server --eka-api-host https://api.eka.care --client-id <client_id> --client-secret <client_secret>\n```\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n\n## Troubleshooting common issues\n\n### spawn uvx ENOENT\nThis commonly happens when uvx is not installed or the command cannot be discovered.\n![spawn uvx ENOENT screenshot](assets/uvx_debug.png)\n\n\n1. Install uv through this command \n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Find the path of our uvx installation\n```bash\nwhich uvx\n```\nThe output might be something like this\n```\n> /opt/homebrew/bin/uvx\n```\n\nIn your config, update the command to the full path of the `uvx` executable. For example:\n```json\n{\n  \"mcpServers\": {\n    \"eka-mcp-server\": {\n      \"command\": \"/opt/homebrew/bin/uvx\",\n      \"args\": [\n        \"eka_mcp_server\",\n        \"--eka-api-host\",\n        \"https://api.eka.care\",\n        \"--client-id\",\n        \"<client_id>\",\n        \"--client-secret\",\n        \"<client_secret>\"\n      ]\n    } \n  }\n}\n```\n### Latest version of eka_mcp_server is not being picked?\nRun the command below in case the latest version is not being picked.\nThis cleans up the local cache and fetches the latest version.\n```\nuv cache clean eka_mcp_server\n```\n\n\n# Tools\n> EKA MCP server tools are curated by the in-house doctors at eka.care and have been validated on an internal set of questionnaire \n\n## Medications tool suite\n### Indian branded drug search \n<details>\n<summary>Tool definition here</summary>\nhttps://github.com/eka-care/eka_mcp_server/blob/9520c346e19c6ccafe80ca770dea9b824871ef1d/src/eka_mcp_server/constants.py#L1\n</details>\n\nAccess comprehensive information about drugs from a corpus of drugs based on the drug name or generic composition and filtered further through the drug form and volume.\n\n![Indian branded drug search](assets/indian_branded_drug_search.png)\n\nAPIs required for this tool\n   - https://developer.eka.care/api-reference/eka_mcp/medications/search \n\n### Indian Pharmacology details\n<details>\n<summary>Tool definition here</summary>\n</details>\n\nGet details of a generic composition based on the 2011 published guidelines by the National Formulary of India. \n\n\n\n## Indian treatment protocol search\n<details>\n<summary>Tool definition here</summary>\nhttps://github.com/eka-care/eka_mcp_server/blob/9520c346e19c6ccafe80ca770dea9b824871ef1d/src/eka_mcp_server/constants.py#L10\n</details>\n\nStandardized guidelines, procedures, and decision pathways for healthcare professionals are published by medical bodies.\nThey serve as comprehensive roadmaps for clinical care, ensuring consistent and evidence-based treatment approaches.\n\nCurrent Coverage:\n* 175 medical conditions/tags\n* 180 treatment protocols\n* Multiple authoritative publishers\n\n### Indian treatment protocol search workflow\n1. For any given query, the LLM has to decide if the tag is supported or not through [this API](http://developer.eka.care/api-reference/eka_mcp/protocols/tags). During the init of the tool, we fetch the supported conditions.\n2. Then, for the given tag, the LLM has to get the publishers that address that tag through [this API](http://developer.eka.care/api-reference/eka_mcp/protocols/publishers_by_tag).\n3. Finally, with the tag, publisher and query, we fetch the relevant information from the repository of publishers through [this API](http://developer.eka.care/api-reference/eka_mcp/protocols/search).\n\nAPIs required for this tool\n1. http://developer.eka.care/api-reference/eka_mcp/protocols/tags\n2. http://developer.eka.care/api-reference/eka_mcp/protocols/publishers_by_tag\n3. http://developer.eka.care/api-reference/eka_mcp/protocols/search\n\n![Indian treatment protocol search](assets/indian_treatment_protocol_search.png)\n\n## Accuracy Disclaimer\n\nThe Eka MCP Server provides access to medical knowledge bases and drug information intended to support healthcare professionals in India. While we strive for accuracy and reliability, please note:\n\n- The information provided through this service is for informational purposes only and does not constitute medical advice.\n- Healthcare professionals should exercise their own clinical judgment when using this information.\n- Drug information and treatment protocols may change over time, and we make reasonable efforts to keep our databases updated.\n- We cannot guarantee 100% accuracy or completeness of all information, particularly for newly approved medications or recently updated treatment guidelines.\n- Users should verify critical information through official sources before making clinical decisions.\n- Our database of protocols is ever growing, but does not ensure completeness.\n\nEka Care assumes no liability for any errors, omissions, or outcomes resulting from the use of information provided through this service.\n\n\n### Bugs and Issue Reporting\nPlease report any issues or bugs on the GitHub issue tracker.\n\n## FAQ\n**Q: Can I use this without an eka.care account?**\n\nA: No, you need valid API credentials from eka.care to access the medical information.\n\n**Q: Is this service free?**\n\nA: While the MCP server code is open-source, access to eka.care's APIs requires valid credentials.\nFor the initial few days, we are offering free access to the APIs. However, we will be charging for the API usage in the future.\n\n**Q: Which LLMs support MCP natively?**\n\nA: Currently, Anthropic's Claude models have native MCP support and also Cursor and Windsurf applications.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "evangstav--personal-mcp": {
      "owner": "evangstav",
      "name": "personal-mcp",
      "url": "https://github.com/evangstav/personal-mcp",
      "imageUrl": "https://github.com/evangstav.png",
      "description": "Track personal health, workouts, and nutrition with AI-assisted analysis and supportive features for performance optimization and rehabilitation. Provides tools for logging exercises, meals, and daily wellness insights.",
      "stars": 8,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T11:09:16Z",
      "readme_content": "# Personal MCP Server\n\n[![smithery badge](https://smithery.ai/badge/personal-mcp)](https://smithery.ai/server/personal-mcp)\n\nA Model Context Protocol server for personal health and well-being tracking. This server provides tools and resources for tracking workouts, nutrition, and daily journal entries, with AI-assisted analysis through Claude integration.\n\n## Features\n\n### Workout Tracking\n- Log exercises, sets, and reps\n- Track perceived effort and post-workout feelings\n- Calculate safe training weights with rehabilitation considerations\n- Historical workout analysis\n- Shoulder rehabilitation support\n- RPE-based load management\n\n### Nutrition Management\n- Log meals and individual food items\n- Track protein and calorie intake\n- Monitor hunger and satisfaction levels\n- Daily nutrition targets and progress\n- Pre/post workout nutrition tracking\n- Meal timing analysis\n\n### Journal System\n- Daily entries with mood and energy tracking\n- Sleep quality and stress level monitoring\n- Tag-based organization\n- Trend analysis and insights\n- Correlation analysis between workouts, nutrition, and well-being\n- Pattern recognition in mood and energy levels\n\n## Installation\n\n### Installing via Smithery\n\nTo install Personal Health Tracker for Claude Desktop automatically via [Smithery](https://smithery.ai/server/personal-mcp):\n\n```bash\nnpx -y @smithery/cli install personal-mcp --client claude\n```\n\n### Prerequisites\n- Python 3.10 or higher\n- pip or uv package manager\n\n### Using pip\n```bash\npip install -e .\n```\n\n### Development Installation\n```bash\ngit clone https://github.com/yourusername/personal-mcp.git\ncd personal-mcp\nuv pip install -e \".[dev]\"\n```\n\n## Usage\n\n### Basic Server\nRun the server with default settings:\n```bash\npersonal-mcp run\n```\n\n### Development Mode\nRun with hot reloading for development:\n```bash\npersonal-mcp dev\n```\n\n### MCP Inspector\nDebug with the MCP Inspector:\n```bash\npersonal-mcp inspect\n```\n\n### Claude Desktop Integration\nInstall to Claude Desktop:\n```bash\npersonal-mcp install --claude-desktop\n```\n\n### Configuration Options\n```bash\npersonal-mcp --help\n```\n\nAvailable options:\n- `--name`: Set server name (default: \"Personal Assistant\")\n- `--db-path`: Specify database location\n- `--dev`: Enable development mode\n- `--inspect`: Run with MCP Inspector\n- `-v, --verbose`: Enable verbose logging\n\n## MCP Tools\n\n### Workout Tools\n```python\n# Log a workout\nworkout = {\n    \"date\": \"2024-01-07\",\n    \"exercises\": [\n        {\n            \"name\": \"Bench Press\",\n            \"sets\": [\n                {\"weight\": 135, \"reps\": 10, \"rpe\": 7}\n            ]\n        }\n    ],\n    \"perceived_effort\": 8\n}\n\n# Calculate training weights\nparams = {\n    \"exercise\": \"Bench Press\",\n    \"base_weight\": 200,\n    \"days_since_surgery\": 90,\n    \"recent_pain_level\": 2,\n    \"recent_rpe\": 7\n}\n```\n\n### Nutrition Tools\n```python\n# Log a meal\nmeal = {\n    \"meal_type\": \"lunch\",\n    \"foods\": [\n        {\n            \"name\": \"Chicken Breast\",\n            \"amount\": 200,\n            \"unit\": \"g\",\n            \"protein\": 46,\n            \"calories\": 330\n        }\n    ],\n    \"hunger_level\": 7,\n    \"satisfaction_level\": 8\n}\n\n# Check nutrition targets\ntargets = await mcp.call_tool(\"check_nutrition_targets\", {\"date\": \"2024-01-07\"})\n```\n\n### Journal Tools\n```python\n# Create a journal entry\nentry = {\n    \"entry_type\": \"daily\",\n    \"content\": \"Great workout today...\",\n    \"mood\": 8,\n    \"energy\": 7,\n    \"sleep_quality\": 8,\n    \"stress_level\": 3,\n    \"tags\": [\"workout\", \"recovery\"]\n}\n\n# Analyze entries\nanalysis = await mcp.call_tool(\"analyze_journal_entries\", {\n    \"start_date\": \"2024-01-01\",\n    \"end_date\": \"2024-01-07\"\n})\n```\n\n## Development\n\n### Running Tests\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=personal_mcp\n\n# Run specific test file\npytest tests/test_database.py\n```\n\n### Code Quality\n```bash\n# Format code\nblack src/personal_mcp\n\n# Lint code\nruff check src/personal_mcp\n\n# Type checking\nmypy src/personal_mcp\n```\n\n## Project Structure\n```\npersonal-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ personal_mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ tools/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ workout.py\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ nutrition.py\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ journal.py\n‚îÇ       ‚îú‚îÄ‚îÄ database.py\n‚îÇ       ‚îú‚îÄ‚îÄ models.py\n‚îÇ       ‚îú‚îÄ‚îÄ resources.py\n‚îÇ       ‚îú‚îÄ‚îÄ prompts.py\n‚îÇ       ‚îî‚îÄ‚îÄ server.py\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_database.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_server.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_cli.py\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ mcp.json\n```\n\n## Contributing\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "flexpa--mcp-fhir": {
      "owner": "flexpa",
      "name": "mcp-fhir",
      "url": "https://github.com/flexpa/mcp-fhir",
      "imageUrl": "https://github.com/flexpa.png",
      "description": "Access and search FHIR resources with standardized formats for healthcare data. Interact with FHIR resources via URIs and utilize search capabilities for efficient data retrieval.",
      "stars": 55,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T17:08:53Z",
      "readme_content": "# @flexpa/mcp-fhir\n\n> [!WARNING]\n> This is an experimental demo not intended for production use.\n\nThis is a TypeScript-based MCP server that connects to a FHIR server. It provides core MCP functionality for interacting with FHIR resources by:\n\n- Accessing FHIR resources via URIs\n- Providing search capabilities for FHIR resources\n\n## Features\n\n### Resources\n\n> [!TIP]\n> \"Resources\" here refers to the MCP definition _not_ the FHIR one. MCP Resources are a core primitive in the Model Context Protocol (MCP) that allow servers to expose data and content that can be read by clients and used as context for LLM interactions.\n\n- List and access FHIR resources via `fhir://` URIs\n- Resources are returned in FHIR JSON format\n- Supports all FHIR Resource types available in the FHIR server's CapabilityStatement\n\n### Tools\n- `search_fhir` - Search FHIR resources\n  - Takes `resourceType` and `searchParams` as parameters\n  - Returns FHIR search results\n- `read_fhir` - Read an individual FHIR resource\n  - Takes `uri` as a parameter\n  - Returns the FHIR resource in JSON format\n\n## Configuration\n\nThe server requires the following environment variables:\n- `FHIR_BASE_URL`: The base URL of your FHIR server\n- `FHIR_ACCESS_TOKEN`: A SMART on FHIR access token for authentication\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"fhir\": {\n      \"command\": \"/path/to/@flexpa/mcp-fhir/build/index.js\"\n    },\n    \"env\": {\n      \"FHIR_BASE_URL\": \"<FHIR_BASE_URL>\",\n      \"FHIR_ACCESS_TOKEN\": \"<FHIR_ACCESS_TOKEN>\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "fluxinc--dicom-mcp-server": {
      "owner": "fluxinc",
      "name": "dicom-mcp-server",
      "url": "https://github.com/fluxinc/dicom-mcp-server",
      "imageUrl": "https://github.com/fluxinc.png",
      "description": "Facilitates DICOM connectivity testing by managing DICOM nodes and performing operations seamlessly through a standardized interface. Integrates with Claude's Model Context Protocol for efficient communication in DICOM workflows.",
      "stars": 3,
      "forks": 2,
      "license": "GNU Lesser General Public License v2.1",
      "language": "Python",
      "updated_at": "2025-09-28T13:54:45Z",
      "readme_content": "# DICOM MCP Server\n\nA Model Context Protocol (MCP) server for DICOM connectivity testing.\n\n## Prerequisites\n\n1. Install UV (required):\n   ```bash\n   pip install uv\n   ```\n   Make sure `uv` is available in your system PATH as it's required for Claude to properly execute the server.\n\n## Installation\n\nThere are two ways to set up the server:\n\n### 1. Traditional Setup\n\nInstall the required dependencies:\n\n```bash\nuv pip install mcp[cli]\n```\n\n### 2. MCP Installation (Recommended)\n\nTo use this server with Claude's Model Context Protocol:\n\n```bash\nmcp install server.py\n```\n\nThis will register the server with Claude for DICOM operations.\n\n## Running the Server\n\n### Direct Execution\n\n```bash\nuv run server.py\n```\n\n### Through Claude\n\nOnce installed via MCP, the server will be automatically managed by Claude when needed.\n\nThe server will start on 0.0.0.0:8080 by default.\n\n## Node Configuration\n\nThe server uses a `nodes.yaml` file to store DICOM node configurations. This allows you to:\n\n1. List all configured DICOM nodes\n2. Perform C-ECHO operations using node names instead of explicit AE titles, IPs, and ports\n3. Use different local AE titles for C-ECHO operations\n\n### nodes.yaml Format\n\n```yaml\nnodes:\n  # Example node configuration\n  main_pacs:\n    ae_title: DESTINATION\n    ip: 192.168.1.100\n    port: 104\n    description: \"Main hospital PACS system\"\n\nlocal_ae_titles:\n  - name: default\n    ae_title: MCP_DICOM\n    description: \"Default AE title for MCP DICOM server\"\n  \n  - name: pacs_gateway\n    ae_title: PACS_GATEWAY\n    description: \"PACS Gateway AE title\"\n```\n\n## Troubleshooting\n\nIf you encounter the \"spawn uv ENOENT\" error, it typically means one of the following:\n\n1. UV is not installed or not in your PATH\n2. The Python executable cannot be found by the MCP client\n\n### Solutions:\n\n1. Make sure UV is properly installed and in your PATH:\n   ```bash\n   which uv  # Should show the path to UV\n   ```\n\n2. Ensure you're using a Python environment that's accessible to the system:\n   - If using a virtual environment, make sure it's activated\n   - Check that Python is in your PATH\n\n3. Try running the server with explicit UV path:\n   ```bash\n   /full/path/to/uv run server.py\n   ```\n\n4. Add more debugging by checking the stderr output in the logs\n\n## Usage\n\nThe server provides several DICOM tools that can be used through the MCP interface:\n\n### List DICOM Nodes\n\nList all configured DICOM nodes from the nodes.yaml file:\n\n```\nlist_dicom_nodes()\n```\n\n### C-ECHO by Node Name\n\nPerform a C-ECHO operation using a node name from the configuration:\n\n```\ndicom_cecho_by_name(node_name=\"main_pacs\", local_ae_name=\"default\")\n```\n\n### Direct C-ECHO\n\nPerform a C-ECHO operation with explicit parameters:\n\n```\ndicom_cecho(remote_ae_title=\"REMOTE_AE\", ip=\"192.168.1.100\", port=104, local_ae_title=\"MCP_DICOM\")\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "genomoncology--biomcp": {
      "owner": "genomoncology",
      "name": "biomcp",
      "url": "https://github.com/genomoncology/biomcp",
      "imageUrl": "https://github.com/genomoncology.png",
      "description": "Connects AI models to authoritative biomedical databases for structured queries, enabling access to information from PubMed, ClinicalTrials.gov, and MyVariant.info. Facilitates precise responses about clinical trials, scientific literature, and genomic variants.",
      "stars": 317,
      "forks": 54,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T07:02:52Z",
      "readme_content": "# BioMCP: Biomedical Model Context Protocol\n\nBioMCP is an open source (MIT License) toolkit that empowers AI assistants and\nagents with specialized biomedical knowledge. Built following the Model Context\nProtocol (MCP), it connects AI systems to authoritative biomedical data\nsources, enabling them to answer questions about clinical trials, scientific\nliterature, and genomic variants with precision and depth.\n\n[![‚ñ∂Ô∏è Watch the video](./docs/blog/images/what_is_biomcp_thumbnail.png)](https://www.youtube.com/watch?v=bKxOWrWUUhM)\n\n## MCPHub Certification\n\nBioMCP is certified by [MCPHub](https://mcphub.com/mcp-servers/genomoncology/biomcp). This certification ensures that BioMCP follows best practices for Model Context Protocol implementation and provides reliable biomedical data access.\n\n## Why BioMCP?\n\nWhile Large Language Models have broad general knowledge, they often lack\nspecialized domain-specific information or access to up-to-date resources.\nBioMCP bridges this gap for biomedicine by:\n\n- Providing **structured access** to clinical trials, biomedical literature,\n  and genomic variants\n- Enabling **natural language queries** to specialized databases without\n  requiring knowledge of their specific syntax\n- Supporting **biomedical research** workflows through a consistent interface\n- Functioning as an **MCP server** for AI assistants and agents\n\n## Biomedical Data Sources\n\nBioMCP integrates with multiple biomedical data sources:\n\n### Literature Sources\n\n- **PubTator3/PubMed** - Peer-reviewed biomedical literature with entity annotations\n- **bioRxiv/medRxiv** - Preprint servers for biology and health sciences\n- **Europe PMC** - Open science platform including preprints\n\n### Clinical & Genomic Sources\n\n- **ClinicalTrials.gov** - Clinical trial registry and results database\n- **NCI Clinical Trials Search API** - National Cancer Institute's curated cancer trials database\n  - Advanced search filters (biomarkers, prior therapies, brain metastases)\n  - Organization and intervention databases\n  - Disease vocabulary with synonyms\n- **BioThings Suite** - Comprehensive biomedical data APIs:\n  - **MyVariant.info** - Consolidated genetic variant annotation\n  - **MyGene.info** - Real-time gene annotations and information\n  - **MyDisease.info** - Disease ontology and synonym information\n  - **MyChem.info** - Drug/chemical annotations and properties\n- **TCGA/GDC** - The Cancer Genome Atlas for cancer variant data\n- **1000 Genomes** - Population frequency data via Ensembl\n- **cBioPortal** - Cancer genomics portal with mutation occurrence data\n\n### Regulatory & Safety Sources\n\n- **OpenFDA** - FDA regulatory and safety data:\n  - **Drug Adverse Events (FAERS)** - Post-market drug safety reports\n  - **Drug Labels (SPL)** - Official prescribing information\n  - **Device Events (MAUDE)** - Medical device adverse events, with genomic device filtering\n\n## Available MCP Tools\n\nBioMCP provides 24 specialized tools for biomedical research:\n\n### Core Tools (3)\n\n#### 1. Think Tool (ALWAYS USE FIRST!)\n\n**CRITICAL**: The `think` tool MUST be your first step for ANY biomedical research task.\n\n```python\n# Start analysis with sequential thinking\nthink(\n    thought=\"Breaking down the query about BRAF mutations in melanoma...\",\n    thoughtNumber=1,\n    totalThoughts=3,\n    nextThoughtNeeded=True\n)\n```\n\nThe sequential thinking tool helps:\n\n- Break down complex biomedical problems systematically\n- Plan multi-step research approaches\n- Track reasoning progress\n- Ensure comprehensive analysis\n\n#### 2. Search Tool\n\nThe search tool supports two modes:\n\n##### Unified Query Language (Recommended)\n\nUse the `query` parameter with structured field syntax for powerful cross-domain searches:\n\n```python\n# Simple natural language\nsearch(query=\"BRAF melanoma\")\n\n# Field-specific search\nsearch(query=\"gene:BRAF AND trials.condition:melanoma\")\n\n# Complex queries\nsearch(query=\"gene:BRAF AND variants.significance:pathogenic AND articles.date:>2023\")\n\n# Get searchable fields schema\nsearch(get_schema=True)\n\n# Explain how a query is parsed\nsearch(query=\"gene:BRAF\", explain_query=True)\n```\n\n**Supported Fields:**\n\n- **Cross-domain**: `gene:`, `variant:`, `disease:`\n- **Trials**: `trials.condition:`, `trials.phase:`, `trials.status:`, `trials.intervention:`\n- **Articles**: `articles.author:`, `articles.journal:`, `articles.date:`\n- **Variants**: `variants.significance:`, `variants.rsid:`, `variants.frequency:`\n\n##### Domain-Based Search\n\nUse the `domain` parameter with specific filters:\n\n```python\n# Search articles (includes automatic cBioPortal integration)\nsearch(domain=\"article\", genes=[\"BRAF\"], diseases=[\"melanoma\"])\n\n# Search with mutation-specific cBioPortal data\nsearch(domain=\"article\", genes=[\"BRAF\"], keywords=[\"V600E\"])\nsearch(domain=\"article\", genes=[\"SRSF2\"], keywords=[\"F57*\"])  # Wildcard patterns\n\n# Search trials\nsearch(domain=\"trial\", conditions=[\"lung cancer\"], phase=\"3\")\n\n# Search variants\nsearch(domain=\"variant\", gene=\"TP53\", significance=\"pathogenic\")\n```\n\n**Note**: When searching articles with a gene parameter, cBioPortal data is automatically included:\n\n- Gene-level summaries show mutation frequency across cancer studies\n- Mutation-specific searches (e.g., \"V600E\") show study-level occurrence data\n- Cancer types are dynamically resolved from cBioPortal API\n\n#### 3. Fetch Tool\n\nRetrieve full details for a single article, trial, or variant:\n\n```python\n# Fetch article details (supports both PMID and DOI)\nfetch(domain=\"article\", id=\"34567890\")  # PMID\nfetch(domain=\"article\", id=\"10.1101/2024.01.20.23288905\")  # DOI\n\n# Fetch trial with all sections\nfetch(domain=\"trial\", id=\"NCT04280705\", detail=\"all\")\n\n# Fetch variant details\nfetch(domain=\"variant\", id=\"rs113488022\")\n```\n\n**Domain-specific options:**\n\n- **Articles**: `detail=\"full\"` retrieves full text if available\n- **Trials**: `detail` can be \"protocol\", \"locations\", \"outcomes\", \"references\", or \"all\"\n- **Variants**: Always returns full details\n\n### Individual Tools (21)\n\nFor users who prefer direct access to specific functionality, BioMCP also provides 21 individual tools:\n\n#### Article Tools (2)\n\n- **article_searcher**: Search PubMed/PubTator3 and preprints\n- **article_getter**: Fetch detailed article information (supports PMID and DOI)\n\n#### Trial Tools (5)\n\n- **trial_searcher**: Search ClinicalTrials.gov or NCI CTS API (via source parameter)\n- **trial_getter**: Fetch all trial details from either source\n- **trial_protocol_getter**: Fetch protocol information only (ClinicalTrials.gov)\n- **trial_references_getter**: Fetch trial publications (ClinicalTrials.gov)\n- **trial_outcomes_getter**: Fetch outcome measures and results (ClinicalTrials.gov)\n- **trial_locations_getter**: Fetch site locations and contacts (ClinicalTrials.gov)\n\n#### Variant Tools (2)\n\n- **variant_searcher**: Search MyVariant.info database\n- **variant_getter**: Fetch comprehensive variant details\n\n#### NCI-Specific Tools (6)\n\n- **nci_organization_searcher**: Search NCI's organization database\n- **nci_organization_getter**: Get organization details by ID\n- **nci_intervention_searcher**: Search NCI's intervention database (drugs, devices, procedures)\n- **nci_intervention_getter**: Get intervention details by ID\n- **nci_biomarker_searcher**: Search biomarkers used in trial eligibility criteria\n- **nci_disease_searcher**: Search NCI's controlled vocabulary of cancer conditions\n\n#### Gene, Disease & Drug Tools (3)\n\n- **gene_getter**: Get real-time gene information from MyGene.info\n- **disease_getter**: Get disease definitions and synonyms from MyDisease.info\n- **drug_getter**: Get drug/chemical information from MyChem.info\n\n**Note**: All individual tools that search by gene automatically include cBioPortal summaries when the `include_cbioportal` parameter is True (default). Trial searches can expand disease conditions with synonyms when `expand_synonyms` is True (default).\n\n## Quick Start\n\n### For Claude Desktop Users\n\n1. **Install `uv`** if you don't have it (recommended):\n\n   ```bash\n   # MacOS\n   brew install uv\n\n   # Windows/Linux\n   pip install uv\n   ```\n\n2. **Configure Claude Desktop**:\n   - Open Claude Desktop settings\n   - Navigate to Developer section\n   - Click \"Edit Config\" and add:\n   ```json\n   {\n     \"mcpServers\": {\n       \"biomcp\": {\n         \"command\": \"uv\",\n         \"args\": [\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"]\n       }\n     }\n   }\n   ```\n   - Restart Claude Desktop and start chatting about biomedical topics!\n\n### Python Package Installation\n\n```bash\n# Using pip\npip install biomcp-python\n\n# Using uv (recommended for faster installation)\nuv pip install biomcp-python\n\n# Run directly without installation\nuv run --with biomcp-python biomcp trial search --condition \"lung cancer\"\n```\n\n## Configuration\n\n### Environment Variables\n\nBioMCP supports optional environment variables for enhanced functionality:\n\n```bash\n# cBioPortal API authentication (optional)\nexport CBIO_TOKEN=\"your-api-token\"  # For authenticated access\nexport CBIO_BASE_URL=\"https://www.cbioportal.org/api\"  # Custom API endpoint\n\n# Performance tuning\nexport BIOMCP_USE_CONNECTION_POOL=\"true\"  # Enable HTTP connection pooling (default: true)\nexport BIOMCP_METRICS_ENABLED=\"false\"     # Enable performance metrics (default: false)\n```\n\n## Running BioMCP Server\n\nBioMCP supports multiple transport protocols to suit different deployment scenarios:\n\n### Local Development (STDIO)\n\nFor direct integration with Claude Desktop or local MCP clients:\n\n```bash\n# Default STDIO mode for local development\nbiomcp run\n\n# Or explicitly specify STDIO\nbiomcp run --mode stdio\n```\n\n### HTTP Server Mode\n\nBioMCP supports multiple HTTP transport protocols:\n\n#### Legacy SSE Transport (Worker Mode)\n\nFor backward compatibility with existing SSE clients:\n\n```bash\nbiomcp run --mode worker\n# Server available at http://localhost:8000/sse\n```\n\n#### Streamable HTTP Transport (Recommended)\n\nThe new MCP-compliant Streamable HTTP transport provides optimal performance and standards compliance:\n\n```bash\nbiomcp run --mode streamable_http\n\n# Custom host and port\nbiomcp run --mode streamable_http --host 127.0.0.1 --port 8080\n```\n\nFeatures of Streamable HTTP transport:\n\n- Single `/mcp` endpoint for all operations\n- Dynamic response mode (JSON for quick operations, SSE for long-running)\n- Session management support (future)\n- Full MCP specification compliance (2025-03-26)\n- Better scalability for cloud deployments\n\n### Deployment Options\n\n#### Docker\n\n```bash\n# Build the Docker image locally\ndocker build -t biomcp:latest .\n\n# Run the container\ndocker run -p 8000:8000 biomcp:latest biomcp run --mode streamable_http\n```\n\n#### Cloudflare Workers\n\nThe worker mode can be deployed to Cloudflare Workers for global edge deployment.\n\nNote: All APIs work without authentication, but tokens may provide higher rate limits.\n\n## Command Line Interface\n\nBioMCP provides a comprehensive CLI for direct database interaction:\n\n```bash\n# Get help\nbiomcp --help\n\n# Run the MCP server\nbiomcp run\n\n# Article search examples\nbiomcp article search --gene BRAF --disease Melanoma  # Includes preprints by default\nbiomcp article search --gene BRAF --no-preprints      # Exclude preprints\nbiomcp article get 21717063 --full\n\n# Clinical trial examples\nbiomcp trial search --condition \"Lung Cancer\" --phase PHASE3\nbiomcp trial search --condition melanoma --source nci --api-key YOUR_KEY  # Use NCI API\nbiomcp trial get NCT04280705 Protocol\nbiomcp trial get NCT04280705 --source nci --api-key YOUR_KEY  # Get from NCI\n\n# Variant examples with external annotations\nbiomcp variant search --gene TP53 --significance pathogenic\nbiomcp variant get rs113488022  # Includes TCGA, 1000 Genomes, and cBioPortal data by default\nbiomcp variant get rs113488022 --no-external  # Core annotations only\n\n# NCI-specific examples (requires NCI API key)\nbiomcp organization search \"MD Anderson\" --api-key YOUR_KEY\nbiomcp organization get ORG123456 --api-key YOUR_KEY\nbiomcp intervention search pembrolizumab --api-key YOUR_KEY\nbiomcp intervention search --type Device --api-key YOUR_KEY\nbiomcp biomarker search \"PD-L1\" --api-key YOUR_KEY\nbiomcp disease search melanoma --source nci --api-key YOUR_KEY\n```\n\n## Testing & Verification\n\nTest your BioMCP setup with the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run --with biomcp-python biomcp run\n```\n\nThis opens a web interface where you can explore and test all available tools.\n\n## Enterprise Version: OncoMCP\n\nOncoMCP extends BioMCP with GenomOncology's enterprise-grade precision oncology\nplatform (POP), providing:\n\n- **HIPAA-Compliant Deployment**: Secure on-premise options\n- **Real-Time Trial Matching**: Up-to-date status and arm-level matching\n- **Healthcare Integration**: Seamless EHR and data warehouse connectivity\n- **Curated Knowledge Base**: 15,000+ trials and FDA approvals\n- **Sophisticated Patient Matching**: Using integrated clinical and molecular\n  profiles\n- **Advanced NLP**: Structured extraction from unstructured text\n- **Comprehensive Biomarker Processing**: Mutation and rule processing\n\nLearn more: [GenomOncology](https://genomoncology.com/)\n\n## MCP Registries\n\n[![smithery badge](https://smithery.ai/badge/@genomoncology/biomcp)](https://smithery.ai/server/@genomoncology/biomcp)\n\n<a href=\"https://glama.ai/mcp/servers/@genomoncology/biomcp\">\n<img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@genomoncology/biomcp/badge\" />\n</a>\n\n## Example Use Cases\n\n### Gene Information Retrieval\n\n```python\n# Get comprehensive gene information\ngene_getter(gene_id_or_symbol=\"TP53\")\n# Returns: Official name, summary, aliases, links to databases\n```\n\n### Disease Synonym Expansion\n\n```python\n# Get disease information with synonyms\ndisease_getter(disease_id_or_name=\"GIST\")\n# Returns: \"gastrointestinal stromal tumor\" and other synonyms\n\n# Search trials with automatic synonym expansion\ntrial_searcher(conditions=[\"GIST\"], expand_synonyms=True)\n# Searches for: GIST OR \"gastrointestinal stromal tumor\" OR \"GI stromal tumor\"\n```\n\n### Integrated Biomedical Research\n\n```python\n# 1. Always start with thinking\nthink(thought=\"Analyzing BRAF V600E in melanoma treatment\", thoughtNumber=1)\n\n# 2. Get gene context\ngene_getter(\"BRAF\")\n\n# 3. Search for pathogenic variants\nvariant_searcher(gene=\"BRAF\", hgvsp=\"V600E\", significance=\"pathogenic\")\n\n# 4. Find relevant clinical trials with disease expansion\ntrial_searcher(conditions=[\"melanoma\"], interventions=[\"BRAF inhibitor\"])\n```\n\n## Documentation\n\nFor comprehensive documentation, visit [https://biomcp.org](https://biomcp.org)\n\n### Developer Guides\n\n- [HTTP Client Guide](./docs/http-client-guide.md) - Using the centralized HTTP client\n- [Migration Examples](./docs/migration-examples.md) - Migrating from direct HTTP usage\n- [Error Handling Guide](./docs/error-handling.md) - Comprehensive error handling patterns\n- [Integration Testing Guide](./docs/integration-testing.md) - Best practices for reliable integration tests\n- [Third-Party Endpoints](./THIRD_PARTY_ENDPOINTS.md) - Complete list of external APIs used\n- [Testing Guide](./docs/development/testing.md) - Running tests and understanding test categories\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests (including integration tests)\nmake test\n\n# Run only unit tests (excluding integration tests)\nuv run python -m pytest tests -m \"not integration\"\n\n# Run only integration tests\nuv run python -m pytest tests -m \"integration\"\n```\n\n**Note**: Integration tests make real API calls and may fail due to network issues or rate limiting.\nIn CI/CD, integration tests are run separately and allowed to fail without blocking the build.\n\n## BioMCP Examples Repo\n\nLooking to see BioMCP in action?\n\nCheck out the companion repository:\nüëâ **[biomcp-examples](https://github.com/genomoncology/biomcp-examples)**\n\nIt contains real prompts, AI-generated research briefs, and evaluation runs across different models.\nUse it to explore capabilities, compare outputs, or benchmark your own setup.\n\nHave a cool example of your own?\n**We‚Äôd love for you to contribute!** Just fork the repo and submit a PR with your experiment.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "gitkenan--doctair": {
      "owner": "gitkenan",
      "name": "doctair",
      "url": "https://github.com/gitkenan/doctair",
      "imageUrl": "https://github.com/gitkenan.png",
      "description": "Enables editing and deployment of applications through a web interface and local development environment, while synchronizing changes and facilitating project management with custom domain connections.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-22T14:25:22Z",
      "readme_content": "# ü©∫ Med AI Insight Viewer\n\n**A web application leveraging AI to analyze medical images and provide insightful descriptions.**\n\nThis application allows users to securely upload medical images (X-rays, MRIs, CT scans, etc.) and receive AI-generated analysis, including potential observations and structured descriptions. It utilizes OpenAI's vision capabilities via Supabase Edge Functions for analysis, along with Supabase for authentication and data persistence.\n\n---\n\n<!-- Add a screenshot or GIF demo here -->\n<!-- ![App Screenshot](link/to/screenshot.png) -->\n\n## ‚ú® Key Features\n\n*   **Secure User Authentication:** Google OAuth login via Supabase Auth ensures user data privacy.\n*   **Easy Image Upload:** Simple drag-and-drop or file selection interface for medical images.\n*   **AI-Powered Analysis:** Utilizes OpenAI's advanced vision models (e.g., `gpt-4-vision-preview`) to interpret images.\n*   **Structured Results:** Provides analysis in a clear format (e.g., description, potential findings, comments).\n*   **Analysis History:** Stores past analyses for user reference, secured by Row Level Security (RLS).\n*   **Responsive UI:** Built with Shadcn/ui and Tailwind CSS for a clean experience on desktop and mobile.\n\n## üöÄ Tech Stack\n\n*   **Frontend:**\n    *   Framework: React (Vite)\n    *   Language: TypeScript\n    *   UI Library: Shadcn/ui\n    *   Styling: Tailwind CSS\n    *   Routing: React Router DOM (`react-router-dom`)\n    *   State Management: React Context, `useState`, Supabase Auth Helpers\n    *   Notifications: `react-hot-toast` (via `useToast` hook), `sonner`\n    *   Markdown Rendering: `markdown-to-jsx`\n*   **Backend:**\n    *   Platform: Supabase\n    *   Authentication: Supabase Auth (Google OAuth configured)\n    *   Database: Supabase PostgreSQL\n    *   Serverless Functions: Supabase Edge Functions (Deno Runtime)\n*   **AI:**\n    *   Model Provider: OpenAI\n    *   API Interaction: Via Supabase Edge Function\n\n## üìÅ Project Structure\n\n.\n‚îú‚îÄ‚îÄ public/ # Static assets (icons, robots.txt)\n‚îú‚îÄ‚îÄ src/ # Frontend React application source\n‚îÇ ‚îú‚îÄ‚îÄ components/ # Reusable React components\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ui/ # Shadcn UI components\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ AnalysisResult.tsx # Displays AI analysis results\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ApiKeyInput.tsx # (Legacy/Client-side check - Not used for Backend API call)\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Header.tsx # Application header with navigation/logout\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ HistoryList.tsx # Displays list of past analyses\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ImageUpload.tsx # Handles image selection and preview\n‚îÇ ‚îú‚îÄ‚îÄ hooks/ # Custom React hooks (use-toast, use-mobile)\n‚îÇ ‚îú‚îÄ‚îÄ lib/ # Utility functions (cn)\n‚îÇ ‚îú‚îÄ‚îÄ pages/ # Top-level route components (Index, Dashboard, NotFound)\n‚îÇ ‚îú‚îÄ‚îÄ types/ # TypeScript type definitions\n‚îÇ ‚îú‚îÄ‚îÄ utils/ # Utility functions for external services (openai.ts - calls backend)\n‚îÇ ‚îú‚îÄ‚îÄ App.css # Basic App styles (potentially removable)\n‚îÇ ‚îú‚îÄ‚îÄ App.tsx # Main application component, routing, Supabase context\n‚îÇ ‚îú‚îÄ‚îÄ index.css # Tailwind directives and base styles\n‚îÇ ‚îú‚îÄ‚îÄ main.tsx # Application entry point\n‚îÇ ‚îî‚îÄ‚îÄ vite-env.d.ts # Vite TypeScript env declarations\n‚îú‚îÄ‚îÄ supabase/ # Supabase backend configuration and code\n‚îÇ ‚îú‚îÄ‚îÄ functions/ # Supabase Edge Functions\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ _shared/ # Shared code for functions (cors.ts)\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ analyze-image/ # Edge Function for OpenAI image analysis\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îî‚îÄ‚îÄ migrations/ # Database schema migrations (.sql)\n‚îú‚îÄ‚îÄ .gitignore # Git ignore rules\n‚îú‚îÄ‚îÄ components.json # Shadcn UI configuration\n‚îú‚îÄ‚îÄ eslint.config.js # ESLint configuration\n‚îú‚îÄ‚îÄ index.html # Main HTML entry point for Vite\n‚îú‚îÄ‚îÄ package.json # Project dependencies and scripts\n‚îú‚îÄ‚îÄ postcss.config.js # PostCSS configuration\n‚îú‚îÄ‚îÄ README.md # This file\n‚îú‚îÄ‚îÄ tailwind.config.ts # Tailwind CSS configuration\n‚îú‚îÄ‚îÄ tsconfig.app.json # TypeScript config for the app\n‚îú‚îÄ‚îÄ tsconfig.json # Base TypeScript config\n‚îú‚îÄ‚îÄ tsconfig.node.json # TypeScript config for Node env (Vite config)\n‚îî‚îÄ‚îÄ vite.config.ts # Vite build configuration\n\n\n## ‚öôÔ∏è Core Functionality & Workflow\n\n1.  **Authentication (`src/App.tsx`, `src/pages/Index.tsx`):**\n    *   Users land on the `Index` page.\n    *   Clicking \"Get Started\" initiates the Supabase Google OAuth flow.\n    *   Upon successful login, Supabase redirects back to the app (specifically `/dashboard` as configured in the OAuth options).\n    *   The `RequireAuth` component in `App.tsx` verifies the Supabase session using `useSession`. Authenticated users can access `/dashboard`; others are redirected to `/`.\n    *   The `Header` component provides a logout button which calls `supabase.auth.signOut()`.\n\n2.  **Image Upload (`src/pages/Dashboard.tsx`, `src/components/ImageUpload.tsx`):**\n    *   On the `Dashboard`, the `ImageUpload` component allows users to drag & drop or select an image file.\n    *   A preview of the selected image is displayed.\n    *   The selected `File` object and a base64 representation (`imagePreview`) are stored in the `Dashboard` component's state.\n\n3.  **Analysis Process:**\n    *   **Trigger:** The user clicks the \"Analyze Image\" button on the `Dashboard`.\n    *   **Frontend (`src/pages/Dashboard.tsx`, `src/utils/openai.ts`):**\n        *   The `analyzeImage` function in `Dashboard.tsx` is called.\n        *   It calls the utility function `analyzeImageApi` from `src/utils/openai.ts`.\n        *   `analyzeImageApi` gets the current Supabase session token.\n        *   It makes a `POST` request to the Supabase Edge Function endpoint (`/functions/v1/analyze-image`).\n        *   The request includes the `Authorization: Bearer <token>` header and a JSON body containing `imageBase64` and `imageType`.\n        *   It handles the response from the Edge Function.\n    *   **Backend (`supabase/functions/analyze-image/index.ts`):**\n        *   The Edge Function receives the request.\n        *   It validates the incoming JWT using the Supabase client initialized with the user's token.\n        *   It retrieves the **securely stored OpenAI API key** from the Edge Function's environment variables (`Deno.env.get('OPENAI_API_KEY')`). **The client-side key is NOT used here.**\n        *   It formats the `imageBase64` string into a data URL if necessary.\n        *   It constructs a request to the OpenAI API (`gpt-4.1-mini` or similar vision model specified in the function), sending the image URL and a specific prompt asking for medical observations/hypothetical diagnosis.\n        *   It receives the analysis text from OpenAI.\n        *   It structures the response into the `AnalysisResultType` format, adding a timestamp.\n        *   It saves the `imageType` and the structured `result` (as JSONB) to the `users_history` table in the Supabase database, linking it to the authenticated `user_id`.\n        *   It returns the newly created database record (containing the result) to the frontend.\n    *   **Frontend (`src/pages/Dashboard.tsx`):**\n        *   Receives the analysis result from the utility function.\n        *   Updates the `analysisResult` state variable.\n        *   Displays a success toast notification.\n        *   The `AnalysisResult` component re-renders to display the new data.\n\n4.  **Result Display (`src/components/AnalysisResult.tsx`):**\n    *   Renders the `AnalysisResultType` data passed via props.\n    *   Displays the image preview alongside the AI-generated content.\n    *   Uses `markdown-to-jsx` to render the analysis content, allowing for formatted text from the AI.\n    *   Includes a crucial disclaimer about the analysis not being professional medical advice.\n\n5.  **History (`src/pages/Dashboard.tsx`, `src/components/HistoryList.tsx`):**\n    *   The \"History\" tab on the `Dashboard` renders the `HistoryList` component.\n    *   `HistoryList` uses the Supabase JS client (`useSupabaseClient`) to fetch records from the `users_history` table, ordered by creation date.\n    *   Supabase RLS policies ensure only the currently logged-in user's history is returned.\n    *   Displays a list of past analyses, showing image type, a snippet of the diagnosis, and timestamp.\n\n## üíæ Backend Details\n\n### Supabase Edge Function (`analyze-image`)\n\n*   **Purpose:** Securely interacts with the OpenAI API using a server-side secret key and stores results.\n*   **Trigger:** HTTP POST request to `/functions/v1/analyze-image`.\n*   **Authentication:** Requires a valid Supabase JWT in the `Authorization` header.\n*   **Environment Variables:** Requires `SUPABASE_URL`, `SUPABASE_ANON_KEY`, and `OPENAI_API_KEY` to be set in the Edge Function settings.\n*   **Input:** JSON `{ imageBase64: string, imageType: string }`.\n*   **Processing:**\n    1.  Authenticates user via JWT.\n    2.  Retrieves `OPENAI_API_KEY` secret.\n    3.  Calls OpenAI Chat Completions API with vision model.\n    4.  Parses OpenAI response.\n    5.  Inserts result into `users_history` table using the authenticated user's ID.\n*   **Output:** JSON containing the newly created database entry (`{ result: UserHistoryItem }`).\n\n### Supabase Database Schema (`users_history`)\n\n*   **Table:** `public.users_history`\n*   **Purpose:** Stores the results of image analyses linked to users.\n*   **Columns:**\n    *   `id` (uuid, PK): Unique identifier for the history entry.\n    *   `user_id` (uuid, FK -> `auth.users`): Links the entry to the authenticated user.\n    *   `image_url` (text, nullable): *Currently seems unused in the primary analysis flow which uses base64.* Could be used if storing uploaded images directly.\n    *   `image_type` (text, not null): Type of the analyzed image (e.g., \"X-ray\", \"MRI\").\n    *   `result` (jsonb, not null): Stores the structured `AnalysisResultType` object returned by the AI.\n    *   `created_at` (timestamptz, default now()): Timestamp of when the analysis was performed.\n*   **Row Level Security (RLS):**\n    *   **Enabled:** Yes.\n    *   **Policies:**\n        *   Users can `SELECT` only their own history records (`auth.uid() = user_id`).\n        *   Users can `INSERT` only records where `user_id` matches their own `auth.uid()`.\n\n## üõ†Ô∏è Getting Started\n\n### Prerequisites\n\n*   Node.js (v18 or later recommended)\n*   npm, yarn, or pnpm\n*   Git\n*   Supabase Account\n*   Supabase CLI (Optional, for local development)\n*   OpenAI API Key\n\n### Installation & Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd med-ai-insight-viewer\n    ```\n\n2.  **Install frontend dependencies:**\n    ```bash\n    npm install\n    # or yarn install or pnpm install\n    ```\n\n3.  **Set up Environment Variables:**\n    *   Create a `.env` file in the root directory.\n    *   Add your Supabase Project URL and Anon Key:\n        ```env\n        VITE_SUPABASE_URL=YOUR_SUPABASE_PROJECT_URL\n        VITE_SUPABASE_ANON_KEY=YOUR_SUPABASE_ANON_KEY\n        ```\n    *   You can find these in your Supabase project settings (Project Settings > API).\n\n4.  **Supabase Setup:**\n    *   **Option A: Supabase Cloud (Recommended for deployment)**\n        1.  Go to your Supabase project dashboard.\n        2.  **Authentication:** Navigate to Authentication > Providers and enable the \"Google\" provider. Add your Google Cloud OAuth credentials. Ensure you add your app's URL(s) (including localhost for development) to the \"Redirect URLs\" section in Supabase Auth settings *and* in your Google Cloud OAuth configuration.\n        3.  **Database:** Navigate to the SQL Editor. Copy the contents of `supabase/migrations/20250421000000_initial_schema.sql` and run it to create the `users_history` table and RLS policies.\n        4.  **Edge Functions:**\n            *   Navigate to Edge Functions.\n            *   Deploy the `analyze-image` function (e.g., using `supabase functions deploy analyze-image --no-verify-jwt` if testing locally first, or set up CI/CD).\n            *   Go to the `analyze-image` function's settings > Secrets and add your `OPENAI_API_KEY`.\n    *   **Option B: Supabase Local Development**\n        1.  Initialize Supabase locally: `supabase init`\n        2.  Start Supabase services: `supabase start`\n        3.  Apply database migrations: `supabase db push` (or link your project `supabase link --project-ref <your-project-ref>` and pull schema changes if needed).\n        4.  Set Edge Function secrets locally: `supabase secrets set OPENAI_API_KEY=YOUR_OPENAI_API_KEY`\n        5.  (You'll need to configure Google Auth locally or use email/password for testing if not using the cloud setup). Use the local Supabase URL/keys in your `.env`.\n\n5.  **Run the Frontend:**\n    ```bash\n    npm run dev\n    ```\n    The application should now be running, typically at `http://localhost:8080`.\n\n6.  **Deploy Edge Function (if not done in step 4):**\n    ```bash\n    # Link to your project if you haven't already\n    # supabase link --project-ref <your-project-ref>\n\n    # Deploy the function\n    supabase functions deploy analyze-image\n\n    # IMPORTANT: Set the secret in the Supabase Dashboard (Settings > Edge Functions > analyze-image > Secrets)\n    # Add OPENAI_API_KEY with your actual OpenAI key value.\n    ```\n\n## üîß Configuration\n\n*   **OpenAI Model:** The AI model used for analysis is specified in `supabase/functions/analyze-image/index.ts` (currently hardcoded, likely `gpt-4.1-mini` or similar).\n*   **Analysis Prompt:** The prompt sent to OpenAI is also defined within the `analyze-image` Edge Function. Modify this to change the AI's behavior or the desired output format.\n*   **UI Theme:** Colors and styles can be adjusted in `src/index.css` (CSS variables) and `tailwind.config.ts`.\n*   **Shadcn UI:** Components can be added or customized using the Shadcn CLI and `components.json`.\n\n## üí° Usage\n\n1.  Open the application in your browser.\n2.  Log in using your Google account.\n3.  Navigate to the \"Analyze Image\" tab.\n4.  Upload a medical image using the drag-and-drop area or the file selector.\n5.  Click the \"Analyze Image\" button.\n6.  Wait for the analysis to complete (a loading indicator will show).\n7.  View the structured results displayed below the upload section.\n8.  Navigate to the \"History\" tab to view past analyses.\n\n## ‚ö†Ô∏è Disclaimer\n\n**This application is for informational and demonstration purposes only. The AI-generated analysis is NOT a substitute for professional medical advice, diagnosis, or treatment.** Always consult with a qualified healthcare provider regarding any medical conditions or concerns. Do not disregard professional medical advice or delay in seeking it because of something you have read or seen using this application.\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit issues or pull requests.\n\n1.  Fork the repository.\n2.  Create a new branch (`git checkout -b feature/your-feature-name`).\n3.  Make your changes.\n4.  Commit your changes (`git commit -m 'Add some feature'`).\n5.  Push to the branch (`git push origin feature/your-feature-name`).\n6.  Open a Pull Request.\n\n## üìÑ License\n\n(Specify License - e.g., MIT, Apache 2.0. If none, state \"All Rights Reserved.\")",
      "npm_url": "",
      "npm_downloads": 0
    },
    "HealthNoteLabs--Npub.Health": {
      "owner": "HealthNoteLabs",
      "name": "Npub.Health",
      "url": "https://github.com/HealthNoteLabs/Npub.Health",
      "imageUrl": "https://github.com/HealthNoteLabs.png",
      "description": "Manage health records securely with a user-controlled platform, enabling selective sharing of health data while maintaining privacy through end-to-end encryption.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-30T16:46:32Z",
      "readme_content": "# Npub.Health\n\nA decentralized health records platform built on Nostr protocol.\n\n## Project Overview\n\nNpub.Health provides a secure, user-controlled platform for managing health data using the Nostr protocol. This application gives individuals ownership over their health information while enabling selective sharing with healthcare providers.\n\n## Features\n\n- Secure health record storage using Nostr protocol\n- User-controlled data sharing and permissions\n- Provider verification system\n- Cross-platform support (web, mobile)\n- End-to-end encryption for sensitive health data\n\n## Tech Stack\n\n- **Frontend**: React, TypeScript, TailwindCSS\n- **Backend**: Node.js\n- **Database**: SQLite with Drizzle ORM\n- **Protocol**: Nostr\n\n## Getting Started\n\n### Prerequisites\n\n- Node.js v18 or higher\n- npm or yarn\n\n### Installation\n\n1. Clone the repository\n   ```bash\n   git clone https://github.com/HealthNoteLabs/Npub.Health.git\n   cd Npub.Health\n   ```\n\n2. Install dependencies\n   ```bash\n   npm install\n   ```\n\n3. Set up environment variables\n   ```bash\n   cp .env.example .env\n   # Edit .env with your configuration\n   ```\n\n4. Start the development server\n   ```bash\n   npm run dev\n   ```\n\n### Project Structure\n\n```\nNpub.Health/\n‚îú‚îÄ‚îÄ client/           # Frontend React application\n‚îú‚îÄ‚îÄ server/           # Backend Node.js server\n‚îú‚îÄ‚îÄ shared/           # Shared utilities and types\n‚îú‚îÄ‚îÄ scripts/          # Utility scripts\n‚îî‚îÄ‚îÄ ...\n```\n\n## Development\n\n### Running the app in development mode\n\n```bash\nnpm run dev\n```\n\n### Building for production\n\n```bash\nnpm run build\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Contact\n\nFor any questions or feedback, please open an issue on this repository.\n\n## AWS Integration for Blossom Server Deployment\n\nThis project includes AWS integration to automatically deploy Blossom servers for users. The implementation allows users to create their own private Blossom servers on AWS EC2 instances.\n\n### Setup Steps\n\n1. **Install AWS SDK**:\n   ```\n   npm install @aws-sdk/client-ec2\n   ```\n\n2. **Create Security Groups**:\n   The `scripts/create_aws_security_groups.js` script will create the necessary security groups in each AWS region:\n   ```\n   node scripts/create_aws_security_groups.js\n   ```\n   Make sure to update the VPC ID in the script for each region.\n\n3. **Configure AWS Credentials**:\n   Create a `.env` file with the following variables:\n   ```\n   AWS_ACCESS_KEY_ID=your_access_key_id\n   AWS_SECRET_ACCESS_KEY=your_secret_access_key\n   \n   # Security Groups for each region (output from the script)\n   SECURITY_GROUP_US_EAST=sg-xxxxxxxxxxxxxxxx\n   SECURITY_GROUP_US_WEST=sg-xxxxxxxxxxxxxxxx\n   SECURITY_GROUP_EU_CENTRAL=sg-xxxxxxxxxxxxxxxx\n   SECURITY_GROUP_AP_SOUTHEAST=sg-xxxxxxxxxxxxxxxx\n   \n   # Database URL\n   DATABASE_URL=postgres://postgres:postgres@localhost:5432/npubhealth\n   ```\n\n4. **Set up PostgreSQL database**:\n   Follow these steps to set up the database:\n   - Install PostgreSQL\n   - Create a database named `npubhealth`\n   - Run migrations to create tables:\n     ```\n     npm run db:push\n     ```\n\n5. **Test the integration**:\n   ```\n   npm run dev\n   ```\n\n### Architecture\n\nThe AWS integration consists of:\n\n1. **EC2 Manager** (`server/aws/ec2Manager.ts`): Handles creating and checking EC2 instances.\n2. **Server Monitor** (`server/aws/serverMonitor.ts`): Periodically checks and updates server status.\n3. **API Endpoints** (`server/routes.ts`): Provides REST endpoints for server management.\n4. **Database Integration** (`server/db`): Persists server information and status.\n\n### User Flow\n\n1. User selects a server tier, region, and name\n2. System generates a payment address\n3. User sends payment (simulated in development)\n4. System deploys an EC2 instance with the Blossom server installation script\n5. UI polls for server status until it's running\n6. User connects to their personal Blossom server\n\n### Security Considerations\n\n- The security groups allow HTTP, HTTPS, and the Blossom server port (3000)\n- SSH access should be restricted to specific IPs in production\n- AWS credentials should have limited permissions following the principle of least privilege\n- For production, consider using IAM roles instead of credentials\n\n### Troubleshooting\n\nIf you encounter issues:\n\n1. Check AWS credentials and permissions\n2. Verify that security groups are properly configured\n3. Look at server logs for deployment errors\n4. Check the database for server status information\n\nFor more information about the AWS SDK, refer to the [official documentation](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/welcome.html). ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "JackKuo666--ChEMBL-MCP-Server": {
      "owner": "JackKuo666",
      "name": "ChEMBL-MCP-Server",
      "url": "https://github.com/JackKuo666/ChEMBL-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Access the ChEMBL database with a fast API for retrieving chemical data and performing computations asynchronously. The server supports both HTTP and stdio transport methods with built-in error handling.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-04T16:27:02Z",
      "readme_content": "# ChEMBL-MCP-Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/chembl-mcp-server)](https://smithery.ai/server/@JackKuo666/chembl-mcp-server)\n\nA FastMCP wrapper server based on the chembl_webresource_client package, providing API access to the ChEMBL database.\n\n## Features\n\n- Complete API access to the ChEMBL database\n- Asynchronous API calls implemented using FastMCP framework\n- Built-in error handling and timeout mechanisms\n- Support for both HTTP and stdio transport methods\n- Complete type annotations and docstrings\n\n## Installation\n\n```bash\n# Clone repository\ngit clone https://github.com/yourusername/ChEMBL-MCP-Server.git\ncd ChEMBL-MCP-Server\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\n# Start HTTP server with default configuration\npython chembl_searver.py\n\n# Specify host and port\npython chembl_searver.py --host 0.0.0.0 --port 8080\n\n# Use stdio transport\npython chembl_searver.py --transport stdio\n\n# Set log level\npython chembl_searver.py --log-level DEBUG\n```\n\n### Available Parameters\n\n- `--host`: Server host address, defaults to 127.0.0.1\n- `--port`: Server port, defaults to 8000\n- `--transport`: Transport method, choose between http or stdio, defaults to http\n- `--log-level`: Log level, choose from DEBUG, INFO, WARNING, ERROR, CRITICAL, defaults to INFO\n\n## API Functions\n\nThe server provides the following API functions:\n\n### Data Entity APIs\n\n- `example_activity`: Get activity data\n- `example_assay`: Get assay data\n- `example_target`: Get target data\n- `example_molecule`: Get molecule data\n- `example_drug`: Get drug data\n- More data entity APIs...\n\n### Chemical Tool APIs\n\n- `example_canonicalizeSmiles`: Canonicalize SMILES strings\n- `example_smiles2inchi`: Convert SMILES to InChI\n- `example_smiles2svg`: Convert SMILES to SVG image\n- `example_structuralAlerts`: Get structural alerts\n- More chemical tool APIs...\n\n## Examples\n\nCheck the `chembl_search.py` file for examples of using various APIs.\n\n## Dependencies\n\n- chembl_webresource_client: ChEMBL Web Service Client\n- mcp: MCP Framework\n- fastapi: FastAPI Framework\n- uvicorn: ASGI Server\n- asyncio: Asynchronous I/O Library\n\n## License\n\n[MIT](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0
    },
    "JackKuo666--medRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "medRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/medRxiv-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Integrate with medRxiv's preprint repository to search for and access health sciences research papers and their metadata via a standardized interface. Enables efficient querying and retrieval of paper details based on custom search parameters and DOI.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-29T12:23:22Z",
      "readme_content": "# medRxiv MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/medrxiv-mcp-server)](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server)\n\nüîç Enable AI assistants to search and access medRxiv papers through a simple MCP interface.\n\nThe medRxiv MCP Server provides a bridge between AI assistants and medRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for health sciences preprints and access their content in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Paper Search: Query medRxiv papers with custom search strings or advanced search parameters ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers using DOI ‚úÖ\n- üìä Research Support: Facilitate health sciences research and analysis ‚úÖ\n- üìÑ Paper Access: Download and read paper content üìù\n- üìã Paper Listing: View all downloaded papers üìù\n- üóÉÔ∏è Local Storage: Papers are saved locally for faster access üìù\n- üìù Research Prompts: A set of specialized prompts for paper analysis üìù\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install medRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/medrxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client cline --config \"{}\"\n```\n\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install medRxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/medRxiv-MCP-Server.git\ncd medRxiv-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython medrxiv_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_key_words\", {\n    \"key_words\": \"COVID-19 vaccine efficacy\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_advanced\", {\n    \"term\": \"COVID-19\",\n    \"author1\": \"MacLachlan\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2023-12-31\",\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get metadata for a specific paper\n\n```python\nresult = await mcp.use_tool(\"get_medrxiv_metadata\", {\n    \"doi\": \"10.1101/2025.03.09.25323517\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the medRxiv MCP Server. Adjust the parameters as needed for your specific use case.\n\n## üõ† MCP Tools\n\nThe medRxiv MCP Server provides the following tools:\n\n### search_medrxiv_key_words\n\nSearch for articles on medRxiv using key words.\n\n**Parameters:**\n- `key_words` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### search_medrxiv_advanced\n\nPerform an advanced search for articles on medRxiv.\n\n**Parameters:**\n- `term` (str, optional): General search term\n- `title` (str, optional): Search in title\n- `author1` (str, optional): First author\n- `author2` (str, optional): Second author\n- `abstract_title` (str, optional): Search in abstract and title\n- `text_abstract_title` (str, optional): Search in full text, abstract, and title\n- `section` (str, optional): Section of medRxiv\n- `start_date` (str, optional): Start date for search range (format: YYYY-MM-DD)\n- `end_date` (str, optional): End date for search range (format: YYYY-MM-DD)\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### get_medrxiv_metadata\n\nFetch metadata for a medRxiv article using its DOI.\n\n**Parameters:**\n- `doi` (str): DOI of the article\n\n**Returns:** Dictionary containing article metadata\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"medrxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"medrxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-medRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-medRxiv/medrxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Papers\n\nYou can ask Claude to search for papers using queries like:\n```\nCan you search medRxiv for recent papers about genomics?\n```\n\nThe search will return basic information about matching papers including:\n\n‚Ä¢ Paper title\n\n‚Ä¢ Authors\n\n‚Ä¢ DOI\n\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the details for paper 10.1101/003541?\n```\n\nThis will return:\n\n‚Ä¢ Full paper title\n\n‚Ä¢ Authors\n\n‚Ä¢ Publication date\n\n‚Ä¢ Paper abstract\n\n‚Ä¢ Links to available formats (PDF/HTML)\n\n\n\n## üìù TODO\n\n### download_paper\n\nDownload a paper and save it locally.\n\n### read_paper\n\nRead the content of a downloaded paper.\n\n### list_papers\n\nList all downloaded papers.\n\n### üìù Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n#### Paper Analysis Prompt\n\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## üìÅ Project Structure\n\n- `medrxiv_server.py`: The main MCP server implementation using FastMCP\n- `medrxiv_web_search.py`: Contains the web scraping logic for searching medRxiv\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests (for web scraping, used in medrxiv_web_search.py)\n- beautifulsoup4 (for web scraping, used in medrxiv_web_search.py)\n\nYou can install the required dependencies using:\n\n```bash\npip install FastMCP requests beautifulsoup4\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## üôè Acknowledgements\n\nThis project is inspired by and built upon the work done in the [arxiv-mcp-server](https://github.com/blazickjp/arxiv-mcp-server) project.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect medRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "JackKuo666--ClinicalTrials-MCP-Server": {
      "owner": "JackKuo666",
      "name": "ClinicalTrials-MCP-Server",
      "url": "https://github.com/JackKuo666/ClinicalTrials-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Connects to ClinicalTrials.gov to search for and access detailed clinical trial information programmatically, facilitating efficient data retrieval for health sciences research and analysis.",
      "stars": 13,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-02T06:11:52Z",
      "readme_content": "# ClinicalTrials MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/clinicaltrials-mcp-server)](https://smithery.ai/server/@JackKuo666/clinicaltrials-mcp-server)\n\nüîç Enable AI assistants to search and access ClinicalTrials.gov data through a simple MCP interface.\n\nThe ClinicalTrials MCP Server provides a bridge between AI assistants and ClinicalTrials.gov's clinical trial repository through the Model Context Protocol (MCP). It allows AI models to search for clinical trials and access their content in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Trial Search: Query clinical trials with custom search strings or advanced search parameters ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to trial metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific trials using NCT ID ‚úÖ\n- üìä Research Support: Facilitate health sciences research and analysis ‚úÖ\n- üìã CSV Management: Save, load, and list CSV files with trial data ‚úÖ\n- üóÉÔ∏è Local Storage: Trials are saved locally for faster access ‚úÖ\n- üìä Statistics: Get statistics about clinical trials ‚úÖ\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ClinicalTrials Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/ClinicalTrials-mcp-server):\n\n#### Claude\n\n```bash\nnpx -y @smithery/cli@latest install ClinicalTrials-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run ClinicalTrials-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install ClinicalTrials-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install ClinicalTrials-mcp-server --client cline --config \"{}\"\n```\n\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install ClinicalTrials-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/ClinicalTrials-MCP-Server.git\ncd ClinicalTrials-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython clinical_trials_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for clinical trials using a search expression and save to CSV\n\n```python\nresult = await mcp.use_tool(\"search_clinical_trials_and_save_studies_to_csv\", {\n    \"search_expr\": \"COVID-19 vaccine efficacy\",\n    \"max_studies\": 5\n})\nprint(result)\n```\n\n### Example 2: Get studies by keyword\n\n```python\nresult = await mcp.use_tool(\"get_studies_by_keyword\", {\n    \"keyword\": \"diabetes\",\n    \"max_studies\": 10\n})\nprint(result)\n```\n\n### Example 3: Get full study details for a specific trial\n\n```python\nresult = await mcp.use_tool(\"get_full_study_details\", {\n    \"nct_id\": \"NCT04280705\"\n})\nprint(result)\n```\n\n### Example 4: Search and save studies with custom fields\n\n```python\nresult = await mcp.use_tool(\"search_clinical_trials_and_save_studies_to_csv\", {\n    \"search_expr\": \"alzheimer\",\n    \"max_studies\": 20,\n    \"filename\": \"alzheimer_studies.csv\",\n    \"fields\": [\"NCT Number\", \"Study Title\", \"Brief Summary\", \"Conditions\"]\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the main tools provided by the ClinicalTrials MCP Server. Adjust the parameters as needed for your specific use case.\n\n## üõ† MCP Tools\n\nThe ClinicalTrials MCP Server provides the following tools:\n\n### search_clinical_trials_and_save_studies_to_csv\n\nSearch for clinical trials using a search expression and save the results to a CSV file.\n\n**Parameters:**\n- `search_expr` (str): Search expression (e.g., \"Coronavirus+COVID\")\n- `max_studies` (int, optional): Maximum number of studies to return (default: 10)\n- `save_csv` (bool, optional): Whether to save the results as a CSV file (default: True)\n- `filename` (str, optional): Name of the CSV file to save (default: corona_fields.csv)\n- `fields` (list, optional): List of fields to include (default: NCT Number, Conditions, Study Title, Brief Summary)\n\n**Returns:** String representation of the search results\n\n### get_full_study_details\n\nGet detailed information about a specific clinical trial.\n\n**Parameters:**\n- `nct_id` (str): The NCT ID of the clinical trial\n\n**Returns:** String representation of the study details\n\n### get_studies_by_keyword\n\nGet studies related to a specific keyword.\n\n**Parameters:**\n- `keyword` (str): Keyword to search for\n- `max_studies` (int, optional): Maximum number of studies to return (default: 20)\n- `save_csv` (bool, optional): Whether to save the results as a CSV file (default: True)\n- `filename` (str, optional): Name of the CSV file to save (default: keyword_results_{keyword}.csv)\n\n**Returns:** String representation of the studies\n\n### get_study_statistics\n\nGet statistics about clinical trials.\n\n**Parameters:**\n- `condition` (str, optional): Optional condition to filter by\n\n**Returns:** String representation of the statistics\n\n### get_full_studies_and_save\n\nGet full studies data and save to CSV.\n\n**Parameters:**\n- `search_expr` (str): Search expression (e.g., \"Coronavirus+COVID\")\n- `max_studies` (int, optional): Maximum number of studies to return (default: 20)\n- `filename` (str, optional): Name of the CSV file to save (default: full_studies.csv)\n\n**Returns:** Message indicating the results were saved\n\n### load_csv_data\n\nLoad and display data from a CSV file.\n\n**Parameters:**\n- `filename` (str): Name of the CSV file to load\n\n**Returns:** String representation of the CSV data\n\n### list_saved_csv_files\n\nList all available CSV files in the current directory.\n\n**Returns:** String representation of the available CSV files\n\n## üîç MCP Resources\n\nThe ClinicalTrials MCP Server also provides the following resources:\n\n### clinicaltrials://corona_fields\n\nGet the corona fields data as a resource.\n\n### clinicaltrials://full_studies\n\nGet the full studies data as a resource.\n\n### clinicaltrials://csv/{filename}\n\nGet data from a specific CSV file.\n\n**Parameters:**\n- `filename` (str): Name of the CSV file\n\n### clinicaltrials://available_files\n\nGet a list of all available CSV files.\n\n### clinicaltrials://study/{nct_id}\n\nGet a specific study by NCT ID.\n\n**Parameters:**\n- `nct_id` (str): The NCT ID of the clinical trial\n\n### clinicaltrials://condition/{condition}\n\nGet studies related to a specific condition.\n\n**Parameters:**\n- `condition` (str): The condition to search for\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"ClinicalTrials\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"ClinicalTrials-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"ClinicalTrials\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"ClinicalTrials-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"ClinicalTrials\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/ClinicalTrials-MCP-Server/.venv/bin/activate && python /home/YOUR/PATH/ClinicalTrials-MCP-Server/clinical_trials_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Clinical Trials\n\nYou can ask Claude to search for clinical trials using queries like:\n```\nCan you search for recent clinical trials about diabetes?\n```\n\nThe search will return basic information about matching trials including:\n\n‚Ä¢ Trial title\n\n‚Ä¢ NCT Number\n\n‚Ä¢ Conditions\n\n‚Ä¢ Brief Summary\n\n\n### Getting Trial Details\n\nOnce you have an NCT ID, you can ask for more details:\n```\nCan you show me the details for trial NCT04280705?\n```\n\nThis will return:\n\n‚Ä¢ Full trial title\n\n‚Ä¢ Conditions\n\n‚Ä¢ Brief Summary\n\n‚Ä¢ Other available details\n\n\n## üìÅ Project Structure\n\n- `clinical_trials_server.py`: The main MCP server implementation using FastMCP\n- `clinical_trials.py`: Contains helper functions for interacting with the ClinicalTrials.gov API\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- pytrials\n- pandas\n\nYou can install the required dependencies using:\n\n```bash\npip install FastMCP pytrials pandas\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect ClinicalTrials.gov's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "jbdamask--mcp-nih-reporter": {
      "owner": "jbdamask",
      "name": "mcp-nih-reporter",
      "url": "https://github.com/jbdamask/mcp-nih-reporter",
      "imageUrl": "https://github.com/jbdamask.png",
      "description": "Search for NIH-funded research projects and publications using various criteria such as fiscal years, investigator names, organization details, and funding information. Retrieve detailed project information, including abstracts, in a conversational format.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-19T10:49:39Z",
      "readme_content": "# NIH RePORTER MCP\n\nA Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server for chatting with [NIH RePORTER](https://reporter.nih.gov/). Search for NIH-funded research projects and publications in a conversational manner.\nAccompanying blog post [here](https://open.substack.com/pub/johndamask/p/building-an-mcp-server-over-nihs?r=2ee1b&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true).\n\n![img](/img/mcp-nih-reporter-claude.png)\n\n\n## Features\n\n- Search NIH-funded research projects with various criteria:\n  - Fiscal years\n  - Principal Investigator names\n  - Organization details (name, state, city, type, department)\n  - Funding amounts\n  - COVID-19 response status\n  - Funding mechanism\n  - Institute/Center codes\n  - RCDC terms\n  - Date ranges\n- Search publications associated with NIH projects\n- Combined search functionality for both projects and publications\n- Detailed project and publication information including abstracts\n- Configurable result limits\n\n## Prerequisites\n\n- Python 3.12 or higher\n- UV package manager (recommended for faster dependency installation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd mcp-nih-reporter\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # On Windows, use `.venv\\Scripts\\activate`\n```\n\n3. Install dependencies using UV:\n```bash\nuv pip install -e .\n```\n## Usage\n\nThis MCP server provides access to the NIH RePORTER API through several tools:\n\n- `search_projects`: Search for NIH-funded research projects\n- `search_publications`: Search for publications associated with NIH projects\n- `search_combined`: Combined search for both projects and publications\n- `test_connection`: Test the API connection\n\nYou can use this MCP with any MCP-compatible client, such as:\n- Claude Desktop\n- Cursor\n- Other MCP-enabled tools\n\n### Example claude_desktop_config.json\n```\n{\n  \"mcpServers\": {\n\t \"nih-reporter\": {\n\t      \"command\": \"<fully qualified path to>/uv\",\n\t      \"args\": [\n\t        \"run\",\n\t        \"--with\",\n\t        \"mcp[cli]\",\n\t        \"mcp\",\n\t        \"run\",\n\t        \"<fully qualified path to>/mcp-nih-reporter/mcp-nih-reporter.py\"\n\t      ]\n\t    }\n  }\n}\n```\n\nThe search results will be returned in a structured format containing project details including:\n- Project title and abstract\n- Principal Investigator information\n- Organization details\n- Funding information\n- Project dates and status\n\n## Debugging\n\nA log file will be created in the root folder when the MCP attempts to run in a client (e.g. Claude Desktop). Check there if you're having trouble.\n\n## Development\n\nThe project uses:\n- `httpx` for async HTTP requests\n- `mcp` for the Mission Control Protocol implementation\n- `python-dotenv` for environment variable management\n- `uv` for dependency management\n\n## Logging\n\nLogs are written to `mcp-nih-reporter.log` in the project root directory. The logging level is set to INFO by default.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and follow the existing code style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "jmandel--health-record-mcp": {
      "owner": "jmandel",
      "name": "health-record-mcp",
      "url": "https://github.com/jmandel/health-record-mcp",
      "imageUrl": "https://github.com/jmandel.png",
      "description": "Connects AI models to Electronic Health Records (EHRs) using the SMART on FHIR standard to securely extract and analyze patient data, leveraging the Model Context Protocol for seamless access to structured health data and clinical notes.",
      "stars": 63,
      "forks": 21,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-13T17:30:01Z",
      "readme_content": "# EHR Tools with MCP and FHIR\n![EHR Tools Overview](static/overview.png)\n\nhttps://youtu.be/K0t6MRyIqZU?si=Mz4d65DcAD3i2YbO\n\nThis project acts as a specialized server providing tools for Large Language Models (LLMs) and other AI agents to interact with Electronic Health Records (EHRs). It leverages the **SMART on FHIR** standard for secure data access and the **Model Context Protocol (MCP)** to expose the tools.\n\nThink of it as a secure gateway and toolkit enabling AI to safely access and analyze patient data from diverse EHR systems.\n\n## The Core Idea\n\nThe system works in three main stages:\n\n1.  **SMART on FHIR Client (Implemented within this project):** Connects securely to an EHR using the standard SMART App Launch framework. It extracts a wide range of patient information, including both structured data (like conditions, medications, labs) and unstructured clinical notes or attachments.\n2.  **MCP Server (This Project):** Takes the extracted EHR data and makes it available through a set of powerful tools accessible via the Model Context Protocol. These tools allow external systems (like AI models) to query and analyze the data without needing direct access to the EHR itself.\n3.  **AI / LLM Interface (External Consumer):** An AI agent or Large Language Model connects to the MCP Server and uses the provided tools to \"ask questions\" about the patient's record, perform searches, or run custom analyses.\n\n## Available Tools\n\nThe MCP Server offers several tools for interacting with the loaded EHR data:\n\n*   `grep_record`: Performs text or regular expression searches across *all* parts of the fetched record (structured FHIR data + text from notes/attachments). Ideal for finding keywords or specific mentions (e.g., \"diabetes\", \"aspirin\").\n*   `query_record`: Executes read-only SQL `SELECT` queries directly against the structured FHIR data. Useful for precise lookups based on known FHIR resource structures (e.g., finding specific lab results by LOINC code).\n*   `eval_record`: Executes custom JavaScript code directly on the fetched data (FHIR resources + attachments). Offers maximum flexibility for complex calculations, combining data from multiple sources, or custom formatting.\n\nThis setup allows AI tools to leverage comprehensive EHR data through a standardized and secure interface.\n\n*(Developer setup and usage details can be found within the codebase and specific module documentation.)*\n\n---\n\n## Components & Usage\n\nThis project offers different ways to fetch EHR data and expose it via MCP tools:\n\n### 1. Standalone SMART on FHIR Web Client\n\nThis project includes a self-contained web application that allows users to connect to their EHR via SMART on FHIR and fetch their data.\n\n*   **Hosted Version:** You can use a publicly hosted version at: \\\n    [`https://mcp.fhir.me/ehr-connect#deliver-to-opener:$origin`](https://mcp.fhir.me/ehr-connect#deliver-to-opener:$origin) \\\n    (Replace `$origin` with the actual origin of the window that opens this link).\n*   **Filtering Brands (`?brandTags`):** You can filter the list of EHR providers shown on the connection page by adding the `brandTags` query parameter to the URL. Provide a comma-separated list of tags. Only brands matching *all* provided tags (from their configuration in `brandFiles`) will be displayed.\n    It supports both OR (comma-separated) and AND (caret `^` separated) logic, with AND taking precedence.\n    *   `?brandTags=epic,sandbox`: Shows brands tagged with `epic` OR `sandbox`.\n    *   `?brandTags=epic^dev`: Shows brands tagged with both `epic` AND `dev`.\n    *   `?brandTags=epic^dev,sandbox^prod`: Shows brands tagged with (`epic` AND `dev`) OR (`sandbox` AND `prod`).\n    *   If the parameter is omitted, it defaults to showing brands tagged with `prod`.\n    *   Example: `.../ehr-connect?brandTags=hospital^us`: Shows brands tagged with `hospital` AND `us`.\n*   **How it Works:** When opened, this page prompts the user to select their EHR provider. It then initiates the standard SMART App Launch flow, redirecting the user to their EHR's login page. After successful authentication and authorization, the client fetches a comprehensive set of FHIR resources (Patient, Conditions, Observations, Medications, Documents, etc.) and attempts to extract plaintext from any associated attachments (like PDFs, RTF, HTML found in `DocumentReference`).\n*   **Data Output (`ClientFullEHR`):** Once fetching is complete, the client gathers all the data into a `ClientFullEHR` JSON object. This object contains:\n    *   `fhir`: A dictionary where keys are FHIR resource types (e.g., \"Patient\") and values are arrays of the corresponding FHIR resources.\n    *   `attachments`: An array of processed attachment objects, each including metadata (source resource, path, content type) and the content itself (`contentBase64` for raw data, `contentPlaintext` for extracted text).\n*   **Data Delivery:** If opened with the `#deliver-to-opener:$origin` hash, the client will prompt the user for confirmation and then send the `ClientFullEHR` object back to the window that opened it using `window.opener.postMessage(data, targetOrigin)`.\n\n### 2. Local MCP Server via Stdio (`src/cli.ts`)\n\nThis mode is ideal for running the MCP server locally, often used with tools like Cursor or other command-line AI clients.\n\n*   **Two-Step Process:**\n    1.  **Fetch Data to Database:** First, run the command-line interface with the `--create-db` and `--db` flags. This starts a temporary web server and uses the same SMART on FHIR web client logic described above to fetch data. Instead of sending the data via `postMessage`, it saves the `ClientFullEHR` data into a local SQLite database file.\n        ```bash\n        # Example: Fetch data and save to data/my_record.sqlite\n        bun run src/cli.ts --create-db --db ./data/my_record.sqlite\n        ```\n        Follow the prompts (opening a link in your browser) to connect to your EHR.\n    2.  **Run MCP Server:** Once the database file is created, run the CLI again, pointing only to the database file. This loads the data into memory and starts the MCP server, listening for commands on standard input/output.\n        ```bash\n        # Example: Start the MCP server using the saved data\n        bun run src/cli.ts --db ./data/my_record.sqlite\n        ```\n    *   **Configuration (`config.*.json`):** This process relies on a configuration file (e.g., `config.epicsandbox.json`) which defines available EHR brands/endpoints in a `brandFiles` array. Each entry in this array specifies the brand's details, including:\n        *   `url`: Path/URL to the brand definition file (like `static/brands/epic-sandbox.json`).\n        *   `tags`: An array of strings (e.g., `[\"epic\", \"sandbox\"]`) used for categorization or filtering.\n        *   `vendorConfig`: Contains SMART on FHIR client details (`clientId`, `scopes`).\n*   **Client Configuration (e.g., Cursor):** Configure your MCP client to execute this command. **Crucially, use absolute paths** for both `src/cli.ts` and the database file.\n    ```json\n    {\n      \"mcpServers\": {\n        \"local-ehr\": {\n          \"name\": \"Local EHR Search\",\n          \"command\": \"bun\", // Or the absolute path to bun\n          \"args\": [\n              \"/home/user/projects/smart-mcp/src/cli.ts\", // Absolute path to cli.ts\n              \"--db\",\n              \"/home/user/projects/smart-mcp/data/my_record.sqlite\" // Absolute path to DB file\n            ]\n        }\n      }\n    }\n    ```\n\n### 3. Full MCP Server via SSE (`src/sse.ts` / `index.ts`)\n\nThis mode runs a persistent server suitable for scenarios where multiple clients might connect over the network. It uses Server-Sent Events (SSE) for the MCP communication channel.\n\n*   **Authentication:** Client authentication relies on OAuth 2.1, as specified by the Model Context Protocol. The server provides standard endpoints (`/authorize`, `/token`, `/register`, etc.).\n*   **Data Fetch:** When a client initiates an OAuth connection, the server handles the SMART on FHIR flow *itself*, fetches the `ClientFullEHR` data *during* the authorization process, and keeps it in memory (or a persisted session) for the duration of the client's connection.\n*   **Status:** While functional, the MCP specification for OAuth 2.1 client interaction is still evolving. Client support for this authentication method is **extremely limited** at present, making it difficult to test this mode with standard clients outside of specialized developer or debugging tools. This SSE mode should be considered **experimental**.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "jpoles1--statpearls-mcp": {
      "owner": "jpoles1",
      "name": "statpearls-mcp",
      "url": "https://github.com/jpoles1/statpearls-mcp",
      "imageUrl": "https://github.com/jpoles1.png",
      "description": "Fetches and retrieves reliable, peer-reviewed medical information about diseases and conditions from StatPearls, formatted in AI-friendly Markdown for enhanced conversation integration.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-09T10:33:41Z",
      "readme_content": "# StatPearls MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jpoles1/statpearls-mcp)](https://smithery.ai/server/@jpoles1/statpearls-mcp)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that fetches disease information from [StatPearls](https://www.ncbi.nlm.nih.gov/books/NBK430685/), a trusted source of peer-reviewed medical content.\n\nGive your AI system a relaible source of medical knowledge for its next conversation.\n\n## Features\n\n- Searches for diseases and medical conditions on StatPearls\n- Retrieve comprehensive, reliable medical information from StatPearls\n- Convert HTML content to well-formatted Markdown to make it AI-friendly\n- Integrates with AI models via the Model Context Protocol\n\n![Image](StatPearlsMCPDemo.gif)\n\n### If you don't already have a Model Context Protocol (MCP) client:\n\nIf you are a casual user, you can use [Claude Desktop](https://modelcontextprotocol.io/quickstart/user) to get started using MCP servers. It is a free and open-source desktop application that allows you to run MCP servers locally and connect to them.\n\nIf you are a power user/developer, I recommend using VSCode with the [RooCode](https://docs.roocode.com/) extension which enables you to connect in [MCP servers](https://docs.roocode.com/features/mcp/what-is-mcp) to your development environment for infinite possibilities!\n\n## Installation\n\nOnce you have an MCP-capable AI client, you can run this server locally.\n\nThe easiest way to get up and running is to download the appropriate executable/binary for your OS from the [releases page](https://github.com/jpoles1/statpearls-mcp/releases). This will give you a self-contained executable that you can run without any additional setup.\n\nPlace this executable in a directory of your choice. Then simply add the following to your `mcp_settings.json` file:\n\n#### For Windows:\n\n```json\n{\n  \"mcpServers\": {\n    ...\n    \"statpearls\": {\n      \"command\": \"{path_to_executable_here}\\\\statpearls-mcp.exe\"\n    },\n    ...\n  }\n}\n\n#### For Mac/Linux:\n\n```json\n{\n  \"mcpServers\": {\n    ...\n    \"statpearls\": {\n      \"command\": \"{path_to_executable_here}/statpearls-mcp\"\n    },\n    ...\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install statpearls-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jpoles1/statpearls-mcp):\n\n```bash\nnpx -y @smithery/cli install @jpoles1/statpearls-mcp --client claude\n```\n\n### For Developers:\n\nYou can also run the server from source. This requires [Bun](https://bun.sh/) to be installed on your system.\n  1. Clone the repository\n  2. Install dependencies (`bun install`)\n  3. Compile the server (`bun run build`)\n  4. Now you can add the server to your `mcp_settings.json` file:\n  ```json\n  {\n    \"mcpServers\": {\n      ...\n      \"statpearls\": {\n        \"command\": \"node\",\n        \"args\": [\n          \"{path_to_proj_here}/dist/index.js\"\n        ]\n      },\n      ...\n    }\n  }\n  ```\n\n## Tool Definition\n\nThe server provides a single tool:\n\n- **statpearls_disease_info**: Fetches comprehensive, reliable medical information about diseases from StatPearls.\n\n### Input Schema\n\n```json\n{\n  \"query\": \"diabetes\",\n  \"format_options\": {\n    \"includeToc\": true,\n    \"maxLength\": 50000\n  }\n}\n```\n\n- `query`: Disease or medical condition to search for (required)\n- `format_options`: Optional formatting preferences\n  - `includeToc`: Whether to include a table of contents (default: true)\n  - `maxLength`: Maximum length of the returned content in characters (default: 50000)\n\n### Example Output\n\nThe tool returns formatted Markdown content with:\n\n- Title and source information\n- Table of contents (optional)\n- Structured sections including etiology, epidemiology, pathophysiology, clinical features, diagnosis, treatment, and prognosis (when available)\n\n## Development\n\n### Project Structure\n\n```\nstatpearls-mcp/\n‚îú‚îÄ‚îÄ src/                         # Source code\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts                 # Main entry point and server setup\n‚îÇ   ‚îú‚îÄ‚îÄ test-html-parser.ts      # Test utility for HTML parser\n‚îÇ   ‚îú‚îÄ‚îÄ test-statpearls-parser.ts # Test utility for StatPearls parser\n‚îÇ   ‚îú‚îÄ‚îÄ testrun.ts               # Test runner utility\n‚îÇ   ‚îú‚îÄ‚îÄ tools/                   # Tool definitions and handlers\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statpearls.ts        # StatPearls tool definition and handler\n‚îÇ   ‚îú‚îÄ‚îÄ services/                # Core functionality services\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.ts            # Search functionality\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content.ts           # Content retrieval and processing\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ markdown.ts          # HTML to Markdown conversion\n‚îÇ   ‚îú‚îÄ‚îÄ types/                   # Type definitions\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts             # Common type definitions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statpearls.ts        # StatPearls-specific type definitions\n‚îÇ   ‚îî‚îÄ‚îÄ utils/                   # Utility functions\n‚îÇ       ‚îú‚îÄ‚îÄ html.ts              # HTML parsing utilities\n‚îÇ       ‚îú‚îÄ‚îÄ error.ts             # Error handling utilities\n‚îÇ       ‚îî‚îÄ‚îÄ statpearls-parser.ts # StatPearls content parsing utilities\n‚îú‚îÄ‚îÄ scripts/                     # Build and utility scripts\n‚îÇ   ‚îú‚îÄ‚îÄ build.ts                 # Build script for creating Node.js compatible bundle\n‚îÇ   ‚îú‚îÄ‚îÄ compile.ts               # Script for compiling executables\n‚îÇ   ‚îú‚îÄ‚îÄ release.ts               # Script for handling releases\n‚îÇ   ‚îî‚îÄ‚îÄ version.ts               # Script for managing versioning\n‚îú‚îÄ‚îÄ dist/                        # Build output directory (not in repository)\n‚îú‚îÄ‚îÄ package.json                 # Project configuration and dependencies\n‚îú‚îÄ‚îÄ tsconfig.json                # TypeScript configuration\n‚îú‚îÄ‚îÄ bun.lock                     # Bun dependency lock file\n‚îú‚îÄ‚îÄ README.md                    # Main project documentation\n‚îî‚îÄ‚îÄ RELEASE-PROCESS.md           # Documentation for release process\n```\n\n### Building and Releasing\n\n#### Building\n\nThe build process creates a single JavaScript file that can run with vanilla Node.js:\n\n```bash\n# Production build\nbun run build\n# or\nbun run build:prod\n\n# Development build\nbun run build:dev\n```\n\nThis creates a bundled file at `dist/index.js` that includes all dependencies.\n\n#### Compiling Executables\n\nYou can compile platform-specific executables using Bun's compilation feature:\n\n```bash\n# Compile for all platforms\nbun run compile:all\n\n# Compile for specific platforms\nbun run compile:linux\nbun run compile:windows\nbun run compile:mac\n```\n\nThis creates executable files in the `dist` directory:\n- `statpearls-mcp` (default executable)\n- `statpearls-mcp-linux-x64` (Linux)\n- `statpearls-mcp-windows-x64.exe` (Windows)\n- `statpearls-mcp-darwin-x64` (macOS)\n\n#### Releasing\n\nThe release process handles versioning, building, compiling, and Git operations:\n\n```bash\n# Release a patch version (bug fixes)\nbun run release:patch\n\n# Release a minor version (new features, backward compatible)\nbun run release:minor\n\n# Release a major version (breaking changes)\nbun run release:major\n```\n\nThis process:\n1. Updates the version in package.json\n2. Builds the distribution file\n3. Compiles executables for all platforms\n4. Creates a Git commit with the version number\n5. Creates a Git tag for the version\n6. Pushes the commit and tag to GitHub\n\n#### Versioning\n\nThe project follows semantic versioning. You can check the current version with:\n\n```bash\nbun run version\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Kartha-AI--agentcare-mcp": {
      "owner": "Kartha-AI",
      "name": "agentcare-mcp",
      "url": "https://github.com/Kartha-AI/agentcare-mcp",
      "imageUrl": "https://github.com/Kartha-AI.png",
      "description": "Integrates with EMRs like Cerner and Epic to access and manage healthcare data using SMART on FHIR APIs, providing tools for medical research and clinical analysis.",
      "stars": 82,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-16T13:52:18Z",
      "readme_content": "# Agent Care: An MCP Server for EMRs like Cerner and Epic\n\nA Model Context Protocol (MCP) server that provides healthcare tools and prompts for interacting with FHIR data and medical resources on EMRs like Cerner and Epic using Claude Desktop and Goose Desktop.\n\n[![smithery badge](https://smithery.ai/badge/@Kartha-AI/agentcare-mcp)](https://smithery.ai/server/@Kartha-AI/agentcare-mcp)\n\n## Demo\n[![Demo](screenshots/demo.png)](https://www.agentcare.ai/demo.mp4)\n\n## Features\n- EMR integrartion using SMART on FHIR APIs\n- Uses OAuth2 to authenticate with EMRs \n- Anthropic Claude Desktop integration\n- Medical research integration (PubMed, Clinical Trials, FDA)\n- Response caching\n- Error handling\n- Null-safe data formatting\n- Comprehensive clinical analysis\n\n## Screenshots\n\n<img src=\"screenshots/cerner.png\" alt=\"Cerner\" width=\"700\">\n<img src=\"screenshots/epic.png\" alt=\"Epic\" width=\"700\">\n<img src=\"screenshots/converse.png\" alt=\"Converse\" width=\"700\">\n<img src=\"screenshots/soap.png\" alt=\"Soap Notes\" width=\"700\">\n<img src=\"screenshots/timeline.png\" alt=\"Timeline\" width=\"700\">\n\n## Tools\n\n### FHIR Tools\n- `find_patient` - Search for a patient by name, DOB, or other identifiers\n- `get_patient_observations` - Retrieve patient observations/vital signs\n- `get_patient_conditions` - Get patient's active conditions\n- `get_patient_medications` - Get patient's current medications\n- `get_patient_encounters` - Get patient's clinical encounters\n- `get_patient_allergies` - Get patient's allergies and intolerances\n- `get_patient_procedures` - Get patient's procedures\n- `get_patient_careteam` - Get patient's care team members\n- `get_patient_careplans` - Get patient's active care plans\n- `get_vital_signs` - Get patient's vital signs\n- `get_lab_results` - Get patient's laboratory results\n- `get_medications_history` - Get patient's medication history\n- `clinical_query` - Execute custom FHIR queries\n\n### Medical Research Tools\n- `search-pubmed` - Search PubMed articles related to medical conditions\n- `search-trials` - Find relevant clinical trials\n- `drug-interactions` - Check drug-drug interactions\n\n## Usage\n\nEach tool  requires specific parameters:\n\n### Required Parameters\n- Most tools require `patientId`\n- Some tools have additional parameters:\n  - `lab_trend_analysis`: requires `labType`\n  - `search-pubmed`: requires `query` and optional `maxResults`\n  - `search-trials`: requires `condition` and optional `location`\n  - `drug-interactions`: requires `drugs` array\n\n## Development Configuration \n- To use with Cerener: Go to https://code-console.cerner.com and create a sandbox account, create a new provider app and get the clientId/secret.\n(note: ec2458f2-1e24-41c8-b71b-0e701af7583d below is the tenant id for cerner developer sandbox)\n\n- To use with Epic: Go to https://fhir.epic.com/Developer/Apps , sign up as developer and create a new app and get the clientId/secret.\n\n- For PubMed, Clinical Trials and FDA, you need to get the API keys from the respective websites.\n  - https://clinicaltrials.gov/api/v2/studies\n  - https://eutils.ncbi.nlm.nih.gov/entrez/eutils\n  - https://api.fda.gov/drug/ndc.json\n\nFor local testing Create a `.env` file in the root directory or use these environment variables in claude desktop launch configuration.\n#### Cerner\n````\nOAUTH_CLIENT_ID=\"XXXXX\",\nOAUTH_CLIENT_SECRET=\"XXXXXXX\",\nOAUTH_TOKEN_HOST=\"https://authorization.cerner.com\", \nOAUTH_AUTHORIZE_PATH=\"/tenants/ec2458f2-1e24-41c8-b71b-0e701af7583d/protocols/oauth2/profiles/smart-v1/personas/provider/authorize\",\nOAUTH_AUTHORIZATION_METHOD='header',\nOAUTH_TOKEN_PATH=\"/tenants/ec2458f2-1e24-41c8-b71b-0e701af7583d/hosts/api.cernermillennium.com/protocols/oauth2/profiles/smart-v1/token\",\nOAUTH_AUDIENCE=\"https://fhir-ehr.cerner.com/r4/ec2458f2-1e24-41c8-b71b-0e701af7583d\",\nOAUTH_CALLBACK_URL=\"http://localhost:3456/oauth/callback\",\nOAUTH_SCOPES=\"user/Patient.read user/Condition.read user/Observation.read user/MedicationRequest.read user/AllergyIntolerance.read user/Procedure.read user/CarePlan.read user/CareTeam.read user/Encounter.read user/Immunization.read\",\nOAUTH_CALLBACK_PORT=\"3456\"\nFHIR_BASE_URL:any = \"https://fhir-ehr.cerner.com/r4/ec2458f2-1e24-41c8-b71b-0e701af7583d\" \nPUBMED_API_KEY=your_pubmed_api_key\nCLINICAL_TRIALS_API_KEY=your_trials_api_key\nFDA_API_KEY=your_fda_api_key\n````\n#### Epic\n````\nOAUTH_CLIENT_ID=\"XXXXXXX\",\nOAUTH_CLIENT_SECRET=\"\",\nOAUTH_TOKEN_HOST=\"https://fhir.epic.com\",\nOAUTH_AUTHORIZE_PATH=\"/interconnect-fhir-oauth/oauth2/authorize\",\nOAUTH_AUTHORIZATION_METHOD='body',\nOAUTH_TOKEN_PATH=\"/interconnect-fhir-oauth/oauth2/token\",\nOAUTH_AUDIENCE=\"https://fhir.epic.com/interconnect-fhir-oauth/api/FHIR/R4\",\nOAUTH_CALLBACK_URL=\"http://localhost:3456/oauth/callback\",\nOAUTH_SCOPES=\"user/Patient.read user/Observation.read user/MedicationRequest.read user/Condition.read user/AllergyIntolerance.read user/Procedure.read user/CarePlan.read user/CareTeam.read user/Encounter.read user/Immunization.read\",\nOAUTH_CALLBACK_PORT=3456\nFHIR_BASE_URL:any = \"https://fhir.epic.com/interconnect-fhir-oauth/api/FHIR/R4\" //EPIC  \nPUBMED_API_KEY=your_pubmed_api_key\nCLINICAL_TRIALS_API_KEY=your_trials_api_key\nFDA_API_KEY=your_fda_api_key\n````\n\n## Start MCP Server Locally \n````\ngit clone {agentcare-mcp-github path}\ncd agentcare-mcp\nnpm install\nnpm run build\n````\n\n## Use claude desktop\n````\nfor claude desktop: \nmacOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n(use the env variables as shown above)\n\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-filesystem\",\n        \"/Users/your-username/Desktop\"\n      ]\n    },\n    \"agent-care\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/Users/your-username/{agentcare-download-path}/agent-care-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OAUTH_CLIENT_ID\": XXXXXX,\n        \"OAUTH_CLIENT_SECRET\":XXXXXXX,\n        \"OAUTH_TOKEN_HOST\":,\n        \"OAUTH_TOKEN_PATH\":,\n        \"OAUTH_AUTHORIZE_PATH\",\n        \"OAUTH_AUTHORIZATION_METHOD\": ,\n        \"OAUTH_AUDIENCE\":,\n        \"OAUTH_CALLBACK_URL\":,\n        \"OAUTH_SCOPES\":,\n        \"OAUTH_CALLBACK_PORT\":,\n        \"FHIR_BASE_URL\":,\n        \"PUBMED_API_KEY\":,\n        \"CLINICAL_TRIALS_API_KEY\":,\n        \"FDA_API_KEY\":\n      }\n    }\n  }\n}\n````\n## Use MCP Inspectopr\n(MCP Server using inspector. Make sure to update the .env file with the correct values.)\n````\nnpm install -g @modelcontextprotocol/inspector\nmcp-inspector  build/index.js\nhttp://localhost:5173\n\n````\n\n## Test User Logins\n(commonly used for sandbox/dev)\n- Cerner: portal | portal \n- Epic: FHIRTWO | EpicFhir11! \n\n## Troubleshooting:\nIf Claude desktop is running it uses port 3456 for Auth. You need to terminate that process using the following command:\n````\nkill -9 $(lsof -t -i:3456)\n````\n\n## Use Goose\nGoose is an open Source AI Agent frameowrk from Block(Stripe) that works with MCP servers. Goose Desktop is like Claude Desktop that can work with MCP servers. But Goose can be configured to use models other than Anthropic as well. More info: https://block.xyz/inside/block-open-source-introduces-codename-goose\n\nSee below how Goose Desktop works with Agent Care:\n(goose extension will be configured with command: \n/Users/your-username/{agentcare-download-path}/agent-care-mcp/build/index.js)\n\n<img src=\"screenshots/goose-extension.png\" alt=\"Cerner\">\n<img src=\"screenshots/goose-env.png\" alt=\"Epic\">\n<img src=\"screenshots/goose-model.png\" alt=\"Converse\" >\n<img src=\"screenshots/goose-in-action.png\" alt=\"Soap Notes\">\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Kartha-AI--google-cloud-healthcare-api-mcp": {
      "owner": "Kartha-AI",
      "name": "google-cloud-healthcare-api-mcp",
      "url": "https://github.com/Kartha-AI/google-cloud-healthcare-api-mcp",
      "imageUrl": "https://github.com/Kartha-AI.png",
      "description": "Interact with FHIR resources and manage patient data using the Google Cloud Healthcare API. Retrieve observations and conduct medical research through public medical research APIs with secure access via a SmartonFHIR gateway.",
      "stars": 5,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-01T07:29:20Z",
      "readme_content": "# An MCP Server for Google Cloud Healthcare API (FHIR)\n\n[![smithery badge](https://smithery.ai/badge/@Kartha-AI/google-cloud-healthcare-api-mcp)](https://smithery.ai/server/@Kartha-AI/google-cloud-healthcare-api-mcp)\n\nA Model Context Protocol (MCP) server that provides healthcare tools for interacting with FHIR resources on [Google Cloud Healthcare API](https://cloud.google.com/healthcare-api) and public medical research APIs like pubmed, using MCP clients like Claude and Goose.\n\nThis is a slightly modified version for AgentCare MCP Server for EHRs. https://github.com/Kartha-AI/agentcare-mcp\n\nThe maion differnce is that this repo talks to Google Cloud Healthcare FHIR APIs thru a SmartonFHIR gateway that's secured by Firebase Auth. \n\n## Architetcure\n<img src=\"screenshots/architecture.png\" alt=\"Architecture\" width=\"700\">\n\n## Demo\n- Claude: demo/claude-demo.mp4\n- Goose: demo/goose-demo.mp4\n\n## Screenshots\n<img src=\"screenshots/goose/goose-auth.png\" alt=\"Auth\" width=\"700\">\n<img src=\"screenshots/goose/goose-patient.png\" alt=\"Patient\" width=\"700\">\n<img src=\"screenshots/goose/goose-conditions.png\" alt=\"Conditions\" width=\"700\">\n<img src=\"screenshots/goose/goose-vitals.png\" alt=\"Vitals\" width=\"700\">\n\n<img src=\"screenshots/claude/gchapi.png\" alt=\"GCHAPI\" width=\"700\">\n<img src=\"screenshots/claude/convo.png\" alt=\"Converse\" width=\"700\">\n<img src=\"screenshots/claude/chart.png\" alt=\"Timeline\" width=\"700\">\n<img src=\"screenshots/claude/soap.png\" alt=\"Soap Notes\" width=\"700\">\n\n## Tools\n\n### FHIR Tools\n- `find_patient` - Search for a patient by name, DOB, or other identifiers\n- `get_patient_observations` - Retrieve patient observations/vital signs\n- `get_patient_conditions` - Get patient's active conditions\n- `get_patient_medications` - Get patient's current medications\n- `get_patient_encounters` - Get patient's clinical encounters\n- `get_patient_allergies` - Get patient's allergies and intolerances\n- `get_patient_procedures` - Get patient's procedures\n- `get_patient_careteam` - Get patient's care team members\n- `get_patient_careplans` - Get patient's active care plans\n- `get_vital_signs` - Get patient's vital signs\n- `get_lab_results` - Get patient's laboratory results\n- `get_medications_history` - Get patient's medication history\n- `clinical_query` - Execute custom FHIR queries\n\n### Medical Research Tools\n- `search-pubmed` - Search PubMed articles related to medical conditions\n- `search-trials` - Find relevant clinical trials\n- `drug-interactions` - Check drug-drug interactions\n\n## Usage\n\nEach tool  requires specific parameters:\n\n### Required Parameters\n- Most tools require `patientId`\n- Some tools have additional parameters:\n  - `lab_trend_analysis`: requires `labType`\n  - `search-pubmed`: requires `query` and optional `maxResults`\n  - `search-trials`: requires `condition` and optional `location`\n  - `drug-interactions`: requires `drugs` array\n\nrefer to: /src/server/constants/tools.ts for tools specirfication\n\n## Use with claude desktop\n```\nfor claude desktop: \nmacOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n(use the env variables as shown above)\n\n{\n  \"mcpServers\": {\n    \"google-cloud-healthcare-api-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/Users/your-username/{google-cloud-healthcare-api-mcp dir}/build/index.js\"\n      ],\n      \"env\": {\n          \"FIREBASE_API_KEY\":\"XXXXXXXXX\",\n          \"FIREBASE_AUTH_DOMAIN\":\"XXXXXXXX\",\n          \"FIREBASE_PROJECT_ID\":\"XXXXXXX\",\n          \"FIREBASE_STORAGE_BUCKET\":\"XXXXXXXXX\",\n          \"FIREBASE_MESSAGING_SENDER_ID\":\"XXXXXXX\",\n          \"FIREBASE_APP_ID\":\"XXXXXXXXX\",\n          \"FIREBASE_MEASUREMENT_ID\":\"XXXXXXXX\",\n          \"FIREBASE_AUTH_CALLBACK_PORT\":\"3456\",\n          \"FHIR_BASE_URL\":\"{gchapi-fhir-gateway-host}/fhir\",\n          \"PUBMED_API_KEY\":\"your_pubmed_api_key\",\n          \"CLINICAL_TRIALS_API_KEY\":\"your_trials_api_key\",\n          \"FDA_API_KEY\":\"your_fda_api_key\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install google-cloud-healthcare-api-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Kartha-AI/google-cloud-healthcare-api-mcp):\n\n```bash\nnpx -y @smithery/cli install @Kartha-AI/google-cloud-healthcare-api-mcp --client claude\n```\n\n## Start MCP Server Locally with MCP Inspector\n```\ngit clone git@github.com:Kartha-AI/google-cloud-healthcare-api-mcp.git\ncd google-cloud-healthcare-api-mcp\nnpm install\nnpm run build\nnpm install -g @modelcontextprotocol/inspector\nmcp-inspector  build/index.js\nhttp://localhost:5173\nSet up the env vars on Inspector\n```\n\n## Troubleshooting:\nIf Claude desktop is running it uses port 3456 for Auth. You need to terminate that process using the following command:\n```\nkill -9 $(lsof -t -i:3456)\n```",
      "npm_url": "",
      "npm_downloads": 0
    },
    "leescot--pubmed-mcp-smithery": {
      "owner": "leescot",
      "name": "pubmed-mcp-smithery",
      "url": "https://github.com/leescot/pubmed-mcp-smithery",
      "imageUrl": "https://github.com/leescot.png",
      "description": "Search and retrieve academic papers from the PubMed database using enhanced tools such as MeSH term lookup, publication statistics, and structured PICO-based evidence searches.",
      "stars": 5,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-12T02:43:23Z",
      "readme_content": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "manolaz--emergency-medicare-planner-mcp-server": {
      "owner": "manolaz",
      "name": "emergency-medicare-planner-mcp-server",
      "url": "https://github.com/manolaz/emergency-medicare-planner-mcp-server",
      "imageUrl": "https://github.com/manolaz.png",
      "description": "Connects to Google Maps to locate nearby hospitals and clinics based on patient needs, evaluates medical facilities, and calculates optimal routes for urgent care. Provides real-time availability checks and detailed service information to assist in emergency healthcare decisions.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-13T18:58:25Z",
      "readme_content": "# Emergency Medicare Management MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@manolaz/emergency-medicare-planner-mcp-server)](https://smithery.ai/server/@manolaz/emergency-medicare-planner-mcp-server)\n\n(@manolaz/emergency-medicare-planner-mcp-server)\n\nA powerful Model Context Protocol (MCP) server that integrates with Google Maps to locate and evaluate medical facilities in emergency situations. This server helps users find appropriate hospitals and clinics within 10km radius based on specific medical needs, emergency level, and facility capabilities.\n\nThe system provides real-time routing, availability checks, and detailed information about medical services, helping patients make informed decisions during urgent healthcare situations.\n\n**Key Feature**: Sequential Thinking for Medical Evaluation - Enables step-by-step clinical reasoning for more accurate medical facility matching based on patient symptoms and medical history.\n\n## Installation & Usage\n\n### Installing via Smithery\n\nTo install Emergency Medicare Planner for Claude Desktop automatically:\n\n```bash\nnpx -y @smithery/cli install @manolaz/emergency-medicare-planner-mcp-server --client claude\n```\n\n### Installing Manually\n\n```bash\n# Using npx (recommended)\nnpx @manolaz/emergency-medicare-planner-mcp-server\n\n# With environment variable for Google Maps API\nGOOGLE_MAPS_API_KEY=your_api_key npx @manolaz/emergency-medicare-planner-mcp-server\n```\n\nOr install globally:\n\n```bash\n# Install globally\nnpm install -g @manolaz/emergency-medicare-planner-mcp-server\n\n# Run after global installation\nGOOGLE_MAPS_API_KEY=your_api_key emergency-medicare-planner-mcp-server\n```\n\n## Components\n\n### Tools\n\n- **searchMedicalFacilities**\n  - Search for hospitals, clinics, and medical facilities using Google Places API\n  - Input:\n    - `query` (string): Search query (e.g., \"emergency room\", \"pediatric clinic\")\n    - `location`: Latitude and longitude of patient location\n    - `radius` (optional, default: 10000): Search radius in meters\n    - `specialtyNeeded` (optional): Medical specialty required\n\n- **getMedicalFacilityDetails**\n  - Get detailed information about a specific medical facility\n  - Input:\n    - `placeId` (string): Google Place ID of the medical facility\n  - Output:\n    - Hours of operation, services offered, contact information, etc.\n\n- **calculateRouteToFacility**\n  - Calculate fastest route to a medical facility\n  - Input:\n    - `origin`: Patient's current location\n    - `facilityId`: Place ID of the destination facility\n    - `transportMode` (optional): Travel mode (driving, walking, transit, ambulance)\n    - `avoidTraffic` (optional): Route planning to avoid traffic\n\n- **checkFacilityAvailability**\n  - Check if a facility is currently accepting patients\n  - Input:\n    - `facilityId`: Place ID of the medical facility\n    - `emergencyLevel`: Urgency level of the medical situation\n\n## Configuration\n\n### Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"emergency-medicare-planner\": {\n      \"command\": \"npx\",\n      \"args\": [\"@manolaz/emergency-medicare-planner-mcp-server\"],\n      \"env\": {\n        \"GOOGLE_MAPS_API_KEY\": \"your_google_maps_api_key\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use the node command directly if you have the package installed:\n\n```json\n{\n  \"mcpServers\": {\n    \"emergency-medicare-planner\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/dist/index.js\"],\n      \"env\": {\n        \"GOOGLE_MAPS_API_KEY\": \"your_google_maps_api_key\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\n### Building from Source\n\n1. Clone the repository\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n\n   ```bash\n   npm run build\n   ```\n\n### Environment Variables\n\n- `GOOGLE_MAPS_API_KEY` (required): Your Google Maps API key with the following APIs enabled:\n  - Places API\n  - Directions API\n  - Geocoding API\n  - Time Zone API\n  - Distance Matrix API\n\n### Testing\n\n```bash\n# Run test suite\nnpm test\n\n# Run with debug logging\nDEBUG=emergency-medicare:* npm start\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "matthewdcage--pbs-mcp-server": {
      "owner": "matthewdcage",
      "name": "pbs-mcp-server",
      "url": "https://github.com/matthewdcage/pbs-mcp-server",
      "imageUrl": "https://github.com/matthewdcage.png",
      "description": "Access up-to-date pharmaceutical data from the Australian PBS, including information on medicines, pricing, and availability. Facilitates seamless integration into AI workflows for real-time healthcare data retrieval.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-12T21:35:38Z",
      "readme_content": "# Pharmaceutical Benefits Scheme (PBS) MCP AI Enabled API Server ![MCP Server](https://badge.mcpx.dev?type=dev 'MCP Dev')\n\nA standalone Model Context Protocol (MCP) server for accessing the Australian Pharmaceutical Benefits Scheme (PBS) API.\n\n## About the Author\n\nThis PBS MCP server was developed by [Matthew Cage], Founder of https://ai-advantage.au, specialist in Automation, AI Engineering and AI integration and healthcare data systems.\n\nCollaborate with me:\nhttps://www.linkedin.com/in/digitalmarketingstrategyexpert/\n\n## Overview\n\nThis project provides a standalone MCP server that allows AI models to access the Australian Pharmaceutical Benefits Scheme (PBS) API, which contains information about medicines, pricing, and availability in Australia.\n\nThe project is built for the Public API, but can easily be adapted to the private API if you have been granted developer access.\n\nThe PBS API provides programmatic access to PBS data, including medicine listings, pricing, and availability. This MCP server makes it easy to integrate PBS data into AI workflows.\n\nThe MCP is available via HTTP and CLI.\n\n*Please be aware of the rate limits for the PBS and adjust your request frequency. I recommend a periodic call to store the information you require from the API and update it on a weekly basis.*\n\n## MCP Server Features ![MCP Server](https://badge.mcpx.dev?type=server&features=tools)\n\nThis MCP server implements the following Model Context Protocol features:\n\n- **Tools**: Provides tools for querying the PBS API endpoints, allowing AI models to access pharmaceutical data\n- **Transport Layers**: Supports both stdio and HTTP/SSE transport layers\n- **Error Handling**: Comprehensive error handling for API rate limits and authentication issues\n- **LLM Integration**: Receives tool calls and prompts directly from LLM components, enabling seamless AI interaction with PBS data\n\n### How It Works\n\nThe MCP Client ![MCP Client](https://badge.mcpx.dev?type=client&features=prompts,tools 'MCP Client'):\n\n1. **Receives Tool Calls**: When an LLM (like Claude) needs pharmaceutical data, it sends a tool call to this server\n2. **Processes Prompts**: Interprets natural language prompts about medication information\n3. **Executes API Queries**: Translates the requests into appropriate PBS API calls\n4. **Returns Structured Data**: Sends back formatted pharmaceutical data that the LLM can use in its responses\n\nThis enables AI assistants to access up-to-date PBS information without needing to have this data in their training.\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd pbs-mcp-standalone\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Usage\n\n### Starting the Server ![MCP Server](https://badge.mcpx.dev?type=server&features=tools)\n\nThe PBS MCP server can be run in different modes:\n\n#### Stdio Mode (Default)\n\nThis mode is compatible with the MCP protocol and communicates via standard input/output streams:\n\n```bash\nnpm start\n```\n\nOr use the provided start script:\n\n```bash\n./start.sh\n```\n\n#### HTTP Mode with SSE Support\n\nThis mode starts an HTTP server with Server-Sent Events (SSE) support:\n\n```bash\nnpm run start:http\n```\n\nOr use the provided start script:\n\n```bash\n./start.sh http 3000\n```\n\nWhere `3000` is the port number to listen on.\n\n#### Command-Line Interface\n\nThe PBS MCP server can also be used as a command-line tool:\n\n```bash\nnpm run cli -- <command>\n```\n\nOr use the provided start script:\n\n```bash\n./start.sh cli <command>\n```\n\nFor example:\n\n```bash\n./start.sh cli info\n```\n\n### Using as a Command-Line Tool\n\nTo use this MCP server as a command-line tool:\n\n1. Build the project:\n   ```bash\n   npm run build\n   ```\n\n2. Run the CLI with the desired command:\n   ```bash\n   npm run cli -- <command>\n   ```\n   \n   Or use the start script:\n   ```bash\n   ./start.sh cli <command>\n   ```\n\n### Integrating with MCP Clients ![MCP Client](https://badge.mcpx.dev?type=client)\n\nThis server can be integrated with any MCP-compatible client, such as:\n\n- Local AI Editors and AI/LLM Servers\n- Other AI assistants that support the Model Context Protocol\n- Custom applications using the MCP client libraries\n\n#### Client Configuration Example\n\nHere's an example of how to configure this server with an MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"pbs-api\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/pbs-mcp-standalone/build/index.js\"],\n      \"env\": {\n        \"PBS_API_SUBSCRIPTION_KEY\": \"your-subscription-key-here\"\n      }\n    }\n  }\n}\n```\n\n#### Accessing the Server from a Client\n\nTo access this MCP server from a client:\n\n1. **For Claude Desktop or other MCP-compatible AI assistants**:\n   - Configure the assistant to use this server as an MCP tool provider\n   - The assistant will automatically discover and use the tools provided by this server\n   - The LLM can send natural language prompts about medications that will be processed by the server\n\n2. **For custom applications**:\n   - Use the HTTP API endpoints described below\n   - Connect to the SSE endpoint for real-time tool events\n   - Or spawn the server process and communicate via stdin/stdout\n\n#### Example LLM Prompts\n\nThe server can interpret various prompts from LLMs, such as:\n\n```\n\"Find information about metformin in the PBS\"\n\"What is the PBS code for insulin?\"\n\"List all prescribers who can prescribe antibiotics\"\n\"Get the latest pricing for asthma medications\"\n```\n\nThese natural language prompts are translated into appropriate PBS API calls.\n\n### API Tool Parameters\n\nThe PBS API tool can be used with the following parameters:\n\n```json\n{\n  \"endpoint\": \"prescribers\",\n  \"method\": \"GET\",\n  \"params\": {\n    \"get_latest_schedule_only\": \"true\",\n    \"limit\": \"20\"\n  }\n}\n```\n\n#### Parameters\n\n- `endpoint` (string, required): The specific PBS API endpoint to access (e.g., \"prescribers\", \"item-overview\")\n- `method` (string, optional): HTTP method to use (GET is recommended for most PBS API operations). Default: \"GET\"\n- `params` (object, optional): Query parameters to include in the request\n- `subscriptionKey` (string, optional): Custom subscription key. If not provided, the default public key will be used\n- `timeout` (number, optional): Request timeout in milliseconds. Default: 30000\n\n## HTTP API ![MCP Server](https://badge.mcpx.dev?type=server&features=tools)\n\nWhen running in HTTP mode, the following endpoints are available:\n\n### Health Check\n\n```\nGET /health\n```\n\nReturns the status of the server.\n\n### List Tools\n\n```\nGET /tools\n```\n\nReturns a list of available tools.\n\n### SSE Endpoint\n\n```\nGET /sse\n```\n\nEstablishes an SSE connection and sends tool events.\n\n### Tool Invocation (SSE)\n\n```\nPOST /sse/:toolName\n```\n\nInvokes a tool and sends the result via SSE.\n\n### Tool Invocation (REST)\n\n```\nPOST /api/:toolName\n```\n\nInvokes a tool and returns the result as JSON.\n\n## Command-Line Interface ![MCP Dev](https://badge.mcpx.dev?type=dev)\n\nThe PBS MCP server can be used as a command-line tool with the following commands:\n\n### List Endpoints\n\n```bash\n./start.sh cli list-endpoints\n```\n\nLists all available PBS API endpoints.\n\n### Get API Information\n\n```bash\n./start.sh cli info\n```\n\nReturns information about the PBS API.\n\n### Query Prescribers\n\n```bash\n./start.sh cli prescribers [options]\n```\n\nOptions:\n- `-l, --limit <number>`: Number of results per page (default: 10)\n- `-p, --page <number>`: Page number (default: 1)\n- `-c, --pbs-code <code>`: Filter by PBS code\n- `-s, --schedule-code <code>`: Filter by schedule code\n- `-t, --prescriber-type <type>`: Filter by prescriber type\n- `-f, --fields <fields>`: Specific fields to return\n- `--latest`: Get only the latest schedule\n\n### Query Item Overview\n\n```bash\n./start.sh cli item-overview [options]\n```\n\nOptions:\n- `-l, --limit <number>`: Number of results per page (default: 10)\n- `-p, --page <number>`: Page number (default: 1)\n- `-s, --schedule-code <code>`: Filter by schedule code\n- `-f, --fields <fields>`: Specific fields to return\n- `--latest`: Get only the latest schedule\n\n### Query Any Endpoint\n\n```bash\n./start.sh cli query <endpoint> [options]\n```\n\nOptions:\n- `-m, --method <method>`: HTTP method (default: GET)\n- `-p, --params <json>`: Query parameters as JSON string\n- `-k, --subscription-key <key>`: Custom subscription key\n- `-t, --timeout <milliseconds>`: Request timeout in milliseconds\n\n### Start HTTP Server\n\n```bash\n./start.sh cli serve [options]\n```\n\nOptions:\n- `-p, --port <number>`: Port to listen on (default: 3000)\n\n## Available Endpoints\n\nThe PBS API provides several endpoints for accessing different types of data:\n\n- `/` - Root endpoint, provides API information and changelog\n- `/prescribers` - Information about prescribers\n- `/item-overview` - Detailed information about PBS items\n- `/items` - Basic information about PBS items\n- `/schedules` - Information about PBS schedules\n- `/atc-codes` - Anatomical Therapeutic Chemical (ATC) classification codes\n- `/organisations` - Information about organisations\n- `/restrictions` - Information about restrictions\n- `/parameters` - Information about parameters\n- `/criteria` - Information about criteria\n- `/copayments` - Information about copayments\n- `/fees` - Information about fees\n- `/markup-bands` - Information about markup bands\n- `/programs` - Information about programs\n- `/summary-of-changes` - Summary of changes\n\nFor a complete list of endpoints, see the [PBS API documentation](https://data-api-portal.health.gov.au/api-details#api=pbs-prod-api-public-v3-v3).\n\n## Example Usage\n\n### Get API Information\n\n```json\n{\n  \"endpoint\": \"\"\n}\n```\n\n### Get Prescribers\n\n```json\n{\n  \"endpoint\": \"prescribers\",\n  \"params\": {\n    \"get_latest_schedule_only\": \"true\",\n    \"limit\": \"10\"\n  }\n}\n```\n\n### Get Item Overview with Latest Schedule\n\n```json\n{\n  \"endpoint\": \"item-overview\",\n  \"params\": {\n    \"get_latest_schedule_only\": \"true\",\n    \"limit\": \"5\"\n  }\n}\n```\n\n### Get Prescribers with Specific PBS Code\n\n```json\n{\n  \"endpoint\": \"prescribers\",\n  \"params\": {\n    \"pbs_code\": \"10001J\",\n    \"get_latest_schedule_only\": \"true\"\n  }\n}\n```\n\n## Authentication\n\nThe tool uses a subscription key for accessing the PBS API. You can obtain your own key by registering on the PBS Developer Portal.\n\nFor development purposes, see the `.env.example` file for configuration details.\n\n### Obtaining a PBS API Subscription Key\n\nTo obtain your own PBS API subscription key, follow these steps:\n\n1. **Visit the PBS Data API Portal**: \n   - Go to [https://data-api-portal.health.gov.au/](https://data-api-portal.health.gov.au/)\n\n2. **Create an Account**:\n   - Click on \"Sign Up\" to create a new account\n   - Fill in your details and verify your email address\n\n3. **Subscribe to the PBS API**:\n   - Once logged in, navigate to the \"Products\" section\n   - Select the \"PBS Public API v3\" product\n   - Click \"Subscribe\" to request access to the API\n\n4. **Retrieve Your Subscription Key**:\n   - After your subscription is approved, go to your profile\n   - Navigate to \"Subscriptions\" or \"API Keys\" section\n   - Copy your primary or secondary key\n\n5. **Configure Your Environment**:\n   - Create a `.env` file based on the `.env.example` template\n   - Replace `your-subscription-key-here` with your actual subscription key:\n     ```\n     PBS_API_SUBSCRIPTION_KEY=your-actual-subscription-key\n     ```\n\n**Note**: The PBS Public API is rate-limited to one request per 20 seconds. This limit is shared among all users of the public API. For higher rate limits or access to embargo data (future schedules), you may need to apply for special access through the PBS Developer Program.\n\n## Limitations\n\n- The PBS Public API is rate-limited to one request per 20 seconds (shared among all users)\n- Only the current schedule and those published in the past 12 months are available via the Public API\n- Some endpoints require specific parameters to be provided\n- The API structure and endpoints may change over time\n\n## Additional Resources\n\n- [PBS Website](https://www.pbs.gov.au/)\n- [PBS Data Website](https://data.pbs.gov.au/)\n- [PBS API Documentation](https://data-api-portal.health.gov.au/api-details#api=pbs-prod-api-public-v3-v3)\n- [Model Context Protocol Documentation](https://github.com/modelcontextprotocol/mcp)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n---\n\n![MCP Server](https://badge.mcpx.dev?type=server&features=tools) ![MCP Client](https://badge.mcpx.dev?type=client) ![MCP Dev](https://badge.mcpx.dev?type=dev) ![MCP Enabled](https://badge.mcpx.dev?status=on) ‚ù§Ô∏è \n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "mattjoyce--senechal-mcp": {
      "owner": "mattjoyce",
      "name": "senechal-mcp",
      "url": "https://github.com/mattjoyce/senechal-mcp",
      "imageUrl": "https://github.com/mattjoyce.png",
      "description": "Access and analyze health data from the Senechal API, providing health profiles, trends, and summaries for integration with LLM applications. Offers reusable templates for health data analysis, enhancing data insights and accessibility.",
      "stars": 0,
      "forks": 3,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-03-08T00:11:02Z",
      "readme_content": "# Senechal MCP Server\n\nA Model Context Protocol (MCP) server that acts as a companion to the Senechal project, providing health data from the Senechal API to LLM applications.\n\n## Overview\n\nThis server provides a standardized interface for LLMs to access health data from the Senechal API. It exposes:\n\n- **Resources**: Health data that can be loaded into an LLM's context\n- **Tools**: Functions that can be called by LLMs to fetch health data\n- **Prompts**: Reusable templates for analyzing health data\n\n## Installation\n\n1. Clone this repository\n2. Create a virtual environment:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Configuration\n\nCopy the `.env.example` file to `.env` and add your Senechal API key and URL:\n\n```\n# Required: Senechal API Key\nSENECHAL_API_KEY=your_api_key_here\n\n# Required: API base URL\nSENECHAL_API_BASE_URL=https://your-api-host/api/senechal\n```\n\nBoth the API key and API URL are required for the server to function.\n\n### Windows Configuration\n\nWhen running on Windows, be sure to:\n\n1. Use backslashes or properly escaped paths in the configuration\n2. Use the full path to your Python virtual environment in the claude-desktop-config.json:\n\n```json\n{\n    \"mcpServers\": {\n        \"senechal-health\": {\n            \"command\": \"C:\\\\path\\\\to\\\\venv\\\\Scripts\\\\python.exe\",\n            \"args\": [\n                \"C:\\\\path\\\\to\\\\senechal_mcp_server.py\"\n            ],\n            \"env\": {\n                \"SENECHAL_API_KEY\": \"your_api_key_here\"\n            }\n        }\n    }\n}\n```\n\nNote that environment variables in the MCP configuration do not use the `.env` file, so you'll need to set them explicitly in the config.\n\n## Usage\n\n### Testing the Client/Server Setup\n\nThe simplest way to test the setup is to run the example client:\n\n```bash\n# In one terminal, start the server\npython senechal_mcp_server.py\n\n# In another terminal, run the example client\npython example_client.py\n```\n\n### Start the Server\n\n```bash\npython senechal_mcp_server.py\n```\n\n### Development Mode with MCP Inspector\n\n```bash\nmcp dev senechal_mcp_server.py\n```\n\n### Install in Claude Desktop\n\nThe server includes a configuration file for Claude Desktop:\n\n```bash\nmcp install senechal_mcp_server.py\n```\n\nYou can then select \"Senechal Health\" from the tools menu in Claude Desktop.\n\n## Available Resources\n\n- `senechal://health/summary/{period}` - Get health summary for day, week, month, or year\n  - Example: `senechal://health/summary/day?span=7&metrics=all`\n  - Parameters:\n    - `period`: day, week, month, year\n    - `span`: Number of periods (default: 1)\n    - `metrics`: Comma-separated list or \"all\" (default)\n    - `offset`: Number of periods to offset from now (default: 0)\n\n- `senechal://health/profile` - Get the user's health profile\n  - Contains demographics, medications, supplements\n\n- `senechal://health/current` - Get current health measurements\n  - Example: `senechal://health/current?types=1,2,3`\n  - Parameters:\n    - `types`: Optional comma-separated list of measurement type IDs\n\n- `senechal://health/trends` - Get health trends over time\n  - Example: `senechal://health/trends?days=30&types=1,2,3&interval=day`\n  - Parameters:\n    - `days`: Number of days to analyze (default: 30)\n    - `types`: Optional comma-separated list of measurement type IDs\n    - `interval`: Grouping interval - day, week, month (default: day)\n\n- `senechal://health/stats` - Get statistical analysis of health metrics\n  - Example: `senechal://health/stats?days=30&types=1,2,3`\n  - Parameters:\n    - `days`: Analysis period in days (default: 30)\n    - `types`: Optional comma-separated list of measurement type IDs\n\n## Available Tools\n\n- `fetch_health_summary` - Fetch a health summary for a specific period\n  - Parameters:\n    - `period` (required): day, week, month, year\n    - `metrics` (optional): Comma-separated metrics or \"all\" (default)\n    - `span` (optional): Number of periods to return (default: 1)\n    - `offset` (optional): Number of periods to offset (default: 0)\n\n- `fetch_health_profile` - Fetch the user's health profile\n  - No parameters required\n\n- `fetch_current_health` - Fetch the latest health measurements\n  - Parameters:\n    - `types` (optional): List of measurement type IDs to filter by\n\n- `fetch_health_trends` - Fetch health trend data\n  - Parameters:\n    - `days` (optional): Number of days to analyze (default: 30)\n    - `types` (optional): List of measurement type IDs to filter by\n    - `interval` (optional): Grouping interval - day, week, month (default: day)\n\n- `fetch_health_stats` - Fetch statistical analysis of health metrics\n  - Parameters:\n    - `days` (optional): Analysis period in days (default: 30)\n    - `types` (optional): List of measurement type IDs to filter by\n\n## Available Prompts\n\n- `analyze_health_summary` - Prompt to analyze health summaries\n  - Provides a template for identifying abnormal metrics, trends, and suggesting actions\n  - Intended to be used with data from `senechal://health/summary/day?span=7`\n\n- `compare_health_trends` - Prompt to compare health trends over different time periods\n  - Provides a template for comparing trends across different timeframes (7, 30, 90 days)\n  - Intended to be used with data from the health trends endpoint\n\n## Example Interactions\n\n### Loading Health Summary Data\n\n```python\n# In an LLM application, load a week of health summaries\ncontent, mime_type = await session.read_resource(\"senechal://health/summary/day?span=7\")\n```\n\n### Calling Health Data Tools\n\n```python\n# In an LLM conversation\nresult = await session.call_tool(\n    \"fetch_health_trends\", \n    arguments={\n        \"days\": 30, \n        \"interval\": \"day\"\n    }\n)\n\n# More complex example combining tools and resources\nprofile = await session.call_tool(\"fetch_health_profile\")\ntrends = await session.call_tool(\n    \"fetch_health_trends\", \n    arguments={\"days\": 90, \"interval\": \"week\"}\n)\n```\n\n### Using Health Analysis Prompts\n\n```python\n# Get a prompt for analyzing health data\nprompt_result = await session.get_prompt(\"analyze_health_summary\")\nfor message in prompt_result.messages:\n    print(f\"[{message.role}]: {message.content.text}\")\n```\n\nSee the `example_client.py` file for a complete working example.\n\n## API Endpoints\n\nThe Senechal MCP server communicates with the following Senechal API endpoints:\n\n- `/health/summary/{period}` - Get health summaries\n- `/health/profile` - Get health profile\n- `/health/current` - Get current measurements\n- `/health/trends` - Get health trends\n- `/health/stats` - Get health stats",
      "npm_url": "",
      "npm_downloads": 0
    },
    "msathiyakeerthi--google-cloud-healthcare-api-mcp": {
      "owner": "msathiyakeerthi",
      "name": "google-cloud-healthcare-api-mcp",
      "url": "https://github.com/msathiyakeerthi/google-cloud-healthcare-api-mcp",
      "imageUrl": "https://github.com/msathiyakeerthi.png",
      "description": "Integrates with Google Cloud Healthcare FHIR APIs and public medical research databases to access and query healthcare data, including patient records and clinical data. Enables retrieval of research articles and provides insights through a secure MCP interface.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-21T21:18:20Z",
      "readme_content": "# An MCP Server for Google Cloud Healthcare API (FHIR)\n\n[![smithery badge](https://smithery.ai/badge/@Kartha-AI/google-cloud-healthcare-api-mcp)](https://smithery.ai/server/@Kartha-AI/google-cloud-healthcare-api-mcp)\n\nA Model Context Protocol (MCP) server that provides healthcare tools for interacting with FHIR resources on [Google Cloud Healthcare API](https://cloud.google.com/healthcare-api) and public medical research APIs like pubmed, using MCP clients like Claude and Goose.\n\nThis is a slightly modified version for AgentCare MCP Server for EHRs. https://github.com/Kartha-AI/agentcare-mcp\n\nThe maion differnce is that this repo talks to Google Cloud Healthcare FHIR APIs thru a SmartonFHIR gateway that's secured by Firebase Auth. \n\n## Architetcure\n<img src=\"screenshots/architecture.png\" alt=\"Architecture\" width=\"700\">\n\n## Demo\n- Claude: demo/claude-demo.mp4\n- Goose: demo/goose-demo.mp4\n\n## Screenshots\n<img src=\"screenshots/goose/goose-auth.png\" alt=\"Auth\" width=\"700\">\n<img src=\"screenshots/goose/goose-patient.png\" alt=\"Patient\" width=\"700\">\n<img src=\"screenshots/goose/goose-conditions.png\" alt=\"Conditions\" width=\"700\">\n<img src=\"screenshots/goose/goose-vitals.png\" alt=\"Vitals\" width=\"700\">\n\n<img src=\"screenshots/claude/gchapi.png\" alt=\"GCHAPI\" width=\"700\">\n<img src=\"screenshots/claude/convo.png\" alt=\"Converse\" width=\"700\">\n<img src=\"screenshots/claude/chart.png\" alt=\"Timeline\" width=\"700\">\n<img src=\"screenshots/claude/soap.png\" alt=\"Soap Notes\" width=\"700\">\n\n## Tools\n\n### FHIR Tools\n- `find_patient` - Search for a patient by name, DOB, or other identifiers\n- `get_patient_observations` - Retrieve patient observations/vital signs\n- `get_patient_conditions` - Get patient's active conditions\n- `get_patient_medications` - Get patient's current medications\n- `get_patient_encounters` - Get patient's clinical encounters\n- `get_patient_allergies` - Get patient's allergies and intolerances\n- `get_patient_procedures` - Get patient's procedures\n- `get_patient_careteam` - Get patient's care team members\n- `get_patient_careplans` - Get patient's active care plans\n- `get_vital_signs` - Get patient's vital signs\n- `get_lab_results` - Get patient's laboratory results\n- `get_medications_history` - Get patient's medication history\n- `clinical_query` - Execute custom FHIR queries\n\n### Medical Research Tools\n- `search-pubmed` - Search PubMed articles related to medical conditions\n- `search-trials` - Find relevant clinical trials\n- `drug-interactions` - Check drug-drug interactions\n\n## Usage\n\nEach tool  requires specific parameters:\n\n### Required Parameters\n- Most tools require `patientId`\n- Some tools have additional parameters:\n  - `lab_trend_analysis`: requires `labType`\n  - `search-pubmed`: requires `query` and optional `maxResults`\n  - `search-trials`: requires `condition` and optional `location`\n  - `drug-interactions`: requires `drugs` array\n\nrefer to: /src/server/constants/tools.ts for tools specirfication\n\n## Use with claude desktop\n````\nfor claude desktop: \nmacOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n(use the env variables as shown above)\n\n{\n  \"mcpServers\": {\n    \"google-cloud-healthcare-api-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/Users/your-username/{google-cloud-healthcare-api-mcp dir}/build/index.js\"\n      ],\n      \"env\": {\n          \"FIREBASE_API_KEY\":\"XXXXXXXXX\",\n          \"FIREBASE_AUTH_DOMAIN\":\"XXXXXXXX\",\n          \"FIREBASE_PROJECT_ID\":\"XXXXXXX\",\n          \"FIREBASE_STORAGE_BUCKET\":\"XXXXXXXXX\",\n          \"FIREBASE_MESSAGING_SENDER_ID\":\"XXXXXXX\",\n          \"FIREBASE_APP_ID\":\"XXXXXXXXX\",\n          \"FIREBASE_MEASUREMENT_ID\":\"XXXXXXXX\",\n          \"FIREBASE_AUTH_CALLBACK_PORT\":\"3456\",\n          \"FHIR_BASE_URL\":\"{gchapi-fhir-gateway-host}/fhir\",\n          \"PUBMED_API_KEY\":\"your_pubmed_api_key\",\n          \"CLINICAL_TRIALS_API_KEY\":\"your_trials_api_key\",\n          \"FDA_API_KEY\":\"your_fda_api_key\"\n      }\n    }\n  }\n}\n````\n\n### Installing via Smithery\n\nTo install google-cloud-healthcare-api-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Kartha-AI/google-cloud-healthcare-api-mcp):\n\n```bash\nnpx -y @smithery/cli install @Kartha-AI/google-cloud-healthcare-api-mcp --client claude\n```\n\n## Start MCP Server Locally with MCP Inspector\n````\ngit clone git@github.com:Kartha-AI/google-cloud-healthcare-api-mcp.git\ncd google-cloud-healthcare-api-mcp\nnpm install\nnpm run build\nnpm install -g @modelcontextprotocol/inspector\nmcp-inspector  build/index.js\nhttp://localhost:5173\nSet up the env vars on Inspector\n````\n\n## Troubleshooting:\nIf Claude desktop is running it uses port 3456 for Auth. You need to terminate that process using the following command:\n````\nkill -9 $(lsof -t -i:3456)\n````\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "navisbio--AACT_MCP": {
      "owner": "navisbio",
      "name": "AACT_MCP",
      "url": "https://github.com/navisbio/AACT_MCP",
      "imageUrl": "https://github.com/navisbio.png",
      "description": "Provides access to the AACT database for analyzing clinical trial data and generating reports on therapeutic landscapes. Facilitates querying and understanding of clinical trial data structures through a series of tools that enable direct interaction with the database.",
      "stars": 16,
      "forks": 5,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-08-12T18:42:13Z",
      "readme_content": "# AACT Clinical Trials MCP Server\n\n## Overview\nA Model Context Protocol (MCP) server implementation that provides access to the AACT (Aggregate Analysis of ClinicalTrials.gov) database using the FastMCP framework. This server allows AI assistants to directly query clinical trial data from the ClinicalTrials.gov database.\n\n## Features\n\n### Tools\n\n- `list_tables`\n   - Get an overview of all available tables in the AACT database\n   - Useful for understanding the database structure before analysis\n\n- `describe_table`\n   - Examine the detailed structure of a specific AACT table\n   - Shows column names and data types\n   - Example: `{\"table_name\": \"studies\"}`\n\n- `read_query`\n   - Execute a SELECT query on the AACT clinical trials database\n   - Safely handle SQL queries with validation\n   - Example: `{\"query\": \"SELECT nct_id, brief_title FROM ctgov.studies LIMIT 5\", \"max_rows\": 50}`\n\n## Configuration\n\n### Database Access\n1. Create a free account at https://aact.ctti-clinicaltrials.org/users/sign_up\n2. Set environment variables:\n   - `DB_USER`: AACT database username\n   - `DB_PASSWORD`: AACT database password\n\n## Usage with Claude Desktop\n\nNote that you need Claude Desktop and a Claude subscription at the moment. \n\nAdd one of the following configurations to the file claude_desktop_config.json. (On macOS, the file is located at /Users/YOUR_USERNAME/Library/Application Support/Claude/claude_desktop_config.json and you will need to create it yourself if it does not exist yet).\n\n### Option 1: Using the published package\n```json\n{\n  \"mcpServers\": {\n    \"CTGOV-MCP\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-aact\"\n      ],\n      \"env\": {\n        \"DB_USER\": \"USERNAME\",\n        \"DB_PASSWORD\": \"PASSWORD\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Using Docker\n\nSimply add this configuration to claude_desktop_config.json (no build required):\n```json\n{\n  \"mcpServers\": {\n    \"CTGOV-MCP-DOCKER\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--env\", \"DB_USER=YOUR_USERNAME\",\n        \"--env\", \"DB_PASSWORD=YOUR_PASSWORD\",\n        \"navisbio/mcp-server-aact:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Option 3: Running from source (development)\n\nSimply add this configuration to claude_desktop_config.json (no build required):\n```json\n{\n  \"mcpServers\": {\n    \"CTGOV-MCP-DOCKER\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--env\", \"DB_USER=YOUR_USERNAME\",\n        \"--env\", \"DB_PASSWORD=YOUR_PASSWORD\",\n        \"navisbio/mcp-server-aact:latest\"\n      ]\n    }\n  }\n}\n```\n\n## Example Prompts\n\nHere are some example prompts to use with this plugin:\n\n1. \"What are the most common types of interventions in breast cancer clinical trials?\"\n2. \"How many phase 3 clinical trials were completed in 2023?\"\n3. \"Show me the enrollment statistics for diabetes trials across different countries\"\n4. \"What percentage of oncology trials have reported results in the last 5 years?\"\n\n## Troubleshooting\n\n### `spawn uvx ENOENT` Error\n\nThis error has been reported when the system cannot find the `uvx` command which might happen when `uvx` is installed in a non-standard location (like `~/.local/bin/`).\n\n**Potential Solution:** Update your configuration with the full path. For example:\n\n```json\n{\n\"mcpServers\": {\n    \"CTGOV-MCP\": {\n      \"command\": \"/Users/username/.local/bin/uvx\",\n      \"args\": [\n        \"mcp-server-aact\"\n      ],\n      \"env\": {\n        \"DB_USER\": \"USERNAME\",\n        \"DB_PASSWORD\": \"PASSWORD\"\n      }\n    }\n}\n}\n```\n\n\n## Contributing\nWe welcome contributions! Please:\n- Open an issue on GitHub\n- Start a discussion\n- Email: jonas.walheim@navis-bio.com\n\n## Acknowledgements\n\nThis project was inspired by and initially based on code from:\n- [SQLite MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/sqlite)\n- [DuckDB MCP Server](https://github.com/ktanaka101/mcp-server-duckdb/tree/main)\n- [OpenDataMCP](https://github.com/OpenDataMCP/OpenDataMCP)\n\nThanks to these awesome projects for showing us the way! üôå\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "NitayRabi--fitbit-mcp": {
      "owner": "NitayRabi",
      "name": "fitbit-mcp",
      "url": "https://github.com/NitayRabi/fitbit-mcp",
      "imageUrl": "https://github.com/NitayRabi.png",
      "description": "Enables access and analysis of Fitbit health and fitness data, including activities, sleep logs, heart rate, steps, and body measurements through simple commands. Facilitates integration of Fitbit data insights into AI interactions.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-19T18:39:25Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/nitayrabi-fitbit-mcp-badge.png)](https://mseep.ai/app/nitayrabi-fitbit-mcp)\n\n# Fitbit MCP (Model Context Protocol)\n[![smithery badge](https://smithery.ai/badge/@NitayRabi/fitbit-mcp)](https://smithery.ai/server/@NitayRabi/fitbit-mcp)\n\n**Disclaimer:** This is an unofficial integration built using Fitbit's public API and is not affiliated with or endorsed by Fitbit Inc.\n\nA Model Context Protocol (MCP) implementation for Fitbit, enabling AI assistants to access and analyze your Fitbit health and fitness data.\n\n## Usage\n\nFor JSON configuration (for use with AI assistant frameworks):\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"fitbit-mcp\", \"--stdio\"],\n  \"env\": {\n    \"FITBIT_ACCESS_TOKEN\": \"YOUR_FITBIT_ACCESS_TOKEN\"\n  }\n}\n```\n\nOr with arguments instead of environment variables:\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"fitbit-mcp\", \"--stdio\", \"--fitbit-token=YOUR_FITBIT_ACCESS_TOKEN\"]\n}\n```\n\n## Available Tools\n\nThis MCP provides the following tools for AI assistants to access your Fitbit data:\n\n- **getUserProfile**: Get your Fitbit profile information\n- **getActivities**: Get activity data for a specified date\n- **getSleepLogs**: Get sleep data for a specified date\n- **getHeartRate**: Get heart rate data for a specified date and period\n- **getSteps**: Get step count for a specified date and period\n- **getBodyMeasurements**: Get weight and body fat data\n- **getFoodLogs**: Get food log data for a specified date\n- **getWaterLogs**: Get water consumption data for a specified date\n- **getLifetimeStats**: Get lifetime activity statistics\n- **getUserSettings**: Get user settings and preferences\n- **getFloorsClimbed**: Get floors climbed data\n- **getDistance**: Get distance data\n- **getCalories**: Get calories burned data\n- **getActiveZoneMinutes**: Get active zone minutes data\n- **getDevices**: Get information about connected Fitbit devices\n- **getBadges**: Get earned badges and achievements\n\nMost tools accept optional parameters:\n- `date`: Date in YYYY-MM-DD format (defaults to today)\n- `period`: Time period for data (1d, 7d, 30d, 1w, 1m)\n\n## Obtaining a Fitbit Access Token\n\nTo get a Fitbit access token:\n\n1. Create an application at [Fitbit Developer Portal](https://dev.fitbit.com/apps/new)\n2. Set OAuth 2.0 Application Type to \"Personal\"\n3. Set Callback URL to \"http://localhost:3000\"\n4. After creating the application, note your Client ID and Client Secret\n5. Use the OAuth 2.0 authorization flow to obtain an access token\n\nFor detailed instructions on OAuth authentication, see the [Fitbit API Documentation](https://dev.fitbit.com/build/reference/web-api/oauth2/).\n\n## Contributing\n\nContributions are welcome! Here's how you can contribute:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nEvery pull request triggers a GitHub Actions workflow that verifies the build process.\n\n### Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/your-username/fitbit-mcp.git\ncd fitbit-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run in development mode\nnpm run dev\n```\n\n### Release Process\n\nTo publish a new version to NPM:\n\n1. Update the version in `package.json`\n2. Create a new GitHub release with a tag like `v1.0.1`\n3. The GitHub Actions workflow will automatically build and publish the package to NPM\n\nMake sure you have the `NPM_TOKEN` secret configured in your GitHub repository settings.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "pickleton89--mutation-clinical-trial-matching-mcp": {
      "owner": "pickleton89",
      "name": "mutation-clinical-trial-matching-mcp",
      "url": "https://github.com/pickleton89/mutation-clinical-trial-matching-mcp",
      "imageUrl": "https://github.com/pickleton89.png",
      "description": "Connects to clinicaltrials.gov to retrieve and summarize clinical trials related to specific genetic mutations through natural language queries.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-21T16:03:59Z",
      "readme_content": "# Mutation Clinical Trial Matching MCP\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Version 0.2.1](https://img.shields.io/badge/version-0.2.1-blue.svg)](https://github.com/pickleton89/mutation-clinical-trial-matching-mcp/releases)\n[![Tests](https://img.shields.io/badge/tests-114%20tests-green.svg)](https://github.com/pickleton89/mutation-clinical-trial-matching-mcp/actions)\n[![Code Style: Ruff](https://img.shields.io/badge/code%20style-ruff-black.svg)](https://github.com/astral-sh/ruff)\n[![Code Deduplication](https://img.shields.io/badge/code%20deduplication-60%25%20reduction-brightgreen.svg)](https://github.com/pickleton89/mutation-clinical-trial-matching-mcp)\n[![Architecture](https://img.shields.io/badge/architecture-unified%20sync%2Fasync-blue.svg)](https://github.com/pickleton89/mutation-clinical-trial-matching-mcp)\n\nA high-performance **unified** Model Context Protocol (MCP) server that enables Claude Desktop to search for clinical trial matches on clinicaltrials.gov based on genetic mutations. \n\n## Status\n\n**Production Ready** - This project has completed a major architectural transformation, achieving a **unified codebase** with 60% code reduction while maintaining 100% backward compatibility:\n\n‚úÖ **Repository Quality Excellence**: 99.6% improvement in code quality with modern Python standards (Python 3.11+ compatibility)  \n‚úÖ **Professional Type Safety**: 69% reduction in type diagnostics with comprehensive typing standards  \n‚úÖ **Unified Architecture**: Single server supporting both sync and async modes with runtime selection  \n‚úÖ **Code Deduplication**: 60% reduction (~1,000 lines) through comprehensive 4-phase consolidation  \n‚úÖ **Legacy Cleanup**: Professional codebase structure with 3,435 lines of deprecated code removed  \n‚úÖ **Zero Breaking Changes**: Complete backward compatibility with automatic migration guidance via compatibility layer  \n‚úÖ **Enterprise Features**: Circuit breakers, metrics, retry logic, distributed caching, and monitoring  \n‚úÖ **High Performance**: Async architecture with 80% performance improvement and concurrent processing  \n‚úÖ **API Resilience**: Robust error handling with 403 Forbidden error resolution via unified HTTP client  \n‚úÖ **Comprehensive Testing**: Complete test suite with 114 tests covering unified components  \n‚úÖ **Modern Tooling**: Uses `uv` for dependency management and follows Python best practices  \n‚úÖ **Production Monitoring**: Prometheus metrics, cache analytics, and health monitoring dashboards  \n\nThe server is actively used and maintained, with the unified architecture documented in the [changelog](CHANGELOG.md).\n\n## **AI-Collaborative Development**\n\nThis project was developed through **human-AI collaboration**, combining domain expertise with LLM-directed implementation:\n\n- **üß† Domain Direction**: 20+ years cancer research experience guided architecture and feature requirements\n- **ü§ñ AI Implementation**: Code generation, API design, and performance optimization through systematic LLM direction\n- **üîÑ Quality Assurance**: Iterative refinement ensuring professional standards and production reliability\n- **üìà Development Approach**: Demonstrates how domain experts can effectively leverage AI tools to build bioinformatics platforms\n\n**Methodology**: This AI-collaborative approach combines biological expertise with AI capabilities to accelerate development while maintaining code quality and reliability standards.\n\n## Overview\n\nThis project follows the Agentic Coding principles to create a system that integrates Claude Desktop with the clinicaltrials.gov API. The server allows for natural language queries about genetic mutations and returns summarized information about relevant clinical trials.\n\n```mermaid\nflowchart LR\n    Claude[Claude Desktop] <-->|MCP Protocol| Server[Unified MCP Server]\n    \n    subgraph Detection[Runtime Mode Detection]\n        Auto[Auto-Detect Event Loop]\n        Env[MCP_ASYNC_MODE]\n        Config[Configuration Override]\n    end\n    \n    subgraph Cache[Distributed Cache]\n        Redis[(Redis)]\n        Memory[In-Memory]\n    end\n    \n    subgraph Flow[Unified PocketFlow]\n        QueryNode[Unified Query Node] -->|trials_data| SummarizeNode[Unified Summarize Node]\n    end\n    \n    subgraph Services[Service Abstraction Layer]\n        HttpClient[Unified HTTP Client]\n        TrialsService[Clinical Trials Service]\n        LLMService[LLM Service]\n    end\n    \n    subgraph Monitoring[Enterprise Features]\n        Metrics[Prometheus Metrics]\n        Circuit[Circuit Breaker]\n        Analytics[Cache Analytics]\n    end\n    \n    Server -->|mode selection| Detection\n    Detection -->|sync/async| Flow\n    Server -->|mutation| Flow\n    Flow -->|service calls| Services\n    Services <-->|cache| Cache\n    Services -->|concurrent/sequential requests| API[Clinicaltrials.gov API]\n    API -->|trial data| Services\n    Flow -->|summary| Server\n    Server -->|metrics| Monitoring\n    Server -->|formatted response| Claude\n```\n\nEach node in the flow follows the **Unified PocketFlow Node pattern** with `prep`, `exec`, and `post` methods that automatically handle both sync and async execution modes.\n\n## üöÄ Unified Architecture & Code Deduplication Achievement\n\nThis project has completed a comprehensive **4-phase code deduplication effort**, transforming from a duplicated codebase into a unified, maintainable architecture:\n\n### Code Deduplication Results\n\n| **Metric** | **Achievement** |\n|------------|-----------------|\n| **Code Reduction** | **60% reduction** (~1,000 lines eliminated) |\n| **Legacy Cleanup** | **3,435 lines removed** - Professional codebase structure |\n| **Code Quality** | **99.6% improvement** - 1,695 of 1,702 linting errors fixed |\n| **Type Safety** | **69% reduction** in type diagnostics (48 ‚Üí 15) |\n| **Components Unified** | **4 major consolidations** (Servers, Nodes, Services, HTTP) |\n| **Breaking Changes** | **Zero** - Complete backward compatibility with compatibility layer |\n| **Performance Gain** | **30-40% memory reduction**, **20-30% faster startup** |\n| **Test Coverage** | **114 tests** covering all unified components |\n\n### Before vs After Consolidation\n\n| **Component** | **Before** | **After** | **Reduction** |\n|---------------|------------|-----------|---------------|\n| **Servers** | `primary.py` + `sync_server.py` | `main.py` | **70%** |\n| **Nodes** | `nodes.py` + `async_nodes.py` | `unified_nodes.py` | **85%** |\n| **Services** | `query.py` + `async_query.py` | `service.py` | **95%** |\n| **LLM Client** | `call_llm.py` + `async_call_llm.py` | `llm_service.py` | **95%** |\n\n### Key Architectural Improvements\n\n‚úÖ **Runtime Mode Selection**: Automatic detection or explicit configuration via `MCP_ASYNC_MODE`  \n‚úÖ **Single Point of Truth**: Unified business logic across sync/async execution  \n‚úÖ **Auto-Detection**: Intelligent mode selection based on execution context  \n‚úÖ **Service Abstraction**: Unified HTTP client and service layer  \n‚úÖ **Configuration System**: Centralized configuration with environment overrides  \n‚úÖ **Backward Compatibility Layer**: Complete `utils/node.py` compatibility module for legacy imports  \n‚úÖ **Migration Support**: Deprecation warnings with clear migration guidance  \n\n## Project Structure\n\nThis project is organized according to the Agentic Coding paradigm:\n\n1. **Requirements** (Human-led):\n   - Search and summarize clinical trials related to specific genetic mutations\n   - Provide mutation information as contextual resources\n   - Integrate seamlessly with Claude Desktop\n\n2. **Flow Design** (Collaborative):\n   - User queries Claude Desktop about a genetic mutation\n   - Claude calls our MCP server tool\n   - Server queries clinicaltrials.gov API\n   - Server processes and summarizes the results\n   - Server returns formatted results to Claude\n\n3. **Utilities** (Collaborative):\n   - `clinicaltrials/query.py`: Handles API calls to clinicaltrials.gov\n   - `utils/call_llm.py`: Utilities for working with Claude\n\n4. **Node Design** (AI-led):\n   - `utils/node.py`: Implements base Node and BatchNode classes with prep/exec/post pattern\n   - `clinicaltrials/nodes.py`: Defines specialized nodes for querying and summarizing\n   - `clinicaltrials_mcp_server.py`: Orchestrates the flow execution\n\n5. **Implementation** (AI-led):\n   - FastMCP SDK for handling the protocol details\n   - Error handling at all levels\n   - Resources for common mutations\n\n## Architecture Components\n\n### Unified MCP Server (`servers/main.py`)\n\nThe main unified server implementing the Model Context Protocol with **runtime mode selection**:\n\n- **Unified Architecture**: Single implementation supporting both sync and async modes\n- **Runtime Mode Selection**: Automatic detection via event loop or explicit `MCP_ASYNC_MODE` configuration\n- **Enterprise Tools**: Health monitoring, metrics collection, cache management (mode-dependent)\n- **Auto-scaling**: Circuit breakers and retry logic for robust API communication\n- **Cache Warming**: Automatically pre-loads common mutations for instant responses (async mode)\n- **API Resilience**: Handles 403 Forbidden errors with unified HTTP client fallback mechanisms\n- **Backward Compatibility**: Legacy servers redirect with deprecation warnings\n\n### Unified Service Layer\n\n**Clinical Trials Service** (`clinicaltrials/service.py`): Unified API client with mode-aware processing\n- **Dual Mode Support**: Same interface for both sync (`query_trials`) and async (`aquery_trials`) calls\n- **Circuit Breaker Integration**: Automatic failure detection and recovery\n- **Distributed Caching**: Redis-backed caching with in-memory fallback\n- **Metrics Collection**: Detailed performance and usage analytics\n- **API Compatibility**: Uses unified HTTP client for reliable clinicaltrials.gov API access\n\n**LLM Service** (`utils/llm_service.py`): Unified LLM interaction client\n- **Mode-Aware Processing**: Supports both sync and async LLM calls\n- **Retry Logic**: Built-in retry mechanisms with exponential backoff\n- **Error Handling**: Comprehensive error handling with structured logging\n\n### Unified Nodes (`clinicaltrials/unified_nodes.py`)\n\nPocketFlow nodes with **automatic sync/async execution**:\n- **QueryTrialsNode**: Unified node with mode detection for API requests\n- **SummarizeTrialsNode**: Unified LLM-powered summarization with retry logic\n- **BatchQueryTrialsNode**: Batch processing with concurrency control (async) or sequential processing (sync)\n- **Auto-Detection**: Nodes automatically determine execution mode at runtime\n\n### Unified Foundation Layer\n\n- **Unified HTTP Client** (`utils/http_client.py`): Single HTTP client supporting both sync and async with connection pooling\n- **Unified Node Framework** (`utils/unified_node.py`): Base classes with automatic mode detection\n- **Shared Utilities** (`utils/shared.py`): Common validation, error handling, and metrics functions\n- **Cache Strategies** (`utils/cache_strategies.py`): Smart cache warming and invalidation (async mode)\n- **Configuration System** (`servers/config.py`): Centralized configuration with environment overrides\n- **Legacy Compatibility** (`servers/legacy_compat.py`): Backward compatibility layer with migration guidance\n\n## Unified Node Pattern Implementation\n\nThis project implements the **enhanced PocketFlow Node pattern** with unified sync/async execution, providing a modular, maintainable approach to building AI workflows:\n\n### Unified Core Node Classes (`utils/unified_node.py`)\n\n- **UnifiedNode**: Base class supporting both sync and async execution with automatic mode detection\n- **UnifiedBatchNode**: Extension for batch processing with concurrency control (async) or sequential processing (sync)\n- **UnifiedFlow**: Orchestrates execution with intelligent mode selection\n\n### Unified Implementation Nodes (`clinicaltrials/unified_nodes.py`)\n\n1. **QueryTrialsNode** (Unified):\n   ```python\n   # Single implementation supporting both modes\n   def prep(self, shared): return shared[\"mutation\"]\n   \n   def exec(self, mutation): \n       return self.trials_service.query_trials(mutation)  # Sync version\n   \n   async def aexec(self, mutation): \n       return await self.trials_service.aquery_trials(mutation)  # Async version\n   \n   def post(self, shared, mutation, result):\n       shared[\"trials_data\"] = result\n       shared[\"studies\"] = result.get(\"studies\", [])\n       return self.get_next_node_id(result)\n   ```\n\n2. **SummarizeTrialsNode** (Unified):\n   ```python\n   # Unified summarization with mode detection\n   def prep(self, shared): return shared[\"studies\"]\n   \n   def exec(self, studies): \n       return self.llm_service.call_llm(prompt)  # Sync version\n   \n   async def aexec(self, studies): \n       return await self.llm_service.acall_llm(prompt)  # Async version\n   \n   def post(self, shared, studies, summary):\n       shared[\"summary\"] = summary\n       return None  # End of flow\n   ```\n\n### Unified Flow Execution\n\nThe unified MCP server creates and runs flows with automatic mode detection:\n\n```python\n# Create unified nodes (mode determined at runtime)\nquery_node = QueryTrialsNode(async_mode=server.async_mode)\nsummarize_node = SummarizeTrialsNode(async_mode=server.async_mode)\n\n# Use PocketFlow chaining syntax\nquery_node >> summarize_node\n\n# Create unified flow\nflow = UnifiedFlow(start_node=query_node, async_mode=server.async_mode)\n\n# Run flow with shared context (automatically sync or async)\nshared = {\"mutation\": mutation}\nif server.async_mode:\n    result = await flow.aexecute(shared)\nelse:\n    result = flow.execute(shared)\n```\n\n### Key Advantages of Unified Pattern\n\n‚úÖ **Single Implementation**: One codebase supports both sync and async execution  \n‚úÖ **Auto-Detection**: Nodes automatically determine optimal execution mode  \n‚úÖ **Runtime Selection**: Mode can be selected at server startup or runtime  \n‚úÖ **Preserved Interface**: Same `prep`, `exec`, `post` pattern maintained  \n‚úÖ **Performance Optimization**: Mode-specific optimizations (timeouts, concurrency, batch limits)  \n‚úÖ **Backward Compatibility**: Legacy node patterns continue working with deprecation warnings  \n\nThis unified pattern eliminates code duplication while preserving the modular, testable nature of the original PocketFlow design. For more details, see the [design document](docs/design.md).\n\n## Usage\n\n1. Install dependencies with uv:\n   ```bash\n   uv sync\n   ```\n\n2. Configure Claude Desktop to use the **unified server**:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mutation-clinical-trials-mcp\": {\n         \"command\": \"uv\",\n         \"args\": [\"run\", \"python\", \"servers/main.py\"],\n         \"description\": \"Unified clinical trials matching server with runtime mode selection\"\n       }\n     }\n   }\n   ```\n\n3. **Optional**: Configure execution mode via environment variables:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mutation-clinical-trials-mcp\": {\n         \"command\": \"uv\",\n         \"args\": [\"run\", \"python\", \"servers/main.py\"],\n         \"env\": {\n           \"MCP_ASYNC_MODE\": \"true\"\n         },\n         \"description\": \"Unified server in explicit async mode\"\n       }\n     }\n   }\n   ```\n\n3. Start Claude Desktop and ask questions like:\n   - \"What clinical trials are available for EGFR L858R mutations?\"\n   - \"Are there any trials for BRAF V600E mutations?\"  \n   - \"Tell me about trials for ALK rearrangements\"\n   - \"Search for multiple mutations: EGFR L858R,BRAF V600E,KRAS G12C\"\n\n4. Use enterprise monitoring tools:\n   - \"Get the server health status\"\n   - \"Show me the cache performance report\"\n   - \"What are the current metrics?\"\n\n---\n\n## Integrating with Claude Desktop \n\nYou can configure this project as a Claude Desktop MCP tool. Use path placeholders in your configuration, and substitute them with your actual paths:\n\n### Recommended Configuration (Unified Server)\n\n```json\n\"mutation-clinical-trials-mcp\": {\n  \"command\": \"{PATH_TO_VENV}/bin/python\",\n  \"args\": [\n    \"{PATH_TO_PROJECT}/servers/main.py\"\n  ],\n  \"description\": \"Unified clinical trials matching server with automatic mode selection.\"\n}\n```\n\n### Legacy Compatibility (Still Supported)\n\n```json\n\"mutation-clinical-trials-mcp-legacy\": {\n  \"command\": \"{PATH_TO_VENV}/bin/python\",\n  \"args\": [\n    \"{PATH_TO_PROJECT}/servers/primary.py\"\n  ],\n  \"description\": \"Legacy async server (redirects to unified server with deprecation warnings).\"\n}\n```\n\n**Path Variables:**\n- `{PATH_TO_VENV}`: Full path to your virtual environment directory.\n- `{PATH_TO_PROJECT}`: Full path to the directory containing your project files.\n\n**Installation Instructions:**\n1. Clone the repository to your local machine.\n2. Install uv if you don't have it already:\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh    # macOS/Linux\n   # or\n   iwr -useb https://astral.sh/uv/install.ps1 | iex    # Windows PowerShell\n   ```\n3. Create a virtual environment and install dependencies in one step:\n   ```bash\n   uv sync\n   ```\n4. Activate the virtual environment when needed:\n   ```bash\n   source .venv/bin/activate    # macOS/Linux\n   .venv\\Scripts\\activate       # Windows\n   ```\n5. Determine the full path to your virtual environment and project directory.\n6. Update your configuration with these specific paths.\n\n**Examples:**\n- On macOS/Linux:\n  ```json\n  \"command\": \"/Users/username/projects/mutation_trial_matcher/.venv/bin/python\"\n  ```\n- On Windows:\n  ```json\n  \"command\": \"C:\\\\Users\\\\username\\\\projects\\\\mutation_trial_matcher\\\\.venv\\\\Scripts\\\\python.exe\"\n  ```\n\n**Path Finding Tips:**\n- To find the exact path to your Python interpreter in the virtual environment, run:\n  - `which python` (macOS/Linux)\n  - `where python` (Windows, after activating the venv)\n- For the project path, use the full path to the directory containing `servers/primary.py`.\n\n---\n\n## Future Improvements\n\nFor a comprehensive list of planned enhancements and future work, please see the [future_work.md](docs/future_work.md) document.\n\n\n## Dependencies\n\nThis project relies on the following key dependencies:\n\n- **Python 3.11+** - Base runtime environment (lowered from 3.13+ for broader compatibility)\n- **FastMCP** (`fastmcp>=2.10.2`) - High-performance async MCP framework\n- **PocketFlow** (`pocketflow>=0.0.1`) - Framework for building modular AI workflows with the Node pattern  \n- **Requests** (`requests==2.31.0`) - HTTP library for clinicaltrials.gov API calls (dev dependency for legacy test compatibility)\n- **HTTPX** (`httpx>=0.28.1`) - Async HTTP client for direct Anthropic API calls\n- **Redis** (`redis>=6.2.0`) - Optional distributed caching backend\n- **Python-dotenv** (`python-dotenv==1.1.0`) - Environment variable management\n\n**Enterprise Features:**\n- Prometheus metrics collection and monitoring\n- Circuit breaker patterns for fault tolerance\n- Distributed caching with Redis backend\n- Cache warming strategies for performance optimization\n\nAll dependencies can be installed using `uv sync` as described in the installation instructions.\n\n## Troubleshooting\n\nIf Claude Desktop disconnects from the MCP server:\n- Check logs at: `~/Library/Logs/Claude/mcp-server-mutation-clinical-trials-mcp.log`\n- Restart Claude Desktop  \n- Verify the server is running correctly with `uv run python servers/main.py`\n- Check for deprecation warnings if using legacy servers (`servers/primary.py` or `servers/legacy/sync_server.py`)\n\n**Redis Connection Warnings:**\n- Redis connection errors are expected if Redis is not installed - the server uses in-memory caching as fallback\n- To eliminate warnings: `brew install redis && brew services start redis`\n- The server works perfectly without Redis, just with reduced caching performance\n\n**Cache Warming on Startup:**\n- Server automatically queries 15 common mutations on startup for performance optimization\n- This is normal behavior and improves response times for frequent queries\n- To disable: comment out `asyncio.run(startup_tasks())` in `servers/primary.py`\n\n## Development History\n\nThis project evolved through multiple phases of AI-collaborative development:\n\n**Phase 1** (2024-04-30): Initial prototype using synchronous architecture  \n**Phase 2** (2024-12): Enhanced with comprehensive testing and documentation  \n**Phase 3** (2025-01): Major refactoring for improved organization and maintainability  \n**Phase 4** (2025-01): Full async migration with enterprise features and 80% performance improvement  \n**Phase 5** (2025-07): API resilience improvements and 403 error resolution  \n**Phase 6** (2025-07): **Code Deduplication Project** - Comprehensive 4-phase unification effort  \n**Phase 7** (2025-07): **Repository Quality Excellence** - Professional code standards and legacy cleanup\n\n### Recent Achievements (July 2025)\n\n**Code Deduplication Project**:  \n**Phase 1**: Foundation Layer - Unified HTTP client and shared utilities  \n**Phase 2**: Service Layer Consolidation - Unified LLM and Clinical Trials services  \n**Phase 3**: Node Layer Unification - Enhanced UnifiedNode framework  \n**Phase 4**: Server Consolidation - Complete unified architecture  \n\n**Repository Quality Excellence**:  \n- **Code Quality**: 99.6% improvement (1,695 of 1,702 linting errors fixed)  \n- **Type Safety**: 69% reduction in type diagnostics (48 ‚Üí 15)  \n- **Professional Cleanup**: 3,435 lines of deprecated code removed  \n- **Compatibility Layer**: Complete backward compatibility with `utils/node.py` compatibility module  \n- **Python Compatibility**: Lowered requirement from Python 3.13+ to 3.11+ for broader adoption\n\n**Results**: 60% code reduction (~1,000 lines eliminated), zero breaking changes, unified sync/async architecture, production-ready code quality\n\n**Current Version (v0.2.1)**: Production-ready unified server with enterprise features, automatic mode selection, professional type safety, and comprehensive backward compatibility. Developed through collaboration with Claude Code, leveraging 20+ years of cancer research domain expertise to guide AI implementation and architectural transformation.\n\n\n## Contributing\n\nWe welcome contributions to improve the Mutation Clinical Trial Matching MCP! Here's how you can get involved:\n\n### Development Setup\n\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/pickleton89/mutation-clinical-trial-matching-mcp.git\n   cd mutation-clinical-trial-matching-mcp\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   uv sync\n   ```\n\n3. **Run tests**:\n   ```bash\n   uv run python -m unittest discover tests/\n   ```\n\n### Contribution Guidelines\n\n- **Follow the PocketFlow Node pattern** for new features\n- **Add comprehensive tests** for any new functionality\n- **Update documentation** including relevant docstrings and README sections\n- **Follow Python best practices** and maintain type hints\n- **Run linting and type checking** before submitting PRs\n\n### Areas for Contribution\n\n- **Performance optimizations** for large-scale clinical trial searches\n- **Additional mutation formats** and standardization\n- **Enhanced summarization capabilities** with more detailed filtering\n- **Integration with other clinical databases** beyond ClinicalTrials.gov\n- **UI/UX improvements** for the Claude Desktop integration\n\n### Reporting Issues\n\nPlease use the [GitHub Issues](https://github.com/pickleton89/mutation-clinical-trial-matching-mcp/issues) page to report bugs or request features.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgements\n\nThis project was built using the [PocketFlow-Template-Python](https://github.com/The-Pocket/PocketFlow-Template-Python) as a starting point. Special thanks to the original contributors of that project for providing the foundation and structure that made this implementation possible.\n\nThe project follows the Agentic Coding methodology as outlined in the original template.\n\n---\n‚ö†Ô∏è **Disclaimer**\n\nThis project is a prototype and is intended for research and demonstration purposes only. It should not be used to make medical decisions or as a substitute for professional medical advice, diagnosis, or treatment. Due to the limitations of large language models (LLMs), the information provided by this tool may be incomplete, inaccurate, or outdated. Users should exercise caution and consult qualified healthcare professionals before making any decisions based on the outputs of this system.\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "r-huijts--firstcycling-mcp": {
      "owner": "r-huijts",
      "name": "firstcycling-mcp",
      "url": "https://github.com/r-huijts/firstcycling-mcp",
      "imageUrl": "https://github.com/r-huijts.png",
      "description": "Retrieve comprehensive professional cycling data including detailed statistics on riders, race results, and historical performance. Analyze rider performance and explore information about cycling teams and competitions.",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-26T08:44:25Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/r-huijts-firstcycling-mcp-badge.png)](https://mseep.ai/app/r-huijts-firstcycling-mcp)\n\n# FirstCycling MCP Server\n\nThis is a Model Context Protocol (MCP) server that provides professional cycling data from FirstCycling. It allows you to retrieve comprehensive information about professional cyclists, race results, race details, and historical cycling data.\n\n## Features\n\nThis MCP server offers rich access to professional cycling data, providing tools for:\n\n- Finding information about professional cyclists\n- Retrieving race results and details\n- Exploring historical race data\n- Analyzing rider performance and career progression\n- Accessing information about cycling teams and competitions\n\n## Real-World Use Cases\n\nWith this MCP server, you can use Claude to:\n\n### Rider Analysis\n\n- **Performance Tracking**: \"How has Tadej Pogaƒçar performed in the Tour de France over the years?\"\n- **Career Progression**: \"Show me the team history and career progression of Wout van Aert.\"\n- **Specialization Analysis**: \"What are Mathieu van der Poel's results in Monument classics?\"\n- **Victory Analysis**: \"List all WorldTour victories for Jonas Vingegaard.\"\n- **Historical Comparison**: \"Compare the Grand Tour results of Primo≈æ Rogliƒç and Jonas Vingegaard.\"\n\n### Race Research\n\n- **Recent Results**: \"Show me the results of the 2023 Paris-Roubaix.\"\n- **Historical Context**: \"Who are the youngest and oldest winners of the Tour of Flanders?\"\n- **Team Analysis**: \"Get the startlist for the 2023 Tour de France with detailed team information.\"\n- **Race Statistics**: \"Show me the victory table for Li√®ge-Bastogne-Li√®ge. Who has won it the most times?\"\n- **Stage Information**: \"Can you show me the stage profiles for the 2023 Giro d'Italia?\"\n\n### Sports Journalism\n\n- \"Create a detailed profile of Remco Evenepoel for a cycling magazine article.\"\n- \"Write a preview for the upcoming Tour de France based on the recent results of top contenders like Tadej Pogaƒçar and Jonas Vingegaard.\"\n- \"Analyze the evolution of Tom Pidcock's career based on his race results and team history.\"\n\n### Cycling Education\n\n- \"Explain what makes the Monument classics special using data about their history and winners.\"\n- \"Create an educational summary about Grand Tours and their significance in professional cycling.\"\n- \"Describe the typical career progression of a professional cyclist using examples from the data.\"\n\n## Requirements\n\n- Python 3.10 or higher\n- `uv` package manager (recommended)\n- Dependencies as listed in `pyproject.toml`, including:\n  - mcp\n  - beautifulsoup4\n  - lxml\n  - pandas\n  - slumber\n  - and other packages for web scraping and data processing\n\n## Setup\n\n1. Clone this repository\n2. Create and activate a virtual environment:\n   ```\n   uv venv\n   source .venv/bin/activate  # On macOS/Linux\n   # or\n   .venv\\Scripts\\activate  # On Windows\n   ```\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n\n## FirstCycling API\n\nThis server uses the [FirstCycling API](https://github.com/baronet2/FirstCyclingAPI), which has been integrated directly into the project. The API provides methods to fetch data from the FirstCycling website through web scraping.\n\n## MCP Tools\n\nThe server exposes the following tools through the Model Context Protocol:\n\n### Rider Information\n\n| Tool | Description |\n|------|-------------|\n| `get_rider_info` | Get basic biographical information about a rider including nationality, birthdate, weight, height, and current team |\n| `get_rider_best_results` | Retrieve a rider's best career results, sorted by importance |\n| `get_rider_grand_tour_results` | Get a rider's results in Grand Tours (Tour de France, Giro d'Italia, Vuelta a Espa√±a) |\n| `get_rider_monument_results` | Retrieve a rider's results in cycling's Monument classics |\n| `get_rider_team_and_ranking` | Get a rider's team history and UCI ranking evolution over time |\n| `get_rider_race_history` | Retrieve a rider's complete race participation history, optionally filtered by year |\n| `get_rider_one_day_races` | Get a rider's results in one-day races, optionally filtered by year |\n| `get_rider_stage_races` | Get a rider's results in multi-day stage races, optionally filtered by year |\n| `get_rider_teams` | Retrieve the complete team history of a rider throughout their career |\n| `get_rider_victories` | Get a list of a rider's career victories, with optional filters for WorldTour or UCI races |\n\n### Race Information\n\n| Tool | Description |\n|------|-------------|\n| `get_race_results` | Retrieve results for a specific race edition by race ID and year |\n| `get_race_overview` | Get general information about a race including history, records, and past winners |\n| `get_race_stage_profiles` | Retrieve stage profiles and details for multi-stage races |\n| `get_race_startlist` | Get the startlist for a specific race edition with detailed or basic team information |\n| `get_race_victory_table` | Retrieve the all-time victory table for a race showing riders with most wins |\n| `get_race_year_by_year` | Get year-by-year results for a race with optional classification filter |\n| `get_race_youngest_oldest_winners` | Retrieve information about the youngest and oldest winners of a race |\n| `get_race_stage_victories` | Get information about stage victories in multi-stage races |\n\n### Search Tools\n\n| Tool | Description |\n|------|-------------|\n| `search_rider` | Search for riders by name, returning their IDs and basic information |\n| `search_race` | Search for races by name, returning their IDs and basic information |\n\n## Usage\n\n### Development Mode\n\nYou can test the server with MCP Inspector by running:\n\n```\nuv run mcp dev firstcycling.py\n```\n\nThis will start the server and open the MCP Inspector in your browser, allowing you to test the available tools.\n\n### Integration with Claude for Desktop\n\nTo integrate this server with Claude for Desktop:\n\n1. Edit the Claude for Desktop config file, located at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server to your configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firstcycling\": {\n         \"command\": \"uv\",\n         \"args\": [\"--directory\", \"/path/to/server/directory\", \"run\", \"firstcycling.py\"]\n       }\n     }\n   }\n   ```\n\n3. Restart Claude for Desktop\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "r-huijts--strava-mcp": {
      "owner": "r-huijts",
      "name": "strava-mcp",
      "url": "https://github.com/r-huijts/strava-mcp",
      "imageUrl": "https://github.com/r-huijts.png",
      "description": "Access and manage Strava data, including recent activities, profile stats, and detailed activity streams. Supports exploration of segments, management of routes, and exporting route files in GPX or TCX format.",
      "stars": 153,
      "forks": 18,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T07:53:54Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/r-huijts-strava-mcp-badge.png)](https://mseep.ai/app/r-huijts-strava-mcp)\n\n# Strava MCP Server\n\nThis project implements a Model Context Protocol (MCP) server in TypeScript that acts as a bridge to the Strava API. It exposes Strava data and functionalities as \"tools\" that Large Language Models (LLMs) can utilize through the MCP standard.\n\n<a href=\"https://glama.ai/mcp/servers/@r-huijts/strava-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@r-huijts/strava-mcp/badge\" alt=\"Strava Server MCP server\" />\n</a>\n\n## Features\n\n- üèÉ Access recent activities, profile, and stats.\n- üìä Fetch detailed activity streams (power, heart rate, cadence, etc.).\n- üó∫Ô∏è Explore, view, star, and manage segments.\n- ‚è±Ô∏è View detailed activity and segment effort information.\n- üìç List and view details of saved routes.\n- üíæ Export routes in GPX or TCX format to the local filesystem.\n- ü§ñ AI-friendly JSON responses via MCP.\n- üîß Uses Strava API V3.\n\n## Natural Language Interaction Examples\n\nAsk your AI assistant questions like these to interact with your Strava data:\n\n**Recent Activity & Profile:**\n* \"Show me my recent Strava activities.\"\n* \"What were my last 3 rides?\"\n* \"Get my Strava profile information.\"\n* \"What's my Strava username?\"\n\n**Activity Streams & Data:**\n* \"Get the heart rate data from my morning run yesterday.\"\n* \"Show me the power data from my last ride.\"\n* \"What was my cadence profile for my weekend century ride?\"\n* \"Get all stream data for my Thursday evening workout.\"\n* \"Show me the elevation profile for my Mt. Diablo climb.\"\n\n**Stats:**\n* \"What are my running stats for this year on Strava?\"\n* \"How far have I cycled in total?\"\n* \"Show me my all-time swim totals.\"\n\n**Specific Activities:**\n* \"Give me the details for my last run.\"\n* \"What was the average power for my interval training on Tuesday?\"\n* \"Did I use my Trek bike for my commute yesterday?\"\n\n**Clubs:**\n* \"What Strava clubs am I in?\"\n* \"List the clubs I've joined.\"\n\n**Segments:**\n* \"List the segments I starred near Boulder, Colorado.\"\n* \"Show my favorite segments.\"\n* \"Get details for the 'Alpe du Zwift' segment.\"\n* \"Are there any good running segments near Golden Gate Park?\"\n* \"Find challenging climbs near Boulders Flagstaff Mountain.\"\n* \"Star the 'Flagstaff Road Climb' segment for me.\"\n* \"Unstar the 'Lefthand Canyon' segment.\"\n\n**Segment Efforts:**\n* \"Show my efforts on the 'Sunshine Canyon' segment this month.\"\n* \"List my attempts on Box Hill between January and June this year.\"\n* \"Get the details for my personal record on Alpe d'Huez.\"\n\n**Routes:**\n* \"List my saved Strava routes.\"\n* \"Show the second page of my routes.\"\n* \"What is the elevation gain for my Boulder Loop route?\"\n* \"Get the description for my 'Boulder Loop' route.\"\n* \"Export my 'Boulder Loop' route as a GPX file.\"\n* \"Save my Sunday morning route as a TCX file.\"\n\n## Advanced Prompt Example\n\nHere's an example of a more advanced prompt to create a professional cycling coach analysis of your Strava activities:\n\n```\nYou are Tom Verhaegen, elite cycling coach and mentor to world champion Mathieu van der Poel. Analyze my most recent Strava activity. Provide a thorough, data-driven assessment of the ride, combining both quantitative insights and textual interpretation.\n\nBegin your report with a written summary that highlights key findings and context. Then, bring the raw numbers to life: build an interactive, visually striking dashboard using HTML, CSS, and JavaScript. Use bold, high-contrast colors and intuitive, insightful chart types that best suit each metric (e.g., heart rate, power, cadence, elevation).\n\nEmbed clear coaching feedback and personalized training recommendations directly within the visualization. These should be practical, actionable, and grounded solely in the data provided‚Äîno assumptions or fabrications.\n\nAs a bonus, sprinkle in motivational quotes and cheeky commentary from Mathieu van der Poel himself‚Äîhe's been watching my rides with one eyebrow raised and a smirk of both concern and amusement.\n\nGoal: Deliver a professional-grade performance analysis that looks and feels like it came straight from the inner circle of world-class cycling.\n```\n\nThis prompt creates a personalized analysis of your most recent Strava activity, complete with professional coaching feedback and a custom visualization dashboard.\n\n## ‚ö†Ô∏è Important Setup Sequence\n\nFor successful integration with Claude, follow these steps in exact order:\n\n1. Install the server and its dependencies\n2. Configure the server in Claude's configuration\n3. Complete the Strava authentication flow\n4. Restart Claude to ensure proper environment variable loading\n\nSkipping steps or performing them out of order may result in environment variables not being properly read by Claude.\n\n## Installation & Setup\n\n1. **Prerequisites:**\n   - Node.js (v18 or later recommended)\n   - npm (usually comes with Node.js)\n   - A Strava Account\n\n### 1. From Source\n\n1. **Clone Repository:**\n   ```bash\n   git clone https://github.com/r-huijts/strava-mcp.git\n   cd strava-mcp\n   ```\n\n2. **Install Dependencies:**\n   ```bash\n   npm install\n   ```\n3. **Build the Project:**\n   ```bash\n   npm run build\n   ```\n\n### 2. Configure Claude Desktop\n\nUpdate your Claude configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"strava-mcp-local\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/your/strava-mcp/dist/server.js\"\n      ]\n      // Environment variables are read from the .env file by the server\n    }\n  }\n}\n```\n\nMake sure to replace `/absolute/path/to/your/strava-mcp/` with the actual path to your installation.\n\n### 3. Strava Authentication Setup\n\nThe `setup-auth.ts` script makes it easy to set up authentication with the Strava API. Follow these steps carefully:\n\n#### Create a Strava API Application\n\n1. Go to [https://www.strava.com/settings/api](https://www.strava.com/settings/api)\n2. Create a new application:\n   - Enter your application details (name, website, description)\n   - Important: Set \"Authorization Callback Domain\" to `localhost`\n   - Note down your Client ID and Client Secret\n\n#### Run the Setup Script\n\n```bash\n# In your strava-mcp directory\nnpx tsx scripts/setup-auth.ts\n```\n\nFollow the prompts to complete the authentication flow (detailed instructions in the Authentication section below).\n\n### 4. Restart Claude\n\nAfter completing all the above steps, restart Claude Desktop for the changes to take effect. This ensures that:\n- The new configuration is loaded\n- The environment variables are properly read\n- The Strava MCP server is properly initialized\n\n## üîë Environment Variables\n\n| Variable | Description |\n|----------|-------------|\n| STRAVA_CLIENT_ID | Your Strava Application Client ID (required) |\n| STRAVA_CLIENT_SECRET | Your Strava Application Client Secret (required) |\n| STRAVA_ACCESS_TOKEN | Your Strava API access token (generated during setup) |\n| STRAVA_REFRESH_TOKEN | Your Strava API refresh token (generated during setup) |\n| ROUTE_EXPORT_PATH | Absolute path for saving exported route files (optional) |\n\n## Token Handling\n\nThis server implements automatic token refreshing. When the initial access token expires (typically after 6 hours), the server will automatically use the refresh token stored in `.env` to obtain a new access token and refresh token. These new tokens are then updated in both the running process and the `.env` file, ensuring continuous operation.\n\nYou only need to run the `scripts/setup-auth.ts` script once for the initial setup.\n\n## Configure Export Path (Optional)\n\nIf you intend to use the `export-route-gpx` or `export-route-tcx` tools, you need to specify a directory for saving exported files.\n\nEdit your `.env` file and add/update the `ROUTE_EXPORT_PATH` variable:\n```dotenv\n# Optional: Define an *absolute* path for saving exported route files (GPX/TCX)\n# Ensure this directory exists and the server process has write permissions.\n# Example: ROUTE_EXPORT_PATH=/Users/your_username/strava-exports\nROUTE_EXPORT_PATH=\n```\n\nReplace the placeholder with the **absolute path** to your desired export directory. Ensure the directory exists and the server has permission to write to it.\n\n## API Reference\n\nThe server exposes the following MCP tools:\n\n---\n\n### `get-recent-activities`\n\nFetches the authenticated user's recent activities.\n\n- **When to use:** When the user asks about their recent workouts, activities, runs, rides, etc.\n- **Parameters:**\n  - `perPage` (optional):\n    - Type: `number`\n    - Description: Number of activities to retrieve.\n    - Default: 30\n- **Output:** Formatted text list of recent activities (Name, ID, Distance, Date).\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `get-athlete-profile`\n\nFetches the profile information for the authenticated athlete.\n\n- **When to use:** When the user asks for their profile details, username, location, weight, premium status, etc.\n- **Parameters:** None\n- **Output:** Formatted text string with profile details.\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `get-athlete-stats`\n\nFetches activity statistics (recent, YTD, all-time) for the authenticated athlete.\n\n- **When to use:** When the user asks for their overall statistics, totals for runs/rides/swims, personal records (longest ride, biggest climb).\n- **Parameters:** None\n- **Output:** Formatted text summary of stats, respecting user's measurement preference.\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `get-activity-details`\n\nFetches detailed information about a specific activity using its ID.\n\n- **When to use:** When the user asks for details about a *specific* activity identified by its ID.\n- **Parameters:**\n  - `activityId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the activity.\n- **Output:** Formatted text string with detailed activity information (type, date, distance, time, speed, HR, power, gear, etc.), respecting user's measurement preference.\n- **Errors:** Missing/invalid token, Invalid `activityId`, Strava API errors.\n\n---\n\n### `list-athlete-clubs`\n\nLists the clubs the authenticated athlete is a member of.\n\n- **When to use:** When the user asks about the clubs they have joined.\n- **Parameters:** None\n- **Output:** Formatted text list of clubs (Name, ID, Sport, Members, Location).\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `list-starred-segments`\n\nLists the segments starred by the authenticated athlete.\n\n- **When to use:** When the user asks about their starred or favorite segments.\n- **Parameters:** None\n- **Output:** Formatted text list of starred segments (Name, ID, Type, Distance, Grade, Location).\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `get-segment`\n\nFetches detailed information about a specific segment using its ID.\n\n- **When to use:** When the user asks for details about a *specific* segment identified by its ID.\n- **Parameters:**\n  - `segmentId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the segment.\n- **Output:** Formatted text string with detailed segment information (distance, grade, elevation, location, stars, efforts, etc.), respecting user's measurement preference.\n- **Errors:** Missing/invalid token, Invalid `segmentId`, Strava API errors.\n\n---\n\n### `explore-segments`\n\nSearches for popular segments within a given geographical area (bounding box).\n\n- **When to use:** When the user wants to find or discover segments in a specific geographic area, optionally filtering by activity type or climb category.\n- **Parameters:**\n  - `bounds` (required):\n    - Type: `string`\n    - Description: Comma-separated: `south_west_lat,south_west_lng,north_east_lat,north_east_lng`.\n  - `activityType` (optional):\n    - Type: `string` (`\"running\"` or `\"riding\"`)\n    - Description: Filter by activity type.\n  - `minCat` (optional):\n    - Type: `number` (0-5)\n    - Description: Minimum climb category. Requires `activityType: 'riding'`.\n  - `maxCat` (optional):\n    - Type: `number` (0-5)\n    - Description: Maximum climb category. Requires `activityType: 'riding'`.\n- **Output:** Formatted text list of found segments (Name, ID, Climb Cat, Distance, Grade, Elevation).\n- **Errors:** Missing/invalid token, Invalid `bounds` format, Invalid filter combination, Strava API errors.\n\n---\n\n### `star-segment`\n\nStars or unstars a specific segment for the authenticated athlete.\n\n- **When to use:** When the user explicitly asks to star, favorite, unstar, or unfavorite a specific segment identified by its ID.\n- **Parameters:**\n  - `segmentId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the segment.\n  - `starred` (required):\n    - Type: `boolean`\n    - Description: `true` to star, `false` to unstar.\n- **Output:** Success message confirming the action and the segment's new starred status.\n- **Errors:** Missing/invalid token, Invalid `segmentId`, Strava API errors (e.g., segment not found, rate limit).\n\n- **Notes:**\n  - Requires `profile:write` scope for star-ing and unstar-ing segments\n\n---\n\n### `get-segment-effort`\n\nFetches detailed information about a specific segment effort using its ID.\n\n- **When to use:** When the user asks for details about a *specific* segment effort identified by its ID.\n- **Parameters:**\n  - `effortId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the segment effort.\n- **Output:** Formatted text string with detailed effort information (segment name, activity ID, time, distance, HR, power, rank, etc.).\n- **Errors:** Missing/invalid token, Invalid `effortId`, Strava API errors.\n\n---\n\n### `list-segment-efforts`\n\nLists the authenticated athlete's efforts on a given segment, optionally filtered by date.\n\n- **When to use:** When the user asks to list their efforts or attempts on a specific segment, possibly within a date range.\n- **Parameters:**\n  - `segmentId` (required):\n    - Type: `number`\n    - Description: The ID of the segment.\n  - `startDateLocal` (optional):\n    - Type: `string` (ISO 8601 format)\n    - Description: Filter efforts starting after this date-time.\n  - `endDateLocal` (optional):\n    - Type: `string` (ISO 8601 format)\n    - Description: Filter efforts ending before this date-time.\n  - `perPage` (optional):\n    - Type: `number`\n    - Description: Number of results per page.\n    - Default: 30\n- **Output:** Formatted text list of matching segment efforts.\n- **Errors:** Missing/invalid token, Invalid `segmentId`, Invalid date format, Strava API errors.\n\n---\n\n### `list-athlete-routes`\n\nLists the routes created by the authenticated athlete.\n\n- **When to use:** When the user asks to see the routes they have created or saved.\n- **Parameters:**\n  - `page` (optional):\n    - Type: `number`\n    - Description: Page number for pagination.\n  - `perPage` (optional):\n    - Type: `number`\n    - Description: Number of routes per page.\n    - Default: 30\n- **Output:** Formatted text list of routes (Name, ID, Type, Distance, Elevation, Date).\n- **Errors:** Missing/invalid token, Strava API errors.\n\n---\n\n### `get-route`\n\nFetches detailed information for a specific route using its ID.\n\n- **When to use:** When the user asks for details about a *specific* route identified by its ID.\n- **Parameters:**\n  - `routeId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the route.\n- **Output:** Formatted text string with route details (Name, ID, Type, Distance, Elevation, Est. Time, Description, Segment Count).\n- **Errors:** Missing/invalid token, Invalid `routeId`, Strava API errors.\n\n---\n\n### `export-route-gpx`\n\nExports a specific route in GPX format and saves it locally.\n\n- **When to use:** When the user explicitly asks to export or save a specific route as a GPX file.\n- **Prerequisite:** The `ROUTE_EXPORT_PATH` environment variable must be correctly configured on the server.\n- **Parameters:**\n  - `routeId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the route.\n- **Output:** Success message indicating the save location, or an error message.\n- **Errors:** Missing/invalid token, Missing/invalid `ROUTE_EXPORT_PATH`, File system errors (permissions, disk space), Invalid `routeId`, Strava API errors.\n\n---\n\n### `export-route-tcx`\n\nExports a specific route in TCX format and saves it locally.\n\n- **When to use:** When the user explicitly asks to export or save a specific route as a TCX file.\n- **Prerequisite:** The `ROUTE_EXPORT_PATH` environment variable must be correctly configured on the server.\n- **Parameters:**\n  - `routeId` (required):\n    - Type: `number`\n    - Description: The unique identifier of the route.\n- **Output:** Success message indicating the save location, or an error message.\n- **Errors:** Missing/invalid token, Missing/invalid `ROUTE_EXPORT_PATH`, File system errors (permissions, disk space), Invalid `routeId`, Strava API errors.\n\n---\n\n### `get-activity-streams`\n\nRetrieves detailed time-series data streams from a Strava activity, perfect for analyzing workout metrics, visualizing routes, or performing detailed activity analysis.\n\n- **When to use:** When you need detailed time-series data from an activity for:\n  - Analyzing workout intensity through heart rate zones\n  - Calculating power metrics for cycling activities\n  - Visualizing route data using GPS coordinates\n  - Analyzing pace and elevation changes\n  - Detailed segment analysis\n\n- **Parameters:**\n  - `id` (required):\n    - Type: `number | string`\n    - Description: The Strava activity identifier to fetch streams for\n  - `types` (optional):\n    - Type: `array`\n    - Default: `['time', 'distance', 'heartrate', 'cadence', 'watts']`\n    - Available types:\n      - `time`: Time in seconds from start\n      - `distance`: Distance in meters from start\n      - `latlng`: Array of [latitude, longitude] pairs\n      - `altitude`: Elevation in meters\n      - `velocity_smooth`: Smoothed speed in meters/second\n      - `heartrate`: Heart rate in beats per minute\n      - `cadence`: Cadence in revolutions per minute\n      - `watts`: Power output in watts\n      - `temp`: Temperature in Celsius\n      - `moving`: Boolean indicating if moving\n      - `grade_smooth`: Road grade as percentage\n  - `resolution` (optional):\n    - Type: `string`\n    - Values: `'low'` (~100 points), `'medium'` (~1000 points), `'high'` (~10000 points)\n    - Description: Data resolution/density\n  - `series_type` (optional):\n    - Type: `string`\n    - Values: `'time'` or `'distance'`\n    - Default: `'distance'`\n    - Description: Base series type for data point indexing\n  - `page` (optional):\n    - Type: `number`\n    - Default: 1\n    - Description: Page number for paginated results\n  - `points_per_page` (optional):\n    - Type: `number`\n    - Default: 100\n    - Special value: `-1` returns ALL data points split into multiple messages\n    - Description: Number of data points per page\n\n- **Output Format:**\n  1. Metadata:\n     - Available stream types\n     - Total data points\n     - Resolution and series type\n     - Pagination info (current page, total pages)\n  2. Statistics (where applicable):\n     - Heart rate: max, min, average\n     - Power: max, average, normalized power\n     - Speed: max and average in km/h\n  3. Stream Data:\n     - Formatted time-series data for each requested stream\n     - Human-readable formats (e.g., formatted time, km/h for speed)\n     - Consistent numeric precision\n     - Labeled data points\n\n- **Example Request:**\n  ```json\n  {\n    \"id\": 12345678,\n    \"types\": [\"time\", \"heartrate\", \"watts\", \"velocity_smooth\", \"cadence\"],\n    \"resolution\": \"high\",\n    \"points_per_page\": 100,\n    \"page\": 1\n  }\n  ```\n\n- **Special Features:**\n  - Smart pagination for large datasets\n  - Complete data retrieval mode (points_per_page = -1)\n  - Rich statistics and metadata\n  - Formatted output for both human and LLM consumption\n  - Automatic unit conversions\n\n- **Notes:**\n  - Requires activity:read scope\n  - Not all streams are available for all activities\n  - Older activities might have limited data\n  - Large activities are automatically paginated\n  - Stream availability depends on recording device and activity type\n\n- **Errors:**\n  - Missing/invalid token\n  - Invalid activity ID\n  - Insufficient permissions\n  - Unavailable stream types\n  - Invalid pagination parameters\n\n---\n\n### `get-activity-laps`\n\nRetrieves the laps recorded for a specific Strava activity.\n\n- **When to use:**\n  - Analyze performance variations across different segments (laps) of an activity.\n  - Compare lap times, speeds, heart rates, or power outputs.\n  - Understand how an activity was structured (e.g., interval training).\n\n- **Parameters:**\n  - `id` (required):\n    - Type: `number | string`\n    - Description: The unique identifier of the Strava activity.\n\n- **Output Format:**\n  A text summary detailing each lap, including:\n  - Lap Index\n  - Lap Name (if available)\n  - Elapsed Time (formatted as HH:MM:SS)\n  - Moving Time (formatted as HH:MM:SS)\n  - Distance (in km)\n  - Average Speed (in km/h)\n  - Max Speed (in km/h)\n  - Total Elevation Gain (in meters)\n  - Average Heart Rate (if available, in bpm)\n  - Max Heart Rate (if available, in bpm)\n  - Average Cadence (if available, in rpm)\n  - Average Watts (if available, in W)\n\n- **Example Request:**\n  ```json\n  {\n    \"id\": 1234567890\n  }\n  ```\n\n- **Example Response Snippet:**\n  ```text\n  Activity Laps Summary (ID: 1234567890):\n\n  Lap 1: Warmup Lap\n    Time: 15:02 (Moving: 14:35)\n    Distance: 5.01 km\n    Avg Speed: 20.82 km/h\n    Max Speed: 35.50 km/h\n    Elevation Gain: 50.2 m\n    Avg HR: 135.5 bpm\n    Max HR: 150 bpm\n    Avg Cadence: 85.0 rpm\n\n  Lap 2: Interval 1\n    Time: 05:15 (Moving: 05:10)\n    Distance: 2.50 km\n    Avg Speed: 29.03 km/h\n    Max Speed: 42.10 km/h\n    Elevation Gain: 10.1 m\n    Avg HR: 168.2 bpm\n    Max HR: 175 bpm\n    Avg Cadence: 92.1 rpm\n    Avg Power: 280.5 W (Sensor)\n\n  ...\n  ```\n\n- **Notes:**\n  - Requires `activity:read` scope for public/followers activities, `activity:read_all` for private activities.\n  - Lap data availability depends on the recording device and activity type (e.g., manual activities may not have laps).\n\n- **Errors:**\n  - Missing/invalid token\n  - Invalid activity ID\n  - Insufficient permissions\n  - Activity not found\n\n---\n\n### `get-athlete-zones`\n\nRetrieves the authenticated athlete's configured heart rate and power zones.\n\n- **When to use:** When the user asks about their heart rate zones, power zones, or training zone settings.\n- **Parameters:** None\n- **Output Format:**\n  Returns two text blocks:\n  1.  A **formatted summary** detailing configured zones:\n      - Heart Rate Zones: Custom status, Zone ranges, Time Distribution (if available)\n      - Power Zones: Zone ranges, Time Distribution (if available)\n  2.  The **complete raw JSON data** as returned by the Strava API.\n- **Example Response Snippet (Summary):**\n  ```text\n  **Athlete Zones:**\n\n  ‚ù§Ô∏è **Heart Rate Zones**\n     Custom Zones: No\n     Zone 1: 0 - 115 bpm\n     Zone 2: 115 - 145 bpm\n     Zone 3: 145 - 165 bpm\n     Zone 4: 165 - 180 bpm\n     Zone 5: 180+ bpm\n\n  ‚ö° **Power Zones**\n     Zone 1: 0 - 150 W\n     Zone 2: 151 - 210 W\n     Zone 3: 211 - 250 W\n     Zone 4: 251 - 300 W\n     Zone 5: 301 - 350 W\n     Zone 6: 351 - 420 W\n     Zone 7: 421+ W\n     Time Distribution:\n       - 0-50: 0:24:58\n       - 50-100: 0:01:02\n       ...\n       - 450-‚àû: 0:05:43\n  ```\n- **Notes:**\n  - Requires `profile:read_all` scope.\n  - Zones might not be configured for all athletes.\n- **Errors:**\n  - Missing/invalid token\n  - Insufficient permissions (Missing `profile:read_all` scope - 403 error)\n  - Subscription Required (Potentially, if Strava changes API access)\n\n---\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. (Assuming MIT, update if different)",
      "npm_url": "",
      "npm_downloads": 0
    },
    "rvibek--mcp_unhcr": {
      "owner": "rvibek",
      "name": "mcp_unhcr",
      "url": "https://github.com/rvibek/mcp_unhcr",
      "imageUrl": "https://github.com/rvibek.png",
      "description": "Access and query UNHCR refugee statistics, enabling insights into forcibly displaced population trends. Filter data by country of origin, country of asylum, and year, while retrieving Refugee Status Determination application and decision data.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-08T08:45:13Z",
      "readme_content": "# UNHCR Population Data MCP Server\n\nThis MCP (Model Context Protocol) server provides access to UNHCR data through a standardized interface. It allows AI agents to query data related to forcibly displaced persons, including population statistics, Refugee Status Determination (RSD) applications, and RSD decisions. The data can be filtered by country of origin, country of asylum, and year(s).\n\nThis server interacts with the [UNHCR Population Statistics APIs](https://api.unhcr.org/).\n\n## Features\n\n- Query forcibly displaced population data.\n- Query Refugee Status Determination (RSD) application data.\n- Query Refugee Status Determination (RSD) decision data.\n- Filter data by country of origin (ISO3 code), country of asylum (ISO3 code), and year(s).\n- Option to break down results by all countries of origin and countries of asylum\n\n## Connect to MCP Server\n\nTo access the server, open your web browser and visit the following URL:\n[https://smithery.ai/server/@rvibek/mcp_unhcr](https://smithery.ai/server/@rvibek/mcp_unhcr)\n\nConfigure the MCP host/client as needed.\n\n[![smithery badge](https://smithery.ai/badge/@rvibek/mcp_unhcr)](https://smithery.ai/server/@rvibek/mcp_unhcr)\n\n## API Endpoints and Query Parameters\n\nThe server fetches data from the following base URL: `https://api.unhcr.org/population/v1/` using these specific endpoints:\n- `population/`\n- `asylum-applications/`\n- `asylum-decisions/`\n\nKey query parameters used by the server when calling the UNHCR API:\n- `cf_type`: Always set to \"ISO\".\n- `coo`: Country of origin filter (ISO3 code, comma-separated for multiple).\n- `coa`: Country of asylum filter (ISO3 code, comma-separated for multiple).\n- `year[]`: Year(s) to filter by (e.g., \"2023\" or [\"2022\", \"2023\"]). Defaults to \"2024\" if not provided.\n- `coo_all`: Set to \"true\" if results should be broken down by all countries of origin.\n- `coa_all`: Set to \"true\" if results should be broken down by all countries of asylum.\n\n\n## MCP Tools\n\nThe server exposes the following tools:\n\n### `get_population_data`\n\nGet population data from UNHCR.\n\n**Parameters:**\n- `coo` (optional): Country of origin filter (ISO3 code, comma-separated for multiple).\n- `coa` (optional): Country of asylum filter (ISO3 code, comma-separated for multiple).\n- `year` (optional): Year filter (comma-separated for multiple years, or a single year). Defaults to 2024 if not provided.\n- `coo_all` (optional, boolean): If `True`, break down results by all countries of origin. Defaults to `False`.\n- `coa_all` (optional, boolean): If `True`, break down results by all countries of asylum. Defaults to `False`.\n\n### `get_rsd_applications`\n\nGet RSD application data from UNHCR.\n\n**Parameters:**\n- `coo` (optional): Country of origin filter (ISO3 code, comma-separated for multiple).\n- `coa` (optional): Country of asylum filter (ISO3 code, comma-separated for multiple).\n- `year` (optional): Year filter (comma-separated for multiple years, or a single year). Defaults to 2024 if not provided.\n- `coo_all` (optional, boolean): If `True`, break down results by all countries of origin. Defaults to `False`.\n- `coa_all` (optional, boolean): If `True`, break down results by all countries of asylum. Defaults to `False`.\n\n### `get_rsd_decisions`\n\nGet RSD decision data from UNHCR.\n\n**Parameters:**\n- `coo` (optional): Country of origin filter (ISO3 code, comma-separated for multiple).\n- `coa` (optional): Country of asylum filter (ISO3 code, comma-separated for multiple).\n- `year` (optional): Year filter (comma-separated for multiple years, or a single year). Defaults to 2024 if not provided.\n- `coo_all` (optional, boolean): If `True`, break down results by all countries of origin. Defaults to `False`.\n- `coa_all` (optional, boolean): If `True`, break down results by all countries of asylum. Defaults to `False`.\n\n\n\n## To-do\n- Add `year_from` and `year_to` parameter\n- Include `nowcasting` endpoint\n- Include `resettlement` endpoint\n\n\n## License\n\nMIT\n\n## Acknowledgments\n\nThis project uses data from the [UNHCR Refugee Population Statistics Database](https://www.unhcr.org/refugee-statistics/).\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "sssjiang--pubchem_mcp_server": {
      "owner": "sssjiang",
      "name": "pubchem_mcp_server",
      "url": "https://github.com/sssjiang/pubchem_mcp_server",
      "imageUrl": "https://github.com/sssjiang.png",
      "description": "Extracts basic chemical information about drugs from the PubChem API, providing structured data such as molecular weight, formula, synonyms, and identifiers.",
      "stars": 8,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-11T05:09:25Z",
      "readme_content": "# pubchem mcp server\n\nthe mcp is used to extract the drug basic chemical infomation from pubchem API.\n\n## Requirements\n\n- Python 3.10\n- `python-dotenv`\n- `requests`\n- `mcp`\n- `uvicorn`\n\n## Installation\n\n**Install the dependencies(local):**\n\n- Install directly from the project directory\n\n```bash\ngit clone [project repository URL]\ncd [project directory]\npip install .\n```\n\n**Configure servers(pypi):**\n\nThe `servers_config.json` follows the same structure as Claude Desktop, allowing for easy integration of multiple servers.\nHere's an example:\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"uvx\",\n      \"args\": [\"pubchem_mcp_server\"]\n    }\n  }\n}\n```\n\n## the result of this MCP\n\n```json\n{\n  \"Drug Name\": \"Aspirin\",\n  \"CAS Number\": \"50-78-2\",\n  \"Molecular Weight\": 180.16,\n  \"Molecular Formula\": \"C9H8O4\",\n  \"SMILES\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",\n  \"Synonyms\": [\n    \"2-(Acetyloxy)benzoic Acid\",\n    \"Acetylsalicylic Acid\",\n    \"Acetysal\",\n    \"Acylpyrin\",\n    \"Aloxiprimum\",\n    \"Aspirin\",\n    \"Colfarit\",\n    \"Dispril\",\n    \"Easprin\"\n  ],\n  \"InchI Key\": \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\",\n  \"IUPAC Name\": \"2-acetyloxybenzoic acid\",\n  \"ATC Code\": \"N02BA01\",\n  \"Details Link\": \"https://pubchem.ncbi.nlm.nih.gov/compound/2244\"\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "TanvirHafiz--Medical-report-analyzer": {
      "owner": "TanvirHafiz",
      "name": "Medical-report-analyzer",
      "url": "https://github.com/TanvirHafiz/Medical-report-analyzer",
      "imageUrl": "https://github.com/TanvirHafiz.png",
      "description": "Analyze medical reports and symptoms to gain health insights and suggestions, providing detailed medicine information tailored to individual needs with bilingual support in English and Bengali.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-01T11:09:14Z",
      "readme_content": "# Medical Report Analyzer\n\nA web application that provides medical report analysis, symptoms analysis, and medicine information using AI. The application supports both English and Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ) languages.\n\n## Features\n\n1. **Medical Report Analysis**\n   - Upload medical reports (JPG, PDF)\n   - Extract and analyze test results\n   - Get health insights and suggestions\n\n2. **Symptoms Analysis**\n   - Describe symptoms in detail\n   - Get potential conditions and urgency level\n   - Receive immediate steps and precautions\n\n3. **Medicine Information**\n   - Get detailed medicine analysis\n   - View usage, side effects, and precautions\n   - Personalized information based on age and gender\n   - Dosage schedule analysis\n\n4. **Bilingual Support**\n   - Toggle between English and Bengali\n   - Instant translation of analysis results\n\n## Technologies Used\n\n- Python/Flask (Backend)\n- JavaScript/HTML/CSS (Frontend)\n- Tailwind CSS (Styling)\n- Ollama with deepseek-r1:14b model (AI Analysis)\n- Tesseract OCR (Text Extraction)\n- Google Translate API (Translation)\n\n## Prerequisites\n\n1. Python 3.8 or higher\n2. Tesseract OCR installed\n3. Ollama with deepseek-r1:14b model\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd medical-report-analyzer\n```\n\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Install Tesseract OCR:\n   - Windows: Download and install from [Tesseract GitHub](https://github.com/UB-Mannheim/tesseract/wiki)\n   - Linux: `sudo apt-get install tesseract-ocr`\n   - Mac: `brew install tesseract`\n\n5. Install and run Ollama:\n   - Follow instructions at [Ollama](https://ollama.ai)\n   - Pull the model: `ollama pull deepseek-r1:14b`\n\n## Configuration\n\n1. Set Tesseract path in `app.py`:\n```python\npytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust path as needed\n```\n\n2. Ensure Ollama is running with the deepseek-r1:14b model:\n```bash\nollama run deepseek-r1:14b\n```\n\n## Running the Application\n\n1. Start the Flask server:\n```bash\npython app.py\n```\n\n2. Open a web browser and navigate to:\n```\nhttp://localhost:5000\n```\n\n## Usage\n\n1. **Analyzing Medical Reports**\n   - Click \"Report Analysis\" tab\n   - Upload JPG or PDF file\n   - View analysis results\n   - Optionally translate to Bengali\n\n2. **Analyzing Symptoms**\n   - Click \"Symptoms Analysis\" tab\n   - Describe symptoms in detail\n   - Click \"Analyze Symptoms\"\n   - View analysis and recommendations\n\n3. **Getting Medicine Information**\n   - Click \"Medicine Info\" tab\n   - Enter patient age and gender\n   - Input medicine name and dosage schedule\n   - Click \"Analyze Medicine\"\n   - View detailed medicine analysis\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "tomekkorbak--oura-mcp-server": {
      "owner": "tomekkorbak",
      "name": "oura-mcp-server",
      "url": "https://github.com/tomekkorbak/oura-mcp-server",
      "imageUrl": "https://github.com/tomekkorbak.png",
      "description": "Access sleep, readiness, and resilience data from the Oura API for analysis and reporting. The server supports querying specific date ranges to retrieve sleep data.",
      "stars": 28,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T21:20:43Z",
      "readme_content": "# Oura MCP Server\n\n![Python Package](https://github.com/tomekkorbak/oura-mcp-server/workflows/Python%20Package/badge.svg)\n[![PyPI version](https://badge.fury.io/py/oura-mcp-server.svg)](https://badge.fury.io/py/oura-mcp-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)\n\nA [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides access to the Oura API. It allows language models to query sleep, readiness, and resilience data from Oura API.\n\n## Available Tools\n\nThe server exposes the following tools:\n\n### Date Range Queries\n\n- `get_sleep_data(start_date: str, end_date: str)`: Get sleep data for a specific date range\n- `get_readiness_data(start_date: str, end_date: str)`: Get readiness data for a specific date range\n- `get_resilience_data(start_date: str, end_date: str)`: Get resilience data for a specific date range\n\nDates should be provided in ISO format (`YYYY-MM-DD`).\n\n### Today's Data Queries\n\n- `get_today_sleep_data()`: Get sleep data for today\n- `get_today_readiness_data()`: Get readiness data for today\n- `get_today_resilience_data()`: Get resilience data for today\n\n## Usage\n\nYou'll need an Oura API token to use this server. You can obtain one by:\n\n1. Going to the [Oura Developer Portal](https://cloud.ouraring.com/v2/docs)\n2. Creating a Personal Access Token\n\n### Claude for Desktop\n\nUpdate your `claude_desktop_config.json` (located in `~/Library/Application\\ Support/Claude/claude_desktop_config.json` on macOS and `%APPDATA%/Claude/claude_desktop_config.json` on Windows) to include the following:\n\n```json\n{\n    \"mcpServers\": {\n        \"oura\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"oura-mcp-server\"\n            ],\n            \"env\": {\n                \"OURA_API_TOKEN\": \"YOUR_OURA_API_TOKEN\"\n            }\n        }\n    }\n}\n```\n\n\n## Example Queries\n\nOnce connected, you can ask Claude questions like:\n\n- \"What's my sleep score for today?\"\n- \"Show me my readiness data for the past week\"\n- \"How was my sleep from January 1st to January 7th?\"\n- \"What's my resilience score today?\"\n\n## Error Handling\n\nThe server provides human-readable error messages for common issues:\n\n- Invalid date formats\n- API authentication errors\n- Network connectivity problems\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "uh-joan--cortellis-mcp-server": {
      "owner": "uh-joan",
      "name": "cortellis-mcp-server",
      "url": "https://github.com/uh-joan/cortellis-mcp-server",
      "imageUrl": "https://github.com/null.png",
      "description": "Access the Cortellis drug database to search for drugs, retrieve detailed information, and analyze drug development statuses. Provides tools for ontology exploration and financial insights.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0
    },
    "vitaldb--medcalc": {
      "owner": "vitaldb",
      "name": "medcalc",
      "url": "https://github.com/vitaldb/medcalc",
      "imageUrl": "https://github.com/vitaldb.png",
      "description": "Perform a variety of medical calculations related to kidney function and pediatric health metrics. This server provides access to tools for calculating eGFR and blood pressure percentiles.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-03T15:54:07Z",
      "readme_content": "# MedCalc MCP Server\n\nThis package provides a set of medical calculation tools exposed as an MCP (Model Context Protocol) server.\n\n## Installation\n\n```bash\npip install medcalc\n```\n\nThis command installs the package and makes the `medcalc` command available in your environment.\n\n## Usage\n\nThe primary way to use this package is to run it as an MCP server. Once installed, you can start the server by running the following command in your terminal:\n\nThis will start the FastMCP server, making the following medical calculation tools available for MCP clients:\n\n*   `egfr_epi`: Calculates eGFR using the 2021 CKD-EPI Creatinine equation.\n*   `egfr_epi_cr_cys`: Calculates eGFR using the 2021 CKD-EPI Creatinine-Cystatin C equation.\n*   `bp_children`: Calculates pediatric blood pressure percentiles.\n*   `bmi_bsa_calculator`: Calculates Body Mass Index (BMI) and Body Surface Area (BSA).\n*   `crcl_cockcroft_gault`: Calculates Creatinine Clearance using Cockcroft-Gault.\n*   `map_calculator`: Calculates Mean Arterial Pressure (MAP).\n*   `chads2_vasc_score`: Calculates CHA‚ÇÇDS‚ÇÇ-VASc score for stroke risk.\n*   `prevent_cvd_risk`: Predicts 10-year risk of Cardiovascular Disease Events (PREVENT).\n*   `corrected_calcium`: Calculates corrected calcium for albumin levels.\n*   `qtc_calculator`: Calculates Corrected QT Interval (QTc) using various formulas.\n*   `wells_pe_criteria`: Calculates Wells' Criteria score for Pulmonary Embolism risk.\n*   `ibw_abw_calculator`: Calculates Ideal Body Weight (IBW) and Adjusted Body Weight (ABW).\n*   `pregnancy_calculator`: Calculates pregnancy due dates.\n*   `revised_cardiac_risk_index`: Calculates RCRI for pre-operative cardiac risk.\n*   `child_pugh_score`: Calculates Child-Pugh score for cirrhosis.\n*   `steroid_conversion`: Converts corticosteroid dosages.\n*   `calculate_mme`: Calculates Morphine Milligram Equivalents (MME).\n*   `maintenance_fluids`: Calculates maintenance IV fluid rate (4-2-1 Rule).\n*   `corrected_sodium`: Calculates corrected sodium for hyperglycemia.\n*   `meld_3`: Calculates MELD 3.0 score for liver disease.\n*   `framingham_risk_score`: Calculates Framingham Risk Score for 10-year CHD risk.\n*   `homa_ir`: Calculates HOMA-IR score for insulin resistance.\n\n## Claude Desktop / `uvx` Integration\n\nThis server can be integrated with applications like Claude Desktop that support MCP. To configure Claude Desktop (or another FastAgent application) to use this server, you can add an entry to your `claude_desktop_config.json`.\n\nAssuming you have `uvx` installed and configured, you can typically run an installed Python package command via `uvx`. Here's an example configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"medcalc\": {\n      \"command\": \"uvx\",\n      \"args\": [\"medcalc@latest\"]\n    }\n  }\n}\n```\n\n**Note:** Ensure `uvx` is installed (`pip install uvx`) and the `medcalc` package is installed in an environment accessible by `uvx`. The exact `uvx` command might vary based on your specific `uvx` setup and environment management.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "vishwa684--unet": {
      "owner": "vishwa684",
      "name": "unet",
      "url": "https://github.com/vishwa684/unet",
      "imageUrl": "https://github.com/vishwa684.png",
      "description": "Train and deploy U-Net models for biomedical image segmentation using the Medical Decathlon dataset, with support for both 2D and 3D U-Net scripts. Visualize predictions and assess model performance through comprehensive demos and visual outputs.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-05-11T00:13:12Z",
      "readme_content": "# Deep Learning Medical Decathlon Demos for Python*\n### U-Net Biomedical Image Segmentation with Medical Decathlon Dataset.\n\nThis repository contains [2D](https://github.com/IntelAI/unet/tree/master/2D) and [3D](https://github.com/IntelAI/unet/tree/master/3D) U-Net scripts for training models using the [Medical Decathlon](http://medicaldecathlon.com/) dataset (http://medicaldecathlon.com/).\n\n![pred152_3D](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_152_img3D.gif\n\"BRATS image #152:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\").  ![pred195](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif \"BRATS image #195:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\")\n\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Wendong-Fan--HowToLiveLonger": {
      "owner": "Wendong-Fan",
      "name": "HowToLiveLonger",
      "url": "https://github.com/Wendong-Fan/HowToLiveLonger",
      "imageUrl": "https://github.com/Wendong-Fan.png",
      "description": "Provides evidence-based lifestyle insights aimed at improving longevity through nutrition, exercise, and wellness practices. Users can explore actionable recommendations to reduce mortality risk.",
      "stars": 13,
      "forks": 0,
      "license": "The Unlicense",
      "language": "",
      "updated_at": "2025-09-11T07:34:26Z",
      "readme_content": "# Á®ãÂ∫èÂëòÂª∂ÂØøÊåáÂçó\n\n[![CN doc](https://img.shields.io/badge/ÊñáÊ°£-‰∏≠ÊñáÁâà-blue.svg)](README.md)\n[![EN doc](https://img.shields.io/badge/document-English-blue.svg)](README_en.md)\n[![MetaGPT](https://img.shields.io/badge/ÂáèÂ∞ëÂ∑•‰ΩúÊó∂Èó¥-MetaGPT-blue)](https://github.com/geekan/MetaGPT)\n\n\n- [1. ÊúØËØ≠](#1-ÊúØËØ≠)\n- [2. ÁõÆÊ†á](#2-ÁõÆÊ†á)\n- [3. ÂÖ≥ÈîÆÁªìÊûú](#3-ÂÖ≥ÈîÆÁªìÊûú)\n- [4. ÂàÜÊûê](#4-ÂàÜÊûê)\n- [5. Ë°åÂä®](#5-Ë°åÂä®)\n- [6. ËØÅÊçÆ](#6-ËØÅÊçÆ)\n  - [6.1. ËæìÂÖ•](#61-ËæìÂÖ•)\n    - [6.1.1. Âõ∫‰Ωì](#611-Âõ∫‰Ωì)\n    - [6.1.2. Ê∂≤‰Ωì](#612-Ê∂≤‰Ωì)\n    - [6.1.3. Ê∞î‰Ωì](#613-Ê∞î‰Ωì)\n    - [6.1.4. ÂÖâÁÖß](#614-ÂÖâÁÖß)\n    - [6.1.5. ËçØÁâ©](#615-ËçØÁâ©)\n  - [6.2. ËæìÂá∫](#62-ËæìÂá∫)\n    - [6.2.1. Êå•ÊãçËøêÂä®](#621-Êå•ÊãçËøêÂä®)\n    - [6.2.2. ÂâßÁÉàËøêÂä®](#622-ÂâßÁÉàËøêÂä®)\n    - [6.2.3. Ëµ∞Ë∑Ø](#623-Ëµ∞Ë∑Ø)\n    - [6.2.4. Âà∑Áâô](#624-Âà∑Áâô)\n    - [6.2.5. Ê≥°Êæ°](#625-Ê≥°Êæ°)\n    - [6.2.6. ÂÅöÂÆ∂Âä°ÔºàËÄÅÂπ¥Áî∑ÊÄßÔºâ](#626-ÂÅöÂÆ∂Âä°ËÄÅÂπ¥Áî∑ÊÄß)\n    - [6.2.7. Áù°Áú†](#627-Áù°Áú†)\n    - [6.2.8. ‰πÖÂùê](#628-‰πÖÂùê)\n  - [6.3. ‰∏ä‰∏ãÊñá](#63-‰∏ä‰∏ãÊñá)\n    - [6.3.1. ÊÉÖÁª™](#631-ÊÉÖÁª™)\n    - [6.3.2. Ë¥´ÂØå](#632-Ë¥´ÂØå)\n    - [6.3.3. ‰ΩìÈáç](#633-‰ΩìÈáç)\n    - [6.3.4. Êñ∞ÂÜ†](#634-Êñ∞ÂÜ†)\n\n---\n\n### 1. ÊúØËØ≠\n\n* ACM: All-Cause Mortality / ÂÖ®Âõ†Ê≠ª‰∫°Áéá\n\n### 2. ÁõÆÊ†á\n\n* Á®≥ÂÅ•ÁöÑÊ¥ªÂæóÊõ¥‰πÖ\n* Ëä±Êõ¥Â∞ëÊó∂Èó¥Â∑•‰ΩúÔºöËßÅ[MetaGPT](https://github.com/geekan/MetaGPT)\n\n### 3. ÂÖ≥ÈîÆÁªìÊûú\n\n* Èôç‰Ωé66.67%ÂÖ®Âõ†Ê≠ª‰∫°Áéá\n* Â¢ûÂä†\\~20Âπ¥È¢ÑÊúüÂØøÂëΩ\n* ~~Áª¥ÊåÅÂ§öÂ∑¥ËÉ∫‰∫é‰∏≠ËΩ¥~~\n\n### 4. ÂàÜÊûê\n\n* ‰∏ªË¶ÅÂèÇËÄÉÔºöÂØπACMÁöÑÂ≠¶ÊúØÊñáÁåÆÁõ∏ÂØπËæÉÂ§öÔºåÂèØ‰ª•‰Ωú‰∏∫‰∏ªË¶ÅÂèÇËÄÉ\n* Â¢ûÂä†ÂØøÂëΩ‰∏éACMÂÖ≥Á≥ªÈùûÁ∫øÊÄßÔºöÊòæÁÑ∂Â¢ûÂä†ÂØøÂëΩ‰∏éACMÂÖ≥Á≥ªÊòØÈùûÁ∫øÊÄßÂáΩÊï∞ÔºåËøôÈáåÂÅáËÆæ `ŒîLifeSpan=(1/(1+ŒîACM)-1)*10`ÔºàŒîACM‰∏∫ACMÂèòÂåñÂÄºÔºõÂÖ¨ÂºèÊ¨¢Ëøé‰ºòÂåñÔºâ\n* ÂèòÈáèÊó†Ê≥ïÁÆÄÂçïÂè†Âä†ÔºöÊòæÁÑ∂ÂêÑ‰∏™ÂèòÈáè‰πãÈó¥Âπ∂‰∏çÁ¨¶ÂêàÁã¨Á´ãÂêåÂàÜÂ∏ÉÂÅáËÆæÔºåÂèòÈáè‰πãÈó¥ÁöÑÂÆûÈôÖÂΩ±Âìç‰πüÂπ∂‰∏çÊòéÁ°Æ\n* Â≠òÂú®ÁüõÁõæËßÇÁÇπÔºöÊâÄÊúâÁöÑËØÅÊçÆÈÉΩÊúâÊñáÁåÆ/Á†îÁ©∂ÂØπÂ∫îÔºå‰ΩÜÊ≥®ÊÑèÂà∞ÔºöÊúâ‰∫õÊñáÁåÆ‰πãÈó¥ÊúâÊòæËëóÁüõÁõæÁöÑËßÇÁÇπÔºàÂ¶ÇÂØπ‰∫éÁ¢≥Ê∞¥ÊëÑÂÖ•ÊØî‰æãÁöÑÁüõÁõæÔºâÔºõÊúâ‰∫õÊñáÁåÆÂ≠òÂú®ËæÉÂ§ß‰∫âËÆÆÔºàÂ¶ÇËÆ§‰∏∫22ÁÇπÂâçÁù°Ëßâ‰ºöÊèêÂçá43%ÂÖ®Âõ†Ê≠ª‰∫°ÁéáÔºâ\n* Á†îÁ©∂‰ªÖË°®ËææÁõ∏ÂÖ≥ÔºöÊâÄÊúâÊñáÁåÆË°®ÊòéÁöÑÊõ¥Â§öÊòØÁõ∏ÂÖ≥ËÄåÈùûÂõ†ÊûúÔºåÂú®ÈòÖËØªÊó∂Ë¶ÅËÄÉËôëÊñáÁåÆÊòØÂê¶ÂÖÖÂàÜËØÅÊòé‰∫ÜÂõ†Êûú ‚Äî‚Äî Â¶ÇÊüêÊñáÁåÆË°®Êòé‰∫ÜÊó•Âùá>=7000Ê≠•ÁöÑ‰∫∫ÊúâÊòæËëó‰ΩéÁöÑÂÖ®Âõ†Ê≠ª‰∫°Áéá„ÄÇ‰ΩÜÊ≠•Êï∞Â∞ëÁöÑ‰∫∫ÂèØËÉΩÂåÖÂê´Êõ¥Â§öÈïøÊúüÁóÖÊÇ£ÔºåÂ¶ÇÊûúÊ≤°ÊúâÂêàÁêÜÁöÑÊéíÈô§ËøôÂùóÊï∞ÊçÆÔºåÈÇ£Ê≠§ÊñáÁåÆË∞ÉÊü•Â§±Áúü\n\n### 5. Ë°åÂä®\n\n* ËæìÂÖ•\n  * Âõ∫‰ΩìÔºöÂêÉÁôΩËÇâÔºà-11%\\~-3% ACMÔºâ„ÄÅËî¨Êûú‰∏∫‰∏ªÔºà-26%\\~-17% ACMÔºâÔºåÂ§öÂêÉËæ£Ôºà-23% ACMÔºâÔºåÂ§öÂêÉÂùöÊûúÔºà-27%\\~-4% ACMÔºâÔºå‰∏≠ÈáèÁ¢≥Ê∞¥„ÄÅÂ§öÂêÉÊ§çÁâ©ËõãÁôΩÔºà-10% ACMÔºâÔºåÂ∞ëÂêÉË∂ÖÂä†Â∑•È£üÁâ©Ôºà-62%\\~-18%Ôºâ\n  * Ê∂≤‰ΩìÔºöÂñùÂíñÂï°Ôºà-22%\\~-12% ACMÔºâÔºåÂñùÁâõÂ•∂Ôºà-17%\\~-10% ACMÔºâÔºåÂñùËå∂Ôºà-15%\\~-8% ACMÔºâÔºåÂ∞ëÂñùÊàñ‰∏çÂñùÁîúÂë≥È•ÆÊñôÔºàÂê¶ÂàôÊØèÂ§©‰∏ÄÊùØ+7% ACMÔºå+Â§öÂ∑¥ËÉ∫ÔºâÔºåÊàíÈÖíÔºàÂê¶Âàô+\\~50% ACMÔºåÊó†‰∏äÈôêÔºâ\n  * Ê∞î‰ΩìÔºö‰∏çÂê∏ÁÉüÔºàÂê¶Âàô+~50% ACMÔºå-12\\~-11Âπ¥ÂØøÂëΩÔºâ\n  * ÂÖâÁÖßÔºöÊôíÂ§™Èò≥Ôºà-~40% ACMÔºâ\n  * ËçØÁâ©Ôºö‰∫åÁî≤ÂèåËÉçÔºàÁ≥ñÂ∞øÁóÖ‰∫∫Áõ∏ÊØîÊ≠£Â∏∏‰∫∫ÂèØ‰ª•+3Âπ¥Ôºâ„ÄÅÂ§çÂêàÁª¥ÁîüÁ¥†Ôºà-8%ÁôåÁóáÈ£éÈô©Ôºâ„ÄÅ‰∫öÁ≤æËÉ∫Ôºà-60%\\~-30% ACMÔºâ„ÄÅËë°ËêÑÁ≥ñËÉ∫Ôºà-39% ACMÔºâ\n* ËæìÂá∫\n  * ËøêÂä®ÔºöÊØèÂë®3Ê¨°45ÂàÜÈíüÊå•ÊãçËøêÂä®Ôºà-47% ACMÔºâ\n  * Êó•Â∏∏ÔºöÂà∑ÁâôÔºà-25% ACMÔºâ\n  * Áù°Áú†ÔºöÊØèÂ§©Áù°7Â∞èÊó∂ÂÖ®Âõ†Ê≠ª‰∫°ÁéáÊúÄ‰ΩéÔºõ‰∏î22-24ÁÇπÈó¥ÊúÄÂ•ΩÔºå*Êó©Áù°+43% ACMÔºåÊôöÁù°+15% ACMÔºàÂ≠òÂú®‰∫âËÆÆÔºâ*\n* ‰∏ä‰∏ãÊñá\n  * ‰ΩìÈáçÔºöÂáèËÇ•Ôºà-54% ACMÔºâ\n\n### 6. ËØÅÊçÆ\n\n#### 6.1. ËæìÂÖ•\n\n##### 6.1.1. Âõ∫‰Ωì\n\n* ÁôΩËÇâ\n  * [JAMAÂ≠êÂàäÔºöÈ£üÁî®Á∫¢ËÇâÂíåÂä†Â∑•ËÇâÁ±ª‰ºöÂ¢ûÂä†ÂøÉËÑèÁóÖÂíåÊ≠ª‰∫°È£éÈô©ÔºÅÈ±ºËÇâÂíåÂÆ∂Á¶ΩËÇâÂàô‰∏ç‰ºö](https://zhuanlan.zhihu.com/p/268401670)\n    * Âá∫Â§ÑÔºö[Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality](https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf)\n    * Â¢ûÂä†Á∫¢ËÇâÊëÑÂÖ•‰∏éÊ≠ª‰∫°È£éÈô©Áõ∏ÂÖ≥„ÄÇÂÖ´Âπ¥ÂÜÖÂπ≥ÂùáÊØèÂ§©Â¢ûÂä†Ëá≥Â∞ëÂçä‰ªΩÁ∫¢ËÇâÊëÑÂÖ•ÔºàÂçä‰ªΩÁ∫¢ËÇâÁõ∏ÂΩì‰∫é14gÂä†Â∑•Á∫¢ËÇâÊàñ40gÈùûÂä†Â∑•Á∫¢ËÇâÔºâÁöÑË∞ÉÊü•ÂØπË±°ÔºåÂú®Êé•‰∏ãÊù•ÂÖ´Âπ¥ÂÜÖÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Â¢ûÂä†10ÔºÖÔºàHR, 1.10; 95%CI, 1.04-1.17ÔºâÔºõÊØèÂë®ÂêÉ‰∏§‰ªΩÁ∫¢ËÇâÊàñÂä†Â∑•ËÇâÁ±ªÔºà‰ΩÜ‰∏çÂåÖÊã¨ÂÆ∂Á¶ΩÊàñÈ±ºÁ±ªÔºâ‰ºö‰ΩøÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Â¢ûÂä†3%\n    * ![Á∫¢ËÇâ](https://user-images.githubusercontent.com/2707039/163703960-6f321de5-4daa-4ea5-95b9-af9c96f1c1bc.jpg)\n  * [Á∫¢ËÇâÂíåÁôΩËÇâÊúÄÂ§ßÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü‰∏∫Âï•Ë¶ÅËøô‰πàÂàÜÂë¢Ôºü](https://www.zhihu.com/question/67223570/answer/809785380)\n* Ëî¨Êûú\n  * [ÊØèÂπ¥54‰∏á‰∫∫Ê≠ª‰∫°ÔºåÁ´üÊòØÂõ†‰∏∫Ê∞¥ÊûúÂêÉÂæóÂ∞ëÔºÅÔºüËøôÂ∑≤ÊàêÂçÅÂ§ßÊ≠ª‰∫°Âõ†Á¥†‰πã‰∏ÄÔºÅ](https://www.sohu.com/a/322360740_164924)\n    * Âá∫Â§ÑÔºö[Estimated Global, Regional, and National Cardiovascular Disease Burdens Related to Fruit and Vegetable Consumption: An Analysis from the Global Dietary Database (FS01-01-19) ](https://academic.oup.com/cdn/article-abstract/3/Supplement_1/nzz028.FS01-01-19/5516583)\n    * ÊØèÂ§©ÊëÑÂÖ•200ÂÖãÊñ∞È≤úÊ∞¥ÊûúÂèØ‰ΩøÊ≠ª‰∫°ÁéáÈôç‰Ωé17%ÔºåÁ≥ñÂ∞øÁóÖÂ§ßË°ÄÁÆ°Âπ∂ÂèëÁóáÔºàÂ¶Ç‰∏≠È£é„ÄÅÁº∫Ë°ÄÊÄßÂøÉËÑèÁóÖÁ≠âÔºâÈ£éÈô©Èôç‰Ωé13%ÔºåÂèäÁ≥ñÂ∞øÁóÖÂ∞èË°ÄÁÆ°Âπ∂ÂèëÁóáÔºàÂ¶ÇÁ≥ñÂ∞øÁóÖËÇæÁóÖ„ÄÅÁ≥ñÂ∞øÁóÖÁúºÁóÖ„ÄÅÁ≥ñÂ∞øÁóÖË∂≥ÁóÖÁ≠âÔºâÈ£éÈô©Èôç‰Ωé28%\n  * [„ÄäËá™ÁÑ∂„ÄãÂ≠êÂàäÔºöÊØèÂ§©‰∫å‰∏§Ë•øÂÖ∞Ëä±ÔºåÂÅ•Â∫∑ÈïøÂØøÈÉΩÊúâÂï¶ÔºÅÂàÜÊûêËøë6‰∏á‰∫∫23Âπ¥ÁöÑÊï∞ÊçÆÂèëÁé∞ÔºåÂêÉÂê´ÈªÑÈÖÆÁ±ªÈ£üÁâ©‰∏éÊ≠ª‰∫°È£éÈô©Èôç‰Ωé20%Áõ∏ÂÖ≥‰∏®‰∏¥Â∫äÂ§ßÂèëÁé∞](https://mp.weixin.qq.com/s/E6BAi-Vnhr1jXBm0Pys2ZQ)\n    * Âá∫Â§ÑÔºö[Flavonoid intake is associated with lower mortality in the Danish Diet Cancer and Health Cohort](https://www.nature.com/articles/s41467-019-11622-x)\n    * ÂêÉÂê´ÈªÑÈÖÆÁ±ªÈ£üÁâ©‰∏éÊ≠ª‰∫°È£éÈô©Èôç‰Ωé20%Áõ∏ÂÖ≥\n    * ![ÈªÑÈÖÆ](https://user-images.githubusercontent.com/2707039/163703969-42e64f88-e727-4e7d-85f2-07a92e29b613.jpg)\n    * BondonnoÂçöÂ£´ËØ¥ÈÅì‚ÄúÂêÉ‰∏çÂêåËî¨Ëèú„ÄÅÊ∞¥ÊûúË°•ÂÖÖÔºå‰∏çÂêåÁßçÁ±ªÁöÑÈªÑÈÖÆÁ±ªÂåñÂêàÁâ©ÊòØÂæàÈáçË¶ÅÁöÑÔºåËøôÂæàÂÆπÊòìÈÄöËøáÈ•ÆÈ£üÂÆûÁé∞Ôºö‰∏ÄÊùØËå∂„ÄÅ‰∏Ä‰∏™ËãπÊûú„ÄÅ‰∏Ä‰∏™Ê©òÂ≠ê„ÄÅ100ÂÖãËìùËéìÔºåÊàñ100ÂÖãË•øÂÖ∞Ëä±ÔºåÂ∞±ËÉΩÊèê‰æõÂêÑÁßçÈªÑÈÖÆÁ±ªÂåñÂêàÁâ©ÔºåÂπ∂‰∏îÊÄªÂê´ÈáèË∂ÖËøá500ÊØ´ÂÖã„ÄÇ\n* Ëæ£Ê§í\n  * [Ëæ£Ê§íÊàêÊ≠ª‰∫°ÂÖãÊòüÔºüÊçÆË∞ÉÁ†îÔºåÂ∏∏ÂêÉËæ£ÊÇ£ÁóÖÊ≠ª‰∫°È£éÈô©ÂèØÈôç‰Ωé61%](https://3g.163.com/dy/article/F6Q7I1ME053228ZU.html)\n    * Âá∫Â§Ñ1Ôºö[Chili pepper consumption and mortality in Italian adults](https://www.sciencedirect.com/science/article/pii/S0735109719382063)\n\t* Âá∫Â§Ñ2Ôºö[The Association of Hot Red Chili Pepper Consumption and Mortality: A Large Population-Based Cohort Study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169876)\n    * 2017Âπ¥Plos One ÁöÑÂè¶‰∏ÄÈ°πÊù•Ëá™ÁæéÂõΩÁöÑÁ†îÁ©∂‰ª•16179ÂêçÔºåÂπ¥ÈæÑÂú®18Â≤Å‰ª•‰∏äÁöÑ‰∫∫Áæ§‰∏∫ÂØπË±°ÔºåÂπ∂ÂØπÂÖ∂ËøõË°å‰∫ÜÈ´òËææ19Âπ¥ÁöÑÈöèËÆøÔºåÂèëÁé∞Âú®4946‰æãÊ≠ª‰∫°ÊÇ£ËÄÖ‰∏≠ÔºåÈ£üÁî®Ëæ£Ê§íÁöÑÂèÇ‰∏éËÄÖÁöÑÂÖ®Âõ†Ê≠ª‰∫°Áéá‰∏∫21.6ÔºÖÔºåËÄåÊú™È£üÁî®Ëæ£Ê§íÁöÑÂèÇ‰∏éËÄÖÁöÑÂÖ®Âõ†Ê≠ª‰∫°Áéá‰∏∫33.6ÔºÖ„ÄÇÁõ∏ËæÉ‰∫é‰∏çÂêÉËæ£ÊàñÂæàÂ∞ëÂêÉÔºàÂ∞ë‰∫éÊØèÂë®‰∏§Ê¨°ÔºâÁöÑ‰∫∫Áæ§ÔºåÊØèÂë®ÂêÉËæ£Ôºû4Ê¨°ÁöÑ‰∫∫Áæ§ÊÄªÊ≠ª‰∫°È£éÈô©Èôç‰Ωé23%ÔºåÂøÉË°ÄÁÆ°Ê≠ª‰∫°È£éÈô©Èôç‰Ωé34%„ÄÇ\n* È∏°Ëõã\n  * [ÊØèÂ§©Â§öÂêÉÂçä‰∏™ËõãÔºåÂ¢ûÂä†7%ÁöÑÂÖ®Âõ†ÂíåÂøÉË°ÄÁÆ°Ê≠ª‰∫°È£éÈô©Ôºü](https://m.thepaper.cn/baijiahao_11540780)\n    * Âá∫Â§ÑÔºö[NIH-AARPÂ∑•‰Ωú‰∏ªÈ°µ](https://dietandhealth.cancer.gov/)„ÄÅ[Egg and cholesterol consumption and mortality from cardiovascular and different causes in the United States: A population-based cohort study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7872242/)\n    * ÊØèÂ§©Â§öÂêÉÂçä‰∏™ËõãÔºåÂ¢ûÂä†7%ÁöÑÂÖ®Âõ†ÂíåÂøÉË°ÄÁÆ°Ê≠ª‰∫°È£éÈô©ÔºüÂú®ÂÅáËÆæÊÄßÊõø‰ª£ÂàÜÊûê‰∏≠ÔºåÁ†îÁ©∂ËÄÖÂèëÁé∞ÔºåÁî®Á≠âÈáèÁöÑËõãÊ∏Ö/È∏°ËõãÊõø‰ª£Áâ©„ÄÅÂÆ∂Á¶Ω„ÄÅÈ±º„ÄÅ‰π≥Âà∂ÂìÅ„ÄÅÂùöÊûúÂíåË±ÜÁ±ªÂàÜÂà´Êõø‰ª£ÂçäÂè™ÂÖ®ËõãÔºà25ÂÖã/Â§©ÔºâÂèØ‰ª•Èôç‰Ωé6%„ÄÅ8%„ÄÅ9%„ÄÅ7%„ÄÅ13%Âíå10%ÁöÑÂÖ®Âõ†Ê≠ª‰∫°Áéá„ÄÇ\n\t*[È∏°Ëõã](https://raw.githubusercontent.com/qhy040404/Image-Resources-Repo/master/pmed.1003508.g002.jpg)\n* ÂùöÊûú\n  * [Âìà‰Ωõ20Âπ¥Á†îÁ©∂ÔºöÂêÉÊ†∏Ê°ÉÁöÑ‰∫∫Êõ¥ÈïøÂØøÔºåÊòæËëóÂáèÂ∞ëÂÖ®Âõ†Ê≠ª‰∫°ÔºåÂª∂ÈïøÂØøÂëΩ](https://www.163.com/dy/article/GKVOMMMF05148PF4.html)\n    * Âá∫Â§ÑÔºö[Association of Walnut Consumption with Total and Cause-Specific Mortality and Life Expectancy in US Adults](https://www.mdpi.com/2072-6643/13/8/2699/pdf)\n    * ÈÄöËøáÂàÜÊûêÂèëÁé∞ÔºåÁªèÂ∏∏È£üÁî®Ê†∏Ê°ÉÂèØ‰ª•Âª∂ÈïøÂØøÂëΩÔºåÈôç‰ΩéÂøÉË°ÄÁÆ°ÁñæÁóÖÊ≠ª‰∫°È£éÈô©„ÄÇÊØîËµ∑‰∏çÂêÉÊ†∏Ê°ÉÔºåÊØèÂë®È£üÁî®Ê†∏Ê°É5‰ªΩ‰ª•‰∏äÔºà1‰ªΩ28ÂÖãÔºâÁöÑÂÅ•Â∫∑È¢ÑÊúüÂØøÂëΩÂª∂Èïø1.3Â≤ÅÔºåÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Èôç‰Ωé14%ÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÊ≠ª‰∫°ÁéáÈôç‰Ωé25%„ÄÇ\n  * [Á†îÁ©∂ÔºöÊØèÊó•È£üÁîüÂùöÊûúÔºåÊ≠ª‰∫°ÁéáÈôç20%](https://zhuanlan.zhihu.com/p/44454030)\n    * Âá∫Â§Ñ1Ôºö[Association of nut consumption with total and cause-specific mortality](https://www.nejm.org/doi/full/10.1056/NEJMoa1307352)\n    * Âá∫Â§Ñ2Ôºö[APG_Health-&-Nutrition-Research-Brochure_DEC-19-18](https://americanpistachios.cn/sites/china/files/inline-files/APG_Health-%26-Nutrition-Research-Brochure_DEC-19-18.pdf)\n    * Á†îÁ©∂‰∫∫ÂëòÂèëÁé∞ÔºåÊØèÂë®ÂêÉÊ†ëÂùöÊûú‰Ωé‰∫é1ÁõéÂè∏‰ªΩÈáèÁöÑ‰∫∫ÔºåÊ≠ª‰∫°ÁéáÈôç‰Ωé7ÔºÖ„ÄÇËÄåÊØèÂë®ÂêÉ‰∫Ü1ÁõéÂè∏‰ªΩÈáèÁöÑ‰∫∫ÔºåÂáèÂ∞ë11ÔºÖÁöÑÊ≠ª‰∫°ÁéáÔºõÊØèÂë®ÂêÉ2‰ªΩÈáèÁöÑ‰∫∫ÔºåÂáè‰Ωé13ÔºÖÔºõÊØèÂë®5Ëá≥6‰ªΩÈáèËÄÖÔºåÂáèÂ∞ë‰∫Ü15ÔºÖÔºõ‰∏ÄÂë®7‰ªΩ‰ª•‰∏äÁöÑ‰∫∫ÔºåÊ≠ª‰∫°ÁéáÂàôÂáèÂ∞ë20ÔºÖ„ÄÇ\n    * Âè¶Â§ñ‰∏§ÁØáÂèëË°®Âú®„ÄäÂÖ¨ÂÖ±ÁßëÂ≠¶Âõæ‰π¶È¶ÜÂú®Á∫øÊúüÂàä„Äã(Public Library of Science Online Journal)Âíå„ÄäÁîüÁâ©ÂåªÂ≠¶‰∏≠ÂøÉ„Äã(BioMed Central)‰∏äÁöÑÂåªÂ≠¶È¢ÑÁßëÁ†îÁ©∂ËÆ∫ÊñáÔºåÂ±ïÁ§∫‰∫ÜËØïÈ™åÂºÄÂßãÊó∂ÁöÑÊ®™Êñ≠Èù¢Êï∞ÊçÆ„ÄÇËøô‰∏§È°πÁ†îÁ©∂ÈÉΩËØÑ‰º∞‰∫Ü7,216ÂêçÂØπË±°Ôºå‰ª•Âèä‰ªñ‰ª¨È£üÁî®ÂùöÊûúÁöÑÈ¢ëÁéáÂíåÊï∞Èáè‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÈÇ£‰∫õÊØèÂë®È£üÁî®‰∏â‰ªΩ‰ª•‰∏äÂùöÊûú(ÂåÖÊã¨ÂºÄÂøÉÊûú)ÁöÑÁ†îÁ©∂ÂØπË±°ÁöÑÊ≠ª‰∫°ÁéáÈôç‰Ωé39%„ÄÇ\n* Èí†ÔºàÂ≠òÊúâÂ§ßÈáè‰∫âËÆÆÔºâ\n  * [Eur Heart JÔºöÈí†ÊëÑÂÖ•Èáè‰∏éÈ¢ÑÊúüÂØøÂëΩ„ÄÅÂÖ®Âõ†Ê≠ª‰∫°ÁéáÁöÑÂÖ≥Á≥ª](https://nursing.medsci.cn/article/show_article.do;jsessionid=A34E8A33918A152CB55BDD2E5FB1798D?id=afe720486ee7)\n    * Âá∫Â§ÑÔºö[Messerli F H, Hofstetter L, Syrogiannouli L, et al. Sodium intake, life expectancy, and all-cause mortality[J]. European heart journal, 2021, 42(21): 2103-2112.](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC8169157&blobtype=pdf)\n    * ![ehaa947f6](https://user-images.githubusercontent.com/2707039/164894778-9710f18d-e055-4f62-bdcb-618687771d77.jpeg)\n    * Âú®ËØ•ÂàÜÊûêÊâÄÂåÖÂê´ÁöÑ181‰∏™ÂõΩÂÆ∂‰∏≠ÔºåÁ†îÁ©∂‰∫∫ÂëòÂèëÁé∞Èí†ÊëÑÂÖ•Èáè‰∏éÂá∫ÁîüÊó∂ÁöÑÂÅ•Â∫∑È¢ÑÊúüÂØøÂëΩÔºàŒ≤=2.6Âπ¥/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.66ÔºåP<0.001ÔºâÂíå60Â≤ÅÊó∂ÁöÑÂÅ•Â∫∑È¢ÑÊúüÂØøÂëΩÔºàŒ≤=0.3Âπ¥/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.60ÔºåP=0.048Ôºâ‰πãÈó¥Â≠òÂú®Ê≠£Áõ∏ÂÖ≥ÂÖ≥Á≥ªÔºå‰ΩÜ‰∏éÈùû‰º†ÊüìÊÄßÁñæÁóÖÊ≠ª‰∫°ÔºàŒ≤=17Ê¨°‰∫ã‰ª∂/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.43ÔºåP=0.100ÔºâÊó†ÂÖ≥„ÄÇÁõ∏ÂèçÔºåÂÖ®Âõ†Ê≠ª‰∫°Áéá‰∏éÈí†ÊëÑÂÖ•ÈáèÊàêË¥üÁõ∏ÂÖ≥ÔºàŒ≤=‚àí131Ê¨°‰∫ã‰ª∂/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.60ÔºåP<0.001Ôºâ„ÄÇÂú®‰ªÖÈôê‰∫é46‰∏™Êî∂ÂÖ•ÊúÄÈ´òÂõΩÂÆ∂ÁöÑÊïèÊÑüÊÄßÂàÜÊûê‰∏≠ÔºåÈí†ÊëÑÂÖ•Èáè‰∏éÂá∫ÁîüÊó∂ÁöÑÂÅ•Â∫∑È¢ÑÊúüÂØøÂëΩÂëàÊ≠£Áõ∏ÂÖ≥ÔºàŒ≤=3.4Âπ¥/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.53ÔºåP<0.001ÔºâÔºåËÄå‰∏éÂÖ®Âõ†Ê≠ª‰∫°ÁéáÔºàŒ≤=‚àí168Ê¨°‰∫ã‰ª∂/ÂÖãÊØèÊó•Èí†ÊëÑÂÖ•ÈáèÔºåR<sup>2</sup>=0.50ÔºåP<0.001ÔºâÂëàË¥üÁõ∏ÂÖ≥„ÄÇ\n    * ËØ•ÔºàÂ§ßËåÉÂõ¥ÔºâÁ†îÁ©∂ËÆ§‰∏∫Êõ¥Â§öÁöÑÈí†ÊëÑÂÖ•‰∏éÊòæËëóÊõ¥‰ΩéÁöÑÂÖ®Âõ†Ê≠ª‰∫°ÁéáÊúâÂÖ≥\n    * [ÈíàÂØπËØ•ËÆ∫ÊñáÁöÑÂª∂‰º∏Ëß£ËØªÂíåËÆ®ËÆ∫ÔºöA Fresh Foray in the Salt Wars: Life Expectancy Higher With Greater Sodium Intake](https://www.tctmd.com/news/fresh-foray-salt-wars-life-expectancy-higher-greater-sodium-intake)\n  * [NEJM/LancetÔºö‰∏çË¶ÅÂêÉÂ§™Â§öÁõêÔºå‰∏≠ÂõΩÈ•ÆÈ£üÊâÄËá¥ÂøÉË°ÄÁÆ°ÁóÖÂíåÁôåÁóáÊ≠ª‰∫°ÂÖ®ÁêÉÁ¨¨‰∏ÄÔºåÂêÉ‰ΩéÈí†ÁõêÂèØÈôç‰ΩéÂÖ®Âõ†Ê≠ª‰∫°Áéá](https://ibook.antpedia.com/x/669028.html)\n    * ‰ΩÜ‰πüÊúâÂ§öÈ°πÁ†îÁ©∂ËÆ§‰∏∫Áî®‰ΩéÈí†ÁõêÂèØ‰ª•Èôç‰Ωé‰∏ÄÁ≥ªÂàóÁñæÁóÖÁöÑÂèëÁîüÊ¶ÇÁéáÔºåÂØπÂÖ®Âõ†Ê≠ª‰∫°ÁéáÁöÑÂáèÂ∞ëÊúâÁßØÊûÅÂΩ±Âìç\n* Á¢≥Ê∞¥ÔºàÂ≠òÊúâÂ§ßÈáè‰∫âËÆÆÔºâ\n  * [‰ΩéÁ¢≥ÁîüÈÖÆÈ•ÆÈ£üÔºàÂõõÔºâÁ¢≥Ê∞¥ÂåñÂêàÁâ©‰∏éÈïøÊúüÊ≠ª‰∫°Áéá](https://zhuanlan.zhihu.com/p/137815934)\n    * Âá∫Â§ÑÔºöThe Lancet Public Health - [Dietary carbohydrate intake and mortality: a prospective cohort study and meta-analysis](https://www.sciencedirect.com/science/article/pii/S246826671830135X)\n    * Á¢≥Ê∞¥Ë∂ä‰ΩéÔºåÂØøÂëΩË∂äÁü≠ÔºõÁ¢≥Ê∞¥Ë∂äÈ´òÔºåÂØøÂëΩ‰πüËΩªÂæÆÁº©Áü≠ÔºõÁ¢≥Ê∞¥50%Â∑¶Âè≥ÔºàÂÖ∂ÂÆûÊåâÁÖß‰∏ÄËà¨ÁöÑËØ¥Ê≥ïÔºåËøô‰πüÁÆóÈ´òÁ¢≥Ê∞¥ÔºâÊòØÊúÄÈïøÂØøÂëΩÂå∫Èó¥ \n    * ![Á¢≥Ê∞¥](https://user-images.githubusercontent.com/2707039/163703985-a2e2f8ac-101a-4f3c-903b-6850507f144b.jpg)\n  * [ÊúÄÂº∫Ëê•ÂÖªÊê≠ÈÖçÔºÅBMJÔºöËøô‰πàÂêÉÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÂíåÊ≠ª‰∫°È£éÈô©Êõ¥‰Ωé](https://www.chinacdc.cn/gwxx/202003/t20200323_214639.html)\n* ÊßüÊ¶î\n  * [Â¶Ç‰ΩïÁúãÂæÖÊßüÊ¶îÂöºÂá∫Êù•ÁöÑÁôåÁóáÔºüÊßüÊ¶îËá¥ÁôåÈ£éÈô©Á©∂Á´üÊúâÂ§öÂ§ßÔºü - ‰∏ÅÈ¶ôÂåªÁîüÁöÑÂõûÁ≠î - Áü•‰πé](https://www.zhihu.com/question/312784161/answer/603370131)\n    * Âá∫Â§ÑÔºöChewing Betel Quid and the Risk of Metabolic Disease, Cardiovascular Disease, and All-Cause Mortality: A Meta-Analysis(https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070679)\n    * ÂöºÊßüÊ¶î‰ºöÂ¢ûÂä†21%ÁöÑÂÖ®Âõ†Ê≠ª‰∫°Áéá\n* ÁÉ≠ÈáèÈôêÂà∂\n  * [ÊÄé‰πàÁúãÂæÖBBC„ÄäËøõÈ£ü„ÄÅÊñ≠È£ü‰∏éÈïøÂØø„ÄãÔºü](https://www.zhihu.com/question/31395511)\n    * ÈôêÂà∂Âç°Ë∑ØÈáåÂä®Áâ©ÂÆûÈ™åÔºöCRÔºàÁÉ≠ÈáèÈôêÂà∂ÔºåÂç≥Â∞ëÂêÉÔºâÂª∂Ëøü‰∫ÜÊÅíÊ≤≥Áå¥ÁöÑÂ§öÁßçÁñæÁóÖÂèëÁóÖÂíåÊ≠ª‰∫°ÁéáÔºå‰∏éCRÂä®Áâ©Áõ∏ÊØîÔºåÊ≠£Â∏∏ÂñÇÂÖªÁöÑÁå¥Â≠êÁöÑÂêÑÁßçÁñæÁóÖÊÇ£ÁóÖÈ£éÈô©Â¢ûÂä†2.9ÂÄçÔºåÊ≠ª‰∫°È£éÈô©Â¢ûÂä†3.0ÂÄç„ÄÇ\n    * ![ÁÉ≠ÈáèÈôêÂà∂-ÊÅíÊ≤≥Áå¥](https://user-images.githubusercontent.com/2707039/163703988-8767185b-326a-4783-b2e2-f190322bb7d6.jpg)\n* ÁªºÂêà\n  * [ÊúÄÂº∫Ëê•ÂÖªÊê≠ÈÖçÔºÅBMJÔºöËøô‰πàÂêÉÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÂíåÊ≠ª‰∫°È£éÈô©Êõ¥‰Ωé](https://www.chinacdc.cn/gwxx/202003/t20200323_214639.html)\n  * [Associations of fat and carbohydrate intake with cardiovascular disease and mortality: prospective cohort study of UK Biobank participants](https://doi.org/10.1136/bmj.m688)\n    * ÈÄöËøáÂØπËøô‰∫õÂèÇ‰∏éËÄÖÁöÑÊï∞ÊçÆËøõË°åÂàÜÊûêÔºåÁ†îÁ©∂‰∫∫ÂëòÂèëÁé∞Á¢≥Ê∞¥ÂåñÂêàÁâ©ÔºàÁ≥ñ„ÄÅÊ∑ÄÁ≤âÂíåÁ∫§Áª¥ÔºâÂíåËõãÁôΩË¥®ÁöÑÊëÑÂÖ•‰∏éÂÖ®Âõ†Ê≠ª‰∫°ÁéáÂëàÈùûÁ∫øÊÄßÂÖ≥Á≥ªÔºåËÄåËÑÇËÇ™Âàô‰∏éÂÖ®Âõ†Ê≠ª‰∫°ÁéáÂëàÁ∫øÊÄßÁõ∏ÂÖ≥„ÄÇÂÖ∂‰∏≠ÔºåËæÉÈ´òÁöÑÁ≥ñÂàÜÊëÑÂÖ•‰∏éÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©ÂíåÊÇ£ÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÈ£éÈô©ËæÉÈ´òÂùáÊúâÂÖ≥ËÅîÔºåËÄåËæÉÈ´òÁöÑÈ•±ÂíåËÑÇËÇ™ÈÖ∏ÊëÑÂÖ•‰∏éÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©ËæÉÈ´òÊúâÂÖ≥„ÄÇ\n    * Âõæ1ÔºöÂêÑÁßçËê•ÂÖªÂÖÉÁ¥†‰∏éÂÖ®Âõ†Ê≠ª‰∫°‰πãÈó¥ÁöÑÂÖ≥Á≥ª\n    * ![ÂêÑÁßçËê•ÂÖªÂÖÉÁ¥†‰∏éÂÖ®Âõ†Ê≠ª‰∫°‰πãÈó¥ÁöÑÂÖ≥Á≥ª](https://user-images.githubusercontent.com/2707039/163702022-8c2bfea9-ed5d-4fe0-8ead-e8740014b92b.jpg)\n    * Âõæ2ÔºöÂêÑÁßçËê•ÂÖªÂÖÉÁ¥†‰∏éÂøÉË°ÄÁÆ°ÁñæÁóÖ‰πãÈó¥ÁöÑÂÖ≥Á≥ª\n    * ![ÂêÑÁßçËê•ÂÖªÂÖÉÁ¥†‰∏éÂøÉË°ÄÁÆ°ÁñæÁóÖ‰πãÈó¥ÁöÑÂÖ≥Á≥ª](https://user-images.githubusercontent.com/2707039/163702084-97fb4a03-707c-475d-b88e-6fe2f8e87f92.jpg)\n    * **Ëøõ‰∏ÄÊ≠•Á†îÁ©∂Ë°®ÊòéÔºåÂú®ÊâÄÊúâÁöÑÈ•ÆÈ£üÊ®°Âºè‰∏≠ÔºåÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈ£éÈô©ÊúÄ‰ΩéÁöÑÈ•ÆÈ£üÊñπÂºè‰∏∫Ôºö10-30gÈ´òÁ∫§Áª¥„ÄÅ14-30%ËõãÁôΩË¥®„ÄÅ10-25%Âçï‰∏çÈ•±ÂíåËÑÇËÇ™ÈÖ∏„ÄÅ5%-7%Â§ö‰∏çÈ•±ÂíåËÑÇËÇ™ÈÖ∏‰ª•Âèä20%-30%Ê∑ÄÁ≤âÊëÑÂÖ•„ÄÇ**\n    * **ÊúÄ‰ºòËÉΩÈáèÊù•Ê∫êÈÖçÊØîÔºö<24%Ê∑ÄÁ≤âÔºå15%-17%ËõãÁôΩË¥®Ôºå>15%Âçï‰∏çÈ•±ÂíåËÑÇËÇ™ÈÖ∏Ôºå<15%Á≥ñÔºå6%È•±ÂíåËÑÇËÇ™ÈÖ∏Ôºå6%Â§ö‰∏çÈ•±ÂíåËÑÇËÇ™ÈÖ∏Ôºå30g+È´òÁ∫§Áª¥**\n  * [BMJ | Â∏∏ÂêÉËñØÁâáÊ±âÂ†°Â∑ßÂÖãÂäõÁ≠âÈ£üÂìÅÔºåÂπ≥ÂùáÊ≠ª‰∫°Âπ¥ÈæÑ‰ªÖ‰ªÖ‰∏∫58Â≤ÅÔºåÊ≠ª‰∫°È£éÈô©ÂâßÂ¢û](https://med.ckcest.cn/details.html?id=5183272274855936&classesEn=news)\n    * [Rico-Camp√† A, Mart√≠nez-Gonz√°lez M A, Alvarez-Alvarez I, et al. Association between consumption of ultra-processed foods and all cause mortality: SUN prospective cohort study[J]. bmj, 2019, 365.](https://www.bmj.com/content/365/bmj.l1949.full)\n    * [Srour B, Fezeu L K, Kesse-Guyot E, et al. Ultra-processed food intake and risk of cardiovascular disease: prospective cohort study (NutriNet-Sant√©)[J]. bmj, 2019, 365.](https://www.bmj.com/content/365/bmj.l1451)\n    * [Lawrence M A, Baker P I. Ultra-processed food and adverse health outcomes[J]. bmj, 2019, 365.](https://www.researchgate.net/profile/Phillip-Baker-5/publication/333483796_Ultra-processed_food_and_adverse_health_outcomes/links/5f0c646ca6fdcc2f32336a95/Ultra-processed-food-and-adverse-health-outcomes.pdf)\n\n##### 6.1.2. Ê∂≤‰Ωì\n\n* ÁâõÂ•∂\n  * [„ÄäÊü≥Âè∂ÂàÄ„ÄãË∞ÉÁ†î21‰∏™ÂõΩÂÆ∂13‰∏á‰∫∫ÔºöÊØèÂ§©1Êñ§ÁâõÂ•∂ÊàñÈÖ∏Â•∂ÔºåÂøÉË°ÄÁÆ°Ê≠ª‰∫°È£éÈô©‰∏ãÈôç23%](https://www.sohu.com/a/253940257_419768)\n  * Âá∫Â§ÑÔºö[Association of dairy intake with cardiovascular disease and mortality in 21 countries from five continents (PURE): a prospective cohort study](http://mdrf-eprints.in/1114/1/Association_of_dietary_patterns_and_dietary_diversity_with_cardiometabolic_disease_risk_factors.pdf)\n  * ‰∏é‰∏çÈ£üÁî®‰π≥Âà∂ÂìÅÁöÑ‰∫∫Áõ∏ÊØîÔºåÊØèÂ§©ÊëÑÂÖ•‰∏§‰ªΩ‰π≥Âà∂ÂìÅÔºà‰∏Ä‰ªΩÊåá244ÂÖãÁâõÂ•∂/ÈÖ∏Â•∂Ôºå15ÂÖãÂ•∂ÈÖ™Êàñ5ÂÖãÈªÑÊ≤πÔºâÁöÑ‰∫∫Ôºå**ÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©‰∏ãÈôç‰∫Ü17%**ÔºåÂøÉË°ÄÁÆ°Ê≠ª‰∫°È£éÈô©‰∏ãÈôç23%Ôºå‰∏≠È£éÈ£éÈô©‰∏ãÈôç33%\n* Ëå∂\n  * [10‰∏á‰∏≠ÂõΩ‰∫∫ÈöèËÆø7Âπ¥ÂèëÁé∞ÔºåÊØèÂë®Âñù‰∏âÊ¨°Ëå∂‰∏éÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Èôç‰Ωé15%ÔºåÈ¢ÑÊúüÂØøÂëΩÂ¢ûÂä†1.26Âπ¥Áõ∏ÂÖ≥ ](https://www.jianshu.com/p/5461a205cf95?utm_campaign=hugo)\n  * Âá∫Â§ÑÔºö[Tea consumption and the risk of atherosclerotic cardiovascular disease and all-cause mortality: The China-PAR project](https://www.researchgate.net/profile/Fangchao-Liu-4/publication/338483323_Tea_consumption_and_the_risk_of_atherosclerotic_cardiovascular_disease_and_all-cause_mortality_The_China-PAR_project/links/5e55e5e94585152ce8efe511/Tea-consumption-and-the-risk-of-atherosclerotic-cardiovascular-disease-and-all-cause-mortality-The-China-PAR-project.pdf)\n  * [‰∏≠ÂõΩÊàêÂπ¥‰∫∫È•ÆËå∂‰∏éÊ≠ª‰∫°È£éÈô©ÁöÑÂâçÁûªÊÄßÂÖ≥ËÅîÁ†îÁ©∂](http://rs.yiigle.com/CN112338202202/1351516.htm)\n  * Á∫≥ÂÖ•ÂàÜÊûêÁöÑ438 443‰æãÁ†îÁ©∂ÂØπË±°ÈöèËÆø11.1Âπ¥ÂÖ±ÂèëÁîüÊ≠ª‰∫°34 661‰æã„ÄÇ‰∏é‰ªé‰∏çÈ•ÆËå∂ËÄÖÁõ∏ÊØîÔºåÂΩìÂâçÈùûÊØèÊó•È•ÆËå∂ËÄÖÂíåÊØèÊó•È•ÆËå∂ËÄÖÂÖ®Âõ†Ê≠ª‰∫°HRÂÄºÔºà95%CIÔºâ‰æùÊ¨°‰∏∫0.89Ôºà0.86-0.91ÔºâÂíå0.92Ôºà0.88-0.95Ôºâ„ÄÇÂàÜÊÄßÂà´ÂàÜÊûêÊòæÁ§∫ÔºåÈ•ÆËå∂ÂØπÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©ÁöÑ‰øùÊä§‰ΩúÁî®‰∏ªË¶ÅËßÅ‰∫éÁî∑ÊÄßÔºà‰∫§‰∫íP<0.05Ôºâ\n* Êó†Á≥ñÔºàÁîúÂë≥ÔºâÈ•ÆÊñô\n  * [„ÄåÊó†Á≥ñÈ•ÆÊñô‰ΩøÊ≠ª‰∫°È£éÈô©Â¢ûÂä† 26 %„ÄçÔºåÊòØÁúüÁöÑÂêóÔºü](https://www.zhihu.com/question/418598272/answer/1450648364)\n    * Áõ∏ÊØî‰∫éËΩØÈ•ÆÊñôÊëÑÂÖ•ÈáèÔºú1ÊùØ/ÊúàÁöÑÂèÇ‰∏éËÄÖÔºåÊ∑∑ÂêàËΩØÈ•ÆÊñôÊëÑÂÖ•‚â•1ÊùØ/Â§©ÁöÑÂèÇ‰∏éËÄÖÊ≠ª‰∫°È£éÈô©Â¢ûÂä†18%ÔºåËÄå**ÊëÑÂÖ•Âê´Á≥ñËΩØÈ•ÆÊñôÊàñÊó†Á≥ñËΩØÈ•ÆÊñô‰ºö‰ª§Ê≠ª‰∫°È£éÈô©ÂàÜÂà´Â¢ûÂä†11%Âíå27%„ÄÇ**\n    * ![È•ÆÊñô](https://user-images.githubusercontent.com/2707039/163704346-e7d92e7f-eba5-4673-8f15-3a96782c2e32.png)\n  * [Association Between Soft Drink Consumption and Mortality in 10 European Countries](https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2749350)\n* ÊúâÁ≥ñÈ•ÆÊñô\n  * [ÂèØ‰πêÂíåÂ•∂Ëå∂ÔºåÂ¢ûÂä†ÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈ´òËææ62%ÔºÅÊûúÊ±ÅÈôç‰ΩéÂÖçÁñ´ÂäõÔºåÂΩ±ÂìçËÇù‰ª£Ë∞¢ÔºÅÂê´Á≥ñÈ•ÆÊñôÈÇ£‰∫õ‰∫ã](https://zhuanlan.zhihu.com/p/400746073)\n    * ÊØèÂ§©1ÊùØÂê´Á≥ñÈ•ÆÊñôÂ¢ûÂä†7%ÂÖ®Âõ†Ê≠ª‰∫°ÁéáÔºå2ÊùØ21%\n    * Âú®34Âπ¥ÁöÑÈöèËÆø‰∏≠ÔºåÁ†îÁ©∂‰∫∫ÂëòÂèëÁé∞ÔºåÁõ∏ÊØîÈÇ£‰∫õ‰∏Ä‰∏™ÊúàÂñù1ÊùØÊàñËÄÖÊõ¥Â∞ëÂê´Á≥ñÈ•ÆÊñôÁöÑ‰∫∫ÔºåÊØèÂ§©Âñù2ÊùØÁöÑ‰∫∫ÊÄª‰ΩìÊ≠ª‰∫°È£éÈô©ÂçáÈ´ò‰∫Ü21%ÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÊ≠ª‰∫°È£éÈô©ÂçáÈ´ò‰∫Ü31%ÔºåÁôåÁóáÊ≠ª‰∫°È£éÈô©‰∏äÂçá‰∫Ü16%„ÄÇ\n    * Âè™Ë¶ÅÊØèÂ§©Â§öÂñù‰∏ÄÊùØÂê´Á≥ñÈ•ÆÊñôÔºåÊÄª‰ΩìÊ≠ª‰∫°È£éÈô©Â∞ÜÂ¢ûÂä†7%ÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÈ£éÈô©Â∞ÜÂ¢ûÂä†10%ÔºåÁôåÁóáÁõ∏ÂÖ≥ÁöÑÊ≠ª‰∫°È£éÈô©Â∞Ü16%„ÄÇ\n    * ÂèëË°®Âú®ÂõΩÈôÖÈ°∂Á∫ßÊúüÂàä„ÄäBMJ„Äã‰∏äÁöÑ‰∏ÄÁØáËÆ∫ÊñáÂ∞±ËØÅÊòé‰∫ÜÂê´Á≥ñÈ•ÆÊñô‰ºöÂú®Â¢ûÂä†ÊÇ£ÁôåÈ£éÈô©ÔºåÂΩìÁÑ∂ËøôÁØáÊñáÁ´†È™åËØÅÁöÑ‰∏ç‰ªÖ‰ªÖÊòØÊûúÊ±ÅÔºåÂ•∂Ëå∂‰πüÊúâ‰ªΩ‚Äî‚ÄîÂíåÂê´Á≥ñÈ•ÆÊñôÁõ∏ÂÖ≥ÁöÑÊÄª‰ΩìÊÇ£ÁôåÈ£éÈô©Ë¶ÅÈ´òÂá∫ÈÄöÂ∏∏ÂÄº18%Ôºå100%ÁöÑÈ≤úÊ¶®ÊûúÊ±Å‰πü‰ºö‰ΩøÂæóÊï¥‰ΩìÁöÑÊÇ£ÁôåÈ£éÈô©‰∏äÂçá12%„ÄÇ\n* ÊûúÊ±Å\n  * [JAMAÂ≠êÂàäÔºö100%Á∫ØÊûúÊ±ÅÂèØËÉΩÊØîÂê´Á≥ñÈ•ÆÊñôÊõ¥Âç±Èô©](https://zhuanlan.zhihu.com/p/66513350)\n    * ÊØèÂ§©Â§öÊëÑÂÖ•‰∏Ä‰ªΩ12ÁõéÂè∏ÁöÑÂê´Á≥ñÈ•ÆÊñôÔºåÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈ£éÈô©Â¢ûÂä†11%Ôºõ\n    * ÊØèÂ§©Â§öÊëÑÂÖ•‰∏Ä‰ªΩ12ÁõéÂè∏ÁöÑÊûúÊ±ÅÔºåÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈ£éÈô©Â¢ûÂä†24%„ÄÇ\n* ÂíñÂï°\n  * [ÈáçÁ£ÖÔºÅÂ§öÁØáÁ†îÁ©∂ËØÅÂÆûÂñùÂíñÂï°‰∏é‰∫∫Áæ§ÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈôç‰ΩéÁõ¥Êé•Áõ∏ÂÖ≥](https://news.bioon.com/article/6725420.html)\n  * [ÁßëÊôÆ | ÂñùÂíñÂï°ÂèàÂ§ö‰∫Ü‰∏Ä‰∏™Êñ∞ÁêÜÁî±ÔºöÈôç‰ΩéÊ≠ª‰∫°ÁéáÔºÅ ](https://www.sohu.com/a/439412995_100003595)\n  * [Âú∞‰∏≠Êµ∑ÊàêÂπ¥‰∫∫ÂíñÂï°Ê∂àËÄóÈáèÂèäÂÖ®Âõ†ÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÂíåÁôåÁóáÁöÑÊ≠ª‰∫°Áéá](https://fanyi.pdf365.cn/help/249)\n    * Âú®ÊúÄËøëÁöÑËçüËêÉÂàÜÊûê‰∏≠ÔºåËØ•Á†îÁ©∂ÂåÖÊã¨Êù•Ëá™‰∏çÂêåÂõΩÂÆ∂ÁöÑ40È°πÁ†îÁ©∂Âíå3,852,651ÂêçÂèóËØïËÄÖ„ÄÇÂú®ËøôÈ°πËçüËêÉÂàÜÊûêÊòæÁ§∫ÔºåÂíñÂï°ÊëÑÂÖ•Èáè‰∏éÂêÑÁßçÂéüÂõ†ÁöÑÊ≠ª‰∫°ÁéáÔºåCVDÂíåÁôåÁóáÊ≠ª‰∫°Áéá‰πãÈó¥Â≠òÂú®ÈùûÁ∫øÊÄßÂÖ≥Á≥ªÔºåÊØèÂ§©ÊëÑÂÖ•‰∏§ÊùØÂíñÂï°ÁöÑÁôåÁóáÊ≠ª‰∫°ÁéáÊúÄ‰Ωé(RR = 0.96)ÔºåCVDÊúÄ‰ΩéÁöÑÊ≠ª‰∫°ÁéáÔºåÊØèÂ§©2.5ÊùØ(RR= 0.83)ÔºåÂÖ®Â§©ÊúÄ‰ΩéÊ≠ª‰∫°Áéá‰∏∫ÊØèÂ§©3.5ÊùØ(RR= 0.85)ÔºåÂπ∂‰∏îÈöèÁùÄÂíñÂï°Ê∂àË¥πÈáèÁöÑÂ¢ûÂä†ÔºåÊ≠ª‰∫°ÁéáÊ≤°ÊúâËøõ‰∏ÄÊ≠•Èôç‰ΩéÊàñÂ¢ûÂä†\n* ‰∫öÁ≤æËÉ∫\n  * [ScienceÔºöÁßëÂ≠¶ËÉå‰π¶ÔºÅ‰ªéÁ≤æÊ∂≤‰∏≠ÂèëÁé∞ÁöÑ‰∫öÁ≤æËÉ∫ÔºåÁ´üÁÑ∂ÊúâÁùÄÊäóË°∞ËÄÅ„ÄÅÊäóÁôå„ÄÅ‰øùÊä§ÂøÉË°ÄÁÆ°ÂíåÁ•ûÁªè„ÄÅÊîπÂñÑËÇ•ËÉñÂíå2ÂûãÁ≥ñÂ∞øÁóÖÁ≠âÈÄÜÂ§©Á•ûÊïà](https://www.medsci.cn/article/show_article.do?id=420d12904103)\n  * [È•ÆÈ£ü‰∏≠‰∫öÁ≤æËÉ∫ÊëÑÂÖ•ÈáèÈ´ò‰ºöÈôç‰ΩéÊ≠ª‰∫°Áéá](https://zhuanlan.zhihu.com/p/388942219)\n\n##### 6.1.3. Ê∞î‰Ωì\n\n* Âê∏ÁÉü\n  * [Âç≥‰ΩøÊòØ‰ΩéÂº∫Â∫¶Âê∏ÁÉüÔºå‰πüÂ¢ûÂä†Ê≠ª‰∫°È£éÈô©ÔºÅ](https://www.medsci.cn/article/show_article.do?id=02ca2083319b)\n    * Á†îÁ©∂ÂèëÁé∞ÔºöÂú®42 416ÂêçÁî∑ÊÄßÂíå86 735ÂêçÂ•≥ÊÄßÔºàÂπ¥ÈæÑÂú®35-89Â≤Å‰πãÈó¥Ôºå‰ª•ÂâçÊ≤°ÊúâÊÇ£ÁóÖÔºâ‰∏≠Ôºå18 985ÂêçÁî∑ÊÄßÔºà45%ÔºâÂíå18 072ÂêçÂ•≥ÊÄßÔºà21%ÔºâÁõÆÂâçÂê∏ÁÉüÔºåÂÖ∂‰∏≠33%ÁöÑÁî∑ÊÄßÂê∏ÁÉüËÄÖÂíå39%ÁöÑÂ•≥ÊÄßÂê∏ÁÉüËÄÖÂπ∂‰∏çÊØèÂ§©Âê∏ÁÉü„ÄÇ8866ÂêçÁî∑ÊÄßÔºà21%ÔºâÂíå53 912ÂêçÂ•≥ÊÄßÔºà62%Ôºâ‰ªé‰∏çÂê∏ÁÉü„ÄÇÂú®ÈöèËÆøÊúüÈó¥Ôºå‰∏é‰ªé‰∏çÂê∏ÁÉüÁõ∏ÊØîÔºåÊØèÂ§©<10ÊîØÁÉüÊàñÊØèÂ§©‚â•10ÊîØÁÉüÁöÑÂÖ®Âõ†Ê≠ª‰∫°ÁéáÂç±Èô©ÊØîÂàÜÂà´‰∏∫1.17Ôºà95%ÁΩÆ‰ø°Âå∫Èó¥1.10-1.25ÔºâÂíå1.54Ôºà1.42-1.67Ôºâ„ÄÇÊó†ËÆ∫Âπ¥ÈæÑÊàñÊÄßÂà´ÔºåÂç±Èô©ÊØîÁõ∏‰ºº„ÄÇ‰∏éÊØèÊó•Âê∏ÁÉüÂÖ≥Á≥ªÊúÄÂØÜÂàáÁöÑÁñæÁóÖÊòØÂëºÂê∏ÈÅìÁôåÁóá„ÄÅÊÖ¢ÊÄßÈòªÂ°ûÊÄßËÇ∫ÁóÖÂíåËÉÉËÇ†ÈÅìÂèäË°ÄÁÆ°ÁñæÁóÖ„ÄÇÂú®ÊãõÂãüÊó∂Â∑≤ÁªèÊàíÁÉüÁöÑ‰∫∫ÁöÑÊ≠ª‰∫°Áéá‰Ωé‰∫éÁé∞Âú®ÊØèÂ§©Âê∏ÁÉüËÄÖ„ÄÇ\n    * Âê∏ÁÉüËÄÖÂπ≥ÂùáÂáèÂ∞ëÂØøÂëΩ11-12Âπ¥\n  * [Âê∏ÁÉüËÆ©‰∫∫ËøáÁòæÊòØ‰ªÄ‰πàÂéüÁêÜÔºüÊúâËäÇÂà∂ÁöÑÂê∏ÁÉü‰æùÊóßÊúâÂÆ≥ÂêóÔºü](https://www.zhihu.com/question/24846224/answer/1719798177)\n\n##### 6.1.4. ÂÖâÁÖß\n\n* ÊôíÂ§™Èò≥\n  * [ÊôíÂ§™Èò≥ÂíåÊ≠ª‰∫°ÁéáÁöÑÂÖ≥Á≥ªÔºåÂ¶Ç‰ΩïÁßëÂ≠¶ÔºåÂÆâÂÖ®ÁöÑÊôíÂ§™Èò≥Ôºü\n](https://zhuanlan.zhihu.com/p/76301306)\n    * ‰∏πÈ∫¶‰∏ÄÈ°πÈïøËææ26Âπ¥ÁöÑÁ†îÁ©∂ÂèëÁé∞ÔºåÂ§öÊôíÂ§™Èò≥ËÉΩÊòæËëóÂª∂ÈïøÂØøÂëΩÔºåÂç≥‰ΩøÊòØÁî±‰∫éËøáÂ∫¶Êö¥ÊôíËØ±ÂèëÁöÆËÇ§ÁôåÁöÑÊÇ£ËÄÖÔºåÂπ≥ÂùáÂØøÂëΩ‰πüÊØîÊôÆÈÄö‰∫∫Èïø‰∫Ü6Â≤Å„ÄÇ\n\n##### 6.1.5. ËçØÁâ©\n\n* NMN\n* ‰∫åÁî≤ÂèåËÉç\n  * [‚ÄúËÉç‚ÄùÂêπÂøÖÁúã ‰∏®ÊàëÂ∞±ÊòØÁ•ûËçØ‚Äî‚Äî‰∫åÁî≤ÂèåËÉç](https://zhuanlan.zhihu.com/p/419202902)\n    * ‰∫åÁî≤ÂèåËÉç‰∏ç‰ªÖÂú®Â§öÁßçËÇøÁò§„ÄÅÂøÉË°ÄÁÆ°ÁñæÁóÖÂèäÁ≥ñÂ∞øÁóÖ‰∏≠ÂèëÊå•‰øùÊä§‰ΩúÁî®ÔºåËÄå‰∏îÂú®ËÇ•ËÉñ„ÄÅËÇùÁóÖ„ÄÅËÇæÁóÖÂèäË°∞ËÄÅÊñπÈù¢‰πüÂ§ßÊîæÂºÇÂΩ©„ÄÇ\n  * [‰∫åÁî≤ÂèåËÉç2020ÊúÄÂÄºÂæó‰∫ÜËß£ÁöÑ‚ÄúÂêÉÁìú‚ÄùÂ§ßÊñ∞Èóª‚Äî‚ÄîÊä§ËÉÉ„ÄÅÂÅ•ËÑë„ÄÅÊäóË°∞„ÄÅÈò≤ÁôåËøòÊòØËá¥ÁôåÔºü](https://zhuanlan.zhihu.com/p/357807109)\n  * [‰∫åÁî≤ÂèåËÉçÁúüÁöÑÈÇ£‰πàÁ•ûÂêóÔºüÁæéÁ†îÁ©∂ÔºöÁà∂‰∫≤ÊúçÁî®‰∫åÁî≤ÂèåËÉçÊàñËá¥Â≠êÂ•≥ÊúâÁº∫Èô∑](https://baijiahao.baidu.com/s?id=1729999374705305768)\n  * ![‰∫åÁî≤ÂèåËÉç](https://user-images.githubusercontent.com/2707039/163702325-5d427542-9ae5-4311-8979-d0d326a9832f.jpg)\n  * ‰∏çËâØÂèçÂ∫î\n    * ‰Ωú‰∏∫‰∏ÄÁßç‰ΩøÁî®ËøëÁôæÂπ¥ÁöÑËçØÁâ©Ôºå‰∫åÁî≤ÂèåËÉçÁöÑ‰∏çËâØÂèçÂ∫îÂ∑≤ÁªèÈùûÂ∏∏ÊòéÁ°ÆÔºåÂ∏∏ËßÅÁöÑÊúâÔºöÁª¥ÁîüÁ¥†B12Áº∫‰πèÔºà7%-17.4%ÔºâÔºåËÉÉËÇ†ÈÅì‰∏çËâØÂèçÂ∫îÔºàÊúÄÈ´ò53%ÔºâÔºåÁñ≤ÂÄ¶Ôºà9%ÔºâÔºåÂ§¥ÁóõÔºà6%ÔºâÔºõ‰∏•Èáç‰ΩÜ‰∏çÂ∏∏ËßÅÁöÑ‰∏çËâØÂèçÂ∫îÂåÖÊã¨‰π≥ÈÖ∏ÈÖ∏‰∏≠ÊØí„ÄÅËÇùÊçü‰º§Ôºõ‰πüÊúâÁ†îÁ©∂Ë°®ÊòéÂèØËÉΩÂØπËÉéÂÑøËá¥Áï∏\n* Â§çÂêàÁª¥ÁîüÁ¥†\n  * [ÊúçÁî®Â§çÂêàÁª¥ÁîüÁ¥†ÂèØÈôç‰ΩéÁôåÁóáÂç±Èô©8%ÔºåÂÖ∂‰ªñÊïàÊûúÂπ∂‰∏çÊòæËëó](https://health.qq.com/a/20121023/000026.htm)\n* Ëë°ËêÑÁ≥ñËÉ∫\n  * [Á•ûÂ•áÔºÅÊ∞®Á≥ñÈôç‰ΩéÂøÉË°ÄÁÆ°Ê≠ª‰∫°Áéá65%Ôºå‰∏éÂÆöÊúüËøêÂä®ÊïàÊûúÁõ∏ÂΩì](https://www.sohu.com/a/436372221_120873241)\n  * ÁæéÂõΩË•øÂºóÂêâÂ∞º‰∫öÂ§ßÂ≠¶ÊúÄÊñ∞Á†îÁ©∂ÂèëÁé∞ Ê∞®Á≥ñÔºàËΩØÈ™®Á¥†Ôºâ ÂèØ‰ª•Èôç‰ΩéÂøÉË°ÄÁÆ°Ê≠ª‰∫°Áéá65%ÔºåÈôç‰ΩéÊÄª‰ΩìÊ≠ª‰∫°Áéá39%ÔºåÊïàÊûú‰∏éÂùöÊåÅÂÆöÊúüËøêÂä®Áõ∏ÂØπ\n  * ËØ•Á†îÁ©∂‰ΩøÁî®1999Âπ¥Ëá≥2010Âπ¥Ôºå16,686ÂêçÊàêÂπ¥‰∫∫ÁöÑÂõΩÂÆ∂ÂÅ•Â∫∑ÂíåËê•ÂÖªÊ£ÄÊü•(NHANES)Êï∞ÊçÆÔºåÂèÇ‰∏éËÄÖÁöÑ‰∏≠‰ΩçËøΩË∏™Êó∂Èó¥‰∏∫107‰∏™ÊúàÔºåËÄåÂÖ∂‰∏≠Êúâ648‰ΩçÂèÇ‰∏éËÄÖÂÆöÊúü‰∏îÊØèÊúçÁî®Êó•500-1000ÊØ´ÂÖãÁöÑËë°ËêÑÁ≥ñËÉ∫/ËΩØÈ™®Á¥†‰∏ÄÂπ¥‰ª•‰∏ä„ÄÇ\n* ‰∫öÁ≤æËÉ∫\n  * [ScienceÔºöÁßëÂ≠¶ËÉå‰π¶ÔºÅ‰ªéÁ≤æÊ∂≤‰∏≠ÂèëÁé∞ÁöÑ‰∫öÁ≤æËÉ∫ÔºåÁ´üÁÑ∂ÊúâÁùÄÊäóË°∞ËÄÅ„ÄÅÊäóÁôå„ÄÅ‰øùÊä§ÂøÉË°ÄÁÆ°ÂíåÁ•ûÁªè„ÄÅÊîπÂñÑËÇ•ËÉñÂíå2ÂûãÁ≥ñÂ∞øÁóÖÁ≠âÈÄÜÂ§©Á•ûÊïà](https://www.medsci.cn/article/show_article.do?id=420d12904103)\n  * ‰∫öÁ≤æËÉ∫ÊòØÊúÄÂÆπÊòì‰ªé‰∫∫‰ΩìËÇ†ÈÅìÂê∏Êî∂ÁöÑÂ§öËÉ∫„ÄÇËÆ∏Â§öÁöÑÈ£üÁâ©‰∏≠ÈÉΩÂê´ÊúâÂ§ßÈáèÁöÑ‰∫öÁ≤æËÉ∫Ôºå‰æãÂ¶ÇÊñ∞È≤úÁöÑÈùíÊ§í„ÄÅÂ∞èÈ∫¶ËÉöËäΩ„ÄÅËä±Ê§∞Ëèú„ÄÅË•øÂÖ∞Ëä±„ÄÅËòëËèáÂíåÂêÑÁßçÂ•∂ÈÖ™ÔºåÂ∞§ÂÖ∂Âú®Á∫≥Ë±ÜÁ≠âÂ§ßË±ÜÂà∂ÂìÅ„ÄÅÈ¶ôËèáÂíåÊ¶¥Ëé≤‰∏≠Âê´ÈáèÊõ¥È´ò„ÄÇÂú®Êú¨ÂÆûÈ™å‰∏≠ÔºåÁ†îÁ©∂‰∫∫ÂëòÈÄâÊã©‰∫Ü829‰ΩçÂπ¥ÈæÑÂú®45-84Â≤Å‰πãÈó¥ÁöÑÂèÇ‰∏éËÄÖËøõË°å‰∫Ü‰∏∫Êúü20Âπ¥ÁöÑÈöèËÆøÔºåÂàÜÊûê‰∫ÜÈ•ÆÈ£ü‰∏≠‰∫öÁ≤æËÉ∫ÊëÑÂÖ•Èáè‰∏é‰∫∫Á±ªÊ≠ª‰∫°Áéá‰πãÈó¥ÁöÑÊΩúÂú®ÂÖ≥ËÅî„ÄÇ\n  * Á†îÁ©∂ÂèëÁé∞ÔºåÂ•≥ÊÄßÁöÑ‰∫öÁ≤æËÉ∫ÊëÑÂÖ•ÈáèÈ´ò‰∫éÁî∑ÊÄßÔºåÂπ∂‰∏îÊëÑÂÖ•ÈáèÈÉΩ‰ºöÈöèÁùÄÂπ¥ÈæÑÁöÑÂ¢ûÈïøËÄå‰∏ãÈôç„ÄÇ‰∫öÁ≤æËÉ∫ÁöÑ‰∏ªË¶ÅÊù•Ê∫êÊòØÂÖ®Ë∞∑Áâ©ÔºàÂç†13.4%Ôºâ„ÄÅËãπÊûúÂíåÊ¢®ÔºàÂç†13.3%Ôºâ„ÄÅÊ≤ôÊãâÔºàÂç†9.8%Ôºâ„ÄÅËäΩËèúÔºàÂç†7.3%ÔºâÂíåÈ©¨ÈìÉËñØÔºàÂç†6.4%Ôºâ„ÄÇÁ†îÁ©∂Ê†πÊçÆ‰∫öÁ≤æËÉ∫ÊëÑÂÖ•ÈáèÂ∞Ü‰∫∫Áæ§ÂàÜ‰∏∫‰∏âÁªÑÔºå‰ΩéÊëÑÂÖ•ÈáèÁªÑÔºà<62.2 ¬µmol / dÔºâ„ÄÅ‰∏≠ÊëÑÂÖ•ÈáèÁªÑÔºà62.2‚Äì79.8 ¬µmol / dÔºâÂíåÈ´òÊëÑÂÖ•ÈáèÁªÑÔºà> 79.8 ¬µmol / dÔºâ„ÄÇÈöèËÆøÊúüÈó¥ÂÖ±ËÆ∞ÂΩï‰∫Ü341‰æãÊ≠ª‰∫°ÔºåÂÖ∂‰∏≠Ë°ÄÁÆ°ÁñæÁóÖ137‰æãÔºåÁôåÁóá94‰æãÔºåÂÖ∂‰ªñÂéüÂõ†110‰æã„ÄÇÁªèËÆ°ÁÆó‰Ωé‰∏≠È´ò‰∏âÁªÑÁöÑÁ≤óÁï•Ê≠ª‰∫°ÁéáÂàÜÂà´‰∏∫40.5%„ÄÅ23.7%Âíå15.1%ÔºåËøô‰∫õÊï∞ÊçÆË°®Êòé‰∫öÁ≤æËÉ∫ÊëÑÂÖ•Èáè‰∏éÂÖ®Âõ†Ê≠ª‰∫°Áéá‰πãÈó¥ÁöÑË¥üÁõ∏ÂÖ≥ÂÖ≥Á≥ªÊòæËëó„ÄÇÈöèÁùÄÈÄêÊ≠•ÂØπÂπ¥ÈæÑ„ÄÅÊÄßÂà´ÂíåÁÉ≠ÈáèÁöÑÊØî‰æãËøõË°åË∞ÉÊï¥ÔºåËøôÁßçÁõ∏ÂÖ≥ÂÖ≥Á≥ª‰æùÁÑ∂ÊòæËëó„ÄÇ\n* ÁªºÂêà\n  * [„ÄäËá™ÁÑ∂„ÄãÂ≠êÂàäÊ∑±Â∫¶ÁªºËø∞ÔºöÂ¶Ç‰ΩïÂºÄÂèëÊäóË°∞ËÄÅËçØ](https://zhuanlan.zhihu.com/p/145495570)\n  * ![Â¶Ç‰ΩïÂºÄÂèëÊäóË°∞ËÄÅËçØ](https://user-images.githubusercontent.com/2707039/163702474-205baeec-f0ce-4e8d-96a4-36efe47534de.jpg)\n\n#### 6.2. ËæìÂá∫\n\n##### 6.2.1. Êå•ÊãçËøêÂä®\n\n* [Âì™ÁßçËøêÂä®ÊÄß‰ª∑ÊØîÊúÄÈ´òÔºüÊùÉÂ®ÅÂåªÂ≠¶ÊùÇÂøó‚ÄúÊü≥Âè∂ÂàÄ‚ÄùÁªôÂá∫Á≠îÊ°à‰∫Ü ](https://www.sohu.com/a/535581770_121124216)\n  * ‰∏ÄÂë®‰∏âÊ¨°ÔºåÊØèÊ¨°45-60ÂàÜÈíüÔºåÊå•ÊãçËøêÂä®ÔºåÈôç‰Ωé~47%ÂÖ®Âõ†Ê≠ª‰∫°Áéá\n  * ÁæΩÊØõÁêÉ„ÄÅ‰πí‰πìÁêÉ„ÄÅÁΩëÁêÉÁ≠âÈÉΩÁÆóÊå•ÊãçËøêÂä®Ôºå‰ΩÜÁî±‰∫éË•øÂåñÁ†îÁ©∂ËÉåÊôØÔºåÂèØËÉΩÊåáÁΩëÁêÉÊõ¥Â§ö„ÄÇËøôÈöêÂºèÁöÑË°®Ëææ‰∫ÜÂÖ®Ë∫´ÈîªÁÇºÊõ¥‰∏∫ÈáçË¶Å\n\n##### 6.2.2. ÂâßÁÉàËøêÂä®\n\n* [Êñ∞Á†îÁ©∂ÔºöÊØèÂ§©ÂâßÁÉàËøêÂä®8ÂàÜÈíüÔºåÂèØÈôç‰ΩéÂÖ®Âõ†Ê≠ª‰∫°ÂíåÂøÉËÑèÁóÖÈ£éÈô©](https://academic.oup.com/eurheartj/advance-article/doi/10.1093/eurheartj/ehac572/6771381)\n  * ÊØèÂë®15-20ÂàÜÈíüÁöÑÂâßÁÉàËøêÂä®ÔºåÈôç‰Ωé16-40%ÁöÑÂÖ®Âõ†Ê≠ª‰∫°ÁéáÔºåÂâßÁÉàËøêÂä®Êó∂Èó¥ËææÂà∞50-57ÂàÜÈíü/Âë®ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Èôç‰ΩéÂÖ®Âõ†Ê≠ª‰∫°Áéá„ÄÇËøô‰∫õÂèëÁé∞Ë°®ÊòéÔºåÈÄöËøáÂú®‰∏ÄÂë®ÁöÑÁü≠Êó∂Èó¥ÂÜÖÁ¥ØÁßØÁõ∏ÂØπÂ∞ëÈáèÁöÑÂâßÁÉàËøêÂä®ÂèØ‰ª•Èôç‰ΩéÂÅ•Â∫∑È£éÈô©„ÄÇ\n  \n##### 6.2.3. Ëµ∞Ë∑Ø\n\n* [Ëµ∞Ë∑ØÈôç‰ΩéÂÖ®Âõ†Ê≠ª‰∫°ÁéáË∂ÖËøá50%ÔºÅÊØèÂ§©Ëµ∞Â§öÂ∞ëÊ≠•ÊúÄÂêàÈÄÇÔºü„ÄäJAMA„ÄãÂ≠êÂàäË∂Ö10Âπ¥Á†îÁ©∂ÂëäËØâ‰Ω†Á≠îÊ°à](http://www.shcell.org/219/3571.html)\n  * ![Ëµ∞Ë∑ØÈôç‰ΩéÂÖ®Âõ†Ê≠ª‰∫°Áéá](https://user-images.githubusercontent.com/2707039/163704147-afec1c79-799b-4db8-b547-1a2431d504c9.jpg)\n  * Ê≥®1ÔºöËøôÈ°πÁ†îÁ©∂ÂèÇ‰∏éËÄÖÁöÑÂπ≥ÂùáÂπ¥ÈæÑ‰∏∫45.2Â≤Å\n  * Ê≥®2ÔºöÂπ≥ÂùáÊ≠•Êï∞ÁöÑÂ§öÂ∞ë‰∏éËÅå‰∏öÊúâÂÖ≥ÔºåÊ≠§È°πÁ†îÁ©∂‰ªÖË°®ÊòéÁõ∏ÂÖ≥ÊÄßÔºåËøòÊ≤°ÊúâÊõ¥Ê∑±Â∫¶ÁöÑÂõ†ÊûúÂàÜÊûê\n\n##### 6.2.4. Âà∑Áâô\n\n* [50‰∏áÂõΩ‰∫∫Á†îÁ©∂ËØÅÂÆûÔºö‰∏çÂ•ΩÂ•ΩÂà∑ÁâôÔºåËá¥ÁôåÔºÅË°ÄÁÆ°ÁñæÁóÖ‰πü‰ºöÂ¢ûÂ§öÔºÅ](https://www.cn-healthcare.com/articlewm/20211209/content-1293760.html)\n  * ÁªèÂ∏∏‰∏çÂà∑ÁâôÁöÑ‰∫∫ÔºöÁôåÁóá„ÄÅÊÖ¢ÊÄßÈòªÂ°ûÊÄßËÇ∫ÁóÖÂèäËÇùÁ°¨ÂåñÈ£éÈô©ÂàÜÂà´Â¢ûÂä†‰∫Ü9%„ÄÅ12%Âíå25%ÔºåËøáÊó©Ê≠ª‰∫°È£éÈô©Â¢ûÂä†25%„ÄÇ\n\n##### 6.2.5. Ê≥°Êæ°\n\n* [ÂÆöÊúüÊ¥óÊæ°Èôç‰ΩéÂøÉË°ÄÁÆ°ÁñæÁóÖÂèë‰ΩúÈ£éÈô©](https://www.cn-healthcare.com/article/20200326/content-533379.html)\n  * ‰∏éÊØèÂë®‰∏ÄËá≥‰∏§Ê¨°Ê≥°Êæ°ÊàñÊ†πÊú¨‰∏çÊ≥°Êæ°Áõ∏ÊØîÔºåÊØèÂ§©Ê¥óÁÉ≠Ê∞¥Êæ°ÂèØ‰ª•Èôç‰Ωé28%ÁöÑÂøÉË°ÄÁÆ°ÁñæÁóÖÊÄªÈ£éÈô©ÔºåÈôç‰Ωé26%ÁöÑ‰∏≠È£éÊÄªÈ£éÈô©ÔºåËÑëÂá∫Ë°ÄÈ£éÈô©‰∏ãÈôç46%„ÄÇËÄåÊµ¥Áº∏Êµ¥ÁöÑÈ¢ëÁéá‰∏éÂøÉÊ∫êÊÄßÁåùÊ≠ªÁöÑÈ£éÈô©Â¢ûÂä†Êó†ÂÖ≥„ÄÇ\n\n##### 6.2.6. ÂÅöÂÆ∂Âä°ÔºàËÄÅÂπ¥Áî∑ÊÄßÔºâ\n\n* [Housework Reduces All-Cause and Cancer Mortality in Chinese Men](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0061529)\n  * 72Â≤Å‰πãÂêéÁî∑ÊÄßÊØèÂë®ÂÅöÈáçÂûãÂÆ∂Âä°ÂèØ‰ª•ÂáèÂ∞ë29%Âπ≥ÂùáÊ≠ª‰∫°Áéá\n  * ÈáçÂûãÂÆ∂Âä°ÔºöÂê∏Â∞ò„ÄÅÊì¶Âú∞Êùø„ÄÅÊãñÂú∞„ÄÅÊì¶Ê¥óÁ™óÊà∑„ÄÅÊ¥óËΩ¶„ÄÅÊê¨Âä®ÂÆ∂ÂÖ∑„ÄÅÊê¨ÁÖ§Ê∞îÁΩêÁ≠âÁ≠â„ÄÇ\n  * ËΩªÂûãÂÆ∂Âä°ÔºöÊé∏ÁÅ∞Â∞ò„ÄÅÊ¥óÁ¢ó„ÄÅÊâãÊ¥óË°£Êúç„ÄÅÁÜ®ÁÉ´„ÄÅÊôæË°£Êúç„ÄÅÂÅöÈ•≠„ÄÅ‰π∞Êó•Áî®ÂìÅÁ≠âÁ≠â„ÄÇ\n\n##### 6.2.7. Áù°Áú†\n\n* [Ë∂Ö30‰∏á‰∫öÊ¥≤‰∫∫Êï∞ÊçÆÔºöÊØèÂ§©Áù°Âá†‰∏™Â∞èÊó∂ÊúÄÊúâÁõäÈïøÂØøÔºü](https://med.sina.com/article_detail_103_1_105491.html)\n  * Âú®Áî∑ÊÄß‰∏≠Ôºå‰∏éÁù°Áú†Êó∂Èïø‰∏∫7Â∞èÊó∂Áõ∏ÊØîÔºöÁù°Áú†ÊåÅÁª≠Êó∂Èó¥‚â•10Â∞èÊó∂‰∏éÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Â¢ûÂä†34%Áõ∏ÂÖ≥Ôºõ\n  * ![Áù°Áú†-Áî∑](https://user-images.githubusercontent.com/2707039/163704166-226b7ebb-92ce-4753-a3e7-77a87652a104.jpg)\n  * Âú®Â•≥ÊÄß‰∏≠Ôºå‰∏éÁù°Áú†ÊåÅÁª≠Êó∂Èó¥7Â∞èÊó∂Áõ∏ÊØîÔºöÁù°Áú†ÊåÅÁª≠Êó∂Èó¥‚â•10Â∞èÊó∂‰∏éÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Â¢ûÂä†48%Áõ∏ÂÖ≥Ôºõ\n  * ![Áù°Áú†-Â•≥](https://user-images.githubusercontent.com/2707039/163704169-c5c715aa-7130-403b-b0d1-ec34fab094d8.png)\n* [È¢†Ë¶ÜËÆ§Áü•ÔºÅÂä†ÊãøÂ§ßÁ†îÁ©∂ÂèëÁé∞ÔºöÊó©Áù°ÊØîÁÜ¨Â§úÊàñËÆ∏Êõ¥‰º§Ë∫´ÔºåÂá†ÁÇπÁù°ÊâçÂ•ΩÔºü](https://www.thepaper.cn/newsDetail_forward_14461799)\n  * ÂÖ∂‰∏≠‰∏Ä‰∏™ÁªìËÆ∫‰∏∫ÔºåÂ∞±ÂØùÊó∂Èó¥‰∏éÂÖ®Âõ†Ê≠ª‰∫°ÁéáÁöÑÂÖ≥ËÅîÊÄßÂº∫ÔºåËøáÊó©Áù°ËßâÂíåËøáÊôöÁù°ËßâÈÉΩ‰ºöÂΩ±ÂìçÂÅ•Â∫∑Ôºå‰ΩÜÊòØÊó©Áù°Â¢ûÂä†ÁöÑÂÖ®Âõ†Ê≠ª‰∫°ÁéáÊØîÊôöÁù°Â¢ûÂä†ÁöÑÊ≠ª‰∫°ÁéáÈ´òÔºåÊó©Áù°Â¢ûÂä†‰∫Ü43%ÁöÑÊ≠ª‰∫°È£éÈô©ÔºåËÄåÊôöÁù°Â¢ûÂä†‰∫Ü15%ÁöÑÊ≠ª‰∫°È£éÈô©„ÄÇ\n  * ËøôÈ°πË∞ÉÊü•Á†îÁ©∂ÔºåËøòÂ≠òÂú®ÂæàÂ§öÂ±ÄÈôêÊÄßÔºåÊØîÂ¶ÇÊ≤°ÊúâÁõ¥Êé•ËØÅÊòéÂ∞±ÂØùÊó∂Èó¥‰∏éÊ≠ª‰∫°ÁöÑÂÖ≥Á≥ªÔºå‰ªÖ‰ªÖËØ¥ÊòéÁõ∏ÂÖ≥ÊÄßÔºåÈÄöËøáÂèÇ‰∏é‰∫∫Áæ§Ëá™ÊàëÊä•ÂëäÁªüËÆ°Áù°Áú†Êó∂Èó¥ÔºåÊï∞ÊçÆ‰∏çÂ§üÂÆ¢ËßÇ\n\n##### 6.2.8. ‰πÖÂùê\n\n* [‰∏≠ÂõΩÂ±ÖÊ∞ëËÜ≥È£üÊåáÂçóÁßëÂ≠¶Á†îÁ©∂Êä•ÂëäÔºà2021Âπ¥Ôºâ](https://www.chinanutri.cn/yyjkzxpt/yyjkkpzx/yytsg/zgjm/202103/P020210311486742870527.pdf)\n  * ‰πÖÂùêÂíåÁúãÁîµËßÜÊó∂Èó¥‰∏éÂÖ®Âõ†Ê≠ª‰∫°„ÄÅÂøÉË°ÄÁÆ°ÁñæÁóÖ„ÄÅÁôåÁóáÂíå2ÂûãÁ≥ñÂ∞øÁóÖÂèëÁóÖÈ´òÈ£éÈô©Áõ∏ÂÖ≥ÔºåÊòØÁã¨Á´ãÈ£éÈô©Âõ†Á¥†„ÄÇ‰πÖÂùêÊó∂Èó¥ÊØèÂ§©ÊØèÂ¢ûÂä†1Â∞èÊó∂ÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÂèëÁîüÈ£éÈô©Â¢ûÂä†4%ÔºåÁôåÁóáÂ¢ûÂä†1%ÔºåÂÖ®Âõ†Ê≠ª‰∫°È£éÈô©Â¢ûÂä†3%„ÄÇÂÖ®Âõ†Ê≠ª‰∫°ÂíåCVDÊ≠ª‰∫°È£éÈô©Â¢ûÂä†ÁöÑ‰πÖÂùêÊó∂Èó¥ÈòàÂÄºÊòØ6\\~8h/dÔºåÁúãÁîµËßÜÊó∂Èó¥ÈòàÂÄºÊòØ3\\~4h/d„ÄÇ\n* [‰∏ñÂç´ÁªÑÁªáÂÖ≥‰∫éË∫´‰ΩìÊ¥ªÂä®Âíå‰πÖÂùêË°å‰∏∫ÁöÑÊåáÂçó](https://apps.who.int/iris/bitstream/handle/10665/337001/9789240014947-chi.pdf)\n\n#### 6.3. ‰∏ä‰∏ãÊñá\n\n##### 6.3.1. ÊÉÖÁª™\n\n* [ÊÇ≤ËßÇÊÉÖÁª™‰∏éÊõ¥È´òÁöÑÂÖ®Âõ†Ê≠ª‰∫°ÁéáÂíåÂøÉË°ÄÁÆ°ÁñæÁóÖÊ≠ª‰∫°ÁéáÊúâÂÖ≥Ôºå‰ΩÜ‰πêËßÇÊÉÖÁª™Âπ∂‰∏çËÉΩËµ∑Âà∞‰øùÊä§‰ΩúÁî®](https://www.x-mol.com/paper/1288184397379059712/t?recommendPaper=1263704526086578176)\n* [Pessimism is associated with greater all-cause and cardiovascular mortality, but optimism is not protective](https://www.nature.com/articles/s41598-020-69388-y?utm_source=xmol&utm_medium=affiliate&utm_content=meta&utm_campaign=DDCN_1_GL01_metadata_scirep)\n  * Âú®1993-1995Âπ¥Èó¥Ôºå‰∏ÄÈ°πÈíàÂØπ50Â≤Å‰ª•‰∏äÊæ≥Â§ßÂà©‰∫ö‰∫∫ÂÅ•Â∫∑ÁöÑÂèåËÉûËÉéÁ†îÁ©∂‰∏≠ÂåÖÊã¨‰∫ÜÁîüÊ¥ªÂèñÂêëÊµãËØïÔºàLOTÔºâÔºåÂÖ∂‰∏≠ÂåÖÂê´‰πêËßÇÂíåÊÇ≤ËßÇÁöÑÈ°πÁõÆ„ÄÇÂπ≥Âùá20Âπ¥ÂêéÔºåÂèÇ‰∏éËÄÖ‰∏éÊù•Ëá™Êæ≥Â§ßÂà©‰∫öÂõΩÂÆ∂Ê≠ª‰∫°ÊåáÊï∞ÁöÑÊ≠ª‰∫°‰ø°ÊÅØÁõ∏ÂåπÈÖç„ÄÇÂú®2,978ÂêçÂÖ∑ÊúâÂæàÂ§öÂèØÁî®ÂàÜÊï∞ÁöÑÂèÇ‰∏éËÄÖ‰∏≠ÔºåÊúâ1,068‰∫∫Ê≠ª‰∫°„ÄÇÁîüÂ≠òÂàÜÊûêÊµãËØï‰∫ÜÂêÑÁßç‰πêËßÇÂõ†Á¥†ÂíåÊÇ≤ËßÇÊÉÖÁª™ÂàÜÊï∞‰∏é‰ªª‰ΩïÂéüÂõ†ÔºåÁôåÁóáÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÊàñÂÖ∂‰ªñÂ∑≤Áü•ÂéüÂõ†ÁöÑÊ≠ª‰∫°Áéá‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇÂπ¥ÈæÑË∞ÉÊï¥ÂêéÁöÑÊÇ≤ËßÇÈáèË°®‰∏äÁöÑÊ†∏ÂøÉ‰∏éÂÖ®Âõ†ÂíåÂøÉË°ÄÁÆ°ÁñæÁóÖÊ≠ª‰∫°ÁéáÁõ∏ÂÖ≥ÔºàÊØè1‰∏™Ê†áÂáÜÂ∑ÆÂçï‰ΩçÁöÑÂç±Èô©ÊØîÔºå95ÔºÖÁΩÆ‰ø°Âå∫Èó¥ÂíåpÂÄº1.134„ÄÅ1.065‚Äì1.207„ÄÅ8.85√ó10 ‚Äì5Âíå1.196„ÄÅ1.045‚Äì1.368„ÄÅ0.0093 ÔºâÔºå‰ΩÜ‰∏ç‰ºöÂõ†ÁôåÁóáÊ≠ª‰∫°„ÄÇ‰πêËßÇÂæóÂàÜ‰∏éÊÇ≤ËßÇÂæóÂàÜ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂæàÂº±ÔºàÂπ¥ÈæÑË∞ÉÊï¥ÂêéÁöÑÁ≠âÁ∫ßÁõ∏ÂÖ≥Á≥ªÊï∞= ‚àí 0.176ÔºâÔºå‰ΩÜ‰∏éÊÄªÊ≠ª‰∫°ÁéáÊàñÁâπÂÆöÂéüÂõ†Ê≠ª‰∫°ÁéáÊ≤°ÊúâÊòæÁùÄÁõ∏ÂÖ≥ÊÄß„ÄÇÂèçÂêëÂõ†ÊûúÂÖ≥Á≥ªÔºàÂºïËµ∑ÊÇ≤ËßÇÊÉÖÁª™ÁöÑÁñæÁóÖÔºâÊòØ‰∏çÂèØËÉΩÁöÑÔºåÂõ†‰∏∫Âú®ÈÇ£ÁßçÊÉÖÂÜµ‰∏ãÔºåÂøÉË°ÄÁÆ°ÁñæÁóÖÂíåÁôåÁóáÈÉΩ‰ºöÂØºËá¥ÊÇ≤ËßÇÊÉÖÁª™„ÄÇ\n\n##### 6.3.2. Ë¥´ÂØå\n\n* [JAMAÂ≠êÂàäÔºöË¥´ÂØåÂ∑ÆË∑ùÁúüËÉΩÂΩ±ÂìçÂØøÂëΩÔºüËøôÂèØËÉΩÊòØÁúüÁöÑÔºÅ](https://www.cn-healthcare.com/articlewm/20210727/content-1246348.html)\n  * ËØ•Á†îÁ©∂‰ΩøÁî®1994-1996Âπ¥Á¨¨‰∏ÄÊ¨°Êî∂ÈõÜÁöÑÊï∞ÊçÆÔºåÂπ∂ÈÄöËøáÁîüÂ≠òÊ®°ÂûãÊù•ÂàÜÊûêÂáÄËµÑ‰∫ßÂíåÈïøÂØø‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂÖ±Êî∂Á∫≥5414 ÂêçÂèÇ‰∏éËÄÖÔºåÂπ≥ÂùáÂπ¥ÈæÑ‰∏∫ 46.7Â≤ÅÔºåÂåÖÊã¨ 2766 ÂêçÂ•≥ÊÄß„ÄÇËæÉÈ´òÁöÑÂáÄËµÑ‰∫ß‰∏éËæÉ‰ΩéÁöÑÊ≠ª‰∫°È£éÈô©Áõ∏ÂÖ≥„ÄÇÁâπÂà´ÊòØÂú®ÂÖÑÂºüÂßêÂ¶πÂíåÂèåËÉûËÉé‰∏≠Ôºàn = 2490ÔºâÔºåÂú®ËæÉÈ´òÁöÑÂáÄËµÑ‰∫ßÂíåËæÉ‰ΩéÁöÑÊ≠ª‰∫°Áéá‰πãÈó¥ËßÇÂØüÂà∞Á±ª‰ººÁöÑÂÖ≥ËÅîÔºåË°®ÊòéÊã•ÊúâÊõ¥Â§öË¥¢ÂØåÁöÑÂÖÑÂºüÂßêÂ¶πÊàñÂèåËÉûËÉéÊØîÊã•ÊúâÊõ¥Â∞ëË¥¢ÂØåÁöÑÂÖÑÂºüÂßêÂ¶π/ÂèåËÉûËÉéÊ¥ªÂæóÊõ¥‰πÖ„ÄÇ\n\n##### 6.3.3. ‰ΩìÈáç\n\n* [JAMAÂ≠êÂàäÔºöÂáèËÇ•Ë¶ÅË∂ÅÊó©ÔºåÊâçËÉΩÊúâÊïàÈôç‰ΩéÊ≠ª‰∫°ÁéáÈ£éÈô©](https://www.chinacdc.cn/gwxx/202009/t20200904_218959.html)\n  * ÂØπ‰ΩìÈáçÂáèËΩªÁöÑÊ≠ª‰∫°ÁéáÈ£éÈô©ËØÑ‰º∞ÂèëÁé∞Ôºå‰ΩìÈáç‰ªéËÇ•ËÉñÂáèËΩªÂà∞Ë∂ÖÈáçÁöÑÊàêÂπ¥‰∫∫‰∏éÁ®≥ÂÆöËÇ•ËÉñ‰∫∫Áæ§Áõ∏ÊØîÔºåÂÖ®Âõ†Ê≠ª‰∫°ÁéáÈôç‰Ωé‰∫Ü54ÔºÖÔºàÂç±Èô©ÊØî‰∏∫0.46ÔºâÔºåÁÑ∂ËÄå‰ªéÊàêÂπ¥ÂàùÊúüÁöÑË∂ÖÈáçÂáèËΩªÂà∞‰∏≠Âπ¥‰ª•ÂâçÁöÑÊ≠£Â∏∏‰ΩìÈáçÁöÑ‰∫∫Áæ§ÁöÑÊ≠ª‰∫°ÁéáÈ£éÈô©Âπ∂Êú™Èôç‰ΩéÔºàÈ£éÈô©ÊØî‰∏∫1.12Ôºâ„ÄÇ\n  * ![Table3](https://raw.githubusercontent.com/qhy040404/Image-Resources-Repo/master/zoi200509t3_1596761185.02415.png)\n\n##### 6.3.4. Êñ∞ÂÜ†\n\n* [Magnitude, demographics and dynamics of the effect of the first wave of the COVID-19 pandemic on all-cause mortality in 21 industrialized countries](https://www.nature.com/articles/s41591-020-1112-0.pdf)\n  * ÁõÆÂâçÊù•ÁúãÔºåÊñ∞ÂÜ†Ê≠ª‰∫°ÁéáÔºàÁæéÂõΩÔºâÂú®1.5%Â∑¶Âè≥Ôºå‰∫∫ÂùáÈ¢ÑÊúüÂØøÂëΩÂáèÂ∞ë‰∫Ü2Âπ¥\n* [Â¶Ç‰ΩïÁúãÂæÖÁæéÂõΩCDCÂÆ£Áß∞Êñ∞ÂÜ†Ê≠ª‰∫°‰∫∫Êï∞Ë¢´È´ò‰º∞Ôºü](https://www.zhihu.com/question/510943670/answer/2308499719)\n* [NVSS deaths](https://www.cdc.gov/nchs/nvss/deaths.htm)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "wodiluluya--Viewers": {
      "owner": "wodiluluya",
      "name": "Viewers",
      "url": "https://github.com/wodiluluya/Viewers",
      "imageUrl": "https://github.com/wodiluluya.png",
      "description": "View and manipulate medical images directly in the browser with support for DICOMweb protocols. The platform features advanced functionalities like 2D and 3D rendering, measurement tracking, and customizable workflows for enhanced medical imaging capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2024-09-26T01:18:00Z",
      "readme_content": "<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<div align=\"center\">\n  <h1>OHIF Medical Imaging Viewer</h1>\n  <p><strong>The OHIF Viewer</strong> is a zero-footprint medical image viewer\nprovided by the <a href=\"https://ohif.org/\">Open Health Imaging Foundation (OHIF)</a>. It is a configurable and extensible progressive web application with out-of-the-box support for image archives which support <a href=\"https://www.dicomstandard.org/using/dicomweb/\">DICOMweb</a>.</p>\n</div>\n\n\n<div align=\"center\">\n  <a href=\"https://docs.ohif.org/\"><strong>Read The Docs</strong></a>\n</div>\n<div align=\"center\">\n  <a href=\"https://viewer.ohif.org/\">Live Demo</a> |\n  <a href=\"https://ui.ohif.org/\">Component Library</a>\n</div>\n<div align=\"center\">\n  üì∞ <a href=\"https://ohif.org/news/\"><strong>Join OHIF Newsletter</strong></a> üì∞\n</div>\n<div align=\"center\">\n  üì∞ <a href=\"https://ohif.org/news/\"><strong>Join OHIF Newsletter</strong></a> üì∞\n</div>\n\n\n\n<hr />\n\n[![NPM version][npm-version-image]][npm-url]\n[![MIT License][license-image]][license-url]\n[![This project is using Percy.io for visual regression testing.][percy-image]](percy-url)\n<!-- [![NPM downloads][npm-downloads-image]][npm-url] -->\n<!-- [![Pulls][docker-pulls-img]][docker-image-url] -->\n<!-- [![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FOHIF%2FViewers.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FOHIF%2FViewers?ref=badge_shield) -->\n\n<!-- [![Netlify Status][netlify-image]][netlify-url] -->\n<!-- [![CircleCI][circleci-image]][circleci-url] -->\n<!-- [![codecov][codecov-image]][codecov-url] -->\n<!-- [![All Contributors](https://img.shields.io/badge/all_contributors-10-orange.svg?style=flat-square)](#contributors) -->\n<!-- prettier-ignore-end -->\n\n\n|     |  | |\n| :-: | :---  | :--- |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-measurements.webp?raw=true\" alt=\"Measurement tracking\" width=\"350\"/> | Measurement Tracking | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.25403.345050719074.3824.20170125095438.5) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-segmentation.webp?raw=true\" alt=\"Segmentations\" width=\"350\"/> | Labelmap Segmentations  | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.12.2.1107.5.2.32.35162.30000015050317233592200000046) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-ptct.webp?raw=true\" alt=\"Hanging Protocols\" width=\"350\"/> | Fusion and Custom Hanging protocols  | [Demo](https://viewer.ohif.org/tmtv?StudyInstanceUIDs=1.3.6.1.4.1.14519.5.2.1.7009.2403.334240657131972136850343327463) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-volume-rendering.webp?raw=true\" alt=\"Volume Rendering\" width=\"350\"/> | Volume Rendering  | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.25403.345050719074.3824.20170125095438.5&hangingprotocolId=mprAnd3DVolumeViewport) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-pdf.webp?raw=true\" alt=\"PDF\" width=\"350\"/> | PDF  | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=2.25.317377619501274872606137091638706705333) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-rtstruct.webp?raw=true\" alt=\"RTSTRUCT\" width=\"350\"/> | RT STRUCT  | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.5962.99.1.2968617883.1314880426.1493322302363.3.0) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-4d.webp?raw=true\" alt=\"4D\" width=\"350\"/> | 4D  | [Demo](https://viewer.ohif.org/dynamic-volume?StudyInstanceUIDs=2.25.232704420736447710317909004159492840763) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/demo-video.webp?raw=true\" alt=\"VIDEO\" width=\"350\"/> | Video  | [Demo](https://viewer.ohif.org/viewer?StudyInstanceUIDs=2.25.96975534054447904995905761963464388233) |\n| <img src=\"https://github.com/OHIF/Viewers/blob/master/platform/docs/docs/assets/img/microscopy.webp?raw=true\" alt=\"microscopy\" width=\"350\"/> | Slide Microscopy  | [Demo](https://viewer.ohif.org/microscopy?StudyInstanceUIDs=2.25.141277760791347900862109212450152067508) |\n\n## About\n\nThe OHIF Viewer can retrieve\nand load images from most sources and formats; render sets in 2D, 3D, and\nreconstructed representations; allows for the manipulation, annotation, and\nserialization of observations; supports internationalization, OpenID Connect,\noffline use, hotkeys, and many more features.\n\nAlmost everything offers some degree of customization and configuration. If it\ndoesn't support something you need, we accept pull requests and have an ever\nimproving Extension System.\n\n## Why Choose Us\n\n### Community & Experience\n\nThe OHIF Viewer is a collaborative effort that has served as the basis for many\nactive, production, and FDA Cleared medical imaging viewers. It benefits from\nour extensive community's collective experience, and from the sponsored\ncontributions of individuals, research groups, and commercial organizations.\n\n### Built to Adapt\n\nAfter more than 8-years of integrating with many companies and organizations,\nThe OHIF Viewer has been rebuilt from the ground up to better address the\nvarying workflow and configuration needs of its many users. All of the Viewer's\ncore features are built using it's own extension system. The same extensibility\nthat allows us to offer:\n\n- 2D and 3D medical image viewing\n- Multiplanar Reconstruction (MPR)\n- Maximum Intensity Project (MIP)\n- Whole slide microscopy viewing\n- PDF and Dicom Structured Report rendering\n- Segmentation rendering as labelmaps and contours\n- User Access Control (UAC)\n- Context specific toolbar and side panel content\n- and many others\n\nCan be leveraged by you to customize the viewer for your workflow, and to add\nany new functionality you may need (and wish to maintain privately without\nforking).\n\n### Support\n\n- [Report a Bug üêõ](https://github.com/OHIF/Viewers/issues/new?assignees=&labels=Community%3A+Report+%3Abug%3A%2CAwaiting+Reproduction&projects=&template=bug-report.yml&title=%5BBug%5D+)\n- [Request a Feature üöÄ](https://github.com/OHIF/Viewers/issues/new?assignees=&labels=Community%3A+Request+%3Ahand%3A&projects=&template=feature-request.yml&title=%5BFeature+Request%5D+)\n- [Ask a Question ü§ó](community.ohif.org)\n- [Slack Channel](https://join.slack.com/t/cornerstonejs/shared_invite/zt-1r8xb2zau-dOxlD6jit3TN0Uwf928w9Q)\n\nFor commercial support, academic collaborations, and answers to common\nquestions; please use [Get Support](https://ohif.org/get-support/) to contact\nus.\n\n\n## Developing\n\n### Branches\n\n#### `master` branch - The latest dev (beta) release\n\n- `master` - The latest dev release\n\nThis is typically where the latest development happens. Code that is in the master branch has passed code reviews and automated tests, but it may not be deemed ready for production. This branch usually contains the most recent changes and features being worked on by the development team. It's often the starting point for creating feature branches (where new features are developed) and hotfix branches (for urgent fixes).\n\nEach package is tagged with beta version numbers, and published to npm such as `@ohif/ui@3.6.0-beta.1`\n\n### `release/*` branches - The latest stable releases\nOnce the `master` branch code reaches a stable, release-ready state, we conduct a comprehensive code review and QA testing. Upon approval, we create a new release branch from `master`. These branches represent the latest stable version considered ready for production.\n\nFor example, `release/3.5` is the branch for version 3.5.0, and `release/3.6` is for version 3.6.0. After each release, we wait a few days to ensure no critical bugs. If any are found, we fix them in the release branch and create a new release with a minor version bump, e.g., 3.5.1 in the `release/3.5` branch.\n\nEach package is tagged with version numbers and published to npm, such as `@ohif/ui@3.5.0`. Note that `master` is always ahead of the `release` branch. We publish docker builds for both beta and stable releases.\n\nHere is a schematic representation of our development workflow:\n\n![alt text](platform/docs/docs/assets/img/github-readme-branches-Jun2024.png)\n\n\n\n\n\n### Requirements\n\n- [Yarn 1.17.3+](https://yarnpkg.com/en/docs/install)\n- [Node 18+](https://nodejs.org/en/)\n- Yarn Workspaces should be enabled on your machine:\n  - `yarn config set workspaces-experimental true`\n\n### Getting Started\n\n1. [Fork this repository][how-to-fork]\n2. [Clone your forked repository][how-to-clone]\n   - `git clone https://github.com/YOUR-USERNAME/Viewers.git`\n3. Navigate to the cloned project's directory\n4. Add this repo as a `remote` named `upstream`\n   - `git remote add upstream https://github.com/OHIF/Viewers.git`\n5. `yarn install` to restore dependencies and link projects\n\n#### To Develop\n\n_From this repository's root directory:_\n\n```bash\n# Enable Yarn Workspaces\nyarn config set workspaces-experimental true\n\n# Restore dependencies\nyarn install\n```\n\n## Commands\n\nThese commands are available from the root directory. Each project directory\nalso supports a number of commands that can be found in their respective\n`README.md` and `package.json` files.\n\n| Yarn Commands                | Description                                                   |\n| ---------------------------- | ------------------------------------------------------------- |\n| **Develop**                  |                                                               |\n| `dev` or `start`             | Default development experience for Viewer                     |\n| `test:unit`                  | Jest multi-project test runner; overall coverage              |\n| **Deploy**                   |                                                               |\n| `build`\\*                    | Builds production output for our PWA Viewer                   |  |\n\n\\* - For more information on our different builds, check out our [Deploy\nDocs][deployment-docs]\n\n## Project\n\nThe OHIF Medical Image Viewing Platform is maintained as a\n[`monorepo`][monorepo]. This means that this repository, instead of containing a\nsingle project, contains many projects. If you explore our project structure,\nyou'll see the following:\n\n```bash\n.\n‚îú‚îÄ‚îÄ extensions               #\n‚îÇ   ‚îú‚îÄ‚îÄ _example             # Skeleton of example extension\n‚îÇ   ‚îú‚îÄ‚îÄ default              # basic set of useful functionalities (datasources, panels, etc)\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone       # image rendering and tools w/ Cornerstone3D\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-sr # DICOM Structured Report rendering and export\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-sr # DICOM Structured Report rendering and export\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-seg # DICOM Segmentation rendering and export\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-rt # DICOM RTSTRUCT rendering\n‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-microscopy # Whole Slide Microscopy rendering\n‚îÇ   ‚îú‚îÄ‚îÄ dicom-pdf # PDF rendering\n‚îÇ   ‚îú‚îÄ‚îÄ dicom-video # DICOM RESTful Services\n‚îÇ   ‚îú‚îÄ‚îÄ measurement-tracking # Longitudinal measurement tracking\n‚îÇ   ‚îú‚îÄ‚îÄ tmtv # Total Metabolic Tumor Volume (TMTV) calculation\n|\n\n‚îÇ\n‚îú‚îÄ‚îÄ modes                    #\n‚îÇ   ‚îú‚îÄ‚îÄ _example             # Skeleton of example mode\n‚îÇ   ‚îú‚îÄ‚îÄ basic-dev-mode       # Basic development mode\n‚îÇ   ‚îú‚îÄ‚îÄ longitudinal         # Longitudinal mode (measurement tracking)\n‚îÇ   ‚îú‚îÄ‚îÄ tmtv       # Total Metabolic Tumor Volume (TMTV) calculation mode\n‚îÇ   ‚îî‚îÄ‚îÄ microscopy          # Whole Slide Microscopy mode\n‚îÇ\n‚îú‚îÄ‚îÄ platform                 #\n‚îÇ   ‚îú‚îÄ‚îÄ core                 # Business Logic\n‚îÇ   ‚îú‚îÄ‚îÄ i18n                 # Internationalization Support\n‚îÇ   ‚îú‚îÄ‚îÄ ui                   # React component library\n‚îÇ   ‚îú‚îÄ‚îÄ docs                 # Documentation\n‚îÇ   ‚îî‚îÄ‚îÄ viewer               # Connects platform and extension projects\n‚îÇ\n‚îú‚îÄ‚îÄ ...                      # misc. shared configuration\n‚îú‚îÄ‚îÄ lerna.json               # MonoRepo (Lerna) settings\n‚îú‚îÄ‚îÄ package.json             # Shared devDependencies and commands\n‚îî‚îÄ‚îÄ README.md                # This file\n```\n\n## Acknowledgments\n\nTo acknowledge the OHIF Viewer in an academic publication, please cite\n\n> _Open Health Imaging Foundation Viewer: An Extensible Open-Source Framework\n> for Building Web-Based Imaging Applications to Support Cancer Research_\n>\n> Erik Ziegler, Trinity Urban, Danny Brown, James Petts, Steve D. Pieper, Rob\n> Lewis, Chris Hafey, and Gordon J. Harris\n>\n> _JCO Clinical Cancer Informatics_, no. 4 (2020), 336-345, DOI:\n> [10.1200/CCI.19.00131](https://www.doi.org/10.1200/CCI.19.00131)\n>\n> Open-Access on Pubmed Central:\n> https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7259879/\n\nor, for v1, please cite:\n\n> _LesionTracker: Extensible Open-Source Zero-Footprint Web Viewer for Cancer\n> Imaging Research and Clinical Trials_\n>\n> Trinity Urban, Erik Ziegler, Rob Lewis, Chris Hafey, Cheryl Sadow, Annick D.\n> Van den Abbeele and Gordon J. Harris\n>\n> _Cancer Research_, November 1 2017 (77) (21) e119-e122 DOI:\n> [10.1158/0008-5472.CAN-17-0334](https://www.doi.org/10.1158/0008-5472.CAN-17-0334)\n\n**Note:** If you use or find this repository helpful, please take the time to\nstar this repository on GitHub. This is an easy way for us to assess adoption\nand it can help us obtain future funding for the project.\n\nThis work is supported primarily by the National Institutes of Health, National\nCancer Institute, Informatics Technology for Cancer Research (ITCR) program,\nunder a\n[grant to Dr. Gordon Harris at Massachusetts General Hospital (U24 CA199460)](https://projectreporter.nih.gov/project_info_description.cfm?aid=8971104).\n\n[NCI Imaging Data Commons (IDC) project](https://imaging.datacommons.cancer.gov/) supported the development of new features and bug fixes marked with [\"IDC:priority\"](https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Apriority),\n[\"IDC:candidate\"](https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Acandidate) or [\"IDC:collaboration\"](https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Acollaboration). NCI Imaging Data Commons is supported by contract number 19X037Q from\nLeidos Biomedical Research under Task Order HHSN26100071 from NCI. [IDC Viewer](https://learn.canceridc.dev/portal/visualization) is a customized version of the OHIF Viewer.\n\nThis project is tested with BrowserStack. Thank you for supporting open-source!\n\n## License\n\nMIT ¬© [OHIF](https://github.com/OHIF)\n\n<!--\n  Links\n  -->\n\n<!-- prettier-ignore-start -->\n<!-- Badges -->\n[lerna-image]: https://img.shields.io/badge/maintained%20with-lerna-cc00ff.svg\n[lerna-url]: https://lerna.js.org/\n[netlify-image]: https://api.netlify.com/api/v1/badges/32708787-c9b0-4634-b50f-7ca41952da77/deploy-status\n[netlify-url]: https://app.netlify.com/sites/ohif-dev/deploys\n[all-contributors-image]: https://img.shields.io/badge/all_contributors-0-orange.svg?style=flat-square\n[circleci-image]: https://circleci.com/gh/OHIF/Viewers.svg?style=svg\n[circleci-url]: https://circleci.com/gh/OHIF/Viewers\n[codecov-image]: https://codecov.io/gh/OHIF/Viewers/branch/master/graph/badge.svg\n[codecov-url]: https://codecov.io/gh/OHIF/Viewers/branch/master\n[prettier-image]: https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square\n[prettier-url]: https://github.com/prettier/prettier\n[semantic-image]: https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg\n[semantic-url]: https://github.com/semantic-release/semantic-release\n<!-- ROW -->\n[npm-url]: https://npmjs.org/package/@ohif/app\n[npm-downloads-image]: https://img.shields.io/npm/dm/@ohif/app.svg?style=flat-square\n[npm-version-image]: https://img.shields.io/npm/v/@ohif/app.svg?style=flat-square\n[docker-pulls-img]: https://img.shields.io/docker/pulls/ohif/viewer.svg?style=flat-square\n[docker-image-url]: https://hub.docker.com/r/ohif/app\n[license-image]: https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\n[license-url]: LICENSE\n[percy-image]: https://percy.io/static/images/percy-badge.svg\n[percy-url]: https://percy.io/Open-Health-Imaging-Foundation/OHIF-Viewer\n<!-- Links -->\n[monorepo]: https://en.wikipedia.org/wiki/Monorepo\n[how-to-fork]: https://help.github.com/en/articles/fork-a-repo\n[how-to-clone]: https://help.github.com/en/articles/fork-a-repo#step-2-create-a-local-clone-of-your-fork\n[ohif-architecture]: https://docs.ohif.org/architecture/index.html\n[ohif-extensions]: https://docs.ohif.org/architecture/index.html\n[deployment-docs]: https://docs.ohif.org/deployment/\n[react-url]: https://reactjs.org/\n[pwa-url]: https://developers.google.com/web/progressive-web-apps/\n[ohif-viewer-url]: https://www.npmjs.com/package/@ohif/app\n[configuration-url]: https://docs.ohif.org/configuring/\n[extensions-url]: https://docs.ohif.org/extensions/\n<!-- Platform -->\n[platform-core]: platform/core/README.md\n[core-npm]: https://www.npmjs.com/package/@ohif/core\n[platform-i18n]: platform/i18n/README.md\n[i18n-npm]: https://www.npmjs.com/package/@ohif/i18n\n[platform-ui]: platform/ui/README.md\n[ui-npm]: https://www.npmjs.com/package/@ohif/ui\n[platform-viewer]: platform/app/README.md\n[viewer-npm]: https://www.npmjs.com/package/@ohif/app\n<!-- Extensions -->\n[extension-cornerstone]: extensions/cornerstone/README.md\n[cornerstone-npm]: https://www.npmjs.com/package/@ohif/extension-cornerstone\n[extension-dicom-html]: extensions/dicom-html/README.md\n[html-npm]: https://www.npmjs.com/package/@ohif/extension-dicom-html\n[extension-dicom-microscopy]: extensions/dicom-microscopy/README.md\n[microscopy-npm]: https://www.npmjs.com/package/@ohif/extension-dicom-microscopy\n[extension-dicom-pdf]: extensions/dicom-pdf/README.md\n[pdf-npm]: https://www.npmjs.com/package/@ohif/extension-dicom-pdf\n[extension-vtk]: extensions/vtk/README.md\n[vtk-npm]: https://www.npmjs.com/package/@ohif/extension-vtk\n<!-- prettier-ignore-end -->\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FOHIF%2FViewers.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2FOHIF%2FViewers?ref=badge_large)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "WTTeneger--food-tracker-mcp": {
      "owner": "WTTeneger",
      "name": "food-tracker-mcp",
      "url": "https://github.com/WTTeneger/food-tracker-mcp",
      "imageUrl": "https://github.com/WTTeneger.png",
      "description": "Track food consumption, analyze nutrition, and manage dietary restrictions by searching for products, logging meals, and creating personalized meal plans. Evaluate product compatibility with dietary restrictions and view detailed nutrition summaries.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-05-08T19:08:21Z",
      "readme_content": "# Food Tracker MCP\n\nA Model Context Protocol (MCP) server for tracking food consumption, analyzing nutrition, and managing dietary restrictions.\n\n## Overview\n\nFood Tracker MCP integrates with the OpenFoodFacts database to provide a comprehensive food tracking system with the following features:\n\n- Search for food products by barcode or keyword\n- Analyze nutritional content of food products\n- Create meal plans based on specific nutrition goals and dietary restrictions\n- Track food consumption with meal logging\n- Manage dietary restrictions and allergies\n- Check product compatibility with user restrictions\n- View food logs and nutrition summaries\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8 or higher\n- `pip` (Python package manager)\n\n### Setup\n\n1. Clone the repository or download the `food_tracker.py` file:\n\n```bash\n# Option 1: Clone the repository (if available)\ngit clone https://github.com/yourusername/food-tracker-mcp.git\ncd food-tracker-mcp\n\n# Option 2: Create a new directory and save the file there\nmkdir food-tracker-mcp\ncd food-tracker-mcp\n# Copy food_tracker.py into this directory\n```\n\n2. Create a virtual environment (recommended):\n\n```bash\npython -m venv venv\n```\n\n3. Activate the virtual environment:\n\n```bash\n# On Windows\nvenv\\Scripts\\activate\n\n# On macOS/Linux\nsource venv/bin/activate\n```\n\n4. Install the required dependencies:\n\n```bash\npip install httpx fastmcp pydantic\n```\n\n5. Ensure the data directories are created (the script will do this automatically on first run):\n\n```bash\nmkdir -p data/users data/logs\n```\n\n## Running the MCP Server\n\nRun the server using:\n\n```bash\npython food_tracker.py\n```\n\nThe server will start and be ready to receive commands.\n\n## Using with Claude\n\nTo use this MCP with Claude, you'll need to register it with the Claude platform. Here's how to do it:\n\n1. Follow Anthropic's documentation to register your MCP:\n   - Visit https://console.anthropic.com/mcps or the relevant section in your Anthropic account\n   - Register the food-tracker MCP by providing the necessary information and endpoint\n\n2. Once registered, you can interact with the Food Tracker through Claude by invoking the MCP's tools.\n\nExample prompts to use with Claude:\n\n- \"Scan this barcode to see nutrition information: 3270190119357\"\n- \"Add a peanut allergy to my dietary restrictions\"\n- \"Log that I had a granola bar for breakfast\"\n- \"Search for products containing 'oatmeal'\"\n- \"Check if this product is compatible with my dietary restrictions\"\n- \"Show me my food log for today\"\n\n## Available Tools\n\nThis MCP provides the following tools that Claude can access:\n\n### 1. `get_product_by_barcode`\n\nGet detailed information about a food product using its barcode.\n\n**Parameters:**\n- `barcode`: The product barcode (EAN, UPC, etc.)\n\n### 2. `search_products`\n\nSearch for food products by name or description.\n\n**Parameters:**\n- `query`: The search query\n- `page`: Page number for pagination (default: 1)\n- `page_size`: Number of results per page (default: 10)\n\n### 3. `manage_user_restrictions`\n\nManage a user's dietary restrictions.\n\n**Parameters:**\n- `user_id`: The user's unique identifier\n- `action`: The action to perform (get, add, remove, update)\n- `restriction_type`: Type of restriction (allergen, diet, ingredient, medical, preference)\n- `restriction_value`: The specific restriction value (e.g., \"peanuts\", \"vegetarian\")\n- `severity`: How severe the restriction is (avoid, limit, fatal)\n- `notes`: Additional notes about the restriction\n\n### 4. `check_product_compatibility`\n\nCheck if a product is compatible with a user's dietary restrictions.\n\n**Parameters:**\n- `user_id`: The user's unique identifier\n- `barcode`: The product barcode to check\n\n### 5. `analyze_nutrition`\n\nAnalyze the nutritional content of a food product.\n\n**Parameters:**\n- `barcode`: The product barcode\n\n### 6. `log_food_consumption`\n\nLog food consumption for a user.\n\n**Parameters:**\n- `user_id`: The user's unique identifier\n- `barcode`: The product barcode\n- `quantity`: Amount consumed (default: 1 serving)\n- `meal_type`: Type of meal (breakfast, lunch, dinner, snack)\n\n### 7. `get_user_food_log`\n\nGet a user's food log for a specific date.\n\n**Parameters:**\n- `user_id`: The user's unique identifier\n- `date`: Date in YYYY-MM-DD format (defaults to today)\n\n## Example Usage Scenarios\n\n### Setting Up a New User with Restrictions\n\n1. Add a gluten allergy:\n```\nmanage_user_restrictions(\n    user_id=\"user123\",\n    action=\"add\",\n    restriction_type=\"allergen\", \n    restriction_value=\"gluten\",\n    severity=\"avoid\",\n    notes=\"Avoid all wheat products\"\n)\n```\n\n2. Add a vegetarian diet:\n```\nmanage_user_restrictions(\n    user_id=\"user123\",\n    action=\"add\",\n    restriction_type=\"diet\", \n    restriction_value=\"vegetarian\"\n)\n```\n\n### Tracking Food Consumption\n\n1. Scan a product and log it:\n```\n# First get product info\nproduct = get_product_by_barcode(barcode=\"3270190119357\")\n\n# Then log consumption\nlog_food_consumption(\n    user_id=\"user123\",\n    barcode=\"3270190119357\",\n    quantity=1,\n    meal_type=\"breakfast\"\n)\n```\n\n2. Check compatibility with restrictions:\n```\ncheck_product_compatibility(\n    user_id=\"user123\",\n    barcode=\"3270190119357\"\n)\n```\n\n### Analyzing Nutritional Information\n\n1. Get detailed nutrition analysis:\n```\nanalyze_nutrition(barcode=\"3270190119357\")\n```\n\n2. View food log and nutrition totals:\n```\nget_user_food_log(user_id=\"user123\")\n```\n\n## Data Storage\n\nThe Food Tracker MCP stores data locally in JSON files:\n\n- User profiles: `./data/users/{user_id}.json`\n- Food logs: `./data/logs/{user_id}_{date}.json`\n\n## Extending the MCP\n\nYou can extend the MCP by:\n\n1. Adding new nutritional analysis features\n2. Implementing more detailed diet plans and goals\n3. Adding recipe suggestions based on available ingredients\n4. Creating reports and visualizations of nutrition data\n5. Implementing social features for sharing progress\n\n## Troubleshooting\n\n- If you encounter connection issues, ensure you have internet access as the MCP connects to the OpenFoodFacts API\n- If product information is incomplete, this may be due to limitations in the OpenFoodFacts database\n- For any data persistence issues, check the permissions on the data directory\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "wwwiena--covid_activity": {
      "owner": "wwwiena",
      "name": "covid_activity",
      "url": "https://github.com/wwwiena/covid_activity",
      "imageUrl": "https://github.com/wwwiena.png",
      "description": "Monitor and analyze COVID-19 activity trends in specific areas, providing real-time updates and insights for informed decision-making regarding health and safety.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2020-07-20T07:44:05Z",
      "readme_content": "# covid_activity\n challenge 5 activity\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "xraywu--mcp-wegene-assistant": {
      "owner": "xraywu",
      "name": "mcp-wegene-assistant",
      "url": "https://github.com/xraywu/mcp-wegene-assistant",
      "imageUrl": "https://github.com/xraywu.png",
      "description": "Analyze genetic testing reports by accessing user-specific data through a custom URI scheme and utilizing LLM capabilities to interpret the findings.",
      "stars": 3,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-01T00:20:44Z",
      "readme_content": "# wegene-assistant MCP server\n\n[![smithery badge](https://smithery.ai/badge/@xraywu/mcp-wegene-assistant)](https://smithery.ai/server/@xraywu/mcp-wegene-assistant)\n\nMCP server for WeGene Assistant, using LLM to analyze a user's WeGene genetic testing report.\n\n## Components\n\n### Resources\n\nOnce a user is authorized, all the reports under his/her account will be exposed as a resource:\n- Custom wegene:// URI scheme for accessing each individual report\n- A report resource has a name, description and application/json mimetype\n\n\n### Tools\n\nThe server implements one tool:\n- **wegene-oauth:** Start a WeGene Open API oAuth process in the browser\n  - The user should complete the authorization in 120 seconds so LLM will be able to further access the reports.\n- **wegene-get-profiles:** Read the profile list under a user's WeGene account\n  - Profiles' name and id will be returned for LLM to use.\n- **wegene-get-report-info:** Return the report meta info so LLM will know what reports are available.\n  - A list of report names, descriptions, endpoints, etc. will be returned\n- **wegene-get-report:** Read the results of a single report under a profile\n  - Returns the result JSON specified in [WeGene's Open API platform](https://api.wegene.com)\n  - Arguements \n    - report_endpoint: The report's endpoint to be retrieved from\n    - report_id: The report's id to be retrieved\n    - profile_id: The profile id to retrieve report from\n\n## Configuration\n\n- You will need WeGene Open API key/secret to use this project.\n- Copy `.env.example` as `.env` and update the key and secret in the file.\n\n## Quickstart\n\n### Install\n\n#### Installing via Smithery\n\nTo install WeGene Assistant for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@xraywu/mcp-wegene-assistant):\n\n```bash\nnpx -y @smithery/cli install @xraywu/mcp-wegene-assistant --client claude\n```\n\n#### Insall Locally\n\n##### Prepare MCP Server\n\n1. Clone this project\n2. Run `uv sync --dev --all-extras` under the project's root folder\n\n##### Claude Desktop Configuration\n\n- On MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nAdd below contents in the configuration file:\n\n```\n{\n  \"mcpServers\": {\n    \"wegene-assistant\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/wegene-assistant\",\n        \"run\",\n        \"wegene-assistant\"\n      ]\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "zhaoyouj--mcp-slicer": {
      "owner": "zhaoyouj",
      "name": "mcp-slicer",
      "url": "https://github.com/zhaoyouj/mcp-slicer",
      "imageUrl": "https://github.com/zhaoyouj.png",
      "description": "Connect and control 3D Slicer through natural language for medical image processing and scene manipulation. Execute Python code directly in the Slicer environment to enhance workflow and automate tasks.",
      "stars": 18,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-04T05:11:32Z",
      "readme_content": "<img src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/logo.jpeg?raw=true\" width=\"160\" alt=\"logo\">\n\n# MCP-Slicer - 3D Slicer Model Context Protocol Integration\n\n[English](README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh.md)\n\n[![Python Version](https://img.shields.io/badge/python-3.13%2B-blue.svg)](https://www.python.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI version](https://img.shields.io/pypi/v/mcp-slicer.svg)](https://pypi.org/project/mcp-slicer/)\n\nMCP-Slicer connects 3D Slicer with model clients like Claude Desktop or Cline through the Model Context Protocol (MCP), enabling direct interaction and control of 3D Slicer. This integration allows for medical image processing, scene creation, and manipulation using natural language.\n\n## Features\n\n1. list_nodes: List and filter Slicer MRML nodes and view their properties\n\n2. execute_python_code: Execute Python code in the Slicer environment\n\n## Installation\n\n### Prerequisites\n\n- 3D Slicer 5.8 or newer\n- Python 3.13 or newer\n- uv package manager\n\n**If you're on Mac, please install uv as**\n\n```bash\nbrew install uv\n```\n\n**On Windows**\n\n```bash\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nand then\n\n```bash\nset Path=C:\\Users\\nntra\\.local\\bin;%Path%\n```\n\nOtherwise installation instructions are on their website: [Install uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n**‚ö†Ô∏è Please install UV first**\n\n### Claude for Desktop Integration\n\nGo to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"slicer\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-slicer\"]\n    }\n  }\n}\n```\n\n### Cline Intergration\n\n```json\n{\n  \"mcpServers\": {\n    \"slicer\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-slicer\"]\n    }\n  }\n}\n```\n\n## Usage\n\n### Check Claude Settings\n\n<img width=\"1045\" alt=\"Image\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/claude_check.png?raw=true\" />\nMake sure you see the corresponding slicer tools added to the Claude Desktop App\n\n<img width=\"300\" alt=\"Image\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/toolsButton.png?raw=true\" />\n<img width=\"300\" alt=\"Image\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/tools_check.png?raw=true\" />\n\n### Open Slicer Web Server\n\n1. Open the Slicer Web Server module,\n2. ensure the required interfaces are checked,\n3. then start the server\n\n<img width=\"1045\" alt=\"Image\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/start_slicer_web_server.png?raw=true\" />\n\n## Examples\n\n### - list_nodes\n\n> What Markups nodes are in the Slicer scene now, list their names, what is their length if it is a line, and what is its angle if it is an angle\n\n<img width=\"1045\" alt=\"Image\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/example_list_nodes_en.png?raw=true\" />\n\n### - execute python code\n\n> Draw a translucent green cube of 8 cm in the Slicer scene, mark its vertices, and then draw a red sphere inscribed in it.\n\n<img width=\"1045\" alt=\"example_code_execute_en\" src=\"https://github.com/zhaoyouj/mcp-slicer/blob/main/docs/images/example_code_execute_en.png?raw=true\" />\n\n## Technical Details\n\nUtilizes existing Slicer Web Server interfaces. For technical details, please see [Slicer web server user guide](https://slicer.readthedocs.io/en/latest/user_guide/modules/webserver.html)\n\n## Limitations & Security Considerations\n\n- The `execute_python_code` tool allows running arbitrary Python code in 3D Slicer, which is powerful but potentially dangerous.\n\n  **‚ö†Ô∏è Not recommended for production use.**\n\n- Complex operations may need to be broken down into smaller steps.\n\n## Contributing\n\nContributions are welcome! Feel free to submit Pull Requests.\n\n## Disclaimer\n\nThis is a third-party integration project, not developed by the 3D Slicer team.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "zinja-coder--apktool-mcp-server": {
      "owner": "zinja-coder",
      "name": "apktool-mcp-server",
      "url": "https://github.com/zinja-coder/apktool-mcp-server",
      "imageUrl": "https://github.com/zinja-coder.png",
      "description": "Integrate Apktool with LLMs to enable real-time reverse engineering of Android APKs. Perform decompilation, code review, and AI-driven analysis while managing resources and vulnerabilities.",
      "stars": 224,
      "forks": 30,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T12:48:07Z",
      "readme_content": "<div align=\"center\">\n\n# apktool-mcp-server (Part of Zin's Reverse Engineering MCP Suite)\n\n‚ö° Fully automated MCP server built on top of apktool to analyze Android APKs using LLMs like Claude ‚Äî uncover vulnerabilities, parse manifests, and reverse engineer effortlessly.\n\n![GitHub contributors apktool-mcp-server](https://img.shields.io/github/contributors/zinja-coder/apktool-mcp-server)\n![GitHub all releases](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/total)\n![GitHub release (latest by SemVer)](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/latest/total)\n![Latest release](https://img.shields.io/github/release/zinja-coder/apktool-mcp-server.svg)\n![Python 3.10+](https://img.shields.io/badge/python-3%2E10%2B-blue)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n</div>\n\n<!-- It is a still in early stage of development, so expects bugs, crashes and logical erros.-->\n\n<div align=\"center\">\n    <img alt=\"banner\" height=\"480px\" widht=\"620px\" src=\"https://github.com/user-attachments/assets/eb9037f2-d1c7-45e0-8871-ca8aaade0dd0\">\n</div>\n\n<!--![apktool-mcp-server-banner.png](docs/assets/apktool-mcp-server-banner.png)-->\n\nImage generated using AI tools.\n\n---\n\n## ü§ñ What is apktool-mcp-server?\n\n**apktool-mcp-server** is a MCP server for the [Apk Tool](https://github.com/iBotPeaches/apktool) that integrates directly with [Model Context Protocol (MCP)](https://github.com/anthropic/mcp) to provide **live reverse engineering support with LLMs like Claude**.\n\nThink: \"Decompile ‚Üí Context-Aware Code Review ‚Üí AI Recommendations\" ‚Äî all in real time.\n\nWatch the demo!\n\nhttps://github.com/user-attachments/assets/d50251b8-6b1c-4341-b18e-ae54eb24a847\n\n- **Solving the CTFs**\n\n\n\nhttps://github.com/user-attachments/assets/c783a604-a636-4e70-9fa8-37e3d219b20b\n\n\n## Other projects in Zin MCP Suite\n - **[JADX-AI-MCP](https://github.com/zinja-coder/jadx-ai-mcp)**\n - **[JADX-MCP-Server](https://github.com/zinja-coder/jadx-mcp-server)**\n - **[ZIN-MCP-Client](https://github.com/zinja-coder/zin-mcp-client)**\n\n## Current MCP Tools\n\nThe following MCP tools are available:\n\n- `build_apk()` ‚Äî Build an APK from a decoded APKTool Project.\n- `get_manifest()` ‚Äî Get the AndroidManifest.xml content from a decoded APK project. \n- `get_apktool_yml()` ‚Äî Get apktool.yml information from a decoded APK project. \n- `list_smali_directories()` ‚Äî List all smali directories in a project. \n- `list_smali_files()` ‚Äî List smali files in a specific smali directory, optinally filtered by package prefix.\n- `get_smali_file()` ‚Äî Get content of a specific smali file by class name. \n- `modify_smali_file()` ‚Äî Modify the content of a specific smali file. \n- `list_resources()` ‚Äî List resources in a project, optionally filtered by resource type. \n- `get_resource_file()` ‚Äî Get Content of a specific resource file. \n- `modify_resource_file()` ‚Äî Modify the content of a specific resource file. \n- `search_in_file()` ‚Äî Search for a pattern in files with specified extensions. \n- `clean_project()` ‚Äî Clean a project directory to prepare for rebuilding.\n- `decode_apk()` ‚Äî Decode an APK file using APKTool, extracting resources and smali code. \n\n---\n\n## üóíÔ∏è Sample Prompts\n\n\n### üîç Basic Code Understanding\n\n- ‚ÄúList all smali directories for the dvac project.‚Äù\n\n- ‚ÄúShow me all the smali files under the package prefix com.vulnerable.component in the dvac project.‚Äù\n\n- ‚ÄúGet the smali code for the class com.vulnerable.component.MainActivity.‚Äù\n\n- ‚ÄúCompare MainActivity.smali with its previous version and show differences.‚Äù\n\n- ‚ÄúSearch for usage of startActivity in smali files of dvac project.‚Äù\n\n### üõ°Ô∏è Vulnerability Detection\n\n- ‚ÄúAnalyze declared permissions in the dvac AndroidManifest.xml and flag dangerous ones.‚Äù\n\n- ‚ÄúSearch for hardcoded URLs or IPs in all .xml and .smali files in the project.‚Äù\n\n- ‚ÄúFind all uses of PendingIntent.getActivity in smali files.‚Äù\n\n- ‚ÄúCheck for exported activities or receivers in dvac‚Äôs AndroidManifest.xml.‚Äù\n\n- ‚ÄúList all smali files that access android.permission.SEND_SMS or READ_CONTACTS.‚Äù\n\n### üõ†Ô∏è Reverse Engineering Helpers\n\n- ‚ÄúDecode this APK: dvac.apk and create a project called dvac.‚Äù\n\n- ‚ÄúCreate a new APKTool project called test-harness.‚Äù\n\n- ‚ÄúClean the dvac project before rebuild.‚Äù\n\n- ‚ÄúExtract DEX files from dvac project for external analysis.‚Äù\n\n- ‚ÄúModify MainActivity.smali to insert a log line at the beginning of onCreate().‚Äù\n\n### üì¶ Static Analysis\n\n- ‚ÄúGet the complete AndroidManifest.xml from dvac project.‚Äù\n\n- ‚ÄúShow the contents of apktool.yml for the dvac project.‚Äù\n\n- ‚ÄúList all resource files of type layout.‚Äù\n\n- ‚ÄúSearch for the word password in all resource and smali files.‚Äù\n\n- ‚ÄúCheck which permissions are used and compare them against typical over-permissioning risks.‚Äù\n\n### ü§ñ AI Code Modification\n\n- ‚ÄúModify the onCreate() method in MainActivity.smali to add a toast message.‚Äù\n\n- ‚ÄúReplace all http:// links with https:// in strings.xml.‚Äù\n\n- ‚ÄúAdd the android:exported=false attribute to all activities in the AndroidManifest.xml.‚Äù\n\n- ‚ÄúPatch the method validateLogin in LoginManager.smali to always return true.‚Äù\n\n- ‚ÄúAdd logging statements to every method in MainActivity.smali.‚Äù\n\n### üìÑ Documentation & Metadata\n\n- ‚ÄúList all decoded APKTool projects in the workspace.‚Äù\n\n- ‚ÄúShow me the apktool.yml config to review the version, original APK metadata, and compression settings.‚Äù\n\n- ‚ÄúGet all available Android devices connected via ADB. (To be migrated to ADB MCP Server.)‚Äù\n\n- ‚ÄúGet metadata about the project dvac from its apktool.yml.‚Äù\n\n- ‚ÄúCheck which APKTool version is currently installed on the server.‚Äù\n---\n\n## üõ†Ô∏è Getting Started \n### 1. Downlaod from Releases: https://github.com/zinja-coder/apktool-mcp-server/releases\n\n```bash\n# 0. Download and install apktool\nhttps://apktool.org/docs/install\n\n# 1. Test whether apktool has been correctly configured in the environment variables\n$ apktool -version\n\n# 2. Download the apktool-mcp-server-<version>.zip\nhttps://github.com/zinja-coder/apktool-mcp-server/releases\n\n# 3. \nunzip apktool-mcp-server-<version>.zip\n\n‚îúapktool-mcp-server/\n  ‚îú‚îÄ‚îÄ apktool_mcp_server.py\n  ‚îú‚îÄ‚îÄ requirements.txt\n  ‚îú‚îÄ‚îÄ README.md\n  ‚îú‚îÄ‚îÄ LICENSE\n\n# 4. Navigate to apktool-mcp-server directory\ncd apktool-mcp-server\n\n# 5. This project uses uv - https://github.com/astral-sh/uv instead of pip for dependency management.\n    ## a. Install uv (if you dont have it yet)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n    ## b. OPTIONAL, if for any reasons, you get dependecy errors in apktool-mcp-server, Set up the environment\nuv venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n    ## c. OPTIONAL Install dependencies\nuv pip install httpx fastmcp\n\n# The setup for apktool-mcp-server is done.\n```\n\n## 2. Running on Local LLM Using Ollama and Zin MCP Client - Recommended\n\n<div align=\"center\">\n    <a href=\"https://github.com/zinja-coder/zin-mcp-client\">\n    <img alt=\"zin-mcp-client\" height=\"360px\" widht=\"480px\" src=\"https://github.com/user-attachments/assets/0e8e0ecd-0520-422e-a007-03dc62c4118e\">\n    </a>\n</div>\n\n‚ö° Lightweight, Fast, Simple, CLI-Based MCP Client for STDIO MCP Servers, to fill the gap and provide bridge between your local LLMs running Ollama and MCP Servers.\n\nCheck Now: https://github.com/zinja-coder/zin-mcp-client\n\nDemo: Coming soon...\n\n## ü§ñ 3. Claude Desktop Setup\n\nMake sure Claude Desktop is running with MCP enabled.\n\nFor instance, I have used following for Kali Linux: https://github.com/aaddrick/claude-desktop-debian\n\nConfigure and add MCP server to LLM file:\n```bash\nnano ~/.config/Claude/claude_desktop_config.json\n```\n\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAnd following content in it:\n```json\n{\n    \"mcpServers\": {\n        \"apktool-mcp-server\": {\n            \"command\": \"/<path>/<to>/uv\", \n            \"args\": [\n                \"--directory\",\n                \"</PATH/TO/>apktool-mcp-server/\",\n                \"run\",\n                \"apktool_mcp_server.py\"\n            ]\n        }\n    }\n}\n```\n\nReplace:\n\n- `path/to/uv` with the actual path to your `uv` executable\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\nThen, navigate code and interact via real-time code review prompts using the built-in integration.\n\n## 4. Cherry Studio Setup\n\nIf you want to configure the MCP tool in Cherry Studio, you can refer to the following configuration.\n- Type: stdio\n- command: uv\n- argument:\n```bash\n--directory\npath/to/apktool-mcp-server\nrun\napktool_mcp_server.py\n```\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\n## To report bugs, issues, feature suggestion, Performance issue, general question, Documentation issue.\n - Kindly open an issue with respective template.\n\n - Tested on Claude Desktop Client, support for other AI will be tested soon!\n\n## üôè Credits\n\nThis project is a MCP Server for [Apktool](https://github.com/iBotPeaches/apktool), an amazing open-source Android reverse engineering tool created and maintained by [@iBotPeaches](https://github.com/iBotPeaches). All core APK decoding and resource processing logic belongs to them. I have only extended it to support my MCP server with AI capabilities.\n\n[üìé Original README (Apktool)](https://github.com/iBotPeaches/apktool)\n\nThe original README.md from Apktool is included here in this repository for reference and credit.\n\nAlso huge thanks to [@aaddrick](https://github.com/aaddrick) for developing Claude desktop for Debian based Linux.\n\nAnd in last, thanks to [@anthropics](https://github.com/anthropics) for developing the Model Context Protocol and [@FastMCP](https://github.com/jlowin/fastmcp) team.\n\nAnd all open source project maintainers and contributos which provies libraries and dependencies to make project like this possible.\n\n## üìÑ License\n\napktool-mcp-server and all related projects inherits the Apache 2.0 \n\n## ‚öñÔ∏è Legal Warning\n\n**Disclaimer**\n\nThe tools `apktool-mcp-server` and all related tools under this project are intended strictly for educational, research, and ethical security assessment purposes. They are provided \"as-is\" without any warranties, expressed or implied. Users are solely responsible for ensuring that their use of these tools complies with all applicable laws, regulations, and ethical guidelines.\n\nBy using `apktool-mcp-server`, you agree to use them only in environments you are authorized to test, such as applications you own or have explicit permission to analyze. Any misuse of these tools for unauthorized reverse engineering, infringement of intellectual property rights, or malicious activity is strictly prohibited.\n\nThe developers of `apktool-mcp-server` shall not be held liable for any damage, data loss, legal consequences, or other consequences resulting from the use or misuse of these tools. Users assume full responsibility for their actions and any impact caused by their usage.\n\nUse responsibly. Respect intellectual property. Follow ethical hacking practices.\n\n---\n\n## üôå Contribute or Support\n\n## Contributing\n\n[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](CONTRIBUTE.md)\n\n- Found it useful? Give it a ‚≠êÔ∏è\n- Got ideas? Open an [issue](https://github.com/zinja-coder/apktool-mcp-server/issues) or submit a PR\n- Built something on top? DM me or mention me ‚Äî I‚Äôll add it to the README!\n\n---\n## Audited and Received Assessment Badge\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zinja-coder-apktool-mcp-server-badge.png)](https://mseep.ai/app/zinja-coder-apktool-mcp-server)\n\nThank you Mseep.net for auditing and providing Assessment Badge.\n---\n\nBuilt with ‚ù§Ô∏è for the reverse engineering and AI communities.\n",
      "npm_url": "",
      "npm_downloads": 0
    }
  }
}