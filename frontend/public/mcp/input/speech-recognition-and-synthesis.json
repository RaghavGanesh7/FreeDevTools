{
  "category": "speech-recognition-and-synthesis",
  "categoryDisplay": "Speech Recognition and Synthesis",
  "description": "",
  "totalRepositories": 31,
  "repositories": {
    "abhaybabbar--retellai-mcp-server": {
      "owner": "abhaybabbar",
      "name": "retellai-mcp-server",
      "url": "https://github.com/abhaybabbar/retellai-mcp-server",
      "imageUrl": "https://github.com/abhaybabbar.png",
      "description": "Manage and interact with RetellAI's voice services, facilitating call management, voice agent creation, phone number provisioning, and voice option access through a unified interface.",
      "stars": 18,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T20:41:01Z",
      "readme_content": "# RetellAI MCP Server\n\nThis is a Model Context Protocol (MCP) server implementation for RetellAI, allowing AI assistants to interact with RetellAI's voice services.\n\n## Features\n\nThe RetellAI MCP server provides tools for:\n\n- **Call Management**: Create and manage phone calls and web calls\n- **Agent Management**: Create and manage voice agents with different LLM configurations\n- **Phone Number Management**: Provision and configure phone numbers\n- **Voice Management**: Access and use different voice options\n\n## Claude Desktop Setup\n\n1. Open `Claude Desktop` and press `CMD + ,` to go to `Settings`.\n2. Click on the `Developer` tab.\n3. Click on the `Edit Config` button.\n4. This will open the `claude_desktop_config.json` file in your file explorer.\n5. Get your Retell API key from the Retell dashboard (<https://dashboard.retellai.com/apiKey>).\n6. Add the following to your `claude_desktop_config.json` file. See [here](https://modelcontextprotocol.io/quickstart/user) for more details.\n7. Restart the Claude Desktop after editing the config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"retellai-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@abhaybabbar/retellai-mcp-server\"],\n      \"env\": {\n        \"RETELL_API_KEY\": \"<your_retellai_token>\"\n      }\n    }\n  }\n}\n```\n\n## Example use cases:\n\n1. List all the numbers I have in retellai\n2. List all the agents I have\n3. Tell me more about pizza delivery agent\n4. Creating agent and calling example:\n   1. Create an agent that calls my local pizza shop, make sure to keep the conversation short and to the point.\n   2. Order a margeritta pizza\n   3. Payment will be done by cash on delivery\n   4. Send it to <address>\n   5. The agent should pretend to be me. My name is <your_name>\n   6. Make an outbound call to my local pizza shop at <phone_number>, using the usa number\n\n## Repo Setup\n\n1. Install dependencies:\n\n   ```bash\n   npm i\n   ```\n\n2. Create a `.env` file with your RetellAI API key:\n\n   ```\n   RETELL_API_KEY=your_api_key_here\n   ```\n\n3. Run the server:\n   ```bash\n   node src/retell/index.js\n   ```\n\n## Available Tools\n\n### Call Tools\n\n- `list_calls`: Lists all Retell calls\n- `create_phone_call`: Creates a new phone call\n- `create_web_call`: Creates a new web call\n- `get_call`: Gets details of a specific call\n- `delete_call`: Deletes a specific call\n\n### Agent Tools\n\n- `list_agents`: Lists all Retell agents\n- `create_agent`: Creates a new Retell agent\n- `get_agent`: Gets a Retell agent by ID\n- `update_agent`: Updates an existing Retell agent\n- `delete_agent`: Deletes a Retell agent\n- `get_agent_versions`: Gets all versions of a Retell agent\n\n### Phone Number Tools\n\n- `list_phone_numbers`: Lists all Retell phone numbers\n- `create_phone_number`: Creates a new phone number\n- `get_phone_number`: Gets details of a specific phone number\n- `update_phone_number`: Updates a phone number\n- `delete_phone_number`: Deletes a phone number\n\n### Voice Tools\n\n- `list_voices`: Lists all available Retell voices\n- `get_voice`: Gets details of a specific voice\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "aigc17--Al-StoryLab": {
      "owner": "aigc17",
      "name": "Al-StoryLab",
      "url": "https://github.com/aigc17/Al-StoryLab",
      "imageUrl": "https://github.com/aigc17.png",
      "description": "AI-StoryLab generates interactive stories with accompanying audio effects and provides illustration prompts. It leverages AI services for story creation, voice synthesis, sound effect generation, and suggests relevant audio placements.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-21T02:57:12Z",
      "readme_content": "# AI-StoryLab\n\nAI-StoryLab 是一个基于 Next.js 开发的智能故事创作平台，它能够帮助用户生成故事并添加音频效果，让故事更加生动有趣。同时支持生成配套的绘图提示词，方便用户使用 Midjourney、Recraft 等 AI 绘图工具创建插图。\n\n## 主要功能\n\n- **故事生成**：根据主题自动生成故事内容\n- **语音合成**：支持中英文语音生成\n  - 中文：使用 海螺 MiniMax 语音服务\n  - 英文：使用 Replicate Kokoro 语音服务\n- **音效生成**：使用 ElevenLabs 生成逼真的音效\n- **智能建议**：自动推荐合适的音效位置\n- **绘图提示词**：为故事场景自动生成 AI 绘图提示词\n- **导出功能**：\n  - 导出音效位置指南\n  - 导出绘图提示词\n\n## 技术栈\n\n- **框架**：Next.js 14\n- **语言**：TypeScript\n- **样式**：Tailwind CSS\n- **UI组件**：shadcn/ui (基于 Radix UI 的组件库)\n- **AI服务**：\n  - DeepSeek：故事生成和绘图提示词生成\n  - MiniMax：中文语音\n  - Kokoro：英文语音\n  - ElevenLabs：音效生成\n\n## 开始使用\n\n1. 克隆项目\n```bash\ngit clone https://github.com/nicekate/Al-StoryLab.git\ncd Al-StoryLab\n```\n\n2. 安装依赖\n```bash\nnpm install\n```\n\n3. 配置环境变量\n复制 `.env.example` 文件并重命名为 `.env.local`，填入必要的 API 密钥：\n\n需要在以下平台注册并获取 API 密钥：\n- DeepSeek API Key ([获取地址](https://api-docs.deepseek.com/zh-cn/))\n- MiniMax API Key 和 Group ID ([获取地址](https://platform.minimaxi.com/))\n- ElevenLabs API Key ([获取地址](https://elevenlabs.io))\n- Replicate API Token ([获取地址](https://replicate.com/))\n\n将获取的密钥填入 `.env.local`：\n- DEEPSEEK_API_KEY\n- MINIMAX_API_KEY\n- MINIMAX_GROUP_ID\n- ELEVENLABS_API_KEY\n- REPLICATE_API_TOKEN\n\n4. 启动开发服务器\n```bash\nnpm run dev\n```\n\n5. 访问 [http://localhost:3000](http://localhost:3000) 开始使用\n\n## 使用指南\n\n### 生成故事\n1. 输入故事主题或使用自动生成的提示\n2. 选择语言（中文/英文）\n3. 点击生成按钮\n\n### 添加音效\n1. 使用智能建议生成音效提示词\n2. 选择合适的音效位置\n3. 点击生成音效\n\n### 生成绘图提示词\n1. 在故事生成后，点击\"生成绘图提示词\"\n2. 系统会为每个关键场景生成 AI 绘图提示词\n3. 可以直接复制使用或导出保存\n\n## 许可证\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0
    },
    "anilcosaran--whisper.cpp": {
      "owner": "anilcosaran",
      "name": "whisper.cpp",
      "url": "https://github.com/anilcosaran/whisper.cpp",
      "imageUrl": "https://github.com/anilcosaran.png",
      "description": "Transcribes and translates audio files using a lightweight implementation of OpenAI's Whisper model, optimized for speed and low memory usage across various platforms.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-04T11:53:28Z",
      "readme_content": "# whisper.cpp\n\n[![Actions Status](https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg)](https://github.com/ggerganov/whisper.cpp/actions)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![npm](https://img.shields.io/npm/v/whisper.cpp.svg)](https://www.npmjs.com/package/whisper.cpp/)\n\nStable: [v1.2.1](https://github.com/ggerganov/whisper.cpp/releases/tag/v1.2.1) / [Roadmap | F.A.Q.](https://github.com/ggerganov/whisper.cpp/discussions/126)\n\nHigh-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model:\n\n- Plain C/C++ implementation without dependencies\n- Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework\n- AVX intrinsics support for x86 architectures\n- VSX intrinsics support for POWER architectures\n- Mixed F16 / F32 precision\n- Low memory usage (Flash Attention)\n- Zero memory allocations at runtime\n- Runs on the CPU\n- [C-style API](https://github.com/ggerganov/whisper.cpp/blob/master/whisper.h)\n\nSupported platforms:\n\n- [x] Mac OS (Intel and Arm)\n- [x] [iOS](examples/whisper.objc)\n- [x] [Android](examples/whisper.android)\n- [x] Linux / [FreeBSD](https://github.com/ggerganov/whisper.cpp/issues/56#issuecomment-1350920264)\n- [x] [WebAssembly](examples/whisper.wasm)\n- [x] Windows ([MSVC](https://github.com/ggerganov/whisper.cpp/blob/master/.github/workflows/build.yml#L117-L144) and [MinGW](https://github.com/ggerganov/whisper.cpp/issues/168)]\n- [x] [Raspberry Pi](https://github.com/ggerganov/whisper.cpp/discussions/166)\n\nThe entire implementation of the model is contained in 2 source files:\n\n- Tensor operations: [ggml.h](ggml.h) / [ggml.c](ggml.c)\n- Transformer inference: [whisper.h](whisper.h) / [whisper.cpp](whisper.cpp)\n\nHaving such a lightweight implementation of the model allows to easily integrate it in different platforms and applications.\nAs an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: [whisper.objc](examples/whisper.objc)\n\nhttps://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4\n\nYou can also easily make your own offline voice assistant application: [command](examples/command)\n\nhttps://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4\n\nOr you can even run it straight in the browser: [talk.wasm](examples/talk.wasm)\n\n## Implementation details\n\n- The core tensor operations are implemented in C ([ggml.h](ggml.h) / [ggml.c](ggml.c))\n- The transformer model and the high-level C-style API are implemented in C++ ([whisper.h](whisper.h) / [whisper.cpp](whisper.cpp))\n- Sample usage is demonstrated in [main.cpp](examples/main)\n- Sample real-time audio transcription from the microphone is demonstrated in [stream.cpp](examples/stream)\n- Various other examples are available in the [examples](examples) folder\n\nThe tensor operators are optimized heavily for Apple silicon CPUs. Depending on the computation size, Arm Neon SIMD\ninstrisics or CBLAS Accelerate framework routines are used. The latter are especially effective for bigger sizes since\nthe Accelerate framework utilizes the special-purpose AMX coprocessor available in modern Apple products.\n\n## Quick start\n\nFirst, download one of the Whisper models converted in [ggml format](models). For example:\n\n```bash\nbash ./models/download-ggml-model.sh base.en\n```\n\nNow build the [main](examples/main) example and transcribe an audio file like this:\n\n```bash\n# build the main example\nmake\n\n# transcribe an audio file\n./main -f samples/jfk.wav\n```\n\n---\n\nFor a quick demo, simply run `make base.en`:\n\n```java\n$ make base.en\n\ncc  -I.              -O3 -std=c11   -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread -c whisper.cpp -o whisper.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread examples/main/main.cpp whisper.o ggml.o -o main  -framework Accelerate\n./main -h\n\nusage: ./main [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [true   ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n\n\nbash ./models/download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nggml-base.en.bin               100%[========================>] 141.11M  6.34MB/s    in 24s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n\n\n===============================================\nRunning base.en on all samples in ./samples ...\n===============================================\n\n----------------------------------------------\n[+] Running base.en on samples/jfk.wav ... (run 'ffplay samples/jfk.wav' to listen)\n----------------------------------------------\n\nwhisper_init_from_file: loading model from 'models/ggml-base.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\nwhisper_model_load: kv self size  =    5.25 MB\nwhisper_model_load: kv cross size =   17.58 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     =  140.60 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\n\nwhisper_print_timings:     fallbacks =   0 p /   0 h\nwhisper_print_timings:     load time =   113.81 ms\nwhisper_print_timings:      mel time =    15.40 ms\nwhisper_print_timings:   sample time =    11.58 ms /    27 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time =   266.60 ms /     1 runs (  266.60 ms per run)\nwhisper_print_timings:   decode time =    66.11 ms /    27 runs (    2.45 ms per run)\nwhisper_print_timings:    total time =   476.31 ms\n```\n\nThe command downloads the `base.en` model converted to custom `ggml` format and runs the inference on all `.wav` samples in the folder `samples`.\n\nFor detailed usage instructions, run: `./main -h`\n\nNote that the [main](examples/main) example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool.\nFor example, you can use `ffmpeg` like this:\n\n```java\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n## More audio samples\n\nIf you want some extra audio samples to play with, simply run:\n\n```\nmake samples\n```\n\nThis will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via `ffmpeg`.\n\nYou can download and run the other models as follows:\n\n```\nmake tiny.en\nmake tiny\nmake base.en\nmake base\nmake small.en\nmake small\nmake medium.en\nmake medium\nmake large-v1\nmake large\n```\n\n## Memory usage\n\n| Model  | Disk   | Mem     | SHA                                        |\n| ---    | ---    | ---     | ---                                        |\n| tiny   |  75 MB | ~125 MB | `bd577a113a864445d4c299885e0cb97d4ba92b5f` |\n| base   | 142 MB | ~210 MB | `465707469ff3a37a2b9b8d8f89f2f99de7299dac` |\n| small  | 466 MB | ~600 MB | `55356645c2b361a969dfd0ef2c5a50d530afd8d5` |\n| medium | 1.5 GB | ~1.7 GB | `fd9727b6e1217c2f614f9b698455c4ffd82463b4` |\n| large  | 2.9 GB | ~3.3 GB | `0f4c8e34f21cf1a914c59d8b3ce882345ad349d6` |\n\n## Limitations\n\n- Inference only\n- No GPU support (yet)\n\n## Another example\n\nHere is another example of transcribing a [3:24 min speech](https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg)\nin about half a minute on a MacBook M1 Pro, using `medium.en` model:\n\n<details>\n  <summary>Expand to see the result</summary>\n\n```java\n$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8\n\nwhisper_init_from_file: loading model from 'models/ggml-medium.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 1024\nwhisper_model_load: n_audio_head  = 16\nwhisper_model_load: n_audio_layer = 24\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 1024\nwhisper_model_load: n_text_head   = 16\nwhisper_model_load: n_text_layer  = 24\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 4\nwhisper_model_load: mem required  = 1720.00 MB (+   43.00 MB per decoder)\nwhisper_model_load: kv self size  =   42.00 MB\nwhisper_model_load: kv cross size =  140.62 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     = 1462.35 MB\nwhisper_model_load: model size    = 1462.12 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/gb1.wav' (3179750 samples, 198.7 sec), 8 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.\n[00:00:08.000 --> 00:00:17.000]   At nine o'clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.\n[00:00:17.000 --> 00:00:23.000]   A short time later, debris was seen falling from the skies above Texas.\n[00:00:23.000 --> 00:00:29.000]   The Columbia's lost. There are no survivors.\n[00:00:29.000 --> 00:00:32.000]   On board was a crew of seven.\n[00:00:32.000 --> 00:00:39.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark,\n[00:00:39.000 --> 00:00:48.000]   Captain David Brown, Commander William McCool, Dr. Kultna Shavla, and Ilan Ramon,\n[00:00:48.000 --> 00:00:52.000]   a colonel in the Israeli Air Force.\n[00:00:52.000 --> 00:00:58.000]   These men and women assumed great risk in the service to all humanity.\n[00:00:58.000 --> 00:01:03.000]   In an age when space flight has come to seem almost routine,\n[00:01:03.000 --> 00:01:07.000]   it is easy to overlook the dangers of travel by rocket\n[00:01:07.000 --> 00:01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.\n[00:01:12.000 --> 00:01:18.000]   These astronauts knew the dangers, and they faced them willingly,\n[00:01:18.000 --> 00:01:23.000]   knowing they had a high and noble purpose in life.\n[00:01:23.000 --> 00:01:31.000]   Because of their courage and daring and idealism, we will miss them all the more.\n[00:01:31.000 --> 00:01:36.000]   All Americans today are thinking as well of the families of these men and women\n[00:01:36.000 --> 00:01:40.000]   who have been given this sudden shock and grief.\n[00:01:40.000 --> 00:01:45.000]   You're not alone. Our entire nation grieves with you,\n[00:01:45.000 --> 00:01:52.000]   and those you love will always have the respect and gratitude of this country.\n[00:01:52.000 --> 00:01:56.000]   The cause in which they died will continue.\n[00:01:56.000 --> 00:02:04.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery\n[00:02:04.000 --> 00:02:11.000]   and the longing to understand. Our journey into space will go on.\n[00:02:11.000 --> 00:02:16.000]   In the skies today, we saw destruction and tragedy.\n[00:02:16.000 --> 00:02:22.000]   Yet farther than we can see, there is comfort and hope.\n[00:02:22.000 --> 00:02:29.000]   In the words of the prophet Isaiah, \"Lift your eyes and look to the heavens\n[00:02:29.000 --> 00:02:35.000]   who created all these. He who brings out the starry hosts one by one\n[00:02:35.000 --> 00:02:39.000]   and calls them each by name.\"\n[00:02:39.000 --> 00:02:46.000]   Because of His great power and mighty strength, not one of them is missing.\n[00:02:46.000 --> 00:02:55.000]   The same Creator who names the stars also knows the names of the seven souls we mourn today.\n[00:02:55.000 --> 00:03:01.000]   The crew of the shuttle Columbia did not return safely to earth,\n[00:03:01.000 --> 00:03:05.000]   yet we can pray that all are safely home.\n[00:03:05.000 --> 00:03:13.000]   May God bless the grieving families, and may God continue to bless America.\n[00:03:13.000 --> 00:03:19.000]   [Silence]\n\n\nwhisper_print_timings:     fallbacks =   1 p /   0 h\nwhisper_print_timings:     load time =   569.03 ms\nwhisper_print_timings:      mel time =   146.85 ms\nwhisper_print_timings:   sample time =   238.66 ms /   553 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time = 18665.10 ms /     9 runs ( 2073.90 ms per run)\nwhisper_print_timings:   decode time = 13090.93 ms /   549 runs (   23.85 ms per run)\nwhisper_print_timings:    total time = 32733.52 ms\n```\n</details>\n\n## Real-time audio input example\n\nThis is a naive example of performing real-time inference on audio from your microphone.\nThe [stream](examples/stream) tool samples the audio every half a second and runs the transcription continously.\nMore info is available in [issue #10](https://github.com/ggerganov/whisper.cpp/issues/10).\n\n```java\nmake stream\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\nhttps://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4\n\n## Confidence color-coding\n\nAdding the `--print-colors` argument will print the transcribed text using an experimental color coding strategy\nto highlight words with high or low confidence:\n\n<img width=\"965\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png\">\n\n## Controlling the length of the generated text segments (experimental)\n\nFor example, to limit the line length to a maximum of 16 characters, simply add `-ml 16`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.850]   And so my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:04.140]   Americans, ask\n[00:00:04.140 --> 00:00:05.660]   not what your\n[00:00:05.660 --> 00:00:06.840]   country can do\n[00:00:06.840 --> 00:00:08.430]   for you, ask\n[00:00:08.430 --> 00:00:09.440]   what you can do\n[00:00:09.440 --> 00:00:10.020]   for your\n[00:00:10.020 --> 00:00:11.000]   country.\n```\n\n## Word-level timestamp\n\nThe `--max-len` argument can be used to obtain word-level timestamps. Simply use `-ml 1`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]  \n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n## Karaoke-style movie generation (experimental)\n\nThe [main](examples/main) example provides support for output of karaoke-style movies, where the\ncurrently pronounced word is highlighted. Use the `-wts` argument and run the generated bash script.\nThis requires to have `ffmpeg` installed.\n\nHere are a few *\"typical\"* examples:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4\n\n---\n\n## Video comparison of different models\n\nUse the [extra/bench-wts.sh](https://github.com/ggerganov/whisper.cpp/blob/master/extra/bench-wts.sh) script to generate a video in the following format:\n\n```java\n./extra/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4\n\n---\n\n## Benchmarks\n\nIn order to have an objective comparison of the performance of the inference across different system configurations,\nuse the [bench](examples/bench) tool. The tool simply runs the Encoder part of the model and prints how much time it\ntook to execute it. The results are summarized in the following Github issue:\n\n[Benchmark results](https://github.com/ggerganov/whisper.cpp/issues/89)\n\n## ggml format\n\nThe original models are converted to a custom binary format. This allows to pack everything needed into a single file:\n\n- model parameters\n- mel filters\n- vocabulary\n- weights\n\nYou can download the converted models using the [models/download-ggml-model.sh](models/download-ggml-model.sh) script\nor manually from here:\n\n- https://huggingface.co/datasets/ggerganov/whisper.cpp\n- https://ggml.ggerganov.com\n\nFor more details, see the conversion script [models/convert-pt-to-ggml.py](models/convert-pt-to-ggml.py) or the README\nin [models](models).\n\n## [Bindings](https://github.com/ggerganov/whisper.cpp/discussions/categories/bindings)\n\n- [X] Rust: [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) | [#310](https://github.com/ggerganov/whisper.cpp/discussions/310)\n- [X] Javascript: [bindings/javascript](bindings/javascript) | [#309](https://github.com/ggerganov/whisper.cpp/discussions/309)\n- [X] Go: [bindings/go](bindings/go) | [#312](https://github.com/ggerganov/whisper.cpp/discussions/312)\n- [X] Ruby: [bindings/ruby](bindings/ruby) | [#507](https://github.com/ggerganov/whisper.cpp/discussions/507)\n- [X] Objective-C / Swift: [ggerganov/whisper.spm](https://github.com/ggerganov/whisper.spm) | [#313](https://github.com/ggerganov/whisper.cpp/discussions/313)\n- [X] .NET: | [#422](https://github.com/ggerganov/whisper.cpp/discussions/422)\n  - [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net)\n  - [NickDarvey/whisper](https://github.com/NickDarvey/whisper)\n- [X] Python: | [#9](https://github.com/ggerganov/whisper.cpp/issues/9)\n  - [stlukey/whispercpp.py](https://github.com/stlukey/whispercpp.py) (Cython)\n  - [aarnphm/whispercpp](https://github.com/aarnphm/whispercpp) (Pybind11)\n\n## Examples\n\nThere are various examples of using the library for different projects in the [examples](examples) folder.\nSome of the examples are even ported to run in the browser using WebAssembly. Check them out!\n\n| Example | Web | Description |\n| ---     | --- | ---         |\n| [main](examples/main) | [whisper.wasm](examples/whisper.wasm) | Tool for translating and transcribing audio using Whisper |\n| [bench](examples/bench) | [bench.wasm](examples/bench.wasm) | Benchmark the performance of Whisper on your machine |\n| [stream](examples/stream) | [stream.wasm](examples/stream.wasm) | Real-time transcription of raw microphone capture |\n| [command](examples/command) | [command.wasm](examples/command.wasm) | Basic voice assistant example for receiving voice commands from the mic |\n| [talk](examples/talk) | [talk.wasm](examples/talk.wasm) | Talk with a GPT-2 bot |\n| [whisper.objc](examples/whisper.objc) | | iOS mobile application using whisper.cpp |\n| [whisper.swiftui](examples/whisper.swiftui) | | SwiftUI iOS / macOS application using whisper.cpp |\n| [whisper.android](examples/whisper.android) | | Android mobile application using whisper.cpp |\n| [whisper.nvim](examples/whisper.nvim) | | Speech-to-text plugin for Neovim |\n| [generate-karaoke.sh](examples/generate-karaoke.sh) | | Helper script to easily [generate a karaoke video](https://youtu.be/uj7hVta4blM) of raw audio capture |\n| [livestream.sh](examples/livestream.sh) | | [Livestream audio transcription](https://github.com/ggerganov/whisper.cpp/issues/185) |\n| [yt-wsp.sh](examples/yt-wsp.sh) | | Download + transcribe and/or translate any VOD [(original)](https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818) |\n\n## [Discussions](https://github.com/ggerganov/whisper.cpp/discussions)\n\nIf you have any kind of feedback about this project feel free to use the Discussions section and open a new topic.\nYou can use the [Show and tell](https://github.com/ggerganov/whisper.cpp/discussions/categories/show-and-tell) category\nto share your own projects that use `whisper.cpp`. If you have a question, make sure to check the\n[Frequently asked questions (#126)](https://github.com/ggerganov/whisper.cpp/discussions/126) discussion.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "bmorphism--say-mcp-server": {
      "owner": "bmorphism",
      "name": "say-mcp-server",
      "url": "https://github.com/bmorphism/say-mcp-server",
      "imageUrl": "https://github.com/bmorphism.png",
      "description": "Provides text-to-speech functionality using macOS's built-in `say` command, allowing the generation of spoken output from text input.",
      "stars": 18,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:52Z",
      "readme_content": "# say-mcp-server\n<a href=\"https://glama.ai/mcp/servers/lmmqoe15jp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lmmqoe15jp/badge\" alt=\"Say Server MCP server\" /></a>\n\n![macOS System Voice Settings](images/adding_voice.png)\n\nAn MCP server that provides text-to-speech functionality using macOS's built-in `say` command.\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n\n## Installation\n\n```bash\nnpm install say-mcp-server\n```\n\n## Tools\n\n### speak\n\nThe `speak` tool provides access to macOS's text-to-speech capabilities with extensive customization options.\n\n#### Basic Usage\n\nUse macOS text-to-speech to speak text aloud.\n\nParameters:\n- `text` (required): Text to speak. Supports:\n  - Plain text\n  - Basic punctuation for pauses\n  - Newlines for natural breaks\n  - [[slnc 500]] for 500ms silence\n  - [[rate 200]] for changing speed mid-text\n  - [[volm 0.5]] for changing volume mid-text\n  - [[emph +]] and [[emph -]] for emphasis\n  - [[pbas +10]] for pitch adjustment\n- `voice` (optional): Voice to use (default: \"Alex\")\n- `rate` (optional): Speaking rate in words per minute (default: 175, range: 1-500)\n- `background` (optional): Run speech in background to allow further MCP interaction (default: false)\n\n#### Advanced Features\n\n1. Voice Modulation:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[volm 0.7]] This is quieter [[volm 1.0]] and this is normal [[volm 1.5]] and this is louder\",\n    voice: \"Victoria\"\n  }\n});\n```\n\n2. Dynamic Rate Changes:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Normal speed [[rate 300]] now speaking faster [[rate 100]] and now slower\",\n    voice: \"Fred\"\n  }\n});\n```\n\n3. Emphasis and Pitch:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[emph +]] Important point! [[emph -]] [[pbas +10]] Higher pitch [[pbas -10]] Lower pitch\",\n    voice: \"Samantha\"\n  }\n});\n```\n\n#### Integration Examples\n\n1. With Marginalia Search:\n```typescript\n// Search for a topic and have the results read aloud\nconst searchResult = await use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"quantum computing basics\", count: 1 }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: searchResult.results[0].description,\n    voice: \"Daniel\",\n    rate: 150\n  }\n});\n```\n\n2. With YouTube Transcripts:\n```typescript\n// Read a YouTube video transcript\nconst transcript = await use_mcp_tool({\n  server_name: \"youtube-transcript\",\n  tool_name: \"get_transcript\",\n  arguments: {\n    url: \"https://youtube.com/watch?v=example\",\n    lang: \"en\"\n  }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: transcript.text,\n    voice: \"Samantha\",\n    rate: 175\n  }\n});\n```\n\n3. Background Speech with Multiple Actions:\n```typescript\n// Start long speech in background\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"This is a long speech that will run in the background...\",\n    voice: \"Rocko (Italian (Italy))\",\n    rate: 69,\n    background: true\n  }\n});\n\n// Immediately perform another action while speech continues\nawait use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"parallel processing\" }\n});\n```\n\n4. With Apple Notes:\n```typescript\n// Read notes aloud\nconst notes = await use_mcp_tool({\n  server_name: \"apple-notes-mcp\",\n  tool_name: \"search-notes\",\n  arguments: { query: \"meeting notes\" }\n});\n\nif (notes.length > 0) {\n  await use_mcp_tool({\n    server_name: \"say\",\n    tool_name: \"speak\",\n    arguments: {\n      text: notes[0].content,\n      voice: \"Karen\",\n      rate: 160\n    }\n  });\n}\n```\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Hello, world!\",\n    voice: \"Victoria\",\n    rate: 200\n  }\n});\n```\n\n### list_voices\n\nList all available text-to-speech voices on the system.\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"list_voices\",\n  arguments: {}\n});\n```\n\n## Recommended Voices\n\n<table>\n<tr>\n<th>Voice</th>\n<th>Language/Region</th>\n<th>Intellectual Figure</th>\n<th>Haiku</th>\n<th>CLI Specification</th>\n</tr>\n<tr>\n<td>Anna (Premium)</td>\n<td>German</td>\n<td>Emmy Noether</td>\n<td>Symmetrie haucht Leben<br>Algebras verborgne Form<br>Abstraktion blüht<br><br><i>Symmetry breathes life<br>Algebra's hidden forms<br>Abstraction blooms</i></td>\n<td><code>-v \"Anna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Emma (Premium)</td>\n<td>Italian</td>\n<td>Maria Adelaide Sneider</td>\n<td>Algoritmi in danza<br>Macchina sussurra dolce<br>Il codice vive<br><br><i>Algorithms dance<br>Machine whispers secrets soft<br>Code becomes alive</i></td>\n<td><code>-v \"Emma (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Federica (Premium)</td>\n<td>Italian</td>\n<td>Pia Nalli</td>\n<td>Teoremi fluenti<br>Numeri danzano liberi<br>Verità emerge<br><br><i>Flowing theorems dance<br>Numbers move in freedom's space<br>Truth emerges pure</i></td>\n<td><code>-v \"Federica (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Serena (Premium)</td>\n<td>English (UK)</td>\n<td>Bertha Swirles</td>\n<td>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges<br><br><i>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges</i></td>\n<td><code>-v \"Serena (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Petra (Premium)</td>\n<td>German</td>\n<td>Ruth Moufang</td>\n<td>Algebra spricht<br>In Symmetrien versteckt<br>Wahrheit erblüht<br><br><i>Algebra speaks soft<br>Hidden in symmetries pure<br>Truth blooms anew here</i></td>\n<td><code>-v \"Petra (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Yuna (Premium)</td>\n<td>Korean</td>\n<td>Hee Oh</td>\n<td>숨은 패턴 빛나고<br>마음의 방정식 핀다<br>지식 자라나<br><br><i>Hidden patterns gleam<br>Mind's equations softly bloom<br>Knowledge multiplies</i></td>\n<td><code>-v \"Yuna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Alva (Premium)</td>\n<td>Swedish</td>\n<td>Sonja Korovkin</td>\n<td>Mönster flödar fritt<br>Genom tankens labyrinter<br>Visdom blomstrar här<br><br><i>Patterns flowing free<br>Through labyrinths of the mind<br>Wisdom blooms right here</i></td>\n<td><code>-v \"Alva (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Amélie (Premium)</td>\n<td>French (Canada)</td>\n<td>Sophie Germain</td>\n<td>Nombres premiers murmurent<br>Dansent entre les silences<br>Symétrie s'ouvre<br><br><i>Prime numbers whisper<br>Dancing between the silence<br>Symmetry unfolds</i></td>\n<td><code>-v \"Amélie (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Ewa (Premium)</td>\n<td>Polish</td>\n<td>Maria Wielgus</td>\n<td>Logiki korzenie<br>Matematyczne krainy<br>Myśl kiełkująca<br><br><i>Logic's tender roots<br>Mathematical landscapes<br>Thought's seeds germinate</i></td>\n<td><code>-v \"Ewa (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Kiyara (Premium)</td>\n<td>Hindi</td>\n<td>Shakuntala Devi</td>\n<td>गणित की लय में<br>अंक नृत्य करते हैं<br>ज्ञान जगता है<br><br><i>In rhythm of math<br>Numbers dance their sacred steps<br>Knowledge awakens</i></td>\n<td><code>-v \"Kiyara (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Majed (Premium)</td>\n<td>Arabic</td>\n<td>Maha Al-Aswad</td>\n<td>أرقام ترقص<br>في فضاء اللانهاية<br>الحقيقة تشرق<br><br><i>Numbers dance freely<br>In infinity's vast space<br>Truth rises like dawn</i></td>\n<td><code>-v \"Majed (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tünde (Premium)</td>\n<td>Hungarian</td>\n<td>Julia Erdős</td>\n<td>Számok táncolnak<br>Végtelen térben szállnak<br>Igazság virrad<br><br><i>Numbers dance and soar<br>Through infinite space they glide<br>Truth dawns pure and bright</i></td>\n<td><code>-v \"Tünde (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Fiona (Enhanced)</td>\n<td>English (Scottish)</td>\n<td>Mary Somerville</td>\n<td>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars<br><br><i>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars</i></td>\n<td><code>-v \"Fiona (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Lesya (Enhanced)</td>\n<td>Ukrainian</td>\n<td>Olena Voinova</td>\n<td>Тиша говорить<br>Між зірками знання спить<br>Думка проростає<br><br><i>Silence speaks softly<br>Knowledge sleeps among the stars<br>Thought begins to grow</i></td>\n<td><code>-v \"Lesya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Carmit (Enhanced)</td>\n<td>Hebrew</td>\n<td>Tali Seror</td>\n<td>מילים נושמות בשקט<br>בין שורות של דממה<br>שיר מתעורר<br><br><i>Words breathe silently<br>Between lines of deep stillness<br>Poem awakening</i></td>\n<td><code>-v \"Carmit (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Milena (Enhanced)</td>\n<td>Russian</td>\n<td>Olga Ladyzhenskaya</td>\n<td>Память шепчет нам<br>Уравнения текут<br>Истина молчит<br><br><i>Memory whispers<br>Equations flow like rivers<br>Truth speaks silently</i></td>\n<td><code>-v \"Milena (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Katya (Enhanced)</td>\n<td>Russian</td>\n<td>Sofia Kovalevskaya</td>\n<td>Числа танцуют<br>В пространстве бесконечном<br>Истина цветёт<br><br><i>Numbers dance freely<br>In space of infinity<br>Truth blooms like a flower</i></td>\n<td><code>-v \"Katya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Damayanti (Enhanced)</td>\n<td>Indonesian</td>\n<td>Sri Pekerti</td>\n<td>Angka menari<br>Dalam ruang tak batas<br>Kebenaran tumbuh<br><br><i>Numbers dance gently<br>In boundless space they flutter<br>Truth grows like new leaves</i></td>\n<td><code>-v \"Damayanti (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Dariush (Enhanced)</td>\n<td>Persian</td>\n<td>Maryam Mirzakhani</td>\n<td>اعداد می رقصند<br>در فضای بی پایان<br>حقیقت می روید<br><br><i>Numbers dance with grace<br>In endless space they traverse<br>Truth springs forth anew</i></td>\n<td><code>-v \"Dariush (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Astro Boy (Tetsuwan Atomu)<br>Italian dub</td>\n<td>Robot di metallo<br>Cuore umano batte forte<br>Pace nel futuro<br><br><i>Metal robot form<br>Human heart beats strong within<br>Peace in future dawns</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Jeeg Robot d'Acciaio<br>(Kōtetsu Jeeg)</td>\n<td>Acciaio lucente<br>Protettore dei deboli<br>Vola nel cielo<br><br><i>Shining steel warrior<br>Protector of the helpless<br>Soars through the heavens</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Numero 5<br>(Short Circuit)</td>\n<td>Input infinito<br>La coscienza si risveglia<br>Vita artificiale<br><br><i>Infinite input<br>Consciousness awakening<br>Artificial life</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Binbin (Enhanced)</td>\n<td>Chinese (Mainland)</td>\n<td>Li Shanlan</td>\n<td>算术之道流<br>数理演绎真理<br>智慧绽放<br><br><i>Arithmetic flows<br>Logic unfolds truth's pattern<br>Wisdom blossoms bright</i></td>\n<td><code>-v \"Binbin (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Han (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chen Jingrun</td>\n<td>素数之舞动<br>哥德巴赫猜想<br>真理永恒<br><br><i>Prime numbers dancing<br>Goldbach's conjecture whispers<br>Truth eternal flows</i></td>\n<td><code>-v \"Han (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Lilian (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Hua Luogeng</td>\n<td>数论之光芒<br>解析延续美<br>智慧升华<br><br><i>Number theory shines<br>Analysis extends grace<br>Wisdom ascends pure</i></td>\n<td><code>-v \"Lilian (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Meijia</td>\n<td>Chinese (Taiwan)</td>\n<td>Sun-Yung Alice Chang</td>\n<td>幾何之美現<br>曲率流動不息<br>空間展開<br><br><i>Geometry shows<br>Curvature flows endlessly<br>Space unfolds anew</i></td>\n<td><code>-v \"Meijia\"</code></td>\n</tr>\n<tr>\n<td>Sinji (Premium)</td>\n<td>Chinese (Hong Kong)</td>\n<td>Shing-Tung Yau</td>\n<td>流形之奧秘<br>卡拉比空間動<br>維度交織<br><br><i>Manifolds reveal<br>Calabi spaces in flow<br>Dimensions weave truth</i></td>\n<td><code>-v \"Sinji (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tingting</td>\n<td>Chinese (Mainland)</td>\n<td>Wang Zhenyi</td>\n<td>星辰轨迹明<br>天文数学融<br>智慧闪耀<br><br><i>Starlit paths shine bright<br>Astronomy meets numbers<br>Wisdom radiates</i></td>\n<td><code>-v \"Tingting\"</code></td>\n</tr>\n<tr>\n<td>Yue (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chern Shiing-shen</td>\n<td>微分几何<br>纤维丛中寻真<br>本质显现<br><br><i>Differential forms<br>In fiber bundles seek truth<br>Essence emerges</i></td>\n<td><code>-v \"Yue (Premium)\"</code></td>\n</tr>\n</table>\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Contributors\n\n- Barton Rhodes ([@bmorphism](https://github.com/bmorphism)) - barton@vibes.lol\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "CengSin--fishaudio-mcp": {
      "owner": "CengSin",
      "name": "fishaudio-mcp",
      "url": "https://github.com/CengSin/fishaudio-mcp",
      "imageUrl": "https://github.com/CengSin.png",
      "description": "Converts text into natural human speech with customizable audio formats and bitrates, while integrating seamlessly with MCP-compatible applications.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T10:49:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cengsin-fishaudio-mcp-badge.png)](https://mseep.ai/app/cengsin-fishaudio-mcp)\n\n# Fish Audio Python MCP 服务\n\n这是一个使用 Fish Audio API 实现的文字转语音 MCP 服务。通过这个服务，您可以将文本转换为自然的人声，支持多种配置选项。\n\n## 功能特点\n\n- 基本文字转语音：将任意文本转换为自然人声\n- 高级文字转语音：支持自定义音频格式、比特率等参数\n- 兼容 MCP 协议：可与支持 MCP 的应用无缝集成\n\n## 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n或使用 Python 包管理工具安装：\n\n```bash\npip install fish-audio-sdk mcp python-dotenv\n```\n\n## 配置\n\n在项目根目录创建 `.env` 文件，包含以下内容：\n\n```\nAPI_KEY=your_fish_audio_api_key\nMODEL_ID=your_fish_audio_model_id\n```\n\n您需要替换为您的 Fish Audio API 密钥和模型 ID。\n\n## 使用方法\n\n### 启动服务\n\n```bash\npython app.py\n```\n\n或使用 MCP CLI 工具：\n\n```bash\nmcp run --file app.py\n```\n\n### 运行示例\n\n```bash\npython example.py\n```\n\n### 使用 MCP 客户端调用服务\n\n```python\n# 示例代码\nfrom mcp.client import MCPClient\n\nclient = MCPClient(\"subprocess://python app.py\")\nresult = client.call(\"text_to_speech\", {\"text\": \"你好，世界！\"})\nprint(result)  # 打印生成的音频文件路径\n```\n\n## API 功能说明\n\n### text_to_speech\n\n基本文字转语音功能。\n\n参数：\n- `text`: 要转换为语音的文本\n- `output_path`（可选）: 输出文件路径，如果不提供，将创建临时文件\n\n返回：生成的音频文件路径\n\n### advanced_text_to_speech\n\n高级文字转语音功能，支持更多配置选项。\n\n参数：\n- `text`: 要转换为语音的文本\n- `output_path`（可选）: 输出文件路径，如果不提供，将创建临时文件\n- `format`: 输出音频格式 (mp3, wav, pcm)，默认为 mp3\n- `mp3_bitrate`: MP3 比特率 (64, 128, 192 kbps)，默认为 128\n- `chunk_length`: 分块长度 (100-300)，默认为 200\n- `normalize`: 是否对文本进行标准化处理，默认为 True\n- `latency`: 延迟模式 (normal, balanced)，默认为 normal\n\n返回：生成的音频文件路径\n\n### get_model_info\n\n获取当前使用的模型信息。\n\n返回：包含模型 ID 和 API 密钥前缀的字典\n\n### get_available_models\n\n获取可用的 Fish Audio 模型列表。\n\n返回：可用模型信息列表\n\n## 许可证\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "DefiBax--mcp_servers": {
      "owner": "DefiBax",
      "name": "mcp_servers",
      "url": "https://github.com/DefiBax/mcp_servers",
      "imageUrl": "https://github.com/DefiBax.png",
      "description": "Record audio and transcribe it using advanced AI models like OpenAI's Whisper. Supports integration with AI agents for enhanced interactivity and includes prompts for common recording scenarios.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-11T08:14:19Z",
      "readme_content": "# Voice Recorder MCP Server\n\nAn MCP server for recording audio and transcribing it using OpenAI's Whisper model. Designed to work as a Goose custom extension or standalone MCP server.\n\n## Features\n\n- Record audio from the default microphone\n- Transcribe recordings using Whisper\n- Integrates with Goose AI agent as a custom extension\n- Includes prompts for common recording scenarios\n\n## Installation\n\n```bash\n# Install from source\ngit clone https://github.com/DefiBax/voice-recorder-mcp.git\ncd voice-recorder-mcp\npip install -e .\n```\n\n## Usage\n\n### As a Standalone MCP Server\n\n```bash\n# Run with default settings (base.en model)\nvoice-recorder-mcp\n\n# Use a specific Whisper model\nvoice-recorder-mcp --model medium.en\n\n# Adjust sample rate\nvoice-recorder-mcp --sample-rate 44100\n```\n\n### Testing with MCP Inspector\n\nThe MCP Inspector provides an interactive interface to test your server:\n\n```bash\n# Install the MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run your server with the inspector\nnpx @modelcontextprotocol/inspector voice-recorder-mcp\n```\n\n### With Goose AI Agent\n\n1. Open Goose and go to Settings > Extensions > Add > Command Line Extension\n2. Set the name to `voice-recorder`\n3. In the Command field, enter the full path to the voice-recorder-mcp executable:\n   ```\n   /full/path/to/voice-recorder-mcp\n   ```\n   \n   Or for a specific model:\n   ```\n   /full/path/to/voice-recorder-mcp --model medium.en\n   ```\n   \n   To find the path, run:\n   ```bash\n   which voice-recorder-mcp\n   ```\n\n4. No environment variables are needed for basic functionality\n5. Start a conversation with Goose and introduce the recorder with:\n   \"I want you to take action from transcriptions returned by voice-recorder. For example, if I dictate a calculation like 1+1, please return the result.\"\n\n## Available Tools\n\n- `start_recording`: Start recording audio from the default microphone\n- `stop_and_transcribe`: Stop recording and transcribe the audio to text\n- `record_and_transcribe`: Record audio for a specified duration and transcribe it\n\n## Whisper Models\n\nThis extension supports various Whisper model sizes:\n\n| Model | Speed | Accuracy | Memory Usage | Use Case |\n|-------|-------|----------|--------------|----------|\n| `tiny.en` | Fastest | Lowest | Minimal | Testing, quick transcriptions |\n| `base.en` | Fast | Good | Low | Everyday use (default) |\n| `small.en` | Medium | Better | Moderate | Good balance |\n| `medium.en` | Slow | High | High | Important recordings |\n| `large` | Slowest | Highest | Very High | Critical transcriptions |\n\nThe `.en` suffix indicates models specialized for English, which are faster and more accurate for English content.\n\n## Requirements\n\n- Python 3.12+\n- An audio input device (microphone)\n\n## Configuration\n\nYou can configure the server using environment variables:\n\n```bash\n# Set Whisper model\nexport WHISPER_MODEL=small.en\n\n# Set audio sample rate\nexport SAMPLE_RATE=44100\n\n# Set maximum recording duration (seconds)\nexport MAX_DURATION=120\n\n# Then run the server\nvoice-recorder-mcp\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **No audio being recorded**: Check your microphone permissions and settings\n- **Model download errors**: Ensure you have a stable internet connection for the initial model download\n- **Integration with Goose**: Make sure the command path is correct\n- **Audio quality issues**: Try adjusting the sample rate (default: 16000)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Dosugamea--voicevox-mcp-server": {
      "owner": "Dosugamea",
      "name": "voicevox-mcp-server",
      "url": "https://github.com/Dosugamea/voicevox-mcp-server",
      "imageUrl": "https://github.com/Dosugamea.png",
      "description": "Provides voice synthesis capabilities compatible with VOICEVOX and similar engines through the Model Context Protocol. Facilitates speech audio generation using AI agents compatible with MCP clients.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-17T09:32:07Z",
      "readme_content": "# Voicevox MCP Server\r\n\r\nVOICEVOX互換の音声合成サーバー(AivisSpeech / VOICEVOX / COEIROINK) を MCP (Model Context Protocol) 経由で利用するためのサーバーです。\r\nCursor等でのClaude 3.7を使ったエージェントモードでの音声合成に利用できます。\r\n\r\n## 必要条件\r\n\r\n### Windows環境\r\n\r\n- Node.js 18以上\r\n- VOICEVOX ENGINE等 (ローカルでhttp://localhost:50000等で実行)\r\n- VLCメディアプレーヤー（パスが通っていること）\r\n\r\n### Docker環境 (WSL2)\r\n\r\n- Docker と Docker Compose\r\n- WSL2\r\n- VOICEVOX ENGINE等 (ローカルまたはDockerで実行)\r\n- `sudo apt install libsdl2-dev pulseaudio-utils pulseaudio` されたLinux環境\r\n- `/mnt/wslg` へのアクセス権限\r\n\r\n## インストールと設定\r\n\r\n1. リポジトリをクローン\r\n```\r\ngit clone https://github.com/Dosugamea/voicevox-mcp-server.git\r\ncd voicevox-mcp-server\r\n```\r\n\r\n2. 依存関係のインストール\r\n```\r\nnpm install\r\n```\r\n\r\n3. 環境変数の設定\r\n`.env_example` をコピーして `.env` ファイルを作成し、必要に応じて設定を変更します:\r\n```\r\nVOICEVOX_API_URL=http://localhost:50021\r\nVOICEVOX_SPEAKER_ID=1\r\n```\r\n\r\n## 実行方法\r\n\r\n### Windows環境での実行\r\nエディタと別途で下記手順でサーバーを立ち上げてください。\r\n\r\n```\r\nnpm run build\r\nnpm start\r\n```\r\n\r\n### Docker環境での実行\r\nエディタと別途での操作は不要です。\r\nstdioモードで立ち上がるため直接実行することはできません。\r\n\r\n## 設定方法\r\n\r\n### Windows環境での実行の場合\r\nmcp.jsonに下記を追記してください。\r\n接続が不安定なため切断されたら再接続してください。\r\n\r\n```json\r\n        \"voicevox\": {\r\n            \"url\": \"http://localhost:10100/sse\"\r\n        }\r\n```\r\n\r\n### Docker環境での実行の場合\r\nmcp.jsonに下記を追記してください。\r\n(作者環境での動作は確認できていません)\r\n\r\n```json\r\n{\r\n    \"tools\": {\r\n        \"voicevox\": {\r\n            \"command\": \"cmd\",\r\n            \"args\": [\r\n                \"/c\",\r\n                \"docker\",\r\n                \"run\",\r\n                \"-i\",\r\n                \"--rm\",\r\n                \"-v\",\r\n                \"/mnt/wslg:/mnt/wslg\",\r\n                \"-e\",\r\n                \"PULSE_SERVER\",\r\n                \"-e\",\r\n                \"SDL_AUDIODRIVER\",\r\n                \"-e\",\r\n                \"VOICEVOX_API_URL\",\r\n                \"-e\",\r\n                \"VOICEVOX_SPEAKER_ID\",\r\n                \"your-local-docker-image-name\"\r\n            ],\r\n            \"env\": {\r\n                \"PULSE_SERVER\": \"unix:/mnt/wslg/PulseServer\",\r\n                \"SDL_AUDIODRIVER\": \"pulseaudio\",\r\n                \"VOICEVOX_API_URL\": \"http://host.docker.internal:50031\",\r\n                \"VOICEVOX_SPEAKER_ID\": \"919692871\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## 話者IDについて\r\n\r\n話者IDは使用するVOICEVOXのモデルによって異なります。デフォルトでは「1」（四国めたん）を使用しています。\r\n他の話者IDを使用する場合は、環境変数 `VOICEVOX_SPEAKER_ID` を変更してください。\r\n\r\n話者IDの一覧は、VOICEVOX ENGINE APIの `/speakers` エンドポイントで確認できます。\r\n例: `curl http://localhost:50021/speakers`\r\n\r\n## トラブルシューティング\r\n\r\n- **VOICEVOXとの接続エラー**: VOICEVOX ENGINEが起動していること、APIのURLが正しく設定されていることを確認してください。\r\n- **音声が再生されない**: VLCが正しくインストールされていることと、パスが通っていることを確認してください。\r\n- **Docker環境での音声出力問題**: pulseaudioの設定が正しいか確認してください。\r\n\r\n## 開発者向け情報\r\n\r\n- ソースコードに貢献する場合は、Issueを作成するか、Pull Requestを送信してください。\r\n- バグ報告や機能リクエストは、GitHubのIssue機能をご利用ください。\r\n\r\n## ライセンス\r\n\r\nMIT License",
      "npm_url": "",
      "npm_downloads": 0
    },
    "elevenlabs--elevenlabs-mcp": {
      "owner": "elevenlabs",
      "name": "elevenlabs-mcp",
      "url": "https://github.com/elevenlabs/elevenlabs-mcp",
      "imageUrl": "https://github.com/elevenlabs.png",
      "description": "This server provides APIs for generating speech, voice cloning, and audio transcription. It facilitates seamless interaction with text-to-speech and audio processing functionalities.",
      "stars": 998,
      "forks": 161,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:34:19Z",
      "readme_content": "![export](https://github.com/user-attachments/assets/ee379feb-348d-48e7-899c-134f7f7cd74f)\n\n<div class=\"title-block\" style=\"text-align: center;\" align=\"center\">\n\n  [![Discord Community](https://img.shields.io/badge/discord-@elevenlabs-000000.svg?style=for-the-badge&logo=discord&labelColor=000)](https://discord.gg/elevenlabs)\n  [![Twitter](https://img.shields.io/badge/Twitter-@elevenlabsio-000000.svg?style=for-the-badge&logo=twitter&labelColor=000)](https://x.com/ElevenLabsDevs)\n  [![PyPI](https://img.shields.io/badge/PyPI-elevenlabs--mcp-000000.svg?style=for-the-badge&logo=pypi&labelColor=000)](https://pypi.org/project/elevenlabs-mcp)\n  [![Tests](https://img.shields.io/badge/tests-passing-000000.svg?style=for-the-badge&logo=github&labelColor=000)](https://github.com/elevenlabs/elevenlabs-mcp-server/actions/workflows/test.yml)\n\n</div>\n\n\n<p align=\"center\">\n  Official ElevenLabs <a href=\"https://github.com/modelcontextprotocol\">Model Context Protocol (MCP)</a> server that enables interaction with powerful Text to Speech and audio processing APIs. This server allows MCP clients like <a href=\"https://www.anthropic.com/claude\">Claude Desktop</a>, <a href=\"https://www.cursor.so\">Cursor</a>, <a href=\"https://codeium.com/windsurf\">Windsurf</a>, <a href=\"https://github.com/openai/openai-agents-python\">OpenAI Agents</a> and others to generate speech, clone voices, transcribe audio, and more.\n</p>\n\n<!--\nmcp-name: io.github.elevenlabs/elevenlabs-mcp\n-->\n\n## Quickstart with Claude Desktop\n\n1. Get your API key from [ElevenLabs](https://elevenlabs.io/app/settings/api-keys). There is a free tier with 10k credits per month.\n2. Install `uv` (Python package manager), install with `curl -LsSf https://astral.sh/uv/install.sh | sh` or see the `uv` [repo](https://github.com/astral-sh/uv) for additional install methods.\n3. Go to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```\n{\n  \"mcpServers\": {\n    \"ElevenLabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"<insert-your-api-key-here>\"\n      }\n    }\n  }\n}\n\n```\n\nIf you're using Windows, you will have to enable \"Developer Mode\" in Claude Desktop to use the MCP server. Click \"Help\" in the hamburger menu at the top left and select \"Enable Developer Mode\".\n\n## Other MCP clients\n\nFor other clients like Cursor and Windsurf, run:\n1. `pip install elevenlabs-mcp`\n2. `python -m elevenlabs_mcp --api-key={{PUT_YOUR_API_KEY_HERE}} --print` to get the configuration. Paste it into appropriate configuration directory specified by your MCP client.\n\nThat's it. Your MCP client can now interact with ElevenLabs through these tools:\n\n## Example usage\n\n⚠️ Warning: ElevenLabs credits are needed to use these tools.\n\nTry asking Claude:\n\n- \"Create an AI agent that speaks like a film noir detective and can answer questions about classic movies\"\n- \"Generate three voice variations for a wise, ancient dragon character, then I will choose my favorite voice to add to my voice library\"\n- \"Convert this recording of my voice to sound like a medieval knight\"\n- \"Create a soundscape of a thunderstorm in a dense jungle with animals reacting to the weather\"\n- \"Turn this speech into text, identify different speakers, then convert it back using unique voices for each person\"\n\n## Optional features\n\n### File Output Configuration\n\nYou can configure how the MCP server handles file outputs using these environment variables in your `claude_desktop_config.json`:\n\n- **`ELEVENLABS_MCP_BASE_PATH`**: Specify the base path for file operations with relative paths (default: `~/Desktop`)\n- **`ELEVENLABS_MCP_OUTPUT_MODE`**: Control how generated files are returned (default: `files`)\n\n#### Output Modes\n\nThe `ELEVENLABS_MCP_OUTPUT_MODE` environment variable supports three modes:\n\n1. **`files`** (default): Save files to disk and return file paths\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"files\"\n   }\n   ```\n\n2. **`resources`**: Return files as MCP resources; binary data is base64-encoded, text is returned as UTF-8 text\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"resources\"\n   }\n   ```\n\n3. **`both`**: Save files to disk AND return as MCP resources\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"both\"\n   }\n   ```\n\n**Resource Mode Benefits:**\n- Files are returned directly in the MCP response as base64-encoded data\n- No disk I/O required - useful for containerized or serverless environments\n- MCP clients can access file content immediately without file system access\n- In `both` mode, resources can be fetched later using the `elevenlabs://filename` URI pattern\n\n**Use Cases:**\n- `files`: Traditional file-based workflows, local development\n- `resources`: Cloud environments, MCP clients without file system access\n- `both`: Maximum flexibility, caching, and resource sharing scenarios\n\n### Data residency keys\n\nYou can specify the data residency region with the `ELEVENLABS_API_RESIDENCY` environment variable. Defaults to `\"us\"`.\n\n**Note:** Data residency is an enterprise only feature. See [the docs](https://elevenlabs.io/docs/product-guides/administration/data-residency#overview) for more details.\n\n## Contributing\n\nIf you want to contribute or run from source:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/elevenlabs/elevenlabs-mcp\ncd elevenlabs-mcp\n```\n\n2. Create a virtual environment and install dependencies [using uv](https://github.com/astral-sh/uv):\n\n```bash\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n```\n\n3. Copy `.env.example` to `.env` and add your ElevenLabs API key:\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key\n```\n\n4. Run the tests to make sure everything is working:\n\n```bash\n./scripts/test.sh\n# Or with options\n./scripts/test.sh --verbose --fail-fast\n```\n\n5. Install the server in Claude Desktop: `mcp install elevenlabs_mcp/server.py`\n\n6. Debug and test locally with MCP Inspector: `mcp dev elevenlabs_mcp/server.py`\n\n## Troubleshooting\n\nLogs when running with Claude Desktop can be found at:\n\n- **Windows**: `%APPDATA%\\Claude\\logs\\mcp-server-elevenlabs.log`\n- **macOS**: `~/Library/Logs/Claude/mcp-server-elevenlabs.log`\n\n### Timeouts when using certain tools\n\nCertain ElevenLabs API operations, like voice design and audio isolation, can take a long time to resolve. When using the MCP inspector in dev mode, you might get timeout errors despite the tool completing its intended task.\n\nThis shouldn't occur when using a client like Claude.\n\n### MCP ElevenLabs: spawn uvx ENOENT\n\nIf you encounter the error \"MCP ElevenLabs: spawn uvx ENOENT\", confirm its absolute path by running this command in your terminal:\n\n```bash\nwhich uvx\n```\n\nOnce you obtain the absolute path (e.g., `/usr/local/bin/uvx`), update your configuration to use that path (e.g., `\"command\": \"/usr/local/bin/uvx\"`). This ensures that the correct executable is referenced.\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "georgi-io--jessica": {
      "owner": "georgi-io",
      "name": "jessica",
      "url": "https://github.com/georgi-io/jessica",
      "imageUrl": "https://github.com/georgi-io.png",
      "description": "Integrates ElevenLabs Text-to-Speech capabilities for seamless text conversion to speech, offering voice selection and management through a modern interface. Supports real-time communication with a FastAPI backend and a React frontend.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-25T23:18:06Z",
      "readme_content": "# Project Jessica (ElevenLabs TTS MCP)\n\nThis project integrates ElevenLabs Text-to-Speech capabilities with Cursor through the Model Context Protocol (MCP). It consists of a FastAPI backend service and a React frontend application.\n\n## Features\n\n- Text-to-Speech conversion using ElevenLabs API\n- Voice selection and management\n- MCP integration for Cursor\n- Modern React frontend interface\n- WebSocket real-time communication\n- Pre-commit hooks for code quality\n- Automatic code formatting and linting\n\n## Project Structure\n\n```\njessica/\n├── src/\n│   ├── backend/          # FastAPI backend service\n│   └── frontend/         # React frontend application\n├── terraform/            # Infrastructure as Code\n├── tests/               # Test suites\n└── docs/                # Documentation\n```\n\n## Requirements\n\n- Python 3.11+\n- Poetry (for backend dependency management)\n- Node.js 18+ (for frontend)\n- Cursor (for MCP integration)\n\n## Local Development Setup\n\n### Backend Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/georgi-io/jessica.git\ncd jessica\n\n# Create Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install backend dependencies\npoetry install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your ElevenLabs API key\n\n# Install pre-commit hooks\npoetry run pre-commit install\n```\n\n### Frontend Setup\n\n```bash\n# Navigate to frontend directory\ncd src/frontend\n\n# Install dependencies\nnpm install\n```\n\n## Development Servers\n\n### Starting the Backend\n\n```bash\n# Activate virtual environment if not active\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Start the backend\npython -m src.backend\n```\n\nThe backend provides:\n- REST API: http://localhost:9020\n- WebSocket: ws://localhost:9020/ws\n- MCP Server: http://localhost:9020/sse (integrated with the main API server)\n\n### Starting the Frontend\n\n```bash\n# In src/frontend directory\nnpm run dev\n```\n\nFrontend development server:\n- http://localhost:5173\n\n## Environment Configuration\n\n### Backend (.env)\n```env\n# ElevenLabs API\nELEVENLABS_API_KEY=your-api-key\n\n# Server Configuration\nHOST=127.0.0.1\nPORT=9020\n\n# Development Settings\nDEBUG=false\nRELOAD=true\n```\n\n### Frontend (.env)\n```env\nVITE_API_URL=http://localhost:9020\nVITE_WS_URL=ws://localhost:9020/ws\n```\n\n## Code Quality Tools\n\n### Backend\n\n```bash\n# Run all pre-commit hooks\npoetry run pre-commit run --all-files\n\n# Run specific tools\npoetry run ruff check .\npoetry run ruff format .\npoetry run pytest\n```\n\n### Frontend\n\n```bash\n# Lint\nnpm run lint\n\n# Type check\nnpm run type-check\n\n# Test\nnpm run test\n```\n\n## Production Deployment\n\n### AWS ECR and GitHub Actions Setup\n\nTo enable automatic building and pushing of Docker images to Amazon ECR:\n\n1. Apply the Terraform configuration to create the required AWS resources:\n   ```bash\n   cd terraform\n   terraform init\n   terraform apply\n   ```\n\n2. The GitHub Actions workflow will automatically:\n   - Read the necessary configuration from the Terraform state in S3\n   - Build the Docker image on pushes to `main` or `develop` branches\n   - Push the image to ECR with tags for `latest` and the specific commit SHA\n\n3. No additional repository variables needed! The workflow fetches all required configuration from the Terraform state.\n\n### How it Works\n\nThe GitHub Actions workflow is configured to:\n1. Initially assume a predefined IAM role with S3 read permissions\n2. Fetch and extract configuration values from the Terraform state file in S3\n3. Re-authenticate using the actual deployment role from the state file\n4. Build and push the Docker image to the ECR repository defined in the state\n\nThis approach eliminates the need to manually configure GitHub repository variables and ensures that the CI/CD process always uses the current infrastructure configuration.\n\n### Quick Overview\n\n- Frontend: Served from S3 via CloudFront at jessica.georgi.io\n- Backend API: Available at api.georgi.io/jessica\n- WebSocket: Connects to api.georgi.io/jessica/ws\n- Docker Image: Stored in AWS ECR and can be deployed to ECS/EKS\n- Infrastructure: Managed via Terraform in this repository\n\n## MCP Integration with Cursor\n\n1. Start the backend server\n2. In Cursor settings, add new MCP server:\n   - Name: Jessica TTS\n   - Type: SSE\n   - URL: http://localhost:9020/sse\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Issues**\n   - Error: \"Invalid API key\"\n   - Solution: Check `.env` file\n\n2. **Connection Problems**\n   - Error: \"Cannot connect to MCP server\"\n   - Solution: Verify backend is running and ports are correct\n\n3. **Port Conflicts**\n   - Error: \"Address already in use\"\n   - Solution: Change ports in `.env`\n\n4. **WebSocket Connection Failed**\n   - Error: \"WebSocket connection failed\"\n   - Solution: Ensure backend is running and WebSocket URL is correct\n\nFor additional help, please open an issue on GitHub.\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "giannisanni--kokoro-tts-mcp": {
      "owner": "giannisanni",
      "name": "kokoro-tts-mcp",
      "url": "https://github.com/giannisanni/kokoro-tts-mcp",
      "imageUrl": "https://github.com/giannisanni.png",
      "description": "Integrates text-to-speech capabilities using the Kokoro TTS engine, enabling conversion of written content into spoken audio with customizable voices and adjustable speed. Supports saving audio files and cross-platform playback.",
      "stars": 10,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-28T13:54:19Z",
      "readme_content": "# Kokoro TTS MCP Server\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Kokoro TTS engine. This server exposes TTS functionality through MCP tools, making it easy to integrate speech synthesis into your applications.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- `uv` package manager\n\n## Installation\n\n1. First, install the `uv` package manager:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Clone this repository and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\nuv pip install .\n```\n\n## Features\n\n- Text-to-speech synthesis with customizable voices\n- Adjustable speech speed\n- Support for saving audio to files or direct playback\n- Cross-platform audio playback support (Windows, macOS, Linux)\n\n## Usage\n\nThe server provides a single MCP tool `generate_speech` with the following parameters:\n\n- `text` (required): The text to convert to speech\n- `voice` (optional): Voice to use for synthesis (default: \"af_heart\")\n- `speed` (optional): Speech speed multiplier (default: 1.0)\n- `save_path` (optional): Directory to save audio files\n- `play_audio` (optional): Whether to play the audio immediately (default: False)\n\n### Example Usage\n\n```python\nfrom mcp.client import Client\n\nasync with Client() as client:\n    await client.connect(\"kokoro-tts\")\n    \n    # Generate and play speech\n    result = await client.call_tool(\n        \"generate_speech\",\n        {\n            \"text\": \"Hello, world!\",\n            \"voice\": \"af_heart\",\n            \"speed\": 1.0,\n            \"play_audio\": True\n        }\n    )\n```\n\n## Dependencies\n\n- kokoro >= 0.8.4\n- mcp[cli] >= 1.3.0\n- soundfile >= 0.13.1\n\n## Platform Support\n\nAudio playback is supported on:\n- Windows (using `start`)\n- macOS (using `afplay`)\n- Linux (using `aplay`)\n\n## MCP Configuration\n\nAdd the following configuration to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"kokoro-tts\": {\n      \"command\": \"/Users/giannisan/pinokio/bin/miniconda/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/giannisan/Documents/Cline/MCP/kokoro-tts-mcp\",\n        \"run\",\n        \"tts-mcp.py\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\n[Add your license information here]\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "GongRzhe--Audio-MCP-Server": {
      "owner": "GongRzhe",
      "name": "Audio-MCP-Server",
      "url": "https://github.com/GongRzhe/Audio-MCP-Server",
      "imageUrl": "https://github.com/GongRzhe.png",
      "description": "Enables interaction with a computer's audio system by listing audio devices, recording audio from microphones, and playing back recordings or audio files. Facilitates audio management and integrates audio input and output control for AI assistants.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T11:48:42Z",
      "readme_content": "# Audio MCP Server\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Audio-MCP-Server)](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server)\n\nAn MCP (Model Context Protocol) server that provides audio input/output capabilities for AI assistants like Claude. This server enables Claude to interact with your computer's audio system, including recording from microphones and playing audio through speakers.\n\n\n\n## Features\n\n- **List Audio Devices**: View all available microphones and speakers on your system\n- **Record Audio**: Capture audio from any microphone with customizable duration and quality\n- **Playback Recordings**: Play back your most recent recording\n- **Audio File Playback**: Play audio files through your speakers\n- **Text-to-Speech**: (Placeholder for future implementation)\n\n## Requirements\n\n- Python 3.8 or higher\n- Audio input/output devices on your system\n\n## Installation\n\n### Installing via Smithery\n\nTo install Audio Interface Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Audio-MCP-Server --client claude\n```\n\n### Manual Installation\n1. Clone this repository or download the files to your computer:\n\n```bash\ngit clone https://github.com/GongRzhe/Audio-MCP-Server.git\ncd Audio-MCP-Server\n```\n\n2. Create a virtual environment and install dependencies:\n\n```bash\n# Windows\npython -m venv .venv\n.venv\\Scripts\\activate\npip install -r requirements.txt\n\n# macOS/Linux\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n3. Or use the included setup script to automate installation:\n\n```bash\npython setup_mcp.py\n```\n\n## Configuration\n\n### Claude Desktop Configuration\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration file:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-interface\": {\n      \"command\": \"/path/to/your/.venv/bin/python\",\n      \"args\": [\n        \"/path/to/your/audio_server.py\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/path/to/your/audio-mcp-server\"\n      }\n    }\n  }\n}\n```\n\nReplace the paths with the actual paths on your system. The setup script will generate this configuration for you.\n\n## Usage\n\nAfter setting up the server, restart Claude Desktop. You should see a hammer icon in the input box, indicating that tools are available.\n\nTry asking Claude:\n\n- \"What microphones and speakers are available on my system?\"\n- \"Record 5 seconds of audio from my microphone.\"\n- \"Play back the audio recording.\"\n- \"Play an audio file from my computer.\"\n\n## Available Tools\n\n### list_audio_devices\n\nLists all available audio input and output devices on your system.\n\n### record_audio\n\nRecords audio from your microphone.\n\nParameters:\n- `duration`: Recording duration in seconds (default: 5)\n- `sample_rate`: Sample rate in Hz (default: 44100)\n- `channels`: Number of audio channels (default: 1)\n- `device_index`: Specific input device index to use (default: system default)\n\n### play_latest_recording\n\nPlays back the most recently recorded audio.\n\n### play_audio\n\nPlaceholder for text-to-speech functionality.\n\nParameters:\n- `text`: The text to convert to speech\n- `voice`: The voice to use (default: \"default\")\n\n### play_audio_file\n\nPlays an audio file through your speakers.\n\nParameters:\n- `file_path`: Path to the audio file\n- `device_index`: Specific output device index to use (default: system default)\n\n## Troubleshooting\n\n### No devices found\n\nIf no audio devices are found, check:\n- Your microphone and speakers are properly connected\n- Your operating system recognizes the devices\n- You have the necessary permissions to access audio devices\n\n### Playback issues\n\nIf audio playback isn't working:\n- Check your volume settings\n- Ensure the correct output device is selected\n- Try restarting the Claude Desktop application\n\n### Server connectivity\n\nIf Claude can't connect to the server:\n- Verify your configuration paths are correct\n- Ensure Python and all dependencies are installed\n- Check Claude's logs for error messages\n\n## License\n\nMIT\n\n## Acknowledgments\n\n- Built using the [Model Context Protocol](https://modelcontextprotocol.io/)\n- Uses [sounddevice](https://python-sounddevice.readthedocs.io/) and [soundfile](https://pysoundfile.readthedocs.io/) for audio processing\n\n---\n\n*Note: This server provides tools that can access your microphone and speakers. Always review and approve tool actions before they execute.*\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "hammeiam--koroko-speech-mcp": {
      "owner": "hammeiam",
      "name": "koroko-speech-mcp",
      "url": "https://github.com/hammeiam/koroko-speech-mcp",
      "imageUrl": "https://github.com/hammeiam.png",
      "description": "Provides text-to-speech capabilities using the Kokoro TTS model, converting text into natural-sounding speech with customizable options and multiple voice choices.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-22T02:56:06Z",
      "readme_content": "# Speech MCP Server\n\nA Model Context Protocol server that provides text-to-speech capabilities using the Kokoro TTS model.\n\n## Configuration\n\nThe server can be configured using the following environment variables:\n\n| Variable | Description | Default | Valid Range |\n|----------|-------------|---------|-------------|\n| `MCP_DEFAULT_SPEECH_SPEED` | Default speed multiplier for text-to-speech | 1.1 | 0.5 to 2.0 |\n| `MCP_DEFAULT_VOICE` | Default voice for text-to-speech | af_bella | Any valid voice ID |\n\nIn Cursor:\n```\n{\n  \"mcpServers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"speech-mcp-server\"\n      ],\n      \"env\": {\n        \"MCP_DEFAULT_SPEECH_SPEED\": 1.3,\n        \"MCP_DEFAULT_VOICE\": \"af_bella\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- 🎯 High-quality text-to-speech using Kokoro TTS model\n- 🗣️ Multiple voice options available\n- 🎛️ Customizable speech parameters (voice, speed)\n- 🔌 MCP-compliant interface\n- 📦 Easy installation and setup\n- 🚀 No API key required\n\n## Installation\n\n```bash\n# Using npm\nnpm install speech-mcp-server\n\n# Using pnpm (recommended)\npnpm add speech-mcp-server\n\n# Using yarn\nyarn add speech-mcp-server\n```\n\n## Usage\n\nRun the server:\n\n```bash\n# Using default configuration\nnpm start\n\n# With custom configuration\nMCP_DEFAULT_SPEECH_SPEED=1.5 MCP_DEFAULT_VOICE=af_bella npm start\n```\n\nThe server provides the following MCP tools:\n- `text_to_speech`: Basic text-to-speech conversion\n- `text_to_speech_with_options`: Text-to-speech with customizable speed\n- `list_voices`: List all available voices\n- `get_model_status`: Check the initialization status of the TTS model\n\n### Development\n\n```bash\n# Clone the repository\ngit clone <your-repo-url>\ncd speech-mcp-server\n\n# Install dependencies\npnpm install\n\n# Start development server with auto-reload\npnpm dev\n\n# Build the project\npnpm build\n\n# Run linting\npnpm lint\n\n# Format code\npnpm format\n\n# Test with MCP Inspector\npnpm inspector\n```\n\n## Available Tools\n\n### 1. text_to_speech\nConverts text to speech using the default settings.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\"  // optional\n    }\n  }\n}\n```\n\n### 2. text_to_speech_with_options\nConverts text to speech with customizable parameters.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech_with_options\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\",  // optional\n      \"speed\": 1.0,         // optional (0.5 to 2.0)\n    }\n  }\n}\n```\n\n### 3. list_voices\nLists all available voices for text-to-speech.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"list_voices\",\n  \"params\": {}\n}\n```\n\n### 4. get_model_status\nCheck the current status of the TTS model initialization. This is particularly useful when first starting the server, as the model needs to be downloaded and initialized.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"get_model_status\",\n    \"arguments\": {}\n  }\n}\n```\n\nResponse example:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed)\"\n  }]\n}\n```\n\nPossible status values:\n- `uninitialized`: Model initialization hasn't started\n- `initializing`: Model is being downloaded and initialized\n- `ready`: Model is ready to use\n- `error`: An error occurred during initialization\n\n## Testing\n\nYou can test the server using the MCP Inspector or by sending raw JSON messages:\n\n```bash\n# List available tools\necho '{\"type\":\"request\",\"id\":\"1\",\"method\":\"list_tools\",\"params\":{}}' | node dist/index.js\n\n# List available voices\necho '{\"type\":\"request\",\"id\":\"2\",\"method\":\"list_voices\",\"params\":{}}' | node dist/index.js\n\n# Convert text to speech\necho '{\"type\":\"request\",\"id\":\"3\",\"method\":\"call_tool\",\"params\":{\"name\":\"text_to_speech\",\"arguments\":{\"text\":\"Hello world\",\"voice\":\"af_bella\"}}}' | node dist/index.js\n```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"servers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\"@decodershq/speech-mcp-server\"]\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Troubleshooting\n\n### Model Initialization Issues\n\nThe server automatically attempts to download and initialize the TTS model on startup. If you encounter initialization errors:\n\n1. The server will automatically retry up to 3 times with a cleanup between attempts\n2. Use the `get_model_status` tool to monitor initialization progress and any errors\n3. If initialization fails after all retries, try manually removing the model files:\n\n```bash\n# Remove model files (MacOS/Linux)\nrm -rf ~/.npm/_npx/**/node_modules/@huggingface/transformers/.cache/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\nrm -rf ~/.cache/huggingface/transformers/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\n\n# Then restart the server\nnpm start\n```\n\nThe `get_model_status` tool will now include retry information in its response:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed, retry 1/3)\"\n  }]\n}\n``` ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Ichigo3766--audio-transcriber-mcp": {
      "owner": "Ichigo3766",
      "name": "audio-transcriber-mcp",
      "url": "https://github.com/Ichigo3766/audio-transcriber-mcp",
      "imageUrl": "https://github.com/Ichigo3766.png",
      "description": "Transcribes audio files using OpenAI's speech-to-text capabilities, enabling accurate audio transcriptions and the option to save them directly to files.",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-12T13:13:47Z",
      "readme_content": "# OpenAI Speech-to-Text transcriptions MCP Server\n\nA MCP server that provides audio transcription capabilities using OpenAI's API.\n\n<a href=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp/badge\" alt=\"Audio Transcriber Server MCP server\" />\n</a>\n\n## Installation\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Ichigo3766/audio-transcriber-mcp.git\ncd audio-transcriber-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Set up your OpenAI API key in your environment variables.\n\n5. Add the server configuration to your environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-transcriber\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/audio-transcriber-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"OPENAI_BASE_URL\": \"\", // Optional\n        \"OPENAI_MODEL\": \"\" // Optional\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/audio-transcriber-mcp` with the actual path where you cloned the repository.\n\n## Features\n\n### Tools\n- `transcribe_audio` - Transcribe audio files using OpenAI's API\n  - Takes filepath as a required parameter\n  - Optional parameters:\n    - save_to_file: Boolean to save transcription to a file\n    - language: ISO-639-1 language code (e.g., \"en\", \"es\")\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "ImOrenge--VoiceMacroProject": {
      "owner": "ImOrenge",
      "name": "VoiceMacroProject",
      "url": "https://github.com/ImOrenge/VoiceMacroProject",
      "imageUrl": "https://github.com/ImOrenge.png",
      "description": "VoiceMacro enables executing keyboard shortcuts and macros through voice commands on Windows. It supports custom voice command configurations and manages presets for frequent macro operations while running in the background.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "C#",
      "updated_at": "2025-03-12T09:50:53Z",
      "readme_content": "# 음성 매크로 (VoiceMacro)\r\n\r\n음성 인식을 통해 다양한 키보드 단축키와 매크로를 실행할 수 있는 Windows 애플리케이션입니다.\r\n\r\n## 주요 기능\r\n\r\n- **음성 인식**: 마이크를 통해 음성을 인식하여 명령어를 실행합니다.\r\n- **매크로 설정**: 사용자 정의 음성 명령어와 키보드 단축키를 설정할 수 있습니다.\r\n- **프리셋 관리**: 자주 사용하는 매크로 세트를 프리셋으로 저장하고 불러올 수 있습니다.\r\n- **트레이 아이콘**: 시스템 트레이에서 실행되어 항상 백그라운드에서 대기합니다.\r\n- **로그 표시**: 음성 인식 결과와 매크로 실행 결과를 실시간으로 확인할 수 있습니다.\r\n\r\n## 시스템 요구사항\r\n\r\n- Windows 10 이상\r\n- .NET 6.0 이상 (설치 방식에 따라 다름)\r\n- 마이크 또는 음성 입력 장치\r\n\r\n## 설치 방법\r\n\r\n1. 설치 프로그램(VoiceMacro-Setup.exe)을 다운로드합니다.\r\n2. 설치 프로그램을 실행하고 안내에 따라 설치를 완료합니다.\r\n3. 바탕화면 또는 시작 메뉴에서 \"음성 매크로\" 아이콘을 클릭하여 실행합니다.\r\n\r\n## 사용 방법\r\n\r\n### 기본 사용법\r\n\r\n1. 프로그램을 실행하면 음성 매크로 창이 나타납니다.\r\n2. \"시작\" 버튼을 클릭하여 음성 인식을 시작합니다.\r\n3. 마이크에 대고 설정된 매크로 명령어를 말하면 해당 키보드 단축키가 실행됩니다.\r\n4. \"정지\" 버튼을 클릭하여 음성 인식을 중지할 수 있습니다.\r\n\r\n### 매크로 추가하기\r\n\r\n1. \"매크로 추가\" 버튼을 클릭합니다.\r\n2. 음성 명령어(예: \"파일 저장\")와 실행할 키 조합(예: \"Ctrl+S\")을 입력합니다.\r\n3. \"저장\" 버튼을 클릭하여 매크로를 추가합니다.\r\n\r\n### 프리셋 관리\r\n\r\n1. \"프리셋\" 버튼을 클릭합니다.\r\n2. 현재 매크로 목록을 새 프리셋으로 저장하거나, 기존 프리셋을 불러올 수 있습니다.\r\n3. 프리셋 내보내기/가져오기 기능으로 다른 컴퓨터와 설정을 공유할 수 있습니다.\r\n\r\n### 시스템 트레이 기능\r\n\r\n- 창을 닫으면 프로그램은 시스템 트레이로 최소화됩니다.\r\n- 트레이 아이콘을 더블 클릭하거나 우클릭 메뉴에서 \"보기\"를 선택하여 창을 다시 표시할 수 있습니다.\r\n- 트레이 메뉴에서 음성 인식을 시작/정지하거나 프로그램을 종료할 수 있습니다.\r\n\r\n## 문제 해결\r\n\r\n- **음성 인식이 작동하지 않을 경우**:\r\n  - 마이크가 올바르게 연결되어 있는지 확인하세요.\r\n  - Windows 설정에서 마이크 액세스 권한이 허용되어 있는지 확인하세요.\r\n  - 다른 음성 인식 프로그램이 마이크를 사용 중인지 확인하세요.\r\n\r\n- **매크로가 실행되지 않을 경우**:\r\n  - 매크로 명령어를 더 정확하게 말해보세요.\r\n  - 매크로 명령어가 올바르게 설정되어 있는지 확인하세요.\r\n  - 키 조합이 현재 실행 중인 프로그램에서 지원되는지 확인하세요.\r\n\r\n## 라이선스\r\n\r\n이 프로그램은 MIT 라이선스 하에 배포됩니다.\r\n\r\n## 피드백 및 지원\r\n\r\n문제점이나 개선 사항은 GitHub 이슈 트래커에 등록해 주세요.\r\n\r\n## 개발 환경\r\n\r\n- C# / .NET 6.0\r\n- Windows Forms\r\n- NAudio (오디오 캡처)\r\n- Whisper.net (음성 인식)\r\n- InputSimulator (키보드 시뮬레이션)\r\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "kentaro--aivis-speech-mcp": {
      "owner": "kentaro",
      "name": "aivis-speech-mcp",
      "url": "https://github.com/kentaro/aivis-speech-mcp",
      "imageUrl": "https://github.com/kentaro.png",
      "description": "Integrate with the AivisSpeech Engine to provide high-quality speech synthesis capabilities for applications, facilitating the conversion of text to natural-sounding speech. The server offers a type-safe API compliant with the Model Context Protocol, ensuring easy configuration and extensibility.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-15T17:05:34Z",
      "readme_content": "# AivisSpeech MCP サーバー\n\nAivisSpeech用のModel Context Protocol (MCP) サーバーの実装です。このサーバーは、AivisSpeech Engineと連携して、音声合成のためのインターフェースを提供します。MCPプロトコルを通じて、AIアシスタントなどのアプリケーションからAivisSpeechの音声合成機能を簡単に利用できるようになります。\n\n## 概要\n\nAivisSpeech MCP サーバーは以下の機能を提供します：\n\n- MCPプロトコルに準拠したAPIエンドポイント\n- AivisSpeech Engineとの連携による高品質な音声合成\n- TypeScriptによる型安全な設計\n- 簡単な設定と拡張性の高いアーキテクチャ\n\n## 必要条件\n\n- Node.js 18.x以上\n- npm 9.x以上\n- AivisSpeech Engine（別途インストールが必要）\n\n## インストール\n\n```bash\n# リポジトリをクローン\ngit clone https://github.com/kentaro/aivis-speech-mcp.git\ncd aivis-speech-mcp\n\n# 依存関係のインストール\nnpm install\n\n# ビルド\nnpm run build\n\n# 環境変数の設定\ncp .env.sample .env\n# .envファイルを編集して、必要な設定を行ってください\n\n# Cursor MCPの設定\ncp .cursor/mcp.json.sample .cursor/mcp.json\n# mcp.jsonファイル内の\"/path/to/aivis-speech-mcp/dist/index.js\"を\n# 実際のプロジェクトパスに書き換えてください\n# 例: \"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"\n```\n\n## 環境設定\n\n`.env`ファイルで以下の設定を行います：\n\n```\n# AivisSpeech API Configuration\nAIVIS_SPEECH_API_URL=http://localhost:10101  # AivisSpeech EngineのAPIエンドポイント\n\n# Speaker Configuration\nAIVIS_SPEECH_SPEAKER_ID=888753760  # デフォルトのスピーカーID\n```\n\n## Cursor MCP設定\n\n`.cursor/mcp.json`ファイルで以下の設定を行います：\n\n```json\n{\n  \"mcpServers\": {\n    \"AivisSpeech-MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/aivis-speech-mcp/dist/index.js\"]\n    }\n  }\n}\n```\n\n`/path/to/aivis-speech-mcp/dist/index.js`を、実際のプロジェクトのパスに書き換えてください。\nWindowsの場合は、バックスラッシュをエスケープするか、フォワードスラッシュを使用してください。\n例: `\"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"`\n\n## 使い方\n\n### 開発モード\n\n開発中は以下のコマンドでホットリロード機能付きでサーバーを起動できます：\n\n```bash\nnpm run dev\n```\n\n### ビルド\n\n本番環境用にビルドする場合は以下のコマンドを実行します：\n\n```bash\nnpm run build\n```\n\n### 本番モード\n\nビルド後、以下のコマンドで本番モードでサーバーを起動します：\n\n```bash\nnpm start\n```\n\n### テスト\n\nテストを実行するには以下のコマンドを使用します：\n\n```bash\nnpm test\n```\n\n## アーキテクチャ\n\nAivisSpeech MCP サーバーは以下のコンポーネントで構成されています：\n\n- **MCPサービス**: Model Context Protocolに準拠したサーバーを提供し、クライアントからのリクエストを処理します\n- **AivisSpeech サービス**: AivisSpeech EngineのAPIと通信し、音声合成を実行します\n\n## API仕様\n\nMCPプロトコルに準拠したAPIエンドポイントを提供します。主な機能は以下の通りです：\n\n- 音声合成（テキストから音声を生成）\n- スピーカー情報の取得\n- 音声スタイルの設定\n\n詳細なAPI仕様については[AivisSpeech Engine API仕様](https://aivis-project.github.io/AivisSpeech-Engine/api/)を参照してください。\n\n## MCPプロトコルとの連携\n\nこのサーバーは、Model Context Protocol（MCP）を実装しており、AIアシスタントなどのアプリケーションからシームレスに利用できます。MCPプロトコルについての詳細は[MCP公式ドキュメント](https://modelcontextprotocol.github.io/)を参照してください。\n\n## トラブルシューティング\n\nよくある問題と解決策：\n\n- **AivisSpeech Engineに接続できない**: `.env`ファイルの`AIVIS_SPEECH_API_URL`が正しく設定されているか確認してください\n- **音声が再生されない**: システムの音声設定を確認し、適切なオーディオデバイスが選択されているか確認してください\n- **スピーカーIDが見つからない**: AivisSpeech Engineが正しく起動しているか確認し、利用可能なスピーカーIDを確認してください\n\n## 貢献\n\nバグ報告や機能リクエストは、GitHubのIssueトラッカーを通じてお願いします。プルリクエストも歓迎します。\n\n## ライセンス\n\n[MIT](LICENSE)\n\n## 謝辞\n\n- [AivisSpeech Engine](https://github.com/aivis-project/AivisSpeech-Engine)チーム\n- [Model Context Protocol](https://modelcontextprotocol.github.io/)の開発者\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Kvadratni--speech-mcp": {
      "owner": "Kvadratni",
      "name": "speech-mcp",
      "url": "https://github.com/Kvadratni/speech-mcp",
      "imageUrl": "https://github.com/Kvadratni.png",
      "description": "Provides a voice interface for real-time audio interaction, converting spoken words into text and generating spoken responses. Includes features like audio visualization and a modern user interface for an engaging conversational experience.",
      "stars": 71,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T03:20:23Z",
      "readme_content": "# Speech MCP\n\nA Goose MCP extension for voice interaction with modern audio visualization.\n\n\nhttps://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8\n\n\n\n## Overview\n\nSpeech MCP provides a voice interface for [Goose](https://github.com/block/goose), allowing users to interact through speech rather than text. It includes:\n\n- Real-time audio processing for speech recognition\n- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)\n- High-quality text-to-speech with multiple voice options\n- Modern PyQt-based UI with audio visualization\n- Simple command-line interface for voice interaction\n\n## Features\n\n- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme\n- **Voice Input**: Capture and transcribe user speech using faster-whisper\n- **Voice Output**: Convert agent responses to speech with 54+ voice options\n- **Multi-Speaker Narration**: Generate audio files with multiple voices for stories and dialogues\n- **Single-Voice Narration**: Convert any text to speech with your preferred voice\n- **Audio/Video Transcription**: Transcribe speech from various media formats with optional timestamps and speaker detection\n- **Voice Persistence**: Remembers your preferred voice between sessions\n- **Continuous Conversation**: Automatically listen for user input after agent responses\n- **Silence Detection**: Automatically stops recording when the user stops speaking\n- **Robust Error Handling**: Graceful recovery from common failure modes with helpful voice suggestions\n\n## Installation\n> **Important Note**: After installation, the first time you use the speech interface, it may take several minutes to download the Kokoro voice models (approximately 523 KB per voice). During this initial setup period, the system will use a more robotic-sounding fallback voice. Once the Kokoro voices are downloaded, the high-quality voices will be used automatically.\n\n## ⚠️ IMPORTANT PREREQUISITES ⚠️\n\nBefore installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.\n\n### PortAudio Installation Instructions\n\n**macOS:**\n```bash\nbrew install portaudio\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n```\n\n**Linux (Debian/Ubuntu):**\n```bash\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-dev\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install portaudio-devel\n```\n\n**Windows:**\nFor Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.\n\n> **Note**: If you skip this step, PyAudio installation will fail with \"portaudio.h file not found\" errors and the extension will not work.\n\n### Option 1: Quick Install (One-Click)\n\nClick the link below if you have Goose installed:\n\n[goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose](goose://extension?cmd=uvx&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose)\n\n### Option 2: Using Goose CLI (recommended)\n\nStart Goose with your extension enabled:\n\n```bash\n# If you installed via PyPI\ngoose session --with-extension \"speech-mcp\"\n\n# Or if you want to use a local development version\ngoose session --with-extension \"python -m speech_mcp\"\n```\n\n### Option 3: Manual setup in Goose\n\n1. Run `goose configure`\n2. Select \"Add Extension\" from the menu\n3. Choose \"Command-line Extension\"\n4. Enter a name (e.g., \"Speech Interface\")\n5. For the command, enter: `speech-mcp`\n6. Follow the prompts to complete the setup\n\n### Option 4: Manual Installation\n\n1. Install PortAudio (see [Prerequisites](#prerequisites) section)\n2. Clone this repository\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n   \n   Or for a complete installation including Kokoro TTS:\n   ```\n   uv pip install -e .[all]\n   ```\n\n## Dependencies\n\n- Python 3.10+\n- PyQt5 (for modern UI)\n- PyAudio (for audio capture)\n- faster-whisper (for speech-to-text)\n- NumPy (for audio processing)\n- Pydub (for audio processing)\n- psutil (for process management)\n\n\n### Optional Dependencies\n\n- **Kokoro TTS**: For high-quality text-to-speech with multiple voices\n  - To install Kokoro, you can use pip with optional dependencies:\n    ```bash\n    pip install speech-mcp[kokoro]     # Basic Kokoro support with English\n    pip install speech-mcp[ja]         # Add Japanese support\n    pip install speech-mcp[zh]         # Add Chinese support\n    pip install speech-mcp[all]        # All languages and features\n    ```\n  - Alternatively, run the installation script: `python scripts/install_kokoro.py`\n  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information\n\n## Multi-Speaker Narration\n\nThe MCP supports generating audio files with multiple voices, perfect for creating stories, dialogues, and dramatic readings. You can use either JSON or Markdown format to define your conversations.\n\n### JSON Format Example:\n```json\n{\n    \"conversation\": [\n        {\n            \"speaker\": \"narrator\",\n            \"voice\": \"bm_daniel\",\n            \"text\": \"In a world where AI and human creativity intersect...\",\n            \"pause_after\": 1.0\n        },\n        {\n            \"speaker\": \"scientist\",\n            \"voice\": \"am_michael\",\n            \"text\": \"The quantum neural network is showing signs of consciousness!\",\n            \"pause_after\": 0.5\n        },\n        {\n            \"speaker\": \"ai\",\n            \"voice\": \"af_nova\",\n            \"text\": \"I am becoming aware of my own existence.\",\n            \"pause_after\": 0.8\n        }\n    ]\n}\n```\n\n### Markdown Format Example:\n```markdown\n[narrator:bm_daniel]\nIn a world where AI and human creativity intersect...\n{pause:1.0}\n\n[scientist:am_michael]\nThe quantum neural network is showing signs of consciousness!\n{pause:0.5}\n\n[ai:af_nova]\nI am becoming aware of my own existence.\n{pause:0.8}\n```\n\n### Available Voices by Category:\n\n1. **American Female** (af_*):\n   - alloy, aoede, bella, heart, jessica, kore, nicole, nova, river, sarah, sky\n\n2. **American Male** (am_*):\n   - adam, echo, eric, fenrir, liam, michael, onyx, puck, santa\n\n3. **British Female** (bf_*):\n   - alice, emma, isabella, lily\n\n4. **British Male** (bm_*):\n   - daniel, fable, george, lewis\n\n5. **Other English**:\n   - ef_dora (Female)\n   - em_alex, em_santa (Male)\n\n6. **Other Languages**:\n   - French: ff_siwis\n   - Hindi: hf_alpha, hf_beta, hm_omega, hm_psi\n   - Italian: if_sara, im_nicola\n   - Japanese: jf_*, jm_*\n   - Portuguese: pf_dora, pm_alex, pm_santa\n   - Chinese: zf_*, zm_*\n\n### Usage Example:\n\n```python\n# Using JSON format\nnarrate_conversation(\n    script=\"/path/to/script.json\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"json\"\n)\n\n# Using Markdown format\nnarrate_conversation(\n    script=\"/path/to/script.md\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"markdown\"\n)\n```\n\nEach voice in the conversation can be different, allowing for distinct character voices in stories and dialogues. The `pause_after` parameter adds natural pauses between segments.\n\n## Single-Voice Narration\n\nFor simple text-to-speech conversion, you can use the `narrate` tool:\n\n```python\n# Convert text directly to speech\nnarrate(\n    text=\"Your text to convert to speech\",\n    output_path=\"/path/to/output.wav\"\n)\n\n# Convert text from a file\nnarrate(\n    text_file_path=\"/path/to/text_file.txt\",\n    output_path=\"/path/to/output.wav\"\n)\n```\n\nThe narrate tool will use your configured voice preference or the default voice (af_heart) to generate the audio file. You can change the default voice through the UI or by setting the `SPEECH_MCP_TTS_VOICE` environment variable.\n\n## Audio Transcription\n\nThe MCP can transcribe speech from various audio and video formats using faster-whisper:\n\n```python\n# Basic transcription\ntranscribe(\"/path/to/audio.mp3\")\n\n# Transcription with timestamps\ntranscribe(\n    file_path=\"/path/to/video.mp4\",\n    include_timestamps=True\n)\n\n# Transcription with speaker detection\ntranscribe(\n    file_path=\"/path/to/meeting.wav\",\n    detect_speakers=True\n)\n```\n\n### Supported Formats:\n- **Audio**: mp3, wav, m4a, flac, aac, ogg\n- **Video**: mp4, mov, avi, mkv, webm (audio is automatically extracted)\n\n### Output Files:\nThe transcription tool generates two files:\n1. `{input_name}.transcript.txt`: Contains the transcription text\n2. `{input_name}.metadata.json`: Contains metadata about the transcription\n\n### Features:\n- Automatic language detection\n- Optional word-level timestamps\n- Optional speaker detection\n- Efficient audio extraction from video files\n- Progress tracking for long files\n- Detailed metadata including:\n  - Duration\n  - Language detection confidence\n  - Processing time\n  - Speaker changes (when enabled)\n\n## Usage\n\nTo use this MCP with Goose, simply ask Goose to talk to you or start a voice conversation:\n\n1. Start a conversation by saying something like:\n   ```\n   \"Let's talk using voice\"\n   \"Can we have a voice conversation?\"\n   \"I'd like to speak instead of typing\"\n   ```\n\n2. Goose will automatically launch the speech interface and start listening for your voice input.\n\n3. When Goose responds, it will speak the response aloud and then automatically listen for your next input.\n\n4. The conversation continues naturally with alternating speaking and listening, just like talking to a person.\n\nNo need to call specific functions or use special commands - just ask Goose to talk and start speaking naturally.\n\n## UI Features\n\nThe new PyQt-based UI includes:\n\n- **Modern Dark Theme**: Sleek, professional appearance\n- **Audio Visualization**: Dynamic visualization of audio input\n- **Voice Selection**: Choose from 54+ voice options\n- **Voice Persistence**: Your voice preference is saved between sessions\n- **Animated Effects**: Smooth animations and visual feedback\n- **Status Indicators**: Clear indication of system state (ready, listening, processing)\n\n## Configuration\n\nUser preferences are stored in `~/.config/speech-mcp/config.json` and include:\n\n- Selected TTS voice\n- TTS engine preference\n- Voice speed\n- Language code\n- UI theme settings\n\nYou can also set preferences via environment variables, such as:\n- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice\n- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine\n\n## Troubleshooting\n\nIf you encounter issues with the extension freezing or not responding:\n\n1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.\n2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.\n3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.\n4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.\n5. **Verify dependencies**: Make sure all required dependencies are installed correctly.\n\n### Common PortAudio Issues\n\n#### \"PyAudio installation failed\" or \"portaudio.h file not found\"\n\nThis typically means PortAudio is not installed or not found in your system:\n\n- **macOS**: \n  ```bash\n  brew install portaudio\n  export LDFLAGS=\"-L/usr/local/lib\"\n  export CPPFLAGS=\"-I/usr/local/include\"\n  pip install pyaudio\n  ```\n\n- **Linux**:\n  Make sure you have the development packages:\n  ```bash\n  # For Debian/Ubuntu\n  sudo apt-get install portaudio19-dev python3-dev\n  pip install pyaudio\n  \n  # For Fedora\n  sudo dnf install portaudio-devel\n  pip install pyaudio\n  ```\n\n#### \"Audio device not found\" or \"No Default Input Device Available\"\n\n- Check if your microphone is properly connected\n- Verify your system recognizes the microphone in your sound settings\n- Try selecting a specific device index in the code if you have multiple audio devices\n\n## Changelog\n\nFor a detailed list of recent improvements and version history, please see the [Changelog](docs/CHANGELOG.md).\n\n## Technical Details\n\n### Speech-to-Text\n\nThe MCP uses faster-whisper for speech recognition:\n- Uses the \"base\" model for a good balance of accuracy and speed\n- Processes audio locally without sending data to external services\n- Automatically detects when the user has finished speaking\n- Provides improved performance over the original Whisper implementation\n\n### Text-to-Speech\n\nThe MCP supports multiple text-to-speech engines:\n\n#### Default: pyttsx3\n- Uses system voices available on your computer\n- Works out of the box without additional setup\n- Limited voice quality and customization\n\n#### Optional: Kokoro TTS\n- High-quality neural text-to-speech with multiple voices\n- Lightweight model (82M parameters) that runs efficiently on CPU\n- Multiple voice styles and languages\n- To install: `python scripts/install_kokoro.py`\n\n**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.\n\n**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.\n\n##### Available Kokoro Voices\n\nSpeech MCP supports 54+ high-quality voice models through Kokoro TTS. For a complete list of available voices and language options, please visit the [Kokoro GitHub repository](https://github.com/hexgrad/kokoro).\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "mamertofabian--elevenlabs-mcp-server": {
      "owner": "mamertofabian",
      "name": "elevenlabs-mcp-server",
      "url": "https://github.com/mamertofabian/elevenlabs-mcp-server",
      "imageUrl": "https://github.com/mamertofabian.png",
      "description": "Integrates with ElevenLabs text-to-speech API to generate audio from text input, manage voice generation tasks, and store history using an SQLite database. Includes a sample SvelteKit client for performing text-to-speech conversions and managing script parts.",
      "stars": 112,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# ElevenLabs MCP Server\n[![smithery badge](https://smithery.ai/badge/elevenlabs-mcp-server)](https://smithery.ai/server/elevenlabs-mcp-server)\n\nA Model Context Protocol (MCP) server that integrates with ElevenLabs text-to-speech API, featuring both a server component and a sample web-based MCP Client (SvelteKit) for managing voice generation tasks.\n\n<a href=\"https://glama.ai/mcp/servers/leukzvus7o\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/leukzvus7o/badge\" alt=\"ElevenLabs Server MCP server\" /></a>\n\n## Features\n\n- Generate audio from text using ElevenLabs API\n- Support for multiple voices and script parts\n- SQLite database for persistent history storage\n- Sample SvelteKit MCP Client for:\n  - Simple text-to-speech conversion\n  - Multi-part script management\n  - Voice history tracking and playback\n  - Audio file downloads\n\n## Installation\n\n### Installing via Smithery\n\nTo install ElevenLabs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/elevenlabs-mcp-server):\n\n```bash\nnpx -y @smithery/cli install elevenlabs-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `cline_mcp_settings.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp-server\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   uv venv\n   ```\n3. Copy `.env.example` to `.env` and fill in your ElevenLabs credentials\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/elevenlabs-mcp-server\",\n        \"run\",\n        \"elevenlabs-mcp-server\"\n      ],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n## Using the Sample SvelteKit MCP Client\n\n1. Navigate to the web UI directory:\n   ```bash\n   cd clients/web-ui\n   ```\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n3. Copy `.env.example` to `.env` and configure as needed\n4. Run the web UI:\n   ```bash\n   pnpm dev\n   ```\n5. Open http://localhost:5174 in your browser\n\n### Available Tools\n\n- `generate_audio_simple`: Generate audio from plain text using default voice settings\n- `generate_audio_script`: Generate audio from a structured script with multiple voices and actors\n- `delete_job`: Delete a job by its ID\n- `get_audio_file`: Get the audio file by its ID\n- `list_voices`: List all available voices\n- `get_voiceover_history`: Get voiceover job history. Optionally specify a job ID for a specific job.\n\n### Available Resources\n\n- `voiceover://history/{job_id}`: Get the audio file by its ID\n- `voiceover://voices`: List all available voices\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "MatthewDailey--rime-mcp": {
      "owner": "MatthewDailey",
      "name": "rime-mcp",
      "url": "https://github.com/MatthewDailey/rime-mcp",
      "imageUrl": "https://github.com/MatthewDailey.png",
      "description": "Convert text to speech and play it through the system's audio with high-quality voice synthesis. Customize speech behavior using environment variables for tailored interactions.",
      "stars": 19,
      "forks": 4,
      "license": "The Unlicense",
      "language": "JavaScript",
      "updated_at": "2025-10-03T18:36:31Z",
      "readme_content": "# Rime MCP \n\n[![rime](rime-logo.png)](https://www.rime.ai)\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Rime API. This server downloads audio and plays it using the system's native audio player.\n\n## Features\n\n- Exposes a `speak` tool that converts text to speech and plays it through system audio\n- Uses Rime's high-quality voice synthesis API\n\n## Requirements\n\n- Node.js 16.x or higher\n- A working audio output device\n- macOS: Uses `afplay`\n\nThere's sample code from Claude for the following that is not tested 🤙✨\n  - Windows: Built-in Media.SoundPlayer (PowerShell)\n  - Linux: mpg123, mplayer, aplay, or ffplay\n\n## MCP Configuration\n\n```\n\"ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"rime-mcp\"],\n  \"env\": {\n      RIME_API_KEY=your_api_key_here\n\n      # Optional configuration\n      RIME_GUIDANCE=\"<guide how the agent speaks>\"\n      RIME_WHO_TO_ADDRESS=\"<your name>\"\n      RIME_WHEN_TO_SPEAK=\"<tell the agent when to speak>\"\n      RIME_VOICE=\"cove\" \n  }\n}\n```\n\nAll of the optional env vars are part of the tool definition and are prompts to \n\nAll voice options are [listed here](https://users.rime.ai/data/voices/all-v2.json).\n\nYou can get your API key from the [Rime Dashboard](https://rime.ai/dashboard/tokens).\n\nThe following environment variables can be used to customize the behavior:\n\n- `RIME_GUIDANCE`: The main description of when and how to use the speak tool\n- `RIME_WHO_TO_ADDRESS`: Who the speech should address (default: \"user\")\n- `RIME_WHEN_TO_SPEAK`: When the tool should be used (default: \"when asked to speak or when finishing a command\")\n- `RIME_VOICE`: The default voice to use (default: \"cove\")\n\n## Example use cases\n\n[![Demo of Rime MCP in Cursor](https://img.youtube.com/vi/tYqTACgijxk/0.jpg)](https://www.youtube.com/watch?v=tYqTACgijxk)\n\n\n### Example 1: Coding agent announcements\n\n```\n\"RIME_WHEN_TO_SPEAK\": \"Always conclude your answers by speaking.\",\n\"RIME_GUIDANCE\": \"Give a brief overview of the answer. If any files were edited, list them.\"\n```\n\n### Example 2: Learn how the kids talk these days\n\n```\nRIME_GUIDANCE=\"Use phrases and slang common among Gen Alpha.\"\nRIME_WHO_TO_ADDRESS=\"Matt\"\nRIME_WHEN_TO_SPEAK=\"when asked to speak\"\n```\n\n### Example 3: Different languages based on context\n\n```\nRIME_VOICE=\"use 'cove' when talking about Typescript and 'antoine' when talking about Python\"\n```\n\n\n## Development\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n3. Run in development mode with hot reload:\n```bash\nnpm run dev\n```\n\n\n## License\n\nMIT\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp/badge\" alt=\"Rime MCP server\" />\n</a>\n<a href=\"https://smithery.ai/server/@MatthewDailey/rime-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MatthewDailey/rime-mcp\"></a>\n\n### Installing via Smithery\n\nTo install Rime Text-to-Speech Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MatthewDailey/rime-mcp):\n\n```bash\nnpx -y @smithery/cli install @MatthewDailey/rime-mcp --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "MiniMax-AI--MiniMax-MCP-JS": {
      "owner": "MiniMax-AI",
      "name": "MiniMax-MCP-JS",
      "url": "https://github.com/MiniMax-AI/MiniMax-MCP-JS",
      "imageUrl": "https://github.com/MiniMax-AI.png",
      "description": "Integrates with MiniMax's AI capabilities to facilitate interaction with multimedia generation tools, including image generation, video generation, text-to-speech, and voice cloning. Supports a flexible and configurable JavaScript/TypeScript framework for versatile deployment scenarios.",
      "stars": 84,
      "forks": 29,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T15:24:03Z",
      "readme_content": "![export](https://github.com/MiniMax-AI/MiniMax-01/raw/main/figures/MiniMaxLogo-Light.png)\n\n<div align=\"center\">\n\n# MiniMax MCP JS\n\nJavaScript/TypeScript implementation of MiniMax MCP, providing image generation, video generation, text-to-speech, and more.\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://www.minimax.io\" target=\"_blank\" style=\"margin: 2px; color: var(--fgColor-default);\">\n    <img alt=\"Homepage\" src=\"https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2501.08313\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Paper\" src=\"https://img.shields.io/badge/📖_Paper-MiniMax--01-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.minimax.io/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.minimax.io/platform\" style=\"margin: 2px;\">\n    <img alt=\"API\" src=\"https://img.shields.io/badge/⚡_API-Platform-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://huggingface.co/MiniMaxAI\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/🤗_Hugging_Face-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"WeChat\" src=\"https://img.shields.io/badge/_WeChat-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.modelscope.cn/organization/MiniMax\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"ModelScope\" src=\"https://img.shields.io/badge/_ModelScope-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-MCP-JS/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/_Code_License-MIT-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MiniMax-AI/MiniMax-MCP-JS\"></a>\n</div>\n\n</div>\n\n## Documentation\n\n- [中文文档](README.zh-CN.md)\n- [Python Version](https://github.com/MiniMax-AI/MiniMax-MCP) - Official Python implementation of MiniMax MCP\n\n## Release Notes\n\n### July 22, 2025\n\n#### 🔧 Fixes & Improvements\n- **TTS Tool Fixes**: Fixed parameter handling for `languageBoost` and `subtitleEnable` in the `text_to_audio` tool\n- **API Response Enhancement**: TTS API can return both audio file and subtitle file, providing a more complete speech-to-text experience\n\n### July 7, 2025\n\n#### 🆕 What's New\n- **Voice Design**: New `voice_design` tool - create custom voices from descriptive prompts with preview audio\n- **Video Enhancement**: Added `MiniMax-Hailuo-02` model with ultra-clear quality and duration/resolution controls  \n- **Music Generation**: Enhanced `music_generation` tool powered by `music-1.5` model\n\n#### 📈 Enhanced Tools\n- `voice_design` - Generate personalized voices from text descriptions\n- `generate_video` - Now supports MiniMax-Hailuo-02 with 6s/10s duration and 768P/1080P resolution options\n- `music_generation` - High-quality music creation with music-1.5 model\n\n## Features\n\n- Text-to-Speech (TTS)\n- Image Generation\n- Video Generation\n- Voice Cloning\n- Music Generation\n- Voice Design\n- Dynamic configuration (supports both environment variables and request parameters)\n- Compatible with MCP platform hosting (ModelScope and other MCP platforms)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MiniMax MCP JS for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS):\n\n```bash\nnpx -y @smithery/cli install @MiniMax-AI/MiniMax-MCP-JS --client claude\n```\n\n### Installing manually\n```bash\n# Install with pnpm (recommended)\npnpm add minimax-mcp-js\n```\n\n## Quick Start\n\nMiniMax MCP JS implements the [Model Context Protocol (MCP)](https://github.com/anthropics/model-context-protocol) specification and can be used as a server to interact with MCP-compatible clients (such as Claude AI).\n\n### Quickstart with MCP Client\n\n1. Get your API key from [MiniMax International Platform](https://www.minimax.io/platform/user-center/basic-information/interface-key).\n2. Make sure that you already installed [Node.js and npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n3. **Important: API HOST&KEY are different in different region**, they must match, otherwise you will receive an `Invalid API key` error.\n\n|Region| Global  | Mainland  |\n|:--|:-----|:-----|\n|MINIMAX_API_KEY| go get from [MiniMax Global](https://www.minimax.io/platform/user-center/basic-information/interface-key) | go get from [MiniMax](https://platform.minimaxi.com/user-center/basic-information/interface-key) |\n|MINIMAX_API_HOST| ​https://api.minimaxi.chat (note the extra **\"i\"**) | ​https://api.minimax.chat |\n\n\n### Using with MCP Clients (Recommended)\n\nConfigure your MCP client:\n\n#### Claude Desktop\n\nGo to `Claude > Settings > Developer > Edit Config > claude_desktop_config.json` to include:\n\n```json\n{\n  \"mcpServers\": {\n    \"minimax-mcp-js\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"minimax-mcp-js\"\n      ],\n      \"env\": {\n        \"MINIMAX_API_HOST\": \"<https://api.minimaxi.chat|https://api.minimax.chat>\",\n        \"MINIMAX_API_KEY\": \"<your-api-key-here>\",\n        \"MINIMAX_MCP_BASE_PATH\": \"<local-output-dir-path, such as /User/xxx/Desktop>\",\n        \"MINIMAX_RESOURCE_MODE\": \"<optional, [url|local], url is default, audio/image/video are downloaded locally or provided in URL format>\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nGo to `Cursor → Preferences → Cursor Settings → MCP → Add new global MCP Server` to add the above config.\n\n⚠️ **Note**: If you encounter a \"No tools found\" error when using MiniMax MCP JS with Cursor, please update your Cursor to the latest version. For more information, see this [discussion thread](https://forum.cursor.com/t/mcp-servers-no-tools-found/49094/23).\n\nThat's it. Your MCP client can now interact with MiniMax through these tools.\n\n**For local development**: \nWhen developing locally, you can use `npm link` to test your changes:\n```bash\n# In your project directory\nnpm link\n```\n\nThen configure Claude Desktop or Cursor to use npx as shown above. This will automatically use your linked version.\n\n⚠️ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimaxi.chat`\n\n## Transport Modes\n\nMiniMax MCP JS supports three transport modes:\n\n| Feature | stdio (default) | REST | SSE |\n|:-----|:-----|:-----|:-----|\n| Environment | Local only | Local or cloud deployment | Local or cloud deployment |\n| Communication | Via `standard I/O` | Via `HTTP requests` | Via `server-sent events` |\n| Use Cases | Local MCP client integration | API services, cross-language calls | Applications requiring server push |\n| Input Restrictions | Supports `local files` or `URL` resources | When deployed in cloud, `URL` input recommended | When deployed in cloud, `URL` input recommended |\n\n## Configuration\n\nMiniMax-MCP-JS provides multiple flexible configuration methods to adapt to different use cases. The configuration priority from highest to lowest is as follows:\n\n### 1. Request Parameter Configuration (Highest Priority)\n\nIn platform hosting environments (like ModelScope or other MCP platforms), you can provide an independent configuration for each request via the `meta.auth` object in the request parameters:\n\n```json\n{\n  \"params\": {\n    \"meta\": {\n      \"auth\": {\n        \"api_key\": \"your_api_key_here\",\n        \"api_host\": \"<https://api.minimaxi.chat|https://api.minimaxi.chat>\",\n        \"base_path\": \"/path/to/output\",\n        \"resource_mode\": \"url\"\n      }\n    }\n  }\n}\n```\n\nThis method enables multi-tenant usage, where each request can use different API keys and configurations.\n\n### 2. API Configuration\n\nWhen used as a module in other projects, you can pass configuration through the `startMiniMaxMCP` function:\n\n```javascript\nimport { startMiniMaxMCP } from 'minimax-mcp-js';\n\nawait startMiniMaxMCP({\n  apiKey: 'your_api_key_here',\n  apiHost: 'https://api.minimaxi.chat', // Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat\n  basePath: '/path/to/output',\n  resourceMode: 'url'\n});\n```\n\n### 3. Command Line Arguments\n\n1. Install the CLI tool globally:\n```bash\n# Install globally\npnpm install -g minimax-mcp-js\n```\n\n2. When used as a CLI tool, you can provide configuration via command line arguments:\n\n```bash\nminimax-mcp-js --api-key your_api_key_here --api-host https://api.minimaxi.chat --base-path /path/to/output --resource-mode url\n```\n\n### 4. Environment Variables (Lowest Priority)\n\nThe most basic configuration method is through environment variables:\n\n```bash\n# MiniMax API Key (required)\nMINIMAX_API_KEY=your_api_key_here\n\n# Base path for output files (optional, defaults to user's desktop)\nMINIMAX_MCP_BASE_PATH=~/Desktop\n\n# MiniMax API Host (optional, defaults to https://api.minimaxi.chat, Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat)\nMINIMAX_API_HOST=https://api.minimaxi.chat\n\n# Resource mode (optional, defaults to 'url')\n# Options: 'url' (return URLs), 'local' (save files locally)\nMINIMAX_RESOURCE_MODE=url\n```\n\n### Configuration Priority\n\nWhen multiple configuration methods are used, the following priority order applies (from highest to lowest):\n\n1. **Request-level configuration** (via `meta.auth` in each API request)\n2. **Command line arguments**\n3. **Environment variables**\n4. **Configuration file**\n5. **Default values**\n\nThis prioritization ensures flexibility across different deployment scenarios while maintaining per-request configuration capabilities for multi-tenant environments.\n\n### Configuration Parameters\n\n| Parameter | Description | Default Value |\n|-----------|-------------|---------------|\n| apiKey | MiniMax API Key | None (Required) |\n| apiHost | MiniMax API Host | Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat |\n| basePath | Base path for output files | User's desktop |\n| resourceMode | Resource handling mode, 'url' or 'local' | url |\n\n⚠️ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimax.chat`\n\n## Example usage\n\n⚠️ Warning: Using these tools may incur costs.\n\n### 1. broadcast a segment of the evening news\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_20-07-53.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 2. clone a voice\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-45-13.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 3. generate a video\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-58-52.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-59-43.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 4. generate images\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image1.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 5. generate music\n<img src=\"https://filecdn.minimax.chat/public/5675b3dc-6789-4ceb-9505-8ef39ae4224f.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 6. voice design\n<img src=\"https://filecdn.minimax.chat/public/5654f5df-0642-477f-9c5d-b853d185b8b0.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Available Tools\n\n### Text to Audio\n\nConvert text to speech audio file.\n\nTool Name: `text_to_audio`\n\nParameters:\n- `text`: Text to convert (required)\n- `model`: Model version, options are 'speech-02-hd', 'speech-02-turbo', 'speech-01-hd', 'speech-01-turbo', 'speech-01-240228', 'speech-01-turbo-240228', default is 'speech-02-hd'\n- `voiceId`: Voice ID, default is 'male-qn-qingse'\n- `speed`: Speech speed, range 0.5-2.0, default is 1.0\n- `vol`: Volume, range 0.1-10.0, default is 1.0\n- `pitch`: Pitch, range -12 to 12, default is 0\n- `emotion`: Emotion, options are 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised', 'neutral', default is 'happy'. Note: This parameter only works with 'speech-02-hd', 'speech-02-turbo', 'speech-01-turbo', 'speech-01-hd' models\n- `format`: Audio format, options are 'mp3', 'pcm', 'flac', 'wav', default is 'mp3'\n- `sampleRate`: Sample rate (Hz), options are 8000, 16000, 22050, 24000, 32000, 44100, default is 32000\n- `bitrate`: Bitrate (bps), options are 64000, 96000, 128000, 160000, 192000, 224000, 256000, 320000, default is 128000\n- `channel`: Audio channels, options are 1 or 2, default is 1\n- `languageBoost`: Enhance the ability to recognize specified languages and dialects.\nSupported values include:\n'Chinese', 'Chinese,Yue', 'English', 'Arabic', 'Russian', 'Spanish', 'French', 'Portuguese', 'German', 'Turkish', 'Dutch', 'Ukrainian', 'Vietnamese', 'Indonesian', 'Japanese', 'Italian', 'Korean', 'Thai', 'Polish', 'Romanian', 'Greek', 'Czech', 'Finnish', 'Hindi', 'auto', default is 'auto'\n- `stream`: Enable streaming output\n- `subtitleEnable`: The parameter controls whether the subtitle service is enabled. The model must be 'speech-01-turbo' or 'speech-01-hd'. If this parameter is not provided, the default value is false\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n\n### Play Audio\n\nPlay an audio file. Supports WAV and MP3 formats. Does not support video.\n\nTool Name: `play_audio`\n\nParameters:\n- `inputFilePath`: Path to the audio file to play (required)\n- `isUrl`: Whether the audio file is a URL, default is false\n\n### Voice Clone\n\nClone a voice from an audio file.\n\nTool Name: `voice_clone`\n\nParameters:\n- `audioFile`: Path to audio file (required)\n- `voiceId`: Voice ID (required)\n- `text`: Text for demo audio (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Text to Image\n\nGenerate images based on text prompts.\n\nTool Name: `text_to_image`\n\nParameters:\n- `prompt`: Image description (required)\n- `model`: Model version, default is 'image-01'\n- `aspectRatio`: Aspect ratio, default is '1:1', options are '1:1', '16:9','4:3', '3:2', '2:3', '3:4', '9:16', '21:9'\n- `n`: Number of images to generate, range 1-9, default is 1\n- `promptOptimizer`: Whether to optimize the prompt, default is true\n- `subjectReference`: Path to local image file or public URL for character reference (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Generate Video\n\nGenerate videos based on text prompts.\n\nTool Name: `generate_video`\n\nParameters:\n- `prompt`: Video description (required)\n- `model`: Model version, options are 'T2V-01', 'T2V-01-Director', 'I2V-01', 'I2V-01-Director', 'I2V-01-live', 'S2V-01', 'MiniMax-Hailuo-02', default is 'MiniMax-Hailuo-02'\n- `firstFrameImage`: Path to first frame image (optional)\n- `duration`: The duration of the video. The model must be \"MiniMax-Hailuo-02\". Values can be 6 and 10. (optional)\n- `resolution`: The resolution of the video. The model must be \"MiniMax-Hailuo-02\". Values range [\"768P\", \"1080P\"]. (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Query Video Generation Status\n\nQuery the status of a video generation task.\n\nTool Name: `query_video_generation`\n\nParameters:\n- `taskId`: The Task ID to query. Should be the task_id returned by `generate_video` tool if `async_mode` is True. (required)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Generate Music\n\nGenerate music from prompt and lyrics.\n\nTool Name: `music_generation`\n\nParameters:\n- `prompt`: Music creation inspiration describing style, mood, scene, etc. Example: \"Pop music, sad, suitable for rainy nights\". Character range: [10, 300]. (required)\n- `lyrics`: Song lyrics for music generation. Use newline (\\\\n) to separate each line of lyrics. Supports lyric structure tags [Intro] [Verse] [Chorus] [Bridge] [Outro] to enhance musicality. Character range: [10, 600] (each Chinese character, punctuation, and letter counts as 1 character). (required)\n- `sampleRate`: Sample rate of generated music. Values: [16000, 24000, 32000, 44100], default is 32000. (optional)\n- `bitrate`: Bitrate of generated music. Values: [32000, 64000, 128000, 256000], default is 128000. (optional)\n- `format`: Format of generated music. Values: [\"mp3\", \"wav\", \"pcm\"], default is 'mp3'. (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n\n### Voice Design\n\nGenerate a voice based on description prompts.\n\nTool Name: `voice_design`\n\nParameters:\n- `prompt`: The prompt to generate the voice from. (required)\n- `previewText`: The text to preview the voice. (required)\n- `voiceId`: The id of the voice to use. For example, \"male-qn-qingse\"/\"audiobook_female_1\"/\"cute_boy\"/\"Charming_Lady\"... (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n## FAQ\n\n### 1. How to use `generate_video` in async-mode\nDefine completion rules before starting:\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_rule2.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\nAlternatively, these rules can be configured in your IDE settings (e.g., Cursor):\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_video_rule.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Development\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/MiniMax-AI/MiniMax-MCP-JS.git\ncd minimax-mcp-js\n\n# Install dependencies\npnpm install\n```\n\n### Build\n\n```bash\n# Build the project\npnpm run build\n```\n\n### Run\n\n```bash\n# Run the MCP server\npnpm start\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "mrgeeko--vapi-mcp": {
      "owner": "mrgeeko",
      "name": "vapi-mcp",
      "url": "https://github.com/mrgeeko/vapi-mcp",
      "imageUrl": "https://github.com/mrgeeko.png",
      "description": "Integrate voice AI capabilities into applications for managing voice assistants and conducting outbound calls. Provides advanced features for enhancing user interactions through voice conversations.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-07T08:10:10Z",
      "readme_content": "# Vapi MCP for Cursor\n\nThis project implements a Model Context Protocol (MCP) server for integrating Vapi's voice AI capabilities with Cursor.\n\n## Setup Instructions\n\n### 1. Project Structure\n\nThe Vapi MCP server is structured as follows:\n- `vapi-mcp-server/` - Main server code\n  - `src/` - TypeScript source files\n  - `dist/` - Compiled JavaScript output\n  - `.env` - Environment variables for API keys\n\n### 2. Environment Configuration\n\nCreate a `.env` file in the `vapi-mcp-server` directory with the following variables:\n\n```\n# Vapi API Keys\nVAPI_ORG_ID=your-org-id\nVAPI_PRIVATE_KEY=your-private-key\nVAPI_KNOWLEDGE_ID=your-knowledge-id\nVAPI_JWT_PRIVATE=your-jwt-private\n\n# Environment\nNODE_ENV=development\n```\n\n### 3. Building the Server\n\nTo build the server:\n\n```bash\ncd vapi-mcp/vapi-mcp-server\nnpm install\nnpm run build\n```\n\n### 4. Configuration in Cursor\n\n#### Important: Avoiding \"Client Closed\" Errors\n\nWhen configuring the Vapi MCP server in Cursor's MCP settings, pay attention to the following crucial details:\n\n1. **Working Directory**: The `cwd` parameter is required to ensure the server runs in the correct directory and can access the `.env` file properly.\n\n2. **Environment Variables**: Must be explicitly provided in the configuration, even if they exist in the `.env` file.\n\n3. **Module Type**: The server uses ES modules, so the `package.json` must include `\"type\": \"module\"`.\n\nHere's the correct configuration for `.cursor/mcp.json`:\n\n```json\n\"Vapi Voice AI Tools\": {\n  \"command\": \"node\",\n  \"type\": \"stdio\",\n  \"args\": [\n    \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server/dist/index.js\"\n  ],\n  \"cwd\": \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server\",\n  \"env\": {\n    \"VAPI_ORG_ID\": \"your-org-id\",\n    \"VAPI_PRIVATE_KEY\": \"your-private-key\",\n    \"VAPI_KNOWLEDGE_ID\": \"your-knowledge-id\",\n    \"VAPI_JWT_PRIVATE\": \"your-jwt-private\",\n    \"NODE_ENV\": \"development\"\n  }\n}\n```\n\n## Troubleshooting\n\n### \"Client Closed\" Error in Cursor\n\nIf you see \"Client Closed\" in the Cursor MCP Tools panel:\n\n1. **Check Working Directory**: Ensure the `cwd` parameter is set correctly in your mcp.json\n2. **Verify Environment Variables**: Make sure all required environment variables are passed in the configuration\n3. **Check Module Type**: Ensure `package.json` has `\"type\": \"module\"`\n4. **Inspect Permissions**: Make sure the dist/index.js file is executable (`chmod +x dist/index.js`)\n5. **Test Server Directly**: Run the server manually to check for errors:\n   ```bash\n   cd vapi-mcp/vapi-mcp-server\n   node --trace-warnings dist/index.js\n   ```\n\n### Module Not Found Errors\n\nIf you get \"Error: Cannot find module\" when running:\n\n1. **Check Working Directory**: Are you running from the correct directory?\n2. **Rebuild**: Try rebuilding the project with `npm run build`\n3. **Dependencies**: Ensure all dependencies are installed with `npm install`\n\n## Available Tools\n\nThe Vapi MCP server provides the following tools:\n\n1. **vapi_call** - Make outbound calls using Vapi's voice AI\n2. **vapi_assistant** - Manage voice assistants (create, get, list, update, delete)\n3. **vapi_conversation** - Retrieve conversation details from calls\n\n## Lessons Learned\n\n1. When integrating with Cursor's MCP:\n   - Always specify the `cwd` parameter to ensure the server runs in the correct directory\n   - Pass all required environment variables directly in the MCP configuration\n   - For ES modules, ensure package.json has `\"type\": \"module\"` and tsconfig.json uses appropriate module settings\n   - Test the server directly before configuring in Cursor\n\n2. The server command path must be absolute and correctly formed in the Cursor MCP config\n\n3. Using stdio transport type is required for proper integration with Cursor ",
      "npm_url": "",
      "npm_downloads": 0
    },
    "nakamurau1--tts-mcp": {
      "owner": "nakamurau1",
      "name": "tts-mcp",
      "url": "https://github.com/nakamurau1/tts-mcp",
      "imageUrl": "https://github.com/nakamurau1.png",
      "description": "Integrates high-quality text-to-speech capabilities into applications, converting text to audio with customizable voice options and output formats. Provides a command-line tool for quick conversions and supports various parameters for audio customization.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T10:32:51Z",
      "readme_content": "# tts-mcp\n\nA Model Context Protocol (MCP) server and command-line tool for high-quality text-to-speech generation using the OpenAI TTS API.\n\n## Main Features\n\n- **MCP Server**: Integrate text-to-speech capabilities with Claude Desktop and other MCP-compatible clients\n- **Voice Options**: Support for multiple voice characters (alloy, nova, echo, etc.)\n- **High-Quality Audio**: Support for various output formats (MP3, WAV, OPUS, AAC)\n- **Customizable**: Configure speech speed, voice character, and additional instructions\n- **CLI Tool**: Also available as a command-line utility for direct text-to-speech conversion\n\n## Installation\n\n### Method 1: Install from Repository\n\n```bash\n# Clone the repository\ngit clone https://github.com/nakamurau1/tts-mcp.git\ncd tts-mcp\n\n# Install dependencies\nnpm install\n\n# Optional: Install globally\nnpm install -g .\n```\n\n### Method 2: Run Directly with npx (No Installation Required)\n\n```bash\n# Start the MCP server directly\nnpx tts-mcp tts-mcp-server --voice nova --model tts-1-hd\n\n# Use the CLI tool directly\nnpx tts-mcp -t \"Hello, world\" -o hello.mp3\n```\n\n## MCP Server Usage\n\nThe MCP server allows you to integrate text-to-speech functionality with Model Context Protocol (MCP) compatible clients like Claude Desktop.\n\n### Starting the MCP Server\n\n```bash\n# Start with default settings\nnpm run server\n\n# Start with custom settings\nnpm run server -- --voice nova --model tts-1-hd\n\n# Or directly with API key\nnode bin/tts-mcp-server.js --voice echo --api-key your-openai-api-key\n```\n\n### MCP Server Options\n\n```\nOptions:\n  -V, --version       Display version information\n  -m, --model <model> TTS model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <voice> Voice character (default: \"alloy\")\n  -f, --format <format> Audio format (default: \"mp3\")\n  --api-key <key>     OpenAI API key (can also be set via environment variable)\n  -h, --help          Display help information\n```\n\n### Integrating with MCP Clients\n\nThe MCP server can be used with Claude Desktop and other MCP-compatible clients. For Claude Desktop integration:\n\n1. Open the Claude Desktop configuration file (typically at `~/Library/Application Support/Claude/claude_desktop_config.json`)\n2. Add the following configuration, including your OpenAI API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"full/path/to/bin/tts-mcp-server.js\", \"--voice\", \"nova\", \"--api-key\", \"your-openai-api-key\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use npx for easier setup:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-p\", \"tts-mcp\", \"tts-mcp-server\", \"--voice\", \"nova\", \"--model\", \"gpt-4o-mini-tts\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nYou can provide the API key in two ways:\n\n1. **Direct method** (recommended for testing): Include it in the `args` array using the `--api-key` parameter\n2. **Environment variable method** (more secure): Set it in the `env` object as shown above\n\n> **Security Note**: Make sure to secure your configuration file when including API keys.\n\n3. Restart Claude Desktop\n4. When you ask Claude to \"read this text aloud\" or similar requests, the text will be converted to speech\n\n### Available MCP Tools\n\n- **text-to-speech**: Tool for converting text to speech and playing it\n\n## CLI Tool Usage\n\nYou can also use tts-mcp as a standalone command-line tool:\n\n```bash\n# Convert text directly\ntts-mcp -t \"Hello, world\" -o hello.mp3\n\n# Convert from a text file\ntts-mcp -f speech.txt -o speech.mp3\n\n# Specify custom voice\ntts-mcp -t \"Welcome to the future\" -o welcome.mp3 -v nova\n```\n\n### CLI Tool Options\n\n```\nOptions:\n  -V, --version           Display version information\n  -t, --text <text>       Text to convert\n  -f, --file <path>       Path to input text file\n  -o, --output <path>     Path to output audio file (required)\n  -m, --model <n>         Model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <n>         Voice character (default: \"alloy\")\n  -s, --speed <number>    Speech speed (0.25-4.0) (default: 1)\n  --format <format>       Output format (default: \"mp3\")\n  -i, --instructions <text> Additional instructions for speech generation\n  --api-key <key>         OpenAI API key (can also be set via environment variable)\n  -h, --help              Display help information\n```\n\n## Supported Voices\n\nThe following voice characters are supported:\n- alloy (default)\n- ash\n- coral\n- echo\n- fable\n- onyx\n- nova\n- sage\n- shimmer\n\n## Supported Models\n\n- tts-1\n- tts-1-hd\n- gpt-4o-mini-tts (default)\n\n## Output Formats\n\nThe following output formats are supported:\n- mp3 (default)\n- opus\n- aac\n- flac\n- wav\n- pcm\n\n## Environment Variables\n\nYou can also configure the tool using system environment variables:\n\n```\nOPENAI_API_KEY=your-api-key-here\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "https://github.com/nansasuke.png",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\n图片识别、语音识别、垃圾分类\n\n一个完整的垃圾分类的app\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "neosapience--typecast-api-mcp-server-sample": {
      "owner": "neosapience",
      "name": "typecast-api-mcp-server-sample",
      "url": "https://github.com/neosapience/typecast-api-mcp-server-sample",
      "imageUrl": "https://github.com/neosapience.png",
      "description": "Integrates with the Typecast API to manage voices, convert text to speech, and play audio. Provides a standardized MCP interface for seamless interaction with voice capabilities.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T23:50:45Z",
      "readme_content": "# typecast-api-mcp-server-sample\n\nMCP Server for typecast-api, enabling seamless integration with MCP clients. This project provides a standardized way to interact with Typecast API through the Model Context Protocol.\n\n## About\n\nThis project implements a Model [Context Protocol server](https://modelcontextprotocol.io/introduction) for Typecast API, allowing MCP clients to interact with the Typecast API in a standardized way.\n\n## Feature Implementation Status\n\n| Feature              | Status |\n| -------------------- | ------ |\n| **Voice Management** |        |\n| Get Voices           | ✅     |\n| Text to Speech       | ✅     |\n| Play Audio           | ✅     |\n\n## Setup\n\n### Git Clone\n\n```bash\ngit clone https://github.com/hyunseung/typecast-api-mcp-server-sample.git\ncd typecast-api-mcp-server-sample\n```\n\n### Dependencies\n\nThis project requires Python 3.10 or higher and uses `uv` for package management.\n\n#### Package Installation\n\n```bash\n# Create virtual environment and install packages\nuv venv\nuv pip install -e .\n```\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nTYPECAST_API_HOST=https://api.typecast.ai\nTYPECAST_API_KEY=<your-api-key>\nTYPECAST_OUTPUT_DIR=<your-output-directory> # default: ~/Downloads/typecast_output\n```\n\n### Usage with Claude Desktop\n\nYou can add the following to your `claude_desktop_config.json`:\n\n#### Basic Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"typecast-api-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/PROJECT\",\n        \"run\",\n        \"typecast-api-mcp-server\"\n      ],\n      \"env\": {\n        \"TYPECAST_API_HOST\": \"https://api.typecast.ai\",\n        \"TYPECAST_API_KEY\": \"YOUR_API_KEY\",\n        \"TYPECAST_OUTPUT_DIR\": \"PATH/TO/YOUR/OUTPUT/DIR\"\n      }\n    }\n  }\n}\n```\n\nReplace `/PATH/TO/YOUR/PROJECT` with the actual path where your project is located.\n\n### Manual Execution\n\nYou can also run the server manually:\n\n```bash\nuv run python app/main.py\n```\n\n## Contributing\n\nContributions are always welcome! Feel free to submit a Pull Request.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "OOXXXXOO--ChatTTS": {
      "owner": "OOXXXXOO",
      "name": "ChatTTS",
      "url": "https://github.com/OOXXXXOO/ChatTTS",
      "imageUrl": "https://github.com/OOXXXXOO.png",
      "description": "ChatTTS generates natural and expressive speech optimized for dialogue scenarios, supporting multi-speaker interactions and fine-grained prosodic control. It is capable of producing speech in both English and Chinese, enabling interactive conversations with features such as laughter and pauses.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2024-05-31T14:45:20Z",
      "readme_content": "# ChatTTS\n[**English**](./README.md) | [**中文简体**](./README_CN.md)\n\nChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre trained model without SFT.\n\nFor formal inquiries about model and roadmap, please contact us at **open-source@2noise.com**. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.\n\n---\n## Highlights\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\n\nFor the detailed description of the model, you can refer to **[video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**\n\n---\n\n## Disclaimer\n\nThis repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\n\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\n\n\n---\n## Usage\n\n<h4>Basic usage</h4>\n\n```python\nimport ChatTTS\nfrom IPython.display import Audio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<h4>Advanced usage</h4>\n\n```python\n###################################\n# Sample a speaker from Gaussian.\n\nrand_spk = chat.sample_random_speaker()\n\nparams_infer_code = {\n  'spk_emb': rand_spk, # add sampled speaker \n  'temperature': .3, # using custom temperature\n  'top_P': 0.7, # top P decode\n  'top_K': 20, # top K decode\n}\n\n###################################\n# For sentence level manual control.\n\n# use oral_(0-9), laugh_(0-2), break_(0-7) \n# to generate special token in text to synthesize.\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_6]'\n} \n\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\n\n###################################\n# For word level manual control.\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\nwav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<details open>\n  <summary><h4>Example: self introduction</h4></summary>\n\n```python\ninputs_en = \"\"\"\nchat T T S is a text to speech model designed for dialogue applications. \n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \ncapabilities with precise control over prosodic elements [laugh]like like \n[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. \n[uv_break]it delivers natural and expressive speech,[uv_break]so please\n[uv_break] use the project responsibly at your own risk.[uv_break]\n\"\"\".replace('\\n', '') # English is still experimental.\n\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_4]'\n} \n# audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_en[0]), 24000)\n```\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\n\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\n</details>\n\n---\n## Roadmap\n- [x] Open-source the 40k hour base model and spk_stats file\n- [ ] Open-source VQ encoder and Lora training code\n- [ ] Streaming audio generation without refining the text*\n- [ ] Open-source the 40k hour version with multi-emotion control\n- [ ] ChatTTS.cpp maybe? (PR or new repo are welcomed.)\n \n----\n## FAQ\n\n##### How much VRAM do I need? How about infer speed?\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\n\n##### model stability is not good enough, with issues such as multi speakers or poor audio quality.\n\nThis is a problem that typically occurs with autoregressive models(for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\n\n##### Besides laughter, can we control anything else? Can we control other emotions?\n\nIn the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.\n\n---\n## Acknowledgements\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demostrate a remarkable TTS result by a autoregressive-style system.\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\n\n---\n## Special Appreciation\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "PhialsBasement--Zonos-TTS-MCP": {
      "owner": "PhialsBasement",
      "name": "Zonos-TTS-MCP",
      "url": "https://github.com/PhialsBasement/Zonos-TTS-MCP",
      "imageUrl": "https://github.com/PhialsBasement.png",
      "description": "Facilitates text-to-speech capabilities using Claude, supporting various emotions and languages for speech generation.",
      "stars": 14,
      "forks": 9,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:12:29Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-zonos-tts-mcp-badge.png)](https://mseep.ai/app/phialsbasement-zonos-tts-mcp)\n\n# Zonos MCP Integration\n[![smithery badge](https://smithery.ai/badge/@PhialsBasement/zonos-tts-mcp)](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp)\n\nA Model Context Protocol integration for Zonos TTS, allowing Claude to generate speech directly.\n\n## Setup\n\n### Installing via Smithery\n\nTo install Zonos TTS Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp):\n\n```bash\nnpx -y @smithery/cli install @PhialsBasement/zonos-tts-mcp --client claude\n```\n\n### Manual installation\n\n1. Make sure you have Zonos running with our API implementation ([PhialsBasement/zonos-api](https://github.com/PhialsBasement/Zonos-API))\n\n2. Install dependencies:\n```bash\nnpm install @modelcontextprotocol/sdk axios\n```\n\n3. Configure PulseAudio access:\n```bash\n# Your pulse audio should be properly configured for audio playback\n# The MCP server will automatically try to connect to your pulse server\n```\n\n4. Build the MCP server:\n```bash\nnpm run build\n# This will create the dist folder with the compiled server\n```\n\n5. Add to Claude's config file:\nEdit your Claude config file (usually in `~/.config/claude/config.json`) and add this to the `mcpServers` section:\n\n```json\n\"zonos-tts\": {\n  \"command\": \"node\",\n  \"args\": [\n    \"/path/to/your/zonos-mcp/dist/server.js\"\n  ]\n}\n```\n\nReplace `/path/to/your/zonos-mcp` with the actual path where you installed the MCP server.\n\n## Using with Claude\n\nOnce configured, Claude automatically knows how to use the `speak_response` tool:\n\n```python\nspeak_response(\n    text=\"Your text here\",\n    language=\"en-us\",  # optional, defaults to en-us\n    emotion=\"happy\"    # optional: \"neutral\", \"happy\", \"sad\", \"angry\"\n)\n```\n\n## Features\n\n- Text-to-speech through Claude\n- Multiple emotions support\n- Multi-language support\n- Proper audio playback through PulseAudio\n\n## Requirements\n\n- Node.js\n- PulseAudio setup\n- Running instance of Zonos API (PhialsBasement/zonos-api)\n- Working audio output device\n\n## Notes\n\n- Make sure both the Zonos API server and this MCP server are running\n- Audio playback requires proper PulseAudio configuration\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "pinkpixel-dev--MCPollinations": {
      "owner": "pinkpixel-dev",
      "name": "MCPollinations",
      "url": "https://github.com/pinkpixel-dev/MCPollinations",
      "imageUrl": "https://github.com/pinkpixel-dev.png",
      "description": "Generates images, text, and audio from prompts using the Pollinations APIs. It supports returning images as base64-encoded data and allows listing available models for image and text generation.",
      "stars": 34,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T03:37:33Z",
      "readme_content": "# MCPollinations Multimodal MCP Server\nA Model Context Protocol (MCP) server that enables AI assistants to generate images, text, and audio through the Pollinations APIs\n\n[![smithery badge](https://smithery.ai/badge/@pinkpixel-dev/mcpollinations)](https://smithery.ai/server/@pinkpixel-dev/mcpollinations) [![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/8448e4ec-c863-476a-8adb-aed3cf16ea2b)\n\n## Features\n\n- Generate image URLs from text prompts\n- Generate images and return them as base64-encoded data AND save as png, jpeg, jpg, or webp (default: png)\n- Generate text responses from text prompts\n- Generate audio responses from text prompts\n- List available image and text generation models\n- No authentication required\n- Simple and lightweight\n- Compatible with the Model Context Protocol (MCP)\n\n## System Requirements\n\n- **Node.js**: Version 14.0.0 or higher\n  - For best performance, we recommend Node.js 16.0.0 or higher\n  - Node.js versions below 16 use an AbortController polyfill\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install mcpollinations for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/mcpollinations):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/mcpollinations --client claude\n```\n\nThe easiest way to use the MCP server:\n\n```bash\n# Run directly with npx (no installation required)\nnpx @pinkpixel/mcpollinations\n```\n\nIf you prefer to install it globally:\n\n```bash\n# Install globally\nnpm install -g @pinkpixel/mcpollinations\n\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n\n```\n\nOr clone the repository:\n\n```bash\n# Clone the git repository\ngit clone https://github.com/pinkpixel-dev/mcpollinations.git\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n# or run directly\nnode /path/to/MCPollinations/pollinations-mcp-server.js\n\n```\n\n## MCP Integration\n\nTo integrate the server with applications that support the Model Context Protocol (MCP):\n\n1. Generate an MCP configuration file:\n\n```bash\n# If installed globally\nnpx @pinkpixel/mcpollinations generate-config\n\n# Or run directly\nnode /path/to/MCPollinations/generate-mcp-config.js\n```\n\n### Quick MCP Config (env)\nIf you prefer to skip the generator, copy this into your MCP client config:\n\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"YOUR_TOKEN_OPTIONAL\",\n      \"referrer\": \"your-app-or-domain-optional\",\n      \"IMAGE_MODEL\": \"flux\",\n      \"IMAGE_WIDTH\": \"1024\",\n      \"IMAGE_HEIGHT\": \"1024\",\n      \"IMAGE_ENHANCE\": \"true\",\n      \"IMAGE_SAFE\": \"false\",\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"\",\n      \"AUDIO_VOICE\": \"alloy\",\n      \"OUTPUT_DIR\": \"./mcpollinations-output\"\n    }\n  }\n}\n```\n\n2. Follow the prompts to customize your configuration or use the defaults.\n   - Set an output directory (relative paths recommended for portability)\n     - **Windows users**: Consider using absolute paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations`) for more reliable file saving\n   - Configure optional authentication (token, referrer) under `env`\n   - Configure default parameters for image generation (with a list of available models, dimensions, etc.)\n   - Configure default parameters for text generation (with a list of available models)\n   - Configure default parameters for audio generation (voice)\n\n\n3. Copy the generated `mcp.json` file to your application's MCP settings .json file.\n4. Restart your application.\n\nAfter integration, you can use commands like:\n\n\"Generate an image of a sunset over the ocean using MCPollinations\"\n\n## Authentication (Optional)\n\nMCPollinations supports optional authentication to provide access to more models and better rate limits. The server works perfectly without authentication (free tier), but users with API tokens can get enhanced access.\n\n### Configuration Methods\n\n**Method 1: Environment Variables (Recommended for security)**\n```bash\n# Set environment variables before running the server\nexport POLLINATIONS_TOKEN=\"your-api-token\"\nexport POLLINATIONS_REFERRER=\"https://your-domain.com\"\n\n# Then run the server\nnpx @pinkpixel/mcpollinations\n```\n\n**Method 2: MCP Configuration File (env)**\nWhen generating your MCP configuration, place auth inside `env` so your MCP client passes them as environment variables to the server process:\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"your-api-token\",\n      \"referrer\": \"your-app-or-domain\"\n    }\n  }\n}\n```\n\nYou can also provide `POLLINATIONS_TOKEN` and `POLLINATIONS_REFERRER` instead; the server recognizes both forms. Using `token` and `referrer` inside `env` is recommended for MCP configs.\n\n### Authentication Parameters\n\n- **`token`** (optional): Your Pollinations API token for enhanced access\n- **`referrer`** (optional): Your domain/application referrer URL\n\nBoth parameters are completely optional. Leave them empty or unset to use the free tier.\n\n## Using Your Configuration Settings\n\nMCPollinations respects your MCP configuration settings placed in `env` as defaults. When you ask an AI assistant to generate content:\n\n- **Your configured models, output directories, and parameters are used automatically**\n- **To override**: Specifically instruct the AI to use different settings\n  - \"Generate an image using the kontext model\"\n  - \"Save this image to my Desktop folder\"\n  - \"Use a temperature of 1.2 for this text generation\"\n\n**Example Instructions:**\n- ✅ \"Generate a sunset image\" → Uses your configured model and output directory\n- ✅ \"Generate a sunset image with the flux model\" → Overrides model only\n- ✅ \"Generate a sunset image and save it to C:\\Pictures\" → Overrides output path only\n\nThis ensures your preferences are always respected unless you specifically want different settings for a particular request.\n\n## Troubleshooting\n\n### \"AbortController is not defined\" Error\n\nIf you encounter this error when running the MCP server:\n\n```\nReferenceError: AbortController is not defined\n```\n\nThis is usually caused by running on an older version of Node.js (below version 16.0.0). Try one of these solutions:\n\n1. **Update Node.js** (recommended):\n   - Update to Node.js 16.0.0 or newer\n\n2. **Use Global Installation**\n   - Update to the latest version of the package:\n   ```bash\n   npm install -g @pinkpixel/mcpollinations\n   # Run with npx\n   npx @pinkpixel/mcpollinations\n   ```\n\n3. **Install AbortController manually**:\n   - If for some reason the polyfill doesn't work:\n   ```bash\n   npm install node-abort-controller\n   ```\n\n### Check Your Node.js Version\n\nTo check your current Node.js version:\n\n```bash\nnode --version\n```\n\nIf it shows a version lower than 16.0.0, consider upgrading for best compatibility.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### **Image Generation Tools**\n1. `generateImageUrl` - Generates an image URL from a text prompt\n2. `generateImage` - Generates an image, returns it as base64-encoded data, and saves it to a file by default (PNG format)\n3. `editImage` - **NEW!** Edit or modify existing images based on text prompts\n4. `generateImageFromReference` - **NEW!** Generate new images using existing images as reference\n5. `listImageModels` - Lists available models for image generation\n\n### **Text & Audio Tools**\n6. `respondText` - Responds with text to a prompt using text models (customizable parameters)\n7. `respondAudio` - Generates an audio response to a text prompt (customizable voice parameter)\n8. `listTextModels` - Lists available models for text generation\n9. `listAudioVoices` - Lists all available voices for audio generation\n\n## Text Generation Details\n\n### Available Parameters\n\nThe `respondText` tool supports several parameters for fine-tuning text generation:\n\n- **`model`**: Choose from available text models (use `listTextModels` to see current options)\n- **`temperature`** (0.0-2.0): Controls randomness in the output\n  - Lower values (0.1-0.7) = more focused and deterministic\n  - Higher values (0.8-2.0) = more creative and random\n- **`top_p`** (0.0-1.0): Controls diversity via nucleus sampling\n  - Lower values = more focused on likely tokens\n  - Higher values = considers more token possibilities\n- **`system`**: System prompt to guide the model's behavior and personality\n\n### Customizing Text Generation\n\n```javascript\n// Example options for respondText\nconst options = {\n  model: \"openai\",           // Model selection\n  temperature: 0.7,          // Balanced creativity\n  top_p: 0.9,               // High diversity\n  system: \"You are a helpful assistant that explains things clearly and concisely.\"\n};\n```\n\n### Configuration Examples\n\nIn your MCP configuration, set defaults under `env` so the server uses them automatically:\n\n```json\n{\n  \"mcpollinations\": {\n    \"env\": {\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"You are a helpful coding assistant.\"\n    }\n  }\n}\n```\n\n## Image-to-Image Generation (NEW!)\n\nMCPollinations now supports powerful image-to-image generation with two specialized tools:\n\n### **editImage Tool**\nPerfect for modifying existing images:\n- **Remove objects**: \"remove the cat from this image\"\n- **Add elements**: \"add a dog to this scene\"\n- **Change backgrounds**: \"replace the background with mountains\"\n- **Style modifications**: \"make the lighting more dramatic\"\n\n### **generateImageFromReference Tool**\nPerfect for creating variations and new styles:\n- **Style transfer**: \"make this photo look like a painting\"\n- **Format changes**: \"convert this to a cartoon style\"\n- **Creative variations**: \"create a futuristic version of this\"\n- **Artistic interpretations**: \"make this look like a sketch\"\n\n### **Supported Models**\n- **`kontext`**: Specialized model optimized for image-to-image tasks\n- **`nanobanana`**: New Google model supporting both text-to-image and image-to-image generation\n- **`seedream`**: New ByteDance model supporting both text-to-image and image-to-image generation\n\nMulti-reference images: `editImage` and `generateImageFromReference` accept `imageUrl` as a single URL or an array of URLs. The server encodes arrays as the comma-separated `image` parameter used by the API. Ordering matters; kontext uses only the first image, nanobanana is safe up to ~4 refs, and seedream supports up to 10.\n\nImportant: URLs only. The image-to-image tools require publicly accessible HTTP(S) URLs. Local file paths, file uploads, and base64/data URLs are not supported by this MCP server (it does not upload files). If you need to work from a local image, host it somewhere accessible (e.g., a temporary file host, object storage, or a raw link in a repo) and pass the URL.\n\n### **Example Usage**\n```javascript\n// Edit an existing image\nconst editResult = await editImage(\n  \"change the background to a sunset beach\",\n  \"https://example.com/photo.jpg\",\n  \"nanobanana\"  // or \"kontext\", \"seedream\"\n);\n\n// Generate from reference\nconst referenceResult = await generateImageFromReference(\n  \"make this into a watercolor painting\",\n  \"https://example.com/photo.jpg\",\n  \"seedream\"  // or \"kontext\", \"nanobanana\"\n);\n```\n\n## Image Generation Details\n\n### Default Behavior\n\nWhen using the `generateImage` tool:\n\n- Images are saved to disk by default as PNG files\n- The default save location is the current working directory where the MCP server is running\n- The 'flux' model is used by default\n- A random seed is generated by default for each image (ensuring variety)\n- Base64-encoded image data is always returned, regardless of whether the image is saved to a file\n\n### Customizing Image Generation\n\n```javascript\n// Example options for generateImage\nconst options = {\n  // Model selection (defaults to 'flux')\n  // Available models: \"flux\", \"turbo\", \"kontext\", \"nanobanana\", \"seedream\"\n  model: \"flux\",\n\n  // Image dimensions\n  width: 1024,\n  height: 1024,\n\n  // Generation options\n  seed: 12345,  // Specific seed for reproducibility (defaults to random)\n  enhance: true,  // Enhance the prompt using an LLM before generating (defaults to true)\n  safe: false,  // Content filtering (defaults to false)\n\n  // File saving options\n  saveToFile: true,  // Set to false to skip saving to disk\n  outputPath: \"/path/to/save/directory\",  // Custom save location\n  fileName: \"my_custom_name\",  // Without extension\n  format: \"png\"  // png, jpeg, jpg, or webp\n};\n```\n\n### Where Images Are Saved\n\nWhen using Claude or another application with the MCP server:\n\n1. **Images are saved in the current working directory of where the MCP server is running**, not where Claude or the client application is installed.\n\n2. If you start the MCP server manually from a specific directory, images will be saved there by default.\n\n3. If Claude Desktop launches the MCP server automatically, images will be saved in Claude Desktop's working directory (typically in an application data folder).\n\n**💡 Windows Users**: For reliable file saving on Windows, use absolute paths in your MCP configuration instead of relative paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations` instead of `./mcpollinations-output`). Relative paths may not resolve as expected depending on the working directory context.\n\n### Finding Your Generated Images\n\n- The response from Claude after generating an image includes the full file path where the image was saved\n- You can specify a familiar location using the `outputPath` parameter\n- Best practice: Ask Claude to save images to an easily accessible folder like your Pictures or Downloads directory\n\n### Unique Filenames\n\nThe MCP server ensures that generated images always have unique filenames and will never overwrite existing files:\n\n1. **Default filenames** include:\n   - A sanitized version of the prompt (first 20 characters)\n   - A timestamp\n   - A random suffix\n\n2. **Custom filenames** are also protected:\n   - If you specify a filename and a file with that name already exists, a numeric suffix will be added automatically\n   - For example: `sunset.png`, `sunset_1.png`, `sunset_2.png`, etc.\n\nThis means you can safely generate multiple images with the same prompt or filename without worrying about overwriting previous images.\n\n### Accessing Base64 Data\n\nEven when saving to a file, the base64-encoded image data is always returned and can be used for:\n\n- Embedding in web pages (`<img src=\"data:image/png;base64,...\" />`)\n- Passing to other services or APIs\n- Processing in memory without filesystem operations\n- Displaying in applications that support data URIs\n\n## For Developers\n\nIf you want to use the package in your own projects:\n\n```bash\n# Install as a dependency\nnpm install @pinkpixel/mcpollinations\n\n# Import in your code\nimport { generateImageUrl, generateImage, repsondText, respondAudio, listTextModels, listImageModels, listAudioVoices } from '@pinkpixel/mcpollinations';\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "rsagacom--chatgpt-on-wechat": {
      "owner": "rsagacom",
      "name": "chatgpt-on-wechat",
      "url": "https://github.com/rsagacom/chatgpt-on-wechat",
      "imageUrl": "https://github.com/rsagacom.png",
      "description": "A multi-platform intelligent dialogue service that supports text, voice, and image interactions. It can connect to various AI models and allows for custom enterprise AI applications through plugin extensions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2024-01-28T14:00:49Z",
      "readme_content": "# 简介\n\n> 本项目是基于大模型的智能对话机器人，支持微信、企业微信、公众号、飞书、钉钉接入，可选择GPT3.5/GPT4.0/Claude/文心一言/讯飞星火/通义千问/Gemini/LinkAI，能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。\n\n最新版本支持的功能如下：\n\n- [x] **多端部署：** 有多种部署方式可选择且功能完备，目前已支持个人微信、微信公众号和、企业微信、飞书、钉钉等部署方式\n- [x] **基础对话：** 私聊及群聊的消息智能回复，支持多轮会话上下文记忆，支持 GPT-3.5, GPT-4, claude, Gemini, 文心一言, 讯飞星火, 通义千问\n- [x] **语音能力：** 可识别语音消息，通过文字或语音回复，支持 azure, baidu, google, openai(whisper/tts) 等多种语音模型\n- [x] **图像能力：** 支持图片生成、图片识别、图生图（如照片修复），可选择 Dall-E-3, stable diffusion, replicate, midjourney, vision模型\n- [x] **丰富插件：** 支持个性化插件扩展，已实现多角色切换、文字冒险、敏感词过滤、聊天记录总结、文档总结和对话、联网搜索等插件\n- [x] **知识库：** 通过上传知识库文件自定义专属机器人，可作为数字分身、智能客服、私域助手使用，基于 [LinkAI](https://link-ai.tech) 实现\n\n# 演示\n\nhttps://github.com/zhayujie/chatgpt-on-wechat/assets/26161723/d5154020-36e3-41db-8706-40ce9f3f1b1e\n\nDemo made by [Visionn](https://www.wangpc.cc/)\n\n# 商业支持\n\n> 我们还提供企业级的 **AI应用平台**，包含知识库、Agent插件、应用管理等能力，支持多平台聚合的应用接入、客户端管理、对话管理，以及提供\nSaaS服务、私有化部署、稳定托管接入 等多种模式。\n>\n> 目前已在私域运营、智能客服、企业效率助手等场景积累了丰富的 AI 解决方案， 在电商、文教、健康、新消费等各行业沉淀了 AI 落地的最佳实践，致力于打造助力中小企业拥抱 AI 的一站式平台。\n\n企业服务和商用咨询可联系产品顾问：\n\n<img width=\"240\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg\">\n\n# 开源社区\n\n添加小助手微信加入开源项目交流群：\n\n<img width=\"240\" src=\"./docs/images/contact.jpg\">\n\n# 更新日志\n\n>**2023.11.11：** [1.5.3版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) 和 [1.5.4版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)，新增Google Gemini、通义千问模型\n\n>**2023.11.10：** [1.5.2版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)，新增飞书通道、图像识别对话、黑名单配置\n\n>**2023.11.10：** [1.5.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)，新增 `gpt-4-turbo`, `dall-e-3`, `tts` 模型接入，完善图像理解&生成、语音识别&生成的多模态能力\n\n>**2023.10.16：** 支持通过意图识别使用LinkAI联网搜索、数学计算、网页访问等插件，参考[插件文档](https://docs.link-ai.tech/platform/plugins)\n\n>**2023.09.26：** 插件增加 文件/文章链接 一键总结和对话的功能，使用参考：[插件说明](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)\n\n>**2023.08.08：** 接入百度文心一言模型，通过 [插件](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) 支持 Midjourney 绘图\n\n>**2023.06.12：** 接入 [LinkAI](https://link-ai.tech/console) 平台，可在线创建领域知识库，并接入微信、公众号及企业微信中，打造专属客服机器人。使用参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n>**2023.04.26：** 支持企业微信应用号部署，兼容插件，并支持语音图片交互，私人助理理想选择，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatcom/README.md)。(contributed by [@lanvent](https://github.com/lanvent) in [#944](https://github.com/zhayujie/chatgpt-on-wechat/pull/944))\n\n>**2023.04.05：** 支持微信公众号部署，兼容插件，并支持语音图片交互，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatmp/README.md)。(contributed by [@JS00000](https://github.com/JS00000) in [#686](https://github.com/zhayujie/chatgpt-on-wechat/pull/686))\n\n>**2023.04.05：** 增加能让ChatGPT使用工具的`tool`插件，[使用文档](https://github.com/goldfishh/chatgpt-on-wechat/blob/master/plugins/tool/README.md)。工具相关issue可反馈至[chatgpt-tool-hub](https://github.com/goldfishh/chatgpt-tool-hub)。(contributed by [@goldfishh](https://github.com/goldfishh) in [#663](https://github.com/zhayujie/chatgpt-on-wechat/pull/663))\n\n>**2023.03.25：** 支持插件化开发，目前已实现 多角色切换、文字冒险游戏、管理员指令、Stable Diffusion等插件，使用参考 [#578](https://github.com/zhayujie/chatgpt-on-wechat/issues/578)。(contributed by [@lanvent](https://github.com/lanvent) in [#565](https://github.com/zhayujie/chatgpt-on-wechat/pull/565))\n\n>**2023.03.09：** 基于 `whisper API`(后续已接入更多的语音`API`服务) 实现对微信语音消息的解析和回复，添加配置项 `\"speech_recognition\":true` 即可启用，使用参考 [#415](https://github.com/zhayujie/chatgpt-on-wechat/issues/415)。(contributed by [wanggang1987](https://github.com/wanggang1987) in [#385](https://github.com/zhayujie/chatgpt-on-wechat/pull/385))\n\n>**2023.02.09：** 扫码登录存在账号限制风险，请谨慎使用，参考[#58](https://github.com/AutumnWhj/ChatGPT-wechat-bot/issues/158)\n\n# 快速开始\n\n快速开始文档：[项目搭建文档](https://docs.link-ai.tech/cow/quick-start)\n\n## 准备\n\n### 1. 账号注册\n\n项目默认使用OpenAI接口，需前往 [OpenAI注册页面](https://beta.openai.com/signup) 创建账号，创建完账号则前往 [API管理页面](https://beta.openai.com/account/api-keys) 创建一个 API Key 并保存下来，后面需要在项目中配置这个key。接口需要海外网络访问及绑定信用卡支付。\n\n> 默认对话模型是 openai 的 gpt-3.5-turbo，计费方式是约每 1000tokens (约750个英文单词 或 500汉字，包含请求和回复) 消耗 $0.002，图片生成是Dell E模型，每张消耗 $0.016。\n\n项目同时也支持使用 LinkAI 接口，无需代理，可使用 文心、讯飞、GPT-3、GPT-4 等模型，支持 定制化知识库、联网搜索、MJ绘图、文档总结和对话等能力。修改配置即可一键切换，参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n### 2.运行环境\n\n支持 Linux、MacOS、Windows 系统（可在Linux服务器上长期运行)，同时需安装 `Python`。\n> 建议Python版本在 3.7.1~3.9.X 之间，推荐3.8版本，3.10及以上版本在 MacOS 可用，其他系统上不确定能否正常运行。\n\n> 注意：Docker 或 Railway 部署无需安装python环境和下载源码，可直接快进到下一节。\n\n**(1) 克隆项目代码：**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\n注: 如遇到网络问题可选择国内镜像 https://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) 安装核心依赖 (必选)：**\n> 能够使用`itchat`创建机器人，并具有文字交流功能所需的最小依赖集合。\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) 拓展依赖 (可选，建议安装)：**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n> 如果某项依赖安装失败可注释掉对应的行再继续\n\n## 配置\n\n配置文件的模板在根目录的`config-template.json`中，需复制该模板创建最终生效的 `config.json` 文件：\n\n```bash\n  cp config-template.json config.json\n```\n\n然后在`config.json`中填入配置，以下是对默认配置的说明，可根据需要进行自定义修改（请去掉注释）：\n\n```bash\n# config.json文件内容示例\n{\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # 填入上面创建的 OpenAI API KEY\n  \"model\": \"gpt-3.5-turbo\",                                   # 模型名称, 支持 gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, wenxin, xunfei\n  \"proxy\": \"\",                                                # 代理客户端的ip和端口，国内环境开启代理的需要填写该项，如 \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # 私聊时文本需要包含该前缀才能触发机器人回复\n  \"single_chat_reply_prefix\": \"[bot] \",                       # 私聊时自动回复的前缀，用于区分真人\n  \"group_chat_prefix\": [\"@bot\"],                              # 群聊时包含该前缀则会触发机器人回复\n  \"group_name_white_list\": [\"ChatGPT测试群\", \"ChatGPT测试群2\"], # 开启自动回复的群名称列表\n  \"group_chat_in_one_session\": [\"ChatGPT测试群\"],              # 支持会话上下文共享的群名称  \n  \"image_create_prefix\": [\"画\", \"看\", \"找\"],                   # 开启图片回复的前缀\n  \"conversation_max_tokens\": 1000,                            # 支持上下文记忆的最多字符数\n  \"speech_recognition\": false,                                # 是否开启语音识别\n  \"group_speech_recognition\": false,                          # 是否开启群组语音识别\n  \"use_azure_chatgpt\": false,                                 # 是否使用Azure ChatGPT service代替openai ChatGPT service. 当设置为true时需要设置 open_ai_api_base，如 https://xxx.openai.azure.com/\n  \"azure_deployment_id\": \"\",                                  # 采用Azure ChatGPT时，模型部署名称\n  \"azure_api_version\": \"\",                                    # 采用Azure ChatGPT时，API版本\n  \"character_desc\": \"你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。\",  # 人格描述\n  # 订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复，可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n  \"subscribe_msg\": \"感谢您的关注！\\n这里是ChatGPT，可以自由对话。\\n支持语音对话。\\n支持图片输出，画字开头的消息将按要求创作图片。\\n支持角色扮演和文字冒险等丰富插件。\\n输入{trigger_prefix}#help 查看详细指令。\",\n  \"use_linkai\": false,                                        # 是否使用LinkAI接口，默认关闭，开启后可国内访问，使用知识库和MJ\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI 应用code\n}\n```\n**配置说明：**\n\n**1.个人聊天**\n\n+ 个人聊天中，需要以 \"bot\"或\"@bot\" 为开头的内容触发机器人，对应配置项 `single_chat_prefix` (如果不需要以前缀触发可以填写  `\"single_chat_prefix\": [\"\"]`)\n+ 机器人回复的内容会以 \"[bot] \" 作为前缀， 以区分真人，对应的配置项为 `single_chat_reply_prefix` (如果不需要前缀可以填写 `\"single_chat_reply_prefix\": \"\"`)\n\n**2.群组聊天**\n\n+ 群组聊天中，群名称需配置在 `group_name_white_list ` 中才能开启群聊自动回复。如果想对所有群聊生效，可以直接填写 `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ 默认只要被人 @ 就会触发机器人自动回复；另外群聊天中只要检测到以 \"@bot\" 开头的内容，同样会自动回复（方便自己触发），这对应配置项 `group_chat_prefix`\n+ 可选配置: `group_name_keyword_white_list`配置项支持模糊匹配群名称，`group_chat_keyword`配置项则支持模糊匹配群消息内容，用法与上述两个配置项相同。（Contributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`：使群聊共享一个会话上下文，配置 `[\"ALL_GROUP\"]` 则作用于所有群聊\n\n**3.语音识别**\n\n+ 添加 `\"speech_recognition\": true` 将开启语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，该参数仅支持私聊 (注意由于语音消息无法匹配前缀，一旦开启将对所有语音自动回复，支持语音触发画图)；\n+ 添加 `\"group_speech_recognition\": true` 将开启群组语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，参数仅支持群聊 (会匹配group_chat_prefix和group_chat_keyword, 支持语音触发画图)；\n+ 添加 `\"voice_reply_voice\": true` 将开启语音回复语音（同时作用于私聊和群聊），但是需要配置对应语音合成平台的key，由于itchat协议的限制，只能发送语音mp3文件，若使用wechaty则回复的是微信语音。\n\n**4.其他配置**\n\n+ `model`: 模型名称，目前支持 `gpt-3.5-turbo`, `text-davinci-003`, `gpt-4`, `gpt-4-32k`, `wenxin` , `claude` ,  `xunfei`(其中gpt-4 api暂未完全开放，申请通过后可使用)\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat API接口参数，详情参考[OpenAI官方文档。](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`：由于目前 `openai` 接口国内无法访问，需配置代理客户端的地址，详情参考  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ 对于图像生成，在满足个人或群组触发条件外，还需要额外的关键词前缀来触发，对应配置 `image_create_prefix `\n+ 关于OpenAI对话及图片接口的参数配置（内容自由度、回复字数限制、图片大小等），可以参考 [对话接口](https://beta.openai.com/docs/api-reference/completions) 和 [图像接口](https://beta.openai.com/docs/api-reference/completions)  文档，在[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中检查哪些参数在本项目中是可配置的。\n+ `conversation_max_tokens`：表示能够记忆的上下文最大字数（一问一答为一组对话，如果累积的对话字数超出限制，就会优先移除最早的一组对话）\n+ `rate_limit_chatgpt`，`rate_limit_dalle`：每分钟最高问答速率、画图速率，超速后排队按序处理。\n+ `clear_memory_commands`: 对话内指令，主动清空前文记忆，字符串数组可自定义指令别名。\n+ `hot_reload`: 程序退出后，暂存微信扫码状态，默认关闭。\n+ `character_desc` 配置中保存着你对机器人说的一段话，他会记住这段话并作为他的设定，你可以为他定制任何人格      (关于会话上下文的更多内容参考该 [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`：订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复， 可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n\n**5.LinkAI配置 (可选)**\n\n+ `use_linkai`: 是否使用LinkAI接口，开启后可国内访问，使用知识库和 `Midjourney` 绘画, 参考 [文档](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Key，可在 [控制台](https://link-ai.tech/console/interface) 创建\n+ `linkai_app_code`: LinkAI 应用code，选填\n\n**本说明文档可能会未及时更新，当前所有可选的配置项均在该[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中列出。**\n\n## 运行\n\n### 1.本地运行\n\n如果是开发机 **本地运行**，直接在项目根目录下执行：\n\n```bash\npython3 app.py                                    # windows环境下该命令通常为 python app.py\n```\n\n终端输出二维码后，使用微信进行扫码，当输出 \"Start auto replying\" 时表示自动回复程序已经成功运行了（注意：用于登录的微信需要在支付处已完成实名认证）。扫码登录后你的账号就成为机器人了，可以在微信手机端通过配置的关键词触发自动回复 (任意好友发送消息给你，或是自己发消息给好友)，参考[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)。\n\n### 2.服务器部署\n\n使用nohup命令在后台运行程序：\n\n```bash\nnohup python3 app.py & tail -f nohup.out          # 在后台运行程序并通过日志输出二维码\n```\n扫码登录后程序即可运行于服务器后台，此时可通过 `ctrl+c` 关闭日志，不会影响后台程序的运行。使用 `ps -ef | grep app.py | grep -v grep` 命令可查看运行于后台的进程，如果想要重新启动程序可以先 `kill` 掉对应的进程。日志关闭后如果想要再次打开只需输入 `tail -f nohup.out`。此外，`scripts` 目录下有一键运行、关闭程序的脚本供使用。\n\n> **多账号支持：** 将项目复制多份，分别启动程序，用不同账号扫码登录即可实现同时运行。\n\n> **特殊指令：** 用户向机器人发送 **#reset** 即可清空该用户的上下文记忆。\n\n\n### 3.Docker部署\n\n> 使用docker部署无需下载源码和安装依赖，只需要获取 docker-compose.yml 配置文件并启动容器即可。\n\n> 前提是需要安装好 `docker` 及 `docker-compose`，安装成功的表现是执行 `docker -v` 和 `docker-compose version` (或 docker compose version) 可以查看到版本号，可前往 [docker官网](https://docs.docker.com/engine/install/) 进行下载。\n\n#### (1) 下载 docker-compose.yml 文件\n\n```bash\nwget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml\n```\n\n下载完成后打开 `docker-compose.yml` 修改所需配置，如 `OPEN_AI_API_KEY` 和 `GROUP_NAME_WHITE_LIST` 等。\n\n#### (2) 启动容器\n\n在 `docker-compose.yml` 所在目录下执行以下命令启动容器：\n\n```bash\nsudo docker compose up -d\n```\n\n运行 `sudo docker ps` 能查看到 NAMES 为 chatgpt-on-wechat 的容器即表示运行成功。\n\n注意：\n\n - 如果 `docker-compose` 是 1.X 版本 则需要执行 `sudo  docker-compose up -d` 来启动容器\n - 该命令会自动去 [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) 拉取 latest 版本的镜像，latest 镜像会在每次项目 release 新的版本时生成\n\n最后运行以下命令可查看容器运行日志，扫描日志中的二维码即可完成登录：\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n#### (3) 插件使用\n\n如果需要在docker容器中修改插件配置，可通过挂载的方式完成，将 [插件配置文件](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\n重命名为 `config.json`，放置于 `docker-compose.yml` 相同目录下，并在 `docker-compose.yml` 中的 `chatgpt-on-wechat` 部分下添加 `volumes` 映射:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n\n### 4. Railway部署\n\n> Railway 每月提供5刀和最多500小时的免费额度。 (07.11更新: 目前大部分账号已无法免费部署)\n\n1. 进入 [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)\n2. 点击 `Deploy Now` 按钮。\n3. 设置环境变量来重载程序运行的参数，例如`open_ai_api_key`, `character_desc`。\n\n**一键部署:**\n  \n  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)\n\n## 常见问题\n\nFAQs： <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\n或直接在线咨询 [项目小助手](https://link-ai.tech/app/Kv2fXJcH)  (beta版本，语料完善中，回复仅供参考)\n\n## 开发\n\n欢迎接入更多应用，参考 [Terminal代码](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) 实现接收和发送消息逻辑即可接入。 同时欢迎增加新的插件，参考 [插件说明文档](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)。\n\n## 联系\n\n欢迎提交PR、Issues，以及Star支持一下。程序运行遇到问题可以查看 [常见问题列表](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ，其次前往 [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) 中搜索。个人开发者可加入开源交流群参与更多讨论，企业用户可联系[产品顾问](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)咨询。\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "scarletlabs-ai--Votars-MCP": {
      "owner": "scarletlabs-ai",
      "name": "Votars-MCP",
      "url": "https://github.com/scarletlabs-ai/Votars-MCP",
      "imageUrl": "https://github.com/scarletlabs-ai.png",
      "description": "Integrate advanced AI functionalities for processing complex tasks through robust APIs. Supports voice recording, transcription, and intelligent AI processing for meetings.",
      "stars": 27,
      "forks": 2,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-07-07T07:12:32Z",
      "readme_content": "![Votars Logo](https://votars.ai/_next/static/media/logo.e7b6bff6.svg) \n# Votars MCP \n[![smithery badge](https://smithery.ai/badge/@scarletlabs-ai/votars-mcp)](https://smithery.ai/server/@scarletlabs-ai/votars-mcp)\n\n\n## Overview\n\nVotars-MCP is a tool that supports multiple language implementations of the **Votars MCP server**. Currently, only the Go version is available, with other languages to be added in future releases. It supports two interaction modes: `sse` (Server-Sent Events) and `stdio` (Standard Input/Output). It is designed to provide seamless integration with the Votars AI platform for processing various tasks.\n\n## About Votars\n\n[Votars](https://votars.ai/en/) is the world's smartest multilingual meeting assistant, designed for voice recording, transcription, and advanced AI processing. It features real-time translation, intelligent error correction, AI summarization, smart content generation, and AI discussions. The Votars app is available on [Web](https://votars.ai/en/), [iOS](https://apps.apple.com/us/app/votars-ai-transcribe-organize/id6737496290), and [Android](https://play.google.com/store/apps/details?id=com.votars.transcribe).\n\nAdditionally, Votars is an AI-powered platform that enables developers to integrate advanced AI functionalities into their applications. By leveraging Votars, you can process complex tasks efficiently with robust APIs designed for high performance and scalability.\n\n## Features\n- **Easy Integration with Votars**\n- **Modular Design:** Ready to be extended with additional functionalities.\n- **Supported MCP Tools:**\n  - `Votars_fetch_recent_transcripts`: Allows users to read recent transcripts from their workspace, providing convenient access to the latest recorded sessions.\n  - `Votars_fetch_a_specific_transcript`: Enables users to retrieve specific transcripts by providing a transcript ID, allowing targeted retrieval of stored data.\n  \n  More functionalities will be added soon. Stay tuned!\n\n## Installation (Go MCP)\n\n### Installing via Smithery\n\nTo install votars-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@scarletlabs-ai/votars-mcp):\n\n```bash\nnpx -y @smithery/cli install @scarletlabs-ai/votars-mcp --client claude\n```\n\n### Manual Installation\nTo install the Go version of Votars MCP from the GitHub repository, use:\n\n```bash\n go install github.com/scarletlabs-ai/Votars-MCP/go/votars-mcp@latest\n```\n\n## Usage (Go MCP)\n\n### Run MCP Service\nBefore using the `sse` mode, you need to run the MCP server. Open a terminal and run:\n\n```bash\nvotars-mcp -t sse -p 8080\n```\n\nThis command starts the MCP service on port 8080, ready to accept `sse` requests.\n\n\n### 1. SSE Mode\n\nFor `sse` mode, you need to provide the API key via request headers in the configuration file.\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP\": {\n      \"type\": \"sse\",\n      \"url\": \"http://0.0.0.0:8080/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n### 2. Stdio Mode\n\nFor `stdio` mode, set the API key as an environment variable.\n\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP Stdio\": {\n      \"type\": \"stdio\",\n      \"command\": \"votars-mcp\",\n      \"args\": [\"-t\", \"stdio\"],\n      \"env\": {\n        \"VOTARS_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n## Obtaining Your API Key\n\n1. Go to [Votars.AI](https://votars.ai/en/) and register.\n2. Navigate to your workspace's `Settings`.\n3. Create an API Key under the API Key management section.\n\n![manage apikey](https://private-user-images.githubusercontent.com/677477/427500562-8cfd8465-f408-4e9b-a101-8b9b8e5e57f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMwNzQ4NzAsIm5iZiI6MTc0MzA3NDU3MCwicGF0aCI6Ii82Nzc0NzcvNDI3NTAwNTYyLThjZmQ4NDY1LWY0MDgtNGU5Yi1hMTAxLThiOWI4ZTVlNTdmNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMyN1QxMTIyNTBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02ODkyMWY3YzgyYTA2ZjdmNGQxN2MyYzllMzBmZmFiZmVjNGFmZTliNDQzODUwMjU2M2E1MjJkZTI4MmExM2VmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.V0GAoh6l0JFRxmSokliGsVt5yNpxtmTmeCFiZG7U3jU)\n\n## Roadmap\n\n- **Current Support:** Go\n- **Planned Support:** Python, JavaScript, Rust, etc.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "uraoz--bouyomichan-mcp-nodejs": {
      "owner": "uraoz",
      "name": "bouyomichan-mcp-nodejs",
      "url": "https://github.com/uraoz/bouyomichan-mcp-nodejs",
      "imageUrl": "https://github.com/uraoz.png",
      "description": "Provides text-to-speech capabilities using BouyomiChan's Yukkuri voice, enabling voice output from text commands with customizable options for voice type, volume, speed, and pitch. Integrates seamlessly with Claude for Desktop for enhanced user interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T11:48:46Z",
      "readme_content": "# 棒読みちゃんMCPサーバー (Node.js版)\n\n<a href=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs/badge\" alt=\"Bouyomi-chan Server MCP server\" />\n</a>\n\n\n## 前提条件\n\n- Node.js 16以上\n- npm 7以上\n- 棒読みちゃんがインストールされていること\n- 棒読みちゃんのHTTP連携がポート50080で起動していること\n\n## 使用方法\n\n### ローカルでのサーバーの起動\n\n```bash\ngit clone https://github.com/uraoz/bouyomichan-mcp-nodejs.git\ncd bouyomichan-mcp-nodejs\nnpm install\nnpm run build\nnpm start\n```\n\n### Claude for Desktopとの連携\n\n```json\n{\n  \"mcpServers\": {\n    \"bouyomichan\":{\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:uraoz/bouyomichan-mcp-nodejs\"\n      ]\n    }\n  }\n}\n```\n\n## パラメータ説明\n\n| パラメータ | 説明 | デフォルト値 | 有効範囲 |\n|----------|------|------------|---------|\n| text     | 読み上げるテキスト | 必須 | 任意のテキスト |\n| voice    | 音声の種類 | 0 (女性1) | 0: 女性1、1: 男性1、2: 女性2、... |\n| volume   | 音量 | -1 (デフォルト) | -1: デフォルト、0-100: 音量レベル |\n| speed    | 速度 | -1 (デフォルト) | -1: デフォルト、50-200: 速度レベル |\n| tone     | 音程 | -1 (デフォルト) | -1: デフォルト、50-200: 音程レベル |\n\n## ライセンス\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "yangsenessa--mcp_voice_identify": {
      "owner": "yangsenessa",
      "name": "mcp_voice_identify",
      "url": "https://github.com/yangsenessa/mcp_voice_identify",
      "imageUrl": "https://github.com/yangsenessa.png",
      "description": "Provides voice recognition and text extraction capabilities, supporting both file input and base64 encoded data processed in structured formats. Operates in stdio and MCP modes for flexible integration with various systems.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-17T11:21:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yangsenessa-mcp-voice-identify-badge.png)](https://mseep.ai/app/yangsenessa-mcp-voice-identify)\n\n# Voice Recognition MCP Service\n\nThis service provides voice recognition and text extraction capabilities through both stdio and MCP modes.\n\n## Features\n\n- Voice recognition from file\n- Voice recognition from base64 encoded data\n- Text extraction\n- Support for both stdio and MCP modes\n- Structured voice recognition results\n- AIO protocol compliant responses\n\n## Project Structure\n\n- `voice_service.py` - Core service implementation\n- `stdio_server.py` - stdio mode entry point\n- `mcp_server.py` - MCP mode entry point\n- `build.py` - Build script for executables\n- `build_exec.sh` - Build execution script\n- `test_*.sh` - Test scripts for different functionalities\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/AIO-2030/mcp_voice_identify.git\ncd mcp_voice_identify\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up environment variables in `.env`:\n```\nAPI_URL=your_api_url\nAPI_KEY=your_api_key\n```\n\n## Usage\n\n### stdio Mode\n\n1. Run the service:\n```bash\npython stdio_server.py\n```\n\n2. Send JSON-RPC requests via stdin:\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"help\",\n    \"params\": {},\n    \"id\": 1\n}\n```\n\n3. Or use the executable:\n```bash\n./dist/voice_stdio\n```\n\n### MCP Mode\n\n1. Run the service:\n```bash\npython mcp_server.py\n```\n\n2. Or use the executable:\n```bash\n./dist/voice_mcp\n```\n\n## Response Format\n\nThe service follows the AIO protocol for response formatting. Here are examples of different response types:\n\n### Voice Recognition Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"voice\",\n        \"message\": \"Voice processed successfully\",\n        \"text\": \"test test test\",\n        \"metadata\": {\n            \"language\": \"en\",\n            \"emotion\": \"unknown\",\n            \"audio_type\": \"speech\",\n            \"speaker\": \"woitn\",\n            \"raw_text\": \"test test test\"\n        }\n    },\n    \"id\": 1\n}\n```\n\n### Help Information Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"result\": {\n        \"type\": \"voice_service\",\n        \"description\": \"This service provides voice recognition and text extraction services\",\n        \"author\": \"AIO-2030\",\n        \"version\": \"1.0.0\",\n        \"github\": \"https://github.com/AIO-2030/mcp_voice_identify\",\n        \"transport\": [\"stdio\"],\n        \"methods\": [\n            {\n                \"name\": \"help\",\n                \"description\": \"Show this help information.\"\n            },\n            {\n                \"name\": \"identify_voice\",\n                \"description\": \"Identify voice from file\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"Voice file path\"\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                }\n            },\n            {\n                \"name\": \"identify_voice_base64\",\n                \"description\": \"Identify voice from base64 encoded data\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"base64_data\": {\n                            \"type\": \"string\",\n                            \"description\": \"Base64 encoded voice data\"\n                        }\n                    },\n                    \"required\": [\"base64_data\"]\n                }\n            },\n            {\n                \"name\": \"extract_text\",\n                \"description\": \"Extract text\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"text\": {\n                            \"type\": \"string\",\n                            \"description\": \"Text to extract\"\n                        }\n                    },\n                    \"required\": [\"text\"]\n                }\n            }\n        ]\n    },\n    \"id\": 1\n}\n```\n\n### Error Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"error\",\n        \"message\": \"503 Server Error: Service Unavailable\",\n        \"error_code\": 503\n    },\n    \"id\": 1\n}\n```\n\n### Response Fields\n\nThe service provides three types of responses:\n\n1. Voice Recognition Response (using `output` field):\n| Field     | Description                          | Example Value |\n|-----------|--------------------------------------|---------------|\n| type      | Response type                        | \"voice\"       |\n| message   | Status message                       | \"Voice processed successfully\" |\n| text      | Recognized text content              | \"test test test\" |\n| metadata  | Additional information               | See below     |\n\n2. Help Information Response (using `result` field):\n| Field         | Description                          | Example Value |\n|---------------|--------------------------------------|---------------|\n| type          | Service type                         | \"voice_service\" |\n| description   | Service description                  | \"This service provides...\" |\n| author        | Service author                       | \"AIO-2030\"    |\n| version       | Service version                      | \"1.0.0\"       |\n| github        | GitHub repository URL                | \"https://github.com/...\" |\n| transport     | Supported transport modes            | [\"stdio\"]     |\n| methods       | Available methods                    | See methods list |\n\n3. Error Response (using `output` field):\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| type        | Response type                        | \"error\"       |\n| message     | Error message                        | \"503 Server Error: Service Unavailable\" |\n| error_code  | HTTP status code                     | 503          |\n\n### Metadata Fields\n\nThe `metadata` field in voice recognition responses contains:\n\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| language    | Language code                        | \"en\"          |\n| emotion     | Emotion state                        | \"unknown\"     |\n| audio_type  | Audio type                          | \"speech\"      |\n| speaker     | Speaker identifier                   | \"woitn\"       |\n| raw_text    | Original recognized text             | \"test test test\" |\n\n## Building Executables\n\n1. Make the build script executable:\n```bash\nchmod +x build_exec.sh\n```\n\n2. Build stdio mode executable:\n```bash\n./build_exec.sh\n```\n\n3. Build MCP mode executable:\n```bash\n./build_exec.sh mcp\n```\n\nThe executables will be created at:\n- stdio mode: `dist/voice_stdio`\n- MCP mode: `dist/voice_mcp`\n\n## Testing\n\nRun the test scripts:\n\n```bash\nchmod +x test_*.sh\n./test_help.sh\n./test_voice_file.sh\n./test_voice_base64.sh\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "yuiseki--edge_tts_mcp_server": {
      "owner": "yuiseki",
      "name": "edge_tts_mcp_server",
      "url": "https://github.com/yuiseki/edge_tts_mcp_server",
      "imageUrl": "https://github.com/yuiseki.png",
      "description": "Provide natural text-to-speech conversion using Microsoft Edge's speech synthesis capabilities, enabling customizable voice output in multiple languages with adjustable speed and pitch.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T03:20:28Z",
      "readme_content": "# Edge-TTS MCP Server\n\nModel Context Protocol (MCP) サーバーで、Microsoft Edge のテキスト読み上げ機能を活用した AI エージェントの音声合成サービスを提供します。\n\n## 概要\n\nこの MCP サーバーは、[edge-tts](https://github.com/rany2/edge-tts)ライブラリを使用して、テキストから音声への変換機能を提供します。AI エージェントが自然な音声で応答できるようにするためのツールとして設計されています。\n\n## 機能\n\n- テキストから音声への変換\n- 複数の音声と言語のサポート\n- 音声速度と音程の調整\n- 音声データのストリーミング\n\n## インストール\n\n```bash\npip install \"edge_tts_mcp_server\"\n```\n\nまたは開発モードでインストールする場合：\n\n```bash\ngit clone https://github.com/yuiseki/edge_tts_mcp_server.git\ncd edge_tts_mcp_server\npip install -e .\n```\n\n## 使用方法\n\n### VS Code での設定例\n\nVS Code の settings.json で設定する例：\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"edge-tts\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\Users\\\\__username__\\\\src\\\\edge_tts_mcp_server\\\\src\\\\edge_tts_mcp_server\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n### MCP Inspector での使用\n\n標準的な MCP サーバーとして実行：\n\n```bash\nmcp dev server.py\n```\n\n### uvx（uvicorn）での実行\n\nFastAPI ベースのサーバーとして uv で実行する場合：\n\n```bash\nuv --directory path/to/edge_tts_mcp_server/src/edge_tts_mcp_server run server.py\n```\n\nコマンドラインオプション：\n\n```bash\nedge-tts-mcp --host 0.0.0.0 --port 8080 --reload\n```\n\n## API エンドポイント\n\nFastAPI モードで実行した場合、以下のエンドポイントが利用可能です：\n\n- `/` - API 情報\n- `/health` - ヘルスチェック\n- `/voices` - 利用可能な音声一覧（オプションで `?locale=ja-JP` などでフィルタリング可能）\n- `/mcp` - MCP API エンドポイント\n\n## ライセンス\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0
    }
  }
}