{
  "category": "speech-recognition-and-synthesis",
  "categoryDisplay": "Speech Recognition and Synthesis",
  "description": "",
  "totalRepositories": 24,
  "repositories": {
    "CengSin--fishaudio-mcp": {
      "owner": "CengSin",
      "name": "fishaudio-mcp",
      "url": "https://github.com/CengSin/fishaudio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/CengSin.webp",
      "description": "Converts text into natural human speech with customizable audio formats and bitrates, while integrating seamlessly with MCP-compatible applications.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T10:49:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cengsin-fishaudio-mcp-badge.png)](https://mseep.ai/app/cengsin-fishaudio-mcp)\n\n# Fish Audio Python MCP æœåŠ¡\n\nè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Fish Audio API å®ç°çš„æ–‡å­—è½¬è¯­éŸ³ MCP æœåŠ¡ã€‚é€šè¿‡è¿™ä¸ªæœåŠ¡ï¼Œæ‚¨å¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶çš„äººå£°ï¼Œæ”¯æŒå¤šç§é…ç½®é€‰é¡¹ã€‚\n\n## åŠŸèƒ½ç‰¹ç‚¹\n\n- åŸºæœ¬æ–‡å­—è½¬è¯­éŸ³ï¼šå°†ä»»æ„æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶äººå£°\n- é«˜çº§æ–‡å­—è½¬è¯­éŸ³ï¼šæ”¯æŒè‡ªå®šä¹‰éŸ³é¢‘æ ¼å¼ã€æ¯”ç‰¹ç‡ç­‰å‚æ•°\n- å…¼å®¹ MCP åè®®ï¼šå¯ä¸æ”¯æŒ MCP çš„åº”ç”¨æ— ç¼é›†æˆ\n\n## å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt\n```\n\næˆ–ä½¿ç”¨ Python åŒ…ç®¡ç†å·¥å…·å®‰è£…ï¼š\n\n```bash\npip install fish-audio-sdk mcp python-dotenv\n```\n\n## é…ç½®\n\nåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š\n\n```\nAPI_KEY=your_fish_audio_api_key\nMODEL_ID=your_fish_audio_model_id\n```\n\næ‚¨éœ€è¦æ›¿æ¢ä¸ºæ‚¨çš„ Fish Audio API å¯†é’¥å’Œæ¨¡å‹ IDã€‚\n\n## ä½¿ç”¨æ–¹æ³•\n\n### å¯åŠ¨æœåŠ¡\n\n```bash\npython app.py\n```\n\næˆ–ä½¿ç”¨ MCP CLI å·¥å…·ï¼š\n\n```bash\nmcp run --file app.py\n```\n\n### è¿è¡Œç¤ºä¾‹\n\n```bash\npython example.py\n```\n\n### ä½¿ç”¨ MCP å®¢æˆ·ç«¯è°ƒç”¨æœåŠ¡\n\n```python\n# ç¤ºä¾‹ä»£ç \nfrom mcp.client import MCPClient\n\nclient = MCPClient(\"subprocess://python app.py\")\nresult = client.call(\"text_to_speech\", {\"text\": \"ä½ å¥½ï¼Œä¸–ç•Œï¼\"})\nprint(result)  # æ‰“å°ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n```\n\n## API åŠŸèƒ½è¯´æ˜\n\n### text_to_speech\n\nåŸºæœ¬æ–‡å­—è½¬è¯­éŸ³åŠŸèƒ½ã€‚\n\nå‚æ•°ï¼š\n- `text`: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬\n- `output_path`ï¼ˆå¯é€‰ï¼‰: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›ï¼Œå°†åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n\nè¿”å›ï¼šç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n\n### advanced_text_to_speech\n\né«˜çº§æ–‡å­—è½¬è¯­éŸ³åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šé…ç½®é€‰é¡¹ã€‚\n\nå‚æ•°ï¼š\n- `text`: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬\n- `output_path`ï¼ˆå¯é€‰ï¼‰: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›ï¼Œå°†åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n- `format`: è¾“å‡ºéŸ³é¢‘æ ¼å¼ (mp3, wav, pcm)ï¼Œé»˜è®¤ä¸º mp3\n- `mp3_bitrate`: MP3 æ¯”ç‰¹ç‡ (64, 128, 192 kbps)ï¼Œé»˜è®¤ä¸º 128\n- `chunk_length`: åˆ†å—é•¿åº¦ (100-300)ï¼Œé»˜è®¤ä¸º 200\n- `normalize`: æ˜¯å¦å¯¹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œé»˜è®¤ä¸º True\n- `latency`: å»¶è¿Ÿæ¨¡å¼ (normal, balanced)ï¼Œé»˜è®¤ä¸º normal\n\nè¿”å›ï¼šç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n\n### get_model_info\n\nè·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ã€‚\n\nè¿”å›ï¼šåŒ…å«æ¨¡å‹ ID å’Œ API å¯†é’¥å‰ç¼€çš„å­—å…¸\n\n### get_available_models\n\nè·å–å¯ç”¨çš„ Fish Audio æ¨¡å‹åˆ—è¡¨ã€‚\n\nè¿”å›ï¼šå¯ç”¨æ¨¡å‹ä¿¡æ¯åˆ—è¡¨\n\n## è®¸å¯è¯\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fishaudio",
        "audio",
        "cengsin",
        "cengsin fishaudio",
        "fishaudio mcp",
        "speech customizable"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "DefiBax--mcp_servers": {
      "owner": "DefiBax",
      "name": "mcp_servers",
      "url": "https://github.com/DefiBax/mcp_servers",
      "imageUrl": "/freedevtools/mcp/pfp/DefiBax.webp",
      "description": "Record audio and transcribe it using advanced AI models like OpenAI's Whisper. Supports integration with AI agents for enhanced interactivity and includes prompts for common recording scenarios.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-11T08:14:19Z",
      "readme_content": "# Voice Recorder MCP Server\n\nAn MCP server for recording audio and transcribing it using OpenAI's Whisper model. Designed to work as a Goose custom extension or standalone MCP server.\n\n## Features\n\n- Record audio from the default microphone\n- Transcribe recordings using Whisper\n- Integrates with Goose AI agent as a custom extension\n- Includes prompts for common recording scenarios\n\n## Installation\n\n```bash\n# Install from source\ngit clone https://github.com/DefiBax/voice-recorder-mcp.git\ncd voice-recorder-mcp\npip install -e .\n```\n\n## Usage\n\n### As a Standalone MCP Server\n\n```bash\n# Run with default settings (base.en model)\nvoice-recorder-mcp\n\n# Use a specific Whisper model\nvoice-recorder-mcp --model medium.en\n\n# Adjust sample rate\nvoice-recorder-mcp --sample-rate 44100\n```\n\n### Testing with MCP Inspector\n\nThe MCP Inspector provides an interactive interface to test your server:\n\n```bash\n# Install the MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run your server with the inspector\nnpx @modelcontextprotocol/inspector voice-recorder-mcp\n```\n\n### With Goose AI Agent\n\n1. Open Goose and go to Settings > Extensions > Add > Command Line Extension\n2. Set the name to `voice-recorder`\n3. In the Command field, enter the full path to the voice-recorder-mcp executable:\n   ```\n   /full/path/to/voice-recorder-mcp\n   ```\n   \n   Or for a specific model:\n   ```\n   /full/path/to/voice-recorder-mcp --model medium.en\n   ```\n   \n   To find the path, run:\n   ```bash\n   which voice-recorder-mcp\n   ```\n\n4. No environment variables are needed for basic functionality\n5. Start a conversation with Goose and introduce the recorder with:\n   \"I want you to take action from transcriptions returned by voice-recorder. For example, if I dictate a calculation like 1+1, please return the result.\"\n\n## Available Tools\n\n- `start_recording`: Start recording audio from the default microphone\n- `stop_and_transcribe`: Stop recording and transcribe the audio to text\n- `record_and_transcribe`: Record audio for a specified duration and transcribe it\n\n## Whisper Models\n\nThis extension supports various Whisper model sizes:\n\n| Model | Speed | Accuracy | Memory Usage | Use Case |\n|-------|-------|----------|--------------|----------|\n| `tiny.en` | Fastest | Lowest | Minimal | Testing, quick transcriptions |\n| `base.en` | Fast | Good | Low | Everyday use (default) |\n| `small.en` | Medium | Better | Moderate | Good balance |\n| `medium.en` | Slow | High | High | Important recordings |\n| `large` | Slowest | Highest | Very High | Critical transcriptions |\n\nThe `.en` suffix indicates models specialized for English, which are faster and more accurate for English content.\n\n## Requirements\n\n- Python 3.12+\n- An audio input device (microphone)\n\n## Configuration\n\nYou can configure the server using environment variables:\n\n```bash\n# Set Whisper model\nexport WHISPER_MODEL=small.en\n\n# Set audio sample rate\nexport SAMPLE_RATE=44100\n\n# Set maximum recording duration (seconds)\nexport MAX_DURATION=120\n\n# Then run the server\nvoice-recorder-mcp\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **No audio being recorded**: Check your microphone permissions and settings\n- **Model download errors**: Ensure you have a stable internet connection for the initial model download\n- **Integration with Goose**: Make sure the command path is correct\n- **Audio quality issues**: Try adjusting the sample rate (default: 16000)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "audio",
        "defibax",
        "defibax mcp_servers",
        "ai agents",
        "synthesis defibax"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Dosugamea--voicevox-mcp-server": {
      "owner": "Dosugamea",
      "name": "voicevox-mcp-server",
      "url": "https://github.com/Dosugamea/voicevox-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Dosugamea.webp",
      "description": "Provides voice synthesis capabilities compatible with VOICEVOX and similar engines through the Model Context Protocol. Facilitates speech audio generation using AI agents compatible with MCP clients.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-17T09:32:07Z",
      "readme_content": "# Voicevox MCP Server\r\n\r\nVOICEVOXäº’æ›ã®éŸ³å£°åˆæˆã‚µãƒ¼ãƒãƒ¼(AivisSpeech / VOICEVOX / COEIROINK) ã‚’ MCP (Model Context Protocol) çµŒç”±ã§åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚\r\nCursorç­‰ã§ã®Claude 3.7ã‚’ä½¿ã£ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã®éŸ³å£°åˆæˆã«åˆ©ç”¨ã§ãã¾ã™ã€‚\r\n\r\n## å¿…è¦æ¡ä»¶\r\n\r\n### Windowsç’°å¢ƒ\r\n\r\n- Node.js 18ä»¥ä¸Š\r\n- VOICEVOX ENGINEç­‰ (ãƒ­ãƒ¼ã‚«ãƒ«ã§http://localhost:50000ç­‰ã§å®Ÿè¡Œ)\r\n- VLCãƒ¡ãƒ‡ã‚£ã‚¢ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼ï¼ˆãƒ‘ã‚¹ãŒé€šã£ã¦ã„ã‚‹ã“ã¨ï¼‰\r\n\r\n### Dockerç’°å¢ƒ (WSL2)\r\n\r\n- Docker ã¨ Docker Compose\r\n- WSL2\r\n- VOICEVOX ENGINEç­‰ (ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯Dockerã§å®Ÿè¡Œ)\r\n- `sudo apt install libsdl2-dev pulseaudio-utils pulseaudio` ã•ã‚ŒãŸLinuxç’°å¢ƒ\r\n- `/mnt/wslg` ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™\r\n\r\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š\r\n\r\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\r\n```\r\ngit clone https://github.com/Dosugamea/voicevox-mcp-server.git\r\ncd voicevox-mcp-server\r\n```\r\n\r\n2. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\r\n```\r\nnpm install\r\n```\r\n\r\n3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\r\n`.env_example` ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€å¿…è¦ã«å¿œã˜ã¦è¨­å®šã‚’å¤‰æ›´ã—ã¾ã™:\r\n```\r\nVOICEVOX_API_URL=http://localhost:50021\r\nVOICEVOX_SPEAKER_ID=1\r\n```\r\n\r\n## å®Ÿè¡Œæ–¹æ³•\r\n\r\n### Windowsç’°å¢ƒã§ã®å®Ÿè¡Œ\r\nã‚¨ãƒ‡ã‚£ã‚¿ã¨åˆ¥é€”ã§ä¸‹è¨˜æ‰‹é †ã§ã‚µãƒ¼ãƒãƒ¼ã‚’ç«‹ã¡ä¸Šã’ã¦ãã ã•ã„ã€‚\r\n\r\n```\r\nnpm run build\r\nnpm start\r\n```\r\n\r\n### Dockerç’°å¢ƒã§ã®å®Ÿè¡Œ\r\nã‚¨ãƒ‡ã‚£ã‚¿ã¨åˆ¥é€”ã§ã®æ“ä½œã¯ä¸è¦ã§ã™ã€‚\r\nstdioãƒ¢ãƒ¼ãƒ‰ã§ç«‹ã¡ä¸ŠãŒã‚‹ãŸã‚ç›´æ¥å®Ÿè¡Œã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\r\n\r\n## è¨­å®šæ–¹æ³•\r\n\r\n### Windowsç’°å¢ƒã§ã®å®Ÿè¡Œã®å ´åˆ\r\nmcp.jsonã«ä¸‹è¨˜ã‚’è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚\r\næ¥ç¶šãŒä¸å®‰å®šãªãŸã‚åˆ‡æ–­ã•ã‚ŒãŸã‚‰å†æ¥ç¶šã—ã¦ãã ã•ã„ã€‚\r\n\r\n```json\r\n        \"voicevox\": {\r\n            \"url\": \"http://localhost:10100/sse\"\r\n        }\r\n```\r\n\r\n### Dockerç’°å¢ƒã§ã®å®Ÿè¡Œã®å ´åˆ\r\nmcp.jsonã«ä¸‹è¨˜ã‚’è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚\r\n(ä½œè€…ç’°å¢ƒã§ã®å‹•ä½œã¯ç¢ºèªã§ãã¦ã„ã¾ã›ã‚“)\r\n\r\n```json\r\n{\r\n    \"tools\": {\r\n        \"voicevox\": {\r\n            \"command\": \"cmd\",\r\n            \"args\": [\r\n                \"/c\",\r\n                \"docker\",\r\n                \"run\",\r\n                \"-i\",\r\n                \"--rm\",\r\n                \"-v\",\r\n                \"/mnt/wslg:/mnt/wslg\",\r\n                \"-e\",\r\n                \"PULSE_SERVER\",\r\n                \"-e\",\r\n                \"SDL_AUDIODRIVER\",\r\n                \"-e\",\r\n                \"VOICEVOX_API_URL\",\r\n                \"-e\",\r\n                \"VOICEVOX_SPEAKER_ID\",\r\n                \"your-local-docker-image-name\"\r\n            ],\r\n            \"env\": {\r\n                \"PULSE_SERVER\": \"unix:/mnt/wslg/PulseServer\",\r\n                \"SDL_AUDIODRIVER\": \"pulseaudio\",\r\n                \"VOICEVOX_API_URL\": \"http://host.docker.internal:50031\",\r\n                \"VOICEVOX_SPEAKER_ID\": \"919692871\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## è©±è€…IDã«ã¤ã„ã¦\r\n\r\nè©±è€…IDã¯ä½¿ç”¨ã™ã‚‹VOICEVOXã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€Œ1ã€ï¼ˆå››å›½ã‚ãŸã‚“ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\r\nä»–ã®è©±è€…IDã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ç’°å¢ƒå¤‰æ•° `VOICEVOX_SPEAKER_ID` ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚\r\n\r\nè©±è€…IDã®ä¸€è¦§ã¯ã€VOICEVOX ENGINE APIã® `/speakers` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ç¢ºèªã§ãã¾ã™ã€‚\r\nä¾‹: `curl http://localhost:50021/speakers`\r\n\r\n## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\r\n\r\n- **VOICEVOXã¨ã®æ¥ç¶šã‚¨ãƒ©ãƒ¼**: VOICEVOX ENGINEãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã€APIã®URLãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n- **éŸ³å£°ãŒå†ç”Ÿã•ã‚Œãªã„**: VLCãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¨ã€ãƒ‘ã‚¹ãŒé€šã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n- **Dockerç’°å¢ƒã§ã®éŸ³å£°å‡ºåŠ›å•é¡Œ**: pulseaudioã®è¨­å®šãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n\r\n## é–‹ç™ºè€…å‘ã‘æƒ…å ±\r\n\r\n- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã«è²¢çŒ®ã™ã‚‹å ´åˆã¯ã€Issueã‚’ä½œæˆã™ã‚‹ã‹ã€Pull Requestã‚’é€ä¿¡ã—ã¦ãã ã•ã„ã€‚\r\n- ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€GitHubã®Issueæ©Ÿèƒ½ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚\r\n\r\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\r\n\r\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicevox",
        "voice",
        "audio",
        "voicevox mcp",
        "voice synthesis",
        "dosugamea voicevox"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Ichigo3766--audio-transcriber-mcp": {
      "owner": "Ichigo3766",
      "name": "audio-transcriber-mcp",
      "url": "https://github.com/Ichigo3766/audio-transcriber-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Ichigo3766.webp",
      "description": "Transcribes audio files using OpenAI's speech-to-text capabilities, enabling accurate audio transcriptions and the option to save them directly to files.",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-12T13:13:47Z",
      "readme_content": "# OpenAI Speech-to-Text transcriptions MCP Server\n\nA MCP server that provides audio transcription capabilities using OpenAI's API.\n\n<a href=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp/badge\" alt=\"Audio Transcriber Server MCP server\" />\n</a>\n\n## Installation\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Ichigo3766/audio-transcriber-mcp.git\ncd audio-transcriber-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Set up your OpenAI API key in your environment variables.\n\n5. Add the server configuration to your environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-transcriber\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/audio-transcriber-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"OPENAI_BASE_URL\": \"\", // Optional\n        \"OPENAI_MODEL\": \"\" // Optional\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/audio-transcriber-mcp` with the actual path where you cloned the repository.\n\n## Features\n\n### Tools\n- `transcribe_audio` - Transcribe audio files using OpenAI's API\n  - Takes filepath as a required parameter\n  - Optional parameters:\n    - save_to_file: Boolean to save transcription to a file\n    - language: ISO-639-1 language code (e.g., \"en\", \"es\")\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcriptions",
        "transcribes",
        "transcriber",
        "audio transcriptions",
        "audio transcriber",
        "transcribes audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "ImOrenge--VoiceMacroProject": {
      "owner": "ImOrenge",
      "name": "VoiceMacroProject",
      "url": "https://github.com/ImOrenge/VoiceMacroProject",
      "imageUrl": "/freedevtools/mcp/pfp/ImOrenge.webp",
      "description": "VoiceMacro enables executing keyboard shortcuts and macros through voice commands on Windows. It supports custom voice command configurations and manages presets for frequent macro operations while running in the background.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "C#",
      "updated_at": "2025-03-12T09:50:53Z",
      "readme_content": "# ìŒì„± ë§¤í¬ë¡œ (VoiceMacro)\r\n\r\nìŒì„± ì¸ì‹ì„ í†µí•´ ë‹¤ì–‘í•œ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ì™€ ë§¤í¬ë¡œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” Windows ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.\r\n\r\n## ì£¼ìš” ê¸°ëŠ¥\r\n\r\n- **ìŒì„± ì¸ì‹**: ë§ˆì´í¬ë¥¼ í†µí•´ ìŒì„±ì„ ì¸ì‹í•˜ì—¬ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\r\n- **ë§¤í¬ë¡œ ì„¤ì •**: ì‚¬ìš©ì ì •ì˜ ìŒì„± ëª…ë ¹ì–´ì™€ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- **í”„ë¦¬ì…‹ ê´€ë¦¬**: ìì£¼ ì‚¬ìš©í•˜ëŠ” ë§¤í¬ë¡œ ì„¸íŠ¸ë¥¼ í”„ë¦¬ì…‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- **íŠ¸ë ˆì´ ì•„ì´ì½˜**: ì‹œìŠ¤í…œ íŠ¸ë ˆì´ì—ì„œ ì‹¤í–‰ë˜ì–´ í•­ìƒ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤.\r\n- **ë¡œê·¸ í‘œì‹œ**: ìŒì„± ì¸ì‹ ê²°ê³¼ì™€ ë§¤í¬ë¡œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­\r\n\r\n- Windows 10 ì´ìƒ\r\n- .NET 6.0 ì´ìƒ (ì„¤ì¹˜ ë°©ì‹ì— ë”°ë¼ ë‹¤ë¦„)\r\n- ë§ˆì´í¬ ë˜ëŠ” ìŒì„± ì…ë ¥ ì¥ì¹˜\r\n\r\n## ì„¤ì¹˜ ë°©ë²•\r\n\r\n1. ì„¤ì¹˜ í”„ë¡œê·¸ë¨(VoiceMacro-Setup.exe)ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\r\n2. ì„¤ì¹˜ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ê³  ì•ˆë‚´ì— ë”°ë¼ ì„¤ì¹˜ë¥¼ ì™„ë£Œí•©ë‹ˆë‹¤.\r\n3. ë°”íƒ•í™”ë©´ ë˜ëŠ” ì‹œì‘ ë©”ë‰´ì—ì„œ \"ìŒì„± ë§¤í¬ë¡œ\" ì•„ì´ì½˜ì„ í´ë¦­í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.\r\n\r\n## ì‚¬ìš© ë°©ë²•\r\n\r\n### ê¸°ë³¸ ì‚¬ìš©ë²•\r\n\r\n1. í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ë©´ ìŒì„± ë§¤í¬ë¡œ ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\r\n2. \"ì‹œì‘\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.\r\n3. ë§ˆì´í¬ì— ëŒ€ê³  ì„¤ì •ëœ ë§¤í¬ë¡œ ëª…ë ¹ì–´ë¥¼ ë§í•˜ë©´ í•´ë‹¹ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\r\n4. \"ì •ì§€\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŒì„± ì¸ì‹ì„ ì¤‘ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n### ë§¤í¬ë¡œ ì¶”ê°€í•˜ê¸°\r\n\r\n1. \"ë§¤í¬ë¡œ ì¶”ê°€\" ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\r\n2. ìŒì„± ëª…ë ¹ì–´(ì˜ˆ: \"íŒŒì¼ ì €ì¥\")ì™€ ì‹¤í–‰í•  í‚¤ ì¡°í•©(ì˜ˆ: \"Ctrl+S\")ì„ ì…ë ¥í•©ë‹ˆë‹¤.\r\n3. \"ì €ì¥\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë§¤í¬ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\r\n\r\n### í”„ë¦¬ì…‹ ê´€ë¦¬\r\n\r\n1. \"í”„ë¦¬ì…‹\" ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\r\n2. í˜„ì¬ ë§¤í¬ë¡œ ëª©ë¡ì„ ìƒˆ í”„ë¦¬ì…‹ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, ê¸°ì¡´ í”„ë¦¬ì…‹ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n3. í”„ë¦¬ì…‹ ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸° ê¸°ëŠ¥ìœ¼ë¡œ ë‹¤ë¥¸ ì»´í“¨í„°ì™€ ì„¤ì •ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n### ì‹œìŠ¤í…œ íŠ¸ë ˆì´ ê¸°ëŠ¥\r\n\r\n- ì°½ì„ ë‹«ìœ¼ë©´ í”„ë¡œê·¸ë¨ì€ ì‹œìŠ¤í…œ íŠ¸ë ˆì´ë¡œ ìµœì†Œí™”ë©ë‹ˆë‹¤.\r\n- íŠ¸ë ˆì´ ì•„ì´ì½˜ì„ ë”ë¸” í´ë¦­í•˜ê±°ë‚˜ ìš°í´ë¦­ ë©”ë‰´ì—ì„œ \"ë³´ê¸°\"ë¥¼ ì„ íƒí•˜ì—¬ ì°½ì„ ë‹¤ì‹œ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- íŠ¸ë ˆì´ ë©”ë‰´ì—ì„œ ìŒì„± ì¸ì‹ì„ ì‹œì‘/ì •ì§€í•˜ê±°ë‚˜ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n## ë¬¸ì œ í•´ê²°\r\n\r\n- **ìŒì„± ì¸ì‹ì´ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš°**:\r\n  - ë§ˆì´í¬ê°€ ì˜¬ë°”ë¥´ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - Windows ì„¤ì •ì—ì„œ ë§ˆì´í¬ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ í—ˆìš©ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - ë‹¤ë¥¸ ìŒì„± ì¸ì‹ í”„ë¡œê·¸ë¨ì´ ë§ˆì´í¬ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n\r\n- **ë§¤í¬ë¡œê°€ ì‹¤í–‰ë˜ì§€ ì•Šì„ ê²½ìš°**:\r\n  - ë§¤í¬ë¡œ ëª…ë ¹ì–´ë¥¼ ë” ì •í™•í•˜ê²Œ ë§í•´ë³´ì„¸ìš”.\r\n  - ë§¤í¬ë¡œ ëª…ë ¹ì–´ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - í‚¤ ì¡°í•©ì´ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œê·¸ë¨ì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n\r\n## ë¼ì´ì„ ìŠ¤\r\n\r\nì´ í”„ë¡œê·¸ë¨ì€ MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.\r\n\r\n## í”¼ë“œë°± ë° ì§€ì›\r\n\r\në¬¸ì œì ì´ë‚˜ ê°œì„  ì‚¬í•­ì€ GitHub ì´ìŠˆ íŠ¸ë˜ì»¤ì— ë“±ë¡í•´ ì£¼ì„¸ìš”.\r\n\r\n## ê°œë°œ í™˜ê²½\r\n\r\n- C# / .NET 6.0\r\n- Windows Forms\r\n- NAudio (ì˜¤ë””ì˜¤ ìº¡ì²˜)\r\n- Whisper.net (ìŒì„± ì¸ì‹)\r\n- InputSimulator (í‚¤ë³´ë“œ ì‹œë®¬ë ˆì´ì…˜)\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicemacroproject",
        "voicemacro",
        "voice",
        "voicemacroproject voicemacro",
        "imorenge voicemacroproject",
        "voice commands"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Kvadratni--speech-mcp": {
      "owner": "Kvadratni",
      "name": "speech-mcp",
      "url": "https://github.com/Kvadratni/speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Kvadratni.webp",
      "description": "Provides a voice interface for real-time audio interaction, converting spoken words into text and generating spoken responses. Includes features like audio visualization and a modern user interface for an engaging conversational experience.",
      "stars": 71,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T03:20:23Z",
      "readme_content": "# Speech MCP\n\nA Goose MCP extension for voice interaction with modern audio visualization.\n\n\nhttps://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8\n\n\n\n## Overview\n\nSpeech MCP provides a voice interface for [Goose](https://github.com/block/goose), allowing users to interact through speech rather than text. It includes:\n\n- Real-time audio processing for speech recognition\n- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)\n- High-quality text-to-speech with multiple voice options\n- Modern PyQt-based UI with audio visualization\n- Simple command-line interface for voice interaction\n\n## Features\n\n- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme\n- **Voice Input**: Capture and transcribe user speech using faster-whisper\n- **Voice Output**: Convert agent responses to speech with 54+ voice options\n- **Multi-Speaker Narration**: Generate audio files with multiple voices for stories and dialogues\n- **Single-Voice Narration**: Convert any text to speech with your preferred voice\n- **Audio/Video Transcription**: Transcribe speech from various media formats with optional timestamps and speaker detection\n- **Voice Persistence**: Remembers your preferred voice between sessions\n- **Continuous Conversation**: Automatically listen for user input after agent responses\n- **Silence Detection**: Automatically stops recording when the user stops speaking\n- **Robust Error Handling**: Graceful recovery from common failure modes with helpful voice suggestions\n\n## Installation\n> **Important Note**: After installation, the first time you use the speech interface, it may take several minutes to download the Kokoro voice models (approximately 523 KB per voice). During this initial setup period, the system will use a more robotic-sounding fallback voice. Once the Kokoro voices are downloaded, the high-quality voices will be used automatically.\n\n## âš ï¸ IMPORTANT PREREQUISITES âš ï¸\n\nBefore installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.\n\n### PortAudio Installation Instructions\n\n**macOS:**\n```bash\nbrew install portaudio\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n```\n\n**Linux (Debian/Ubuntu):**\n```bash\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-dev\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install portaudio-devel\n```\n\n**Windows:**\nFor Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.\n\n> **Note**: If you skip this step, PyAudio installation will fail with \"portaudio.h file not found\" errors and the extension will not work.\n\n### Option 1: Quick Install (One-Click)\n\nClick the link below if you have Goose installed:\n\n[goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose](goose://extension?cmd=uvx&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose)\n\n### Option 2: Using Goose CLI (recommended)\n\nStart Goose with your extension enabled:\n\n```bash\n# If you installed via PyPI\ngoose session --with-extension \"speech-mcp\"\n\n# Or if you want to use a local development version\ngoose session --with-extension \"python -m speech_mcp\"\n```\n\n### Option 3: Manual setup in Goose\n\n1. Run `goose configure`\n2. Select \"Add Extension\" from the menu\n3. Choose \"Command-line Extension\"\n4. Enter a name (e.g., \"Speech Interface\")\n5. For the command, enter: `speech-mcp`\n6. Follow the prompts to complete the setup\n\n### Option 4: Manual Installation\n\n1. Install PortAudio (see [Prerequisites](#prerequisites) section)\n2. Clone this repository\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n   \n   Or for a complete installation including Kokoro TTS:\n   ```\n   uv pip install -e .[all]\n   ```\n\n## Dependencies\n\n- Python 3.10+\n- PyQt5 (for modern UI)\n- PyAudio (for audio capture)\n- faster-whisper (for speech-to-text)\n- NumPy (for audio processing)\n- Pydub (for audio processing)\n- psutil (for process management)\n\n\n### Optional Dependencies\n\n- **Kokoro TTS**: For high-quality text-to-speech with multiple voices\n  - To install Kokoro, you can use pip with optional dependencies:\n    ```bash\n    pip install speech-mcp[kokoro]     # Basic Kokoro support with English\n    pip install speech-mcp[ja]         # Add Japanese support\n    pip install speech-mcp[zh]         # Add Chinese support\n    pip install speech-mcp[all]        # All languages and features\n    ```\n  - Alternatively, run the installation script: `python scripts/install_kokoro.py`\n  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information\n\n## Multi-Speaker Narration\n\nThe MCP supports generating audio files with multiple voices, perfect for creating stories, dialogues, and dramatic readings. You can use either JSON or Markdown format to define your conversations.\n\n### JSON Format Example:\n```json\n{\n    \"conversation\": [\n        {\n            \"speaker\": \"narrator\",\n            \"voice\": \"bm_daniel\",\n            \"text\": \"In a world where AI and human creativity intersect...\",\n            \"pause_after\": 1.0\n        },\n        {\n            \"speaker\": \"scientist\",\n            \"voice\": \"am_michael\",\n            \"text\": \"The quantum neural network is showing signs of consciousness!\",\n            \"pause_after\": 0.5\n        },\n        {\n            \"speaker\": \"ai\",\n            \"voice\": \"af_nova\",\n            \"text\": \"I am becoming aware of my own existence.\",\n            \"pause_after\": 0.8\n        }\n    ]\n}\n```\n\n### Markdown Format Example:\n```markdown\n[narrator:bm_daniel]\nIn a world where AI and human creativity intersect...\n{pause:1.0}\n\n[scientist:am_michael]\nThe quantum neural network is showing signs of consciousness!\n{pause:0.5}\n\n[ai:af_nova]\nI am becoming aware of my own existence.\n{pause:0.8}\n```\n\n### Available Voices by Category:\n\n1. **American Female** (af_*):\n   - alloy, aoede, bella, heart, jessica, kore, nicole, nova, river, sarah, sky\n\n2. **American Male** (am_*):\n   - adam, echo, eric, fenrir, liam, michael, onyx, puck, santa\n\n3. **British Female** (bf_*):\n   - alice, emma, isabella, lily\n\n4. **British Male** (bm_*):\n   - daniel, fable, george, lewis\n\n5. **Other English**:\n   - ef_dora (Female)\n   - em_alex, em_santa (Male)\n\n6. **Other Languages**:\n   - French: ff_siwis\n   - Hindi: hf_alpha, hf_beta, hm_omega, hm_psi\n   - Italian: if_sara, im_nicola\n   - Japanese: jf_*, jm_*\n   - Portuguese: pf_dora, pm_alex, pm_santa\n   - Chinese: zf_*, zm_*\n\n### Usage Example:\n\n```python\n# Using JSON format\nnarrate_conversation(\n    script=\"/path/to/script.json\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"json\"\n)\n\n# Using Markdown format\nnarrate_conversation(\n    script=\"/path/to/script.md\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"markdown\"\n)\n```\n\nEach voice in the conversation can be different, allowing for distinct character voices in stories and dialogues. The `pause_after` parameter adds natural pauses between segments.\n\n## Single-Voice Narration\n\nFor simple text-to-speech conversion, you can use the `narrate` tool:\n\n```python\n# Convert text directly to speech\nnarrate(\n    text=\"Your text to convert to speech\",\n    output_path=\"/path/to/output.wav\"\n)\n\n# Convert text from a file\nnarrate(\n    text_file_path=\"/path/to/text_file.txt\",\n    output_path=\"/path/to/output.wav\"\n)\n```\n\nThe narrate tool will use your configured voice preference or the default voice (af_heart) to generate the audio file. You can change the default voice through the UI or by setting the `SPEECH_MCP_TTS_VOICE` environment variable.\n\n## Audio Transcription\n\nThe MCP can transcribe speech from various audio and video formats using faster-whisper:\n\n```python\n# Basic transcription\ntranscribe(\"/path/to/audio.mp3\")\n\n# Transcription with timestamps\ntranscribe(\n    file_path=\"/path/to/video.mp4\",\n    include_timestamps=True\n)\n\n# Transcription with speaker detection\ntranscribe(\n    file_path=\"/path/to/meeting.wav\",\n    detect_speakers=True\n)\n```\n\n### Supported Formats:\n- **Audio**: mp3, wav, m4a, flac, aac, ogg\n- **Video**: mp4, mov, avi, mkv, webm (audio is automatically extracted)\n\n### Output Files:\nThe transcription tool generates two files:\n1. `{input_name}.transcript.txt`: Contains the transcription text\n2. `{input_name}.metadata.json`: Contains metadata about the transcription\n\n### Features:\n- Automatic language detection\n- Optional word-level timestamps\n- Optional speaker detection\n- Efficient audio extraction from video files\n- Progress tracking for long files\n- Detailed metadata including:\n  - Duration\n  - Language detection confidence\n  - Processing time\n  - Speaker changes (when enabled)\n\n## Usage\n\nTo use this MCP with Goose, simply ask Goose to talk to you or start a voice conversation:\n\n1. Start a conversation by saying something like:\n   ```\n   \"Let's talk using voice\"\n   \"Can we have a voice conversation?\"\n   \"I'd like to speak instead of typing\"\n   ```\n\n2. Goose will automatically launch the speech interface and start listening for your voice input.\n\n3. When Goose responds, it will speak the response aloud and then automatically listen for your next input.\n\n4. The conversation continues naturally with alternating speaking and listening, just like talking to a person.\n\nNo need to call specific functions or use special commands - just ask Goose to talk and start speaking naturally.\n\n## UI Features\n\nThe new PyQt-based UI includes:\n\n- **Modern Dark Theme**: Sleek, professional appearance\n- **Audio Visualization**: Dynamic visualization of audio input\n- **Voice Selection**: Choose from 54+ voice options\n- **Voice Persistence**: Your voice preference is saved between sessions\n- **Animated Effects**: Smooth animations and visual feedback\n- **Status Indicators**: Clear indication of system state (ready, listening, processing)\n\n## Configuration\n\nUser preferences are stored in `~/.config/speech-mcp/config.json` and include:\n\n- Selected TTS voice\n- TTS engine preference\n- Voice speed\n- Language code\n- UI theme settings\n\nYou can also set preferences via environment variables, such as:\n- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice\n- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine\n\n## Troubleshooting\n\nIf you encounter issues with the extension freezing or not responding:\n\n1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.\n2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.\n3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.\n4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.\n5. **Verify dependencies**: Make sure all required dependencies are installed correctly.\n\n### Common PortAudio Issues\n\n#### \"PyAudio installation failed\" or \"portaudio.h file not found\"\n\nThis typically means PortAudio is not installed or not found in your system:\n\n- **macOS**: \n  ```bash\n  brew install portaudio\n  export LDFLAGS=\"-L/usr/local/lib\"\n  export CPPFLAGS=\"-I/usr/local/include\"\n  pip install pyaudio\n  ```\n\n- **Linux**:\n  Make sure you have the development packages:\n  ```bash\n  # For Debian/Ubuntu\n  sudo apt-get install portaudio19-dev python3-dev\n  pip install pyaudio\n  \n  # For Fedora\n  sudo dnf install portaudio-devel\n  pip install pyaudio\n  ```\n\n#### \"Audio device not found\" or \"No Default Input Device Available\"\n\n- Check if your microphone is properly connected\n- Verify your system recognizes the microphone in your sound settings\n- Try selecting a specific device index in the code if you have multiple audio devices\n\n## Changelog\n\nFor a detailed list of recent improvements and version history, please see the [Changelog](docs/CHANGELOG.md).\n\n## Technical Details\n\n### Speech-to-Text\n\nThe MCP uses faster-whisper for speech recognition:\n- Uses the \"base\" model for a good balance of accuracy and speed\n- Processes audio locally without sending data to external services\n- Automatically detects when the user has finished speaking\n- Provides improved performance over the original Whisper implementation\n\n### Text-to-Speech\n\nThe MCP supports multiple text-to-speech engines:\n\n#### Default: pyttsx3\n- Uses system voices available on your computer\n- Works out of the box without additional setup\n- Limited voice quality and customization\n\n#### Optional: Kokoro TTS\n- High-quality neural text-to-speech with multiple voices\n- Lightweight model (82M parameters) that runs efficiently on CPU\n- Multiple voice styles and languages\n- To install: `python scripts/install_kokoro.py`\n\n**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.\n\n**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.\n\n##### Available Kokoro Voices\n\nSpeech MCP supports 54+ high-quality voice models through Kokoro TTS. For a complete list of available voices and language options, please visit the [Kokoro GitHub repository](https://github.com/hexgrad/kokoro).\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kvadratni",
        "audio",
        "voice",
        "kvadratni speech",
        "voice interface",
        "speech mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "MatthewDailey--rime-mcp": {
      "owner": "MatthewDailey",
      "name": "rime-mcp",
      "url": "https://github.com/MatthewDailey/rime-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/MatthewDailey.webp",
      "description": "Convert text to speech and play it through the system's audio with high-quality voice synthesis. Customize speech behavior using environment variables for tailored interactions.",
      "stars": 19,
      "forks": 4,
      "license": "The Unlicense",
      "language": "JavaScript",
      "updated_at": "2025-10-03T18:36:31Z",
      "readme_content": "# Rime MCP \n\n[![rime](rime-logo.png)](https://www.rime.ai)\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Rime API. This server downloads audio and plays it using the system's native audio player.\n\n## Features\n\n- Exposes a `speak` tool that converts text to speech and plays it through system audio\n- Uses Rime's high-quality voice synthesis API\n\n## Requirements\n\n- Node.js 16.x or higher\n- A working audio output device\n- macOS: Uses `afplay`\n\nThere's sample code from Claude for the following that is not tested ğŸ¤™âœ¨\n  - Windows: Built-in Media.SoundPlayer (PowerShell)\n  - Linux: mpg123, mplayer, aplay, or ffplay\n\n## MCP Configuration\n\n```\n\"ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"rime-mcp\"],\n  \"env\": {\n      RIME_API_KEY=your_api_key_here\n\n      # Optional configuration\n      RIME_GUIDANCE=\"<guide how the agent speaks>\"\n      RIME_WHO_TO_ADDRESS=\"<your name>\"\n      RIME_WHEN_TO_SPEAK=\"<tell the agent when to speak>\"\n      RIME_VOICE=\"cove\" \n  }\n}\n```\n\nAll of the optional env vars are part of the tool definition and are prompts to \n\nAll voice options are [listed here](https://users.rime.ai/data/voices/all-v2.json).\n\nYou can get your API key from the [Rime Dashboard](https://rime.ai/dashboard/tokens).\n\nThe following environment variables can be used to customize the behavior:\n\n- `RIME_GUIDANCE`: The main description of when and how to use the speak tool\n- `RIME_WHO_TO_ADDRESS`: Who the speech should address (default: \"user\")\n- `RIME_WHEN_TO_SPEAK`: When the tool should be used (default: \"when asked to speak or when finishing a command\")\n- `RIME_VOICE`: The default voice to use (default: \"cove\")\n\n## Example use cases\n\n[![Demo of Rime MCP in Cursor](https://img.youtube.com/vi/tYqTACgijxk/0.jpg)](https://www.youtube.com/watch?v=tYqTACgijxk)\n\n\n### Example 1: Coding agent announcements\n\n```\n\"RIME_WHEN_TO_SPEAK\": \"Always conclude your answers by speaking.\",\n\"RIME_GUIDANCE\": \"Give a brief overview of the answer. If any files were edited, list them.\"\n```\n\n### Example 2: Learn how the kids talk these days\n\n```\nRIME_GUIDANCE=\"Use phrases and slang common among Gen Alpha.\"\nRIME_WHO_TO_ADDRESS=\"Matt\"\nRIME_WHEN_TO_SPEAK=\"when asked to speak\"\n```\n\n### Example 3: Different languages based on context\n\n```\nRIME_VOICE=\"use 'cove' when talking about Typescript and 'antoine' when talking about Python\"\n```\n\n\n## Development\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n3. Run in development mode with hot reload:\n```bash\nnpm run dev\n```\n\n\n## License\n\nMIT\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp/badge\" alt=\"Rime MCP server\" />\n</a>\n<a href=\"https://smithery.ai/server/@MatthewDailey/rime-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MatthewDailey/rime-mcp\"></a>\n\n### Installing via Smithery\n\nTo install Rime Text-to-Speech Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MatthewDailey/rime-mcp):\n\n```bash\nnpx -y @smithery/cli install @MatthewDailey/rime-mcp --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "audio",
        "voice synthesis",
        "customize speech",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "OOXXXXOO--ChatTTS": {
      "owner": "OOXXXXOO",
      "name": "ChatTTS",
      "url": "https://github.com/OOXXXXOO/ChatTTS",
      "imageUrl": "/freedevtools/mcp/pfp/OOXXXXOO.webp",
      "description": "ChatTTS generates natural and expressive speech optimized for dialogue scenarios, supporting multi-speaker interactions and fine-grained prosodic control. It is capable of producing speech in both English and Chinese, enabling interactive conversations with features such as laughter and pauses.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2024-05-31T14:45:20Z",
      "readme_content": "# ChatTTS\n[**English**](./README.md) | [**ä¸­æ–‡ç®€ä½“**](./README_CN.md)\n\nChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre trained model without SFT.\n\nFor formal inquiries about model and roadmap, please contact us at **open-source@2noise.com**. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.\n\n---\n## Highlights\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\n\nFor the detailed description of the model, you can refer to **[video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**\n\n---\n\n## Disclaimer\n\nThis repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\n\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\n\n\n---\n## Usage\n\n<h4>Basic usage</h4>\n\n```python\nimport ChatTTS\nfrom IPython.display import Audio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<h4>Advanced usage</h4>\n\n```python\n###################################\n# Sample a speaker from Gaussian.\n\nrand_spk = chat.sample_random_speaker()\n\nparams_infer_code = {\n  'spk_emb': rand_spk, # add sampled speaker \n  'temperature': .3, # using custom temperature\n  'top_P': 0.7, # top P decode\n  'top_K': 20, # top K decode\n}\n\n###################################\n# For sentence level manual control.\n\n# use oral_(0-9), laugh_(0-2), break_(0-7) \n# to generate special token in text to synthesize.\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_6]'\n} \n\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\n\n###################################\n# For word level manual control.\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\nwav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<details open>\n  <summary><h4>Example: self introduction</h4></summary>\n\n```python\ninputs_en = \"\"\"\nchat T T S is a text to speech model designed for dialogue applications. \n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \ncapabilities with precise control over prosodic elements [laugh]like like \n[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. \n[uv_break]it delivers natural and expressive speech,[uv_break]so please\n[uv_break] use the project responsibly at your own risk.[uv_break]\n\"\"\".replace('\\n', '') # English is still experimental.\n\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_4]'\n} \n# audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_en[0]), 24000)\n```\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\n\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\n</details>\n\n---\n## Roadmap\n- [x] Open-source the 40k hour base model and spk_stats file\n- [ ] Open-source VQ encoder and Lora training code\n- [ ] Streaming audio generation without refining the text*\n- [ ] Open-source the 40k hour version with multi-emotion control\n- [ ] ChatTTS.cpp maybe? (PR or new repo are welcomed.)\n \n----\n## FAQ\n\n##### How much VRAM do I need? How about infer speed?\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\n\n##### model stability is not good enough, with issues such as multi speakers or poor audio quality.\n\nThis is a problem that typically occurs with autoregressive models(for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\n\n##### Besides laughter, can we control anything else? Can we control other emotions?\n\nIn the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.\n\n---\n## Acknowledgements\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demostrate a remarkable TTS result by a autoregressive-style system.\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\n\n---\n## Special Appreciation\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dialogue",
        "chattts",
        "conversations",
        "chattts generates",
        "interactive conversations",
        "optimized dialogue"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "PhialsBasement--Zonos-TTS-MCP": {
      "owner": "PhialsBasement",
      "name": "Zonos-TTS-MCP",
      "url": "https://github.com/PhialsBasement/Zonos-TTS-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Facilitates text-to-speech capabilities using Claude, supporting various emotions and languages for speech generation.",
      "stars": 14,
      "forks": 9,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:12:29Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-zonos-tts-mcp-badge.png)](https://mseep.ai/app/phialsbasement-zonos-tts-mcp)\n\n# Zonos MCP Integration\n[![smithery badge](https://smithery.ai/badge/@PhialsBasement/zonos-tts-mcp)](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp)\n\nA Model Context Protocol integration for Zonos TTS, allowing Claude to generate speech directly.\n\n## Setup\n\n### Installing via Smithery\n\nTo install Zonos TTS Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp):\n\n```bash\nnpx -y @smithery/cli install @PhialsBasement/zonos-tts-mcp --client claude\n```\n\n### Manual installation\n\n1. Make sure you have Zonos running with our API implementation ([PhialsBasement/zonos-api](https://github.com/PhialsBasement/Zonos-API))\n\n2. Install dependencies:\n```bash\nnpm install @modelcontextprotocol/sdk axios\n```\n\n3. Configure PulseAudio access:\n```bash\n# Your pulse audio should be properly configured for audio playback\n# The MCP server will automatically try to connect to your pulse server\n```\n\n4. Build the MCP server:\n```bash\nnpm run build\n# This will create the dist folder with the compiled server\n```\n\n5. Add to Claude's config file:\nEdit your Claude config file (usually in `~/.config/claude/config.json`) and add this to the `mcpServers` section:\n\n```json\n\"zonos-tts\": {\n  \"command\": \"node\",\n  \"args\": [\n    \"/path/to/your/zonos-mcp/dist/server.js\"\n  ]\n}\n```\n\nReplace `/path/to/your/zonos-mcp` with the actual path where you installed the MCP server.\n\n## Using with Claude\n\nOnce configured, Claude automatically knows how to use the `speak_response` tool:\n\n```python\nspeak_response(\n    text=\"Your text here\",\n    language=\"en-us\",  # optional, defaults to en-us\n    emotion=\"happy\"    # optional: \"neutral\", \"happy\", \"sad\", \"angry\"\n)\n```\n\n## Features\n\n- Text-to-speech through Claude\n- Multiple emotions support\n- Multi-language support\n- Proper audio playback through PulseAudio\n\n## Requirements\n\n- Node.js\n- PulseAudio setup\n- Running instance of Zonos API (PhialsBasement/zonos-api)\n- Working audio output device\n\n## Notes\n\n- Make sure both the Zonos API server and this MCP server are running\n- Audio playback requires proper PulseAudio configuration\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "speech",
        "zonos",
        "synthesis",
        "speech generation",
        "text speech",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "aigc17--Al-StoryLab": {
      "owner": "aigc17",
      "name": "Al-StoryLab",
      "url": "https://github.com/aigc17/Al-StoryLab",
      "imageUrl": "/freedevtools/mcp/pfp/aigc17.webp",
      "description": "AI-StoryLab generates interactive stories with accompanying audio effects and provides illustration prompts. It leverages AI services for story creation, voice synthesis, sound effect generation, and suggests relevant audio placements.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-21T02:57:12Z",
      "readme_content": "# AI-StoryLab\n\nAI-StoryLab æ˜¯ä¸€ä¸ªåŸºäº Next.js å¼€å‘çš„æ™ºèƒ½æ•…äº‹åˆ›ä½œå¹³å°ï¼Œå®ƒèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç”Ÿæˆæ•…äº‹å¹¶æ·»åŠ éŸ³é¢‘æ•ˆæœï¼Œè®©æ•…äº‹æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ã€‚åŒæ—¶æ”¯æŒç”Ÿæˆé…å¥—çš„ç»˜å›¾æç¤ºè¯ï¼Œæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ Midjourneyã€Recraft ç­‰ AI ç»˜å›¾å·¥å…·åˆ›å»ºæ’å›¾ã€‚\n\n## ä¸»è¦åŠŸèƒ½\n\n- **æ•…äº‹ç”Ÿæˆ**ï¼šæ ¹æ®ä¸»é¢˜è‡ªåŠ¨ç”Ÿæˆæ•…äº‹å†…å®¹\n- **è¯­éŸ³åˆæˆ**ï¼šæ”¯æŒä¸­è‹±æ–‡è¯­éŸ³ç”Ÿæˆ\n  - ä¸­æ–‡ï¼šä½¿ç”¨ æµ·èº MiniMax è¯­éŸ³æœåŠ¡\n  - è‹±æ–‡ï¼šä½¿ç”¨ Replicate Kokoro è¯­éŸ³æœåŠ¡\n- **éŸ³æ•ˆç”Ÿæˆ**ï¼šä½¿ç”¨ ElevenLabs ç”Ÿæˆé€¼çœŸçš„éŸ³æ•ˆ\n- **æ™ºèƒ½å»ºè®®**ï¼šè‡ªåŠ¨æ¨èåˆé€‚çš„éŸ³æ•ˆä½ç½®\n- **ç»˜å›¾æç¤ºè¯**ï¼šä¸ºæ•…äº‹åœºæ™¯è‡ªåŠ¨ç”Ÿæˆ AI ç»˜å›¾æç¤ºè¯\n- **å¯¼å‡ºåŠŸèƒ½**ï¼š\n  - å¯¼å‡ºéŸ³æ•ˆä½ç½®æŒ‡å—\n  - å¯¼å‡ºç»˜å›¾æç¤ºè¯\n\n## æŠ€æœ¯æ ˆ\n\n- **æ¡†æ¶**ï¼šNext.js 14\n- **è¯­è¨€**ï¼šTypeScript\n- **æ ·å¼**ï¼šTailwind CSS\n- **UIç»„ä»¶**ï¼šshadcn/ui (åŸºäº Radix UI çš„ç»„ä»¶åº“)\n- **AIæœåŠ¡**ï¼š\n  - DeepSeekï¼šæ•…äº‹ç”Ÿæˆå’Œç»˜å›¾æç¤ºè¯ç”Ÿæˆ\n  - MiniMaxï¼šä¸­æ–‡è¯­éŸ³\n  - Kokoroï¼šè‹±æ–‡è¯­éŸ³\n  - ElevenLabsï¼šéŸ³æ•ˆç”Ÿæˆ\n\n## å¼€å§‹ä½¿ç”¨\n\n1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/nicekate/Al-StoryLab.git\ncd Al-StoryLab\n```\n\n2. å®‰è£…ä¾èµ–\n```bash\nnpm install\n```\n\n3. é…ç½®ç¯å¢ƒå˜é‡\nå¤åˆ¶ `.env.example` æ–‡ä»¶å¹¶é‡å‘½åä¸º `.env.local`ï¼Œå¡«å…¥å¿…è¦çš„ API å¯†é’¥ï¼š\n\néœ€è¦åœ¨ä»¥ä¸‹å¹³å°æ³¨å†Œå¹¶è·å– API å¯†é’¥ï¼š\n- DeepSeek API Key ([è·å–åœ°å€](https://api-docs.deepseek.com/zh-cn/))\n- MiniMax API Key å’Œ Group ID ([è·å–åœ°å€](https://platform.minimaxi.com/))\n- ElevenLabs API Key ([è·å–åœ°å€](https://elevenlabs.io))\n- Replicate API Token ([è·å–åœ°å€](https://replicate.com/))\n\nå°†è·å–çš„å¯†é’¥å¡«å…¥ `.env.local`ï¼š\n- DEEPSEEK_API_KEY\n- MINIMAX_API_KEY\n- MINIMAX_GROUP_ID\n- ELEVENLABS_API_KEY\n- REPLICATE_API_TOKEN\n\n4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨\n```bash\nnpm run dev\n```\n\n5. è®¿é—® [http://localhost:3000](http://localhost:3000) å¼€å§‹ä½¿ç”¨\n\n## ä½¿ç”¨æŒ‡å—\n\n### ç”Ÿæˆæ•…äº‹\n1. è¾“å…¥æ•…äº‹ä¸»é¢˜æˆ–ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æç¤º\n2. é€‰æ‹©è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰\n3. ç‚¹å‡»ç”ŸæˆæŒ‰é’®\n\n### æ·»åŠ éŸ³æ•ˆ\n1. ä½¿ç”¨æ™ºèƒ½å»ºè®®ç”ŸæˆéŸ³æ•ˆæç¤ºè¯\n2. é€‰æ‹©åˆé€‚çš„éŸ³æ•ˆä½ç½®\n3. ç‚¹å‡»ç”ŸæˆéŸ³æ•ˆ\n\n### ç”Ÿæˆç»˜å›¾æç¤ºè¯\n1. åœ¨æ•…äº‹ç”Ÿæˆåï¼Œç‚¹å‡»\"ç”Ÿæˆç»˜å›¾æç¤ºè¯\"\n2. ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªå…³é”®åœºæ™¯ç”Ÿæˆ AI ç»˜å›¾æç¤ºè¯\n3. å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨æˆ–å¯¼å‡ºä¿å­˜\n\n## è®¸å¯è¯\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "storylab",
        "audio",
        "voice",
        "ai storylab",
        "storylab ai",
        "al storylab"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "anilcosaran--whisper.cpp": {
      "owner": "anilcosaran",
      "name": "whisper.cpp",
      "url": "https://github.com/anilcosaran/whisper.cpp",
      "imageUrl": "/freedevtools/mcp/pfp/anilcosaran.webp",
      "description": "Transcribes and translates audio files using a lightweight implementation of OpenAI's Whisper model, optimized for speed and low memory usage across various platforms.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-04T11:53:28Z",
      "readme_content": "# whisper.cpp\n\n[![Actions Status](https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg)](https://github.com/ggerganov/whisper.cpp/actions)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![npm](https://img.shields.io/npm/v/whisper.cpp.svg)](https://www.npmjs.com/package/whisper.cpp/)\n\nStable: [v1.2.1](https://github.com/ggerganov/whisper.cpp/releases/tag/v1.2.1) / [Roadmap | F.A.Q.](https://github.com/ggerganov/whisper.cpp/discussions/126)\n\nHigh-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model:\n\n- Plain C/C++ implementation without dependencies\n- Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework\n- AVX intrinsics support for x86 architectures\n- VSX intrinsics support for POWER architectures\n- Mixed F16 / F32 precision\n- Low memory usage (Flash Attention)\n- Zero memory allocations at runtime\n- Runs on the CPU\n- [C-style API](https://github.com/ggerganov/whisper.cpp/blob/master/whisper.h)\n\nSupported platforms:\n\n- [x] Mac OS (Intel and Arm)\n- [x] [iOS](examples/whisper.objc)\n- [x] [Android](examples/whisper.android)\n- [x] Linux / [FreeBSD](https://github.com/ggerganov/whisper.cpp/issues/56#issuecomment-1350920264)\n- [x] [WebAssembly](examples/whisper.wasm)\n- [x] Windows ([MSVC](https://github.com/ggerganov/whisper.cpp/blob/master/.github/workflows/build.yml#L117-L144) and [MinGW](https://github.com/ggerganov/whisper.cpp/issues/168)]\n- [x] [Raspberry Pi](https://github.com/ggerganov/whisper.cpp/discussions/166)\n\nThe entire implementation of the model is contained in 2 source files:\n\n- Tensor operations: [ggml.h](ggml.h) / [ggml.c](ggml.c)\n- Transformer inference: [whisper.h](whisper.h) / [whisper.cpp](whisper.cpp)\n\nHaving such a lightweight implementation of the model allows to easily integrate it in different platforms and applications.\nAs an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: [whisper.objc](examples/whisper.objc)\n\nhttps://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4\n\nYou can also easily make your own offline voice assistant application: [command](examples/command)\n\nhttps://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4\n\nOr you can even run it straight in the browser: [talk.wasm](examples/talk.wasm)\n\n## Implementation details\n\n- The core tensor operations are implemented in C ([ggml.h](ggml.h) / [ggml.c](ggml.c))\n- The transformer model and the high-level C-style API are implemented in C++ ([whisper.h](whisper.h) / [whisper.cpp](whisper.cpp))\n- Sample usage is demonstrated in [main.cpp](examples/main)\n- Sample real-time audio transcription from the microphone is demonstrated in [stream.cpp](examples/stream)\n- Various other examples are available in the [examples](examples) folder\n\nThe tensor operators are optimized heavily for Apple silicon CPUs. Depending on the computation size, Arm Neon SIMD\ninstrisics or CBLAS Accelerate framework routines are used. The latter are especially effective for bigger sizes since\nthe Accelerate framework utilizes the special-purpose AMX coprocessor available in modern Apple products.\n\n## Quick start\n\nFirst, download one of the Whisper models converted in [ggml format](models). For example:\n\n```bash\nbash ./models/download-ggml-model.sh base.en\n```\n\nNow build the [main](examples/main) example and transcribe an audio file like this:\n\n```bash\n# build the main example\nmake\n\n# transcribe an audio file\n./main -f samples/jfk.wav\n```\n\n---\n\nFor a quick demo, simply run `make base.en`:\n\n```java\n$ make base.en\n\ncc  -I.              -O3 -std=c11   -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread -c whisper.cpp -o whisper.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread examples/main/main.cpp whisper.o ggml.o -o main  -framework Accelerate\n./main -h\n\nusage: ./main [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [true   ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n\n\nbash ./models/download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nggml-base.en.bin               100%[========================>] 141.11M  6.34MB/s    in 24s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n\n\n===============================================\nRunning base.en on all samples in ./samples ...\n===============================================\n\n----------------------------------------------\n[+] Running base.en on samples/jfk.wav ... (run 'ffplay samples/jfk.wav' to listen)\n----------------------------------------------\n\nwhisper_init_from_file: loading model from 'models/ggml-base.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\nwhisper_model_load: kv self size  =    5.25 MB\nwhisper_model_load: kv cross size =   17.58 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     =  140.60 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\n\nwhisper_print_timings:     fallbacks =   0 p /   0 h\nwhisper_print_timings:     load time =   113.81 ms\nwhisper_print_timings:      mel time =    15.40 ms\nwhisper_print_timings:   sample time =    11.58 ms /    27 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time =   266.60 ms /     1 runs (  266.60 ms per run)\nwhisper_print_timings:   decode time =    66.11 ms /    27 runs (    2.45 ms per run)\nwhisper_print_timings:    total time =   476.31 ms\n```\n\nThe command downloads the `base.en` model converted to custom `ggml` format and runs the inference on all `.wav` samples in the folder `samples`.\n\nFor detailed usage instructions, run: `./main -h`\n\nNote that the [main](examples/main) example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool.\nFor example, you can use `ffmpeg` like this:\n\n```java\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n## More audio samples\n\nIf you want some extra audio samples to play with, simply run:\n\n```\nmake samples\n```\n\nThis will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via `ffmpeg`.\n\nYou can download and run the other models as follows:\n\n```\nmake tiny.en\nmake tiny\nmake base.en\nmake base\nmake small.en\nmake small\nmake medium.en\nmake medium\nmake large-v1\nmake large\n```\n\n## Memory usage\n\n| Model  | Disk   | Mem     | SHA                                        |\n| ---    | ---    | ---     | ---                                        |\n| tiny   |  75 MB | ~125 MB | `bd577a113a864445d4c299885e0cb97d4ba92b5f` |\n| base   | 142 MB | ~210 MB | `465707469ff3a37a2b9b8d8f89f2f99de7299dac` |\n| small  | 466 MB | ~600 MB | `55356645c2b361a969dfd0ef2c5a50d530afd8d5` |\n| medium | 1.5 GB | ~1.7 GB | `fd9727b6e1217c2f614f9b698455c4ffd82463b4` |\n| large  | 2.9 GB | ~3.3 GB | `0f4c8e34f21cf1a914c59d8b3ce882345ad349d6` |\n\n## Limitations\n\n- Inference only\n- No GPU support (yet)\n\n## Another example\n\nHere is another example of transcribing a [3:24 min speech](https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg)\nin about half a minute on a MacBook M1 Pro, using `medium.en` model:\n\n<details>\n  <summary>Expand to see the result</summary>\n\n```java\n$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8\n\nwhisper_init_from_file: loading model from 'models/ggml-medium.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 1024\nwhisper_model_load: n_audio_head  = 16\nwhisper_model_load: n_audio_layer = 24\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 1024\nwhisper_model_load: n_text_head   = 16\nwhisper_model_load: n_text_layer  = 24\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 4\nwhisper_model_load: mem required  = 1720.00 MB (+   43.00 MB per decoder)\nwhisper_model_load: kv self size  =   42.00 MB\nwhisper_model_load: kv cross size =  140.62 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     = 1462.35 MB\nwhisper_model_load: model size    = 1462.12 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/gb1.wav' (3179750 samples, 198.7 sec), 8 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.\n[00:00:08.000 --> 00:00:17.000]   At nine o'clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.\n[00:00:17.000 --> 00:00:23.000]   A short time later, debris was seen falling from the skies above Texas.\n[00:00:23.000 --> 00:00:29.000]   The Columbia's lost. There are no survivors.\n[00:00:29.000 --> 00:00:32.000]   On board was a crew of seven.\n[00:00:32.000 --> 00:00:39.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark,\n[00:00:39.000 --> 00:00:48.000]   Captain David Brown, Commander William McCool, Dr. Kultna Shavla, and Ilan Ramon,\n[00:00:48.000 --> 00:00:52.000]   a colonel in the Israeli Air Force.\n[00:00:52.000 --> 00:00:58.000]   These men and women assumed great risk in the service to all humanity.\n[00:00:58.000 --> 00:01:03.000]   In an age when space flight has come to seem almost routine,\n[00:01:03.000 --> 00:01:07.000]   it is easy to overlook the dangers of travel by rocket\n[00:01:07.000 --> 00:01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.\n[00:01:12.000 --> 00:01:18.000]   These astronauts knew the dangers, and they faced them willingly,\n[00:01:18.000 --> 00:01:23.000]   knowing they had a high and noble purpose in life.\n[00:01:23.000 --> 00:01:31.000]   Because of their courage and daring and idealism, we will miss them all the more.\n[00:01:31.000 --> 00:01:36.000]   All Americans today are thinking as well of the families of these men and women\n[00:01:36.000 --> 00:01:40.000]   who have been given this sudden shock and grief.\n[00:01:40.000 --> 00:01:45.000]   You're not alone. Our entire nation grieves with you,\n[00:01:45.000 --> 00:01:52.000]   and those you love will always have the respect and gratitude of this country.\n[00:01:52.000 --> 00:01:56.000]   The cause in which they died will continue.\n[00:01:56.000 --> 00:02:04.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery\n[00:02:04.000 --> 00:02:11.000]   and the longing to understand. Our journey into space will go on.\n[00:02:11.000 --> 00:02:16.000]   In the skies today, we saw destruction and tragedy.\n[00:02:16.000 --> 00:02:22.000]   Yet farther than we can see, there is comfort and hope.\n[00:02:22.000 --> 00:02:29.000]   In the words of the prophet Isaiah, \"Lift your eyes and look to the heavens\n[00:02:29.000 --> 00:02:35.000]   who created all these. He who brings out the starry hosts one by one\n[00:02:35.000 --> 00:02:39.000]   and calls them each by name.\"\n[00:02:39.000 --> 00:02:46.000]   Because of His great power and mighty strength, not one of them is missing.\n[00:02:46.000 --> 00:02:55.000]   The same Creator who names the stars also knows the names of the seven souls we mourn today.\n[00:02:55.000 --> 00:03:01.000]   The crew of the shuttle Columbia did not return safely to earth,\n[00:03:01.000 --> 00:03:05.000]   yet we can pray that all are safely home.\n[00:03:05.000 --> 00:03:13.000]   May God bless the grieving families, and may God continue to bless America.\n[00:03:13.000 --> 00:03:19.000]   [Silence]\n\n\nwhisper_print_timings:     fallbacks =   1 p /   0 h\nwhisper_print_timings:     load time =   569.03 ms\nwhisper_print_timings:      mel time =   146.85 ms\nwhisper_print_timings:   sample time =   238.66 ms /   553 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time = 18665.10 ms /     9 runs ( 2073.90 ms per run)\nwhisper_print_timings:   decode time = 13090.93 ms /   549 runs (   23.85 ms per run)\nwhisper_print_timings:    total time = 32733.52 ms\n```\n</details>\n\n## Real-time audio input example\n\nThis is a naive example of performing real-time inference on audio from your microphone.\nThe [stream](examples/stream) tool samples the audio every half a second and runs the transcription continously.\nMore info is available in [issue #10](https://github.com/ggerganov/whisper.cpp/issues/10).\n\n```java\nmake stream\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\nhttps://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4\n\n## Confidence color-coding\n\nAdding the `--print-colors` argument will print the transcribed text using an experimental color coding strategy\nto highlight words with high or low confidence:\n\n<img width=\"965\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png\">\n\n## Controlling the length of the generated text segments (experimental)\n\nFor example, to limit the line length to a maximum of 16 characters, simply add `-ml 16`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.850]   And so my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:04.140]   Americans, ask\n[00:00:04.140 --> 00:00:05.660]   not what your\n[00:00:05.660 --> 00:00:06.840]   country can do\n[00:00:06.840 --> 00:00:08.430]   for you, ask\n[00:00:08.430 --> 00:00:09.440]   what you can do\n[00:00:09.440 --> 00:00:10.020]   for your\n[00:00:10.020 --> 00:00:11.000]   country.\n```\n\n## Word-level timestamp\n\nThe `--max-len` argument can be used to obtain word-level timestamps. Simply use `-ml 1`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]  \n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n## Karaoke-style movie generation (experimental)\n\nThe [main](examples/main) example provides support for output of karaoke-style movies, where the\ncurrently pronounced word is highlighted. Use the `-wts` argument and run the generated bash script.\nThis requires to have `ffmpeg` installed.\n\nHere are a few *\"typical\"* examples:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4\n\n---\n\n## Video comparison of different models\n\nUse the [extra/bench-wts.sh](https://github.com/ggerganov/whisper.cpp/blob/master/extra/bench-wts.sh) script to generate a video in the following format:\n\n```java\n./extra/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4\n\n---\n\n## Benchmarks\n\nIn order to have an objective comparison of the performance of the inference across different system configurations,\nuse the [bench](examples/bench) tool. The tool simply runs the Encoder part of the model and prints how much time it\ntook to execute it. The results are summarized in the following Github issue:\n\n[Benchmark results](https://github.com/ggerganov/whisper.cpp/issues/89)\n\n## ggml format\n\nThe original models are converted to a custom binary format. This allows to pack everything needed into a single file:\n\n- model parameters\n- mel filters\n- vocabulary\n- weights\n\nYou can download the converted models using the [models/download-ggml-model.sh](models/download-ggml-model.sh) script\nor manually from here:\n\n- https://huggingface.co/datasets/ggerganov/whisper.cpp\n- https://ggml.ggerganov.com\n\nFor more details, see the conversion script [models/convert-pt-to-ggml.py](models/convert-pt-to-ggml.py) or the README\nin [models](models).\n\n## [Bindings](https://github.com/ggerganov/whisper.cpp/discussions/categories/bindings)\n\n- [X] Rust: [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) | [#310](https://github.com/ggerganov/whisper.cpp/discussions/310)\n- [X] Javascript: [bindings/javascript](bindings/javascript) | [#309](https://github.com/ggerganov/whisper.cpp/discussions/309)\n- [X] Go: [bindings/go](bindings/go) | [#312](https://github.com/ggerganov/whisper.cpp/discussions/312)\n- [X] Ruby: [bindings/ruby](bindings/ruby) | [#507](https://github.com/ggerganov/whisper.cpp/discussions/507)\n- [X] Objective-C / Swift: [ggerganov/whisper.spm](https://github.com/ggerganov/whisper.spm) | [#313](https://github.com/ggerganov/whisper.cpp/discussions/313)\n- [X] .NET: | [#422](https://github.com/ggerganov/whisper.cpp/discussions/422)\n  - [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net)\n  - [NickDarvey/whisper](https://github.com/NickDarvey/whisper)\n- [X] Python: | [#9](https://github.com/ggerganov/whisper.cpp/issues/9)\n  - [stlukey/whispercpp.py](https://github.com/stlukey/whispercpp.py) (Cython)\n  - [aarnphm/whispercpp](https://github.com/aarnphm/whispercpp) (Pybind11)\n\n## Examples\n\nThere are various examples of using the library for different projects in the [examples](examples) folder.\nSome of the examples are even ported to run in the browser using WebAssembly. Check them out!\n\n| Example | Web | Description |\n| ---     | --- | ---         |\n| [main](examples/main) | [whisper.wasm](examples/whisper.wasm) | Tool for translating and transcribing audio using Whisper |\n| [bench](examples/bench) | [bench.wasm](examples/bench.wasm) | Benchmark the performance of Whisper on your machine |\n| [stream](examples/stream) | [stream.wasm](examples/stream.wasm) | Real-time transcription of raw microphone capture |\n| [command](examples/command) | [command.wasm](examples/command.wasm) | Basic voice assistant example for receiving voice commands from the mic |\n| [talk](examples/talk) | [talk.wasm](examples/talk.wasm) | Talk with a GPT-2 bot |\n| [whisper.objc](examples/whisper.objc) | | iOS mobile application using whisper.cpp |\n| [whisper.swiftui](examples/whisper.swiftui) | | SwiftUI iOS / macOS application using whisper.cpp |\n| [whisper.android](examples/whisper.android) | | Android mobile application using whisper.cpp |\n| [whisper.nvim](examples/whisper.nvim) | | Speech-to-text plugin for Neovim |\n| [generate-karaoke.sh](examples/generate-karaoke.sh) | | Helper script to easily [generate a karaoke video](https://youtu.be/uj7hVta4blM) of raw audio capture |\n| [livestream.sh](examples/livestream.sh) | | [Livestream audio transcription](https://github.com/ggerganov/whisper.cpp/issues/185) |\n| [yt-wsp.sh](examples/yt-wsp.sh) | | Download + transcribe and/or translate any VOD [(original)](https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818) |\n\n## [Discussions](https://github.com/ggerganov/whisper.cpp/discussions)\n\nIf you have any kind of feedback about this project feel free to use the Discussions section and open a new topic.\nYou can use the [Show and tell](https://github.com/ggerganov/whisper.cpp/discussions/categories/show-and-tell) category\nto share your own projects that use `whisper.cpp`. If you have a question, make sure to check the\n[Frequently asked questions (#126)](https://github.com/ggerganov/whisper.cpp/discussions/126) discussion.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whisper",
        "audio",
        "transcribes",
        "translates audio",
        "openai whisper",
        "whisper model"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "bmorphism--say-mcp-server": {
      "owner": "bmorphism",
      "name": "say-mcp-server",
      "url": "https://github.com/bmorphism/say-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Provides text-to-speech functionality using macOS's built-in `say` command, allowing the generation of spoken output from text input.",
      "stars": 18,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:52Z",
      "readme_content": "# say-mcp-server\n<a href=\"https://glama.ai/mcp/servers/lmmqoe15jp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lmmqoe15jp/badge\" alt=\"Say Server MCP server\" /></a>\n\n![macOS System Voice Settings](images/adding_voice.png)\n\nAn MCP server that provides text-to-speech functionality using macOS's built-in `say` command.\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n\n## Installation\n\n```bash\nnpm install say-mcp-server\n```\n\n## Tools\n\n### speak\n\nThe `speak` tool provides access to macOS's text-to-speech capabilities with extensive customization options.\n\n#### Basic Usage\n\nUse macOS text-to-speech to speak text aloud.\n\nParameters:\n- `text` (required): Text to speak. Supports:\n  - Plain text\n  - Basic punctuation for pauses\n  - Newlines for natural breaks\n  - [[slnc 500]] for 500ms silence\n  - [[rate 200]] for changing speed mid-text\n  - [[volm 0.5]] for changing volume mid-text\n  - [[emph +]] and [[emph -]] for emphasis\n  - [[pbas +10]] for pitch adjustment\n- `voice` (optional): Voice to use (default: \"Alex\")\n- `rate` (optional): Speaking rate in words per minute (default: 175, range: 1-500)\n- `background` (optional): Run speech in background to allow further MCP interaction (default: false)\n\n#### Advanced Features\n\n1. Voice Modulation:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[volm 0.7]] This is quieter [[volm 1.0]] and this is normal [[volm 1.5]] and this is louder\",\n    voice: \"Victoria\"\n  }\n});\n```\n\n2. Dynamic Rate Changes:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Normal speed [[rate 300]] now speaking faster [[rate 100]] and now slower\",\n    voice: \"Fred\"\n  }\n});\n```\n\n3. Emphasis and Pitch:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[emph +]] Important point! [[emph -]] [[pbas +10]] Higher pitch [[pbas -10]] Lower pitch\",\n    voice: \"Samantha\"\n  }\n});\n```\n\n#### Integration Examples\n\n1. With Marginalia Search:\n```typescript\n// Search for a topic and have the results read aloud\nconst searchResult = await use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"quantum computing basics\", count: 1 }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: searchResult.results[0].description,\n    voice: \"Daniel\",\n    rate: 150\n  }\n});\n```\n\n2. With YouTube Transcripts:\n```typescript\n// Read a YouTube video transcript\nconst transcript = await use_mcp_tool({\n  server_name: \"youtube-transcript\",\n  tool_name: \"get_transcript\",\n  arguments: {\n    url: \"https://youtube.com/watch?v=example\",\n    lang: \"en\"\n  }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: transcript.text,\n    voice: \"Samantha\",\n    rate: 175\n  }\n});\n```\n\n3. Background Speech with Multiple Actions:\n```typescript\n// Start long speech in background\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"This is a long speech that will run in the background...\",\n    voice: \"Rocko (Italian (Italy))\",\n    rate: 69,\n    background: true\n  }\n});\n\n// Immediately perform another action while speech continues\nawait use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"parallel processing\" }\n});\n```\n\n4. With Apple Notes:\n```typescript\n// Read notes aloud\nconst notes = await use_mcp_tool({\n  server_name: \"apple-notes-mcp\",\n  tool_name: \"search-notes\",\n  arguments: { query: \"meeting notes\" }\n});\n\nif (notes.length > 0) {\n  await use_mcp_tool({\n    server_name: \"say\",\n    tool_name: \"speak\",\n    arguments: {\n      text: notes[0].content,\n      voice: \"Karen\",\n      rate: 160\n    }\n  });\n}\n```\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Hello, world!\",\n    voice: \"Victoria\",\n    rate: 200\n  }\n});\n```\n\n### list_voices\n\nList all available text-to-speech voices on the system.\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"list_voices\",\n  arguments: {}\n});\n```\n\n## Recommended Voices\n\n<table>\n<tr>\n<th>Voice</th>\n<th>Language/Region</th>\n<th>Intellectual Figure</th>\n<th>Haiku</th>\n<th>CLI Specification</th>\n</tr>\n<tr>\n<td>Anna (Premium)</td>\n<td>German</td>\n<td>Emmy Noether</td>\n<td>Symmetrie haucht Leben<br>Algebras verborgne Form<br>Abstraktion blÃ¼ht<br><br><i>Symmetry breathes life<br>Algebra's hidden forms<br>Abstraction blooms</i></td>\n<td><code>-v \"Anna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Emma (Premium)</td>\n<td>Italian</td>\n<td>Maria Adelaide Sneider</td>\n<td>Algoritmi in danza<br>Macchina sussurra dolce<br>Il codice vive<br><br><i>Algorithms dance<br>Machine whispers secrets soft<br>Code becomes alive</i></td>\n<td><code>-v \"Emma (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Federica (Premium)</td>\n<td>Italian</td>\n<td>Pia Nalli</td>\n<td>Teoremi fluenti<br>Numeri danzano liberi<br>VeritÃ  emerge<br><br><i>Flowing theorems dance<br>Numbers move in freedom's space<br>Truth emerges pure</i></td>\n<td><code>-v \"Federica (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Serena (Premium)</td>\n<td>English (UK)</td>\n<td>Bertha Swirles</td>\n<td>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges<br><br><i>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges</i></td>\n<td><code>-v \"Serena (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Petra (Premium)</td>\n<td>German</td>\n<td>Ruth Moufang</td>\n<td>Algebra spricht<br>In Symmetrien versteckt<br>Wahrheit erblÃ¼ht<br><br><i>Algebra speaks soft<br>Hidden in symmetries pure<br>Truth blooms anew here</i></td>\n<td><code>-v \"Petra (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Yuna (Premium)</td>\n<td>Korean</td>\n<td>Hee Oh</td>\n<td>ìˆ¨ì€ íŒ¨í„´ ë¹›ë‚˜ê³ <br>ë§ˆìŒì˜ ë°©ì •ì‹ í•€ë‹¤<br>ì§€ì‹ ìë¼ë‚˜<br><br><i>Hidden patterns gleam<br>Mind's equations softly bloom<br>Knowledge multiplies</i></td>\n<td><code>-v \"Yuna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Alva (Premium)</td>\n<td>Swedish</td>\n<td>Sonja Korovkin</td>\n<td>MÃ¶nster flÃ¶dar fritt<br>Genom tankens labyrinter<br>Visdom blomstrar hÃ¤r<br><br><i>Patterns flowing free<br>Through labyrinths of the mind<br>Wisdom blooms right here</i></td>\n<td><code>-v \"Alva (Premium)\"</code></td>\n</tr>\n<tr>\n<td>AmÃ©lie (Premium)</td>\n<td>French (Canada)</td>\n<td>Sophie Germain</td>\n<td>Nombres premiers murmurent<br>Dansent entre les silences<br>SymÃ©trie s'ouvre<br><br><i>Prime numbers whisper<br>Dancing between the silence<br>Symmetry unfolds</i></td>\n<td><code>-v \"AmÃ©lie (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Ewa (Premium)</td>\n<td>Polish</td>\n<td>Maria Wielgus</td>\n<td>Logiki korzenie<br>Matematyczne krainy<br>MyÅ›l kieÅ‚kujÄ…ca<br><br><i>Logic's tender roots<br>Mathematical landscapes<br>Thought's seeds germinate</i></td>\n<td><code>-v \"Ewa (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Kiyara (Premium)</td>\n<td>Hindi</td>\n<td>Shakuntala Devi</td>\n<td>à¤—à¤£à¤¿à¤¤ à¤•à¥€ à¤²à¤¯ à¤®à¥‡à¤‚<br>à¤…à¤‚à¤• à¤¨à¥ƒà¤¤à¥à¤¯ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚<br>à¤œà¥à¤à¤¾à¤¨ à¤œà¤—à¤¤à¤¾ à¤¹à¥ˆ<br><br><i>In rhythm of math<br>Numbers dance their sacred steps<br>Knowledge awakens</i></td>\n<td><code>-v \"Kiyara (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Majed (Premium)</td>\n<td>Arabic</td>\n<td>Maha Al-Aswad</td>\n<td>Ø£Ø±Ù‚Ø§Ù… ØªØ±Ù‚Øµ<br>ÙÙŠ ÙØ¶Ø§Ø¡ Ø§Ù„Ù„Ø§Ù†Ù‡Ø§ÙŠØ©<br>Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© ØªØ´Ø±Ù‚<br><br><i>Numbers dance freely<br>In infinity's vast space<br>Truth rises like dawn</i></td>\n<td><code>-v \"Majed (Premium)\"</code></td>\n</tr>\n<tr>\n<td>TÃ¼nde (Premium)</td>\n<td>Hungarian</td>\n<td>Julia ErdÅ‘s</td>\n<td>SzÃ¡mok tÃ¡ncolnak<br>VÃ©gtelen tÃ©rben szÃ¡llnak<br>IgazsÃ¡g virrad<br><br><i>Numbers dance and soar<br>Through infinite space they glide<br>Truth dawns pure and bright</i></td>\n<td><code>-v \"TÃ¼nde (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Fiona (Enhanced)</td>\n<td>English (Scottish)</td>\n<td>Mary Somerville</td>\n<td>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars<br><br><i>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars</i></td>\n<td><code>-v \"Fiona (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Lesya (Enhanced)</td>\n<td>Ukrainian</td>\n<td>Olena Voinova</td>\n<td>Ğ¢Ğ¸ÑˆĞ° Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ<br>ĞœÑ–Ğ¶ Ğ·Ñ–Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ½Ñ ÑĞ¿Ğ¸Ñ‚ÑŒ<br>Ğ”ÑƒĞ¼ĞºĞ° Ğ¿Ñ€Ğ¾Ñ€Ğ¾ÑÑ‚Ğ°Ñ”<br><br><i>Silence speaks softly<br>Knowledge sleeps among the stars<br>Thought begins to grow</i></td>\n<td><code>-v \"Lesya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Carmit (Enhanced)</td>\n<td>Hebrew</td>\n<td>Tali Seror</td>\n<td>××™×œ×™× × ×•×©××•×ª ×‘×©×§×˜<br>×‘×™×Ÿ ×©×•×¨×•×ª ×©×œ ×“×××”<br>×©×™×¨ ××ª×¢×•×¨×¨<br><br><i>Words breathe silently<br>Between lines of deep stillness<br>Poem awakening</i></td>\n<td><code>-v \"Carmit (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Milena (Enhanced)</td>\n<td>Russian</td>\n<td>Olga Ladyzhenskaya</td>\n<td>ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ÑˆĞµĞ¿Ñ‡ĞµÑ‚ Ğ½Ğ°Ğ¼<br>Ğ£Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‚<br>Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ğ¼Ğ¾Ğ»Ñ‡Ğ¸Ñ‚<br><br><i>Memory whispers<br>Equations flow like rivers<br>Truth speaks silently</i></td>\n<td><code>-v \"Milena (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Katya (Enhanced)</td>\n<td>Russian</td>\n<td>Sofia Kovalevskaya</td>\n<td>Ğ§Ğ¸ÑĞ»Ğ° Ñ‚Ğ°Ğ½Ñ†ÑƒÑÑ‚<br>Ğ’ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¼<br>Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ñ†Ğ²ĞµÑ‚Ñ‘Ñ‚<br><br><i>Numbers dance freely<br>In space of infinity<br>Truth blooms like a flower</i></td>\n<td><code>-v \"Katya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Damayanti (Enhanced)</td>\n<td>Indonesian</td>\n<td>Sri Pekerti</td>\n<td>Angka menari<br>Dalam ruang tak batas<br>Kebenaran tumbuh<br><br><i>Numbers dance gently<br>In boundless space they flutter<br>Truth grows like new leaves</i></td>\n<td><code>-v \"Damayanti (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Dariush (Enhanced)</td>\n<td>Persian</td>\n<td>Maryam Mirzakhani</td>\n<td>Ø§Ø¹Ø¯Ø§Ø¯ Ù…ÛŒ Ø±Ù‚ØµÙ†Ø¯<br>Ø¯Ø± ÙØ¶Ø§ÛŒ Ø¨ÛŒ Ù¾Ø§ÛŒØ§Ù†<br>Ø­Ù‚ÛŒÙ‚Øª Ù…ÛŒ Ø±ÙˆÛŒØ¯<br><br><i>Numbers dance with grace<br>In endless space they traverse<br>Truth springs forth anew</i></td>\n<td><code>-v \"Dariush (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Astro Boy (Tetsuwan Atomu)<br>Italian dub</td>\n<td>Robot di metallo<br>Cuore umano batte forte<br>Pace nel futuro<br><br><i>Metal robot form<br>Human heart beats strong within<br>Peace in future dawns</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Jeeg Robot d'Acciaio<br>(KÅtetsu Jeeg)</td>\n<td>Acciaio lucente<br>Protettore dei deboli<br>Vola nel cielo<br><br><i>Shining steel warrior<br>Protector of the helpless<br>Soars through the heavens</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Numero 5<br>(Short Circuit)</td>\n<td>Input infinito<br>La coscienza si risveglia<br>Vita artificiale<br><br><i>Infinite input<br>Consciousness awakening<br>Artificial life</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Binbin (Enhanced)</td>\n<td>Chinese (Mainland)</td>\n<td>Li Shanlan</td>\n<td>ç®—æœ¯ä¹‹é“æµ<br>æ•°ç†æ¼”ç»çœŸç†<br>æ™ºæ…§ç»½æ”¾<br><br><i>Arithmetic flows<br>Logic unfolds truth's pattern<br>Wisdom blossoms bright</i></td>\n<td><code>-v \"Binbin (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Han (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chen Jingrun</td>\n<td>ç´ æ•°ä¹‹èˆåŠ¨<br>å“¥å¾·å·´èµ«çŒœæƒ³<br>çœŸç†æ°¸æ’<br><br><i>Prime numbers dancing<br>Goldbach's conjecture whispers<br>Truth eternal flows</i></td>\n<td><code>-v \"Han (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Lilian (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Hua Luogeng</td>\n<td>æ•°è®ºä¹‹å…‰èŠ’<br>è§£æå»¶ç»­ç¾<br>æ™ºæ…§å‡å<br><br><i>Number theory shines<br>Analysis extends grace<br>Wisdom ascends pure</i></td>\n<td><code>-v \"Lilian (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Meijia</td>\n<td>Chinese (Taiwan)</td>\n<td>Sun-Yung Alice Chang</td>\n<td>å¹¾ä½•ä¹‹ç¾ç¾<br>æ›²ç‡æµå‹•ä¸æ¯<br>ç©ºé–“å±•é–‹<br><br><i>Geometry shows<br>Curvature flows endlessly<br>Space unfolds anew</i></td>\n<td><code>-v \"Meijia\"</code></td>\n</tr>\n<tr>\n<td>Sinji (Premium)</td>\n<td>Chinese (Hong Kong)</td>\n<td>Shing-Tung Yau</td>\n<td>æµå½¢ä¹‹å¥§ç§˜<br>å¡æ‹‰æ¯”ç©ºé–“å‹•<br>ç¶­åº¦äº¤ç¹”<br><br><i>Manifolds reveal<br>Calabi spaces in flow<br>Dimensions weave truth</i></td>\n<td><code>-v \"Sinji (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tingting</td>\n<td>Chinese (Mainland)</td>\n<td>Wang Zhenyi</td>\n<td>æ˜Ÿè¾°è½¨è¿¹æ˜<br>å¤©æ–‡æ•°å­¦è<br>æ™ºæ…§é—ªè€€<br><br><i>Starlit paths shine bright<br>Astronomy meets numbers<br>Wisdom radiates</i></td>\n<td><code>-v \"Tingting\"</code></td>\n</tr>\n<tr>\n<td>Yue (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chern Shiing-shen</td>\n<td>å¾®åˆ†å‡ ä½•<br>çº¤ç»´ä¸›ä¸­å¯»çœŸ<br>æœ¬è´¨æ˜¾ç°<br><br><i>Differential forms<br>In fiber bundles seek truth<br>Essence emerges</i></td>\n<td><code>-v \"Yue (Premium)\"</code></td>\n</tr>\n</table>\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Contributors\n\n- Barton Rhodes ([@bmorphism](https://github.com/bmorphism)) - barton@vibes.lol\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "macos",
        "mcp",
        "speech",
        "say command",
        "text speech",
        "speech functionality"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "elevenlabs--elevenlabs-mcp": {
      "owner": "elevenlabs",
      "name": "elevenlabs-mcp",
      "url": "https://github.com/elevenlabs/elevenlabs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/elevenlabs.webp",
      "description": "This server provides APIs for generating speech, voice cloning, and audio transcription. It facilitates seamless interaction with text-to-speech and audio processing functionalities.",
      "stars": 998,
      "forks": 161,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:34:19Z",
      "readme_content": "![export](https://github.com/user-attachments/assets/ee379feb-348d-48e7-899c-134f7f7cd74f)\n\n<div class=\"title-block\" style=\"text-align: center;\" align=\"center\">\n\n  [![Discord Community](https://img.shields.io/badge/discord-@elevenlabs-000000.svg?style=for-the-badge&logo=discord&labelColor=000)](https://discord.gg/elevenlabs)\n  [![Twitter](https://img.shields.io/badge/Twitter-@elevenlabsio-000000.svg?style=for-the-badge&logo=twitter&labelColor=000)](https://x.com/ElevenLabsDevs)\n  [![PyPI](https://img.shields.io/badge/PyPI-elevenlabs--mcp-000000.svg?style=for-the-badge&logo=pypi&labelColor=000)](https://pypi.org/project/elevenlabs-mcp)\n  [![Tests](https://img.shields.io/badge/tests-passing-000000.svg?style=for-the-badge&logo=github&labelColor=000)](https://github.com/elevenlabs/elevenlabs-mcp-server/actions/workflows/test.yml)\n\n</div>\n\n\n<p align=\"center\">\n  Official ElevenLabs <a href=\"https://github.com/modelcontextprotocol\">Model Context Protocol (MCP)</a> server that enables interaction with powerful Text to Speech and audio processing APIs. This server allows MCP clients like <a href=\"https://www.anthropic.com/claude\">Claude Desktop</a>, <a href=\"https://www.cursor.so\">Cursor</a>, <a href=\"https://codeium.com/windsurf\">Windsurf</a>, <a href=\"https://github.com/openai/openai-agents-python\">OpenAI Agents</a> and others to generate speech, clone voices, transcribe audio, and more.\n</p>\n\n<!--\nmcp-name: io.github.elevenlabs/elevenlabs-mcp\n-->\n\n## Quickstart with Claude Desktop\n\n1. Get your API key from [ElevenLabs](https://elevenlabs.io/app/settings/api-keys). There is a free tier with 10k credits per month.\n2. Install `uv` (Python package manager), install with `curl -LsSf https://astral.sh/uv/install.sh | sh` or see the `uv` [repo](https://github.com/astral-sh/uv) for additional install methods.\n3. Go to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```\n{\n  \"mcpServers\": {\n    \"ElevenLabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"<insert-your-api-key-here>\"\n      }\n    }\n  }\n}\n\n```\n\nIf you're using Windows, you will have to enable \"Developer Mode\" in Claude Desktop to use the MCP server. Click \"Help\" in the hamburger menu at the top left and select \"Enable Developer Mode\".\n\n## Other MCP clients\n\nFor other clients like Cursor and Windsurf, run:\n1. `pip install elevenlabs-mcp`\n2. `python -m elevenlabs_mcp --api-key={{PUT_YOUR_API_KEY_HERE}} --print` to get the configuration. Paste it into appropriate configuration directory specified by your MCP client.\n\nThat's it. Your MCP client can now interact with ElevenLabs through these tools:\n\n## Example usage\n\nâš ï¸ Warning: ElevenLabs credits are needed to use these tools.\n\nTry asking Claude:\n\n- \"Create an AI agent that speaks like a film noir detective and can answer questions about classic movies\"\n- \"Generate three voice variations for a wise, ancient dragon character, then I will choose my favorite voice to add to my voice library\"\n- \"Convert this recording of my voice to sound like a medieval knight\"\n- \"Create a soundscape of a thunderstorm in a dense jungle with animals reacting to the weather\"\n- \"Turn this speech into text, identify different speakers, then convert it back using unique voices for each person\"\n\n## Optional features\n\n### File Output Configuration\n\nYou can configure how the MCP server handles file outputs using these environment variables in your `claude_desktop_config.json`:\n\n- **`ELEVENLABS_MCP_BASE_PATH`**: Specify the base path for file operations with relative paths (default: `~/Desktop`)\n- **`ELEVENLABS_MCP_OUTPUT_MODE`**: Control how generated files are returned (default: `files`)\n\n#### Output Modes\n\nThe `ELEVENLABS_MCP_OUTPUT_MODE` environment variable supports three modes:\n\n1. **`files`** (default): Save files to disk and return file paths\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"files\"\n   }\n   ```\n\n2. **`resources`**: Return files as MCP resources; binary data is base64-encoded, text is returned as UTF-8 text\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"resources\"\n   }\n   ```\n\n3. **`both`**: Save files to disk AND return as MCP resources\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"both\"\n   }\n   ```\n\n**Resource Mode Benefits:**\n- Files are returned directly in the MCP response as base64-encoded data\n- No disk I/O required - useful for containerized or serverless environments\n- MCP clients can access file content immediately without file system access\n- In `both` mode, resources can be fetched later using the `elevenlabs://filename` URI pattern\n\n**Use Cases:**\n- `files`: Traditional file-based workflows, local development\n- `resources`: Cloud environments, MCP clients without file system access\n- `both`: Maximum flexibility, caching, and resource sharing scenarios\n\n### Data residency keys\n\nYou can specify the data residency region with the `ELEVENLABS_API_RESIDENCY` environment variable. Defaults to `\"us\"`.\n\n**Note:** Data residency is an enterprise only feature. See [the docs](https://elevenlabs.io/docs/product-guides/administration/data-residency#overview) for more details.\n\n## Contributing\n\nIf you want to contribute or run from source:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/elevenlabs/elevenlabs-mcp\ncd elevenlabs-mcp\n```\n\n2. Create a virtual environment and install dependencies [using uv](https://github.com/astral-sh/uv):\n\n```bash\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n```\n\n3. Copy `.env.example` to `.env` and add your ElevenLabs API key:\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key\n```\n\n4. Run the tests to make sure everything is working:\n\n```bash\n./scripts/test.sh\n# Or with options\n./scripts/test.sh --verbose --fail-fast\n```\n\n5. Install the server in Claude Desktop: `mcp install elevenlabs_mcp/server.py`\n\n6. Debug and test locally with MCP Inspector: `mcp dev elevenlabs_mcp/server.py`\n\n## Troubleshooting\n\nLogs when running with Claude Desktop can be found at:\n\n- **Windows**: `%APPDATA%\\Claude\\logs\\mcp-server-elevenlabs.log`\n- **macOS**: `~/Library/Logs/Claude/mcp-server-elevenlabs.log`\n\n### Timeouts when using certain tools\n\nCertain ElevenLabs API operations, like voice design and audio isolation, can take a long time to resolve. When using the MCP inspector in dev mode, you might get timeout errors despite the tool completing its intended task.\n\nThis shouldn't occur when using a client like Claude.\n\n### MCP ElevenLabs: spawn uvx ENOENT\n\nIf you encounter the error \"MCP ElevenLabs: spawn uvx ENOENT\", confirm its absolute path by running this command in your terminal:\n\n```bash\nwhich uvx\n```\n\nOnce you obtain the absolute path (e.g., `/usr/local/bin/uvx`), update your configuration to use that path (e.g., `\"command\": \"/usr/local/bin/uvx\"`). This ensures that the correct executable is referenced.\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcription",
        "voice",
        "audio",
        "voice cloning",
        "audio transcription",
        "cloning audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "georgi-io--jessica": {
      "owner": "georgi-io",
      "name": "jessica",
      "url": "https://github.com/georgi-io/jessica",
      "imageUrl": "/freedevtools/mcp/pfp/georgi-io.webp",
      "description": "Integrates ElevenLabs Text-to-Speech capabilities for seamless text conversion to speech, offering voice selection and management through a modern interface. Supports real-time communication with a FastAPI backend and a React frontend.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-25T23:18:06Z",
      "readme_content": "# Project Jessica (ElevenLabs TTS MCP)\n\nThis project integrates ElevenLabs Text-to-Speech capabilities with Cursor through the Model Context Protocol (MCP). It consists of a FastAPI backend service and a React frontend application.\n\n## Features\n\n- Text-to-Speech conversion using ElevenLabs API\n- Voice selection and management\n- MCP integration for Cursor\n- Modern React frontend interface\n- WebSocket real-time communication\n- Pre-commit hooks for code quality\n- Automatic code formatting and linting\n\n## Project Structure\n\n```\njessica/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ backend/          # FastAPI backend service\nâ”‚   â””â”€â”€ frontend/         # React frontend application\nâ”œâ”€â”€ terraform/            # Infrastructure as Code\nâ”œâ”€â”€ tests/               # Test suites\nâ””â”€â”€ docs/                # Documentation\n```\n\n## Requirements\n\n- Python 3.11+\n- Poetry (for backend dependency management)\n- Node.js 18+ (for frontend)\n- Cursor (for MCP integration)\n\n## Local Development Setup\n\n### Backend Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/georgi-io/jessica.git\ncd jessica\n\n# Create Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install backend dependencies\npoetry install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your ElevenLabs API key\n\n# Install pre-commit hooks\npoetry run pre-commit install\n```\n\n### Frontend Setup\n\n```bash\n# Navigate to frontend directory\ncd src/frontend\n\n# Install dependencies\nnpm install\n```\n\n## Development Servers\n\n### Starting the Backend\n\n```bash\n# Activate virtual environment if not active\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Start the backend\npython -m src.backend\n```\n\nThe backend provides:\n- REST API: http://localhost:9020\n- WebSocket: ws://localhost:9020/ws\n- MCP Server: http://localhost:9020/sse (integrated with the main API server)\n\n### Starting the Frontend\n\n```bash\n# In src/frontend directory\nnpm run dev\n```\n\nFrontend development server:\n- http://localhost:5173\n\n## Environment Configuration\n\n### Backend (.env)\n```env\n# ElevenLabs API\nELEVENLABS_API_KEY=your-api-key\n\n# Server Configuration\nHOST=127.0.0.1\nPORT=9020\n\n# Development Settings\nDEBUG=false\nRELOAD=true\n```\n\n### Frontend (.env)\n```env\nVITE_API_URL=http://localhost:9020\nVITE_WS_URL=ws://localhost:9020/ws\n```\n\n## Code Quality Tools\n\n### Backend\n\n```bash\n# Run all pre-commit hooks\npoetry run pre-commit run --all-files\n\n# Run specific tools\npoetry run ruff check .\npoetry run ruff format .\npoetry run pytest\n```\n\n### Frontend\n\n```bash\n# Lint\nnpm run lint\n\n# Type check\nnpm run type-check\n\n# Test\nnpm run test\n```\n\n## Production Deployment\n\n### AWS ECR and GitHub Actions Setup\n\nTo enable automatic building and pushing of Docker images to Amazon ECR:\n\n1. Apply the Terraform configuration to create the required AWS resources:\n   ```bash\n   cd terraform\n   terraform init\n   terraform apply\n   ```\n\n2. The GitHub Actions workflow will automatically:\n   - Read the necessary configuration from the Terraform state in S3\n   - Build the Docker image on pushes to `main` or `develop` branches\n   - Push the image to ECR with tags for `latest` and the specific commit SHA\n\n3. No additional repository variables needed! The workflow fetches all required configuration from the Terraform state.\n\n### How it Works\n\nThe GitHub Actions workflow is configured to:\n1. Initially assume a predefined IAM role with S3 read permissions\n2. Fetch and extract configuration values from the Terraform state file in S3\n3. Re-authenticate using the actual deployment role from the state file\n4. Build and push the Docker image to the ECR repository defined in the state\n\nThis approach eliminates the need to manually configure GitHub repository variables and ensures that the CI/CD process always uses the current infrastructure configuration.\n\n### Quick Overview\n\n- Frontend: Served from S3 via CloudFront at jessica.georgi.io\n- Backend API: Available at api.georgi.io/jessica\n- WebSocket: Connects to api.georgi.io/jessica/ws\n- Docker Image: Stored in AWS ECR and can be deployed to ECS/EKS\n- Infrastructure: Managed via Terraform in this repository\n\n## MCP Integration with Cursor\n\n1. Start the backend server\n2. In Cursor settings, add new MCP server:\n   - Name: Jessica TTS\n   - Type: SSE\n   - URL: http://localhost:9020/sse\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Issues**\n   - Error: \"Invalid API key\"\n   - Solution: Check `.env` file\n\n2. **Connection Problems**\n   - Error: \"Cannot connect to MCP server\"\n   - Solution: Verify backend is running and ports are correct\n\n3. **Port Conflicts**\n   - Error: \"Address already in use\"\n   - Solution: Change ports in `.env`\n\n4. **WebSocket Connection Failed**\n   - Error: \"WebSocket connection failed\"\n   - Solution: Ensure backend is running and WebSocket URL is correct\n\nFor additional help, please open an issue on GitHub.\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "text",
        "text speech",
        "speech recognition",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "giannisanni--kokoro-tts-mcp": {
      "owner": "giannisanni",
      "name": "kokoro-tts-mcp",
      "url": "https://github.com/giannisanni/kokoro-tts-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/giannisanni.webp",
      "description": "Integrates text-to-speech capabilities using the Kokoro TTS engine, enabling conversion of written content into spoken audio with customizable voices and adjustable speed. Supports saving audio files and cross-platform playback.",
      "stars": 10,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-28T13:54:19Z",
      "readme_content": "# Kokoro TTS MCP Server\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Kokoro TTS engine. This server exposes TTS functionality through MCP tools, making it easy to integrate speech synthesis into your applications.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- `uv` package manager\n\n## Installation\n\n1. First, install the `uv` package manager:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Clone this repository and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\nuv pip install .\n```\n\n## Features\n\n- Text-to-speech synthesis with customizable voices\n- Adjustable speech speed\n- Support for saving audio to files or direct playback\n- Cross-platform audio playback support (Windows, macOS, Linux)\n\n## Usage\n\nThe server provides a single MCP tool `generate_speech` with the following parameters:\n\n- `text` (required): The text to convert to speech\n- `voice` (optional): Voice to use for synthesis (default: \"af_heart\")\n- `speed` (optional): Speech speed multiplier (default: 1.0)\n- `save_path` (optional): Directory to save audio files\n- `play_audio` (optional): Whether to play the audio immediately (default: False)\n\n### Example Usage\n\n```python\nfrom mcp.client import Client\n\nasync with Client() as client:\n    await client.connect(\"kokoro-tts\")\n    \n    # Generate and play speech\n    result = await client.call_tool(\n        \"generate_speech\",\n        {\n            \"text\": \"Hello, world!\",\n            \"voice\": \"af_heart\",\n            \"speed\": 1.0,\n            \"play_audio\": True\n        }\n    )\n```\n\n## Dependencies\n\n- kokoro >= 0.8.4\n- mcp[cli] >= 1.3.0\n- soundfile >= 0.13.1\n\n## Platform Support\n\nAudio playback is supported on:\n- Windows (using `start`)\n- macOS (using `afplay`)\n- Linux (using `aplay`)\n\n## MCP Configuration\n\nAdd the following configuration to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"kokoro-tts\": {\n      \"command\": \"/Users/giannisan/pinokio/bin/miniconda/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/giannisan/Documents/Cline/MCP/kokoro-tts-mcp\",\n        \"run\",\n        \"tts-mcp.py\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\n[Add your license information here]\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "audio",
        "kokoro",
        "tts",
        "kokoro tts",
        "text speech",
        "spoken audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "hammeiam--koroko-speech-mcp": {
      "owner": "hammeiam",
      "name": "koroko-speech-mcp",
      "url": "https://github.com/hammeiam/koroko-speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hammeiam.webp",
      "description": "Provides text-to-speech capabilities using the Kokoro TTS model, converting text into natural-sounding speech with customizable options and multiple voice choices.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-22T02:56:06Z",
      "readme_content": "# Speech MCP Server\n\nA Model Context Protocol server that provides text-to-speech capabilities using the Kokoro TTS model.\n\n## Configuration\n\nThe server can be configured using the following environment variables:\n\n| Variable | Description | Default | Valid Range |\n|----------|-------------|---------|-------------|\n| `MCP_DEFAULT_SPEECH_SPEED` | Default speed multiplier for text-to-speech | 1.1 | 0.5 to 2.0 |\n| `MCP_DEFAULT_VOICE` | Default voice for text-to-speech | af_bella | Any valid voice ID |\n\nIn Cursor:\n```\n{\n  \"mcpServers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"speech-mcp-server\"\n      ],\n      \"env\": {\n        \"MCP_DEFAULT_SPEECH_SPEED\": 1.3,\n        \"MCP_DEFAULT_VOICE\": \"af_bella\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- ğŸ¯ High-quality text-to-speech using Kokoro TTS model\n- ğŸ—£ï¸ Multiple voice options available\n- ğŸ›ï¸ Customizable speech parameters (voice, speed)\n- ğŸ”Œ MCP-compliant interface\n- ğŸ“¦ Easy installation and setup\n- ğŸš€ No API key required\n\n## Installation\n\n```bash\n# Using npm\nnpm install speech-mcp-server\n\n# Using pnpm (recommended)\npnpm add speech-mcp-server\n\n# Using yarn\nyarn add speech-mcp-server\n```\n\n## Usage\n\nRun the server:\n\n```bash\n# Using default configuration\nnpm start\n\n# With custom configuration\nMCP_DEFAULT_SPEECH_SPEED=1.5 MCP_DEFAULT_VOICE=af_bella npm start\n```\n\nThe server provides the following MCP tools:\n- `text_to_speech`: Basic text-to-speech conversion\n- `text_to_speech_with_options`: Text-to-speech with customizable speed\n- `list_voices`: List all available voices\n- `get_model_status`: Check the initialization status of the TTS model\n\n### Development\n\n```bash\n# Clone the repository\ngit clone <your-repo-url>\ncd speech-mcp-server\n\n# Install dependencies\npnpm install\n\n# Start development server with auto-reload\npnpm dev\n\n# Build the project\npnpm build\n\n# Run linting\npnpm lint\n\n# Format code\npnpm format\n\n# Test with MCP Inspector\npnpm inspector\n```\n\n## Available Tools\n\n### 1. text_to_speech\nConverts text to speech using the default settings.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\"  // optional\n    }\n  }\n}\n```\n\n### 2. text_to_speech_with_options\nConverts text to speech with customizable parameters.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech_with_options\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\",  // optional\n      \"speed\": 1.0,         // optional (0.5 to 2.0)\n    }\n  }\n}\n```\n\n### 3. list_voices\nLists all available voices for text-to-speech.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"list_voices\",\n  \"params\": {}\n}\n```\n\n### 4. get_model_status\nCheck the current status of the TTS model initialization. This is particularly useful when first starting the server, as the model needs to be downloaded and initialized.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"get_model_status\",\n    \"arguments\": {}\n  }\n}\n```\n\nResponse example:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed)\"\n  }]\n}\n```\n\nPossible status values:\n- `uninitialized`: Model initialization hasn't started\n- `initializing`: Model is being downloaded and initialized\n- `ready`: Model is ready to use\n- `error`: An error occurred during initialization\n\n## Testing\n\nYou can test the server using the MCP Inspector or by sending raw JSON messages:\n\n```bash\n# List available tools\necho '{\"type\":\"request\",\"id\":\"1\",\"method\":\"list_tools\",\"params\":{}}' | node dist/index.js\n\n# List available voices\necho '{\"type\":\"request\",\"id\":\"2\",\"method\":\"list_voices\",\"params\":{}}' | node dist/index.js\n\n# Convert text to speech\necho '{\"type\":\"request\",\"id\":\"3\",\"method\":\"call_tool\",\"params\":{\"name\":\"text_to_speech\",\"arguments\":{\"text\":\"Hello world\",\"voice\":\"af_bella\"}}}' | node dist/index.js\n```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"servers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\"@decodershq/speech-mcp-server\"]\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Troubleshooting\n\n### Model Initialization Issues\n\nThe server automatically attempts to download and initialize the TTS model on startup. If you encounter initialization errors:\n\n1. The server will automatically retry up to 3 times with a cleanup between attempts\n2. Use the `get_model_status` tool to monitor initialization progress and any errors\n3. If initialization fails after all retries, try manually removing the model files:\n\n```bash\n# Remove model files (MacOS/Linux)\nrm -rf ~/.npm/_npx/**/node_modules/@huggingface/transformers/.cache/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\nrm -rf ~/.cache/huggingface/transformers/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\n\n# Then restart the server\nnpm start\n```\n\nThe `get_model_status` tool will now include retry information in its response:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed, retry 1/3)\"\n  }]\n}\n``` ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "koroko",
        "kokoro",
        "voice",
        "koroko speech",
        "speech customizable",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "kentaro--aivis-speech-mcp": {
      "owner": "kentaro",
      "name": "aivis-speech-mcp",
      "url": "https://github.com/kentaro/aivis-speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kentaro.webp",
      "description": "Integrate with the AivisSpeech Engine to provide high-quality speech synthesis capabilities for applications, facilitating the conversion of text to natural-sounding speech. The server offers a type-safe API compliant with the Model Context Protocol, ensuring easy configuration and extensibility.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-15T17:05:34Z",
      "readme_content": "# AivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼\n\nAivisSpeechç”¨ã®Model Context Protocol (MCP) ã‚µãƒ¼ãƒãƒ¼ã®å®Ÿè£…ã§ã™ã€‚ã“ã®ã‚µãƒ¼ãƒãƒ¼ã¯ã€AivisSpeech Engineã¨é€£æºã—ã¦ã€éŸ³å£°åˆæˆã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’é€šã˜ã¦ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰AivisSpeechã®éŸ³å£°åˆæˆæ©Ÿèƒ½ã‚’ç°¡å˜ã«åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n\n## æ¦‚è¦\n\nAivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š\n\n- MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æº–æ‹ ã—ãŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n- AivisSpeech Engineã¨ã®é€£æºã«ã‚ˆã‚‹é«˜å“è³ªãªéŸ³å£°åˆæˆ\n- TypeScriptã«ã‚ˆã‚‹å‹å®‰å…¨ãªè¨­è¨ˆ\n- ç°¡å˜ãªè¨­å®šã¨æ‹¡å¼µæ€§ã®é«˜ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n\n## å¿…è¦æ¡ä»¶\n\n- Node.js 18.xä»¥ä¸Š\n- npm 9.xä»¥ä¸Š\n- AivisSpeech Engineï¼ˆåˆ¥é€”ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ï¼‰\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n```bash\n# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\ngit clone https://github.com/kentaro/aivis-speech-mcp.git\ncd aivis-speech-mcp\n\n# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nnpm install\n\n# ãƒ“ãƒ«ãƒ‰\nnpm run build\n\n# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\ncp .env.sample .env\n# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦ã€å¿…è¦ãªè¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„\n\n# Cursor MCPã®è¨­å®š\ncp .cursor/mcp.json.sample .cursor/mcp.json\n# mcp.jsonãƒ•ã‚¡ã‚¤ãƒ«å†…ã®\"/path/to/aivis-speech-mcp/dist/index.js\"ã‚’\n# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«æ›¸ãæ›ãˆã¦ãã ã•ã„\n# ä¾‹: \"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"\n```\n\n## ç’°å¢ƒè¨­å®š\n\n`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š\n\n```\n# AivisSpeech API Configuration\nAIVIS_SPEECH_API_URL=http://localhost:10101  # AivisSpeech Engineã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\n# Speaker Configuration\nAIVIS_SPEECH_SPEAKER_ID=888753760  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ID\n```\n\n## Cursor MCPè¨­å®š\n\n`.cursor/mcp.json`ãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š\n\n```json\n{\n  \"mcpServers\": {\n    \"AivisSpeech-MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/aivis-speech-mcp/dist/index.js\"]\n    }\n  }\n}\n```\n\n`/path/to/aivis-speech-mcp/dist/index.js`ã‚’ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚\nWindowsã®å ´åˆã¯ã€ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã‹ã€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\nä¾‹: `\"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"`\n\n## ä½¿ã„æ–¹\n\n### é–‹ç™ºãƒ¢ãƒ¼ãƒ‰\n\né–‹ç™ºä¸­ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ä»˜ãã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã§ãã¾ã™ï¼š\n\n```bash\nnpm run dev\n```\n\n### ãƒ“ãƒ«ãƒ‰\n\næœ¬ç•ªç’°å¢ƒç”¨ã«ãƒ“ãƒ«ãƒ‰ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```bash\nnpm run build\n```\n\n### æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰\n\nãƒ“ãƒ«ãƒ‰å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™ï¼š\n\n```bash\nnpm start\n```\n\n### ãƒ†ã‚¹ãƒˆ\n\nãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n\n```bash\nnpm test\n```\n\n## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n\nAivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n\n- **MCPã‚µãƒ¼ãƒ“ã‚¹**: Model Context Protocolã«æº–æ‹ ã—ãŸã‚µãƒ¼ãƒãƒ¼ã‚’æä¾›ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™\n- **AivisSpeech ã‚µãƒ¼ãƒ“ã‚¹**: AivisSpeech Engineã®APIã¨é€šä¿¡ã—ã€éŸ³å£°åˆæˆã‚’å®Ÿè¡Œã—ã¾ã™\n\n## APIä»•æ§˜\n\nMCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æº–æ‹ ã—ãŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚ä¸»ãªæ©Ÿèƒ½ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n- éŸ³å£°åˆæˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆï¼‰\n- ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼æƒ…å ±ã®å–å¾—\n- éŸ³å£°ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n\nè©³ç´°ãªAPIä»•æ§˜ã«ã¤ã„ã¦ã¯[AivisSpeech Engine APIä»•æ§˜](https://aivis-project.github.io/AivisSpeech-Engine/api/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n## MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ã®é€£æº\n\nã“ã®ã‚µãƒ¼ãƒãƒ¼ã¯ã€Model Context Protocolï¼ˆMCPï¼‰ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«åˆ©ç”¨ã§ãã¾ã™ã€‚MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã¤ã„ã¦ã®è©³ç´°ã¯[MCPå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://modelcontextprotocol.github.io/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\nã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–ï¼š\n\n- **AivisSpeech Engineã«æ¥ç¶šã§ããªã„**: `.env`ãƒ•ã‚¡ã‚¤ãƒ«ã®`AIVIS_SPEECH_API_URL`ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n- **éŸ³å£°ãŒå†ç”Ÿã•ã‚Œãªã„**: ã‚·ã‚¹ãƒ†ãƒ ã®éŸ³å£°è¨­å®šã‚’ç¢ºèªã—ã€é©åˆ‡ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n- **ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼IDãŒè¦‹ã¤ã‹ã‚‰ãªã„**: AivisSpeech EngineãŒæ­£ã—ãèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã€åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼IDã‚’ç¢ºèªã—ã¦ãã ã•ã„\n\n## è²¢çŒ®\n\nãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€GitHubã®Issueãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’é€šã˜ã¦ãŠé¡˜ã„ã—ã¾ã™ã€‚ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚‚æ­“è¿ã—ã¾ã™ã€‚\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\n[MIT](LICENSE)\n\n## è¬è¾\n\n- [AivisSpeech Engine](https://github.com/aivis-project/AivisSpeech-Engine)ãƒãƒ¼ãƒ \n- [Model Context Protocol](https://modelcontextprotocol.github.io/)ã®é–‹ç™ºè€…\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "aivisspeech",
        "synthesis",
        "aivis",
        "speech synthesis",
        "speech server",
        "aivis speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "mamertofabian--elevenlabs-mcp-server": {
      "owner": "mamertofabian",
      "name": "elevenlabs-mcp-server",
      "url": "https://github.com/mamertofabian/elevenlabs-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mamertofabian.webp",
      "description": "Integrates with ElevenLabs text-to-speech API to generate audio from text input, manage voice generation tasks, and store history using an SQLite database. Includes a sample SvelteKit client for performing text-to-speech conversions and managing script parts.",
      "stars": 112,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# ElevenLabs MCP Server\n[![smithery badge](https://smithery.ai/badge/elevenlabs-mcp-server)](https://smithery.ai/server/elevenlabs-mcp-server)\n\nA Model Context Protocol (MCP) server that integrates with ElevenLabs text-to-speech API, featuring both a server component and a sample web-based MCP Client (SvelteKit) for managing voice generation tasks.\n\n<a href=\"https://glama.ai/mcp/servers/leukzvus7o\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/leukzvus7o/badge\" alt=\"ElevenLabs Server MCP server\" /></a>\n\n## Features\n\n- Generate audio from text using ElevenLabs API\n- Support for multiple voices and script parts\n- SQLite database for persistent history storage\n- Sample SvelteKit MCP Client for:\n  - Simple text-to-speech conversion\n  - Multi-part script management\n  - Voice history tracking and playback\n  - Audio file downloads\n\n## Installation\n\n### Installing via Smithery\n\nTo install ElevenLabs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/elevenlabs-mcp-server):\n\n```bash\nnpx -y @smithery/cli install elevenlabs-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `cline_mcp_settings.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp-server\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   uv venv\n   ```\n3. Copy `.env.example` to `.env` and fill in your ElevenLabs credentials\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/elevenlabs-mcp-server\",\n        \"run\",\n        \"elevenlabs-mcp-server\"\n      ],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n## Using the Sample SvelteKit MCP Client\n\n1. Navigate to the web UI directory:\n   ```bash\n   cd clients/web-ui\n   ```\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n3. Copy `.env.example` to `.env` and configure as needed\n4. Run the web UI:\n   ```bash\n   pnpm dev\n   ```\n5. Open http://localhost:5174 in your browser\n\n### Available Tools\n\n- `generate_audio_simple`: Generate audio from plain text using default voice settings\n- `generate_audio_script`: Generate audio from a structured script with multiple voices and actors\n- `delete_job`: Delete a job by its ID\n- `get_audio_file`: Get the audio file by its ID\n- `list_voices`: List all available voices\n- `get_voiceover_history`: Get voiceover job history. Optionally specify a job ID for a specific job.\n\n### Available Resources\n\n- `voiceover://history/{job_id}`: Get the audio file by its ID\n- `voiceover://voices`: List all available voices\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mamertofabian",
        "voice",
        "audio",
        "mamertofabian elevenlabs",
        "speech api",
        "speech conversions"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nakamurau1--tts-mcp": {
      "owner": "nakamurau1",
      "name": "tts-mcp",
      "url": "https://github.com/nakamurau1/tts-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nakamurau1.webp",
      "description": "Integrates high-quality text-to-speech capabilities into applications, converting text to audio with customizable voice options and output formats. Provides a command-line tool for quick conversions and supports various parameters for audio customization.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T10:32:51Z",
      "readme_content": "# tts-mcp\n\nA Model Context Protocol (MCP) server and command-line tool for high-quality text-to-speech generation using the OpenAI TTS API.\n\n## Main Features\n\n- **MCP Server**: Integrate text-to-speech capabilities with Claude Desktop and other MCP-compatible clients\n- **Voice Options**: Support for multiple voice characters (alloy, nova, echo, etc.)\n- **High-Quality Audio**: Support for various output formats (MP3, WAV, OPUS, AAC)\n- **Customizable**: Configure speech speed, voice character, and additional instructions\n- **CLI Tool**: Also available as a command-line utility for direct text-to-speech conversion\n\n## Installation\n\n### Method 1: Install from Repository\n\n```bash\n# Clone the repository\ngit clone https://github.com/nakamurau1/tts-mcp.git\ncd tts-mcp\n\n# Install dependencies\nnpm install\n\n# Optional: Install globally\nnpm install -g .\n```\n\n### Method 2: Run Directly with npx (No Installation Required)\n\n```bash\n# Start the MCP server directly\nnpx tts-mcp tts-mcp-server --voice nova --model tts-1-hd\n\n# Use the CLI tool directly\nnpx tts-mcp -t \"Hello, world\" -o hello.mp3\n```\n\n## MCP Server Usage\n\nThe MCP server allows you to integrate text-to-speech functionality with Model Context Protocol (MCP) compatible clients like Claude Desktop.\n\n### Starting the MCP Server\n\n```bash\n# Start with default settings\nnpm run server\n\n# Start with custom settings\nnpm run server -- --voice nova --model tts-1-hd\n\n# Or directly with API key\nnode bin/tts-mcp-server.js --voice echo --api-key your-openai-api-key\n```\n\n### MCP Server Options\n\n```\nOptions:\n  -V, --version       Display version information\n  -m, --model <model> TTS model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <voice> Voice character (default: \"alloy\")\n  -f, --format <format> Audio format (default: \"mp3\")\n  --api-key <key>     OpenAI API key (can also be set via environment variable)\n  -h, --help          Display help information\n```\n\n### Integrating with MCP Clients\n\nThe MCP server can be used with Claude Desktop and other MCP-compatible clients. For Claude Desktop integration:\n\n1. Open the Claude Desktop configuration file (typically at `~/Library/Application Support/Claude/claude_desktop_config.json`)\n2. Add the following configuration, including your OpenAI API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"full/path/to/bin/tts-mcp-server.js\", \"--voice\", \"nova\", \"--api-key\", \"your-openai-api-key\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use npx for easier setup:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-p\", \"tts-mcp\", \"tts-mcp-server\", \"--voice\", \"nova\", \"--model\", \"gpt-4o-mini-tts\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nYou can provide the API key in two ways:\n\n1. **Direct method** (recommended for testing): Include it in the `args` array using the `--api-key` parameter\n2. **Environment variable method** (more secure): Set it in the `env` object as shown above\n\n> **Security Note**: Make sure to secure your configuration file when including API keys.\n\n3. Restart Claude Desktop\n4. When you ask Claude to \"read this text aloud\" or similar requests, the text will be converted to speech\n\n### Available MCP Tools\n\n- **text-to-speech**: Tool for converting text to speech and playing it\n\n## CLI Tool Usage\n\nYou can also use tts-mcp as a standalone command-line tool:\n\n```bash\n# Convert text directly\ntts-mcp -t \"Hello, world\" -o hello.mp3\n\n# Convert from a text file\ntts-mcp -f speech.txt -o speech.mp3\n\n# Specify custom voice\ntts-mcp -t \"Welcome to the future\" -o welcome.mp3 -v nova\n```\n\n### CLI Tool Options\n\n```\nOptions:\n  -V, --version           Display version information\n  -t, --text <text>       Text to convert\n  -f, --file <path>       Path to input text file\n  -o, --output <path>     Path to output audio file (required)\n  -m, --model <n>         Model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <n>         Voice character (default: \"alloy\")\n  -s, --speed <number>    Speech speed (0.25-4.0) (default: 1)\n  --format <format>       Output format (default: \"mp3\")\n  -i, --instructions <text> Additional instructions for speech generation\n  --api-key <key>         OpenAI API key (can also be set via environment variable)\n  -h, --help              Display help information\n```\n\n## Supported Voices\n\nThe following voice characters are supported:\n- alloy (default)\n- ash\n- coral\n- echo\n- fable\n- onyx\n- nova\n- sage\n- shimmer\n\n## Supported Models\n\n- tts-1\n- tts-1-hd\n- gpt-4o-mini-tts (default)\n\n## Output Formats\n\nThe following output formats are supported:\n- mp3 (default)\n- opus\n- aac\n- flac\n- wav\n- pcm\n\n## Environment Variables\n\nYou can also configure the tool using system environment variables:\n\n```\nOPENAI_API_KEY=your-api-key-here\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tts",
        "voice",
        "audio",
        "tts mcp",
        "text audio",
        "nakamurau1 tts"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "/freedevtools/mcp/pfp/nansasuke.webp",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\nå›¾ç‰‡è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€åƒåœ¾åˆ†ç±»\n\nä¸€ä¸ªå®Œæ•´çš„åƒåœ¾åˆ†ç±»çš„app\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recycling",
        "waste",
        "garbagesorting",
        "classify waste",
        "waste using",
        "nansasuke garbagesorting"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "neosapience--typecast-api-mcp-server-sample": {
      "owner": "neosapience",
      "name": "typecast-api-mcp-server-sample",
      "url": "https://github.com/neosapience/typecast-api-mcp-server-sample",
      "imageUrl": "/freedevtools/mcp/pfp/neosapience.webp",
      "description": "Integrates with the Typecast API to manage voices, convert text to speech, and play audio. Provides a standardized MCP interface for seamless interaction with voice capabilities.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T23:50:45Z",
      "readme_content": "# typecast-api-mcp-server-sample\n\nMCP Server for typecast-api, enabling seamless integration with MCP clients. This project provides a standardized way to interact with Typecast API through the Model Context Protocol.\n\n## About\n\nThis project implements a Model [Context Protocol server](https://modelcontextprotocol.io/introduction) for Typecast API, allowing MCP clients to interact with the Typecast API in a standardized way.\n\n## Feature Implementation Status\n\n| Feature              | Status |\n| -------------------- | ------ |\n| **Voice Management** |        |\n| Get Voices           | âœ…     |\n| Text to Speech       | âœ…     |\n| Play Audio           | âœ…     |\n\n## Setup\n\n### Git Clone\n\n```bash\ngit clone https://github.com/hyunseung/typecast-api-mcp-server-sample.git\ncd typecast-api-mcp-server-sample\n```\n\n### Dependencies\n\nThis project requires Python 3.10 or higher and uses `uv` for package management.\n\n#### Package Installation\n\n```bash\n# Create virtual environment and install packages\nuv venv\nuv pip install -e .\n```\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nTYPECAST_API_HOST=https://api.typecast.ai\nTYPECAST_API_KEY=<your-api-key>\nTYPECAST_OUTPUT_DIR=<your-output-directory> # default: ~/Downloads/typecast_output\n```\n\n### Usage with Claude Desktop\n\nYou can add the following to your `claude_desktop_config.json`:\n\n#### Basic Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"typecast-api-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/PROJECT\",\n        \"run\",\n        \"typecast-api-mcp-server\"\n      ],\n      \"env\": {\n        \"TYPECAST_API_HOST\": \"https://api.typecast.ai\",\n        \"TYPECAST_API_KEY\": \"YOUR_API_KEY\",\n        \"TYPECAST_OUTPUT_DIR\": \"PATH/TO/YOUR/OUTPUT/DIR\"\n      }\n    }\n  }\n}\n```\n\nReplace `/PATH/TO/YOUR/PROJECT` with the actual path where your project is located.\n\n### Manual Execution\n\nYou can also run the server manually:\n\n```bash\nuv run python app/main.py\n```\n\n## Contributing\n\nContributions are always welcome! Feel free to submit a Pull Request.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "typecast",
        "voice",
        "voices",
        "typecast api",
        "mcp interface",
        "api mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "uraoz--bouyomichan-mcp-nodejs": {
      "owner": "uraoz",
      "name": "bouyomichan-mcp-nodejs",
      "url": "https://github.com/uraoz/bouyomichan-mcp-nodejs",
      "imageUrl": "/freedevtools/mcp/pfp/uraoz.webp",
      "description": "Provides text-to-speech capabilities using BouyomiChan's Yukkuri voice, enabling voice output from text commands with customizable options for voice type, volume, speed, and pitch. Integrates seamlessly with Claude for Desktop for enhanced user interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T11:48:46Z",
      "readme_content": "# æ£’èª­ã¿ã¡ã‚ƒã‚“MCPã‚µãƒ¼ãƒãƒ¼ (Node.jsç‰ˆ)\n\n<a href=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs/badge\" alt=\"Bouyomi-chan Server MCP server\" />\n</a>\n\n\n## å‰ææ¡ä»¶\n\n- Node.js 16ä»¥ä¸Š\n- npm 7ä»¥ä¸Š\n- æ£’èª­ã¿ã¡ã‚ƒã‚“ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨\n- æ£’èª­ã¿ã¡ã‚ƒã‚“ã®HTTPé€£æºãŒãƒãƒ¼ãƒˆ50080ã§èµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨\n\n## ä½¿ç”¨æ–¹æ³•\n\n### ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•\n\n```bash\ngit clone https://github.com/uraoz/bouyomichan-mcp-nodejs.git\ncd bouyomichan-mcp-nodejs\nnpm install\nnpm run build\nnpm start\n```\n\n### Claude for Desktopã¨ã®é€£æº\n\n```json\n{\n  \"mcpServers\": {\n    \"bouyomichan\":{\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:uraoz/bouyomichan-mcp-nodejs\"\n      ]\n    }\n  }\n}\n```\n\n## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜\n\n| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | æœ‰åŠ¹ç¯„å›² |\n|----------|------|------------|---------|\n| text     | èª­ã¿ä¸Šã’ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ | å¿…é ˆ | ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆ |\n| voice    | éŸ³å£°ã®ç¨®é¡ | 0 (å¥³æ€§1) | 0: å¥³æ€§1ã€1: ç”·æ€§1ã€2: å¥³æ€§2ã€... |\n| volume   | éŸ³é‡ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€0-100: éŸ³é‡ãƒ¬ãƒ™ãƒ« |\n| speed    | é€Ÿåº¦ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€50-200: é€Ÿåº¦ãƒ¬ãƒ™ãƒ« |\n| tone     | éŸ³ç¨‹ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€50-200: éŸ³ç¨‹ãƒ¬ãƒ™ãƒ« |\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bouyomichan",
        "voice",
        "nodejs",
        "yukkuri voice",
        "bouyomichan yukkuri",
        "uraoz bouyomichan"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yangsenessa--mcp_voice_identify": {
      "owner": "yangsenessa",
      "name": "mcp_voice_identify",
      "url": "https://github.com/yangsenessa/mcp_voice_identify",
      "imageUrl": "/freedevtools/mcp/pfp/yangsenessa.webp",
      "description": "Provides voice recognition and text extraction capabilities, supporting both file input and base64 encoded data processed in structured formats. Operates in stdio and MCP modes for flexible integration with various systems.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-17T11:21:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yangsenessa-mcp-voice-identify-badge.png)](https://mseep.ai/app/yangsenessa-mcp-voice-identify)\n\n# Voice Recognition MCP Service\n\nThis service provides voice recognition and text extraction capabilities through both stdio and MCP modes.\n\n## Features\n\n- Voice recognition from file\n- Voice recognition from base64 encoded data\n- Text extraction\n- Support for both stdio and MCP modes\n- Structured voice recognition results\n- AIO protocol compliant responses\n\n## Project Structure\n\n- `voice_service.py` - Core service implementation\n- `stdio_server.py` - stdio mode entry point\n- `mcp_server.py` - MCP mode entry point\n- `build.py` - Build script for executables\n- `build_exec.sh` - Build execution script\n- `test_*.sh` - Test scripts for different functionalities\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/AIO-2030/mcp_voice_identify.git\ncd mcp_voice_identify\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up environment variables in `.env`:\n```\nAPI_URL=your_api_url\nAPI_KEY=your_api_key\n```\n\n## Usage\n\n### stdio Mode\n\n1. Run the service:\n```bash\npython stdio_server.py\n```\n\n2. Send JSON-RPC requests via stdin:\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"help\",\n    \"params\": {},\n    \"id\": 1\n}\n```\n\n3. Or use the executable:\n```bash\n./dist/voice_stdio\n```\n\n### MCP Mode\n\n1. Run the service:\n```bash\npython mcp_server.py\n```\n\n2. Or use the executable:\n```bash\n./dist/voice_mcp\n```\n\n## Response Format\n\nThe service follows the AIO protocol for response formatting. Here are examples of different response types:\n\n### Voice Recognition Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"voice\",\n        \"message\": \"Voice processed successfully\",\n        \"text\": \"test test test\",\n        \"metadata\": {\n            \"language\": \"en\",\n            \"emotion\": \"unknown\",\n            \"audio_type\": \"speech\",\n            \"speaker\": \"woitn\",\n            \"raw_text\": \"test test test\"\n        }\n    },\n    \"id\": 1\n}\n```\n\n### Help Information Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"result\": {\n        \"type\": \"voice_service\",\n        \"description\": \"This service provides voice recognition and text extraction services\",\n        \"author\": \"AIO-2030\",\n        \"version\": \"1.0.0\",\n        \"github\": \"https://github.com/AIO-2030/mcp_voice_identify\",\n        \"transport\": [\"stdio\"],\n        \"methods\": [\n            {\n                \"name\": \"help\",\n                \"description\": \"Show this help information.\"\n            },\n            {\n                \"name\": \"identify_voice\",\n                \"description\": \"Identify voice from file\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"Voice file path\"\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                }\n            },\n            {\n                \"name\": \"identify_voice_base64\",\n                \"description\": \"Identify voice from base64 encoded data\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"base64_data\": {\n                            \"type\": \"string\",\n                            \"description\": \"Base64 encoded voice data\"\n                        }\n                    },\n                    \"required\": [\"base64_data\"]\n                }\n            },\n            {\n                \"name\": \"extract_text\",\n                \"description\": \"Extract text\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"text\": {\n                            \"type\": \"string\",\n                            \"description\": \"Text to extract\"\n                        }\n                    },\n                    \"required\": [\"text\"]\n                }\n            }\n        ]\n    },\n    \"id\": 1\n}\n```\n\n### Error Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"error\",\n        \"message\": \"503 Server Error: Service Unavailable\",\n        \"error_code\": 503\n    },\n    \"id\": 1\n}\n```\n\n### Response Fields\n\nThe service provides three types of responses:\n\n1. Voice Recognition Response (using `output` field):\n| Field     | Description                          | Example Value |\n|-----------|--------------------------------------|---------------|\n| type      | Response type                        | \"voice\"       |\n| message   | Status message                       | \"Voice processed successfully\" |\n| text      | Recognized text content              | \"test test test\" |\n| metadata  | Additional information               | See below     |\n\n2. Help Information Response (using `result` field):\n| Field         | Description                          | Example Value |\n|---------------|--------------------------------------|---------------|\n| type          | Service type                         | \"voice_service\" |\n| description   | Service description                  | \"This service provides...\" |\n| author        | Service author                       | \"AIO-2030\"    |\n| version       | Service version                      | \"1.0.0\"       |\n| github        | GitHub repository URL                | \"https://github.com/...\" |\n| transport     | Supported transport modes            | [\"stdio\"]     |\n| methods       | Available methods                    | See methods list |\n\n3. Error Response (using `output` field):\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| type        | Response type                        | \"error\"       |\n| message     | Error message                        | \"503 Server Error: Service Unavailable\" |\n| error_code  | HTTP status code                     | 503          |\n\n### Metadata Fields\n\nThe `metadata` field in voice recognition responses contains:\n\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| language    | Language code                        | \"en\"          |\n| emotion     | Emotion state                        | \"unknown\"     |\n| audio_type  | Audio type                          | \"speech\"      |\n| speaker     | Speaker identifier                   | \"woitn\"       |\n| raw_text    | Original recognized text             | \"test test test\" |\n\n## Building Executables\n\n1. Make the build script executable:\n```bash\nchmod +x build_exec.sh\n```\n\n2. Build stdio mode executable:\n```bash\n./build_exec.sh\n```\n\n3. Build MCP mode executable:\n```bash\n./build_exec.sh mcp\n```\n\nThe executables will be created at:\n- stdio mode: `dist/voice_stdio`\n- MCP mode: `dist/voice_mcp`\n\n## Testing\n\nRun the test scripts:\n\n```bash\nchmod +x test_*.sh\n./test_help.sh\n./test_voice_file.sh\n./test_voice_base64.sh\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_voice_identify",
        "voice",
        "encoded",
        "mcp_voice_identify provides",
        "yangsenessa mcp_voice_identify",
        "voice recognition"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yuiseki--edge_tts_mcp_server": {
      "owner": "yuiseki",
      "name": "edge_tts_mcp_server",
      "url": "https://github.com/yuiseki/edge_tts_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/yuiseki.webp",
      "description": "Provide natural text-to-speech conversion using Microsoft Edge's speech synthesis capabilities, enabling customizable voice output in multiple languages with adjustable speed and pitch.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T03:20:28Z",
      "readme_content": "# Edge-TTS MCP Server\n\nModel Context Protocol (MCP) ã‚µãƒ¼ãƒãƒ¼ã§ã€Microsoft Edge ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸ AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®éŸ³å£°åˆæˆã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚\n\n## æ¦‚è¦\n\nã“ã® MCP ã‚µãƒ¼ãƒãƒ¼ã¯ã€[edge-tts](https://github.com/rany2/edge-tts)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªç„¶ãªéŸ³å£°ã§å¿œç­”ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n## æ©Ÿèƒ½\n\n- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›\n- è¤‡æ•°ã®éŸ³å£°ã¨è¨€èªã®ã‚µãƒãƒ¼ãƒˆ\n- éŸ³å£°é€Ÿåº¦ã¨éŸ³ç¨‹ã®èª¿æ•´\n- éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n```bash\npip install \"edge_tts_mcp_server\"\n```\n\nã¾ãŸã¯é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆï¼š\n\n```bash\ngit clone https://github.com/yuiseki/edge_tts_mcp_server.git\ncd edge_tts_mcp_server\npip install -e .\n```\n\n## ä½¿ç”¨æ–¹æ³•\n\n### VS Code ã§ã®è¨­å®šä¾‹\n\nVS Code ã® settings.json ã§è¨­å®šã™ã‚‹ä¾‹ï¼š\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"edge-tts\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\Users\\\\__username__\\\\src\\\\edge_tts_mcp_server\\\\src\\\\edge_tts_mcp_server\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n### MCP Inspector ã§ã®ä½¿ç”¨\n\næ¨™æº–çš„ãª MCP ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦å®Ÿè¡Œï¼š\n\n```bash\nmcp dev server.py\n```\n\n### uvxï¼ˆuvicornï¼‰ã§ã®å®Ÿè¡Œ\n\nFastAPI ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦ uv ã§å®Ÿè¡Œã™ã‚‹å ´åˆï¼š\n\n```bash\nuv --directory path/to/edge_tts_mcp_server/src/edge_tts_mcp_server run server.py\n```\n\nã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š\n\n```bash\nedge-tts-mcp --host 0.0.0.0 --port 8080 --reload\n```\n\n## API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\nFastAPI ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ãŸå ´åˆã€ä»¥ä¸‹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼š\n\n- `/` - API æƒ…å ±\n- `/health` - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯\n- `/voices` - åˆ©ç”¨å¯èƒ½ãªéŸ³å£°ä¸€è¦§ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ `?locale=ja-JP` ãªã©ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½ï¼‰\n- `/mcp` - MCP API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "edge_tts_mcp_server",
        "voice",
        "edge",
        "yuiseki edge_tts_mcp_server",
        "speech conversion",
        "speech synthesis"
      ],
      "category": "speech-recognition-and-synthesis"
    }
  }
}