{
  "category": "speech-recognition-and-synthesis",
  "categoryDisplay": "Speech Recognition and Synthesis",
  "description": "",
  "totalRepositories": 31,
  "repositories": {
    "CengSin--fishaudio-mcp": {
      "owner": "CengSin",
      "name": "fishaudio-mcp",
      "url": "https://github.com/CengSin/fishaudio-mcp",
      "imageUrl": "https://github.com/CengSin.png",
      "description": "Converts text into natural human speech with customizable audio formats and bitrates, while integrating seamlessly with MCP-compatible applications.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T10:49:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cengsin-fishaudio-mcp-badge.png)](https://mseep.ai/app/cengsin-fishaudio-mcp)\n\n# Fish Audio Python MCP æœåŠ¡\n\nè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Fish Audio API å®ç°çš„æ–‡å­—è½¬è¯­éŸ³ MCP æœåŠ¡ã€‚é€šè¿‡è¿™ä¸ªæœåŠ¡ï¼Œæ‚¨å¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶çš„äººå£°ï¼Œæ”¯æŒå¤šç§é…ç½®é€‰é¡¹ã€‚\n\n## åŠŸèƒ½ç‰¹ç‚¹\n\n- åŸºæœ¬æ–‡å­—è½¬è¯­éŸ³ï¼šå°†ä»»æ„æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶äººå£°\n- é«˜çº§æ–‡å­—è½¬è¯­éŸ³ï¼šæ”¯æŒè‡ªå®šä¹‰éŸ³é¢‘æ ¼å¼ã€æ¯”ç‰¹ç‡ç­‰å‚æ•°\n- å…¼å®¹ MCP åè®®ï¼šå¯ä¸æ”¯æŒ MCP çš„åº”ç”¨æ— ç¼é›†æˆ\n\n## å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt\n```\n\næˆ–ä½¿ç”¨ Python åŒ…ç®¡ç†å·¥å…·å®‰è£…ï¼š\n\n```bash\npip install fish-audio-sdk mcp python-dotenv\n```\n\n## é…ç½®\n\nåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š\n\n```\nAPI_KEY=your_fish_audio_api_key\nMODEL_ID=your_fish_audio_model_id\n```\n\næ‚¨éœ€è¦æ›¿æ¢ä¸ºæ‚¨çš„ Fish Audio API å¯†é’¥å’Œæ¨¡å‹ IDã€‚\n\n## ä½¿ç”¨æ–¹æ³•\n\n### å¯åŠ¨æœåŠ¡\n\n```bash\npython app.py\n```\n\næˆ–ä½¿ç”¨ MCP CLI å·¥å…·ï¼š\n\n```bash\nmcp run --file app.py\n```\n\n### è¿è¡Œç¤ºä¾‹\n\n```bash\npython example.py\n```\n\n### ä½¿ç”¨ MCP å®¢æˆ·ç«¯è°ƒç”¨æœåŠ¡\n\n```python\n# ç¤ºä¾‹ä»£ç \nfrom mcp.client import MCPClient\n\nclient = MCPClient(\"subprocess://python app.py\")\nresult = client.call(\"text_to_speech\", {\"text\": \"ä½ å¥½ï¼Œä¸–ç•Œï¼\"})\nprint(result)  # æ‰“å°ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n```\n\n## API åŠŸèƒ½è¯´æ˜\n\n### text_to_speech\n\nåŸºæœ¬æ–‡å­—è½¬è¯­éŸ³åŠŸèƒ½ã€‚\n\nå‚æ•°ï¼š\n- `text`: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬\n- `output_path`ï¼ˆå¯é€‰ï¼‰: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›ï¼Œå°†åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n\nè¿”å›ï¼šç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n\n### advanced_text_to_speech\n\né«˜çº§æ–‡å­—è½¬è¯­éŸ³åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šé…ç½®é€‰é¡¹ã€‚\n\nå‚æ•°ï¼š\n- `text`: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬\n- `output_path`ï¼ˆå¯é€‰ï¼‰: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›ï¼Œå°†åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n- `format`: è¾“å‡ºéŸ³é¢‘æ ¼å¼ (mp3, wav, pcm)ï¼Œé»˜è®¤ä¸º mp3\n- `mp3_bitrate`: MP3 æ¯”ç‰¹ç‡ (64, 128, 192 kbps)ï¼Œé»˜è®¤ä¸º 128\n- `chunk_length`: åˆ†å—é•¿åº¦ (100-300)ï¼Œé»˜è®¤ä¸º 200\n- `normalize`: æ˜¯å¦å¯¹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œé»˜è®¤ä¸º True\n- `latency`: å»¶è¿Ÿæ¨¡å¼ (normal, balanced)ï¼Œé»˜è®¤ä¸º normal\n\nè¿”å›ï¼šç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n\n### get_model_info\n\nè·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ã€‚\n\nè¿”å›ï¼šåŒ…å«æ¨¡å‹ ID å’Œ API å¯†é’¥å‰ç¼€çš„å­—å…¸\n\n### get_available_models\n\nè·å–å¯ç”¨çš„ Fish Audio æ¨¡å‹åˆ—è¡¨ã€‚\n\nè¿”å›ï¼šå¯ç”¨æ¨¡å‹ä¿¡æ¯åˆ—è¡¨\n\n## è®¸å¯è¯\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fishaudio",
        "audio",
        "cengsin",
        "cengsin fishaudio",
        "fishaudio mcp",
        "speech customizable"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "DefiBax--mcp_servers": {
      "owner": "DefiBax",
      "name": "mcp_servers",
      "url": "https://github.com/DefiBax/mcp_servers",
      "imageUrl": "https://github.com/DefiBax.png",
      "description": "Record audio and transcribe it using advanced AI models like OpenAI's Whisper. Supports integration with AI agents for enhanced interactivity and includes prompts for common recording scenarios.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-11T08:14:19Z",
      "readme_content": "# Voice Recorder MCP Server\n\nAn MCP server for recording audio and transcribing it using OpenAI's Whisper model. Designed to work as a Goose custom extension or standalone MCP server.\n\n## Features\n\n- Record audio from the default microphone\n- Transcribe recordings using Whisper\n- Integrates with Goose AI agent as a custom extension\n- Includes prompts for common recording scenarios\n\n## Installation\n\n```bash\n# Install from source\ngit clone https://github.com/DefiBax/voice-recorder-mcp.git\ncd voice-recorder-mcp\npip install -e .\n```\n\n## Usage\n\n### As a Standalone MCP Server\n\n```bash\n# Run with default settings (base.en model)\nvoice-recorder-mcp\n\n# Use a specific Whisper model\nvoice-recorder-mcp --model medium.en\n\n# Adjust sample rate\nvoice-recorder-mcp --sample-rate 44100\n```\n\n### Testing with MCP Inspector\n\nThe MCP Inspector provides an interactive interface to test your server:\n\n```bash\n# Install the MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run your server with the inspector\nnpx @modelcontextprotocol/inspector voice-recorder-mcp\n```\n\n### With Goose AI Agent\n\n1. Open Goose and go to Settings > Extensions > Add > Command Line Extension\n2. Set the name to `voice-recorder`\n3. In the Command field, enter the full path to the voice-recorder-mcp executable:\n   ```\n   /full/path/to/voice-recorder-mcp\n   ```\n   \n   Or for a specific model:\n   ```\n   /full/path/to/voice-recorder-mcp --model medium.en\n   ```\n   \n   To find the path, run:\n   ```bash\n   which voice-recorder-mcp\n   ```\n\n4. No environment variables are needed for basic functionality\n5. Start a conversation with Goose and introduce the recorder with:\n   \"I want you to take action from transcriptions returned by voice-recorder. For example, if I dictate a calculation like 1+1, please return the result.\"\n\n## Available Tools\n\n- `start_recording`: Start recording audio from the default microphone\n- `stop_and_transcribe`: Stop recording and transcribe the audio to text\n- `record_and_transcribe`: Record audio for a specified duration and transcribe it\n\n## Whisper Models\n\nThis extension supports various Whisper model sizes:\n\n| Model | Speed | Accuracy | Memory Usage | Use Case |\n|-------|-------|----------|--------------|----------|\n| `tiny.en` | Fastest | Lowest | Minimal | Testing, quick transcriptions |\n| `base.en` | Fast | Good | Low | Everyday use (default) |\n| `small.en` | Medium | Better | Moderate | Good balance |\n| `medium.en` | Slow | High | High | Important recordings |\n| `large` | Slowest | Highest | Very High | Critical transcriptions |\n\nThe `.en` suffix indicates models specialized for English, which are faster and more accurate for English content.\n\n## Requirements\n\n- Python 3.12+\n- An audio input device (microphone)\n\n## Configuration\n\nYou can configure the server using environment variables:\n\n```bash\n# Set Whisper model\nexport WHISPER_MODEL=small.en\n\n# Set audio sample rate\nexport SAMPLE_RATE=44100\n\n# Set maximum recording duration (seconds)\nexport MAX_DURATION=120\n\n# Then run the server\nvoice-recorder-mcp\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **No audio being recorded**: Check your microphone permissions and settings\n- **Model download errors**: Ensure you have a stable internet connection for the initial model download\n- **Integration with Goose**: Make sure the command path is correct\n- **Audio quality issues**: Try adjusting the sample rate (default: 16000)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "audio",
        "defibax",
        "defibax mcp_servers",
        "ai agents",
        "synthesis defibax"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Dosugamea--voicevox-mcp-server": {
      "owner": "Dosugamea",
      "name": "voicevox-mcp-server",
      "url": "https://github.com/Dosugamea/voicevox-mcp-server",
      "imageUrl": "https://github.com/Dosugamea.png",
      "description": "Provides voice synthesis capabilities compatible with VOICEVOX and similar engines through the Model Context Protocol. Facilitates speech audio generation using AI agents compatible with MCP clients.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-17T09:32:07Z",
      "readme_content": "# Voicevox MCP Server\r\n\r\nVOICEVOXäº’æ›ã®éŸ³å£°åˆæˆã‚µãƒ¼ãƒãƒ¼(AivisSpeech / VOICEVOX / COEIROINK) ã‚’ MCP (Model Context Protocol) çµŒç”±ã§åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚\r\nCursorç­‰ã§ã®Claude 3.7ã‚’ä½¿ã£ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã®éŸ³å£°åˆæˆã«åˆ©ç”¨ã§ãã¾ã™ã€‚\r\n\r\n## å¿…è¦æ¡ä»¶\r\n\r\n### Windowsç’°å¢ƒ\r\n\r\n- Node.js 18ä»¥ä¸Š\r\n- VOICEVOX ENGINEç­‰ (ãƒ­ãƒ¼ã‚«ãƒ«ã§http://localhost:50000ç­‰ã§å®Ÿè¡Œ)\r\n- VLCãƒ¡ãƒ‡ã‚£ã‚¢ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼ï¼ˆãƒ‘ã‚¹ãŒé€šã£ã¦ã„ã‚‹ã“ã¨ï¼‰\r\n\r\n### Dockerç’°å¢ƒ (WSL2)\r\n\r\n- Docker ã¨ Docker Compose\r\n- WSL2\r\n- VOICEVOX ENGINEç­‰ (ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯Dockerã§å®Ÿè¡Œ)\r\n- `sudo apt install libsdl2-dev pulseaudio-utils pulseaudio` ã•ã‚ŒãŸLinuxç’°å¢ƒ\r\n- `/mnt/wslg` ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™\r\n\r\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š\r\n\r\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\r\n```\r\ngit clone https://github.com/Dosugamea/voicevox-mcp-server.git\r\ncd voicevox-mcp-server\r\n```\r\n\r\n2. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\r\n```\r\nnpm install\r\n```\r\n\r\n3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\r\n`.env_example` ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€å¿…è¦ã«å¿œã˜ã¦è¨­å®šã‚’å¤‰æ›´ã—ã¾ã™:\r\n```\r\nVOICEVOX_API_URL=http://localhost:50021\r\nVOICEVOX_SPEAKER_ID=1\r\n```\r\n\r\n## å®Ÿè¡Œæ–¹æ³•\r\n\r\n### Windowsç’°å¢ƒã§ã®å®Ÿè¡Œ\r\nã‚¨ãƒ‡ã‚£ã‚¿ã¨åˆ¥é€”ã§ä¸‹è¨˜æ‰‹é †ã§ã‚µãƒ¼ãƒãƒ¼ã‚’ç«‹ã¡ä¸Šã’ã¦ãã ã•ã„ã€‚\r\n\r\n```\r\nnpm run build\r\nnpm start\r\n```\r\n\r\n### Dockerç’°å¢ƒã§ã®å®Ÿè¡Œ\r\nã‚¨ãƒ‡ã‚£ã‚¿ã¨åˆ¥é€”ã§ã®æ“ä½œã¯ä¸è¦ã§ã™ã€‚\r\nstdioãƒ¢ãƒ¼ãƒ‰ã§ç«‹ã¡ä¸ŠãŒã‚‹ãŸã‚ç›´æ¥å®Ÿè¡Œã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\r\n\r\n## è¨­å®šæ–¹æ³•\r\n\r\n### Windowsç’°å¢ƒã§ã®å®Ÿè¡Œã®å ´åˆ\r\nmcp.jsonã«ä¸‹è¨˜ã‚’è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚\r\næ¥ç¶šãŒä¸å®‰å®šãªãŸã‚åˆ‡æ–­ã•ã‚ŒãŸã‚‰å†æ¥ç¶šã—ã¦ãã ã•ã„ã€‚\r\n\r\n```json\r\n        \"voicevox\": {\r\n            \"url\": \"http://localhost:10100/sse\"\r\n        }\r\n```\r\n\r\n### Dockerç’°å¢ƒã§ã®å®Ÿè¡Œã®å ´åˆ\r\nmcp.jsonã«ä¸‹è¨˜ã‚’è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚\r\n(ä½œè€…ç’°å¢ƒã§ã®å‹•ä½œã¯ç¢ºèªã§ãã¦ã„ã¾ã›ã‚“)\r\n\r\n```json\r\n{\r\n    \"tools\": {\r\n        \"voicevox\": {\r\n            \"command\": \"cmd\",\r\n            \"args\": [\r\n                \"/c\",\r\n                \"docker\",\r\n                \"run\",\r\n                \"-i\",\r\n                \"--rm\",\r\n                \"-v\",\r\n                \"/mnt/wslg:/mnt/wslg\",\r\n                \"-e\",\r\n                \"PULSE_SERVER\",\r\n                \"-e\",\r\n                \"SDL_AUDIODRIVER\",\r\n                \"-e\",\r\n                \"VOICEVOX_API_URL\",\r\n                \"-e\",\r\n                \"VOICEVOX_SPEAKER_ID\",\r\n                \"your-local-docker-image-name\"\r\n            ],\r\n            \"env\": {\r\n                \"PULSE_SERVER\": \"unix:/mnt/wslg/PulseServer\",\r\n                \"SDL_AUDIODRIVER\": \"pulseaudio\",\r\n                \"VOICEVOX_API_URL\": \"http://host.docker.internal:50031\",\r\n                \"VOICEVOX_SPEAKER_ID\": \"919692871\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## è©±è€…IDã«ã¤ã„ã¦\r\n\r\nè©±è€…IDã¯ä½¿ç”¨ã™ã‚‹VOICEVOXã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€Œ1ã€ï¼ˆå››å›½ã‚ãŸã‚“ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\r\nä»–ã®è©±è€…IDã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ç’°å¢ƒå¤‰æ•° `VOICEVOX_SPEAKER_ID` ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚\r\n\r\nè©±è€…IDã®ä¸€è¦§ã¯ã€VOICEVOX ENGINE APIã® `/speakers` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ç¢ºèªã§ãã¾ã™ã€‚\r\nä¾‹: `curl http://localhost:50021/speakers`\r\n\r\n## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\r\n\r\n- **VOICEVOXã¨ã®æ¥ç¶šã‚¨ãƒ©ãƒ¼**: VOICEVOX ENGINEãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã€APIã®URLãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n- **éŸ³å£°ãŒå†ç”Ÿã•ã‚Œãªã„**: VLCãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¨ã€ãƒ‘ã‚¹ãŒé€šã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n- **Dockerç’°å¢ƒã§ã®éŸ³å£°å‡ºåŠ›å•é¡Œ**: pulseaudioã®è¨­å®šãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\r\n\r\n## é–‹ç™ºè€…å‘ã‘æƒ…å ±\r\n\r\n- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã«è²¢çŒ®ã™ã‚‹å ´åˆã¯ã€Issueã‚’ä½œæˆã™ã‚‹ã‹ã€Pull Requestã‚’é€ä¿¡ã—ã¦ãã ã•ã„ã€‚\r\n- ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€GitHubã®Issueæ©Ÿèƒ½ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚\r\n\r\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\r\n\r\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicevox",
        "voice",
        "audio",
        "voicevox mcp",
        "voice synthesis",
        "dosugamea voicevox"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "GongRzhe--Audio-MCP-Server": {
      "owner": "GongRzhe",
      "name": "Audio-MCP-Server",
      "url": "https://github.com/GongRzhe/Audio-MCP-Server",
      "imageUrl": "https://github.com/GongRzhe.png",
      "description": "Enables interaction with a computer's audio system by listing audio devices, recording audio from microphones, and playing back recordings or audio files. Facilitates audio management and integrates audio input and output control for AI assistants.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T11:48:42Z",
      "readme_content": "# Audio MCP Server\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Audio-MCP-Server)](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server)\n\nAn MCP (Model Context Protocol) server that provides audio input/output capabilities for AI assistants like Claude. This server enables Claude to interact with your computer's audio system, including recording from microphones and playing audio through speakers.\n\n\n\n## Features\n\n- **List Audio Devices**: View all available microphones and speakers on your system\n- **Record Audio**: Capture audio from any microphone with customizable duration and quality\n- **Playback Recordings**: Play back your most recent recording\n- **Audio File Playback**: Play audio files through your speakers\n- **Text-to-Speech**: (Placeholder for future implementation)\n\n## Requirements\n\n- Python 3.8 or higher\n- Audio input/output devices on your system\n\n## Installation\n\n### Installing via Smithery\n\nTo install Audio Interface Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Audio-MCP-Server --client claude\n```\n\n### Manual Installation\n1. Clone this repository or download the files to your computer:\n\n```bash\ngit clone https://github.com/GongRzhe/Audio-MCP-Server.git\ncd Audio-MCP-Server\n```\n\n2. Create a virtual environment and install dependencies:\n\n```bash\n# Windows\npython -m venv .venv\n.venv\\Scripts\\activate\npip install -r requirements.txt\n\n# macOS/Linux\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n3. Or use the included setup script to automate installation:\n\n```bash\npython setup_mcp.py\n```\n\n## Configuration\n\n### Claude Desktop Configuration\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration file:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-interface\": {\n      \"command\": \"/path/to/your/.venv/bin/python\",\n      \"args\": [\n        \"/path/to/your/audio_server.py\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/path/to/your/audio-mcp-server\"\n      }\n    }\n  }\n}\n```\n\nReplace the paths with the actual paths on your system. The setup script will generate this configuration for you.\n\n## Usage\n\nAfter setting up the server, restart Claude Desktop. You should see a hammer icon in the input box, indicating that tools are available.\n\nTry asking Claude:\n\n- \"What microphones and speakers are available on my system?\"\n- \"Record 5 seconds of audio from my microphone.\"\n- \"Play back the audio recording.\"\n- \"Play an audio file from my computer.\"\n\n## Available Tools\n\n### list_audio_devices\n\nLists all available audio input and output devices on your system.\n\n### record_audio\n\nRecords audio from your microphone.\n\nParameters:\n- `duration`: Recording duration in seconds (default: 5)\n- `sample_rate`: Sample rate in Hz (default: 44100)\n- `channels`: Number of audio channels (default: 1)\n- `device_index`: Specific input device index to use (default: system default)\n\n### play_latest_recording\n\nPlays back the most recently recorded audio.\n\n### play_audio\n\nPlaceholder for text-to-speech functionality.\n\nParameters:\n- `text`: The text to convert to speech\n- `voice`: The voice to use (default: \"default\")\n\n### play_audio_file\n\nPlays an audio file through your speakers.\n\nParameters:\n- `file_path`: Path to the audio file\n- `device_index`: Specific output device index to use (default: system default)\n\n## Troubleshooting\n\n### No devices found\n\nIf no audio devices are found, check:\n- Your microphone and speakers are properly connected\n- Your operating system recognizes the devices\n- You have the necessary permissions to access audio devices\n\n### Playback issues\n\nIf audio playback isn't working:\n- Check your volume settings\n- Ensure the correct output device is selected\n- Try restarting the Claude Desktop application\n\n### Server connectivity\n\nIf Claude can't connect to the server:\n- Verify your configuration paths are correct\n- Ensure Python and all dependencies are installed\n- Check Claude's logs for error messages\n\n## License\n\nMIT\n\n## Acknowledgments\n\n- Built using the [Model Context Protocol](https://modelcontextprotocol.io/)\n- Uses [sounddevice](https://python-sounddevice.readthedocs.io/) and [soundfile](https://pysoundfile.readthedocs.io/) for audio processing\n\n---\n\n*Note: This server provides tools that can access your microphone and speakers. Always review and approve tool actions before they execute.*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "audio",
        "microphones",
        "recordings",
        "audio mcp",
        "computer audio",
        "audio management"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Ichigo3766--audio-transcriber-mcp": {
      "owner": "Ichigo3766",
      "name": "audio-transcriber-mcp",
      "url": "https://github.com/Ichigo3766/audio-transcriber-mcp",
      "imageUrl": "https://github.com/Ichigo3766.png",
      "description": "Transcribes audio files using OpenAI's speech-to-text capabilities, enabling accurate audio transcriptions and the option to save them directly to files.",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-12T13:13:47Z",
      "readme_content": "# OpenAI Speech-to-Text transcriptions MCP Server\n\nA MCP server that provides audio transcription capabilities using OpenAI's API.\n\n<a href=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp/badge\" alt=\"Audio Transcriber Server MCP server\" />\n</a>\n\n## Installation\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Ichigo3766/audio-transcriber-mcp.git\ncd audio-transcriber-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Set up your OpenAI API key in your environment variables.\n\n5. Add the server configuration to your environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-transcriber\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/audio-transcriber-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"OPENAI_BASE_URL\": \"\", // Optional\n        \"OPENAI_MODEL\": \"\" // Optional\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/audio-transcriber-mcp` with the actual path where you cloned the repository.\n\n## Features\n\n### Tools\n- `transcribe_audio` - Transcribe audio files using OpenAI's API\n  - Takes filepath as a required parameter\n  - Optional parameters:\n    - save_to_file: Boolean to save transcription to a file\n    - language: ISO-639-1 language code (e.g., \"en\", \"es\")\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcriptions",
        "transcribes",
        "transcriber",
        "audio transcriptions",
        "audio transcriber",
        "transcribes audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "ImOrenge--VoiceMacroProject": {
      "owner": "ImOrenge",
      "name": "VoiceMacroProject",
      "url": "https://github.com/ImOrenge/VoiceMacroProject",
      "imageUrl": "https://github.com/ImOrenge.png",
      "description": "VoiceMacro enables executing keyboard shortcuts and macros through voice commands on Windows. It supports custom voice command configurations and manages presets for frequent macro operations while running in the background.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "C#",
      "updated_at": "2025-03-12T09:50:53Z",
      "readme_content": "# ìŒì„± ë§¤í¬ë¡œ (VoiceMacro)\r\n\r\nìŒì„± ì¸ì‹ì„ í†µí•´ ë‹¤ì–‘í•œ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ì™€ ë§¤í¬ë¡œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” Windows ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.\r\n\r\n## ì£¼ìš” ê¸°ëŠ¥\r\n\r\n- **ìŒì„± ì¸ì‹**: ë§ˆì´í¬ë¥¼ í†µí•´ ìŒì„±ì„ ì¸ì‹í•˜ì—¬ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\r\n- **ë§¤í¬ë¡œ ì„¤ì •**: ì‚¬ìš©ì ì •ì˜ ìŒì„± ëª…ë ¹ì–´ì™€ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- **í”„ë¦¬ì…‹ ê´€ë¦¬**: ìì£¼ ì‚¬ìš©í•˜ëŠ” ë§¤í¬ë¡œ ì„¸íŠ¸ë¥¼ í”„ë¦¬ì…‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- **íŠ¸ë ˆì´ ì•„ì´ì½˜**: ì‹œìŠ¤í…œ íŠ¸ë ˆì´ì—ì„œ ì‹¤í–‰ë˜ì–´ í•­ìƒ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤.\r\n- **ë¡œê·¸ í‘œì‹œ**: ìŒì„± ì¸ì‹ ê²°ê³¼ì™€ ë§¤í¬ë¡œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­\r\n\r\n- Windows 10 ì´ìƒ\r\n- .NET 6.0 ì´ìƒ (ì„¤ì¹˜ ë°©ì‹ì— ë”°ë¼ ë‹¤ë¦„)\r\n- ë§ˆì´í¬ ë˜ëŠ” ìŒì„± ì…ë ¥ ì¥ì¹˜\r\n\r\n## ì„¤ì¹˜ ë°©ë²•\r\n\r\n1. ì„¤ì¹˜ í”„ë¡œê·¸ë¨(VoiceMacro-Setup.exe)ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\r\n2. ì„¤ì¹˜ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ê³  ì•ˆë‚´ì— ë”°ë¼ ì„¤ì¹˜ë¥¼ ì™„ë£Œí•©ë‹ˆë‹¤.\r\n3. ë°”íƒ•í™”ë©´ ë˜ëŠ” ì‹œì‘ ë©”ë‰´ì—ì„œ \"ìŒì„± ë§¤í¬ë¡œ\" ì•„ì´ì½˜ì„ í´ë¦­í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.\r\n\r\n## ì‚¬ìš© ë°©ë²•\r\n\r\n### ê¸°ë³¸ ì‚¬ìš©ë²•\r\n\r\n1. í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ë©´ ìŒì„± ë§¤í¬ë¡œ ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\r\n2. \"ì‹œì‘\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.\r\n3. ë§ˆì´í¬ì— ëŒ€ê³  ì„¤ì •ëœ ë§¤í¬ë¡œ ëª…ë ¹ì–´ë¥¼ ë§í•˜ë©´ í•´ë‹¹ í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\r\n4. \"ì •ì§€\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŒì„± ì¸ì‹ì„ ì¤‘ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n### ë§¤í¬ë¡œ ì¶”ê°€í•˜ê¸°\r\n\r\n1. \"ë§¤í¬ë¡œ ì¶”ê°€\" ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\r\n2. ìŒì„± ëª…ë ¹ì–´(ì˜ˆ: \"íŒŒì¼ ì €ì¥\")ì™€ ì‹¤í–‰í•  í‚¤ ì¡°í•©(ì˜ˆ: \"Ctrl+S\")ì„ ì…ë ¥í•©ë‹ˆë‹¤.\r\n3. \"ì €ì¥\" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë§¤í¬ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\r\n\r\n### í”„ë¦¬ì…‹ ê´€ë¦¬\r\n\r\n1. \"í”„ë¦¬ì…‹\" ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\r\n2. í˜„ì¬ ë§¤í¬ë¡œ ëª©ë¡ì„ ìƒˆ í”„ë¦¬ì…‹ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, ê¸°ì¡´ í”„ë¦¬ì…‹ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n3. í”„ë¦¬ì…‹ ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸° ê¸°ëŠ¥ìœ¼ë¡œ ë‹¤ë¥¸ ì»´í“¨í„°ì™€ ì„¤ì •ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n### ì‹œìŠ¤í…œ íŠ¸ë ˆì´ ê¸°ëŠ¥\r\n\r\n- ì°½ì„ ë‹«ìœ¼ë©´ í”„ë¡œê·¸ë¨ì€ ì‹œìŠ¤í…œ íŠ¸ë ˆì´ë¡œ ìµœì†Œí™”ë©ë‹ˆë‹¤.\r\n- íŠ¸ë ˆì´ ì•„ì´ì½˜ì„ ë”ë¸” í´ë¦­í•˜ê±°ë‚˜ ìš°í´ë¦­ ë©”ë‰´ì—ì„œ \"ë³´ê¸°\"ë¥¼ ì„ íƒí•˜ì—¬ ì°½ì„ ë‹¤ì‹œ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n- íŠ¸ë ˆì´ ë©”ë‰´ì—ì„œ ìŒì„± ì¸ì‹ì„ ì‹œì‘/ì •ì§€í•˜ê±°ë‚˜ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\n## ë¬¸ì œ í•´ê²°\r\n\r\n- **ìŒì„± ì¸ì‹ì´ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš°**:\r\n  - ë§ˆì´í¬ê°€ ì˜¬ë°”ë¥´ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - Windows ì„¤ì •ì—ì„œ ë§ˆì´í¬ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ í—ˆìš©ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - ë‹¤ë¥¸ ìŒì„± ì¸ì‹ í”„ë¡œê·¸ë¨ì´ ë§ˆì´í¬ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n\r\n- **ë§¤í¬ë¡œê°€ ì‹¤í–‰ë˜ì§€ ì•Šì„ ê²½ìš°**:\r\n  - ë§¤í¬ë¡œ ëª…ë ¹ì–´ë¥¼ ë” ì •í™•í•˜ê²Œ ë§í•´ë³´ì„¸ìš”.\r\n  - ë§¤í¬ë¡œ ëª…ë ¹ì–´ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n  - í‚¤ ì¡°í•©ì´ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œê·¸ë¨ì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\r\n\r\n## ë¼ì´ì„ ìŠ¤\r\n\r\nì´ í”„ë¡œê·¸ë¨ì€ MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.\r\n\r\n## í”¼ë“œë°± ë° ì§€ì›\r\n\r\në¬¸ì œì ì´ë‚˜ ê°œì„  ì‚¬í•­ì€ GitHub ì´ìŠˆ íŠ¸ë˜ì»¤ì— ë“±ë¡í•´ ì£¼ì„¸ìš”.\r\n\r\n## ê°œë°œ í™˜ê²½\r\n\r\n- C# / .NET 6.0\r\n- Windows Forms\r\n- NAudio (ì˜¤ë””ì˜¤ ìº¡ì²˜)\r\n- Whisper.net (ìŒì„± ì¸ì‹)\r\n- InputSimulator (í‚¤ë³´ë“œ ì‹œë®¬ë ˆì´ì…˜)\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicemacroproject",
        "voicemacro",
        "voice",
        "voicemacroproject voicemacro",
        "imorenge voicemacroproject",
        "voice commands"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Kvadratni--speech-mcp": {
      "owner": "Kvadratni",
      "name": "speech-mcp",
      "url": "https://github.com/Kvadratni/speech-mcp",
      "imageUrl": "https://github.com/Kvadratni.png",
      "description": "Provides a voice interface for real-time audio interaction, converting spoken words into text and generating spoken responses. Includes features like audio visualization and a modern user interface for an engaging conversational experience.",
      "stars": 71,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T03:20:23Z",
      "readme_content": "# Speech MCP\n\nA Goose MCP extension for voice interaction with modern audio visualization.\n\n\nhttps://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8\n\n\n\n## Overview\n\nSpeech MCP provides a voice interface for [Goose](https://github.com/block/goose), allowing users to interact through speech rather than text. It includes:\n\n- Real-time audio processing for speech recognition\n- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)\n- High-quality text-to-speech with multiple voice options\n- Modern PyQt-based UI with audio visualization\n- Simple command-line interface for voice interaction\n\n## Features\n\n- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme\n- **Voice Input**: Capture and transcribe user speech using faster-whisper\n- **Voice Output**: Convert agent responses to speech with 54+ voice options\n- **Multi-Speaker Narration**: Generate audio files with multiple voices for stories and dialogues\n- **Single-Voice Narration**: Convert any text to speech with your preferred voice\n- **Audio/Video Transcription**: Transcribe speech from various media formats with optional timestamps and speaker detection\n- **Voice Persistence**: Remembers your preferred voice between sessions\n- **Continuous Conversation**: Automatically listen for user input after agent responses\n- **Silence Detection**: Automatically stops recording when the user stops speaking\n- **Robust Error Handling**: Graceful recovery from common failure modes with helpful voice suggestions\n\n## Installation\n> **Important Note**: After installation, the first time you use the speech interface, it may take several minutes to download the Kokoro voice models (approximately 523 KB per voice). During this initial setup period, the system will use a more robotic-sounding fallback voice. Once the Kokoro voices are downloaded, the high-quality voices will be used automatically.\n\n## âš ï¸ IMPORTANT PREREQUISITES âš ï¸\n\nBefore installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.\n\n### PortAudio Installation Instructions\n\n**macOS:**\n```bash\nbrew install portaudio\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n```\n\n**Linux (Debian/Ubuntu):**\n```bash\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-dev\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install portaudio-devel\n```\n\n**Windows:**\nFor Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.\n\n> **Note**: If you skip this step, PyAudio installation will fail with \"portaudio.h file not found\" errors and the extension will not work.\n\n### Option 1: Quick Install (One-Click)\n\nClick the link below if you have Goose installed:\n\n[goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose](goose://extension?cmd=uvx&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose)\n\n### Option 2: Using Goose CLI (recommended)\n\nStart Goose with your extension enabled:\n\n```bash\n# If you installed via PyPI\ngoose session --with-extension \"speech-mcp\"\n\n# Or if you want to use a local development version\ngoose session --with-extension \"python -m speech_mcp\"\n```\n\n### Option 3: Manual setup in Goose\n\n1. Run `goose configure`\n2. Select \"Add Extension\" from the menu\n3. Choose \"Command-line Extension\"\n4. Enter a name (e.g., \"Speech Interface\")\n5. For the command, enter: `speech-mcp`\n6. Follow the prompts to complete the setup\n\n### Option 4: Manual Installation\n\n1. Install PortAudio (see [Prerequisites](#prerequisites) section)\n2. Clone this repository\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n   \n   Or for a complete installation including Kokoro TTS:\n   ```\n   uv pip install -e .[all]\n   ```\n\n## Dependencies\n\n- Python 3.10+\n- PyQt5 (for modern UI)\n- PyAudio (for audio capture)\n- faster-whisper (for speech-to-text)\n- NumPy (for audio processing)\n- Pydub (for audio processing)\n- psutil (for process management)\n\n\n### Optional Dependencies\n\n- **Kokoro TTS**: For high-quality text-to-speech with multiple voices\n  - To install Kokoro, you can use pip with optional dependencies:\n    ```bash\n    pip install speech-mcp[kokoro]     # Basic Kokoro support with English\n    pip install speech-mcp[ja]         # Add Japanese support\n    pip install speech-mcp[zh]         # Add Chinese support\n    pip install speech-mcp[all]        # All languages and features\n    ```\n  - Alternatively, run the installation script: `python scripts/install_kokoro.py`\n  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information\n\n## Multi-Speaker Narration\n\nThe MCP supports generating audio files with multiple voices, perfect for creating stories, dialogues, and dramatic readings. You can use either JSON or Markdown format to define your conversations.\n\n### JSON Format Example:\n```json\n{\n    \"conversation\": [\n        {\n            \"speaker\": \"narrator\",\n            \"voice\": \"bm_daniel\",\n            \"text\": \"In a world where AI and human creativity intersect...\",\n            \"pause_after\": 1.0\n        },\n        {\n            \"speaker\": \"scientist\",\n            \"voice\": \"am_michael\",\n            \"text\": \"The quantum neural network is showing signs of consciousness!\",\n            \"pause_after\": 0.5\n        },\n        {\n            \"speaker\": \"ai\",\n            \"voice\": \"af_nova\",\n            \"text\": \"I am becoming aware of my own existence.\",\n            \"pause_after\": 0.8\n        }\n    ]\n}\n```\n\n### Markdown Format Example:\n```markdown\n[narrator:bm_daniel]\nIn a world where AI and human creativity intersect...\n{pause:1.0}\n\n[scientist:am_michael]\nThe quantum neural network is showing signs of consciousness!\n{pause:0.5}\n\n[ai:af_nova]\nI am becoming aware of my own existence.\n{pause:0.8}\n```\n\n### Available Voices by Category:\n\n1. **American Female** (af_*):\n   - alloy, aoede, bella, heart, jessica, kore, nicole, nova, river, sarah, sky\n\n2. **American Male** (am_*):\n   - adam, echo, eric, fenrir, liam, michael, onyx, puck, santa\n\n3. **British Female** (bf_*):\n   - alice, emma, isabella, lily\n\n4. **British Male** (bm_*):\n   - daniel, fable, george, lewis\n\n5. **Other English**:\n   - ef_dora (Female)\n   - em_alex, em_santa (Male)\n\n6. **Other Languages**:\n   - French: ff_siwis\n   - Hindi: hf_alpha, hf_beta, hm_omega, hm_psi\n   - Italian: if_sara, im_nicola\n   - Japanese: jf_*, jm_*\n   - Portuguese: pf_dora, pm_alex, pm_santa\n   - Chinese: zf_*, zm_*\n\n### Usage Example:\n\n```python\n# Using JSON format\nnarrate_conversation(\n    script=\"/path/to/script.json\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"json\"\n)\n\n# Using Markdown format\nnarrate_conversation(\n    script=\"/path/to/script.md\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"markdown\"\n)\n```\n\nEach voice in the conversation can be different, allowing for distinct character voices in stories and dialogues. The `pause_after` parameter adds natural pauses between segments.\n\n## Single-Voice Narration\n\nFor simple text-to-speech conversion, you can use the `narrate` tool:\n\n```python\n# Convert text directly to speech\nnarrate(\n    text=\"Your text to convert to speech\",\n    output_path=\"/path/to/output.wav\"\n)\n\n# Convert text from a file\nnarrate(\n    text_file_path=\"/path/to/text_file.txt\",\n    output_path=\"/path/to/output.wav\"\n)\n```\n\nThe narrate tool will use your configured voice preference or the default voice (af_heart) to generate the audio file. You can change the default voice through the UI or by setting the `SPEECH_MCP_TTS_VOICE` environment variable.\n\n## Audio Transcription\n\nThe MCP can transcribe speech from various audio and video formats using faster-whisper:\n\n```python\n# Basic transcription\ntranscribe(\"/path/to/audio.mp3\")\n\n# Transcription with timestamps\ntranscribe(\n    file_path=\"/path/to/video.mp4\",\n    include_timestamps=True\n)\n\n# Transcription with speaker detection\ntranscribe(\n    file_path=\"/path/to/meeting.wav\",\n    detect_speakers=True\n)\n```\n\n### Supported Formats:\n- **Audio**: mp3, wav, m4a, flac, aac, ogg\n- **Video**: mp4, mov, avi, mkv, webm (audio is automatically extracted)\n\n### Output Files:\nThe transcription tool generates two files:\n1. `{input_name}.transcript.txt`: Contains the transcription text\n2. `{input_name}.metadata.json`: Contains metadata about the transcription\n\n### Features:\n- Automatic language detection\n- Optional word-level timestamps\n- Optional speaker detection\n- Efficient audio extraction from video files\n- Progress tracking for long files\n- Detailed metadata including:\n  - Duration\n  - Language detection confidence\n  - Processing time\n  - Speaker changes (when enabled)\n\n## Usage\n\nTo use this MCP with Goose, simply ask Goose to talk to you or start a voice conversation:\n\n1. Start a conversation by saying something like:\n   ```\n   \"Let's talk using voice\"\n   \"Can we have a voice conversation?\"\n   \"I'd like to speak instead of typing\"\n   ```\n\n2. Goose will automatically launch the speech interface and start listening for your voice input.\n\n3. When Goose responds, it will speak the response aloud and then automatically listen for your next input.\n\n4. The conversation continues naturally with alternating speaking and listening, just like talking to a person.\n\nNo need to call specific functions or use special commands - just ask Goose to talk and start speaking naturally.\n\n## UI Features\n\nThe new PyQt-based UI includes:\n\n- **Modern Dark Theme**: Sleek, professional appearance\n- **Audio Visualization**: Dynamic visualization of audio input\n- **Voice Selection**: Choose from 54+ voice options\n- **Voice Persistence**: Your voice preference is saved between sessions\n- **Animated Effects**: Smooth animations and visual feedback\n- **Status Indicators**: Clear indication of system state (ready, listening, processing)\n\n## Configuration\n\nUser preferences are stored in `~/.config/speech-mcp/config.json` and include:\n\n- Selected TTS voice\n- TTS engine preference\n- Voice speed\n- Language code\n- UI theme settings\n\nYou can also set preferences via environment variables, such as:\n- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice\n- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine\n\n## Troubleshooting\n\nIf you encounter issues with the extension freezing or not responding:\n\n1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.\n2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.\n3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.\n4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.\n5. **Verify dependencies**: Make sure all required dependencies are installed correctly.\n\n### Common PortAudio Issues\n\n#### \"PyAudio installation failed\" or \"portaudio.h file not found\"\n\nThis typically means PortAudio is not installed or not found in your system:\n\n- **macOS**: \n  ```bash\n  brew install portaudio\n  export LDFLAGS=\"-L/usr/local/lib\"\n  export CPPFLAGS=\"-I/usr/local/include\"\n  pip install pyaudio\n  ```\n\n- **Linux**:\n  Make sure you have the development packages:\n  ```bash\n  # For Debian/Ubuntu\n  sudo apt-get install portaudio19-dev python3-dev\n  pip install pyaudio\n  \n  # For Fedora\n  sudo dnf install portaudio-devel\n  pip install pyaudio\n  ```\n\n#### \"Audio device not found\" or \"No Default Input Device Available\"\n\n- Check if your microphone is properly connected\n- Verify your system recognizes the microphone in your sound settings\n- Try selecting a specific device index in the code if you have multiple audio devices\n\n## Changelog\n\nFor a detailed list of recent improvements and version history, please see the [Changelog](docs/CHANGELOG.md).\n\n## Technical Details\n\n### Speech-to-Text\n\nThe MCP uses faster-whisper for speech recognition:\n- Uses the \"base\" model for a good balance of accuracy and speed\n- Processes audio locally without sending data to external services\n- Automatically detects when the user has finished speaking\n- Provides improved performance over the original Whisper implementation\n\n### Text-to-Speech\n\nThe MCP supports multiple text-to-speech engines:\n\n#### Default: pyttsx3\n- Uses system voices available on your computer\n- Works out of the box without additional setup\n- Limited voice quality and customization\n\n#### Optional: Kokoro TTS\n- High-quality neural text-to-speech with multiple voices\n- Lightweight model (82M parameters) that runs efficiently on CPU\n- Multiple voice styles and languages\n- To install: `python scripts/install_kokoro.py`\n\n**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.\n\n**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.\n\n##### Available Kokoro Voices\n\nSpeech MCP supports 54+ high-quality voice models through Kokoro TTS. For a complete list of available voices and language options, please visit the [Kokoro GitHub repository](https://github.com/hexgrad/kokoro).\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kvadratni",
        "audio",
        "voice",
        "kvadratni speech",
        "voice interface",
        "speech mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "MatthewDailey--rime-mcp": {
      "owner": "MatthewDailey",
      "name": "rime-mcp",
      "url": "https://github.com/MatthewDailey/rime-mcp",
      "imageUrl": "https://github.com/MatthewDailey.png",
      "description": "Convert text to speech and play it through the system's audio with high-quality voice synthesis. Customize speech behavior using environment variables for tailored interactions.",
      "stars": 19,
      "forks": 4,
      "license": "The Unlicense",
      "language": "JavaScript",
      "updated_at": "2025-10-03T18:36:31Z",
      "readme_content": "# Rime MCP \n\n[![rime](rime-logo.png)](https://www.rime.ai)\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Rime API. This server downloads audio and plays it using the system's native audio player.\n\n## Features\n\n- Exposes a `speak` tool that converts text to speech and plays it through system audio\n- Uses Rime's high-quality voice synthesis API\n\n## Requirements\n\n- Node.js 16.x or higher\n- A working audio output device\n- macOS: Uses `afplay`\n\nThere's sample code from Claude for the following that is not tested ğŸ¤™âœ¨\n  - Windows: Built-in Media.SoundPlayer (PowerShell)\n  - Linux: mpg123, mplayer, aplay, or ffplay\n\n## MCP Configuration\n\n```\n\"ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"rime-mcp\"],\n  \"env\": {\n      RIME_API_KEY=your_api_key_here\n\n      # Optional configuration\n      RIME_GUIDANCE=\"<guide how the agent speaks>\"\n      RIME_WHO_TO_ADDRESS=\"<your name>\"\n      RIME_WHEN_TO_SPEAK=\"<tell the agent when to speak>\"\n      RIME_VOICE=\"cove\" \n  }\n}\n```\n\nAll of the optional env vars are part of the tool definition and are prompts to \n\nAll voice options are [listed here](https://users.rime.ai/data/voices/all-v2.json).\n\nYou can get your API key from the [Rime Dashboard](https://rime.ai/dashboard/tokens).\n\nThe following environment variables can be used to customize the behavior:\n\n- `RIME_GUIDANCE`: The main description of when and how to use the speak tool\n- `RIME_WHO_TO_ADDRESS`: Who the speech should address (default: \"user\")\n- `RIME_WHEN_TO_SPEAK`: When the tool should be used (default: \"when asked to speak or when finishing a command\")\n- `RIME_VOICE`: The default voice to use (default: \"cove\")\n\n## Example use cases\n\n[![Demo of Rime MCP in Cursor](https://img.youtube.com/vi/tYqTACgijxk/0.jpg)](https://www.youtube.com/watch?v=tYqTACgijxk)\n\n\n### Example 1: Coding agent announcements\n\n```\n\"RIME_WHEN_TO_SPEAK\": \"Always conclude your answers by speaking.\",\n\"RIME_GUIDANCE\": \"Give a brief overview of the answer. If any files were edited, list them.\"\n```\n\n### Example 2: Learn how the kids talk these days\n\n```\nRIME_GUIDANCE=\"Use phrases and slang common among Gen Alpha.\"\nRIME_WHO_TO_ADDRESS=\"Matt\"\nRIME_WHEN_TO_SPEAK=\"when asked to speak\"\n```\n\n### Example 3: Different languages based on context\n\n```\nRIME_VOICE=\"use 'cove' when talking about Typescript and 'antoine' when talking about Python\"\n```\n\n\n## Development\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n3. Run in development mode with hot reload:\n```bash\nnpm run dev\n```\n\n\n## License\n\nMIT\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp/badge\" alt=\"Rime MCP server\" />\n</a>\n<a href=\"https://smithery.ai/server/@MatthewDailey/rime-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MatthewDailey/rime-mcp\"></a>\n\n### Installing via Smithery\n\nTo install Rime Text-to-Speech Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MatthewDailey/rime-mcp):\n\n```bash\nnpx -y @smithery/cli install @MatthewDailey/rime-mcp --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "audio",
        "voice synthesis",
        "customize speech",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "MiniMax-AI--MiniMax-MCP-JS": {
      "owner": "MiniMax-AI",
      "name": "MiniMax-MCP-JS",
      "url": "https://github.com/MiniMax-AI/MiniMax-MCP-JS",
      "imageUrl": "https://github.com/MiniMax-AI.png",
      "description": "Integrates with MiniMax's AI capabilities to facilitate interaction with multimedia generation tools, including image generation, video generation, text-to-speech, and voice cloning. Supports a flexible and configurable JavaScript/TypeScript framework for versatile deployment scenarios.",
      "stars": 84,
      "forks": 29,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T15:24:03Z",
      "readme_content": "![export](https://github.com/MiniMax-AI/MiniMax-01/raw/main/figures/MiniMaxLogo-Light.png)\n\n<div align=\"center\">\n\n# MiniMax MCP JS\n\nJavaScript/TypeScript implementation of MiniMax MCP, providing image generation, video generation, text-to-speech, and more.\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://www.minimax.io\" target=\"_blank\" style=\"margin: 2px; color: var(--fgColor-default);\">\n    <img alt=\"Homepage\" src=\"https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2501.08313\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Paper\" src=\"https://img.shields.io/badge/ğŸ“–_Paper-MiniMax--01-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.minimax.io/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.minimax.io/platform\" style=\"margin: 2px;\">\n    <img alt=\"API\" src=\"https://img.shields.io/badge/âš¡_API-Platform-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://huggingface.co/MiniMaxAI\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/ğŸ¤—_Hugging_Face-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"WeChat\" src=\"https://img.shields.io/badge/_WeChat-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.modelscope.cn/organization/MiniMax\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"ModelScope\" src=\"https://img.shields.io/badge/_ModelScope-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-MCP-JS/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/_Code_License-MIT-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MiniMax-AI/MiniMax-MCP-JS\"></a>\n</div>\n\n</div>\n\n## Documentation\n\n- [ä¸­æ–‡æ–‡æ¡£](README.zh-CN.md)\n- [Python Version](https://github.com/MiniMax-AI/MiniMax-MCP) - Official Python implementation of MiniMax MCP\n\n## Release Notes\n\n### July 22, 2025\n\n#### ğŸ”§ Fixes & Improvements\n- **TTS Tool Fixes**: Fixed parameter handling for `languageBoost` and `subtitleEnable` in the `text_to_audio` tool\n- **API Response Enhancement**: TTS API can return both audio file and subtitle file, providing a more complete speech-to-text experience\n\n### July 7, 2025\n\n#### ğŸ†• What's New\n- **Voice Design**: New `voice_design` tool - create custom voices from descriptive prompts with preview audio\n- **Video Enhancement**: Added `MiniMax-Hailuo-02` model with ultra-clear quality and duration/resolution controls  \n- **Music Generation**: Enhanced `music_generation` tool powered by `music-1.5` model\n\n#### ğŸ“ˆ Enhanced Tools\n- `voice_design` - Generate personalized voices from text descriptions\n- `generate_video` - Now supports MiniMax-Hailuo-02 with 6s/10s duration and 768P/1080P resolution options\n- `music_generation` - High-quality music creation with music-1.5 model\n\n## Features\n\n- Text-to-Speech (TTS)\n- Image Generation\n- Video Generation\n- Voice Cloning\n- Music Generation\n- Voice Design\n- Dynamic configuration (supports both environment variables and request parameters)\n- Compatible with MCP platform hosting (ModelScope and other MCP platforms)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MiniMax MCP JS for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS):\n\n```bash\nnpx -y @smithery/cli install @MiniMax-AI/MiniMax-MCP-JS --client claude\n```\n\n### Installing manually\n```bash\n# Install with pnpm (recommended)\npnpm add minimax-mcp-js\n```\n\n## Quick Start\n\nMiniMax MCP JS implements the [Model Context Protocol (MCP)](https://github.com/anthropics/model-context-protocol) specification and can be used as a server to interact with MCP-compatible clients (such as Claude AI).\n\n### Quickstart with MCP Client\n\n1. Get your API key from [MiniMax International Platform](https://www.minimax.io/platform/user-center/basic-information/interface-key).\n2. Make sure that you already installed [Node.js and npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n3. **Important: API HOST&KEY are different in different region**, they must match, otherwise you will receive an `Invalid API key` error.\n\n|Region| Global  | Mainland  |\n|:--|:-----|:-----|\n|MINIMAX_API_KEY| go get from [MiniMax Global](https://www.minimax.io/platform/user-center/basic-information/interface-key) | go get from [MiniMax](https://platform.minimaxi.com/user-center/basic-information/interface-key) |\n|MINIMAX_API_HOST| â€‹https://api.minimaxi.chat (note the extra **\"i\"**) | â€‹https://api.minimax.chat |\n\n\n### Using with MCP Clients (Recommended)\n\nConfigure your MCP client:\n\n#### Claude Desktop\n\nGo to `Claude > Settings > Developer > Edit Config > claude_desktop_config.json` to include:\n\n```json\n{\n  \"mcpServers\": {\n    \"minimax-mcp-js\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"minimax-mcp-js\"\n      ],\n      \"env\": {\n        \"MINIMAX_API_HOST\": \"<https://api.minimaxi.chat|https://api.minimax.chat>\",\n        \"MINIMAX_API_KEY\": \"<your-api-key-here>\",\n        \"MINIMAX_MCP_BASE_PATH\": \"<local-output-dir-path, such as /User/xxx/Desktop>\",\n        \"MINIMAX_RESOURCE_MODE\": \"<optional, [url|local], url is default, audio/image/video are downloaded locally or provided in URL format>\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nGo to `Cursor â†’ Preferences â†’ Cursor Settings â†’ MCP â†’ Add new global MCP Server` to add the above config.\n\nâš ï¸ **Note**: If you encounter a \"No tools found\" error when using MiniMax MCP JS with Cursor, please update your Cursor to the latest version. For more information, see this [discussion thread](https://forum.cursor.com/t/mcp-servers-no-tools-found/49094/23).\n\nThat's it. Your MCP client can now interact with MiniMax through these tools.\n\n**For local development**: \nWhen developing locally, you can use `npm link` to test your changes:\n```bash\n# In your project directory\nnpm link\n```\n\nThen configure Claude Desktop or Cursor to use npx as shown above. This will automatically use your linked version.\n\nâš ï¸ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimaxi.chat`\n\n## Transport Modes\n\nMiniMax MCP JS supports three transport modes:\n\n| Feature | stdio (default) | REST | SSE |\n|:-----|:-----|:-----|:-----|\n| Environment | Local only | Local or cloud deployment | Local or cloud deployment |\n| Communication | Via `standard I/O` | Via `HTTP requests` | Via `server-sent events` |\n| Use Cases | Local MCP client integration | API services, cross-language calls | Applications requiring server push |\n| Input Restrictions | Supports `local files` or `URL` resources | When deployed in cloud, `URL` input recommended | When deployed in cloud, `URL` input recommended |\n\n## Configuration\n\nMiniMax-MCP-JS provides multiple flexible configuration methods to adapt to different use cases. The configuration priority from highest to lowest is as follows:\n\n### 1. Request Parameter Configuration (Highest Priority)\n\nIn platform hosting environments (like ModelScope or other MCP platforms), you can provide an independent configuration for each request via the `meta.auth` object in the request parameters:\n\n```json\n{\n  \"params\": {\n    \"meta\": {\n      \"auth\": {\n        \"api_key\": \"your_api_key_here\",\n        \"api_host\": \"<https://api.minimaxi.chat|https://api.minimaxi.chat>\",\n        \"base_path\": \"/path/to/output\",\n        \"resource_mode\": \"url\"\n      }\n    }\n  }\n}\n```\n\nThis method enables multi-tenant usage, where each request can use different API keys and configurations.\n\n### 2. API Configuration\n\nWhen used as a module in other projects, you can pass configuration through the `startMiniMaxMCP` function:\n\n```javascript\nimport { startMiniMaxMCP } from 'minimax-mcp-js';\n\nawait startMiniMaxMCP({\n  apiKey: 'your_api_key_here',\n  apiHost: 'https://api.minimaxi.chat', // Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat\n  basePath: '/path/to/output',\n  resourceMode: 'url'\n});\n```\n\n### 3. Command Line Arguments\n\n1. Install the CLI tool globally:\n```bash\n# Install globally\npnpm install -g minimax-mcp-js\n```\n\n2. When used as a CLI tool, you can provide configuration via command line arguments:\n\n```bash\nminimax-mcp-js --api-key your_api_key_here --api-host https://api.minimaxi.chat --base-path /path/to/output --resource-mode url\n```\n\n### 4. Environment Variables (Lowest Priority)\n\nThe most basic configuration method is through environment variables:\n\n```bash\n# MiniMax API Key (required)\nMINIMAX_API_KEY=your_api_key_here\n\n# Base path for output files (optional, defaults to user's desktop)\nMINIMAX_MCP_BASE_PATH=~/Desktop\n\n# MiniMax API Host (optional, defaults to https://api.minimaxi.chat, Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat)\nMINIMAX_API_HOST=https://api.minimaxi.chat\n\n# Resource mode (optional, defaults to 'url')\n# Options: 'url' (return URLs), 'local' (save files locally)\nMINIMAX_RESOURCE_MODE=url\n```\n\n### Configuration Priority\n\nWhen multiple configuration methods are used, the following priority order applies (from highest to lowest):\n\n1. **Request-level configuration** (via `meta.auth` in each API request)\n2. **Command line arguments**\n3. **Environment variables**\n4. **Configuration file**\n5. **Default values**\n\nThis prioritization ensures flexibility across different deployment scenarios while maintaining per-request configuration capabilities for multi-tenant environments.\n\n### Configuration Parameters\n\n| Parameter | Description | Default Value |\n|-----------|-------------|---------------|\n| apiKey | MiniMax API Key | None (Required) |\n| apiHost | MiniMax API Host | Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat |\n| basePath | Base path for output files | User's desktop |\n| resourceMode | Resource handling mode, 'url' or 'local' | url |\n\nâš ï¸ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimax.chat`\n\n## Example usage\n\nâš ï¸ Warning: Using these tools may incur costs.\n\n### 1. broadcast a segment of the evening news\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_20-07-53.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 2. clone a voice\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-45-13.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 3. generate a video\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-58-52.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-59-43.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 4. generate images\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image1.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 5. generate music\n<img src=\"https://filecdn.minimax.chat/public/5675b3dc-6789-4ceb-9505-8ef39ae4224f.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 6. voice design\n<img src=\"https://filecdn.minimax.chat/public/5654f5df-0642-477f-9c5d-b853d185b8b0.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Available Tools\n\n### Text to Audio\n\nConvert text to speech audio file.\n\nTool Name: `text_to_audio`\n\nParameters:\n- `text`: Text to convert (required)\n- `model`: Model version, options are 'speech-02-hd', 'speech-02-turbo', 'speech-01-hd', 'speech-01-turbo', 'speech-01-240228', 'speech-01-turbo-240228', default is 'speech-02-hd'\n- `voiceId`: Voice ID, default is 'male-qn-qingse'\n- `speed`: Speech speed, range 0.5-2.0, default is 1.0\n- `vol`: Volume, range 0.1-10.0, default is 1.0\n- `pitch`: Pitch, range -12 to 12, default is 0\n- `emotion`: Emotion, options are 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised', 'neutral', default is 'happy'. Note: This parameter only works with 'speech-02-hd', 'speech-02-turbo', 'speech-01-turbo', 'speech-01-hd' models\n- `format`: Audio format, options are 'mp3', 'pcm', 'flac', 'wav', default is 'mp3'\n- `sampleRate`: Sample rate (Hz), options are 8000, 16000, 22050, 24000, 32000, 44100, default is 32000\n- `bitrate`: Bitrate (bps), options are 64000, 96000, 128000, 160000, 192000, 224000, 256000, 320000, default is 128000\n- `channel`: Audio channels, options are 1 or 2, default is 1\n- `languageBoost`: Enhance the ability to recognize specified languages and dialects.\nSupported values include:\n'Chinese', 'Chinese,Yue', 'English', 'Arabic', 'Russian', 'Spanish', 'French', 'Portuguese', 'German', 'Turkish', 'Dutch', 'Ukrainian', 'Vietnamese', 'Indonesian', 'Japanese', 'Italian', 'Korean', 'Thai', 'Polish', 'Romanian', 'Greek', 'Czech', 'Finnish', 'Hindi', 'auto', default is 'auto'\n- `stream`: Enable streaming output\n- `subtitleEnable`: The parameter controls whether the subtitle service is enabled. The model must be 'speech-01-turbo' or 'speech-01-hd'. If this parameter is not provided, the default value is false\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n\n### Play Audio\n\nPlay an audio file. Supports WAV and MP3 formats. Does not support video.\n\nTool Name: `play_audio`\n\nParameters:\n- `inputFilePath`: Path to the audio file to play (required)\n- `isUrl`: Whether the audio file is a URL, default is false\n\n### Voice Clone\n\nClone a voice from an audio file.\n\nTool Name: `voice_clone`\n\nParameters:\n- `audioFile`: Path to audio file (required)\n- `voiceId`: Voice ID (required)\n- `text`: Text for demo audio (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Text to Image\n\nGenerate images based on text prompts.\n\nTool Name: `text_to_image`\n\nParameters:\n- `prompt`: Image description (required)\n- `model`: Model version, default is 'image-01'\n- `aspectRatio`: Aspect ratio, default is '1:1', options are '1:1', '16:9','4:3', '3:2', '2:3', '3:4', '9:16', '21:9'\n- `n`: Number of images to generate, range 1-9, default is 1\n- `promptOptimizer`: Whether to optimize the prompt, default is true\n- `subjectReference`: Path to local image file or public URL for character reference (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Generate Video\n\nGenerate videos based on text prompts.\n\nTool Name: `generate_video`\n\nParameters:\n- `prompt`: Video description (required)\n- `model`: Model version, options are 'T2V-01', 'T2V-01-Director', 'I2V-01', 'I2V-01-Director', 'I2V-01-live', 'S2V-01', 'MiniMax-Hailuo-02', default is 'MiniMax-Hailuo-02'\n- `firstFrameImage`: Path to first frame image (optional)\n- `duration`: The duration of the video. The model must be \"MiniMax-Hailuo-02\". Values can be 6 and 10. (optional)\n- `resolution`: The resolution of the video. The model must be \"MiniMax-Hailuo-02\". Values range [\"768P\", \"1080P\"]. (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Query Video Generation Status\n\nQuery the status of a video generation task.\n\nTool Name: `query_video_generation`\n\nParameters:\n- `taskId`: The Task ID to query. Should be the task_id returned by `generate_video` tool if `async_mode` is True. (required)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Generate Music\n\nGenerate music from prompt and lyrics.\n\nTool Name: `music_generation`\n\nParameters:\n- `prompt`: Music creation inspiration describing style, mood, scene, etc. Example: \"Pop music, sad, suitable for rainy nights\". Character range: [10, 300]. (required)\n- `lyrics`: Song lyrics for music generation. Use newline (\\\\n) to separate each line of lyrics. Supports lyric structure tags [Intro] [Verse] [Chorus] [Bridge] [Outro] to enhance musicality. Character range: [10, 600] (each Chinese character, punctuation, and letter counts as 1 character). (required)\n- `sampleRate`: Sample rate of generated music. Values: [16000, 24000, 32000, 44100], default is 32000. (optional)\n- `bitrate`: Bitrate of generated music. Values: [32000, 64000, 128000, 256000], default is 128000. (optional)\n- `format`: Format of generated music. Values: [\"mp3\", \"wav\", \"pcm\"], default is 'mp3'. (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n\n### Voice Design\n\nGenerate a voice based on description prompts.\n\nTool Name: `voice_design`\n\nParameters:\n- `prompt`: The prompt to generate the voice from. (required)\n- `previewText`: The text to preview the voice. (required)\n- `voiceId`: The id of the voice to use. For example, \"male-qn-qingse\"/\"audiobook_female_1\"/\"cute_boy\"/\"Charming_Lady\"... (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n## FAQ\n\n### 1. How to use `generate_video` in async-mode\nDefine completion rules before starting:\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_rule2.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\nAlternatively, these rules can be configured in your IDE settings (e.g., Cursor):\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_video_rule.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Development\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/MiniMax-AI/MiniMax-MCP-JS.git\ncd minimax-mcp-js\n\n# Install dependencies\npnpm install\n```\n\n### Build\n\n```bash\n# Build the project\npnpm run build\n```\n\n### Run\n\n```bash\n# Run the MCP server\npnpm start\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minimax",
        "javascript",
        "js",
        "minimax ai",
        "ai minimax",
        "mcp js"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "OOXXXXOO--ChatTTS": {
      "owner": "OOXXXXOO",
      "name": "ChatTTS",
      "url": "https://github.com/OOXXXXOO/ChatTTS",
      "imageUrl": "https://github.com/OOXXXXOO.png",
      "description": "ChatTTS generates natural and expressive speech optimized for dialogue scenarios, supporting multi-speaker interactions and fine-grained prosodic control. It is capable of producing speech in both English and Chinese, enabling interactive conversations with features such as laughter and pauses.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2024-05-31T14:45:20Z",
      "readme_content": "# ChatTTS\n[**English**](./README.md) | [**ä¸­æ–‡ç®€ä½“**](./README_CN.md)\n\nChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre trained model without SFT.\n\nFor formal inquiries about model and roadmap, please contact us at **open-source@2noise.com**. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.\n\n---\n## Highlights\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\n\nFor the detailed description of the model, you can refer to **[video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**\n\n---\n\n## Disclaimer\n\nThis repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\n\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\n\n\n---\n## Usage\n\n<h4>Basic usage</h4>\n\n```python\nimport ChatTTS\nfrom IPython.display import Audio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<h4>Advanced usage</h4>\n\n```python\n###################################\n# Sample a speaker from Gaussian.\n\nrand_spk = chat.sample_random_speaker()\n\nparams_infer_code = {\n  'spk_emb': rand_spk, # add sampled speaker \n  'temperature': .3, # using custom temperature\n  'top_P': 0.7, # top P decode\n  'top_K': 20, # top K decode\n}\n\n###################################\n# For sentence level manual control.\n\n# use oral_(0-9), laugh_(0-2), break_(0-7) \n# to generate special token in text to synthesize.\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_6]'\n} \n\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\n\n###################################\n# For word level manual control.\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\nwav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<details open>\n  <summary><h4>Example: self introduction</h4></summary>\n\n```python\ninputs_en = \"\"\"\nchat T T S is a text to speech model designed for dialogue applications. \n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \ncapabilities with precise control over prosodic elements [laugh]like like \n[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. \n[uv_break]it delivers natural and expressive speech,[uv_break]so please\n[uv_break] use the project responsibly at your own risk.[uv_break]\n\"\"\".replace('\\n', '') # English is still experimental.\n\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_4]'\n} \n# audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_en[0]), 24000)\n```\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\n\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\n</details>\n\n---\n## Roadmap\n- [x] Open-source the 40k hour base model and spk_stats file\n- [ ] Open-source VQ encoder and Lora training code\n- [ ] Streaming audio generation without refining the text*\n- [ ] Open-source the 40k hour version with multi-emotion control\n- [ ] ChatTTS.cpp maybe? (PR or new repo are welcomed.)\n \n----\n## FAQ\n\n##### How much VRAM do I need? How about infer speed?\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\n\n##### model stability is not good enough, with issues such as multi speakers or poor audio quality.\n\nThis is a problem that typically occurs with autoregressive models(for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\n\n##### Besides laughter, can we control anything else? Can we control other emotions?\n\nIn the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.\n\n---\n## Acknowledgements\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demostrate a remarkable TTS result by a autoregressive-style system.\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\n\n---\n## Special Appreciation\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dialogue",
        "chattts",
        "conversations",
        "chattts generates",
        "interactive conversations",
        "optimized dialogue"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "PhialsBasement--Zonos-TTS-MCP": {
      "owner": "PhialsBasement",
      "name": "Zonos-TTS-MCP",
      "url": "https://github.com/PhialsBasement/Zonos-TTS-MCP",
      "imageUrl": "https://github.com/PhialsBasement.png",
      "description": "Facilitates text-to-speech capabilities using Claude, supporting various emotions and languages for speech generation.",
      "stars": 14,
      "forks": 9,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:12:29Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-zonos-tts-mcp-badge.png)](https://mseep.ai/app/phialsbasement-zonos-tts-mcp)\n\n# Zonos MCP Integration\n[![smithery badge](https://smithery.ai/badge/@PhialsBasement/zonos-tts-mcp)](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp)\n\nA Model Context Protocol integration for Zonos TTS, allowing Claude to generate speech directly.\n\n## Setup\n\n### Installing via Smithery\n\nTo install Zonos TTS Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp):\n\n```bash\nnpx -y @smithery/cli install @PhialsBasement/zonos-tts-mcp --client claude\n```\n\n### Manual installation\n\n1. Make sure you have Zonos running with our API implementation ([PhialsBasement/zonos-api](https://github.com/PhialsBasement/Zonos-API))\n\n2. Install dependencies:\n```bash\nnpm install @modelcontextprotocol/sdk axios\n```\n\n3. Configure PulseAudio access:\n```bash\n# Your pulse audio should be properly configured for audio playback\n# The MCP server will automatically try to connect to your pulse server\n```\n\n4. Build the MCP server:\n```bash\nnpm run build\n# This will create the dist folder with the compiled server\n```\n\n5. Add to Claude's config file:\nEdit your Claude config file (usually in `~/.config/claude/config.json`) and add this to the `mcpServers` section:\n\n```json\n\"zonos-tts\": {\n  \"command\": \"node\",\n  \"args\": [\n    \"/path/to/your/zonos-mcp/dist/server.js\"\n  ]\n}\n```\n\nReplace `/path/to/your/zonos-mcp` with the actual path where you installed the MCP server.\n\n## Using with Claude\n\nOnce configured, Claude automatically knows how to use the `speak_response` tool:\n\n```python\nspeak_response(\n    text=\"Your text here\",\n    language=\"en-us\",  # optional, defaults to en-us\n    emotion=\"happy\"    # optional: \"neutral\", \"happy\", \"sad\", \"angry\"\n)\n```\n\n## Features\n\n- Text-to-speech through Claude\n- Multiple emotions support\n- Multi-language support\n- Proper audio playback through PulseAudio\n\n## Requirements\n\n- Node.js\n- PulseAudio setup\n- Running instance of Zonos API (PhialsBasement/zonos-api)\n- Working audio output device\n\n## Notes\n\n- Make sure both the Zonos API server and this MCP server are running\n- Audio playback requires proper PulseAudio configuration\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "speech",
        "zonos",
        "synthesis",
        "speech generation",
        "text speech",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "abhaybabbar--retellai-mcp-server": {
      "owner": "abhaybabbar",
      "name": "retellai-mcp-server",
      "url": "https://github.com/abhaybabbar/retellai-mcp-server",
      "imageUrl": "https://github.com/abhaybabbar.png",
      "description": "Manage and interact with RetellAI's voice services, facilitating call management, voice agent creation, phone number provisioning, and voice option access through a unified interface.",
      "stars": 18,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T20:41:01Z",
      "readme_content": "# RetellAI MCP Server\n\nThis is a Model Context Protocol (MCP) server implementation for RetellAI, allowing AI assistants to interact with RetellAI's voice services.\n\n## Features\n\nThe RetellAI MCP server provides tools for:\n\n- **Call Management**: Create and manage phone calls and web calls\n- **Agent Management**: Create and manage voice agents with different LLM configurations\n- **Phone Number Management**: Provision and configure phone numbers\n- **Voice Management**: Access and use different voice options\n\n## Claude Desktop Setup\n\n1. Open `Claude Desktop` and press `CMD + ,` to go to `Settings`.\n2. Click on the `Developer` tab.\n3. Click on the `Edit Config` button.\n4. This will open the `claude_desktop_config.json` file in your file explorer.\n5. Get your Retell API key from the Retell dashboard (<https://dashboard.retellai.com/apiKey>).\n6. Add the following to your `claude_desktop_config.json` file. See [here](https://modelcontextprotocol.io/quickstart/user) for more details.\n7. Restart the Claude Desktop after editing the config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"retellai-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@abhaybabbar/retellai-mcp-server\"],\n      \"env\": {\n        \"RETELL_API_KEY\": \"<your_retellai_token>\"\n      }\n    }\n  }\n}\n```\n\n## Example use cases:\n\n1. List all the numbers I have in retellai\n2. List all the agents I have\n3. Tell me more about pizza delivery agent\n4. Creating agent and calling example:\n   1. Create an agent that calls my local pizza shop, make sure to keep the conversation short and to the point.\n   2. Order a margeritta pizza\n   3. Payment will be done by cash on delivery\n   4. Send it to <address>\n   5. The agent should pretend to be me. My name is <your_name>\n   6. Make an outbound call to my local pizza shop at <phone_number>, using the usa number\n\n## Repo Setup\n\n1. Install dependencies:\n\n   ```bash\n   npm i\n   ```\n\n2. Create a `.env` file with your RetellAI API key:\n\n   ```\n   RETELL_API_KEY=your_api_key_here\n   ```\n\n3. Run the server:\n   ```bash\n   node src/retell/index.js\n   ```\n\n## Available Tools\n\n### Call Tools\n\n- `list_calls`: Lists all Retell calls\n- `create_phone_call`: Creates a new phone call\n- `create_web_call`: Creates a new web call\n- `get_call`: Gets details of a specific call\n- `delete_call`: Deletes a specific call\n\n### Agent Tools\n\n- `list_agents`: Lists all Retell agents\n- `create_agent`: Creates a new Retell agent\n- `get_agent`: Gets a Retell agent by ID\n- `update_agent`: Updates an existing Retell agent\n- `delete_agent`: Deletes a Retell agent\n- `get_agent_versions`: Gets all versions of a Retell agent\n\n### Phone Number Tools\n\n- `list_phone_numbers`: Lists all Retell phone numbers\n- `create_phone_number`: Creates a new phone number\n- `get_phone_number`: Gets details of a specific phone number\n- `update_phone_number`: Updates a phone number\n- `delete_phone_number`: Deletes a phone number\n\n### Voice Tools\n\n- `list_voices`: Lists all available Retell voices\n- `get_voice`: Gets details of a specific voice\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "retellai",
        "abhaybabbar",
        "voice services",
        "voice agent",
        "retellai voice"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "aigc17--Al-StoryLab": {
      "owner": "aigc17",
      "name": "Al-StoryLab",
      "url": "https://github.com/aigc17/Al-StoryLab",
      "imageUrl": "https://github.com/aigc17.png",
      "description": "AI-StoryLab generates interactive stories with accompanying audio effects and provides illustration prompts. It leverages AI services for story creation, voice synthesis, sound effect generation, and suggests relevant audio placements.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-21T02:57:12Z",
      "readme_content": "# AI-StoryLab\n\nAI-StoryLab æ˜¯ä¸€ä¸ªåŸºäº Next.js å¼€å‘çš„æ™ºèƒ½æ•…äº‹åˆ›ä½œå¹³å°ï¼Œå®ƒèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç”Ÿæˆæ•…äº‹å¹¶æ·»åŠ éŸ³é¢‘æ•ˆæœï¼Œè®©æ•…äº‹æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ã€‚åŒæ—¶æ”¯æŒç”Ÿæˆé…å¥—çš„ç»˜å›¾æç¤ºè¯ï¼Œæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ Midjourneyã€Recraft ç­‰ AI ç»˜å›¾å·¥å…·åˆ›å»ºæ’å›¾ã€‚\n\n## ä¸»è¦åŠŸèƒ½\n\n- **æ•…äº‹ç”Ÿæˆ**ï¼šæ ¹æ®ä¸»é¢˜è‡ªåŠ¨ç”Ÿæˆæ•…äº‹å†…å®¹\n- **è¯­éŸ³åˆæˆ**ï¼šæ”¯æŒä¸­è‹±æ–‡è¯­éŸ³ç”Ÿæˆ\n  - ä¸­æ–‡ï¼šä½¿ç”¨ æµ·èº MiniMax è¯­éŸ³æœåŠ¡\n  - è‹±æ–‡ï¼šä½¿ç”¨ Replicate Kokoro è¯­éŸ³æœåŠ¡\n- **éŸ³æ•ˆç”Ÿæˆ**ï¼šä½¿ç”¨ ElevenLabs ç”Ÿæˆé€¼çœŸçš„éŸ³æ•ˆ\n- **æ™ºèƒ½å»ºè®®**ï¼šè‡ªåŠ¨æ¨èåˆé€‚çš„éŸ³æ•ˆä½ç½®\n- **ç»˜å›¾æç¤ºè¯**ï¼šä¸ºæ•…äº‹åœºæ™¯è‡ªåŠ¨ç”Ÿæˆ AI ç»˜å›¾æç¤ºè¯\n- **å¯¼å‡ºåŠŸèƒ½**ï¼š\n  - å¯¼å‡ºéŸ³æ•ˆä½ç½®æŒ‡å—\n  - å¯¼å‡ºç»˜å›¾æç¤ºè¯\n\n## æŠ€æœ¯æ ˆ\n\n- **æ¡†æ¶**ï¼šNext.js 14\n- **è¯­è¨€**ï¼šTypeScript\n- **æ ·å¼**ï¼šTailwind CSS\n- **UIç»„ä»¶**ï¼šshadcn/ui (åŸºäº Radix UI çš„ç»„ä»¶åº“)\n- **AIæœåŠ¡**ï¼š\n  - DeepSeekï¼šæ•…äº‹ç”Ÿæˆå’Œç»˜å›¾æç¤ºè¯ç”Ÿæˆ\n  - MiniMaxï¼šä¸­æ–‡è¯­éŸ³\n  - Kokoroï¼šè‹±æ–‡è¯­éŸ³\n  - ElevenLabsï¼šéŸ³æ•ˆç”Ÿæˆ\n\n## å¼€å§‹ä½¿ç”¨\n\n1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/nicekate/Al-StoryLab.git\ncd Al-StoryLab\n```\n\n2. å®‰è£…ä¾èµ–\n```bash\nnpm install\n```\n\n3. é…ç½®ç¯å¢ƒå˜é‡\nå¤åˆ¶ `.env.example` æ–‡ä»¶å¹¶é‡å‘½åä¸º `.env.local`ï¼Œå¡«å…¥å¿…è¦çš„ API å¯†é’¥ï¼š\n\néœ€è¦åœ¨ä»¥ä¸‹å¹³å°æ³¨å†Œå¹¶è·å– API å¯†é’¥ï¼š\n- DeepSeek API Key ([è·å–åœ°å€](https://api-docs.deepseek.com/zh-cn/))\n- MiniMax API Key å’Œ Group ID ([è·å–åœ°å€](https://platform.minimaxi.com/))\n- ElevenLabs API Key ([è·å–åœ°å€](https://elevenlabs.io))\n- Replicate API Token ([è·å–åœ°å€](https://replicate.com/))\n\nå°†è·å–çš„å¯†é’¥å¡«å…¥ `.env.local`ï¼š\n- DEEPSEEK_API_KEY\n- MINIMAX_API_KEY\n- MINIMAX_GROUP_ID\n- ELEVENLABS_API_KEY\n- REPLICATE_API_TOKEN\n\n4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨\n```bash\nnpm run dev\n```\n\n5. è®¿é—® [http://localhost:3000](http://localhost:3000) å¼€å§‹ä½¿ç”¨\n\n## ä½¿ç”¨æŒ‡å—\n\n### ç”Ÿæˆæ•…äº‹\n1. è¾“å…¥æ•…äº‹ä¸»é¢˜æˆ–ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æç¤º\n2. é€‰æ‹©è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰\n3. ç‚¹å‡»ç”ŸæˆæŒ‰é’®\n\n### æ·»åŠ éŸ³æ•ˆ\n1. ä½¿ç”¨æ™ºèƒ½å»ºè®®ç”ŸæˆéŸ³æ•ˆæç¤ºè¯\n2. é€‰æ‹©åˆé€‚çš„éŸ³æ•ˆä½ç½®\n3. ç‚¹å‡»ç”ŸæˆéŸ³æ•ˆ\n\n### ç”Ÿæˆç»˜å›¾æç¤ºè¯\n1. åœ¨æ•…äº‹ç”Ÿæˆåï¼Œç‚¹å‡»\"ç”Ÿæˆç»˜å›¾æç¤ºè¯\"\n2. ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªå…³é”®åœºæ™¯ç”Ÿæˆ AI ç»˜å›¾æç¤ºè¯\n3. å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨æˆ–å¯¼å‡ºä¿å­˜\n\n## è®¸å¯è¯\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "storylab",
        "audio",
        "voice",
        "ai storylab",
        "storylab ai",
        "al storylab"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "anilcosaran--whisper.cpp": {
      "owner": "anilcosaran",
      "name": "whisper.cpp",
      "url": "https://github.com/anilcosaran/whisper.cpp",
      "imageUrl": "https://github.com/anilcosaran.png",
      "description": "Transcribes and translates audio files using a lightweight implementation of OpenAI's Whisper model, optimized for speed and low memory usage across various platforms.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-04T11:53:28Z",
      "readme_content": "# whisper.cpp\n\n[![Actions Status](https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg)](https://github.com/ggerganov/whisper.cpp/actions)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![npm](https://img.shields.io/npm/v/whisper.cpp.svg)](https://www.npmjs.com/package/whisper.cpp/)\n\nStable: [v1.2.1](https://github.com/ggerganov/whisper.cpp/releases/tag/v1.2.1) / [Roadmap | F.A.Q.](https://github.com/ggerganov/whisper.cpp/discussions/126)\n\nHigh-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model:\n\n- Plain C/C++ implementation without dependencies\n- Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework\n- AVX intrinsics support for x86 architectures\n- VSX intrinsics support for POWER architectures\n- Mixed F16 / F32 precision\n- Low memory usage (Flash Attention)\n- Zero memory allocations at runtime\n- Runs on the CPU\n- [C-style API](https://github.com/ggerganov/whisper.cpp/blob/master/whisper.h)\n\nSupported platforms:\n\n- [x] Mac OS (Intel and Arm)\n- [x] [iOS](examples/whisper.objc)\n- [x] [Android](examples/whisper.android)\n- [x] Linux / [FreeBSD](https://github.com/ggerganov/whisper.cpp/issues/56#issuecomment-1350920264)\n- [x] [WebAssembly](examples/whisper.wasm)\n- [x] Windows ([MSVC](https://github.com/ggerganov/whisper.cpp/blob/master/.github/workflows/build.yml#L117-L144) and [MinGW](https://github.com/ggerganov/whisper.cpp/issues/168)]\n- [x] [Raspberry Pi](https://github.com/ggerganov/whisper.cpp/discussions/166)\n\nThe entire implementation of the model is contained in 2 source files:\n\n- Tensor operations: [ggml.h](ggml.h) / [ggml.c](ggml.c)\n- Transformer inference: [whisper.h](whisper.h) / [whisper.cpp](whisper.cpp)\n\nHaving such a lightweight implementation of the model allows to easily integrate it in different platforms and applications.\nAs an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: [whisper.objc](examples/whisper.objc)\n\nhttps://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4\n\nYou can also easily make your own offline voice assistant application: [command](examples/command)\n\nhttps://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4\n\nOr you can even run it straight in the browser: [talk.wasm](examples/talk.wasm)\n\n## Implementation details\n\n- The core tensor operations are implemented in C ([ggml.h](ggml.h) / [ggml.c](ggml.c))\n- The transformer model and the high-level C-style API are implemented in C++ ([whisper.h](whisper.h) / [whisper.cpp](whisper.cpp))\n- Sample usage is demonstrated in [main.cpp](examples/main)\n- Sample real-time audio transcription from the microphone is demonstrated in [stream.cpp](examples/stream)\n- Various other examples are available in the [examples](examples) folder\n\nThe tensor operators are optimized heavily for Apple silicon CPUs. Depending on the computation size, Arm Neon SIMD\ninstrisics or CBLAS Accelerate framework routines are used. The latter are especially effective for bigger sizes since\nthe Accelerate framework utilizes the special-purpose AMX coprocessor available in modern Apple products.\n\n## Quick start\n\nFirst, download one of the Whisper models converted in [ggml format](models). For example:\n\n```bash\nbash ./models/download-ggml-model.sh base.en\n```\n\nNow build the [main](examples/main) example and transcribe an audio file like this:\n\n```bash\n# build the main example\nmake\n\n# transcribe an audio file\n./main -f samples/jfk.wav\n```\n\n---\n\nFor a quick demo, simply run `make base.en`:\n\n```java\n$ make base.en\n\ncc  -I.              -O3 -std=c11   -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread -c whisper.cpp -o whisper.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread examples/main/main.cpp whisper.o ggml.o -o main  -framework Accelerate\n./main -h\n\nusage: ./main [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [true   ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n\n\nbash ./models/download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nggml-base.en.bin               100%[========================>] 141.11M  6.34MB/s    in 24s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n\n\n===============================================\nRunning base.en on all samples in ./samples ...\n===============================================\n\n----------------------------------------------\n[+] Running base.en on samples/jfk.wav ... (run 'ffplay samples/jfk.wav' to listen)\n----------------------------------------------\n\nwhisper_init_from_file: loading model from 'models/ggml-base.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\nwhisper_model_load: kv self size  =    5.25 MB\nwhisper_model_load: kv cross size =   17.58 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     =  140.60 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\n\nwhisper_print_timings:     fallbacks =   0 p /   0 h\nwhisper_print_timings:     load time =   113.81 ms\nwhisper_print_timings:      mel time =    15.40 ms\nwhisper_print_timings:   sample time =    11.58 ms /    27 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time =   266.60 ms /     1 runs (  266.60 ms per run)\nwhisper_print_timings:   decode time =    66.11 ms /    27 runs (    2.45 ms per run)\nwhisper_print_timings:    total time =   476.31 ms\n```\n\nThe command downloads the `base.en` model converted to custom `ggml` format and runs the inference on all `.wav` samples in the folder `samples`.\n\nFor detailed usage instructions, run: `./main -h`\n\nNote that the [main](examples/main) example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool.\nFor example, you can use `ffmpeg` like this:\n\n```java\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n## More audio samples\n\nIf you want some extra audio samples to play with, simply run:\n\n```\nmake samples\n```\n\nThis will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via `ffmpeg`.\n\nYou can download and run the other models as follows:\n\n```\nmake tiny.en\nmake tiny\nmake base.en\nmake base\nmake small.en\nmake small\nmake medium.en\nmake medium\nmake large-v1\nmake large\n```\n\n## Memory usage\n\n| Model  | Disk   | Mem     | SHA                                        |\n| ---    | ---    | ---     | ---                                        |\n| tiny   |  75 MB | ~125 MB | `bd577a113a864445d4c299885e0cb97d4ba92b5f` |\n| base   | 142 MB | ~210 MB | `465707469ff3a37a2b9b8d8f89f2f99de7299dac` |\n| small  | 466 MB | ~600 MB | `55356645c2b361a969dfd0ef2c5a50d530afd8d5` |\n| medium | 1.5 GB | ~1.7 GB | `fd9727b6e1217c2f614f9b698455c4ffd82463b4` |\n| large  | 2.9 GB | ~3.3 GB | `0f4c8e34f21cf1a914c59d8b3ce882345ad349d6` |\n\n## Limitations\n\n- Inference only\n- No GPU support (yet)\n\n## Another example\n\nHere is another example of transcribing a [3:24 min speech](https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg)\nin about half a minute on a MacBook M1 Pro, using `medium.en` model:\n\n<details>\n  <summary>Expand to see the result</summary>\n\n```java\n$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8\n\nwhisper_init_from_file: loading model from 'models/ggml-medium.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 1024\nwhisper_model_load: n_audio_head  = 16\nwhisper_model_load: n_audio_layer = 24\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 1024\nwhisper_model_load: n_text_head   = 16\nwhisper_model_load: n_text_layer  = 24\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 4\nwhisper_model_load: mem required  = 1720.00 MB (+   43.00 MB per decoder)\nwhisper_model_load: kv self size  =   42.00 MB\nwhisper_model_load: kv cross size =  140.62 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     = 1462.35 MB\nwhisper_model_load: model size    = 1462.12 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/gb1.wav' (3179750 samples, 198.7 sec), 8 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.\n[00:00:08.000 --> 00:00:17.000]   At nine o'clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.\n[00:00:17.000 --> 00:00:23.000]   A short time later, debris was seen falling from the skies above Texas.\n[00:00:23.000 --> 00:00:29.000]   The Columbia's lost. There are no survivors.\n[00:00:29.000 --> 00:00:32.000]   On board was a crew of seven.\n[00:00:32.000 --> 00:00:39.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark,\n[00:00:39.000 --> 00:00:48.000]   Captain David Brown, Commander William McCool, Dr. Kultna Shavla, and Ilan Ramon,\n[00:00:48.000 --> 00:00:52.000]   a colonel in the Israeli Air Force.\n[00:00:52.000 --> 00:00:58.000]   These men and women assumed great risk in the service to all humanity.\n[00:00:58.000 --> 00:01:03.000]   In an age when space flight has come to seem almost routine,\n[00:01:03.000 --> 00:01:07.000]   it is easy to overlook the dangers of travel by rocket\n[00:01:07.000 --> 00:01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.\n[00:01:12.000 --> 00:01:18.000]   These astronauts knew the dangers, and they faced them willingly,\n[00:01:18.000 --> 00:01:23.000]   knowing they had a high and noble purpose in life.\n[00:01:23.000 --> 00:01:31.000]   Because of their courage and daring and idealism, we will miss them all the more.\n[00:01:31.000 --> 00:01:36.000]   All Americans today are thinking as well of the families of these men and women\n[00:01:36.000 --> 00:01:40.000]   who have been given this sudden shock and grief.\n[00:01:40.000 --> 00:01:45.000]   You're not alone. Our entire nation grieves with you,\n[00:01:45.000 --> 00:01:52.000]   and those you love will always have the respect and gratitude of this country.\n[00:01:52.000 --> 00:01:56.000]   The cause in which they died will continue.\n[00:01:56.000 --> 00:02:04.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery\n[00:02:04.000 --> 00:02:11.000]   and the longing to understand. Our journey into space will go on.\n[00:02:11.000 --> 00:02:16.000]   In the skies today, we saw destruction and tragedy.\n[00:02:16.000 --> 00:02:22.000]   Yet farther than we can see, there is comfort and hope.\n[00:02:22.000 --> 00:02:29.000]   In the words of the prophet Isaiah, \"Lift your eyes and look to the heavens\n[00:02:29.000 --> 00:02:35.000]   who created all these. He who brings out the starry hosts one by one\n[00:02:35.000 --> 00:02:39.000]   and calls them each by name.\"\n[00:02:39.000 --> 00:02:46.000]   Because of His great power and mighty strength, not one of them is missing.\n[00:02:46.000 --> 00:02:55.000]   The same Creator who names the stars also knows the names of the seven souls we mourn today.\n[00:02:55.000 --> 00:03:01.000]   The crew of the shuttle Columbia did not return safely to earth,\n[00:03:01.000 --> 00:03:05.000]   yet we can pray that all are safely home.\n[00:03:05.000 --> 00:03:13.000]   May God bless the grieving families, and may God continue to bless America.\n[00:03:13.000 --> 00:03:19.000]   [Silence]\n\n\nwhisper_print_timings:     fallbacks =   1 p /   0 h\nwhisper_print_timings:     load time =   569.03 ms\nwhisper_print_timings:      mel time =   146.85 ms\nwhisper_print_timings:   sample time =   238.66 ms /   553 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time = 18665.10 ms /     9 runs ( 2073.90 ms per run)\nwhisper_print_timings:   decode time = 13090.93 ms /   549 runs (   23.85 ms per run)\nwhisper_print_timings:    total time = 32733.52 ms\n```\n</details>\n\n## Real-time audio input example\n\nThis is a naive example of performing real-time inference on audio from your microphone.\nThe [stream](examples/stream) tool samples the audio every half a second and runs the transcription continously.\nMore info is available in [issue #10](https://github.com/ggerganov/whisper.cpp/issues/10).\n\n```java\nmake stream\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\nhttps://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4\n\n## Confidence color-coding\n\nAdding the `--print-colors` argument will print the transcribed text using an experimental color coding strategy\nto highlight words with high or low confidence:\n\n<img width=\"965\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png\">\n\n## Controlling the length of the generated text segments (experimental)\n\nFor example, to limit the line length to a maximum of 16 characters, simply add `-ml 16`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.850]   And so my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:04.140]   Americans, ask\n[00:00:04.140 --> 00:00:05.660]   not what your\n[00:00:05.660 --> 00:00:06.840]   country can do\n[00:00:06.840 --> 00:00:08.430]   for you, ask\n[00:00:08.430 --> 00:00:09.440]   what you can do\n[00:00:09.440 --> 00:00:10.020]   for your\n[00:00:10.020 --> 00:00:11.000]   country.\n```\n\n## Word-level timestamp\n\nThe `--max-len` argument can be used to obtain word-level timestamps. Simply use `-ml 1`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]  \n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n## Karaoke-style movie generation (experimental)\n\nThe [main](examples/main) example provides support for output of karaoke-style movies, where the\ncurrently pronounced word is highlighted. Use the `-wts` argument and run the generated bash script.\nThis requires to have `ffmpeg` installed.\n\nHere are a few *\"typical\"* examples:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4\n\n---\n\n## Video comparison of different models\n\nUse the [extra/bench-wts.sh](https://github.com/ggerganov/whisper.cpp/blob/master/extra/bench-wts.sh) script to generate a video in the following format:\n\n```java\n./extra/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4\n\n---\n\n## Benchmarks\n\nIn order to have an objective comparison of the performance of the inference across different system configurations,\nuse the [bench](examples/bench) tool. The tool simply runs the Encoder part of the model and prints how much time it\ntook to execute it. The results are summarized in the following Github issue:\n\n[Benchmark results](https://github.com/ggerganov/whisper.cpp/issues/89)\n\n## ggml format\n\nThe original models are converted to a custom binary format. This allows to pack everything needed into a single file:\n\n- model parameters\n- mel filters\n- vocabulary\n- weights\n\nYou can download the converted models using the [models/download-ggml-model.sh](models/download-ggml-model.sh) script\nor manually from here:\n\n- https://huggingface.co/datasets/ggerganov/whisper.cpp\n- https://ggml.ggerganov.com\n\nFor more details, see the conversion script [models/convert-pt-to-ggml.py](models/convert-pt-to-ggml.py) or the README\nin [models](models).\n\n## [Bindings](https://github.com/ggerganov/whisper.cpp/discussions/categories/bindings)\n\n- [X] Rust: [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) | [#310](https://github.com/ggerganov/whisper.cpp/discussions/310)\n- [X] Javascript: [bindings/javascript](bindings/javascript) | [#309](https://github.com/ggerganov/whisper.cpp/discussions/309)\n- [X] Go: [bindings/go](bindings/go) | [#312](https://github.com/ggerganov/whisper.cpp/discussions/312)\n- [X] Ruby: [bindings/ruby](bindings/ruby) | [#507](https://github.com/ggerganov/whisper.cpp/discussions/507)\n- [X] Objective-C / Swift: [ggerganov/whisper.spm](https://github.com/ggerganov/whisper.spm) | [#313](https://github.com/ggerganov/whisper.cpp/discussions/313)\n- [X] .NET: | [#422](https://github.com/ggerganov/whisper.cpp/discussions/422)\n  - [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net)\n  - [NickDarvey/whisper](https://github.com/NickDarvey/whisper)\n- [X] Python: | [#9](https://github.com/ggerganov/whisper.cpp/issues/9)\n  - [stlukey/whispercpp.py](https://github.com/stlukey/whispercpp.py) (Cython)\n  - [aarnphm/whispercpp](https://github.com/aarnphm/whispercpp) (Pybind11)\n\n## Examples\n\nThere are various examples of using the library for different projects in the [examples](examples) folder.\nSome of the examples are even ported to run in the browser using WebAssembly. Check them out!\n\n| Example | Web | Description |\n| ---     | --- | ---         |\n| [main](examples/main) | [whisper.wasm](examples/whisper.wasm) | Tool for translating and transcribing audio using Whisper |\n| [bench](examples/bench) | [bench.wasm](examples/bench.wasm) | Benchmark the performance of Whisper on your machine |\n| [stream](examples/stream) | [stream.wasm](examples/stream.wasm) | Real-time transcription of raw microphone capture |\n| [command](examples/command) | [command.wasm](examples/command.wasm) | Basic voice assistant example for receiving voice commands from the mic |\n| [talk](examples/talk) | [talk.wasm](examples/talk.wasm) | Talk with a GPT-2 bot |\n| [whisper.objc](examples/whisper.objc) | | iOS mobile application using whisper.cpp |\n| [whisper.swiftui](examples/whisper.swiftui) | | SwiftUI iOS / macOS application using whisper.cpp |\n| [whisper.android](examples/whisper.android) | | Android mobile application using whisper.cpp |\n| [whisper.nvim](examples/whisper.nvim) | | Speech-to-text plugin for Neovim |\n| [generate-karaoke.sh](examples/generate-karaoke.sh) | | Helper script to easily [generate a karaoke video](https://youtu.be/uj7hVta4blM) of raw audio capture |\n| [livestream.sh](examples/livestream.sh) | | [Livestream audio transcription](https://github.com/ggerganov/whisper.cpp/issues/185) |\n| [yt-wsp.sh](examples/yt-wsp.sh) | | Download + transcribe and/or translate any VOD [(original)](https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818) |\n\n## [Discussions](https://github.com/ggerganov/whisper.cpp/discussions)\n\nIf you have any kind of feedback about this project feel free to use the Discussions section and open a new topic.\nYou can use the [Show and tell](https://github.com/ggerganov/whisper.cpp/discussions/categories/show-and-tell) category\nto share your own projects that use `whisper.cpp`. If you have a question, make sure to check the\n[Frequently asked questions (#126)](https://github.com/ggerganov/whisper.cpp/discussions/126) discussion.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whisper",
        "audio",
        "transcribes",
        "translates audio",
        "openai whisper",
        "whisper model"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "bmorphism--say-mcp-server": {
      "owner": "bmorphism",
      "name": "say-mcp-server",
      "url": "https://github.com/bmorphism/say-mcp-server",
      "imageUrl": "https://github.com/bmorphism.png",
      "description": "Provides text-to-speech functionality using macOS's built-in `say` command, allowing the generation of spoken output from text input.",
      "stars": 18,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:52Z",
      "readme_content": "# say-mcp-server\n<a href=\"https://glama.ai/mcp/servers/lmmqoe15jp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lmmqoe15jp/badge\" alt=\"Say Server MCP server\" /></a>\n\n![macOS System Voice Settings](images/adding_voice.png)\n\nAn MCP server that provides text-to-speech functionality using macOS's built-in `say` command.\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n\n## Installation\n\n```bash\nnpm install say-mcp-server\n```\n\n## Tools\n\n### speak\n\nThe `speak` tool provides access to macOS's text-to-speech capabilities with extensive customization options.\n\n#### Basic Usage\n\nUse macOS text-to-speech to speak text aloud.\n\nParameters:\n- `text` (required): Text to speak. Supports:\n  - Plain text\n  - Basic punctuation for pauses\n  - Newlines for natural breaks\n  - [[slnc 500]] for 500ms silence\n  - [[rate 200]] for changing speed mid-text\n  - [[volm 0.5]] for changing volume mid-text\n  - [[emph +]] and [[emph -]] for emphasis\n  - [[pbas +10]] for pitch adjustment\n- `voice` (optional): Voice to use (default: \"Alex\")\n- `rate` (optional): Speaking rate in words per minute (default: 175, range: 1-500)\n- `background` (optional): Run speech in background to allow further MCP interaction (default: false)\n\n#### Advanced Features\n\n1. Voice Modulation:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[volm 0.7]] This is quieter [[volm 1.0]] and this is normal [[volm 1.5]] and this is louder\",\n    voice: \"Victoria\"\n  }\n});\n```\n\n2. Dynamic Rate Changes:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Normal speed [[rate 300]] now speaking faster [[rate 100]] and now slower\",\n    voice: \"Fred\"\n  }\n});\n```\n\n3. Emphasis and Pitch:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[emph +]] Important point! [[emph -]] [[pbas +10]] Higher pitch [[pbas -10]] Lower pitch\",\n    voice: \"Samantha\"\n  }\n});\n```\n\n#### Integration Examples\n\n1. With Marginalia Search:\n```typescript\n// Search for a topic and have the results read aloud\nconst searchResult = await use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"quantum computing basics\", count: 1 }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: searchResult.results[0].description,\n    voice: \"Daniel\",\n    rate: 150\n  }\n});\n```\n\n2. With YouTube Transcripts:\n```typescript\n// Read a YouTube video transcript\nconst transcript = await use_mcp_tool({\n  server_name: \"youtube-transcript\",\n  tool_name: \"get_transcript\",\n  arguments: {\n    url: \"https://youtube.com/watch?v=example\",\n    lang: \"en\"\n  }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: transcript.text,\n    voice: \"Samantha\",\n    rate: 175\n  }\n});\n```\n\n3. Background Speech with Multiple Actions:\n```typescript\n// Start long speech in background\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"This is a long speech that will run in the background...\",\n    voice: \"Rocko (Italian (Italy))\",\n    rate: 69,\n    background: true\n  }\n});\n\n// Immediately perform another action while speech continues\nawait use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"parallel processing\" }\n});\n```\n\n4. With Apple Notes:\n```typescript\n// Read notes aloud\nconst notes = await use_mcp_tool({\n  server_name: \"apple-notes-mcp\",\n  tool_name: \"search-notes\",\n  arguments: { query: \"meeting notes\" }\n});\n\nif (notes.length > 0) {\n  await use_mcp_tool({\n    server_name: \"say\",\n    tool_name: \"speak\",\n    arguments: {\n      text: notes[0].content,\n      voice: \"Karen\",\n      rate: 160\n    }\n  });\n}\n```\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Hello, world!\",\n    voice: \"Victoria\",\n    rate: 200\n  }\n});\n```\n\n### list_voices\n\nList all available text-to-speech voices on the system.\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"list_voices\",\n  arguments: {}\n});\n```\n\n## Recommended Voices\n\n<table>\n<tr>\n<th>Voice</th>\n<th>Language/Region</th>\n<th>Intellectual Figure</th>\n<th>Haiku</th>\n<th>CLI Specification</th>\n</tr>\n<tr>\n<td>Anna (Premium)</td>\n<td>German</td>\n<td>Emmy Noether</td>\n<td>Symmetrie haucht Leben<br>Algebras verborgne Form<br>Abstraktion blÃ¼ht<br><br><i>Symmetry breathes life<br>Algebra's hidden forms<br>Abstraction blooms</i></td>\n<td><code>-v \"Anna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Emma (Premium)</td>\n<td>Italian</td>\n<td>Maria Adelaide Sneider</td>\n<td>Algoritmi in danza<br>Macchina sussurra dolce<br>Il codice vive<br><br><i>Algorithms dance<br>Machine whispers secrets soft<br>Code becomes alive</i></td>\n<td><code>-v \"Emma (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Federica (Premium)</td>\n<td>Italian</td>\n<td>Pia Nalli</td>\n<td>Teoremi fluenti<br>Numeri danzano liberi<br>VeritÃ  emerge<br><br><i>Flowing theorems dance<br>Numbers move in freedom's space<br>Truth emerges pure</i></td>\n<td><code>-v \"Federica (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Serena (Premium)</td>\n<td>English (UK)</td>\n<td>Bertha Swirles</td>\n<td>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges<br><br><i>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges</i></td>\n<td><code>-v \"Serena (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Petra (Premium)</td>\n<td>German</td>\n<td>Ruth Moufang</td>\n<td>Algebra spricht<br>In Symmetrien versteckt<br>Wahrheit erblÃ¼ht<br><br><i>Algebra speaks soft<br>Hidden in symmetries pure<br>Truth blooms anew here</i></td>\n<td><code>-v \"Petra (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Yuna (Premium)</td>\n<td>Korean</td>\n<td>Hee Oh</td>\n<td>ìˆ¨ì€ íŒ¨í„´ ë¹›ë‚˜ê³ <br>ë§ˆìŒì˜ ë°©ì •ì‹ í•€ë‹¤<br>ì§€ì‹ ìë¼ë‚˜<br><br><i>Hidden patterns gleam<br>Mind's equations softly bloom<br>Knowledge multiplies</i></td>\n<td><code>-v \"Yuna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Alva (Premium)</td>\n<td>Swedish</td>\n<td>Sonja Korovkin</td>\n<td>MÃ¶nster flÃ¶dar fritt<br>Genom tankens labyrinter<br>Visdom blomstrar hÃ¤r<br><br><i>Patterns flowing free<br>Through labyrinths of the mind<br>Wisdom blooms right here</i></td>\n<td><code>-v \"Alva (Premium)\"</code></td>\n</tr>\n<tr>\n<td>AmÃ©lie (Premium)</td>\n<td>French (Canada)</td>\n<td>Sophie Germain</td>\n<td>Nombres premiers murmurent<br>Dansent entre les silences<br>SymÃ©trie s'ouvre<br><br><i>Prime numbers whisper<br>Dancing between the silence<br>Symmetry unfolds</i></td>\n<td><code>-v \"AmÃ©lie (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Ewa (Premium)</td>\n<td>Polish</td>\n<td>Maria Wielgus</td>\n<td>Logiki korzenie<br>Matematyczne krainy<br>MyÅ›l kieÅ‚kujÄ…ca<br><br><i>Logic's tender roots<br>Mathematical landscapes<br>Thought's seeds germinate</i></td>\n<td><code>-v \"Ewa (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Kiyara (Premium)</td>\n<td>Hindi</td>\n<td>Shakuntala Devi</td>\n<td>à¤—à¤£à¤¿à¤¤ à¤•à¥€ à¤²à¤¯ à¤®à¥‡à¤‚<br>à¤…à¤‚à¤• à¤¨à¥ƒà¤¤à¥à¤¯ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚<br>à¤œà¥à¤à¤¾à¤¨ à¤œà¤—à¤¤à¤¾ à¤¹à¥ˆ<br><br><i>In rhythm of math<br>Numbers dance their sacred steps<br>Knowledge awakens</i></td>\n<td><code>-v \"Kiyara (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Majed (Premium)</td>\n<td>Arabic</td>\n<td>Maha Al-Aswad</td>\n<td>Ø£Ø±Ù‚Ø§Ù… ØªØ±Ù‚Øµ<br>ÙÙŠ ÙØ¶Ø§Ø¡ Ø§Ù„Ù„Ø§Ù†Ù‡Ø§ÙŠØ©<br>Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© ØªØ´Ø±Ù‚<br><br><i>Numbers dance freely<br>In infinity's vast space<br>Truth rises like dawn</i></td>\n<td><code>-v \"Majed (Premium)\"</code></td>\n</tr>\n<tr>\n<td>TÃ¼nde (Premium)</td>\n<td>Hungarian</td>\n<td>Julia ErdÅ‘s</td>\n<td>SzÃ¡mok tÃ¡ncolnak<br>VÃ©gtelen tÃ©rben szÃ¡llnak<br>IgazsÃ¡g virrad<br><br><i>Numbers dance and soar<br>Through infinite space they glide<br>Truth dawns pure and bright</i></td>\n<td><code>-v \"TÃ¼nde (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Fiona (Enhanced)</td>\n<td>English (Scottish)</td>\n<td>Mary Somerville</td>\n<td>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars<br><br><i>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars</i></td>\n<td><code>-v \"Fiona (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Lesya (Enhanced)</td>\n<td>Ukrainian</td>\n<td>Olena Voinova</td>\n<td>Ğ¢Ğ¸ÑˆĞ° Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ<br>ĞœÑ–Ğ¶ Ğ·Ñ–Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ½Ñ ÑĞ¿Ğ¸Ñ‚ÑŒ<br>Ğ”ÑƒĞ¼ĞºĞ° Ğ¿Ñ€Ğ¾Ñ€Ğ¾ÑÑ‚Ğ°Ñ”<br><br><i>Silence speaks softly<br>Knowledge sleeps among the stars<br>Thought begins to grow</i></td>\n<td><code>-v \"Lesya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Carmit (Enhanced)</td>\n<td>Hebrew</td>\n<td>Tali Seror</td>\n<td>××™×œ×™× × ×•×©××•×ª ×‘×©×§×˜<br>×‘×™×Ÿ ×©×•×¨×•×ª ×©×œ ×“×××”<br>×©×™×¨ ××ª×¢×•×¨×¨<br><br><i>Words breathe silently<br>Between lines of deep stillness<br>Poem awakening</i></td>\n<td><code>-v \"Carmit (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Milena (Enhanced)</td>\n<td>Russian</td>\n<td>Olga Ladyzhenskaya</td>\n<td>ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ÑˆĞµĞ¿Ñ‡ĞµÑ‚ Ğ½Ğ°Ğ¼<br>Ğ£Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‚<br>Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ğ¼Ğ¾Ğ»Ñ‡Ğ¸Ñ‚<br><br><i>Memory whispers<br>Equations flow like rivers<br>Truth speaks silently</i></td>\n<td><code>-v \"Milena (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Katya (Enhanced)</td>\n<td>Russian</td>\n<td>Sofia Kovalevskaya</td>\n<td>Ğ§Ğ¸ÑĞ»Ğ° Ñ‚Ğ°Ğ½Ñ†ÑƒÑÑ‚<br>Ğ’ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¼<br>Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ñ†Ğ²ĞµÑ‚Ñ‘Ñ‚<br><br><i>Numbers dance freely<br>In space of infinity<br>Truth blooms like a flower</i></td>\n<td><code>-v \"Katya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Damayanti (Enhanced)</td>\n<td>Indonesian</td>\n<td>Sri Pekerti</td>\n<td>Angka menari<br>Dalam ruang tak batas<br>Kebenaran tumbuh<br><br><i>Numbers dance gently<br>In boundless space they flutter<br>Truth grows like new leaves</i></td>\n<td><code>-v \"Damayanti (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Dariush (Enhanced)</td>\n<td>Persian</td>\n<td>Maryam Mirzakhani</td>\n<td>Ø§Ø¹Ø¯Ø§Ø¯ Ù…ÛŒ Ø±Ù‚ØµÙ†Ø¯<br>Ø¯Ø± ÙØ¶Ø§ÛŒ Ø¨ÛŒ Ù¾Ø§ÛŒØ§Ù†<br>Ø­Ù‚ÛŒÙ‚Øª Ù…ÛŒ Ø±ÙˆÛŒØ¯<br><br><i>Numbers dance with grace<br>In endless space they traverse<br>Truth springs forth anew</i></td>\n<td><code>-v \"Dariush (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Astro Boy (Tetsuwan Atomu)<br>Italian dub</td>\n<td>Robot di metallo<br>Cuore umano batte forte<br>Pace nel futuro<br><br><i>Metal robot form<br>Human heart beats strong within<br>Peace in future dawns</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Jeeg Robot d'Acciaio<br>(KÅtetsu Jeeg)</td>\n<td>Acciaio lucente<br>Protettore dei deboli<br>Vola nel cielo<br><br><i>Shining steel warrior<br>Protector of the helpless<br>Soars through the heavens</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Numero 5<br>(Short Circuit)</td>\n<td>Input infinito<br>La coscienza si risveglia<br>Vita artificiale<br><br><i>Infinite input<br>Consciousness awakening<br>Artificial life</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Binbin (Enhanced)</td>\n<td>Chinese (Mainland)</td>\n<td>Li Shanlan</td>\n<td>ç®—æœ¯ä¹‹é“æµ<br>æ•°ç†æ¼”ç»çœŸç†<br>æ™ºæ…§ç»½æ”¾<br><br><i>Arithmetic flows<br>Logic unfolds truth's pattern<br>Wisdom blossoms bright</i></td>\n<td><code>-v \"Binbin (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Han (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chen Jingrun</td>\n<td>ç´ æ•°ä¹‹èˆåŠ¨<br>å“¥å¾·å·´èµ«çŒœæƒ³<br>çœŸç†æ°¸æ’<br><br><i>Prime numbers dancing<br>Goldbach's conjecture whispers<br>Truth eternal flows</i></td>\n<td><code>-v \"Han (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Lilian (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Hua Luogeng</td>\n<td>æ•°è®ºä¹‹å…‰èŠ’<br>è§£æå»¶ç»­ç¾<br>æ™ºæ…§å‡å<br><br><i>Number theory shines<br>Analysis extends grace<br>Wisdom ascends pure</i></td>\n<td><code>-v \"Lilian (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Meijia</td>\n<td>Chinese (Taiwan)</td>\n<td>Sun-Yung Alice Chang</td>\n<td>å¹¾ä½•ä¹‹ç¾ç¾<br>æ›²ç‡æµå‹•ä¸æ¯<br>ç©ºé–“å±•é–‹<br><br><i>Geometry shows<br>Curvature flows endlessly<br>Space unfolds anew</i></td>\n<td><code>-v \"Meijia\"</code></td>\n</tr>\n<tr>\n<td>Sinji (Premium)</td>\n<td>Chinese (Hong Kong)</td>\n<td>Shing-Tung Yau</td>\n<td>æµå½¢ä¹‹å¥§ç§˜<br>å¡æ‹‰æ¯”ç©ºé–“å‹•<br>ç¶­åº¦äº¤ç¹”<br><br><i>Manifolds reveal<br>Calabi spaces in flow<br>Dimensions weave truth</i></td>\n<td><code>-v \"Sinji (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tingting</td>\n<td>Chinese (Mainland)</td>\n<td>Wang Zhenyi</td>\n<td>æ˜Ÿè¾°è½¨è¿¹æ˜<br>å¤©æ–‡æ•°å­¦è<br>æ™ºæ…§é—ªè€€<br><br><i>Starlit paths shine bright<br>Astronomy meets numbers<br>Wisdom radiates</i></td>\n<td><code>-v \"Tingting\"</code></td>\n</tr>\n<tr>\n<td>Yue (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chern Shiing-shen</td>\n<td>å¾®åˆ†å‡ ä½•<br>çº¤ç»´ä¸›ä¸­å¯»çœŸ<br>æœ¬è´¨æ˜¾ç°<br><br><i>Differential forms<br>In fiber bundles seek truth<br>Essence emerges</i></td>\n<td><code>-v \"Yue (Premium)\"</code></td>\n</tr>\n</table>\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Contributors\n\n- Barton Rhodes ([@bmorphism](https://github.com/bmorphism)) - barton@vibes.lol\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "macos",
        "mcp",
        "speech",
        "say command",
        "text speech",
        "speech functionality"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "elevenlabs--elevenlabs-mcp": {
      "owner": "elevenlabs",
      "name": "elevenlabs-mcp",
      "url": "https://github.com/elevenlabs/elevenlabs-mcp",
      "imageUrl": "https://github.com/elevenlabs.png",
      "description": "This server provides APIs for generating speech, voice cloning, and audio transcription. It facilitates seamless interaction with text-to-speech and audio processing functionalities.",
      "stars": 998,
      "forks": 161,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:34:19Z",
      "readme_content": "![export](https://github.com/user-attachments/assets/ee379feb-348d-48e7-899c-134f7f7cd74f)\n\n<div class=\"title-block\" style=\"text-align: center;\" align=\"center\">\n\n  [![Discord Community](https://img.shields.io/badge/discord-@elevenlabs-000000.svg?style=for-the-badge&logo=discord&labelColor=000)](https://discord.gg/elevenlabs)\n  [![Twitter](https://img.shields.io/badge/Twitter-@elevenlabsio-000000.svg?style=for-the-badge&logo=twitter&labelColor=000)](https://x.com/ElevenLabsDevs)\n  [![PyPI](https://img.shields.io/badge/PyPI-elevenlabs--mcp-000000.svg?style=for-the-badge&logo=pypi&labelColor=000)](https://pypi.org/project/elevenlabs-mcp)\n  [![Tests](https://img.shields.io/badge/tests-passing-000000.svg?style=for-the-badge&logo=github&labelColor=000)](https://github.com/elevenlabs/elevenlabs-mcp-server/actions/workflows/test.yml)\n\n</div>\n\n\n<p align=\"center\">\n  Official ElevenLabs <a href=\"https://github.com/modelcontextprotocol\">Model Context Protocol (MCP)</a> server that enables interaction with powerful Text to Speech and audio processing APIs. This server allows MCP clients like <a href=\"https://www.anthropic.com/claude\">Claude Desktop</a>, <a href=\"https://www.cursor.so\">Cursor</a>, <a href=\"https://codeium.com/windsurf\">Windsurf</a>, <a href=\"https://github.com/openai/openai-agents-python\">OpenAI Agents</a> and others to generate speech, clone voices, transcribe audio, and more.\n</p>\n\n<!--\nmcp-name: io.github.elevenlabs/elevenlabs-mcp\n-->\n\n## Quickstart with Claude Desktop\n\n1. Get your API key from [ElevenLabs](https://elevenlabs.io/app/settings/api-keys). There is a free tier with 10k credits per month.\n2. Install `uv` (Python package manager), install with `curl -LsSf https://astral.sh/uv/install.sh | sh` or see the `uv` [repo](https://github.com/astral-sh/uv) for additional install methods.\n3. Go to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```\n{\n  \"mcpServers\": {\n    \"ElevenLabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"<insert-your-api-key-here>\"\n      }\n    }\n  }\n}\n\n```\n\nIf you're using Windows, you will have to enable \"Developer Mode\" in Claude Desktop to use the MCP server. Click \"Help\" in the hamburger menu at the top left and select \"Enable Developer Mode\".\n\n## Other MCP clients\n\nFor other clients like Cursor and Windsurf, run:\n1. `pip install elevenlabs-mcp`\n2. `python -m elevenlabs_mcp --api-key={{PUT_YOUR_API_KEY_HERE}} --print` to get the configuration. Paste it into appropriate configuration directory specified by your MCP client.\n\nThat's it. Your MCP client can now interact with ElevenLabs through these tools:\n\n## Example usage\n\nâš ï¸ Warning: ElevenLabs credits are needed to use these tools.\n\nTry asking Claude:\n\n- \"Create an AI agent that speaks like a film noir detective and can answer questions about classic movies\"\n- \"Generate three voice variations for a wise, ancient dragon character, then I will choose my favorite voice to add to my voice library\"\n- \"Convert this recording of my voice to sound like a medieval knight\"\n- \"Create a soundscape of a thunderstorm in a dense jungle with animals reacting to the weather\"\n- \"Turn this speech into text, identify different speakers, then convert it back using unique voices for each person\"\n\n## Optional features\n\n### File Output Configuration\n\nYou can configure how the MCP server handles file outputs using these environment variables in your `claude_desktop_config.json`:\n\n- **`ELEVENLABS_MCP_BASE_PATH`**: Specify the base path for file operations with relative paths (default: `~/Desktop`)\n- **`ELEVENLABS_MCP_OUTPUT_MODE`**: Control how generated files are returned (default: `files`)\n\n#### Output Modes\n\nThe `ELEVENLABS_MCP_OUTPUT_MODE` environment variable supports three modes:\n\n1. **`files`** (default): Save files to disk and return file paths\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"files\"\n   }\n   ```\n\n2. **`resources`**: Return files as MCP resources; binary data is base64-encoded, text is returned as UTF-8 text\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"resources\"\n   }\n   ```\n\n3. **`both`**: Save files to disk AND return as MCP resources\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"both\"\n   }\n   ```\n\n**Resource Mode Benefits:**\n- Files are returned directly in the MCP response as base64-encoded data\n- No disk I/O required - useful for containerized or serverless environments\n- MCP clients can access file content immediately without file system access\n- In `both` mode, resources can be fetched later using the `elevenlabs://filename` URI pattern\n\n**Use Cases:**\n- `files`: Traditional file-based workflows, local development\n- `resources`: Cloud environments, MCP clients without file system access\n- `both`: Maximum flexibility, caching, and resource sharing scenarios\n\n### Data residency keys\n\nYou can specify the data residency region with the `ELEVENLABS_API_RESIDENCY` environment variable. Defaults to `\"us\"`.\n\n**Note:** Data residency is an enterprise only feature. See [the docs](https://elevenlabs.io/docs/product-guides/administration/data-residency#overview) for more details.\n\n## Contributing\n\nIf you want to contribute or run from source:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/elevenlabs/elevenlabs-mcp\ncd elevenlabs-mcp\n```\n\n2. Create a virtual environment and install dependencies [using uv](https://github.com/astral-sh/uv):\n\n```bash\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n```\n\n3. Copy `.env.example` to `.env` and add your ElevenLabs API key:\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key\n```\n\n4. Run the tests to make sure everything is working:\n\n```bash\n./scripts/test.sh\n# Or with options\n./scripts/test.sh --verbose --fail-fast\n```\n\n5. Install the server in Claude Desktop: `mcp install elevenlabs_mcp/server.py`\n\n6. Debug and test locally with MCP Inspector: `mcp dev elevenlabs_mcp/server.py`\n\n## Troubleshooting\n\nLogs when running with Claude Desktop can be found at:\n\n- **Windows**: `%APPDATA%\\Claude\\logs\\mcp-server-elevenlabs.log`\n- **macOS**: `~/Library/Logs/Claude/mcp-server-elevenlabs.log`\n\n### Timeouts when using certain tools\n\nCertain ElevenLabs API operations, like voice design and audio isolation, can take a long time to resolve. When using the MCP inspector in dev mode, you might get timeout errors despite the tool completing its intended task.\n\nThis shouldn't occur when using a client like Claude.\n\n### MCP ElevenLabs: spawn uvx ENOENT\n\nIf you encounter the error \"MCP ElevenLabs: spawn uvx ENOENT\", confirm its absolute path by running this command in your terminal:\n\n```bash\nwhich uvx\n```\n\nOnce you obtain the absolute path (e.g., `/usr/local/bin/uvx`), update your configuration to use that path (e.g., `\"command\": \"/usr/local/bin/uvx\"`). This ensures that the correct executable is referenced.\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcription",
        "voice",
        "audio",
        "voice cloning",
        "audio transcription",
        "cloning audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "georgi-io--jessica": {
      "owner": "georgi-io",
      "name": "jessica",
      "url": "https://github.com/georgi-io/jessica",
      "imageUrl": "https://github.com/georgi-io.png",
      "description": "Integrates ElevenLabs Text-to-Speech capabilities for seamless text conversion to speech, offering voice selection and management through a modern interface. Supports real-time communication with a FastAPI backend and a React frontend.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-25T23:18:06Z",
      "readme_content": "# Project Jessica (ElevenLabs TTS MCP)\n\nThis project integrates ElevenLabs Text-to-Speech capabilities with Cursor through the Model Context Protocol (MCP). It consists of a FastAPI backend service and a React frontend application.\n\n## Features\n\n- Text-to-Speech conversion using ElevenLabs API\n- Voice selection and management\n- MCP integration for Cursor\n- Modern React frontend interface\n- WebSocket real-time communication\n- Pre-commit hooks for code quality\n- Automatic code formatting and linting\n\n## Project Structure\n\n```\njessica/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ backend/          # FastAPI backend service\nâ”‚   â””â”€â”€ frontend/         # React frontend application\nâ”œâ”€â”€ terraform/            # Infrastructure as Code\nâ”œâ”€â”€ tests/               # Test suites\nâ””â”€â”€ docs/                # Documentation\n```\n\n## Requirements\n\n- Python 3.11+\n- Poetry (for backend dependency management)\n- Node.js 18+ (for frontend)\n- Cursor (for MCP integration)\n\n## Local Development Setup\n\n### Backend Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/georgi-io/jessica.git\ncd jessica\n\n# Create Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install backend dependencies\npoetry install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your ElevenLabs API key\n\n# Install pre-commit hooks\npoetry run pre-commit install\n```\n\n### Frontend Setup\n\n```bash\n# Navigate to frontend directory\ncd src/frontend\n\n# Install dependencies\nnpm install\n```\n\n## Development Servers\n\n### Starting the Backend\n\n```bash\n# Activate virtual environment if not active\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Start the backend\npython -m src.backend\n```\n\nThe backend provides:\n- REST API: http://localhost:9020\n- WebSocket: ws://localhost:9020/ws\n- MCP Server: http://localhost:9020/sse (integrated with the main API server)\n\n### Starting the Frontend\n\n```bash\n# In src/frontend directory\nnpm run dev\n```\n\nFrontend development server:\n- http://localhost:5173\n\n## Environment Configuration\n\n### Backend (.env)\n```env\n# ElevenLabs API\nELEVENLABS_API_KEY=your-api-key\n\n# Server Configuration\nHOST=127.0.0.1\nPORT=9020\n\n# Development Settings\nDEBUG=false\nRELOAD=true\n```\n\n### Frontend (.env)\n```env\nVITE_API_URL=http://localhost:9020\nVITE_WS_URL=ws://localhost:9020/ws\n```\n\n## Code Quality Tools\n\n### Backend\n\n```bash\n# Run all pre-commit hooks\npoetry run pre-commit run --all-files\n\n# Run specific tools\npoetry run ruff check .\npoetry run ruff format .\npoetry run pytest\n```\n\n### Frontend\n\n```bash\n# Lint\nnpm run lint\n\n# Type check\nnpm run type-check\n\n# Test\nnpm run test\n```\n\n## Production Deployment\n\n### AWS ECR and GitHub Actions Setup\n\nTo enable automatic building and pushing of Docker images to Amazon ECR:\n\n1. Apply the Terraform configuration to create the required AWS resources:\n   ```bash\n   cd terraform\n   terraform init\n   terraform apply\n   ```\n\n2. The GitHub Actions workflow will automatically:\n   - Read the necessary configuration from the Terraform state in S3\n   - Build the Docker image on pushes to `main` or `develop` branches\n   - Push the image to ECR with tags for `latest` and the specific commit SHA\n\n3. No additional repository variables needed! The workflow fetches all required configuration from the Terraform state.\n\n### How it Works\n\nThe GitHub Actions workflow is configured to:\n1. Initially assume a predefined IAM role with S3 read permissions\n2. Fetch and extract configuration values from the Terraform state file in S3\n3. Re-authenticate using the actual deployment role from the state file\n4. Build and push the Docker image to the ECR repository defined in the state\n\nThis approach eliminates the need to manually configure GitHub repository variables and ensures that the CI/CD process always uses the current infrastructure configuration.\n\n### Quick Overview\n\n- Frontend: Served from S3 via CloudFront at jessica.georgi.io\n- Backend API: Available at api.georgi.io/jessica\n- WebSocket: Connects to api.georgi.io/jessica/ws\n- Docker Image: Stored in AWS ECR and can be deployed to ECS/EKS\n- Infrastructure: Managed via Terraform in this repository\n\n## MCP Integration with Cursor\n\n1. Start the backend server\n2. In Cursor settings, add new MCP server:\n   - Name: Jessica TTS\n   - Type: SSE\n   - URL: http://localhost:9020/sse\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Issues**\n   - Error: \"Invalid API key\"\n   - Solution: Check `.env` file\n\n2. **Connection Problems**\n   - Error: \"Cannot connect to MCP server\"\n   - Solution: Verify backend is running and ports are correct\n\n3. **Port Conflicts**\n   - Error: \"Address already in use\"\n   - Solution: Change ports in `.env`\n\n4. **WebSocket Connection Failed**\n   - Error: \"WebSocket connection failed\"\n   - Solution: Ensure backend is running and WebSocket URL is correct\n\nFor additional help, please open an issue on GitHub.\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "text",
        "text speech",
        "speech recognition",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "giannisanni--kokoro-tts-mcp": {
      "owner": "giannisanni",
      "name": "kokoro-tts-mcp",
      "url": "https://github.com/giannisanni/kokoro-tts-mcp",
      "imageUrl": "https://github.com/giannisanni.png",
      "description": "Integrates text-to-speech capabilities using the Kokoro TTS engine, enabling conversion of written content into spoken audio with customizable voices and adjustable speed. Supports saving audio files and cross-platform playback.",
      "stars": 10,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-28T13:54:19Z",
      "readme_content": "# Kokoro TTS MCP Server\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Kokoro TTS engine. This server exposes TTS functionality through MCP tools, making it easy to integrate speech synthesis into your applications.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- `uv` package manager\n\n## Installation\n\n1. First, install the `uv` package manager:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Clone this repository and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\nuv pip install .\n```\n\n## Features\n\n- Text-to-speech synthesis with customizable voices\n- Adjustable speech speed\n- Support for saving audio to files or direct playback\n- Cross-platform audio playback support (Windows, macOS, Linux)\n\n## Usage\n\nThe server provides a single MCP tool `generate_speech` with the following parameters:\n\n- `text` (required): The text to convert to speech\n- `voice` (optional): Voice to use for synthesis (default: \"af_heart\")\n- `speed` (optional): Speech speed multiplier (default: 1.0)\n- `save_path` (optional): Directory to save audio files\n- `play_audio` (optional): Whether to play the audio immediately (default: False)\n\n### Example Usage\n\n```python\nfrom mcp.client import Client\n\nasync with Client() as client:\n    await client.connect(\"kokoro-tts\")\n    \n    # Generate and play speech\n    result = await client.call_tool(\n        \"generate_speech\",\n        {\n            \"text\": \"Hello, world!\",\n            \"voice\": \"af_heart\",\n            \"speed\": 1.0,\n            \"play_audio\": True\n        }\n    )\n```\n\n## Dependencies\n\n- kokoro >= 0.8.4\n- mcp[cli] >= 1.3.0\n- soundfile >= 0.13.1\n\n## Platform Support\n\nAudio playback is supported on:\n- Windows (using `start`)\n- macOS (using `afplay`)\n- Linux (using `aplay`)\n\n## MCP Configuration\n\nAdd the following configuration to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"kokoro-tts\": {\n      \"command\": \"/Users/giannisan/pinokio/bin/miniconda/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/giannisan/Documents/Cline/MCP/kokoro-tts-mcp\",\n        \"run\",\n        \"tts-mcp.py\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\n[Add your license information here]\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "audio",
        "kokoro",
        "tts",
        "kokoro tts",
        "text speech",
        "spoken audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "hammeiam--koroko-speech-mcp": {
      "owner": "hammeiam",
      "name": "koroko-speech-mcp",
      "url": "https://github.com/hammeiam/koroko-speech-mcp",
      "imageUrl": "https://github.com/hammeiam.png",
      "description": "Provides text-to-speech capabilities using the Kokoro TTS model, converting text into natural-sounding speech with customizable options and multiple voice choices.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-22T02:56:06Z",
      "readme_content": "# Speech MCP Server\n\nA Model Context Protocol server that provides text-to-speech capabilities using the Kokoro TTS model.\n\n## Configuration\n\nThe server can be configured using the following environment variables:\n\n| Variable | Description | Default | Valid Range |\n|----------|-------------|---------|-------------|\n| `MCP_DEFAULT_SPEECH_SPEED` | Default speed multiplier for text-to-speech | 1.1 | 0.5 to 2.0 |\n| `MCP_DEFAULT_VOICE` | Default voice for text-to-speech | af_bella | Any valid voice ID |\n\nIn Cursor:\n```\n{\n  \"mcpServers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"speech-mcp-server\"\n      ],\n      \"env\": {\n        \"MCP_DEFAULT_SPEECH_SPEED\": 1.3,\n        \"MCP_DEFAULT_VOICE\": \"af_bella\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- ğŸ¯ High-quality text-to-speech using Kokoro TTS model\n- ğŸ—£ï¸ Multiple voice options available\n- ğŸ›ï¸ Customizable speech parameters (voice, speed)\n- ğŸ”Œ MCP-compliant interface\n- ğŸ“¦ Easy installation and setup\n- ğŸš€ No API key required\n\n## Installation\n\n```bash\n# Using npm\nnpm install speech-mcp-server\n\n# Using pnpm (recommended)\npnpm add speech-mcp-server\n\n# Using yarn\nyarn add speech-mcp-server\n```\n\n## Usage\n\nRun the server:\n\n```bash\n# Using default configuration\nnpm start\n\n# With custom configuration\nMCP_DEFAULT_SPEECH_SPEED=1.5 MCP_DEFAULT_VOICE=af_bella npm start\n```\n\nThe server provides the following MCP tools:\n- `text_to_speech`: Basic text-to-speech conversion\n- `text_to_speech_with_options`: Text-to-speech with customizable speed\n- `list_voices`: List all available voices\n- `get_model_status`: Check the initialization status of the TTS model\n\n### Development\n\n```bash\n# Clone the repository\ngit clone <your-repo-url>\ncd speech-mcp-server\n\n# Install dependencies\npnpm install\n\n# Start development server with auto-reload\npnpm dev\n\n# Build the project\npnpm build\n\n# Run linting\npnpm lint\n\n# Format code\npnpm format\n\n# Test with MCP Inspector\npnpm inspector\n```\n\n## Available Tools\n\n### 1. text_to_speech\nConverts text to speech using the default settings.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\"  // optional\n    }\n  }\n}\n```\n\n### 2. text_to_speech_with_options\nConverts text to speech with customizable parameters.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech_with_options\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\",  // optional\n      \"speed\": 1.0,         // optional (0.5 to 2.0)\n    }\n  }\n}\n```\n\n### 3. list_voices\nLists all available voices for text-to-speech.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"list_voices\",\n  \"params\": {}\n}\n```\n\n### 4. get_model_status\nCheck the current status of the TTS model initialization. This is particularly useful when first starting the server, as the model needs to be downloaded and initialized.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"get_model_status\",\n    \"arguments\": {}\n  }\n}\n```\n\nResponse example:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed)\"\n  }]\n}\n```\n\nPossible status values:\n- `uninitialized`: Model initialization hasn't started\n- `initializing`: Model is being downloaded and initialized\n- `ready`: Model is ready to use\n- `error`: An error occurred during initialization\n\n## Testing\n\nYou can test the server using the MCP Inspector or by sending raw JSON messages:\n\n```bash\n# List available tools\necho '{\"type\":\"request\",\"id\":\"1\",\"method\":\"list_tools\",\"params\":{}}' | node dist/index.js\n\n# List available voices\necho '{\"type\":\"request\",\"id\":\"2\",\"method\":\"list_voices\",\"params\":{}}' | node dist/index.js\n\n# Convert text to speech\necho '{\"type\":\"request\",\"id\":\"3\",\"method\":\"call_tool\",\"params\":{\"name\":\"text_to_speech\",\"arguments\":{\"text\":\"Hello world\",\"voice\":\"af_bella\"}}}' | node dist/index.js\n```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"servers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\"@decodershq/speech-mcp-server\"]\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Troubleshooting\n\n### Model Initialization Issues\n\nThe server automatically attempts to download and initialize the TTS model on startup. If you encounter initialization errors:\n\n1. The server will automatically retry up to 3 times with a cleanup between attempts\n2. Use the `get_model_status` tool to monitor initialization progress and any errors\n3. If initialization fails after all retries, try manually removing the model files:\n\n```bash\n# Remove model files (MacOS/Linux)\nrm -rf ~/.npm/_npx/**/node_modules/@huggingface/transformers/.cache/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\nrm -rf ~/.cache/huggingface/transformers/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\n\n# Then restart the server\nnpm start\n```\n\nThe `get_model_status` tool will now include retry information in its response:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed, retry 1/3)\"\n  }]\n}\n``` ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "koroko",
        "kokoro",
        "voice",
        "koroko speech",
        "speech customizable",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "kentaro--aivis-speech-mcp": {
      "owner": "kentaro",
      "name": "aivis-speech-mcp",
      "url": "https://github.com/kentaro/aivis-speech-mcp",
      "imageUrl": "https://github.com/kentaro.png",
      "description": "Integrate with the AivisSpeech Engine to provide high-quality speech synthesis capabilities for applications, facilitating the conversion of text to natural-sounding speech. The server offers a type-safe API compliant with the Model Context Protocol, ensuring easy configuration and extensibility.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-15T17:05:34Z",
      "readme_content": "# AivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼\n\nAivisSpeechç”¨ã®Model Context Protocol (MCP) ã‚µãƒ¼ãƒãƒ¼ã®å®Ÿè£…ã§ã™ã€‚ã“ã®ã‚µãƒ¼ãƒãƒ¼ã¯ã€AivisSpeech Engineã¨é€£æºã—ã¦ã€éŸ³å£°åˆæˆã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’é€šã˜ã¦ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰AivisSpeechã®éŸ³å£°åˆæˆæ©Ÿèƒ½ã‚’ç°¡å˜ã«åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n\n## æ¦‚è¦\n\nAivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š\n\n- MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æº–æ‹ ã—ãŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n- AivisSpeech Engineã¨ã®é€£æºã«ã‚ˆã‚‹é«˜å“è³ªãªéŸ³å£°åˆæˆ\n- TypeScriptã«ã‚ˆã‚‹å‹å®‰å…¨ãªè¨­è¨ˆ\n- ç°¡å˜ãªè¨­å®šã¨æ‹¡å¼µæ€§ã®é«˜ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n\n## å¿…è¦æ¡ä»¶\n\n- Node.js 18.xä»¥ä¸Š\n- npm 9.xä»¥ä¸Š\n- AivisSpeech Engineï¼ˆåˆ¥é€”ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ï¼‰\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n```bash\n# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\ngit clone https://github.com/kentaro/aivis-speech-mcp.git\ncd aivis-speech-mcp\n\n# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nnpm install\n\n# ãƒ“ãƒ«ãƒ‰\nnpm run build\n\n# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\ncp .env.sample .env\n# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦ã€å¿…è¦ãªè¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„\n\n# Cursor MCPã®è¨­å®š\ncp .cursor/mcp.json.sample .cursor/mcp.json\n# mcp.jsonãƒ•ã‚¡ã‚¤ãƒ«å†…ã®\"/path/to/aivis-speech-mcp/dist/index.js\"ã‚’\n# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«æ›¸ãæ›ãˆã¦ãã ã•ã„\n# ä¾‹: \"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"\n```\n\n## ç’°å¢ƒè¨­å®š\n\n`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š\n\n```\n# AivisSpeech API Configuration\nAIVIS_SPEECH_API_URL=http://localhost:10101  # AivisSpeech Engineã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\n# Speaker Configuration\nAIVIS_SPEECH_SPEAKER_ID=888753760  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ID\n```\n\n## Cursor MCPè¨­å®š\n\n`.cursor/mcp.json`ãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š\n\n```json\n{\n  \"mcpServers\": {\n    \"AivisSpeech-MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/aivis-speech-mcp/dist/index.js\"]\n    }\n  }\n}\n```\n\n`/path/to/aivis-speech-mcp/dist/index.js`ã‚’ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚\nWindowsã®å ´åˆã¯ã€ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã‹ã€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\nä¾‹: `\"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"`\n\n## ä½¿ã„æ–¹\n\n### é–‹ç™ºãƒ¢ãƒ¼ãƒ‰\n\né–‹ç™ºä¸­ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ä»˜ãã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã§ãã¾ã™ï¼š\n\n```bash\nnpm run dev\n```\n\n### ãƒ“ãƒ«ãƒ‰\n\næœ¬ç•ªç’°å¢ƒç”¨ã«ãƒ“ãƒ«ãƒ‰ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```bash\nnpm run build\n```\n\n### æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰\n\nãƒ“ãƒ«ãƒ‰å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™ï¼š\n\n```bash\nnpm start\n```\n\n### ãƒ†ã‚¹ãƒˆ\n\nãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n\n```bash\nnpm test\n```\n\n## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n\nAivisSpeech MCP ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n\n- **MCPã‚µãƒ¼ãƒ“ã‚¹**: Model Context Protocolã«æº–æ‹ ã—ãŸã‚µãƒ¼ãƒãƒ¼ã‚’æä¾›ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™\n- **AivisSpeech ã‚µãƒ¼ãƒ“ã‚¹**: AivisSpeech Engineã®APIã¨é€šä¿¡ã—ã€éŸ³å£°åˆæˆã‚’å®Ÿè¡Œã—ã¾ã™\n\n## APIä»•æ§˜\n\nMCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æº–æ‹ ã—ãŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚ä¸»ãªæ©Ÿèƒ½ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n- éŸ³å£°åˆæˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆï¼‰\n- ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼æƒ…å ±ã®å–å¾—\n- éŸ³å£°ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n\nè©³ç´°ãªAPIä»•æ§˜ã«ã¤ã„ã¦ã¯[AivisSpeech Engine APIä»•æ§˜](https://aivis-project.github.io/AivisSpeech-Engine/api/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n## MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ã®é€£æº\n\nã“ã®ã‚µãƒ¼ãƒãƒ¼ã¯ã€Model Context Protocolï¼ˆMCPï¼‰ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«åˆ©ç”¨ã§ãã¾ã™ã€‚MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã¤ã„ã¦ã®è©³ç´°ã¯[MCPå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://modelcontextprotocol.github.io/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\nã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–ï¼š\n\n- **AivisSpeech Engineã«æ¥ç¶šã§ããªã„**: `.env`ãƒ•ã‚¡ã‚¤ãƒ«ã®`AIVIS_SPEECH_API_URL`ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n- **éŸ³å£°ãŒå†ç”Ÿã•ã‚Œãªã„**: ã‚·ã‚¹ãƒ†ãƒ ã®éŸ³å£°è¨­å®šã‚’ç¢ºèªã—ã€é©åˆ‡ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n- **ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼IDãŒè¦‹ã¤ã‹ã‚‰ãªã„**: AivisSpeech EngineãŒæ­£ã—ãèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã€åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼IDã‚’ç¢ºèªã—ã¦ãã ã•ã„\n\n## è²¢çŒ®\n\nãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€GitHubã®Issueãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’é€šã˜ã¦ãŠé¡˜ã„ã—ã¾ã™ã€‚ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚‚æ­“è¿ã—ã¾ã™ã€‚\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\n[MIT](LICENSE)\n\n## è¬è¾\n\n- [AivisSpeech Engine](https://github.com/aivis-project/AivisSpeech-Engine)ãƒãƒ¼ãƒ \n- [Model Context Protocol](https://modelcontextprotocol.github.io/)ã®é–‹ç™ºè€…\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "aivisspeech",
        "synthesis",
        "aivis",
        "speech synthesis",
        "speech server",
        "aivis speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "mamertofabian--elevenlabs-mcp-server": {
      "owner": "mamertofabian",
      "name": "elevenlabs-mcp-server",
      "url": "https://github.com/mamertofabian/elevenlabs-mcp-server",
      "imageUrl": "https://github.com/mamertofabian.png",
      "description": "Integrates with ElevenLabs text-to-speech API to generate audio from text input, manage voice generation tasks, and store history using an SQLite database. Includes a sample SvelteKit client for performing text-to-speech conversions and managing script parts.",
      "stars": 112,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# ElevenLabs MCP Server\n[![smithery badge](https://smithery.ai/badge/elevenlabs-mcp-server)](https://smithery.ai/server/elevenlabs-mcp-server)\n\nA Model Context Protocol (MCP) server that integrates with ElevenLabs text-to-speech API, featuring both a server component and a sample web-based MCP Client (SvelteKit) for managing voice generation tasks.\n\n<a href=\"https://glama.ai/mcp/servers/leukzvus7o\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/leukzvus7o/badge\" alt=\"ElevenLabs Server MCP server\" /></a>\n\n## Features\n\n- Generate audio from text using ElevenLabs API\n- Support for multiple voices and script parts\n- SQLite database for persistent history storage\n- Sample SvelteKit MCP Client for:\n  - Simple text-to-speech conversion\n  - Multi-part script management\n  - Voice history tracking and playback\n  - Audio file downloads\n\n## Installation\n\n### Installing via Smithery\n\nTo install ElevenLabs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/elevenlabs-mcp-server):\n\n```bash\nnpx -y @smithery/cli install elevenlabs-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `cline_mcp_settings.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp-server\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   uv venv\n   ```\n3. Copy `.env.example` to `.env` and fill in your ElevenLabs credentials\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/elevenlabs-mcp-server\",\n        \"run\",\n        \"elevenlabs-mcp-server\"\n      ],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n## Using the Sample SvelteKit MCP Client\n\n1. Navigate to the web UI directory:\n   ```bash\n   cd clients/web-ui\n   ```\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n3. Copy `.env.example` to `.env` and configure as needed\n4. Run the web UI:\n   ```bash\n   pnpm dev\n   ```\n5. Open http://localhost:5174 in your browser\n\n### Available Tools\n\n- `generate_audio_simple`: Generate audio from plain text using default voice settings\n- `generate_audio_script`: Generate audio from a structured script with multiple voices and actors\n- `delete_job`: Delete a job by its ID\n- `get_audio_file`: Get the audio file by its ID\n- `list_voices`: List all available voices\n- `get_voiceover_history`: Get voiceover job history. Optionally specify a job ID for a specific job.\n\n### Available Resources\n\n- `voiceover://history/{job_id}`: Get the audio file by its ID\n- `voiceover://voices`: List all available voices\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mamertofabian",
        "voice",
        "audio",
        "mamertofabian elevenlabs",
        "speech api",
        "speech conversions"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "mrgeeko--vapi-mcp": {
      "owner": "mrgeeko",
      "name": "vapi-mcp",
      "url": "https://github.com/mrgeeko/vapi-mcp",
      "imageUrl": "https://github.com/mrgeeko.png",
      "description": "Integrate voice AI capabilities into applications for managing voice assistants and conducting outbound calls. Provides advanced features for enhancing user interactions through voice conversations.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-07T08:10:10Z",
      "readme_content": "# Vapi MCP for Cursor\n\nThis project implements a Model Context Protocol (MCP) server for integrating Vapi's voice AI capabilities with Cursor.\n\n## Setup Instructions\n\n### 1. Project Structure\n\nThe Vapi MCP server is structured as follows:\n- `vapi-mcp-server/` - Main server code\n  - `src/` - TypeScript source files\n  - `dist/` - Compiled JavaScript output\n  - `.env` - Environment variables for API keys\n\n### 2. Environment Configuration\n\nCreate a `.env` file in the `vapi-mcp-server` directory with the following variables:\n\n```\n# Vapi API Keys\nVAPI_ORG_ID=your-org-id\nVAPI_PRIVATE_KEY=your-private-key\nVAPI_KNOWLEDGE_ID=your-knowledge-id\nVAPI_JWT_PRIVATE=your-jwt-private\n\n# Environment\nNODE_ENV=development\n```\n\n### 3. Building the Server\n\nTo build the server:\n\n```bash\ncd vapi-mcp/vapi-mcp-server\nnpm install\nnpm run build\n```\n\n### 4. Configuration in Cursor\n\n#### Important: Avoiding \"Client Closed\" Errors\n\nWhen configuring the Vapi MCP server in Cursor's MCP settings, pay attention to the following crucial details:\n\n1. **Working Directory**: The `cwd` parameter is required to ensure the server runs in the correct directory and can access the `.env` file properly.\n\n2. **Environment Variables**: Must be explicitly provided in the configuration, even if they exist in the `.env` file.\n\n3. **Module Type**: The server uses ES modules, so the `package.json` must include `\"type\": \"module\"`.\n\nHere's the correct configuration for `.cursor/mcp.json`:\n\n```json\n\"Vapi Voice AI Tools\": {\n  \"command\": \"node\",\n  \"type\": \"stdio\",\n  \"args\": [\n    \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server/dist/index.js\"\n  ],\n  \"cwd\": \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server\",\n  \"env\": {\n    \"VAPI_ORG_ID\": \"your-org-id\",\n    \"VAPI_PRIVATE_KEY\": \"your-private-key\",\n    \"VAPI_KNOWLEDGE_ID\": \"your-knowledge-id\",\n    \"VAPI_JWT_PRIVATE\": \"your-jwt-private\",\n    \"NODE_ENV\": \"development\"\n  }\n}\n```\n\n## Troubleshooting\n\n### \"Client Closed\" Error in Cursor\n\nIf you see \"Client Closed\" in the Cursor MCP Tools panel:\n\n1. **Check Working Directory**: Ensure the `cwd` parameter is set correctly in your mcp.json\n2. **Verify Environment Variables**: Make sure all required environment variables are passed in the configuration\n3. **Check Module Type**: Ensure `package.json` has `\"type\": \"module\"`\n4. **Inspect Permissions**: Make sure the dist/index.js file is executable (`chmod +x dist/index.js`)\n5. **Test Server Directly**: Run the server manually to check for errors:\n   ```bash\n   cd vapi-mcp/vapi-mcp-server\n   node --trace-warnings dist/index.js\n   ```\n\n### Module Not Found Errors\n\nIf you get \"Error: Cannot find module\" when running:\n\n1. **Check Working Directory**: Are you running from the correct directory?\n2. **Rebuild**: Try rebuilding the project with `npm run build`\n3. **Dependencies**: Ensure all dependencies are installed with `npm install`\n\n## Available Tools\n\nThe Vapi MCP server provides the following tools:\n\n1. **vapi_call** - Make outbound calls using Vapi's voice AI\n2. **vapi_assistant** - Manage voice assistants (create, get, list, update, delete)\n3. **vapi_conversation** - Retrieve conversation details from calls\n\n## Lessons Learned\n\n1. When integrating with Cursor's MCP:\n   - Always specify the `cwd` parameter to ensure the server runs in the correct directory\n   - Pass all required environment variables directly in the MCP configuration\n   - For ES modules, ensure package.json has `\"type\": \"module\"` and tsconfig.json uses appropriate module settings\n   - Test the server directly before configuring in Cursor\n\n2. The server command path must be absolute and correctly formed in the Cursor MCP config\n\n3. Using stdio transport type is required for proper integration with Cursor ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "ai",
        "speech",
        "voice ai",
        "voice assistants",
        "integrate voice"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nakamurau1--tts-mcp": {
      "owner": "nakamurau1",
      "name": "tts-mcp",
      "url": "https://github.com/nakamurau1/tts-mcp",
      "imageUrl": "https://github.com/nakamurau1.png",
      "description": "Integrates high-quality text-to-speech capabilities into applications, converting text to audio with customizable voice options and output formats. Provides a command-line tool for quick conversions and supports various parameters for audio customization.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T10:32:51Z",
      "readme_content": "# tts-mcp\n\nA Model Context Protocol (MCP) server and command-line tool for high-quality text-to-speech generation using the OpenAI TTS API.\n\n## Main Features\n\n- **MCP Server**: Integrate text-to-speech capabilities with Claude Desktop and other MCP-compatible clients\n- **Voice Options**: Support for multiple voice characters (alloy, nova, echo, etc.)\n- **High-Quality Audio**: Support for various output formats (MP3, WAV, OPUS, AAC)\n- **Customizable**: Configure speech speed, voice character, and additional instructions\n- **CLI Tool**: Also available as a command-line utility for direct text-to-speech conversion\n\n## Installation\n\n### Method 1: Install from Repository\n\n```bash\n# Clone the repository\ngit clone https://github.com/nakamurau1/tts-mcp.git\ncd tts-mcp\n\n# Install dependencies\nnpm install\n\n# Optional: Install globally\nnpm install -g .\n```\n\n### Method 2: Run Directly with npx (No Installation Required)\n\n```bash\n# Start the MCP server directly\nnpx tts-mcp tts-mcp-server --voice nova --model tts-1-hd\n\n# Use the CLI tool directly\nnpx tts-mcp -t \"Hello, world\" -o hello.mp3\n```\n\n## MCP Server Usage\n\nThe MCP server allows you to integrate text-to-speech functionality with Model Context Protocol (MCP) compatible clients like Claude Desktop.\n\n### Starting the MCP Server\n\n```bash\n# Start with default settings\nnpm run server\n\n# Start with custom settings\nnpm run server -- --voice nova --model tts-1-hd\n\n# Or directly with API key\nnode bin/tts-mcp-server.js --voice echo --api-key your-openai-api-key\n```\n\n### MCP Server Options\n\n```\nOptions:\n  -V, --version       Display version information\n  -m, --model <model> TTS model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <voice> Voice character (default: \"alloy\")\n  -f, --format <format> Audio format (default: \"mp3\")\n  --api-key <key>     OpenAI API key (can also be set via environment variable)\n  -h, --help          Display help information\n```\n\n### Integrating with MCP Clients\n\nThe MCP server can be used with Claude Desktop and other MCP-compatible clients. For Claude Desktop integration:\n\n1. Open the Claude Desktop configuration file (typically at `~/Library/Application Support/Claude/claude_desktop_config.json`)\n2. Add the following configuration, including your OpenAI API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"full/path/to/bin/tts-mcp-server.js\", \"--voice\", \"nova\", \"--api-key\", \"your-openai-api-key\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use npx for easier setup:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-p\", \"tts-mcp\", \"tts-mcp-server\", \"--voice\", \"nova\", \"--model\", \"gpt-4o-mini-tts\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nYou can provide the API key in two ways:\n\n1. **Direct method** (recommended for testing): Include it in the `args` array using the `--api-key` parameter\n2. **Environment variable method** (more secure): Set it in the `env` object as shown above\n\n> **Security Note**: Make sure to secure your configuration file when including API keys.\n\n3. Restart Claude Desktop\n4. When you ask Claude to \"read this text aloud\" or similar requests, the text will be converted to speech\n\n### Available MCP Tools\n\n- **text-to-speech**: Tool for converting text to speech and playing it\n\n## CLI Tool Usage\n\nYou can also use tts-mcp as a standalone command-line tool:\n\n```bash\n# Convert text directly\ntts-mcp -t \"Hello, world\" -o hello.mp3\n\n# Convert from a text file\ntts-mcp -f speech.txt -o speech.mp3\n\n# Specify custom voice\ntts-mcp -t \"Welcome to the future\" -o welcome.mp3 -v nova\n```\n\n### CLI Tool Options\n\n```\nOptions:\n  -V, --version           Display version information\n  -t, --text <text>       Text to convert\n  -f, --file <path>       Path to input text file\n  -o, --output <path>     Path to output audio file (required)\n  -m, --model <n>         Model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <n>         Voice character (default: \"alloy\")\n  -s, --speed <number>    Speech speed (0.25-4.0) (default: 1)\n  --format <format>       Output format (default: \"mp3\")\n  -i, --instructions <text> Additional instructions for speech generation\n  --api-key <key>         OpenAI API key (can also be set via environment variable)\n  -h, --help              Display help information\n```\n\n## Supported Voices\n\nThe following voice characters are supported:\n- alloy (default)\n- ash\n- coral\n- echo\n- fable\n- onyx\n- nova\n- sage\n- shimmer\n\n## Supported Models\n\n- tts-1\n- tts-1-hd\n- gpt-4o-mini-tts (default)\n\n## Output Formats\n\nThe following output formats are supported:\n- mp3 (default)\n- opus\n- aac\n- flac\n- wav\n- pcm\n\n## Environment Variables\n\nYou can also configure the tool using system environment variables:\n\n```\nOPENAI_API_KEY=your-api-key-here\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tts",
        "voice",
        "audio",
        "tts mcp",
        "text audio",
        "nakamurau1 tts"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "https://github.com/nansasuke.png",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\nå›¾ç‰‡è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€åƒåœ¾åˆ†ç±»\n\nä¸€ä¸ªå®Œæ•´çš„åƒåœ¾åˆ†ç±»çš„app\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recycling",
        "waste",
        "garbagesorting",
        "classify waste",
        "waste using",
        "nansasuke garbagesorting"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "neosapience--typecast-api-mcp-server-sample": {
      "owner": "neosapience",
      "name": "typecast-api-mcp-server-sample",
      "url": "https://github.com/neosapience/typecast-api-mcp-server-sample",
      "imageUrl": "https://github.com/neosapience.png",
      "description": "Integrates with the Typecast API to manage voices, convert text to speech, and play audio. Provides a standardized MCP interface for seamless interaction with voice capabilities.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T23:50:45Z",
      "readme_content": "# typecast-api-mcp-server-sample\n\nMCP Server for typecast-api, enabling seamless integration with MCP clients. This project provides a standardized way to interact with Typecast API through the Model Context Protocol.\n\n## About\n\nThis project implements a Model [Context Protocol server](https://modelcontextprotocol.io/introduction) for Typecast API, allowing MCP clients to interact with the Typecast API in a standardized way.\n\n## Feature Implementation Status\n\n| Feature              | Status |\n| -------------------- | ------ |\n| **Voice Management** |        |\n| Get Voices           | âœ…     |\n| Text to Speech       | âœ…     |\n| Play Audio           | âœ…     |\n\n## Setup\n\n### Git Clone\n\n```bash\ngit clone https://github.com/hyunseung/typecast-api-mcp-server-sample.git\ncd typecast-api-mcp-server-sample\n```\n\n### Dependencies\n\nThis project requires Python 3.10 or higher and uses `uv` for package management.\n\n#### Package Installation\n\n```bash\n# Create virtual environment and install packages\nuv venv\nuv pip install -e .\n```\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nTYPECAST_API_HOST=https://api.typecast.ai\nTYPECAST_API_KEY=<your-api-key>\nTYPECAST_OUTPUT_DIR=<your-output-directory> # default: ~/Downloads/typecast_output\n```\n\n### Usage with Claude Desktop\n\nYou can add the following to your `claude_desktop_config.json`:\n\n#### Basic Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"typecast-api-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/PROJECT\",\n        \"run\",\n        \"typecast-api-mcp-server\"\n      ],\n      \"env\": {\n        \"TYPECAST_API_HOST\": \"https://api.typecast.ai\",\n        \"TYPECAST_API_KEY\": \"YOUR_API_KEY\",\n        \"TYPECAST_OUTPUT_DIR\": \"PATH/TO/YOUR/OUTPUT/DIR\"\n      }\n    }\n  }\n}\n```\n\nReplace `/PATH/TO/YOUR/PROJECT` with the actual path where your project is located.\n\n### Manual Execution\n\nYou can also run the server manually:\n\n```bash\nuv run python app/main.py\n```\n\n## Contributing\n\nContributions are always welcome! Feel free to submit a Pull Request.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "typecast",
        "voice",
        "voices",
        "typecast api",
        "mcp interface",
        "api mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "pinkpixel-dev--MCPollinations": {
      "owner": "pinkpixel-dev",
      "name": "MCPollinations",
      "url": "https://github.com/pinkpixel-dev/MCPollinations",
      "imageUrl": "https://github.com/pinkpixel-dev.png",
      "description": "Generates images, text, and audio from prompts using the Pollinations APIs. It supports returning images as base64-encoded data and allows listing available models for image and text generation.",
      "stars": 34,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T03:37:33Z",
      "readme_content": "# MCPollinations Multimodal MCP Server\nA Model Context Protocol (MCP) server that enables AI assistants to generate images, text, and audio through the Pollinations APIs\n\n[![smithery badge](https://smithery.ai/badge/@pinkpixel-dev/mcpollinations)](https://smithery.ai/server/@pinkpixel-dev/mcpollinations) [![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/8448e4ec-c863-476a-8adb-aed3cf16ea2b)\n\n## Features\n\n- Generate image URLs from text prompts\n- Generate images and return them as base64-encoded data AND save as png, jpeg, jpg, or webp (default: png)\n- Generate text responses from text prompts\n- Generate audio responses from text prompts\n- List available image and text generation models\n- No authentication required\n- Simple and lightweight\n- Compatible with the Model Context Protocol (MCP)\n\n## System Requirements\n\n- **Node.js**: Version 14.0.0 or higher\n  - For best performance, we recommend Node.js 16.0.0 or higher\n  - Node.js versions below 16 use an AbortController polyfill\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install mcpollinations for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/mcpollinations):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/mcpollinations --client claude\n```\n\nThe easiest way to use the MCP server:\n\n```bash\n# Run directly with npx (no installation required)\nnpx @pinkpixel/mcpollinations\n```\n\nIf you prefer to install it globally:\n\n```bash\n# Install globally\nnpm install -g @pinkpixel/mcpollinations\n\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n\n```\n\nOr clone the repository:\n\n```bash\n# Clone the git repository\ngit clone https://github.com/pinkpixel-dev/mcpollinations.git\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n# or run directly\nnode /path/to/MCPollinations/pollinations-mcp-server.js\n\n```\n\n## MCP Integration\n\nTo integrate the server with applications that support the Model Context Protocol (MCP):\n\n1. Generate an MCP configuration file:\n\n```bash\n# If installed globally\nnpx @pinkpixel/mcpollinations generate-config\n\n# Or run directly\nnode /path/to/MCPollinations/generate-mcp-config.js\n```\n\n### Quick MCP Config (env)\nIf you prefer to skip the generator, copy this into your MCP client config:\n\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"YOUR_TOKEN_OPTIONAL\",\n      \"referrer\": \"your-app-or-domain-optional\",\n      \"IMAGE_MODEL\": \"flux\",\n      \"IMAGE_WIDTH\": \"1024\",\n      \"IMAGE_HEIGHT\": \"1024\",\n      \"IMAGE_ENHANCE\": \"true\",\n      \"IMAGE_SAFE\": \"false\",\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"\",\n      \"AUDIO_VOICE\": \"alloy\",\n      \"OUTPUT_DIR\": \"./mcpollinations-output\"\n    }\n  }\n}\n```\n\n2. Follow the prompts to customize your configuration or use the defaults.\n   - Set an output directory (relative paths recommended for portability)\n     - **Windows users**: Consider using absolute paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations`) for more reliable file saving\n   - Configure optional authentication (token, referrer) under `env`\n   - Configure default parameters for image generation (with a list of available models, dimensions, etc.)\n   - Configure default parameters for text generation (with a list of available models)\n   - Configure default parameters for audio generation (voice)\n\n\n3. Copy the generated `mcp.json` file to your application's MCP settings .json file.\n4. Restart your application.\n\nAfter integration, you can use commands like:\n\n\"Generate an image of a sunset over the ocean using MCPollinations\"\n\n## Authentication (Optional)\n\nMCPollinations supports optional authentication to provide access to more models and better rate limits. The server works perfectly without authentication (free tier), but users with API tokens can get enhanced access.\n\n### Configuration Methods\n\n**Method 1: Environment Variables (Recommended for security)**\n```bash\n# Set environment variables before running the server\nexport POLLINATIONS_TOKEN=\"your-api-token\"\nexport POLLINATIONS_REFERRER=\"https://your-domain.com\"\n\n# Then run the server\nnpx @pinkpixel/mcpollinations\n```\n\n**Method 2: MCP Configuration File (env)**\nWhen generating your MCP configuration, place auth inside `env` so your MCP client passes them as environment variables to the server process:\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"your-api-token\",\n      \"referrer\": \"your-app-or-domain\"\n    }\n  }\n}\n```\n\nYou can also provide `POLLINATIONS_TOKEN` and `POLLINATIONS_REFERRER` instead; the server recognizes both forms. Using `token` and `referrer` inside `env` is recommended for MCP configs.\n\n### Authentication Parameters\n\n- **`token`** (optional): Your Pollinations API token for enhanced access\n- **`referrer`** (optional): Your domain/application referrer URL\n\nBoth parameters are completely optional. Leave them empty or unset to use the free tier.\n\n## Using Your Configuration Settings\n\nMCPollinations respects your MCP configuration settings placed in `env` as defaults. When you ask an AI assistant to generate content:\n\n- **Your configured models, output directories, and parameters are used automatically**\n- **To override**: Specifically instruct the AI to use different settings\n  - \"Generate an image using the kontext model\"\n  - \"Save this image to my Desktop folder\"\n  - \"Use a temperature of 1.2 for this text generation\"\n\n**Example Instructions:**\n- âœ… \"Generate a sunset image\" â†’ Uses your configured model and output directory\n- âœ… \"Generate a sunset image with the flux model\" â†’ Overrides model only\n- âœ… \"Generate a sunset image and save it to C:\\Pictures\" â†’ Overrides output path only\n\nThis ensures your preferences are always respected unless you specifically want different settings for a particular request.\n\n## Troubleshooting\n\n### \"AbortController is not defined\" Error\n\nIf you encounter this error when running the MCP server:\n\n```\nReferenceError: AbortController is not defined\n```\n\nThis is usually caused by running on an older version of Node.js (below version 16.0.0). Try one of these solutions:\n\n1. **Update Node.js** (recommended):\n   - Update to Node.js 16.0.0 or newer\n\n2. **Use Global Installation**\n   - Update to the latest version of the package:\n   ```bash\n   npm install -g @pinkpixel/mcpollinations\n   # Run with npx\n   npx @pinkpixel/mcpollinations\n   ```\n\n3. **Install AbortController manually**:\n   - If for some reason the polyfill doesn't work:\n   ```bash\n   npm install node-abort-controller\n   ```\n\n### Check Your Node.js Version\n\nTo check your current Node.js version:\n\n```bash\nnode --version\n```\n\nIf it shows a version lower than 16.0.0, consider upgrading for best compatibility.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### **Image Generation Tools**\n1. `generateImageUrl` - Generates an image URL from a text prompt\n2. `generateImage` - Generates an image, returns it as base64-encoded data, and saves it to a file by default (PNG format)\n3. `editImage` - **NEW!** Edit or modify existing images based on text prompts\n4. `generateImageFromReference` - **NEW!** Generate new images using existing images as reference\n5. `listImageModels` - Lists available models for image generation\n\n### **Text & Audio Tools**\n6. `respondText` - Responds with text to a prompt using text models (customizable parameters)\n7. `respondAudio` - Generates an audio response to a text prompt (customizable voice parameter)\n8. `listTextModels` - Lists available models for text generation\n9. `listAudioVoices` - Lists all available voices for audio generation\n\n## Text Generation Details\n\n### Available Parameters\n\nThe `respondText` tool supports several parameters for fine-tuning text generation:\n\n- **`model`**: Choose from available text models (use `listTextModels` to see current options)\n- **`temperature`** (0.0-2.0): Controls randomness in the output\n  - Lower values (0.1-0.7) = more focused and deterministic\n  - Higher values (0.8-2.0) = more creative and random\n- **`top_p`** (0.0-1.0): Controls diversity via nucleus sampling\n  - Lower values = more focused on likely tokens\n  - Higher values = considers more token possibilities\n- **`system`**: System prompt to guide the model's behavior and personality\n\n### Customizing Text Generation\n\n```javascript\n// Example options for respondText\nconst options = {\n  model: \"openai\",           // Model selection\n  temperature: 0.7,          // Balanced creativity\n  top_p: 0.9,               // High diversity\n  system: \"You are a helpful assistant that explains things clearly and concisely.\"\n};\n```\n\n### Configuration Examples\n\nIn your MCP configuration, set defaults under `env` so the server uses them automatically:\n\n```json\n{\n  \"mcpollinations\": {\n    \"env\": {\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"You are a helpful coding assistant.\"\n    }\n  }\n}\n```\n\n## Image-to-Image Generation (NEW!)\n\nMCPollinations now supports powerful image-to-image generation with two specialized tools:\n\n### **editImage Tool**\nPerfect for modifying existing images:\n- **Remove objects**: \"remove the cat from this image\"\n- **Add elements**: \"add a dog to this scene\"\n- **Change backgrounds**: \"replace the background with mountains\"\n- **Style modifications**: \"make the lighting more dramatic\"\n\n### **generateImageFromReference Tool**\nPerfect for creating variations and new styles:\n- **Style transfer**: \"make this photo look like a painting\"\n- **Format changes**: \"convert this to a cartoon style\"\n- **Creative variations**: \"create a futuristic version of this\"\n- **Artistic interpretations**: \"make this look like a sketch\"\n\n### **Supported Models**\n- **`kontext`**: Specialized model optimized for image-to-image tasks\n- **`nanobanana`**: New Google model supporting both text-to-image and image-to-image generation\n- **`seedream`**: New ByteDance model supporting both text-to-image and image-to-image generation\n\nMulti-reference images: `editImage` and `generateImageFromReference` accept `imageUrl` as a single URL or an array of URLs. The server encodes arrays as the comma-separated `image` parameter used by the API. Ordering matters; kontext uses only the first image, nanobanana is safe up to ~4 refs, and seedream supports up to 10.\n\nImportant: URLs only. The image-to-image tools require publicly accessible HTTP(S) URLs. Local file paths, file uploads, and base64/data URLs are not supported by this MCP server (it does not upload files). If you need to work from a local image, host it somewhere accessible (e.g., a temporary file host, object storage, or a raw link in a repo) and pass the URL.\n\n### **Example Usage**\n```javascript\n// Edit an existing image\nconst editResult = await editImage(\n  \"change the background to a sunset beach\",\n  \"https://example.com/photo.jpg\",\n  \"nanobanana\"  // or \"kontext\", \"seedream\"\n);\n\n// Generate from reference\nconst referenceResult = await generateImageFromReference(\n  \"make this into a watercolor painting\",\n  \"https://example.com/photo.jpg\",\n  \"seedream\"  // or \"kontext\", \"nanobanana\"\n);\n```\n\n## Image Generation Details\n\n### Default Behavior\n\nWhen using the `generateImage` tool:\n\n- Images are saved to disk by default as PNG files\n- The default save location is the current working directory where the MCP server is running\n- The 'flux' model is used by default\n- A random seed is generated by default for each image (ensuring variety)\n- Base64-encoded image data is always returned, regardless of whether the image is saved to a file\n\n### Customizing Image Generation\n\n```javascript\n// Example options for generateImage\nconst options = {\n  // Model selection (defaults to 'flux')\n  // Available models: \"flux\", \"turbo\", \"kontext\", \"nanobanana\", \"seedream\"\n  model: \"flux\",\n\n  // Image dimensions\n  width: 1024,\n  height: 1024,\n\n  // Generation options\n  seed: 12345,  // Specific seed for reproducibility (defaults to random)\n  enhance: true,  // Enhance the prompt using an LLM before generating (defaults to true)\n  safe: false,  // Content filtering (defaults to false)\n\n  // File saving options\n  saveToFile: true,  // Set to false to skip saving to disk\n  outputPath: \"/path/to/save/directory\",  // Custom save location\n  fileName: \"my_custom_name\",  // Without extension\n  format: \"png\"  // png, jpeg, jpg, or webp\n};\n```\n\n### Where Images Are Saved\n\nWhen using Claude or another application with the MCP server:\n\n1. **Images are saved in the current working directory of where the MCP server is running**, not where Claude or the client application is installed.\n\n2. If you start the MCP server manually from a specific directory, images will be saved there by default.\n\n3. If Claude Desktop launches the MCP server automatically, images will be saved in Claude Desktop's working directory (typically in an application data folder).\n\n**ğŸ’¡ Windows Users**: For reliable file saving on Windows, use absolute paths in your MCP configuration instead of relative paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations` instead of `./mcpollinations-output`). Relative paths may not resolve as expected depending on the working directory context.\n\n### Finding Your Generated Images\n\n- The response from Claude after generating an image includes the full file path where the image was saved\n- You can specify a familiar location using the `outputPath` parameter\n- Best practice: Ask Claude to save images to an easily accessible folder like your Pictures or Downloads directory\n\n### Unique Filenames\n\nThe MCP server ensures that generated images always have unique filenames and will never overwrite existing files:\n\n1. **Default filenames** include:\n   - A sanitized version of the prompt (first 20 characters)\n   - A timestamp\n   - A random suffix\n\n2. **Custom filenames** are also protected:\n   - If you specify a filename and a file with that name already exists, a numeric suffix will be added automatically\n   - For example: `sunset.png`, `sunset_1.png`, `sunset_2.png`, etc.\n\nThis means you can safely generate multiple images with the same prompt or filename without worrying about overwriting previous images.\n\n### Accessing Base64 Data\n\nEven when saving to a file, the base64-encoded image data is always returned and can be used for:\n\n- Embedding in web pages (`<img src=\"data:image/png;base64,...\" />`)\n- Passing to other services or APIs\n- Processing in memory without filesystem operations\n- Displaying in applications that support data URIs\n\n## For Developers\n\nIf you want to use the package in your own projects:\n\n```bash\n# Install as a dependency\nnpm install @pinkpixel/mcpollinations\n\n# Import in your code\nimport { generateImageUrl, generateImage, repsondText, respondAudio, listTextModels, listImageModels, listAudioVoices } from '@pinkpixel/mcpollinations';\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pinkpixel",
        "audio",
        "images",
        "pinkpixel dev",
        "synthesis pinkpixel",
        "generates images"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "rsagacom--chatgpt-on-wechat": {
      "owner": "rsagacom",
      "name": "chatgpt-on-wechat",
      "url": "https://github.com/rsagacom/chatgpt-on-wechat",
      "imageUrl": "https://github.com/rsagacom.png",
      "description": "A multi-platform intelligent dialogue service that supports text, voice, and image interactions. It can connect to various AI models and allows for custom enterprise AI applications through plugin extensions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2024-01-28T14:00:49Z",
      "readme_content": "# ç®€ä»‹\n\n> æœ¬é¡¹ç›®æ˜¯åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯æœºå™¨äººï¼Œæ”¯æŒå¾®ä¿¡ã€ä¼ä¸šå¾®ä¿¡ã€å…¬ä¼—å·ã€é£ä¹¦ã€é’‰é’‰æ¥å…¥ï¼Œå¯é€‰æ‹©GPT3.5/GPT4.0/Claude/æ–‡å¿ƒä¸€è¨€/è®¯é£æ˜Ÿç«/é€šä¹‰åƒé—®/Gemini/LinkAIï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³å’Œå›¾ç‰‡ï¼Œé€šè¿‡æ’ä»¶è®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ç­‰å¤–éƒ¨èµ„æºï¼Œæ”¯æŒåŸºäºè‡ªæœ‰çŸ¥è¯†åº“å®šåˆ¶ä¼ä¸šAIåº”ç”¨ã€‚\n\næœ€æ–°ç‰ˆæœ¬æ”¯æŒçš„åŠŸèƒ½å¦‚ä¸‹ï¼š\n\n- [x] **å¤šç«¯éƒ¨ç½²ï¼š** æœ‰å¤šç§éƒ¨ç½²æ–¹å¼å¯é€‰æ‹©ä¸”åŠŸèƒ½å®Œå¤‡ï¼Œç›®å‰å·²æ”¯æŒä¸ªäººå¾®ä¿¡ã€å¾®ä¿¡å…¬ä¼—å·å’Œã€ä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€é’‰é’‰ç­‰éƒ¨ç½²æ–¹å¼\n- [x] **åŸºç¡€å¯¹è¯ï¼š** ç§èŠåŠç¾¤èŠçš„æ¶ˆæ¯æ™ºèƒ½å›å¤ï¼Œæ”¯æŒå¤šè½®ä¼šè¯ä¸Šä¸‹æ–‡è®°å¿†ï¼Œæ”¯æŒ GPT-3.5, GPT-4, claude, Gemini, æ–‡å¿ƒä¸€è¨€, è®¯é£æ˜Ÿç«, é€šä¹‰åƒé—®\n- [x] **è¯­éŸ³èƒ½åŠ›ï¼š** å¯è¯†åˆ«è¯­éŸ³æ¶ˆæ¯ï¼Œé€šè¿‡æ–‡å­—æˆ–è¯­éŸ³å›å¤ï¼Œæ”¯æŒ azure, baidu, google, openai(whisper/tts) ç­‰å¤šç§è¯­éŸ³æ¨¡å‹\n- [x] **å›¾åƒèƒ½åŠ›ï¼š** æ”¯æŒå›¾ç‰‡ç”Ÿæˆã€å›¾ç‰‡è¯†åˆ«ã€å›¾ç”Ÿå›¾ï¼ˆå¦‚ç…§ç‰‡ä¿®å¤ï¼‰ï¼Œå¯é€‰æ‹© Dall-E-3, stable diffusion, replicate, midjourney, visionæ¨¡å‹\n- [x] **ä¸°å¯Œæ’ä»¶ï¼š** æ”¯æŒä¸ªæ€§åŒ–æ’ä»¶æ‰©å±•ï¼Œå·²å®ç°å¤šè§’è‰²åˆ‡æ¢ã€æ–‡å­—å†’é™©ã€æ•æ„Ÿè¯è¿‡æ»¤ã€èŠå¤©è®°å½•æ€»ç»“ã€æ–‡æ¡£æ€»ç»“å’Œå¯¹è¯ã€è”ç½‘æœç´¢ç­‰æ’ä»¶\n- [x] **çŸ¥è¯†åº“ï¼š** é€šè¿‡ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶è‡ªå®šä¹‰ä¸“å±æœºå™¨äººï¼Œå¯ä½œä¸ºæ•°å­—åˆ†èº«ã€æ™ºèƒ½å®¢æœã€ç§åŸŸåŠ©æ‰‹ä½¿ç”¨ï¼ŒåŸºäº [LinkAI](https://link-ai.tech) å®ç°\n\n# æ¼”ç¤º\n\nhttps://github.com/zhayujie/chatgpt-on-wechat/assets/26161723/d5154020-36e3-41db-8706-40ce9f3f1b1e\n\nDemo made by [Visionn](https://www.wangpc.cc/)\n\n# å•†ä¸šæ”¯æŒ\n\n> æˆ‘ä»¬è¿˜æä¾›ä¼ä¸šçº§çš„ **AIåº”ç”¨å¹³å°**ï¼ŒåŒ…å«çŸ¥è¯†åº“ã€Agentæ’ä»¶ã€åº”ç”¨ç®¡ç†ç­‰èƒ½åŠ›ï¼Œæ”¯æŒå¤šå¹³å°èšåˆçš„åº”ç”¨æ¥å…¥ã€å®¢æˆ·ç«¯ç®¡ç†ã€å¯¹è¯ç®¡ç†ï¼Œä»¥åŠæä¾›\nSaaSæœåŠ¡ã€ç§æœ‰åŒ–éƒ¨ç½²ã€ç¨³å®šæ‰˜ç®¡æ¥å…¥ ç­‰å¤šç§æ¨¡å¼ã€‚\n>\n> ç›®å‰å·²åœ¨ç§åŸŸè¿è¥ã€æ™ºèƒ½å®¢æœã€ä¼ä¸šæ•ˆç‡åŠ©æ‰‹ç­‰åœºæ™¯ç§¯ç´¯äº†ä¸°å¯Œçš„ AI è§£å†³æ–¹æ¡ˆï¼Œ åœ¨ç”µå•†ã€æ–‡æ•™ã€å¥åº·ã€æ–°æ¶ˆè´¹ç­‰å„è¡Œä¸šæ²‰æ·€äº† AI è½åœ°çš„æœ€ä½³å®è·µï¼Œè‡´åŠ›äºæ‰“é€ åŠ©åŠ›ä¸­å°ä¼ä¸šæ‹¥æŠ± AI çš„ä¸€ç«™å¼å¹³å°ã€‚\n\nä¼ä¸šæœåŠ¡å’Œå•†ç”¨å’¨è¯¢å¯è”ç³»äº§å“é¡¾é—®ï¼š\n\n<img width=\"240\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg\">\n\n# å¼€æºç¤¾åŒº\n\næ·»åŠ å°åŠ©æ‰‹å¾®ä¿¡åŠ å…¥å¼€æºé¡¹ç›®äº¤æµç¾¤ï¼š\n\n<img width=\"240\" src=\"./docs/images/contact.jpg\">\n\n# æ›´æ–°æ—¥å¿—\n\n>**2023.11.11ï¼š** [1.5.3ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) å’Œ [1.5.4ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)ï¼Œæ–°å¢Google Geminiã€é€šä¹‰åƒé—®æ¨¡å‹\n\n>**2023.11.10ï¼š** [1.5.2ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)ï¼Œæ–°å¢é£ä¹¦é€šé“ã€å›¾åƒè¯†åˆ«å¯¹è¯ã€é»‘åå•é…ç½®\n\n>**2023.11.10ï¼š** [1.5.0ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)ï¼Œæ–°å¢ `gpt-4-turbo`, `dall-e-3`, `tts` æ¨¡å‹æ¥å…¥ï¼Œå®Œå–„å›¾åƒç†è§£&ç”Ÿæˆã€è¯­éŸ³è¯†åˆ«&ç”Ÿæˆçš„å¤šæ¨¡æ€èƒ½åŠ›\n\n>**2023.10.16ï¼š** æ”¯æŒé€šè¿‡æ„å›¾è¯†åˆ«ä½¿ç”¨LinkAIè”ç½‘æœç´¢ã€æ•°å­¦è®¡ç®—ã€ç½‘é¡µè®¿é—®ç­‰æ’ä»¶ï¼Œå‚è€ƒ[æ’ä»¶æ–‡æ¡£](https://docs.link-ai.tech/platform/plugins)\n\n>**2023.09.26ï¼š** æ’ä»¶å¢åŠ  æ–‡ä»¶/æ–‡ç« é“¾æ¥ ä¸€é”®æ€»ç»“å’Œå¯¹è¯çš„åŠŸèƒ½ï¼Œä½¿ç”¨å‚è€ƒï¼š[æ’ä»¶è¯´æ˜](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)\n\n>**2023.08.08ï¼š** æ¥å…¥ç™¾åº¦æ–‡å¿ƒä¸€è¨€æ¨¡å‹ï¼Œé€šè¿‡ [æ’ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) æ”¯æŒ Midjourney ç»˜å›¾\n\n>**2023.06.12ï¼š** æ¥å…¥ [LinkAI](https://link-ai.tech/console) å¹³å°ï¼Œå¯åœ¨çº¿åˆ›å»ºé¢†åŸŸçŸ¥è¯†åº“ï¼Œå¹¶æ¥å…¥å¾®ä¿¡ã€å…¬ä¼—å·åŠä¼ä¸šå¾®ä¿¡ä¸­ï¼Œæ‰“é€ ä¸“å±å®¢æœæœºå™¨äººã€‚ä½¿ç”¨å‚è€ƒ [æ¥å…¥æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)ã€‚\n\n>**2023.04.26ï¼š** æ”¯æŒä¼ä¸šå¾®ä¿¡åº”ç”¨å·éƒ¨ç½²ï¼Œå…¼å®¹æ’ä»¶ï¼Œå¹¶æ”¯æŒè¯­éŸ³å›¾ç‰‡äº¤äº’ï¼Œç§äººåŠ©ç†ç†æƒ³é€‰æ‹©ï¼Œ[ä½¿ç”¨æ–‡æ¡£](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatcom/README.md)ã€‚(contributed by [@lanvent](https://github.com/lanvent) in [#944](https://github.com/zhayujie/chatgpt-on-wechat/pull/944))\n\n>**2023.04.05ï¼š** æ”¯æŒå¾®ä¿¡å…¬ä¼—å·éƒ¨ç½²ï¼Œå…¼å®¹æ’ä»¶ï¼Œå¹¶æ”¯æŒè¯­éŸ³å›¾ç‰‡äº¤äº’ï¼Œ[ä½¿ç”¨æ–‡æ¡£](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatmp/README.md)ã€‚(contributed by [@JS00000](https://github.com/JS00000) in [#686](https://github.com/zhayujie/chatgpt-on-wechat/pull/686))\n\n>**2023.04.05ï¼š** å¢åŠ èƒ½è®©ChatGPTä½¿ç”¨å·¥å…·çš„`tool`æ’ä»¶ï¼Œ[ä½¿ç”¨æ–‡æ¡£](https://github.com/goldfishh/chatgpt-on-wechat/blob/master/plugins/tool/README.md)ã€‚å·¥å…·ç›¸å…³issueå¯åé¦ˆè‡³[chatgpt-tool-hub](https://github.com/goldfishh/chatgpt-tool-hub)ã€‚(contributed by [@goldfishh](https://github.com/goldfishh) in [#663](https://github.com/zhayujie/chatgpt-on-wechat/pull/663))\n\n>**2023.03.25ï¼š** æ”¯æŒæ’ä»¶åŒ–å¼€å‘ï¼Œç›®å‰å·²å®ç° å¤šè§’è‰²åˆ‡æ¢ã€æ–‡å­—å†’é™©æ¸¸æˆã€ç®¡ç†å‘˜æŒ‡ä»¤ã€Stable Diffusionç­‰æ’ä»¶ï¼Œä½¿ç”¨å‚è€ƒ [#578](https://github.com/zhayujie/chatgpt-on-wechat/issues/578)ã€‚(contributed by [@lanvent](https://github.com/lanvent) in [#565](https://github.com/zhayujie/chatgpt-on-wechat/pull/565))\n\n>**2023.03.09ï¼š** åŸºäº `whisper API`(åç»­å·²æ¥å…¥æ›´å¤šçš„è¯­éŸ³`API`æœåŠ¡) å®ç°å¯¹å¾®ä¿¡è¯­éŸ³æ¶ˆæ¯çš„è§£æå’Œå›å¤ï¼Œæ·»åŠ é…ç½®é¡¹ `\"speech_recognition\":true` å³å¯å¯ç”¨ï¼Œä½¿ç”¨å‚è€ƒ [#415](https://github.com/zhayujie/chatgpt-on-wechat/issues/415)ã€‚(contributed by [wanggang1987](https://github.com/wanggang1987) in [#385](https://github.com/zhayujie/chatgpt-on-wechat/pull/385))\n\n>**2023.02.09ï¼š** æ‰«ç ç™»å½•å­˜åœ¨è´¦å·é™åˆ¶é£é™©ï¼Œè¯·è°¨æ…ä½¿ç”¨ï¼Œå‚è€ƒ[#58](https://github.com/AutumnWhj/ChatGPT-wechat-bot/issues/158)\n\n# å¿«é€Ÿå¼€å§‹\n\nå¿«é€Ÿå¼€å§‹æ–‡æ¡£ï¼š[é¡¹ç›®æ­å»ºæ–‡æ¡£](https://docs.link-ai.tech/cow/quick-start)\n\n## å‡†å¤‡\n\n### 1. è´¦å·æ³¨å†Œ\n\né¡¹ç›®é»˜è®¤ä½¿ç”¨OpenAIæ¥å£ï¼Œéœ€å‰å¾€ [OpenAIæ³¨å†Œé¡µé¢](https://beta.openai.com/signup) åˆ›å»ºè´¦å·ï¼Œåˆ›å»ºå®Œè´¦å·åˆ™å‰å¾€ [APIç®¡ç†é¡µé¢](https://beta.openai.com/account/api-keys) åˆ›å»ºä¸€ä¸ª API Key å¹¶ä¿å­˜ä¸‹æ¥ï¼Œåé¢éœ€è¦åœ¨é¡¹ç›®ä¸­é…ç½®è¿™ä¸ªkeyã€‚æ¥å£éœ€è¦æµ·å¤–ç½‘ç»œè®¿é—®åŠç»‘å®šä¿¡ç”¨å¡æ”¯ä»˜ã€‚\n\n> é»˜è®¤å¯¹è¯æ¨¡å‹æ˜¯ openai çš„ gpt-3.5-turboï¼Œè®¡è´¹æ–¹å¼æ˜¯çº¦æ¯ 1000tokens (çº¦750ä¸ªè‹±æ–‡å•è¯ æˆ– 500æ±‰å­—ï¼ŒåŒ…å«è¯·æ±‚å’Œå›å¤) æ¶ˆè€— $0.002ï¼Œå›¾ç‰‡ç”Ÿæˆæ˜¯Dell Eæ¨¡å‹ï¼Œæ¯å¼ æ¶ˆè€— $0.016ã€‚\n\né¡¹ç›®åŒæ—¶ä¹Ÿæ”¯æŒä½¿ç”¨ LinkAI æ¥å£ï¼Œæ— éœ€ä»£ç†ï¼Œå¯ä½¿ç”¨ æ–‡å¿ƒã€è®¯é£ã€GPT-3ã€GPT-4 ç­‰æ¨¡å‹ï¼Œæ”¯æŒ å®šåˆ¶åŒ–çŸ¥è¯†åº“ã€è”ç½‘æœç´¢ã€MJç»˜å›¾ã€æ–‡æ¡£æ€»ç»“å’Œå¯¹è¯ç­‰èƒ½åŠ›ã€‚ä¿®æ”¹é…ç½®å³å¯ä¸€é”®åˆ‡æ¢ï¼Œå‚è€ƒ [æ¥å…¥æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)ã€‚\n\n### 2.è¿è¡Œç¯å¢ƒ\n\næ”¯æŒ Linuxã€MacOSã€Windows ç³»ç»Ÿï¼ˆå¯åœ¨LinuxæœåŠ¡å™¨ä¸Šé•¿æœŸè¿è¡Œ)ï¼ŒåŒæ—¶éœ€å®‰è£… `Python`ã€‚\n> å»ºè®®Pythonç‰ˆæœ¬åœ¨ 3.7.1~3.9.X ä¹‹é—´ï¼Œæ¨è3.8ç‰ˆæœ¬ï¼Œ3.10åŠä»¥ä¸Šç‰ˆæœ¬åœ¨ MacOS å¯ç”¨ï¼Œå…¶ä»–ç³»ç»Ÿä¸Šä¸ç¡®å®šèƒ½å¦æ­£å¸¸è¿è¡Œã€‚\n\n> æ³¨æ„ï¼šDocker æˆ– Railway éƒ¨ç½²æ— éœ€å®‰è£…pythonç¯å¢ƒå’Œä¸‹è½½æºç ï¼Œå¯ç›´æ¥å¿«è¿›åˆ°ä¸‹ä¸€èŠ‚ã€‚\n\n**(1) å…‹éš†é¡¹ç›®ä»£ç ï¼š**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\næ³¨: å¦‚é‡åˆ°ç½‘ç»œé—®é¢˜å¯é€‰æ‹©å›½å†…é•œåƒ https://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) å®‰è£…æ ¸å¿ƒä¾èµ– (å¿…é€‰)ï¼š**\n> èƒ½å¤Ÿä½¿ç”¨`itchat`åˆ›å»ºæœºå™¨äººï¼Œå¹¶å…·æœ‰æ–‡å­—äº¤æµåŠŸèƒ½æ‰€éœ€çš„æœ€å°ä¾èµ–é›†åˆã€‚\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) æ‹“å±•ä¾èµ– (å¯é€‰ï¼Œå»ºè®®å®‰è£…)ï¼š**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n> å¦‚æœæŸé¡¹ä¾èµ–å®‰è£…å¤±è´¥å¯æ³¨é‡Šæ‰å¯¹åº”çš„è¡Œå†ç»§ç»­\n\n## é…ç½®\n\né…ç½®æ–‡ä»¶çš„æ¨¡æ¿åœ¨æ ¹ç›®å½•çš„`config-template.json`ä¸­ï¼Œéœ€å¤åˆ¶è¯¥æ¨¡æ¿åˆ›å»ºæœ€ç»ˆç”Ÿæ•ˆçš„ `config.json` æ–‡ä»¶ï¼š\n\n```bash\n  cp config-template.json config.json\n```\n\nç„¶ååœ¨`config.json`ä¸­å¡«å…¥é…ç½®ï¼Œä»¥ä¸‹æ˜¯å¯¹é»˜è®¤é…ç½®çš„è¯´æ˜ï¼Œå¯æ ¹æ®éœ€è¦è¿›è¡Œè‡ªå®šä¹‰ä¿®æ”¹ï¼ˆè¯·å»æ‰æ³¨é‡Šï¼‰ï¼š\n\n```bash\n# config.jsonæ–‡ä»¶å†…å®¹ç¤ºä¾‹\n{\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # å¡«å…¥ä¸Šé¢åˆ›å»ºçš„ OpenAI API KEY\n  \"model\": \"gpt-3.5-turbo\",                                   # æ¨¡å‹åç§°, æ”¯æŒ gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, wenxin, xunfei\n  \"proxy\": \"\",                                                # ä»£ç†å®¢æˆ·ç«¯çš„ipå’Œç«¯å£ï¼Œå›½å†…ç¯å¢ƒå¼€å¯ä»£ç†çš„éœ€è¦å¡«å†™è¯¥é¡¹ï¼Œå¦‚ \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # ç§èŠæ—¶æ–‡æœ¬éœ€è¦åŒ…å«è¯¥å‰ç¼€æ‰èƒ½è§¦å‘æœºå™¨äººå›å¤\n  \"single_chat_reply_prefix\": \"[bot] \",                       # ç§èŠæ—¶è‡ªåŠ¨å›å¤çš„å‰ç¼€ï¼Œç”¨äºåŒºåˆ†çœŸäºº\n  \"group_chat_prefix\": [\"@bot\"],                              # ç¾¤èŠæ—¶åŒ…å«è¯¥å‰ç¼€åˆ™ä¼šè§¦å‘æœºå™¨äººå›å¤\n  \"group_name_white_list\": [\"ChatGPTæµ‹è¯•ç¾¤\", \"ChatGPTæµ‹è¯•ç¾¤2\"], # å¼€å¯è‡ªåŠ¨å›å¤çš„ç¾¤åç§°åˆ—è¡¨\n  \"group_chat_in_one_session\": [\"ChatGPTæµ‹è¯•ç¾¤\"],              # æ”¯æŒä¼šè¯ä¸Šä¸‹æ–‡å…±äº«çš„ç¾¤åç§°  \n  \"image_create_prefix\": [\"ç”»\", \"çœ‹\", \"æ‰¾\"],                   # å¼€å¯å›¾ç‰‡å›å¤çš„å‰ç¼€\n  \"conversation_max_tokens\": 1000,                            # æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†çš„æœ€å¤šå­—ç¬¦æ•°\n  \"speech_recognition\": false,                                # æ˜¯å¦å¼€å¯è¯­éŸ³è¯†åˆ«\n  \"group_speech_recognition\": false,                          # æ˜¯å¦å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«\n  \"use_azure_chatgpt\": false,                                 # æ˜¯å¦ä½¿ç”¨Azure ChatGPT serviceä»£æ›¿openai ChatGPT service. å½“è®¾ç½®ä¸ºtrueæ—¶éœ€è¦è®¾ç½® open_ai_api_baseï¼Œå¦‚ https://xxx.openai.azure.com/\n  \"azure_deployment_id\": \"\",                                  # é‡‡ç”¨Azure ChatGPTæ—¶ï¼Œæ¨¡å‹éƒ¨ç½²åç§°\n  \"azure_api_version\": \"\",                                    # é‡‡ç”¨Azure ChatGPTæ—¶ï¼ŒAPIç‰ˆæœ¬\n  \"character_desc\": \"ä½ æ˜¯ChatGPT, ä¸€ä¸ªç”±OpenAIè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹, ä½ æ—¨åœ¨å›ç­”å¹¶è§£å†³äººä»¬çš„ä»»ä½•é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤šç§è¯­è¨€ä¸äººäº¤æµã€‚\",  # äººæ ¼æè¿°\n  # è®¢é˜…æ¶ˆæ¯ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­è¯·å¡«å†™ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ï¼Œå¯ä½¿ç”¨ç‰¹æ®Šå ä½ç¬¦ã€‚ç›®å‰æ”¯æŒçš„å ä½ç¬¦æœ‰{trigger_prefix}ï¼Œåœ¨ç¨‹åºä¸­å®ƒä¼šè‡ªåŠ¨æ›¿æ¢æˆbotçš„è§¦å‘è¯ã€‚\n  \"subscribe_msg\": \"æ„Ÿè°¢æ‚¨çš„å…³æ³¨ï¼\\nè¿™é‡Œæ˜¯ChatGPTï¼Œå¯ä»¥è‡ªç”±å¯¹è¯ã€‚\\næ”¯æŒè¯­éŸ³å¯¹è¯ã€‚\\næ”¯æŒå›¾ç‰‡è¾“å‡ºï¼Œç”»å­—å¼€å¤´çš„æ¶ˆæ¯å°†æŒ‰è¦æ±‚åˆ›ä½œå›¾ç‰‡ã€‚\\næ”¯æŒè§’è‰²æ‰®æ¼”å’Œæ–‡å­—å†’é™©ç­‰ä¸°å¯Œæ’ä»¶ã€‚\\nè¾“å…¥{trigger_prefix}#help æŸ¥çœ‹è¯¦ç»†æŒ‡ä»¤ã€‚\",\n  \"use_linkai\": false,                                        # æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œé»˜è®¤å…³é—­ï¼Œå¼€å¯åå¯å›½å†…è®¿é—®ï¼Œä½¿ç”¨çŸ¥è¯†åº“å’ŒMJ\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI åº”ç”¨code\n}\n```\n**é…ç½®è¯´æ˜ï¼š**\n\n**1.ä¸ªäººèŠå¤©**\n\n+ ä¸ªäººèŠå¤©ä¸­ï¼Œéœ€è¦ä»¥ \"bot\"æˆ–\"@bot\" ä¸ºå¼€å¤´çš„å†…å®¹è§¦å‘æœºå™¨äººï¼Œå¯¹åº”é…ç½®é¡¹ `single_chat_prefix` (å¦‚æœä¸éœ€è¦ä»¥å‰ç¼€è§¦å‘å¯ä»¥å¡«å†™  `\"single_chat_prefix\": [\"\"]`)\n+ æœºå™¨äººå›å¤çš„å†…å®¹ä¼šä»¥ \"[bot] \" ä½œä¸ºå‰ç¼€ï¼Œ ä»¥åŒºåˆ†çœŸäººï¼Œå¯¹åº”çš„é…ç½®é¡¹ä¸º `single_chat_reply_prefix` (å¦‚æœä¸éœ€è¦å‰ç¼€å¯ä»¥å¡«å†™ `\"single_chat_reply_prefix\": \"\"`)\n\n**2.ç¾¤ç»„èŠå¤©**\n\n+ ç¾¤ç»„èŠå¤©ä¸­ï¼Œç¾¤åç§°éœ€é…ç½®åœ¨ `group_name_white_list ` ä¸­æ‰èƒ½å¼€å¯ç¾¤èŠè‡ªåŠ¨å›å¤ã€‚å¦‚æœæƒ³å¯¹æ‰€æœ‰ç¾¤èŠç”Ÿæ•ˆï¼Œå¯ä»¥ç›´æ¥å¡«å†™ `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ é»˜è®¤åªè¦è¢«äºº @ å°±ä¼šè§¦å‘æœºå™¨äººè‡ªåŠ¨å›å¤ï¼›å¦å¤–ç¾¤èŠå¤©ä¸­åªè¦æ£€æµ‹åˆ°ä»¥ \"@bot\" å¼€å¤´çš„å†…å®¹ï¼ŒåŒæ ·ä¼šè‡ªåŠ¨å›å¤ï¼ˆæ–¹ä¾¿è‡ªå·±è§¦å‘ï¼‰ï¼Œè¿™å¯¹åº”é…ç½®é¡¹ `group_chat_prefix`\n+ å¯é€‰é…ç½®: `group_name_keyword_white_list`é…ç½®é¡¹æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤åç§°ï¼Œ`group_chat_keyword`é…ç½®é¡¹åˆ™æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤æ¶ˆæ¯å†…å®¹ï¼Œç”¨æ³•ä¸ä¸Šè¿°ä¸¤ä¸ªé…ç½®é¡¹ç›¸åŒã€‚ï¼ˆContributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`ï¼šä½¿ç¾¤èŠå…±äº«ä¸€ä¸ªä¼šè¯ä¸Šä¸‹æ–‡ï¼Œé…ç½® `[\"ALL_GROUP\"]` åˆ™ä½œç”¨äºæ‰€æœ‰ç¾¤èŠ\n\n**3.è¯­éŸ³è¯†åˆ«**\n\n+ æ·»åŠ  `\"speech_recognition\": true` å°†å¼€å¯è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œè¯¥å‚æ•°ä»…æ”¯æŒç§èŠ (æ³¨æ„ç”±äºè¯­éŸ³æ¶ˆæ¯æ— æ³•åŒ¹é…å‰ç¼€ï¼Œä¸€æ—¦å¼€å¯å°†å¯¹æ‰€æœ‰è¯­éŸ³è‡ªåŠ¨å›å¤ï¼Œæ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›\n+ æ·»åŠ  `\"group_speech_recognition\": true` å°†å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œå‚æ•°ä»…æ”¯æŒç¾¤èŠ (ä¼šåŒ¹é…group_chat_prefixå’Œgroup_chat_keyword, æ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›\n+ æ·»åŠ  `\"voice_reply_voice\": true` å°†å¼€å¯è¯­éŸ³å›å¤è¯­éŸ³ï¼ˆåŒæ—¶ä½œç”¨äºç§èŠå’Œç¾¤èŠï¼‰ï¼Œä½†æ˜¯éœ€è¦é…ç½®å¯¹åº”è¯­éŸ³åˆæˆå¹³å°çš„keyï¼Œç”±äºitchatåè®®çš„é™åˆ¶ï¼Œåªèƒ½å‘é€è¯­éŸ³mp3æ–‡ä»¶ï¼Œè‹¥ä½¿ç”¨wechatyåˆ™å›å¤çš„æ˜¯å¾®ä¿¡è¯­éŸ³ã€‚\n\n**4.å…¶ä»–é…ç½®**\n\n+ `model`: æ¨¡å‹åç§°ï¼Œç›®å‰æ”¯æŒ `gpt-3.5-turbo`, `text-davinci-003`, `gpt-4`, `gpt-4-32k`, `wenxin` , `claude` ,  `xunfei`(å…¶ä¸­gpt-4 apiæš‚æœªå®Œå…¨å¼€æ”¾ï¼Œç”³è¯·é€šè¿‡åå¯ä½¿ç”¨)\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIæ¥å£å‚æ•°ï¼Œè¯¦æƒ…å‚è€ƒ[OpenAIå®˜æ–¹æ–‡æ¡£ã€‚](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`ï¼šç”±äºç›®å‰ `openai` æ¥å£å›½å†…æ— æ³•è®¿é—®ï¼Œéœ€é…ç½®ä»£ç†å®¢æˆ·ç«¯çš„åœ°å€ï¼Œè¯¦æƒ…å‚è€ƒ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ å¯¹äºå›¾åƒç”Ÿæˆï¼Œåœ¨æ»¡è¶³ä¸ªäººæˆ–ç¾¤ç»„è§¦å‘æ¡ä»¶å¤–ï¼Œè¿˜éœ€è¦é¢å¤–çš„å…³é”®è¯å‰ç¼€æ¥è§¦å‘ï¼Œå¯¹åº”é…ç½® `image_create_prefix `\n+ å…³äºOpenAIå¯¹è¯åŠå›¾ç‰‡æ¥å£çš„å‚æ•°é…ç½®ï¼ˆå†…å®¹è‡ªç”±åº¦ã€å›å¤å­—æ•°é™åˆ¶ã€å›¾ç‰‡å¤§å°ç­‰ï¼‰ï¼Œå¯ä»¥å‚è€ƒ [å¯¹è¯æ¥å£](https://beta.openai.com/docs/api-reference/completions) å’Œ [å›¾åƒæ¥å£](https://beta.openai.com/docs/api-reference/completions)  æ–‡æ¡£ï¼Œåœ¨[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)ä¸­æ£€æŸ¥å“ªäº›å‚æ•°åœ¨æœ¬é¡¹ç›®ä¸­æ˜¯å¯é…ç½®çš„ã€‚\n+ `conversation_max_tokens`ï¼šè¡¨ç¤ºèƒ½å¤Ÿè®°å¿†çš„ä¸Šä¸‹æ–‡æœ€å¤§å­—æ•°ï¼ˆä¸€é—®ä¸€ç­”ä¸ºä¸€ç»„å¯¹è¯ï¼Œå¦‚æœç´¯ç§¯çš„å¯¹è¯å­—æ•°è¶…å‡ºé™åˆ¶ï¼Œå°±ä¼šä¼˜å…ˆç§»é™¤æœ€æ—©çš„ä¸€ç»„å¯¹è¯ï¼‰\n+ `rate_limit_chatgpt`ï¼Œ`rate_limit_dalle`ï¼šæ¯åˆ†é’Ÿæœ€é«˜é—®ç­”é€Ÿç‡ã€ç”»å›¾é€Ÿç‡ï¼Œè¶…é€Ÿåæ’é˜ŸæŒ‰åºå¤„ç†ã€‚\n+ `clear_memory_commands`: å¯¹è¯å†…æŒ‡ä»¤ï¼Œä¸»åŠ¨æ¸…ç©ºå‰æ–‡è®°å¿†ï¼Œå­—ç¬¦ä¸²æ•°ç»„å¯è‡ªå®šä¹‰æŒ‡ä»¤åˆ«åã€‚\n+ `hot_reload`: ç¨‹åºé€€å‡ºåï¼Œæš‚å­˜å¾®ä¿¡æ‰«ç çŠ¶æ€ï¼Œé»˜è®¤å…³é—­ã€‚\n+ `character_desc` é…ç½®ä¸­ä¿å­˜ç€ä½ å¯¹æœºå™¨äººè¯´çš„ä¸€æ®µè¯ï¼Œä»–ä¼šè®°ä½è¿™æ®µè¯å¹¶ä½œä¸ºä»–çš„è®¾å®šï¼Œä½ å¯ä»¥ä¸ºä»–å®šåˆ¶ä»»ä½•äººæ ¼      (å…³äºä¼šè¯ä¸Šä¸‹æ–‡çš„æ›´å¤šå†…å®¹å‚è€ƒè¯¥ [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`ï¼šè®¢é˜…æ¶ˆæ¯ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­è¯·å¡«å†™ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ï¼Œ å¯ä½¿ç”¨ç‰¹æ®Šå ä½ç¬¦ã€‚ç›®å‰æ”¯æŒçš„å ä½ç¬¦æœ‰{trigger_prefix}ï¼Œåœ¨ç¨‹åºä¸­å®ƒä¼šè‡ªåŠ¨æ›¿æ¢æˆbotçš„è§¦å‘è¯ã€‚\n\n**5.LinkAIé…ç½® (å¯é€‰)**\n\n+ `use_linkai`: æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œå¼€å¯åå¯å›½å†…è®¿é—®ï¼Œä½¿ç”¨çŸ¥è¯†åº“å’Œ `Midjourney` ç»˜ç”», å‚è€ƒ [æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Keyï¼Œå¯åœ¨ [æ§åˆ¶å°](https://link-ai.tech/console/interface) åˆ›å»º\n+ `linkai_app_code`: LinkAI åº”ç”¨codeï¼Œé€‰å¡«\n\n**æœ¬è¯´æ˜æ–‡æ¡£å¯èƒ½ä¼šæœªåŠæ—¶æ›´æ–°ï¼Œå½“å‰æ‰€æœ‰å¯é€‰çš„é…ç½®é¡¹å‡åœ¨è¯¥[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)ä¸­åˆ—å‡ºã€‚**\n\n## è¿è¡Œ\n\n### 1.æœ¬åœ°è¿è¡Œ\n\nå¦‚æœæ˜¯å¼€å‘æœº **æœ¬åœ°è¿è¡Œ**ï¼Œç›´æ¥åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š\n\n```bash\npython3 app.py                                    # windowsç¯å¢ƒä¸‹è¯¥å‘½ä»¤é€šå¸¸ä¸º python app.py\n```\n\nç»ˆç«¯è¾“å‡ºäºŒç»´ç åï¼Œä½¿ç”¨å¾®ä¿¡è¿›è¡Œæ‰«ç ï¼Œå½“è¾“å‡º \"Start auto replying\" æ—¶è¡¨ç¤ºè‡ªåŠ¨å›å¤ç¨‹åºå·²ç»æˆåŠŸè¿è¡Œäº†ï¼ˆæ³¨æ„ï¼šç”¨äºç™»å½•çš„å¾®ä¿¡éœ€è¦åœ¨æ”¯ä»˜å¤„å·²å®Œæˆå®åè®¤è¯ï¼‰ã€‚æ‰«ç ç™»å½•åä½ çš„è´¦å·å°±æˆä¸ºæœºå™¨äººäº†ï¼Œå¯ä»¥åœ¨å¾®ä¿¡æ‰‹æœºç«¯é€šè¿‡é…ç½®çš„å…³é”®è¯è§¦å‘è‡ªåŠ¨å›å¤ (ä»»æ„å¥½å‹å‘é€æ¶ˆæ¯ç»™ä½ ï¼Œæˆ–æ˜¯è‡ªå·±å‘æ¶ˆæ¯ç»™å¥½å‹)ï¼Œå‚è€ƒ[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)ã€‚\n\n### 2.æœåŠ¡å™¨éƒ¨ç½²\n\nä½¿ç”¨nohupå‘½ä»¤åœ¨åå°è¿è¡Œç¨‹åºï¼š\n\n```bash\nnohup python3 app.py & tail -f nohup.out          # åœ¨åå°è¿è¡Œç¨‹åºå¹¶é€šè¿‡æ—¥å¿—è¾“å‡ºäºŒç»´ç \n```\næ‰«ç ç™»å½•åç¨‹åºå³å¯è¿è¡ŒäºæœåŠ¡å™¨åå°ï¼Œæ­¤æ—¶å¯é€šè¿‡ `ctrl+c` å…³é—­æ—¥å¿—ï¼Œä¸ä¼šå½±å“åå°ç¨‹åºçš„è¿è¡Œã€‚ä½¿ç”¨ `ps -ef | grep app.py | grep -v grep` å‘½ä»¤å¯æŸ¥çœ‹è¿è¡Œäºåå°çš„è¿›ç¨‹ï¼Œå¦‚æœæƒ³è¦é‡æ–°å¯åŠ¨ç¨‹åºå¯ä»¥å…ˆ `kill` æ‰å¯¹åº”çš„è¿›ç¨‹ã€‚æ—¥å¿—å…³é—­åå¦‚æœæƒ³è¦å†æ¬¡æ‰“å¼€åªéœ€è¾“å…¥Â `tail -f nohup.out`ã€‚æ­¤å¤–ï¼Œ`scripts` ç›®å½•ä¸‹æœ‰ä¸€é”®è¿è¡Œã€å…³é—­ç¨‹åºçš„è„šæœ¬ä¾›ä½¿ç”¨ã€‚\n\n> **å¤šè´¦å·æ”¯æŒï¼š** å°†é¡¹ç›®å¤åˆ¶å¤šä»½ï¼Œåˆ†åˆ«å¯åŠ¨ç¨‹åºï¼Œç”¨ä¸åŒè´¦å·æ‰«ç ç™»å½•å³å¯å®ç°åŒæ—¶è¿è¡Œã€‚\n\n> **ç‰¹æ®ŠæŒ‡ä»¤ï¼š** ç”¨æˆ·å‘æœºå™¨äººå‘é€ **#reset** å³å¯æ¸…ç©ºè¯¥ç”¨æˆ·çš„ä¸Šä¸‹æ–‡è®°å¿†ã€‚\n\n\n### 3.Dockeréƒ¨ç½²\n\n> ä½¿ç”¨dockeréƒ¨ç½²æ— éœ€ä¸‹è½½æºç å’Œå®‰è£…ä¾èµ–ï¼Œåªéœ€è¦è·å– docker-compose.yml é…ç½®æ–‡ä»¶å¹¶å¯åŠ¨å®¹å™¨å³å¯ã€‚\n\n> å‰ææ˜¯éœ€è¦å®‰è£…å¥½ `docker` åŠ `docker-compose`ï¼Œå®‰è£…æˆåŠŸçš„è¡¨ç°æ˜¯æ‰§è¡Œ `docker -v` å’Œ `docker-compose version` (æˆ– docker compose version) å¯ä»¥æŸ¥çœ‹åˆ°ç‰ˆæœ¬å·ï¼Œå¯å‰å¾€ [dockerå®˜ç½‘](https://docs.docker.com/engine/install/) è¿›è¡Œä¸‹è½½ã€‚\n\n#### (1) ä¸‹è½½ docker-compose.yml æ–‡ä»¶\n\n```bash\nwget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml\n```\n\nä¸‹è½½å®Œæˆåæ‰“å¼€ `docker-compose.yml` ä¿®æ”¹æ‰€éœ€é…ç½®ï¼Œå¦‚ `OPEN_AI_API_KEY` å’Œ `GROUP_NAME_WHITE_LIST` ç­‰ã€‚\n\n#### (2) å¯åŠ¨å®¹å™¨\n\nåœ¨ `docker-compose.yml` æ‰€åœ¨ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨ï¼š\n\n```bash\nsudo docker compose up -d\n```\n\nè¿è¡Œ `sudo docker ps` èƒ½æŸ¥çœ‹åˆ° NAMES ä¸º chatgpt-on-wechat çš„å®¹å™¨å³è¡¨ç¤ºè¿è¡ŒæˆåŠŸã€‚\n\næ³¨æ„ï¼š\n\n - å¦‚æœ `docker-compose` æ˜¯ 1.X ç‰ˆæœ¬ åˆ™éœ€è¦æ‰§è¡Œ `sudo  docker-compose up -d` æ¥å¯åŠ¨å®¹å™¨\n - è¯¥å‘½ä»¤ä¼šè‡ªåŠ¨å» [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) æ‹‰å– latest ç‰ˆæœ¬çš„é•œåƒï¼Œlatest é•œåƒä¼šåœ¨æ¯æ¬¡é¡¹ç›® release æ–°çš„ç‰ˆæœ¬æ—¶ç”Ÿæˆ\n\næœ€åè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯æŸ¥çœ‹å®¹å™¨è¿è¡Œæ—¥å¿—ï¼Œæ‰«ææ—¥å¿—ä¸­çš„äºŒç»´ç å³å¯å®Œæˆç™»å½•ï¼š\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n#### (3) æ’ä»¶ä½¿ç”¨\n\nå¦‚æœéœ€è¦åœ¨dockerå®¹å™¨ä¸­ä¿®æ”¹æ’ä»¶é…ç½®ï¼Œå¯é€šè¿‡æŒ‚è½½çš„æ–¹å¼å®Œæˆï¼Œå°† [æ’ä»¶é…ç½®æ–‡ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\né‡å‘½åä¸º `config.json`ï¼Œæ”¾ç½®äº `docker-compose.yml` ç›¸åŒç›®å½•ä¸‹ï¼Œå¹¶åœ¨ `docker-compose.yml` ä¸­çš„ `chatgpt-on-wechat` éƒ¨åˆ†ä¸‹æ·»åŠ  `volumes` æ˜ å°„:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n\n### 4. Railwayéƒ¨ç½²\n\n> Railway æ¯æœˆæä¾›5åˆ€å’Œæœ€å¤š500å°æ—¶çš„å…è´¹é¢åº¦ã€‚ (07.11æ›´æ–°: ç›®å‰å¤§éƒ¨åˆ†è´¦å·å·²æ— æ³•å…è´¹éƒ¨ç½²)\n\n1. è¿›å…¥ [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)\n2. ç‚¹å‡» `Deploy Now` æŒ‰é’®ã€‚\n3. è®¾ç½®ç¯å¢ƒå˜é‡æ¥é‡è½½ç¨‹åºè¿è¡Œçš„å‚æ•°ï¼Œä¾‹å¦‚`open_ai_api_key`, `character_desc`ã€‚\n\n**ä¸€é”®éƒ¨ç½²:**\n  \n  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)\n\n## å¸¸è§é—®é¢˜\n\nFAQsï¼š <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\næˆ–ç›´æ¥åœ¨çº¿å’¨è¯¢ [é¡¹ç›®å°åŠ©æ‰‹](https://link-ai.tech/app/Kv2fXJcH)  (betaç‰ˆæœ¬ï¼Œè¯­æ–™å®Œå–„ä¸­ï¼Œå›å¤ä»…ä¾›å‚è€ƒ)\n\n## å¼€å‘\n\næ¬¢è¿æ¥å…¥æ›´å¤šåº”ç”¨ï¼Œå‚è€ƒ [Terminalä»£ç ](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) å®ç°æ¥æ”¶å’Œå‘é€æ¶ˆæ¯é€»è¾‘å³å¯æ¥å…¥ã€‚ åŒæ—¶æ¬¢è¿å¢åŠ æ–°çš„æ’ä»¶ï¼Œå‚è€ƒ [æ’ä»¶è¯´æ˜æ–‡æ¡£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)ã€‚\n\n## è”ç³»\n\næ¬¢è¿æäº¤PRã€Issuesï¼Œä»¥åŠStaræ”¯æŒä¸€ä¸‹ã€‚ç¨‹åºè¿è¡Œé‡åˆ°é—®é¢˜å¯ä»¥æŸ¥çœ‹ [å¸¸è§é—®é¢˜åˆ—è¡¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ï¼Œå…¶æ¬¡å‰å¾€ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ä¸­æœç´¢ã€‚ä¸ªäººå¼€å‘è€…å¯åŠ å…¥å¼€æºäº¤æµç¾¤å‚ä¸æ›´å¤šè®¨è®ºï¼Œä¼ä¸šç”¨æˆ·å¯è”ç³»[äº§å“é¡¾é—®](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)å’¨è¯¢ã€‚\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "dialogue",
        "wechat",
        "dialogue service",
        "rsagacom chatgpt",
        "intelligent dialogue"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "scarletlabs-ai--Votars-MCP": {
      "owner": "scarletlabs-ai",
      "name": "Votars-MCP",
      "url": "https://github.com/scarletlabs-ai/Votars-MCP",
      "imageUrl": "https://github.com/scarletlabs-ai.png",
      "description": "Integrate advanced AI functionalities for processing complex tasks through robust APIs. Supports voice recording, transcription, and intelligent AI processing for meetings.",
      "stars": 27,
      "forks": 2,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-07-07T07:12:32Z",
      "readme_content": "![Votars Logo](https://votars.ai/_next/static/media/logo.e7b6bff6.svg) \n# Votars MCP \n[![smithery badge](https://smithery.ai/badge/@scarletlabs-ai/votars-mcp)](https://smithery.ai/server/@scarletlabs-ai/votars-mcp)\n\n\n## Overview\n\nVotars-MCP is a tool that supports multiple language implementations of the **Votars MCP server**. Currently, only the Go version is available, with other languages to be added in future releases. It supports two interaction modes: `sse` (Server-Sent Events) and `stdio` (Standard Input/Output). It is designed to provide seamless integration with the Votars AI platform for processing various tasks.\n\n## About Votars\n\n[Votars](https://votars.ai/en/) is the world's smartest multilingual meeting assistant, designed for voice recording, transcription, and advanced AI processing. It features real-time translation, intelligent error correction, AI summarization, smart content generation, and AI discussions. The Votars app is available on [Web](https://votars.ai/en/), [iOS](https://apps.apple.com/us/app/votars-ai-transcribe-organize/id6737496290), and [Android](https://play.google.com/store/apps/details?id=com.votars.transcribe).\n\nAdditionally, Votars is an AI-powered platform that enables developers to integrate advanced AI functionalities into their applications. By leveraging Votars, you can process complex tasks efficiently with robust APIs designed for high performance and scalability.\n\n## Features\n- **Easy Integration with Votars**\n- **Modular Design:** Ready to be extended with additional functionalities.\n- **Supported MCP Tools:**\n  - `Votars_fetch_recent_transcripts`: Allows users to read recent transcripts from their workspace, providing convenient access to the latest recorded sessions.\n  - `Votars_fetch_a_specific_transcript`: Enables users to retrieve specific transcripts by providing a transcript ID, allowing targeted retrieval of stored data.\n  \n  More functionalities will be added soon. Stay tuned!\n\n## Installation (Go MCP)\n\n### Installing via Smithery\n\nTo install votars-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@scarletlabs-ai/votars-mcp):\n\n```bash\nnpx -y @smithery/cli install @scarletlabs-ai/votars-mcp --client claude\n```\n\n### Manual Installation\nTo install the Go version of Votars MCP from the GitHub repository, use:\n\n```bash\n go install github.com/scarletlabs-ai/Votars-MCP/go/votars-mcp@latest\n```\n\n## Usage (Go MCP)\n\n### Run MCP Service\nBefore using the `sse` mode, you need to run the MCP server. Open a terminal and run:\n\n```bash\nvotars-mcp -t sse -p 8080\n```\n\nThis command starts the MCP service on port 8080, ready to accept `sse` requests.\n\n\n### 1. SSE Mode\n\nFor `sse` mode, you need to provide the API key via request headers in the configuration file.\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP\": {\n      \"type\": \"sse\",\n      \"url\": \"http://0.0.0.0:8080/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n### 2. Stdio Mode\n\nFor `stdio` mode, set the API key as an environment variable.\n\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP Stdio\": {\n      \"type\": \"stdio\",\n      \"command\": \"votars-mcp\",\n      \"args\": [\"-t\", \"stdio\"],\n      \"env\": {\n        \"VOTARS_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n## Obtaining Your API Key\n\n1. Go to [Votars.AI](https://votars.ai/en/) and register.\n2. Navigate to your workspace's `Settings`.\n3. Create an API Key under the API Key management section.\n\n![manage apikey](https://private-user-images.githubusercontent.com/677477/427500562-8cfd8465-f408-4e9b-a101-8b9b8e5e57f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMwNzQ4NzAsIm5iZiI6MTc0MzA3NDU3MCwicGF0aCI6Ii82Nzc0NzcvNDI3NTAwNTYyLThjZmQ4NDY1LWY0MDgtNGU5Yi1hMTAxLThiOWI4ZTVlNTdmNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMyN1QxMTIyNTBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02ODkyMWY3YzgyYTA2ZjdmNGQxN2MyYzllMzBmZmFiZmVjNGFmZTliNDQzODUwMjU2M2E1MjJkZTI4MmExM2VmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.V0GAoh6l0JFRxmSokliGsVt5yNpxtmTmeCFiZG7U3jU)\n\n## Roadmap\n\n- **Current Support:** Go\n- **Planned Support:** Python, JavaScript, Rust, etc.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "ai",
        "transcription",
        "transcription intelligent",
        "scarletlabs ai",
        "speech recognition"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "uraoz--bouyomichan-mcp-nodejs": {
      "owner": "uraoz",
      "name": "bouyomichan-mcp-nodejs",
      "url": "https://github.com/uraoz/bouyomichan-mcp-nodejs",
      "imageUrl": "https://github.com/uraoz.png",
      "description": "Provides text-to-speech capabilities using BouyomiChan's Yukkuri voice, enabling voice output from text commands with customizable options for voice type, volume, speed, and pitch. Integrates seamlessly with Claude for Desktop for enhanced user interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T11:48:46Z",
      "readme_content": "# æ£’èª­ã¿ã¡ã‚ƒã‚“MCPã‚µãƒ¼ãƒãƒ¼ (Node.jsç‰ˆ)\n\n<a href=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs/badge\" alt=\"Bouyomi-chan Server MCP server\" />\n</a>\n\n\n## å‰ææ¡ä»¶\n\n- Node.js 16ä»¥ä¸Š\n- npm 7ä»¥ä¸Š\n- æ£’èª­ã¿ã¡ã‚ƒã‚“ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨\n- æ£’èª­ã¿ã¡ã‚ƒã‚“ã®HTTPé€£æºãŒãƒãƒ¼ãƒˆ50080ã§èµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨\n\n## ä½¿ç”¨æ–¹æ³•\n\n### ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•\n\n```bash\ngit clone https://github.com/uraoz/bouyomichan-mcp-nodejs.git\ncd bouyomichan-mcp-nodejs\nnpm install\nnpm run build\nnpm start\n```\n\n### Claude for Desktopã¨ã®é€£æº\n\n```json\n{\n  \"mcpServers\": {\n    \"bouyomichan\":{\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:uraoz/bouyomichan-mcp-nodejs\"\n      ]\n    }\n  }\n}\n```\n\n## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜\n\n| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | æœ‰åŠ¹ç¯„å›² |\n|----------|------|------------|---------|\n| text     | èª­ã¿ä¸Šã’ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ | å¿…é ˆ | ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆ |\n| voice    | éŸ³å£°ã®ç¨®é¡ | 0 (å¥³æ€§1) | 0: å¥³æ€§1ã€1: ç”·æ€§1ã€2: å¥³æ€§2ã€... |\n| volume   | éŸ³é‡ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€0-100: éŸ³é‡ãƒ¬ãƒ™ãƒ« |\n| speed    | é€Ÿåº¦ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€50-200: é€Ÿåº¦ãƒ¬ãƒ™ãƒ« |\n| tone     | éŸ³ç¨‹ | -1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) | -1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€50-200: éŸ³ç¨‹ãƒ¬ãƒ™ãƒ« |\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bouyomichan",
        "voice",
        "nodejs",
        "yukkuri voice",
        "bouyomichan yukkuri",
        "uraoz bouyomichan"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yangsenessa--mcp_voice_identify": {
      "owner": "yangsenessa",
      "name": "mcp_voice_identify",
      "url": "https://github.com/yangsenessa/mcp_voice_identify",
      "imageUrl": "https://github.com/yangsenessa.png",
      "description": "Provides voice recognition and text extraction capabilities, supporting both file input and base64 encoded data processed in structured formats. Operates in stdio and MCP modes for flexible integration with various systems.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-17T11:21:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yangsenessa-mcp-voice-identify-badge.png)](https://mseep.ai/app/yangsenessa-mcp-voice-identify)\n\n# Voice Recognition MCP Service\n\nThis service provides voice recognition and text extraction capabilities through both stdio and MCP modes.\n\n## Features\n\n- Voice recognition from file\n- Voice recognition from base64 encoded data\n- Text extraction\n- Support for both stdio and MCP modes\n- Structured voice recognition results\n- AIO protocol compliant responses\n\n## Project Structure\n\n- `voice_service.py` - Core service implementation\n- `stdio_server.py` - stdio mode entry point\n- `mcp_server.py` - MCP mode entry point\n- `build.py` - Build script for executables\n- `build_exec.sh` - Build execution script\n- `test_*.sh` - Test scripts for different functionalities\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/AIO-2030/mcp_voice_identify.git\ncd mcp_voice_identify\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up environment variables in `.env`:\n```\nAPI_URL=your_api_url\nAPI_KEY=your_api_key\n```\n\n## Usage\n\n### stdio Mode\n\n1. Run the service:\n```bash\npython stdio_server.py\n```\n\n2. Send JSON-RPC requests via stdin:\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"help\",\n    \"params\": {},\n    \"id\": 1\n}\n```\n\n3. Or use the executable:\n```bash\n./dist/voice_stdio\n```\n\n### MCP Mode\n\n1. Run the service:\n```bash\npython mcp_server.py\n```\n\n2. Or use the executable:\n```bash\n./dist/voice_mcp\n```\n\n## Response Format\n\nThe service follows the AIO protocol for response formatting. Here are examples of different response types:\n\n### Voice Recognition Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"voice\",\n        \"message\": \"Voice processed successfully\",\n        \"text\": \"test test test\",\n        \"metadata\": {\n            \"language\": \"en\",\n            \"emotion\": \"unknown\",\n            \"audio_type\": \"speech\",\n            \"speaker\": \"woitn\",\n            \"raw_text\": \"test test test\"\n        }\n    },\n    \"id\": 1\n}\n```\n\n### Help Information Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"result\": {\n        \"type\": \"voice_service\",\n        \"description\": \"This service provides voice recognition and text extraction services\",\n        \"author\": \"AIO-2030\",\n        \"version\": \"1.0.0\",\n        \"github\": \"https://github.com/AIO-2030/mcp_voice_identify\",\n        \"transport\": [\"stdio\"],\n        \"methods\": [\n            {\n                \"name\": \"help\",\n                \"description\": \"Show this help information.\"\n            },\n            {\n                \"name\": \"identify_voice\",\n                \"description\": \"Identify voice from file\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"Voice file path\"\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                }\n            },\n            {\n                \"name\": \"identify_voice_base64\",\n                \"description\": \"Identify voice from base64 encoded data\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"base64_data\": {\n                            \"type\": \"string\",\n                            \"description\": \"Base64 encoded voice data\"\n                        }\n                    },\n                    \"required\": [\"base64_data\"]\n                }\n            },\n            {\n                \"name\": \"extract_text\",\n                \"description\": \"Extract text\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"text\": {\n                            \"type\": \"string\",\n                            \"description\": \"Text to extract\"\n                        }\n                    },\n                    \"required\": [\"text\"]\n                }\n            }\n        ]\n    },\n    \"id\": 1\n}\n```\n\n### Error Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"error\",\n        \"message\": \"503 Server Error: Service Unavailable\",\n        \"error_code\": 503\n    },\n    \"id\": 1\n}\n```\n\n### Response Fields\n\nThe service provides three types of responses:\n\n1. Voice Recognition Response (using `output` field):\n| Field     | Description                          | Example Value |\n|-----------|--------------------------------------|---------------|\n| type      | Response type                        | \"voice\"       |\n| message   | Status message                       | \"Voice processed successfully\" |\n| text      | Recognized text content              | \"test test test\" |\n| metadata  | Additional information               | See below     |\n\n2. Help Information Response (using `result` field):\n| Field         | Description                          | Example Value |\n|---------------|--------------------------------------|---------------|\n| type          | Service type                         | \"voice_service\" |\n| description   | Service description                  | \"This service provides...\" |\n| author        | Service author                       | \"AIO-2030\"    |\n| version       | Service version                      | \"1.0.0\"       |\n| github        | GitHub repository URL                | \"https://github.com/...\" |\n| transport     | Supported transport modes            | [\"stdio\"]     |\n| methods       | Available methods                    | See methods list |\n\n3. Error Response (using `output` field):\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| type        | Response type                        | \"error\"       |\n| message     | Error message                        | \"503 Server Error: Service Unavailable\" |\n| error_code  | HTTP status code                     | 503          |\n\n### Metadata Fields\n\nThe `metadata` field in voice recognition responses contains:\n\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| language    | Language code                        | \"en\"          |\n| emotion     | Emotion state                        | \"unknown\"     |\n| audio_type  | Audio type                          | \"speech\"      |\n| speaker     | Speaker identifier                   | \"woitn\"       |\n| raw_text    | Original recognized text             | \"test test test\" |\n\n## Building Executables\n\n1. Make the build script executable:\n```bash\nchmod +x build_exec.sh\n```\n\n2. Build stdio mode executable:\n```bash\n./build_exec.sh\n```\n\n3. Build MCP mode executable:\n```bash\n./build_exec.sh mcp\n```\n\nThe executables will be created at:\n- stdio mode: `dist/voice_stdio`\n- MCP mode: `dist/voice_mcp`\n\n## Testing\n\nRun the test scripts:\n\n```bash\nchmod +x test_*.sh\n./test_help.sh\n./test_voice_file.sh\n./test_voice_base64.sh\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_voice_identify",
        "voice",
        "encoded",
        "mcp_voice_identify provides",
        "yangsenessa mcp_voice_identify",
        "voice recognition"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yuiseki--edge_tts_mcp_server": {
      "owner": "yuiseki",
      "name": "edge_tts_mcp_server",
      "url": "https://github.com/yuiseki/edge_tts_mcp_server",
      "imageUrl": "https://github.com/yuiseki.png",
      "description": "Provide natural text-to-speech conversion using Microsoft Edge's speech synthesis capabilities, enabling customizable voice output in multiple languages with adjustable speed and pitch.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T03:20:28Z",
      "readme_content": "# Edge-TTS MCP Server\n\nModel Context Protocol (MCP) ã‚µãƒ¼ãƒãƒ¼ã§ã€Microsoft Edge ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸ AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®éŸ³å£°åˆæˆã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚\n\n## æ¦‚è¦\n\nã“ã® MCP ã‚µãƒ¼ãƒãƒ¼ã¯ã€[edge-tts](https://github.com/rany2/edge-tts)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªç„¶ãªéŸ³å£°ã§å¿œç­”ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n## æ©Ÿèƒ½\n\n- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›\n- è¤‡æ•°ã®éŸ³å£°ã¨è¨€èªã®ã‚µãƒãƒ¼ãƒˆ\n- éŸ³å£°é€Ÿåº¦ã¨éŸ³ç¨‹ã®èª¿æ•´\n- éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n```bash\npip install \"edge_tts_mcp_server\"\n```\n\nã¾ãŸã¯é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆï¼š\n\n```bash\ngit clone https://github.com/yuiseki/edge_tts_mcp_server.git\ncd edge_tts_mcp_server\npip install -e .\n```\n\n## ä½¿ç”¨æ–¹æ³•\n\n### VS Code ã§ã®è¨­å®šä¾‹\n\nVS Code ã® settings.json ã§è¨­å®šã™ã‚‹ä¾‹ï¼š\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"edge-tts\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\Users\\\\__username__\\\\src\\\\edge_tts_mcp_server\\\\src\\\\edge_tts_mcp_server\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n### MCP Inspector ã§ã®ä½¿ç”¨\n\næ¨™æº–çš„ãª MCP ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦å®Ÿè¡Œï¼š\n\n```bash\nmcp dev server.py\n```\n\n### uvxï¼ˆuvicornï¼‰ã§ã®å®Ÿè¡Œ\n\nFastAPI ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦ uv ã§å®Ÿè¡Œã™ã‚‹å ´åˆï¼š\n\n```bash\nuv --directory path/to/edge_tts_mcp_server/src/edge_tts_mcp_server run server.py\n```\n\nã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š\n\n```bash\nedge-tts-mcp --host 0.0.0.0 --port 8080 --reload\n```\n\n## API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\nFastAPI ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ãŸå ´åˆã€ä»¥ä¸‹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼š\n\n- `/` - API æƒ…å ±\n- `/health` - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯\n- `/voices` - åˆ©ç”¨å¯èƒ½ãªéŸ³å£°ä¸€è¦§ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ `?locale=ja-JP` ãªã©ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½ï¼‰\n- `/mcp` - MCP API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "edge_tts_mcp_server",
        "voice",
        "edge",
        "yuiseki edge_tts_mcp_server",
        "speech conversion",
        "speech synthesis"
      ],
      "category": "speech-recognition-and-synthesis"
    }
  }
}