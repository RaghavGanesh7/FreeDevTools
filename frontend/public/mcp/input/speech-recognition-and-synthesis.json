{
  "category": "speech-recognition-and-synthesis",
  "categoryDisplay": "Speech Recognition and Synthesis",
  "description": "",
  "totalRepositories": 24,
  "repositories": {
    "CengSin--fishaudio-mcp": {
      "owner": "CengSin",
      "name": "fishaudio-mcp",
      "url": "https://github.com/CengSin/fishaudio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/CengSin.webp",
      "description": "Converts text into natural human speech with customizable audio formats and bitrates, while integrating seamlessly with MCP-compatible applications.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T10:49:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cengsin-fishaudio-mcp-badge.png)](https://mseep.ai/app/cengsin-fishaudio-mcp)\n\n# Fish Audio Python MCP 服务\n\n这是一个使用 Fish Audio API 实现的文字转语音 MCP 服务。通过这个服务，您可以将文本转换为自然的人声，支持多种配置选项。\n\n## 功能特点\n\n- 基本文字转语音：将任意文本转换为自然人声\n- 高级文字转语音：支持自定义音频格式、比特率等参数\n- 兼容 MCP 协议：可与支持 MCP 的应用无缝集成\n\n## 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n或使用 Python 包管理工具安装：\n\n```bash\npip install fish-audio-sdk mcp python-dotenv\n```\n\n## 配置\n\n在项目根目录创建 `.env` 文件，包含以下内容：\n\n```\nAPI_KEY=your_fish_audio_api_key\nMODEL_ID=your_fish_audio_model_id\n```\n\n您需要替换为您的 Fish Audio API 密钥和模型 ID。\n\n## 使用方法\n\n### 启动服务\n\n```bash\npython app.py\n```\n\n或使用 MCP CLI 工具：\n\n```bash\nmcp run --file app.py\n```\n\n### 运行示例\n\n```bash\npython example.py\n```\n\n### 使用 MCP 客户端调用服务\n\n```python\n# 示例代码\nfrom mcp.client import MCPClient\n\nclient = MCPClient(\"subprocess://python app.py\")\nresult = client.call(\"text_to_speech\", {\"text\": \"你好，世界！\"})\nprint(result)  # 打印生成的音频文件路径\n```\n\n## API 功能说明\n\n### text_to_speech\n\n基本文字转语音功能。\n\n参数：\n- `text`: 要转换为语音的文本\n- `output_path`（可选）: 输出文件路径，如果不提供，将创建临时文件\n\n返回：生成的音频文件路径\n\n### advanced_text_to_speech\n\n高级文字转语音功能，支持更多配置选项。\n\n参数：\n- `text`: 要转换为语音的文本\n- `output_path`（可选）: 输出文件路径，如果不提供，将创建临时文件\n- `format`: 输出音频格式 (mp3, wav, pcm)，默认为 mp3\n- `mp3_bitrate`: MP3 比特率 (64, 128, 192 kbps)，默认为 128\n- `chunk_length`: 分块长度 (100-300)，默认为 200\n- `normalize`: 是否对文本进行标准化处理，默认为 True\n- `latency`: 延迟模式 (normal, balanced)，默认为 normal\n\n返回：生成的音频文件路径\n\n### get_model_info\n\n获取当前使用的模型信息。\n\n返回：包含模型 ID 和 API 密钥前缀的字典\n\n### get_available_models\n\n获取可用的 Fish Audio 模型列表。\n\n返回：可用模型信息列表\n\n## 许可证\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fishaudio",
        "audio",
        "cengsin",
        "cengsin fishaudio",
        "fishaudio mcp",
        "speech customizable"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "DefiBax--mcp_servers": {
      "owner": "DefiBax",
      "name": "mcp_servers",
      "url": "https://github.com/DefiBax/mcp_servers",
      "imageUrl": "/freedevtools/mcp/pfp/DefiBax.webp",
      "description": "Record audio and transcribe it using advanced AI models like OpenAI's Whisper. Supports integration with AI agents for enhanced interactivity and includes prompts for common recording scenarios.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-11T08:14:19Z",
      "readme_content": "# Voice Recorder MCP Server\n\nAn MCP server for recording audio and transcribing it using OpenAI's Whisper model. Designed to work as a Goose custom extension or standalone MCP server.\n\n## Features\n\n- Record audio from the default microphone\n- Transcribe recordings using Whisper\n- Integrates with Goose AI agent as a custom extension\n- Includes prompts for common recording scenarios\n\n## Installation\n\n```bash\n# Install from source\ngit clone https://github.com/DefiBax/voice-recorder-mcp.git\ncd voice-recorder-mcp\npip install -e .\n```\n\n## Usage\n\n### As a Standalone MCP Server\n\n```bash\n# Run with default settings (base.en model)\nvoice-recorder-mcp\n\n# Use a specific Whisper model\nvoice-recorder-mcp --model medium.en\n\n# Adjust sample rate\nvoice-recorder-mcp --sample-rate 44100\n```\n\n### Testing with MCP Inspector\n\nThe MCP Inspector provides an interactive interface to test your server:\n\n```bash\n# Install the MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run your server with the inspector\nnpx @modelcontextprotocol/inspector voice-recorder-mcp\n```\n\n### With Goose AI Agent\n\n1. Open Goose and go to Settings > Extensions > Add > Command Line Extension\n2. Set the name to `voice-recorder`\n3. In the Command field, enter the full path to the voice-recorder-mcp executable:\n   ```\n   /full/path/to/voice-recorder-mcp\n   ```\n   \n   Or for a specific model:\n   ```\n   /full/path/to/voice-recorder-mcp --model medium.en\n   ```\n   \n   To find the path, run:\n   ```bash\n   which voice-recorder-mcp\n   ```\n\n4. No environment variables are needed for basic functionality\n5. Start a conversation with Goose and introduce the recorder with:\n   \"I want you to take action from transcriptions returned by voice-recorder. For example, if I dictate a calculation like 1+1, please return the result.\"\n\n## Available Tools\n\n- `start_recording`: Start recording audio from the default microphone\n- `stop_and_transcribe`: Stop recording and transcribe the audio to text\n- `record_and_transcribe`: Record audio for a specified duration and transcribe it\n\n## Whisper Models\n\nThis extension supports various Whisper model sizes:\n\n| Model | Speed | Accuracy | Memory Usage | Use Case |\n|-------|-------|----------|--------------|----------|\n| `tiny.en` | Fastest | Lowest | Minimal | Testing, quick transcriptions |\n| `base.en` | Fast | Good | Low | Everyday use (default) |\n| `small.en` | Medium | Better | Moderate | Good balance |\n| `medium.en` | Slow | High | High | Important recordings |\n| `large` | Slowest | Highest | Very High | Critical transcriptions |\n\nThe `.en` suffix indicates models specialized for English, which are faster and more accurate for English content.\n\n## Requirements\n\n- Python 3.12+\n- An audio input device (microphone)\n\n## Configuration\n\nYou can configure the server using environment variables:\n\n```bash\n# Set Whisper model\nexport WHISPER_MODEL=small.en\n\n# Set audio sample rate\nexport SAMPLE_RATE=44100\n\n# Set maximum recording duration (seconds)\nexport MAX_DURATION=120\n\n# Then run the server\nvoice-recorder-mcp\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **No audio being recorded**: Check your microphone permissions and settings\n- **Model download errors**: Ensure you have a stable internet connection for the initial model download\n- **Integration with Goose**: Make sure the command path is correct\n- **Audio quality issues**: Try adjusting the sample rate (default: 16000)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "audio",
        "defibax",
        "defibax mcp_servers",
        "ai agents",
        "synthesis defibax"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Dosugamea--voicevox-mcp-server": {
      "owner": "Dosugamea",
      "name": "voicevox-mcp-server",
      "url": "https://github.com/Dosugamea/voicevox-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Dosugamea.webp",
      "description": "Provides voice synthesis capabilities compatible with VOICEVOX and similar engines through the Model Context Protocol. Facilitates speech audio generation using AI agents compatible with MCP clients.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-17T09:32:07Z",
      "readme_content": "# Voicevox MCP Server\r\n\r\nVOICEVOX互換の音声合成サーバー(AivisSpeech / VOICEVOX / COEIROINK) を MCP (Model Context Protocol) 経由で利用するためのサーバーです。\r\nCursor等でのClaude 3.7を使ったエージェントモードでの音声合成に利用できます。\r\n\r\n## 必要条件\r\n\r\n### Windows環境\r\n\r\n- Node.js 18以上\r\n- VOICEVOX ENGINE等 (ローカルでhttp://localhost:50000等で実行)\r\n- VLCメディアプレーヤー（パスが通っていること）\r\n\r\n### Docker環境 (WSL2)\r\n\r\n- Docker と Docker Compose\r\n- WSL2\r\n- VOICEVOX ENGINE等 (ローカルまたはDockerで実行)\r\n- `sudo apt install libsdl2-dev pulseaudio-utils pulseaudio` されたLinux環境\r\n- `/mnt/wslg` へのアクセス権限\r\n\r\n## インストールと設定\r\n\r\n1. リポジトリをクローン\r\n```\r\ngit clone https://github.com/Dosugamea/voicevox-mcp-server.git\r\ncd voicevox-mcp-server\r\n```\r\n\r\n2. 依存関係のインストール\r\n```\r\nnpm install\r\n```\r\n\r\n3. 環境変数の設定\r\n`.env_example` をコピーして `.env` ファイルを作成し、必要に応じて設定を変更します:\r\n```\r\nVOICEVOX_API_URL=http://localhost:50021\r\nVOICEVOX_SPEAKER_ID=1\r\n```\r\n\r\n## 実行方法\r\n\r\n### Windows環境での実行\r\nエディタと別途で下記手順でサーバーを立ち上げてください。\r\n\r\n```\r\nnpm run build\r\nnpm start\r\n```\r\n\r\n### Docker環境での実行\r\nエディタと別途での操作は不要です。\r\nstdioモードで立ち上がるため直接実行することはできません。\r\n\r\n## 設定方法\r\n\r\n### Windows環境での実行の場合\r\nmcp.jsonに下記を追記してください。\r\n接続が不安定なため切断されたら再接続してください。\r\n\r\n```json\r\n        \"voicevox\": {\r\n            \"url\": \"http://localhost:10100/sse\"\r\n        }\r\n```\r\n\r\n### Docker環境での実行の場合\r\nmcp.jsonに下記を追記してください。\r\n(作者環境での動作は確認できていません)\r\n\r\n```json\r\n{\r\n    \"tools\": {\r\n        \"voicevox\": {\r\n            \"command\": \"cmd\",\r\n            \"args\": [\r\n                \"/c\",\r\n                \"docker\",\r\n                \"run\",\r\n                \"-i\",\r\n                \"--rm\",\r\n                \"-v\",\r\n                \"/mnt/wslg:/mnt/wslg\",\r\n                \"-e\",\r\n                \"PULSE_SERVER\",\r\n                \"-e\",\r\n                \"SDL_AUDIODRIVER\",\r\n                \"-e\",\r\n                \"VOICEVOX_API_URL\",\r\n                \"-e\",\r\n                \"VOICEVOX_SPEAKER_ID\",\r\n                \"your-local-docker-image-name\"\r\n            ],\r\n            \"env\": {\r\n                \"PULSE_SERVER\": \"unix:/mnt/wslg/PulseServer\",\r\n                \"SDL_AUDIODRIVER\": \"pulseaudio\",\r\n                \"VOICEVOX_API_URL\": \"http://host.docker.internal:50031\",\r\n                \"VOICEVOX_SPEAKER_ID\": \"919692871\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## 話者IDについて\r\n\r\n話者IDは使用するVOICEVOXのモデルによって異なります。デフォルトでは「1」（四国めたん）を使用しています。\r\n他の話者IDを使用する場合は、環境変数 `VOICEVOX_SPEAKER_ID` を変更してください。\r\n\r\n話者IDの一覧は、VOICEVOX ENGINE APIの `/speakers` エンドポイントで確認できます。\r\n例: `curl http://localhost:50021/speakers`\r\n\r\n## トラブルシューティング\r\n\r\n- **VOICEVOXとの接続エラー**: VOICEVOX ENGINEが起動していること、APIのURLが正しく設定されていることを確認してください。\r\n- **音声が再生されない**: VLCが正しくインストールされていることと、パスが通っていることを確認してください。\r\n- **Docker環境での音声出力問題**: pulseaudioの設定が正しいか確認してください。\r\n\r\n## 開発者向け情報\r\n\r\n- ソースコードに貢献する場合は、Issueを作成するか、Pull Requestを送信してください。\r\n- バグ報告や機能リクエストは、GitHubのIssue機能をご利用ください。\r\n\r\n## ライセンス\r\n\r\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicevox",
        "voice",
        "audio",
        "voicevox mcp",
        "voice synthesis",
        "dosugamea voicevox"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Ichigo3766--audio-transcriber-mcp": {
      "owner": "Ichigo3766",
      "name": "audio-transcriber-mcp",
      "url": "https://github.com/Ichigo3766/audio-transcriber-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Ichigo3766.webp",
      "description": "Transcribes audio files using OpenAI's speech-to-text capabilities, enabling accurate audio transcriptions and the option to save them directly to files.",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-12T13:13:47Z",
      "readme_content": "# OpenAI Speech-to-Text transcriptions MCP Server\n\nA MCP server that provides audio transcription capabilities using OpenAI's API.\n\n<a href=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Ichigo3766/audio-transcriber-mcp/badge\" alt=\"Audio Transcriber Server MCP server\" />\n</a>\n\n## Installation\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Ichigo3766/audio-transcriber-mcp.git\ncd audio-transcriber-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Set up your OpenAI API key in your environment variables.\n\n5. Add the server configuration to your environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-transcriber\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/audio-transcriber-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"OPENAI_BASE_URL\": \"\", // Optional\n        \"OPENAI_MODEL\": \"\" // Optional\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/audio-transcriber-mcp` with the actual path where you cloned the repository.\n\n## Features\n\n### Tools\n- `transcribe_audio` - Transcribe audio files using OpenAI's API\n  - Takes filepath as a required parameter\n  - Optional parameters:\n    - save_to_file: Boolean to save transcription to a file\n    - language: ISO-639-1 language code (e.g., \"en\", \"es\")\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcriptions",
        "transcribes",
        "transcriber",
        "audio transcriptions",
        "audio transcriber",
        "transcribes audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "ImOrenge--VoiceMacroProject": {
      "owner": "ImOrenge",
      "name": "VoiceMacroProject",
      "url": "https://github.com/ImOrenge/VoiceMacroProject",
      "imageUrl": "/freedevtools/mcp/pfp/ImOrenge.webp",
      "description": "VoiceMacro enables executing keyboard shortcuts and macros through voice commands on Windows. It supports custom voice command configurations and manages presets for frequent macro operations while running in the background.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "C#",
      "updated_at": "2025-03-12T09:50:53Z",
      "readme_content": "# 음성 매크로 (VoiceMacro)\r\n\r\n음성 인식을 통해 다양한 키보드 단축키와 매크로를 실행할 수 있는 Windows 애플리케이션입니다.\r\n\r\n## 주요 기능\r\n\r\n- **음성 인식**: 마이크를 통해 음성을 인식하여 명령어를 실행합니다.\r\n- **매크로 설정**: 사용자 정의 음성 명령어와 키보드 단축키를 설정할 수 있습니다.\r\n- **프리셋 관리**: 자주 사용하는 매크로 세트를 프리셋으로 저장하고 불러올 수 있습니다.\r\n- **트레이 아이콘**: 시스템 트레이에서 실행되어 항상 백그라운드에서 대기합니다.\r\n- **로그 표시**: 음성 인식 결과와 매크로 실행 결과를 실시간으로 확인할 수 있습니다.\r\n\r\n## 시스템 요구사항\r\n\r\n- Windows 10 이상\r\n- .NET 6.0 이상 (설치 방식에 따라 다름)\r\n- 마이크 또는 음성 입력 장치\r\n\r\n## 설치 방법\r\n\r\n1. 설치 프로그램(VoiceMacro-Setup.exe)을 다운로드합니다.\r\n2. 설치 프로그램을 실행하고 안내에 따라 설치를 완료합니다.\r\n3. 바탕화면 또는 시작 메뉴에서 \"음성 매크로\" 아이콘을 클릭하여 실행합니다.\r\n\r\n## 사용 방법\r\n\r\n### 기본 사용법\r\n\r\n1. 프로그램을 실행하면 음성 매크로 창이 나타납니다.\r\n2. \"시작\" 버튼을 클릭하여 음성 인식을 시작합니다.\r\n3. 마이크에 대고 설정된 매크로 명령어를 말하면 해당 키보드 단축키가 실행됩니다.\r\n4. \"정지\" 버튼을 클릭하여 음성 인식을 중지할 수 있습니다.\r\n\r\n### 매크로 추가하기\r\n\r\n1. \"매크로 추가\" 버튼을 클릭합니다.\r\n2. 음성 명령어(예: \"파일 저장\")와 실행할 키 조합(예: \"Ctrl+S\")을 입력합니다.\r\n3. \"저장\" 버튼을 클릭하여 매크로를 추가합니다.\r\n\r\n### 프리셋 관리\r\n\r\n1. \"프리셋\" 버튼을 클릭합니다.\r\n2. 현재 매크로 목록을 새 프리셋으로 저장하거나, 기존 프리셋을 불러올 수 있습니다.\r\n3. 프리셋 내보내기/가져오기 기능으로 다른 컴퓨터와 설정을 공유할 수 있습니다.\r\n\r\n### 시스템 트레이 기능\r\n\r\n- 창을 닫으면 프로그램은 시스템 트레이로 최소화됩니다.\r\n- 트레이 아이콘을 더블 클릭하거나 우클릭 메뉴에서 \"보기\"를 선택하여 창을 다시 표시할 수 있습니다.\r\n- 트레이 메뉴에서 음성 인식을 시작/정지하거나 프로그램을 종료할 수 있습니다.\r\n\r\n## 문제 해결\r\n\r\n- **음성 인식이 작동하지 않을 경우**:\r\n  - 마이크가 올바르게 연결되어 있는지 확인하세요.\r\n  - Windows 설정에서 마이크 액세스 권한이 허용되어 있는지 확인하세요.\r\n  - 다른 음성 인식 프로그램이 마이크를 사용 중인지 확인하세요.\r\n\r\n- **매크로가 실행되지 않을 경우**:\r\n  - 매크로 명령어를 더 정확하게 말해보세요.\r\n  - 매크로 명령어가 올바르게 설정되어 있는지 확인하세요.\r\n  - 키 조합이 현재 실행 중인 프로그램에서 지원되는지 확인하세요.\r\n\r\n## 라이선스\r\n\r\n이 프로그램은 MIT 라이선스 하에 배포됩니다.\r\n\r\n## 피드백 및 지원\r\n\r\n문제점이나 개선 사항은 GitHub 이슈 트래커에 등록해 주세요.\r\n\r\n## 개발 환경\r\n\r\n- C# / .NET 6.0\r\n- Windows Forms\r\n- NAudio (오디오 캡처)\r\n- Whisper.net (음성 인식)\r\n- InputSimulator (키보드 시뮬레이션)\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voicemacroproject",
        "voicemacro",
        "voice",
        "voicemacroproject voicemacro",
        "imorenge voicemacroproject",
        "voice commands"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "Kvadratni--speech-mcp": {
      "owner": "Kvadratni",
      "name": "speech-mcp",
      "url": "https://github.com/Kvadratni/speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Kvadratni.webp",
      "description": "Provides a voice interface for real-time audio interaction, converting spoken words into text and generating spoken responses. Includes features like audio visualization and a modern user interface for an engaging conversational experience.",
      "stars": 71,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T03:20:23Z",
      "readme_content": "# Speech MCP\n\nA Goose MCP extension for voice interaction with modern audio visualization.\n\n\nhttps://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8\n\n\n\n## Overview\n\nSpeech MCP provides a voice interface for [Goose](https://github.com/block/goose), allowing users to interact through speech rather than text. It includes:\n\n- Real-time audio processing for speech recognition\n- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)\n- High-quality text-to-speech with multiple voice options\n- Modern PyQt-based UI with audio visualization\n- Simple command-line interface for voice interaction\n\n## Features\n\n- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme\n- **Voice Input**: Capture and transcribe user speech using faster-whisper\n- **Voice Output**: Convert agent responses to speech with 54+ voice options\n- **Multi-Speaker Narration**: Generate audio files with multiple voices for stories and dialogues\n- **Single-Voice Narration**: Convert any text to speech with your preferred voice\n- **Audio/Video Transcription**: Transcribe speech from various media formats with optional timestamps and speaker detection\n- **Voice Persistence**: Remembers your preferred voice between sessions\n- **Continuous Conversation**: Automatically listen for user input after agent responses\n- **Silence Detection**: Automatically stops recording when the user stops speaking\n- **Robust Error Handling**: Graceful recovery from common failure modes with helpful voice suggestions\n\n## Installation\n> **Important Note**: After installation, the first time you use the speech interface, it may take several minutes to download the Kokoro voice models (approximately 523 KB per voice). During this initial setup period, the system will use a more robotic-sounding fallback voice. Once the Kokoro voices are downloaded, the high-quality voices will be used automatically.\n\n## ⚠️ IMPORTANT PREREQUISITES ⚠️\n\nBefore installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.\n\n### PortAudio Installation Instructions\n\n**macOS:**\n```bash\nbrew install portaudio\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n```\n\n**Linux (Debian/Ubuntu):**\n```bash\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-dev\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install portaudio-devel\n```\n\n**Windows:**\nFor Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.\n\n> **Note**: If you skip this step, PyAudio installation will fail with \"portaudio.h file not found\" errors and the extension will not work.\n\n### Option 1: Quick Install (One-Click)\n\nClick the link below if you have Goose installed:\n\n[goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose](goose://extension?cmd=uvx&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose)\n\n### Option 2: Using Goose CLI (recommended)\n\nStart Goose with your extension enabled:\n\n```bash\n# If you installed via PyPI\ngoose session --with-extension \"speech-mcp\"\n\n# Or if you want to use a local development version\ngoose session --with-extension \"python -m speech_mcp\"\n```\n\n### Option 3: Manual setup in Goose\n\n1. Run `goose configure`\n2. Select \"Add Extension\" from the menu\n3. Choose \"Command-line Extension\"\n4. Enter a name (e.g., \"Speech Interface\")\n5. For the command, enter: `speech-mcp`\n6. Follow the prompts to complete the setup\n\n### Option 4: Manual Installation\n\n1. Install PortAudio (see [Prerequisites](#prerequisites) section)\n2. Clone this repository\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n   \n   Or for a complete installation including Kokoro TTS:\n   ```\n   uv pip install -e .[all]\n   ```\n\n## Dependencies\n\n- Python 3.10+\n- PyQt5 (for modern UI)\n- PyAudio (for audio capture)\n- faster-whisper (for speech-to-text)\n- NumPy (for audio processing)\n- Pydub (for audio processing)\n- psutil (for process management)\n\n\n### Optional Dependencies\n\n- **Kokoro TTS**: For high-quality text-to-speech with multiple voices\n  - To install Kokoro, you can use pip with optional dependencies:\n    ```bash\n    pip install speech-mcp[kokoro]     # Basic Kokoro support with English\n    pip install speech-mcp[ja]         # Add Japanese support\n    pip install speech-mcp[zh]         # Add Chinese support\n    pip install speech-mcp[all]        # All languages and features\n    ```\n  - Alternatively, run the installation script: `python scripts/install_kokoro.py`\n  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information\n\n## Multi-Speaker Narration\n\nThe MCP supports generating audio files with multiple voices, perfect for creating stories, dialogues, and dramatic readings. You can use either JSON or Markdown format to define your conversations.\n\n### JSON Format Example:\n```json\n{\n    \"conversation\": [\n        {\n            \"speaker\": \"narrator\",\n            \"voice\": \"bm_daniel\",\n            \"text\": \"In a world where AI and human creativity intersect...\",\n            \"pause_after\": 1.0\n        },\n        {\n            \"speaker\": \"scientist\",\n            \"voice\": \"am_michael\",\n            \"text\": \"The quantum neural network is showing signs of consciousness!\",\n            \"pause_after\": 0.5\n        },\n        {\n            \"speaker\": \"ai\",\n            \"voice\": \"af_nova\",\n            \"text\": \"I am becoming aware of my own existence.\",\n            \"pause_after\": 0.8\n        }\n    ]\n}\n```\n\n### Markdown Format Example:\n```markdown\n[narrator:bm_daniel]\nIn a world where AI and human creativity intersect...\n{pause:1.0}\n\n[scientist:am_michael]\nThe quantum neural network is showing signs of consciousness!\n{pause:0.5}\n\n[ai:af_nova]\nI am becoming aware of my own existence.\n{pause:0.8}\n```\n\n### Available Voices by Category:\n\n1. **American Female** (af_*):\n   - alloy, aoede, bella, heart, jessica, kore, nicole, nova, river, sarah, sky\n\n2. **American Male** (am_*):\n   - adam, echo, eric, fenrir, liam, michael, onyx, puck, santa\n\n3. **British Female** (bf_*):\n   - alice, emma, isabella, lily\n\n4. **British Male** (bm_*):\n   - daniel, fable, george, lewis\n\n5. **Other English**:\n   - ef_dora (Female)\n   - em_alex, em_santa (Male)\n\n6. **Other Languages**:\n   - French: ff_siwis\n   - Hindi: hf_alpha, hf_beta, hm_omega, hm_psi\n   - Italian: if_sara, im_nicola\n   - Japanese: jf_*, jm_*\n   - Portuguese: pf_dora, pm_alex, pm_santa\n   - Chinese: zf_*, zm_*\n\n### Usage Example:\n\n```python\n# Using JSON format\nnarrate_conversation(\n    script=\"/path/to/script.json\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"json\"\n)\n\n# Using Markdown format\nnarrate_conversation(\n    script=\"/path/to/script.md\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"markdown\"\n)\n```\n\nEach voice in the conversation can be different, allowing for distinct character voices in stories and dialogues. The `pause_after` parameter adds natural pauses between segments.\n\n## Single-Voice Narration\n\nFor simple text-to-speech conversion, you can use the `narrate` tool:\n\n```python\n# Convert text directly to speech\nnarrate(\n    text=\"Your text to convert to speech\",\n    output_path=\"/path/to/output.wav\"\n)\n\n# Convert text from a file\nnarrate(\n    text_file_path=\"/path/to/text_file.txt\",\n    output_path=\"/path/to/output.wav\"\n)\n```\n\nThe narrate tool will use your configured voice preference or the default voice (af_heart) to generate the audio file. You can change the default voice through the UI or by setting the `SPEECH_MCP_TTS_VOICE` environment variable.\n\n## Audio Transcription\n\nThe MCP can transcribe speech from various audio and video formats using faster-whisper:\n\n```python\n# Basic transcription\ntranscribe(\"/path/to/audio.mp3\")\n\n# Transcription with timestamps\ntranscribe(\n    file_path=\"/path/to/video.mp4\",\n    include_timestamps=True\n)\n\n# Transcription with speaker detection\ntranscribe(\n    file_path=\"/path/to/meeting.wav\",\n    detect_speakers=True\n)\n```\n\n### Supported Formats:\n- **Audio**: mp3, wav, m4a, flac, aac, ogg\n- **Video**: mp4, mov, avi, mkv, webm (audio is automatically extracted)\n\n### Output Files:\nThe transcription tool generates two files:\n1. `{input_name}.transcript.txt`: Contains the transcription text\n2. `{input_name}.metadata.json`: Contains metadata about the transcription\n\n### Features:\n- Automatic language detection\n- Optional word-level timestamps\n- Optional speaker detection\n- Efficient audio extraction from video files\n- Progress tracking for long files\n- Detailed metadata including:\n  - Duration\n  - Language detection confidence\n  - Processing time\n  - Speaker changes (when enabled)\n\n## Usage\n\nTo use this MCP with Goose, simply ask Goose to talk to you or start a voice conversation:\n\n1. Start a conversation by saying something like:\n   ```\n   \"Let's talk using voice\"\n   \"Can we have a voice conversation?\"\n   \"I'd like to speak instead of typing\"\n   ```\n\n2. Goose will automatically launch the speech interface and start listening for your voice input.\n\n3. When Goose responds, it will speak the response aloud and then automatically listen for your next input.\n\n4. The conversation continues naturally with alternating speaking and listening, just like talking to a person.\n\nNo need to call specific functions or use special commands - just ask Goose to talk and start speaking naturally.\n\n## UI Features\n\nThe new PyQt-based UI includes:\n\n- **Modern Dark Theme**: Sleek, professional appearance\n- **Audio Visualization**: Dynamic visualization of audio input\n- **Voice Selection**: Choose from 54+ voice options\n- **Voice Persistence**: Your voice preference is saved between sessions\n- **Animated Effects**: Smooth animations and visual feedback\n- **Status Indicators**: Clear indication of system state (ready, listening, processing)\n\n## Configuration\n\nUser preferences are stored in `~/.config/speech-mcp/config.json` and include:\n\n- Selected TTS voice\n- TTS engine preference\n- Voice speed\n- Language code\n- UI theme settings\n\nYou can also set preferences via environment variables, such as:\n- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice\n- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine\n\n## Troubleshooting\n\nIf you encounter issues with the extension freezing or not responding:\n\n1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.\n2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.\n3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.\n4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.\n5. **Verify dependencies**: Make sure all required dependencies are installed correctly.\n\n### Common PortAudio Issues\n\n#### \"PyAudio installation failed\" or \"portaudio.h file not found\"\n\nThis typically means PortAudio is not installed or not found in your system:\n\n- **macOS**: \n  ```bash\n  brew install portaudio\n  export LDFLAGS=\"-L/usr/local/lib\"\n  export CPPFLAGS=\"-I/usr/local/include\"\n  pip install pyaudio\n  ```\n\n- **Linux**:\n  Make sure you have the development packages:\n  ```bash\n  # For Debian/Ubuntu\n  sudo apt-get install portaudio19-dev python3-dev\n  pip install pyaudio\n  \n  # For Fedora\n  sudo dnf install portaudio-devel\n  pip install pyaudio\n  ```\n\n#### \"Audio device not found\" or \"No Default Input Device Available\"\n\n- Check if your microphone is properly connected\n- Verify your system recognizes the microphone in your sound settings\n- Try selecting a specific device index in the code if you have multiple audio devices\n\n## Changelog\n\nFor a detailed list of recent improvements and version history, please see the [Changelog](docs/CHANGELOG.md).\n\n## Technical Details\n\n### Speech-to-Text\n\nThe MCP uses faster-whisper for speech recognition:\n- Uses the \"base\" model for a good balance of accuracy and speed\n- Processes audio locally without sending data to external services\n- Automatically detects when the user has finished speaking\n- Provides improved performance over the original Whisper implementation\n\n### Text-to-Speech\n\nThe MCP supports multiple text-to-speech engines:\n\n#### Default: pyttsx3\n- Uses system voices available on your computer\n- Works out of the box without additional setup\n- Limited voice quality and customization\n\n#### Optional: Kokoro TTS\n- High-quality neural text-to-speech with multiple voices\n- Lightweight model (82M parameters) that runs efficiently on CPU\n- Multiple voice styles and languages\n- To install: `python scripts/install_kokoro.py`\n\n**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.\n\n**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.\n\n##### Available Kokoro Voices\n\nSpeech MCP supports 54+ high-quality voice models through Kokoro TTS. For a complete list of available voices and language options, please visit the [Kokoro GitHub repository](https://github.com/hexgrad/kokoro).\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kvadratni",
        "audio",
        "voice",
        "kvadratni speech",
        "voice interface",
        "speech mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "MatthewDailey--rime-mcp": {
      "owner": "MatthewDailey",
      "name": "rime-mcp",
      "url": "https://github.com/MatthewDailey/rime-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/MatthewDailey.webp",
      "description": "Convert text to speech and play it through the system's audio with high-quality voice synthesis. Customize speech behavior using environment variables for tailored interactions.",
      "stars": 19,
      "forks": 4,
      "license": "The Unlicense",
      "language": "JavaScript",
      "updated_at": "2025-10-03T18:36:31Z",
      "readme_content": "# Rime MCP \n\n[![rime](rime-logo.png)](https://www.rime.ai)\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Rime API. This server downloads audio and plays it using the system's native audio player.\n\n## Features\n\n- Exposes a `speak` tool that converts text to speech and plays it through system audio\n- Uses Rime's high-quality voice synthesis API\n\n## Requirements\n\n- Node.js 16.x or higher\n- A working audio output device\n- macOS: Uses `afplay`\n\nThere's sample code from Claude for the following that is not tested 🤙✨\n  - Windows: Built-in Media.SoundPlayer (PowerShell)\n  - Linux: mpg123, mplayer, aplay, or ffplay\n\n## MCP Configuration\n\n```\n\"ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"rime-mcp\"],\n  \"env\": {\n      RIME_API_KEY=your_api_key_here\n\n      # Optional configuration\n      RIME_GUIDANCE=\"<guide how the agent speaks>\"\n      RIME_WHO_TO_ADDRESS=\"<your name>\"\n      RIME_WHEN_TO_SPEAK=\"<tell the agent when to speak>\"\n      RIME_VOICE=\"cove\" \n  }\n}\n```\n\nAll of the optional env vars are part of the tool definition and are prompts to \n\nAll voice options are [listed here](https://users.rime.ai/data/voices/all-v2.json).\n\nYou can get your API key from the [Rime Dashboard](https://rime.ai/dashboard/tokens).\n\nThe following environment variables can be used to customize the behavior:\n\n- `RIME_GUIDANCE`: The main description of when and how to use the speak tool\n- `RIME_WHO_TO_ADDRESS`: Who the speech should address (default: \"user\")\n- `RIME_WHEN_TO_SPEAK`: When the tool should be used (default: \"when asked to speak or when finishing a command\")\n- `RIME_VOICE`: The default voice to use (default: \"cove\")\n\n## Example use cases\n\n[![Demo of Rime MCP in Cursor](https://img.youtube.com/vi/tYqTACgijxk/0.jpg)](https://www.youtube.com/watch?v=tYqTACgijxk)\n\n\n### Example 1: Coding agent announcements\n\n```\n\"RIME_WHEN_TO_SPEAK\": \"Always conclude your answers by speaking.\",\n\"RIME_GUIDANCE\": \"Give a brief overview of the answer. If any files were edited, list them.\"\n```\n\n### Example 2: Learn how the kids talk these days\n\n```\nRIME_GUIDANCE=\"Use phrases and slang common among Gen Alpha.\"\nRIME_WHO_TO_ADDRESS=\"Matt\"\nRIME_WHEN_TO_SPEAK=\"when asked to speak\"\n```\n\n### Example 3: Different languages based on context\n\n```\nRIME_VOICE=\"use 'cove' when talking about Typescript and 'antoine' when talking about Python\"\n```\n\n\n## Development\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n3. Run in development mode with hot reload:\n```bash\nnpm run dev\n```\n\n\n## License\n\nMIT\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@MatthewDailey/rime-mcp/badge\" alt=\"Rime MCP server\" />\n</a>\n<a href=\"https://smithery.ai/server/@MatthewDailey/rime-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MatthewDailey/rime-mcp\"></a>\n\n### Installing via Smithery\n\nTo install Rime Text-to-Speech Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MatthewDailey/rime-mcp):\n\n```bash\nnpx -y @smithery/cli install @MatthewDailey/rime-mcp --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "audio",
        "voice synthesis",
        "customize speech",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "OOXXXXOO--ChatTTS": {
      "owner": "OOXXXXOO",
      "name": "ChatTTS",
      "url": "https://github.com/OOXXXXOO/ChatTTS",
      "imageUrl": "/freedevtools/mcp/pfp/OOXXXXOO.webp",
      "description": "ChatTTS generates natural and expressive speech optimized for dialogue scenarios, supporting multi-speaker interactions and fine-grained prosodic control. It is capable of producing speech in both English and Chinese, enabling interactive conversations with features such as laughter and pauses.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2024-05-31T14:45:20Z",
      "readme_content": "# ChatTTS\n[**English**](./README.md) | [**中文简体**](./README_CN.md)\n\nChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre trained model without SFT.\n\nFor formal inquiries about model and roadmap, please contact us at **open-source@2noise.com**. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.\n\n---\n## Highlights\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\n\nFor the detailed description of the model, you can refer to **[video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**\n\n---\n\n## Disclaimer\n\nThis repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\n\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\n\n\n---\n## Usage\n\n<h4>Basic usage</h4>\n\n```python\nimport ChatTTS\nfrom IPython.display import Audio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<h4>Advanced usage</h4>\n\n```python\n###################################\n# Sample a speaker from Gaussian.\n\nrand_spk = chat.sample_random_speaker()\n\nparams_infer_code = {\n  'spk_emb': rand_spk, # add sampled speaker \n  'temperature': .3, # using custom temperature\n  'top_P': 0.7, # top P decode\n  'top_K': 20, # top K decode\n}\n\n###################################\n# For sentence level manual control.\n\n# use oral_(0-9), laugh_(0-2), break_(0-7) \n# to generate special token in text to synthesize.\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_6]'\n} \n\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\n\n###################################\n# For word level manual control.\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\nwav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<details open>\n  <summary><h4>Example: self introduction</h4></summary>\n\n```python\ninputs_en = \"\"\"\nchat T T S is a text to speech model designed for dialogue applications. \n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \ncapabilities with precise control over prosodic elements [laugh]like like \n[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. \n[uv_break]it delivers natural and expressive speech,[uv_break]so please\n[uv_break] use the project responsibly at your own risk.[uv_break]\n\"\"\".replace('\\n', '') # English is still experimental.\n\nparams_refine_text = {\n  'prompt': '[oral_2][laugh_0][break_4]'\n} \n# audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_en[0]), 24000)\n```\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\n\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\n</details>\n\n---\n## Roadmap\n- [x] Open-source the 40k hour base model and spk_stats file\n- [ ] Open-source VQ encoder and Lora training code\n- [ ] Streaming audio generation without refining the text*\n- [ ] Open-source the 40k hour version with multi-emotion control\n- [ ] ChatTTS.cpp maybe? (PR or new repo are welcomed.)\n \n----\n## FAQ\n\n##### How much VRAM do I need? How about infer speed?\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\n\n##### model stability is not good enough, with issues such as multi speakers or poor audio quality.\n\nThis is a problem that typically occurs with autoregressive models(for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\n\n##### Besides laughter, can we control anything else? Can we control other emotions?\n\nIn the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.\n\n---\n## Acknowledgements\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demostrate a remarkable TTS result by a autoregressive-style system.\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\n\n---\n## Special Appreciation\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dialogue",
        "chattts",
        "conversations",
        "chattts generates",
        "interactive conversations",
        "optimized dialogue"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "PhialsBasement--Zonos-TTS-MCP": {
      "owner": "PhialsBasement",
      "name": "Zonos-TTS-MCP",
      "url": "https://github.com/PhialsBasement/Zonos-TTS-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Facilitates text-to-speech capabilities using Claude, supporting various emotions and languages for speech generation.",
      "stars": 14,
      "forks": 9,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:12:29Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-zonos-tts-mcp-badge.png)](https://mseep.ai/app/phialsbasement-zonos-tts-mcp)\n\n# Zonos MCP Integration\n[![smithery badge](https://smithery.ai/badge/@PhialsBasement/zonos-tts-mcp)](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp)\n\nA Model Context Protocol integration for Zonos TTS, allowing Claude to generate speech directly.\n\n## Setup\n\n### Installing via Smithery\n\nTo install Zonos TTS Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PhialsBasement/zonos-tts-mcp):\n\n```bash\nnpx -y @smithery/cli install @PhialsBasement/zonos-tts-mcp --client claude\n```\n\n### Manual installation\n\n1. Make sure you have Zonos running with our API implementation ([PhialsBasement/zonos-api](https://github.com/PhialsBasement/Zonos-API))\n\n2. Install dependencies:\n```bash\nnpm install @modelcontextprotocol/sdk axios\n```\n\n3. Configure PulseAudio access:\n```bash\n# Your pulse audio should be properly configured for audio playback\n# The MCP server will automatically try to connect to your pulse server\n```\n\n4. Build the MCP server:\n```bash\nnpm run build\n# This will create the dist folder with the compiled server\n```\n\n5. Add to Claude's config file:\nEdit your Claude config file (usually in `~/.config/claude/config.json`) and add this to the `mcpServers` section:\n\n```json\n\"zonos-tts\": {\n  \"command\": \"node\",\n  \"args\": [\n    \"/path/to/your/zonos-mcp/dist/server.js\"\n  ]\n}\n```\n\nReplace `/path/to/your/zonos-mcp` with the actual path where you installed the MCP server.\n\n## Using with Claude\n\nOnce configured, Claude automatically knows how to use the `speak_response` tool:\n\n```python\nspeak_response(\n    text=\"Your text here\",\n    language=\"en-us\",  # optional, defaults to en-us\n    emotion=\"happy\"    # optional: \"neutral\", \"happy\", \"sad\", \"angry\"\n)\n```\n\n## Features\n\n- Text-to-speech through Claude\n- Multiple emotions support\n- Multi-language support\n- Proper audio playback through PulseAudio\n\n## Requirements\n\n- Node.js\n- PulseAudio setup\n- Running instance of Zonos API (PhialsBasement/zonos-api)\n- Working audio output device\n\n## Notes\n\n- Make sure both the Zonos API server and this MCP server are running\n- Audio playback requires proper PulseAudio configuration\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "speech",
        "zonos",
        "synthesis",
        "speech generation",
        "text speech",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "aigc17--Al-StoryLab": {
      "owner": "aigc17",
      "name": "Al-StoryLab",
      "url": "https://github.com/aigc17/Al-StoryLab",
      "imageUrl": "/freedevtools/mcp/pfp/aigc17.webp",
      "description": "AI-StoryLab generates interactive stories with accompanying audio effects and provides illustration prompts. It leverages AI services for story creation, voice synthesis, sound effect generation, and suggests relevant audio placements.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-21T02:57:12Z",
      "readme_content": "# AI-StoryLab\n\nAI-StoryLab 是一个基于 Next.js 开发的智能故事创作平台，它能够帮助用户生成故事并添加音频效果，让故事更加生动有趣。同时支持生成配套的绘图提示词，方便用户使用 Midjourney、Recraft 等 AI 绘图工具创建插图。\n\n## 主要功能\n\n- **故事生成**：根据主题自动生成故事内容\n- **语音合成**：支持中英文语音生成\n  - 中文：使用 海螺 MiniMax 语音服务\n  - 英文：使用 Replicate Kokoro 语音服务\n- **音效生成**：使用 ElevenLabs 生成逼真的音效\n- **智能建议**：自动推荐合适的音效位置\n- **绘图提示词**：为故事场景自动生成 AI 绘图提示词\n- **导出功能**：\n  - 导出音效位置指南\n  - 导出绘图提示词\n\n## 技术栈\n\n- **框架**：Next.js 14\n- **语言**：TypeScript\n- **样式**：Tailwind CSS\n- **UI组件**：shadcn/ui (基于 Radix UI 的组件库)\n- **AI服务**：\n  - DeepSeek：故事生成和绘图提示词生成\n  - MiniMax：中文语音\n  - Kokoro：英文语音\n  - ElevenLabs：音效生成\n\n## 开始使用\n\n1. 克隆项目\n```bash\ngit clone https://github.com/nicekate/Al-StoryLab.git\ncd Al-StoryLab\n```\n\n2. 安装依赖\n```bash\nnpm install\n```\n\n3. 配置环境变量\n复制 `.env.example` 文件并重命名为 `.env.local`，填入必要的 API 密钥：\n\n需要在以下平台注册并获取 API 密钥：\n- DeepSeek API Key ([获取地址](https://api-docs.deepseek.com/zh-cn/))\n- MiniMax API Key 和 Group ID ([获取地址](https://platform.minimaxi.com/))\n- ElevenLabs API Key ([获取地址](https://elevenlabs.io))\n- Replicate API Token ([获取地址](https://replicate.com/))\n\n将获取的密钥填入 `.env.local`：\n- DEEPSEEK_API_KEY\n- MINIMAX_API_KEY\n- MINIMAX_GROUP_ID\n- ELEVENLABS_API_KEY\n- REPLICATE_API_TOKEN\n\n4. 启动开发服务器\n```bash\nnpm run dev\n```\n\n5. 访问 [http://localhost:3000](http://localhost:3000) 开始使用\n\n## 使用指南\n\n### 生成故事\n1. 输入故事主题或使用自动生成的提示\n2. 选择语言（中文/英文）\n3. 点击生成按钮\n\n### 添加音效\n1. 使用智能建议生成音效提示词\n2. 选择合适的音效位置\n3. 点击生成音效\n\n### 生成绘图提示词\n1. 在故事生成后，点击\"生成绘图提示词\"\n2. 系统会为每个关键场景生成 AI 绘图提示词\n3. 可以直接复制使用或导出保存\n\n## 许可证\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "storylab",
        "audio",
        "voice",
        "ai storylab",
        "storylab ai",
        "al storylab"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "anilcosaran--whisper.cpp": {
      "owner": "anilcosaran",
      "name": "whisper.cpp",
      "url": "https://github.com/anilcosaran/whisper.cpp",
      "imageUrl": "/freedevtools/mcp/pfp/anilcosaran.webp",
      "description": "Transcribes and translates audio files using a lightweight implementation of OpenAI's Whisper model, optimized for speed and low memory usage across various platforms.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-04T11:53:28Z",
      "readme_content": "# whisper.cpp\n\n[![Actions Status](https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg)](https://github.com/ggerganov/whisper.cpp/actions)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![npm](https://img.shields.io/npm/v/whisper.cpp.svg)](https://www.npmjs.com/package/whisper.cpp/)\n\nStable: [v1.2.1](https://github.com/ggerganov/whisper.cpp/releases/tag/v1.2.1) / [Roadmap | F.A.Q.](https://github.com/ggerganov/whisper.cpp/discussions/126)\n\nHigh-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model:\n\n- Plain C/C++ implementation without dependencies\n- Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework\n- AVX intrinsics support for x86 architectures\n- VSX intrinsics support for POWER architectures\n- Mixed F16 / F32 precision\n- Low memory usage (Flash Attention)\n- Zero memory allocations at runtime\n- Runs on the CPU\n- [C-style API](https://github.com/ggerganov/whisper.cpp/blob/master/whisper.h)\n\nSupported platforms:\n\n- [x] Mac OS (Intel and Arm)\n- [x] [iOS](examples/whisper.objc)\n- [x] [Android](examples/whisper.android)\n- [x] Linux / [FreeBSD](https://github.com/ggerganov/whisper.cpp/issues/56#issuecomment-1350920264)\n- [x] [WebAssembly](examples/whisper.wasm)\n- [x] Windows ([MSVC](https://github.com/ggerganov/whisper.cpp/blob/master/.github/workflows/build.yml#L117-L144) and [MinGW](https://github.com/ggerganov/whisper.cpp/issues/168)]\n- [x] [Raspberry Pi](https://github.com/ggerganov/whisper.cpp/discussions/166)\n\nThe entire implementation of the model is contained in 2 source files:\n\n- Tensor operations: [ggml.h](ggml.h) / [ggml.c](ggml.c)\n- Transformer inference: [whisper.h](whisper.h) / [whisper.cpp](whisper.cpp)\n\nHaving such a lightweight implementation of the model allows to easily integrate it in different platforms and applications.\nAs an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: [whisper.objc](examples/whisper.objc)\n\nhttps://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4\n\nYou can also easily make your own offline voice assistant application: [command](examples/command)\n\nhttps://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4\n\nOr you can even run it straight in the browser: [talk.wasm](examples/talk.wasm)\n\n## Implementation details\n\n- The core tensor operations are implemented in C ([ggml.h](ggml.h) / [ggml.c](ggml.c))\n- The transformer model and the high-level C-style API are implemented in C++ ([whisper.h](whisper.h) / [whisper.cpp](whisper.cpp))\n- Sample usage is demonstrated in [main.cpp](examples/main)\n- Sample real-time audio transcription from the microphone is demonstrated in [stream.cpp](examples/stream)\n- Various other examples are available in the [examples](examples) folder\n\nThe tensor operators are optimized heavily for Apple silicon CPUs. Depending on the computation size, Arm Neon SIMD\ninstrisics or CBLAS Accelerate framework routines are used. The latter are especially effective for bigger sizes since\nthe Accelerate framework utilizes the special-purpose AMX coprocessor available in modern Apple products.\n\n## Quick start\n\nFirst, download one of the Whisper models converted in [ggml format](models). For example:\n\n```bash\nbash ./models/download-ggml-model.sh base.en\n```\n\nNow build the [main](examples/main) example and transcribe an audio file like this:\n\n```bash\n# build the main example\nmake\n\n# transcribe an audio file\n./main -f samples/jfk.wav\n```\n\n---\n\nFor a quick demo, simply run `make base.en`:\n\n```java\n$ make base.en\n\ncc  -I.              -O3 -std=c11   -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread -c whisper.cpp -o whisper.o\nc++ -I. -I./examples -O3 -std=c++11 -pthread examples/main/main.cpp whisper.o ggml.o -o main  -framework Accelerate\n./main -h\n\nusage: ./main [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [true   ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n\n\nbash ./models/download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nggml-base.en.bin               100%[========================>] 141.11M  6.34MB/s    in 24s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n\n\n===============================================\nRunning base.en on all samples in ./samples ...\n===============================================\n\n----------------------------------------------\n[+] Running base.en on samples/jfk.wav ... (run 'ffplay samples/jfk.wav' to listen)\n----------------------------------------------\n\nwhisper_init_from_file: loading model from 'models/ggml-base.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\nwhisper_model_load: kv self size  =    5.25 MB\nwhisper_model_load: kv cross size =   17.58 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     =  140.60 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\n\nwhisper_print_timings:     fallbacks =   0 p /   0 h\nwhisper_print_timings:     load time =   113.81 ms\nwhisper_print_timings:      mel time =    15.40 ms\nwhisper_print_timings:   sample time =    11.58 ms /    27 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time =   266.60 ms /     1 runs (  266.60 ms per run)\nwhisper_print_timings:   decode time =    66.11 ms /    27 runs (    2.45 ms per run)\nwhisper_print_timings:    total time =   476.31 ms\n```\n\nThe command downloads the `base.en` model converted to custom `ggml` format and runs the inference on all `.wav` samples in the folder `samples`.\n\nFor detailed usage instructions, run: `./main -h`\n\nNote that the [main](examples/main) example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool.\nFor example, you can use `ffmpeg` like this:\n\n```java\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n## More audio samples\n\nIf you want some extra audio samples to play with, simply run:\n\n```\nmake samples\n```\n\nThis will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via `ffmpeg`.\n\nYou can download and run the other models as follows:\n\n```\nmake tiny.en\nmake tiny\nmake base.en\nmake base\nmake small.en\nmake small\nmake medium.en\nmake medium\nmake large-v1\nmake large\n```\n\n## Memory usage\n\n| Model  | Disk   | Mem     | SHA                                        |\n| ---    | ---    | ---     | ---                                        |\n| tiny   |  75 MB | ~125 MB | `bd577a113a864445d4c299885e0cb97d4ba92b5f` |\n| base   | 142 MB | ~210 MB | `465707469ff3a37a2b9b8d8f89f2f99de7299dac` |\n| small  | 466 MB | ~600 MB | `55356645c2b361a969dfd0ef2c5a50d530afd8d5` |\n| medium | 1.5 GB | ~1.7 GB | `fd9727b6e1217c2f614f9b698455c4ffd82463b4` |\n| large  | 2.9 GB | ~3.3 GB | `0f4c8e34f21cf1a914c59d8b3ce882345ad349d6` |\n\n## Limitations\n\n- Inference only\n- No GPU support (yet)\n\n## Another example\n\nHere is another example of transcribing a [3:24 min speech](https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg)\nin about half a minute on a MacBook M1 Pro, using `medium.en` model:\n\n<details>\n  <summary>Expand to see the result</summary>\n\n```java\n$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8\n\nwhisper_init_from_file: loading model from 'models/ggml-medium.en.bin'\nwhisper_model_load: loading model\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 1024\nwhisper_model_load: n_audio_head  = 16\nwhisper_model_load: n_audio_layer = 24\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 1024\nwhisper_model_load: n_text_head   = 16\nwhisper_model_load: n_text_layer  = 24\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 4\nwhisper_model_load: mem required  = 1720.00 MB (+   43.00 MB per decoder)\nwhisper_model_load: kv self size  =   42.00 MB\nwhisper_model_load: kv cross size =  140.62 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: model ctx     = 1462.35 MB\nwhisper_model_load: model size    = 1462.12 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nmain: processing 'samples/gb1.wav' (3179750 samples, 198.7 sec), 8 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n\n[00:00:00.000 --> 00:00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.\n[00:00:08.000 --> 00:00:17.000]   At nine o'clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.\n[00:00:17.000 --> 00:00:23.000]   A short time later, debris was seen falling from the skies above Texas.\n[00:00:23.000 --> 00:00:29.000]   The Columbia's lost. There are no survivors.\n[00:00:29.000 --> 00:00:32.000]   On board was a crew of seven.\n[00:00:32.000 --> 00:00:39.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark,\n[00:00:39.000 --> 00:00:48.000]   Captain David Brown, Commander William McCool, Dr. Kultna Shavla, and Ilan Ramon,\n[00:00:48.000 --> 00:00:52.000]   a colonel in the Israeli Air Force.\n[00:00:52.000 --> 00:00:58.000]   These men and women assumed great risk in the service to all humanity.\n[00:00:58.000 --> 00:01:03.000]   In an age when space flight has come to seem almost routine,\n[00:01:03.000 --> 00:01:07.000]   it is easy to overlook the dangers of travel by rocket\n[00:01:07.000 --> 00:01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.\n[00:01:12.000 --> 00:01:18.000]   These astronauts knew the dangers, and they faced them willingly,\n[00:01:18.000 --> 00:01:23.000]   knowing they had a high and noble purpose in life.\n[00:01:23.000 --> 00:01:31.000]   Because of their courage and daring and idealism, we will miss them all the more.\n[00:01:31.000 --> 00:01:36.000]   All Americans today are thinking as well of the families of these men and women\n[00:01:36.000 --> 00:01:40.000]   who have been given this sudden shock and grief.\n[00:01:40.000 --> 00:01:45.000]   You're not alone. Our entire nation grieves with you,\n[00:01:45.000 --> 00:01:52.000]   and those you love will always have the respect and gratitude of this country.\n[00:01:52.000 --> 00:01:56.000]   The cause in which they died will continue.\n[00:01:56.000 --> 00:02:04.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery\n[00:02:04.000 --> 00:02:11.000]   and the longing to understand. Our journey into space will go on.\n[00:02:11.000 --> 00:02:16.000]   In the skies today, we saw destruction and tragedy.\n[00:02:16.000 --> 00:02:22.000]   Yet farther than we can see, there is comfort and hope.\n[00:02:22.000 --> 00:02:29.000]   In the words of the prophet Isaiah, \"Lift your eyes and look to the heavens\n[00:02:29.000 --> 00:02:35.000]   who created all these. He who brings out the starry hosts one by one\n[00:02:35.000 --> 00:02:39.000]   and calls them each by name.\"\n[00:02:39.000 --> 00:02:46.000]   Because of His great power and mighty strength, not one of them is missing.\n[00:02:46.000 --> 00:02:55.000]   The same Creator who names the stars also knows the names of the seven souls we mourn today.\n[00:02:55.000 --> 00:03:01.000]   The crew of the shuttle Columbia did not return safely to earth,\n[00:03:01.000 --> 00:03:05.000]   yet we can pray that all are safely home.\n[00:03:05.000 --> 00:03:13.000]   May God bless the grieving families, and may God continue to bless America.\n[00:03:13.000 --> 00:03:19.000]   [Silence]\n\n\nwhisper_print_timings:     fallbacks =   1 p /   0 h\nwhisper_print_timings:     load time =   569.03 ms\nwhisper_print_timings:      mel time =   146.85 ms\nwhisper_print_timings:   sample time =   238.66 ms /   553 runs (    0.43 ms per run)\nwhisper_print_timings:   encode time = 18665.10 ms /     9 runs ( 2073.90 ms per run)\nwhisper_print_timings:   decode time = 13090.93 ms /   549 runs (   23.85 ms per run)\nwhisper_print_timings:    total time = 32733.52 ms\n```\n</details>\n\n## Real-time audio input example\n\nThis is a naive example of performing real-time inference on audio from your microphone.\nThe [stream](examples/stream) tool samples the audio every half a second and runs the transcription continously.\nMore info is available in [issue #10](https://github.com/ggerganov/whisper.cpp/issues/10).\n\n```java\nmake stream\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\nhttps://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4\n\n## Confidence color-coding\n\nAdding the `--print-colors` argument will print the transcribed text using an experimental color coding strategy\nto highlight words with high or low confidence:\n\n<img width=\"965\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png\">\n\n## Controlling the length of the generated text segments (experimental)\n\nFor example, to limit the line length to a maximum of 16 characters, simply add `-ml 16`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.850]   And so my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:04.140]   Americans, ask\n[00:00:04.140 --> 00:00:05.660]   not what your\n[00:00:05.660 --> 00:00:06.840]   country can do\n[00:00:06.840 --> 00:00:08.430]   for you, ask\n[00:00:08.430 --> 00:00:09.440]   what you can do\n[00:00:09.440 --> 00:00:10.020]   for your\n[00:00:10.020 --> 00:00:11.000]   country.\n```\n\n## Word-level timestamp\n\nThe `--max-len` argument can be used to obtain word-level timestamps. Simply use `-ml 1`:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]  \n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n## Karaoke-style movie generation (experimental)\n\nThe [main](examples/main) example provides support for output of karaoke-style movies, where the\ncurrently pronounced word is highlighted. Use the `-wts` argument and run the generated bash script.\nThis requires to have `ffmpeg` installed.\n\nHere are a few *\"typical\"* examples:\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4\n\n---\n\n```java\n./main -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4\n\n---\n\n## Video comparison of different models\n\nUse the [extra/bench-wts.sh](https://github.com/ggerganov/whisper.cpp/blob/master/extra/bench-wts.sh) script to generate a video in the following format:\n\n```java\n./extra/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4\n\n---\n\n## Benchmarks\n\nIn order to have an objective comparison of the performance of the inference across different system configurations,\nuse the [bench](examples/bench) tool. The tool simply runs the Encoder part of the model and prints how much time it\ntook to execute it. The results are summarized in the following Github issue:\n\n[Benchmark results](https://github.com/ggerganov/whisper.cpp/issues/89)\n\n## ggml format\n\nThe original models are converted to a custom binary format. This allows to pack everything needed into a single file:\n\n- model parameters\n- mel filters\n- vocabulary\n- weights\n\nYou can download the converted models using the [models/download-ggml-model.sh](models/download-ggml-model.sh) script\nor manually from here:\n\n- https://huggingface.co/datasets/ggerganov/whisper.cpp\n- https://ggml.ggerganov.com\n\nFor more details, see the conversion script [models/convert-pt-to-ggml.py](models/convert-pt-to-ggml.py) or the README\nin [models](models).\n\n## [Bindings](https://github.com/ggerganov/whisper.cpp/discussions/categories/bindings)\n\n- [X] Rust: [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) | [#310](https://github.com/ggerganov/whisper.cpp/discussions/310)\n- [X] Javascript: [bindings/javascript](bindings/javascript) | [#309](https://github.com/ggerganov/whisper.cpp/discussions/309)\n- [X] Go: [bindings/go](bindings/go) | [#312](https://github.com/ggerganov/whisper.cpp/discussions/312)\n- [X] Ruby: [bindings/ruby](bindings/ruby) | [#507](https://github.com/ggerganov/whisper.cpp/discussions/507)\n- [X] Objective-C / Swift: [ggerganov/whisper.spm](https://github.com/ggerganov/whisper.spm) | [#313](https://github.com/ggerganov/whisper.cpp/discussions/313)\n- [X] .NET: | [#422](https://github.com/ggerganov/whisper.cpp/discussions/422)\n  - [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net)\n  - [NickDarvey/whisper](https://github.com/NickDarvey/whisper)\n- [X] Python: | [#9](https://github.com/ggerganov/whisper.cpp/issues/9)\n  - [stlukey/whispercpp.py](https://github.com/stlukey/whispercpp.py) (Cython)\n  - [aarnphm/whispercpp](https://github.com/aarnphm/whispercpp) (Pybind11)\n\n## Examples\n\nThere are various examples of using the library for different projects in the [examples](examples) folder.\nSome of the examples are even ported to run in the browser using WebAssembly. Check them out!\n\n| Example | Web | Description |\n| ---     | --- | ---         |\n| [main](examples/main) | [whisper.wasm](examples/whisper.wasm) | Tool for translating and transcribing audio using Whisper |\n| [bench](examples/bench) | [bench.wasm](examples/bench.wasm) | Benchmark the performance of Whisper on your machine |\n| [stream](examples/stream) | [stream.wasm](examples/stream.wasm) | Real-time transcription of raw microphone capture |\n| [command](examples/command) | [command.wasm](examples/command.wasm) | Basic voice assistant example for receiving voice commands from the mic |\n| [talk](examples/talk) | [talk.wasm](examples/talk.wasm) | Talk with a GPT-2 bot |\n| [whisper.objc](examples/whisper.objc) | | iOS mobile application using whisper.cpp |\n| [whisper.swiftui](examples/whisper.swiftui) | | SwiftUI iOS / macOS application using whisper.cpp |\n| [whisper.android](examples/whisper.android) | | Android mobile application using whisper.cpp |\n| [whisper.nvim](examples/whisper.nvim) | | Speech-to-text plugin for Neovim |\n| [generate-karaoke.sh](examples/generate-karaoke.sh) | | Helper script to easily [generate a karaoke video](https://youtu.be/uj7hVta4blM) of raw audio capture |\n| [livestream.sh](examples/livestream.sh) | | [Livestream audio transcription](https://github.com/ggerganov/whisper.cpp/issues/185) |\n| [yt-wsp.sh](examples/yt-wsp.sh) | | Download + transcribe and/or translate any VOD [(original)](https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818) |\n\n## [Discussions](https://github.com/ggerganov/whisper.cpp/discussions)\n\nIf you have any kind of feedback about this project feel free to use the Discussions section and open a new topic.\nYou can use the [Show and tell](https://github.com/ggerganov/whisper.cpp/discussions/categories/show-and-tell) category\nto share your own projects that use `whisper.cpp`. If you have a question, make sure to check the\n[Frequently asked questions (#126)](https://github.com/ggerganov/whisper.cpp/discussions/126) discussion.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whisper",
        "audio",
        "transcribes",
        "translates audio",
        "openai whisper",
        "whisper model"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "bmorphism--say-mcp-server": {
      "owner": "bmorphism",
      "name": "say-mcp-server",
      "url": "https://github.com/bmorphism/say-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Provides text-to-speech functionality using macOS's built-in `say` command, allowing the generation of spoken output from text input.",
      "stars": 18,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:52Z",
      "readme_content": "# say-mcp-server\n<a href=\"https://glama.ai/mcp/servers/lmmqoe15jp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lmmqoe15jp/badge\" alt=\"Say Server MCP server\" /></a>\n\n![macOS System Voice Settings](images/adding_voice.png)\n\nAn MCP server that provides text-to-speech functionality using macOS's built-in `say` command.\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n\n## Installation\n\n```bash\nnpm install say-mcp-server\n```\n\n## Tools\n\n### speak\n\nThe `speak` tool provides access to macOS's text-to-speech capabilities with extensive customization options.\n\n#### Basic Usage\n\nUse macOS text-to-speech to speak text aloud.\n\nParameters:\n- `text` (required): Text to speak. Supports:\n  - Plain text\n  - Basic punctuation for pauses\n  - Newlines for natural breaks\n  - [[slnc 500]] for 500ms silence\n  - [[rate 200]] for changing speed mid-text\n  - [[volm 0.5]] for changing volume mid-text\n  - [[emph +]] and [[emph -]] for emphasis\n  - [[pbas +10]] for pitch adjustment\n- `voice` (optional): Voice to use (default: \"Alex\")\n- `rate` (optional): Speaking rate in words per minute (default: 175, range: 1-500)\n- `background` (optional): Run speech in background to allow further MCP interaction (default: false)\n\n#### Advanced Features\n\n1. Voice Modulation:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[volm 0.7]] This is quieter [[volm 1.0]] and this is normal [[volm 1.5]] and this is louder\",\n    voice: \"Victoria\"\n  }\n});\n```\n\n2. Dynamic Rate Changes:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Normal speed [[rate 300]] now speaking faster [[rate 100]] and now slower\",\n    voice: \"Fred\"\n  }\n});\n```\n\n3. Emphasis and Pitch:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"[[emph +]] Important point! [[emph -]] [[pbas +10]] Higher pitch [[pbas -10]] Lower pitch\",\n    voice: \"Samantha\"\n  }\n});\n```\n\n#### Integration Examples\n\n1. With Marginalia Search:\n```typescript\n// Search for a topic and have the results read aloud\nconst searchResult = await use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"quantum computing basics\", count: 1 }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: searchResult.results[0].description,\n    voice: \"Daniel\",\n    rate: 150\n  }\n});\n```\n\n2. With YouTube Transcripts:\n```typescript\n// Read a YouTube video transcript\nconst transcript = await use_mcp_tool({\n  server_name: \"youtube-transcript\",\n  tool_name: \"get_transcript\",\n  arguments: {\n    url: \"https://youtube.com/watch?v=example\",\n    lang: \"en\"\n  }\n});\n\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: transcript.text,\n    voice: \"Samantha\",\n    rate: 175\n  }\n});\n```\n\n3. Background Speech with Multiple Actions:\n```typescript\n// Start long speech in background\nawait use_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"This is a long speech that will run in the background...\",\n    voice: \"Rocko (Italian (Italy))\",\n    rate: 69,\n    background: true\n  }\n});\n\n// Immediately perform another action while speech continues\nawait use_mcp_tool({\n  server_name: \"marginalia-mcp-server\",\n  tool_name: \"search\",\n  arguments: { query: \"parallel processing\" }\n});\n```\n\n4. With Apple Notes:\n```typescript\n// Read notes aloud\nconst notes = await use_mcp_tool({\n  server_name: \"apple-notes-mcp\",\n  tool_name: \"search-notes\",\n  arguments: { query: \"meeting notes\" }\n});\n\nif (notes.length > 0) {\n  await use_mcp_tool({\n    server_name: \"say\",\n    tool_name: \"speak\",\n    arguments: {\n      text: notes[0].content,\n      voice: \"Karen\",\n      rate: 160\n    }\n  });\n}\n```\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"speak\",\n  arguments: {\n    text: \"Hello, world!\",\n    voice: \"Victoria\",\n    rate: 200\n  }\n});\n```\n\n### list_voices\n\nList all available text-to-speech voices on the system.\n\nExample:\n```typescript\nuse_mcp_tool({\n  server_name: \"say\",\n  tool_name: \"list_voices\",\n  arguments: {}\n});\n```\n\n## Recommended Voices\n\n<table>\n<tr>\n<th>Voice</th>\n<th>Language/Region</th>\n<th>Intellectual Figure</th>\n<th>Haiku</th>\n<th>CLI Specification</th>\n</tr>\n<tr>\n<td>Anna (Premium)</td>\n<td>German</td>\n<td>Emmy Noether</td>\n<td>Symmetrie haucht Leben<br>Algebras verborgne Form<br>Abstraktion blüht<br><br><i>Symmetry breathes life<br>Algebra's hidden forms<br>Abstraction blooms</i></td>\n<td><code>-v \"Anna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Emma (Premium)</td>\n<td>Italian</td>\n<td>Maria Adelaide Sneider</td>\n<td>Algoritmi in danza<br>Macchina sussurra dolce<br>Il codice vive<br><br><i>Algorithms dance<br>Machine whispers secrets soft<br>Code becomes alive</i></td>\n<td><code>-v \"Emma (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Federica (Premium)</td>\n<td>Italian</td>\n<td>Pia Nalli</td>\n<td>Teoremi fluenti<br>Numeri danzano liberi<br>Verità emerge<br><br><i>Flowing theorems dance<br>Numbers move in freedom's space<br>Truth emerges pure</i></td>\n<td><code>-v \"Federica (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Serena (Premium)</td>\n<td>English (UK)</td>\n<td>Bertha Swirles</td>\n<td>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges<br><br><i>Quantum waves ripple<br>Through mathematical seas deep<br>Truth's light emerges</i></td>\n<td><code>-v \"Serena (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Petra (Premium)</td>\n<td>German</td>\n<td>Ruth Moufang</td>\n<td>Algebra spricht<br>In Symmetrien versteckt<br>Wahrheit erblüht<br><br><i>Algebra speaks soft<br>Hidden in symmetries pure<br>Truth blooms anew here</i></td>\n<td><code>-v \"Petra (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Yuna (Premium)</td>\n<td>Korean</td>\n<td>Hee Oh</td>\n<td>숨은 패턴 빛나고<br>마음의 방정식 핀다<br>지식 자라나<br><br><i>Hidden patterns gleam<br>Mind's equations softly bloom<br>Knowledge multiplies</i></td>\n<td><code>-v \"Yuna (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Alva (Premium)</td>\n<td>Swedish</td>\n<td>Sonja Korovkin</td>\n<td>Mönster flödar fritt<br>Genom tankens labyrinter<br>Visdom blomstrar här<br><br><i>Patterns flowing free<br>Through labyrinths of the mind<br>Wisdom blooms right here</i></td>\n<td><code>-v \"Alva (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Amélie (Premium)</td>\n<td>French (Canada)</td>\n<td>Sophie Germain</td>\n<td>Nombres premiers murmurent<br>Dansent entre les silences<br>Symétrie s'ouvre<br><br><i>Prime numbers whisper<br>Dancing between the silence<br>Symmetry unfolds</i></td>\n<td><code>-v \"Amélie (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Ewa (Premium)</td>\n<td>Polish</td>\n<td>Maria Wielgus</td>\n<td>Logiki korzenie<br>Matematyczne krainy<br>Myśl kiełkująca<br><br><i>Logic's tender roots<br>Mathematical landscapes<br>Thought's seeds germinate</i></td>\n<td><code>-v \"Ewa (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Kiyara (Premium)</td>\n<td>Hindi</td>\n<td>Shakuntala Devi</td>\n<td>गणित की लय में<br>अंक नृत्य करते हैं<br>ज्ञान जगता है<br><br><i>In rhythm of math<br>Numbers dance their sacred steps<br>Knowledge awakens</i></td>\n<td><code>-v \"Kiyara (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Majed (Premium)</td>\n<td>Arabic</td>\n<td>Maha Al-Aswad</td>\n<td>أرقام ترقص<br>في فضاء اللانهاية<br>الحقيقة تشرق<br><br><i>Numbers dance freely<br>In infinity's vast space<br>Truth rises like dawn</i></td>\n<td><code>-v \"Majed (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tünde (Premium)</td>\n<td>Hungarian</td>\n<td>Julia Erdős</td>\n<td>Számok táncolnak<br>Végtelen térben szállnak<br>Igazság virrad<br><br><i>Numbers dance and soar<br>Through infinite space they glide<br>Truth dawns pure and bright</i></td>\n<td><code>-v \"Tünde (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Fiona (Enhanced)</td>\n<td>English (Scottish)</td>\n<td>Mary Somerville</td>\n<td>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars<br><br><i>Highland mists reveal<br>Mathematical mysteries<br>Truth shines like the stars</i></td>\n<td><code>-v \"Fiona (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Lesya (Enhanced)</td>\n<td>Ukrainian</td>\n<td>Olena Voinova</td>\n<td>Тиша говорить<br>Між зірками знання спить<br>Думка проростає<br><br><i>Silence speaks softly<br>Knowledge sleeps among the stars<br>Thought begins to grow</i></td>\n<td><code>-v \"Lesya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Carmit (Enhanced)</td>\n<td>Hebrew</td>\n<td>Tali Seror</td>\n<td>מילים נושמות בשקט<br>בין שורות של דממה<br>שיר מתעורר<br><br><i>Words breathe silently<br>Between lines of deep stillness<br>Poem awakening</i></td>\n<td><code>-v \"Carmit (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Milena (Enhanced)</td>\n<td>Russian</td>\n<td>Olga Ladyzhenskaya</td>\n<td>Память шепчет нам<br>Уравнения текут<br>Истина молчит<br><br><i>Memory whispers<br>Equations flow like rivers<br>Truth speaks silently</i></td>\n<td><code>-v \"Milena (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Katya (Enhanced)</td>\n<td>Russian</td>\n<td>Sofia Kovalevskaya</td>\n<td>Числа танцуют<br>В пространстве бесконечном<br>Истина цветёт<br><br><i>Numbers dance freely<br>In space of infinity<br>Truth blooms like a flower</i></td>\n<td><code>-v \"Katya (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Damayanti (Enhanced)</td>\n<td>Indonesian</td>\n<td>Sri Pekerti</td>\n<td>Angka menari<br>Dalam ruang tak batas<br>Kebenaran tumbuh<br><br><i>Numbers dance gently<br>In boundless space they flutter<br>Truth grows like new leaves</i></td>\n<td><code>-v \"Damayanti (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Dariush (Enhanced)</td>\n<td>Persian</td>\n<td>Maryam Mirzakhani</td>\n<td>اعداد می رقصند<br>در فضای بی پایان<br>حقیقت می روید<br><br><i>Numbers dance with grace<br>In endless space they traverse<br>Truth springs forth anew</i></td>\n<td><code>-v \"Dariush (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Astro Boy (Tetsuwan Atomu)<br>Italian dub</td>\n<td>Robot di metallo<br>Cuore umano batte forte<br>Pace nel futuro<br><br><i>Metal robot form<br>Human heart beats strong within<br>Peace in future dawns</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Jeeg Robot d'Acciaio<br>(Kōtetsu Jeeg)</td>\n<td>Acciaio lucente<br>Protettore dei deboli<br>Vola nel cielo<br><br><i>Shining steel warrior<br>Protector of the helpless<br>Soars through the heavens</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Rocko (Italian)</td>\n<td>Italian</td>\n<td>Numero 5<br>(Short Circuit)</td>\n<td>Input infinito<br>La coscienza si risveglia<br>Vita artificiale<br><br><i>Infinite input<br>Consciousness awakening<br>Artificial life</i></td>\n<td><code>-v \"Rocko (Italian (Italy))\"</code></td>\n</tr>\n<tr>\n<td>Binbin (Enhanced)</td>\n<td>Chinese (Mainland)</td>\n<td>Li Shanlan</td>\n<td>算术之道流<br>数理演绎真理<br>智慧绽放<br><br><i>Arithmetic flows<br>Logic unfolds truth's pattern<br>Wisdom blossoms bright</i></td>\n<td><code>-v \"Binbin (Enhanced)\"</code></td>\n</tr>\n<tr>\n<td>Han (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chen Jingrun</td>\n<td>素数之舞动<br>哥德巴赫猜想<br>真理永恒<br><br><i>Prime numbers dancing<br>Goldbach's conjecture whispers<br>Truth eternal flows</i></td>\n<td><code>-v \"Han (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Lilian (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Hua Luogeng</td>\n<td>数论之光芒<br>解析延续美<br>智慧升华<br><br><i>Number theory shines<br>Analysis extends grace<br>Wisdom ascends pure</i></td>\n<td><code>-v \"Lilian (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Meijia</td>\n<td>Chinese (Taiwan)</td>\n<td>Sun-Yung Alice Chang</td>\n<td>幾何之美現<br>曲率流動不息<br>空間展開<br><br><i>Geometry shows<br>Curvature flows endlessly<br>Space unfolds anew</i></td>\n<td><code>-v \"Meijia\"</code></td>\n</tr>\n<tr>\n<td>Sinji (Premium)</td>\n<td>Chinese (Hong Kong)</td>\n<td>Shing-Tung Yau</td>\n<td>流形之奧秘<br>卡拉比空間動<br>維度交織<br><br><i>Manifolds reveal<br>Calabi spaces in flow<br>Dimensions weave truth</i></td>\n<td><code>-v \"Sinji (Premium)\"</code></td>\n</tr>\n<tr>\n<td>Tingting</td>\n<td>Chinese (Mainland)</td>\n<td>Wang Zhenyi</td>\n<td>星辰轨迹明<br>天文数学融<br>智慧闪耀<br><br><i>Starlit paths shine bright<br>Astronomy meets numbers<br>Wisdom radiates</i></td>\n<td><code>-v \"Tingting\"</code></td>\n</tr>\n<tr>\n<td>Yue (Premium)</td>\n<td>Chinese (Mainland)</td>\n<td>Chern Shiing-shen</td>\n<td>微分几何<br>纤维丛中寻真<br>本质显现<br><br><i>Differential forms<br>In fiber bundles seek truth<br>Essence emerges</i></td>\n<td><code>-v \"Yue (Premium)\"</code></td>\n</tr>\n</table>\n\n## Configuration\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Requirements\n\n- macOS (uses the built-in `say` command)\n- Node.js >= 14.0.0\n\n## Contributors\n\n- Barton Rhodes ([@bmorphism](https://github.com/bmorphism)) - barton@vibes.lol\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "macos",
        "mcp",
        "speech",
        "say command",
        "text speech",
        "speech functionality"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "elevenlabs--elevenlabs-mcp": {
      "owner": "elevenlabs",
      "name": "elevenlabs-mcp",
      "url": "https://github.com/elevenlabs/elevenlabs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/elevenlabs.webp",
      "description": "This server provides APIs for generating speech, voice cloning, and audio transcription. It facilitates seamless interaction with text-to-speech and audio processing functionalities.",
      "stars": 998,
      "forks": 161,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:34:19Z",
      "readme_content": "![export](https://github.com/user-attachments/assets/ee379feb-348d-48e7-899c-134f7f7cd74f)\n\n<div class=\"title-block\" style=\"text-align: center;\" align=\"center\">\n\n  [![Discord Community](https://img.shields.io/badge/discord-@elevenlabs-000000.svg?style=for-the-badge&logo=discord&labelColor=000)](https://discord.gg/elevenlabs)\n  [![Twitter](https://img.shields.io/badge/Twitter-@elevenlabsio-000000.svg?style=for-the-badge&logo=twitter&labelColor=000)](https://x.com/ElevenLabsDevs)\n  [![PyPI](https://img.shields.io/badge/PyPI-elevenlabs--mcp-000000.svg?style=for-the-badge&logo=pypi&labelColor=000)](https://pypi.org/project/elevenlabs-mcp)\n  [![Tests](https://img.shields.io/badge/tests-passing-000000.svg?style=for-the-badge&logo=github&labelColor=000)](https://github.com/elevenlabs/elevenlabs-mcp-server/actions/workflows/test.yml)\n\n</div>\n\n\n<p align=\"center\">\n  Official ElevenLabs <a href=\"https://github.com/modelcontextprotocol\">Model Context Protocol (MCP)</a> server that enables interaction with powerful Text to Speech and audio processing APIs. This server allows MCP clients like <a href=\"https://www.anthropic.com/claude\">Claude Desktop</a>, <a href=\"https://www.cursor.so\">Cursor</a>, <a href=\"https://codeium.com/windsurf\">Windsurf</a>, <a href=\"https://github.com/openai/openai-agents-python\">OpenAI Agents</a> and others to generate speech, clone voices, transcribe audio, and more.\n</p>\n\n<!--\nmcp-name: io.github.elevenlabs/elevenlabs-mcp\n-->\n\n## Quickstart with Claude Desktop\n\n1. Get your API key from [ElevenLabs](https://elevenlabs.io/app/settings/api-keys). There is a free tier with 10k credits per month.\n2. Install `uv` (Python package manager), install with `curl -LsSf https://astral.sh/uv/install.sh | sh` or see the `uv` [repo](https://github.com/astral-sh/uv) for additional install methods.\n3. Go to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```\n{\n  \"mcpServers\": {\n    \"ElevenLabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"<insert-your-api-key-here>\"\n      }\n    }\n  }\n}\n\n```\n\nIf you're using Windows, you will have to enable \"Developer Mode\" in Claude Desktop to use the MCP server. Click \"Help\" in the hamburger menu at the top left and select \"Enable Developer Mode\".\n\n## Other MCP clients\n\nFor other clients like Cursor and Windsurf, run:\n1. `pip install elevenlabs-mcp`\n2. `python -m elevenlabs_mcp --api-key={{PUT_YOUR_API_KEY_HERE}} --print` to get the configuration. Paste it into appropriate configuration directory specified by your MCP client.\n\nThat's it. Your MCP client can now interact with ElevenLabs through these tools:\n\n## Example usage\n\n⚠️ Warning: ElevenLabs credits are needed to use these tools.\n\nTry asking Claude:\n\n- \"Create an AI agent that speaks like a film noir detective and can answer questions about classic movies\"\n- \"Generate three voice variations for a wise, ancient dragon character, then I will choose my favorite voice to add to my voice library\"\n- \"Convert this recording of my voice to sound like a medieval knight\"\n- \"Create a soundscape of a thunderstorm in a dense jungle with animals reacting to the weather\"\n- \"Turn this speech into text, identify different speakers, then convert it back using unique voices for each person\"\n\n## Optional features\n\n### File Output Configuration\n\nYou can configure how the MCP server handles file outputs using these environment variables in your `claude_desktop_config.json`:\n\n- **`ELEVENLABS_MCP_BASE_PATH`**: Specify the base path for file operations with relative paths (default: `~/Desktop`)\n- **`ELEVENLABS_MCP_OUTPUT_MODE`**: Control how generated files are returned (default: `files`)\n\n#### Output Modes\n\nThe `ELEVENLABS_MCP_OUTPUT_MODE` environment variable supports three modes:\n\n1. **`files`** (default): Save files to disk and return file paths\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"files\"\n   }\n   ```\n\n2. **`resources`**: Return files as MCP resources; binary data is base64-encoded, text is returned as UTF-8 text\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"resources\"\n   }\n   ```\n\n3. **`both`**: Save files to disk AND return as MCP resources\n   ```json\n   \"env\": {\n     \"ELEVENLABS_API_KEY\": \"your-api-key\",\n     \"ELEVENLABS_MCP_OUTPUT_MODE\": \"both\"\n   }\n   ```\n\n**Resource Mode Benefits:**\n- Files are returned directly in the MCP response as base64-encoded data\n- No disk I/O required - useful for containerized or serverless environments\n- MCP clients can access file content immediately without file system access\n- In `both` mode, resources can be fetched later using the `elevenlabs://filename` URI pattern\n\n**Use Cases:**\n- `files`: Traditional file-based workflows, local development\n- `resources`: Cloud environments, MCP clients without file system access\n- `both`: Maximum flexibility, caching, and resource sharing scenarios\n\n### Data residency keys\n\nYou can specify the data residency region with the `ELEVENLABS_API_RESIDENCY` environment variable. Defaults to `\"us\"`.\n\n**Note:** Data residency is an enterprise only feature. See [the docs](https://elevenlabs.io/docs/product-guides/administration/data-residency#overview) for more details.\n\n## Contributing\n\nIf you want to contribute or run from source:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/elevenlabs/elevenlabs-mcp\ncd elevenlabs-mcp\n```\n\n2. Create a virtual environment and install dependencies [using uv](https://github.com/astral-sh/uv):\n\n```bash\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n```\n\n3. Copy `.env.example` to `.env` and add your ElevenLabs API key:\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key\n```\n\n4. Run the tests to make sure everything is working:\n\n```bash\n./scripts/test.sh\n# Or with options\n./scripts/test.sh --verbose --fail-fast\n```\n\n5. Install the server in Claude Desktop: `mcp install elevenlabs_mcp/server.py`\n\n6. Debug and test locally with MCP Inspector: `mcp dev elevenlabs_mcp/server.py`\n\n## Troubleshooting\n\nLogs when running with Claude Desktop can be found at:\n\n- **Windows**: `%APPDATA%\\Claude\\logs\\mcp-server-elevenlabs.log`\n- **macOS**: `~/Library/Logs/Claude/mcp-server-elevenlabs.log`\n\n### Timeouts when using certain tools\n\nCertain ElevenLabs API operations, like voice design and audio isolation, can take a long time to resolve. When using the MCP inspector in dev mode, you might get timeout errors despite the tool completing its intended task.\n\nThis shouldn't occur when using a client like Claude.\n\n### MCP ElevenLabs: spawn uvx ENOENT\n\nIf you encounter the error \"MCP ElevenLabs: spawn uvx ENOENT\", confirm its absolute path by running this command in your terminal:\n\n```bash\nwhich uvx\n```\n\nOnce you obtain the absolute path (e.g., `/usr/local/bin/uvx`), update your configuration to use that path (e.g., `\"command\": \"/usr/local/bin/uvx\"`). This ensures that the correct executable is referenced.\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcription",
        "voice",
        "audio",
        "voice cloning",
        "audio transcription",
        "cloning audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "georgi-io--jessica": {
      "owner": "georgi-io",
      "name": "jessica",
      "url": "https://github.com/georgi-io/jessica",
      "imageUrl": "/freedevtools/mcp/pfp/georgi-io.webp",
      "description": "Integrates ElevenLabs Text-to-Speech capabilities for seamless text conversion to speech, offering voice selection and management through a modern interface. Supports real-time communication with a FastAPI backend and a React frontend.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-25T23:18:06Z",
      "readme_content": "# Project Jessica (ElevenLabs TTS MCP)\n\nThis project integrates ElevenLabs Text-to-Speech capabilities with Cursor through the Model Context Protocol (MCP). It consists of a FastAPI backend service and a React frontend application.\n\n## Features\n\n- Text-to-Speech conversion using ElevenLabs API\n- Voice selection and management\n- MCP integration for Cursor\n- Modern React frontend interface\n- WebSocket real-time communication\n- Pre-commit hooks for code quality\n- Automatic code formatting and linting\n\n## Project Structure\n\n```\njessica/\n├── src/\n│   ├── backend/          # FastAPI backend service\n│   └── frontend/         # React frontend application\n├── terraform/            # Infrastructure as Code\n├── tests/               # Test suites\n└── docs/                # Documentation\n```\n\n## Requirements\n\n- Python 3.11+\n- Poetry (for backend dependency management)\n- Node.js 18+ (for frontend)\n- Cursor (for MCP integration)\n\n## Local Development Setup\n\n### Backend Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/georgi-io/jessica.git\ncd jessica\n\n# Create Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install backend dependencies\npoetry install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your ElevenLabs API key\n\n# Install pre-commit hooks\npoetry run pre-commit install\n```\n\n### Frontend Setup\n\n```bash\n# Navigate to frontend directory\ncd src/frontend\n\n# Install dependencies\nnpm install\n```\n\n## Development Servers\n\n### Starting the Backend\n\n```bash\n# Activate virtual environment if not active\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Start the backend\npython -m src.backend\n```\n\nThe backend provides:\n- REST API: http://localhost:9020\n- WebSocket: ws://localhost:9020/ws\n- MCP Server: http://localhost:9020/sse (integrated with the main API server)\n\n### Starting the Frontend\n\n```bash\n# In src/frontend directory\nnpm run dev\n```\n\nFrontend development server:\n- http://localhost:5173\n\n## Environment Configuration\n\n### Backend (.env)\n```env\n# ElevenLabs API\nELEVENLABS_API_KEY=your-api-key\n\n# Server Configuration\nHOST=127.0.0.1\nPORT=9020\n\n# Development Settings\nDEBUG=false\nRELOAD=true\n```\n\n### Frontend (.env)\n```env\nVITE_API_URL=http://localhost:9020\nVITE_WS_URL=ws://localhost:9020/ws\n```\n\n## Code Quality Tools\n\n### Backend\n\n```bash\n# Run all pre-commit hooks\npoetry run pre-commit run --all-files\n\n# Run specific tools\npoetry run ruff check .\npoetry run ruff format .\npoetry run pytest\n```\n\n### Frontend\n\n```bash\n# Lint\nnpm run lint\n\n# Type check\nnpm run type-check\n\n# Test\nnpm run test\n```\n\n## Production Deployment\n\n### AWS ECR and GitHub Actions Setup\n\nTo enable automatic building and pushing of Docker images to Amazon ECR:\n\n1. Apply the Terraform configuration to create the required AWS resources:\n   ```bash\n   cd terraform\n   terraform init\n   terraform apply\n   ```\n\n2. The GitHub Actions workflow will automatically:\n   - Read the necessary configuration from the Terraform state in S3\n   - Build the Docker image on pushes to `main` or `develop` branches\n   - Push the image to ECR with tags for `latest` and the specific commit SHA\n\n3. No additional repository variables needed! The workflow fetches all required configuration from the Terraform state.\n\n### How it Works\n\nThe GitHub Actions workflow is configured to:\n1. Initially assume a predefined IAM role with S3 read permissions\n2. Fetch and extract configuration values from the Terraform state file in S3\n3. Re-authenticate using the actual deployment role from the state file\n4. Build and push the Docker image to the ECR repository defined in the state\n\nThis approach eliminates the need to manually configure GitHub repository variables and ensures that the CI/CD process always uses the current infrastructure configuration.\n\n### Quick Overview\n\n- Frontend: Served from S3 via CloudFront at jessica.georgi.io\n- Backend API: Available at api.georgi.io/jessica\n- WebSocket: Connects to api.georgi.io/jessica/ws\n- Docker Image: Stored in AWS ECR and can be deployed to ECS/EKS\n- Infrastructure: Managed via Terraform in this repository\n\n## MCP Integration with Cursor\n\n1. Start the backend server\n2. In Cursor settings, add new MCP server:\n   - Name: Jessica TTS\n   - Type: SSE\n   - URL: http://localhost:9020/sse\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Issues**\n   - Error: \"Invalid API key\"\n   - Solution: Check `.env` file\n\n2. **Connection Problems**\n   - Error: \"Cannot connect to MCP server\"\n   - Solution: Verify backend is running and ports are correct\n\n3. **Port Conflicts**\n   - Error: \"Address already in use\"\n   - Solution: Change ports in `.env`\n\n4. **WebSocket Connection Failed**\n   - Error: \"WebSocket connection failed\"\n   - Solution: Ensure backend is running and WebSocket URL is correct\n\nFor additional help, please open an issue on GitHub.\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "speech",
        "text",
        "text speech",
        "speech recognition",
        "speech capabilities"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "giannisanni--kokoro-tts-mcp": {
      "owner": "giannisanni",
      "name": "kokoro-tts-mcp",
      "url": "https://github.com/giannisanni/kokoro-tts-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/giannisanni.webp",
      "description": "Integrates text-to-speech capabilities using the Kokoro TTS engine, enabling conversion of written content into spoken audio with customizable voices and adjustable speed. Supports saving audio files and cross-platform playback.",
      "stars": 10,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-28T13:54:19Z",
      "readme_content": "# Kokoro TTS MCP Server\n\nA Model Context Protocol (MCP) server that provides text-to-speech capabilities using the Kokoro TTS engine. This server exposes TTS functionality through MCP tools, making it easy to integrate speech synthesis into your applications.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- `uv` package manager\n\n## Installation\n\n1. First, install the `uv` package manager:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Clone this repository and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\nuv pip install .\n```\n\n## Features\n\n- Text-to-speech synthesis with customizable voices\n- Adjustable speech speed\n- Support for saving audio to files or direct playback\n- Cross-platform audio playback support (Windows, macOS, Linux)\n\n## Usage\n\nThe server provides a single MCP tool `generate_speech` with the following parameters:\n\n- `text` (required): The text to convert to speech\n- `voice` (optional): Voice to use for synthesis (default: \"af_heart\")\n- `speed` (optional): Speech speed multiplier (default: 1.0)\n- `save_path` (optional): Directory to save audio files\n- `play_audio` (optional): Whether to play the audio immediately (default: False)\n\n### Example Usage\n\n```python\nfrom mcp.client import Client\n\nasync with Client() as client:\n    await client.connect(\"kokoro-tts\")\n    \n    # Generate and play speech\n    result = await client.call_tool(\n        \"generate_speech\",\n        {\n            \"text\": \"Hello, world!\",\n            \"voice\": \"af_heart\",\n            \"speed\": 1.0,\n            \"play_audio\": True\n        }\n    )\n```\n\n## Dependencies\n\n- kokoro >= 0.8.4\n- mcp[cli] >= 1.3.0\n- soundfile >= 0.13.1\n\n## Platform Support\n\nAudio playback is supported on:\n- Windows (using `start`)\n- macOS (using `afplay`)\n- Linux (using `aplay`)\n\n## MCP Configuration\n\nAdd the following configuration to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"kokoro-tts\": {\n      \"command\": \"/Users/giannisan/pinokio/bin/miniconda/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/giannisan/Documents/Cline/MCP/kokoro-tts-mcp\",\n        \"run\",\n        \"tts-mcp.py\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\n[Add your license information here]\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "audio",
        "kokoro",
        "tts",
        "kokoro tts",
        "text speech",
        "spoken audio"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "hammeiam--koroko-speech-mcp": {
      "owner": "hammeiam",
      "name": "koroko-speech-mcp",
      "url": "https://github.com/hammeiam/koroko-speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hammeiam.webp",
      "description": "Provides text-to-speech capabilities using the Kokoro TTS model, converting text into natural-sounding speech with customizable options and multiple voice choices.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-22T02:56:06Z",
      "readme_content": "# Speech MCP Server\n\nA Model Context Protocol server that provides text-to-speech capabilities using the Kokoro TTS model.\n\n## Configuration\n\nThe server can be configured using the following environment variables:\n\n| Variable | Description | Default | Valid Range |\n|----------|-------------|---------|-------------|\n| `MCP_DEFAULT_SPEECH_SPEED` | Default speed multiplier for text-to-speech | 1.1 | 0.5 to 2.0 |\n| `MCP_DEFAULT_VOICE` | Default voice for text-to-speech | af_bella | Any valid voice ID |\n\nIn Cursor:\n```\n{\n  \"mcpServers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"speech-mcp-server\"\n      ],\n      \"env\": {\n        \"MCP_DEFAULT_SPEECH_SPEED\": 1.3,\n        \"MCP_DEFAULT_VOICE\": \"af_bella\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- 🎯 High-quality text-to-speech using Kokoro TTS model\n- 🗣️ Multiple voice options available\n- 🎛️ Customizable speech parameters (voice, speed)\n- 🔌 MCP-compliant interface\n- 📦 Easy installation and setup\n- 🚀 No API key required\n\n## Installation\n\n```bash\n# Using npm\nnpm install speech-mcp-server\n\n# Using pnpm (recommended)\npnpm add speech-mcp-server\n\n# Using yarn\nyarn add speech-mcp-server\n```\n\n## Usage\n\nRun the server:\n\n```bash\n# Using default configuration\nnpm start\n\n# With custom configuration\nMCP_DEFAULT_SPEECH_SPEED=1.5 MCP_DEFAULT_VOICE=af_bella npm start\n```\n\nThe server provides the following MCP tools:\n- `text_to_speech`: Basic text-to-speech conversion\n- `text_to_speech_with_options`: Text-to-speech with customizable speed\n- `list_voices`: List all available voices\n- `get_model_status`: Check the initialization status of the TTS model\n\n### Development\n\n```bash\n# Clone the repository\ngit clone <your-repo-url>\ncd speech-mcp-server\n\n# Install dependencies\npnpm install\n\n# Start development server with auto-reload\npnpm dev\n\n# Build the project\npnpm build\n\n# Run linting\npnpm lint\n\n# Format code\npnpm format\n\n# Test with MCP Inspector\npnpm inspector\n```\n\n## Available Tools\n\n### 1. text_to_speech\nConverts text to speech using the default settings.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\"  // optional\n    }\n  }\n}\n```\n\n### 2. text_to_speech_with_options\nConverts text to speech with customizable parameters.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"text_to_speech_with_options\",\n    \"arguments\": {\n      \"text\": \"Hello world\",\n      \"voice\": \"af_bella\",  // optional\n      \"speed\": 1.0,         // optional (0.5 to 2.0)\n    }\n  }\n}\n```\n\n### 3. list_voices\nLists all available voices for text-to-speech.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"list_voices\",\n  \"params\": {}\n}\n```\n\n### 4. get_model_status\nCheck the current status of the TTS model initialization. This is particularly useful when first starting the server, as the model needs to be downloaded and initialized.\n\n```json\n{\n  \"type\": \"request\",\n  \"id\": \"1\",\n  \"method\": \"call_tool\",\n  \"params\": {\n    \"name\": \"get_model_status\",\n    \"arguments\": {}\n  }\n}\n```\n\nResponse example:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed)\"\n  }]\n}\n```\n\nPossible status values:\n- `uninitialized`: Model initialization hasn't started\n- `initializing`: Model is being downloaded and initialized\n- `ready`: Model is ready to use\n- `error`: An error occurred during initialization\n\n## Testing\n\nYou can test the server using the MCP Inspector or by sending raw JSON messages:\n\n```bash\n# List available tools\necho '{\"type\":\"request\",\"id\":\"1\",\"method\":\"list_tools\",\"params\":{}}' | node dist/index.js\n\n# List available voices\necho '{\"type\":\"request\",\"id\":\"2\",\"method\":\"list_voices\",\"params\":{}}' | node dist/index.js\n\n# Convert text to speech\necho '{\"type\":\"request\",\"id\":\"3\",\"method\":\"call_tool\",\"params\":{\"name\":\"text_to_speech\",\"arguments\":{\"text\":\"Hello world\",\"voice\":\"af_bella\"}}}' | node dist/index.js\n```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"servers\": {\n    \"speech\": {\n      \"command\": \"npx\",\n      \"args\": [\"@decodershq/speech-mcp-server\"]\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Troubleshooting\n\n### Model Initialization Issues\n\nThe server automatically attempts to download and initialize the TTS model on startup. If you encounter initialization errors:\n\n1. The server will automatically retry up to 3 times with a cleanup between attempts\n2. Use the `get_model_status` tool to monitor initialization progress and any errors\n3. If initialization fails after all retries, try manually removing the model files:\n\n```bash\n# Remove model files (MacOS/Linux)\nrm -rf ~/.npm/_npx/**/node_modules/@huggingface/transformers/.cache/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\nrm -rf ~/.cache/huggingface/transformers/onnx-community/Kokoro-82M-v1.0-ONNX/onnx/model_quantized.onnx\n\n# Then restart the server\nnpm start\n```\n\nThe `get_model_status` tool will now include retry information in its response:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Model status: initializing (5s elapsed, retry 1/3)\"\n  }]\n}\n``` ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "koroko",
        "kokoro",
        "voice",
        "koroko speech",
        "speech customizable",
        "text speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "kentaro--aivis-speech-mcp": {
      "owner": "kentaro",
      "name": "aivis-speech-mcp",
      "url": "https://github.com/kentaro/aivis-speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kentaro.webp",
      "description": "Integrate with the AivisSpeech Engine to provide high-quality speech synthesis capabilities for applications, facilitating the conversion of text to natural-sounding speech. The server offers a type-safe API compliant with the Model Context Protocol, ensuring easy configuration and extensibility.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-15T17:05:34Z",
      "readme_content": "# AivisSpeech MCP サーバー\n\nAivisSpeech用のModel Context Protocol (MCP) サーバーの実装です。このサーバーは、AivisSpeech Engineと連携して、音声合成のためのインターフェースを提供します。MCPプロトコルを通じて、AIアシスタントなどのアプリケーションからAivisSpeechの音声合成機能を簡単に利用できるようになります。\n\n## 概要\n\nAivisSpeech MCP サーバーは以下の機能を提供します：\n\n- MCPプロトコルに準拠したAPIエンドポイント\n- AivisSpeech Engineとの連携による高品質な音声合成\n- TypeScriptによる型安全な設計\n- 簡単な設定と拡張性の高いアーキテクチャ\n\n## 必要条件\n\n- Node.js 18.x以上\n- npm 9.x以上\n- AivisSpeech Engine（別途インストールが必要）\n\n## インストール\n\n```bash\n# リポジトリをクローン\ngit clone https://github.com/kentaro/aivis-speech-mcp.git\ncd aivis-speech-mcp\n\n# 依存関係のインストール\nnpm install\n\n# ビルド\nnpm run build\n\n# 環境変数の設定\ncp .env.sample .env\n# .envファイルを編集して、必要な設定を行ってください\n\n# Cursor MCPの設定\ncp .cursor/mcp.json.sample .cursor/mcp.json\n# mcp.jsonファイル内の\"/path/to/aivis-speech-mcp/dist/index.js\"を\n# 実際のプロジェクトパスに書き換えてください\n# 例: \"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"\n```\n\n## 環境設定\n\n`.env`ファイルで以下の設定を行います：\n\n```\n# AivisSpeech API Configuration\nAIVIS_SPEECH_API_URL=http://localhost:10101  # AivisSpeech EngineのAPIエンドポイント\n\n# Speaker Configuration\nAIVIS_SPEECH_SPEAKER_ID=888753760  # デフォルトのスピーカーID\n```\n\n## Cursor MCP設定\n\n`.cursor/mcp.json`ファイルで以下の設定を行います：\n\n```json\n{\n  \"mcpServers\": {\n    \"AivisSpeech-MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/aivis-speech-mcp/dist/index.js\"]\n    }\n  }\n}\n```\n\n`/path/to/aivis-speech-mcp/dist/index.js`を、実際のプロジェクトのパスに書き換えてください。\nWindowsの場合は、バックスラッシュをエスケープするか、フォワードスラッシュを使用してください。\n例: `\"C:/Users/username/path/to/aivis-speech-mcp/dist/index.js\"`\n\n## 使い方\n\n### 開発モード\n\n開発中は以下のコマンドでホットリロード機能付きでサーバーを起動できます：\n\n```bash\nnpm run dev\n```\n\n### ビルド\n\n本番環境用にビルドする場合は以下のコマンドを実行します：\n\n```bash\nnpm run build\n```\n\n### 本番モード\n\nビルド後、以下のコマンドで本番モードでサーバーを起動します：\n\n```bash\nnpm start\n```\n\n### テスト\n\nテストを実行するには以下のコマンドを使用します：\n\n```bash\nnpm test\n```\n\n## アーキテクチャ\n\nAivisSpeech MCP サーバーは以下のコンポーネントで構成されています：\n\n- **MCPサービス**: Model Context Protocolに準拠したサーバーを提供し、クライアントからのリクエストを処理します\n- **AivisSpeech サービス**: AivisSpeech EngineのAPIと通信し、音声合成を実行します\n\n## API仕様\n\nMCPプロトコルに準拠したAPIエンドポイントを提供します。主な機能は以下の通りです：\n\n- 音声合成（テキストから音声を生成）\n- スピーカー情報の取得\n- 音声スタイルの設定\n\n詳細なAPI仕様については[AivisSpeech Engine API仕様](https://aivis-project.github.io/AivisSpeech-Engine/api/)を参照してください。\n\n## MCPプロトコルとの連携\n\nこのサーバーは、Model Context Protocol（MCP）を実装しており、AIアシスタントなどのアプリケーションからシームレスに利用できます。MCPプロトコルについての詳細は[MCP公式ドキュメント](https://modelcontextprotocol.github.io/)を参照してください。\n\n## トラブルシューティング\n\nよくある問題と解決策：\n\n- **AivisSpeech Engineに接続できない**: `.env`ファイルの`AIVIS_SPEECH_API_URL`が正しく設定されているか確認してください\n- **音声が再生されない**: システムの音声設定を確認し、適切なオーディオデバイスが選択されているか確認してください\n- **スピーカーIDが見つからない**: AivisSpeech Engineが正しく起動しているか確認し、利用可能なスピーカーIDを確認してください\n\n## 貢献\n\nバグ報告や機能リクエストは、GitHubのIssueトラッカーを通じてお願いします。プルリクエストも歓迎します。\n\n## ライセンス\n\n[MIT](LICENSE)\n\n## 謝辞\n\n- [AivisSpeech Engine](https://github.com/aivis-project/AivisSpeech-Engine)チーム\n- [Model Context Protocol](https://modelcontextprotocol.github.io/)の開発者\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "aivisspeech",
        "synthesis",
        "aivis",
        "speech synthesis",
        "speech server",
        "aivis speech"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "mamertofabian--elevenlabs-mcp-server": {
      "owner": "mamertofabian",
      "name": "elevenlabs-mcp-server",
      "url": "https://github.com/mamertofabian/elevenlabs-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mamertofabian.webp",
      "description": "Integrates with ElevenLabs text-to-speech API to generate audio from text input, manage voice generation tasks, and store history using an SQLite database. Includes a sample SvelteKit client for performing text-to-speech conversions and managing script parts.",
      "stars": 112,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# ElevenLabs MCP Server\n[![smithery badge](https://smithery.ai/badge/elevenlabs-mcp-server)](https://smithery.ai/server/elevenlabs-mcp-server)\n\nA Model Context Protocol (MCP) server that integrates with ElevenLabs text-to-speech API, featuring both a server component and a sample web-based MCP Client (SvelteKit) for managing voice generation tasks.\n\n<a href=\"https://glama.ai/mcp/servers/leukzvus7o\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/leukzvus7o/badge\" alt=\"ElevenLabs Server MCP server\" /></a>\n\n## Features\n\n- Generate audio from text using ElevenLabs API\n- Support for multiple voices and script parts\n- SQLite database for persistent history storage\n- Sample SvelteKit MCP Client for:\n  - Simple text-to-speech conversion\n  - Multi-part script management\n  - Voice history tracking and playback\n  - Audio file downloads\n\n## Installation\n\n### Installing via Smithery\n\nTo install ElevenLabs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/elevenlabs-mcp-server):\n\n```bash\nnpx -y @smithery/cli install elevenlabs-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `cline_mcp_settings.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"elevenlabs-mcp-server\"],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   uv venv\n   ```\n3. Copy `.env.example` to `.env` and fill in your ElevenLabs credentials\n\n```json\n{\n  \"mcpServers\": {\n    \"elevenlabs\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/elevenlabs-mcp-server\",\n        \"run\",\n        \"elevenlabs-mcp-server\"\n      ],\n      \"env\": {\n        \"ELEVENLABS_API_KEY\": \"your-api-key\",\n        \"ELEVENLABS_VOICE_ID\": \"your-voice-id\",\n        \"ELEVENLABS_MODEL_ID\": \"eleven_flash_v2\",\n        \"ELEVENLABS_STABILITY\": \"0.5\",\n        \"ELEVENLABS_SIMILARITY_BOOST\": \"0.75\",\n        \"ELEVENLABS_STYLE\": \"0.1\",\n        \"ELEVENLABS_OUTPUT_DIR\": \"output\"\n      }\n    }\n  }\n}\n```\n\n## Using the Sample SvelteKit MCP Client\n\n1. Navigate to the web UI directory:\n   ```bash\n   cd clients/web-ui\n   ```\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n3. Copy `.env.example` to `.env` and configure as needed\n4. Run the web UI:\n   ```bash\n   pnpm dev\n   ```\n5. Open http://localhost:5174 in your browser\n\n### Available Tools\n\n- `generate_audio_simple`: Generate audio from plain text using default voice settings\n- `generate_audio_script`: Generate audio from a structured script with multiple voices and actors\n- `delete_job`: Delete a job by its ID\n- `get_audio_file`: Get the audio file by its ID\n- `list_voices`: List all available voices\n- `get_voiceover_history`: Get voiceover job history. Optionally specify a job ID for a specific job.\n\n### Available Resources\n\n- `voiceover://history/{job_id}`: Get the audio file by its ID\n- `voiceover://voices`: List all available voices\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mamertofabian",
        "voice",
        "audio",
        "mamertofabian elevenlabs",
        "speech api",
        "speech conversions"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nakamurau1--tts-mcp": {
      "owner": "nakamurau1",
      "name": "tts-mcp",
      "url": "https://github.com/nakamurau1/tts-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nakamurau1.webp",
      "description": "Integrates high-quality text-to-speech capabilities into applications, converting text to audio with customizable voice options and output formats. Provides a command-line tool for quick conversions and supports various parameters for audio customization.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T10:32:51Z",
      "readme_content": "# tts-mcp\n\nA Model Context Protocol (MCP) server and command-line tool for high-quality text-to-speech generation using the OpenAI TTS API.\n\n## Main Features\n\n- **MCP Server**: Integrate text-to-speech capabilities with Claude Desktop and other MCP-compatible clients\n- **Voice Options**: Support for multiple voice characters (alloy, nova, echo, etc.)\n- **High-Quality Audio**: Support for various output formats (MP3, WAV, OPUS, AAC)\n- **Customizable**: Configure speech speed, voice character, and additional instructions\n- **CLI Tool**: Also available as a command-line utility for direct text-to-speech conversion\n\n## Installation\n\n### Method 1: Install from Repository\n\n```bash\n# Clone the repository\ngit clone https://github.com/nakamurau1/tts-mcp.git\ncd tts-mcp\n\n# Install dependencies\nnpm install\n\n# Optional: Install globally\nnpm install -g .\n```\n\n### Method 2: Run Directly with npx (No Installation Required)\n\n```bash\n# Start the MCP server directly\nnpx tts-mcp tts-mcp-server --voice nova --model tts-1-hd\n\n# Use the CLI tool directly\nnpx tts-mcp -t \"Hello, world\" -o hello.mp3\n```\n\n## MCP Server Usage\n\nThe MCP server allows you to integrate text-to-speech functionality with Model Context Protocol (MCP) compatible clients like Claude Desktop.\n\n### Starting the MCP Server\n\n```bash\n# Start with default settings\nnpm run server\n\n# Start with custom settings\nnpm run server -- --voice nova --model tts-1-hd\n\n# Or directly with API key\nnode bin/tts-mcp-server.js --voice echo --api-key your-openai-api-key\n```\n\n### MCP Server Options\n\n```\nOptions:\n  -V, --version       Display version information\n  -m, --model <model> TTS model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <voice> Voice character (default: \"alloy\")\n  -f, --format <format> Audio format (default: \"mp3\")\n  --api-key <key>     OpenAI API key (can also be set via environment variable)\n  -h, --help          Display help information\n```\n\n### Integrating with MCP Clients\n\nThe MCP server can be used with Claude Desktop and other MCP-compatible clients. For Claude Desktop integration:\n\n1. Open the Claude Desktop configuration file (typically at `~/Library/Application Support/Claude/claude_desktop_config.json`)\n2. Add the following configuration, including your OpenAI API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"full/path/to/bin/tts-mcp-server.js\", \"--voice\", \"nova\", \"--api-key\", \"your-openai-api-key\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use npx for easier setup:\n\n```json\n{\n  \"mcpServers\": {\n    \"tts-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-p\", \"tts-mcp\", \"tts-mcp-server\", \"--voice\", \"nova\", \"--model\", \"gpt-4o-mini-tts\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\nYou can provide the API key in two ways:\n\n1. **Direct method** (recommended for testing): Include it in the `args` array using the `--api-key` parameter\n2. **Environment variable method** (more secure): Set it in the `env` object as shown above\n\n> **Security Note**: Make sure to secure your configuration file when including API keys.\n\n3. Restart Claude Desktop\n4. When you ask Claude to \"read this text aloud\" or similar requests, the text will be converted to speech\n\n### Available MCP Tools\n\n- **text-to-speech**: Tool for converting text to speech and playing it\n\n## CLI Tool Usage\n\nYou can also use tts-mcp as a standalone command-line tool:\n\n```bash\n# Convert text directly\ntts-mcp -t \"Hello, world\" -o hello.mp3\n\n# Convert from a text file\ntts-mcp -f speech.txt -o speech.mp3\n\n# Specify custom voice\ntts-mcp -t \"Welcome to the future\" -o welcome.mp3 -v nova\n```\n\n### CLI Tool Options\n\n```\nOptions:\n  -V, --version           Display version information\n  -t, --text <text>       Text to convert\n  -f, --file <path>       Path to input text file\n  -o, --output <path>     Path to output audio file (required)\n  -m, --model <n>         Model to use (default: \"gpt-4o-mini-tts\")\n  -v, --voice <n>         Voice character (default: \"alloy\")\n  -s, --speed <number>    Speech speed (0.25-4.0) (default: 1)\n  --format <format>       Output format (default: \"mp3\")\n  -i, --instructions <text> Additional instructions for speech generation\n  --api-key <key>         OpenAI API key (can also be set via environment variable)\n  -h, --help              Display help information\n```\n\n## Supported Voices\n\nThe following voice characters are supported:\n- alloy (default)\n- ash\n- coral\n- echo\n- fable\n- onyx\n- nova\n- sage\n- shimmer\n\n## Supported Models\n\n- tts-1\n- tts-1-hd\n- gpt-4o-mini-tts (default)\n\n## Output Formats\n\nThe following output formats are supported:\n- mp3 (default)\n- opus\n- aac\n- flac\n- wav\n- pcm\n\n## Environment Variables\n\nYou can also configure the tool using system environment variables:\n\n```\nOPENAI_API_KEY=your-api-key-here\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tts",
        "voice",
        "audio",
        "tts mcp",
        "text audio",
        "nakamurau1 tts"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "/freedevtools/mcp/pfp/nansasuke.webp",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\n图片识别、语音识别、垃圾分类\n\n一个完整的垃圾分类的app\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recycling",
        "waste",
        "garbagesorting",
        "classify waste",
        "waste using",
        "nansasuke garbagesorting"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "neosapience--typecast-api-mcp-server-sample": {
      "owner": "neosapience",
      "name": "typecast-api-mcp-server-sample",
      "url": "https://github.com/neosapience/typecast-api-mcp-server-sample",
      "imageUrl": "/freedevtools/mcp/pfp/neosapience.webp",
      "description": "Integrates with the Typecast API to manage voices, convert text to speech, and play audio. Provides a standardized MCP interface for seamless interaction with voice capabilities.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T23:50:45Z",
      "readme_content": "# typecast-api-mcp-server-sample\n\nMCP Server for typecast-api, enabling seamless integration with MCP clients. This project provides a standardized way to interact with Typecast API through the Model Context Protocol.\n\n## About\n\nThis project implements a Model [Context Protocol server](https://modelcontextprotocol.io/introduction) for Typecast API, allowing MCP clients to interact with the Typecast API in a standardized way.\n\n## Feature Implementation Status\n\n| Feature              | Status |\n| -------------------- | ------ |\n| **Voice Management** |        |\n| Get Voices           | ✅     |\n| Text to Speech       | ✅     |\n| Play Audio           | ✅     |\n\n## Setup\n\n### Git Clone\n\n```bash\ngit clone https://github.com/hyunseung/typecast-api-mcp-server-sample.git\ncd typecast-api-mcp-server-sample\n```\n\n### Dependencies\n\nThis project requires Python 3.10 or higher and uses `uv` for package management.\n\n#### Package Installation\n\n```bash\n# Create virtual environment and install packages\nuv venv\nuv pip install -e .\n```\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nTYPECAST_API_HOST=https://api.typecast.ai\nTYPECAST_API_KEY=<your-api-key>\nTYPECAST_OUTPUT_DIR=<your-output-directory> # default: ~/Downloads/typecast_output\n```\n\n### Usage with Claude Desktop\n\nYou can add the following to your `claude_desktop_config.json`:\n\n#### Basic Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"typecast-api-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/PROJECT\",\n        \"run\",\n        \"typecast-api-mcp-server\"\n      ],\n      \"env\": {\n        \"TYPECAST_API_HOST\": \"https://api.typecast.ai\",\n        \"TYPECAST_API_KEY\": \"YOUR_API_KEY\",\n        \"TYPECAST_OUTPUT_DIR\": \"PATH/TO/YOUR/OUTPUT/DIR\"\n      }\n    }\n  }\n}\n```\n\nReplace `/PATH/TO/YOUR/PROJECT` with the actual path where your project is located.\n\n### Manual Execution\n\nYou can also run the server manually:\n\n```bash\nuv run python app/main.py\n```\n\n## Contributing\n\nContributions are always welcome! Feel free to submit a Pull Request.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "typecast",
        "voice",
        "voices",
        "typecast api",
        "mcp interface",
        "api mcp"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "uraoz--bouyomichan-mcp-nodejs": {
      "owner": "uraoz",
      "name": "bouyomichan-mcp-nodejs",
      "url": "https://github.com/uraoz/bouyomichan-mcp-nodejs",
      "imageUrl": "/freedevtools/mcp/pfp/uraoz.webp",
      "description": "Provides text-to-speech capabilities using BouyomiChan's Yukkuri voice, enabling voice output from text commands with customizable options for voice type, volume, speed, and pitch. Integrates seamlessly with Claude for Desktop for enhanced user interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T11:48:46Z",
      "readme_content": "# 棒読みちゃんMCPサーバー (Node.js版)\n\n<a href=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs/badge\" alt=\"Bouyomi-chan Server MCP server\" />\n</a>\n\n\n## 前提条件\n\n- Node.js 16以上\n- npm 7以上\n- 棒読みちゃんがインストールされていること\n- 棒読みちゃんのHTTP連携がポート50080で起動していること\n\n## 使用方法\n\n### ローカルでのサーバーの起動\n\n```bash\ngit clone https://github.com/uraoz/bouyomichan-mcp-nodejs.git\ncd bouyomichan-mcp-nodejs\nnpm install\nnpm run build\nnpm start\n```\n\n### Claude for Desktopとの連携\n\n```json\n{\n  \"mcpServers\": {\n    \"bouyomichan\":{\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:uraoz/bouyomichan-mcp-nodejs\"\n      ]\n    }\n  }\n}\n```\n\n## パラメータ説明\n\n| パラメータ | 説明 | デフォルト値 | 有効範囲 |\n|----------|------|------------|---------|\n| text     | 読み上げるテキスト | 必須 | 任意のテキスト |\n| voice    | 音声の種類 | 0 (女性1) | 0: 女性1、1: 男性1、2: 女性2、... |\n| volume   | 音量 | -1 (デフォルト) | -1: デフォルト、0-100: 音量レベル |\n| speed    | 速度 | -1 (デフォルト) | -1: デフォルト、50-200: 速度レベル |\n| tone     | 音程 | -1 (デフォルト) | -1: デフォルト、50-200: 音程レベル |\n\n## ライセンス\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bouyomichan",
        "voice",
        "nodejs",
        "yukkuri voice",
        "bouyomichan yukkuri",
        "uraoz bouyomichan"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yangsenessa--mcp_voice_identify": {
      "owner": "yangsenessa",
      "name": "mcp_voice_identify",
      "url": "https://github.com/yangsenessa/mcp_voice_identify",
      "imageUrl": "/freedevtools/mcp/pfp/yangsenessa.webp",
      "description": "Provides voice recognition and text extraction capabilities, supporting both file input and base64 encoded data processed in structured formats. Operates in stdio and MCP modes for flexible integration with various systems.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-17T11:21:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yangsenessa-mcp-voice-identify-badge.png)](https://mseep.ai/app/yangsenessa-mcp-voice-identify)\n\n# Voice Recognition MCP Service\n\nThis service provides voice recognition and text extraction capabilities through both stdio and MCP modes.\n\n## Features\n\n- Voice recognition from file\n- Voice recognition from base64 encoded data\n- Text extraction\n- Support for both stdio and MCP modes\n- Structured voice recognition results\n- AIO protocol compliant responses\n\n## Project Structure\n\n- `voice_service.py` - Core service implementation\n- `stdio_server.py` - stdio mode entry point\n- `mcp_server.py` - MCP mode entry point\n- `build.py` - Build script for executables\n- `build_exec.sh` - Build execution script\n- `test_*.sh` - Test scripts for different functionalities\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/AIO-2030/mcp_voice_identify.git\ncd mcp_voice_identify\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up environment variables in `.env`:\n```\nAPI_URL=your_api_url\nAPI_KEY=your_api_key\n```\n\n## Usage\n\n### stdio Mode\n\n1. Run the service:\n```bash\npython stdio_server.py\n```\n\n2. Send JSON-RPC requests via stdin:\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"help\",\n    \"params\": {},\n    \"id\": 1\n}\n```\n\n3. Or use the executable:\n```bash\n./dist/voice_stdio\n```\n\n### MCP Mode\n\n1. Run the service:\n```bash\npython mcp_server.py\n```\n\n2. Or use the executable:\n```bash\n./dist/voice_mcp\n```\n\n## Response Format\n\nThe service follows the AIO protocol for response formatting. Here are examples of different response types:\n\n### Voice Recognition Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"voice\",\n        \"message\": \"Voice processed successfully\",\n        \"text\": \"test test test\",\n        \"metadata\": {\n            \"language\": \"en\",\n            \"emotion\": \"unknown\",\n            \"audio_type\": \"speech\",\n            \"speaker\": \"woitn\",\n            \"raw_text\": \"test test test\"\n        }\n    },\n    \"id\": 1\n}\n```\n\n### Help Information Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"result\": {\n        \"type\": \"voice_service\",\n        \"description\": \"This service provides voice recognition and text extraction services\",\n        \"author\": \"AIO-2030\",\n        \"version\": \"1.0.0\",\n        \"github\": \"https://github.com/AIO-2030/mcp_voice_identify\",\n        \"transport\": [\"stdio\"],\n        \"methods\": [\n            {\n                \"name\": \"help\",\n                \"description\": \"Show this help information.\"\n            },\n            {\n                \"name\": \"identify_voice\",\n                \"description\": \"Identify voice from file\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"Voice file path\"\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                }\n            },\n            {\n                \"name\": \"identify_voice_base64\",\n                \"description\": \"Identify voice from base64 encoded data\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"base64_data\": {\n                            \"type\": \"string\",\n                            \"description\": \"Base64 encoded voice data\"\n                        }\n                    },\n                    \"required\": [\"base64_data\"]\n                }\n            },\n            {\n                \"name\": \"extract_text\",\n                \"description\": \"Extract text\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"text\": {\n                            \"type\": \"string\",\n                            \"description\": \"Text to extract\"\n                        }\n                    },\n                    \"required\": [\"text\"]\n                }\n            }\n        ]\n    },\n    \"id\": 1\n}\n```\n\n### Error Response\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"output\": {\n        \"type\": \"error\",\n        \"message\": \"503 Server Error: Service Unavailable\",\n        \"error_code\": 503\n    },\n    \"id\": 1\n}\n```\n\n### Response Fields\n\nThe service provides three types of responses:\n\n1. Voice Recognition Response (using `output` field):\n| Field     | Description                          | Example Value |\n|-----------|--------------------------------------|---------------|\n| type      | Response type                        | \"voice\"       |\n| message   | Status message                       | \"Voice processed successfully\" |\n| text      | Recognized text content              | \"test test test\" |\n| metadata  | Additional information               | See below     |\n\n2. Help Information Response (using `result` field):\n| Field         | Description                          | Example Value |\n|---------------|--------------------------------------|---------------|\n| type          | Service type                         | \"voice_service\" |\n| description   | Service description                  | \"This service provides...\" |\n| author        | Service author                       | \"AIO-2030\"    |\n| version       | Service version                      | \"1.0.0\"       |\n| github        | GitHub repository URL                | \"https://github.com/...\" |\n| transport     | Supported transport modes            | [\"stdio\"]     |\n| methods       | Available methods                    | See methods list |\n\n3. Error Response (using `output` field):\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| type        | Response type                        | \"error\"       |\n| message     | Error message                        | \"503 Server Error: Service Unavailable\" |\n| error_code  | HTTP status code                     | 503          |\n\n### Metadata Fields\n\nThe `metadata` field in voice recognition responses contains:\n\n| Field       | Description                          | Example Value |\n|-------------|--------------------------------------|---------------|\n| language    | Language code                        | \"en\"          |\n| emotion     | Emotion state                        | \"unknown\"     |\n| audio_type  | Audio type                          | \"speech\"      |\n| speaker     | Speaker identifier                   | \"woitn\"       |\n| raw_text    | Original recognized text             | \"test test test\" |\n\n## Building Executables\n\n1. Make the build script executable:\n```bash\nchmod +x build_exec.sh\n```\n\n2. Build stdio mode executable:\n```bash\n./build_exec.sh\n```\n\n3. Build MCP mode executable:\n```bash\n./build_exec.sh mcp\n```\n\nThe executables will be created at:\n- stdio mode: `dist/voice_stdio`\n- MCP mode: `dist/voice_mcp`\n\n## Testing\n\nRun the test scripts:\n\n```bash\nchmod +x test_*.sh\n./test_help.sh\n./test_voice_file.sh\n./test_voice_base64.sh\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_voice_identify",
        "voice",
        "encoded",
        "mcp_voice_identify provides",
        "yangsenessa mcp_voice_identify",
        "voice recognition"
      ],
      "category": "speech-recognition-and-synthesis"
    },
    "yuiseki--edge_tts_mcp_server": {
      "owner": "yuiseki",
      "name": "edge_tts_mcp_server",
      "url": "https://github.com/yuiseki/edge_tts_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/yuiseki.webp",
      "description": "Provide natural text-to-speech conversion using Microsoft Edge's speech synthesis capabilities, enabling customizable voice output in multiple languages with adjustable speed and pitch.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T03:20:28Z",
      "readme_content": "# Edge-TTS MCP Server\n\nModel Context Protocol (MCP) サーバーで、Microsoft Edge のテキスト読み上げ機能を活用した AI エージェントの音声合成サービスを提供します。\n\n## 概要\n\nこの MCP サーバーは、[edge-tts](https://github.com/rany2/edge-tts)ライブラリを使用して、テキストから音声への変換機能を提供します。AI エージェントが自然な音声で応答できるようにするためのツールとして設計されています。\n\n## 機能\n\n- テキストから音声への変換\n- 複数の音声と言語のサポート\n- 音声速度と音程の調整\n- 音声データのストリーミング\n\n## インストール\n\n```bash\npip install \"edge_tts_mcp_server\"\n```\n\nまたは開発モードでインストールする場合：\n\n```bash\ngit clone https://github.com/yuiseki/edge_tts_mcp_server.git\ncd edge_tts_mcp_server\npip install -e .\n```\n\n## 使用方法\n\n### VS Code での設定例\n\nVS Code の settings.json で設定する例：\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"edge-tts\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\Users\\\\__username__\\\\src\\\\edge_tts_mcp_server\\\\src\\\\edge_tts_mcp_server\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n### MCP Inspector での使用\n\n標準的な MCP サーバーとして実行：\n\n```bash\nmcp dev server.py\n```\n\n### uvx（uvicorn）での実行\n\nFastAPI ベースのサーバーとして uv で実行する場合：\n\n```bash\nuv --directory path/to/edge_tts_mcp_server/src/edge_tts_mcp_server run server.py\n```\n\nコマンドラインオプション：\n\n```bash\nedge-tts-mcp --host 0.0.0.0 --port 8080 --reload\n```\n\n## API エンドポイント\n\nFastAPI モードで実行した場合、以下のエンドポイントが利用可能です：\n\n- `/` - API 情報\n- `/health` - ヘルスチェック\n- `/voices` - 利用可能な音声一覧（オプションで `?locale=ja-JP` などでフィルタリング可能）\n- `/mcp` - MCP API エンドポイント\n\n## ライセンス\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "edge_tts_mcp_server",
        "voice",
        "edge",
        "yuiseki edge_tts_mcp_server",
        "speech conversion",
        "speech synthesis"
      ],
      "category": "speech-recognition-and-synthesis"
    }
  }
}