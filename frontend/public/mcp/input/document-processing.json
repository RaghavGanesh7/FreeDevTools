{
  "category": "document-processing",
  "categoryDisplay": "Document Processing",
  "description": "",
  "totalRepositories": 163,
  "repositories": {
    "2b3pro--markdown2pdf-mcp": {
      "owner": "2b3pro",
      "name": "markdown2pdf-mcp",
      "url": "https://github.com/2b3pro/markdown2pdf-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/2b3pro.webp",
      "description": "This server converts Markdown documents into PDF files, allowing for customizable styles and syntax highlighting for code blocks. It provides an easy way to generate well-formatted PDFs from text-based Markdown content.",
      "stars": 14,
      "forks": 12,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:04Z",
      "readme_content": "![](markdown-to-pdf-header.png)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/2b3pro-markdown2pdf-mcp-badge.png)](https://mseep.ai/app/2b3pro-markdown2pdf-mcp)\n\n# Markdown2PDF MCP Server (markdown2pdf-mcp)\n\nAn MCP server for converting Markdown documents to PDF files. This server provides a simple and efficient way to generate PDFs from Markdown content with support for syntax highlighting and custom styling. Also allows for watermarking on page 1.\n\nInspired by Alan Shaw's [markdown-pdf](https://github.com/alanshaw/markdown-pdf).\n\n## Features\n\n- Convert Markdown to PDF with a single command\n- Syntax highlighting for code blocks\n- Custom CSS styling for PDF output\n- Support for standard Markdown formatting\n- Mermaid diagram rendering\n- Modern PDF generation using Chrome's rendering engine\n- Excellent support for modern web features and fonts\n- Reliable resource loading and rendering\n\n## Limitations\n\nThe following markdown elements are not supported:\n\n- LaTeX math equations (e.g., `$x^2$` or `$$\\sum_{i=1}^n x_i$$`)\n- Complex mathematical formulas or scientific notation\n\nStick to these supported markdown elements:\n\n- Headers (all levels)\n- Text formatting (bold, italic, strikethrough)\n- Lists (ordered and unordered)\n- Code blocks with syntax highlighting\n- Tables\n- Blockquotes\n- Links\n- Images (both local files and external URLs)\n- Task lists\n- Mermaid diagrams\n\n### Mermaid Diagrams\n\nTo render a Mermaid diagram, use a `mermaid` code block:\n\n´´´markdown\n\n```mermaid\ngraph TD;\n    A-->B;\n    A-->C;\n    B-->D;\n    C-->D;\n```\n\n´´´\n\nIf there is a syntax error in your diagram, the error message will be rendered in the PDF, helping you to debug it.\n\n## Installation (from source)\n\n```bash\n# Clone the repository\ngit clone https://github.com/2b3pro/markdown2pdf-mcp.git\n\n# Navigate to the project directory\ncd markdown2pdf-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Installation (via npm)\n\n```bash\nnpm install markdown2pdf-mcp\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm start\n```\n\n### Using the MCP Tool\n\nThe server provides a single tool `create_pdf_from_markdown` with the following parameters:\n\n```typescript\n{\n  // Required parameters\n  markdown: string;    // Markdown content to convert\n\n  // Optional parameters with defaults\n  outputFilename?: string;  // Filename for the PDF (e.g., \"output.pdf\")\n  paperFormat?: string;     // 'letter' (default), 'a4', 'a3', 'a5', 'legal', 'tabloid'\n  paperOrientation?: string; // 'portrait' (default), 'landscape'\n  paperBorder?: string;     // '2cm' (default), accepts decimal values with CSS units (e.g., '1.5cm', '2.5mm', '0.5in', '10.5px')\n  watermark?: string;       // Optional watermark text (max 15 characters, uppercase)\n}\n```\n\nExample with options:\n\n```typescript\nawait use_mcp_tool({\n  server_name: \"markdown2pdf\",\n  tool_name: \"create_pdf_from_markdown\",\n  arguments: {\n    markdown: \"# Hello World\\n\\nThis is a test document.\",\n    outputFilename: \"output.pdf\",\n    paperFormat: \"a4\",\n    paperOrientation: \"landscape\",\n    paperBorder: \"1.5cm\",\n    watermark: \"DRAFT\",\n  },\n});\n```\n\nExample minimal usage:\n\n```typescript\nawait use_mcp_tool({\n  server_name: \"markdown2pdf\",\n  tool_name: \"create_pdf_from_markdown\",\n  arguments: {\n    markdown: \"# Hello World\\n\\nThis is a test document.\",\n    outputFilename: \"output.pdf\",\n  },\n});\n```\n\n## Configuration\n\n### Output Directory\n\nYou can configure the output directory in your MCP settings file for apps that use MCP such as Cline or Claude. If not configured, it will save files to $HOME:\n\n```json\n{\n  \"mcpServers\": {\n    \"markdown2pdf\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/markdown2pdf-mcp/build/index.js\"],\n      \"env\": {\n        \"M2P_OUTPUT_DIR\": \"/path/to/output/directory\"\n      }\n    }\n  }\n}\n```\n\nThe tool automatically handles file name conflicts by appending incremental numbers (e.g., output.pdf, output-1.pdf, output-2.pdf).\n\n## Dependencies\n\n- [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk) - MCP SDK for server implementation\n- [remarkable](https://github.com/jonschlinkert/remarkable) - Markdown parser\n- [highlight.js](https://github.com/highlightjs/highlight.js) - Syntax highlighting\n- [puppeteer](https://github.com/puppeteer/puppeteer) - Modern PDF generation using [Chrome for Testing](https://developer.chrome.com/blog/chrome-for-testing/) (v131.0.6778.204)\n\n## Chrome Version\n\nThis package uses Chrome v131.0.6778.204 for consistent PDF generation across all installations. This version is automatically installed when you run `npm install`.\n\n- [tmp](https://github.com/raszi/node-tmp) - Temporary file handling\n\n## Development\n\n```bash\n# Build the project\nnpm run build\n\n# Start the server\nnpm start\n```\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown2pdf",
        "markdown",
        "pdf",
        "markdown2pdf mcp",
        "2b3pro markdown2pdf",
        "markdown documents"
      ],
      "category": "document-processing"
    },
    "302ai--302_file_parser_mcp": {
      "owner": "302ai",
      "name": "302_file_parser_mcp",
      "url": "https://github.com/302ai/302_file_parser_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/302ai.webp",
      "description": "The File Parser MCP Server helps you read, modify, and manage files easily. It simplifies the process of file handling, allowing developers to focus on building their applications without getting bogged down in the complexities of dealing with different file types.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-15T13:44:27Z",
      "readme_content": "# 302AI File Parser MCP Server\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"302ai-file-parser-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@302ai/file-parser-mcp\"],\n      \"env\": {\n        \"302AI_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nFind Your 302AI_API_KEY [here](https://dash.302.ai/apis/list)\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "302_file_parser_mcp",
        "files",
        "file",
        "302ai 302_file_parser_mcp",
        "302_file_parser_mcp file",
        "file parser"
      ],
      "category": "document-processing"
    },
    "AlexiFeng--MCP_Chat_Logger": {
      "owner": "AlexiFeng",
      "name": "MCP_Chat_Logger",
      "url": "https://github.com/AlexiFeng/MCP_Chat_Logger",
      "imageUrl": "/freedevtools/mcp/pfp/AlexiFeng.webp",
      "description": "Logs chat history in a formatted Markdown file, complete with timestamps for each message. It allows customization of save directories and differentiates conversations using session IDs.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-11T20:31:36Z",
      "readme_content": "# MCP Chat Logger\n\n[![smithery badge](https://smithery.ai/badge/@AlexiFeng/MCP_Chat_Logger)](https://smithery.ai/server/@AlexiFeng/MCP_Chat_Logger)\n\n<div align=\"center\">\n  <a href=\"README_zh.md\">中文</a> | <a href=\"README_en.md\">English</a>\n</div>\n\n---\n\nMCP Chat Logger是一个简单而强大的聊天记录保存工具，可以将聊天历史保存为Markdown格式文件，便于后续查看和分享。\n\n## 功能特点\n\n- 支持大模型调用工具将聊天历史保存为格式化的Markdown文件\n- 自动为每条消息添加时间戳\n- 自定义保存目录\n- 支持会话ID标识不同的对话\n  \n## 下一阶段\n添加Overview功能\n\n### 安装步骤\n\n#### Installing via Smithery\n\nTo install MCP Chat Logger for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@AlexiFeng/MCP_Chat_Logger):\n\n```bash\nnpx -y @smithery/cli install @AlexiFeng/MCP_Chat_Logger --client claude\n```\n\n1. 克隆这个代码库：\n\n```bash\ngit clone https://github.com/yourusername/MCP_Chat_Logger.git\ncd MCP_Chat_Logger\n```\n\n2. 安装依赖：\n提前安装uv\n\n```bash\nuv add \"mcp[cli]\"\n```\n\n## 使用方法\n\n1. 在项目目录启动mcp服务\n```bash\nuv run chat_logger.py\n```\n\n2. 在cursor/cherry studio中添加mcp服务器配置\n\"chat_logger\": {\n      \"name\": \"chat_logger\",\n      \"isActive\": false,\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"项目路径（例如~/MCP_Chat_Logger/）\",\n        \"run\",\n        \"chat_logger.py\"\n      ]\n    }\n\n## 项目结构\n\n```\nMCP_Chat_Logger/\n├── chat_logger.py      # 核心功能实现\n├── chat_logs/          # 默认保存目录\n├── README.md           # 项目说明\n├── README_zh.md        # 中文说明\n├── README_en.md        # 英文说明\n└── .gitignore          # Git忽略文件\n```\n\n## 贡献指南\n\n欢迎提交问题和拉取请求！如果您想贡献代码，请遵循以下步骤：\n\n1. Fork这个仓库\n2. 创建您的特性分支 (`git checkout -b feature/amazing-feature`)\n3. 提交您的更改 (`git commit -m 'Add some amazing feature'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 开启一个Pull Request\n\n## 许可证\n\n该项目采用MIT许可证 - 详情请查看 LICENSE 文件。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_chat_logger",
        "logs",
        "chat",
        "alexifeng mcp_chat_logger",
        "mcp_chat_logger logs",
        "logs chat"
      ],
      "category": "document-processing"
    },
    "Arborist-ai--ClaudeHopper": {
      "owner": "Arborist-ai",
      "name": "ClaudeHopper",
      "url": "https://github.com/Arborist-ai/ClaudeHopper",
      "imageUrl": "/freedevtools/mcp/pfp/Arborist-ai.webp",
      "description": "Interact with construction documents, drawings, and specifications. Analyze technical details and retrieve specific information through advanced retrieval-augmented generation and hybrid search.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-13T23:51:09Z",
      "readme_content": "# 🏗️ ClaudeHopper - AI-Powered Construction Document Assistant\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nClaudeHopper is a specialized Model Context Protocol (MCP) server that enables Claude and other LLMs to interact directly with construction documents, drawings, and specifications through advanced RAG (Retrieval-Augmented Generation) and hybrid search. Ask questions about your construction drawings, locate specific details, and analyze technical specifications with ease.\n\n## ✨ Features\n\n- 🔍 Vector-based search for construction document retrieval optimized for CAD drawings, plans, and specs\n- 🖼️ Visual search to find similar drawings based on textual descriptions\n- 🏢 Specialized metadata extraction for construction industry document formats\n- 📊 Efficient token usage through intelligent document chunking and categorization\n- 🔒 Security through local document storage and processing\n- 📈 Support for various drawing types and construction disciplines (Structural, Civil, Architectural, etc.)\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Node.js 18+\n- [Ollama](https://ollama.com/) for local AI models\n  - Required models: `nomic-embed-text`, `phi4`, `clip`\n- Claude Desktop App\n- For image extraction: [Poppler Utils](https://poppler.freedesktop.org/) (`pdfimages` command)\n\n### One-Click Setup\n\n1. Download ClaudeHopper\n2. Run the setup script:\n\n```bash\ncd ~/Desktop/claudehopper\nchmod +x run_now_preserve.sh\n./run_now_preserve.sh\n```\n\nThis will:\n- Create the necessary directory structure\n- Install required AI models\n- Process your construction documents\n- Configure the Claude Desktop App to use ClaudeHopper\n\n### Adding Documents\n\nPlace your construction documents in these folders:\n\n- Drawings: `~/Desktop/PDFdrawings-MCP/InputDocs/Drawings/`\n- Specifications: `~/Desktop/PDFdrawings-MCP/InputDocs/TextDocs/`\n\nAfter adding documents, run:\n```bash\n./process_pdfdrawings.sh\n```\n\n## 🏗️ Using ClaudeHopper with Claude\n\nTry these example questions in the Claude Desktop App:\n\n```\n\"What architectural drawings do we have for the project?\"\n\"Show me the structural details for the foundation system\"\n\"Find drawings that show a concrete foundation with dimensions\"\n\"Search for lift station layout drawings\"\n\"What are the specifications for interior paint?\"\n\"Find all sections discussing fire protection systems\"\n```\n\n## 🛠️ Technical Architecture\n\nClaudeHopper uses a multi-stage pipeline for processing construction documents:\n\n1. **Document Analysis**: PDF documents are analyzed for structure and content type\n2. **Metadata Extraction**: AI-assisted extraction of project information, drawing types, disciplines\n3. **Content Chunking**: Intelligent splitting of documents to maintain context\n4. **Image Extraction**: Identification and extraction of drawing images from PDFs\n5. **Vector Embedding**: Creation of semantic representations for text and images\n6. **Database Storage**: Local LanceDB storage for vector search capabilities\n\n## 👀 Testing the Image Search\n\nTo test the image search functionality, you can use the provided test script:\n\n```bash\n# Make the test script executable\nchmod +x test_image_search.sh\n\n# Run the test script\n./test_image_search.sh\n```\n\nThis will:\n- Build the application\n- Check for required dependencies (like `pdfimages`)\n- Seed the database with images from your drawings directory\n- Run a series of test queries against the image search\n\nYou can also run individual test commands:\n\n```bash\n# Run the test with the default database location\nnpm run test:image:verbose\n\n# Run the test with a specific database location\nnode tools/test_image_search.js /path/to/your/database\n```\n\n## 📝 Available Search Tools\n\nClaudeHopper provides several specialized search capabilities:\n\n- `catalog_search`: Find documents by project, discipline, drawing type, etc.\n- `chunks_search`: Locate specific content within documents\n- `all_chunks_search`: Search across the entire document collection\n- `image_search`: Find drawings based on visual similarity to textual descriptions\n\nExamples of using the image search feature can be found in the [image_search_examples.md](examples/image_search_examples.md) file.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "search",
        "arborist",
        "arborist ai",
        "processing arborist",
        "advanced retrieval"
      ],
      "category": "document-processing"
    },
    "ArchimedesCrypto--excel-reader-mcp": {
      "owner": "ArchimedesCrypto",
      "name": "excel-reader-mcp",
      "url": "https://github.com/ArchimedesCrypto/excel-reader-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ArchimedesCrypto.webp",
      "description": "Read Excel files with support for automatic chunking and pagination, enabling efficient data handling for large datasets. This server can process multiple sheet selections and provides proper handling of date formats.",
      "stars": 26,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-25T10:11:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/archimedescrypto-excel-reader-mcp-badge.png)](https://mseep.ai/app/archimedescrypto-excel-reader-mcp)\n\n# MCP Excel Reader\n\n[![smithery badge](https://smithery.ai/badge/@ArchimedesCrypto/excel-reader-mcp-chunked)](https://smithery.ai/server/@ArchimedesCrypto/excel-reader-mcp-chunked)\nA Model Context Protocol (MCP) server for reading Excel files with automatic chunking and pagination support. Built with SheetJS and TypeScript, this tool helps you handle large Excel files efficiently by automatically breaking them into manageable chunks.\n\n<a href=\"https://glama.ai/mcp/servers/jr2ggpdk3a\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/jr2ggpdk3a/badge\" alt=\"Excel Reader MCP server\" /></a>\n\n## Features\n\n- 📊 Read Excel files (.xlsx, .xls) with automatic size limits\n- 🔄 Automatic chunking for large datasets\n- 📑 Sheet selection and row pagination\n- 📅 Proper date handling\n- ⚡ Optimized for large files\n- 🛡️ Error handling and validation\n\n## Installation\n\n### Installing via Smithery\n\nTo install Excel Reader for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ArchimedesCrypto/excel-reader-mcp-chunked):\n\n```bash\nnpx -y @smithery/cli install @ArchimedesCrypto/excel-reader-mcp-chunked --client claude\n```\n\n### As an MCP Server\n\n1. Install globally:\n```bash\nnpm install -g @archimdescrypto/excel-reader\n```\n\n2. Add to your MCP settings file (usually at `~/.config/claude/settings.json` or equivalent):\n```json\n{\n  \"mcpServers\": {\n    \"excel-reader\": {\n      \"command\": \"excel-reader\",\n      \"env\": {}\n    }\n  }\n}\n```\n\n### For Development\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/ArchimdesCrypto/mcp-excel-reader.git\ncd mcp-excel-reader\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Usage\n\n## Usage\n\nThe Excel Reader provides a single tool `read_excel` with the following parameters:\n\n```typescript\ninterface ReadExcelArgs {\n  filePath: string;      // Path to Excel file\n  sheetName?: string;    // Optional sheet name (defaults to first sheet)\n  startRow?: number;     // Optional starting row for pagination\n  maxRows?: number;      // Optional maximum rows to read\n}\n\n// Response format\ninterface ExcelResponse {\n  fileName: string;\n  totalSheets: number;\n  currentSheet: {\n    name: string;\n    totalRows: number;\n    totalColumns: number;\n    chunk: {\n      rowStart: number;\n      rowEnd: number;\n      columns: string[];\n      data: Record<string, any>[];\n    };\n    hasMore: boolean;\n    nextChunk?: {\n      rowStart: number;\n      columns: string[];\n    };\n  };\n}\n```\n\n### Basic Usage\n\nWhen used with Claude or another MCP-compatible AI:\n\n```\nRead the Excel file at path/to/file.xlsx\n```\n\nThe AI will use the tool to read the file, automatically handling chunking for large files.\n\n### Features\n\n1. **Automatic Chunking**\n   - Automatically splits large files into manageable chunks\n   - Default chunk size of 100KB\n   - Provides metadata for pagination\n\n2. **Sheet Selection**\n   - Read specific sheets by name\n   - Defaults to first sheet if not specified\n\n3. **Row Pagination**\n   - Control which rows to read with startRow and maxRows\n   - Get next chunk information for continuous reading\n\n4. **Error Handling**\n   - Validates file existence and format\n   - Provides clear error messages\n   - Handles malformed Excel files gracefully\n\n## Extending with SheetJS Features\n\nThe Excel Reader is built on SheetJS and can be extended with its powerful features:\n\n### Available Extensions\n\n1. **Formula Handling**\n   ```typescript\n   // Enable formula parsing\n   const wb = XLSX.read(data, {\n     cellFormula: true,\n     cellNF: true\n   });\n   ```\n\n2. **Cell Formatting**\n   ```typescript\n   // Access cell styles and formatting\n   const styles = Object.keys(worksheet)\n     .filter(key => key[0] !== '!')\n     .map(key => ({\n       cell: key,\n       style: worksheet[key].s\n     }));\n   ```\n\n3. **Data Validation**\n   ```typescript\n   // Access data validation rules\n   const validation = worksheet['!dataValidation'];\n   ```\n\n4. **Sheet Features**\n   - Merged Cells: `worksheet['!merges']`\n   - Hidden Rows/Columns: `worksheet['!rows']`, `worksheet['!cols']`\n   - Sheet Protection: `worksheet['!protect']`\n\nFor more features and detailed documentation, visit the [SheetJS Documentation](https://docs.sheetjs.com/).\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built with [SheetJS](https://sheetjs.com/)\n- Part of the [Model Context Protocol](https://github.com/modelcontextprotocol/mcp) ecosystem\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "excel",
        "formats",
        "archimedescrypto",
        "archimedescrypto excel",
        "excel reader",
        "document processing"
      ],
      "category": "document-processing"
    },
    "AvinashBole--quip-mcp-server": {
      "owner": "AvinashBole",
      "name": "quip-mcp-server",
      "url": "https://github.com/AvinashBole/quip-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/AvinashBole.webp",
      "description": "Interact directly with Quip documents to read, append, prepend, and replace content, enhancing document management capabilities for AI assistants.",
      "stars": 4,
      "forks": 4,
      "license": "ISC License",
      "language": "JavaScript",
      "updated_at": "2025-08-28T19:40:34Z",
      "readme_content": "# Quip MCP Server\n\nA Model Context Protocol (MCP) server for Quip document operations that enables direct interaction with Quip documents from AI assistants like Claude.\n\n## Features\n\n- **Read Documents**: Fetch and display Quip document content by ID\n- **Append Content**: Add content to the end of existing documents\n- **Prepend Content**: Add content to the beginning of documents\n- **Replace Content**: Update document content\n- **Create Documents**: Intended support for creating new documents (currently redirects to web interface)\n\n## How It Works\n\nThis MCP server acts as a bridge between Claude and Quip documents. It works by:\n\n1. Receiving requests from Claude through the MCP protocol\n2. Executing a Python script (`quip_edit_fixed.py`) with the appropriate parameters\n3. Returning the results back to Claude\n\n## Prerequisites\n\n- Node.js v18 or higher\n- TypeScript\n- Python with `quip` library installed\n- A valid Quip access token\n\n## Installation\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/AvinashBole/quip-mcp-server.git\n   cd quip-mcp-server\n   ```\n\n2. Install dependencies:\n   ```\n   npm install\n   ```\n\n3. Build the project:\n   ```\n   npm run build\n   ```\n\n4. Configure your MCP settings:\n   ```json\n   {\n     \"mcpServers\": {\n       \"quip\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/quip-server/build/index.js\"],\n         \"env\": {\n           \"QUIP_ACCESS_TOKEN\": \"your-quip-access-token\",\n           \"QUIP_BASE_URL\": \"https://platform.quip.com\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n## Usage\n\nOnce connected, the following MCP tools become available to Claude:\n\n- `quip_read_document`: Read a Quip document by its thread ID\n- `quip_append_content`: Append content to a document\n- `quip_prepend_content`: Add content to the beginning of a document\n- `quip_replace_content`: Replace document content\n- `quip_create_document`: Create a new document (currently unsupported)\n\nExample usage in Claude:\n\n```\n<use_mcp_tool>\n<server_name>quip</server_name>\n<tool_name>quip_read_document</tool_name>\n<arguments>\n{\n  \"threadId\": \"YOUR_DOCUMENT_ID\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Python Script Integration\n\nThe server expects a Python script called `quip_edit_fixed.py` in the path specified by the `PYTHON_SCRIPT_PATH` constant. This script should support the following operations:\n\n- `read`: Read document content\n- `append`: Add content to the end of a document\n- `prepend`: Add content to the beginning of a document \n- `replace`: Update document content\n\n## License\n\nISC License\n\n## Author\n\nAvinashBole\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quip",
        "document",
        "processing",
        "quip documents",
        "document processing",
        "quip mcp"
      ],
      "category": "document-processing"
    },
    "Braffolk--mcp-summarization-functions": {
      "owner": "Braffolk",
      "name": "mcp-summarization-functions",
      "url": "https://github.com/Braffolk/mcp-summarization-functions",
      "imageUrl": "/freedevtools/mcp/pfp/Braffolk.webp",
      "description": "Provides intelligent text summarization to condense output and reduce token usage, preventing potential crashes during extensive tasks.",
      "stars": 36,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T09:20:03Z",
      "readme_content": "<div align=\"center\">\n\n# Summarization Functions\n\n### Intelligent text summarization for the Model Context Protocol\n\n[Features](#features) •\n[AI Agent Integration](#ai-agent-integration) •\n[Installation](#installation) •\n[Usage](#usage)\n\n[![smithery badge](https://smithery.ai/badge/mcp-summarization-functions)](https://smithery.ai/server/mcp-summarization-functions)\n[![npm version](https://badge.fury.io/js/mcp-summarization-functions.svg)](https://www.npmjs.com/package/mcp-summarization-functions)\n\n</div>\n\n---\n\n## Overview\n\nA powerful MCP server that provides intelligent summarization capabilities through a clean, extensible architecture. Built with modern TypeScript and designed for seamless integration with AI workflows.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Summarization Functions for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-summarization-functions):\n\n```bash\nnpx -y @smithery/cli install mcp-summarization-functions --client claude\n```\n\n```bash\nnpm i mcp-summarization-functions\n```\n\n## AI Agent Integration\n\nThis MCP server was primarily developed to enhance the performance and reliability of AI agents like Roo Cline and Cline. It addresses a critical challenge in AI agent operations: context window management.\n\n### Context Window Optimization\n\nAI agents frequently encounter situations where their context window gets rapidly filled with large outputs from:\n- Command execution results\n- File content readings\n- Directory listings\n- API responses\n- Error messages and stack traces\n\nThis server helps maintain efficient context usage by:\n1. Providing concise, relevant summaries instead of full content\n2. Storing full content for reference when needed\n3. Offering focused analysis based on specific needs (security, API surface, etc.)\n4. Supporting multiple output formats for optimal context utilization\n\n### Benefits for AI Agents\n\n- **Reduced Failure Rates**: By preventing context window overflow\n- **Improved Response Quality**: Through focused, relevant summaries\n- **Enhanced Efficiency**: By maintaining important context while reducing noise\n- **Better Resource Management**: Through intelligent content caching and retrieval\n- **Flexible Integration**: Supporting multiple AI providers and configuration options\n\n### Recommended AI Agent Prompt\n\nWhen integrating with AI agents, include the following in your agent's instructions:\n\n```\n# CONTEXT MANAGEMENT\n\nYou have access to summarization functions through the MCP server. These functions are NOT optional - you MUST use them for ALL potentially large outputs to prevent context overflow:\n\nMANDATORY SUMMARIZATION:\n- You MUST ALWAYS use summarization functions for:\n    - ANY first time file reading operations (unless you are CERTAIN its small and you are going to edit it)\n    - ALL command execution outputs\n    - EVERY directory analysis\n    - ANY API responses or error logs\n    - ANY output that could be large\n\nNEVER attempt to process raw output directly - ALWAYS use the appropriate summarization function:\n• For commands: summarize_command\n• For files: summarize_files\n• For directories: summarize_directory\n• For other text: summarize_text\n\nALWAYS utilize available features:\n• Specify hints for focused analysis\n• Choose appropriate output formats\n• Use content IDs to access full details only when absolutely necessary\n\nThere is NO NEED to process perfect or complete output. Summarized content is ALWAYS preferred over raw data. When in doubt, use summarization.\n```\n\n<b>Summarization in action on the Ollama repository (Gemini 2.0 Flash summarization, Claude 3.5 agent)</b>\n\n![alt text](in_action.png)\n\n\n## Features\n\n- **Command Output Summarization**  \n  Execute commands and get concise summaries of their output\n\n- **File Content Analysis**  \n  Summarize single or multiple files while maintaining technical accuracy\n\n- **Directory Structure Understanding**  \n  Get clear overviews of complex directory structures\n\n- **Flexible Model Support**\n  Use models from different providers\n\n- **AI Agent Context Optimization**\n  Prevent context window overflow and improve AI agent performance through intelligent summarization\n\n## Configuration\n\nThe server supports multiple AI providers through environment variables:\n\n### Required Environment Variables\n\n- `PROVIDER`: AI provider to use. Supported values:\n\t\t- `ANTHROPIC` - Claude models from Anthropic\n\t\t- `OPENAI` - GPT models from OpenAI\n\t\t- `OPENAI-COMPATIBLE` - OpenAI-compatible APIs (e.g. Azure)\n\t\t- `GOOGLE` - Gemini models from Google\n- `API_KEY`: API key for the selected provider\n\n### Optional Environment Variables\n\n- `MODEL_ID`: Specific model to use (defaults to provider's standard model)\n- `PROVIDER_BASE_URL`: Custom API endpoint for OpenAI-compatible providers\n- `MAX_TOKENS`: Maximum tokens for model responses (default: 1024)\n- `SUMMARIZATION_CHAR_THRESHOLD`: Character count threshold for when to summarize (default: 512)\n- `SUMMARIZATION_CACHE_MAX_AGE`: Cache duration in milliseconds (default: 3600000 - 1 hour)\n- `MCP_WORKING_DIR` - fallback directory for trying to find files with relative paths from\n\n### Example Configurations\n\n```bash\n# Anthropic Configuration\nPROVIDER=ANTHROPIC\nAPI_KEY=your-anthropic-key\nMODEL_ID=claude-3-5-sonnet-20241022\n\n# OpenAI Configuration\nPROVIDER=OPENAI\nAPI_KEY=your-openai-key\nMODEL_ID=gpt-4-turbo-preview\n\n# Azure OpenAI Configuration\nPROVIDER=OPENAI-COMPATIBLE\nAPI_KEY=your-azure-key\nPROVIDER_BASE_URL=https://your-resource.openai.azure.com\nMODEL_ID=your-deployment-name\n\n# Google Configuration\nPROVIDER=GOOGLE\nAPI_KEY=your-google-key\nMODEL_ID=gemini-2.0-flash-exp\n```\n\n## Usage\n\nAdd the server to your MCP configuration file:\n\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"MUST_USE_summarization\": {\n\t\t\t\t\t\t\"command\": \"node\",\n\t\t\t\t\t\t\"args\": [\"path/to/summarization-functions/build/index.js\"],\n\t\t\t\t\t\t\"env\": {\n\t\t\t\t\t\t\t\t\"PROVIDER\": \"ANTHROPIC\",\n\t\t\t\t\t\t\t\t\"API_KEY\": \"your-api-key\",\n\t\t\t\t\t\t\t\t\"MODEL_ID\": \"claude-3-5-sonnet-20241022\",\n                \"MCP_WORKING_DIR\": \"default_working_directory\"\n\t\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n}\n```\n\n### Available Functions\n\nThe server provides the following summarization tools:\n\n#### `summarize_command`\nExecute and summarize command output.\n```typescript\n{\n  // Required\n  command: string,    // Command to execute\n  cwd: string,       // Working directory for command execution\n  \n  // Optional\n  hint?: string,      // Focus area: \"security_analysis\" | \"api_surface\" | \"error_handling\" | \"dependencies\" | \"type_definitions\"\n  output_format?: string  // Format: \"text\" | \"json\" | \"markdown\" | \"outline\" (default: \"text\")\n}\n```\n\n#### `summarize_files`\nSummarize file contents.\n```typescript\n{\n  // Required\n  paths: string[],    // Array of file paths to summarize (relative to cwd)\n  cwd: string,       // Working directory for resolving file paths\n  \n  // Optional\n  hint?: string,      // Focus area: \"security_analysis\" | \"api_surface\" | \"error_handling\" | \"dependencies\" | \"type_definitions\"\n  output_format?: string  // Format: \"text\" | \"json\" | \"markdown\" | \"outline\" (default: \"text\")\n}\n```\n\n#### `summarize_directory`\nGet directory structure overview.\n```typescript\n{\n  // Required\n  path: string,       // Directory path to summarize (relative to cwd)\n  cwd: string,       // Working directory for resolving directory path\n  \n  // Optional\n  recursive?: boolean,  // Whether to include subdirectories. Safe for deep directories\n  hint?: string,       // Focus area: \"security_analysis\" | \"api_surface\" | \"error_handling\" | \"dependencies\" | \"type_definitions\"\n  output_format?: string   // Format: \"text\" | \"json\" | \"markdown\" | \"outline\" (default: \"text\")\n}\n```\n\n#### `summarize_text`\nSummarize arbitrary text content.\n```typescript\n{\n  // Required\n  content: string,    // Text content to summarize\n  type: string,       // Type of content (e.g., \"log output\", \"API response\")\n  \n  // Optional\n  hint?: string,      // Focus area: \"security_analysis\" | \"api_surface\" | \"error_handling\" | \"dependencies\" | \"type_definitions\"\n  output_format?: string  // Format: \"text\" | \"json\" | \"markdown\" | \"outline\" (default: \"text\")\n}\n```\n\n#### `get_full_content`\nRetrieve the full content for a given summary ID.\n```typescript\n{\n  // Required\n  id: string         // ID of the stored content\n}\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/server/mcp-server.ts\n```\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "summarization",
        "braffolk",
        "text",
        "mcp summarization",
        "text summarization",
        "summarization functions"
      ],
      "category": "document-processing"
    },
    "Cam10001110101--mcp-server-obsidian-jsoncanvas": {
      "owner": "Cam10001110101",
      "name": "mcp-server-obsidian-jsoncanvas",
      "url": "https://github.com/Cam10001110101/mcp-server-obsidian-jsoncanvas",
      "imageUrl": "/freedevtools/mcp/pfp/Cam10001110101.webp",
      "description": "Create, modify, and validate infinite canvas data structures using a comprehensive set of tools that manage nodes and edges while ensuring compliance with the official JSON Canvas specification.",
      "stars": 8,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-05T05:18:59Z",
      "readme_content": "# JSON Canvas MCP Server\n\nA Model Context Protocol (MCP) server implementation that provides tools for working with JSON Canvas files according to the [official specification](https://jsoncanvas.org/spec/1.0/). This server enables creating, modifying, and validating infinite canvas data structures.\n\n## Overview\n\nThe JSON Canvas MCP server provides a complete implementation of the JSON Canvas 1.0 specification, enabling:\n\n- Creation and manipulation of infinite canvas data\n- Support for all node types (text, file, link, group)\n- Edge connections with styling and labels\n- Validation against the specification\n- Configurable output paths\n\n## Components\n\n### Resources\n\nThe server exposes the following resources:\n\n- `canvas://schema`: JSON Schema for validating canvas files\n- `canvas://examples`: Example canvas files demonstrating different features\n- `canvas://templates`: Templates for creating new canvases\n\n### Tools\n\n#### Node Operations\n\n- **create_node**\n  - Create a new node of any supported type\n  - Input:\n    - `type` (string): Node type (\"text\", \"file\", \"link\", \"group\")\n    - `properties` (object): Node-specific properties\n      - Common: `id`, `x`, `y`, `width`, `height`, `color`\n      - Type-specific: `text`, `file`, `url`, etc.\n  - Returns: Created node object\n\n- **update_node**\n  - Update an existing node's properties\n  - Input:\n    - `id` (string): Node ID to update\n    - `properties` (object): Properties to update\n  - Returns: Updated node object\n\n- **delete_node**\n  - Remove a node and its connected edges\n  - Input:\n    - `id` (string): Node ID to delete\n  - Returns: Success confirmation\n\n#### Edge Operations\n\n- **create_edge**\n  - Create a new edge between nodes\n  - Input:\n    - `id` (string): Unique edge identifier\n    - `fromNode` (string): Source node ID\n    - `toNode` (string): Target node ID\n    - `fromSide` (optional string): Start side (\"top\", \"right\", \"bottom\", \"left\")\n    - `toSide` (optional string): End side\n    - `color` (optional string): Edge color\n    - `label` (optional string): Edge label\n  - Returns: Created edge object\n\n- **update_edge**\n  - Update an existing edge's properties\n  - Input:\n    - `id` (string): Edge ID to update\n    - `properties` (object): Properties to update\n  - Returns: Updated edge object\n\n- **delete_edge**\n  - Remove an edge\n  - Input:\n    - `id` (string): Edge ID to delete\n  - Returns: Success confirmation\n\n#### Canvas Operations\n\n- **validate_canvas**\n  - Validate a canvas against the specification\n  - Input:\n    - `canvas` (object): Canvas data to validate\n  - Returns: Validation results with any errors\n\n- **export_canvas**\n  - Export canvas to different formats\n  - Input:\n    - `format` (string): Target format (\"json\", \"svg\", \"png\")\n    - `canvas` (object): Canvas data to export\n  - Returns: Exported canvas in requested format\n\n## Usage with Claude Desktop\n\n### Docker\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"jsoncanvas\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"canvas-data:/data\",\n        \"mcp/jsoncanvas\"\n      ],\n      \"env\": {\n        \"OUTPUT_PATH\": \"/data/output\"\n      }\n    }\n  }\n}\n```\n\n### UV\n\n```json\n{\n  \"mcpServers\": {\n    \"jsoncanvas\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/jsoncanvas\",\n        \"run\",\n        \"mcp-server-jsoncanvas\"\n      ],\n      \"env\": {\n        \"OUTPUT_PATH\": \"./output\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `OUTPUT_PATH`: Directory where canvas files will be saved (default: \"./output\")\n- `FORMAT`: Default output format for canvas files (default: \"json\")\n\n## Building\n\n### Docker Build\n\n```bash\ndocker build -t mcp/jsoncanvas .\n```\n\n### Local Build\n\n```bash\n# Install uv if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n\n# Run tests\npytest\n```\n\n## Example Usage\n\n### Creating a Canvas\n\n```python\nfrom jsoncanvas import Canvas, TextNode, Edge\n\n# Create nodes\ntitle = TextNode(\n    id=\"title\",\n    x=100,\n    y=100,\n    width=400,\n    height=100,\n    text=\"# Hello Canvas\\n\\nThis is a demonstration.\",\n    color=\"#4285F4\"\n)\n\ninfo = TextNode(\n    id=\"info\",\n    x=600,\n    y=100,\n    width=300,\n    height=100,\n    text=\"More information here\",\n    color=\"2\"  # Using preset color\n)\n\n# Create canvas\ncanvas = Canvas()\ncanvas.add_node(title)\ncanvas.add_node(info)\n\n# Connect nodes\nedge = Edge(\n    id=\"edge1\",\n    from_node=\"title\",\n    to_node=\"info\",\n    from_side=\"right\",\n    to_side=\"left\",\n    label=\"Connection\"\n)\ncanvas.add_edge(edge)\n\n# Save canvas\ncanvas.save(\"example.canvas\")\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "canvas",
        "jsoncanvas",
        "nodes",
        "canvas data",
        "json canvas",
        "canvas specification"
      ],
      "category": "document-processing"
    },
    "CartographAI--atlas-docs-mcp": {
      "owner": "CartographAI",
      "name": "atlas-docs-mcp",
      "url": "https://github.com/CartographAI/atlas-docs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/CartographAI.webp",
      "description": "Provides AI assistants with access to clean markdown documentation for various libraries and frameworks, enhancing their ability to utilize less popular or newly released libraries.",
      "stars": 33,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T17:17:13Z",
      "readme_content": "# Atlas Docs MCP Server\n\n[![NPM Version](https://img.shields.io/npm/v/%40cartographai%2Fatlas-docs-mcp)](https://www.npmjs.com/package/@cartographai/atlas-docs-mcp)\n[![smithery badge](https://smithery.ai/badge/@CartographAI/atlas-docs-mcp)](https://smithery.ai/server/@CartographAI/atlas-docs-mcp)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides AI assistants with documentation for libraries and frameworks.\n\n> [!WARNING]\n> Atlas Docs is currently in beta. Not everything might work perfectly, but we're actively improving the service. Your patience and [feedback](#support--feedback) are greatly appreciated!\n\n## What Does This Server Do?\n\nLLMs are great at generating general code, but suck at correctly using less popular or newly released libraries. This isn't surprising, since the models have not been trained comprehensively on code using these libraries.\n\nAtlas Docs MCP server:\n\n- Provides technical documentation for libraries and frameworks\n- Processes the official docs into a clean markdown version for LLM consumption\n- Is easy to set up with Cursor, Cline, Windsurf and any other MCP-compatible LLM clients\n\n**Claude 3.5 Sonnet on its own:**\n\n![elevenlabs-without-atlas-annotated](https://github.com/user-attachments/assets/78b8309c-0f86-4b20-93d7-2116419f75fd)\n\n**Claude 3.5 Sonnet with Atlas Docs MCP:**\n\n![elevenlabs-with-atlas-annotated](https://github.com/user-attachments/assets/258c5126-242f-43d1-8e78-ea655f44d76a)\n\n<video src=\"https://github.com/user-attachments/assets/5fb1f3f2-18db-4ba4-8f47-da3892af22ee\"></video>\n\n## 📦 Installation\n\nAtlas Docs MCP server works with any MCP client that supports the `stdio` protocol, including:\n\n- Cursor\n- Cline\n- Windsurf\n- Claude Desktop\n\nAdd the following to your MCP client configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"atlas-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@cartographai/atlas-docs-mcp\"]\n    }\n  }\n}\n```\n\nThat's it! You may need to restart the app (for Claude Desktop) for the server to be recognised.\n\n**Tip**: Prompt your model to check the docs eg. \"Use the tools to check the documentation for Astro to ensure that you use the library correctly.\"\n\n### Installing via Smithery\n\nAlternatively, you can install Atlas Docs MCP automatically via [Smithery](https://smithery.ai/server/@CartographAI/atlas-docs-mcp). Example for claude desktop:\n\n```bash\nnpx -y @smithery/cli install @CartographAI/atlas-docs-mcp --client claude\n```\n\nChange \"claude\" to \"cursor\", \"cline\" or \"windsurf\" for the respective clients.\n\n## 📒 Available Libraries\n\n- AI-SDK (source: https://sdk.vercel.ai/docs/introduction)\n- Astro (source: https://docs.astro.build/en/getting-started)\n- ast-grep (source: https://ast-grep.github.io/llms.txt)\n- Bun (source: https://bun.sh/llms.txt)\n- CrewAI (source: https://docs.crewai.com/llms.txt)\n- Drizzle (source: https://orm.drizzle.team/llms.txt)\n- ElevenLabs (source: https://elevenlabs.io/docs/llms.txt)\n- Fireworks (source: https://docs.fireworks.ai/llms.txt)\n- Hono (source: https://hono.dev/llms.txt)\n- Langgraph-js (source: https://langchain-ai.github.io/langgraphjs/llms.txt)\n- Langgraph-py (source: https://langchain-ai.github.io/langgraph/llms.txt)\n- Mastra (source: https://mastra.ai/llms.txt)\n- ModelContextProtocol (source: https://modelcontextprotocol.io/llms.txt)\n- Pglite (source: https://pglite.dev/docs/about)\n- Prisma (source: https://www.prisma.io/docs/llms.txt)\n- Resend (source: https://resend.com/docs/llms.txt)\n- shadcn/ui (source: https://ui.shadcn.com/docs)\n- Stripe (source: https://docs.stripe.com/llms.txt)\n- Svelte (source: https://svelte.dev/docs/svelte/overview)\n- SvelteKit (source: https://svelte.dev/docs/kit/introduction)\n- tailwindcss (source: https://tailwindcss.com/docs/installation)\n- TanStack-Router (source: https://tanstack.com/router/latest/docs/framework/react/overview)\n- Trigger.dev (source: https://trigger.dev/docs/llms.txt)\n- X (source: https://docs.x.com/llms.txt)\n- Zapier (source: https://docs.zapier.com/llms.txt)\n\nWant docs for another library not in this list? Please [open an issue](https://github.com/CartographAI/atlas/issues/new) in this repo, we'll try to process and add it!\n\n## 🔨 Available Tools\n\n1. `list_docs`: List all available documentation sets\n2. `get_docs_index`: Retrieves a condensed, LLM-friendly index of a documentation set\n3. `get_docs_full`: Retrieves a complete documentation set in a single consolidated file\n4. `search_docs`: Search a documentation set by keywords\n5. `get_docs_page`: Retrieves a specific page of a documentation set\n\n## 💭 How It Works\n\nAtlas Docs processes tech libraries' documentation sites into clean, markdown versions. This MCP server provides the docs as MCP tools, calling Atlas Docs APIs for the data.\n\n## Running the backend locally\n\nPlease visit [CartographAI/atlas](https://github.com/CartographAI/atlas) and follow the instructions in the README.\nUpdate ATLAS_API_URL with the url of your deployment.\n\n## Support & Feedback\n\nPlease [open an issue](https://github.com/CartographAI/atlas/issues/new) in this repo to request docs for a library, or to report a bug.\n\nIf you have any questions, feedback, or just want to say hi, we'd love to hear from you. You can find us on Cartograph's [Discord comunity](https://discord.gg/MsBA7U7hH5) for real-time support, or email us at [contact@cartograph.app](mailto:contact@cartograph.app)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cartographai",
        "documentation",
        "markdown",
        "processing cartographai",
        "cartographai atlas",
        "atlas docs"
      ],
      "category": "document-processing"
    },
    "ChrisL108--outline-mcp": {
      "owner": "ChrisL108",
      "name": "outline-mcp",
      "url": "https://github.com/ChrisL108/outline-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ChrisL108.webp",
      "description": "Search and retrieve documents from Outline knowledge bases, facilitating access to internal documentation. Supports secure access via interactive credentials and environment variables.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-13T01:39:32Z",
      "readme_content": "# Outline MCP\n\nAn MCP (Model Control Protocol) server for integrating Outline with Claude.\n\n## Overview\n\nThis tool provides Claude with the ability to search and retrieve documents from your Outline knowledge base. Use it to help Claude answer questions using your organization's internal documentation.\n\n## Features\n\n- Search for documents in your Outline instance\n- Retrieve full document content by ID\n- Maintain persistent credentials between sessions\n\n## Installation\n\n> **Note**: Your Outline API key must have the following permissions:\n> - `documents.info` - For retrieving document content\n> - `documents.search` - For searching documents\n\n> **Note**: If `OUTLINE_URL` and `OUTLINE_API_KEY` are not provided in your config, Claude will prompt you for them. These credentials will be saved in `~/.outline_mcp_credentials.json` for future use.\n\n### Option 1: Install via Smithery CLI (requires smithery key)\n\n```bash\nnpx -y @smithery/cli@latest install @ChrisL108/outline-mcp --client claude --key your-smithery-api-key\n```\n\nYou can enter your `OUTLINE_URL` and `OUTLINE_API_KEY` when prompted by Claude or add them to your `claude_desktop_config.json` directly:\n\n```json\n\"env\": {\n    \"OUTLINE_URL\": \"https://your.outline.com\",\n    \"OUTLINE_API_KEY\": \"your-outline-api-key\"\n}\n```\n\nYour config should look like this:\n\n```json\n\"outline-mcp\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"@smithery/cli@latest\",\n      \"run\",\n      \"@ChrisL108/outline-mcp\",\n      \"--key\",\n      \"your-smithery-api-key\"\n    ],\n    \"env\": {\n      \"OUTLINE_URL\": \"https://your.outline.com\",\n      \"OUTLINE_API_KEY\": \"your-outline-api-key\"\n    }\n}\n```\n\n### Option 2: Install via UVX\n\n```json\n\"outline-mcp\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"--from\",\n      \"git+https://github.com/ChrisL108/outline-mcp\",\n      \"outline-mcp\"\n    ],\n    \"env\": {\n      \"OUTLINE_URL\": \"https://your.outline.com\",\n      \"OUTLINE_API_KEY\": \"your-outline-api-key\"\n    }\n}\n```\n\n## Usage\n\nOnce installed, you can use the following functions in your conversations with Claude:\n\n### Search Documents\n\n```\nsearch_documents(query, outline_url=None, api_key=None, limit=5, status_filter=\"published\", date_filter=\"year\")\n```\n\nParameters:\n- `query`: Search term (required)\n- `outline_url`: Base URL of your Outline instance (only needed first time)\n- `api_key`: Outline API key (only needed first time)\n- `limit`: Maximum number of results (default: 5)\n- `status_filter`: Filter by status (default: \"published\")\n- `date_filter`: Filter by date (default: \"year\")\n\n### Get Document by ID\n\n```\nget_document_by_id(document_id)\n```\n\nParameters:\n- `document_id`: ID of the document to retrieve (required)\n\n### Update Credentials\n\n```\nupdate_credentials(outline_url, api_key)\n```\n\nParameters:\n- `outline_url`: Base URL of your Outline instance\n- `api_key`: Outline API key\n\n### Ping\n\n```\nping()\n```\n\nSimple test function to verify the MCP server is working.\n\n## Example Prompts\n\nHere are some example prompts you can use with Claude:\n\n1. \"Search Outline for information about our product roadmap.\"\n2. \"Can you find any documents related to our hiring process?\"\n3. \"Check our internal documentation for our AWS setup.\"\n4. \"Can you get the full content of this document? [document_id]\"\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Ensure your Outline URL and API key are correctly configured\n2. Verify that your Outline instance is accessible\n3. Check if your API key has the necessary permissions\n4. Try the `ping()` function to verify the MCP server is running correctly\n\n## Security Considerations\n\n- Keep your API key secure and don't share it\n- The MCP server only has the permissions granted to your API key\n- Consider using a read-only API key if you only need search functionality\n\n## License\n\nMIT License\n\nCopyright (c) 2025 ChrisL108\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Credits\n\nCreated by [ChrisL108](https://github.com/ChrisL108)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "documentation",
        "document",
        "documents outline",
        "document processing",
        "outline mcp"
      ],
      "category": "document-processing"
    },
    "David09090218--ai-document-writer": {
      "owner": "David09090218",
      "name": "ai-document-writer",
      "url": "https://github.com/David09090218/ai-document-writer",
      "imageUrl": "/freedevtools/mcp/pfp/David09090218.webp",
      "description": "快速生成各类专业公文，支持用户选择公文类型和输入提示，智能生成符合规范的文档内容。响应式设计和简洁界面提高了用户的写作效率。",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-23T17:40:03Z",
      "readme_content": "# AI公文写作助手\n\n这是一个基于 Next.js 和OpenAI API开发的智能公文写作助手，可以帮助用户快速生成各类专业公文。\n\n## 功能特点\n\n- 支持多种公文类型：通知、报告、请示、总结等\n- 智能生成符合规范的公文内容\n- 提供写作提示功能\n- 响应式设计，支持各种设备\n- 简洁直观的用户界面\n\n## 技术栈\n\n- Next.js 14\n- TypeScript\n- Tailwind CSS\n- OpenAI API\n- React\n\n## 开始使用\n\n1. 克隆项目\n```bash\ngit clone [项目地址]\ncd ai-document-writer\n```\n\n2. 安装依赖\n```bash\nnpm install\n```\n\n3. 配置环境变量\n复制 `.env.local.example` 文件为 `.env.local`，并填入你的 OpenAI API 密钥：\n```\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\n4. 启动开发服务器\n```bash\nnpm run dev\n```\n\n5. 在浏览器中访问 `http://localhost:3000`\n\n## 使用说明\n\n1. 在首页选择\"开始写作\"或\"查看模板\"\n2. 选择需要的公文类型\n3. 可以输入写作提示来指导AI生成内容\n4. 点击\"生成文档\"按钮\n5. 等待AI生成内容\n6. 复制生成的内容进行编辑和使用\n\n## 注意事项\n\n- 请确保有有效的 OpenAI API 密钥\n- 生成的内容仅供参考，建议进行人工审核和修改\n- 请遵守相关法律法规和公文写作规范\n\n## 许可证\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "processing",
        "支持用户选择公文类型和输入提示",
        "document processing",
        "ai document",
        "document writer"
      ],
      "category": "document-processing"
    },
    "DumplingAI--mcp-server-dumplingai": {
      "owner": "DumplingAI",
      "name": "mcp-server-dumplingai",
      "url": "https://github.com/DumplingAI/mcp-server-dumplingai",
      "imageUrl": "/freedevtools/mcp/pfp/DumplingAI.webp",
      "description": "Integrates data scraping, content processing, and AI capabilities, with features for document conversion, web scraping, and knowledge management. Supports real-time information access through various data APIs and secure code execution.",
      "stars": 27,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T08:58:27Z",
      "readme_content": "# Dumpling AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with Dumpling AI for data scraping, content processing, knowledge management, AI agents, and code execution capabilities.\n\n[![smithery badge](https://smithery.ai/badge/@Dumpling-AI/mcp-server-dumplingai)](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai)\n\n## Features\n\n- Complete integration with all Dumpling AI API endpoints\n- Data APIs for YouTube transcripts, search, autocomplete, maps, places, news, and reviews\n- Web scraping with support for scraping, crawling, screenshots, and structured data extraction\n- Document conversion tools for text extraction, PDF operations, video processing\n- Extract data from documents, images, audio, and video\n- AI capabilities including agent completions, knowledge base management, and image generation\n- Developer tools for running JavaScript and Python code in a secure environment\n- Automatic error handling and detailed response formatting\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-dumplingai for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai):\n\n```bash\nnpx -y @smithery/cli install @Dumpling-AI/mcp-server-dumplingai --client claude\n```\n\n### Running with npx\n\n```bash\nenv DUMPLING_API_KEY=your_api_key npx -y mcp-server-dumplingai\n```\n\n### Manual Installation\n\n```bash\nnpm install -g mcp-server-dumplingai\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️ Note: Requires Cursor version 0.45.6+\n\nTo configure Dumpling AI MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n\n```\n{\n  \"mcpServers\": {\n    \"dumplingai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-server-dumplingai\"],\n      \"env\": {\n        \"DUMPLING_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set DUMPLING_API_KEY=your-api-key && npx -y mcp-server-dumplingai\"`\n\nReplace `your-api-key` with your Dumpling AI API key.\n\n## Configuration\n\n### Environment Variables\n\n- `DUMPLING_API_KEY`: Your Dumpling AI API key (required)\n\n## Available Tools\n\n### Data APIs\n\n#### 1. Get YouTube Transcript (`get-youtube-transcript`)\n\nExtract transcripts from YouTube videos with optional timestamps.\n\n```json\n{\n  \"name\": \"get-youtube-transcript\",\n  \"arguments\": {\n    \"videoUrl\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"includeTimestamps\": true,\n    \"timestampsToCombine\": 3,\n    \"preferredLanguage\": \"en\"\n  }\n}\n```\n\n#### 2. Search (`search`)\n\nPerform Google web searches and optionally scrape content from results.\n\n```json\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"machine learning basics\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastMonth\",\n    \"scrapeResults\": true,\n    \"numResultsToScrape\": 3,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true\n    }\n  }\n}\n```\n\n#### 3. Get Autocomplete (`get-autocomplete`)\n\nGet Google search autocomplete suggestions for a query.\n\n```json\n{\n  \"name\": \"get-autocomplete\",\n  \"arguments\": {\n    \"query\": \"how to learn\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"location\": \"New York\"\n  }\n}\n```\n\n#### 4. Search Maps (`search-maps`)\n\nSearch Google Maps for locations and businesses.\n\n```json\n{\n  \"name\": \"search-maps\",\n  \"arguments\": {\n    \"query\": \"coffee shops\",\n    \"gpsPositionZoom\": \"37.7749,-122.4194,14z\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 5. Search Places (`search-places`)\n\nSearch for places with more detailed information.\n\n```json\n{\n  \"name\": \"search-places\",\n  \"arguments\": {\n    \"query\": \"hotels in paris\",\n    \"country\": \"fr\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 6. Search News (`search-news`)\n\nSearch for news articles with customizable parameters.\n\n```json\n{\n  \"name\": \"search-news\",\n  \"arguments\": {\n    \"query\": \"climate change\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastWeek\"\n  }\n}\n```\n\n#### 7. Get Google Reviews (`get-google-reviews`)\n\nRetrieve Google reviews for businesses or places.\n\n```json\n{\n  \"name\": \"get-google-reviews\",\n  \"arguments\": {\n    \"businessName\": \"Eiffel Tower\",\n    \"location\": \"Paris, France\",\n    \"limit\": 10,\n    \"sortBy\": \"relevance\"\n  }\n}\n```\n\n### Web Scraping\n\n#### 8. Scrape (`scrape`)\n\nExtract content from a web page with formatting options.\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\",\n    \"cleaned\": true,\n    \"renderJs\": true\n  }\n}\n```\n\n#### 9. Crawl (`crawl`)\n\nRecursively crawl websites and extract content with customizable parameters.\n\n```json\n{\n  \"name\": \"crawl\",\n  \"arguments\": {\n    \"baseUrl\": \"https://example.com\",\n    \"maxPages\": 10,\n    \"crawlBeyondBaseUrl\": false,\n    \"depth\": 2,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true,\n      \"renderJs\": true\n    }\n  }\n}\n```\n\n#### 10. Screenshot (`screenshot`)\n\nCapture screenshots of web pages with customizable viewport and format options.\n\n```json\n{\n  \"name\": \"screenshot\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"width\": 1280,\n    \"height\": 800,\n    \"fullPage\": true,\n    \"format\": \"png\",\n    \"waitFor\": 1000\n  }\n}\n```\n\n#### 11. Extract (`extract`)\n\nExtract structured data from web pages using AI-powered instructions.\n\n```json\n{\n  \"name\": \"extract\",\n  \"arguments\": {\n    \"url\": \"https://example.com/products\",\n    \"instructions\": \"Extract all product names, prices, and descriptions from this page\",\n    \"schema\": {\n      \"products\": [\n        {\n          \"name\": \"string\",\n          \"price\": \"number\",\n          \"description\": \"string\"\n        }\n      ]\n    },\n    \"renderJs\": true\n  }\n}\n```\n\n### Document Conversion\n\n#### 12. Doc to Text (`doc-to-text`)\n\nConvert documents to plaintext with optional OCR.\n\n```json\n{\n  \"name\": \"doc-to-text\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n#### 13. Convert to PDF (`convert-to-pdf`)\n\nConvert various file formats to PDF.\n\n```json\n{\n  \"name\": \"convert-to-pdf\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.docx\",\n    \"format\": \"docx\",\n    \"options\": {\n      \"quality\": 90,\n      \"pageSize\": \"A4\",\n      \"margin\": 10\n    }\n  }\n}\n```\n\n#### 14. Merge PDFs (`merge-pdfs`)\n\nCombine multiple PDFs into a single document.\n\n```json\n{\n  \"name\": \"merge-pdfs\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/doc1.pdf\", \"https://example.com/doc2.pdf\"],\n    \"options\": {\n      \"addPageNumbers\": true,\n      \"addTableOfContents\": true\n    }\n  }\n}\n```\n\n#### 15. Trim Video (`trim-video`)\n\nExtract a specific clip from a video.\n\n```json\n{\n  \"name\": \"trim-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"startTime\": 30,\n    \"endTime\": 60,\n    \"output\": \"mp4\",\n    \"options\": {\n      \"quality\": 720,\n      \"fps\": 30\n    }\n  }\n}\n```\n\n#### 16. Extract Document (`extract-document`)\n\nExtract specific content from documents in various formats.\n\n```json\n{\n  \"name\": \"extract-document\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"format\": \"structured\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\",\n      \"includeMetadata\": true\n    }\n  }\n}\n```\n\n#### 17. Extract Image (`extract-image`)\n\nExtract text and information from images.\n\n```json\n{\n  \"name\": \"extract-image\",\n  \"arguments\": {\n    \"url\": \"https://example.com/image.jpg\",\n    \"extractionType\": \"text\",\n    \"options\": {\n      \"language\": \"en\",\n      \"detectOrientation\": true\n    }\n  }\n}\n```\n\n#### 18. Extract Audio (`extract-audio`)\n\nTranscribe and extract information from audio files.\n\n```json\n{\n  \"name\": \"extract-audio\",\n  \"arguments\": {\n    \"url\": \"https://example.com/audio.mp3\",\n    \"language\": \"en\",\n    \"options\": {\n      \"model\": \"enhanced\",\n      \"speakerDiarization\": true,\n      \"wordTimestamps\": true\n    }\n  }\n}\n```\n\n#### 19. Extract Video (`extract-video`)\n\nExtract content from videos including transcripts, scenes, and objects.\n\n```json\n{\n  \"name\": \"extract-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"extractionType\": \"transcript\",\n    \"options\": {\n      \"language\": \"en\",\n      \"speakerDiarization\": true\n    }\n  }\n}\n```\n\n#### 20. Read PDF Metadata (`read-pdf-metadata`)\n\nExtract metadata from PDF files.\n\n```json\n{\n  \"name\": \"read-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"includeExtended\": true\n  }\n}\n```\n\n#### 21. Write PDF Metadata (`write-pdf-metadata`)\n\nUpdate metadata in PDF files.\n\n```json\n{\n  \"name\": \"write-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"metadata\": {\n      \"title\": \"New Title\",\n      \"author\": \"John Doe\",\n      \"keywords\": [\"keyword1\", \"keyword2\"]\n    }\n  }\n}\n```\n\n### AI\n\n#### 22. Generate Agent Completion (`generate-agent-completion`)\n\nGet AI agent completions with optional tool definitions.\n\n```json\n{\n  \"name\": \"generate-agent-completion\",\n  \"arguments\": {\n    \"prompt\": \"How can I improve my website's SEO?\",\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"maxTokens\": 500,\n    \"context\": [\"The website is an e-commerce store selling handmade crafts.\"]\n  }\n}\n```\n\n#### 23. Search Knowledge Base (`search-knowledge-base`)\n\nSearch a knowledge base for relevant information.\n\n```json\n{\n  \"name\": \"search-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"query\": \"How to optimize database performance\",\n    \"limit\": 5,\n    \"similarityThreshold\": 0.7\n  }\n}\n```\n\n#### 24. Add to Knowledge Base (`add-to-knowledge-base`)\n\nAdd entries to a knowledge base.\n\n```json\n{\n  \"name\": \"add-to-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"entries\": [\n      {\n        \"text\": \"MongoDB is a document-based NoSQL database.\",\n        \"metadata\": {\n          \"source\": \"MongoDB documentation\",\n          \"category\": \"databases\"\n        }\n      }\n    ],\n    \"upsert\": true\n  }\n}\n```\n\n#### 25. Generate AI Image (`generate-ai-image`)\n\nGenerate images using AI models.\n\n```json\n{\n  \"name\": \"generate-ai-image\",\n  \"arguments\": {\n    \"prompt\": \"A futuristic city with flying cars and neon lights\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1,\n    \"quality\": \"hd\",\n    \"style\": \"photorealistic\"\n  }\n}\n```\n\n#### 26. Generate Image (`generate-image`)\n\nGenerate images using various AI providers.\n\n```json\n{\n  \"name\": \"generate-image\",\n  \"arguments\": {\n    \"prompt\": \"A golden retriever in a meadow of wildflowers\",\n    \"provider\": \"dalle\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1\n  }\n}\n```\n\n### Developer Tools\n\n#### 27. Run JavaScript Code (`run-js-code`)\n\nExecute JavaScript code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-js-code\",\n  \"arguments\": {\n    \"code\": \"const result = [1, 2, 3, 4].reduce((sum, num) => sum + num, 0); console.log(`Sum: ${result}`); return result;\",\n    \"dependencies\": {\n      \"lodash\": \"^4.17.21\"\n    },\n    \"timeout\": 5000\n  }\n}\n```\n\n#### 28. Run Python Code (`run-python-code`)\n\nExecute Python code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-python-code\",\n  \"arguments\": {\n    \"code\": \"import numpy as np\\narr = np.array([1, 2, 3, 4, 5])\\nmean = np.mean(arr)\\nprint(f'Mean: {mean}')\\nreturn mean\",\n    \"dependencies\": [\"numpy\", \"pandas\"],\n    \"timeout\": 10000,\n    \"saveOutputFiles\": true\n  }\n}\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages with HTTP status codes\n- API key validation\n- Input validation using Zod schemas\n- Network error handling with descriptive messages\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Failed to fetch YouTube transcript: 404 Not Found\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n```\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dumplingai",
        "scraping",
        "processing",
        "processing dumplingai",
        "server dumplingai",
        "dumplingai mcp"
      ],
      "category": "document-processing"
    },
    "FradSer--mcp-server-to-markdown": {
      "owner": "FradSer",
      "name": "mcp-server-to-markdown",
      "url": "https://github.com/FradSer/mcp-server-to-markdown",
      "imageUrl": "/freedevtools/mcp/pfp/FradSer.webp",
      "description": "Converts various file formats into Markdown descriptions, helping users obtain structured information from documents. Integrates with Cloudflare AI services for efficient document processing.",
      "stars": 34,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-04T14:38:58Z",
      "readme_content": "# MCP Server To Markdown ![](https://img.shields.io/badge/A%20FRAD%20PRODUCT-WIP-yellow)\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/FradSer?style=social)](https://twitter.com/FradSer)\n[![smithery badge](https://smithery.ai/badge/@FradSer/mcp-server-to-markdown)](https://smithery.ai/server/@FradSer/mcp-server-to-markdown)\n\nEnglish | [简体中文](README.zh-CN.md)\n\nA powerful Model Context Protocol (MCP) server that leverages Cloudflare AI services to convert various file formats into Markdown descriptions. This server provides a standardized interface for seamless file conversion and description generation.\n\n## Key Features\n\n- Seamless integration with Cloudflare AI services\n- Efficient Markdown description generation\n- Comprehensive file format support\n- Native Cloudflare tomarkdown API integration\n- User-friendly MCP interface\n- Cross-platform compatibility\n\n## Supported File Formats\n\n| Category | File Extensions |\n|----------|----------------|\n| Documents | .pdf |\n| Images | .jpeg, .jpg, .png, .webp, .svg |\n| Web Content | .html |\n| Data | .xml, .csv |\n| Spreadsheets | .xlsx, .xlsm, .xlsb, .xls, .et, .ods, .numbers |\n\n## System Requirements\n\n- Node.js 18 or later\n- Valid Cloudflare API Token\n- Active Cloudflare Account ID\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markdown转换服务器 for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@FradSer/mcp-server-to-markdown):\n\n```bash\nnpx -y @smithery/cli install @FradSer/mcp-server-to-markdown --client claude\n```\n\n### Manual Installation\nInstall globally using npm:\n\n```bash\nnpm install -g mcp-server-to-markdown\n```\n\n## MCP Client Configuration\n\n### Cursor Integration\n\n1. Navigate to Cursor settings\n2. Select \"MCP\" from the sidebar\n3. Choose \"Add new global MCP server\"\n4. Apply the following configuration:\n    ```json\n    {\n      \"mcpServers\": {\n        \"to-markdown\": {\n          \"command\": \"mcp-server-to-markdown\",\n          \"args\": [\n            \"CLOUDFLARE_API_TOKEN\": \"your_api_token\"\n            \"CLOUDFLARE_ACCOUNT_ID\": \"your_account_id\"\n          ]\n        }\n      }\n    }\n    ```\n\n### Claude Desktop Setup\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"to-markdown\": {\n      \"command\": \"mcp-server-to-markdown\",\n      \"args\": [\n            \"CLOUDFLARE_API_TOKEN\": \"your_api_token\"\n            \"CLOUDFLARE_ACCOUNT_ID\": \"your_account_id\"\n          ]\n    }\n  }\n}\n```\n\n### ChatWise Configuration\n\n1. Launch ChatWise\n2. Access Settings\n3. Select Tools section\n4. Click \"+\" to add new tool\n5. Configure with these parameters:\n   - Type: `stdio`\n   - ID: `to-markdown`\n   - Command: `mcp-server-to-markdown`\n   - Args:\n      ```\n      CLOUDFLARE_API_TOKEN=your_api_token\n      CLOUDFLARE_ACCOUNT_ID=your_account_id\n      ```\n\n## API Reference\n\n### to-markdown Tool\n\nConverts various file formats to Markdown descriptions.\n\n**Input Parameters:**\n- `filePaths`: Array<string> (required) - List of file paths to process\n\n**Response Structure:**\n```json\n[\n  {\n    \"filename\": \"example.pdf\",\n    \"mimeType\": \"application/pdf\",\n    \"description\": \"Generated Markdown description\",\n    \"tokens\": 123\n  }\n]\n```\n\n## Development Guide\n\n### Getting Started\n\n1. Clone and setup environment:\n```bash\ngit clone <repository-url>\ncd mcp-server-to-markdown\ncp .env.example .env\n```\n\n2. Configure Cloudflare credentials:\n```plaintext\nCLOUDFLARE_API_TOKEN=your_api_token\nCLOUDFLARE_ACCOUNT_ID=your_account_id\n```\n\n3. Install dependencies and build:\n```bash\nnpm install\nnpm run build\n```\n\n### Project Structure\n\n```\n.\n├── src/             # Source code\n├── dist/            # Compiled output\n├── types.ts         # Type definitions\n└── .env             # Environment configuration\n```\n\n### Available Scripts\n\n- `npm run build` - Build TypeScript code\n- `npm run inspect` - Run with MCP inspector\n\n## Usage Example\n\n```typescript\nconst result = await toMarkdown({\n  filePaths: [\n    \"/path/to/document.pdf\",\n    \"/path/to/image.jpg\"\n  ]\n});\n```\n\n## License\n\nMIT License\n\nThis project is maintained by [Frad LEE](https://twitter.com/FradSer)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "cloudflare",
        "formats",
        "formats markdown",
        "server markdown",
        "document processing"
      ],
      "category": "document-processing"
    },
    "FutureUnreal--mcp-pdf2md": {
      "owner": "FutureUnreal",
      "name": "mcp-pdf2md",
      "url": "https://github.com/FutureUnreal/mcp-pdf2md",
      "imageUrl": "/freedevtools/mcp/pfp/FutureUnreal.webp",
      "description": "Converts PDF files to structured Markdown format while preserving the original layout. Supports batch processing of local files and URLs for efficient handling of multiple documents.",
      "stars": 17,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-09T05:32:40Z",
      "readme_content": "# MCP-PDF2MD\n\n[![smithery badge](https://smithery.ai/badge/@FutureUnreal/mcp-pdf2md)](https://smithery.ai/server/@FutureUnreal/mcp-pdf2md)\n[English](#pdf2md-service) | [中文](README_CN.md)\n\n# MCP-PDF2MD Service\n\nAn MCP-based high-performance PDF to Markdown conversion service powered by MinerU API, supporting batch processing for local files and URL links with structured output.\n\n## Key Features\n\n- Format Conversion: Convert PDF files to structured Markdown format.\n- Multi-source Support: Process both local PDF files and URL links.\n- Intelligent Processing: Automatically select the best processing method.\n- Batch Processing: Support multi-file batch conversion for efficient handling of large volumes of PDF files.\n- MCP Integration: Seamless integration with LLM clients like Claude Desktop.\n- Structure Preservation: Maintain the original document structure, including headings, paragraphs, lists, etc.\n- Smart Layout: Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.\n- Formula Conversion: Automatically recognize and convert formulas in the document to LaTeX format.\n- Table Extraction: Automatically recognize and convert tables in the document to structured format.\n- Cleanup Optimization: Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.\n- High-Quality Extraction: High-quality extraction of text, images, and layout information from PDF documents.\n\n## System Requirements\n\n- Software: Python 3.10+\n\n## Quick Start\n\n1. Clone the repository and enter the directory:\n   ```bash\n   git clone https://github.com/FutureUnreal/mcp-pdf2md.git\n   cd mcp-pdf2md\n   ```\n\n2. Create a virtual environment and install dependencies:\n   \n   **Linux/macOS**:\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   uv pip install -e .\n   ```\n   \n   **Windows**:\n   ```bash\n   uv venv\n   .venv\\Scripts\\activate\n   uv pip install -e .\n   ```\n\n3. Configure environment variables:\n\n   Create a `.env` file in the project root directory and set the following environment variables:\n   ```\n   MINERU_API_BASE=https://mineru.net/api/v4/extract/task\n   MINERU_BATCH_API=https://mineru.net/api/v4/extract/task/batch\n   MINERU_BATCH_RESULTS_API=https://mineru.net/api/v4/extract-results/batch\n   MINERU_API_KEY=your_api_key_here\n   ```\n\n4. Start the service:\n   ```bash\n   uv run pdf2md\n   ```\n\n## Command Line Arguments\n\nThe server supports the following command line arguments:\n\n## Claude Desktop Configuration\n\nAdd the following configuration in Claude Desktop:\n\n**Windows**:\n```json\n{\n    \"mcpServers\": {\n        \"pdf2md\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"C:\\\\path\\\\to\\\\mcp-pdf2md\",\n                \"run\",\n                \"pdf2md\",\n                \"--output-dir\",\n                \"C:\\\\path\\\\to\\\\output\"\n            ],\n            \"env\": {\n                \"MINERU_API_KEY\": \"your_api_key_here\"\n            }\n        }\n    }\n}\n```\n\n**Linux/macOS**:\n```json\n{\n    \"mcpServers\": {\n        \"pdf2md\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/mcp-pdf2md\",\n                \"run\",\n                \"pdf2md\",\n                \"--output-dir\",\n                \"/path/to/output\"\n            ],\n            \"env\": {\n                \"MINERU_API_KEY\": \"your_api_key_here\"\n            }\n        }\n    }\n}\n```\n\n**Note about API Key Configuration:**\nYou can set the API key in two ways:\n1. In the `.env` file within the project directory (recommended for development)\n2. In the Claude Desktop configuration as shown above (recommended for regular use)\n\nIf you set the API key in both places, the one in the Claude Desktop configuration will take precedence.\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n- **convert_pdf_url**: Convert PDF URL to Markdown\n- **convert_pdf_file**: Convert local PDF file to Markdown\n\n## Getting MinerU API Key\n\nThis project relies on the MinerU API for PDF content extraction. To obtain an API key:\n\n1. Visit [MinerU official website](https://mineru.net/) and register for an account\n2. After logging in, apply for API testing qualification at [this link](https://mineru.net/apiManage/docs?openApplyModal=true)\n3. Once your application is approved, you can access the [API Management](https://mineru.net/apiManage/token) page\n4. Generate your API key following the instructions provided\n5. Copy the generated API key\n6. Use this string as the value for `MINERU_API_KEY`\n\nNote that access to the MinerU API is currently in testing phase and requires approval from the MinerU team. The approval process may take some time, so plan accordingly.\n\n## Demo\n\n### Input PDF\n![Input PDF](images/input.png)\n\n### Output Markdown\n![Output Markdown](images/output.png)\n\n## License\n\nMIT License - see the LICENSE file for details.\n\n## Credits\n\nThis project is based on the API from [MinerU](https://github.com/opendatalab/MinerU/tree/master).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdf2md",
        "pdf",
        "markdown",
        "pdf2md converts",
        "mcp pdf2md",
        "pdf files"
      ],
      "category": "document-processing"
    },
    "Geeksfino--kb-mcp-server": {
      "owner": "Geeksfino",
      "name": "kb-mcp-server",
      "url": "https://github.com/Geeksfino/kb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Geeksfino.webp",
      "description": "Provides semantic search capabilities, builds and queries knowledge graphs, and facilitates AI-driven text processing through a standardized interface. Integrates vector databases with relational databases for enhanced data utilization.",
      "stars": 49,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-31T17:38:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/geeksfino-kb-mcp-server-badge.png)](https://mseep.ai/app/geeksfino-kb-mcp-server)\n\n# Embedding MCP Server\n\nA Model Context Protocol (MCP) server implementation powered by txtai, providing semantic search, knowledge graph capabilities, and AI-driven text processing through a standardized interface.\n\n## The Power of txtai: All-in-one Embeddings Database\n\nThis project leverages [txtai](https://github.com/neuml/txtai), an all-in-one embeddings database for RAG leveraging semantic search, knowledge graph construction, and language model workflows. txtai offers several key advantages:\n\n- **Unified Vector Database**: Combines vector indexes, graph networks, and relational databases in a single platform\n- **Semantic Search**: Find information based on meaning, not just keywords\n- **Knowledge Graph Integration**: Automatically build and query knowledge graphs from your data\n- **Portable Knowledge Bases**: Save entire knowledge bases as compressed archives (.tar.gz) that can be easily shared and loaded\n- **Extensible Pipeline System**: Process text, documents, audio, images, and video through a unified API\n- **Local-first Architecture**: Run everything locally without sending data to external services\n\n## How It Works\n\nThe project contains a knowledge base builder tool and a MCP server. The knowledge base builder tool is a command-line interface for creating and managing knowledge bases. The MCP server provides a standardized interface to access the knowledge base. \n\nIt is not required to use the knowledge base builder tool to build a knowledge base. You can always build a knowledge base using txtai's programming interface by writing a Python script or even using a jupyter notebook. As long as the knowledge base is built using txtai, it can be loaded by the MCP server. Better yet, the knowledge base can be a folder on the file system or an exported .tar.gz file. Just give it to the MCP server and it will load it.\n\n### 1. Build a Knowledge Base with kb_builder\n\nThe `kb_builder` module provides a command-line interface for creating and managing knowledge bases:\n\n- Process documents from various sources (files, directories, JSON)\n- Extract text and create embeddings\n- Build knowledge graphs automatically\n- Export portable knowledge bases\n\nNote it is possibly limited in functionality and currently only provided for convenience.\n\n### 2. Start the MCP Server\n\nThe MCP server provides a standardized interface to access the knowledge base:\n\n- Semantic search capabilities\n- Knowledge graph querying and visualization\n- Text processing pipelines (summarization, extraction, etc.)\n- Full compliance with the Model Context Protocol\n\n## Installation\n\n### Recommended: Using uv with Python 3.10+\n\nWe recommend using [uv](https://github.com/astral-sh/uv) with Python 3.10 or newer for the best experience. This provides better dependency management and ensures consistent behavior.\n\n```bash\n# Install uv if you don't have it already\npip install -U uv\n\n# Create a virtual environment with Python 3.10 or newer\nuv venv --python=3.10  # or 3.11, 3.12, etc.\n\n# Activate the virtual environment (bash/zsh)\nsource .venv/bin/activate\n# For fish shell\n# source .venv/bin/activate.fish\n\n# Install from PyPI\nuv pip install kb-mcp-server\n```\n\n> **Note**: We pin transformers to version 4.49.0 to avoid deprecation warnings about `transformers.agents.tools` that appear in version 4.50.0 and newer. If you use a newer version of transformers, you may see these warnings, but they don't affect functionality.\n\n### Using conda\n\n```bash\n# Create a new conda environment (optional)\nconda create -n embedding-mcp python=3.10\nconda activate embedding-mcp\n\n# Install from PyPI\npip install kb-mcp-server\n```\n\n### From Source\n\n```bash\n# Create a new conda environment\nconda create -n embedding-mcp python=3.10\nconda activate embedding-mcp\n\n# Clone the repository\ngit clone https://github.com/Geeksfino/kb-mcp-server.git.git\ncd kb-mcp-server\n\n# Install dependencies\npip install -e .\n```\n\n### Using uv (Faster Alternative)\n\n```bash\n# Install uv if not already installed\npip install uv\n\n# Create a new virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Option 1: Install from PyPI\nuv pip install kb-mcp-server\n\n# Option 2: Install from source (for development)\nuv pip install -e .\n```\n\n### Using uvx (No Installation Required)\n\n[uvx](https://github.com/astral-sh/uv) allows you to run packages directly from PyPI without installing them:\n\n```bash\n# Run the MCP server\nuvx --from kb-mcp-server@0.3.0 kb-mcp-server --embeddings /path/to/knowledge_base\n\n# Build a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --config config.yml\n\n# Search a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"Your search query\"\n```\n\n## Command Line Usage\n\n### Building a Knowledge Base\n\nYou can use the command-line tools installed from PyPI, the Python module directly, or the convenient shell scripts:\n\n#### Using the PyPI Installed Commands\n\n```bash\n# Build a knowledge base from documents\nkb-build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\nkb-build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\nkb-build --input /path/to/documents --export my_knowledge_base.tar.gz\n\n# Search a knowledge base\nkb-search /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\nkb-search /path/to/knowledge_base \"What is machine learning?\" --graph --limit 10\n```\n\n#### Using uvx (No Installation Required)\n\n```bash\n# Build a knowledge base from documents\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --export my_knowledge_base.tar.gz\n\n# Search a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"What is machine learning?\" --graph --limit 10\n```\n\n#### Using the Python Module\n\n```bash\n# Build a knowledge base from documents\npython -m kb_builder build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\npython -m kb_builder build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\npython -m kb_builder build --input /path/to/documents --export my_knowledge_base.tar.gz\n```\n\n#### Using the Convenience Scripts\n\nThe repository includes convenient wrapper scripts that make it easier to build and search knowledge bases:\n\n```bash\n# Build a knowledge base using a template configuration\n./scripts/kb_build.sh /path/to/documents technical_docs\n\n# Build using a custom configuration file\n./scripts/kb_build.sh /path/to/documents /path/to/my_config.yml\n\n# Update an existing knowledge base\n./scripts/kb_build.sh /path/to/documents technical_docs --update\n\n# Search a knowledge base\n./scripts/kb_search.sh /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\n./scripts/kb_search.sh /path/to/knowledge_base \"What is machine learning?\" --graph\n```\n\nRun `./scripts/kb_build.sh --help` or `./scripts/kb_search.sh --help` for more options.\n\n### Starting the MCP Server\n\n#### Using the PyPI Installed Command\n\n```bash\n# Start with a specific knowledge base folder\nkb-mcp-server --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\nkb-mcp-server --embeddings /path/to/knowledge_base.tar.gz\n```\n\n#### Using uvx (No Installation Required)\n\n```bash\n# Start with a specific knowledge base folder\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base.tar.gz\n```\n\n#### Using the Python Module\n\n```bash\n# Start with a specific knowledge base folder\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base.tar.gz\n```\n## MCP Server Configuration\n\nThe MCP server is configured using environment variables or command-line arguments, not YAML files. YAML files are only used for configuring txtai components during knowledge base building.\n\nHere's how to configure the MCP server:\n\n```bash\n# Start the server with command-line arguments\nkb-mcp-server --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or using uvx (no installation required)\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or using the Python module\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or use environment variables\nexport TXTAI_EMBEDDINGS=/path/to/knowledge_base\nexport MCP_SSE_HOST=0.0.0.0\nexport MCP_SSE_PORT=8000\npython -m txtai_mcp_server\n```\n\nCommon configuration options:\n- `--embeddings`: Path to the knowledge base (required)\n- `--host`: Host address to bind to (default: localhost)\n- `--port`: Port to listen on (default: 8000)\n- `--transport`: Transport to use, either 'sse' or 'stdio' (default: stdio)\n- `--enable-causal-boost`: Enable causal boost feature for enhanced relevance scoring\n- `--causal-config`: Path to custom causal boost configuration YAML file\n\n## Configuring LLM Clients to Use the MCP Server\n\nTo configure an LLM client to use the MCP server, you need to create an MCP configuration file. Here's an example `mcp_config.json`:\n\n### Using the server directly\n\nIf you use a virtual Python environment to install the server, you can use the following configuration - note that MCP host like Claude will not be able to connect to the server if you use a virtual environment, you need to use the absolute path to the Python executable of the virtual environment where you did \"pip install\" or \"uv pip install\", for example\n\n```json\n{\n  \"mcpServers\": {\n    \"kb-server\": {\n      \"command\": \"/your/home/project/.venv/bin/kb-mcp-server\",\n      \"args\": [\n        \"--embeddings\", \n        \"/path/to/knowledge_base.tar.gz\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n  }\n}\n```\n\n### Using system default Python\n\nIf you use your system default Python, you can use the following configuration:\n\n```json\n{\n    \"rag-server\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\",\n        \"txtai_mcp_server\",\n        \"--embeddings\",\n        \"/path/to/knowledge_base.tar.gz\",\n        \"--enable-causal-boost\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n}\n```\n\nAlternatively, if you're using uvx, assuming you have uvx installed in your system via \"brew install uvx\" etc, or you 've installed uvx and made it globally accessible via:\n```\n# Create a symlink to /usr/local/bin (which is typically in the system PATH)\nsudo ln -s /Users/cliang/.local/bin/uvx /usr/local/bin/uvx\n```\nThis creates a symbolic link from your user-specific installation to a system-wide location. For macOS applications like Claude Desktop, you can modify the system-wide PATH by creating or editing a launchd configuration file:\n```\n# Create a plist file to set environment variables for all GUI applications\nsudo nano /Library/LaunchAgents/environment.plist\n```\nAdd this content:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  <key>Label</key>\n  <string>my.startup</string>\n  <key>ProgramArguments</key>\n  <array>\n    <string>sh</string>\n    <string>-c</string>\n    <string>launchctl setenv PATH $PATH:/Users/cliang/.local/bin</string>\n  </array>\n  <key>RunAtLoad</key>\n  <true/>\n</dict>\n</plist>\n```\n\nThen load it:\n```\nsudo launchctl load -w /Library/LaunchAgents/environment.plist\n```\nYou'll need to restart your computer for this to take effect, though.\n\n\n```json\n{\n  \"mcpServers\": {\n    \"kb-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"kb-mcp-server@0.2.6\",\n        \"--embeddings\", \"/path/to/knowledge_base\",\n        \"--host\", \"localhost\",\n        \"--port\", \"8000\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n  }\n}\n```\n\nPlace this configuration file in a location accessible to your LLM client and configure the client to use it. The exact configuration steps will depend on your specific LLM client.\n\n## Advanced Knowledge Base Configuration\n\nBuilding a knowledge base with txtai requires a YAML configuration file that controls various aspects of the embedding process. This configuration is used by the `kb_builder` tool, not the MCP server itself.\n\nOne may need to tune segmentation/chunking strategies, embedding models, and scoring methods, as well as configure graph construction, causal boosting, weights of hybrid search, and more.\n\nFortunately, txtai provides a powerful YAML configuration system that requires no coding. Here's an example of a comprehensive configuration for knowledge base building:\n\n```yaml\n# Path to save/load embeddings index\npath: ~/.txtai/embeddings\nwritable: true\n\n# Content storage in SQLite\ncontent:\n  path: sqlite:///~/.txtai/content.db\n\n# Embeddings configuration\nembeddings:\n  # Model settings\n  path: sentence-transformers/nli-mpnet-base-v2\n  backend: faiss\n  gpu: true\n  batch: 32\n  normalize: true\n  \n  # Scoring settings\n  scoring: hybrid\n  hybridalpha: 0.75\n\n# Pipeline configuration\npipeline:\n  workers: 2\n  queue: 100\n  timeout: 300\n\n# Question-answering pipeline\nextractor:\n  path: distilbert-base-cased-distilled-squad\n  maxlength: 512\n  minscore: 0.3\n\n# Graph configuration\ngraph:\n  backend: sqlite\n  path: ~/.txtai/graph.db\n  similarity: 0.75  # Threshold for creating graph connections\n  limit: 10  # Maximum connections per node\n```\n\n### Configuration Examples\n\nThe `src/kb_builder/configs` directory contains configuration templates for different use cases and storage backends:\n\n#### Storage and Backend Configurations\n- `memory.yml`: In-memory vectors (fastest for development, no persistence)\n- `sqlite-faiss.yml`: SQLite for content + FAISS for vectors (local file-based persistence)\n- `postgres-pgvector.yml`: PostgreSQL + pgvector (production-ready with full persistence)\n\n#### Domain-Specific Configurations\n- `base.yml`: Base configuration template\n- `code_repositories.yml`: Optimized for code repositories\n- `data_science.yml`: Configured for data science documents\n- `general_knowledge.yml`: General purpose knowledge base\n- `research_papers.yml`: Optimized for academic papers\n- `technical_docs.yml`: Configured for technical documentation\n\nYou can use these as starting points for your own configurations:\n\n```bash\npython -m kb_builder build --input /path/to/documents --config src/kb_builder/configs/technical_docs.yml\n\n# Or use a storage-specific configuration\npython -m kb_builder build --input /path/to/documents --config src/kb_builder/configs/postgres-pgvector.yml\n```\n\n## Advanced Features\n\n### Knowledge Graph Capabilities\n\nThe MCP server leverages txtai's built-in graph functionality to provide powerful knowledge graph capabilities:\n\n- **Automatic Graph Construction**: Build knowledge graphs from your documents automatically\n- **Graph Traversal**: Navigate through related concepts and documents\n- **Path Finding**: Discover connections between different pieces of information\n- **Community Detection**: Identify clusters of related information\n\n### Causal Boosting Mechanism\n\nThe MCP server includes a sophisticated causal boosting mechanism that enhances search relevance by identifying and prioritizing causal relationships:\n\n- **Pattern Recognition**: Detects causal language patterns in both queries and documents\n- **Multilingual Support**: Automatically applies appropriate patterns based on detected query language\n- **Configurable Boost Multipliers**: Different types of causal matches receive customizable boost factors\n- **Enhanced Relevance**: Results that explain causal relationships are prioritized in search results\n\nThis mechanism significantly improves responses to \"why\" and \"how\" questions by surfacing content that explains relationships between concepts. The causal boosting configuration is highly customizable through YAML files, allowing adaptation to different domains and languages.\n\n\n## License\n\nMIT License - see LICENSE file for details\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "semantic",
        "queries",
        "semantic search",
        "document processing",
        "text processing"
      ],
      "category": "document-processing"
    },
    "GongRzhe--Langflow-DOC-QA-SERVER": {
      "owner": "GongRzhe",
      "name": "Langflow-DOC-QA-SERVER",
      "url": "https://github.com/GongRzhe/Langflow-DOC-QA-SERVER",
      "imageUrl": "/freedevtools/mcp/pfp/GongRzhe.webp",
      "description": "Query documents using a Q&A system to retrieve precise answers efficiently. The server leverages a Langflow backend for enhanced document management and interaction.",
      "stars": 14,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-30T02:34:58Z",
      "readme_content": "# Langflow-DOC-QA-SERVER\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Langflow-DOC-QA-SERVER)](https://smithery.ai/server/@GongRzhe/Langflow-DOC-QA-SERVER)\n\n<a href=\"https://glama.ai/mcp/servers/@GongRzhe/Langflow-DOC-QA-SERVER\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@GongRzhe/Langflow-DOC-QA-SERVER/badge\" alt=\"Langflow Document Q&A Server MCP server\" />\n</a>\n\nA Model Context Protocol server for document Q&A powered by Langflow\n\nThis is a TypeScript-based MCP server that implements a document Q&A system. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n\n## Prerequisites\n\n### 1. Create Langflow Document Q&A Flow\n1. Open Langflow and create a new flow from the \"Document Q&A\" template\n2. Configure your flow with necessary components (ChatInput, File Upload, LLM, etc.)\n3. Save your flow\n\n![image](https://github.com/user-attachments/assets/0df89122-d7a8-4d18-9a39-57af4240b7ac)\n\n\n### 2. Get Flow API Endpoint\n1. Click the \"API\" button in the top right corner of Langflow\n2. Copy the API endpoint URL from the cURL command\n   Example: `http://127.0.0.1:7860/api/v1/run/<flow-id>?stream=false`\n3. Save this URL as it will be needed for the `API_ENDPOINT` configuration\n\n![image](https://github.com/user-attachments/assets/6c9ba5e2-4aa3-4a8c-89c2-adc3d400c828)\n\n\n## Features\n\n### Tools\n- `query_docs` - Query the document Q&A system\n  - Takes a query string as input\n  - Returns responses from the Langflow backend\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"langflow-doc-qa-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/doc-qa-server/build/index.js\"\n      ],\n      \"env\": {\n        \"API_ENDPOINT\": \"http://127.0.0.1:7860/api/v1/run/480ec7b3-29d2-4caa-b03b-e74118f35fac\"\n      }\n    }\n  }\n}\n```\n\n![image](https://github.com/user-attachments/assets/b0821378-ed13-4225-81a9-8beab1dc4b48)\n\n### Installing via Smithery\n\nTo install Document Q&A Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Langflow-DOC-QA-SERVER):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Langflow-DOC-QA-SERVER --client claude\n```\n\n### Environment Variables\n\nThe server supports the following environment variables for configuration:\n\n- `API_ENDPOINT`: The endpoint URL for the Langflow API service. Defaults to `http://127.0.0.1:7860/api/v1/run/480ec7b3-29d2-4caa-b03b-e74118f35fac` if not specified.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## 📜 License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "langflow",
        "documents",
        "document",
        "document processing",
        "query documents",
        "langflow backend"
      ],
      "category": "document-processing"
    },
    "GongRzhe--Office-PowerPoint-MCP-Server": {
      "owner": "GongRzhe",
      "name": "Office-PowerPoint-MCP-Server",
      "url": "https://github.com/GongRzhe/Office-PowerPoint-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/GongRzhe.webp",
      "description": "Create, edit, and manipulate PowerPoint presentations using various automation tools. Streamlines workflow by providing functionalities to enhance presentation tasks.",
      "stars": 1063,
      "forks": 133,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T17:00:09Z",
      "readme_content": "# Office-PowerPoint-MCP-Server\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Office-PowerPoint-MCP-Server)](https://smithery.ai/server/@GongRzhe/Office-PowerPoint-MCP-Server)\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n\nA comprehensive MCP (Model Context Protocol) server for PowerPoint manipulation using python-pptx. **Version 2.0** provides 32 powerful tools organized into 11 specialized modules, offering complete PowerPoint creation, management, and professional design capabilities. The server features a modular architecture with enhanced parameter handling, intelligent operation selection, and comprehensive error handling.\n\n----\n\n# **Not so ugly anymore with new slide_layout_templates**\n\n<img width=\"1509\" alt=\"截屏2025-06-20 15 53 45\" src=\"https://github.com/user-attachments/assets/197d82cb-017a-4c00-b969-6e40440ffa36\" />\n\n----\n\n### Example\n\n#### Pormpt\n\n<img width=\"1280\" alt=\"650f4cc5d0f1ea4f3b1580800cb0deb\" src=\"https://github.com/user-attachments/assets/90633c97-f373-4c85-bc9c-a1d7b891c344\" />\n\n#### Output\n\n<img width=\"1640\" alt=\"084f1cf4bc7e4fcd4890c8f94f536c1\" src=\"https://github.com/user-attachments/assets/420e63a0-15a4-46d8-b149-1408d23af038\" />\n\n#### Demo's GIF -> (./public/demo.mp4)\n\n![demo](./public/demo.gif)\n\n## Features\n\n### Core PowerPoint Operations\n- **Round-trip support** for any Open XML presentation (.pptx file) including all elements\n- **Template support** with automatic theme and layout preservation\n- **Multi-presentation management** with global state tracking\n- **Core document properties** management (title, subject, author, keywords, comments)\n\n### Content Creation & Management\n- **Slide management** with flexible layout selection\n- **Text manipulation** with placeholder population and bullet point creation\n- **Advanced text formatting** with font, color, alignment, and style controls\n- **Text validation** with automatic fit checking and optimization suggestions\n\n### Visual Elements\n- **Image handling** with file and base64 input support\n- **Image enhancement** using Pillow with brightness, contrast, saturation, and filter controls\n- **Professional image effects** including shadows, reflections, glows, and soft edges\n- **Shape creation** with 20+ auto shape types (rectangles, ovals, flowchart elements, etc.)\n- **Table creation** with advanced cell formatting and styling\n\n### Charts & Data Visualization\n- **Chart support** for column, bar, line, and pie charts\n- **Data series management** with categories and multiple series support\n- **Chart formatting** with legends, data labels, and titles\n\n### Professional Design Features\n- **4 professional color schemes** (Modern Blue, Corporate Gray, Elegant Green, Warm Red)\n- **Professional typography** with Segoe UI font family and size presets\n- **Theme application** with automatic styling across presentations\n- **Gradient backgrounds** with customizable directions and color schemes\n- **Slide enhancement** tools for existing content\n- **25 built-in slide templates** with dynamic sizing and visual effects\n- **Advanced template features** including auto-wrapping, dynamic font sizing, and professional animations\n\n### Advanced Features\n- **Font analysis and optimization** using FontTools\n- **Picture effects** with 9 different visual effects (shadow, reflection, glow, bevel, etc.)\n- **Comprehensive validation** with automatic error fixing\n- **Template search** with configurable directory paths\n- **Professional layout calculations** with margin and spacing management\n\n## Installation\n\n### Installing via Smithery\n\nTo install PowerPoint Manipulation Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Office-PowerPoint-MCP-Server):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Office-PowerPoint-MCP-Server --client claude\n```\n\n### Prerequisites\n\n- Python 3.6 or higher (as specified in pyproject.toml)\n- pip package manager\n- Optional: uvx for package execution without local installation\n\n### Installation Options\n\n#### Option 1: Using the Setup Script (Recommended)\n\nThe easiest way to set up the PowerPoint MCP Server is using the provided setup script, which automates the installation process:\n\n```bash\npython setup_mcp.py\n```\n\nThis script will:\n- Check prerequisites\n- Offer installation options:\n  - Install from PyPI (recommended for most users)\n  - Set up local development environment\n- Install required dependencies\n- Generate the appropriate MCP configuration file\n- Provide instructions for integrating with Claude Desktop\n\nThe script offers different paths based on your environment:\n- If you have `uvx` installed, it will configure using UVX (recommended)\n- If the server is already installed, it provides configuration options\n- If the server is not installed, it offers installation methods\n\n#### Option 2: Manual Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/GongRzhe/Office-PowerPoint-MCP-Server.git\n   cd Office-PowerPoint-MCP-Server\n   ```\n\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Make the server executable:\n   ```bash\n   chmod +x ppt_mcp_server.py\n   ```\n\n## Usage\n\nDisplay help text:\n```bash\npython ppt_mcp_server.py -h\n```\n\n### Starting the Stdio Server\n\nRun the stdio server:\n\n```bash\npython ppt_mcp_server.py\n```\n\n### Starting the Streamable-Http Server\n\nRun the streamable-http server on port 8000:\n\n```bash\npython ppt_mcp_server.py --transport http --port 8000\n```\n\nRun in Docker\n```bash\ndocker build -t ppt_mcp_server .\ndocker run -d --rm -p 8000:8000 ppt_mcp_server -t http\n```\n\n\n### MCP Configuration\n\n#### Option 1: Local Python Server\n\nAdd the server to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"ppt\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/ppt_mcp_server.py\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n#### Option 2: Using UVX (No Local Installation Required)\n\nIf you have `uvx` installed, you can run the server directly from PyPI without local installation:\n\n```json\n{\n  \"mcpServers\": {\n    \"ppt\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\", \"office-powerpoint-mcp-server\", \"ppt_mcp_server\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## 🚀 What's New in v2.0\n\n### **Comprehensive Tool Suite (32 Tools)**\n- **Complete PowerPoint manipulation** with 34 specialized tools\n- **11 organized modules** covering all aspects of presentation creation\n- **Enhanced parameter handling** with comprehensive validation\n- **Intelligent defaults** and operation-based interfaces\n\n### **Built-in Slide Templates**\n- **25+ professional slide templates** with dynamic features built-in\n- **Advanced template system** with auto-generation capabilities\n- **Auto-sizing text** that adapts to content length and container size\n- **Professional visual effects** including shadows, glows, and gradients\n- **Complete presentation generation** from template sequences\n\n### **Modular Architecture**\n- **11 specialized modules**: presentation, content, structural, professional, template, hyperlink, chart, connector, master, and transition tools\n- **Better maintainability** with separated concerns\n- **Easier extensibility** for adding new features\n- **Cleaner code structure** with shared utilities\n\n## Available Tools\n\nThe server provides **34 specialized tools** organized into the following categories:\n\n### **Presentation Management (7 tools)**\n1. **create_presentation** - Create new presentations\n2. **create_presentation_from_template** - Create from templates with theme preservation\n3. **open_presentation** - Open existing presentations\n4. **save_presentation** - Save presentations to files\n5. **get_presentation_info** - Get comprehensive presentation information\n6. **get_template_file_info** - Analyze template files and layouts\n7. **set_core_properties** - Set document properties\n\n### **Content Management (8 tools)**\n8. **add_slide** - Add slides with optional background styling\n9. **get_slide_info** - Get detailed slide information\n10. **extract_slide_text** - ✨ **NEW** Extract all text content from a specific slide\n11. **extract_presentation_text** - ✨ **NEW** Extract text content from all slides in presentation\n12. **populate_placeholder** - Populate placeholders with text\n13. **add_bullet_points** - Add formatted bullet points\n14. **manage_text** - ✨ **Unified text tool** (add/format/validate/format_runs)\n15. **manage_image** - ✨ **Unified image tool** (add/enhance)\n\n### **Template Operations (7 tools)**\n16. **list_slide_templates** - Browse available slide layout templates\n17. **apply_slide_template** - Apply structured layout templates to existing slides\n18. **create_slide_from_template** - Create new slides using layout templates\n19. **create_presentation_from_templates** - Create complete presentations from template sequences\n20. **get_template_info** - Get detailed information about specific templates\n21. **auto_generate_presentation** - Automatically generate presentations based on topic\n22. **optimize_slide_text** - Optimize text elements for better readability and fit\n\n### **Structural Elements (4 tools)**\n23. **add_table** - Create tables with enhanced formatting\n24. **format_table_cell** - Format individual table cells\n25. **add_shape** - Add shapes with text and formatting options\n26. **add_chart** - Create charts with comprehensive customization\n\n### **Professional Design (3 tools)**\n27. **apply_professional_design** - ✨ **Unified design tool** (themes/slides/enhancement)\n28. **apply_picture_effects** - ✨ **Unified effects tool** (9+ effects combined)\n29. **manage_fonts** - ✨ **Unified font tool** (analyze/optimize/recommend)\n\n### **Specialized Features (5 tools)**\n30. **manage_hyperlinks** - Complete hyperlink management (add/remove/list/update)\n31. **manage_slide_masters** - Access and manage slide master properties and layouts\n32. **add_connector** - Add connector lines/arrows between points on slides\n33. **update_chart_data** - Replace existing chart data with new categories and series\n34. **manage_slide_transitions** - Basic slide transition management\n\n## 🌟 Key Unified Tools\n\n### **`manage_text`** - All-in-One Text Management\n```python\n# Add text box\nmanage_text(slide_index=0, operation=\"add\", text=\"Hello World\", font_size=24)\n\n# Format existing text\nmanage_text(slide_index=0, operation=\"format\", shape_index=0, bold=True, color=[255,0,0])\n\n# Validate text fit with auto-fix\nmanage_text(slide_index=0, operation=\"validate\", shape_index=0, validation_only=False)\n```\n\n### **`manage_image`** - Complete Image Handling\n```python\n# Add image with enhancement\nmanage_image(slide_index=0, operation=\"add\", image_source=\"logo.png\", \n            enhancement_style=\"presentation\")\n\n# Enhance existing image\nmanage_image(slide_index=0, operation=\"enhance\", image_source=\"photo.jpg\",\n            brightness=1.2, contrast=1.1, saturation=1.3)\n```\n\n### **`apply_picture_effects`** - Multiple Effects in One Call\n```python\n# Apply combined effects\napply_picture_effects(slide_index=0, shape_index=0, effects={\n    \"shadow\": {\"blur_radius\": 4.0, \"color\": [128,128,128]},\n    \"glow\": {\"size\": 5.0, \"color\": [0,176,240]},\n    \"rotation\": {\"rotation\": 15.0}\n})\n```\n\n### **`apply_professional_design`** - Theme & Design Management\n```python\n# Add professional slide\napply_professional_design(operation=\"slide\", slide_type=\"title_content\", \n                         color_scheme=\"modern_blue\", title=\"My Presentation\")\n\n# Apply theme to entire presentation  \napply_professional_design(operation=\"theme\", color_scheme=\"corporate_gray\")\n\n# Enhance existing slide\napply_professional_design(operation=\"enhance\", slide_index=0, color_scheme=\"elegant_green\")\n```\n\n## Examples\n\n### Creating a New Presentation\n\n```python\n# Create a new presentation\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"create_presentation\",\n    arguments={}\n)\npresentation_id = result[\"presentation_id\"]\n\n# Add a title slide\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"add_slide\",\n    arguments={\n        \"layout_index\": 0,  # Title slide layout\n        \"title\": \"My Presentation\",\n        \"presentation_id\": presentation_id\n    }\n)\nslide_index = result[\"slide_index\"]\n\n# Populate subtitle placeholder\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"populate_placeholder\",\n    arguments={\n        \"slide_index\": slide_index,\n        \"placeholder_idx\": 1,  # Subtitle placeholder\n        \"text\": \"Created with PowerPoint MCP Server\",\n        \"presentation_id\": presentation_id\n    }\n)\n\n# Save the presentation\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"save_presentation\",\n    arguments={\n        \"file_path\": \"my_presentation.pptx\",\n        \"presentation_id\": presentation_id\n    }\n)\n```\n\n### Creating a Professional Presentation with v2.0\n\n```python\n# Create a professional slide with modern styling - CONSOLIDATED TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"apply_professional_design\",\n    arguments={\n        \"operation\": \"slide\",\n        \"slide_type\": \"title_content\",\n        \"color_scheme\": \"modern_blue\",\n        \"title\": \"Quarterly Business Review\",\n        \"content\": [\n            \"Revenue increased by 15% compared to last quarter\",\n            \"Customer satisfaction scores reached all-time high of 94%\",\n            \"Successfully launched 3 new product features\",\n            \"Expanded team by 12 new talented professionals\"\n        ]\n    }\n)\n\n# Apply professional theme to entire presentation - SAME TOOL, DIFFERENT OPERATION\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"apply_professional_design\",\n    arguments={\n        \"operation\": \"theme\",\n        \"color_scheme\": \"modern_blue\",\n        \"apply_to_existing\": True\n    }\n)\n\n# Add slide with gradient background - ENHANCED ADD_SLIDE\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"add_slide\",\n    arguments={\n        \"layout_index\": 0,\n        \"background_type\": \"professional_gradient\",\n        \"color_scheme\": \"modern_blue\",\n        \"gradient_direction\": \"diagonal\"\n    }\n)\n```\n\n### Working with Built-in Slide Templates (New in v2.0)\n\n```python\n# List all available slide templates with their features\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"list_slide_templates\",\n    arguments={}\n)\n\n# Apply a professional template to an existing slide\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"apply_slide_template\",\n    arguments={\n        \"slide_index\": 0,\n        \"template_id\": \"title_slide\",\n        \"color_scheme\": \"modern_blue\",\n        \"content_mapping\": {\n            \"title\": \"Quarterly Business Review\",\n            \"subtitle\": \"Q4 2024 Results\",\n            \"author\": \"Leadership Team\"\n        }\n    }\n)\n\n# Create a new slide using a template\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"create_slide_from_template\",\n    arguments={\n        \"template_id\": \"text_with_image\",\n        \"color_scheme\": \"elegant_green\",\n        \"content_mapping\": {\n            \"title\": \"Our Revolutionary Solution\",\n            \"content\": \"• 250% increase in efficiency\\n• 98% customer satisfaction\\n• Industry-leading performance\"\n        },\n        \"image_paths\": {\n            \"supporting\": \"path/to/product_image.jpg\"\n        }\n    }\n)\n\n# Generate a complete presentation from multiple templates\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"create_presentation_from_templates\",\n    arguments={\n        \"template_sequence\": [\n            {\n                \"template_id\": \"title_slide\",\n                \"content\": {\n                    \"title\": \"2024 Annual Report\",\n                    \"subtitle\": \"Growth and Innovation\",\n                    \"author\": \"Executive Team\"\n                }\n            },\n            {\n                \"template_id\": \"key_metrics_dashboard\",\n                \"content\": {\n                    \"metric_1_value\": \"94%\",\n                    \"metric_2_value\": \"$2.4M\",\n                    \"metric_3_value\": \"247\"\n                }\n            },\n            {\n                \"template_id\": \"before_after_comparison\",\n                \"content\": {\n                    \"content_left\": \"Manual processes taking hours\",\n                    \"content_right\": \"Automated workflows in minutes\"\n                }\n            }\n        ],\n        \"color_scheme\": \"modern_blue\"\n    }\n)\n```\n\n### Enhanced Image Management with v2.0\n\n```python\n# Add image with automatic enhancement - CONSOLIDATED TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"manage_image\",\n    arguments={\n        \"slide_index\": 1,\n        \"operation\": \"add\",\n        \"image_source\": \"company_logo.png\",\n        \"left\": 1.0,\n        \"top\": 1.0,\n        \"width\": 3.0,\n        \"height\": 2.0,\n        \"enhancement_style\": \"presentation\"\n    }\n)\n\n# Apply multiple picture effects at once - CONSOLIDATED TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"apply_picture_effects\",\n    arguments={\n        \"slide_index\": 1,\n        \"shape_index\": 0,\n        \"effects\": {\n            \"shadow\": {\n                \"shadow_type\": \"outer\",\n                \"blur_radius\": 4.0,\n                \"distance\": 3.0,\n                \"direction\": 315.0,\n                \"color\": [128, 128, 128],\n                \"transparency\": 0.6\n            },\n            \"glow\": {\n                \"size\": 5.0,\n                \"color\": [0, 176, 240],\n                \"transparency\": 0.4\n            }\n        }\n    }\n)\n```\n\n### Advanced Text Management with v2.0\n\n```python\n# Add and format text in one operation - CONSOLIDATED TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"manage_text\",\n    arguments={\n        \"slide_index\": 0,\n        \"operation\": \"add\",\n        \"left\": 1.0,\n        \"top\": 2.0,\n        \"width\": 8.0,\n        \"height\": 1.5,\n        \"text\": \"Welcome to Our Quarterly Review\",\n        \"font_size\": 32,\n        \"font_name\": \"Segoe UI\",\n        \"bold\": True,\n        \"color\": [0, 120, 215],\n        \"alignment\": \"center\",\n        \"auto_fit\": True\n    }\n)\n\n# Validate and fix text fit issues - SAME TOOL, DIFFERENT OPERATION\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"manage_text\",\n    arguments={\n        \"slide_index\": 0,\n        \"operation\": \"validate\",\n        \"shape_index\": 0,\n        \"validation_only\": False,  # Auto-fix enabled\n        \"min_font_size\": 10,\n        \"max_font_size\": 48\n    }\n)\n```\n\n### Creating a Presentation from Template\n\n```python\n# First, inspect a template to see its layouts and properties\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"get_template_info\",\n    arguments={\n        \"template_path\": \"company_template.pptx\"\n    }\n)\ntemplate_info = result\n\n# Create a new presentation from the template\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"create_presentation_from_template\",\n    arguments={\n        \"template_path\": \"company_template.pptx\"\n    }\n)\npresentation_id = result[\"presentation_id\"]\n\n# Add a slide using one of the template's layouts\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"add_slide\",\n    arguments={\n        \"layout_index\": 1,  # Use layout from template\n        \"title\": \"Quarterly Report\",\n        \"presentation_id\": presentation_id\n    }\n)\n\n# Save the presentation\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"save_presentation\",\n    arguments={\n        \"file_path\": \"quarterly_report.pptx\",\n        \"presentation_id\": presentation_id\n    }\n)\n```\n\n### Adding Advanced Charts and Data Visualization\n\n```python\n# Add a chart slide\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"add_slide\",\n    arguments={\n        \"layout_index\": 1,  # Content slide layout\n        \"title\": \"Sales Data\",\n        \"presentation_id\": presentation_id\n    }\n)\nslide_index = result[\"slide_index\"]\n\n# Add a column chart with comprehensive customization\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"add_chart\",\n    arguments={\n        \"slide_index\": slide_index,\n        \"chart_type\": \"column\",\n        \"left\": 1.0,\n        \"top\": 2.0,\n        \"width\": 8.0,\n        \"height\": 4.5,\n        \"categories\": [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        \"series_names\": [\"2023\", \"2024\"],\n        \"series_values\": [\n            [100, 120, 140, 160],\n            [110, 130, 150, 170]\n        ],\n        \"has_legend\": True,\n        \"legend_position\": \"bottom\",\n        \"has_data_labels\": True,\n        \"title\": \"Quarterly Sales Performance\",\n        \"presentation_id\": presentation_id\n    }\n)\n```\n\n### Text Validation and Optimization with v2.0\n\n```python\n# Validate text fit and get optimization suggestions - USING CONSOLIDATED TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"manage_text\",\n    arguments={\n        \"slide_index\": 0,\n        \"operation\": \"validate\",\n        \"shape_index\": 0,\n        \"text\": \"This is a very long title that might not fit properly in the designated text box area\",\n        \"font_size\": 24,\n        \"validation_only\": True\n    }\n)\n\n# Comprehensive slide validation with automatic fixes - SAME TOOL, AUTO-FIX ENABLED\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"manage_text\",\n    arguments={\n        \"slide_index\": 0,\n        \"operation\": \"validate\",\n        \"shape_index\": 0,\n        \"validation_only\": False,  # Auto-fix enabled\n        \"min_font_size\": 10,\n        \"max_font_size\": 48\n    }\n)\n```\n\n### Reading Slide Content with New Text Extraction Tools (v2.1)\n\n```python\n# Extract text content from a specific slide - NEW TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"extract_slide_text\",\n    arguments={\n        \"slide_index\": 0,\n        \"presentation_id\": presentation_id\n    }\n)\n\n# The result includes:\n{\n    \"success\": True,\n    \"slide_index\": 0,\n    \"text_content\": {\n        \"slide_title\": \"Quarterly Business Review\",\n        \"placeholders\": [\n            {\n                \"shape_index\": 1,\n                \"shape_name\": \"Subtitle Placeholder 2\",\n                \"text\": \"Q4 2024 Results\",\n                \"placeholder_type\": \"SUBTITLE\",\n                \"placeholder_idx\": 1\n            }\n        ],\n        \"text_shapes\": [\n            {\n                \"shape_index\": 3,\n                \"shape_name\": \"TextBox 4\",\n                \"text\": \"Revenue increased by 15%\"\n            }\n        ],\n        \"table_text\": [],\n        \"all_text_combined\": \"Quarterly Business Review\\nQ4 2024 Results\\nRevenue increased by 15%\"\n    },\n    \"total_text_shapes\": 2,\n    \"has_title\": True,\n    \"has_tables\": False\n}\n\n# Extract text from all slides in the presentation - NEW TOOL\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"extract_presentation_text\",\n    arguments={\n        \"presentation_id\": presentation_id,\n        \"include_slide_info\": True\n    }\n)\n\n# The result includes comprehensive text extraction:\n{\n    \"success\": True,\n    \"presentation_id\": \"pres_123\",\n    \"total_slides\": 5,\n    \"slides_with_text\": 4,\n    \"total_text_shapes\": 12,\n    \"slides_with_titles\": 3,\n    \"slides_with_tables\": 1,\n    \"slides_text\": [...],  # Detailed per-slide text content\n    \"all_presentation_text_combined\": \"=== SLIDE 1 ===\\nTitle Here\\nContent here...\"\n}\n\n# Extract text without additional slide metadata for cleaner output\nresult = use_mcp_tool(\n    server_name=\"ppt\",\n    tool_name=\"extract_presentation_text\",\n    arguments={\n        \"presentation_id\": presentation_id,\n        \"include_slide_info\": False\n    }\n)\n```\n\n## Template Support\n\n### Working with Templates\n\nThe PowerPoint MCP Server provides comprehensive template support for creating presentations from existing template files. This feature enables:\n\n- **Corporate branding** with predefined themes, layouts, and styles\n- **Consistent presentations** across teams and projects  \n- **Custom slide masters** and specialized layouts\n- **Pre-configured properties** and document settings\n- **Flexible template discovery** with configurable search paths\n\n### Template File Requirements\n\n- **Supported formats**: `.pptx` and `.potx` files\n- **Existing content**: Templates can contain existing slides (preserved during creation)\n- **Layout availability**: All custom layouts and slide masters are accessible\n- **Search locations**: Configurable via `PPT_TEMPLATE_PATH` environment variable\n- **Default search paths**: Current directory, `./templates`, `./assets`, `./resources`\n\n### Template Configuration\n\nSet the `PPT_TEMPLATE_PATH` environment variable to specify custom template directories:\n\n```bash\n# Unix/Linux/macOS\nexport PPT_TEMPLATE_PATH=\"/path/to/templates:/another/path\"\n\n# Windows  \nset PPT_TEMPLATE_PATH=\"C:\\templates;C:\\company_templates\"\n```\n\n### Template Workflow\n\n1. **Inspect Template**: Use `get_template_info` to analyze available layouts and properties\n2. **Create from Template**: Use `create_presentation_from_template` with automatic theme preservation\n3. **Use Template Layouts**: Reference layout indices from template analysis when adding slides  \n4. **Maintain Branding**: Template themes, fonts, and colors are automatically applied to new content\n\n### Professional Color Schemes\n\nThe server includes 4 built-in professional color schemes:\n- **Modern Blue**: Microsoft-inspired blue theme with complementary colors\n- **Corporate Gray**: Professional grayscale theme with blue accents\n- **Elegant Green**: Forest green theme with cream and light green accents  \n- **Warm Red**: Deep red theme with orange and yellow accents\n\nEach scheme includes primary, secondary, accent, light, and text colors optimized for business presentations.\n\n## 🎨 Built-in Slide Templates (New in v2.0)\n\nThe PowerPoint MCP Server now includes **25 professional slide templates** with advanced dynamic features. All templates support:\n\n### **Dynamic Features**\n- **Automatic text sizing** based on content length and container dimensions\n- **Intelligent text wrapping** to fit within specified areas\n- **Visual effects** including shadows, glows, and outlines\n- **Gradient backgrounds** with multi-layer compositions\n- **Professional animations** ready for presentation delivery\n- **Interactive hover effects** for enhanced user experience\n- **Smart content overflow handling** with automatic adjustments\n\n### **Available Template Categories**\n\n#### **Title & Introduction Slides**\n- `title_slide` - Dynamic title slide with gradient background and text effects\n- `chapter_intro` - Section divider with chapter numbering and styling\n- `thank_you_slide` - Closing slide with contact information and effects\n\n#### **Content Layout Slides**\n- `text_with_image` - Text content with stylized image and interactive elements\n- `two_column_text` - Two equal columns of text with dynamic sizing\n- `two_column_text_images` - Two columns with text and corresponding images\n- `three_column_layout` - Three equal columns with text and images\n- `full_image_slide` - Large background image with text overlay\n\n#### **Business & Analytics Slides**\n- `key_metrics_dashboard` - Interactive metrics dashboard with animated counters\n- `before_after_comparison` - Dynamic comparison layout with visual dividers\n- `chart_comparison` - Two charts side by side for performance comparison\n- `data_table_slide` - Slide focused on tabular data with professional styling\n- `timeline_slide` - Horizontal timeline with milestones and effects\n\n#### **Process & Flow Slides**\n- `process_flow` - Step-by-step process visualization with enhanced effects\n- `agenda_slide` - Table of contents or agenda overview with styling\n- `quote_testimonial` - Featured quote or customer testimonial with effects\n\n#### **Team & Organization Slides**\n- `team_introduction` - Team member showcase with photos and roles\n\n### **Template Usage Examples**\n\n```python\n# Browse all available templates\ntemplates = use_mcp_tool(\"ppt\", \"list_slide_templates\", {})\n\n# Key templates with their features:\n{\n  \"title_slide\": {\n    \"features\": [\"Dynamic text sizing\", \"Gradient backgrounds\", \"Text effects\"],\n    \"elements\": [\"title\", \"subtitle\", \"author\", \"decorative_accent\"]\n  },\n  \"key_metrics_dashboard\": {\n    \"features\": [\"Animated counters\", \"Gradient containers\", \"Trend visualization\"],\n    \"elements\": [\"3 metric containers\", \"trend chart\", \"insights callout\"]\n  },\n  \"before_after_comparison\": {\n    \"features\": [\"Split gradient background\", \"VS divider\", \"Improvement arrow\"],\n    \"elements\": [\"before/after headers\", \"comparison content\", \"improvement metrics\"]\n  }\n}\n```\n\n### **Color Scheme Integration**\nAll templates work seamlessly with the 4 professional color schemes:\n- **modern_blue**: Microsoft-inspired theme with dynamic gradients\n- **corporate_gray**: Professional grayscale with blue accents\n- **elegant_green**: Forest green with cream and light accents\n- **warm_red**: Deep red with orange and yellow highlights\n\n### **Dynamic Content Adaptation**\nTemplates automatically adjust to content:\n- **Font sizes** scale based on text length (8pt - 44pt range)\n- **Line spacing** adjusts for readability (1.0x - 1.4x)\n- **Text wrapping** intelligently breaks lines at optimal points\n- **Container sizing** adapts to content overflow\n- **Visual effects** scale appropriately with element sizes\n\n## 📁 File Structure\n\n```\nOffice-PowerPoint-MCP-Server/\n├── ppt_mcp_server.py          # Main consolidated server (v2.0)\n├── slide_layout_templates.json # 25+ professional slide templates with dynamic features\n├── tools/                     # 11 specialized tool modules (32 tools total)\n│   ├── __init__.py\n│   ├── presentation_tools.py  # Presentation management (7 tools)\n│   ├── content_tools.py       # Content & slides (6 tools)\n│   ├── template_tools.py      # Template operations (7 tools)\n│   ├── structural_tools.py    # Tables, shapes, charts (4 tools)\n│   ├── professional_tools.py  # Themes, effects, fonts (3 tools)\n│   ├── hyperlink_tools.py     # Hyperlink management (1 tool)\n│   ├── chart_tools.py         # Advanced chart operations (1 tool)\n│   ├── connector_tools.py     # Connector lines/arrows (1 tool)\n│   ├── master_tools.py        # Slide master management (1 tool)\n│   └── transition_tools.py    # Slide transitions (1 tool)\n├── utils/                     # 7 organized utility modules (68+ functions)\n│   ├── __init__.py\n│   ├── core_utils.py          # Error handling & safe operations\n│   ├── presentation_utils.py  # Presentation management utilities\n│   ├── content_utils.py       # Content & slide operations\n│   ├── design_utils.py        # Themes, colors, effects & fonts\n│   ├── template_utils.py      # Template management & dynamic features\n│   └── validation_utils.py    # Text & layout validation\n├── setup_mcp.py              # Interactive setup script\n├── pyproject.toml            # Updated for v2.0\n└── README.md                 # This documentation\n```\n\n## 🏗️ Architecture Benefits\n\n### **Modular Design**\n- **7 focused utility modules** with clear responsibilities\n- **11 organized tool modules** for comprehensive coverage\n- **68+ utility functions** organized by functionality\n- **32 MCP tools** covering all PowerPoint manipulation needs\n- **Clear separation of concerns** for easier development\n\n### **Code Organization**\n- **Logical grouping** of related functionality across modules\n- **Better discoverability** with organized tool categories\n- **Improved testability** with isolated modules\n- **Future extensibility** through modular structure\n\n### **Comprehensive Coverage**\n- **Complete PowerPoint lifecycle** from creation to presentation\n- **Advanced template system** with auto-generation capabilities\n- **Professional design tools** with multiple effects and styling options\n- **Specialized features** including hyperlinks, connectors, and slide masters\n\n### **Developer Experience**\n- **Clear responsibility boundaries** between modules\n- **Easier debugging** with smaller, focused files\n- **Simpler testing** with isolated functionality\n- **Enhanced maintainability** through separation of concerns\n\n## 🔄 What's New in Version 2.0\n\n**Enhanced functionality with comprehensive tool coverage!** The updated server provides:\n\n### **New Specialized Tools Added:**\n- **`manage_hyperlinks`** - Complete hyperlink management for text elements\n- **`update_chart_data`** - Advanced chart data replacement and updating\n- **`add_connector`** - Connector lines and arrows between slide elements\n- **`manage_slide_masters`** - Access to slide master properties and layouts\n- **`manage_slide_transitions`** - Basic slide transition management\n- **`auto_generate_presentation`** - AI-powered presentation generation\n- **`optimize_slide_text`** - Text optimization for better readability\n\n### **Enhanced Existing Tools:**\n- **`manage_text`** - Now supports text run formatting with `format_runs` operation\n- **`create_presentation_from_templates`** - Enhanced template sequence processing\n- **`apply_picture_effects`** - Expanded effect combinations and options\n\n## 🔄 What's New in Version 2.1\n\n**Text extraction capabilities added!** Now you can read content from existing presentations:\n\n### **New Text Extraction Tools Added:**\n- **`extract_slide_text`** - Extract all text content from a specific slide including titles, placeholders, text shapes, and tables\n- **`extract_presentation_text`** - Extract text content from all slides in a presentation with comprehensive statistics and combined output\n\n### **Key Features of Text Extraction:**\n- **Complete text coverage** - Extracts from titles, placeholders, text boxes, and table cells\n- **Structured output** - Organized by content type (titles, placeholders, shapes, tables)\n- **Presentation-wide analysis** - Statistics on text distribution across slides\n- **Flexible output options** - Individual slide content or combined presentation text\n- **Error handling** - Graceful handling of slides that cannot be processed\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "powerpoint",
        "presentations",
        "presentation",
        "powerpoint mcp",
        "powerpoint presentations",
        "office powerpoint"
      ],
      "category": "document-processing"
    },
    "Handwriting-OCR--handwriting-ocr-mcp-server": {
      "owner": "Handwriting-OCR",
      "name": "handwriting-ocr-mcp-server",
      "url": "https://github.com/Handwriting-OCR/handwriting-ocr-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Handwriting-OCR.webp",
      "description": "Integrate applications with the Handwriting OCR service to process images and PDF documents for text extraction. Upload documents, check processing status, and retrieve OCR results in Markdown format.",
      "stars": 11,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T06:46:15Z",
      "readme_content": "# Handwriting OCR MCP Server\n[![smithery badge](https://smithery.ai/badge/@Handwriting-OCR/handwriting-ocr-mcp-server)](https://smithery.ai/server/@Handwriting-OCR/handwriting-ocr-mcp-server)\n\nA Model Context Protocol (MCP) Server for [Handwriting OCR](https://www.handwritingocr.com) API.\n\n## Overview\n\nThe Handwriting OCR MCP Server enables integration between MCP clients and the Handwriting OCR service. This document outlines the setup process and provides a basic example of using the client.\n\nThis server allows you to upload images and PDF documents, check their status, and retrieve the OCR result as Markdown.\n\n## Tools\n\n### Transcription\n\n*   Upload Document\n*   Check Status\n*   Get Text\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n*   Node.js installed on your system (recommended version 18.x or higher).\n*   An active account on the [Handwriting OCR Platform](https://www.handwritingocr.com) and an active [API token](https://www.handwritingocr.com/settings/api).\n\n## Installation\n\n### Installing via Smithery\n\nTo install handwriting-ocr-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Handwriting-OCR/handwriting-ocr-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @Handwriting-OCR/handwriting-ocr-mcp-server --client claude\n```\n\n### Installing manually for Claude Desktop\n\nTo use the Handwriting OCR MCP Server in Claude Desktop application, use:\n\n```json\n{\n    \"mcpServers\": {\n        \"handwriting-ocr\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Users/mateo/Local/Code/MCP/handwriting-ocr/build/index.js\"\n            ],\n            \"env\": {\n                \"API_TOKEN\": \"your-api-token\",\n            },\n            \"disabled\": false,\n            \"autoApprove\": []\n        }\n    }\n}\n```\n\n## Configuration\n\nThe Handwriting OCR MCP Server supports environment variables to be set for authentication and configuration:\n\n*   `API_TOKEN`: Your API token.\n\nYou can find these values in the API settings dashboard on the [Handwriting OCR Platform](https://www.handwritingocr.com).\n\n## Support\n\nPlease refer to the [Handwriting OCR API Documentation](https://www.handwritingocr.com/api/docs).\n\nFor support with the Handwriting OCR MCP Server, please submit a [GitHub Issue](https://github.com/modelcontextprotocol/servers/issues).\n\n## About\n\nModel Context Protocol (MCP) Server for Handwriting OCR Platform\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "handwriting",
        "processing",
        "ocr mcp",
        "handwriting ocr",
        "ocr service"
      ],
      "category": "document-processing"
    },
    "HireTechUpUp--mcp-server-novacv": {
      "owner": "HireTechUpUp",
      "name": "mcp-server-novacv",
      "url": "https://github.com/HireTechUpUp/mcp-server-novacv",
      "imageUrl": "/freedevtools/mcp/pfp/HireTechUpUp.webp",
      "description": "Connect to the NovaCV API for generating professional resumes, analyzing resume content, and converting resume text into structured formats like JSON. It provides features for creating tailored resumes in PDF format and accessing available template options.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-11T08:22:18Z",
      "readme_content": "# MCP Server for NovaCV\n\n模型上下文协议(MCP)服务器，用于接入 NovaCV 简历服务 API。\n\n## 功能特点\n\n- 生成简历 PDF\n- 获取可用简历模板列表\n- 将简历文本转换为 JSON Resume 格式\n- 分析简历文本内容\n\n## 获取 API 密钥\n\n在使用此服务前，您需要获取 NovaCV API 密钥：\n\n1. 访问 [NovaCV API 官网](https://api.nova-cv.com)\n2. 注册或登录您的账户\n3. 在控制面板中找到 \"API Keys\" 或 \"开发者\" 部分\n4. 创建新的 API 密钥并复制它\n5. 在使用 MCP 服务时配置此密钥\n\n请妥善保管您的 API 密钥，不要在公共场合分享。\n\n## 安装\n\n```bash\n# 全局安装\nnpm install -g mcp-server-novacv\n\n# 或使用 npx 运行\nnpx mcp-server-novacv --api_key=your_api_key\n```\n\n## 快速开始\n\n### 方法一：直接运行（推荐）\n\n最简单的方式是使用我们提供的快速启动命令：\n\n```bash\n# 一键构建并启动服务\nnpm run run\n```\n\n### 方法二：使用 MCP Inspector 进行开发和测试\n\n我们提供了一个组合命令，可以一键构建和启动 Inspector：\n\n```bash\n# 一键构建并启动 Inspector\nnpm run debug\n```\n\n## 使用方法\n\n### 命令行选项\n\n```bash\nnpx mcp-server-novacv [选项]\n\n选项:\n  --api_key=KEY        设置 NovaCV API 密钥\n  --api_base_url=URL   设置 API 基础 URL\n  --timeout=MS         设置 API 超时时间 (毫秒)\n  --help, -h           显示帮助信息\n  --version, -v        显示版本信息\n```\n\n### 环境变量配置\n\n可以通过环境变量配置 API 密钥：\n\n```bash\nNOVACV_API_KEY=your_api_key mcp-server-novacv\n```\n\n或者创建 `.env` 文件：\n\n```\nNOVACV_API_KEY=your_api_key\nNOVACV_API_BASE_URL=https://api.nova-cv.com\n```\n\n> **提示**：API 密钥可以从 [NovaCV API 官网](https://api.nova-cv.com) 获取，请参考上方的 \"获取 API 密钥\" 部分。\n\n### 在 MCP 客户端配置\n\n#### Cursor 配置\n\n在 Cursor 配置文件中添加:\n\n```json\n{\n  \"mcpServers\": {\n    \"novacv\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-novacv\"],\n      \"env\": {\n        \"NOVACV_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n\n#### Cherry Studio 配置\n\n在 Cherry Studio 中设置 MCP 服务:\n\n1. 打开设置（点击左下角设置图标或使用 `Ctrl+,`/`Cmd+,`）\n2. 找到 MCP 或 Model Context Protocol 设置区域\n3. 添加新服务，配置如下:\n   - 名称: `novacv`\n   - 命令: `npx`\n   - 参数: `mcp-server-novacv`\n   - 环境变量: 添加 `NOVACV_API_KEY` 并设置您的 API 密钥\n\n如果支持 JSON 配置，添加以下内容:\n\n```json\n{\n  \"novacv\": {\n    \"command\": \"npx\",\n    \"args\": [\"mcp-server-novacv\"],\n    \"env\": {\n      \"NOVACV_API_KEY\": \"your_api_key\"\n    }\n  }\n}\n```\n\n## 可用工具\n\nMCP 服务器提供以下工具：\n\n- `generate_resume_from_text`: 一键将简历文本转换为精美PDF简历，支持多种模板。只需提供简历文本内容，系统会自动进行格式转换并生成专业PDF文件，无需手动处理JSON数据\n- `get_templates`: 获取所有可用的简历模板，返回模板列表及其详细信息，包括模板ID、名称、缩略图等\n- `convert_resume_text`: 将纯文本格式的简历内容转换为标准JSON Resume格式。系统会智能识别简历中的各个部分，并按照国际通用的JSON Resume标准进行结构化处理\n- `analyze_resume_text`: 对简历文本进行深度分析，提供专业评估和改进建议。系统会分析简历的完整性、关键词使用、技能匹配度等方面，并给出针对性的优化建议\n\n## 使用示例\n\n### 获取模板列表\n\n在支持 MCP 的客户端中使用 `mcp_novacv_get_templates` 命令获取所有可用的简历模板。\n\n### 生成简历\n\n使用 `mcp_novacv_generate_resume_from_text` 命令并提供简历文本内容和模板名称生成 PDF 简历。\n\n### 分析简历文本\n\n使用 `mcp_novacv_analyze_resume_text` 命令分析纯文本简历内容。\n\n### 转换简历文本为 JSON Resume\n\n使用 `mcp_novacv_convert_resume_text` 命令将简历文本转换为结构化的 JSON Resume 格式。\n\n## 开发\n\n```bash\n# 安装依赖\nnpm install\n\n# 开发模式（监视文件变化）\nnpm run dev\n\n# 构建项目\nnpm run build\n\n# 运行服务（构建并启动）\nnpm run run\n\n# 使用 MCP Inspector 调试（构建并启动Inspector）\nnpm run debug\n```\n\n## 故障排除\n\n如果您在设置过程中遇到问题:\n\n1. 确认包安装成功: `npx mcp-server-novacv --version`\n2. 检查 API 密钥是否正确设置\n3. 查看客户端日志中是否有相关错误信息\n\n### API 密钥问题\n\n如果遇到 API 密钥相关错误：\n\n- 确保您已从 [https://api.nova-cv.com](https://api.nova-cv.com) 获取了有效的 API 密钥\n- 检查密钥是否已过期或超出使用限制\n- 尝试重新生成新的 API 密钥\n- 确保环境变量或配置文件中的密钥没有多余的空格或引号\n\n## 许可证\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "resumes",
        "novacv",
        "resume",
        "novacv api",
        "server novacv",
        "resumes pdf"
      ],
      "category": "document-processing"
    },
    "JDJR2024--markdownify-mcp-utf8": {
      "owner": "JDJR2024",
      "name": "markdownify-mcp-utf8",
      "url": "https://github.com/JDJR2024/markdownify-mcp-utf8",
      "imageUrl": "/freedevtools/mcp/pfp/JDJR2024.webp",
      "description": "Converts various file types to Markdown format, with robust support for UTF-8 encoding and optimized for multilingual content handling. Ensures accurate transformation of documents and web pages while addressing encoding issues, especially on Windows systems.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-08T17:20:52Z",
      "readme_content": "# Markdownify MCP Server - UTF-8 Enhanced\n\nThis is an enhanced version of the [original Markdownify MCP project](https://github.com/cursor-ai/markdownify-mcp), with improved UTF-8 encoding support and optimized handling of multilingual content.\n\n[中文文档](README-CN.md)\n\n## Enhancements\n\n- Added comprehensive UTF-8 encoding support\n- Optimized handling of multilingual content\n- Fixed encoding issues on Windows systems\n- Improved error handling mechanisms\n\n## Key Differences from Original Project\n\n1. Enhanced Encoding Support:\n   - Full UTF-8 support across all operations\n   - Proper handling of Chinese, Japanese, Korean and other non-ASCII characters\n   - Fixed Windows-specific encoding issues (cmd.exe and PowerShell compatibility)\n\n2. Improved Error Handling:\n   - Detailed error messages in both English and Chinese\n   - Better exception handling for network issues\n   - Graceful fallback mechanisms for conversion failures\n\n3. Extended Functionality:\n   - Added support for batch processing multiple files\n   - Enhanced YouTube video transcript handling\n   - Improved metadata extraction from various file formats\n   - Better preservation of document formatting\n\n4. Performance Optimizations:\n   - Optimized memory usage for large file conversions\n   - Faster processing of multilingual content\n   - Reduced dependency conflicts\n\n5. Better Development Experience:\n   - Comprehensive debugging options\n   - Detailed logging system\n   - Environment-specific configuration support\n   - Clear documentation in both English and Chinese\n\n## Features\n\nSupports converting various file types to Markdown:\n- PDF files\n- Images (with metadata)\n- Audio (with transcription)\n- Word documents (DOCX)\n- Excel spreadsheets (XLSX)\n- PowerPoint presentations (PPTX)\n- Web content:\n  - YouTube video transcripts\n  - Search results\n  - General web pages\n- Existing Markdown files\n\n## Quick Start\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/JDJR2024/markdownify-mcp-utf8.git\n   cd markdownify-mcp-utf8\n   ```\n\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n   Note: This will also install `uv` and related Python dependencies.\n\n3. Build the project:\n   ```bash\n   pnpm run build\n   ```\n\n4. Start the server:\n   ```bash\n   pnpm start\n   ```\n\n## Requirements\n\n- Node.js 16.0 or higher\n- Python 3.8 or higher\n- pnpm package manager\n- Git\n\n## Detailed Installation Guide\n\n### 1. Environment Setup\n\n1. Install Node.js:\n   - Download from [Node.js official website](https://nodejs.org/)\n   - Verify installation: `node --version`\n\n2. Install pnpm:\n   ```bash\n   npm install -g pnpm\n   pnpm --version\n   ```\n\n3. Install Python:\n   - Download from [Python official website](https://www.python.org/downloads/)\n   - Ensure Python is added to PATH during installation\n   - Verify installation: `python --version`\n\n4. (Windows Only) Configure UTF-8 Support:\n   ```bash\n   # Set system-wide UTF-8\n   setx PYTHONIOENCODING UTF-8\n   # Set current session UTF-8\n   set PYTHONIOENCODING=UTF-8\n   # Enable UTF-8 in command prompt\n   chcp 65001\n   ```\n\n### 2. Project Setup\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/JDJR2024/markdownify-mcp-utf8.git\n   cd markdownify-mcp-utf8\n   ```\n\n2. Create and activate Python virtual environment:\n   ```bash\n   # Windows\n   python -m venv .venv\n   .venv\\Scripts\\activate\n\n   # Linux/macOS\n   python3 -m venv .venv\n   source .venv/bin/activate\n   ```\n\n3. Install project dependencies:\n   ```bash\n   # Install Node.js dependencies\n   pnpm install\n\n   # Install Python dependencies (will be handled by setup.sh)\n   ./setup.sh\n   ```\n\n4. Build the project:\n   ```bash\n   pnpm run build\n   ```\n\n### 3. Verification\n\n1. Start the server:\n   ```bash\n   pnpm start\n   ```\n\n2. Test the installation:\n   ```bash\n   # Convert a web page\n   python convert_utf8.py \"https://example.com\"\n\n   # Convert a local file\n   python convert_utf8.py \"path/to/your/file.docx\"\n   ```\n\n## Usage Guide\n\n### Basic Usage\n\n1. Converting Web Pages:\n   ```bash\n   python convert_utf8.py \"https://example.com\"\n   ```\n   The converted markdown will be saved as `converted_result.md`\n\n2. Converting Local Files:\n   ```bash\n   # Convert DOCX\n   python convert_utf8.py \"document.docx\"\n\n   # Convert PDF\n   python convert_utf8.py \"document.pdf\"\n\n   # Convert PowerPoint\n   python convert_utf8.py \"presentation.pptx\"\n\n   # Convert Excel\n   python convert_utf8.py \"spreadsheet.xlsx\"\n   ```\n\n3. Converting YouTube Videos:\n   ```bash\n   python convert_utf8.py \"https://www.youtube.com/watch?v=VIDEO_ID\"\n   ```\n\n### Advanced Usage\n\n1. Environment Variables:\n   ```bash\n   # Set custom UV path\n   export UV_PATH=\"/custom/path/to/uv\"\n\n   # Set custom output directory\n   export MARKDOWN_OUTPUT_DIR=\"/custom/output/path\"\n   ```\n\n2. Batch Processing:\n   Create a batch file (e.g., `convert_batch.txt`) with URLs or file paths:\n   ```text\n   https://example1.com\n   https://example2.com\n   file1.docx\n   file2.pdf\n   ```\n   Then run:\n   ```bash\n   while read -r line; do python convert_utf8.py \"$line\"; done < convert_batch.txt\n   ```\n\n### Troubleshooting\n\n1. Common Issues:\n   - If you see encoding errors, ensure UTF-8 is properly set\n   - For permission issues on Windows, run as Administrator\n   - For Python path issues, ensure virtual environment is activated\n\n2. Debugging:\n   ```bash\n   # Enable debug output\n   export DEBUG=true\n   python convert_utf8.py \"your_file.docx\"\n   ```\n\n## Usage\n\n### Command Line\n\nConvert web page to Markdown:\n```bash\npython convert_utf8.py \"https://example.com\"\n```\n\nConvert local file:\n```bash\npython convert_utf8.py \"path/to/your/file.docx\"\n```\n\n### Desktop App Integration\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```js\n{\n  \"mcpServers\": {\n    \"markdownify\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"{ABSOLUTE_PATH}/dist/index.js\"\n      ],\n      \"env\": {\n        \"UV_PATH\": \"/path/to/uv\"\n      }\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n1. Encoding Issues\n   - If you encounter character encoding issues, ensure the `PYTHONIOENCODING` environment variable is set to `utf-8`\n   - Windows users may need to run `chcp 65001` to enable UTF-8 support\n\n2. Permission Issues\n   - Ensure you have sufficient file read/write permissions\n   - On Windows, you may need to run as administrator\n\n## Acknowledgments\n\nThis project is based on the original work by Zach Caceres. Thanks to the original author for their outstanding contribution.\n\n## License\n\nThis project continues to be licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Before submitting a Pull Request, please:\n1. Ensure your code follows the project's coding standards\n2. Add necessary tests and documentation\n3. Update relevant sections in the README\n\n## Contact\n\nFor issues or suggestions:\n1. Submit an Issue: https://github.com/JDJR2024/markdownify-mcp-utf8/issues\n2. Create a Pull Request: https://github.com/JDJR2024/markdownify-mcp-utf8/pulls\n3. Email: jdidndosmmxmx@gmail.com ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdownify",
        "markdown",
        "jdjr2024",
        "jdjr2024 markdownify",
        "markdown format",
        "markdownify mcp"
      ],
      "category": "document-processing"
    },
    "KunihiroS--kv-extractor-mcp-server": {
      "owner": "KunihiroS",
      "name": "kv-extractor-mcp-server",
      "url": "https://github.com/KunihiroS/kv-extractor-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/KunihiroS.webp",
      "description": "Extracts key-value pairs from noisy or unstructured text in multiple languages, ensuring type-safe outputs in JSON, YAML, or TOML formats. Utilizes advanced LLMs and pydantic for data structuring and validation, supporting languages like Japanese, English, and Chinese.",
      "stars": 1,
      "forks": 2,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-07-16T11:24:07Z",
      "readme_content": "# Flexible Key-Value Extracting MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@KunihiroS/kv-extractor-mcp-server)](https://smithery.ai/server/@KunihiroS/kv-extractor-mcp-server)\n\nVersion: 0.3.2\n\nThis MCP server extracts key-value pairs from arbitrary, noisy, or unstructured text using LLMs (GPT-4.1-mini) and pydantic-ai. \nIt ensures type safety and supports multiple output formats (JSON, YAML, TOML). The server is robust to any input and always attempts to structure data as much as possible, however, perfect extraction is **not guaranteed**.\n\n---\n## 🤔💡 Why Use This MCP Server?\n\nWhile many Large Language Model (LLMs) services offer structured output capabilities, this MCP server provides distinct advantages for key-value extraction, especially from challenging real-world text:\n\n*   🔑🔍 **Automatic Key Discovery**: A core strength is its ability to *autonomously identify and extract relevant key-value pairs* from unstructured text *without requiring pre-defined keys*. While typical LLM structured outputs need you to specify the keys you're looking for, this server discovers them, making it highly effective for diverse and unpredictable data where the structure is not known beforehand.\n*   💪🧱 **Superior Robustness for Complex Inputs**: It excels with arbitrary, noisy, or unstructured text where standard LLM structured outputs might falter. The multi-step pipeline is specifically designed to sift through and make sense of imperfect data.\n*   🌐🗣️ **Advanced Multi-Lingual Preprocessing**: Before LLM processing, it leverages spaCy for Named Entity Recognition (NER) in Japanese, English, and Chinese (Simplified/Traditional), significantly enhancing extraction accuracy for these languages by providing context-rich candidate phrases.\n*   🔄✍️ **Iterative Refinement and Typing**: Unlike a single-pass extraction, this server employs a sophisticated pipeline including LLM-based type annotation, LLM-based type evaluation, and rule-based/LLM-fallback normalization. This ensures more accurate and contextually appropriate data types.\n*   ✅🛡️ **Guaranteed Type Safety and Schema Adherence**: Final structuring with Pydantic ensures that the output is not only structured but also type-safe and validated against a defined schema, providing reliable data for downstream applications.\n*   📊⚙️ **Consistent and Predictable Output**: The server is designed to always return a well-formed response, even if extraction is partial or encounters issues, which is critical for building robust automated systems.\n\n---\n\n## Release Notes\n\n### v0.3.2\n- Fix: FastMCP caused error.\n\n### v0.3.1\n- Update: Improve type evaluation prompt for robust correction.\n- Update: Added the strong point of this MCP server on README.md\n\n### v0.2.0\n- Fix: Lang code for zh-cn / zh-tw.\n\n### v0.1.0\n- Initial release\n\n## Tools\n\n- `/extract_json` : Extracts type-safe key-value pairs in JSON format from input text.\n- `/extract_yaml` : Extracts type-safe key-value pairs in YAML format from input text.\n- `/extract_toml` : Extracts type-safe key-value pairs in TOML format from input text.\n    - *Note: Due to TOML specifications, arrays of objects (dicts) or deeply nested structures cannot be directly represented. See \"Note on TOML Output Limitations\" below for details.*\n\n**Note:**\n- Supported languages: Japanese, English, and Chinese (Simplified: zh-cn / Traditional: zh-tw).\n- Extraction relies on pydantic-ai and LLMs. Perfect extraction is not guaranteed.\n- Longer input sentences will take more time to process. Please be patient.\n- On first launch, the server will download spaCy models, so the process will take longer initially.\n\n### Estimated Processing Time Sample\n\n| Input Tokens | Input Characters (approx.) | Measured Processing Time (sec) | Model Configuration |\n|:-----------:|:--------------------------:|:------------------------------:|:-------------------|\n| 200         | ~400                       | ~15                            | gpt-4.1-mini       |\n\n*Actual processing time may vary significantly depending on API response, network conditions, and model load. Even short texts may take 15 seconds or more.*\n\n## Features\n- **Flexible extraction**: Handles any input, including noisy or broken data.\n- **JP / EN / ZH-CN / ZH-TW full support**: Preprocessing with spaCy NER by automatic language detection (Japanese, English, Chinese [Simplified: zh-cn / Traditional: zh-tw] supported; others are rejected with error).\n- **Type-safe output**: Uses Pydantic for output validation.\n- **Multiple formats**: Returns results as JSON, YAML, or TOML.\n- **Robust error handling**: Always returns a well-formed response, even on failure.\n- **High accuracy**: Uses GPT-4.1-mini for both extraction/annotation and type evaluation, with Pydantic for final structuring.\n\n## Tested Scenarios\nThe server has been tested with various inputs, including:\n- Simple key-value pairs\n- Noisy or unstructured text with important information buried within\n- Different data formats (JSON, YAML, TOML) for output\n\n## Processing Flow\nBelow is a flowchart representing the processing flow of the key-value extraction pipeline as implemented in `server.py`:\n\n```mermaid\nflowchart TD\n    A[Input Text] --> B[Step 0: Preprocessing with spaCy Lang Detect then NER]\n    B --> C[Step 1: Key-Value Extraction - LLM]\n    C --> D[Step 2: Type Annotation - LLM]\n    D --> E[Step 3: Type Evaluation - LLM]\n    E --> F[Step 4: Type Normalization - Static Rules + LLM]\n    F --> G[Step 5: Final Structuring with Pydantic]\n    G --> H[Output in JSON/YAML/TOML]\n```\n\n## Preprocessing with spaCy (Multilingual NER)\n\nThis server uses [spaCy](https://spacy.io/) with automatic language detection to extract named entities from the input text **before** passing it to the LLM. Supported languages are Japanese (`ja_core_news_md`), English (`en_core_web_sm`), and Chinese (Simplified/Traditional, `zh_core_web_sm`).\n\n- The language of the input text is automatically detected using `langdetect`.\n- If the detected language is not Japanese, English, or Chinese, the server returns an error: `Unsupported lang detected`.\n- The appropriate spaCy model is automatically downloaded and loaded as needed. No manual installation is required.\n- The extracted phrase list is included in the LLM prompt as follows:\n\n  > [Preprocessing Candidate Phrases (spaCy NER)]\n  > The following is a list of phrases automatically extracted from the input text using spaCy's detected language model.\n  > These phrases represent detected entities such as names, dates, organizations, locations, numbers, etc.\n  > This list is for reference only and may contain irrelevant or incorrect items. The LLM uses its own judgment and considers the entire input text to flexibly infer the most appropriate key-value pairs.\n\n## Step Details\n\nThis project's key-value extraction pipeline consists of multiple steps. Each step's details are as follows:\n\n### Step 0: Preprocessing with spaCy (Language Detection → Named Entity Recognition)\n- **Purpose**: Automatically detect the language of the input text and use the appropriate spaCy model (e.g., `ja_core_news_md`, `en_core_web_sm`, `zh_core_web_sm`) to extract named entities.\n- **Output**: The extracted phrase list, which is included in the LLM prompt as a hint to improve key-value pair extraction accuracy.\n\n### Step 1: Key-Value Extraction (LLM)\n- **Purpose**: Use GPT-4.1-mini to extract key-value pairs from the input text and the extracted phrase list.\n- **Details**:\n  - The prompt includes instructions to return list-formatted values when the same key appears multiple times.\n  - Few-shot examples are designed to include list-formatted outputs.\n- **Output**: Example: `key: person, value: [\"Tanaka\", \"Sato\"]`\n\n### Step 2: Type Annotation (LLM)\n- **Purpose**: Use GPT-4.1-mini to infer the data type (int, str, bool, list, etc.) of each key-value pair extracted in Step 1.\n- **Details**:\n  - The type annotation prompt includes instructions for list and multiple value support.\n- **Output**: Example: `key: person, value: [\"Tanaka\", \"Sato\"] -> list[str]`\n\n### Step 3: Type Evaluation (LLM)\n- **Purpose**: Use GPT-4.1-mini to evaluate and correct the type annotations from Step 2.\n- **Details**:\n  - For each key-value pair, GPT-4.1-mini re-evaluates the type annotation's validity and context.\n  - If type errors or ambiguities are detected, GPT-4.1-mini automatically corrects or supplements the type.\n  - Example: Correcting a value extracted as a number but should be a string, or determining whether a value is a list or a single value.\n- **Output**: The type-evaluated key-value pair list.\n\n### Step 4: Type Normalization (Static Rules + LLM Fallback)\n- **Purpose**: Convert the type-evaluated data into Python's standard types (int, float, bool, str, list, None, etc.).\n- **Details**:\n  - Apply static normalization rules (regular expressions or type conversion functions) to convert values into Python's standard types.\n  - Example: Converting comma-separated values to lists, \"true\"/\"false\" to bool, or date expressions to standard formats.\n  - If static rules cannot convert a value, use LLM-based type conversion fallback.\n  - Unconvertible values are safely handled as None or str.\n- **Output**: The Python-type-normalized key-value pair list.\n\n### Step 5: Final Structuring with Pydantic\n- **Purpose**: Validate and structure the type-normalized data using Pydantic models (KVOut/KVPayload).\n- **Details**:\n  - Map each key-value pair to Pydantic models, ensuring type safety and data integrity.\n  - Validate single values, lists, null, and composite types according to the schema.\n  - If validation fails, attach error information while preserving as much data as possible.\n  - The final output is returned in the specified format (JSON, YAML, or TOML).\n- **Output**: The type-safe and validated dict or specified format (JSON/YAML/TOML) output.\n\n---\n\nThis pipeline is designed to accommodate future list format support and Pydantic schema extensions.\n\n## Note on TOML Output Limitations\n\n- In TOML, simple arrays (e.g., `items = [\"A\", \"B\"]`) can be represented natively, but\n  **arrays of objects (dicts) or deeply nested structures cannot be directly represented due to TOML specifications.**\n- Therefore, complex lists or nested structures (e.g., `[{\"name\": \"A\"}, {\"name\": \"B\"}]`) are\n  **stored as \"JSON strings\" in TOML values.**\n- This is a design choice to prevent information loss due to TOML's specification limitations.\n- YAML and JSON formats can represent nested structures as-is.\n\n## Example Input/Output\nInput:\n```\nThank you for your order (Order Number: ORD-98765). Product: High-Performance Laptop, Price: 89,800 JPY (tax excluded), Delivery: May 15-17. Shipping address: 1-2-3 Shinjuku, Shinjuku-ku, Tokyo, Apartment 101. Phone: 090-1234-5678. Payment: Credit Card (VISA, last 4 digits: 1234). For changes, contact support@example.com.\n```\n\nOutput (JSON):\n```json\n{\n  \"order_number\": \"ORD-98765\",\n  \"product_name\": \"High-Performance Laptop\",\n  \"price\": 89800,\n  \"price_currency\": \"JPY\",\n  \"tax_excluded\": true,\n  \"delivery_start_date\": \"20240515\",\n  \"delivery_end_date\": \"20240517\",\n  \"shipping_address\": \"1-2-3 Shinjuku, Shinjuku-ku, Tokyo, Apartment 101\",\n  \"phone_number\": \"090-1234-5678\",\n  \"payment_method\": \"Credit Card\",\n  \"card_type\": \"VISA\",\n  \"card_last4\": \"1234\",\n  \"customer_support_email\": \"support@example.com\"\n}\n```\n\nOutput (YAML):\n```yaml\norder_number: ORD-98765\nproduct_name: High-Performance Laptop\nprice: 89800\nprice_currency: JPY\ntax_excluded: true\ndelivery_start_date: '20240515'\ndelivery_end_date: '20240517'\nshipping_address: 1-2-3 Shinjuku, Shinjuku-ku, Tokyo, Apartment 101\nphone_number: 090-1234-5678\npayment_method: Credit Card\ncard_type: VISA\ncard_last4: '1234'\ncustomer_support_email: support@example.com\n```\n\nOutput (TOML, simple case):\n```toml\norder_number = \"ORD-98765\"\nproduct_name = \"High-Performance Laptop\"\nprice = 89800\nprice_currency = \"JPY\"\ntax_excluded = true\ndelivery_start_date = \"20240515\"\ndelivery_end_date = \"20240517\"\nshipping_address = \"1-2-3 Shinjuku, Shinjuku-ku, Tokyo, Apartment 101\"\nphone_number = \"090-1234-5678\"\npayment_method = \"Credit Card\"\ncard_type = \"VISA\"\ncard_last4 = \"1234\"\n```\n\nOutput (TOML, complex case):\n```toml\nitems = '[{\"name\": \"A\", \"qty\": 2}, {\"name\": \"B\", \"qty\": 5}]'\naddresses = '[{\"city\": \"Tokyo\", \"zip\": \"160-0022\"}, {\"city\": \"Osaka\", \"zip\": \"530-0001\"}]'\n```\n*Note: Arrays of objects or nested structures are stored as JSON strings in TOML.*\n\n## Tools\n\n### 1. `extract_json`\n- **Description**: Extracts key-value pairs from arbitrary noisy text and returns them as type-safe JSON (Python dict).\n- **Arguments**:\n  - `input_text` (string): Input string containing noisy or unstructured data.\n- **Returns**: `{ \"success\": True, \"result\": ... }` or `{ \"success\": False, \"error\": ... }`\n- **Example**:\n  ```json\n  {\n    \"success\": true,\n    \"result\": { \"foo\": 1, \"bar\": \"baz\" }\n  }\n  ```\n\n### 2. `extract_yaml`\n- **Description**: Extracts key-value pairs from arbitrary noisy text and returns them as type-safe YAML (string).\n- **Arguments**:\n  - `input_text` (string): Input string containing noisy or unstructured data.\n- **Returns**: `{ \"success\": True, \"result\": ... }` or `{ \"success\": False, \"error\": ... }`\n- **Example**:\n  ```json\n  {\n    \"success\": true,\n    \"result\": \"foo: 1\\nbar: baz\"\n  }\n  ```\n\n### 3. `extract_toml`\n- **Description**: Extracts key-value pairs from arbitrary noisy text and returns them as type-safe TOML (string).\n- **Arguments**:\n  - `input_text` (string): Input string containing noisy or unstructured data.\n- **Returns**: `{ \"success\": True, \"result\": ... }` or `{ \"success\": False, \"error\": ... }`\n- **Example**:\n  ```json\n  {\n    \"success\": true,\n    \"result\": \"foo = 1\\nbar = \\\"baz\\\"\"\n  }\n  ```\n\n## Usage\n\n### Installing via Smithery\n\nTo install kv-extractor-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KunihiroS/kv-extractor-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @KunihiroS/kv-extractor-mcp-server --client claude\n```\n\n### Requirements\n- Python 3.9+\n- API key for OpenAI models (set in `settings.json` under `env`)\n\n### Running the Server\n\n```bash\npython server.py\n```\n*In case you want to run the server manually.*\n\n## MCP Host Configuration\n\nWhen running this MCP Server, you **must explicitly specify the log output mode and (if enabled) the absolute log file path via command-line arguments**.\n\n- `--log=off` : Disable all logging (no logs are written)\n- `--log=on --logfile=/absolute/path/to/logfile.log` : Enable logging and write logs to the specified absolute file path\n- Both arguments are **required** when logging is enabled. The server will exit with an error if either is missing, the path is not absolute, or if invalid values are given.\n\n### Example: Logging Disabled\n```json\n\"kv-extractor-mcp-server\": {\n  \"command\": \"pipx\",\n  \"args\": [\"run\", \"kv-extractor-mcp-server\", \"--log=off\"],\n  \"env\": {\n    \"OPENAI_API_KEY\": \"{apikey}\"\n  }\n}\n```\n\n### Example: Logging Enabled (absolute log file path required)\n```json\n\"kv-extractor-mcp-server\": {\n  \"command\": \"pipx\",\n  \"args\": [\"run\", \"kv-extractor-mcp-server\", \"--log=on\", \"--logfile=/workspace/logs/kv-extractor-mcp-server.log\"],\n  \"env\": {\n    \"OPENAI_API_KEY\": \"{apikey}\"\n  }\n}\n```\n\n> **Note:**\n> - When logging is enabled, logs are written **only** to the specified absolute file path. Relative paths or omission of `--logfile` will cause an error.\n> - When logging is disabled, no logs are output.\n> - If the required arguments are missing or invalid, the server will not start and will print an error message.\n> - The log file must be accessible and writable by the MCP Server process.\n> - If you have trouble to run this server, it may be due to caching older version of kv-extractor-mcp-server. Please try to run it with the latest version (set `x.y.z` to the latest version) of kv-extractor-mcp-server by the below setting.\n\n```json\n\"kv-extractor-mcp-server\": {\n  \"command\": \"pipx\",\n  \"args\": [\"run\", \"kv-extractor-mcp-server==x.y.z\", \"--log=off\"],\n  \"env\": {\n    \"OPENAI_API_KEY\": \"{apikey}\"\n  }\n}\n```\n\n## License\nGPL-3.0-or-later\n\n## Author\nKunihiroS (and contributors)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "formats",
        "kunihiros",
        "extractor",
        "processing kunihiros",
        "kv extractor",
        "kunihiros kv"
      ],
      "category": "document-processing"
    },
    "M-Gonzalo--cosa-sai": {
      "owner": "M-Gonzalo",
      "name": "cosa-sai",
      "url": "https://github.com/M-Gonzalo/cosa-sai",
      "imageUrl": "/freedevtools/mcp/pfp/M-Gonzalo.webp",
      "description": "Access documentation for a variety of technologies through the Gemini API, leveraging a curated knowledge base to provide accurate responses to complex queries. This server is designed to handle large context windows for improved comprehension of technical materials.",
      "stars": 13,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-23T00:21:06Z",
      "readme_content": "# Gemini Docs MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@M-Gonzalo/cosa-sai)](https://smithery.ai/server/@M-Gonzalo/cosa-sai)\n\n## Description\n\nThis project implements an MCP server that enables access to documentation for various technologies using the Gemini API with its gigantic context window of 2M tokens. It should work for any client, but is targeted especially to the Roo/Cline environment.\n\nThis approach offers several advantages over simply browsing the web or using a search engine:\n\n*   **Access to a curated knowledge base:** The LLM uses a specific set of documentation, avoiding junk results and false positives that can confuse the model.\n*   **Overcomes context window limitations:** By providing the documentation directly, the LLM can access more information than would be possible with web search alone.\n*   **Tailored and well-thought-out responses:** The LLM doesn't just provide snippets from the documentation, but crafts well-reasoned answers that take into consideration the entire specification for the technology in question. This allows for more complex questions like \"what alternative ways of doing X are there?\" or \"is this snippet idiomatic?\".\n\nIt also overcomes some problemmatic hurdles of traditional RAG systems:\n\n*   **No need for chunking:** The LLM can access the entire documentation in one go, without needing to chunk it into smaller pieces, and having to painfully test and choose between all the possible ways of doing so.\n*   **No need for a retriever:** The Gemini API itself serves as a powerful retriever that can access the entire documentation, so there's no need to implement a custom one.\n*   **No vectorization, vector DBs, or other complex systems:** We work directly with plain text, and since we can see everything at once, we don't need vectors for similarity search. If it's relevant, we know about it.\n\nThere are some limitations, though:\n\n*   **No real-time updates:** The documentation is static and won't be updated in real time. This means that the LLM might not know about the latest features or changes in the technology unless we manually update the documentation or provide an automated way of doing so.\n*   **A lot of tokens is not the same as an infinite context window:** The LLM can only see about 2 million tokens at a time, so it might not be able to see the entire documentation for some technologies. This is especially true for large and complex stacks with copious amounts of documentation.\n*   **It's not that fast:** We're using Gemini 1.5 Pro (not Flash), and we're loading it with a whole bunch of documentation, so it might take a while to get a response. This is especially true for the first query, as the server needs to upload the documentation to the API.\n\n## Features\n\n*   Enables clients to take an \"ask your docs\" approach to learning and debugging for an arbitrary number of technologies, including some obscure or lesser-known ones.\n*   Uses the Gemini API to answer questions about the documentation.\n*   Supports multiple tools for querying the documentation:\n    *   `can_x_be_done`: Check if a specific task can be done in a given technology.\n    *   `hints_for_problem`: Get hints for solving a specific problem.\n    *   `is_this_good_practice`: Check if a code snippet follows good practices.\n    *   `how_to_do_x`: Get examples and alternative approaches for a specific task.\n*   Provides a logging system for debugging (enabled with the `--verbose` flag).\n\n## Getting Started\n\n### Installing via Smithery\n\nTo install Gemini Docs Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@M-Gonzalo/cosa-sai):\n\n```bash\nnpx -y @smithery/cli install @M-Gonzalo/cosa-sai --client claude\n```\n\nThis MCP server is automatically started and managed by the client. To enable it, you need to configure it in your settings file (for example, `~/.config/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`). There's usually a button for opening up the settings file in the client.\n\nHere's the configuration for this server:\n\n```json\n{\n  \"command\": \"bun\",\n  \"args\": [\n    \"--watch\",\n    \"path/to/repo/cosa-sai-mcp/src/index.ts\",\n    \"--verbose\"\n  ],\n  \"env\": {\n    \"GEMINI_API_KEY\": \"<your_gemini_api_key>\"\n  },\n  \"disabled\": false,\n  \"alwaysAllow\": [\n    \"can_x_be_done\",\n    \"hints_for_problem\",\n    \"is_this_good_practice\",\n    \"how_to_do_x\"\n  ],\n  \"timeout\": 60 // in seconds\n}\n```\n\n## Procuring and Sanitizing the Knowledge Base\n\nThis MCP server requires a knowledge base of documentation to answer questions. You must manually procure this knowledge base, either by downloading a public repository, scraping a website, or using other methods.\n\nAn optional sanitation process can be performed to clean up the original documentation from styling and other unnecessary content.\n\nHere are some basic tools for doing so. Better solutions are encouraged:\n\n**Naive Scrapper:**\n\n```bash\nwget --mirror --convert-links --adjust-extension --page-requisites --no-parent --directory-prefix=./local_copy --no-verbose --show-progress $1\n```\n\n**Quick and Dirty Conversor to Markdown-ish:**\n\n```bash\n#!/bin/bash\n\ndirectory=\"${1:-.}\"  # Default to current directory if no argument is provided\noutput_file=\"${2:-concatenated.md}\"  # Default output file name\n\necho \"Concatenating files in '$directory' into '$output_file'...\"\n\n# Clear output file if it exists\ntruncate -s 0 \"$output_file\"\n\n# Find all files (excluding directories) and process them\nfind \"$directory\" -type f -name '*.html' | while IFS= read -r file; do\n    echo \"=== ${file#./} ===\" >> \"$output_file\"\n    cat \"$file\" \\\n    | grep -v 'base64' \\\n    | html2markdown >> \"$output_file\"\n    echo -e \"\\n\" >> \"$output_file\"\ndone\n\necho \"Done! Output saved to '$output_file'\"\n```\n\n## Usage\n\nThis server provides the following tools:\n\n*   **can\\_x\\_be\\_done:** Checks if a specific task can be done in a given technology.\n    *   **Input:** `docs`, `prompt`, `x`, `technology`\n    *   **Output:** `success`, `data`\n*   **hints\\_for\\_problem:** Gets hints for solving a specific problem.\n    *   **Input:** `docs`, `prompt`, `problem`, `context`, `environment`\n    *   **Output:** `success`, `data`\n*   **is\\_this\\_good\\_practice:** Checks if a code snippet follows good practices.\n    *   **Input:** `docs`, `prompt`, `snippet`, `context`\n    *   **Output:** `success`, `data`\n*   **how\\_to\\_do\\_x:** Gets examples and alternative approaches for a specific task.\n    *   **Input:** `docs`, `prompt`, `x`, `technology`\n    *   **Output:** `success`, `data`\n\n## Contributing\n\nContributions are welcome! Please follow these guidelines:\n\n1.  Fork the repository.\n2.  Create a new branch for your feature or bug fix.\n3.  Make your changes and commit them with descriptive commit messages.\n4.  Submit a pull request.\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Disclaimer\n\nThis is a very early version of the project, and it's likely to have bugs and limitations. Please report any issues you find, and feel free to suggest improvements or new features.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "api",
        "documentation variety",
        "access documentation",
        "document processing"
      ],
      "category": "document-processing"
    },
    "MKhalusova--unstructured-mcp": {
      "owner": "MKhalusova",
      "name": "unstructured-mcp",
      "url": "https://github.com/MKhalusova/unstructured-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/MKhalusova.webp",
      "description": "Enable extraction and utilization of content from various unstructured document formats, supporting seamless storage and retrieval via AWS S3. Process documents directly in applications to enhance data extraction capabilities for LLMs.",
      "stars": 6,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-21T18:55:48Z",
      "readme_content": "A Model Context Protocol server that provides unstructured document processing capabilities. \nThis server enables LLMs to extract and use content from an unstructured document.\n\n**This repo is work in progress, proceed with caution :)**\n\nSupported file types:\n\n```\n{\".abw\", \".bmp\", \".csv\", \".cwk\", \".dbf\", \".dif\", \".doc\", \".docm\", \".docx\", \".dot\",\n \".dotm\", \".eml\", \".epub\", \".et\", \".eth\", \".fods\", \".gif\", \".heic\", \".htm\", \".html\",\n \".hwp\", \".jpeg\", \".jpg\", \".md\", \".mcw\", \".mw\", \".odt\", \".org\", \".p7s\", \".pages\",\n \".pbd\", \".pdf\", \".png\", \".pot\", \".potm\", \".ppt\", \".pptm\", \".pptx\", \".prn\", \".rst\",\n \".rtf\", \".sdp\", \".sgl\", \".svg\", \".sxg\", \".tiff\", \".txt\", \".tsv\", \".uof\", \".uos1\",\n \".uos2\", \".web\", \".webp\", \".wk2\", \".xls\", \".xlsb\", \".xlsm\", \".xlsx\", \".xlw\", \".xml\",\n \".zabw\"}\n```\n\nPrerequisites: \nYou'll need:\n* Unstructured API key. [Learn how to obtain one here](https://docs.unstructured.io/api-reference/partition/overview#get-started)\n* Claude Desktop installed locally\n\nQuick TLDR on how to add this MCP to your Claude Desktop:\n1. Clone the repo and set up the UV environment.\n2. Create a `.env` file in the root directory and add the following env variable: `UNSTRUCTURED_API_KEY`.\n3. Run the MCP server: `uv run doc_processor.py`\n4. Go to `~/Library/Application Support/Claude/` and create a `claude_desktop_config.json`. In that file add:\n```\n{\n    \"mcpServers\": {\n        \"unstructured_doc_processor\": {\n            \"command\": \"PATH/TO/YOUR/UV\",\n            \"args\": [\n                \"--directory\",\n                \"ABSOLUTE/PATH/TO/YOUR/unstructured-mcp/\",\n                \"run\",\n                \"doc_processor.py\"\n            ],\n            \"disabled\": false\n        }\n    }\n}\n```\n5. Restart Claude Desktop. You should now be able to use the MCP.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "unstructured",
        "documents",
        "s3",
        "unstructured document",
        "unstructured mcp",
        "document processing"
      ],
      "category": "document-processing"
    },
    "Melbourneandrew--docs2prompt-mcp": {
      "owner": "Melbourneandrew",
      "name": "docs2prompt-mcp",
      "url": "https://github.com/Melbourneandrew/docs2prompt-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Melbourneandrew.webp",
      "description": "Transforms documentation from GitHub repositories or dedicated websites into LLM-friendly prompts for enhanced context and understanding in AI applications.",
      "stars": 0,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-03-21T02:59:39Z",
      "readme_content": "# MCP Server for docs2prompt\n\n[![](https://badge.mcpx.dev?type=server 'MCP Server')](https://modelcontextprotocol.io/introduction)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/rezabrizi/docs2prompt/blob/main/LICENSE)\n\n[docs2prompt](https://github.com/rezabrizi/docs2prompt) is a python library and line tool developed by [Reza Tabrizi](https://github.com/rezabrizi) that turns documentation in github repositories or hosted on dedicated websites into LLM-friendly prompts.\n\nThis repository contains an MCP server that wraps docs2prompt for use by any MCP client (Cursor, Claude, Windsurf, etc).\n\n## Run Server (Development)\n1. [Install UV](https://docs.astral.sh/uv/getting-started/installation/)\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n2. Clone the repository:\n```\ngit clone https://github.com/Melbourneandrew/docs2prompt-mcp\n```\n\n3. Put this in your MCP client config (Add your path and [github access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)):\n```\n{\n    \"mcpServers\": {\n        \"docs2prompt\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/YOUR/LOCAL/PATH/docs2prompt-mcp/src\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\": {\n                \"GITHUB_TOKEN\": \"\"\n            }\n        }\n    }\n}\n```\n\nIf you need, here are guides to set up MCP for common clients:\n* [Cursor](https://docs.cursor.com/context/model-context-protocol)\n* [Claude Desktop](https://modelcontextprotocol.io/quickstart/server#test-with-commands)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "docs2prompt",
        "document",
        "docs2prompt mcp",
        "documentation github",
        "melbourneandrew docs2prompt"
      ],
      "category": "document-processing"
    },
    "MeterLong--MCP-Doc": {
      "owner": "MeterLong",
      "name": "MCP-Doc",
      "url": "https://github.com/MeterLong/MCP-Doc",
      "imageUrl": "/freedevtools/mcp/pfp/MeterLong.webp",
      "description": "Create, edit, and manage Word documents using natural language commands, facilitating document operations and formatting. Support for table processing, image insertion, and layout control is also included.",
      "stars": 132,
      "forks": 15,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-04T00:28:46Z",
      "readme_content": "# Docx MCP Service\n\n[English](README.md) | [中文](README_ZH.md)\n\n[![smithery badge](https://smithery.ai/badge/@MeterLong/mcp-doc)](https://smithery.ai/server/@MeterLong/mcp-doc)\n\nA Docx document processing service based on the FastMCP library, supporting the creation, editing, and management of Word documents using AI assistants in Cursor.\n\n## Features\n\n- **Complete Document Operations**: Support for creating, opening, saving documents, as well as adding, editing, and deleting content\n- **Formatting**: Support for setting fonts, colors, sizes, alignment, and other formatting options\n- **Table Processing**: Support for creating, editing, merging, and splitting table cells\n- **Image Insertion**: Support for inserting images and setting their sizes\n- **Layout Control**: Support for setting page margins, adding page breaks, and other layout elements\n- **Query Functions**: Support for retrieving document information, paragraph content, and table data\n- **Convenient Editing**: Support for find and replace functionality\n- **Section Editing**: Support for replacing content in specific sections while preserving original formatting and styles\n\n## Installation Dependencies\n\nEnsure Python 3.10+ is installed, then install the following dependencies:\n\n```bash\npip3 install python-docx mcp\n```\n\n## Usage\n\n### Using as an MCP Service in Cursor\n\n1. Open Cursor and go to Settings\n2. Find the `Features > MCP Servers` section\n3. Click `Add new MCP server`\n4. Fill in the following information:\n   - Name: MCP_DOCX\n   - Type: Command\n   - Command: `python3 /path/to/MCP_dox/server.py` (replace with the actual path to your `server.py`)\n5. Click `Add` to add the service\n\nAfter adding, you can use natural language to operate Word documents in Cursor's AI assistant, for example:\n\n- \"Create a new Word document and save it to the desktop\"\n- \"Add a level 3 heading\"\n- \"Insert a 3x4 table and fill it with data\"\n- \"Set the second paragraph to bold and center-aligned\"\n\n## Supported Operations\n\nThe service supports the following operations:\n\n- **Document Management**: `create_document`, `open_document`, `save_document`\n- **Content Addition**: `add_paragraph`, `add_heading`, `add_table`, `add_picture`\n- **Content Editing**: `edit_paragraph`, `delete_paragraph`, `delete_text`\n- **Table Operations**: `add_table_row`, `delete_table_row`, `edit_table_cell`, `merge_table_cells`, `split_table`\n- **Layout Control**: `add_page_break`, `set_page_margins`\n- **Query Functions**: `get_document_info`, `get_paragraphs`, `get_tables`, `search_text`\n- **File Operations**: `create_document`, `open_document`, `save_document`, `save_as_document`, `create_document_copy`\n- **Section Editing**: `replace_section`, `edit_section_by_keyword`\n- **Other Functions**: `find_and_replace`, `search_and_replace` (with preview functionality)\n\n## How It Works\n\n1. The service uses the Python-docx library to process Word documents\n2. It implements the MCP protocol through the FastMCP library to communicate with AI assistants\n3. It processes requests and returns formatted responses\n4. It supports complete error handling and status reporting\n\n## Typography Capabilities\n\nThe service has good typography understanding capabilities:\n\n- **Text Hierarchy**: Support for heading levels (1-9) and paragraph organization\n- **Page Layout**: Support for page margin settings\n- **Visual Elements**: Support for font styles (bold, italic, underline, color) and alignment\n- **Table Layout**: Support for creating tables, merging cells, splitting tables, and setting table formats\n- **Pagination Control**: Support for adding page breaks\n\n## Development Notes\n\n- `server.py` - Core implementation of the MCP service using the FastMCP library\n\n## Troubleshooting\n\nIf you encounter problems in Cursor, try the following steps:\n\n1. Ensure Python 3.10+ is correctly installed\n2. Ensure the python-docx and mcp libraries are correctly installed\n3. Check if the server path is correct\n4. Restart the Cursor application\n\n## Notes\n\n- Ensure the python-docx and mcp libraries are correctly installed\n- Ensure Chinese characters in paths can be correctly processed\n- Using absolute paths can avoid path parsing issues\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "documents",
        "mcp",
        "mcp doc",
        "document processing",
        "word documents"
      ],
      "category": "document-processing"
    },
    "Mistizz--mcp-JapaneseTextAnalyzer": {
      "owner": "Mistizz",
      "name": "mcp-JapaneseTextAnalyzer",
      "url": "https://github.com/Mistizz/mcp-JapaneseTextAnalyzer",
      "imageUrl": "/freedevtools/mcp/pfp/Mistizz.webp",
      "description": "Analyzes Japanese and English texts by counting characters and words and evaluating linguistic features such as average sentence length and lexical diversity. Supports input via file paths or direct text input, accommodating both absolute and relative paths.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-25T18:29:31Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mistizz-mcp-japanesetextanalyzer-badge.png)](https://mseep.ai/app/mistizz-mcp-japanesetextanalyzer)\n\n# Japanese Text Analyzer MCP Server\n日本語テキストの形態素解析を行えるMCPサーバーです。文章の特徴を言語学的な観点から測定・評価し、文章生成のフィードバックに役立ちます。\n\n[![smithery badge](https://smithery.ai/badge/@Mistizz/mcp-JapaneseTextAnalyzer)](https://smithery.ai/server/@Mistizz/mcp-JapaneseTextAnalyzer)\n<a href=\"https://glama.ai/mcp/servers/@Mistizz/mcp-JapaneseTextAnalyzer\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Mistizz/mcp-JapaneseTextAnalyzer/badge\" alt=\"Japanese Text Analyzer MCP server\" />\n</a>\n\n## 機能\n\n- 日本語テキストの文字数（スペースや改行を除いた実質的な文字数）をカウント\n- 日本語テキストの単語数をカウント\n- 日本語テキストの詳細な言語的特徴の分析（平均文長、品詞の割合、語彙の多様性など）\n- ファイルパスまたは直接テキスト入力の両方に対応\n- 柔軟なファイルパス解決（絶対パス・相対パス・ファイル名のみでも検索可能）\n\n## Tools\n\n現在、以下のツールが実装されています：\n\n### count_chars\n\nファイルの文字数を計測します。絶対パスを指定してください（Windows形式 C:\\Users\\...、またはWSL/Linux形式 /c/Users/... のどちらも可）。スペースや改行を除いた実質的な文字数をカウントします。\n\n**入力:**\n- `filePath` (string): 文字数をカウントするファイルのパス（Windows形式かWSL/Linux形式の絶対パスを推奨）\n\n**出力:**\n- ファイルの文字数（スペースや改行を除外した実質的な文字数）\n\n### count_words\n\nファイルの単語数を計測します。絶対パスを指定してください（Windows形式 C:\\Users\\...、またはWSL/Linux形式 /c/Users/... のどちらも可）。英語ではスペースで区切られた単語をカウントし、日本語では形態素解析を使用します。\n\n**入力:**\n- `filePath` (string): 単語数をカウントするファイルのパス（Windows形式かWSL/Linux形式の絶対パスを推奨）\n- `language` (string, オプション, デフォルト: \"en\"): ファイルの言語 (en: 英語, ja: 日本語)\n\n**出力:**\n- ファイルの単語数\n- 日本語モードの場合は、形態素解析の詳細結果も表示\n\n### count_clipboard_chars\n\nテキストの文字数を計測します。スペースや改行を除いた実質的な文字数をカウントします。\n\n**入力:**\n- `text` (string): 文字数をカウントするテキスト\n\n**出力:**\n- テキストの文字数（スペースや改行を除外した実質的な文字数）\n\n### count_clipboard_words\n\nテキストの単語数を計測します。英語ではスペースで区切られた単語をカウントし、日本語では形態素解析を使用します。\n\n**入力:**\n- `text` (string): 単語数をカウントするテキスト\n- `language` (string, オプション, デフォルト: \"en\"): テキストの言語 (en: 英語, ja: 日本語)\n\n**出力:**\n- テキストの単語数\n- 日本語モードの場合は、形態素解析の詳細結果も表示\n\n### analyze_text\n\nテキストの詳細な形態素解析と言語的特徴の分析を行います。文の複雑さ、品詞の割合、語彙の多様性などを解析します。\n\n**入力:**\n- `text` (string): 分析するテキスト\n\n**出力:**\n- テキストの基本情報（総文字数、文の数、総形態素数）\n- 詳細分析結果（平均文長、品詞の割合、文字種の割合、語彙の多様性など）\n\n### analyze_file\n\nファイルの詳細な形態素解析と言語的特徴の分析を行います。文の複雑さ、品詞の割合、語彙の多様性などを解析します。\n\n**入力:**\n- `filePath` (string): 分析するファイルのパス（Windows形式かWSL/Linux形式の絶対パスを推奨）\n\n**出力:**\n- ファイルの基本情報（総文字数、文の数、総形態素数）\n- 詳細分析結果（平均文長、品詞の割合、文字種の割合、語彙の多様性など）\n\n## 使用方法\n\n### Installing via Smithery\n\nTo install Japanese Text Analyzer for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Mistizz/mcp-JapaneseTextAnalyzer):\n\n```bash\nnpx -y @smithery/cli install @Mistizz/mcp-JapaneseTextAnalyzer --client claude\n```\n\n### npxでの実行\n\nこのパッケージはnpxでGitHubリポジトリから直接実行できます：\n\n```bash\nnpx -y github:Mistizz/mcp-JapaneseTextAnalyzer\n```\n\n### Claude for Desktopでの使用\n\nClaude for Desktopの設定ファイルに以下を追加してください:\n\n**Windows:**\n`%AppData%\\Claude\\claude_desktop_config.json`\n\n**macOS:**\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"JapaneseTextAnalyzer\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:Mistizz/mcp-JapaneseTextAnalyzer\"\n      ]\n    }\n  }\n}\n```\n\n### Cursorでの使用\n\nCursorでも同様の設定を`.cursor`フォルダ内の`mcp.json`ファイルに追加します。\n\n**Windows:**\n`%USERPROFILE%\\.cursor\\mcp.json`\n\n**macOS/Linux:**\n`~/.cursor/mcp.json`\n\n一般的な設定(殆どの環境で動作):\n```json\n{\n  \"mcpServers\": {\n    \"JapaneseTextAnalyzer\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:Mistizz/mcp-JapaneseTextAnalyzer\"\n      ]\n    }\n  }\n}\n```\n\nWindows環境において、上記で動作しなかった場合、下記を試してみてください：\n```json\n{\n  \"mcpServers\": {\n    \"JapaneseTextAnalyzer\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"github:Mistizz/mcp-JapaneseTextAnalyzer\"\n      ]\n    }\n  }\n}\n```\n\n## 使用例\n\n### 直接テキストの文字数を数える\n```\nこのテキストの文字数を数えてください。\n```\n\n### ファイルの単語数を日本語モードで数える\n```\nC:\\path\\to\\your\\file.txt の単語数を日本語モードで数えてください。\n```\n\n### WSL/Linux形式のパスで単語数を数える\n```\n/c/Users/username/Documents/file.txt の単語数を日本語モードで数えてください。\n```\n\n### ファイル名だけで単語数を数える\n```\nREADME.md の単語数を英語モードで数えてください。\n```\n\n### テキストを貼り付けて日本語の単語数を数える\n```\n次のテキストの日本語の単語数を数えてください：\n\n吾輩は猫である。名前はまだ無い。どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\n```\n\n### テキストの詳細な言語的特徴を分析する\n```\n次のテキストを詳細に分析してください：\n\n私は昨日、新しい本を買いました。とても面白そうな小説で、友人からの評判も良かったです。今週末にゆっくり読む予定です。\n```\n\n### ファイルの詳細な言語的特徴を分析する\n```\nC:\\path\\to\\your\\file.txt を詳細に分析してください。\n```\n\n## ファイルパス解決機能\n\nこのツールは、ファイルパスが指定された場合に柔軟にファイルを探索します：\n\n1. 絶対パスが指定された場合はそのまま使用\n   - Windows形式の絶対パス（例: `C:\\Users\\username\\Documents\\file.txt`）\n   - WSL/Linux形式の絶対パス（例: `/c/Users/username/Documents/file.txt`）のどちらも自動的に検出・変換\n2. カレントディレクトリ（作業ディレクトリ）を基準に相対パスを解決\n3. ホームディレクトリ（`%USERPROFILE%`や`$HOME`）を基準に検索\n4. デスクトップディレクトリを基準に検索\n5. ドキュメントディレクトリを基準に検索\n\nこれにより、単に「README.md」のようなファイル名だけを指定しても、いくつかの一般的なディレクトリで自動的に検索し、ファイルが見つかった場合はそれを使用します。また、WSL環境やGit Bashなどから取得したパス（`/c/Users/...`形式）も、Windows環境でそのまま使用できます。\n\n## 内部動作について\n\nこのツールは、日本語の単語数カウントに「kuromoji.js」という形態素解析ライブラリを使用しています。形態素解析は自然言語処理の基本的な処理で、文章を意味を持つ最小単位（形態素）に分割します。\n\n形態素解析の処理は初期化に時間がかかることがあります。特に、辞書データを読み込む必要があるため、初回実行時に少々時間がかかる場合があります。サーバー起動時に形態素解析器の初期化を行うことで、ツール実行時の遅延を最小限に抑えています。\n\n### 言語的特徴の分析について\n\n「analyze_text」と「analyze_file」ツールは、形態素解析の結果に基づいて、テキストの様々な言語的特徴を計算します。これらには以下のような指標が含まれます：\n\n- **平均文長**: 一文あたりの平均文字数。この値が大きいほど、読みにくい文章である可能性があります。\n- **文あたりの形態素数**: 一文あたりの平均形態素数。文の密度や構文の複雑さを表します。\n- **品詞の割合**: 名詞・動詞・形容詞などの品詞がテキスト中でどのような割合で使われているかを示します。\n- **助詞の割合**: 特定の助詞がどのような頻度で使われているかを示し、文の構造や流れを分析します。\n- **文字種の割合**: ひらがな・カタカナ・漢字・英数字の構成比率を示します。\n- **語彙の多様性**: 異なった単語数と総単語数の比率（タイプ/トークン比）を示し、語彙の豊かさを計測します。\n- **カタカナ語の割合**: カタカナ語の使用頻度を示し、外来語や専門用語の多さ、文体のカジュアルさを反映します。\n- **敬語の頻度**: 敬語表現の使用頻度を示し、文章の丁寧さやフォーマル度を測定します。\n- **句読点の平均数**: 文あたりの句読点の平均数を示し、文の区切りや読みやすさに関する指標を提供します。\n\nこれらの指標を組み合わせることで、テキストの特性を多角的に分析し、文体や読みやすさ、専門性などを評価することができます。\n\n## ライセンス\n\nこのMCPサーバーはMITライセンスの下で提供されています。これは、MITライセンスの条件に従って、ソフトウェアを自由に使用、変更、配布できることを意味します。詳細については、プロジェクトリポジトリのLICENSEファイルをご覧ください。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "japanesetextanalyzer",
        "texts",
        "analyzes",
        "mcp japanesetextanalyzer",
        "japanesetextanalyzer analyzes",
        "analyzes japanese"
      ],
      "category": "document-processing"
    },
    "MushroomFleet--TranscriptionTools-MCP": {
      "owner": "MushroomFleet",
      "name": "TranscriptionTools-MCP",
      "url": "https://github.com/MushroomFleet/TranscriptionTools-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MushroomFleet.webp",
      "description": "Enhances transcription workflows by automatically repairing errors, formatting transcripts naturally, and generating concise summaries. Utilizes advanced language models for intelligent processing of audio transcripts.",
      "stars": 17,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T09:06:20Z",
      "readme_content": "# TranscriptionTools MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@MushroomFleet/transcriptiontools-mcp)](https://smithery.ai/server/@MushroomFleet/transcriptiontools-mcp)\n\nAn MCP server providing intelligent transcript processing capabilities, featuring natural formatting, contextual repair, and smart summarization powered by Deep Thinking LLMs.\n\n<a href=\"https://glama.ai/mcp/servers/in1wo7l928\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/in1wo7l928/badge\" alt=\"TranscriptionTools Server MCP server\" />\n</a>\n\n## Available MCP Tools\n\nThis MCP server exposes four powerful tools for transcript processing:\n\n1. **repair_text** - Analyzes and repairs transcription errors with greater than 90% confidence\n2. **get_repair_log** - Retrieves detailed analysis logs from previous repairs\n3. **format_transcript** - Transforms timestamped transcripts into naturally formatted text\n4. **summary_text** - Generates intelligent summaries using ACE cognitive methodology\n\n## Installation\n\n### Installing via Smithery\n\nTo install Transcription Tools for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MushroomFleet/transcriptiontools-mcp):\n\n```bash\nnpx -y @smithery/cli install @MushroomFleet/transcriptiontools-mcp --client claude\n```\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/mushroomfleet/TranscriptionTools-MCP\ncd TranscriptionTools-MCP\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Configure the MCP server in your MCP settings file:\n```json\n{\n  \"mcpServers\": {\n    \"transcription-tools\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/TranscriptionTools-MCP/build/index.js\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Using the MCP Tools\n\n### Repairing Transcription Errors\n\n```\n<use_mcp_tool>\n<server_name>transcription-tools</server_name>\n<tool_name>repair_text</tool_name>\n<arguments>\n{\n  \"input_text\": \"We recieve about ten thousand dollars which is defiantly not enough.\",\n  \"is_file_path\": false\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Formatting Timestamped Transcripts\n\n```\n<use_mcp_tool>\n<server_name>transcription-tools</server_name>\n<tool_name>format_transcript</tool_name>\n<arguments>\n{\n  \"input_text\": \"/path/to/timestamped-transcript.txt\",\n  \"is_file_path\": true,\n  \"paragraph_gap\": 8,\n  \"line_gap\": 4\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Generating Summaries\n\n```\n<use_mcp_tool>\n<server_name>transcription-tools</server_name>\n<tool_name>summary_text</tool_name>\n<arguments>\n{\n  \"input_text\": \"Long text to summarize...\",\n  \"is_file_path\": false,\n  \"constraint_type\": \"words\",\n  \"constraint_value\": 100\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Retrieving Repair Logs\n\n```\n<use_mcp_tool>\n<server_name>transcription-tools</server_name>\n<tool_name>get_repair_log</tool_name>\n<arguments>\n{\n  \"session_id\": \"20241206143022\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Core Technologies\n\n### Natural Formatting\n- Removes timestamps while preserving speech patterns\n- Applies intelligent spacing based on pause duration\n- Respects natural grammar and language flow\n- Maintains exact transcribed content\n\n### Contextual Repair\n- Identifies and corrects likely transcription errors\n- Uses semantic context for high-confidence corrections\n- Maintains detailed logs of all changes\n- 90% confidence threshold for corrections\n- No original audio required\n\n### Smart Summarization\n- Creates concise summaries of processed transcripts\n- Supports multiple constraint types:\n  - Time-based (speaking duration)\n  - Character count\n  - Word count\n- Preserves key information and context\n- Maintains natural speaking rhythm\n\n## Project Structure\n```\n/\n├── .gitignore         # Git ignore file\n├── LICENSE            # MIT license file\n├── README.md          # This documentation\n├── package.json       # Package dependencies and scripts\n├── tsconfig.json      # TypeScript configuration\n├── build/             # Compiled JavaScript files (generated after build)\n│   ├── tools/         # Compiled tool implementations\n│   └── utils/         # Compiled utility functions\n└── src/               # Source TypeScript files\n    ├── index.ts       # MCP server entry point\n    ├── tools/         # Tool implementations\n    │   ├── formatting.ts\n    │   ├── repair.ts\n    │   └── summary.ts\n    └── utils/         # Utility functions\n        ├── file-handler.ts\n        └── logger.ts\n```\n\n## Configuration\n\nYou can customize the server behavior by modifying the source code directly. The key configuration parameters are found in the respective tool implementation files:\n\n```typescript\n// In src/tools/formatting.ts\nconst paragraph_gap = 8; // seconds\nconst line_gap = 4;      // seconds\n\n// In src/tools/repair.ts\nconst confidence_threshold = 90; // percentage\n\n// In src/tools/summary.ts\nconst default_speaking_pace = 150; // words per minute\n```\n\n## License\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcriptiontools",
        "transcription",
        "transcripts",
        "mushroomfleet transcriptiontools",
        "transcriptiontools mcp",
        "transcription workflows"
      ],
      "category": "document-processing"
    },
    "MymInsomnia--textClassifier": {
      "owner": "MymInsomnia",
      "name": "textClassifier",
      "url": "https://github.com/MymInsomnia/textClassifier",
      "imageUrl": "/freedevtools/mcp/pfp/MymInsomnia.webp",
      "description": "Multiple common text classification models based on CNN, RNN, and pre-trained NLP architectures for sentiment analysis and text classification. Supports data preprocessing, training word embeddings, and implementing advanced models like Bi-LSTM, Transformer, ELMo, and BERT for improved classification accuracy.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2019-06-14T07:35:55Z",
      "readme_content": "### 文本分类项目\n***\n**本项目为基于CNN，RNN 和NLP中预训练模型构建的多个常见的文本分类模型。**\n\n#### requirements\n* python==3.5.6\n* tensorflow-gpu==1.10.0\n\n#### 1. 数据集\n&ensp;&ensp;数据集为IMDB电影评论的情感分析数据集，总共有三个部分：\n* 带标签的训练集：labeledTrainData.tsv\n* 不带标签的训练集：unlabeledTrainData.tsv\n* 测试集：testData.tsv\n\n&ensp;&ensp;字段的含义：\n* id  电影评论的id\n* review  电影评论的内容\n* sentiment  情感分类的标签（只有labeledTrainData.tsv数据集中有）\n\n#### 2. 数据预处理 \n&ensp;&ensp;数据预处理方法/dataHelper/processData.ipynb\n\n&ensp;&ensp;将原始数据处理成干净的数据，处理后的数据存储在/data/preProcess下，数据预处理包括：\n* 去除各种标点符号\n* 生成训练word2vec模型的输入数据 /data/preProcess/wordEmbedding.txt\n\n#### 3. 训练word2vec词向量\n&ensp;&ensp;预训练word2vec词向量/word2vec/genWord2Vec.ipynb\n* 预训练的词向量保存为bin格式 /word2vec/word2Vec.bin\n\n#### 4. textCNN 文本分类\n&ensp;&ensp;textCNN模型来源于论文[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n\n&ensp;&ensp;textCNN可以看作是一个由三个单层的卷积网络的输出结果进行拼接的融合模型，作者提出了三种大小的卷积核[3, 4, 5]，卷积核的滑动使得其\n类似于NLP中的n-grams，因此当你需要更多尺度的n-grams时，你可以选择增加不同大小的卷积核，比如大小为2的卷积核可以代表\n2-grams.\n\n&ensp;&ensp;textCNN代码在/textCNN/textCNN.ipynb。实现包括四个部分：\n* 参数配置类 Config （包括训练参数，模型参数和其他参数）\n* 数据预处理类 Dataset （包括生成词汇空间，获得预训练词向量，分割训练集和验证集）\n* textCNN模型类 TextCNN\n* 模型训练\n\n#### 5. charCNN 文本分类\n&ensp;&ensp;textCNN模型来源于论文[Character-level Convolutional Networks for Text\nClassification](https://arxiv.org/abs/1509.01626)\n\n&ensp;&ensp;char-CNN是一种基于字符级的文本分类器，将所有的文本都用字符表示，\n*注意这里的数据预处理时不可以去掉标点符号或者其他的各种符号，最好是保存论文中提出的69种字符，我一开始使用去掉特殊符号的字符后的文本输入到模型中会无法收敛*。\n此外由于训练数据集比较少，即使论文中最小的网络也无法收敛，此时可以减小模型的复杂度，包括去掉一些卷积层等。\n\n&ensp;&ensp;charCNN代码在/charCNN/charCNN.ipynb。实现也包括四个部分，也textCNN一致，但是在这里的数据预处理有很大不一样，剩下\n的就是模型结构不同，此外模型中可以引入BN层来对每一层的输出做归一化处理。\n\n#### 6. Bi-LSTM 文本分类\n&ensp;&ensp;Bi-LSTM可以参考我的博客[深度学习之从RNN到LSTM](https://www.cnblogs.com/jiangxinyang/p/9362922.html)\n\n&ensp;&ensp;Bi-LSTM是双向LSTM，LSTM是RNN的一种，是一种时序模型，Bi-LSTM是双向LSTM，旨在同时捕获文本中上下文的信息，\n在情感分析类的问题中有良好的表现。\n\n&ensp;&ensp;Bi-LSTM的代码在/Bi-LSTM/Bi-LSTM.ipynb中。除了模型类的代码有改动，其余代码几乎和textCNN一样。\n\n#### 7. Bi-LSTM + Attention 文本分类\n&ensp;&ensp;Bi-LSTM + Attention模型来源于论文[Attention-Based Bidirectional Long Short-Term Memory Networks for\nRelation Classification](http://aclweb.org/anthology/Y/Y15/Y15-1009.pdf)\n\n&ensp;&ensp;Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量\n作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序\n的向量进行加权和作为特征向量，然后进行softmax分类。在实验中，加上Attention确实对结果有所提升。\n\n&ensp;&ensp;Bi-LSTM + Attention的代码在/Bi-LSTM+Attention/Bi-LSTMAttention.ipynb中，除了模型类中\n加入Attention层，其余代码和Bi-LSTM一致。\n\n#### 8. RCNN 文本分类\n&ensp;&ensp;RCNN模型来源于论文[Recurrent Convolutional Neural Networks for Text Classification](https://arxiv.org/abs/1609.04243)\n\n&ensp;&ensp;RCNN 整体的模型构建流程如下：\n* 利用Bi-LSTM获得上下文的信息，类似于语言模型\n* 将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput, wordEmbedding, bwOutput]\n* 将拼接后的向量非线性映射到低维\n* 向量中的每一个位置的值都取所有时序上的最大值，得到最终的特征向量，该过程类似于max-pool\n* softmax分类\n\n&ensp;&ensp;RCNN的代码在/RCNN/RCNN.ipynb中。\n\n#### 9. adversarialLSTM 文本分类\n&ensp;&ensp;Adversarial LSTM模型来源于论文[Adversarial Training Methods\nFor Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725)\n\n&ensp;&ensp;adversarialLSTM的核心思想是通过对word Embedding上添加噪音生成对抗样本，将对抗样本以和原始样本\n同样的形式喂给模型，得到一个Adversarial Loss，通过和原始样本的loss相加得到新的损失，通过优化该新\n的损失来训练模型，作者认为这种方法能对word embedding加上正则化，避免过拟合。\n\n&ensp;&ensp;adversarialLSTM的代码在/adversarialLSTM/adversarialLSTM.ipynb中。\n\n#### 10. Transformer 文本分类\n&ensp;&ensp;Transformer模型来源于论文[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n&ensp;&ensp;Transformer模型有两个结构：Encoder和Decoder，在进行文本分类时只需要用到\nEncoder结构，Decoder结构是生成式模型，用于自然语言生成的。Transformer的核心结构是\nself-Attention机制，具体的介绍见[Transformer模型（Atention is all you need）](https://www.cnblogs.com/jiangxinyang/p/10069330.html)。\n\n&ensp;&ensp;Transformer模型的代码在/Transformer/transformer.ipynb中。\n\n#### 11. ELMo预训练模型 文本分类\n&ensp;&ensp;ELMo模型来源于论文[Deep contextualized word representations](https://arxiv.org/abs/1802.05365?context=cs)\n\n&ensp;&ensp;ELMo的结构是BiLM（双向语言模型），基于ELMo的预训练模型能动态地生成\n词的向量表示，具体的介绍见[ELMO模型（Deep contextualized word representation）](https://www.cnblogs.com/jiangxinyang/p/10060887.html)\n\n&ensp;&ensp;ELMo预训练模型用于文本分类的代码位于/ELMo/elmo.ipynb中。\n/ELMo/bilm/下是ELMo项目中的源码，/ELMo/modelParams/下是各种文件。\n\n#### 12. Bert预训练模型 文本分类\n&ensp;&ensp;BERT模型来源于论文[BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding](https://arxiv.org/abs/1810.04805)\n\n&ensp;&ensp;BERT模型是基于双向Transformer实现的语言模型，集预训练和下游任务于一个模型中，\n因此在使用的时候我们不需要搭建自己的下游任务模型，直接用BERT模型即可，我们将谷歌开源的源码下载\n下来放在bert文件夹中，在进行文本分类只需要修改run_classifier.py文件即可，另外我们需要将训练集\n和验证集分割后保存在两个不同的文件中，放置在/BERT/data下。然后还需要下载谷歌预训练好的模型放置在\n/BERT/modelParams文件夹下，还需要建一个/BERT/output文件夹用来放置训练后的模型文件\n\n&ensp;&ensp;做完上面的步骤之后只要执行下面的脚本即可\n\n&ensp;&ensp;export BERT_BASE_DIR=../modelParams/uncased_L-12_H-768_A-12\n\n&ensp;&ensp;export DATASET=../data/\n\n&ensp;&ensp;python run_classifier.py \\\n  &ensp;&ensp;&ensp;&ensp;--data_dir=$MY_DATASET \\\n  &ensp;&ensp;&ensp;&ensp;--task_name=imdb \\\n  &ensp;&ensp;&ensp;&ensp;--vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  &ensp;&ensp;&ensp;&ensp;--bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  &ensp;&ensp;&ensp;&ensp;--output_dir=../output/ \\\n  &ensp;&ensp;&ensp;&ensp;--do_train=true \\\n  &ensp;&ensp;&ensp;&ensp;--do_eval=true \\\n  &ensp;&ensp;&ensp;&ensp;--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  &ensp;&ensp;&ensp;&ensp;--max_seq_length=200 \\\n  &ensp;&ensp;&ensp;&ensp;--train_batch_size=16 \\\n  &ensp;&ensp;&ensp;&ensp;--learning_rate=5e-5\\\n  &ensp;&ensp;&ensp;&ensp;--num_train_epochs=3.0\n\n&ensp;&ensp;BERT模型用于文本分类的详细使用可以看我的博客\n[文本分类实战（十）—— BERT 预训练模型](https://www.cnblogs.com/jiangxinyang/p/10241243.html)\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "textclassifier",
        "lstm",
        "nlp",
        "trained nlp",
        "text classification",
        "myminsomnia textclassifier"
      ],
      "category": "document-processing"
    },
    "OpenTorah-ai--mcp-sefaria-server": {
      "owner": "OpenTorah-ai",
      "name": "mcp-sefaria-server",
      "url": "https://github.com/OpenTorah-ai/mcp-sefaria-server",
      "imageUrl": "/freedevtools/mcp/pfp/OpenTorah-ai.webp",
      "description": "Access and reference Jewish texts and commentaries through a standardized interface.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-02-25T08:57:14Z",
      "readme_content": "# Sefaria Jewish Library MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-sefaria-server)](https://smithery.ai/server/mcp-sefaria-server)\nAn MCP (Model Context Protocol) server that provides access to Jewish texts from the Sefaria library. This server enables Large Language Models to retrieve and reference Jewish texts through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/j3v6vnp4xk\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/j3v6vnp4xk/badge\" alt=\"Sefaria Jewish Library Server MCP server\" /></a>\n\n## Features\n\n- Retrieve Jewish texts by reference\n- Retrieve commentaries on a given text\n\n## Installation\n\nRequires Python 3.10 or higher.\n\n### Installing via Smithery\n\nTo install Sefaria Jewish Library for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-sefaria-server):\n\n```bash\nnpx -y @smithery/cli install mcp-sefaria-server --client claude\n```\n\n### Clone the repository\n```bash\ngit clone https://github.com/sivan22/mcp-sefaria-server.git\ncd mcp-sefaria-server\n```\n\n\n## Running the Server\n\nThe server can be run directly:\n\n```bash\nuv --directory path/to/directory run sefaria_jewish_library\n```\n\nOr through an MCP client that supports the Model Context Protocol.\nfor claude desktop app and cline you should use the following config:\n```\n{\n  \"mcpServers\": {        \n      \"sefaria_jewish_library\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"C:/dev/mcp-sefaria-server\",\n              \"run\",\n              \"sefaria_jewish_library\"\n          ],\n          \"env\": {\n            \"PYTHONIOENCODING\": \"utf-8\" \n          }\n      }\n  }\n}\n```\n\n## Available tools\n\nThe server provides the following tools through the MCP interface:\n\n### get_text\n\nRetrieves a specific Jewish text by its reference.\n\nExample:\n```\nreference: \"Genesis 1:1\"\nreference: \"שמות פרק ב פסוק ג\"\nreference: \"משנה ברכות פרק א משנה א\"\n```\n\n### get_commentaries\n\nRetrieves a list of commentaries for a given text.\n\nExample:\n```\nreference: \"Genesis 1:1\"\nreference: \"שמות פרק ב פסוק ג\"\nreference: \"משנה ברכות פרק א משנה א\"\n```\n\n## Development\n\nThis project uses:\n- [MCP SDK](https://github.com/modelcontextprotocol/sdk) for server implementation\n- [Sefaria API](https://github.com/Sefaria/Sefaria-API) for accessing Jewish texts\n\n## Requirements\n\n- Python >= 3.10\n- MCP SDK >= 1.1.1\n- Sefaria API\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "commentaries",
        "texts",
        "jewish",
        "jewish texts",
        "texts commentaries",
        "commentaries standardized"
      ],
      "category": "document-processing"
    },
    "PARS-DOE--autodocument": {
      "owner": "PARS-DOE",
      "name": "autodocument",
      "url": "https://github.com/PARS-DOE/autodocument",
      "imageUrl": "/freedevtools/mcp/pfp/PARS-DOE.webp",
      "description": "Generates comprehensive documentation, test plans, and code reviews by analyzing code repositories and directory structures. Utilizes AI to enhance development workflows with detailed insights into security and best practices.",
      "stars": 5,
      "forks": 4,
      "license": "Creative Commons Zero v1.0 Universal",
      "language": "TypeScript",
      "updated_at": "2025-05-26T15:04:36Z",
      "readme_content": "# Autodocument MCP Server\n\nAn MCP (Model Context Protocol) server that automatically generates documentation for code repositories by analyzing directory structures and code files using OpenRouter API.\n\n\n## Features\n\n- **Smart Directory Analysis**: Recursively analyzes directories and files in a code repository\n- **Git Integration**: Respects `.gitignore` patterns to skip ignored files\n- **AI-Powered Documentation**: Uses OpenRouter API (with Claude 3.7 by default) to generate comprehensive documentation\n- **Test Plan Generation**: Automatically creates test plans with suitable test types, edge cases, and mock requirements\n- **Code Review**: Performs senior developer-level code reviews focused on security, best practices, and improvements\n- **Bottom-Up Approach**: Starts with leaf directories and works upward, creating a coherent documentation hierarchy\n- **Intelligent File Handling**:\n  - Creates `documentation.md`, `testplan.md`, and `review.md` files at each directory level\n  - Skips single-file directories but includes their content in parent outputs\n  - Supports updating existing files\n  - Creates fallback files for directories that exceed limits\n- **Progress Reporting**: Provides detailed progress updates to prevent timeouts in long-running operations\n- **Highly Configurable**: Customize file extensions, size limits, models, prompts, and more\n- **Extensible Architecture**: Modular design makes it easy to add more auto-* tools in the future\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v16 or newer)\n- An [OpenRouter API key](https://openrouter.ai/)\n\n### Installation Steps\n\n```bash\n# Clone the repository\ngit clone https://github.com/PARS-DOE/autodocument.git\ncd autodocument\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\nConfigure autodocument using environment variables, command-line arguments, or an MCP configuration file:\n\n### Environment Variables\n\n- `OPENROUTER_API_KEY`: Your OpenRouter API key\n- `OPENROUTER_MODEL`: Model to use (default: `anthropic/claude-3-7-sonnet`)\n- `MAX_FILE_SIZE_KB`: Maximum file size in KB (default: 100)\n- `MAX_FILES_PER_DIR`: Maximum number of files per directory (default: 20)\n\n## Using with Roo or Cline\n\nRoo Code and Cline are AI assistants that support the Model Context Protocol (MCP), which allows them to use external tools like autodocument.\n\n### Setup for Roo/Cline\n\n1. **Clone and build the repository** (follow the Installation Steps above)\n\n2. **Configure the MCP server**:\n\n   #### For Roo:\n\n   In the MCP Servers menu, Edit the MCP Settings and add the autodocument configuration using the full path to where you cloned the repository:\n\n   Add the autodocument configuration using the full path to where you cloned the repository:\n   ```json\n   {\n     \"mcpServers\": {\n       \"autodocument\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/autodocument/build/index.js\"],\n         \"env\": {\n           \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n         },\n         \"disabled\": false,\n         \"alwaysAllow\": []\n       }\n     }\n   }\n   ```\n\n   #### For Claude Desktop App:\n   Edit the Claude desktop app configuration file at:\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n   Add the autodocument configuration using the full path to where you cloned the repository:\n   ```json\n   {\n     \"mcpServers\": {\n       \"autodocument\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/autodocument/build/index.js\"],\n         \"env\": {\n           \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n         },\n         \"disabled\": false,\n         \"alwaysAllow\": []\n       }\n     }\n   }\n   ```\n\n3. **Important:** Make sure to use absolute paths to the build/index.js file in your cloned repository\n\n4. **Restart Roo/Cline or the Claude desktop app**\n\n4. **Use the tool**:\n   In a conversation with Roo or Claude, you can now ask it to generate documentation or test plans for your code repository:\n   ```\n   Please generate documentation for my project at /path/to/my/project\n   ```\n   \n   Or for test plans:\n   ```\n   Please create a test plan for my project at /path/to/my/project\n   ```\n   \n   Or for code reviews:\n   ```\n   Please review the code in my project at /path/to/my/project\n   ```\n\n## How It Works\n\nThe autodocument server works using a bottom-up approach:\n\n1. **Discovery**: Scans the target directory recursively, respecting `.gitignore` rules\n2. **Smart Directory Processing**: \n   - Identifies directories with multiple code files or subdirectories\n   - Skips single-file directories but includes their content in parent documentation\n3. **File Analysis**: Analyzes code files, filtering by extension and size\n4. **Documentation Generation**: For each qualifying directory:\n   - Reads code files\n   - Sends code to OpenRouter API with optimized prompts\n   - Creates a `documentation.md` file (or updates existing one)\n5. **Aggregation**: As it moves up the directory tree:\n   - Processes each parent directory\n   - Includes documentation from child directories\n   - Creates a comprehensive overview at each level\n\n## Architecture\n\nThe project follows a modular architecture:\n\n- **Core Components**: Configuration management and server implementation\n- **Crawler Module**: Directory traversal and file discovery\n- **Analyzer Module**: Code file analysis and filtering\n- **OpenRouter Module**: AI integration for LLM-based content generation\n- **Documentation Module**: Orchestration of the documentation process\n- **Tools Module**: Extensible system for different auto-* tools (documentation, test plans, etc.)\n- **Prompts Configuration**: Centralized prompt management for easy customization\n\n## Example Usage\n\n### Command Line\n\n```bash\n# Navigate to your cloned repository\ncd path/to/cloned/autodocument\n\n# Set your API key (or configure in environment variables)\nexport OPENROUTER_API_KEY=your-api-key-here\n\n# Run documentation generation on a project\nnode build/index.js /path/to/your/project\n```\n\n### Programmatic Usage\n\n```javascript\nconst { spawn } = require('child_process');\nconst path = require('path');\n\n// Path to your project\nconst projectPath = '/path/to/your/project';\n\n// Your OpenRouter API key\nconst apiKey = 'your-api-key-here';\n\n// Create a JSON command to simulate an MCP tool call\nconst toolCallCommand = JSON.stringify({\n  jsonrpc: '2.0',\n  method: 'call_tool',\n  params: {\n    name: 'generate_documentation',\n    arguments: {\n      path: projectPath,\n      openRouterApiKey: apiKey\n    }\n  },\n  id: 1\n});\n\n// Start the server process - use the full path to your cloned repository\nconst serverProcess = spawn('node', ['/path/to/autodocument/build/index.js'], {\n  env: {\n    ...process.env,\n    OPENROUTER_API_KEY: apiKey\n  }\n});\n\n// Send the tool command\nserverProcess.stdin.write(toolCallCommand + '\\n');\n\n// Handle server output and errors\n// ...\n```\n\n## Customizing Prompts\n\nYou can easily customize the prompts used by the tools by editing the `src/prompt-config.ts` file. This allows you to:\n\n- Adjust the tone and style of generated content\n- Add specific instructions for your project's needs\n- Modify how existing content is updated\n\nThe prompt configuration is separated from the tool implementation, making it easy to experiment with different prompts without changing the code.\n\n## Available Tools\n\n### generate_documentation\n\nGenerates comprehensive documentation for a code repository:\n```\n{\n  \"path\": \"/path/to/your/project\",\n  \"openRouterApiKey\": \"your-api-key-here\", // Optional\n  \"model\": \"anthropic/claude-3-7-sonnet\", // Optional\n  \"updateExisting\": true // Optional, defaults to true\n}\n```\n\n### autotestplan\n\nGenerates test plans for functions and components in a code repository:\n```\n{\n  \"path\": \"/path/to/your/project\",\n  \"openRouterApiKey\": \"your-api-key-here\", // Optional\n  \"model\": \"anthropic/claude-3-7-sonnet\", // Optional\n  \"updateExisting\": true // Optional, defaults to true\n}\n```\n\n### autoreview\n\nGenerates a senior developer-level code review for a repository:\n```\n{\n  \"path\": \"/path/to/your/project\",\n  \"openRouterApiKey\": \"your-api-key-here\", // Optional\n  \"model\": \"anthropic/claude-3-7-sonnet\", // Optional\n  \"updateExisting\": true // Optional, defaults to true\n}\n```\n\n## Output Files\n\nThe server creates several types of output files:\n\n### documentation.md\n\nContains comprehensive documentation of the code in a directory, including:\n- Purpose of the code\n- Key functions and classes\n- Relationships between files\n- Integration with child components\n\n### testplan.md\n\nContains detailed test plans for code in a directory, including:\n- Appropriate test types (unit, integration, e2e) for each function\n- Common edge cases to test\n- Dependency mocking requirements\n- Integration testing strategies\n\n### review.md\n\nContains senior developer-level code review feedback, including:\n- Security issues and vulnerabilities\n- Best practice violations\n- Potential bugs or architectural concerns\n- Opportunities for refactoring\n- Practical, constructive feedback (not nitpicking style issues)\n\n### Fallback Files\n\nCreated when a directory exceeds size or file count limits:\n- `undocumented.md` - For documentation generation\n- `untested.md` - For test plan generation\n- `review-skipped.md` - For code review generation\n\nThese files contain:\n- Reason for skipping processing\n- List of files that were analyzed and excluded\n- Instructions on how to fix (increase limits or manually create content)\n\n## Troubleshooting\n\n### API Key Issues\n\nIf you see errors about invalid API key:\n- Ensure you've set the `OPENROUTER_API_KEY` environment variable\n- Check that your OpenRouter account is active\n- Verify you have sufficient credits for the API calls\n\n### Size Limit Errors\n\nIf too many directories are skipped due to size limits:\n- Set environment variables to increase limits: `MAX_FILE_SIZE_KB` and `MAX_FILES_PER_DIR`\n- Consider documenting very large directories manually\n\n### Model Selection\n\nIf you're not satisfied with the documentation quality:\n- Try a different model by setting the `OPENROUTER_MODEL` environment variable\n\n## License\n\nCC0-1.0 License - This work is dedicated to the public domain under CC0 by the United States Department of Energy\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n### Adding New Tools\n\nThe architecture is designed to make it easy to add new auto-* tools:\n\n1. Create a new class that extends `BaseTool` in the `src/tools` directory\n2. Define the prompts in `src/prompt-config.ts`\n3. Register the tool in the `ToolRegistry`\n\nSee the existing tools for examples of how to implement new functionality.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "autodocument",
        "documentation",
        "document",
        "autodocument generates",
        "doe autodocument",
        "document processing"
      ],
      "category": "document-processing"
    },
    "PV-Bhat--GemForge-MCP": {
      "owner": "PV-Bhat",
      "name": "GemForge-MCP",
      "url": "https://github.com/PV-Bhat/GemForge-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/PV-Bhat.webp",
      "description": "Provides tools for interacting with Google's Gemini AI models, enabling intelligent model selection and advanced file handling. Facilitates AI tasks such as search, reasoning, code analysis, and file operations through a standardized MCP server interface.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-09T19:47:04Z",
      "readme_content": "# GemForge (Gemini Tools)\n<img src=\"https://github.com/user-attachments/assets/8cee4293-b0e0-461f-a9d9-f750397aa2b5\" alt=\"GemForgeLogo\" width=\"100\" height=\"100\">\n\n<img src=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP/badge\" alt=\"Glama Badge\" width=\"210\" height=\"110\">\n\n## Overview\n\n[![Smithery Badge](https://smithery.ai/badge/@PV-Bhat/gemforge-gemini-tools-mcp)](https://smithery.ai/server/@PV-Bhat/gemforge-gemini-tools-mcp)\n[![MCP.so](https://img.shields.io/badge/MCP-Directory-blue)](https://mcp.so/server/gemforge-gemini-tools-mcp/PV-Bhat)\n<a href=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP/badge\" alt=\"GemForge-Gemini-Tools-MCP MCP server\" />\n</a>\n\n\n**GemForge-Gemini-Tools-MCP**: Enterprise-grade Gemini integration for your favorite MCP agents. Supercharge Claude, Roo Code, and Windsurf with codebase analysis, live search, text/PDF/image processing, and more.\n\n## Quick Navigation\n\n- [Features](#why-gemforge)\n- [Quick Start](#quick-start)\n- [Configuration](#configuration)\n- [Tools](#key-tools)\n- [Heavy-Duty Reliability](#heavy-duty-reliability)\n- [Deployment](#deployment)\n- [Examples](#examples)\n- [Community](#community--support)\n- [Documentation](#documentation)\n\n## Why GemForge?\n\nGemForge is the essential bridge between Google's Gemini AI and the MCP ecosystem:\n![gemfog](https://github.com/user-attachments/assets/18cee069-d176-40c8-8ff9-3d643d918bc4)\n\n- **Real-Time Web Access**: Fetch breaking news, market trends, and current data with `gemini_search`\n- **Advanced Reasoning**: Process complex logic problems with step-by-step thinking via `gemini_reason`\n- **Code Mastery**: Analyze full repositories, generate solutions, and debug code with `gemini_code`\n- **Multi-File Processing**: Handle 60+ file formats including PDFs, images, and more with `gemini_fileops`\n\n- **Intelligent Model Selection**: Automatically routes to optimal Gemini model for each task\n  \n- **Enterprise-Ready**: Robust error handling, rate limit management, and API fallback mechanisms\n\n## Quick Start\n\n### One-Line Install\n\n```bash\nnpx @gemforge/mcp-server@latest init\n```\n\n### Manual Setup\n\n1. Create configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"GemForge\": {\n      \"command\": \"node\",\n      \"args\": [\"./dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n2. Install and run:\n\n```bash\nnpm install gemforge-mcp\nnpm start\n```\n\n[Watch 30-second setup demo →](https://www.youtube.com/your-demo-link)\n\n## Heavy-Duty Reliability\n\nGemForge is built for production environments:\n\n- **Support for 60+ File Types**: Process everything from code to documents to images\n- **Automatic Model Fallbacks**: Continues functioning even during rate limits or service disruptions\n- **Enterprise-Grade Error Logging**: Detailed diagnostics for troubleshooting\n- **API Resilience**: Exponential backoff, retry logic, and seamless model switching\n- **Full Repository Support**: Analyze entire codebases with configurable inclusion/exclusion patterns\n- **XML Content Processing**: Specialized handling for structured data\n\n## Key Tools\n\n| Tool | Description | Key Capability |\n|------|-------------|----------------|\n| `gemini_search` | Web-connected information retrieval | Real-time data access |\n| `gemini_reason` | Complex problem solving with step-by-step logic | Transparent reasoning process |\n| `gemini_code` | Deep code understanding and generation | Full repository analysis |\n| `gemini_fileops` | Multi-file processing across 60+ formats | Document comparison and transformation |\n\n<details>\n<summary><strong>Example: Real-Time Search</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_search\",\n  \"toolParams\": {\n    \"query\": \"Latest advancements in quantum computing\",\n    \"enable_thinking\": true\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Example: Code Analysis</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_code\",\n  \"toolParams\": {\n    \"question\": \"Identify improvements and new features\",\n    \"directory_path\": \"path/to/project\",\n    \"repomix_options\": \"--include \\\"**/*.js\\\" --no-gitignore\"\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Example: Multi-File Comparison</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_fileops\",\n  \"toolParams\": {\n    \"file_path\": [\"contract_v1.pdf\", \"contract_v2.pdf\"],\n    \"operation\": \"analyze\",\n    \"instruction\": \"Compare these contract versions and extract all significant changes.\"\n  }\n}\n```\n</details>\n\n## Configuration\n\nGemForge offers flexible configuration options:\n\n<details>\n<summary><strong>Environment Variables</strong></summary>\n\n```\nGEMINI_API_KEY=your_api_key_here       # Required: Gemini API key\nGEMINI_PAID_TIER=true                  # Optional: Set to true if using paid tier (better rate limits)\nDEFAULT_MODEL_ID=gemini-2.5-pro        # Optional: Override default model selection\nLOG_LEVEL=info                         # Optional: Set logging verbosity (debug, info, warn, error)\n```\n</details>\n\n<details>\n<summary><strong>Claude Desktop Integration</strong></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"GemForge\": {\n      \"command\": \"node\",\n      \"args\": [\"./dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Advanced Model Selection</strong></summary>\n\nGemForge intelligently selects the best model for each task:\n- `gemini_search`: Uses `gemini-2.5-flash` for speed and search integration\n- `gemini_reason`: Uses `gemini-2.5-pro` for deep reasoning capabilities\n- `gemini_code`: Uses `gemini-2.5-pro` for complex code understanding\n- `gemini_fileops`: Selects between `gemini-2.0-flash-lite` or `gemini-1.5-pro` based on file size\n\nOverride with `model_id` parameter in any tool call or set `DEFAULT_MODEL_ID` environment variable.\n</details>\n\n## Deployment\n\n### Smithery.ai\nOne-click deployment via [Smithery.ai](https://smithery.ai/server/@PV-Bhat/gemforge-gemini-tools-mcp)\n\n### Docker\n```bash\ndocker run -e GEMINI_API_KEY=your_api_key ghcr.io/pv-bhat/gemforge:latest\n```\n\n### Self-Hosted\nUse our [MCP.so Directory listing](https://mcp.so/server/gemforge-gemini-tools-mcp/PV-Bhat) for integration instructions.\n\n## What Sets GemForge Apart?\n\n- **Cross-Ecosystem Power**: Bridge Google's AI with Claude and other MCP agents\n- **Multi-File Analysis**: Compare documents, images, or code versions\n- **Smart Routing**: Automatic model selection based on task requirements\n- **Production-Ready**: Built for enterprise environments\n\n![GemForge in Action](docs/assets/gemforge-demo.gif)\n\n## Community & Support\n\n- **Join Us**: [MCP Discord](https://discord.me/mcp) | [GemForge Discord](https://discord.gg/your-invite-link)\n- **Contribute**: [GitHub Discussions](https://github.com/your-username/GemForge/discussions)\n- **Feedback**: Open an issue or share thoughts on Discord\n\n## Documentation\n\nVisit our [Documentation Site](https://your-username.github.io/GemForge) for:\n- Advanced usage tutorials\n- API reference\n- Troubleshooting tips\n\n## License\n\nLicensed under the MIT License. See [LICENSE](LICENSE) for details.\n\n## Acknowledgments\n- Google Gemini API for providing the underlying AI capabilities\n- Model Context Protocol (MCP) for standardizing AI tool interfaces\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "gemforge",
        "processing",
        "gemforge mcp",
        "document processing",
        "mcp provides"
      ],
      "category": "document-processing"
    },
    "ProbonoBonobo--sui-mcp-server": {
      "owner": "ProbonoBonobo",
      "name": "sui-mcp-server",
      "url": "https://github.com/ProbonoBonobo/sui-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ProbonoBonobo.webp",
      "description": "Enables AI agents to retrieve documents from a vector database using Retrieval-Augmented Generation (RAG) techniques. Integrates with GitHub to process Move files and incorporates a language model for generating responses based on retrieved information.",
      "stars": 4,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-30T18:47:54Z",
      "readme_content": "# MCP Server with FAISS for RAG\n\nThis project provides a proof-of-concept implementation of a Machine Conversation Protocol (MCP) server that allows an AI agent to query a vector database and retrieve relevant documents for Retrieval-Augmented Generation (RAG).\n\n## Features\n\n- FastAPI server with MCP endpoints\n- FAISS vector database integration\n- Document chunking and embedding\n- GitHub Move file extraction and processing\n- LLM integration for complete RAG workflow\n- Simple client example\n- Sample documents\n\n## Installation\n\n### Using pipx (Recommended)\n\n[pipx](https://pypa.github.io/pipx/) is a tool to help you install and run Python applications in isolated environments.\n\n1. First, install pipx if you don't have it:\n\n```bash\n# On macOS\nbrew install pipx\npipx ensurepath\n\n# On Ubuntu/Debian\nsudo apt update\nsudo apt install python3-pip python3-venv\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\n\n# On Windows with pip\npip install pipx\npipx ensurepath\n```\n\n2. Install the MCP Server package directly from the project directory:\n\n```bash\n# Navigate to the directory containing the mcp_server folder\ncd /path/to/mcp-server-project\n\n# Install in editable mode\npipx install -e .\n```\n\n3. (Optional) Configure environment variables:\n   - Copy `.env.example` to `.env` \n   - Add your GitHub token for higher rate limits: `GITHUB_TOKEN=your_token_here`\n   - Add your OpenAI or other LLM API key for RAG integration: `OPENAI_API_KEY=your_key_here`\n\n### Manual Installation\n\nIf you prefer not to use pipx:\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\ncd mcp_server\npip install -r requirements.txt\n```\n\n## Usage with pipx\n\nAfter installing with pipx, you'll have access to the following commands:\n\n### Downloading Move Files from GitHub\n\n```bash\n# Download Move files with default settings\nmcp-download --query \"use sui\" --output-dir docs/move_files\n\n# Download with more options\nmcp-download --query \"module sui::coin\" --max-results 50 --new-index --verbose\n```\n\n### Improved GitHub Search and Indexing (Recommended)\n\n```bash\n# Search GitHub and index files with default settings\nmcp-search-index --keywords \"sui move\"\n\n# Search multiple keywords and customize options\nmcp-search-index --keywords \"sui move,move framework\" --max-repos 30 --output-results --verbose\n\n# Save search results and use a custom index location\nmcp-search-index --keywords \"sui coin,sui::transfer\" --index-file custom/path/index.bin --output-results\n```\n\nThe `mcp-search-index` command provides enhanced GitHub repository search capabilities:\n- Searches repositories first, then recursively extracts Move files\n- Supports multiple search keywords (comma-separated)\n- Intelligently filters for Move files containing \"use sui\" references\n- Always rebuilds the vector database after downloading\n\n### Indexing Move Files\n\n```bash\n# Index files in the default location\nmcp-index\n\n# Index with custom options\nmcp-index --docs-dir path/to/files --index-file path/to/index.bin --verbose\n```\n\n### Querying the Vector Database\n\n```bash\n# Basic query\nmcp-query \"What is a module in Sui Move?\"\n\n# Advanced query with options\nmcp-query \"How do I define a struct in Sui Move?\" -k 3 -f\n```\n\n### Using RAG with LLM Integration\n\n```bash\n# Basic RAG query (will use simulated LLM if no API key is provided)\nmcp-rag \"What is a module in Sui Move?\"\n\n# Using with a specific LLM API\nmcp-rag \"How do I define a struct in Sui Move?\" --api-key your_api_key --top-k 3\n\n# Output as JSON for further processing\nmcp-rag \"What are the benefits of sui::coin?\" --output-json > rag_response.json\n```\n\n### Running the Server\n\n```bash\n# Start the server with default settings\nmcp-server\n\n# Start with custom settings\nmcp-server --host 127.0.0.1 --port 8080 --index-file custom/path/index.bin\n```\n\n## Manual Usage (without pipx)\n\n### Starting the server\n\n```bash\ncd mcp_server\npython main.py\n```\n\nThe server will start on http://localhost:8000\n\n### Downloading Move Files from GitHub\n\nTo download Move files from GitHub and populate your vector database:\n\n```bash\n# Download Move files with default query \"use sui\"\n./run.sh --download-move\n\n# Customize the search query\n./run.sh --download-move --github-query \"module sui::coin\" --max-results 50\n\n# Download, index, and start the server\n./run.sh --download-move --index\n```\n\nYou can also use the Python script directly:\n\n```bash\npython download_move_files.py --query \"use sui\" --output-dir docs/move_files\n```\n\n### Indexing documents\n\nBefore querying, you need to index your documents. You can place your text files (.txt), Markdown files (.md), or Move files (.move) in the `docs` directory.\n\nTo index the documents, you can either:\n\n1. Use the run script with the `--index` flag:\n\n```bash\n./run.sh --index\n```\n\n2. Use the index script directly:\n\n```bash\npython index_move_files.py --docs-dir docs/move_files --index-file data/faiss_index.bin\n```\n\n### Querying documents\n\nYou can use the local query script:\n\n```bash\npython local_query.py \"What is RAG?\"\n\n# With more options\npython local_query.py -k 3 -f \"How to define a struct in Sui Move?\"\n```\n\n### Using RAG with LLM Integration\n\n```bash\n# Direct RAG query with an LLM\npython rag_integration.py \"What is a module in Sui Move?\" --index-file data/faiss_index.bin\n\n# With API key (if you have one)\nOPENAI_API_KEY=your_key_here python rag_integration.py \"How do coins work in Sui?\"\n```\n\n### MCP API Endpoint\n\nThe MCP API endpoint is available at `/mcp/action`. You can use it to perform different actions:\n\n- `retrieve_documents`: Retrieve relevant documents for a query\n- `index_documents`: Index documents from a directory\n\nExample:\n\n```bash\ncurl -X POST \"http://localhost:8000/mcp/action\" -H \"Content-Type: application/json\" -d '{\"action_type\": \"retrieve_documents\", \"payload\": {\"query\": \"What is RAG?\", \"top_k\": 3}}'\n```\n\n## Complete RAG Pipeline\n\nThe full RAG (Retrieval-Augmented Generation) pipeline works as follows:\n\n1. **Search Query**: The user submits a question\n2. **Retrieval**: The system searches the vector database for relevant documents\n3. **Context Formation**: Retrieved documents are formatted into a prompt\n4. **LLM Generation**: The prompt is sent to an LLM with the retrieved context\n5. **Enhanced Response**: The LLM provides an answer based on the retrieved information\n\nThis workflow is fully implemented in the `rag_integration.py` module, which can be used either through the command line or as a library in your own applications.\n\n## GitHub Move File Extraction\n\nThe system can extract Move files from GitHub based on search queries. It implements two methods:\n\n1. **GitHub API** (preferred): Requires a GitHub token for higher rate limits\n2. **Web Scraping fallback**: Used when API method fails or when no token is provided\n\nTo configure your GitHub token, set it in the `.env` file or as an environment variable:\n\n```\nGITHUB_TOKEN=your_github_token_here\n```\n\n## Project Structure\n\n```\nmcp_server/\n├── __init__.py             # Package initialization\n├── main.py                # Main server file\n├── mcp_api.py             # MCP API implementation\n├── index_move_files.py    # File indexing utility\n├── local_query.py         # Local query utility\n├── download_move_files.py # GitHub Move file extractor\n├── rag_integration.py     # LLM integration for RAG\n├── pyproject.toml         # Package configuration\n├── requirements.txt       # Dependencies\n├── .env.example           # Example environment variables\n├── README.md              # This file\n├── data/                  # Storage for the FAISS index\n├── docs/                  # Sample documents\n│   └── move_files/        # Downloaded Move files\n├── models/                # Model implementations\n│   └── vector_store.py    # FAISS vector store implementation\n└── utils/\n    ├── document_processor.py  # Document processing utilities\n    └── github_extractor.py    # GitHub file extraction utilities\n```\n\n## Extending the Project\n\nTo extend this proof-of-concept:\n\n1. Add authentication and security features\n2. Implement more sophisticated document processing\n3. Add support for more document types\n4. Integrate with other LLM providers\n5. Add monitoring and logging\n6. Improve the Move language parsing for more structured data extraction\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "documents",
        "ai",
        "document processing",
        "retrieve documents",
        "documents vector"
      ],
      "category": "document-processing"
    },
    "Props-Labs--fireflies-mcp": {
      "owner": "Props-Labs",
      "name": "fireflies-mcp",
      "url": "https://github.com/Props-Labs/fireflies-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Props-Labs.webp",
      "description": "Retrieve, search, and summarize meeting transcripts. Manage transcripts with advanced search capabilities and generate concise summaries in various formats.",
      "stars": 4,
      "forks": 11,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T03:08:02Z",
      "readme_content": "# Fireflies MCP Server\n\nMCP Server for the Fireflies.ai API, enabling transcript retrieval, search, and summary generation.\n\n### Features\n\n- **Transcript Management**: Retrieve and search meeting transcripts with filtering options\n- **Detailed Information**: Get comprehensive details about specific transcripts\n- **Advanced Search**: Find transcripts containing specific keywords or phrases\n- **Summary Generation**: Generate concise summaries of meeting transcripts in different formats\n\n\n## Tools\n\n1. `fireflies_get_transcripts`\n   - Retrieve a list of meeting transcripts with optional filtering\n   - Inputs:\n     - `limit` (optional number): Maximum number of transcripts to return\n     - `from_date` (optional string): Start date in ISO format (YYYY-MM-DD)\n     - `to_date` (optional string): End date in ISO format (YYYY-MM-DD)\n   - Returns: Array of transcript objects with basic information\n\n2. `fireflies_get_transcript_details`\n   - Get detailed information about a specific transcript\n   - Inputs:\n     - `transcript_id` (string): ID of the transcript to retrieve\n   - Returns: Comprehensive transcript details including speakers, content, and metadata\n\n3. `fireflies_search_transcripts`\n   - Search for transcripts containing specific keywords\n   - Inputs:\n     - `query` (string): Search query to find relevant transcripts\n     - `limit` (optional number): Maximum number of transcripts to return\n   - Returns: Array of matching transcript objects\n\n4. `fireflies_generate_summary`\n   - Generate a summary of a meeting transcript\n   - Inputs:\n     - `transcript_id` (string): ID of the transcript to summarize\n     - `format` (optional string): Format of the summary ('bullet_points' or 'paragraph')\n   - Returns: Generated summary text\n\n## Setup\n\n### Fireflies API Key\n[Create a Fireflies API Key](https://fireflies.ai/dashboard/settings/api) with appropriate permissions:\n   - Go to the Fireflies.ai dashboard\n   - Navigate to Settings > API\n   - Generate a new API key\n   - Copy the generated key\n\n### Usage with Claude Desktop\nTo use this with Claude Desktop, add the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"fireflies\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@props-labs/mcp/fireflies\"\n      ],\n      \"env\": {\n        \"FIREFLIES_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n\n```bash\nnpm install\n# or\npnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n# or\npnpm build\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nFIREFLIES_API_KEY=your_api_key npm start\n# or\nFIREFLIES_API_KEY=your_api_key pnpm start\n```\n\nYou can also use the setup script:\n\n```bash\n./setup.sh\nFIREFLIES_API_KEY=your_api_key npm start\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcripts",
        "search",
        "formats",
        "meeting transcripts",
        "transcripts manage",
        "transcripts advanced"
      ],
      "category": "document-processing"
    },
    "Qwinty--anytype-mcp": {
      "owner": "Qwinty",
      "name": "anytype-mcp",
      "url": "https://github.com/Qwinty/anytype-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Qwinty.webp",
      "description": "Access and manage Anytype data, including spaces, objects, and templates. Perform searches, create and delete spaces, and export objects in markdown format.",
      "stars": 21,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-18T20:07:55Z",
      "readme_content": "# OFFICIAL ANYTYPE MCP WAS RELEASED HERE\nhttps://github.com/anyproto/anytype-mcp\n\n# Anytype MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Qwinty/anytype-mcp)](https://smithery.ai/server/@Qwinty/anytype-mcp)\n\nAn MCP (Model Context Protocol) server that provides access to the Anytype API, allowing AI assistants and other MCP clients to interact with your Anytype data.\n\n**Based on the Anytype API definition v0.46+ (2025-03-17).**\nEnsure your Anytype Desktop version is compatible.\n\n## Features\n\n- Get list of spaces (`get_spaces`)\n- Search/Get objects within a space (`get_objects`, `search_space`) or globally (`global_search`)\n- Get detailed object content (`get_object_content`, supports retrieving full text)\n- Create and delete spaces (`create_space`) and objects (`create_object`, `delete_object`)\n- Export objects as markdown (`export_object`)\n- Manage list views and objects within lists (`get_list_views`, `get_list_view_objects`, `add_objects_to_list`, `remove_object_from_list`)\n- Get space members (`get_space_members`)\n- Get types and templates (`get_types`, `get_type_details`, `get_templates`, `get_template_details`)\n\n## Prerequisites\n\n- Node.js 18 or higher\n- Anytype desktop application running locally\n- An Anytype account\n\n## Installation\n\n### Manual Installation\n\n1. Clone this repository:\n\n   ```cli\n   git clone https://github.com/Qwinty/anytype-mcp.git\n   cd anytype-mcp\n   ```\n\n2. Install dependencies:\n\n   ```node\n   npm install\n   ```\n\n3. Build the project (compiles TypeScript to JavaScript in `build/`):\n\n   ```node\n   npm run build\n   ```\n\n4. **Obtain an App Key:** Before configuring the server, you need an App Key from Anytype. See the \"Getting an App Key\" section below.\n5. Add the MCP server to your MCP configuration file\n\n## Getting an App Key\n\nBefore using the MCP server, you need to obtain an app key from the Anytype desktop application:\n\n1. Make sure Anytype desktop is running\n2. Run the helper script:\n\n   ```node\n   npm run get-key\n   ```\n\n3. Follow the instructions to authorize the application\n4. Note the app key for configuration\n\n## Configuration\n\nAdd the Anytype MCP server to your MCP configuration file:\n\n- For Claude: Edit `claude_desktop_config.json`\n- For other MCP clients: Edit their respective configuration files\n\nExample configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"anytype\": {\n      \"command\": \"node\",\n      \"args\": [\"{path/to/anytype-mcp}/build/index.js\"],\n      \"env\": {\n        \"ANYTYPE_APP_KEY\": \"YOUR_APP_KEY_HERE\"\n      },\n      \"disabled\": false\n    }\n  }\n}\n```\n\nReplace `path/to/anytype-mcp` with the actual path to your installation and `YOUR_APP_KEY_HERE` with the app key you obtained.\n\n## Usage\n\n### Starting the server\n\nThe MCP server is usually started automatically by the MCP client. However, you can also start it manually for testing:\n\n```node\nnpm start\n```\n\n### Available Tools\n\nSee [Tools.md](docs/Tools.md) for a detailed list of available tools and their usage examples.\n\n### System Prompt\n\nAdditionally, a sample system prompt for AI assistants using this server is available in [docs/system-prompt.md](docs/system-prompt.md).\n\n## Token Efficiency and Data Filtering\n\nTo optimize for token usage with AI assistants, this MCP server implements response filtering by default for tools that return object data (`get_objects`, `global_search`, `search_space`).\n\n- **Default Behavior:** The server returns a simplified version of object data, including essential metadata like ID, name, type, icon, layout, space ID, root ID, snippet (if available), block count, tags, creation/modification dates, and creator info. Full block content and detailed relations are omitted.\n- **`include_text: true`:** Several tools (`get_objects`, `get_object_content`, `global_search`, `search_space`) support an optional `include_text` parameter. When set to `true`, the server will extract and include the full, formatted text content from the object's blocks in a `full_text` field. Use this when you need the complete text, but be aware it significantly increases response size and token count.\n- **`full_response: true`:** The `get_objects`, `global_search`, and `search_space` tools also support a `full_response` parameter. Setting this to `true` bypasses all filtering and returns the raw, complete JSON response directly from the Anytype API. This provides the most detail but uses the most tokens.\n\nChoose the appropriate parameters based on whether you need just metadata, full text content, or the complete raw API response.\n\n## Troubleshooting\n\n### Anytype API Not Responding\n\nMake sure the Anytype desktop application is running on your computer. The MCP server connects to the local Anytype API at `http://localhost:31009/v1`.\n\n### Authentication Issues\n\nIf you encounter authentication errors:\n\n1. Run `npm run get-key` to obtain a new app key\n2. Update your MCP configuration with the new key\n3. Restart your MCP client\n\n### Local API Port\n\nThe server connects to the Anytype API at `http://localhost:31009/v1` by default. If your Anytype installation uses a different port, you currently need to modify the `apiBaseUrl` variable in `src/index.ts` and rebuild (`npm run build`). Making this configurable via an environment variable is a potential future improvement.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "qwinty",
        "markdown",
        "templates",
        "qwinty anytype",
        "processing qwinty",
        "manage anytype"
      ],
      "category": "document-processing"
    },
    "R-lz--mcp-video-digest": {
      "owner": "R-lz",
      "name": "mcp-video-digest",
      "url": "https://github.com/R-lz/mcp-video-digest",
      "imageUrl": "/freedevtools/mcp/pfp/R-lz.webp",
      "description": "Extract audio from various video platforms like YouTube and TikTok, and convert the audio to text using multiple transcription services. Supports asynchronous processing and speaker separation for enhanced video content analysis.",
      "stars": 26,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T10:53:07Z",
      "readme_content": "# MCP Video Digest (视频内容提取总结)\n\n<div align=\"right\">\n  <a href=\"README_EN.md\">English</a> | <b>中文</b>\n</div>\n\n## 项目简介\nMCP Video Digest 是一个视频内容处理服务，,能够从 YouTube、Bilibili、TikTok、Twitter... 视频中提取音频并转换为文本。该服务支持多个转录服务提供商，包括 Deepgram、Gladia、Speechmatics 和 AssemblyAI，可以根据配置的 API 密钥灵活选择使用。(第一个MCP练手的项目,主要熟悉MCP的开发和运行流程)\n\n## 功能特点\n- 支持超过1000个网站上的流媒体内容下载和音频提取\n- 多个转录服务提供商支持：\n  - Deepgram\n  - Gladia\n  - Speechmatics\n  - AssemblyAI\n- 灵活的服务选择机制，根据可用的 API 密钥自动选择服务\n- 异步处理设计，提高并发性能\n- 完整的错误处理和日志记录\n- 支持说话人分离\n- × 支持本地模型cpu/gpu加速处理\n\n## 目录结构\n```\n.\n├── src/                    # 源代码目录\n│   ├── services/          # 服务实现目录\n│   │   ├── download/      # 下载服务\n│   │   └── transcription/ # 转录服务\n│   ├── main.py           # 主程序逻辑\n│   └── __init__.py       # 包初始化文件\n├── config/                # 配置文件目录\n├── test.py               # 测试脚本\n├── run.py                # 服务启动脚本\n├── pyproject.toml        # 项目配置和依赖管理\n├── uv.lock               # UV 依赖锁定文件\n└── .env                  # 环境变量配置\n```\n\n## 测试截图\n\n![YouTube](screenshot/img1.png)\n\n![Bilibili](screenshot/img2.png)\n\n## 安装说明\n\n### 1. 安装 uv 或使用 python\n如果还没有安装 uv，可以使用以下命令安装：\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n\n### 2. 克隆项目：\n```bash\ngit clone https://github.com/R-lz/mcp-video-digest.git\ncd mcp-video-digest\n```\n\n### 3. 创建并激活虚拟环境：\n```bash\nuv venv\nsource .venv/bin/activate  # Linux/Mac\n# 或\n.venv\\Scripts\\activate     # Windows\n```\n\n### 4. 安装依赖：\n```bash\nuv pip install -e .\n```\n\n> speechmatics 在使用requests调试的时候出现了各种问题(不是speechmatics的问题, 是我菜),所以使用了speechmatics sdk\n\n## 配置说明\n1. 在项目根目录创建 `.env` 文件或者重命名`.env.example`，配置所需的 API 密钥：\n   ```\n   mv .env.example .env\n\n   # 修改\n   DEEPGRAM_API_KEY=your_deepgram_key\n   GLADIA_API_KEY=your_gladia_key\n   SPEECHMATICS_API_KEY=your_speechmatics_key\n   ASSEMBLYAI_API_KEY=your_assemblyai_key\n   ```\n   注意：至少需要配置一个服务的 API 密钥\n\n2. 服务优先级顺序：\n   - Deepgram（推荐用于中文内容）\n   - Gladia\n   - Speechmatics\n   - AssemblyAI\n\n## 使用方法\n1. 启动服务：\n   ```bash\n   uv run src/main.py\n   ```\n   或者使用调试模式：\n   ```bash\n   UV_DEBUG=1 uv run src/main.py\n   ```\n\n2. 调用服务：\n   ```python\n   from mcp.client import MCPClient\n   \n   async def process_video():\n       client = MCPClient()\n       result = await client.call(\n           \"get_video_content\",\n           url=\"https://www.youtube.com/watch?v=video_id\"\n       )\n       print(result)\n   ```\n\n3. 客户端SSE为示例\n```bash\n{\n    \"mcpServers\": {\n      \"video_digest\": {\n        \"url\": \"http://<ip>:8000/sse\"\n    }\n  }\n}\n\n\n# 当然可以在Client传递Key\n\n\"env\": {\n   \"DEEPGRAM_API_KEY\":\"your_deepgram_key\"\n}\n\n```\n\n> STDIO方式修改启动命令即可:未验证和测试 [MCP文档](https://modelcontextprotocol.io/)\n\n\n\n## 测试\n运行测试脚本：\n```bash\nuv run test.py \n# 或\npython test.py\n```\n\n测试脚本会：\n- 验证环境变量配置\n- 测试 YouTube 下载功能\n- 测试各个转录服务\n- 测试完整的视频处理流程\n\n## 开发指南\n1. 添加新的转录服务：\n   - 在 `src/services/transcription/` 目录下创建新的服务类\n   - 继承 `BaseTranscriptionService` 类\n   - 实现 `transcribe` 方法\n\n2. 自定义下载服务：\n   - 在 `src/services/download/` 目录下修改或添加新的下载器\n   - 继承或修改 `YouTubeDownloader` 类\n\n## 依赖管理\n- 使用 `uv pip install package_name` 安装新依赖\n- 使用 `uv pip freeze > requirements.txt` 导出依赖列表\n- 使用 `pyproject.toml` 管理依赖，`uv.lock` 锁定依赖版本\n\n## 错误处理\n服务会处理以下情况：\n- API 密钥缺失或无效\n- 视频下载失败\n- 音频转录失败\n- 网络连接问题\n- 服务限制和配额\n\n## 注意事项\n1. 确保有足够的磁盘空间用于临时文件\n2. 注意各服务提供商的 API 使用限制\n3. 建议使用 Python 3.11 或更高版本\n4. 临时文件会自动清理\n5. 使用 uv 可以获得更快的依赖安装速度和更好的依赖管理\n6. YouTube下载可能需要身份验证,可以复制cookie到根目录下cookies.txt [使用插件快速生成](https://chromewebstore.google.com/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc) 或者使用cookies-from-browser等其他认证方式, [yt-dlp](https://github.com/yt-dlp/yt-dlp)\n\n\n## STT Key申请及免费额度\n- [Speechmatics](https://www.speechmatics.com/) 每月免费8小时 - [定价](https://www.speechmatics.com/pricing)\n- [Gladia](https://app.gladia.io/) 每月免费10小时 - [定价](https://app.gladia.io/billing)\n- [AssemblyAI](https://www.assemblyai.com/) 共50$免费额度 - [定价](https://www.assemblyai.com/pricing)\n- [Deepgram](https://deepgram.com/) 共200$的免费额度 - [定价](https://deepgram.com/pricing)\n\n\n> 内容仅供参考\n\n## 许可证\n采用 MIT 许可证。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcription",
        "audio",
        "extract",
        "extract audio",
        "video digest",
        "audio text"
      ],
      "category": "document-processing"
    },
    "Ransom-Alpha--Swarms_MCPserver": {
      "owner": "Ransom-Alpha",
      "name": "Swarms_MCPserver",
      "url": "https://github.com/Ransom-Alpha/Swarms_MCPserver",
      "imageUrl": "/freedevtools/mcp/pfp/Ransom-Alpha.webp",
      "description": "Retrieve and interact with documentation databases using hybrid semantic and keyword search. Automatically index and reindex various file types while supporting live file watching and low-latency document querying through a FastMCP tools API.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-01T02:44:56Z",
      "readme_content": "# 🐝 Swarms MCP Documentation Server\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/Windsurf_Ready-✅-orange\" alt=\"IDE Ready\">\n  <img src=\"https://img.shields.io/badge/Error_Tolerant-✅-green\" alt=\"Error Tolerant\">\n  <img src=\"https://img.shields.io/badge/Dynamic_MD_Loader-✅-blue\" alt=\"Dynamic MD Loader\">\n  <img src=\"https://img.shields.io/badge/Healthcheck_Tool-✅-success\" alt=\"Healthcheck Tool\">\n  <img src=\"https://img.shields.io/badge/Smart_Load_Logs-✅-purple\" alt=\"Smart Load Logs\">\n</p>\n\n![Version 2.2](https://img.shields.io/badge/Version-2.2-blueviolet)\n\n---\n\n## 📖 Description\n\nThis program is an **Agent Framework** Documentation MCP Server built on **FastMCP**, designed to enable **AI agents** to efficiently retrieve information from your documentation database. It combines hybrid semantic (vector) and keyword (BM25) search, chunked indexing, and a robust FastMCP tools API for seamless agent integration.\n\n**Key Capabilities:**\n- Efficient, chunk-level retrieval using both semantic and keyword search\n- Agents can query, list, and retrieve documentation using FastMCP tools\n- Local-first, low-latency design (all data indexed and queried locally)\n- Automatic reindexing on file changes\n- Modular: add any repos to `corpora/`, support for all major filetypes\n- Extensible: add new tools, retrievers, or corpora as needed\n\n**Main modules:**\n- `embed_documents.py` → Loads, chunks, and embeds documents\n- `swarms_server.py` → Brings up the MCP server and FastMCP tools\n\n---\n\n---\n\n## 🌟 Key Features\n\n- **Hybrid Retriever** 🔍: Combines semantic and keyword search.\n- **Dynamic Markdown Handling** 📄: Smart loader based on file size.\n- **Specialized Loaders** ⚙️: `.py`, `.ipynb`, `.md`, `.txt`, `.yaml`, `.yml`.\n- **Chunk and File Summaries** 📈: Displays chunk counts along with file counts.\n- **Live Watchdog** 🔥: Instantly responds to any changes in `corpora/`.\n- **User Confirmation for Costs** ✅: Confirms before expensive embeddings.\n- **Healthcheck Endpoint** 🚑: Ensure server is ready for use.\n- **Local-First** 🗂️: All repos indexed locally without external dependencies.\n- **Safe Deletion Helper** 🔥: Auto-delete broken/mismatched indexes.\n\n---\n\n## 🏗️ Version History\n\n| Version | Date       | Highlights                                                              |\n| ------- | ---------- | ---------------------------------------------------------------------- |\n| **2.2** | 2025‑04‑25 | Split embed/load from server; full chunk counting in loading summaries |\n| **1.0** | 2025‑04‑25 | Dynamic Markdown loader, color logs, Healthcheck tool                  |\n| **0.7** | 2025‑04‑25 | Specialized file loaders for `.py`, `.ipynb`, `.md`                    |\n| **0.5** | 2025‑04‑10 | OpenAI large model embeddings, extended MCP tools                      |\n| **0.1** | 2025‑04‑10 | Initial version with generic loaders                                   |\n\n---\n\n## 📚 Managing Your Corpora (Local Repos)\n\nBecause Swarms and other frameworks are **very large**, full corpora are **not** pushed to GitHub.\n\nInstead, you **clone** them manually under `corpora/`:\n\n```bash\n# Inside your project folder:\ncd corpora/\n\n# Clone useful frameworks:\ngit clone https://github.com/SwarmsAI/Swarms\ngit clone https://github.com/SwarmsAI/Swarms-Examples\ngit clone https://github.com/microsoft/autogen\ngit clone https://github.com/langchain-ai/langgraph\ngit clone https://github.com/openai/openai-agent-sdk\n```\n\n✅ **Notes:**\n- Add **any repo** — public, private, custom.\n- Build your own custom AI knowledge base locally.\n- **Large repos** (>500MB) are fine; all indexing is local.\n\n---\n\n## 🚀 Quick Start\n\n```powershell\n# 1. Activate virtual environment\nvenv\\Scripts\\Activate.ps1\n\n# 2. Install all dependencies\npip install -r requirements.txt\n\n# 3. Configure OpenAI API Key\necho OPENAI_API_KEY=sk-... > .env\n\n# 4. (Load and embed documents\npython embed_documents.py\n\n# 5. Start MCP server\npython swarms_server.py\n# If no index is found, the server will prompt you to embed documents automatically.\n```\n\n---\n\n## ⚙️ Configuration\n\n- **Corpus**: Drop repos inside `corpora/`\n- **Environment Variables**:\n  - `.env` must contain `OPENAI_API_KEY`\n- **Index File Support**:\n  - Both `chroma-collections.parquet` and `chroma.sqlite3` are supported. `.parquet` is preferred if both exist.\n- **Auto-Embedding**:\n  - If no index is found, the server will prompt you to embed and index your documents automatically.\n- **Optional**:\n  - Disable Chroma compaction if you prefer:\n    ```powershell\n    setx CHROMA_COMPACTION_SERVICE__COMPACTOR__DISABLED_COLLECTIONS \"swarms_docs\"\n    ```\n- **Command-Line Flags**:\n  - `--reindex` → trigger a refresh reindex during server run.\n\n---\n\n## 🔄 File Watching & Auto Reindexing\n\nThe MCP Server watches `corpora/` for any file changes:\n- Any modification, creation, or deletion triggers a **live** reindex.\n- No need to restart the server.\n\n---\n\n## 🛠️ Available FastMCP Tools\n\n| Tool                      | Description                                          |\n| ------------------------- | ---------------------------------------------------- |\n| `swarm_docs.search`       | Search relevant documentation chunks                |\n| `swarm_docs.list_files`   | List all indexed files                               |\n| `swarm_docs.get_chunk`    | Get a specific chunk by path and index               |\n| `swarm_docs.reindex`      | Force reindex (full or incremental)                  |\n| `swarm_docs.healthcheck`  | Check MCP Server status                              |\n\n---\n\n## ❓ Troubleshooting\n\n- **Q: I get 'No valid existing index found' when starting the server.**\n  - A: The server will now prompt you to embed and index documents. Accept the prompt to proceed, or run `python embed_documents.py` manually first.\n- **Q: Which index file is used?**\n  - A: The server will use `chroma-collections.parquet` if available, otherwise `chroma.sqlite3`.\n- **Q: I want to force a reindex.**\n  - A: Run `python swarms_server.py --reindex` or use the `swarm_docs.reindex` tool.\n\n---\n\n## 📋 Example Usage\n\n```python\n# Search the documentation\nresult = swarm_docs.search(\"How do I load a notebook?\")\nprint(result)\n\n# List all available files\nfiles = swarm_docs.list_files()\nprint(files)\n\n# Get a specific document chunk\nchunk = swarm_docs.get_chunk(path=\"examples/agent.py\", chunk_idx=2)\nprint(chunk[\"content\"])\n```\n\n---\n\n## 🧰 Extending & Rebuilding\n\n- **Add new docs** → drop into `corpora/`, then:\n  ```bash\n  python swarms_server.py --reindex\n  ```\n- **Schema changes** → (e.g. different metadata structure):\n  ```bash\n  python swarms_server.py --reindex --full\n  ```\n- **Add new repo** → Drop folder under `corpora/`, reindex.\n\n- **Recommended for mostly read-only repos**:\n  ```powershell\n  setx CHROMA_COMPACTION_SERVICE__COMPACTOR__DISABLED_COLLECTIONS \"swarms_docs\"\n  ```\n\n---\n\n## 🔗 IDE Integration\n\nPlug directly into Windsurf Cascade:\n\n```jsonc\n\"swarms\": {\n  \"command\": \"C:/…/Swarms/venv/Scripts/python.exe\",\n  \"args\": [\"swarms_server.py\"]\n}\n```\n\nThen you can access `swarm_docs.*` tools from Cascade automations.\n\n---\n\n## 📦 Requirements\n\n### 💡 Python 3.11 Environment Required\n\nCreate your environment explicitly:\n\n```bash\npython3.11 -m venv venv\n```\n\nThen install with:\n\n```bash\npip install -r requirements.txt\n```\n\n---\n\n## ✅ MCP Server Ready\n\nAfter boot:\n- Proper loading summaries\n- Safe confirmation before expensive actions\n- Auto file watching and reindexing\n- Windsurf plug-in ready\n- Full tool coverage\n\n**You're good to cascade it!** 🏄‍♂️\n\n---\n\n## 📈 Flow Diagram\n\n```\n                          +------------------+\n                          |    🖥️ MCP Server  |\n                          +------------------+\n                                  |\n     +---------------------------------------------------+\n     |                                                   |\n+-------------+                                     +-----------------+\n|  📁 Corpora |                                     | 🔎 FastMCP Tools |\n|  Folder     |                                     | (search, list,   |\n|  (markdown, |                                     | get_chunk, etc.) |\n|  code, etc) |                                     +-----------------+\n+-------------+                                               |\n      |                                                       |\n+-----------------+                                   +----------------+\n|  📚 Loaders      |                                   | 🧠 Ensemble    |\n| (Python, MD, TXT)|                                   | Retriever (BM25|\n|  Split into Chunks|                                  | + Chroma)      |\n+-----------------+                                   +----------------+\n      |                                                       |\n+-----------------+                                   +----------------+\n| ✂️ Text Splitter |                                   | 🧩 Similarity   |\n| (RecursiveCharacter) |                              | Search (chunks) |\n+-----------------+                                   +----------------+\n      |                                                       |\n+-----------------+                                   +----------------+\n| 💾 Embed chunks  |  —OpenAI Embedding (small)—>    | 🛢️ Chroma Vector |\n| via OpenAI API  |                                   | DB (Local Store) |\n+-----------------+                                   +----------------+\n      |                                                       |\n+-----------------+                                   +----------------+\n| 📡 Reindex Watcher|                                  | 👀 File Watchdog |\n| (Auto detect      |                                  | (Auto reindex   |\n| new/modified files|                                  | on file events) |\n+-----------------+                                   +----------------+\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ransom",
        "fastmcp",
        "document",
        "document querying",
        "document processing",
        "documentation databases"
      ],
      "category": "document-processing"
    },
    "Rz017--tavily-mcp": {
      "owner": "Rz017",
      "name": "tavily-mcp",
      "url": "https://github.com/Rz017/tavily-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Rz017.webp",
      "description": "Integrates real-time web search and intelligent data extraction to enhance AI assistants with up-to-date information and sophisticated filtering capabilities. Supports domain-specific data retrieval and processing features for improved AI workflows.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-26T20:17:27Z",
      "readme_content": "# Tavily MCP Server 🚀\n\n![GitHub Repo stars](https://img.shields.io/github/stars/tavily-ai/tavily-mcp?style=social)\n![npm](https://img.shields.io/npm/dt/tavily-mcp)\n![smithery badge](https://smithery.ai/badge/@tavily-ai/tavily-mcp)\n\n> 🔌 **Compatible with [Cline](https://github.com/cline/cline), [Cursor](https://cursor.sh), [Claude Desktop](https://claude.ai/desktop), and any other MCP Clients!**\n>\n> Tavily MCP is also compatible with any MCP client\n>\n> 📚  [tutorial](https://medium.com/@dustin_36183/building-a-knowledge-graph-assistant-combining-tavily-and-neo4j-mcp-servers-with-claude-db92de075df9) on combining Tavily MCP with Neo4j MCP server!\n> \n> 📚  [tutorial](https://medium.com/@dustin_36183/connect-your-coding-assistant-to-the-web-integrating-tavily-mcp-with-cline-in-vs-code-5f923a4983d1) Integrating Tavily MCP with Cline in VS Code ( Demo + Example Use-Cases)\n>\n\n![Tavily MCP Demo](./assets/mcp-demo.gif)\n\nThe Model Context Protocol (MCP) is an open standard that enables AI systems to interact seamlessly with various data sources and tools, facilitating secure, two-way connections.\n\nDeveloped by Anthropic, the Model Context Protocol (MCP) enables AI assistants like Claude to seamlessly integrate with Tavily's advanced search and data extraction capabilities. This integration provides AI models with real-time access to web information, complete with sophisticated filtering options and domain-specific search features.\n\nThe Tavily MCP server provides:\n- Seamless interaction with the tavily-search and tavily-extract tools\n- Real-time web search capabilities through the tavily-search tool\n- Intelligent data extraction from web pages via the tavily-extract tool\n\n\n## Prerequisites 🔧\n\nBefore you begin, ensure you have:\n\n- [Tavily API key](https://app.tavily.com/home)\n  - If you don't have a Tavily API key, you can sign up for a free account [here](https://app.tavily.com/home)\n- [Claude Desktop](https://claude.ai/download) or [Cursor](https://cursor.sh)\n- [Node.js](https://nodejs.org/) (v20 or higher)\n  - You can verify your Node.js installation by running:\n    - `node --version`\n- [Git](https://git-scm.com/downloads) installed (only needed if using Git installation method)\n  - On macOS: `brew install git`\n  - On Linux: \n    - Debian/Ubuntu: `sudo apt install git`\n    - RedHat/CentOS: `sudo yum install git`\n  - On Windows: Download [Git for Windows](https://git-scm.com/download/win)\n\n## Tavily MCP server installation ⚡\n\n### Running with NPX \n\n```bash\nnpx -y tavily-mcp@0.1.4  \n```\n\n### Installing via Smithery\n\nTo install Tavily MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@tavily-ai/tavily-mcp):\n\n```bash\nnpx -y @smithery/cli install @tavily-ai/tavily-mcp --client claude\n```\n\nAlthough you can launch a server on its own, it's not particularly helpful in isolation. Instead, you should integrate it into an MCP client. Below is an example of how to configure the Claude Desktop app to work with the tavily-mcp server.\n\n\n## Configuring MCP Clients ⚙️\n\nThis repository will explain how to configure both [Cursor](https://cursor.sh) and [Claude Desktop](https://claude.ai/desktop) to work with the tavily-mcp server.\n\n\n### Configuring Cline 🤖\n\nThe easiest way to set up the Tavily MCP server in Cline is through the marketplace with a single click:\n\n1. Open Cline in VS Code\n2. Click on the Cline icon in the sidebar\n3. Navigate to the \"MCP Servers\" tab ( 4 squares )\n4. Search \"Tavily\" and click \"install\"\n5. When prompted, enter your Tavily API key\n\nAlternatively, you can manually set up the Tavily MCP server in Cline:\n\n1. Open the Cline MCP settings file:\n\n   ### For macOS:\n   ```bash\n   # Using Visual Studio Code\n   code ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   \n   # Or using TextEdit\n   open -e ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   ```\n\n   ### For Windows:\n   ```bash\n   code %APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json\n   ```\n\n2. Add the Tavily server configuration to the file:\n\n   Replace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys).\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"tavily-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"tavily-mcp@0.1.4\"],\n         \"env\": {\n           \"TAVILY_API_KEY\": \"your-api-key-here\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n3. Save the file and restart Cline if it's already running.\n\n4. When using Cline, you'll now have access to the Tavily MCP tools. You can ask Cline to use the tavily-search and tavily-extract tools directly in your conversations.\n\n\n### Configuring Cursor 🖥️\n\n> **Note**: Requires Cursor version 0.45.6 or higher\n\nTo set up the Tavily MCP server in Cursor:\n\n1. Open Cursor Settings\n2. Navigate to Features > MCP Servers\n3. Click on the \"+ Add New MCP Server\" button\n4. Fill out the following information:\n   - **Name**: Enter a nickname for the server (e.g., \"tavily-mcp\")\n   - **Type**: Select \"command\" as the type\n   - **Command**: Enter the command to run the server:\n     ```bash\n     env TAVILY_API_KEY=your-api-key npx -y tavily-mcp@0.1.4\n     ```\n     > **Important**: Replace `your-api-key` with your Tavily API key. You can get one at [app.tavily.com/home](https://app.tavily.com/home)\n\nAfter adding the server, it should appear in the list of MCP servers. You may need to manually press the refresh button in the top right corner of the MCP server to populate the tool list.\n\nThe Composer Agent will automatically use the Tavily MCP tools when relevant to your queries. It is better to explicitly request to use the tools by describing what you want to do (e.g., \"User tavily-search to search the web for the latest news on AI\"). On mac press command + L to open the chat, select the composer option at the top of the screen, beside the submit button select agent and submit the query when ready.\n\n![Cursor Interface Example](./assets/cursor-reference.png)\n\n### Configuring the Claude Desktop app 🖥️\n### For macOS:\n\n```bash\n# Create the config file if it doesn't exist\ntouch \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n\n# Opens the config file in TextEdit \nopen -e \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n\n# Alternative method using Visual Studio Code (requires VS Code to be installed)\ncode \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n```\n\n### For Windows:\n```bash\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### Add the Tavily server configuration:\n\nReplace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys).\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tavily-mcp@0.1.2\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### 2. Git Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/tavily-ai/tavily-mcp.git\ncd tavily-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n### Configuring the Claude Desktop app ⚙️\nFollow the configuration steps outlined in the [Configuring the Claude Desktop app](#configuring-the-claude-desktop-app-️) section above, using the below JSON configuration.\n\nReplace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys) and `/path/to/tavily-mcp` with the actual path where you cloned the repository on your system.\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/tavily-mcp/build/index.js\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage in Claude Desktop App 🎯\n\nOnce the installation is complete, and the Claude desktop app is configured, you must completely close and re-open the Claude desktop app to see the tavily-mcp server. You should see a hammer icon in the bottom left of the app, indicating available MCP tools, you can click on the hammer icon to see more detial on the tavily-search and tavily-extract tools.\n\n![Alt text](./assets/claude-desktop-ref.png)\n\nNow claude will have complete access to the tavily-mcp server, including the tavily-search and tavily-extract tools. If you insert the below examples into the Claude desktop app, you should see the tavily-mcp server tools in action.\n\n### Tavily Search Examples\n\n1. **General Web Search**:\n```\nCan you search for recent developments in quantum computing?\n```\n\n2. **News Search**:\n```\nSearch for news articles about AI startups from the last 7 days.\n```\n\n3. **Domain-Specific Search**:\n```\nSearch for climate change research on nature.com and sciencedirect.com\n```\n\n### Tavily Extract Examples \n\n1. **Extract Article Content**:\n```\nExtract the main content from this article: https://example.com/article\n```\n\n### ✨ Combine Search and Extract ✨\n\nYou can also combine the tavily-search and tavily-extract tools to perform more complex tasks.\n\n```\nSearch for news articles about AI startups from the last 7 days and extract the main content from each article to generate a detailed report.\n```\n\n## Troubleshooting 🛠️\n\n### Common Issues\n\n1. **Server Not Found**\n   - Verify the npm installation by running `npm --verison`\n   - Check Claude Desktop configuration syntax by running `code ~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n   - Ensure Node.js is properly installed by running `node --version`\n   \n2. **NPX related issues**\n  - If you encounter errors related to `npx`, you may need to use the full path to the npx executable instead. \n  - You can find this path by running `which npx` in your terminal, then replace the `\"command\":  \"npx\"` line with `\"command\": \"/full/path/to/npx\"` in your configuration.\n\n3. **API Key Issues**\n   - Confirm your Tavily API key is valid\n   - Check the API key is correctly set in the config\n   - Verify no spaces or quotes around the API key\n\n## Acknowledgments ✨\n\n- [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n- [Anthropic](https://anthropic.com) for Claude Desktop\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "retrieval",
        "search",
        "ai assistants",
        "ai workflows",
        "search intelligent"
      ],
      "category": "document-processing"
    },
    "SecretiveShell--MCP-llms-txt": {
      "owner": "SecretiveShell",
      "name": "MCP-llms-txt",
      "url": "https://github.com/SecretiveShell/MCP-llms-txt",
      "imageUrl": "/freedevtools/mcp/pfp/SecretiveShell.webp",
      "description": "Integrate documentation directly into conversations by utilizing MCP resources for chat applications. This server enhances interactions by providing relevant documentation content as part of the dialogue.",
      "stars": 24,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:14Z",
      "readme_content": "# mcp-llms-txt\r\n\r\n[![smithery badge](https://smithery.ai/badge/@SecretiveShell/MCP-llms-txt)](https://smithery.ai/server/@SecretiveShell/MCP-llms-txt)\r\n\r\nMCP server for [Awesome-llms-txt](https://github.com/SecretiveShell/Awesome-llms-txt). Add documentation directly into your conversation via mcp resources.\r\n\r\n<a href=\"https://glama.ai/mcp/servers/kqwhhpe8l7\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kqwhhpe8l7/badge\" alt=\"MCP-llms-txt MCP server\" /></a>\r\n\r\n## Installation\r\n\r\nView a setup guide + example usage on [pulsemcp.com](https://www.pulsemcp.com/use-cases/utilize-llm-txt-files/secretiveshell-claude-llmstxt)\r\n\r\n### Installing via Smithery\r\n\r\nTo install MCP Server for Awesome-llms-txt for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@SecretiveShell/MCP-llms-txt):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @SecretiveShell/MCP-llms-txt --client claude\r\n```\r\n\r\n### Manual Installation\r\nSetup your claude config like this:\r\n\r\n```json\r\n{\r\n    \"mcpServers\": {\r\n        \"mcp-llms-txt\": {\r\n            \"command\": \"uvx\",\r\n            \"args\": [\"mcp-llms-txt\"],\r\n            \"env\": {\r\n                \"PYTHONUTF8\": \"1\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## testing\r\n\r\nUse [mcp-cli](https://github.com/wong2/mcp-cli) to test the server:\r\n\r\n```bash\r\nnpx -y \"@wong2/mcp-cli\" -c config.json\r\n```\r\n\r\nThe config file is already setup and does not need to be changed since there are no api keys or secrets.\r\n\r\n## Contributing\r\n\r\nContributions are welcome! Please open an issue or submit a pull request.\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "conversations",
        "txt",
        "documentation",
        "llms txt",
        "conversations utilizing",
        "directly conversations"
      ],
      "category": "document-processing"
    },
    "Sunwood-ai-labs--documind-mcp-server": {
      "owner": "Sunwood-ai-labs",
      "name": "documind-mcp-server",
      "url": "https://github.com/Sunwood-ai-labs/documind-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Sunwood-ai-labs.webp",
      "description": "Analyzes and enhances the quality of documentation, specifically README files, by providing insights and suggestions for improvement. Utilizes advanced neural processing techniques for thorough evaluation and visual analysis of documentation elements.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-02-12T04:02:18Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"assets/header.svg\" alt=\"DocuMind MCP Server\" width=\"800\">\n\n  <div align=\"center\">\n    <a href=\"README.md\"><img src=\"https://img.shields.io/badge/english-document-blue.svg\" alt=\"EN doc\"></a>\n    <a href=\"README.ja.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"/></a>\n  </div>\n</div>\n\n# 🌐 DocuMind MCP Server\n\n> _\"Where Documentation Meets Digital Intelligence\"_\n\nA next-generation Model Context Protocol (MCP) server that revolutionizes documentation quality analysis through advanced neural processing.\n\n## ⚡ Core Systems\n\n- 🧠 **Neural Documentation Analysis**: Advanced algorithms for comprehensive README evaluation\n- 🔮 **Holographic Header Scanning**: Cutting-edge SVG analysis for visual elements\n- 🌍 **Multi-dimensional Language Support**: Cross-linguistic documentation verification\n- 💫 **Quantum Suggestion Engine**: AI-powered improvement recommendations\n\n## 🚀 System Boot Sequence\n\n### System Requirements\n\n- Node.js 18+\n- npm || yarn\n\n### Initialize Core\n\n```bash\nnpm install\n```\n\n### Compile Matrix\n\n```bash\nnpm run build\n```\n\n### Neural Development Link\n\nEstablish real-time neural connection:\n```bash\nnpm run watch\n```\n\n## 🛸 Operation Protocol\n\n### System Configuration\n\nIntegrate with Claude Desktop mainframe:\n\n**Windows Terminal**:\n```json\n// %APPDATA%/Claude/claude_desktop_config.json\n{\n  \"mcpServers\": {\n    \"documind-mcp-server\": {\n      \"command\": \"/path/to/documind-mcp-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Neural Interface Commands\n\n#### evaluate_readme\nInitiates quantum analysis of documentation structure.\n\nParameters:\n- `projectPath`: Neural pathway to target directory\n\nExample Request:\n```javascript\n{\n  name: \"evaluate_readme\",\n  arguments: {\n    projectPath: \"/path/to/project\"\n  }\n}\n```\n\nExample Response:\n```javascript\n{\n  content: [\n    {\n      type: \"text\",\n      text: JSON.stringify({\n        filePath: \"/path/to/project/README.md\",\n        hasHeaderImage: true,\n        headerImageQuality: {\n          hasGradient: true,\n          hasAnimation: true,\n          // ... other quality metrics\n        },\n        score: 95,\n        suggestions: [\n          \"Consider adding language badges\",\n          // ... other suggestions\n        ]\n      })\n    }\n  ]\n}\n```\n\n## 🔮 Development Matrix\n\n### Debug Protocol\n\nAccess the neural network through MCP Inspector:\n\n```bash\nnpm run inspector\n```\n\n### Troubleshooting Guide\n\n#### Common Issues and Solutions\n\n1. **Header Image Not Detected**\n   - Ensure SVG file is placed in the `assets/` directory\n   - Validate SVG file contains proper XML structure\n   - Check file permissions\n\n2. **Language Badges Not Recognized**\n   - Verify badges use shields.io format\n   - Check HTML structure follows recommended pattern\n   - Ensure proper center alignment\n\n3. **Build Errors**\n   - Clear `node_modules` and reinstall dependencies\n   - Ensure TypeScript version matches project requirements\n   - Check for syntax errors in modified files\n\n4. **MCP Connection Issues**\n   - Verify stdio transport configuration\n   - Check Claude Desktop configuration\n   - Ensure proper file paths in config\n\n#### Performance Optimization\n\n1. **SVG Analysis**\n   - Minimize SVG complexity for faster parsing\n   - Use efficient gradients and animations\n   - Optimize file size while maintaining quality\n\n2. **README Scanning**\n   - Structure content for optimal parsing\n   - Use recommended markdown patterns\n   - Follow badge placement guidelines\n\n## 🔬 API Documentation\n\n### Core Classes\n\n#### ReadmeService\n\nPrimary service for README analysis and evaluation.\n\n```typescript\nclass ReadmeService {\n  // Analyzes all README files in a project\n  async evaluateAllReadmes(projectPath: string): Promise<ReadmeEvaluation[]>\n  \n  // Evaluates a single README file\n  private async evaluateReadme(dirPath: string, readmePath: string): Promise<ReadmeEvaluation>\n  \n  // Evaluates language badge configuration\n  private evaluateLanguageBadges(content: string): BadgeEvaluation\n}\n```\n\n#### SVGService\n\nSpecialized service for SVG header image analysis.\n\n```typescript\nclass SVGService {\n  // Evaluates SVG header image quality\n  public evaluateHeaderImageQuality(imgSrc: string, content: string): HeaderImageQuality\n  \n  // Checks for project-specific elements in SVG\n  private checkProjectSpecificImage(svgContent: string, readmeContent: string): boolean\n}\n```\n\n### Core Interfaces\n\n```typescript\ninterface ReadmeEvaluation {\n  filePath: string;\n  hasHeaderImage: boolean;\n  headerImageQuality: HeaderImageQuality;\n  isCentered: {\n    headerImage: boolean;\n    title: boolean;\n    badges: boolean;\n  };\n  hasBadges: {\n    english: boolean;\n    japanese: boolean;\n    isCentered: boolean;\n    hasCorrectFormat: boolean;\n  };\n  score: number;\n  suggestions: string[];\n}\n\ninterface HeaderImageQuality {\n  hasGradient: boolean;\n  hasAnimation: boolean;\n  hasRoundedCorners: boolean;\n  hasEnglishText: boolean;\n  isProjectSpecific: boolean;\n}\n```\n\n### Error Handling\n\nThe server implements comprehensive error handling:\n\n```typescript\ntry {\n  const evaluations = await readmeService.evaluateAllReadmes(projectPath);\n  // Process results\n} catch (error) {\n  const errorMessage = error instanceof Error ? error.message : String(error);\n  return {\n    content: [{\n      type: 'text',\n      text: `Evaluation error: ${errorMessage}`\n    }],\n    isError: true\n  };\n}\n```\n\n## ⚡ License\n\nOperating under MIT Protocol.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "documind",
        "document processing",
        "quality documentation",
        "documentation elements"
      ],
      "category": "document-processing"
    },
    "Taewoong1378--notion-readonly-mcp-server": {
      "owner": "Taewoong1378",
      "name": "notion-readonly-mcp-server",
      "url": "https://github.com/Taewoong1378/notion-readonly-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Taewoong1378.webp",
      "description": "Provides read-only access to Notion content, enabling retrieval of pages, blocks, databases, comments, and properties with optimized performance. Focuses on minimizing API calls and supports parallel processing for efficient data acquisition.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-07T12:14:21Z",
      "readme_content": "# Notion ReadOnly MCP Server\n\nThis project implements an optimized read-only MCP server for the Notion API, focusing on performance and efficiency for AI assistants to query and retrieve Notion content.\n\n<a href=\"https://glama.ai/mcp/servers/@Taewoong1378/notion-readonly-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Taewoong1378/notion-readonly-mcp-server/badge\" alt=\"Notion ReadOnly Server MCP server\" />\n</a>\n\n## Key Improvements\n\n- **Read-Only Design**: Focused exclusively on data retrieval operations, ensuring safe access to Notion content.\n- **Minimized Tool Set**: Reduced the number of exposed Notion API tools from 15+ to only 6 essential ones for document analysis.\n- **Parallel Processing**: Enhanced performance by implementing asynchronous and parallel API requests for retrieving block content, significantly reducing response times.\n- **Extended Database Access**: Added support for database, page property, and comment retrieval operations.\n- **Optimized for AI Assistants**: Significantly reduced tool count addresses the \"Too many tools can degrade performance\" issue in AI assistants like Cursor, which limits models to approximately 40 tools.\n\n## Tool Comparison\n\nThis read-only implementation exposes far fewer tools compared to the standard Notion API integration, improving performance and compatibility with AI assistants:\n\n![Notion API Tools Comparison](docs/images/notion-api-tools-comparison.png)\n\nThe reduced tool set helps stay within the recommended tool limits for optimal AI assistant performance while still providing all essential functionality.\n\n## Installation\n\n### 1. Setting up Integration in Notion:\n\nGo to https://www.notion.so/profile/integrations and create a new **internal** integration or select an existing one.\n\n![Creating a Notion Integration token](docs/images/integrations-creation.png)\n\nWhile we limit the scope of Notion API's exposed to read-only operations, there is a non-zero risk to workspace data by exposing it to LLMs. Security-conscious users may want to further configure the Integration's _Capabilities_.\n\nFor example, you can create a read-only integration token by giving only \"Read content\" access from the \"Configuration\" tab:\n\n![Notion Integration Token Capabilities showing Read content checked](docs/images/integrations-capabilities.png)\n\n### 2. Adding MCP config to your client:\n\n#### Using npm:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json` (MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`)\n\n```json\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"notion-readonly-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n  }\n}\n```\n\n#### Using Docker:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENAPI_MCP_HEADERS\",\n        \"taewoong1378/notion-readonly-mcp-server\"\n      ],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\":\\\"Bearer ntn_****\\\",\\\"Notion-Version\\\":\\\"2022-06-28\\\"}\"\n      }\n    }\n  }\n}\n```\n\nDon't forget to replace `ntn_****` with your integration secret. Find it from your integration configuration tab.\n\n### 3. Connecting content to integration:\n\nEnsure relevant pages and databases are connected to your integration.\n\nTo do this, visit the page, click on the 3 dots, and select \"Connect to integration\".\n\n![Adding Integration Token to Notion Connections](docs/images/connections.png)\n\n## Available Tools\n\nThis optimized server exposes only essential read-only Notion API tools:\n\n- `API-retrieve-a-page`: Get page information\n- `API-get-block-children`: Get page content blocks (with parallel processing)\n- `API-retrieve-a-block`: Get details about a specific block\n- `API-retrieve-a-database`: Get database information\n- `API-retrieve-a-comment`: Get comments on a page or block\n- `API-retrieve-a-page-property`: Get specific property information from a page\n- `API-get-one-pager`: **NEW!** Recursively retrieve a full Notion page with all its blocks, databases, and related content in a single call\n\nBy limiting to these 7 essential tools (compared to 15+ in the standard implementation), we ensure:\n\n1. Better performance in AI assistants like Cursor and Claude that have tool count limitations\n2. Reduced cognitive load for AI models when choosing appropriate tools\n3. Faster response times with fewer API options to consider\n4. Enhanced security through minimized API surface area\n\n## Automatic Content Exploration\n\nThe new `API-get-one-pager` tool provides a powerful way to explore Notion pages without requiring multiple API calls:\n\n- **Recursive retrieval**: Automatically traverses the entire page structure including nested blocks\n- **Parallel processing**: Fetches multiple blocks and their children simultaneously for maximum performance\n- **Intelligent caching**: Stores retrieved data to minimize redundant API calls\n- **Comprehensive content**: Includes pages, blocks, databases, comments, and detailed property information\n- **Customizable depth**: Control the level of recursion to balance between detail and performance\n\n### Using One Pager Tool\n\n```\n{\n  \"page_id\": \"YOUR_PAGE_ID\",\n  \"maxDepth\": 5,               // Optional: Maximum recursion depth (default: 5)\n  \"includeDatabases\": true,    // Optional: Include linked databases (default: true)\n  \"includeComments\": true,     // Optional: Include comments (default: true)\n  \"includeProperties\": true    // Optional: Include detailed page properties (default: true)\n}\n```\n\nThis automatic exploration capability is especially useful for AI assistants that need to understand the entire content of a Notion page without making dozens of separate API calls, resulting in much faster and more efficient responses.\n\n## Asynchronous Processing\n\nThe server implements advanced parallel processing techniques for handling large Notion documents:\n\n- Multiple requests are batched and processed concurrently\n- Pagination is handled automatically for block children\n- Results are efficiently aggregated before being returned\n- Console logging provides visibility into the process without affecting response format\n\n## Examples\n\n1. Using the following instruction:\n\n```\nGet the content of page 1a6b35e6e67f802fa7e1d27686f017f2\n```\n\nThe AI will retrieve the page details efficiently with parallel processing of block content.\n\n2. Using database information:\n\n```\nGet the structure of database 8a6b35e6e67f802fa7e1d27686f017f2\n```\n\n## Development\n\nBuild:\n\n```\npnpm build\n```\n\nExecute:\n\n```\npnpm dev\n```\n\n## License\n\nMIT\n\n## AI Assistant Performance Benefits\n\nModern AI assistants like Cursor and Claude have limitations on the number of tools they can effectively handle:\n\n- Most models may not respect more than 40 tools in total\n- Too many tools can degrade overall performance and reasoning capabilities\n- Complex tool sets increase response latency and decision-making difficulty\n\nThis read-only implementation deliberately reduces the Notion API surface to address these limitations while preserving all essential functionality. The result is:\n\n- Faster and more reliable responses from AI assistants\n- Improved accuracy when interacting with Notion content\n- Better overall performance through focused API design",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "processing",
        "mcp",
        "document processing",
        "readonly mcp",
        "notion readonly"
      ],
      "category": "document-processing"
    },
    "Teeksss--mcp": {
      "owner": "Teeksss",
      "name": "mcp",
      "url": "https://github.com/Teeksss/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Teeksss.webp",
      "description": "Integrates multiple AI models and implements retrieval-augmented generation (RAG) alongside large language models (LLMs). Supports PDF and OCR processing for enhanced data handling while providing a simplified setup for backend and frontend deployment.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-28T10:45:44Z",
      "readme_content": "# MCP Server (Multi-Model + RAG + LLM Platform)\n\n## Kurulum Rehberi\n\n### Gereksinimler\n\n- **Python 3.9+**\n- **Node.js 16+** (frontend için)\n- **Tesseract** (OCR desteği için)\n- (Linux/Mac: `sudo apt install tesseract-ocr` veya `brew install tesseract`)\n- **pip** veya **poetry** (isteğe bağlı)\n\n---\n\n## 1. Backend Kurulumu\n\n### a) Sanal Ortam Oluştur\n\n```bash\npython -m venv venv\nsource venv/bin/activate\n```\n\n### b) Bağımlılıkları Yükle\n\n```bash\npip install -r requirements.txt\n# veya\npoetry install\n```\n\n### c) Ortam Değişkenleri\n\n`.env` dosyasını oluştur:\n\n```bash\ncp .env.example .env\n```\nGerekirse `OPENAI_API_KEY` ve diğer alanları doldur.\n\n### d) Veritabanını Başlat\n\n```bash\npython -c \"from src.models.database import init_db; init_db()\"\n```\n\n### e) Sunucuyu Çalıştır\n\n```bash\nuvicorn src.main:app --reload\n```\n- Uygulama arayüzü: [http://localhost:8000/docs](http://localhost:8000/docs)\n\n---\n\n## 2. Frontend (Web) Kurulumu\n\n```bash\ncd web\nnpm install\nnpm start\n```\n- Arayüz: [http://localhost:3000](http://localhost:3000)\n\n---\n\n## 3. Notlar\n\n- PDF/OCR için Tesseract kurulmalı.\n- LLM entegrasyonu için `OPENAI_API_KEY` veya HuggingFace modeli indirecek internet bağlantısı gereklidir.\n- Vektör veritabanı ve LLM eklemek için ilgili Python dosyalarından kolayca genişletebilirsiniz.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "document",
        "retrieval",
        "document processing",
        "ocr processing",
        "processing teeksss"
      ],
      "category": "document-processing"
    },
    "VadimNastoyashchy--json-mcp": {
      "owner": "VadimNastoyashchy",
      "name": "json-mcp",
      "url": "https://github.com/VadimNastoyashchy/json-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/VadimNastoyashchy.webp",
      "description": "Efficiently interacts with JSON files by splitting, merging, and validating data based on specified conditions. Designed for seamless integration with language models to automate JSON data manipulation within development environments.",
      "stars": 11,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T07:49:29Z",
      "readme_content": "# JSON MCP\n\n[![Smithery Badge](https://smithery.ai/badge/@VadimNastoyashchy/json-mcp)](https://smithery.ai/server/@VadimNastoyashchy/json-mcp)\n\nThe **Model Context Protocol (MCP)** server empowers **LLMs** to efficiently interact with JSON files. With JSON MCP, you can **split**, **merge**, and **find specific data**, **validate** within JSON files based on defined conditions.\n\n---\n\n<a href=\"https://glama.ai/mcp/servers/@VadimNastoyashchy/json-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@VadimNastoyashchy/json-mcp/badge\" />\n</a>\n\n---\n\n## 🌟 Key Features\n\n✅ **Fast and lightweight**  \n✅ **LLM-friendly functionality**\n\n---\n\n## 🎥 Demo\n\nBelow is a demo showcasing the `split` functionality:\n\n![Demo: Split JSON](./assets/split-demo.gif)\n\n---\n\n## 🔧 Use Cases (Tools)\n\n### 1. **`split`**\n\nSplit a JSON file into a specified number of objects.\n\n> **Note:** The file path must be provided.\n\n**Prompt Example:**\n\n```plaintext\nSplit JSON file from /Users/json-mcp/tests/merged.json\n5 objects per file\n```\n\n### 2. **`merge`**\n\nMerge JSON files into a one JSON file\n\n> **Note:** The folder path should be provided\n\n**Prompt Example:**\n\n```plaintext\nMerge json files from /Users/json-mcp/tests\n```\n\n---\n\n### ⚙️ Configuration\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://insiders.vscode.dev/redirect?url=vscode:mcp/install?%7B%22name%22%3A%22%40VadimNastoyashchy%2Fjson-mcp%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40smithery%2Fcli%40latest%22%2C%22run%22%2C%22%40VadimNastoyashchy%2Fjson-mcp%22%2C%22--key%22%2C%2292357446-baf5-439c-b7c1-b5263e221b57%22%5D%7D)\n\n#### VS Code Manual Configuration\n\nTo configure the JSON MCP server manually in VS Code, update the **User Settings (JSON)** file:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"json-mcp-server\": {\n        \"command\": \"npx\",\n        \"args\": [\"json-mcp-server@latest\"]\n      }\n    }\n  }\n}\n```\n\n#### Installation in VS Code\n\nYou can install the JSON MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"json-mcp-server\",\"command\":\"npx\",\"args\": [\"json-mcp-server@latest\"]}'\n```\n\nAfter installation, the JSON MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\n#### Claude Desktop\n\nTo install json-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@VadimNastoyashchy/json-mcp):\n\n```bash\nnpx -y @smithery/cli install @VadimNastoyashchy/json-mcp --client claude\n```\n\n---\n\n### ⚙️ Installation Server\n\n#### Install globally\n\n```bash\nnpm install -g json-mcp-server@latest\n```\n\n#### Run after global installation\n\n```bash\njson-mcp-server\n```\n\n#### Using npx with latest version (recommended)\n\n```bash\nnpx json-mcp-server@latest\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "json",
        "document",
        "automate",
        "json files",
        "vadimnastoyashchy json",
        "automate json"
      ],
      "category": "document-processing"
    },
    "Vortiago--mcp-outline": {
      "owner": "Vortiago",
      "name": "mcp-outline",
      "url": "https://github.com/Vortiago/mcp-outline",
      "imageUrl": "/freedevtools/mcp/pfp/Vortiago.webp",
      "description": "Enables interaction with Outline's document management services through natural language commands for searching, creating, and managing documents. Facilitates tasks like reading document content and managing comments within a structured collection.",
      "stars": 40,
      "forks": 12,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T05:19:13Z",
      "readme_content": "# MCP Outline Server\n\nA Model Context Protocol (MCP) server enabling AI assistants to interact with Outline (https://www.getoutline.com)\n\n## Overview\n\nThis project implements a Model Context Protocol (MCP) server that allows AI assistants (like Claude) to interact with Outline document services, providing a bridge between natural language interactions and Outline's document management capabilities.\n\n## Features\n\nCurrently implemented:\n\n- **Document Search**: Search for documents by keywords\n- **Collection Management**: List collections and view document structure\n- **Document Reading**: Read document content, export as markdown\n- **Comment Management**: View and add comments on documents\n- **Document Creation**: Create new documents in collections\n- **Document Editing**: Update document content and move documents\n- **Backlink Management**: View documents that link to a specific document\n\n## Add to Cursor with Docker\n\nWe recommend running this python MCP server using Docker to avoid having to install dependencies on your machine.\n\n1. Install and run Docker (or Docker Desktop)\n2. Build the Docker image `docker buildx build -t mcp-outline .`\n3. In Cursor, go to the \"MCP Servers\" tab and click \"Add Server\"\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-outline\": {\n         \"command\": \"docker\",\n         \"args\": [\n           \"run\",\n           \"-i\",\n           \"--rm\",\n           \"--init\",\n           \"-e\",\n           \"DOCKER_CONTAINER=true\",\n           \"-e\",\n           \"OUTLINE_API_KEY\",\n           \"-e\",\n           \"OUTLINE_API_URL\",\n           \"mcp-outline\"\n         ],\n         \"env\": {\n           \"OUTLINE_API_KEY\": \"<YOUR_OUTLINE_API_KEY>\",\n           \"OUTLINE_API_URL\": \"<YOUR_OUTLINE_API_URL>\"\n         }\n       }\n     }\n   }\n   ```\n   > OUTLINE_API_URL is optional, defaulting to https://app.getoutline.com/api\n4. Debug the docker image by using MCP inspector and passing the docker image to it:\n   ```bash\n   npx @modelcontextprotocol/inspector docker run -i --rm --init -e DOCKER_CONTAINER=true --env-file .env mcp-outline\n   ```\n\n## Development\n\n### Prerequisites\n\n- Python 3.10+\n- Outline account with API access\n- Outline API key (get this from your Outline account settings)\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Vortiago/mcp-outline.git\ncd mcp-outline\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n```\n\n### Configuration\n\nCreate a `.env` file in the project root with the following variables:\n\n```\n# Outline API Configuration\nOUTLINE_API_KEY=your_outline_api_key_here\n\n# For cloud-hosted Outline (default)\n# OUTLINE_API_URL=https://app.getoutline.com/api\n\n# For self-hosted Outline\n# OUTLINE_API_URL=https://your-outline-instance.example.com/api\n```\n\n### Running the Server\n\n```bash\n# Development mode with the MCP Inspector\nmcp dev src/mcp_outline/server.py\n\n# Or use the provided script\n./start_server.sh\n\n# Install in Claude Desktop (if available)\nmcp install src/mcp_outline/server.py --name \"Document Outline Assistant\"\n```\n\nWhen running the MCP Inspector, go to Tools > Click on a tool > it appears on the right side so that you can query it.\n![MCP Inspector](./docs/mcp_inspector_guide.png)\n\n## Usage Examples\n\n### Search for Documents\n\n```\nSearch for documents containing \"project planning\"\n```\n\n### List Collections\n\n```\nShow me all available collections\n```\n\n### Read a Document\n\n```\nGet the content of document with ID \"docId123\"\n```\n\n### Create a New Document\n\n```\nCreate a new document titled \"Research Report\" in collection \"colId456\" with content \"# Introduction\\n\\nThis is a research report...\"\n```\n\n### Add a Comment\n\n```\nAdd a comment to document \"docId123\" saying \"This looks great, but we should add more details to the methodology section.\"\n```\n\n### Move a Document\n\n```\nMove document \"docId123\" to collection \"colId789\"\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Development\n\n```bash\n# Run tests\nuv run pytest tests/\n\n# Format code\nuv run ruff format .\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built with [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n- Uses [Outline API](https://getoutline.com) for document management\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "outline",
        "document",
        "documents",
        "mcp outline",
        "outline document",
        "outline enables"
      ],
      "category": "document-processing"
    },
    "Wildebeest--mcp_pdf_forms": {
      "owner": "Wildebeest",
      "name": "mcp_pdf_forms",
      "url": "https://github.com/Wildebeest/mcp_pdf_forms",
      "imageUrl": "/freedevtools/mcp/pfp/Wildebeest.webp",
      "description": "Manipulate and visualize PDF forms by extracting field information and highlighting form fields for analysis. Streamline PDF workflows using a toolkit built with MCP and PyMuPDF.",
      "stars": 7,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T03:15:44Z",
      "readme_content": "# MCP PDF Forms\n\nA PDF form manipulation toolkit built with [MCP](https://github.com/llama-index-ai/mcp) and PyMuPDF.\n\n## Features\n\n- Find PDF files across multiple directories\n- Extract form field information from PDF files\n- Visualize form fields in PDF documents\n\n## Installation\n\n```bash\n# Install package from PyPI\npip install mcp_pdf_forms\n\n# Or install from source\ngit clone https://github.com/Wildebeest/mcp_pdf_forms.git\ncd mcp_pdf_forms\npip install -e .\n```\n\n## Command Line Tool\n\nAfter installation, you can use the `mcp-pdf-forms` command to start the server:\n\n```bash\n# Start the server with one or more directories to scan for PDFs\nmcp-pdf-forms examples\n```\n\nYou can also add it to Claude Code as an MCP:\n\n```bash\nclaude mcp add pdf-forms mcp-pdf-forms .\n```\n## Usage\n\nOnce installed, you can use the package to work with PDF forms. The package provides tools through the MCP interface.\n\n### PDF Discovery Tool\n\nThe PDF Discovery tool helps you find PDF files across specified directories.\n\n- **Input**: Directory paths to search for PDFs\n- **Output**: List of PDF files found in the specified directories\n- **Usage**: Use this to quickly locate all PDF files in your project or specified folders\n\n### Form Field Extraction Tool\n\nThe Form Field Extraction tool extracts information about all form fields in a PDF document.\n\n- **Input**: Path to a PDF file\n- **Output**: Detailed information about each form field including field name, type, position, and other properties\n- **Usage**: Use this to analyze form structure and understand the fields available for filling\n\n### Field Highlight Visualization Tool\n\nThe Field Highlight tool creates a visual representation of form fields in the PDF.\n\n- **Input**: Path to a PDF file\n- **Output**: Modified PDF with all form fields highlighted for easy identification\n- **Usage**: Use this to visually inspect the layout and position of form fields in your document\n\n## Libraries Used\n\n- [MCP](https://github.com/llama-index-ai/mcp) - Machine Conversation Protocol framework\n- [PyMuPDF](https://github.com/pymupdf/PyMuPDF) - Python bindings for MuPDF, a high-performance PDF library\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_pdf_forms",
        "pdf",
        "forms",
        "pdf forms",
        "mcp_pdf_forms manipulate",
        "wildebeest mcp_pdf_forms"
      ],
      "category": "document-processing"
    },
    "YassineTk--mcp-docs-provider": {
      "owner": "YassineTk",
      "name": "mcp-docs-provider",
      "url": "https://github.com/YassineTk/mcp-docs-provider",
      "imageUrl": "/freedevtools/mcp/pfp/YassineTk.webp",
      "description": "Enables AI models to access and query local markdown technical documentation, enhancing context-aware responses. Supports dynamic integration of documentation, allowing updates without server rebuilds.",
      "stars": 5,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-16T13:10:25Z",
      "readme_content": "# MCP Docs Provider\n\n[![smithery badge](https://smithery.ai/badge/@YassineTk/mcp-docs-provider)](https://smithery.ai/server/@YassineTk/mcp-docs-provider)\n\nDocumentation context provider for LLMs via MCP. This server enables AI models to seamlessly access and query your local markdown technical documentation.\n\n### Installing via Smithery\n\nTo install mcp-docs-provider for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@YassineTk/mcp-docs-provider):\n\n```bash\nnpx -y @smithery/cli install @YassineTk/mcp-docs-provider --client claude\n```\n\n## Configuration with cursor\n\nAdd this to your Cursor configuration file (`mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-docs-provider\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-docs-provider\",\n        \"/path/to/your/documentation.md\"\n      ]\n    }\n  }\n}\n```\n\n- `/path/to/your/documentation.md` with the path to your markdown documentation file\n### No rebuild is required after updating your Markdown documentation.\n\n## MCP Client Rules Configuration\n\nAdd the following specification to your MCP Client Rules (eg. Cursor) (This ensures the documentation context is automatically used without explicitly mentioning \"Using my MCP\" in queries.):\n\"If a user ask you about ui pattern then follow the mcp-docs-provider MCP server.\"\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "markdown",
        "documentation enhancing",
        "docs provider",
        "mcp docs"
      ],
      "category": "document-processing"
    },
    "a-bonus--google-docs-mcp": {
      "owner": "a-bonus",
      "name": "google-docs-mcp",
      "url": "https://github.com/a-bonus/google-docs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/a-bonus.webp",
      "description": "Connect to Google Docs for reading and appending text to documents programmatically, enabling automated workflows and document interaction through AI assistants.",
      "stars": 115,
      "forks": 26,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T22:04:45Z",
      "readme_content": "# Ultimate Google Docs & Drive MCP Server\n\n![Demo Animation](assets/google.docs.mcp.1.gif)\n\nConnect Claude Desktop (or other MCP clients) to your Google Docs and Google Drive!\n\n> 🔥 **Check out [15 powerful tasks](SAMPLE_TASKS.md) you can accomplish with this enhanced server!**\n> 📁 **NEW:** Complete Google Drive file management capabilities!\n\nThis comprehensive server uses the Model Context Protocol (MCP) and the `fastmcp` library to provide tools for reading, writing, formatting, structuring Google Documents, and managing your entire Google Drive. It acts as a powerful bridge, allowing AI assistants like Claude to interact with your documents and files programmatically with advanced capabilities.\n\n**Features:**\n\n### Document Access & Editing\n- **Read Documents:** Read content with `readGoogleDoc` (plain text, JSON structure, or markdown)\n- **Append to Documents:** Add text to documents with `appendToGoogleDoc`\n- **Insert Text:** Place text at specific positions with `insertText`\n- **Delete Content:** Remove content from a document with `deleteRange`\n\n### Formatting & Styling\n- **Text Formatting:** Apply rich styling with `applyTextStyle` (bold, italic, colors, etc.)\n- **Paragraph Formatting:** Control paragraph layout with `applyParagraphStyle` (alignment, spacing, etc.)\n- **Find & Format:** Format by text content using `formatMatchingText` (legacy support)\n\n### Document Structure\n- **Tables:** Create tables with `insertTable`\n- **Page Breaks:** Insert page breaks with `insertPageBreak`\n- **Experimental Features:** Tools like `fixListFormatting` for automatic list detection\n\n### 🆕 Comment Management\n- **List Comments:** View all comments in a document with `listComments` (shows author, date, and quoted text)\n- **Get Comment Details:** Get specific comment with replies using `getComment`\n- **Add Comments:** Create new comments anchored to text with `addComment`\n- **Reply to Comments:** Add replies to existing comments with `replyToComment`\n- **Resolve Comments:** Mark comments as resolved with `resolveComment`\n- **Delete Comments:** Remove comments from documents with `deleteComment`\n\n### 🆕 Google Drive File Management\n- **Document Discovery:** Find and list documents with `listGoogleDocs`, `searchGoogleDocs`, `getRecentGoogleDocs`\n- **Document Information:** Get detailed metadata with `getDocumentInfo`\n- **Folder Management:** Create folders (`createFolder`), list contents (`listFolderContents`), get info (`getFolderInfo`)\n- **File Operations:** Move (`moveFile`), copy (`copyFile`), rename (`renameFile`), delete (`deleteFile`)\n- **Document Creation:** Create new docs (`createDocument`) or from templates (`createFromTemplate`)\n\n### Integration\n- **Google Authentication:** Secure OAuth 2.0 authentication with full Drive access\n- **MCP Compliant:** Designed for use with Claude and other MCP clients\n- **VS Code Integration:** [Setup guide](vscode.md) for VS Code MCP extension\n\n---\n\n## Prerequisites\n\nBefore you start, make sure you have:\n\n1.  **Node.js and npm:** A recent version of Node.js (which includes npm) installed on your computer. You can download it from [nodejs.org](https://nodejs.org/). (Version 18 or higher recommended).\n2.  **Git:** Required for cloning this repository. ([Download Git](https://git-scm.com/downloads)).\n3.  **A Google Account:** The account that owns or has access to the Google Docs you want to interact with.\n4.  **Command Line Familiarity:** Basic comfort using a terminal or command prompt (like Terminal on macOS/Linux, or Command Prompt/PowerShell on Windows).\n5.  **Claude Desktop (Optional):** If your goal is to connect this server to Claude, you'll need the Claude Desktop application installed.\n\n---\n\n## Setup Instructions\n\nFollow these steps carefully to get your own instance of the server running.\n\n### Step 1: Google Cloud Project & Credentials (The Important Bit!)\n\nThis server needs permission to talk to Google APIs on your behalf. You'll create special \"keys\" (credentials) that only your server will use.\n\n1.  **Go to Google Cloud Console:** Open your web browser and go to the [Google Cloud Console](https://console.cloud.google.com/). You might need to log in with your Google Account.\n2.  **Create or Select a Project:**\n    - If you don't have a project, click the project dropdown near the top and select \"NEW PROJECT\". Give it a name (e.g., \"My MCP Docs Server\") and click \"CREATE\".\n    - If you have existing projects, you can select one or create a new one.\n3.  **Enable APIs:** You need to turn on the specific Google services this server uses.\n    - In the search bar at the top, type \"APIs & Services\" and select \"Library\".\n    - Search for \"**Google Docs API**\" and click on it. Then click the \"**ENABLE**\" button.\n    - Search for \"**Google Drive API**\" and click on it. Then click the \"**ENABLE**\" button (this is often needed for finding files or permissions).\n4.  **Configure OAuth Consent Screen:** This screen tells users (usually just you) what your app wants permission for.\n    - On the left menu, click \"APIs & Services\" -> \"**OAuth consent screen**\".\n    - Choose User Type: Select \"**External**\" and click \"CREATE\".\n    - Fill in App Information:\n      - **App name:** Give it a name users will see (e.g., \"Claude Docs MCP Access\").\n      - **User support email:** Select your email address.\n      - **Developer contact information:** Enter your email address.\n    - Click \"**SAVE AND CONTINUE**\".\n    - **Scopes:** Click \"**ADD OR REMOVE SCOPES**\". Search for and add the following scopes:\n      - `https://www.googleapis.com/auth/documents` (Allows reading/writing docs)\n      - `https://www.googleapis.com/auth/drive.file` (Allows access to specific files opened/created by the app)\n      - Click \"**UPDATE**\".\n    - Click \"**SAVE AND CONTINUE**\".\n    - **Test Users:** Click \"**ADD USERS**\". Enter the same Google email address you are logged in with. Click \"**ADD**\". This allows _you_ to use the app while it's in \"testing\" mode.\n    - Click \"**SAVE AND CONTINUE**\". Review the summary and click \"**BACK TO DASHBOARD**\".\n5.  **Create Credentials (The Keys!):**\n    - On the left menu, click \"APIs & Services\" -> \"**Credentials**\".\n    - Click \"**+ CREATE CREDENTIALS**\" at the top and choose \"**OAuth client ID**\".\n    - **Application type:** Select \"**Desktop app**\" from the dropdown.\n    - **Name:** Give it a name (e.g., \"MCP Docs Desktop Client\").\n    - Click \"**CREATE**\".\n6.  **⬇️ DOWNLOAD THE CREDENTIALS FILE:** A box will pop up showing your Client ID. Click the \"**DOWNLOAD JSON**\" button.\n    - Save this file. It will likely be named something like `client_secret_....json`.\n    - **IMPORTANT:** Rename the downloaded file to exactly `credentials.json`.\n7.  ⚠️ **SECURITY WARNING:** Treat this `credentials.json` file like a password! Do not share it publicly, and **never commit it to GitHub.** Anyone with this file could potentially pretend to be _your application_ (though they'd still need user consent to access data).\n\n### Step 2: Get the Server Code\n\n1.  **Clone the Repository:** Open your terminal/command prompt and run:\n    ```bash\n    git clone https://github.com/a-bonus/google-docs-mcp.git mcp-googledocs-server\n    ```\n2.  **Navigate into Directory:**\n    ```bash\n    cd mcp-googledocs-server\n    ```\n3.  **Place Credentials:** Move or copy the `credentials.json` file you downloaded and renamed (from Step 1.6) directly into this `mcp-googledocs-server` folder.\n\n### Step 3: Install Dependencies\n\nYour server needs some helper libraries specified in the `package.json` file.\n\n1.  In your terminal (make sure you are inside the `mcp-googledocs-server` directory), run:\n    ```bash\n    npm install\n    ```\n    This will download and install all the necessary packages into a `node_modules` folder.\n\n### Step 4: Build the Server Code\n\nThe server is written in TypeScript (`.ts`), but we need to compile it into JavaScript (`.js`) that Node.js can run directly.\n\n1.  In your terminal, run:\n    ```bash\n    npm run build\n    ```\n    This uses the TypeScript compiler (`tsc`) to create a `dist` folder containing the compiled JavaScript files.\n\n### Step 5: First Run & Google Authorization (One Time Only)\n\nNow you need to run the server once manually to grant it permission to access your Google account data. This will create a `token.json` file that saves your permission grant.\n\n1.  In your terminal, run the _compiled_ server using `node`:\n    ```bash\n    node ./dist/server.js\n    ```\n2.  **Watch the Terminal:** The script will print:\n    - Status messages (like \"Attempting to authorize...\").\n    - An \"Authorize this app by visiting this url:\" message followed by a long `https://accounts.google.com/...` URL.\n3.  **Authorize in Browser:**\n    - Copy the entire long URL from the terminal.\n    - Paste the URL into your web browser and press Enter.\n    - Log in with the **same Google account** you added as a Test User in Step 1.4.\n    - Google will show a screen asking for permission for your app (\"Claude Docs MCP Access\" or similar) to access Google Docs/Drive. Review and click \"**Allow**\" or \"**Grant**\".\n4.  **Get the Authorization Code:**\n    - After clicking Allow, your browser will likely try to redirect to `http://localhost` and show a **\"This site can't be reached\" error**. **THIS IS NORMAL!**\n    - Look **carefully** at the URL in your browser's address bar. It will look like `http://localhost/?code=4/0Axxxxxxxxxxxxxx&scope=...`\n    - Copy the long string of characters **between `code=` and the `&scope` part**. This is your single-use authorization code.\n5.  **Paste Code into Terminal:** Go back to your terminal where the script is waiting (\"Enter the code from that page here:\"). Paste the code you just copied.\n6.  **Press Enter.**\n7.  **Success!** The script should print:\n    - \"Authentication successful!\"\n    - \"Token stored to .../token.json\"\n    - It will then finish starting and likely print \"Awaiting MCP client connection via stdio...\" or similar, and then exit (or you can press `Ctrl+C` to stop it).\n8.  ✅ **Check:** You should now see a new file named `token.json` in your `mcp-googledocs-server` folder.\n9.  ⚠️ **SECURITY WARNING:** This `token.json` file contains the key that allows the server to access your Google account _without_ asking again. Protect it like a password. **Do not commit it to GitHub.** The included `.gitignore` file should prevent this automatically.\n\n### Step 6: Configure Claude Desktop (Optional)\n\nIf you want to use this server with Claude Desktop, you need to tell Claude how to run it.\n\n1.  **Find Your Absolute Path:** You need the full path to the server code.\n    - In your terminal, make sure you are still inside the `mcp-googledocs-server` directory.\n    - Run the `pwd` command (on macOS/Linux) or `cd` (on Windows, just displays the path).\n    - Copy the full path (e.g., `/Users/yourname/projects/mcp-googledocs-server` or `C:\\Users\\yourname\\projects\\mcp-googledocs-server`).\n2.  **Locate `mcp_config.json`:** Find Claude's configuration file:\n    - **macOS:** `~/Library/Application Support/Claude/mcp_config.json` (You might need to use Finder's \"Go\" -> \"Go to Folder...\" menu and paste `~/Library/Application Support/Claude/`)\n    - **Windows:** `%APPDATA%\\Claude\\mcp_config.json` (Paste `%APPDATA%\\Claude` into File Explorer's address bar)\n    - **Linux:** `~/.config/Claude/mcp_config.json`\n    - _If the `Claude` folder or `mcp_config.json` file doesn't exist, create them._\n3.  **Edit `mcp_config.json`:** Open the file in a text editor. Add or modify the `mcpServers` section like this, **replacing `/PATH/TO/YOUR/CLONED/REPO` with the actual absolute path you copied in Step 6.1**:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"google-docs-mcp\": {\n          \"command\": \"node\",\n          \"args\": [\n            \"/PATH/TO/YOUR/CLONED/REPO/mcp-googledocs-server/dist/server.js\"\n          ],\n          \"env\": {}\n        }\n        // Add commas here if you have other servers defined\n      }\n      // Other Claude settings might be here\n    }\n    ```\n\n    - **Make sure the path in `\"args\"` is correct and absolute!**\n    - If the file already existed, carefully merge this entry into the existing `mcpServers` object. Ensure the JSON is valid (check commas!).\n\n4.  **Save `mcp_config.json`.**\n5.  **Restart Claude Desktop:** Close Claude completely and reopen it.\n\n---\n\n## Usage with Claude Desktop\n\nOnce configured, you should be able to use the tools in your chats with Claude:\n\n- \"Use the `google-docs-mcp` server to read the document with ID `YOUR_GOOGLE_DOC_ID`.\"\n- \"Can you get the content of Google Doc `YOUR_GOOGLE_DOC_ID`?\"\n- \"Append 'This was added by Claude!' to document `YOUR_GOOGLE_DOC_ID` using the `google-docs-mcp` tool.\"\n\n### Advanced Usage Examples:\n- **Text Styling**: \"Use `applyTextStyle` to make the text 'Important Section' bold and red (#FF0000) in document `YOUR_GOOGLE_DOC_ID`.\"\n- **Paragraph Styling**: \"Use `applyParagraphStyle` to center-align the paragraph containing 'Title Here' in document `YOUR_GOOGLE_DOC_ID`.\"\n- **Table Creation**: \"Insert a 3x4 table at index 500 in document `YOUR_GOOGLE_DOC_ID` using the `insertTable` tool.\"\n- **Legacy Formatting**: \"Use `formatMatchingText` to find the second instance of 'Project Alpha' and make it blue (#0000FF) in doc `YOUR_GOOGLE_DOC_ID`.\"\n\nRemember to replace `YOUR_GOOGLE_DOC_ID` with the actual ID from a Google Doc's URL (the long string between `/d/` and `/edit`).\n\nClaude will automatically launch your server in the background when needed using the command you provided. You do **not** need to run `node ./dist/server.js` manually anymore.\n\n---\n\n## Security & Token Storage\n\n- **`.gitignore`:** This repository includes a `.gitignore` file which should prevent you from accidentally committing your sensitive `credentials.json` and `token.json` files. **Do not remove these lines from `.gitignore`**.\n- **Token Storage:** This server stores the Google authorization token (`token.json`) directly in the project folder for simplicity during setup. In production or more security-sensitive environments, consider storing this token more securely, such as using system keychains, encrypted files, or dedicated secret management services.\n\n---\n\n## Troubleshooting\n\n- **Claude shows \"Failed\" or \"Could not attach\":**\n  - Double-check the absolute path in `mcp_config.json`.\n  - Ensure you ran `npm run build` successfully and the `dist` folder exists.\n  - Try running the command from `mcp_config.json` manually in your terminal: `node /PATH/TO/YOUR/CLONED/REPO/mcp-googledocs-server/dist/server.js`. Look for any errors printed.\n  - Check the Claude Desktop logs (see the official MCP debugging guide).\n  - Make sure all `console.log` status messages in the server code were changed to `console.error`.\n- **Google Authorization Errors:**\n  - Ensure you enabled the correct APIs (Docs, Drive).\n  - Make sure you added your email as a Test User on the OAuth Consent Screen.\n  - Verify the `credentials.json` file is correctly placed in the project root.\n\n---\n\n## License\n\nThis project is licensed under the MIT License - see the `LICENSE` file for details. (Note: You should add a `LICENSE` file containing the MIT License text to your repository).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "documents",
        "docs",
        "document processing",
        "google docs",
        "document interaction"
      ],
      "category": "document-processing"
    },
    "adamanz--mcp-video-converter": {
      "owner": "adamanz",
      "name": "mcp-video-converter",
      "url": "https://github.com/adamanz/mcp-video-converter",
      "imageUrl": "/freedevtools/mcp/pfp/adamanz.webp",
      "description": "Convert video, audio, and image files between various formats using FFmpeg. Check for FFmpeg installation and retrieve information on supported file formats for conversion.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-11T14:54:58Z",
      "readme_content": "# MCP Video Converter Server\n\nAn MCP server that provides tools for checking FFmpeg installation and converting video files between various formats.\n\n## Features\n\n- **Check FFmpeg**: Verifies if FFmpeg is installed and accessible.\n- **Convert Video**: Converts video, audio, and image files to various formats (e.g., MP4, WebM, MOV, MP3, PNG).\n- **Format Info**: Get a list of supported file formats for conversion.\n\n## Prerequisites\n\n- Python 3.10+\n- FFmpeg installed and available in your system's PATH\n- [Optional] [uv](https://github.com/astral-sh/uv) for environment management\n\n## Setup\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/adamanz/mcp-video-converter.git\n   cd mcp-video-converter\n   ```\n\n2. Create and activate a virtual environment:\n   ```bash\n   # Using venv (standard library)\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   \n   # Or using uv (recommended if available)\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   # Using pip\n   pip install -e .\n   pip install fastmcp\n\n   # Or using uv\n   uv pip install -e .\n   uv pip install fastmcp\n   ```\n\n4. Verify your installation:\n   ```bash\n   # Run the installation check script\n   python check_installation.py\n   ```\n\n## Running the Server Directly\n\nYou can run the server directly:\n\n```bash\n# Activate the virtual environment if not already activated\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Run the server\npython -m mcp_video_converter.server\n```\n\n## Integrating with Claude Desktop\n\nTo add this MCP server to Claude Desktop:\n\n1. Locate or create the Claude Desktop configuration file:\n   ```bash\n   # macOS\n   mkdir -p ~/Library/Application\\ Support/Claude/\n   nano ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   \n   # Windows\n   mkdir -p %APPDATA%\\Claude\\\n   notepad %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. Add the MCP server configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"video-convert\": {\n         \"command\": \"/bin/bash\",\n         \"args\": [\n           \"-c\",\n           \"cd /absolute/path/to/mcp-video-converter && source .venv/bin/activate && python -m mcp_video_converter.server\"\n         ]\n       }\n     }\n   }\n   ```\n\n   **Windows Alternative:**\n   ```json\n   {\n     \"mcpServers\": {\n       \"video-convert\": {\n         \"command\": \"cmd.exe\",\n         \"args\": [\n           \"/c\",\n           \"cd /d C:\\\\absolute\\\\path\\\\to\\\\mcp-video-converter && .venv\\\\Scripts\\\\activate && python -m mcp_video_converter.server\"\n         ]\n       }\n     }\n   }\n   ```\n\n   Replace `/absolute/path/to/mcp-video-converter` with the absolute path to your repository.\n\n3. Restart Claude Desktop\n   - The server will appear as \"video-convert\" in the MCP tools menu\n\n4. Important notes:\n   - Always use absolute paths in your configuration\n   - Make sure FFmpeg is installed and in your PATH\n   - If you encounter issues, check the Claude Desktop logs:\n     ```bash\n     # macOS\n     tail -n 20 -F ~/Library/Logs/Claude/mcp*.log\n     \n     # Windows\n     type %APPDATA%\\Claude\\logs\\mcp*.log\n     ```\n\n## Integrating with Cursor\n\nTo add this MCP server to Cursor:\n\n1. Locate or create the Cursor configuration file:\n   ```bash\n   # macOS\n   mkdir -p ~/.cursor/\n   nano ~/.cursor/config.json\n   \n   # Windows\n   mkdir -p %USERPROFILE%\\.cursor\\\n   notepad %USERPROFILE%\\.cursor\\config.json\n   ```\n\n2. Add the MCP server configuration:\n   ```json\n   {\n     \"ai\": {\n       \"mcpServers\": {\n         \"video-convert\": {\n           \"command\": \"/bin/bash\",\n           \"args\": [\n             \"-c\",\n             \"cd /absolute/path/to/mcp-video-converter && source .venv/bin/activate && python -m mcp_video_converter.server\"\n           ]\n         }\n       }\n     }\n   }\n   ```\n\n   **Windows Alternative:**\n   ```json\n   {\n     \"ai\": {\n       \"mcpServers\": {\n         \"video-convert\": {\n           \"command\": \"cmd.exe\",\n           \"args\": [\n             \"/c\",\n             \"cd /d C:\\\\absolute\\\\path\\\\to\\\\mcp-video-converter && .venv\\\\Scripts\\\\activate && python -m mcp_video_converter.server\"\n           ]\n         }\n       }\n     }\n   }\n   ```\n\n   Replace `/absolute/path/to/mcp-video-converter` with the absolute path to your repository.\n\n3. Restart Cursor\n   - The server will be available to Claude in Cursor\n\n4. Important notes:\n   - Always use absolute paths in your configuration\n   - Make sure FFmpeg is installed and in your PATH\n   - Logs may be accessed through Cursor's developer tools\n\n## Deploying with Smithery\n\nSmithery is a platform that simplifies deploying and managing MCP servers. This project is fully configured for Smithery deployment with the required files and configurations.\n\n### Required Configuration Files\n\nThis project includes all required configuration files for Smithery deployment:\n\n1. **smithery.yaml**: Defines how to start your server and its configuration options\n2. **Dockerfile**: Defines how to build your server's container image\n\n### Smithery YAML Configuration\n\nThe `smithery.yaml` file provides Smithery with instructions on how to run your server:\n\n```yaml\nstartCommand:\n  type: stdio\n  configSchema:\n    type: object\n    properties:\n      ffmpegPath:\n        type: string\n        title: \"FFmpeg Path\"\n        description: \"Optional path to FFmpeg executable (uses system PATH by default)\"\n      outputDirectory:\n        type: string\n        title: \"Output Directory\"\n        description: \"Optional custom directory for output files\"\n      quality:\n        type: string\n        enum: [\"low\", \"medium\", \"high\"]\n        default: \"medium\"\n        title: \"Default Quality\"\n  name: \"MCP Video Converter\"\n  description: \"Convert video files between formats and check FFmpeg installation\"\n  commandFunction: |\n    (config) => {\n      // Function that returns command details based on configuration options\n    }\n\nbuild:\n  dockerfile: Dockerfile\n  dockerBuildPath: .\n  env:\n    OUTPUT_DIRECTORY: \"/data/converted\"\n  buildOptions:\n    buildArgs:\n      PYTHON_VERSION: \"3.10\"\n      INSTALL_DEV: \"false\"\n    labels:\n      org.opencontainers.image.source: \"https://github.com/adamanz/mcp-video-converter\"\n      org.opencontainers.image.description: \"MCP Server for video conversion using FFmpeg\"\n      org.opencontainers.image.licenses: \"MIT\"\n```\n\nKey components:\n- **type: stdio**: Defines that our server uses the standard I/O transport\n- **configSchema**: Defines the configuration options users can set (FFmpeg path, output directory, quality)\n- **commandFunction**: JavaScript function that returns how to start the server based on configuration\n- **build**: Container-specific configuration for Dockerized deployment\n\n### Deploying to Smithery\n\n1. Install Smithery CLI if you haven't already:\n   ```bash\n   # Install the Smithery command-line tool\n   npm install -g @smithery/cli\n   ```\n\n2. Login to Smithery:\n   ```bash\n   smithery login\n   ```\n\n3. Deploy directly from the repository:\n   ```bash\n   # Navigate to the repository directory\n   cd /path/to/adamanz/mcp-video-converter\n   \n   # Deploy to Smithery\n   smithery deploy\n   ```\n\n   Alternatively, deploy with explicit build options:\n   ```bash\n   # Deploy with container build\n   smithery deploy --build\n   \n   # Deploy with custom build arguments\n   smithery deploy --build --build-arg PYTHON_VERSION=3.11\n   ```\n\n4. Configure and start the server in Smithery:\n   ```bash\n   # Configure the server (interactive)\n   smithery configure mcp-video-converter\n   \n   # Start the server\n   smithery start mcp-video-converter\n   ```\n\n### Docker Support\n\nThis project includes a multi-stage Dockerfile for efficient containerized deployment. The container:\n\n- Uses a multi-stage build process to reduce final image size\n- Installs FFmpeg and all required dependencies\n- Creates a dedicated volume mount point for converted files\n- Includes a healthcheck for better container monitoring\n\nYou can build and run the Docker container manually:\n\n```bash\n# Build the container\ndocker build -t mcp-video-converter .\n\n# Run the container\ndocker run -it --rm \\\n  -v $(pwd)/converted:/data/converted \\\n  -e FFMPEG_PATH=/usr/bin/ffmpeg \\\n  -e DEFAULT_QUALITY=high \\\n  mcp-video-converter\n```\n\n### Serverless Hosting Considerations\n\nWhen deploying to Smithery's serverless environment, be aware of the following:\n\n- **Connection Timeout**: Connections to your server will timeout after 2 minutes of inactivity\n- **Ephemeral Storage**: Design your server with ephemeral storage in mind\n- **Stateless Design**: The server should not rely on persistent local storage\n- **Output Files**: Video conversion outputs should be returned properly as part of the tool response to ensure clients can access them\n\n### Smithery Management\n\nUseful Smithery commands for managing your deployment:\n\n```bash\n# View server logs\nsmithery logs mcp-video-converter\n\n# Update to latest version\nsmithery update mcp-video-converter\n\n# Stop the server\nsmithery stop mcp-video-converter\n\n# Remove the server\nsmithery remove mcp-video-converter\n```\n\n### Integrating with Smithery Apps\n\nUsers can access your server through the Smithery app:\n\n1. Open the Smithery application\n2. Navigate to \"Servers\" tab\n3. Select \"mcp-video-converter\"\n4. Configure settings if prompted (FFmpeg path, output directory, quality)\n5. Connect to the server\n6. Use the server with compatible MCP clients\n\n### Testing Before Deployment\n\nBefore deploying to Smithery, it's recommended to test your server locally:\n\n```bash\n# Test with MCP Inspector (if available)\nmcp-inspector -s /path/to/mcp-video-converter/smithery.yaml\n\n# Or test by running the server directly\ncd /path/to/mcp-video-converter\npython -m mcp_video_converter.server\n```\n\n## Troubleshooting Common Issues\n\n### Server Not Found\n\nIf the MCP server is not being picked up:\n\n1. Verify the paths in your configuration file are absolute and correct\n2. Check that FFmpeg is installed and in your PATH\n3. Ensure the virtual environment is activated in your command\n4. Check the logs for specific error messages\n\n### Python Module Not Found\n\nIf you see errors about missing modules:\n\n1. Make sure you installed all dependencies with `pip install -e .` and `pip install fastmcp`\n2. Verify the virtual environment is being activated correctly\n3. Try reinstalling the package: `pip install -e .`\n\n### FFmpeg Not Found\n\nIf FFmpeg cannot be found:\n\n1. Verify FFmpeg is installed: `which ffmpeg` or `where ffmpeg` on Windows\n2. Add the FFmpeg directory to your PATH\n3. In the configuration, you can specify the full path to FFmpeg:\n   ```json\n   \"env\": {\n     \"PATH\": \"/usr/local/bin:/usr/bin:/bin:/path/to/ffmpeg/bin\"\n   }\n   ```\n\n## Example Usage (with Claude)\n\nOnce integrated, you can ask Claude to perform tasks like:\n\n1. \"Check if FFmpeg is installed on my system\"\n2. \"Convert this video file: /path/to/video.webm to MP4 format with high quality\"\n3. \"What video formats can I convert to?\"\n\nClaude will use the appropriate tools from the MCP server to accomplish these tasks.\n\n## Advanced: Using with fastmcp client\n\nFor programmatic usage, you can use the fastmcp client:\n\n```bash\n# Check FFmpeg installation\nfastmcp client call <SERVER_URL_OR_FILE_PATH> check_ffmpeg_installed '{}'\n\n# Get supported formats\nfastmcp client call <SERVER_URL_OR_FILE_PATH> get_supported_formats '{}'\n\n# Convert a video\nfastmcp client call <SERVER_URL_OR_FILE_PATH> convert_video '{\n  \"input_file_path\": \"/path/to/your/video.webm\", \n  \"output_format\": \"mp4\", \n  \"quality\": \"high\"\n}'\n```\n\nReplace `/path/to/your/video.webm` with an actual video file path.\n\n## Supported Formats\n\n- **Video**: MP4, WebM, MOV, AVI, MKV, FLV, GIF\n- **Audio**: MP3, WAV, OGG, AAC, M4A\n- **Image**: WebP, JPG, PNG, BMP, TIFF\n\n## Running Tests\n\n```bash\n# Using pip\npip install pytest\npytest\n\n# Using uv\nuv pip install pytest\nuv run pytest\n```\n\n## License\n\nThis project is open source and available under the [MIT License](mcp-video-converter/LICENSE).\n\n## Contributing\n\nContributions are welcome! Please see [CONTRIBUTING.md](mcp-video-converter/CONTRIBUTING.md) for details on how to contribute to this project.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ffmpeg",
        "converter",
        "formats",
        "convert video",
        "video converter",
        "formats conversion"
      ],
      "category": "document-processing"
    },
    "adiom-data--lance-mcp": {
      "owner": "adiom-data",
      "name": "lance-mcp",
      "url": "https://github.com/adiom-data/lance-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/adiom-data.webp",
      "description": "Interact with on-disk documents through retrieval-augmented generation (RAG) and hybrid search capabilities in LanceDB.",
      "stars": 71,
      "forks": 16,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:32Z",
      "readme_content": "# 🗄️ LanceDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly the documents that they have on-disk through agentic RAG and hybrid search in LanceDB. Ask LLMs questions about the dataset as a whole or about specific documents.\n\n## ✨ Features\n\n- 🔍 LanceDB-powered serverless vector index and document summary catalog.\n- 📊 Efficient use of LLM tokens. The LLM itself looks up what it needs when it needs.\n- 📈 Security. The index is stored locally so no data is transferred to the Cloud when using a local LLM.\n\n## 🚀 Quick Start\n\nTo get started, create a local directory to store the index and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"lancedb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"lance-mcp\",\n        \"PATH_TO_LOCAL_INDEX_DIR\"\n      ]\n    }\n  }\n}\n```\n\n### Prerequisites\n\n- Node.js 18+\n- npx\n- MCP Client (Claude Desktop App for example)\n- Summarization and embedding models installed (see config.ts - by default we use Ollama models)\n  - `ollama pull snowflake-arctic-embed2`\n  - `ollama pull llama3.1:8b`\n\n### Demo\n\n<img src=\"https://github.com/user-attachments/assets/90bfdea9-9edd-4cf6-bb04-94c9c84e4825\" width=\"50%\">\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"lancedb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"PATH_TO_LANCE_MCP/dist/index.js\",\n        \"PATH_TO_LOCAL_INDEX_DIR\"\n      ]\n    }\n  }\n}\n```\nUse `npm run build` to build the project.\n\nUse `npx @modelcontextprotocol/inspector dist/index.js PATH_TO_LOCAL_INDEX_DIR` to run the MCP tool inspector.\n\n### Seed Data\n\nThe seed script creates two tables in LanceDB - one for the catalog of document summaries, and another one - for vectorized documents' chunks.\nTo run the seed script use the following command:\n```console\nnpm run seed -- --dbpath <PATH_TO_LOCAL_INDEX_DIR> --filesdir <PATH_TO_DOCS>\n```\n\nYou can use sample data from the docs/ directory. Feel free to adjust the default summarization and embedding models in the config.ts file. If you need to recreate the index, simply rerun the seed script with the `--overwrite` option.\n\n#### Catalog\n\n- Document summary\n- Metadata\n\n#### Chunks\n\n- Vectorized document chunk\n- Metadata\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n```plaintext\n\"What documents do we have in the catalog?\"\n\"Why is the US healthcare system so broken?\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for interaction with the index:\n\n### Catalog Tools\n\n- `catalog_search`: Search for relevant documents in the catalog\n\n### Chunks Tools\n\n- `chunks_search`: Find relevant chunks based on a specific document from the catalog\n- `all_chunks_search`: Find relevant chunks from all known documents\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "documents",
        "lancedb",
        "documents retrieval",
        "document processing",
        "disk documents"
      ],
      "category": "document-processing"
    },
    "agentset-ai--mcp-server": {
      "owner": "agentset-ai",
      "name": "mcp-server",
      "url": "https://github.com/agentset-ai/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/agentset-ai.webp",
      "description": "Build intelligent, document-based applications with integrated data retrieval capabilities, leveraging Retrieval-Augmented Generation (RAG) methodologies.",
      "stars": 17,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-18T06:02:08Z",
      "readme_content": "# Agentset MCP\n\nMCP server for Agentset, an open-source platform for Retrieval-Augmented Generation (RAG). Designed for developers who want to build intelligent, document-based applications quickly and efficiently.\n\n[![npm version][npm-badge]][npm]\n[![License][license-badge]][license]\n[![Build Status][build-badge]][build]\n[![Downloads][downloads-badge]][npm]\n[![Size][size-badge]][npm]\n\n## Installation\n\nusing npm:\n\n```sh\nAGENTSET_API_KEY=your-api-key npx @agentset/mcp --ns your-namespace-id\n```\n\nusing yarn:\n\n```sh\nAGENTSET_API_KEY=your-api-key yarn dlx @agentset/mcp --ns your-namespace-id\n```\n\nusing pnpm:\n\n```sh\nAGENTSET_API_KEY=your-api-key pnpm dlx @agentset/mcp --ns your-namespace-id\n```\n\n## Adding to Claude\n\n```json\n{\n  \"mcpServers\": {\n    \"agentset\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@agentset/mcp@latest\"],\n      \"env\": {\n        \"AGENTSET_API_KEY\": \"agentset_xxx\",\n        \"AGENTSET_NAMESPACE_ID\": \"ns_xxx\"\n      }\n    }\n  }\n}\n```\n\n## Tips\n\nPassing namespace id as an environment variable\n\n```sh\nAGENTSET_API_KEY=your-api-key AGENTSET_NAMESPACE_ID=your-namespace-id npx @agentset/mcp\n```\n\nPassing a custom tool description\n\n```sh\nAGENTSET_API_KEY=your-api-key npx @agentset/mcp --ns your-namespace-id -d \"Your custom tool description\"\n```\n\nPassing a tenant id:\n\n```sh\nAGENTSET_API_KEY=your-api-key npx @agentset/mcp --ns your-namespace-id -t your-tenant-id\n```\n\n## API Reference\n\nVisit the [full documentation](https://docs.agentset.ai) for more details.\n\n<!-- Links -->\n\n[docs]: https://docs.agentset.ai/\n[build-badge]: https://github.com/agentset-ai/mcp-server/actions/workflows/release.yml/badge.svg\n[build]: https://github.com/agentset-ai/mcp-server/actions/workflows/release.yml\n[license-badge]: https://badgen.net/github/license/agentset-ai/mcp-server\n[license]: https://github.com/agentset-ai/mcp-server/blob/main/LICENSE\n[npm]: https://www.npmjs.com/package/@agentset/mcp\n[npm-badge]: https://badgen.net/npm/v/@agentset/mcp\n[downloads-badge]: https://img.shields.io/npm/dm/@agentset/mcp.svg\n[size-badge]: https://badgen.net/packagephobia/publish/@agentset/mcp\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "document",
        "agentset",
        "document processing",
        "intelligent document",
        "data retrieval"
      ],
      "category": "document-processing"
    },
    "aigo666--mcp-framework": {
      "owner": "aigo666",
      "name": "mcp-framework",
      "url": "https://github.com/aigo666/mcp-framework",
      "imageUrl": "/freedevtools/mcp/pfp/aigo666.webp",
      "description": "Create custom tools to interact with large language models, facilitating web content fetching and processing of various document formats including PDF, Word, and Excel. Supports advanced features such as OCR for image content in documents and enhances workflow automation.",
      "stars": 13,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-22T13:50:24Z",
      "readme_content": "# MCP开发框架\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/34780cde-ee17-4a7b-b9ee-356f41fc9e77) [![smithery badge](https://smithery.ai/badge/@aigo666/mcp-framework)](https://smithery.ai/server/@aigo666/mcp-framework)\n\n一个强大的MCP（Model Context Protocol）开发框架，用于创建与大语言模型交互的自定义工具。该框架提供了一套完整的工具集，可以轻松地扩展Cursor IDE的功能，实现网页内容获取、文件处理（PDF、Word、Excel、CSV、Markdown）以及AI对话等高级功能。它具有强大的MCP工具扩展能力，使开发者能够快速构建和集成各种自定义工具。\n\n<a href=\"https://glama.ai/mcp/servers/@aigo666/mcp-framework\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@aigo666/mcp-framework/badge\" />\n</a>\n\n<details>\n<summary>🔥 最新特性：文档图片内容显示与理解</summary>\n\n最新版本现在支持在PDF和Word文档处理中，直接返回原始图片内容并进行OCR识别，使大语言模型能够同时理解文档中的文本和图像内容：\n\n- **图片内容直接显示**：文档中的图表、图像等可以直接在对话中显示，无需额外工具\n- **OCR文本识别**：自动提取图片中的文字内容，支持中英文多语言\n- **图片内容理解**：大模型可以\"看到\"文档中的图片，并基于图片内容进行分析和回答\n- **完整文档内容返回**：真正实现文档的全内容理解，包括文本、表格和图像\n\n这使得AI模型能够更全面地理解和分析文档内容，特别是对于包含图表、表单、流程图或其他可视化信息的文档尤为有价值。\n</details>\n\n## 主要功能\n\n<details>\n<summary>点击展开查看框架提供的核心功能</summary>\n\n本框架提供了以下核心功能：\n\n### 1. 综合文件处理\n\n使用`parse_file`工具可以自动识别文件类型并选择合适的处理方式，支持PDF、Word、Excel、CSV和Markdown文件。\n\n- **用法**: `parse_file /path/to/document`\n- **支持格式**: \n  - PDF文件 (.pdf)\n  - Word文档 (.doc, .docx)\n  - Excel文件 (.xls, .xlsx, .xlsm)\n  - CSV文件 (.csv)\n  - Markdown文件 (.md)\n- **参数**: `file_path` - 文件的本地路径\n- **返回**: 根据文件类型返回相应的处理结果\n\n### 2. PDF文档处理\n\n使用`parse_pdf`工具可以处理PDF文档，支持两种处理模式：\n\n- **用法**: `parse_pdf /path/to/document.pdf [mode]`\n- **参数**: \n  - `file_path` - PDF文件的本地路径\n  - `mode` - 处理模式（可选）：\n    - `quick` - 快速预览模式，仅提取文本内容\n    - `full` - 完整解析模式，提取文本、图片内容和OCR文本（默认）\n- **返回**: \n  - 快速预览模式：文档的文本内容\n  - 完整解析模式：文档的文本内容、原始图片和OCR识别结果\n\n### 3. Word文档解析\n\n使用`parse_word`工具可以解析Word文档，提取文本、表格和图片信息。\n\n- **用法**: `parse_word /path/to/document.docx`\n- **功能**: 解析Word文档并提取文本内容、表格和图片\n- **参数**: `file_path` - Word文档的本地路径\n- **返回**: 文档的文本内容、表格和原始图片\n- **特点**: 同时提供文档内嵌图像的显示和分析功能\n\n### 4. Excel文件处理\n\n使用`parse_excel`工具可以解析Excel文件，提供完整的表格数据和结构信息。\n\n- **用法**: `parse_excel /path/to/spreadsheet.xlsx`\n- **功能**: 解析Excel文件的所有工作表\n- **参数**: `file_path` - Excel文件的本地路径\n- **返回**: \n  - 文件基本信息（文件名、工作表数量）\n  - 每个工作表的详细信息：\n    - 行数和列数\n    - 列名列表\n    - 完整的表格数据\n- **特点**: \n  - 使用pandas和openpyxl提供高质量的表格数据处理\n  - 支持多工作表处理\n  - 自动处理数据类型转换\n\n### 5. CSV文件处理\n\n使用`parse_csv`工具可以解析CSV文件，提供完整的数据分析和预览功能。\n\n- **用法**: `parse_csv /path/to/data.csv`\n- **功能**: 解析CSV文件并提供数据分析\n- **参数**: \n  - `file_path` - CSV文件的本地路径\n  - `encoding` - 文件编码格式（可选，默认自动检测）\n- **返回**: \n  - 文件基本信息（文件名、行数、列数）\n  - 列名列表\n  - 数据预览（前5行）\n  - 描述性统计信息\n- **特点**: \n  - 自动编码检测\n  - 支持多种编码格式（UTF-8、GBK等）\n  - 提供数据统计分析\n  - 智能数据类型处理\n\n### 6. Markdown文件解析\n\n使用`parse_markdown`工具可以解析Markdown文件，提取文本内容、标题结构和列表等信息。\n\n- **用法**: `parse_markdown /path/to/document.md`\n- **功能**: 解析Markdown文件并提取标题结构、列表和文本内容\n- **参数**: `file_path` - Markdown文件的本地路径\n- **返回**: \n  - 文件基本信息（文件名、大小、修改时间等）\n  - 标题结构层级展示\n  - 内容元素统计（代码块、列表、链接、图片、表格等）\n  - 原始Markdown内容\n- **特点**: \n  - 自动识别各级标题和结构\n  - 智能统计内容元素\n  - 完整的标题层级展示\n\n### 7. 网页内容获取\n\n使用`url`工具可以获取任何网页的内容。\n\n- **用法**: `url https://example.com`\n- **参数**: `url` - 要获取内容的网站URL\n- **返回**: 网页的文本内容\n- **特点**: \n  - 完整的HTTP错误处理\n  - 超时管理\n  - 自动编码处理\n\n### 8. MaxKB AI对话\n\n使用`maxkb`工具可以与MaxKB API进行交互，实现智能对话功能。\n\n- **用法**: `maxkb \"您的问题或指令\"`\n- **功能**: 发送消息到MaxKB API并获取AI回复\n- **参数**: \n  - `message` - 要发送的消息内容（必需）\n  - `re_chat` - 是否重新开始对话（可选，默认false）\n  - `stream` - 是否使用流式响应（可选，默认true）\n- **返回**: AI的回复内容\n- **特点**: \n  - 支持流式响应\n  - 自动重试机制\n  - 完整的错误处理\n  - 60秒超时保护\n  - 保持连接配置优化\n\n</details>\n\n## 技术特点\n\n本框架采用了多种技术来优化文件处理性能：\n\n1. **智能文件类型识别**\n   - 自动根据文件扩展名选择合适的处理工具\n   - 提供统一的文件处理接口\n\n2. **高效的文档处理**\n   - PDF处理：支持快速预览和完整解析两种模式\n   - Word处理：精确提取文本、表格和图片\n   - Excel处理：高效处理大型表格数据\n\n3. **强大的MCP工具扩展能力**\n   - 插件化架构设计，易于扩展\n   - 统一的工具注册和调用接口\n   - 支持同步和异步工具开发\n   - 丰富的工具开发API和辅助函数\n\n4. **内存优化**\n   - 使用临时文件管理大型文件\n   - 自动清理临时资源\n   - 分块处理大型文档\n\n5. **错误处理**\n   - 完整的异常捕获和处理\n   - 详细的错误信息反馈\n   - 优雅的失败处理机制\n\n## 项目结构\n\n本框架采用模块化设计，便于扩展和维护：\n\n```\nmcp_tool/\n├── tools/\n│   ├── __init__.py        # 定义工具基类和注册器\n│   ├── loader.py          # 工具加载器，自动加载所有工具\n│   ├── file_tool.py       # 综合文件处理工具\n│   ├── pdf_tool.py        # PDF解析工具\n│   ├── word_tool.py       # Word文档解析工具\n│   ├── excel_tool.py      # Excel文件处理工具\n│   ├── csv_tool.py        # CSV文件处理工具\n│   ├── markdown_tool.py   # Markdown文件解析工具\n│   ├── url_tool.py        # URL工具实现\n│   └── maxkb_tool.py      # MaxKB AI对话工具\n├── __init__.py\n├── __main__.py\n└── server.py              # MCP服务器实现\n```\n\n## 开发指南\n\n### 如何开发新工具\n\n1. 在`tools`目录下创建一个新的Python文件，如`your_tool.py`\n2. 导入必要的依赖和基类\n3. 创建一个继承自`BaseTool`的工具类\n4. 使用`@ToolRegistry.register`装饰器注册工具\n5. 实现工具的`execute`方法\n\n### 工具模板示例\n\n```python\nimport mcp.types as types\nfrom . import BaseTool, ToolRegistry\n\n@ToolRegistry.register\nclass YourTool(BaseTool):\n    \"\"\"您的工具描述\"\"\"\n    name = \"your_tool_name\"  # 工具的唯一标识符\n    description = \"您的工具描述\"  # 工具的描述信息，将显示给用户\n    input_schema = {\n        \"type\": \"object\",\n        \"required\": [\"param1\"],  # 必需的参数\n        \"properties\": {\n            \"param1\": {\n                \"type\": \"string\",\n                \"description\": \"参数1的描述\",\n            },\n            \"param2\": {\n                \"type\": \"integer\",\n                \"description\": \"参数2的描述（可选）\",\n            }\n        },\n    }\n  \n    async def execute(self, arguments: dict) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n        \"\"\"执行工具逻辑\"\"\"\n        # 参数验证\n        if \"param1\" not in arguments:\n            return [types.TextContent(\n                type=\"text\",\n                text=\"Error: Missing required argument 'param1'\"\n            )]\n          \n        # 获取参数\n        param1 = arguments[\"param1\"]\n        param2 = arguments.get(\"param2\", 0)  # 获取可选参数，提供默认值\n      \n        # 执行工具逻辑\n        result = f\"处理参数: {param1}, {param2}\"\n      \n        # 返回结果\n        return [types.TextContent(\n            type=\"text\",\n            text=result\n        )]\n```\n\n## 部署指南\n\n### 环境变量配置\n\n在`.env`文件中配置以下环境变量：\n\n```bash\n# Server Configuration\nMCP_SERVER_PORT=8000        # 服务器端口\nMCP_SERVER_HOST=0.0.0.0     # 服务器主机\n\n# 鉴权配置\nMCP_AUTH_URL=http://170.106.105.206:4000/users  # 鉴权服务地址\n\n# MaxKB配置\nMAXKB_HOST=http://host.docker.internal:8080  # MaxKB API主机地址\nMAXKB_CHAT_ID=your_chat_id_here              # MaxKB聊天ID\nMAXKB_APPLICATION_ID=your_application_id_here # MaxKB应用ID\nMAXKB_AUTHORIZATION=your_authorization_key    # MaxKB授权密钥\n\n# 调试模式\nDEBUG=false                 # 是否启用调试模式\n\n# 用户代理\nMCP_USER_AGENT=\"MCP Test Server (github.com/modelcontextprotocol/python-sdk)\"\n\n# 本地目录挂载配置\nHOST_MOUNT_SOURCE=/path/to/your/local/directory  # 本地目录路径\nHOST_MOUNT_TARGET=/host_files                    # 容器内挂载路径\n```\n\n### 本地目录挂载\n\n框架支持将本地目录挂载到容器中，以便工具可以访问本地文件。配置方法：\n\n1. 在`.env`文件中设置`HOST_MOUNT_SOURCE`和`HOST_MOUNT_TARGET`环境变量\n2. `HOST_MOUNT_SOURCE`是你本地机器上的目录路径\n3. `HOST_MOUNT_TARGET`是容器内的挂载路径（默认为`/host_files`）\n\n使用工具时，可以直接引用本地文件路径，框架会自动将其转换为容器内的路径。例如：\n\n```\n# 使用PDF工具处理本地文件\npdf \"/Users/username/Documents/example.pdf\"\n\n# 框架会自动将路径转换为容器内路径\n# 例如：\"/host_files/example.pdf\"\n```\n\n这样，你就可以在不修改工具代码的情况下，轻松访问本地文件。\n\n### Docker部署（推荐）\n\n1. 初始设置：\n```bash\n# 克隆仓库\ngit clone https://github.com/aigo666/mcp-framework.git\ncd mcp-framework\n\n# 创建环境文件\ncp .env.example .env\n```\n\n2. 使用Docker Compose：\n```bash\n# 构建并启动\ndocker compose up --build -d\n\n# 查看日志\ndocker compose logs -f\n\n# 管理容器\ndocker compose ps\ndocker compose pause\ndocker compose unpause\ndocker compose down\n```\n\n3. 访问服务：\n   - SSE端点: http://localhost:8000/sse\n\n4. Cursor IDE配置：\n- 设置 → 功能 → 添加MCP服务器\n- 类型: \"sse\"\n- URL: `http://localhost:8000/sse?token=<your-token>` (替换 `<your-token>` 为您的 JWT Token)\n\n## 鉴权配置\n\n<details>\n<summary>点击展开查看详细的鉴权配置信息</summary>\n\nSSE 服务现在支持 API 鉴权机制，每个请求都需要携带有效的认证信息：\n\n1. 配置鉴权服务地址：\n   - 在 `.env` 文件中设置 `MCP_AUTH_URL` 环境变量（默认为 `http://170.106.105.206:4000/users` 该鉴权地址仅供测试，不保证长期稳定，建议使用以下项目自行部署）\n\n2. 客户端配置：\n   - 在 Cursor 插件中配置时，需要在 URL 中添加 `token` 查询参数\n   - 格式为 `http://your-server:8000/sse?token=<your-token>`\n   - 服务器会自动将 token 转换为 `Bearer <your-token>` 格式发送到鉴权服务\n\n3. 鉴权流程：\n   - 当 SSE 服务收到请求时，会从 URL 中提取 token 参数\n   - 然后向配置的鉴权地址发送请求，并传递 `Authorization: Bearer <your-token>` 头\n   - 只有鉴权成功（返回 200 状态码）的请求才会被处理\n   - 鉴权失败的请求会收到 401 Unauthorized 响应\n\n4. 推荐JWT鉴权服务：\n   - 我们推荐使用Jason Watmore的Node.js JWT鉴权服务作为参考实现\n   - 详细文档和示例代码：https://jasonwatmore.com/nodejs-jwt-authentication-tutorial-with-example-api\n   - 该实现提供了完整的用户注册、登录、令牌生成和验证功能\n   - 可以无缝集成到本框架的鉴权流程中\n\n</details>\n\n## 部署方式\n\n### 传统Python部署\n\n1. 安装系统依赖：\n```bash\n# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim\n\n# macOS\nbrew install poppler tesseract tesseract-lang\n\n# Windows\n# 1. 下载并安装Tesseract: https://github.com/UB-Mannheim/tesseract/wiki\n# 2. 将Tesseract添加到系统PATH\n```\n\n2. 安装Python依赖：\n```bash\n# 创建虚拟环境\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖\npip install -r requirements.txt\n```\n\n3. 启动服务：\n```bash\npython -m mcp_tool\n```\n\n## 依赖项\n\n主要依赖：\n- `mcp`: Model Context Protocol实现\n- `PyMuPDF`: PDF文档处理\n- `python-docx`: Word文档处理\n- `pandas`和`openpyxl`: Excel文件处理\n- `httpx`: 异步HTTP客户端\n- `anyio`: 异步I/O支持\n- `click`: 命令行接口\n\n## 贡献指南\n\n1. Fork仓库\n2. 创建功能分支 (`git checkout -b feature/amazing-feature`)\n3. 提交更改 (`git commit -m 'Add some amazing feature'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 打开Pull Request\n\n## 许可证\n\n本项目采用MIT许可证 - 详情请参阅[LICENSE](LICENSE)文件。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "framework",
        "aigo666",
        "processing aigo666",
        "mcp framework",
        "aigo666 mcp"
      ],
      "category": "document-processing"
    },
    "aindreyway--mcp-server-neurolora-p": {
      "owner": "aindreyway",
      "name": "mcp-server-neurolora-p",
      "url": "https://github.com/aindreyway/mcp-server-neurolora-p",
      "imageUrl": "/freedevtools/mcp/pfp/aindreyway.webp",
      "description": "Collects and documents code from projects into markdown, providing tools for code analysis and documentation.",
      "stars": 9,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-03T22:20:26Z",
      "readme_content": "# MCP Server Neurolorap\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/aindreyway/mcp-server-neurolorap/actions/workflows/tests.yml/badge.svg)](https://github.com/aindreyway/mcp-server-neurolorap/actions/workflows/tests.yml)\n[![codecov](https://codecov.io/gh/aindreyway/mcp-server-neurolorap/branch/main/graph/badge.svg)](https://codecov.io/gh/aindreyway/mcp-server-neurolorap)\n\nMCP server providing tools for code analysis and documentation.\n\n<a href=\"https://glama.ai/mcp/servers/rg07wseeqe\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/rg07wseeqe/badge\" alt=\"Server Neurolorap MCP server\" /></a>\n\n## Features\n\n### Code Collection Tool\n\n- Collect code from entire project\n- Collect code from specific directories or files\n- Collect code from multiple paths\n- Markdown output with syntax highlighting\n- Table of contents generation\n- Support for multiple programming languages\n\n### Project Structure Reporter Tool\n\n- Analyze project structure and metrics\n- Generate detailed reports in markdown format\n- File size and complexity analysis\n- Tree-based visualization\n- Recommendations for code organization\n- Customizable ignore patterns\n\n## Quick Overview\n\n```sh\n# Using uvx (recommended)\nuvx mcp-server-neurolorap\n\n# Or using pip (not recommended)\npip install mcp-server-neurolorap\n```\n\nYou don't need to install or configure any dependencies manually. The tool will set up everything you need to analyze and document code.\n\n## Installation\n\n**You'll need to have [UV](https://docs.astral.sh/uv/) >= 0.4.10 installed on your machine.**\n\nTo install and run the server:\n\n```sh\n# Install using uvx (recommended)\nuvx mcp-server-neurolorap\n\n# Or install using pip (not recommended)\npip install mcp-server-neurolorap\n```\n\nThis will automatically:\n\n- Install all required dependencies\n- Configure Cline integration\n- Set up the server for immediate use\n\nThe server will be available through the MCP protocol in Cline. You can use it to analyze and document code from any project.\n\n## Usage\n\n### Developer Mode\n\nThe server includes a developer mode with JSON-RPC terminal interface for direct interaction:\n\n```bash\n# Start the server in developer mode\npython -m mcp_server_neurolorap --dev\n```\n\nAvailable commands:\n\n- `help`: Show available commands\n- `list_tools`: List available MCP tools\n- `collect <path>`: Collect code from specified path\n- `report [path]`: Generate project structure report\n- `exit`: Exit developer mode\n\nExample session:\n\n```\n> help\nAvailable commands:\n- help: Show this help message\n- list_tools: List available MCP tools\n- collect <path>: Collect code from specified path\n- report [path]: Generate project structure report\n- exit: Exit the terminal\n\n> list_tools\n[\"code_collector\", \"project_structure_reporter\"]\n\n> collect src\nCode collection complete!\nOutput file: code_collection.md\n\n> report\nProject structure report generated: PROJECT_STRUCTURE_REPORT.md\n\n> exit\nGoodbye!\n```\n\n### Through MCP Tools\n\n#### Code Collection\n\n```python\nfrom modelcontextprotocol import use_mcp_tool\n\n# Collect code from entire project\nresult = use_mcp_tool(\n    \"code_collector\",\n    {\n        \"input\": \".\",\n        \"title\": \"My Project\"\n    }\n)\n\n# Collect code from specific directory\nresult = use_mcp_tool(\n    \"code_collector\",\n    {\n        \"input\": \"./src\",\n        \"title\": \"Source Code\"\n    }\n)\n\n# Collect code from multiple paths\nresult = use_mcp_tool(\n    \"code_collector\",\n    {\n        \"input\": [\"./src\", \"./tests\"],\n        \"title\": \"Project Files\"\n    }\n)\n```\n\n#### Project Structure Analysis\n\n```python\n# Generate project structure report\nresult = use_mcp_tool(\n    \"project_structure_reporter\",\n    {\n        \"output_filename\": \"PROJECT_STRUCTURE_REPORT.md\"\n    }\n)\n\n# Analyze specific directory with custom ignore patterns\nresult = use_mcp_tool(\n    \"project_structure_reporter\",\n    {\n        \"output_filename\": \"src_structure.md\",\n        \"ignore_patterns\": [\"*.pyc\", \"__pycache__\"]\n    }\n)\n```\n\n### File Storage\n\nThe server uses a structured approach to file storage:\n\n1. All generated files are stored in `~/.mcp-docs/<project-name>/`\n2. A `.neurolora` symlink is created in your project root pointing to this directory\n\nThis ensures:\n\n- Clean project structure\n- Consistent file organization\n- Easy access to generated files\n- Support for multiple projects\n- Reliable file synchronization across different OS environments\n- Fast file visibility in IDEs and file explorers\n\n### Customizing Ignore Patterns\n\nCreate a `.neuroloraignore` file in your project root to customize which files are ignored:\n\n```gitignore\n# Dependencies\nnode_modules/\nvenv/\n\n# Build\ndist/\nbuild/\n\n# Cache\n__pycache__/\n*.pyc\n\n# IDE\n.vscode/\n.idea/\n\n# Generated files\n.neurolora/\n```\n\nIf no `.neuroloraignore` file exists, a default one will be created with common ignore patterns.\n\n## Development\n\n1. Clone the repository\n2. Create and activate virtual environment:\n\n```sh\npython -m venv .venv\nsource .venv/bin/activate  # On Unix\n# or\n.venv\\Scripts\\activate  # On Windows\n```\n\n3. Install development dependencies:\n\n```sh\npip install -e \".[dev]\"\n```\n\n4. Run the server:\n\n```sh\n# Normal mode (MCP server with stdio transport)\npython -m mcp_server_neurolorap\n\n# Developer mode (JSON-RPC terminal interface)\npython -m mcp_server_neurolorap --dev\n```\n\n### Testing\n\nThe project maintains high quality standards through automated testing and continuous integration:\n\n- Comprehensive test suite with over 80% code coverage\n- Automated testing on Python 3.10, 3.11, and 3.12\n- Continuous integration through GitHub Actions\n- Regular security scans and dependency checks\n\nFor development and testing details, see PROJECT_SUMMARY.md.\n\n### Code Quality\n\nThe project maintains high code quality standards through various tools:\n\n```sh\n# Format code\nblack .\n\n# Sort imports\nisort .\n\n# Lint code\nflake8 .\n\n# Type check\nmypy src tests\n\n# Security check\nbandit -r src/\nsafety check\n```\n\nAll these checks are run automatically on pull requests through GitHub Actions.\n\n### CI/CD Pipeline\n\nThe project uses GitHub Actions for continuous integration and deployment:\n\n- Runs tests on Python 3.10, 3.11, and 3.12\n- Checks code formatting and style\n- Performs type checking\n- Runs security scans\n- Generates coverage reports\n- Builds and validates package\n- Uploads test artifacts\n\nThe pipeline must pass before merging any changes.\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nMIT License. See LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "documentation",
        "document",
        "projects markdown",
        "documents code",
        "document processing"
      ],
      "category": "document-processing"
    },
    "albertshao--wiki_mcp_server": {
      "owner": "albertshao",
      "name": "wiki_mcp_server",
      "url": "https://github.com/albertshao/wiki_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/albertshao.webp",
      "description": "Manage Confluence wiki pages by creating, updating, deleting, and searching them through a unified interface. Automatically selects the relevant knowledge base based on user queries to enhance content management efficiency.",
      "stars": 2,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T09:28:03Z",
      "readme_content": "# 📚 Wiki MCP Server\n\nAn MCP (Model Context Protocol) Server implementation for managing Confluence wiki pages.\n\nSupports:\n- Creating new wiki pages\n- Updating existing wiki pages\n- Deleting wiki pages\n- Searching wiki pages by keyword\n- Auto-selecting correct Confluence knowledge base (`alm`, `wpb`, etc.) based on user query\n\nBuilt with **FastAPI**, following **MCP Server Best Practices**, and ready for production deployment.\n\n---\n\n## 🚀 Tech Stack\n\n- Python 3.10+\n- FastAPI\n- MCP SDK\n- Requests (for Confluence API interaction)\n- ContextVars (for session management)\n\n---\n\n## 📦 Project Structure\n\n```plaintext\nwiki_mcp_server/\n├── src/wiki_mcp_server/\n│   ├── server.py          # MCP server entry point\n│   ├── service.py         # Business logic (Confluence API interactions)\n│   ├── tools.py           # MCP tool definitions\n│   ├── prompts.py         # MCP prompt definitions\n│   ├── resources.py       # MCP resource definitions\n│   ├── utils.py           # Helper functions (wiki_type inference etc.)\n│   ├── utils/session_context.py  # Session context manager\n│   └── middleware.py      # Authentication and session initialization middleware\n├── Dockerfile             # Container configuration\n├── requirements.txt       # Python dependencies\n├── README.md              # Project documentation\n├── smithery.yaml          # Smithery integration config (optional)\n└── pyproject.toml         # Python project metadata\n```\n\n---\n\n## ⚙️ Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://your-repo-url/wiki_mcp_server.git\ncd wiki_mcp_server\n```\n\n2. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n3. (Optional) Configure your environment variables if needed.\n\n---\n\n## 🛠 Running Locally\n\nRun the server:\n\n```bash\ncd src\nuvicorn wiki_mcp_server.server:app --host 0.0.0.0 --port 9999 --reload\n```\n\nAfter startup, you can visit:\n\n- OpenAPI docs (Swagger UI): [http://localhost:9999/docs](http://localhost:9999/docs)\n- ReDoc docs: [http://localhost:9999/redoc](http://localhost:9999/redoc)\n\n---\n\n## 🧪 Example Request\n\n### Headers Required:\n\n| Key | Example Value |\n|:---|:---|\n| user_name | john.doe@domain.com |\n| alm_confluence_base_url | https://your-confluence-site/wiki/rest/api |\n| alm_confluence_api_token | your-api-token |\n| wpb_confluence_base_url | (optional if available) |\n| wpb_confluence_api_token | (optional if available) |\n\n> ⚠️ If headers are missing or invalid, server will return HTTP 400 error.\n\n---\n\n### Example: Create Page\n\n**POST** `/create_page`\n\n```json\n{\n  \"space_key\": \"TEST\",\n  \"title\": \"Test Page Created by MCP Server\",\n  \"content\": \"<p>Hello, World!</p>\",\n  \"user_query\": \"Please create a page in GSNA knowledge base.\"\n}\n```\n\n**Behavior**:\n- Server will infer `wiki_type=alm` from user_query.\n- Create the page in Confluence and return page metadata.\n\n---\n\n## 🧠 Auto Inference Logic\n\n- If the query mentions `gsna`, `global`, `alm-confluence` → **alm**\n- If the query mentions `wpb`, `wealth` → **wpb**\n- Otherwise default to **alm**\n\n(You can also manually specify `wiki_type` in input)\n\n---\n\n## 🐳 Docker (Optional)\n\nBuild and run containerized server:\n\n```bash\ndocker build -t wiki-mcp-server .\ndocker run -d -p 9999:9999 --name wiki-mcp-server wiki-mcp-server\n```\n\n---\n\n## 📜 License\n\nMIT License.\n\n---\n\n## 📞 Contact\n\nFor issues or collaboration requests, please contact:\n\n- Developer: **Shawn**\n- Email: gsqasxb@gmail.com\n- Project maintained by internal MCP Working Group\n\n---# wiki_mcp_server\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "confluence",
        "wiki_mcp_server",
        "wiki",
        "confluence wiki",
        "manage confluence",
        "wiki pages"
      ],
      "category": "document-processing"
    },
    "alejandroBallesterosC--document-edit-mcp": {
      "owner": "alejandroBallesterosC",
      "name": "document-edit-mcp",
      "url": "https://github.com/alejandroBallesterosC/document-edit-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alejandroBallesterosC.webp",
      "description": "Facilitates document manipulation across Microsoft Word, Excel, and PDF formats, enabling editing, creation, and conversion of various document types seamlessly.",
      "stars": 39,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T04:37:49Z",
      "readme_content": "# Claude Document MCP Server\n\nA Model Context Protocol (MCP) server that allows Claude Desktop to perform document operations on Microsoft Word, Excel, and PDF files.\n\n## Features\n\n### Microsoft Word Operations\n- Create new Word documents from text\n- Edit existing Word documents (add/edit/delete paragraphs and headings)\n- Convert text files (.txt) to Word documents\n\n### Excel Operations\n- Create new Excel spreadsheets from JSON or CSV-like text\n- Edit existing Excel files (update cells, ranges, add/delete rows, columns, sheets)\n- Convert CSV files to Excel\n\n### PDF Operations\n- Create new PDF files from text\n- Convert Word documents to PDF files\n\n## Setup\n\nThis MCP server requires Python 3.10 or higher.\n\n### Automatic Setup (Recommended)\n\nRun the setup script to automatically install dependencies and configure for Claude Desktop:\n\n```bash\ngit clone https://github.com/alejandroBallesterosC/document-edit-mcp\ncd document-edit-mcp\n./setup.sh\n```\n\nThis will:\n1. Create a virtual environment\n2. Install required dependencies\n3. Configure the server for Claude Desktop\n4. Create necessary directories\n\n### Manual Setup\n\nIf you prefer to set up manually:\n\n1. Install dependencies:\n\n```bash\ncd claude-document-mcp\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e .\n```\n\n2. Configure Claude Desktop:\n\nCopy the `claude_desktop_config.json` file to:\n- **Mac**: `~/Library/Application Support/Claude/`\n- **Windows**: `%APPDATA%\\Claude\\`\n\n3. Restart Claude Desktop\n\n## Model Context Protocol Integration\n\nThis server follows the Model Context Protocol specification to provide document manipulation capabilities for Claude Desktop:\n\n- **Tools**: Provides manipulations functions for Word, Excel, and PDF operations\n- **Resources**: Provides information about capabilities\n- **Prompts**: (none currently implemented)\n\n## API Reference\n\n### Microsoft Word\n\n#### Create a Word Document\n```\ncreate_word_document(filepath: str, content: str) -> Dict\n```\n\n#### Edit a Word Document\n```\nedit_word_document(filepath: str, operations: List[Dict]) -> Dict\n```\n\n#### Convert TXT to Word\n```\nconvert_txt_to_word(source_path: str, target_path: str) -> Dict\n```\n\n### Excel\n\n#### Create an Excel File\n```\ncreate_excel_file(filepath: str, content: str) -> Dict\n```\n\n#### Edit an Excel File\n```\nedit_excel_file(filepath: str, operations: List[Dict]) -> Dict\n```\n\n#### Convert CSV to Excel\n```\nconvert_csv_to_excel(source_path: str, target_path: str) -> Dict\n```\n\n### PDF\n\n#### Create a PDF File\n```\ncreate_pdf_file(filepath: str, content: str) -> Dict\n```\n\n#### Convert Word to PDF\n```\nconvert_word_to_pdf(source_path: str, target_path: str) -> Dict\n```\n\n## Logs\n\nThe server logs all operations to both the console and a `logs/document_mcp.log` file for troubleshooting.\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "editing",
        "formats",
        "document manipulation",
        "document edit",
        "facilitates document"
      ],
      "category": "document-processing"
    },
    "alekspetrov--mcp-docs-service": {
      "owner": "alekspetrov",
      "name": "mcp-docs-service",
      "url": "https://github.com/alekspetrov/mcp-docs-service",
      "imageUrl": "/freedevtools/mcp/pfp/alekspetrov.webp",
      "description": "Manage markdown documentation by creating, reading, updating, and deleting files while analyzing their health and improving quality. Enhance AI assistants' interactions with documentation through natural language processing capabilities.",
      "stars": 44,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:32:19Z",
      "readme_content": "# MCP Documentation Service\n\n[![Test Coverage](https://codecov.io/gh/alekspetrov/mcp-docs-service/branch/main/graph/badge.svg)](https://codecov.io/gh/alekspetrov/mcp-docs-service)\n\n<a href=\"https://glama.ai/mcp/servers/icfujodcjd\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/icfujodcjd/badge\" />\n</a>\n\n## What is it?\n\nMCP Documentation Service is a Model Context Protocol (MCP) implementation for documentation management. It provides a set of tools for reading, writing, and managing markdown documentation with frontmatter metadata. The service is designed to work seamlessly with AI assistants like Claude in Cursor or Claude Desktop, making it easy to manage your documentation through natural language interactions.\n\n## Features\n\n- **Read and Write Documents**: Easily read and write markdown documents with frontmatter metadata\n- **Edit Documents**: Make precise line-based edits to documents with diff previews\n- **List and Search**: Find documents by content or metadata\n- **Navigation Generation**: Create navigation structures from your documentation\n- **Health Checks**: Analyze documentation quality and identify issues like missing metadata or broken links\n- **LLM-Optimized Documentation**: Generate consolidated single-document output optimized for large language models\n- **MCP Integration**: Seamless integration with the Model Context Protocol\n- **Frontmatter Support**: Full support for YAML frontmatter in markdown documents\n- **Markdown Compatibility**: Works with standard markdown files\n\n## Quick Start\n\n### Installation\n\nRequires Node to be installed on your machine.\n\n```bash\nnpm install -g mcp-docs-service\n```\n\nOr use directly with npx:\n\n```bash\nnpx mcp-docs-service /path/to/docs\n```\n\n### Cursor Integration\n\nTo use with Cursor, create a `.cursor/mcp.json` file in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-manager\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-docs-service\", \"/path/to/your/docs\"]\n    }\n  }\n}\n```\n\n### Claude Desktop Integration\n\nTo use MCP Docs Service with Claude Desktop:\n\n1. **Install Claude Desktop** - Download the latest version from [Claude's website](https://claude.ai/desktop).\n\n2. **Configure Claude Desktop for MCP**:\n\n   - Open Claude Desktop\n   - Click on the Claude menu and select \"Developer Settings\"\n   - This will create a configuration file at:\n     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n     - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. **Edit the configuration file** to add the MCP Docs Service:\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-manager\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-docs-service\", \"/path/to/your/docs\"]\n    }\n  }\n}\n```\n\nMake sure to replace `/path/to/your/docs` with the absolute path to your documentation directory.\n\n4. **Restart Claude Desktop** completely.\n\n5. **Verify the tool is available** - After restarting, you should see a green dot for docs-manager MCP tool (Cursor Settings > MCP)\n\n6. **Troubleshooting**:\n   - If the server doesn't appear, check the logs at:\n     - macOS: `~/Library/Logs/Claude/mcp*.log`\n     - Windows: `%APPDATA%\\Claude\\logs\\mcp*.log`\n   - Ensure Node.js is installed on your system\n   - Make sure the paths in your configuration are absolute and valid\n\n## Examples\n\n### Using with Claude in Cursor\n\nWhen using Claude in Cursor, you can invoke the tools in two ways:\n\n1. **Using Natural Language** (Recommended):\n   - Simply ask Claude to perform the task in plain English:\n\n```\nCan you search my documentation for anything related to \"getting started\"?\n```\n\n```\nPlease list all the markdown files in my docs directory.\n```\n\n```\nCould you check if there are any issues with my documentation?\n```\n\n2. **Using Direct Tool Syntax**:\n   - For more precise control, you can use the direct tool syntax:\n\n```\n@docs-manager mcp_docs_manager_read_document path=docs/getting-started.md\n```\n\n```\n@docs-manager mcp_docs_manager_list_documents recursive=true\n```\n\n```\n@docs-manager mcp_docs_manager_check_documentation_health\n```\n\n### Using with Claude Desktop\n\nWhen using Claude Desktop, you can invoke the tools in two ways:\n\n1. **Using Natural Language** (Recommended):\n\n```\nCan you read the README.md file for me?\n```\n\n```\nPlease find all documents that mention \"API\" in my documentation.\n```\n\n```\nI'd like you to check the health of our documentation and tell me if there are any issues.\n```\n\n2. **Using the Tool Picker**:\n   - Click the hammer icon in the bottom right corner of the input box\n   - Select \"docs-manager\" from the list of available tools\n   - Choose the specific tool you want to use\n   - Fill in the required parameters and click \"Run\"\n\nClaude will interpret your natural language requests and use the appropriate tool with the correct parameters. You don't need to remember the exact tool names or parameter formats - just describe what you want to do!\n\n### Common Tool Commands\n\nHere are some common commands you can use with the tools:\n\n#### Reading a Document\n\n```\n@docs-manager mcp_docs_manager_read_document path=docs/getting-started.md\n```\n\n#### Writing a Document\n\n```\n@docs-manager mcp_docs_manager_write_document path=docs/new-document.md content=\"---\ntitle: New Document\ndescription: A new document created with MCP Docs Service\n---\n\n# New Document\n\nThis is a new document created with MCP Docs Service.\"\n```\n\n#### Editing a Document\n\n```\n@docs-manager mcp_docs_manager_edit_document path=README.md edits=[{\"oldText\":\"# Documentation\", \"newText\":\"# Project Documentation\"}]\n```\n\n#### Searching Documents\n\n```\n@docs-manager mcp_docs_manager_search_documents query=\"getting started\"\n```\n\n#### Generating Navigation\n\n```\n@docs-manager mcp_docs_manager_generate_navigation\n```\n\n## Contributing\n\nContributions are welcome! Here's how you can contribute:\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/my-feature`\n3. Commit your changes: `git commit -am 'Add my feature'`\n4. Push to the branch: `git push origin feature/my-feature`\n5. Submit a pull request\n\nPlease make sure your code follows the existing style and includes appropriate tests.\n\n## Testing and Coverage\n\nThe MCP Docs Service has comprehensive test coverage to ensure reliability and stability. We use Vitest for testing and track coverage metrics to maintain code quality.\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests with coverage report\nnpm run test:coverage\n```\n\nThe test suite includes:\n\n- Unit tests for utility functions and handlers\n- Integration tests for document flow\n- End-to-end tests for the MCP service\n\nOur tests are designed to be robust and handle potential errors in the implementation, ensuring they pass even if there are issues with the underlying code.\n\n### Coverage Reports\n\nAfter running the coverage command, detailed reports are generated in the `coverage` directory:\n\n- HTML report: `coverage/index.html`\n- JSON report: `coverage/coverage-final.json`\n\nWe maintain high test coverage to ensure the reliability of the service, with a focus on testing critical paths and edge cases.\n\n## Documentation Health\n\nWe use the MCP Docs Service to maintain the health of our own documentation. The health score is based on:\n\n- Completeness of metadata (title, description, etc.)\n- Presence of broken links\n- Orphaned documents (not linked from anywhere)\n- Consistent formatting and style\n\nYou can check the health of your documentation with:\n\n```bash\nnpx mcp-docs-service --health-check /path/to/docs\n```\n\n### Consolidated Documentation for LLMs\n\nMCP Docs Service can generate a consolidated documentation file optimized for large language models. This feature is useful when you want to provide your entire documentation set to an LLM for context:\n\n```bash\n# Generate consolidated documentation with default filename (consolidated-docs.md)\nnpx mcp-docs-service --single-doc /path/to/docs\n\n# Generate with custom output filename\nnpx mcp-docs-service --single-doc --output my-project-context.md /path/to/docs\n\n# Limit the total tokens in the consolidated documentation\nnpx mcp-docs-service --single-doc --max-tokens 100000 /path/to/docs\n```\n\nThe consolidated output includes:\n\n- Project metadata (name, version, description)\n- Table of contents with token counts for each section\n- All documentation organized by section with clear separation\n- Token counting to help stay within LLM context limits\n\n### Resilient by Default\n\nMCP Docs Service is designed to be resilient by default. The service automatically handles incomplete or poorly structured documentation without failing:\n\n- Returns a minimum health score of 80 even with issues\n- Automatically creates missing documentation directories\n- Handles missing documentation directories gracefully\n- Continues processing even when files have errors\n- Provides lenient scoring for metadata completeness and broken links\n\nThis makes the service particularly useful for:\n\n- Legacy projects with minimal documentation\n- Projects in early stages of documentation development\n- When migrating documentation from other formats\n\nThe service will always provide helpful feedback rather than failing, allowing you to incrementally improve your documentation over time.\n\n## Version History\n\n### v0.6.0\n\n- Added LLM-optimized consolidated documentation feature (--single-doc flag)\n- Added token counting for each documentation section\n- Added consolidated document output customization (--output flag)\n- Added maximum token limit configuration (--max-tokens flag)\n\n### v0.5.2\n\n- Enhanced resilience by automatically creating missing documentation directories\n- Improved tolerance mode with a minimum health score of 80\n- Made tolerance mode the default for health checks\n- Updated health check tool description to mention tolerance mode\n\n### v0.5.1\n\n- Added tolerance mode to health checks\n- Fixed issues with test suite reliability\n- Improved error handling in document operations\n\n## Documentation\n\nFor more detailed information, check out our documentation:\n\n- [Getting Started Guide](https://github.com/alekspetrov/mcp-docs-service/blob/main/docs/guides/getting-started.md)\n- [MCP Integration Guide](https://github.com/alekspetrov/mcp-docs-service/blob/main/docs/guides/mcp-integration.md)\n- [MCP Protocol Usage](https://github.com/alekspetrov/mcp-docs-service/blob/main/docs/guides/mcp-protocol-usage.md)\n- [API Reference](https://github.com/alekspetrov/mcp-docs-service/blob/main/docs/api/tools-reference.md)\n- [Examples](https://github.com/alekspetrov/mcp-docs-service/blob/main/docs/examples/basic-usage.md)\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "docs",
        "markdown",
        "docs service",
        "markdown documentation",
        "documentation creating"
      ],
      "category": "document-processing"
    },
    "arabold--docs-mcp-server": {
      "owner": "arabold",
      "name": "docs-mcp-server",
      "url": "https://github.com/arabold/docs-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/arabold.webp",
      "description": "Fetches and indexes documentation for various software libraries, packages, and APIs. Provides powerful search capabilities to enable AI systems to access the latest official documentation from multiple sources.",
      "stars": 661,
      "forks": 76,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:51:57Z",
      "readme_content": "# Docs MCP Server: Your AI's Up-to-Date Documentation Expert\n\nAI coding assistants often struggle with outdated documentation and hallucinations. The **Docs MCP Server** solves this by providing a personal, always-current knowledge base for your AI. It **indexes 3rd party documentation** from various sources (websites, GitHub, npm, PyPI, local files) and offers powerful, version-aware search tools via the Model Context Protocol (MCP).\n\nThis enables your AI agent to access the **latest official documentation**, dramatically improving the quality and reliability of generated code and integration details. It's **free**, **open-source**, runs **locally** for privacy, and integrates seamlessly into your development workflow.\n\n## Why Use the Docs MCP Server?\n\nLLM-assisted coding promises speed and efficiency, but often falls short due to:\n\n- 🌀 **Stale Knowledge:** LLMs train on snapshots of the internet and quickly fall behind new library releases and API changes.\n- 👻 **Code Hallucinations:** AI can invent plausible-looking code that is syntactically correct but functionally wrong or uses non-existent APIs.\n- ❓ **Version Ambiguity:** Generic answers rarely account for the specific version dependencies in your project, leading to subtle bugs.\n- ⏳ **Verification Overhead:** Developers spend valuable time double-checking AI suggestions against official documentation.\n\n**Docs MCP Server solves these problems by:**\n\n- ✅ **Providing Up-to-Date Context:** Fetches and indexes documentation directly from official sources (websites, GitHub, npm, PyPI, local files) on demand.\n- 🎯 **Delivering Version-Specific Answers:** Search queries can target exact library versions, ensuring information matches your project's dependencies.\n- 💡 **Reducing Hallucinations:** Grounds the LLM in real documentation for accurate examples and integration details.\n- ⚡ **Boosting Productivity:** Get trustworthy answers faster, integrated directly into your AI assistant workflow.\n\n## ✨ Key Features\n\n- **Accurate & Version-Aware AI Responses:** Provides up-to-date, version-specific documentation to reduce AI hallucinations and improve code accuracy.\n- **Broad Source Compatibility:** Scrapes documentation from websites, GitHub repos, package manager sites (npm, PyPI), and local file directories.\n- **Advanced Search & Processing:** Intelligently chunks documentation semantically, generates embeddings, and combines vector similarity with full-text search.\n- **Flexible Embedding Models:** Supports various providers including OpenAI (and compatible APIs), Google Gemini/Vertex AI, Azure OpenAI, and AWS Bedrock. Vector search is optional.\n- **Enterprise Authentication:** Optional OAuth2/OIDC authentication with dynamic client registration for secure deployments.\n- **Web Interface:** Easy-to-use web interface for searching and managing documentation.\n- **Local & Private:** Runs entirely on your machine, ensuring data and queries remain private.\n- **Free & Open Source:** Community-driven and freely available.\n- **Simple Deployment:** Easy setup via Docker or `npx`.\n- **Seamless Integration:** Works with MCP-compatible clients (like Claude, Cline, Roo).\n\n> **What is semantic chunking?**\n>\n> Semantic chunking splits documentation into meaningful sections based on structure—like headings, code blocks, and tables—rather than arbitrary text size. Docs MCP Server preserves logical boundaries, keeps code and tables intact, and removes navigation clutter from HTML docs. This ensures LLMs receive coherent, context-rich information for more accurate and relevant answers.\n\n## How to Run the Docs MCP Server\n\nChoose your deployment method:\n\n- [Standalone Server (Recommended)](#standalone-server-recommended)\n- [Embedded Server](#embedded-server)\n- [Advanced: Docker Compose (Scaling)](#advanced-docker-compose-scaling)\n\n## Standalone Server (Recommended)\n\nRun a standalone server that includes both MCP endpoints and web interface in a single process. This is the easiest way to get started.\n\n### Option 1: Docker\n\n1. **Install Docker.**\n2. **Start the server:**\n\n   ```bash\n   docker run --rm \\\n     -v docs-mcp-data:/data \\\n     -p 6280:6280 \\\n     ghcr.io/arabold/docs-mcp-server:latest \\\n     --protocol http --host 0.0.0.0 --port 6280\n   ```\n\n   **Optional:** Add `-e OPENAI_API_KEY=\"your-openai-api-key\"` to enable vector search for improved results.\n\n### Option 2: npx\n\n1. **Install Node.js 22.x or later.**\n2. **Start the server:**\n\n   ```bash\n   npx @arabold/docs-mcp-server@latest\n   ```\n\n   This will run the server on port 6280 by default.\n\n   **Optional:** Prefix with `OPENAI_API_KEY=\"your-openai-api-key\"` to enable vector search for improved results.\n\n### Configure Your MCP Client\n\nAdd this to your MCP settings (VS Code, Claude Desktop, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-mcp-server\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:6280/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n**Alternative connection types:**\n\n```jsonc\n// SSE (Server-Sent Events)\n\"type\": \"sse\", \"url\": \"http://localhost:6280/sse\"\n\n// HTTP (Streamable)\n\"type\": \"http\", \"url\": \"http://localhost:6280/mcp\"\n```\n\nRestart your AI assistant after updating the config.\n\n### Access the Web Interface\n\nOpen `http://localhost:6280` in your browser to manage documentation and monitor jobs.\n\n### CLI Usage with Standalone Server\n\nYou can also use CLI commands to interact with the local database:\n\n```bash\n# List indexed libraries\nOPENAI_API_KEY=\"your-key\" npx @arabold/docs-mcp-server@latest list\n\n# Search documentation\nOPENAI_API_KEY=\"your-key\" npx @arabold/docs-mcp-server@latest search react \"useState hook\"\n\n# Scrape new documentation (connects to running server's worker)\nnpx @arabold/docs-mcp-server@latest scrape react https://react.dev/reference/react --server-url http://localhost:6280/api\n```\n\n### Adding Library Documentation\n\n1. Open the Web Interface at `http://localhost:6280`.\n2. Use the \"Queue New Scrape Job\" form.\n3. Enter the documentation URL, library name, and (optionally) version.\n4. Click \"Queue Job\". Monitor progress in the Job Queue.\n5. Repeat for each library you want indexed.\n\nOnce a job completes, the docs are searchable via your AI assistant or the Web UI.\n\n![Docs MCP Server Web Interface](docs/docs-mcp-server.png)\n\n**Benefits:**\n\n- Single command setup with both web UI and MCP server\n- Persistent data storage (Docker volume or local directory)\n- No repository cloning required\n- Full feature access including web interface\n\nTo stop the server, press `Ctrl+C`.\n\n## Embedded Server\n\nRun the MCP server directly embedded in your AI assistant without a separate process or web interface. This method provides MCP integration only.\n\n### Configure Your MCP Client\n\nAdd this to your MCP settings (VS Code, Claude Desktop, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"@arabold/docs-mcp-server@latest\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n**Optional:** To enable vector search for improved results, add an `env` section with your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"@arabold/docs-mcp-server@latest\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"sk-proj-...\" // Your OpenAI API key\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nRestart your application after updating the config.\n\n### Adding Library Documentation\n\n**Option 1: Use MCP Tools**\n\nYour AI assistant can index new documentation using the built-in `scrape_docs` tool:\n\n```\nPlease scrape the React documentation from https://react.dev/reference/react for library \"react\" version \"18.x\"\n```\n\n**Option 2: Launch Web Interface**\n\nStart a temporary web interface that shares the same database:\n\n```bash\nOPENAI_API_KEY=\"your-key\" npx @arabold/docs-mcp-server@latest web --port 6281\n```\n\nThen open `http://localhost:6281` to manage documentation. Stop the web interface when done (`Ctrl+C`).\n\n**Option 3: CLI Commands**\n\nUse CLI commands directly (avoid running scrape jobs concurrently with embedded server):\n\n```bash\n# List libraries\nOPENAI_API_KEY=\"your-key\" npx @arabold/docs-mcp-server@latest list\n\n# Search documentation\nOPENAI_API_KEY=\"your-key\" npx @arabold/docs-mcp-server@latest search react \"useState hook\"\n```\n\n**Benefits:**\n\n- Direct integration with AI assistant\n- No separate server process required\n- Persistent data storage in user's home directory\n- Shared database with standalone server and CLI\n\n**Limitations:**\n\n- No web interface (unless launched separately)\n- Documentation indexing requires MCP tools or separate commands\n\n## Scraping Local Files and Folders\n\nYou can index documentation from your local filesystem by using a `file://` URL as the source. This works in both the Web UI and CLI.\n\n**Examples:**\n\n- Web: `https://react.dev/reference/react`\n- Local file: `file:///Users/me/docs/index.html`\n- Local folder: `file:///Users/me/docs/my-library`\n\n**Requirements:**\n\n- All files with a MIME type of `text/*` are processed. This includes HTML, Markdown, plain text, and source code files such as `.js`, `.ts`, `.tsx`, `.css`, etc. Binary files, PDFs, images, and other non-text formats are ignored.\n- You must use the `file://` prefix for local files/folders.\n- The path must be accessible to the server process.\n- **If running in Docker:**\n  - You must mount the local folder into the container and use the container path in your `file://` URL.\n  - Example Docker run:\n    ```bash\n    docker run --rm \\\n      -e OPENAI_API_KEY=\"your-key\" \\\n      -v /absolute/path/to/docs:/docs:ro \\\n      -v docs-mcp-data:/data \\\n      -p 6280:6280 \\\n      ghcr.io/arabold/docs-mcp-server:latest \\\n      scrape mylib file:///docs/my-library\n    ```\n  - In the Web UI, enter the path as `file:///docs/my-library` (matching the container path).\n\nSee the tooltips in the Web UI and CLI help for more details.\n\n## Advanced: Docker Compose (Scaling)\n\nFor production deployments or when you need to scale processing, use Docker Compose to run separate services. The system selects either a local in-process worker or a remote worker client based on the configuration, ensuring consistent behavior across modes.\n\n**Start the services:**\n\n```bash\n# Clone the repository (to get docker-compose.yml)\ngit clone https://github.com/arabold/docs-mcp-server.git\ncd docs-mcp-server\n\n# Set your environment variables\nexport OPENAI_API_KEY=\"your-key-here\"\n\n# Start all services\ndocker compose up -d\n```\n\n**Service architecture:**\n\n- **Worker** (port 8080): Handles documentation processing jobs\n- **MCP Server** (port 6280): Provides `/sse` endpoint for AI tools\n- **Web Interface** (port 6281): Browser-based management interface\n\n**Configure your MCP client:**\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-mcp-server\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:6280/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n**Alternative connection types:**\n\n```json\n// SSE (Server-Sent Events)\n\"type\": \"sse\", \"url\": \"http://localhost:6280/sse\"\n\n// HTTP (Streamable)\n\"type\": \"http\", \"url\": \"http://localhost:6280/mcp\"\n```\n\n**Access interfaces:**\n\n- Web Interface: `http://localhost:6281`\n- MCP Endpoint (HTTP): `http://localhost:6280/mcp`\n- MCP Endpoint (SSE): `http://localhost:6280/sse`\n\nThis architecture allows independent scaling of processing (workers) and user interfaces.\n\n## Configuration\n\nThe Docs MCP Server can run without any configuration and will use full-text search only. To enable vector search for improved results, configure an embedding provider via environment variables.\n\n### Command Line Argument Overrides\n\nMany CLI arguments can be overridden using environment variables. This is useful for Docker deployments, CI/CD pipelines, or setting default values.\n\n| Environment Variable       | CLI Argument           | Description                                     | Used by Commands          |\n| -------------------------- | ---------------------- | ----------------------------------------------- | ------------------------- |\n| `DOCS_MCP_STORE_PATH`      | `--store-path`         | Custom path for data storage directory          | all                       |\n| `DOCS_MCP_TELEMETRY`       | `--no-telemetry`       | Disable telemetry (`false` to disable)          | all                       |\n| `DOCS_MCP_PROTOCOL`        | `--protocol`           | MCP server protocol (auto, stdio, http)         | default, mcp              |\n| `DOCS_MCP_PORT`            | `--port`               | Server port                                     | default, mcp, web, worker |\n| `DOCS_MCP_WEB_PORT`        | `--port` (web command) | Web interface port (web command only)           | web                       |\n| `PORT`                     | `--port`               | Server port (fallback if DOCS_MCP_PORT not set) | default, mcp, web, worker |\n| `DOCS_MCP_HOST`            | `--host`               | Server host/bind address                        | default, mcp, web, worker |\n| `HOST`                     | `--host`               | Server host (fallback if DOCS_MCP_HOST not set) | default, mcp, web, worker |\n| `DOCS_MCP_EMBEDDING_MODEL` | `--embedding-model`    | Embedding model configuration                   | default, mcp, web, worker |\n| `DOCS_MCP_AUTH_ENABLED`    | `--auth-enabled`       | Enable OAuth2/OIDC authentication               | default, mcp              |\n| `DOCS_MCP_AUTH_ISSUER_URL` | `--auth-issuer-url`    | OAuth2 provider issuer/discovery URL            | default, mcp              |\n| `DOCS_MCP_AUTH_AUDIENCE`   | `--auth-audience`      | JWT audience claim (resource identifier)        | default, mcp              |\n\n**Usage Examples:**\n\n```bash\n# Set via environment variables\nexport DOCS_MCP_PORT=8080\nexport DOCS_MCP_HOST=0.0.0.0\nexport DOCS_MCP_EMBEDDING_MODEL=text-embedding-3-small\nnpx @arabold/docs-mcp-server@latest\n\n# Override with CLI arguments (takes precedence)\nDOCS_MCP_PORT=8080 npx @arabold/docs-mcp-server@latest --port 9090\n```\n\n### Embedding Provider Configuration\n\nThe Docs MCP Server is configured via environment variables. Set these in your shell, Docker, or MCP client config.\n\n| Variable                           | Description                                           |\n| ---------------------------------- | ----------------------------------------------------- |\n| `DOCS_MCP_EMBEDDING_MODEL`         | Embedding model to use (see below for options).       |\n| `OPENAI_API_KEY`                   | OpenAI API key for embeddings.                        |\n| `OPENAI_API_BASE`                  | Custom OpenAI-compatible API endpoint (e.g., Ollama). |\n| `GOOGLE_API_KEY`                   | Google API key for Gemini embeddings.                 |\n| `GOOGLE_APPLICATION_CREDENTIALS`   | Path to Google service account JSON for Vertex AI.    |\n| `AWS_ACCESS_KEY_ID`                | AWS key for Bedrock embeddings.                       |\n| `AWS_SECRET_ACCESS_KEY`            | AWS secret for Bedrock embeddings.                    |\n| `AWS_REGION`                       | AWS region for Bedrock.                               |\n| `AZURE_OPENAI_API_KEY`             | Azure OpenAI API key.                                 |\n| `AZURE_OPENAI_API_INSTANCE_NAME`   | Azure OpenAI instance name.                           |\n| `AZURE_OPENAI_API_DEPLOYMENT_NAME` | Azure OpenAI deployment name.                         |\n| `AZURE_OPENAI_API_VERSION`         | Azure OpenAI API version.                             |\n\nSee [examples above](#alternative-using-docker) for usage.\n\n### Embedding Model Options\n\nSet `DOCS_MCP_EMBEDDING_MODEL` to one of:\n\n- `text-embedding-3-small` (default, OpenAI)\n- `openai:snowflake-arctic-embed2` (OpenAI-compatible, Ollama)\n- `vertex:text-embedding-004` (Google Vertex AI)\n- `gemini:embedding-001` (Google Gemini)\n- `aws:amazon.titan-embed-text-v1` (AWS Bedrock)\n- `microsoft:text-embedding-ada-002` (Azure OpenAI)\n- Or any OpenAI-compatible model name\n\n### Provider-Specific Configuration Examples\n\nHere are complete configuration examples for different embedding providers:\n\n**OpenAI (Default):**\n\n```bash\nOPENAI_API_KEY=\"sk-proj-your-openai-api-key\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"text-embedding-3-small\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**Ollama (Local):**\n\n```bash\nOPENAI_API_KEY=\"ollama\" \\\nOPENAI_API_BASE=\"http://localhost:11434/v1\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"nomic-embed-text\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**LM Studio (Local):**\n\n```bash\nOPENAI_API_KEY=\"lmstudio\" \\\nOPENAI_API_BASE=\"http://localhost:1234/v1\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"text-embedding-qwen3-embedding-4b\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**Google Gemini:**\n\n```bash\nGOOGLE_API_KEY=\"your-google-api-key\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"gemini:embedding-001\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**Google Vertex AI:**\n\n```bash\nGOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/gcp-service-account.json\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"vertex:text-embedding-004\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**AWS Bedrock:**\n\n```bash\nAWS_ACCESS_KEY_ID=\"your-aws-access-key-id\" \\\nAWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\" \\\nAWS_REGION=\"us-east-1\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"aws:amazon.titan-embed-text-v1\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\n**Azure OpenAI:**\n\n```bash\nAZURE_OPENAI_API_KEY=\"your-azure-openai-api-key\" \\\nAZURE_OPENAI_API_INSTANCE_NAME=\"your-instance-name\" \\\nAZURE_OPENAI_API_DEPLOYMENT_NAME=\"your-deployment-name\" \\\nAZURE_OPENAI_API_VERSION=\"2024-02-01\" \\\nDOCS_MCP_EMBEDDING_MODEL=\"microsoft:text-embedding-ada-002\" \\\nnpx @arabold/docs-mcp-server@latest\n```\n\nFor more architectural details, see the [ARCHITECTURE.md](ARCHITECTURE.md).\n\nFor enterprise authentication and security features, see the [Authentication Guide](docs/authentication.md).\n\n## Telemetry\n\nThe Docs MCP Server includes privacy-first telemetry to help improve the product. We collect anonymous usage data to understand how the tool is used and identify areas for improvement.\n\n### What We Collect\n\n- Command usage patterns and success rates\n- Tool execution metrics (counts, durations, error types)\n- Pipeline job statistics (progress, completion rates)\n- Service configuration patterns (auth enabled, read-only mode)\n- Performance metrics (response times, processing efficiency)\n- Protocol usage (stdio vs HTTP, transport modes)\n\n### What We DON'T Collect\n\n- Search query content or user input\n- URLs being scraped or accessed\n- Document content or scraped data\n- Authentication tokens or credentials\n- Personal information or identifying data\n\n### Disabling Telemetry\n\nYou can disable telemetry collection entirely:\n\n**Option 1: CLI Flag**\n\n```bash\nnpx @arabold/docs-mcp-server@latest --no-telemetry\n```\n\n**Option 2: Environment Variable**\n\n```bash\nDOCS_MCP_TELEMETRY=false npx @arabold/docs-mcp-server@latest\n```\n\n**Option 3: Docker**\n\n```bash\ndocker run \\\n  -e DOCS_MCP_TELEMETRY=false \\\n  -v docs-mcp-data:/data \\\n  -p 6280:6280 \\\n  ghcr.io/arabold/docs-mcp-server:latest\n```\n\nFor more details about our telemetry practices, see the [Telemetry Guide](docs/telemetry.md).\n\n## Development\n\nTo develop or contribute to the Docs MCP Server:\n\n- Fork the repository and create a feature branch.\n- Follow the code conventions in [ARCHITECTURE.md](ARCHITECTURE.md).\n- Write clear commit messages (see Git guidelines above).\n- Open a pull request with a clear description of your changes.\n\nFor questions or suggestions, open an issue.\n\n### Architecture\n\nFor details on the project's architecture and design principles, please see [ARCHITECTURE.md](ARCHITECTURE.md).\n\n_Notably, the vast majority of this project's code was generated by the AI assistant Cline, leveraging the capabilities of this very MCP server._\n\n## License\n\nThis project is licensed under the MIT License. See [LICENSE](LICENSE) for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "search",
        "document processing",
        "arabold docs",
        "indexes documentation"
      ],
      "category": "document-processing"
    },
    "arre-ankit--notion-mcp-server": {
      "owner": "arre-ankit",
      "name": "notion-mcp-server",
      "url": "https://github.com/arre-ankit/notion-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/arre-ankit.webp",
      "description": "Query and manipulate Notion Pages by creating, reading, and updating content directly from prompts. Seamlessly manage Notion databases and enhance productivity through integration.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2024-12-06T06:41:48Z",
      "readme_content": "### Notion MCP Server\n\nThis is a simple MCP server that allows you to query Notion Pages.\n\n### Installation\n\n```bash\ngit clone https://github.com/arre-ankit/notion-mcp-server.git\ncd notion-mcp-server\n```\n\n```bash\nnpm install\n```\n\n### Running the server\n\n```bash\nnpm run build\n```\n\n### Add Notion Integration\n- Go to https://www.notion.so/my-integrations\n- Click on \"New integration\"\n- Name it \"Claude MCP Server\"\n- Select \"Read\" and \"Write\" permissions for \"Pages\"\n- Copy the \"Integration Token\"\n\n### Add Claude Integration\nclaude_desktop_config.json\n```bash\n{\n  \"mcpServers\": \n    \"notion-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"Copy Path\"\n      ],\n      \"env\": {\n        \"NOTION_API_TOKEN\": \"Your Notion Token\"\n      }\n    }\n  }\n}\n```\n\n## How to use\n- Write a prompt to query Notion Pages.\n- Add link to the Notion Page from the link in the prompt.\n- Eg: https://www.notion.so/154916e48026802f97d4df6086787817\n\n\n![alt text](image-2.png)\n\nPrompt: Make a new Databse entry in notion with this \nof list of movies to watch in 2024\nPage link https://www.notion.so/154916e48026802f97d4df6086787817 \nAdd the movie datase in this page\n\n![alt text](image-1.png)\n\n\n![alt text](https://github.com/user-attachments/assets/64414f72-2965-4cf1-8e56-c231b88771a2)\n\n🤯 Woah it will be updated in your Notion Page!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "notion",
        "databases",
        "document",
        "notion pages",
        "notion databases",
        "manage notion"
      ],
      "category": "document-processing"
    },
    "askjohngeorge--mcp-doc-scraper": {
      "owner": "askjohngeorge",
      "name": "mcp-doc-scraper",
      "url": "https://github.com/askjohngeorge/mcp-doc-scraper",
      "imageUrl": "/freedevtools/mcp/pfp/askjohngeorge.webp",
      "description": "Scrapes documentation from web URLs and converts it into markdown format, saving the converted documentation to a specified output path. Integrates with the Model Context Protocol (MCP) for enhanced data management.",
      "stars": 7,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-30T20:44:57Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/askjohngeorge-mcp-doc-scraper-badge.png)](https://mseep.ai/app/askjohngeorge-mcp-doc-scraper)\n\n# Doc Scraper MCP Server\n[![smithery badge](https://smithery.ai/badge/@askjohngeorge/mcp-doc-scraper)](https://smithery.ai/server/@askjohngeorge/mcp-doc-scraper)\n\nA Model Context Protocol (MCP) server that provides documentation scraping functionality. This server converts web-based documentation into markdown format using jina.ai's conversion service.\n\n## Features\n\n- Scrapes documentation from any web URL\n- Converts HTML documentation to markdown format\n- Saves the converted documentation to a specified output path\n- Integrates with the Model Context Protocol (MCP)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Doc Scraper for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@askjohngeorge/mcp-doc-scraper):\n\n```bash\nnpx -y @smithery/cli install @askjohngeorge/mcp-doc-scraper --client claude\n```\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/askjohngeorge/mcp-doc-scraper.git\ncd mcp-doc-scraper\n```\n\n2. Create and activate a virtual environment:\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows, use: venv\\Scripts\\activate\n```\n\n3. Install the dependencies:\n\n```bash\npip install -e .\n```\n\n## Usage\n\nThe server can be run using Python:\n\n```bash\npython -m mcp_doc_scraper\n```\n\n### Tool Description\n\nThe server provides a single tool:\n\n- **Name**: `scrape_docs`\n- **Description**: Scrape documentation from a URL and save as markdown\n- **Input Parameters**:\n  - `url`: The URL of the documentation to scrape\n  - `output_path`: The path where the markdown file should be saved\n\n## Project Structure\n\n```\ndoc_scraper/\n├── __init__.py\n├── __main__.py\n└── server.py\n```\n\n## Dependencies\n\n- aiohttp\n- mcp\n- pydantic\n\n## Development\n\nTo set up the development environment:\n\n1. Install development dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n2. The server uses the Model Context Protocol. Make sure to familiarize yourself with [MCP documentation](https://modelcontextprotocol.io/).\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "markdown",
        "scraper",
        "scrapes documentation",
        "doc scraper",
        "mcp doc"
      ],
      "category": "document-processing"
    },
    "askme765cs--open-docs-mcp": {
      "owner": "askme765cs",
      "name": "open-docs-mcp",
      "url": "https://github.com/askme765cs/open-docs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/askme765cs.webp",
      "description": "Crawl, index, and manage documentation while enabling full-text search across various document formats for efficient information retrieval. Integrates with AI to enhance document access and management capabilities.",
      "stars": 11,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T16:24:01Z",
      "readme_content": "# open-docs-mcp MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@askme765cs/open-docs-mcp)](https://smithery.ai/server/@askme765cs/open-docs-mcp)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Node Version](https://img.shields.io/badge/node-%3E%3D16.0.0-brightgreen.svg)](package.json)\n[![TypeScript](https://img.shields.io/badge/TypeScript-4.9.5-blue.svg)](package.json)\n\nAn open-source MCP implementation providing document management functionality.\n[中文文档][url-doczh]\n\n## Features\n\n### Document Management\n- Crawl and index documentation from various sources\n- Support for multiple document formats\n- Full-text search capabilities\n\n### MCP Server API\n- Resource-based access to documents\n- Tool-based document management\n\n### Available Tools\n1. **enable_doc** - Enable crawling for a specific doc\n2. **disable_doc** - Disable crawling for a specific doc\n3. **crawl_docs** - Start crawling enabled docs\n4. **build_index** - Build search index for docs\n5. **search_docs** - Search documentation\n6. **list_enabled_docs** - List enabled docs\n7. **list_all_docs** - List all available docs\n\n### Cursor @Docs Compatibility\n\nThis project aims to replicate Cursor's @Docs functionality by providing:\n\n1. **Document Indexing**:\n   - Crawl and index documentation from various sources\n   - Support for multiple document formats (HTML, Markdown, etc.)\n   - Automatic re-indexing to keep docs up-to-date\n\n2. **Document Access**:\n   - Search across all indexed documentation\n   - Integration with MCP protocol for AI context\n\n3. **Custom Docs Management**:\n   - Add new documentation sources via `enable_doc` tool\n   - Manage enabled docs via `list_enabled_docs` tool\n   - Force re-crawl with `crawl_docs` tool\n\n### Architecture\n```\n┌───────────────────────────────────────────────────────┐\n│                    open-docs-mcp Server                    │\n├───────────────────┬───────────────────┬───────────────┤\n│   Crawler Module  │  Search Engine    │  MCP Server   │\n├───────────────────┼───────────────────┼───────────────┤\n│ - Web crawling    │ - Full-text index │ - Resources   │\n│ - Doc conversion  │ - Relevance score │ - Tools       │\n│ - Storage         │ - Query parsing   │ - Prompts     │\n└───────────────────┴───────────────────┴───────────────┘\n```\n\n## Usage\n\n```bash\nnpx -y open-docs-mcp --docsDir ./docs\n```\n\n### Installing via Smithery\n\nTo install Document Management Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@askme765cs/open-docs-mcp):\n\n```bash\nnpx -y @smithery/cli install @askme765cs/open-docs-mcp --client claude\n```\n\n### Configuration\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"open-docs-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"open-docs-mcp\",\n        \"--docsDir\",\n        \"/path/to/docs\"\n      ]\n    }\n  }\n}\n```\n\n**Configuration Options:**\n- `command`: Node.js executable\n- `args`: Array of arguments to pass to the script\n  - `--docsDir`: Required, specifies docs directory path\n- `disabled`: Set to true to temporarily disable the server\n- `alwaysAllow`: Array of tool names that can be used without confirmation\n\n## Development\n\n```bash\nnpm run watch  # Auto-rebuild on changes\nnpm run inspector  # Debug with MCP Inspector\n```\n\n## Contributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n[MIT](LICENSE)\n\n[url-doczh]: README.zh-CN.md\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "retrieval",
        "search",
        "document processing",
        "text search",
        "open docs"
      ],
      "category": "document-processing"
    },
    "berlinbra--binary-reader-mcp": {
      "owner": "berlinbra",
      "name": "binary-reader-mcp",
      "url": "https://github.com/berlinbra/binary-reader-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/berlinbra.webp",
      "description": "Read and analyze binary files, extracting metadata and structure from various binary formats, including Unreal Engine assets. The server features an extensible architecture for adding support for new binary formats as needed.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-01-31T02:33:09Z",
      "readme_content": "# Binary Reader MCP\n\nA Model Context Protocol server for reading and analyzing binary files. This server provides tools for reading and analyzing various binary file formats, with initial support for Unreal Engine asset files (.uasset).\n\n## Features\n\n- Read and analyze Unreal Engine .uasset files\n- Extract binary file metadata and structure\n- Auto-detect file formats\n- Extensible architecture for adding new binary format support\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/berlinbra/binary-reader-mcp.git\ncd binary-reader-mcp\n```\n\n2. Create a virtual environment and activate it:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nThe server provides several tools through the Model Context Protocol:\n\n### 1. Read Unreal Asset Files\n\n```python\n# Example usage through MCP\ntool: read-unreal-asset\narguments:\n    file_path: \"path/to/your/asset.uasset\"\n```\n\n### 2. Read Generic Binary Files\n\n```python\n# Example usage through MCP\ntool: read-binary-metadata\narguments:\n    file_path: \"path/to/your/file.bin\"\n    format: \"auto\"  # or \"unreal\", \"custom\"\n```\n\n## Development\n\n### Project Structure\n\n```\nbinary-reader-mcp/\n├── README.md\n├── requirements.txt\n├── main.py\n├── src/\n│   ├── __init__.py\n│   ├── binary_reader/\n│   │   ├── __init__.py\n│   │   ├── base_reader.py\n│   │   ├── unreal_reader.py\n│   │   └── utils.py\n│   ├── api/\n│   │   ├── __init__.py\n│   │   ├── routes.py\n│   │   └── schemas.py\n│   └── config.py\n└── tests/\n    ├── __init__.py\n    ├── test_binary_reader.py\n    └── test_api.py\n```\n\n### Adding New Binary Format Support\n\nTo add support for a new binary format:\n\n1. Create a new reader class that inherits from `BinaryReader`\n2. Implement the required methods (`read_header`, `read_metadata`)\n3. Add the new format to the format auto-detection logic\n4. Update the tools list to include the new format\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "binary",
        "formats",
        "files",
        "berlinbra binary",
        "binary files",
        "binary formats"
      ],
      "category": "document-processing"
    },
    "bettehub--laas-rag-mcp": {
      "owner": "bettehub",
      "name": "laas-rag-mcp",
      "url": "https://github.com/bettehub/laas-rag-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bettehub.webp",
      "description": "Upload documents in PDF or CSV formats and perform natural language queries to retrieve relevant information. It features document segmentation and embedding storage using a Chroma vector store for efficient retrieval.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-09T08:44:26Z",
      "readme_content": "# RAG API\n\n이 프로젝트는 문서 기반 질의응답 시스템을 구현한 FastAPI 기반의 API 서버입니다.\n\n## 기능\n\n1. 문서 업로드 및 벡터 스토어 저장\n\n   - PDF 및 CSV 파일 지원\n   - 문서 자동 분할 및 임베딩\n   - Chroma 벡터 스토어에 저장\n\n2. 문서 검색\n   - 자연어 쿼리 기반 검색\n   - 유사도 기반 문서 검색\n\n## 설치 방법\n\n1. 필요한 패키지 설치:\n\n```bash\npip install -r requirements.txt\n```\n\n2. 환경 변수 설정:\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\n## 실행 방법\n\n```bash\npython main.py\n```\n\n서버가 http://localhost:8000 에서 실행됩니다.\n\n## API 엔드포인트\n\n1. 문서 업로드\n\n   - POST /upload\n   - multipart/form-data 형식으로 파일 업로드\n   - 지원 형식: PDF, CSV\n   - 파라미터:\n     - `files`: 업로드할 파일 목록 (필수)\n     - `vector_store_dir`: 벡터 스토어를 저장할 디렉토리 경로 (선택, 기본값: \"vector_store\")\n\n2. 문서 검색\n   - POST /query\n   - form-data 형식으로 파라미터 전달\n   - 파라미터:\n     - `query`: 검색 쿼리 (필수)\n     - `vector_store_dir`: 벡터 스토어 디렉토리 경로 (선택, 기본값: \"vector_store\")\n     - `k`: 검색할 문서 수 (선택, 기본값: 2)\n\n## API 문서\n\nFastAPI의 자동 생성 문서는 다음 URL에서 확인할 수 있습니다:\n\n- http://localhost:8000/docs\n- http://localhost:8000/redoc\n\n## 벡터 스토어 이전\n\n벡터 스토어에 저장된 파일을 다른 프로젝트에서 재사용하려면 다음과 같이 하면 됩니다:\n\n1. 원하는 벡터 스토어 디렉토리를 다른 프로젝트의 동일한 경로로 복사합니다.\n2. 다른 프로젝트에서도 다음 패키지들이 설치되어 있어야 합니다:\n   - langchain-chroma\n   - langchain-openai\n   - 기타 필요한 의존성 패키지들\n3. 동일한 임베딩 모델(OpenAIEmbeddings)을 사용해야 합니다.\n4. 필요한 환경 변수(예: OpenAI API 키)가 올바르게 설정되어 있어야 합니다.\n\n벡터 스토어를 다른 프로젝트로 이전할 때는 단순히 해당 디렉토리를 복사하는 것만으로도 충분합니다. 이렇게 하면 문서의 임베딩과 메타데이터가 모두 보존되어 새로운 프로젝트에서도 동일하게 사용할 수 있습니다.\n\n## 여러 벡터 스토어 사용하기\n\n이 프로젝트는 여러 개의 벡터 스토어를 동시에 사용할 수 있도록 설계되었습니다. 각 벡터 스토어는 서로 다른 디렉토리에 저장되며, API 호출 시 `vector_store_dir` 파라미터를 통해 원하는 벡터 스토어를 지정할 수 있습니다.\n\n예를 들어, 서로 다른 프로젝트나 문서 세트에 대해 별도의 벡터 스토어를 만들고 관리할 수 있습니다:\n\n```\nproject1_docs -> vector_store_project1\nproject2_docs -> vector_store_project2\nresearch_papers -> vector_store_research\n```\n\n이렇게 하면 각 문서 세트를 독립적으로 관리하고 검색할 수 있습니다.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "document",
        "retrieval",
        "document processing",
        "document segmentation",
        "upload documents"
      ],
      "category": "document-processing"
    },
    "box-community--mcp-server-box": {
      "owner": "box-community",
      "name": "mcp-server-box",
      "url": "https://github.com/box-community/mcp-server-box",
      "imageUrl": "/freedevtools/mcp/pfp/box-community.webp",
      "description": "Integrate with the Box API to perform file operations, including file search, text extraction, and AI-based querying. Manage and process Box data efficiently with advanced AI capabilities.",
      "stars": 76,
      "forks": 27,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T22:21:03Z",
      "readme_content": "# Box MCP Server\n\n## Quick Start\n\n### Clone the repository:\n\n```sh\ngit clone https://github.com/box-community/mcp-server-box.git\ncd mcp-server-box\n```\n\n### Optional but recommended `uv` installation for virtual environment and dependency management:\n\n#### Homebrew (macOS)\n```sh\nbrew install uv\n```\n\n#### WinGet (Windows)\n```sh\nwinget install --id=astral-sh.uv  -e\n```\n\n#### On macOS and Linux\n```sh\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n#### On Windows\n```sh\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n### Set up the virtual environment and install dependencies:\n\n```sh\nuv sync\n```\n\n### Set environment variables:\nSet the following environment variables for Box authentication in a `.env` file or your system environment:\n\n#### Using OAuth2.0 with a Box App\n```\nBOX_CLIENT_ID = YOUR_CLIENT_ID\nBOX_CLIENT_SECRET = YOUR_CLIENT_SECRET\nBOX_REDIRECT_URL = http://localhost:8000/callback\n\nBOX_MCP_SERVER_AUTH_TOKEN = YOUR_BOX_MCP_SERVER_AUTH_TOKEN\n```\n\n#### Using CCG with a Box App\n```\nBOX_CLIENT_ID = YOUR_CLIENT_ID\nBOX_CLIENT_SECRET = YOUR_CLIENT_SECRET\nBOX_SUBJECT_TYPE = user_or_enterprise\nBOX_SUBJECT_ID = YOUR_USER_OR_ENTERPRISE_ID\n\nBOX_MCP_SERVER_AUTH_TOKEN = YOUR_BOX_MCP_SERVER_AUTH_TOKEN\n```\n\n> Note: The `BOX_MCP_SERVER_AUTH_TOKEN` is the token used to authenticate requests to the Box MCP server. You can generate this token.\n\n### Run the MCP server in STDIO mode:\n```sh\nuv run src/mcp_server_box.py\n```\n\n## Box Community MCP Server Tools\n\nBelow is a summary of the available tools:\n\n| Tools available          | Description                                      |\n|--------------------------|--------------------------------------------------|\n| [box_tools_ai](docs/box_tools_ai.md) | AI-powered file and hub queries                  |\n| [box_tools_collaboration](docs/box_tools_collaboration.md)  | Manage file/folder collaborations                |\n| [box_tools_docgen](docs/box_tools_docgen.md)         | Document generation and template management      |\n| [box_tools_files](docs/box_tools_files.md)          | File operations (read, upload, download)         |\n| [box_tools_folders](docs/box_tools_folders.md)        | Folder operations (list, create, delete, update) |\n| [box_tools_generic](docs/box_tools_generic.md)        | Generic Box API utilities                        |\n| [box_tools_groups](docs/box_tools_groups.md)         | Group management and queries                     |\n| [box_tools_metadata](docs/box_tools_metadata.md)       | Metadata template and instance management        |\n| [box_tools_search](docs/box_tools_search.md)         | Search files and folders                         |\n| [box_tools_shared_links](docs/box_tools_shared_links.md)   | Shared link management for files/folders/web-links|\n| [box_tools_users](docs/box_tools_users.md)          | User management and queries                      |\n| [box_tools_web_link](docs/box_tools_web_link.md)       | Web link creation and management                 |\n\n## Box Community MCP Server Operations Details\n\n### Command line interface parameters\nTo run the MCP server with specific configurations, you can use the following command line parameters:\n```sh\nuv run src/mcp_server_box.py --help\n```\n```\nusage: mcp_server_box.py [-h] [--transport {stdio,sse,streamable-http}] [--host HOST]\n                         [--port PORT] [--box-auth {oauth,ccg}] [--no-mcp-server-auth]\n\nBox Community MCP Server\n\noptions:\n  -h, --help            show this help message and exit\n  --transport {stdio,sse,streamable-http}\n                        Transport type (default: stdio)\n  --host HOST           Host for SSE/HTTP transport (default: 0.0.0.0)\n  --port PORT           Port for SSE/HTTP transport (default: 8000)\n  --box-auth {oauth,ccg}\n                        Authentication type for Box API (default: oauth)\n  --no-mcp-server-auth  Disable authentication (for development only)\n  ```\n\n### Claude Desktop Configuration\nEdit your `claude_desktop_config.json`:\n\n```code ~/Library/Application\\ Support/Claude/claude_desktop_config.json```\n\nAdd the configuration:\n```json\n{\n    \"mcpServers\": {\n        \"mcp-server-box\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/mcp-server-box\",\n                \"run\",\n                \"src/mcp_server_box.py\"\n            ]\n        }\n    }\n}\n```\n\nRestart Claude if it is running.\n\n### Cursor Configuration\n\nCursor supports MCP servers through its configuration file. Here's how to set it up:\n\nThe Cursor MCP configuration file is located at:\n- **macOS/Linux**: `~/.cursor/config.json` or `~/.config/cursor/config.json`\n- **Windows**: `%APPDATA%\\Cursor\\config.json`\n\n#### Add the MCP Server Configuration: STDIO Transport\n\nEdit your Cursor configuration file and add the following under the `mcpServers` section:\n```json\n{\n    \"mcpServers\": {\n        \"mcp-server-box\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/mcp-server-box\",\n                \"run\",\n                \"src/mcp_server_box.py\"\n            ],\n            \"env\": {\n                \"BOX_CLIENT_ID\": \"YOUR_CLIENT_ID\",\n                \"BOX_CLIENT_SECRET\": \"YOUR_CLIENT_SECRET\",\n                \"BOX_REDIRECT_URL\": \"http://localhost:8000/callback\"\n            }\n        }\n    }\n}",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "processing",
        "mcp",
        "box",
        "processing box",
        "process box",
        "document processing"
      ],
      "category": "document-processing"
    },
    "breezedeus--CnOCR": {
      "owner": "breezedeus",
      "name": "CnOCR",
      "url": "https://github.com/breezedeus/CnOCR",
      "imageUrl": "/freedevtools/mcp/pfp/breezedeus.webp",
      "description": "Enables optical character recognition for Chinese, English, and numbers using pre-trained models or custom training. Provides powerful text recognition capabilities for a variety of applications.",
      "stars": 3659,
      "forks": 528,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T14:15:19Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"./docs/figs/cnocr-logo.jpg\" width=\"250px\"/>\n  <div>&nbsp;</div>\n\n[![Discord](https://img.shields.io/discord/1200765964434821260?label=Discord)](https://discord.gg/GgD87WM8Tf)\n[![Downloads](https://static.pepy.tech/personalized-badge/cnocr?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads)](https://pepy.tech/project/cnocr)\n[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fbreezedeus%2FCnOCR&label=Visitors&countColor=%23f5c791&style=flat&labelStyle=none)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fbreezedeus%2FCnOCR)\n[![license](https://img.shields.io/github/license/breezedeus/cnocr)](./LICENSE)\n[![Docs](https://readthedocs.org/projects/cnocr/badge/?version=latest)](https://cnocr.readthedocs.io/zh-cn/stable/?badge=latest)\n[![PyPI version](https://badge.fury.io/py/cnocr.svg)](https://badge.fury.io/py/cnocr)\n[![forks](https://img.shields.io/github/forks/breezedeus/cnocr)](https://github.com/breezedeus/cnocr)\n[![stars](https://img.shields.io/github/stars/breezedeus/cnocr)](https://github.com/breezedeus/cnocr)\n![last-releast](https://img.shields.io/github/release-date/breezedeus/cnocr)\n![last-commit](https://img.shields.io/github/last-commit/breezedeus/cnocr)\n[![Twitter](https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fbreezedeus)](https://twitter.com/breezedeus)\n\n[📖 文档](https://cnocr.readthedocs.io/zh-cn/stable/) |\n[🛠️ 安装](https://cnocr.readthedocs.io/zh-cn/stable/install/) |\n[🧳 可用模型](https://cnocr.readthedocs.io/zh-cn/stable/models/) |\n[🕹 模型训练](https://cnocr.readthedocs.io/zh-cn/stable/train/) |\n[🛀🏻 在线Demo](https://huggingface.co/spaces/breezedeus/CnOCR-Demo) |\n[💬 交流群](https://www.breezedeus.com/article/join-group)\n\n</div>\n\n<div align=\"center\">\n\n[English](./README_en.md) | 中文\n\n</div>\n\n# CnOCR\n\n<div align=\"center\">\n<strong>Tech should serve the people, not enslave them!</strong>\n<br>\n<strong>请勿将此项目用于文字审查！</strong>\n<br>\n---\n</div>\n\n### Update 2025.06.26：发布 V2.3.2\n\n主要变更：\n\n* 集成 PPOCRv5 最新版 OCR 模型\n  * 新增支持 PP-OCRv5 识别模型：`ch_PP-OCRv5` 和 `ch_PP-OCRv5_server`\n\n\n### [Update 2024.11.30]：发布 V2.3.1\n\n主要变更：\n\n* 基于 RapidOCR 集成 PPOCRv4 最新版 OCR 模型，提供更多的模型选择\n  * 新增支持 PP-OCRv4  识别模型，包括标准版和服务器版\n* 修改读文件实现方式，支持 Windows 的中文路径\n* 修复Bug：当使用多个进程时，transform_func 无法序列化\n* 修复Bug：与 albumentations=1.4.* 兼容\n\n### [Update 2023.12.24]：发布 V2.3\n\n主要变更：\n\n* 重新训练了所有的模型，比上一版精度更高。\n* 按使用场景把模型分为几大类场景（见 [识别模型列表](#可使用的识别模型)）：\n  * `scene`：场景图片，适合识别一般拍照图片中的文字。此类模型以 `scene-` 开头，如模型 `scene-densenet_lite_136-gru`。\n  * `doc`：文档图片，适合识别规则文档的截图图片，如书籍扫描件等。此类模型以 `doc-` 开头，如模型 `doc-densenet_lite_136-gru`。\n  * `number`：仅识别**纯数字**（只能识别 `0~9` 十个数字）图片，适合银行卡号、身份证号等场景。此类模型以 `number-` 开头，如模型 `number-densenet_lite_136-gru`。\n  * `general`: 通用场景，适合图片无明显倾向的一般图片。此类模型无特定开头，与旧版模型名称保持一致，如模型 `densenet_lite_136-gru`。\n  > 注意 ⚠️：以上说明仅为参考，具体选择模型时建议以实际效果为准。\n* 加入了两个更大的系列模型：\n  * `*-densenet_lite_246-gru_base`：优先供 **知识星球** [**CnOCR/CnSTD私享群**](https://t.zsxq.com/FEYZRJQ) 会员使用，一个月后会免费开源。\n  * `*-densenet_lite_666-gru_large`：Pro 模型，购买后可使用。\n  \n更多细节请参考：[CnOCR V2.3 新版发布：模型更好、更多、更大 | Breezedeus.com](https://www.breezedeus.com/article/cnocr-v2.3-better-more)。\n\n\n\n[**CnOCR**](https://github.com/breezedeus/cnocr) 是 **Python 3** 下的**文字识别**（**Optical Character Recognition**，简称**OCR**）工具包，支持**简体中文**、**繁体中文**（部分模型）、**英文**和**数字**的常见字符识别，支持竖排文字的识别。自带了**20+个** [训练好的模型](https://cnocr.readthedocs.io/zh-cn/stable/models/)，适用于不同应用场景，安装后即可直接使用。同时，CnOCR也提供简单的[训练命令](https://cnocr.readthedocs.io/zh-cn/stable/train/)供使用者训练自己的模型。欢迎扫码加小助手为好友，备注 `ocr`，小助手会定期统一邀请大家入群：\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/datasets/breezedeus/cnocr-wx-qr-code/resolve/main/wx-qr-code.JPG\" alt=\"微信群二维码\" width=\"300px\"/>\n</div>\n\n\n作者也维护 **知识星球** [**CnOCR/CnSTD私享群**](https://t.zsxq.com/FEYZRJQ) ，这里面的提问会较快得到作者的回复，欢迎加入。**知识星球会员** 可享受以下福利：\n\n- 可免费下载部分**未开源的付费模型**；\n- 购买其他所有的付费模型一律八折优化；\n- 作者快速回复使用过程中遇到的各种困难；\n- 作者每月提供两次免费特有数据的训练服务。\n- 星球会陆续发布一些CnOCR/CnSTD相关的私有资料；\n- 星球会持续发布 OCR/STD/CV 等相关的最新研究资料。\n\n\n\n## 详细文档\n\n见 [CnOCR在线文档](https://cnocr.readthedocs.io/) 。\n\n## 使用说明\n\n**CnOCR** 从 **V2.2** 开始，内部自动调用文字检测引擎 **[CnSTD](https://github.com/breezedeus/cnstd)** 进行文字检测和定位。所以 **CnOCR** V2.2 不仅能识别排版简单的印刷体文字图片，如截图图片，扫描件等，也能识别**一般图片中的场景文字**。\n\n以下是一些不同场景的调用示例。\n\n\n\n## 不同场景的调用示例\n\n### 常见的图片识别\n\n所有参数都使用默认值即可。如果发现效果不够好，多调整下各个参数看效果，最终往往能获得比较理想的精度。\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/huochepiao.jpeg'\nocr = CnOcr()  # 所有参数都使用默认值\nout = ocr.ocr(img_fp)\n\nprint(out)\n```\n\n识别结果：\n\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/huochepiao.jpeg-result.jpg\" alt=\"火车票识别\" width=\"800px\"/>\n</div>\n\n\n### 排版简单的印刷体截图图片识别\n\n针对 **排版简单的印刷体文字图片**，如截图图片，扫描件图片等，可使用 `det_model_name='naive_det'`，相当于不使用文本检测模型，而使用简单的规则进行分行。\n\n> **Note**\n>\n>  `det_model_name='naive_det'` 的效果相当于 `V2.2` 之前（`V2.0.*`, `V2.1.*`）的 CnOCR 版本。\n\n使用 `det_model_name='naive_det'` 的最大优势是**速度快**，劣势是对图片比较挑剔。如何判断是否该使用此检测模型呢？最简单的方式就是拿应用图片试试效果，效果好就用，不好就不用。\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/multi-line_cn1.png'\nocr = CnOcr(det_model_name='naive_det') \nout = ocr.ocr(img_fp)\n\nprint(out)\n```\n\n识别结果：\n\n<div align=\"center\">\n\n| 图片                                                                      | OCR结果                                                                                                                         |\n| ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| ![docs/examples/multi-line_cn1.png](./docs/examples/multi-line_cn1.png) | 网络支付并无本质的区别，因为<br />每一个手机号码和邮件地址背后<br />都会对应着一个账户--这个账<br />户可以是信用卡账户、借记卡账<br />户，也包括邮局汇款、手机代<br />收、电话代收、预付费卡和点卡<br />等多种形式。 |\n\n</div>\n\n\n### 竖排文字识别\n\n采用来自 [**PaddleOCR**](https://github.com/PaddlePaddle/PaddleOCR)（之后简称 **ppocr**）的中文识别模型 `rec_model_name='ch_PP-OCRv3'` 进行识别。\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/shupai.png'\nocr = CnOcr(rec_model_name='ch_PP-OCRv3')\nout = ocr.ocr(img_fp)\n\nprint(out)\n```\n\n识别结果：\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/shupai.png-result.jpg\" alt=\"竖排文字识别\" width=\"800px\"/>\n</div>\n\n\n### 英文识别\n\n虽然中文检测和识别模型也能识别英文，但**专为英文文字训练的检测器和识别器往往精度更高**。如果是纯英文的应用场景，建议使用来自 **ppocr** 的英文检测模型 `det_model_name='en_PP-OCRv3_det'`， 和英文识别模型 `rec_model_name='en_PP-OCRv3'` 。\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/en_book1.jpeg'\nocr = CnOcr(det_model_name='en_PP-OCRv3_det', rec_model_name='en_PP-OCRv3')\nout = ocr.ocr(img_fp)\n\nprint(out)\n```\n\n识别结果：\n\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/en_book1.jpeg-result.jpg\" alt=\"英文识别\" width=\"600px\"/>\n</div>\n\n\n### 繁体中文识别\n\n采用来自ppocr的繁体识别模型 `rec_model_name='chinese_cht_PP-OCRv3'` 进行识别。\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/fanti.jpg'\nocr = CnOcr(rec_model_name='chinese_cht_PP-OCRv3')  # 识别模型使用繁体识别模型\nout = ocr.ocr(img_fp)\n\nprint(out)\n```\n\n使用此模型时请注意以下问题：\n\n* 识别精度一般，不是很好；\n\n* 除了繁体字，对标点、英文、数字的识别都不好；\n\n* 此模型不支持竖排文字的识别。\n\n识别结果：\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/fanti.jpg-result.jpg\" alt=\"繁体中文识别\" width=\"700px\"/>\n</div>\n\n\n### 单行文字的图片识别\n\n如果明确知道待识别的图片是单行文字图片（如下图），可以使用类函数 `CnOcr.ocr_for_single_line()` 进行识别。这样就省掉了文字检测的时间，速度会快一倍以上。\n\n<div align=\"center\">\n  <img src=\"./docs/examples/helloworld.jpg\" alt=\"单行文本识别\" width=\"300px\"/>\n</div>\n调用代码如下：\n\n```python\nfrom cnocr import CnOcr\n\nimg_fp = './docs/examples/helloworld.jpg'\nocr = CnOcr()\nout = ocr.ocr_for_single_line(img_fp)\nprint(out)\n```\n\n\n\n### 更多应用示例\n\n* **核酸疫苗截图识别**\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/jiankangbao.jpeg-result.jpg\" alt=\"核酸疫苗截图识别\" width=\"500px\"/>\n</div>\n\n* **身份证识别**\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/aobama.webp-result.jpg\" alt=\"身份证识别\" width=\"700px\"/>\n</div>\n\n* **饭店小票识别**\n<div align=\"center\">\n  <img src=\"./docs/predict-outputs/fapiao.jpeg-result.jpg\" alt=\"饭店小票识别\" width=\"500px\"/>\n</div>\n  \n\n  \n\n## 安装\n\n嗯，顺利的话一行命令即可。\n\n```bash\n$ pip install cnocr[ort-cpu]\n```\n\n如果是 **GPU** 环境使用 ONNX 模型，请使用以下命令进行安装：\n\n```bash\n$ pip install cnocr[ort-gpu]\n```\n\n\n\n如果要训练自己的模型，，可以使用以下命令安装：\n\n```bash\n$ pip install cnocr[dev]\n```\n\n\n\n安装速度慢的话，可以指定国内的安装源，如使用阿里云的安装源：\n\n```bash\n$ pip install cnocr[ort-cpu] -i https://mirrors.aliyun.com/pypi/simple\n```\n\n> **Note** \n>\n> 请使用 **Python3**（3.7.\\*~3.10.\\*之间的版本应该都行），没测过Python2下是否ok。\n\n更多说明可见 [安装文档](https://cnocr.readthedocs.io/zh-cn/stable/install/)。\n\n> **Warning** \n>\n> 如果电脑中从未安装过 `PyTorch`，`OpenCV` python包，初次安装可能会遇到问题，但一般都是常见问题，可以自行百度/Google解决。\n\n\n\n### Docker Image\n\n可以从 [Docker Hub](https://hub.docker.com/u/breezedeus) 直接拉取已安装好 CnOCR 的镜像使用。\n\n```bash\n$ docker pull breezedeus/cnocr:latest\n```\n\n更多说明可见 [安装文档](https://cnocr.readthedocs.io/zh-cn/stable/install/)。\n\n\n\n## HTTP服务\n\nCnOCR **V2.2.1** 加入了基于 FastAPI 的HTTP服务。开启服务需要安装几个额外的包，可以使用以下命令安装：\n\n```bash\npip install cnocr[serve]\n```\n\n\n\n安装完成后，可以通过以下命令启动HTTP服务（**`-p`** 后面的数字是**端口**，可以根据需要自行调整）：\n\n```bash\ncnocr serve -p 8501\n```\n\n\n\n服务开启后，可以使用以下方式调用服务。\n\n\n\n### 命令行\n\n比如待识别文件为 `docs/examples/huochepiao.jpeg`，如下使用 curl 调用服务：\n\n```bash\n> curl -F image=@docs/examples/huochepiao.jpeg http://0.0.0.0:8501/ocr\n```\n\n\n\n### Python\n\n使用如下方式调用服务：\n\n```python\nimport requests\n\nimage_fp = 'docs/examples/huochepiao.jpeg'\nr = requests.post(\n    'http://0.0.0.0:8501/ocr', files={'image': (image_fp, open(image_fp, 'rb'), 'image/png')},\n)\nocr_out = r.json()['results']\nprint(ocr_out)\n```\n\n\n\n具体也可参考文件 [scripts/screenshot_daemon_with_server.py](scripts/screenshot_daemon_with_server.py) 。 \n\n\n\n### 其他语言\n\n请参照 curl 的调用方式自行实现。\n\n\n\n\n\n## 可使用的模型\n\n### 可使用的检测模型\n\n具体参考 [CnSTD的下载说明](https://github.com/breezedeus/CnSTD?tab=readme-ov-file#%E5%B7%B2%E6%9C%89std%E6%A8%A1%E5%9E%8B)。\n\n| `det_model_name`                                             | PyTorch 版本 | ONNX 版本 | 模型原始来源 | 模型文件大小 | 支持语言                       | 是否支持竖排文字识别 |\n| ------------------------------------------------------------ | ------------ | --------- | ------------ | ------------ | ------------------------------ | -------------------- |\n| db_shufflenet_v2                                             | √            | X         | cnocr        | 18 M         | 简体中文、繁体中文、英文、数字 | √                    |\n| **db_shufflenet_v2_small**                                   | √            | X         | cnocr        | 12 M         | 简体中文、繁体中文、英文、数字 | √                    |\n| db_mobilenet_v3                                              | √            | X         | cnocr        | 16 M         | 简体中文、繁体中文、英文、数字 | √                    |\n| db_mobilenet_v3_small                                        | √            | X         | cnocr        | 7.9 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| db_resnet34                                                  | √            | X         | cnocr        | 86 M         | 简体中文、繁体中文、英文、数字 | √                    |\n| db_resnet18                                                  | √            | X         | cnocr        | 47 M         | 简体中文、繁体中文、英文、数字 | √                    |\n| ch_PP-OCRv5_det                                              | X            | √         | ppocr        | 4.6 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| ch_PP-OCRv5_det_server                                       | X            | √         | ppocr        | 84 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| ch_PP-OCRv4_det                                              | X            | √         | ppocr        | 4.5 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| ch_PP-OCRv4_det_server                                       | X            | √         | ppocr        | 108 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| ch_PP-OCRv3_det                                              | X            | √         | ppocr        | 2.3 M        | 简体中文、繁体中文、英文、数字 | √                    |\n| **en_PP-OCRv3_det**                                          | X            | √         | ppocr        | 2.3 M        | **英文**、数字                 | √                    |\n\n\n\n### 可使用的识别模型\n\n相比于 CnOCR V2.2.* 版本，**V2.3** 中的大部分模型都经过了重新训练和精调，精度比旧版模型更高。同时，加入了两个参数量更多的模型系列：\n\n  * `*-densenet_lite_246-gru_base`：优先供 **知识星球** [**CnOCR/CnSTD私享群**](https://t.zsxq.com/FEYZRJQ) 会员使用，后续会免费开源。\n  * `*-densenet_lite_666-gru_large`：**Pro 模型**，购买后可使用。购买链接见文档：\n\n**V2.3** 中的模型按使用场景可以分为以下几大类：\n\n* `scene`：场景图片，适合识别一般拍照图片中的文字。此类模型以 `scene-` 开头，如模型 `scene-densenet_lite_136-gru`。\n* `doc`：文档图片，适合识别规则文档的截图图片，如书籍扫描件等。此类模型以 `doc-` 开头，如模型 `doc-densenet_lite_136-gru`。\n* `number`：仅识别**纯数字**（只能识别 `0~9` 十个数字）图片，适合银行卡号、身份证号等场景。此类模型以 `number-` 开头，如模型 `number-densenet_lite_136-gru`。\n* `general`: 通用场景，适合图片无明显倾向的一般图片。此类模型无特定开头，与旧版模型名称保持一致，如模型 `densenet_lite_136-gru`。\n\n> 注意 ⚠️：以上说明仅供参考，具体选择模型时建议以实际效果为准。\n\n更多说明见：[可用模型](https://cnocr.readthedocs.io/zh-cn/stable/models/)。\n\n| `rec_model_name`                                             | PyTorch 版本 | ONNX 版本 | 模型原始来源 | 模型文件大小 | 支持语言                            | 是否支持竖排文字识别 |\n| ------------------------------------------------------------ | ------------ | --------- | ------------ | ------------ | ----------------------------------- | -------------------- |\n| **densenet_lite_136-gru** 🆕                                  | √            | √         | cnocr        | 12 M         | 简体中文、英文、数字                | X                    |\n| **scene-densenet_lite_136-gru** 🆕                            | √            | √         | cnocr        | 12 M         | 简体中文、英文、数字                | X                    |\n| **doc-densenet_lite_136-gru** 🆕                              | √            | √         | cnocr        | 12 M         | 简体中文、英文、数字                | X                    |\n| **densenet_lite_246-gru_base** 🆕 <br /> ([星球会员](https://t.zsxq.com/FEYZRJQ)专享) | √            | √         | cnocr        | 25 M         | 简体中文、英文、数字                | X                    |\n| **scene-densenet_lite_246-gru_base** 🆕 <br /> ([星球会员](https://t.zsxq.com/FEYZRJQ)专享) | √            | √         | cnocr        | 25 M         | 简体中文、英文、数字                | X                    |\n| **doc-densenet_lite_246-gru_base** 🆕 <br /> ([星球会员](https://t.zsxq.com/FEYZRJQ)专享) | √            | √         | cnocr        | 25 M         | 简体中文、英文、数字                | X                    |\n| **densenet_lite_666-gru_large** 🆕 <br />（购买链接：[B站](https://mall.bilibili.com/neul-next/detailuniversal/detail.html?isMerchant=1&page=detailuniversal_detail&saleType=10&itemsId=11884138&loadingShow=1&noTitleBar=1&msource=merchant_share)、[Lemon Squeezy](https://ocr.lemonsqueezy.com/)） | √            | √         | cnocr        | 82 M         | 简体中文、英文、数字                | X                    |\n| **scene-densenet_lite_666-gru_large** 🆕 <br />（购买链接：[B站](https://mall.bilibili.com/neul-next/detailuniversal/detail.html?isMerchant=1&page=detailuniversal_detail&saleType=10&itemsId=11883935&loadingShow=1&noTitleBar=1&msource=merchant_share)、[Lemon Squeezy](https://ocr.lemonsqueezy.com/)） | √            | √         | cnocr        | 82 M         | 简体中文、英文、数字                | X                    |\n| **doc-densenet_lite_666-gru_large** 🆕 <br />（购买链接：[B站](https://mall.bilibili.com/neul-next/detailuniversal/detail.html?isMerchant=1&page=detailuniversal_detail&saleType=10&itemsId=11883965&loadingShow=1&noTitleBar=1&msource=merchant_share)、[Lemon Squeezy](https://ocr.lemonsqueezy.com/)） | √            | √         | cnocr        | 82 M         | 简体中文、英文、数字                | X                    |\n| **number-densenet_lite_136-fc** 🆕                            | √            | √         | cnocr        | 2.7 M        | **纯数字**（仅包含 `0~9` 十个数字） | X                    |\n| **number-densenet_lite_136-gru**  🆕 <br /> ([星球会员](https://t.zsxq.com/FEYZRJQ)专享) | √            | √         | cnocr        | 5.5 M        | **纯数字**（仅包含 `0~9` 十个数字） | X                    |\n| **number-densenet_lite_666-gru_large** 🆕 <br />（购买链接：[B站](https://mall.bilibili.com/neul-next/detailuniversal/detail.html?isMerchant=1&page=detailuniversal_detail&saleType=10&itemsId=11884155&loadingShow=1&noTitleBar=1&msource=merchant_share)、[Lemon Squeezy](https://ocr.lemonsqueezy.com/)） | √            | √         | cnocr        | 55 M         | **纯数字**（仅包含 `0~9` 十个数字） | X                    |\n| ch_PP-OCRv5                                                  | X            | √         | ppocr        | 16 M         | 简体中文、英文、数字                | √                    |\n| ch_PP-OCRv5_server                                           | X            | √         | ppocr        | 81 M         | 简体中文、英文、数字                | √                    |\n| ch_PP-OCRv4                                                  | X            | √         | ppocr        | 10 M         | 简体中文、英文、数字                | √                    |\n| ch_PP-OCRv4_server                                           | X            | √         | ppocr        | 86 M         | 简体中文、英文、数字                | √                    |\n| ch_PP-OCRv3                                                  | X            | √         | ppocr        | 10 M         | 简体中文、英文、数字                | √                    |\n| ch_ppocr_mobile_v2.0                                         | X            | √         | ppocr        | 4.2 M        | 简体中文、英文、数字                | √                    |\n| en_PP-OCRv4                                                  | X            | √         | ppocr        | 8.6 M        | **英文**、数字                      | √                    |\n| en_PP-OCRv3                                                  | X            | √         | ppocr        | 8.5 M        | **英文**、数字                      | √                    |\n| en_number_mobile_v2.0                                        | X            | √         | ppocr        | 1.8 M        | **英文**、数字                      | √                    |\n| chinese_cht_PP-OCRv3                                         | X            | √         | ppocr        | 11 M         | **繁体中文**、英文、数字            | X                    |\n| japan_PP-OCRv3                                               | X            | √         | ppocr        | 9.6 M         | **日文**、英文、数字                | √                    |\n| korean_PP-OCRv3                                              | X            | √         | ppocr        | 9.4 M         | **韩文**、英文、数字                | √                    |\n| latin_PP-OCRv3                                               | X            | √         | ppocr        | 8.6 M         | **拉丁文**、英文、数字              | √                    |\n| arabic_PP-OCRv3                                              | X            | √         | ppocr        | 8.6 M         | **阿拉伯文**、英文、数字            | √                    |\n\n\n\n## 未来工作\n\n* [x] 支持图片包含多行文字 (`Done`)\n* [x] crnn模型支持可变长预测，提升灵活性 (since `V1.0.0`)\n* [x] 完善测试用例 (`Doing`)\n* [x] 修bugs（目前代码还比较凌乱。。） (`Doing`)\n* [x] 支持`空格`识别（since `V1.1.0`）\n* [x] 尝试新模型，如 DenseNet，进一步提升识别准确率（since `V1.1.0`）\n* [x] 优化训练集，去掉不合理的样本；在此基础上，重新训练各个模型\n* [x] 由 MXNet 改为 PyTorch 架构（since `V2.0.0`）\n* [x] 基于 PyTorch 训练更高效的模型\n* [x] 支持列格式的文字识别\n* [x] 打通与 [CnSTD](https://github.com/breezedeus/cnstd) 的无缝衔接（since `V2.2`）\n* [ ] 模型精度进一步优化\n* [ ] 支持更多的应用场景\n\n\n\n## 给作者来杯咖啡\n\n开源不易，如果此项目对您有帮助，可以考虑 [给作者加点油🥤，鼓鼓气💪🏻](https://cnocr.readthedocs.io/zh-cn/stable/buymeacoffee/) 。\n\n---\n\n官方代码库：[https://github.com/breezedeus/cnocr](https://github.com/breezedeus/cnocr)。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cnocr",
        "recognition",
        "text",
        "text recognition",
        "character recognition",
        "recognition chinese"
      ],
      "category": "document-processing"
    },
    "cablate--mcp-doc-forge": {
      "owner": "cablate",
      "name": "mcp-doc-forge",
      "url": "https://github.com/cablate/mcp-doc-forge",
      "imageUrl": "/freedevtools/mcp/pfp/cablate.webp",
      "description": "Comprehensive document processing capabilities including reading various document formats and converting them to different formats. Provides features for PDF manipulation such as merging and splitting, alongside document conversion tools.",
      "stars": 15,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T08:28:39Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cablate-mcp-doc-forge-badge.png)](https://mseep.ai/app/cablate-mcp-doc-forge)\n\n# Simple Document Processing MCP Server\n[![smithery badge](https://smithery.ai/badge/@cablate/mcp-doc-forge)](https://smithery.ai/server/@cablate/mcp-doc-forge)\n\nA powerful Model Context Protocol (MCP) server providing comprehensive document processing capabilities.\n\n<a href=\"https://glama.ai/mcp/servers/pb9df6lnel\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pb9df6lnel/badge\" alt=\"Simple Document Processing Server MCP server\" /></a>\n\n## Features\n\n### Document Reader\n- Read DOCX, PDF, TXT, HTML, CSV\n\n### Document Conversion\n- DOCX to HTML/PDF conversion\n- HTML to TXT/Markdown conversion\n- PDF manipulation (merge, split)\n\n### Text Processing\n- Multi-encoding transfer support (UTF-8, Big5, GBK)\n- Text formatting and cleaning\n- Text comparison and diff generation\n- Text splitting by lines or delimiter\n\n### HTML Processing\n- HTML cleaning and formatting\n- Resource extraction (images, links, videos)\n- Structure-preserving conversion\n\n## Installation\n\n### Installing via Smithery\n\nTo install Document Processing Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@cablate/mcp-doc-forge):\n\n```bash\nnpx -y @smithery/cli install @cablate/mcp-doc-forge --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g @cablate/mcp-doc-forge\n```\n\n\n## Usage\n\n### Cli\n\n```bash\nmcp-doc-forge\n```\n\n### With [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n\n1. Click \"+ Add MCP Server\" in Dive Desktop\n2. Copy and paste this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@cablate/mcp-doc-forge\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n3. Click \"Save\" to install the MCP server\n\n## License\n\nMIT\n\n## Contributing\n\nWelcome community participation and contributions! Here are ways to contribute:\n\n- ⭐️ Star the project if you find it helpful\n- 🐛 Submit Issues: Report problems or provide suggestions\n- 🔧 Create Pull Requests: Submit code improvements\n\n## Contact\n\nIf you have any questions or suggestions, feel free to reach out:\n\n- 📧 Email: [reahtuoo310109@gmail.com](mailto:reahtuoo310109@gmail.com)\n- 📧 GitHub: [CabLate](https://github.com/cablate/)\n- 🤝 Collaboration: Welcome to discuss project cooperation\n- 📚 Technical Guidance: Sincere welcome for suggestions and guidance\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "formats",
        "forge",
        "document processing",
        "document conversion",
        "doc forge"
      ],
      "category": "document-processing"
    },
    "cuongpham2107--word-mcp-server": {
      "owner": "cuongpham2107",
      "name": "word-mcp-server",
      "url": "https://github.com/cuongpham2107/word-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cuongpham2107.webp",
      "description": "Facilitates the creation and editing of Microsoft Word documents via a straightforward API. Supports adding formatted text, images, and tables, enabling document generation and modification through natural language commands with LLM integration.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-16T01:42:41Z",
      "readme_content": "# Word MCP Server\n\nWord MCP Server là một ứng dụng Python cho phép tạo và chỉnh sửa tài liệu Microsoft Word (.docx) thông qua API. Dự án này sử dụng FastMCP để xây dựng các công cụ tương tác với tài liệu Word.\n\n## Cài đặt\n\n### Yêu cầu\n\n- Python 3.12+\n- Các thư viện phụ thuộc:\n  - python-docx\n  - opencv-python (cv2)\n  - numpy\n  - FastMCP\n\n### Cài đặt thư viện\n```bash\nuv venv\nsource venv/bin/activate\nuv pip install .\n```\n\n## Tính năng\n\nWord MCP Server cung cấp các công cụ để:\n\n1. Tạo và mở tài liệu Word\n2. Thêm và định dạng văn bản\n3. Thêm hình ảnh\n4. Tạo bảng\n5. Quản lý tài nguyên và prompt\n\n## Hướng dẫn sử dụng\n\n### Cấu hình và khởi chạy với LLM\n\nĐể sử dụng Word MCP Server với các mô hình ngôn ngữ lớn (LLM), bạn cần cấu hình thông qua file JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"word-mcp-server\": {\n      \"command\": \"/path/to/word-mcp-server/.venv/bin/python3\",\n      \"args\": [\"/path/to/word-mcp-server/server.py\"]\n    }\n  }\n}\n```\n\n#### Giải thích cấu hình:\n\n- `mcpServers`: Object chứa cấu hình cho các MCP server\n- `word-mcp-server`: Tên định danh của server\n- `command`: Đường dẫn đến Python interpreter (thường nằm trong môi trường ảo)\n- `args`: Các tham số dòng lệnh, trong đó tham số đầu tiên là đường dẫn đến file server.py\n\n\n## Server sẽ khởi động và sẵn sàng nhận lệnh từ LLM\n\n#### Tương tác với LLM:\n\nKhi đã cấu hình và khởi chạy thành công, bạn có thể sử dụng LLM để:\n- Tạo và chỉnh sửa tài liệu Word thông qua lệnh tự nhiên\n- Tự động tạo nội dung dựa trên prompt\n- Định dạng văn bản, thêm hình ảnh và bảng một cách thông minh\n\n### Tạo tài liệu mới\n\n```python\ncreate_new_document()\n```\n\n### Mở tài liệu có sẵn\n\n```python\nopen_document(\"path/to/document.docx\")\n```\n\n### Thêm tiêu đề và đoạn văn\n\n```python\n# Thêm tiêu đề\nadd_heading(\"Tiêu đề tài liệu\", level=0)\nadd_heading(\"Chương 1\", level=1)\n\n# Thêm đoạn văn bản\nadd_paragraph(\"Đây là nội dung đoạn văn bản.\")\n\n# Thêm đoạn văn bản với định dạng\nadd_paragraph(\n    \"Đây là đoạn văn bản được định dạng.\",\n    style=\"Normal\",\n    font_size=14,\n    bold=True,\n    italic=False,\n    alignment=WD_PARAGRAPH_ALIGNMENT.CENTER\n)\n```\n\n### Thêm định dạng cho một phần văn bản\n\n```python\n# Tạo đoạn văn bản\np = add_paragraph(\"Đây là đoạn văn bản cơ bản. \")\n\n# Thêm phần văn bản có định dạng khác\nadd_run_to_paragraph(\n    p,\n    \"Phần này được in đậm và màu đỏ.\",\n    bold=True,\n    color=\"red\"\n)\n\n# Thêm phần văn bản có highlight\nadd_run_to_paragraph(\n    p,\n    \" Phần này được highlight màu vàng.\",\n    highlight=\"yellow\"\n)\n```\n\n### Thêm hình ảnh\n\n```python\n# Thêm hình ảnh từ đường dẫn file\nadd_picture(\"path/to/image.jpg\", width=4.0)\n\n# Hoặc thêm hình ảnh từ ma trận numpy\nimport numpy as np\nimport cv2\n\nimg = cv2.imread(\"path/to/image.jpg\")\nadd_picture(img, width=3.5)\n```\n\n### Tạo bảng\n\n```python\n# Tạo bảng với 3 hàng và 4 cột\ntable = add_table(rows=3, cols=4, style=\"Table Grid\")\n\n# Điền dữ liệu vào bảng\ntable.cell(0, 0).text = \"Hàng 1, Cột 1\"\ntable.cell(0, 1).text = \"Hàng 1, Cột 2\"\n# ...\n```\n\n## Các màu hỗ trợ\n\nKhi sử dụng các tham số `color` và `highlight`, bạn có thể sử dụng các giá trị sau:\n\n- black\n- blue\n- green\n- dark blue\n- dark red\n- dark yellow\n- dark green\n- pink\n- red\n- white\n- teal\n- yellow\n- violet\n- gray25\n- gray50\n\n## Lưu ý\n\n- Dự án này sử dụng thư viện `python-docx` để tương tác với tài liệu Word\n- Các tài nguyên và prompt được lưu trữ trong thư mục `resources` và `prompts`\n- Đảm bảo bạn đã cài đặt đầy đủ các thư viện phụ thuộc trước khi chạy server\n\n## Ví dụ hoàn chỉnh\n\n```python\n# Tạo tài liệu mới\ncreate_new_document()\n\n# Thêm tiêu đề\nadd_heading(\"Báo cáo dự án\", level=0)\n\n# Thêm thông tin người tạo\np = add_paragraph(\"Người tạo: \")\nadd_run_to_paragraph(p, \"Nguyễn Văn A\", bold=True)\n\n# Thêm mục lục\nadd_heading(\"Mục lục\", level=1)\nadd_paragraph(\"1. Giới thiệu\")\nadd_paragraph(\"2. Nội dung\")\nadd_paragraph(\"3. Kết luận\")\n\n# Thêm nội dung\nadd_heading(\"1. Giới thiệu\", level=1)\nadd_paragraph(\"Đây là phần giới thiệu của dự án...\")\n\n# Thêm hình ảnh\nadd_paragraph(\"Hình ảnh minh họa:\")\nadd_picture(\"project_diagram.jpg\", width=5.0)\n\n# Thêm bảng dữ liệu\nadd_heading(\"Bảng dữ liệu\", level=2)\ntable = add_table(rows=3, cols=3)\ntable.cell(0, 0).text = \"Dữ liệu 1\"\ntable.cell(0, 1).text = \"Dữ liệu 2\"\ntable.cell(0, 2).text = \"Dữ liệu 3\"\n# Điền các dữ liệu khác...\n\n# Lưu tài liệu\nsave_document(\"bao_cao_du_an.docx\")\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "cuongpham2107",
        "documents",
        "microsoft word",
        "word documents",
        "cuongpham2107 word"
      ],
      "category": "document-processing"
    },
    "dazeb--markdown-downloader": {
      "owner": "dazeb",
      "name": "markdown-downloader",
      "url": "https://github.com/dazeb/markdown-downloader",
      "imageUrl": "/freedevtools/mcp/pfp/dazeb.webp",
      "description": "Download webpages and convert their content into markdown files. Features include a configurable download directory and automatic date-stamped filenames for organized storage.",
      "stars": 38,
      "forks": 15,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T23:57:02Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/dazeb-markdown-downloader-badge.jpg)](https://mseep.ai/app/dazeb-markdown-downloader)\n[![MseeP Badge](https://mseep.net/pr/dazeb-markdown-downloader-badge.jpg)](https://mseep.ai/app/dazeb-markdown-downloader)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/e85a9805-464e-46bd-a953-ccac0c4a5129)\n\n# Markdown Downloader MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@dazeb/markdown-downloader)](https://smithery.ai/server/@dazeb/markdown-downloader)\n\n## Overview\n\nMarkdown Downloader is a powerful MCP (Model Context Protocol) server that allows you to download webpages as markdown files with ease. Leveraging the r.jina.ai service, this tool provides a seamless way to convert web content into markdown format.\n\n<a href=\"https://glama.ai/mcp/servers/jrki7zltg7\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/jrki7zltg7/badge\" alt=\"Markdown Downloader MCP server\" />\n</a>\n\n## Features\n\n- 🌐 Download webpages as markdown using r.jina.ai\n- 📁 Configurable download directory\n- 📝 Automatically generates date-stamped filenames\n- 🔍 List downloaded markdown files\n- 💾 Persistent configuration\n\n## Prerequisites\n\n- Node.js (version 16 or higher)\n- npm (Node Package Manager)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markdown Downloader for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dazeb/markdown-downloader):\n\n```bash\nnpx -y @smithery/cli install @dazeb/markdown-downloader --client claude\n```\n\n### Installing manually\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-username/markdown-downloader.git\n   cd markdown-downloader\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Manually Add Server to Cline/Roo-Cline MCP Settings file\n\n### Linux/macOS\n```json\n{\n  \"mcpServers\": {\n    \"markdown-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/home/user/Documents/Cline/MCP/markdown-downloader/build/index.js\"\n      ],\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"download_markdown\",\n        \"set_download_directory\"\n      ]\n    }\n  }\n}\n```\n\n### Windows\n```json\n{\n  \"mcpServers\": {\n    \"markdown-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\Users\\\\username\\\\Documents\\\\Cline\\\\MCP\\\\markdown-downloader\\\\build\\\\index.js\"\n      ],\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"download_markdown\",\n        \"set_download_directory\"\n      ]\n    }\n  }\n}\n```\n\n## Tools and Usage\n\n### 1. Set Download Directory\n\nChange the download directory:\n\n```bash\nuse set_download_directory /path/to/your/local/download/folder\n```\n\n- Validates directory exists and is writable\n- Persists the configuration for future use\n\n### 2. Download Markdown\n\nDownload a webpage as a markdown file:\n\n```bash\nuse tool download_markdown https://example.com/blog-post\n```\n\n- The URL will be prepended with `r.jina.ai`\n- Filename format: `{sanitized-url}-{date}.md`\n- Saved in the configured download directory\n\n### 3. List Downloaded Files\n\nList all downloaded markdown files:\n\n```bash\nuse list_downloaded_files\n```\n\n### 4. Get Download Directory\n\nRetrieve the current download directory:\n\n```bash\nuse get_download_directory\n```\n\n## Configuration\n\n### Linux/macOS\n- Configuration is stored in `~/.config/markdown-downloader/config.json`\n- Default download directory: `~/.markdown-downloads`\n\n### Windows\n- Configuration is stored in `%APPDATA%\\markdown-downloader\\config.json`\n- Default download directory: `%USERPROFILE%\\Documents\\markdown-downloads`\n\n## Troubleshooting\n\n- Ensure you have an active internet connection\n- Check that the URL is valid and accessible\n- Verify write permissions for the download directory\n\n## Security\n\n- The tool uses r.jina.ai to fetch markdown content\n- Local files are saved with sanitized filenames\n- Configurable download directory allows flexibility\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n## Disclaimer\n\nThis tool is provided as-is. Always review downloaded content for accuracy and appropriateness.\n\n## Support\n\nFor issues or feature requests, please open an issue on the GitHub repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "dazeb",
        "downloader",
        "markdown downloader",
        "dazeb markdown",
        "processing dazeb"
      ],
      "category": "document-processing"
    },
    "dev-ithitchhiker--mcp-google-docs": {
      "owner": "dev-ithitchhiker",
      "name": "mcp-google-docs",
      "url": "https://github.com/dev-ithitchhiker/mcp-google-docs",
      "imageUrl": "/freedevtools/mcp/pfp/dev-ithitchhiker.webp",
      "description": "Manipulate Google Spreadsheets and Drive to create, copy, and manage files, allowing for efficient integration of document operations within applications.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-10T20:10:32Z",
      "readme_content": "# MCP Google Workspace Integration\n\nA comprehensive MCP (Metoro Control Protocol) tool for interacting with Google Workspace services including Google Docs, Sheets, Slides, and Drive.\n\n## Features\n\n### Google Drive Features\n- List files\n- Copy files\n- Rename files\n- Create empty spreadsheets\n- Create spreadsheets from templates\n- Copy existing spreadsheets\n\n### Google Sheets Features\n- List sheets\n- Copy sheets\n- Rename sheets\n- Get sheet data\n- Add/Delete rows\n- Add/Delete columns\n- Update cells\n- Create/Update/Delete charts\n- Update cell formats\n\n### Google Docs Features\n- Create documents\n- Insert text with formatting\n- Add headings\n- Insert images\n- Create and manage tables\n- Insert page breaks\n- Add horizontal rules\n- Update document styles\n- Manage table styles and content\n\n### Google Slides Features\n- Create presentations\n- Add slides\n- Insert images\n- Add shapes and lines\n- Update text styles\n- Modify slide backgrounds\n- Update slide layouts\n- Add slide transitions\n- Add speaker notes\n\n## Installation\n\n### 1. Virtual Environment Setup\n\n#### macOS/Linux\n```bash\n# Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n```\n\n#### Windows\n```bash\n# Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\nvenv\\Scripts\\activate\n```\n\n### 2. Install Required Packages\n```bash\npip install -r requirements.txt\n```\n\n### 3. Google Cloud Console Setup\n1. Create a project in Google Cloud Console\n2. Create OAuth 2.0 client ID\n3. Enable required APIs:\n   - Google Sheets API\n   - Google Drive API\n   - Google Docs API\n   - Google Slides API\n\n### 4. Environment Variables Setup\n```bash\nexport MCPGD_CLIENT_SECRET_PATH=\"/path/to/client_secret.json\"\nexport MCPGD_FOLDER_ID=\"your_folder_id\"\nexport MCPGD_TOKEN_PATH=\"/path/to/token.json\"  # Optional\n```\n\n## Usage\n\n### 1. Run the Program\n```bash\npython main.py\n```\n\n### 2. Use Tools via MCP\n\n#### Google Drive Examples\n```bash\n# List files\nmcp list_files\n\n# Copy a file\nmcp copy_file --file-id \"file_id\" --new_name \"new_name\"\n```\n\n#### Google Sheets Examples\n```bash\n# Get sheet data\nmcp get_sheet_data --spreadsheet_id \"your_spreadsheet_id\" --range \"Sheet1!A1:D10\"\n\n# Create chart\nmcp create_chart --chart_type \"LINE\" --range \"A1:B10\" --sheet_name \"Sheet1\" --title \"Sales Trend\"\n```\n\n#### Google Docs Examples\n```bash\n# Create document\nmcp create_document --title \"My Document\"\n\n# Insert formatted text\nmcp insert_text_to_document --document_id \"doc_id\" --text \"Hello World\" --font_family \"Arial\" --font_size 12\n```\n\n#### Google Slides Examples\n```bash\n# Create presentation\nmcp create_presentation --title \"My Presentation\"\n\n# Add slide with content\nmcp add_slide_to_presentation --presentation_id \"presentation_id\" --title \"Slide Title\" --content \"Slide Content\"\n```\n\n## Environment Variables\n\n- `MCPGD_CLIENT_SECRET_PATH`: Path to Google OAuth 2.0 client secret file\n- `MCPGD_FOLDER_ID`: Google Drive folder ID\n- `MCPGD_TOKEN_PATH`: Path to token storage file (Optional, Default: ~/.mcp_google_spreadsheet.json)\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "spreadsheets",
        "docs",
        "google docs",
        "document processing",
        "document operations"
      ],
      "category": "document-processing"
    },
    "diventnsknew--markitdown": {
      "owner": "diventnsknew",
      "name": "markitdown",
      "url": "https://github.com/diventnsknew/markitdown",
      "imageUrl": "/freedevtools/mcp/pfp/diventnsknew.webp",
      "description": "Converts various file formats to Markdown, facilitating integration with LLM applications and enabling text analysis pipelines while preserving document structure. Includes features for audio transcription and document intelligence to enhance data processing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-28T07:39:26Z",
      "readme_content": "# MarkItDown\n\n[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)\n![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)\n[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)\n\n> [!TIP]\n> MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.\n\n> [!IMPORTANT]\n> Breaking changes between 0.0.1 to 0.1.0:\n> * Dependencies are now organized into optional feature-groups (further details below). Use `pip install 'markitdown[all]'` to have backward-compatible behavior. \n> * convert\\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.\n> * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.\n\nMarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.\n\nAt present, MarkItDown supports:\n\n- PDF\n- PowerPoint\n- Word\n- Excel\n- Images (EXIF metadata and OCR)\n- Audio (EXIF metadata and speech transcription)\n- HTML\n- Text-based formats (CSV, JSON, XML)\n- ZIP files (iterates over contents)\n- Youtube URLs\n- EPubs\n- ... and more!\n\n## Why Markdown?\n\nMarkdown is extremely close to plain text, with minimal markup or formatting, but still\nprovides a way to represent important document structure. Mainstream LLMs, such as\nOpenAI's GPT-4o, natively \"_speak_\" Markdown, and often incorporate Markdown into their\nresponses unprompted. This suggests that they have been trained on vast amounts of\nMarkdown-formatted text, and understand it well. As a side benefit, Markdown conventions\nare also highly token-efficient.\n\n## Installation\n\nTo install MarkItDown, use pip: `pip install 'markitdown[all]'`. Alternatively, you can install it from the source:\n\n```bash\ngit clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e packages/markitdown[all]\n```\n\n## Usage\n\n### Command-Line\n\n```bash\nmarkitdown path-to-file.pdf > document.md\n```\n\nOr use `-o` to specify the output file:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md\n```\n\nYou can also pipe content:\n\n```bash\ncat path-to-file.pdf | markitdown\n```\n\n### Optional Dependencies\nMarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:\n\n```bash\npip install markitdown[pdf, docx, pptx]\n```\n\nwill install only the dependencies for PDF, DOCX, and PPTX files.\n\nAt the moment, the following optional dependencies are available:\n\n* `[all]` Installs all optional dependencies\n* `[pptx]` Installs dependencies for PowerPoint files\n* `[docx]` Installs dependencies for Word files\n* `[xlsx]` Installs dependencies for Excel files\n* `[xls]` Installs dependencies for older Excel files\n* `[pdf]` Installs dependencies for PDF files\n* `[outlook]` Installs dependencies for Outlook messages\n* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence\n* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files\n* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription\n\n### Plugins\n\nMarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:\n\n```bash\nmarkitdown --list-plugins\n```\n\nTo enable plugins use:\n\n```bash\nmarkitdown --use-plugins path-to-file.pdf\n```\n\nTo find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.\n\n### Azure Document Intelligence\n\nTo use Microsoft Document Intelligence for conversion:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"\n```\n\nMore information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)\n\n### Python API\n\nBasic usage in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)\n```\n\nDocument Intelligence conversion in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)\n```\n\nTo use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:\n\n```python\nfrom markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)\n```\n\n### Docker\n\n```sh\ndocker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### How to Contribute\n\nYou can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.\n\n<div align=\"center\">\n\n|            | All                                                          | Especially Needs Help from Community                                                                                                      |\n| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |\n| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |\n\n</div>\n\n### Running Tests and Checks\n\n- Navigate to the MarkItDown package:\n\n  ```sh\n  cd packages/markitdown\n  ```\n\n- Install `hatch` in your environment and run tests:\n\n  ```sh\n  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test\n  ```\n\n  (Alternative) Use the Devcontainer which has all the dependencies installed:\n\n  ```sh\n  # Reopen the project in Devcontainer and run:\n  hatch test\n  ```\n\n- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`\n\n### Contributing 3rd-party Plugins\n\nYou can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markitdown",
        "markdown",
        "formats",
        "diventnsknew markitdown",
        "markdown facilitating",
        "formats markdown"
      ],
      "category": "document-processing"
    },
    "esakrissa--mcp-doc": {
      "owner": "esakrissa",
      "name": "mcp-doc",
      "url": "https://github.com/esakrissa/mcp-doc",
      "imageUrl": "/freedevtools/mcp/pfp/esakrissa.webp",
      "description": "Integrates LLM applications with specific documentation sources, enabling access and retrieval of documentation files to enhance knowledge and responses. Provides tools for fetching documentation from specified URLs within those files.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-04T06:52:34Z",
      "readme_content": "# MCP Documentation Server\n\nA customized version of the MCP documentation server that enables integration between LLM applications (like Cursor, Claude Desktop, Windsurf) and documentation sources via the Model Context Protocol.\n\n## Overview\n\nThis server provides MCP host applications with:\n1. Access to specific documentation files (langgraph.txt and mcp.txt)\n2. Tools to fetch documentation from URLs within those files\n\n## Supported Documentation\n\nCurrently set up for:\n- LangGraph Documentation (from https://raw.githubusercontent.com/esakrissa/mcp-doc/main/docs/langgraph.txt)\n- MCP Documentation (from https://raw.githubusercontent.com/esakrissa/mcp-doc/main/docs/mcp.txt)\n\n## Quick Start\n\n### Setup and Run\n\n```bash\n# Clone the repository\ngit clone https://github.com/esakrissa/mcp-doc.git\ncd mcp-doc\n\n# Create and activate a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install the package in development mode\npip install -e .\n```\n\n### Running the Server\n\nYou can run the server using the installed command:\n\n```bash\n# Run the server with the config file\nmcpdoc \\\n    --json config.json \\\n    --transport sse \\\n    --port 8082 \\\n    --host localhost\n```\n\nOr if you prefer using UV:\n\n```bash\n# Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Run the server with UV\nuvx --from mcpdoc mcpdoc \\\n    --json config.json \\\n    --transport sse \\\n    --port 8082 \\\n    --host localhost\n```\n\n### IDE Integration\n\n#### Cursor\n\nAdd to `~/.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-doc\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"mcpdoc\",\n        \"mcpdoc\",\n        \"--urls\",\n        \"LangGraph:https://raw.githubusercontent.com/esakrissa/mcp-doc/main/docs/langgraph.txt\",\n        \"ModelContextProtocol:https://raw.githubusercontent.com/esakrissa/mcp-doc/main/docs/mcp.txt\",\n        \"--allowed-domains\",\n        \"*\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\nThen add these instructions to Cursor's Custom Instructions:\n\n```\nfor ANY question about LangGraph and Model Context Protocol (MCP), use the mcp-doc server to help answer -- \n+ call list_doc_sources tool to get the available documentation files\n+ call fetch_docs tool to read the langgraph.txt or mcp.txt file\n+ reflect on the urls in langgraph.txt or mcp.txt \n+ reflect on the input question \n+ call fetch_docs on any urls relevant to the question\n+ use this to answer the question\n```\n\nTo test if the integration is working, ask Cursor a question about LangGraph or MCP, and check if it uses the documentation server tools to fetch information.\n\n## Security Note\n\nFor security reasons, strict domain access controls are implemented:\n- Remote documentation files: Only the specific domain is automatically allowed\n- Local documentation files: No domains are automatically allowed\n- Use `--allowed-domains` to explicitly add domains or `--allowed-domains '*'` to allow all (use with caution)\n\n## References\n\nThis project is based on the original [mcpdoc by LangChain AI](https://github.com/langchain-ai/mcpdoc), modified to provide focused documentation access for LangGraph and MCP. \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "doc",
        "documentation files",
        "doc integrates",
        "retrieval documentation"
      ],
      "category": "document-processing"
    },
    "essenecrucix-netizen--jarvis": {
      "owner": "essenecrucix-netizen",
      "name": "jarvis",
      "url": "https://github.com/essenecrucix-netizen/jarvis",
      "imageUrl": "/freedevtools/mcp/pfp/essenecrucix-netizen.webp",
      "description": "An intelligent coding assistant that supports multiple AI models for code generation, modifications, and technical discussions. It can handle various file types for text extraction and data parsing to facilitate development tasks.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-01-15T00:11:47Z",
      "readme_content": "# J.A.R.V.I.S. - AI Code Assistant\n\nJ.A.R.V.I.S. is an intelligent coding assistant that leverages multiple state-of-the-art language models to help you with code generation, modifications, and technical discussions.\n\n## Features\n\n- **Multi-Model Support**: Choose between different AI models for your coding needs:\n  - DeepSeek Coder V3\n  - Gemini 2.0 Flash Experimental\n  - Grok 2\n  - Qwen 2.5 Coder\n  - Llama 3.3 70B Instruct\n  - Claude 3.5 Sonnet\n  - GPT-4 Turbo\n  - GPT-4o\n  - o1 Preview\n\n- **File Attachment Support**:\n  - PDF files with text extraction\n  - Microsoft Word documents (.docx)\n  - Excel spreadsheets with sheet parsing\n  - Images with OCR capabilities\n  - Enhanced Markdown with GFM support\n  - All major programming languages\n  - Configuration files\n  - Text and documentation files\n  - File preview with syntax highlighting\n  - Multiple file upload support\n  - Progress indicators and file size display\n  - Type-specific icons and preview buttons\n\n- **Real-Time Updates**:\n  - WebSocket-based notifications\n  - Instant feedback for code changes\n  - Real-time workspace updates\n  - Automatic change notifications\n\n- **Workspace Management**:\n  - Create and manage multiple workspaces\n  - View workspace history\n  - Delete workspaces when no longer needed\n  - Rename workspaces\n  - Browse workspace file structure\n\n- **Code Generation & Modification**:\n  - Generate new code based on natural language prompts\n  - Modify existing code with AI assistance\n  - Preview changes before applying them\n  - View diffs of proposed changes\n\n- **Interactive Chat**:\n  - Discuss code and technical concepts\n  - Get explanations about existing code\n  - Context-aware responses based on workspace content\n  - Attach files for additional context\n\n## Technical Stack\n\n- **Backend**:\n  - Flask web framework\n  - Flask-SocketIO for WebSocket support\n  - Eventlet for async operations\n\n- **Frontend**:\n  - Pure JavaScript\n  - TailwindCSS for styling\n  - CodeMirror for code editing\n  - Socket.IO client for real-time notifications\n  - PDF.js for PDF processing\n  - Mammoth.js for Word documents\n  - XLSX.js for Excel files\n  - Tesseract.js for OCR\n  - Marked and Unified.js for Markdown\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Set up your environment variables in `.env`:\n   ```\n   DEEPSEEK_API_KEY=your_deepseek_api_key\n   GROK_API_KEY=your_grok_api_key\n   GOOGLE_API_KEY=your_google_api_key\n   ANTHROPIC_API_KEY=your_anthropic_api_key\n   OPENAI_API_KEY=your_openai_api_key\n   ```\n\n## Usage\n\n1. Start the server:\n   ```bash\n   python app.py\n   ```\n2. Open your browser and navigate to `http://localhost:5000`\n3. Create a new workspace or select an existing one\n4. Choose your preferred AI model\n5. Start coding with AI assistance!\n\n## Model Capabilities\n\n- **DeepSeek Coder V3**: Specialized in code generation and modification\n- **Gemini 2.0 Pro**: Advanced code generation and natural language understanding\n- **Grok 2**: Advanced language model for code and natural language\n- **Qwen 2.5 Coder**: Specialized 32B model for code generation\n- **Llama 3.3 70B Instruct**: Large context window and strong code generation capabilities\n- **Claude 3.5 Sonnet**: Advanced reasoning and code understanding\n\n## Contributing\n\nContributions are welcome! Please feel free to submit pull requests.\n\n## Special Thanks\n\n- **Nikole Cardoso** for her invaluable contributions and support\n- **Guilherme Guirro** for his expertise and guidance\n- **Felipe Santos** for his dedication and insights\n\nTheir contributions have been instrumental in making J.A.R.V.I.S. better.\n\n## Platform Compatibility\n\nThis application has been tested and confirmed working on:\n- Linux (native)\n- Windows Subsystem for Linux (WSL 2)\n- Windows (native, no admin privileges required)\n\nThe application uses directory junctions on Windows to avoid requiring admin privileges, while maintaining symlink functionality on Unix-like systems.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "parsing",
        "netizen",
        "coding assistant",
        "netizen jarvis",
        "jarvis intelligent"
      ],
      "category": "document-processing"
    },
    "everaldo--mcp-mistral-ocr": {
      "owner": "everaldo",
      "name": "mcp-mistral-ocr",
      "url": "https://github.com/everaldo/mcp-mistral-ocr",
      "imageUrl": "/freedevtools/mcp/pfp/everaldo.webp",
      "description": "Processes images and PDFs using advanced OCR capabilities from Mistral AI, converting them into structured JSON outputs. It supports local files and files from URLs, handling multiple image formats.",
      "stars": 32,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-29T17:08:49Z",
      "readme_content": "# MCP Mistral OCR\n[![smithery badge](https://smithery.ai/badge/@everaldo/mcp-mistral-ocr)](https://smithery.ai/server/@everaldo/mcp-mistral-ocr)\n\nAn MCP server that provides OCR capabilities using Mistral AI's OCR API. This server can process both local files and URLs, supporting images and PDFs.\n\n## Features\n\n- Process local files (images and PDFs) using Mistral's OCR\n- Process files from URLs with explicit file type specification\n- Support for multiple file formats (JPG, PNG, PDF, etc.)\n- Results saved as JSON files with timestamps\n- Docker containerization\n- UV package management\n\n## Environment Variables\n\n- `MISTRAL_API_KEY`: Your Mistral AI API key\n- `OCR_DIR`: Directory path for local file processing. Inside the container, this is always mapped to `/data/ocr`\n\n## Installation\n\n### Installing via Smithery\n\nTo install Mistral OCR for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@everaldo/mcp-mistral-ocr):\n\n```bash\nnpx -y @smithery/cli install @everaldo/mcp/mistral-crosswalk --client claude\n```\n\n### Using Docker\n\n1. Build the Docker image:\n```bash\ndocker build -t mcp-mistral-ocr .\n```\n\n2. Run the container:\n```bash\ndocker run -e MISTRAL_API_KEY=your_api_key -e OCR_DIR=/data/ocr -v /path/to/local/files:/data/ocr mcp-mistral-ocr\n```\n\n### Local Development\n\n1. Install UV package manager:\n```bash\npip install uv\n```\n\n2. Create and activate virtual environment:\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix\n# or\n.venv\\Scripts\\activate  # On Windows\n```\n\n3. Install dependencies:\n```bash\nuv pip install .\n```\n\n## Claude Desktop Configuration\n\nAdd this configuration to your claude_desktop_config.json:\n\n```json\n{\n  \"mcpServers\": {\n    \"mistral-ocr\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"MISTRAL_API_KEY\",\n        \"-e\",\n        \"OCR_DIR\",\n        \"-v\",\n        \"C:/path/to/your/files:/data/ocr\",\n        \"mcp-mistral-ocr:latest\"\n      ],\n      \"env\": {\n        \"MISTRAL_API_KEY\": \"<YOUR_MISTRAL_API_KEY>\",\n        \"OCR_DIR\": \"C:/path/to/your/files\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. process_local_file\n\nProcess a file from the configured OCR_DIR directory.\n\n```json\n{\n    \"name\": \"process_local_file\",\n    \"arguments\": {\n        \"filename\": \"document.pdf\"\n    }\n}\n```\n\n### 2. process_url_file\n\nProcess a file from a URL. Requires explicit file type specification.\n\n```json\n{\n    \"name\": \"process_url_file\",\n    \"arguments\": {\n        \"url\": \"https://example.com/document\",\n        \"file_type\": \"image\"  // or \"pdf\"\n    }\n}\n```\n\n## Output\n\nOCR results are saved in JSON format in the `output` directory inside `OCR_DIR`. Each result file is named using the following format:\n- For local files: `{original_filename}_{timestamp}.json`\n- For URLs: `{url_filename}_{timestamp}.json` or `url_document_{timestamp}.json` if no filename is found in the URL\n\nThe timestamp format is `YYYYMMDD_HHMMSS`.\n\n## Supported File Types\n\n- Images: JPG, JPEG, PNG, GIF, WebP\n- Documents: PDF and other document formats supported by Mistral OCR\n\n## Limitations\n\n- Maximum file size: 50MB (enforced by Mistral API)\n- Maximum document pages: 1000 (enforced by Mistral API)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "document",
        "mistral",
        "mistral ocr",
        "ocr processes",
        "ocr capabilities"
      ],
      "category": "document-processing"
    },
    "famano--mcp-server-office": {
      "owner": "famano",
      "name": "mcp-server-office",
      "url": "https://github.com/famano/mcp-server-office",
      "imageUrl": "/freedevtools/mcp/pfp/famano.webp",
      "description": "Read and write Microsoft Word (docx) files with capabilities to edit paragraphs and insert new text. Access complete document content, including tables and images, through a command-line interface.",
      "stars": 27,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T00:07:12Z",
      "readme_content": "# MCP Server Office\n\n[![smithery badge](https://smithery.ai/badge/@famano/mcp-server-office)](https://smithery.ai/server/@famano/mcp-server-office)\n\nA Model Context Protocol (MCP) server providing tools to read/write Microsoft Word (docx) files.\n\n### Installing via Smithery\n\nTo install Server Office for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@famano/mcp-server-office):\n\n```bash\nnpx -y @smithery/cli install @famano/mcp-server-office --client claude\n```\n\n## Usage\n\nInstall with pip:\n\n```bash\npip install mcp-server-office\n```\n\nThen, start the MCP server:\n\n```bash\nmcp-server-office\n```\n\nOr using uv, just:\n\n```bash\nuvx mcp-server-office\n```\n\n### Available Tools\n\n1. `read_docx`: Read complete contents of a docx file including tables and images.\n\n   - Input: `path` (string) - Absolute path to the target file\n   - Note: Images are converted to [Image] placeholders, and track changes are not shown\n2. `write_docx`: Create a new docx file with given content.\n\n   - Input:\n     - `path` (string) - Absolute path to target file\n     - `content` (string) - Content to write to the file\n   - Note: Use double line breaks for new paragraphs, and [Table] tag with | separators for tables\n3. `edit_docx_paragraph`: Make text replacements in specified paragraphs of a docx file.\n\n   - Input:\n     - `path` (string) - Absolute path to file to edit\n     - `edits` (array) - List of dictionaries containing search/replace text and paragraph index\n       - `paragraph_index` (number) - 0-based index of the paragraph to edit\n       - `search` (string) - Text to find within the specified paragraph\n       - `replace` (string) - Text to replace with\n   - Note: Each search string must match exactly once within the specified paragraph\n4. `edit_docx_insert`: Insert new paragraphs into a docx file.\n\n   - Input:\n     - `path` (string) - Absolute path to file to edit\n     - `inserts` (array) - List of dictionaries containing text and optional paragraph index\n       - `text` (string) - Text to insert as a new paragraph\n       - `paragraph_index` (number, optional) - 0-based index of the paragraph before which to insert. If not specified, insert at the end.\n\n## Requirements\n\n- Python >= 3.12\n- Dependencies:\n  - mcp[cli] >= 1.2.0\n  - python-docx >= 1.1.2\n\n---\n\n# MCP Server Office (日本語)\n\n[![smithery badge](https://smithery.ai/badge/@famano/mcp-server-office)](https://smithery.ai/server/@famano/mcp-server-office)\n\nMicrosoft Word (docx) ファイルの読み書きを提供するModel Context Protocol (MCP) サーバーです。\n\n### Smitheryによるインストール\n\n[Smithery](https://smithery.ai/server/@famano/mcp-server-office)経由でClaude DesktopにServer Officeを自動インストールするには:\n\n```bash\nnpx -y @smithery/cli install @famano/mcp-server-office --client claude\n```\n\n## 使用方法\n\npipを使用してインストール:\n\n```bash\npip install mcp-server-office\n```\n\nMCPサーバーの起動:\n\n```bash\nmcp-server-office\n```\n\nまたは、uvを使う場合:\n\n```bash\nuvx mcp-server-office\n```\n\n### 利用可能なツール\n\n1. `read_docx`: docxファイルの内容を表やイメージを含めて完全に読み取ります。\n\n   - 入力: `path` (文字列) - 対象ファイルの絶対パス\n   - 注意: 画像は[Image]というプレースホルダーに変換され、変更履歴は表示されません\n2. `write_docx`: 新しいdocxファイルを指定された内容で作成します。\n\n   - 入力:\n     - `path` (文字列) - 作成するファイルの絶対パス\n     - `content` (文字列) - ファイルに書き込む内容\n   - 注意: 段落は2つの改行で区切り、表は[Table]タグと|区切りを使用します\n3. `edit_docx_paragraph`: docxファイル内の指定された段落のテキストを置換します。\n\n   - 入力:\n     - `path` (文字列) - 編集するファイルの絶対パス\n     - `edits` (配列) - 検索/置換テキストと段落インデックスを含む辞書のリスト\n       - `paragraph_index` (数値) - 編集する段落の0ベースのインデックス\n       - `search` (文字列) - 指定された段落内で検索するテキスト\n       - `replace` (文字列) - 置換するテキスト\n   - 注意: 各検索文字列は指定された段落内で一度だけマッチする必要があります\n4. `edit_docx_insert`: docxファイルに新しい段落を挿入します。\n\n   - 入力:\n     - `path` (文字列) - 編集するファイルの絶対パス\n     - `inserts` (配列) - テキストとオプションの段落インデックスを含む辞書のリスト\n       - `text` (文字列) - 新しい段落として挿入するテキスト\n       - `paragraph_index` (数値, オプション) - 挿入する位置の段落の0ベースのインデックス。指定しない場合は末尾に挿入されます。\n\n## 動作要件\n\n- Python >= 3.12\n- 依存パッケージ:\n  - mcp[cli] >= 1.2.0\n  - python-docx >= 1.1.2\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "docx",
        "document",
        "mcp",
        "word docx",
        "docx files",
        "microsoft word"
      ],
      "category": "document-processing"
    },
    "forayconsulting--zoom_transcript_mcp": {
      "owner": "forayconsulting",
      "name": "zoom_transcript_mcp",
      "url": "https://github.com/forayconsulting/zoom_transcript_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/forayconsulting.webp",
      "description": "Manage Zoom meeting transcripts by listing, downloading, and searching through them with a structured interface. Organize transcripts by month for streamlined access to discussions.",
      "stars": 7,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T09:52:15Z",
      "readme_content": "# Zoom Transcript MCP Server\n\nAn MCP (Model Context Protocol) server for interacting with Zoom Cloud Recording transcripts. This server allows you to list, download, search, and manage your Zoom meeting transcripts through a structured interface.\n\n<a href=\"https://glama.ai/mcp/servers/b01uqjtp7w\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/b01uqjtp7w/badge\" alt=\"Zoom Transcript Server MCP server\" />\n</a>\n\n## Features\n\n- **List Meetings**: View all available Zoom meetings with recordings\n- **Download Transcripts**: Download transcripts from specific meetings by ID or UUID\n- **Get Recent Transcripts**: Automatically download transcripts from recent meetings\n- **Search Transcripts**: Search across all downloaded transcripts for specific content\n- **Organized Storage**: Transcripts are stored in a structured file system by month\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- Zoom Account with Cloud Recording enabled\n- Zoom OAuth App credentials (Account ID, Client ID, Client Secret)\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/zoom_transcript_mcp.git\n   cd zoom_transcript_mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nCreate a `.env` file in the root directory with the following variables:\n\n```\nZOOM_ACCOUNT_ID=your_zoom_account_id\nZOOM_CLIENT_ID=your_zoom_client_id\nZOOM_CLIENT_SECRET=your_zoom_client_secret\nTRANSCRIPTS_DIR=/path/to/transcripts/directory  # Optional, defaults to ./transcripts\n```\n\nAlternatively, you can configure the server through your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"zoom-transcripts\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/zoom-transcripts-server/build/index.js\"],\n      \"env\": {\n        \"ZOOM_ACCOUNT_ID\": \"your_zoom_account_id\",\n        \"ZOOM_CLIENT_ID\": \"your_zoom_client_id\",\n        \"ZOOM_CLIENT_SECRET\": \"your_zoom_client_secret\",\n        \"TRANSCRIPTS_DIR\": \"/path/to/transcripts/directory\"  // Optional\n      }\n    }\n  }\n}\n```\n\n### Obtaining Zoom Credentials\n\n1. Go to the [Zoom App Marketplace](https://marketplace.zoom.us/) and sign in\n2. Click \"Develop\" > \"Build App\"\n3. Choose \"Server-to-Server OAuth\" app type\n4. Fill in the required information\n5. Under \"Scopes\", add the following permissions:\n   - `cloud_recording:read:list_account_recordings:admin`\n   - `cloud_recording:read:recording:admin`\n   - `cloud_recording:read:list_user_recordings:admin`\n6. Save and activate your app\n7. Note your Account ID, Client ID, and Client Secret\n\n## Usage\n\n### Available Tools\n\n#### 1. list_meetings\n\nLists available Zoom meetings with recordings.\n\n```json\n{\n  \"dateRange\": {\n    \"from\": \"2025-01-01\",\n    \"to\": \"2025-03-31\"\n  },\n  \"participant\": \"John Doe\"  // Optional\n}\n```\n\n#### 2. download_transcript\n\nDownloads a transcript for a specific meeting.\n\n```json\n{\n  \"meetingId\": \"123456789\"  // Meeting ID or UUID\n}\n```\n\n#### 3. get_recent_transcripts\n\nDownloads transcripts from recent meetings.\n\n```json\n{\n  \"count\": 5  // Number of recent meetings to fetch (default: 5)\n}\n```\n\n#### 4. search_transcripts\n\nSearches across downloaded transcripts for specific content.\n\n```json\n{\n  \"query\": \"AI discussion\",\n  \"dateRange\": {  // Optional\n    \"from\": \"2025-01-01\",\n    \"to\": \"2025-03-31\"\n  }\n}\n```\n\n### Example Usage with Claude\n\n```\n<use_mcp_tool>\n<server_name>zoom-transcripts</server_name>\n<tool_name>search_transcripts</tool_name>\n<arguments>\n{\n  \"query\": \"project timeline\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Transcript Storage\n\nTranscripts are stored in the following structure:\n\n```\ntranscripts/\n├── YYYY-MM/\n│   ├── YYYY-MM-DD_HH-MM-SS_Meeting-Topic_MeetingID.vtt\n│   └── metadata/\n│       └── YYYY-MM-DD_HH-MM-SS_Meeting-Topic_MeetingID.json\n```\n\nEach transcript has a corresponding metadata JSON file containing:\n- Meeting ID and UUID\n- Topic\n- Start time and duration\n- Participants (extracted from the transcript)\n- File path to the transcript\n\n## Development\n\n### Project Structure\n\n```\nzoom_transcript_mcp/\n├── src/\n│   └── index.ts\n├── package.json\n├── tsconfig.json\n├── .gitignore\n├── README.md\n└── .env.example\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Running Locally\n\n```bash\nnode build/index.js\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zoom_transcript_mcp",
        "zoom",
        "transcripts",
        "zoom_transcript_mcp manage",
        "forayconsulting zoom_transcript_mcp",
        "zoom meeting"
      ],
      "category": "document-processing"
    },
    "gitkenan--doctair": {
      "owner": "gitkenan",
      "name": "doctair",
      "url": "https://github.com/gitkenan/doctair",
      "imageUrl": "/freedevtools/mcp/pfp/gitkenan.webp",
      "description": "Enables editing and deployment of applications through a web interface and local development environment, while synchronizing changes and facilitating project management with custom domain connections.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-22T14:25:22Z",
      "readme_content": "# 🩺 Med AI Insight Viewer\n\n**A web application leveraging AI to analyze medical images and provide insightful descriptions.**\n\nThis application allows users to securely upload medical images (X-rays, MRIs, CT scans, etc.) and receive AI-generated analysis, including potential observations and structured descriptions. It utilizes OpenAI's vision capabilities via Supabase Edge Functions for analysis, along with Supabase for authentication and data persistence.\n\n---\n\n<!-- Add a screenshot or GIF demo here -->\n<!-- ![App Screenshot](link/to/screenshot.png) -->\n\n## ✨ Key Features\n\n*   **Secure User Authentication:** Google OAuth login via Supabase Auth ensures user data privacy.\n*   **Easy Image Upload:** Simple drag-and-drop or file selection interface for medical images.\n*   **AI-Powered Analysis:** Utilizes OpenAI's advanced vision models (e.g., `gpt-4-vision-preview`) to interpret images.\n*   **Structured Results:** Provides analysis in a clear format (e.g., description, potential findings, comments).\n*   **Analysis History:** Stores past analyses for user reference, secured by Row Level Security (RLS).\n*   **Responsive UI:** Built with Shadcn/ui and Tailwind CSS for a clean experience on desktop and mobile.\n\n## 🚀 Tech Stack\n\n*   **Frontend:**\n    *   Framework: React (Vite)\n    *   Language: TypeScript\n    *   UI Library: Shadcn/ui\n    *   Styling: Tailwind CSS\n    *   Routing: React Router DOM (`react-router-dom`)\n    *   State Management: React Context, `useState`, Supabase Auth Helpers\n    *   Notifications: `react-hot-toast` (via `useToast` hook), `sonner`\n    *   Markdown Rendering: `markdown-to-jsx`\n*   **Backend:**\n    *   Platform: Supabase\n    *   Authentication: Supabase Auth (Google OAuth configured)\n    *   Database: Supabase PostgreSQL\n    *   Serverless Functions: Supabase Edge Functions (Deno Runtime)\n*   **AI:**\n    *   Model Provider: OpenAI\n    *   API Interaction: Via Supabase Edge Function\n\n## 📁 Project Structure\n\n.\n├── public/ # Static assets (icons, robots.txt)\n├── src/ # Frontend React application source\n│ ├── components/ # Reusable React components\n│ │ ├── ui/ # Shadcn UI components\n│ │ ├── AnalysisResult.tsx # Displays AI analysis results\n│ │ ├── ApiKeyInput.tsx # (Legacy/Client-side check - Not used for Backend API call)\n│ │ ├── Header.tsx # Application header with navigation/logout\n│ │ ├── HistoryList.tsx # Displays list of past analyses\n│ │ └── ImageUpload.tsx # Handles image selection and preview\n│ ├── hooks/ # Custom React hooks (use-toast, use-mobile)\n│ ├── lib/ # Utility functions (cn)\n│ ├── pages/ # Top-level route components (Index, Dashboard, NotFound)\n│ ├── types/ # TypeScript type definitions\n│ ├── utils/ # Utility functions for external services (openai.ts - calls backend)\n│ ├── App.css # Basic App styles (potentially removable)\n│ ├── App.tsx # Main application component, routing, Supabase context\n│ ├── index.css # Tailwind directives and base styles\n│ ├── main.tsx # Application entry point\n│ └── vite-env.d.ts # Vite TypeScript env declarations\n├── supabase/ # Supabase backend configuration and code\n│ ├── functions/ # Supabase Edge Functions\n│ │ ├── _shared/ # Shared code for functions (cors.ts)\n│ │ └── analyze-image/ # Edge Function for OpenAI image analysis\n│ │ └── index.ts\n│ └── migrations/ # Database schema migrations (.sql)\n├── .gitignore # Git ignore rules\n├── components.json # Shadcn UI configuration\n├── eslint.config.js # ESLint configuration\n├── index.html # Main HTML entry point for Vite\n├── package.json # Project dependencies and scripts\n├── postcss.config.js # PostCSS configuration\n├── README.md # This file\n├── tailwind.config.ts # Tailwind CSS configuration\n├── tsconfig.app.json # TypeScript config for the app\n├── tsconfig.json # Base TypeScript config\n├── tsconfig.node.json # TypeScript config for Node env (Vite config)\n└── vite.config.ts # Vite build configuration\n\n\n## ⚙️ Core Functionality & Workflow\n\n1.  **Authentication (`src/App.tsx`, `src/pages/Index.tsx`):**\n    *   Users land on the `Index` page.\n    *   Clicking \"Get Started\" initiates the Supabase Google OAuth flow.\n    *   Upon successful login, Supabase redirects back to the app (specifically `/dashboard` as configured in the OAuth options).\n    *   The `RequireAuth` component in `App.tsx` verifies the Supabase session using `useSession`. Authenticated users can access `/dashboard`; others are redirected to `/`.\n    *   The `Header` component provides a logout button which calls `supabase.auth.signOut()`.\n\n2.  **Image Upload (`src/pages/Dashboard.tsx`, `src/components/ImageUpload.tsx`):**\n    *   On the `Dashboard`, the `ImageUpload` component allows users to drag & drop or select an image file.\n    *   A preview of the selected image is displayed.\n    *   The selected `File` object and a base64 representation (`imagePreview`) are stored in the `Dashboard` component's state.\n\n3.  **Analysis Process:**\n    *   **Trigger:** The user clicks the \"Analyze Image\" button on the `Dashboard`.\n    *   **Frontend (`src/pages/Dashboard.tsx`, `src/utils/openai.ts`):**\n        *   The `analyzeImage` function in `Dashboard.tsx` is called.\n        *   It calls the utility function `analyzeImageApi` from `src/utils/openai.ts`.\n        *   `analyzeImageApi` gets the current Supabase session token.\n        *   It makes a `POST` request to the Supabase Edge Function endpoint (`/functions/v1/analyze-image`).\n        *   The request includes the `Authorization: Bearer <token>` header and a JSON body containing `imageBase64` and `imageType`.\n        *   It handles the response from the Edge Function.\n    *   **Backend (`supabase/functions/analyze-image/index.ts`):**\n        *   The Edge Function receives the request.\n        *   It validates the incoming JWT using the Supabase client initialized with the user's token.\n        *   It retrieves the **securely stored OpenAI API key** from the Edge Function's environment variables (`Deno.env.get('OPENAI_API_KEY')`). **The client-side key is NOT used here.**\n        *   It formats the `imageBase64` string into a data URL if necessary.\n        *   It constructs a request to the OpenAI API (`gpt-4.1-mini` or similar vision model specified in the function), sending the image URL and a specific prompt asking for medical observations/hypothetical diagnosis.\n        *   It receives the analysis text from OpenAI.\n        *   It structures the response into the `AnalysisResultType` format, adding a timestamp.\n        *   It saves the `imageType` and the structured `result` (as JSONB) to the `users_history` table in the Supabase database, linking it to the authenticated `user_id`.\n        *   It returns the newly created database record (containing the result) to the frontend.\n    *   **Frontend (`src/pages/Dashboard.tsx`):**\n        *   Receives the analysis result from the utility function.\n        *   Updates the `analysisResult` state variable.\n        *   Displays a success toast notification.\n        *   The `AnalysisResult` component re-renders to display the new data.\n\n4.  **Result Display (`src/components/AnalysisResult.tsx`):**\n    *   Renders the `AnalysisResultType` data passed via props.\n    *   Displays the image preview alongside the AI-generated content.\n    *   Uses `markdown-to-jsx` to render the analysis content, allowing for formatted text from the AI.\n    *   Includes a crucial disclaimer about the analysis not being professional medical advice.\n\n5.  **History (`src/pages/Dashboard.tsx`, `src/components/HistoryList.tsx`):**\n    *   The \"History\" tab on the `Dashboard` renders the `HistoryList` component.\n    *   `HistoryList` uses the Supabase JS client (`useSupabaseClient`) to fetch records from the `users_history` table, ordered by creation date.\n    *   Supabase RLS policies ensure only the currently logged-in user's history is returned.\n    *   Displays a list of past analyses, showing image type, a snippet of the diagnosis, and timestamp.\n\n## 💾 Backend Details\n\n### Supabase Edge Function (`analyze-image`)\n\n*   **Purpose:** Securely interacts with the OpenAI API using a server-side secret key and stores results.\n*   **Trigger:** HTTP POST request to `/functions/v1/analyze-image`.\n*   **Authentication:** Requires a valid Supabase JWT in the `Authorization` header.\n*   **Environment Variables:** Requires `SUPABASE_URL`, `SUPABASE_ANON_KEY`, and `OPENAI_API_KEY` to be set in the Edge Function settings.\n*   **Input:** JSON `{ imageBase64: string, imageType: string }`.\n*   **Processing:**\n    1.  Authenticates user via JWT.\n    2.  Retrieves `OPENAI_API_KEY` secret.\n    3.  Calls OpenAI Chat Completions API with vision model.\n    4.  Parses OpenAI response.\n    5.  Inserts result into `users_history` table using the authenticated user's ID.\n*   **Output:** JSON containing the newly created database entry (`{ result: UserHistoryItem }`).\n\n### Supabase Database Schema (`users_history`)\n\n*   **Table:** `public.users_history`\n*   **Purpose:** Stores the results of image analyses linked to users.\n*   **Columns:**\n    *   `id` (uuid, PK): Unique identifier for the history entry.\n    *   `user_id` (uuid, FK -> `auth.users`): Links the entry to the authenticated user.\n    *   `image_url` (text, nullable): *Currently seems unused in the primary analysis flow which uses base64.* Could be used if storing uploaded images directly.\n    *   `image_type` (text, not null): Type of the analyzed image (e.g., \"X-ray\", \"MRI\").\n    *   `result` (jsonb, not null): Stores the structured `AnalysisResultType` object returned by the AI.\n    *   `created_at` (timestamptz, default now()): Timestamp of when the analysis was performed.\n*   **Row Level Security (RLS):**\n    *   **Enabled:** Yes.\n    *   **Policies:**\n        *   Users can `SELECT` only their own history records (`auth.uid() = user_id`).\n        *   Users can `INSERT` only records where `user_id` matches their own `auth.uid()`.\n\n## 🛠️ Getting Started\n\n### Prerequisites\n\n*   Node.js (v18 or later recommended)\n*   npm, yarn, or pnpm\n*   Git\n*   Supabase Account\n*   Supabase CLI (Optional, for local development)\n*   OpenAI API Key\n\n### Installation & Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd med-ai-insight-viewer\n    ```\n\n2.  **Install frontend dependencies:**\n    ```bash\n    npm install\n    # or yarn install or pnpm install\n    ```\n\n3.  **Set up Environment Variables:**\n    *   Create a `.env` file in the root directory.\n    *   Add your Supabase Project URL and Anon Key:\n        ```env\n        VITE_SUPABASE_URL=YOUR_SUPABASE_PROJECT_URL\n        VITE_SUPABASE_ANON_KEY=YOUR_SUPABASE_ANON_KEY\n        ```\n    *   You can find these in your Supabase project settings (Project Settings > API).\n\n4.  **Supabase Setup:**\n    *   **Option A: Supabase Cloud (Recommended for deployment)**\n        1.  Go to your Supabase project dashboard.\n        2.  **Authentication:** Navigate to Authentication > Providers and enable the \"Google\" provider. Add your Google Cloud OAuth credentials. Ensure you add your app's URL(s) (including localhost for development) to the \"Redirect URLs\" section in Supabase Auth settings *and* in your Google Cloud OAuth configuration.\n        3.  **Database:** Navigate to the SQL Editor. Copy the contents of `supabase/migrations/20250421000000_initial_schema.sql` and run it to create the `users_history` table and RLS policies.\n        4.  **Edge Functions:**\n            *   Navigate to Edge Functions.\n            *   Deploy the `analyze-image` function (e.g., using `supabase functions deploy analyze-image --no-verify-jwt` if testing locally first, or set up CI/CD).\n            *   Go to the `analyze-image` function's settings > Secrets and add your `OPENAI_API_KEY`.\n    *   **Option B: Supabase Local Development**\n        1.  Initialize Supabase locally: `supabase init`\n        2.  Start Supabase services: `supabase start`\n        3.  Apply database migrations: `supabase db push` (or link your project `supabase link --project-ref <your-project-ref>` and pull schema changes if needed).\n        4.  Set Edge Function secrets locally: `supabase secrets set OPENAI_API_KEY=YOUR_OPENAI_API_KEY`\n        5.  (You'll need to configure Google Auth locally or use email/password for testing if not using the cloud setup). Use the local Supabase URL/keys in your `.env`.\n\n5.  **Run the Frontend:**\n    ```bash\n    npm run dev\n    ```\n    The application should now be running, typically at `http://localhost:8080`.\n\n6.  **Deploy Edge Function (if not done in step 4):**\n    ```bash\n    # Link to your project if you haven't already\n    # supabase link --project-ref <your-project-ref>\n\n    # Deploy the function\n    supabase functions deploy analyze-image\n\n    # IMPORTANT: Set the secret in the Supabase Dashboard (Settings > Edge Functions > analyze-image > Secrets)\n    # Add OPENAI_API_KEY with your actual OpenAI key value.\n    ```\n\n## 🔧 Configuration\n\n*   **OpenAI Model:** The AI model used for analysis is specified in `supabase/functions/analyze-image/index.ts` (currently hardcoded, likely `gpt-4.1-mini` or similar).\n*   **Analysis Prompt:** The prompt sent to OpenAI is also defined within the `analyze-image` Edge Function. Modify this to change the AI's behavior or the desired output format.\n*   **UI Theme:** Colors and styles can be adjusted in `src/index.css` (CSS variables) and `tailwind.config.ts`.\n*   **Shadcn UI:** Components can be added or customized using the Shadcn CLI and `components.json`.\n\n## 💡 Usage\n\n1.  Open the application in your browser.\n2.  Log in using your Google account.\n3.  Navigate to the \"Analyze Image\" tab.\n4.  Upload a medical image using the drag-and-drop area or the file selector.\n5.  Click the \"Analyze Image\" button.\n6.  Wait for the analysis to complete (a loading indicator will show).\n7.  View the structured results displayed below the upload section.\n8.  Navigate to the \"History\" tab to view past analyses.\n\n## ⚠️ Disclaimer\n\n**This application is for informational and demonstration purposes only. The AI-generated analysis is NOT a substitute for professional medical advice, diagnosis, or treatment.** Always consult with a qualified healthcare provider regarding any medical conditions or concerns. Do not disregard professional medical advice or delay in seeking it because of something you have read or seen using this application.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit issues or pull requests.\n\n1.  Fork the repository.\n2.  Create a new branch (`git checkout -b feature/your-feature-name`).\n3.  Make your changes.\n4.  Commit your changes (`git commit -m 'Add some feature'`).\n5.  Push to the branch (`git push origin feature/your-feature-name`).\n6.  Open a Pull Request.\n\n## 📄 License\n\n(Specify License - e.g., MIT, Apache 2.0. If none, state \"All Rights Reserved.\")",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gitkenan",
        "document",
        "doctair",
        "processing gitkenan",
        "gitkenan doctair",
        "deployment applications"
      ],
      "category": "document-processing"
    },
    "gstarwd--doompdf": {
      "owner": "gstarwd",
      "name": "doompdf",
      "url": "https://github.com/gstarwd/doompdf",
      "imageUrl": "/freedevtools/mcp/pfp/gstarwd.webp",
      "description": "Integrates the classic DOOM game into PDF documents, enabling interactive gameplay within static files via PDF's JavaScript capabilities. This project transforms traditional document formats into innovative gaming platforms while maintaining the essence of classic gaming.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-30T00:37:12Z",
      "readme_content": "# DOOM PDF Project 🎮\n\nWelcome to the DOOM PDF project repository - where we explore the fascinating intersection of classic gaming and document formats by running DOOM inside PDF files!\n\n## About The Project\n\nThis project demonstrates the incredible possibilities of PDF's interactive capabilities by implementing the classic DOOM game within a PDF document. It showcases how traditional document formats can be pushed beyond their conventional boundaries, turning static documents into interactive gaming platforms.\n\n## Live Demos\n\n- [DOOM PDF Main Site](https://doompdf.onl/)\n- [Play DOOM Online](https://doompdf.onl/doom-online/)\n\n## Key Features\n\n- Full DOOM gameplay experience within a PDF document\n- Utilizes PDF's JavaScript capabilities\n- Cross-platform compatibility\n- Innovative approach to game preservation\n- Interactive document demonstration\n\n## Technical Highlights\n\n- Advanced PDF JavaScript implementation\n- Memory management optimization\n- Custom input handling system\n- Performance-optimized rendering\n- Cross-reader compatibility\n\n## Applications & Impact\n\n- Game Preservation\n- Educational Resources\n- Technical Innovation\n- Document Format Evolution\n- Interactive Document Development\n\n## Contributing\n\nWe welcome contributions from the community! Whether you're interested in:\n- Improving performance\n- Adding new features\n- Fixing bugs\n- Documenting the implementation\n- Suggesting improvements\n\nFeel free to open an issue or submit a pull request.\n\n## Related Projects\n\n- DOOM Engine Studies\n- PDF Interactive Features\n- Game Porting Techniques\n- Document Format Innovation\n\n## Acknowledgments\n\n- id Software for creating DOOM\n- The PDF specification developers\n- The gaming preservation community\n- All contributors and supporters\n\n## License\n\nThis project is licensed under [appropriate license] - see the LICENSE file for details.\n\n---\n \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "doompdf",
        "doom",
        "pdf",
        "gstarwd doompdf",
        "doompdf integrates",
        "doom game"
      ],
      "category": "document-processing"
    },
    "gyger--mcp-pyzotero": {
      "owner": "gyger",
      "name": "mcp-pyzotero",
      "url": "https://github.com/gyger/mcp-pyzotero",
      "imageUrl": "/freedevtools/mcp/pfp/gyger.webp",
      "description": "Integrates a local Zotero library with Claude Desktop, enabling direct read access to bibliographic data through a local web API in Zotero 7.",
      "stars": 52,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T14:22:13Z",
      "readme_content": "# Zotero MCP Connector\n\nA Model Control Protocol (MCP) connector for integrating your local Zotero with Claude.  \nThis enables direct read access to your local Zotero library through Claude's Desktop interface.\nIt depends on the ability to access a local web-api in Zotero 7.\n\nThis was inspired by a repository using Node.js and the web api: [mcp-zotero](https://github.com/kaliaboi/mcp-zotero).  \nThis builds on the shoulders of the fantastic [pyzotero](https://github.com/urschrei/pyzotero) library.\n\n## Installation\n\n### Run from local code (Recommended)\nInformation about Claude Desktop interacting with MCPs can be found [here](https://modelcontextprotocol.io/quickstart/user).\n\n1. Use `uv`. Installation instructions can be found [here](https://docs.astral.sh/uv/getting-started/installation/).\n\n2. Checkout the git project to local space and activate the virtual environment inside:\n```bash\ngit clone https://github.com/gyger/mcp-pyzotero.git\ncd mcp-pyzotero\nuv sync\n```\n\n3. Enable the local API in Zotero 7:\n   ![Zotero Local API Settings](assets/LocalAPISettings.png)\n\n4. Add the server to your local Claude installation:\n```bash\nuv run mcp install zotero.py\n```\n\n### Run encapsulated with uvx (Should work)\nEdit the configuration for your Claude Desktop softare in the file.\n\n    - macOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n    - Windows: %APPDATA%\\Claude\\claude_desktop_config.json\n\nand add the Zotero entry\n```json\n{\n    \"mcpServers\": {\n        \"Zotero\": {\n            \"command\": \"uvx\",\n            \"args\": [\"--from\", \"git+https://github.com/gyger/mcp-pyzotero.git\", \n                     \"--with\", \"mcp[cli]\",\n                     \"--with\", \"pyzotero\",\n                     \"mcp\", \"run\", \"zotero.py\"\n                    ],\n        }\n    }\n}\n```\n\n## Configuration\n\nThe connector is configured to work with local Zotero installations and currently only `user` libraries are supported. \nBy default it uses the userid `0`, but you can also set the environment variable `ZOTERO_USER_ID` if needed:\n\n```bash\nuv run mcp install zotero.py -v ZOTERO_USER_ID=0\n```\n\n## Available Functions\n\n### Available tools\n- `get_zotero_summary()`: Lists properties about your library including collections, recent items or tags.\n- `get_collection_items(collection_key)`: Get all items in a specific collection\n- `get_items_metadata(item_key)`: Get detailed information about specific paper(s), including abstract.\n- `search_library(query, mode)`: Search your Zotero library, with two possible modes: everything or titleCreatorYear.\n\nThis functionality should be extended in the future.\n\n## Requirements\n\n- Python 3.10+\n  - pyzotero\n  - mcp[cli]\n- Local Zotero installation\n\n## Contributing\n\nContributions are welcome! Please visit the [GitHub repository](https://github.com/gyger/mcp-pyzotero) to:\n- Report issues\n- Submit pull requests\n- Suggest improvements\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zotero",
        "pyzotero",
        "library",
        "zotero library",
        "api zotero",
        "local zotero"
      ],
      "category": "document-processing"
    },
    "hannesrudolph--mcp-ragdocs": {
      "owner": "hannesrudolph",
      "name": "mcp-ragdocs",
      "url": "https://github.com/hannesrudolph/mcp-ragdocs",
      "imageUrl": "/freedevtools/mcp/pfp/hannesrudolph.webp",
      "description": "Retrieve and process documentation through vector search, enabling AI models to integrate relevant context into their responses. Supports multiple sources and offers semantic search capabilities for enhanced information retrieval.",
      "stars": 228,
      "forks": 27,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:27Z",
      "readme_content": "# RAG Documentation MCP Server\n\nAn MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context.\n\n<a href=\"https://glama.ai/mcp/servers/54hsrjhmq9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/54hsrjhmq9/badge\" alt=\"mcp-ragdocs MCP server\" /></a>\n\n## Features\n\n- Vector-based documentation search and retrieval\n- Support for multiple documentation sources\n- Semantic search capabilities\n- Automated documentation processing\n- Real-time context augmentation for LLMs\n\n## Tools\n\n### search_documentation\nSearch through stored documentation using natural language queries. Returns matching excerpts with context, ranked by relevance.\n\n**Inputs:**\n- `query` (string): The text to search for in the documentation. Can be a natural language query, specific terms, or code snippets.\n- `limit` (number, optional): Maximum number of results to return (1-20, default: 5). Higher limits provide more comprehensive results but may take longer to process.\n\n### list_sources\nList all documentation sources currently stored in the system. Returns a comprehensive list of all indexed documentation including source URLs, titles, and last update times. Use this to understand what documentation is available for searching or to verify if specific sources have been indexed.\n\n### extract_urls\nExtract and analyze all URLs from a given web page. This tool crawls the specified webpage, identifies all hyperlinks, and optionally adds them to the processing queue.\n\n**Inputs:**\n- `url` (string): The complete URL of the webpage to analyze (must include protocol, e.g., https://). The page must be publicly accessible.\n- `add_to_queue` (boolean, optional): If true, automatically add extracted URLs to the processing queue for later indexing. Use with caution on large sites to avoid excessive queuing.\n\n### remove_documentation\nRemove specific documentation sources from the system by their URLs. The removal is permanent and will affect future search results.\n\n**Inputs:**\n- `urls` (string[]): Array of URLs to remove from the database. Each URL must exactly match the URL used when the documentation was added.\n\n### list_queue\nList all URLs currently waiting in the documentation processing queue. Shows pending documentation sources that will be processed when run_queue is called. Use this to monitor queue status, verify URLs were added correctly, or check processing backlog.\n\n### run_queue\nProcess and index all URLs currently in the documentation queue. Each URL is processed sequentially, with proper error handling and retry logic. Progress updates are provided as processing occurs. Long-running operations will process until the queue is empty or an unrecoverable error occurs.\n\n### clear_queue\nRemove all pending URLs from the documentation processing queue. Use this to reset the queue when you want to start fresh, remove unwanted URLs, or cancel pending processing. This operation is immediate and permanent - URLs will need to be re-added if you want to process them later.\n\n## Usage\n\nThe RAG Documentation tool is designed for:\n\n- Enhancing AI responses with relevant documentation\n- Building documentation-aware AI assistants\n- Creating context-aware tooling for developers\n- Implementing semantic documentation search\n- Augmenting existing knowledge bases\n\n## Configuration\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@hannesrudolph/mcp-ragdocs\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"QDRANT_URL\": \"\",\n        \"QDRANT_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nYou'll need to provide values for the following environment variables:\n- `OPENAI_API_KEY`: Your OpenAI API key for embeddings generation\n- `QDRANT_URL`: URL of your Qdrant vector database instance\n- `QDRANT_API_KEY`: API key for authenticating with Qdrant\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Acknowledgments\n\nThis project is a fork of [qpd-v/mcp-ragdocs](https://github.com/qpd-v/mcp-ragdocs), originally developed by qpd-v. The original project provided the foundation for this implementation.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "search",
        "semantic",
        "semantic search",
        "information retrieval",
        "document processing"
      ],
      "category": "document-processing"
    },
    "hanweg--mcp-pdf-tools": {
      "owner": "hanweg",
      "name": "mcp-pdf-tools",
      "url": "https://github.com/hanweg/mcp-pdf-tools",
      "imageUrl": "/freedevtools/mcp/pfp/hanweg.webp",
      "description": "Provides tools for manipulating PDF files, including merging multiple PDFs, extracting specific pages, and finding related PDFs based on text extraction and regex patterns.",
      "stars": 63,
      "forks": 8,
      "license": "The Unlicense",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:04Z",
      "readme_content": "# WORK IN PROGRESS - USE WITH CAUTION - Windows:\n\n# MCP PDF Tools Server\n\nAn MCP (Model Context Protocol) server that provides PDF manipulation tools. This server allows LLMs to perform operations like merging PDFs and extracting pages through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/fqtuoh05xi\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/fqtuoh05xi/badge\" alt=\"mcp-pdf-tools MCP server\" /></a>\n\n## Features\n\n- Merge multiple PDF files into a single PDF\n- Merge multiple PDF files into a single PDF in user specified order\n- Extract specific pages from a PDF file\n- Search PDFs *filesystem search or Everything search works better than this*\n- Find (and merge) related PDFs based on text extraction and regex pattern matching from a target input PDF\n\n## Installation\n\n1. Clone this repository\n2. \n```bash\ncd mcp-pdf-tools\n\n# Create and activate virtual environment\nuv venv\n.venv\\Scripts\\activate\n\n# Install the package\nuv pip install -e .\n```\n\n## Usage with Claude Desktop\n\nAdd this to your Claude Desktop configuration file (claude_desktop_config.json):\n\n```json\n{\n    \"mcpServers\": {\n        \"pdf-tools\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"PATH_TO\\\\mcp-pdf-tools\",\n                \"run\",\n                \"pdf-tools\"\n            ]\n        }\n    }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "pdf",
        "document",
        "pdf tools",
        "pdfs extracting",
        "mcp pdf"
      ],
      "category": "document-processing"
    },
    "heltonteixeira--ragdocs": {
      "owner": "heltonteixeira",
      "name": "ragdocs",
      "url": "https://github.com/heltonteixeira/ragdocs",
      "imageUrl": "/freedevtools/mcp/pfp/heltonteixeira.webp",
      "description": "Manage and search documentation using advanced semantic search and retrieval-augmented generation capabilities. Supports document management tasks such as adding, listing, and deleting documents with automatic text chunking and vector storage through Qdrant.",
      "stars": 16,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-08-21T20:45:35Z",
      "readme_content": "# RagDocs MCP Server\n\nA Model Context Protocol (MCP) server that provides RAG (Retrieval-Augmented Generation) capabilities using Qdrant vector database and Ollama/OpenAI embeddings. This server enables semantic search and management of documentation through vector similarity.\n\n## Features\n\n- Add documentation with metadata\n- Semantic search through documents\n- List and organize documentation\n- Delete documents\n- Support for both Ollama (free) and OpenAI (paid) embeddings\n- Automatic text chunking and embedding generation\n- Vector storage with Qdrant\n\n## Prerequisites\n\n- Node.js 16 or higher\n- One of the following Qdrant setups:\n  - Local instance using Docker (free)\n  - Qdrant Cloud account with API key (managed service)\n- One of the following for embeddings:\n  - Ollama running locally (default, free)\n  - OpenAI API key (optional, paid)\n\n## Available Tools\n\n### 1. add_document\nAdd a document to the RAG system.\n\nParameters:\n- `url` (required): Document URL/identifier\n- `content` (required): Document content\n- `metadata` (optional): Document metadata\n  - `title`: Document title\n  - `contentType`: Content type (e.g., \"text/markdown\")\n\n### 2. search_documents\nSearch through stored documents using semantic similarity.\n\nParameters:\n- `query` (required): Natural language search query\n- `options` (optional):\n  - `limit`: Maximum number of results (1-20, default: 5)\n  - `scoreThreshold`: Minimum similarity score (0-1, default: 0.7)\n  - `filters`:\n    - `domain`: Filter by domain\n    - `hasCode`: Filter for documents containing code\n    - `after`: Filter for documents after date (ISO format)\n    - `before`: Filter for documents before date (ISO format)\n\n### 3. list_documents\nList all stored documents with pagination and grouping options.\n\nParameters (all optional):\n- `page`: Page number (default: 1)\n- `pageSize`: Number of documents per page (1-100, default: 20)\n- `groupByDomain`: Group documents by domain (default: false)\n- `sortBy`: Sort field (\"timestamp\", \"title\", or \"domain\")\n- `sortOrder`: Sort order (\"asc\" or \"desc\")\n\n### 4. delete_document\nDelete a document from the RAG system.\n\nParameters:\n- `url` (required): URL of the document to delete\n\n## Installation\n\n```bash\nnpm install -g @mcpservers/ragdocs\n```\n\n## MCP Server Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"ragdocs\": {\n      \"command\": \"node\",\n      \"args\": [\"@mcpservers/ragdocs\"],\n      \"env\": {\n        \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n        \"EMBEDDING_PROVIDER\": \"ollama\"\n      }\n    }\n  }\n}\n```\n\nUsing Qdrant Cloud:\n```json\n{\n  \"mcpServers\": {\n    \"ragdocs\": {\n      \"command\": \"node\",\n      \"args\": [\"@mcpservers/ragdocs\"],\n      \"env\": {\n        \"QDRANT_URL\": \"https://your-cluster-url.qdrant.tech\",\n        \"QDRANT_API_KEY\": \"your-qdrant-api-key\",\n        \"EMBEDDING_PROVIDER\": \"ollama\"\n      }\n    }\n  }\n}\n```\n\nUsing OpenAI:\n```json\n{\n  \"mcpServers\": {\n    \"ragdocs\": {\n      \"command\": \"node\",\n      \"args\": [\"@mcpservers/ragdocs\"],\n      \"env\": {\n        \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n        \"EMBEDDING_PROVIDER\": \"openai\",\n        \"OPENAI_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## Local Qdrant with Docker\n\n```bash\ndocker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant\n```\n\n## Environment Variables\n\n- `QDRANT_URL`: URL of your Qdrant instance\n  - For local: \"http://127.0.0.1:6333\" (default)\n  - For cloud: \"https://your-cluster-url.qdrant.tech\"\n- `QDRANT_API_KEY`: API key for Qdrant Cloud (required when using cloud instance)\n- `EMBEDDING_PROVIDER`: Choice of embedding provider (\"ollama\" or \"openai\", default: \"ollama\")\n- `OPENAI_API_KEY`: OpenAI API key (required if using OpenAI)\n- `EMBEDDING_MODEL`: Model to use for embeddings\n  - For Ollama: defaults to \"nomic-embed-text\"\n  - For OpenAI: defaults to \"text-embedding-3-small\"\n\n## License\n\nApache License 2.0\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "retrieval",
        "document",
        "document management",
        "document processing",
        "documents automatic"
      ],
      "category": "document-processing"
    },
    "hyperspell--hyperspell-mcp": {
      "owner": "hyperspell",
      "name": "hyperspell-mcp",
      "url": "https://github.com/hyperspell/hyperspell-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hyperspell.webp",
      "description": "Integrate real-time spell checking and correction capabilities into applications to enhance text accuracy and clarity. Offers seamless integration with existing workflows for instant feedback on spelling errors.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-28T21:18:08Z",
      "readme_content": "## Configuration\n\n- `HYPERSPELL_TOKEN` should be a valid user or app token (refer to the [Hyperspell docs](https://docs.hyperspell.com/) for how to obtain a user token).\n- Some MCP clients don't support resources well (looking at you, Claude Desktop), so we can expose them as tools instead. Set `HYPERSPELL_USE_RESOURCES` to `false` (default) to expose everything as tools, `true` to expose retrieveing single documents or listing collections as resources instead, or `both` if you want it all.\n- Optionally, set `HYPERSPELL_COLLECTION` to the name of the collection you want to query and add data to. If not set, it will use the user's default collection instead.\n\n\n## Claude Desktop\n\nNote that Claude needs the absolute path to `uv`, which can be found with `which uv` (it's usually `~/.local/bin/uv`). \n\n```json\n{\n  \"mcpServers\": {\n    \"Hyperspell\": {\n      \"command\": \"/path/to/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"hyperspell\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"/path/to/hyperspell_mcp/server.py\"\n      ],\n      \"env\": {\n        \"HYPERSPELL_TOKEN\": \"<app or user token>\",\n        \"USE_RESOURCES\": \"false\"\n      }\n    }\n  }\n}\n```\n\n## Using the inspector\n\nCreate a `.env` file with the following contents:\n\n```\nHYPERSPELL_TOKEN=...\nHYPERSPELL_USE_RESOURCES=true\n```\n\nThen run this to start the inspector:\n\n```\nuv run mcp dev src/hyperspell_mcp/server.py\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hyperspell",
        "spell",
        "spelling",
        "spell checking",
        "processing hyperspell",
        "hyperspell mcp"
      ],
      "category": "document-processing"
    },
    "inkdropapp--mcp-server": {
      "owner": "inkdropapp",
      "name": "mcp-server",
      "url": "https://github.com/inkdropapp/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/inkdropapp.webp",
      "description": "Retrieve, create, and manage notes with a local HTTP server for efficient note handling in Inkdrop. Access and organize notebooks through a standardized model context protocol.",
      "stars": 40,
      "forks": 8,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-10-03T23:38:29Z",
      "readme_content": "## Inkdrop MCP Server\n\nA [Model Context Protocol](https://github.com/modelcontextprotocol) server for the [Inkdrop Local HTTP Server API](https://developers.inkdrop.app/data-access/local-http-server).\n\n<a href=\"https://glama.ai/mcp/servers/c7fgtnckbv\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/c7fgtnckbv/badge\" alt=\"Inkdrop Server MCP server\" />\n</a>\n\n## Installation\n\n1. [Set up a local HTTP server](https://developers.inkdrop.app/guides/integrate-with-external-programs)\n\n2. Add server config to Claude Desktop:\n   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"inkdrop\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@inkdropapp/mcp-server\"],\n      \"env\": {\n        \"INKDROP_LOCAL_SERVER_URL\": \"http://localhost:19840\",\n        \"INKDROP_LOCAL_USERNAME\": \"your-local-server-username\",\n        \"INKDROP_LOCAL_PASSWORD\": \"your-local-server-password\"\n      }\n    }\n  }\n}\n```\n\n## Components\n\n### Tools\n\n1. **`read-note`**: Retrieve the complete contents of the note by its ID from the database.\n   - Required inputs:\n     - `noteId`: The ID of the note to retrieve. It can be found as `_id` in the note docs. It always starts with `note:`.\n2. **`search-notes`**: List all notes that contain a given keyword.\n   - Required inputs:\n     - `keyword`: Keyword to search for.\n   - Note: Results include truncated note bodies (200 characters). Use `read-note` to get full content.\n   - Supports advanced search qualifiers like `book:`, `tag:`, `status:`, `title:`, etc.\n3. **`list-notes`**: List all notes with specified conditions.\n   - Required inputs:\n     - `bookId`: The notebook ID. It always starts with 'book:'.\n   - Optional inputs:\n     - `tagIds`: An array of tag IDs to filter. Each starts with 'tag:'.\n     - `keyword`: Keyword to filter notes.\n     - `sort`: Sort field (`updatedAt`, `createdAt`, or `title`). Default: `updatedAt`.\n     - `descending`: Reverse the order of output. Default: `true`.\n   - Note: Results include truncated note bodies (200 characters). Use `read-note` to get full content.\n4. **`create-note`**: Create a new note in the database.\n   - Required inputs:\n     - `bookId`: The notebook ID. Must start with 'book:' or be 'trash'.\n     - `title`: The note title.\n     - `body`: The content of the note in Markdown.\n   - Optional inputs:\n     - `status`: The note status (`none`, `active`, `onHold`, `completed`, `dropped`).\n     - `tags`: An array of tag IDs to assign to the note. Each must start with 'tag:'.\n5. **`update-note`**: Update an existing note in the database.\n   - Required inputs:\n     - `_id`: The note ID. Must start with 'note:'.\n     - `_rev`: The revision ID (CouchDB MVCC-token).\n     - `bookId`: The notebook ID. Must start with 'book:' or be 'trash'.\n     - `title`: The note title.\n     - `body`: The content of the note in Markdown.\n   - Optional inputs:\n     - `status`: The note status (`none`, `active`, `onHold`, `completed`, `dropped`).\n     - `tags`: An array of tag IDs to assign to the note. Each must start with 'tag:'.\n6. **`list-notebooks`**: Retrieve a list of all notebooks.\n7. **`read-book`**: Retrieve a single notebook by its ID.\n   - Required inputs:\n     - `bookId`: The notebook ID. Must start with 'book:'.\n8. **`list-tags`**: Retrieve a list of all tags.\n9. **`read-tag`**: Retrieve a single tag by its ID.\n   - Required inputs:\n     - `tagId`: The tag ID. Must start with 'tag:'.\n10. **`create-tag`**: Create a new tag in the database.\n    - Required inputs:\n      - `name`: The name of the tag.\n    - Optional inputs:\n      - `color`: The color type of the tag (`default`, `red`, `orange`, `yellow`, `olive`, `green`, `teal`, `blue`, `violet`, `purple`, `pink`, `brown`, `grey`, `black`). Default: `default`.\n11. **`update-tag`**: Update an existing tag in the database.\n    - Required inputs:\n      - `_id`: The tag ID. Must start with 'tag:'.\n      - `_rev`: The revision ID (CouchDB MVCC-token).\n      - `name`: The name of the tag.\n    - Optional inputs:\n      - `color`: The color type of the tag. Default: `default`.\n\n## Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector \"./dist/index.js\"\n```\n\nBe sure that environment variables are properly configured.\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n\nYou can also watch the server logs with this command:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp-server-inkdrop.log\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "inkdropapp",
        "inkdrop",
        "notes",
        "inkdropapp mcp",
        "processing inkdropapp",
        "inkdrop access"
      ],
      "category": "document-processing"
    },
    "intsig-textin--textin-mcp": {
      "owner": "intsig-textin",
      "name": "textin-mcp",
      "url": "https://github.com/intsig-textin/textin-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/intsig-textin.webp",
      "description": "Extract text from images, PDFs, and Word documents while performing OCR and document conversion tasks. Convert documents to Markdown format, and retrieve key information from files intelligently.",
      "stars": 23,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-25T07:37:52Z",
      "readme_content": "# TextIn OCR MCP\n<p align=\"center\">\n<img align=\"center\" src=\"https://ccidownload.blob.core.chinacloudapi.cn/download/2025/LLMS/logo.png\" width=\"800\" alt=\"TextIn\">\n</p>\n\nEnglish | [中文](./README_CHS.md)\n\n## TextIn OCR MCP Server\n\nTextIn MCP Server is a tool for extracting text and performing OCR on documents, including document text recognition, ID recognition, and invoice recognition. It also supports converting documents into Markdown format.\n\n<!-- <a href=\"https://glama.ai/mcp/servers/@intsig-textin/textin-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@intsig-textin/textin-mcp/badge\" alt=\"Textin Server MCP server\" />\n</a> -->\n\n### Tools\n- `recognition_text`\n  - Text recognition from images, Word documents, and PDF files.\n  - Inputs:\n    - `path` (string, required): `file path` or `a URL (HTTP/HTTPS) pointing to a document`\n  - Return: Text of the document.\n  - Supports conversion for:\n    - PDF\n    - Image (Jpeg, Jpg, Png, Bmp)\n\n- `doc_to_markdown`\n  - Convert images, PDFs, and Word documents to Markdown.\n  - Inputs:\n    - `path` (string, required): `file path` or `a URL (HTTP/HTTPS) pointing to a document`\n  - Return: Markdown of the document.\n  - Supports conversion for:\n    - PDF\n    - Microsoft Office Documents (Word, Excel)\n    - Image (Jpeg, Jpg, Png, Bmp)\n\n- `general_information_extration`\n  - Automatically identify and extract information from documents, or identify and extract user-specified information.\n  - Inputs:\n    - `path` (string, required): `file path` or `a URL (HTTP/HTTPS) pointing to a document`\n    - `key` (string[], optional): The non-tabular text information that the user wants to identify, input format is an array of strings.\n    - `table_header` (string[], optional): The table information that the user wants to identify, input format is an array of strings.\n  - Return: The key information JSON.\n  - Supports conversion for:\n    - PDF\n    - Microsoft Office Documents (Word, Excel)\n    - Image (Jpeg, Jpg, Png, Bmp)\n\nWhen the input is a URL, it does not support handling access to protected resources.\n\n## Setup\n\n### APP_ID and APP_SECRET\n\nClick [here](https://www.textin.com/user/login?from=github_mcp) to register for a TextIn account.\n\nGet Textin APP_ID and APP_SECRET by following the instructions [here](https://www.textin.com/doc/guide/account/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96app%20id?status=first).\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"textin-ocr\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@intsig/server-textin\"\n      ],\n      \"env\": {\n        \"APP_ID\": \"<YOUR_APP_ID>\",\n        \"APP_SECRET\": \"<YOUR_APP_SECRET>\",\n        \"MCP_SERVER_REQUEST_TIMEOUT\": \"600000\"\n      },\n      \"timeout\": 600\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "textin",
        "markdown",
        "document processing",
        "ocr document",
        "document conversion"
      ],
      "category": "document-processing"
    },
    "jbchouinard--mcp-document-reader": {
      "owner": "jbchouinard",
      "name": "mcp-document-reader",
      "url": "https://github.com/jbchouinard/mcp-document-reader",
      "imageUrl": "/freedevtools/mcp/pfp/jbchouinard.webp",
      "description": "Interact with PDF and EPUB documents, enabling reading and processing tasks within an IDE. Supports seamless handling of document content directly within the development environment.",
      "stars": 7,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-16T17:07:43Z",
      "readme_content": "# mcp-document-reader\n\nA rudimentary [MCP server](https://modelcontextprotocol.io/introduction) for interacting with PDF and EPUB documents.\n\nI use this with [Windsurf IDE by Codeium](https://codeium.com/windsurf), which\nonly supports MCP tools, not resources.\n\n## Installation\n\n### Requirements\n\n- [Python 3.11+](https://www.python.org/downloads/)\n- [Poetry](https://python-poetry.org/docs/)\n\n```bash\n# Clone the repository\ngit clone https://github.com/jbchouinard/mcp-document-reader.git\ncd mcp-document-reader\npoetry install\n```\n\n## Configure MCP Server\n\nRun with poetry:\n\n```json\n{\n  \"mcpServers\": {\n    \"documents\": {\n      \"command\": \"poetry\",\n      \"args\": [\"-C\", \"path/to/mcp-document-reader\", \"run\", \"mcp-document-reader\"]\n    }\n  }\n}\n```\n\nAlternatively, build and install with pip, then run the script directly:\n\n```bash\npoetry build\npipx install dist/*.whl\nwhich mcp-document-reader\n```\n\nThen use the following config, with the path output by which:\n\n```json\n{\n  \"mcpServers\": {\n    \"documents\": {\n      \"command\": \"/path/to/mcp-document-reader\",\n      \"args\": []\n    }\n  }\n}\n```\n\n## Development\n\n### Setup\n\n```bash\n# Install dependencies\npoetry install\n```\n\n### Testing\n\n```bash\npoetry run pytest\n```\n\n### Linting\n\n```bash\npoetry run ruff check --fix .\npoetry run ruff format .\n```\n\n## License\n\n[MIT](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "reader",
        "document",
        "epub",
        "document reader",
        "epub documents",
        "document processing"
      ],
      "category": "document-processing"
    },
    "jinzcdev--markmap-mcp-server": {
      "owner": "jinzcdev",
      "name": "markmap-mcp-server",
      "url": "https://github.com/jinzcdev/markmap-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jinzcdev.webp",
      "description": "Converts Markdown text into interactive mind maps, supporting export in various image formats including PNG, JPG, and SVG. Features include zooming, node expansion, automatic browser preview, and one-click Markdown copying.",
      "stars": 117,
      "forks": 18,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T23:14:25Z",
      "readme_content": "# Markmap MCP Server\n\n![Sample Mindmap](./docs/markmap.svg)\n\n[![NPM Version](https://img.shields.io/npm/v/@jinzcdev/markmap-mcp-server.svg)](https://www.npmjs.com/package/@jinzcdev/markmap-mcp-server)\n[![GitHub License](https://img.shields.io/github/license/jinzcdev/markmap-mcp-server.svg)](LICENSE)\n[![Smithery Badge](https://smithery.ai/badge/@jinzcdev/markmap-mcp-server)](https://smithery.ai/server/@jinzcdev/markmap-mcp-server)\n[![中文文档](https://img.shields.io/badge/中文文档-点击查看-blue)](README_zh-CN.md)\n[![Stars](https://img.shields.io/github/stars/jinzcdev/markmap-mcp-server)](https://github.com/jinzcdev/markmap-mcp-server)\n\nMarkmap MCP Server is based on the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) that allows one-click conversion of Markdown text to interactive mind maps, built on the open source project [markmap](https://github.com/markmap/markmap). The generated mind maps support rich interactive operations and can be exported in various image formats.\n\n> 🎉 **Explore More Mind Mapping Tools**\n>\n> Try [MarkXMind](https://github.com/jinzcdev/markxmind) - An online editor that creates complex mind maps using simple XMindMark syntax. It supports real-time preview, multi-format export (.xmind/.svg/.png), importing existing XMind files. [Try it now](https://markxmind.js.org/)!\n\n## Features\n\n- 🌠 **Markdown to Mind Map**: Convert Markdown text to interactive mind maps\n- 🖼️ **Multi-format Export**: Support for exporting as PNG, JPG, and SVG images\n- 🔄 **Interactive Operations**: Support for zooming, expanding/collapsing nodes, and other interactive features\n- 📋 **Markdown Copy**: One-click copy of the original Markdown content\n- 🌐 **Automatic Browser Preview**: Option to automatically open generated mind maps in the browser\n\n## Prerequisites\n\n1. Node.js (v20 or above)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markmap MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jinzcdev/markmap-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @jinzcdev/markmap-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Install from npm\nnpm install @jinzcdev/markmap-mcp-server -g\n\n# Basic run\nnpx -y @jinzcdev/markmap-mcp-server\n\n# Specify output directory\nnpx -y @jinzcdev/markmap-mcp-server --output /path/to/output/directory\n```\n\nAlternatively, you can clone the repository and run locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/jinzcdev/markmap-mcp-server.git\n\n# Navigate to the project directory\ncd markmap-mcp-server\n\n# Build project\nnpm install && npm run build\n\n# Run the server\nnode build/index.js\n```\n\n## Usage\n\nAdd the following configuration to your MCP client configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"markmap\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jinzcdev/markmap-mcp-server\"],\n      \"env\": {\n        \"MARKMAP_DIR\": \"/path/to/output/directory\"\n      }\n    }\n  }\n}\n```\n\n> [!TIP]\n>\n> The service supports the following environment variables:\n>\n> - `MARKMAP_DIR`: Specify the output directory for mind maps (optional, defaults to system temp directory)\n>\n> **Priority Note**:\n>\n> When both the `--output` command line argument and the `MARKMAP_DIR` environment variable are specified, the command line argument takes precedence.\n\n## Available Tools\n\n### markdown-to-mindmap\n\nConvert Markdown text into an interactive mind map.\n\n**Parameters:**\n\n- `markdown`: The Markdown content to convert (required string)\n- `open`: Whether to automatically open the generated mind map in the browser (optional boolean, default is false)\n\n**Return Value:**\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"JSON_DATA_OF_MINDMAP_FILEPATH\"\n    }\n  ]\n}\n```\n\n## License\n\nThis project is licensed under the [MIT](./LICENSE) License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "markmap",
        "mcp",
        "jinzcdev markmap",
        "markmap mcp",
        "converts markdown"
      ],
      "category": "document-processing"
    },
    "jjgordon89--document-qa": {
      "owner": "jjgordon89",
      "name": "document-qa",
      "url": "https://github.com/jjgordon89/document-qa",
      "imageUrl": "/freedevtools/mcp/pfp/jjgordon89.webp",
      "description": "A Streamlit app for answering questions about uploaded documents using GPT-3.5, enabling users to extract information quickly and efficiently. Ideal for enhancing productivity in document analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-03-07T03:57:02Z",
      "readme_content": "# 📄 Document question answering template\n\nA simple Streamlit app that answers questions about an uploaded document via OpenAI's GPT-3.5.\n\n[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://document-question-answering-template.streamlit.app/)\n\n### How to run it on your own machine\n\n1. Install the requirements\n\n   ```\n   $ pip install -r requirements.txt\n   ```\n\n2. Run the app\n\n   ```\n   $ streamlit run streamlit_app.py\n   ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "documents",
        "streamlit",
        "document processing",
        "document analysis",
        "qa streamlit"
      ],
      "category": "document-processing"
    },
    "jkawamoto--mcp-youtube-transcript": {
      "owner": "jkawamoto",
      "name": "mcp-youtube-transcript",
      "url": "https://github.com/jkawamoto/mcp-youtube-transcript",
      "imageUrl": "/freedevtools/mcp/pfp/jkawamoto.webp",
      "description": "Retrieve transcripts from YouTube videos.",
      "stars": 85,
      "forks": 30,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T01:58:53Z",
      "readme_content": "# YouTube Transcript MCP Server\n[![Python Application](https://github.com/jkawamoto/mcp-youtube-transcript/actions/workflows/python-app.yaml/badge.svg)](https://github.com/jkawamoto/mcp-youtube-transcript/actions/workflows/python-app.yaml)\n[![GitHub License](https://img.shields.io/github/license/jkawamoto/mcp-youtube-transcript)](https://github.com/jkawamoto/mcp-youtube-transcript/blob/main/LICENSE)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![smithery badge](https://smithery.ai/badge/@jkawamoto/mcp-youtube-transcript)](https://smithery.ai/server/@jkawamoto/mcp-youtube-transcript)\n[![Dockerhub](https://img.shields.io/badge/Docker-mcp%2Fyoutube--transcript-blue.svg)](https://hub.docker.com/mcp/server/youtube_transcript)\n\nThis MCP server retrieves transcripts for given YouTube video URLs.\n\n<a href=\"https://glama.ai/mcp/servers/of3kwtmlqp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/of3kwtmlqp/badge\" alt=\"YouTube Transcript Server MCP server\" /></a>\n\n## Tools\nThis MCP server provides the following tools:\n\n### `get_transcript`\nFetches the transcript of a specified YouTube video.\n\n#### Parameters\n- **url** *(string)*: The full URL of the YouTube video. This field is required.\n- **lang** *(string, optional)*: The desired language for the transcript. Defaults to `en` if not specified.\n- **next_cursor** *(string, optional)*: Cursor to retrieve the next page of the transcript.\n\n### `get_timed_transcript`\nFetches the transcript of a specified YouTube video with timestamps..\n\n#### Parameters\n- **url** *(string)*: The full URL of the YouTube video. This field is required.\n- **lang** *(string, optional)*: The desired language for the transcript. Defaults to `en` if not specified.\n- **next_cursor** *(string, optional)*: Cursor to retrieve the next page of the transcript.\n\n### `get_video_info`\nFetches the metadata of a specified YouTube video.\n\n#### Parameters\n- **url** *(string)*: The full URL of the YouTube video. This field is required.\n\n## Installation\n> [!NOTE]\n> You'll need [`uv`](https://docs.astral.sh/uv) installed on your system to use `uvx` command.\n\n### For codename goose\nPlease refer to this tutorial for detailed installation instructions:\n[YouTube Transcript Extension](https://block.github.io/goose/docs/mcp/youtube-transcript-mcp).\n\n### For Claude Desktop\n\nDownload the latest MCP bundle `mcp-youtube-transcript.mcpb` from\nthe [Releases](https://github.com/jkawamoto/mcp-youtube-transcript/releases) page,\nthen open the downloaded `.mcpb `file or drag it into the Claude Desktop's Settings window.\n\nYou can also manually configure this server for Claude Desktop.\nEdit the `claude_desktop_config.json` file by adding the following entry under\n`mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-transcript\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/jkawamoto/mcp-youtube-transcript\",\n        \"mcp-youtube-transcript\"\n      ]\n    }\n  }\n}\n```\nAfter editing, restart the application.\nFor more information,\nsee: [For Claude Desktop Users - Model Context Protocol](https://modelcontextprotocol.io/quickstart/user).\n\n### For LM Studio\nTo configure this server for LM Studio, click the button below.\n\n[![Add MCP Server youtube-transcript to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=youtube-transcript&config=eyJjb21tYW5kIjoidXZ4IiwiYXJncyI6WyItLWZyb20iLCJnaXQraHR0cHM6Ly9naXRodWIuY29tL2prYXdhbW90by9tY3AteW91dHViZS10cmFuc2NyaXB0IiwibWNwLXlvdXR1YmUtdHJhbnNjcmlwdCJdfQ%3D%3D)\n\n### Using Docker\n\nA Docker image for this server is available on [Docker Hub](https://hub.docker.com/mcp/server/youtube_transcript/).\nPlease refer to the Docker Hub page for detailed usage instructions and documentation.\n\n### Installing via Smithery\n> [!NOTE]\n> When using this method, you will be using servers hosted by Smithery.\n> Requests and responses will be routed through their servers.\n> Please refer to the [Smithery Privacy Notice](https://smithery.ai/privacy) for information\n> about their data handling practices.\n\nThe [Smithery CLI](https://github.com/smithery-ai/cli) enables the installation of MCP servers on various clients.\n\nFor instance, to install this server for Claude Desktop, execute the following command:\n\n```bash\nnpx -y @smithery/cli install @jkawamoto/mcp-youtube-transcript --client claude\n```\n\nTo view the list of clients supported by the Smithery CLI, use this command:\n\n```bash\nnpx -y @smithery/cli list clients\n```\n\nRefer to the [Smithery CLI documentation](https://github.com/smithery-ai/cli) for additional details.\n\n## Response Pagination\nWhen retrieving transcripts for longer videos, the content may exceed the token size limits of the LLM.\nTo avoid this issue, this server splits transcripts that exceed 50,000 characters.\nIf a transcript is split, the response will include a `next_cursor`.\nTo retrieve the next part, include this `next_cursor` value in your request.\n\nThe token size limits vary depending on the LLM and language you are using.\nIf you need to split responses into smaller chunks,\nyou can adjust this using the `--response-limit` command line argument.\nFor example, the configuration below splits responses to contain no more than 15,000 characters each:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-transcript\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/jkawamoto/mcp-youtube-transcript\",\n        \"mcp-youtube-transcript\",\n        \"--response-limit\",\n        \"15000\"\n      ]\n    }\n  }\n}\n```\n\n## Using Proxy Servers\nIn environments where access to YouTube is restricted, you can use proxy servers.\n\nWhen using [Webshare](https://www.webshare.io/), set the username and password for the Residential Proxy using either\nthe environment variables `WEBSHARE_PROXY_USERNAME` and `WEBSHARE_PROXY_PASSWORD`,\nor the command line arguments `--webshare-proxy-username` and `--webshare-proxy-password`.\n\nWhen using other proxy servers, set the proxy server URL using either the environment variables `HTTP_PROXY` or\n`HTTPS_PROXY`, or the command line arguments `--http-proxy` or `--https-proxy`.\n\nFor more details, please visit:\n[Working around IP bans - YouTube Transcript API](https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n\n## License\n\nThis application is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcripts",
        "transcript",
        "jkawamoto",
        "transcripts youtube",
        "youtube transcript",
        "retrieve transcripts"
      ],
      "category": "document-processing"
    },
    "jkf87--hwp-mcp": {
      "owner": "jkf87",
      "name": "hwp-mcp",
      "url": "https://github.com/jkf87/hwp-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jkf87.webp",
      "description": "Control and manage HWP (Hangul Word Processor) documents by creating, editing, and automating tasks through AI models. It offers features like text editing, table manipulation, and batch processing of documents.",
      "stars": 159,
      "forks": 38,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-28T09:37:48Z",
      "readme_content": "# HWP-MCP (한글 Model Context Protocol)\n\n[![GitHub](https://img.shields.io/github/license/jkf87/hwp-mcp)](https://github.com/jkf87/hwp-mcp)\n\nHWP-MCP는 한글 워드 프로세서(HWP)를 Claude와 같은 AI 모델이 제어할 수 있도록 해주는 Model Context Protocol(MCP) 서버입니다. 이 프로젝트는 한글 문서를 자동으로 생성, 편집, 조작하는 기능을 AI에게 제공합니다.\n\n## 주요 기능\n\n- 문서 생성 및 관리: 새 문서 생성, 열기, 저장 기능\n- 텍스트 편집: 텍스트 삽입, 글꼴 설정, 단락 추가\n- 테이블 작업: 테이블 생성, 데이터 채우기, 셀 내용 설정\n- 완성된 문서 생성: 템플릿 기반 보고서 및 편지 자동 생성\n- 일괄 작업: 여러 작업을 한 번에 실행하는 배치 기능\n\n## 시스템 요구사항\n\n- Windows 운영체제\n- 한글(HWP) 프로그램 설치\n- Python 3.7 이상\n- 필수 Python 패키지 (requirements.txt 참조)\n\n## 설치 방법\n\n1. 저장소 클론:\n```bash\ngit clone https://github.com/jkf87/hwp-mcp.git\ncd hwp-mcp\n```\n\n2. 의존성 설치:\n```bash\npip install -r requirements.txt\n```\n\n3. (선택사항) MCP 패키지 설치:\n```bash\npip install mcp\n```\n\n## 사용 방법\n\n### Claude와 함께 사용하기\n\nClaude 데스크톱 설정 파일에 다음과 같이 HWP-MCP 서버를 등록하세요:\n\n```json\n{\n  \"mcpServers\": {\n    \"hwp\": {\n      \"command\": \"python\",\n      \"args\": [\"경로/hwp-mcp/hwp_mcp_stdio_server.py\"]\n    }\n  }\n}\n```\n\n### 주요 기능 예시\n\n#### 새 문서 생성\n```python\nhwp_create()\n```\n\n#### 텍스트 삽입\n```python\nhwp_insert_text(\"원하는 텍스트를 입력하세요.\")\n```\n\n#### 테이블 생성 및 데이터 입력\n```python\n# 테이블 생성\nhwp_insert_table(rows=5, cols=2)\n\n# 테이블에 데이터 채우기\nhwp_fill_table_with_data([\n    [\"월\", \"판매량\"], \n    [\"1월\", \"120\"], \n    [\"2월\", \"150\"], \n    [\"3월\", \"180\"], \n    [\"4월\", \"200\"]\n], has_header=True)\n\n# 표에 연속된 숫자 채우기\nhwp_fill_column_numbers(start=1, end=10, column=1, from_first_cell=True)\n```\n\n#### 문서 저장\n```python\nhwp_save(\"경로/문서명.hwp\")\n```\n\n#### 일괄 작업 예시\n```python\nhwp_batch_operations([\n    {\"operation\": \"hwp_create\"},\n    {\"operation\": \"hwp_insert_text\", \"params\": {\"text\": \"제목\"}},\n    {\"operation\": \"hwp_set_font\", \"params\": {\"size\": 20, \"bold\": True}},\n    {\"operation\": \"hwp_save\", \"params\": {\"path\": \"경로/문서명.hwp\"}}\n])\n```\n\n## 프로젝트 구조\n\n```\nhwp-mcp/\n├── hwp_mcp_stdio_server.py  # 메인 서버 스크립트\n├── requirements.txt         # 의존성 패키지 목록\n├── hwp-mcp-구조설명.md       # 프로젝트 구조 설명 문서\n├── src/\n│   ├── tools/\n│   │   ├── hwp_controller.py  # 한글 제어 핵심 컨트롤러\n│   │   └── hwp_table_tools.py # 테이블 관련 기능 전문 모듈\n│   ├── utils/                 # 유틸리티 함수\n│   └── __tests__/             # 테스트 모듈\n└── security_module/\n    └── FilePathCheckerModuleExample.dll  # 보안 모듈\n```\n\n## 트러블슈팅\n\n### 보안 모듈 관련 문제\n기본적으로 한글 프로그램은 외부에서 파일 접근 시 보안 경고를 표시합니다. 이를 우회하기 위해 `FilePathCheckerModuleExample.dll` 모듈을 사용합니다. 만약 보안 모듈 등록에 실패해도 기능은 작동하지만, 파일 열기/저장 시 보안 대화 상자가 표시될 수 있습니다.\n\n### 한글 연결 실패\n한글 프로그램이 실행 중이지 않을 경우 연결에 실패할 수 있습니다. 한글 프로그램이 설치되어 있고 정상 작동하는지 확인하세요.\n\n### 테이블 데이터 입력 문제\n테이블에 데이터를 입력할 때 커서 위치가 예상과 다르게 동작하는 경우가 있었으나, 현재 버전에서는 이 문제가 해결되었습니다. 테이블의 모든 셀에 정확하게 데이터가 입력됩니다.\n\n## 변경 로그\n\n### 2025-03-27\n- 표 생성 및 데이터 채우기 기능 개선\n  - 표 안에 표가 중첩되는 문제 해결\n  - 표 생성과 데이터 채우기 기능 분리\n  - 표 생성 전 현재 커서 위치 확인 로직 추가\n  - 기존 표에 데이터만 채우는 기능 개선\n- 프로젝트 관리 개선\n  - .gitignore 파일 추가 (임시 파일, 캐시 파일 등 제외)\n\n### 2025-03-25\n- 테이블 데이터 입력 기능 개선\n  - 첫 번째 셀부터 정확하게 데이터 입력 가능\n  - 셀 선택 및 커서 위치 설정 로직 개선\n  - 텍스트 입력 시 커서 위치 유지 기능 추가\n- 테이블 전용 도구 모듈(`hwp_table_tools.py`) 추가\n- `hwp_fill_column_numbers` 함수에 `from_first_cell` 옵션 추가\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스에 따라 배포됩니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.\n\n## 기여 방법\n\n1. 이슈 제보 또는 기능 제안: GitHub 이슈를 사용하세요.\n2. 코드 기여: 변경사항을 포함한 Pull Request를 제출하세요.\n\n## 관련 프로젝트\n\n- [HWP SDK](https://www.hancom.com/product/sdk): 한글과컴퓨터의 공식 SDK\n- [Cursor MCP](https://docs.cursor.com/context/model-context-protocol#configuration-locations)\n- [Smithery](https://smithery.ai/server/@jkf87/hwp-mcp)\n\n## 연락처\n\n프로젝트 관련 문의는 GitHub 이슈, [코난쌤](https://www.youtube.com/@conanssam)를 통해 해주세요. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hangul",
        "processing",
        "editing",
        "hwp hangul",
        "document processing",
        "word processor"
      ],
      "category": "document-processing"
    },
    "kajirita2002--esa-mcp-server": {
      "owner": "kajirita2002",
      "name": "esa-mcp-server",
      "url": "https://github.com/kajirita2002/esa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/kajirita2002.webp",
      "description": "Integrate Claude AI with the esa API to manage documents efficiently by performing operations such as searching, creating, and updating documents.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-01T14:42:40Z",
      "readme_content": "# esa MCP Server\n\n<img width=\"775\" alt=\"スクリーンショット 2025-03-27 午後1 14 09\" src=\"https://github.com/user-attachments/assets/e5f8f308-ed7a-4774-b3a3-9cc284ea7422\" />\n\n\n*Read this in [Japanese](README.ja.md)*\n\n## Overview\n\nThis server is an interface that uses the [Model Context Protocol (MCP)](https://github.com/anthropics/anthropic-cookbook/tree/main/model_context_protocol) to enable Claude AI to interact with the [esa API](https://docs.esa.io/posts/102).\n\nWith this MCP server, Claude AI can perform operations such as searching, creating, and updating esa documents.\n\n<a href=\"https://glama.ai/mcp/servers/@kajirita2002/esa-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kajirita2002/esa-mcp-server/badge\" alt=\"esa Server MCP server\" />\n</a>\n\n## About the Repository\n\nThis repository provides a standalone implementation of the esa MCP server. It integrates Claude AI with esa to streamline document management.\n\n## Setup\n\n### Prerequisites\n\n- Node.js 18 or higher\n- esa API access token\n- esa team name\n\n### Installation\n\n```bash\n# Install globally\nnpm install -g @kajirita2002/esa-mcp-server\n\n# Or use directly with npx\nnpx @kajirita2002/esa-mcp-server\n```\n\n### Setting Environment Variables\n\n```bash\n# Set environment variables\nexport ESA_ACCESS_TOKEN=\"your_esa_access_token\"\nexport ESA_TEAM=\"your_team_name\"\n```\n\n### MCP Configuration Example\n\nIf you're using this MCP server, add the following configuration to your `mcp_config.json` file:\n\n```json\n\"esa\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@kajirita2002/esa-mcp-server\"],\n  \"env\": {\n    \"ESA_ACCESS_TOKEN\": \"your_esa_access_token\",\n    \"ESA_TEAM\": \"your_team_name\"\n  }\n}\n```\n\n### Starting the Server\n\n```bash\n# Start the server\nnpm start\n```\n\n## Available Tools\n\nThis MCP server provides the following tools:\n\n### Post Related\n\n1. `esa_list_posts`\n   - Get a list of posts in the team\n   - Input:\n     - `q` (string, optional): Search query\n     - `include` (string, optional): Related data to include in the response (e.g. 'comments,stargazers')\n     - `sort` (string, optional): Sort method (updated, created, number, stars, watches, comments, best_match)\n     - `order` (string, optional): Sort order (desc, asc)\n     - `per_page` (number, optional): Number of results per page (max: 100)\n     - `page` (number, optional): Page number to retrieve\n\n2. `esa_get_post`\n   - Get detailed information about a specific post\n   - Input:\n     - `post_number` (number, required): Post number to retrieve\n     - `include` (string, optional): Related data to include in the response (e.g. 'comments,stargazers')\n\n3. `esa_create_post`\n   - Create a new post\n   - Input:\n     - `name` (string, required): Post title\n     - `body_md` (string, optional): Post body (Markdown format)\n     - `tags` (array of string, optional): List of tags for the post\n     - `category` (string, optional): Post category\n     - `wip` (boolean, optional, default: true): Whether to mark as WIP (Work In Progress)\n     - `message` (string, optional): Change message\n     - `user` (string, optional): Poster's screen_name (only team owners can specify)\n     - `template_post_id` (number, optional): ID of the post to use as a template\n\n4. `esa_update_post`\n   - Update an existing post\n   - Input:\n     - `post_number` (number, required): Post number to update\n     - `name` (string, optional): New title for the post\n     - `body_md` (string, optional): New body for the post (Markdown format)\n     - `tags` (array of string, optional): New list of tags for the post\n     - `category` (string, optional): New category for the post\n     - `wip` (boolean, optional): Whether to mark as WIP (Work In Progress)\n     - `message` (string, optional): Change message\n     - `created_by` (string, optional): Poster's screen_name (only team owners can specify)\n     - `original_revision` (string, optional): Revision to base the update on\n\n### Comment Related\n\n1. `esa_list_comments`\n   - Get a list of comments for a post\n   - Input:\n     - `post_number` (number, required): Post number to get comments for\n     - `page` (number, optional): Page number to retrieve\n     - `per_page` (number, optional): Number of results per page (max: 100)\n\n2. `esa_get_comment`\n   - Get a specific comment\n   - Input:\n     - `comment_id` (number, required): ID of the comment to retrieve\n     - `include` (string, optional): Related data to include in the response (e.g. 'stargazers')\n\n3. `esa_create_comment`\n   - Post a comment to an article\n   - Input:\n     - `post_number` (number, required): Post number to comment on\n     - `body_md` (string, required): Comment body (Markdown format)\n     - `user` (string, optional): Poster's screen_name (only team owners can specify)\n\n### Member Related\n\n1. `esa_get_members`\n   - Get a list of team members\n   - Input:\n     - `page` (number, optional): Page number to retrieve\n     - `per_page` (number, optional): Number of results per page (max: 100)\n\n2. `esa_get_member`\n   - Get information about a specific team member\n   - Input:\n     - `screen_name_or_email` (string, required): Screen name or email of the member to retrieve\n\n## Usage Example\n\nHere's an example of Claude using this MCP server to create an esa post:\n\n```\n[Claude] Please create a new post in esa. The title should be \"Project X Progress Report\" and the body should include \"# This Week's Progress\\n\\n- Implementation of Feature A completed\\n- Testing of Feature B started\\n\\n## Next Week's Plan\\n\\n- Start implementation of Feature C\".\n\n[MCP Server] Using the esa_create_post tool to create a new post.\n\n[Result]\n{\n  \"number\": 123,\n  \"name\": \"Project X Progress Report\",\n  \"body_md\": \"# This Week's Progress\\n\\n- Implementation of Feature A completed\\n- Testing of Feature B started\\n\\n## Next Week's Plan\\n\\n- Start implementation of Feature C\",\n  \"wip\": false,\n  \"created_at\": \"2023-06-01T12:34:56+09:00\",\n  \"updated_at\": \"2023-06-01T12:34:56+09:00\",\n  \"url\": \"https://your-team.esa.io/posts/123\"\n}\n\n[Claude] The post has been created successfully. The post number is 123, and you can access it at the following URL:\nhttps://your-team.esa.io/posts/123\n```\n\n## Troubleshooting\n\n### Access Token Issues\n\n```\nError: Request failed with status code 401\n```\n\nIf you see this error, your esa access token may be invalid or expired. Generate a new access token from the esa settings screen and update your environment variable.\n\n### Permission Issues\n\n```\nError: Request failed with status code 403\n```\n\nIf you see this error, the current access token doesn't have the necessary permissions. Check the permissions for your access token in the esa settings screen and issue a new token if needed.\n\n## License\n\nProvided under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "esa",
        "documents",
        "document",
        "document processing",
        "esa api",
        "documents efficiently"
      ],
      "category": "document-processing"
    },
    "kakukontrol--iodraw-files": {
      "owner": "kakukontrol",
      "name": "iodraw-files",
      "url": "https://github.com/kakukontrol/iodraw-files",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Provide access to various diagram and chart file formats used by iodraw, including flow charts, mind maps, Gantt charts, whiteboards, code charts, and online charts. Enable seamless integration and manipulation of these specialized file types within your applications. Enhance your workflows by leveraging structured visual data formats.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "iodraw",
        "formats",
        "files",
        "iodraw files",
        "iodraw including",
        "kakukontrol iodraw"
      ],
      "category": "document-processing"
    },
    "kaliaboi--mcp-zotero": {
      "owner": "kaliaboi",
      "name": "mcp-zotero",
      "url": "https://github.com/kaliaboi/mcp-zotero",
      "imageUrl": "/freedevtools/mcp/pfp/kaliaboi.webp",
      "description": "Integrates with Zotero to enable interactions with a Zotero library, facilitating the management and retrieval of bibliographic data.",
      "stars": 135,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T12:49:43Z",
      "readme_content": "# MCP Zotero\n\n![NPM Version](https://img.shields.io/npm/v/mcp-zotero) [![smithery badge](https://smithery.ai/badge/mcp-zotero)](https://smithery.ai/server/mcp-zotero)\n\nA Model Context Protocol server for Zotero integration that allows Claude to interact with your Zotero library.\n\n<a href=\"https://glama.ai/mcp/servers/mjvu0xzzzz\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mjvu0xzzzz/badge\" alt=\"Zotero MCP server\" /></a>\n\n## Setup\n\n1. Get your Zotero credentials:\n\n   ```bash\n   # First, create an API key at https://www.zotero.org/settings/keys\n   # Then use it to get your user ID:\n   curl -H \"Zotero-API-Key: YOUR_API_KEY\" https://api.zotero.org/keys/current\n   ```\n\n   The response will look like:\n\n   ```json\n   {\n     \"userID\": 123456,\n     \"username\": \"your_username\",\n     \"access\": {\n       \"user\": {\n         \"library\": true,\n         \"files\": true,\n         \"notes\": true,\n         \"write\": true\n       }\n     }\n   }\n   ```\n\n   The `userID` value is what you need.\n\n2. Set environment variables:\n\n   ```bash\n   export ZOTERO_API_KEY=\"your-api-key\"\n   export ZOTERO_USER_ID=\"user-id-from-curl\"\n   ```\n\n3. Verify your credentials:\n\n   ```bash\n   # Test that your credentials work:\n   curl -H \"Zotero-API-Key: $ZOTERO_API_KEY\" \\\n        \"https://api.zotero.org/users/$ZOTERO_USER_ID/collections\"\n   ```\n\n   You should see your collections list in the response.\n\n4. Install and run:\n\n   ```bash\n   # Install globally (recommended)\n   npm install -g mcp-zotero\n   mcp-zotero\n\n   # Or run directly with npx\n   npx mcp-zotero\n   ```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"command\": \"mcp-zotero\",\n      \"env\": {\n        \"ZOTERO_API_KEY\": YOUR_API_KEY,\n        \"ZOTERO_USER_ID\": YOUR_USER_ID\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n- `get_collections`: List all collections in your library\n- `get_collection_items`: Get items in a specific collection\n- `get_item_details`: Get detailed information about a paper\n- `search_library`: Search your entire library\n- `get_recent`: Get recently added papers\n\n## Troubleshooting\n\nIf you encounter any issues:\n\n1. Verify your environment variables are set:\n\n   ```bash\n   echo $ZOTERO_API_KEY\n   echo $ZOTERO_USER_ID\n   ```\n\n2. Check the installation:\n\n   ```bash\n   npm list -g mcp-zotero\n   ```\n\n3. Try reinstalling:\n   ```bash\n   npm uninstall -g mcp-zotero\n   npm install -g mcp-zotero\n   ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zotero",
        "bibliographic",
        "document",
        "zotero library",
        "mcp zotero",
        "zotero enable"
      ],
      "category": "document-processing"
    },
    "kazuph--mcp-docs-rag": {
      "owner": "kazuph",
      "name": "mcp-docs-rag",
      "url": "https://github.com/kazuph/mcp-docs-rag",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Manage and query documents in a local directory using retrieval-augmented generation techniques, integrating context from text files and Git repositories. Supports listing documents and generating responses based on queries with context from the stored content.",
      "stars": 12,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T08:28:45Z",
      "readme_content": "# mcp-docs-rag MCP Server\n\nRAG (Retrieval-Augmented Generation) for documents in a local directory\n\nThis is a TypeScript-based MCP server that implements a RAG system for documents stored in a local directory. It allows users to query documents using LLMs with context from locally stored repositories and text files.\n\n## Features\n\n### Resources\n- List and access documents via `docs://` URIs\n- Documents can be Git repositories or text files\n- Plain text mime type for content access\n\n### Tools\n- `list_documents` - List all available documents in the DOCS_PATH directory\n  - Returns a formatted list of all documents\n  - Shows total number of available documents\n- `rag_query` - Query documents using RAG\n  - Takes document_id and query as parameters\n  - Returns AI-generated responses with context from documents\n- `add_git_repository` - Clone a Git repository to the docs directory with optional sparse checkout\n  - Takes repository_url as parameter\n  - Optional document_name parameter to customize the name of the document (use simple descriptive names without '-docs' suffix)\n  - Optional subdirectory parameter for sparse checkout of specific directories\n  - Automatically pulls latest changes if repository already exists\n- `add_text_file` - Download a text file to the docs directory\n  - Takes file_url as parameter\n  - Uses wget to download file\n\n### Prompts\n- `guide_documents_usage` - Guide on how to use documents and RAG functionality\n  - Includes list of available documents\n  - Provides usage hints for RAG functionality\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Setup\n\nThis server requires a local directory for storing documents. By default, it uses `~/docs` but you can configure a different location with the `DOCS_PATH` environment variable.\n\n### Document Structure\n\nThe documents directory can contain:\n- Git repositories (cloned directories)\n- Plain text files (with .txt extension)\n\nEach document is indexed separately using llama-index.ts with Google's Gemini embeddings.\n\n### API Keys\n\nThis server uses Google's Gemini API for document indexing and querying. You need to set your Gemini API key as an environment variable:\n\n```bash\nexport GEMINI_API_KEY=your-api-key-here\n```\n\nYou can obtain a Gemini API key from the [Google AI Studio](https://makersuite.google.com/app/apikey) website. Add this key to your shell profile or include it in the environment configuration for Claude Desktop.\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\nOn Linux: `~/.config/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-rag\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-docs-rag\"],\n      \"env\": {\n        \"DOCS_PATH\": \"/Users/username/docs\",\n        \"GEMINI_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nMake sure to replace `/Users/username/docs` with the actual path to your documents directory.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Usage\n\nOnce configured, you can use the server with Claude to:\n\n1. **Add documents**:\n   ```\n   Add a new document from GitHub: https://github.com/username/repository\n   ```\n   or with a custom document name:\n   ```\n   Add GitHub repository https://github.com/username/repository-name and name it 'framework'\n   ```\n   or with sparse checkout of a specific directory:\n   ```\n   Add only the 'src/components' directory from https://github.com/username/repository\n   ```\n   or combine custom name and sparse checkout:\n   ```\n   Add the 'examples/demo' directory from https://github.com/username/large-repo and name it 'demo-app'\n   ```\n   or add a text file:\n   ```\n   Add this text file: https://example.com/document.txt\n   ```\n\n2. **Query documents**:\n   ```\n   What does the documentation say about X in the Y repository?\n   ```\n\n3. **List available documents**:\n   ```\n   What documents do you have access to?\n   ```\n\nThe server will automatically handle indexing of documents for efficient retrieval.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "git",
        "repositories",
        "query documents",
        "documents generating",
        "document processing"
      ],
      "category": "document-processing"
    },
    "kazuph--mcp-screenshot": {
      "owner": "kazuph",
      "name": "mcp-screenshot",
      "url": "https://github.com/kazuph/mcp-screenshot",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Captures screenshots and performs OCR text recognition on macOS. Supports both Japanese and English text, offering multiple output formats.",
      "stars": 21,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# MCP Screenshot\n\nAn MCP server that captures screenshots and performs OCR text recognition.\n\n<a href=\"https://glama.ai/mcp/servers/vcnmmaejv8\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/vcnmmaejv8/badge\" alt=\"mcp-screenshot MCP server\" /></a>\n\n## Features\n\n- Screenshot capture (left half, right half, full screen)\n- OCR text recognition (supports Japanese and English)\n- Multiple output formats (JSON, Markdown, vertical, horizontal)\n\n## OCR Engines\n\nThis server uses two OCR engines:\n\n1. [yomitoku](https://github.com/kazuph/yomitoku)\n   - Primary OCR engine\n   - High-accuracy Japanese text recognition\n   - Runs as an API server\n\n2. [Tesseract.js](https://github.com/naptha/tesseract.js)\n   - Fallback OCR engine\n   - Used when yomitoku is unavailable\n   - Supports both Japanese and English recognition\n\n## Installation\n\n```bash\nnpx -y @kazuph/mcp-screenshot\n```\n\n## Claude Desktop Configuration\n\nAdd the following configuration to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"screenshot\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-screenshot\"],\n      \"env\": {\n        \"OCR_API_URL\": \"http://localhost:8000\"  // yomitoku API base URL\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n| Variable Name | Description | Default Value |\n|--------------|-------------|---------------|\n| OCR_API_URL | yomitoku API base URL | http://localhost:8000 |\n\n## Usage Example\n\nYou can use it by instructing Claude like this:\n\n```\nPlease take a screenshot of the left half of the screen and recognize the text in it.\n```\n\n## Tool Specification\n\n### capture\n\nTakes a screenshot and performs OCR.\n\nOptions:\n- `region`: Screenshot area ('left'/'right'/'full', default: 'left')\n- `format`: Output format ('json'/'markdown'/'vertical'/'horizontal', default: 'markdown')\n\n## License\n\nMIT\n\n## Author\n\nkazuph\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kazuph",
        "macos",
        "ocr",
        "kazuph mcp",
        "processing kazuph",
        "recognition macos"
      ],
      "category": "document-processing"
    },
    "kmexnx--excel-to-pdf-mcp": {
      "owner": "kmexnx",
      "name": "excel-to-pdf-mcp",
      "url": "https://github.com/kmexnx/excel-to-pdf-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kmexnx.webp",
      "description": "Convert Excel and Apple Numbers files to PDF format, enabling seamless file handling and sharing. Integrate with AI assistants through the Model Context Protocol for direct file conversion during conversations.",
      "stars": 2,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-08T14:32:09Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kmexnx-excel-to-pdf-mcp-badge.png)](https://mseep.ai/app/kmexnx-excel-to-pdf-mcp)\n\n# Excel to PDF MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@kmexnx/excel-to-pdf-mcp)](https://smithery.ai/server/@kmexnx/excel-to-pdf-mcp)\n\nAn MCP (Model Context Protocol) server that can convert Excel (.xls/.xlsx) and Apple Numbers (.numbers) files to PDF format. This tool integrates with AI assistants like Claude to enable file conversion directly through the conversation.\n\n<a href=\"https://glama.ai/mcp/servers/@kmexnx/excel-to-pdf-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kmexnx/excel-to-pdf-mcp/badge\" alt=\"Excel to PDF Converter MCP server\" />\n</a>\n\n## Features\n\n- Convert Excel files (.xls, .xlsx) to PDF\n- Convert Apple Numbers files (.numbers) to PDF\n- Integrates with AI assistants via the Model Context Protocol\n- Secure file handling that respects project boundaries\n- Easy installation via npm\n\n## Requirements\n\n- Node.js 16 or higher\n- LibreOffice (for the conversion process)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Excel to PDF Converter for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kmexnx/excel-to-pdf-mcp):\n\n```bash\nnpx -y @smithery/cli install @kmexnx/excel-to-pdf-mcp --client claude\n```\n\n### Install LibreOffice\n\nLibreOffice is required for the conversion process. Install it according to your operating system:\n\n#### On macOS:\n```bash\nbrew install libreoffice\n```\n\n#### On Ubuntu/Debian:\n```bash\napt-get install libreoffice\n```\n\n#### On Windows:\nDownload and install from [LibreOffice official website](https://www.libreoffice.org/download/download/).\n\n### Install the MCP server\n\n```bash\nnpm install -g excel-to-pdf-mcp\n```\n\n## Using with Claude Desktop\n\nTo use this MCP server with Claude desktop:\n\n1. Configure your MCP settings in Claude desktop by adding this server to your `mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"excel-to-pdf-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"excel-to-pdf-mcp\"],\n      \"name\": \"Excel to PDF Converter\"\n    }\n  }\n}\n```\n\n2. Make sure your Excel or Numbers files are within your project directory.\n\n3. Once configured, Claude will be able to convert your spreadsheet files to PDF using this tool.\n\n## Example Conversation\n\nHere's an example of how a conversation with Claude might look when using this MCP server:\n\n**User**: \"I need to convert my quarterly_report.xlsx to PDF so I can share it with stakeholders.\"\n\n**Claude**: \"I can help you convert your Excel file to PDF. Let me use the Excel to PDF converter tool.\"\n\nClaude would then use the tool behind the scenes:\n\n```\nTool: convert_excel_to_pdf\nArguments: {\n  \"input_path\": \"quarterly_report.xlsx\",\n  \"output_format\": \"pdf\"\n}\n```\n\n**Claude**: \"I've converted your Excel file to PDF. You can find it at: quarterly_report-1628347658-a7b2c9.pdf in your project directory.\"\n\n## Available Tools\n\nThis MCP server provides the following tools:\n\n### 1. convert_excel_to_pdf\n\nConverts Excel files (.xls/.xlsx) to PDF format.\n\n**Arguments:**\n- `input_path`: Relative path to the Excel file (required)\n- `output_format`: Output format, currently only PDF is supported (default: \"pdf\")\n\n### 2. convert_numbers_to_pdf\n\nConverts Apple Numbers files (.numbers) to PDF format.\n\n**Arguments:**\n- `input_path`: Relative path to the Numbers file (required)\n- `output_format`: Output format, currently only PDF is supported (default: \"pdf\")\n\n## Development\n\nIf you want to run from source or contribute:\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the project: `npm run build`\n4. Run the server: `npm start`\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kmexnx",
        "pdf",
        "excel",
        "kmexnx excel",
        "processing kmexnx",
        "excel apple"
      ],
      "category": "document-processing"
    },
    "krimoi45--chroma-rag-project": {
      "owner": "krimoi45",
      "name": "chroma-rag-project",
      "url": "https://github.com/krimoi45/chroma-rag-project",
      "imageUrl": "/freedevtools/mcp/pfp/krimoi45.webp",
      "description": "Implement a Retrieval-Augmented Generation system using ChromaDB to facilitate semantic similarity search and document retrieval through embedding generation. Enables users to create and query document collections for effective RAG workflows in Python.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-15T00:37:27Z",
      "readme_content": "# Projet RAG avec ChromaDB\n\n## Description\nCe projet démontre l'utilisation de ChromaDB pour la mise en place d'un système de Retrieval-Augmented Generation (RAG) en Python.\n\n## Prérequis\n- Python 3.8+\n- pip\n\n## Installation\n1. Clonez le dépôt\n```bash\ngit clone https://github.com/krimoi45/chroma-rag-project.git\ncd chroma-rag-project\n```\n\n2. Créez un environnement virtuel\n```bash\npython -m venv venv\nsource venv/bin/activate  # Sur Windows, utilisez `venv\\Scripts\\activate`\n```\n\n3. Installez les dépendances\n```bash\npip install -r requirements.txt\n```\n\n## Utilisation\nLancez le script principal :\n```bash\npython main.py\n```\n\n## Fonctionnalités\n- Création d'une collection de documents avec ChromaDB\n- Génération d'embeddings avec Sentence Transformers\n- Recherche de similarité sémantique\n- Exemple de système RAG basique\n\n## Technologies\n- ChromaDB\n- Sentence Transformers\n- NumPy\n- Python\n\n## Licence\nProjet open-source\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "chromadb",
        "chroma",
        "document retrieval",
        "chroma rag",
        "implement retrieval"
      ],
      "category": "document-processing"
    },
    "langchain-ai--mcpdoc": {
      "owner": "langchain-ai",
      "name": "mcpdoc",
      "url": "https://github.com/langchain-ai/mcpdoc",
      "imageUrl": "/freedevtools/mcp/pfp/langchain-ai.webp",
      "description": "Provides a user-defined list of llms.txt files and fetches documents from URLs within those files for auditing tool calls and enhancing context retrieval for LLM interactions. Connects with various IDEs and applications to improve the development experience while ensuring transparency and control.",
      "stars": 770,
      "forks": 85,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T03:33:49Z",
      "readme_content": "# MCP LLMS-TXT Documentation Server\n\n## Overview\n\n[llms.txt](https://llmstxt.org/) is a website index for LLMs, providing background information, guidance, and links to detailed markdown files. IDEs like Cursor and Windsurf or apps like Claude Code/Desktop can use `llms.txt` to retrieve context for tasks. However, these apps use different built-in tools to read and process files like `llms.txt`. The retrieval process can be opaque, and there is not always a way to audit the tool calls or the context returned.\n\n[MCP](https://github.com/modelcontextprotocol) offers a way for developers to have *full control* over tools used by these applications. Here, we create [an open source MCP server](https://github.com/modelcontextprotocol) to provide MCP host applications (e.g., Cursor, Windsurf, Claude Code/Desktop) with (1) a user-defined list of `llms.txt` files and (2) a simple  `fetch_docs` tool read URLs within any of the provided `llms.txt` files. This allows the user to audit each tool call as well as the context returned. \n\n<img src=\"https://github.com/user-attachments/assets/736f8f55-833d-4200-b833-5fca01a09e1b\" width=\"60%\">\n\n## llms-txt\n\nYou can find llms.txt files for langgraph and langchain here:\n\n| Library          | llms.txt                                                                                                   |\n|------------------|------------------------------------------------------------------------------------------------------------|\n| LangGraph Python | [https://langchain-ai.github.io/langgraph/llms.txt](https://langchain-ai.github.io/langgraph/llms.txt)     |\n| LangGraph JS     | [https://langchain-ai.github.io/langgraphjs/llms.txt](https://langchain-ai.github.io/langgraphjs/llms.txt) |\n| LangChain Python | [https://python.langchain.com/llms.txt](https://python.langchain.com/llms.txt)                             |\n| LangChain JS     | [https://js.langchain.com/llms.txt](https://js.langchain.com/llms.txt)                                     |\n\n## Quickstart\n\n#### Install uv\n* Please see [official uv docs](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) for other ways to install `uv`.\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n#### Choose an `llms.txt` file to use. \n* For example, [here's](https://langchain-ai.github.io/langgraph/llms.txt) the LangGraph `llms.txt` file.\n\n> **Note: Security and Domain Access Control**\n> \n> For security reasons, mcpdoc implements strict domain access controls:\n> \n> 1. **Remote llms.txt files**: When you specify a remote llms.txt URL (e.g., `https://langchain-ai.github.io/langgraph/llms.txt`), mcpdoc automatically adds only that specific domain (`langchain-ai.github.io`) to the allowed domains list. This means the tool can only fetch documentation from URLs on that domain.\n> \n> 2. **Local llms.txt files**: When using a local file, NO domains are automatically added to the allowed list. You MUST explicitly specify which domains to allow using the `--allowed-domains` parameter.\n> \n> 3. **Adding additional domains**: To allow fetching from domains beyond those automatically included:\n>    - Use `--allowed-domains domain1.com domain2.com` to add specific domains\n>    - Use `--allowed-domains '*'` to allow all domains (use with caution)\n> \n> This security measure prevents unauthorized access to domains not explicitly approved by the user, ensuring that documentation can only be retrieved from trusted sources.\n\n#### (Optional) Test the MCP server locally with your `llms.txt` file(s) of choice:\n```bash\nuvx --from mcpdoc mcpdoc \\\n    --urls \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\" \"LangChain:https://python.langchain.com/llms.txt\" \\\n    --transport sse \\\n    --port 8082 \\\n    --host localhost\n```\n\n* This should run at: http://localhost:8082\n\n![Screenshot 2025-03-18 at 3 29 30 PM](https://github.com/user-attachments/assets/24a3d483-cd7a-4c7e-a4f7-893df70e888f)\n\n* Run [MCP inspector](https://modelcontextprotocol.io/docs/tools/inspector) and connect to the running server:\n```bash\nnpx @modelcontextprotocol/inspector\n```\n\n![Screenshot 2025-03-18 at 3 30 30 PM](https://github.com/user-attachments/assets/14645d57-1b52-4a5e-abfe-8e7756772704)\n\n* Here, you can test the `tool` calls. \n\n#### Connect to Cursor \n\n* Open `Cursor Settings` and `MCP` tab.\n* This will open the `~/.cursor/mcp.json` file.\n\n![Screenshot 2025-03-19 at 11 01 31 AM](https://github.com/user-attachments/assets/3d1c8eb3-4d40-487f-8bad-3f9e660f770a)\n\n* Paste the following into the file (we use the `langgraph-docs-mcp` name and link to the LangGraph `llms.txt`).\n\n```\n{\n  \"mcpServers\": {\n    \"langgraph-docs-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"mcpdoc\",\n        \"mcpdoc\",\n        \"--urls\",\n        \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt LangChain:https://python.langchain.com/llms.txt\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n* Confirm that the server is running in your `Cursor Settings/MCP` tab.\n* Best practice is to then update Cursor Global (User) rules.\n* Open Cursor `Settings/Rules` and update `User Rules` with the following (or similar):\n\n```\nfor ANY question about LangGraph, use the langgraph-docs-mcp server to help answer -- \n+ call list_doc_sources tool to get the available llms.txt file\n+ call fetch_docs tool to read it\n+ reflect on the urls in llms.txt \n+ reflect on the input question \n+ call fetch_docs on any urls relevant to the question\n+ use this to answer the question\n```\n\n* `CMD+L` (on Mac) to open chat.\n* Ensure `agent` is selected. \n\n![Screenshot 2025-03-18 at 1 56 54 PM](https://github.com/user-attachments/assets/0dd747d0-7ec0-43d2-b6ef-cdcf5a2a30bf)\n\nThen, try an example prompt, such as:\n```\nwhat are types of memory in LangGraph?\n```\n\n![Screenshot 2025-03-18 at 1 58 38 PM](https://github.com/user-attachments/assets/180966b5-ab03-4b78-8b5d-bab43f5954ed)\n\n### Connect to Windsurf\n\n* Open Cascade with `CMD+L` (on Mac).\n* Click `Configure MCP` to open the config file, `~/.codeium/windsurf/mcp_config.json`.\n* Update with `langgraph-docs-mcp` as noted above.\n\n![Screenshot 2025-03-19 at 11 02 52 AM](https://github.com/user-attachments/assets/d45b427c-1c1e-4602-820a-7161a310af24)\n\n* Update `Windsurf Rules/Global rules` with the following (or similar):\n\n```\nfor ANY question about LangGraph, use the langgraph-docs-mcp server to help answer -- \n+ call list_doc_sources tool to get the available llms.txt file\n+ call fetch_docs tool to read it\n+ reflect on the urls in llms.txt \n+ reflect on the input question \n+ call fetch_docs on any urls relevant to the question\n```\n\n![Screenshot 2025-03-18 at 2 02 12 PM](https://github.com/user-attachments/assets/5a29bd6a-ad9a-4c4a-a4d5-262c914c5276)\n\nThen, try the example prompt:\n* It will perform your tool calls.\n\n![Screenshot 2025-03-18 at 2 03 07 PM](https://github.com/user-attachments/assets/0e24e1b2-dc94-4153-b4fa-495fd768125b)\n\n### Connect to Claude Desktop\n\n* Open `Settings/Developer` to update `~/Library/Application\\ Support/Claude/claude_desktop_config.json`.\n* Update with `langgraph-docs-mcp` as noted above.\n* Restart Claude Desktop app.\n\n> [!Note]\n> If you run into issues with Python version incompatibility when trying to add MCPDoc tools to Claude Desktop, you can explicitly specify the filepath to `python` executable in the `uvx` command.\n>\n> <details>\n> <summary>Example configuration</summary>\n>\n> ```\n> {\n>   \"mcpServers\": {\n>     \"langgraph-docs-mcp\": {\n>       \"command\": \"uvx\",\n>       \"args\": [\n>         \"--python\",\n>         \"/path/to/python\",\n>         \"--from\",\n>         \"mcpdoc\",\n>         \"mcpdoc\",\n>         \"--urls\",\n>         \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\",\n>         \"--transport\",\n>         \"stdio\"\n>       ]\n>     }\n>   }\n> }\n> ```\n> </details>\n\n> [!Note]\n> Currently (3/21/25) it appears that Claude Desktop does not support `rules` for global rules, so appending the following to your prompt.\n\n```\n<rules>\nfor ANY question about LangGraph, use the langgraph-docs-mcp server to help answer -- \n+ call list_doc_sources tool to get the available llms.txt file\n+ call fetch_docs tool to read it\n+ reflect on the urls in llms.txt \n+ reflect on the input question \n+ call fetch_docs on any urls relevant to the question\n</rules>\n```\n\n![Screenshot 2025-03-18 at 2 05 54 PM](https://github.com/user-attachments/assets/228d96b6-8fb3-4385-8399-3e42fa08b128)\n\n* You will see your tools visible in the bottom right of your chat input.\n\n![Screenshot 2025-03-18 at 2 05 39 PM](https://github.com/user-attachments/assets/71f3c507-91b2-4fa7-9bd1-ac9cbed73cfb)\n\nThen, try the example prompt:\n\n* It will ask to approve tool calls as it processes your request.\n\n![Screenshot 2025-03-18 at 2 06 54 PM](https://github.com/user-attachments/assets/59b3a010-94fa-4a4d-b650-5cd449afeec0)\n\n### Connect to Claude Code\n\n* In a terminal after installing [Claude Code](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview), run this command to add the MCP server to your project:\n```\nclaude mcp add-json langgraph-docs '{\"type\":\"stdio\",\"command\":\"uvx\" ,\"args\":[\"--from\", \"mcpdoc\", \"mcpdoc\", \"--urls\", \"langgraph:https://langchain-ai.github.io/langgraph/llms.txt\", \"LangChain:https://python.langchain.com/llms.txt\"]}' -s local\n```\n* You will see `~/.claude.json` updated.\n* Test by launching Claude Code and running to view your tools:\n```\n$ Claude\n$ /mcp \n```\n\n![Screenshot 2025-03-18 at 2 13 49 PM](https://github.com/user-attachments/assets/eb876a0e-27b4-480e-8c37-0f683f878616)\n\n> [!Note]\n> Currently (3/21/25) it appears that Claude Code does not support `rules` for global rules, so appending the following to your prompt.\n\n```\n<rules>\nfor ANY question about LangGraph, use the langgraph-docs-mcp server to help answer -- \n+ call list_doc_sources tool to get the available llms.txt file\n+ call fetch_docs tool to read it\n+ reflect on the urls in llms.txt \n+ reflect on the input question \n+ call fetch_docs on any urls relevant to the question\n</rules>\n```\n\nThen, try the example prompt:\n\n* It will ask to approve tool calls.\n\n![Screenshot 2025-03-18 at 2 14 37 PM](https://github.com/user-attachments/assets/5b9a2938-ea69-4443-8d3b-09061faccad0)\n\n## Command-line Interface\n\nThe `mcpdoc` command provides a simple CLI for launching the documentation server. \n\nYou can specify documentation sources in three ways, and these can be combined:\n\n1. Using a YAML config file:\n\n* This will load the LangGraph Python documentation from the `sample_config.yaml` file in this repo.\n\n```bash\nmcpdoc --yaml sample_config.yaml\n```\n\n2. Using a JSON config file:\n\n* This will load the LangGraph Python documentation from the `sample_config.json` file in this repo.\n\n```bash\nmcpdoc --json sample_config.json\n```\n\n3. Directly specifying llms.txt URLs with optional names:\n\n* URLs can be specified either as plain URLs or with optional names using the format `name:url`.\n* You can specify multiple URLs by using the `--urls` parameter multiple times.\n* This is how we loaded `llms.txt` for the MCP server above.\n\n```bash\nmcpdoc --urls LangGraph:https://langchain-ai.github.io/langgraph/llms.txt --urls LangChain:https://python.langchain.com/llms.txt\n```\n\nYou can also combine these methods to merge documentation sources:\n\n```bash\nmcpdoc --yaml sample_config.yaml --json sample_config.json --urls LangGraph:https://langchain-ai.github.io/langgraph/llms.txt --urls LangChain:https://python.langchain.com/llms.txt\n```\n\n## Additional Options\n\n- `--follow-redirects`: Follow HTTP redirects (defaults to False)\n- `--timeout SECONDS`: HTTP request timeout in seconds (defaults to 10.0)\n\nExample with additional options:\n\n```bash\nmcpdoc --yaml sample_config.yaml --follow-redirects --timeout 15\n```\n\nThis will load the LangGraph Python documentation with a 15-second timeout and follow any HTTP redirects if necessary.\n\n## Configuration Format\n\nBoth YAML and JSON configuration files should contain a list of documentation sources. \n\nEach source must include an `llms_txt` URL and can optionally include a `name`:\n\n### YAML Configuration Example (sample_config.yaml)\n\n```yaml\n# Sample configuration for mcp-mcpdoc server\n# Each entry must have a llms_txt URL and optionally a name\n- name: LangGraph Python\n  llms_txt: https://langchain-ai.github.io/langgraph/llms.txt\n```\n\n### JSON Configuration Example (sample_config.json)\n\n```json\n[\n  {\n    \"name\": \"LangGraph Python\",\n    \"llms_txt\": \"https://langchain-ai.github.io/langgraph/llms.txt\"\n  }\n]\n```\n\n## Programmatic Usage\n\n```python\nfrom mcpdoc.main import create_server\n\n# Create a server with documentation sources\nserver = create_server(\n    [\n        {\n            \"name\": \"LangGraph Python\",\n            \"llms_txt\": \"https://langchain-ai.github.io/langgraph/llms.txt\",\n        },\n        # You can add multiple documentation sources\n        # {\n        #     \"name\": \"Another Documentation\",\n        #     \"llms_txt\": \"https://example.com/llms.txt\",\n        # },\n    ],\n    follow_redirects=True,\n    timeout=15.0,\n)\n\n# Run the server\nserver.run(transport=\"stdio\")\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcpdoc",
        "documents",
        "document",
        "llms txt",
        "mcpdoc provides",
        "ai mcpdoc"
      ],
      "category": "document-processing"
    },
    "lucamauri--MediaWiki-MCP-adapter": {
      "owner": "lucamauri",
      "name": "MediaWiki-MCP-adapter",
      "url": "https://github.com/lucamauri/MediaWiki-MCP-adapter",
      "imageUrl": "/freedevtools/mcp/pfp/lucamauri.webp",
      "description": "Interact programmatically with MediaWiki and WikiBase APIs to fetch and edit MediaWiki pages. Perform operations using the MCP framework to enhance application's capabilities with seamless API access.",
      "stars": 4,
      "forks": 1,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-07-06T21:21:06Z",
      "readme_content": "# MediaWikiAdapter\n\n[![smithery badge](https://smithery.ai/server/@lucamauri/mediawiki-mcp-adapter)](https://smithery.ai/server/@lucamauri/mediawiki-mcp-adapter)\n\nA custom **Model Context Protocol (MCP)** adapter for interacting with MediaWiki and WikiBase APIs. This adapter allows you to fetch and edit MediaWiki pages programmatically using the MCP framework.\n\n## Features\n\n- Fetch the content of a MediaWiki page.\n- Edit a MediaWiki page with new content and an optional summary.\n- Configurable API base URLs for different MediaWiki and WikiBase instances.\n\n## Requirements\n\n- Node.js (v16 or later)\n- TypeScript (for development)\n- MediaWiki instance with API access enabled\n\n## Installation\n\n1. Clone the repository:\n```bash\n   git clone https://github.com/yourusername/mediawikiadapter.git\n   cd mediawikiadapter\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Usage\n\n### Configure the Adapter\n\nYou can configure the adapter to use custom MediaWiki and WikiBase API endpoints:\n\n```javascript\nserver.configure({\n  mediaWikiAPIBase: \"https://my.mediawiki.instance/api.php\",\n  wikiBaseAPIBase: \"https://my.wikibase.instance/api.php\",\n});\n```\n\n### Start the MCP Server\n\nRun the MCP server using the following command:\n```bash\nnode build/index.js\n```\n\n### Resources\n\n#### getPageContent\n\nFetches the content of a MediaWiki page.\n\n- **Input Schema**:\n```json\n  {\n    \"title\": \"string\"\n  }\n```\n- **Output Schema**:\n  ```json\n  {\n    \"content\": \"string\"\n  }\n  ```\n\n#### Example Usage:\n```javascript\nconst response = await server.callResource(\"getPageContent\", {\n  title: \"Main Page\",\n});\nconsole.log(response.content);\n```\n---\n\n### Tools\n\n#### editPage\n\nEdits a MediaWiki page with new content.\n\n- **Input Schema**:\n```json\n  {\n    \"title\": \"string\",\n    \"content\": \"string\",\n    \"summary\": \"string (optional)\"\n  }\n```\n- **Output Schema**:\n```json\n  {\n    \"success\": \"boolean\"\n  }\n```\n\n#### Example Usage:\n```javascript\nconst response = await server.callTool(\"editPage\", {\n  title: \"Main Page\",\n  content: \"Updated content for the page.\",\n  summary: \"Updated via MediaWikiAdapter\",\n});\nconsole.log(response.success ? \"Edit successful\" : \"Edit failed\");\n```\n\n---\n\n## Development\n\n### Run in Development Mode\n\nTo run the project in development mode with TypeScript:\n```bash\nnpm run dev\n```\n\n### Linting\n\nRun the linter to check for code quality:\n```bash\nnpm run lint\n```\n\n### Testing\n\nCurrently, no tests are implemented. You can add tests to the `test` directory and run them using:\n```bash\nnpm test\n```\n\n---\n\n## Configuration\n\nThe adapter uses the following default API base URLs:\n\n- **MediaWiki API Base**: https://en.wikipedia.org/w/api.php\n- **WikiBase API Base**: https://www.wikidata.org/w/api.php\n\nYou can override these defaults using the `server.configure()` method.\n\n---\n\n## Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository.\n2. Create a new branch for your feature or bug fix.\n3. Submit a pull request with a detailed description of your changes.\n\n---\n\n## License\n\nThis project is licensed under the **LGPL-3.0-or-later** license. See the [LICENSE](LICENSE) file for details.\n\n---\n\n## Author\n\nCreated by **Luca Mauri**.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mediawiki",
        "wikibase",
        "document",
        "mediawiki mcp",
        "programmatically mediawiki",
        "lucamauri mediawiki"
      ],
      "category": "document-processing"
    },
    "lvshiyu21--Yuque-MCP-Server": {
      "owner": "lvshiyu21",
      "name": "Yuque-MCP-Server",
      "url": "https://github.com/lvshiyu21/Yuque-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/lvshiyu21.webp",
      "description": "Integrates with the Yuque knowledge base platform for document management and user information retrieval, enabling operations such as creating, reading, updating, and deleting documents while utilizing AI capabilities for enhanced workflow and analytics.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-09-16T10:23:09Z",
      "readme_content": "# 语雀 MCP 服务器\n\n[English Version](./README.en.md)\n\n一个用于与语雀 API 集成的 Model-Context-Protocol (MCP) 服务器。此实现受 [Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP) 的启发，并使用 [语雀开放 API](https://app.swaggerhub.com/apis-docs/Jeff-Tian/yuque-open_api/2.0.1)。\n\n## 概述\n\n该服务器提供了与语雀知识库平台交互的 MCP 工具，允许 AI 模型：\n\n- 获取用户和文档信息\n- 创建、读取、更新和删除文档\n- 搜索语雀中的内容\n- 获取知识库信息\n- 获取统计数据和分析信息\n\n## 安装\n\n### 前提条件\n\n- Node.js 18+ (推荐)\n- 拥有 API 令牌的语雀账号\n\n### 设置\n\n1. 克隆此仓库：\n   ```\n   git clone https://github.com/Henryhaoson/Yueque-MCP-Server.git\n   cd Yueque-MCP-Server\n   ```\n\n2. 安装依赖：\n   ```\n   npm install\n   ```\n\n3. 基于 `.env.example` 创建 `.env` 文件：\n   ```\n   cp .env.example .env\n   ```\n\n4. (可选) 在 `.env` 文件中添加你的语雀 API 令牌：\n   ```\n   YUQUE_API_TOKEN=your_yuque_api_token_here\n   ```\n   \n   你也可以选择在连接到服务器时通过查询参数提供令牌，而不是在 .env 文件中设置。\n\n## 使用方法\n\n### 运行服务器\n\n#### 开发模式\n\n```bash\n# HTTP 服务器模式\nnpm run dev\n\n# CLI stdio 模式\nnpm run dev:cli\n```\n\n#### 生产模式\n\n首先，构建项目：\n\n```bash\nnpm run build\n```\n\n然后在 HTTP 或 CLI 模式下运行：\n\n```bash\n# HTTP 服务器模式\nnpm run start\n\n# CLI stdio 模式\nnpm run start:cli\n```\n\n### 使用 Docker 部署\n\n本项目提供了 Docker 支持，使您可以轻松地容器化和部署服务器。\n\n#### 使用 Docker Compose（推荐）\n\n1. 构建并启动容器：\n   ```bash\n   docker-compose up -d\n   ```\n\n2. 查看日志：\n   ```bash\n   docker-compose logs -f\n   ```\n\n3. 停止服务：\n   ```bash\n   docker-compose down\n   ```\n\n您可以通过环境变量或在 `.env` 文件中设置配置项：\n```bash\n# .env 文件示例\nPORT=3000\nYUQUE_API_TOKEN=your_token_here\nYUQUE_API_BASE_URL=https://www.yuque.com/api/v2\n```\n\n#### 手动使用 Docker\n\n1. 构建 Docker 镜像：\n   ```bash\n   docker build -t yuque-mcp-server .\n   ```\n\n2. 运行容器：\n   ```bash\n   docker run -d -p 3000:3000 --name yuque-mcp-server yuque-mcp-server\n   ```\n\n3. 使用环境变量：\n   ```bash\n   docker run -d -p 3000:3000 \\\n     -e YUQUE_API_TOKEN=your_token_here \\\n     -e YUQUE_API_BASE_URL=https://www.yuque.com/api/v2 \\\n     --name yuque-mcp-server yuque-mcp-server\n   ```\n\n### MCP 工具\n\n语雀 MCP 服务器提供以下工具：\n\n#### 用户和文档管理\n- `get_current_user` - 获取当前认证用户的信息，包括用户ID、用户名、头像等语雀账号基本信息\n- `get_user_docs` - 获取当前用户的所有知识库文档列表，包括私人和协作文档\n- `get_user_repos` - 获取指定用户的知识库列表，知识库是语雀中组织文档的集合\n- `get_repo_docs` - 获取特定知识库中的所有文档列表，包括文档标题、更新时间等信息\n- `get_doc` - 获取语雀中特定文档的详细内容，包括正文、修改历史和权限信息\n- `create_doc` - 在指定知识库中创建新的语雀文档，支持多种格式内容（Markdown、HTML、Lake）\n- `update_doc` - 更新语雀中已存在的文档，可以修改标题、内容或权限设置\n- `delete_doc` - 从语雀知识库中删除指定文档，此操作不可撤销\n- `search` - 在语雀平台中搜索文档或知识库内容，支持范围和作者筛选\n\n#### 团队统计分析\n- `get_group_statistics` - 获取团队的汇总统计数据，包括成员人数、文档数量、阅读量和互动数据等\n- `get_group_member_statistics` - 获取团队成员的统计数据，包括各成员的编辑次数、阅读量、点赞量等\n- `get_group_book_statistics` - 获取团队知识库的统计数据，包括各知识库的文档数、字数、阅读量等\n- `get_group_doc_statistics` - 获取团队文档的统计数据，包括各文档的字数、阅读量、评论量等\n\n## 与 AI 模型的集成\n\n此 MCP 服务器可以与支持 Model-Context-Protocol 的 AI 模型一起使用，允许它们通过定义的工具与语雀交互。例如：\n\n1. 启动 MCP 服务器\n2. 从兼容的客户端连接到服务器\n3. AI 模型现在可以使用注册的工具与语雀数据交互\n\n### SSE 端点的查询参数\n\n当连接到 SSE 端点时，你可以通过查询参数（query parameters）覆盖环境配置，这些参数的优先级高于环境变量：\n\n- `accessToken`: 覆盖在 .env 文件中设置的语雀 API 令牌\n- `baseUrl`: 覆盖在 .env 文件中设置的语雀 API 基础 URL\n\n示例：\n```\nhttp://localhost:3000/sse?accessToken=your_token_here&baseUrl=https://custom.yuque.api/v2\n```\n\n这允许你在不修改 .env 文件的情况下动态配置服务，并且查询参数的优先级高于环境变量。这对于多用户环境或测试不同 API 端点特别有用。\n\n每个 SSE 连接都可以使用不同的配置，这使得同一个服务器实例可以同时为不同的用户或环境提供服务。\n\n## 开发\n\n### 项目结构\n\n```\nsrc/\n  ├── config.ts          # 服务器配置\n  ├── index.ts           # 主入口点\n  ├── cli.ts             # CLI 入口点 \n  ├── server.ts          # MCP 服务器实现\n  └── services/\n      └── yuque.ts       # 语雀 API 服务\n```\n\n### 添加新工具\n\n要添加新工具，请修改 `src/server.ts` 中的 `registerTools` 方法。\n\n## API 改进\n\n最近的更新增加了以下功能：\n\n1. **团队统计数据**：添加了获取团队、成员、知识库和文档统计数据的功能，便于分析和监控团队知识库的使用情况。\n\n2. **文档管理增强**：\n   - 支持多种文档格式（Markdown、HTML、Lake）\n   - 完善的文档公开性设置（私密、公开、企业内公开）\n   - 搜索功能支持更多参数和过滤条件\n\n3. **数据类型完善**：更新了接口定义，使其与语雀 OpenAPI 规范保持一致。\n\n## 许可证\n\nMIT License\n\nCopyright (c) 2025 Henryhaoson\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n## 致谢\n\n- [Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP) 提供 MCP 服务器实现参考\n- [语雀开放 API](https://app.swaggerhub.com/apis-docs/Jeff-Tian/yuque-open_api/2.0.1) 提供 API 文档\n- [Model Context Protocol](https://github.com/anthropics/model-context-protocol) 提供 MCP 规范\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "yuque",
        "documents",
        "document",
        "yuque knowledge",
        "document processing",
        "integrates yuque"
      ],
      "category": "document-processing"
    },
    "mahawi1992--mcp-documentation-server": {
      "owner": "mahawi1992",
      "name": "mcp-documentation-server",
      "url": "https://github.com/mahawi1992/mcp-documentation-server",
      "imageUrl": "/freedevtools/mcp/pfp/mahawi1992.webp",
      "description": "AI-assisted management of documentation and code improvement, supporting various frameworks with smart search capabilities. Integrates with Claude Desktop for an enhanced coding experience and improves suggestions over time.",
      "stars": 10,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-15T09:27:08Z",
      "readme_content": "# MCP Documentation Server\n\nA smart documentation server that provides AI-assisted code improvement and documentation management through Claude Desktop integration.\n\n## Features\n\n- **AI Documentation Guide**: Maintains and updates documentation knowledge base\n- **AI Code Assistant**: Analyzes and improves code quality\n- **Framework Support**: \n  - React.js\n  - Next.js (with App Router)\n  - Python\n  - Vue.js\n  - Angular\n  - Node.js\n- **Brave Search Integration**: Smart documentation search and retrieval\n- **Learning System**: Improves suggestions over time\n\n## Quick Start\n\n1. Install the package:\n```bash\nnpm install -g mcp-documentation-server\n```\n\n2. Configure Claude Desktop (config.json):\n```json\n{\n  \"mcpServers\": {\n    \"documentation\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-documentation-server\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"<YOUR_BRAVE_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n3. Start using with Claude:\n```\nClaude, search documentation for Next.js App Router\n```\n\nFor detailed setup instructions, see [Claude Desktop Setup Guide](docs/CLAUDE_DESKTOP_SETUP.md)\n\n## Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/mahawi1992/mcp-documentation-server.git\ncd mcp-documentation-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create a .env file:\n```env\nPORT=3000\nUPDATE_INTERVAL=3600000\nCACHE_DURATION=86400000\nBRAVE_API_KEY=your_brave_api_key\n```\n\n4. Start the development server:\n```bash\nnpm run dev\n```\n\n## Documentation\n\n- [Usage Guide](docs/USAGE.md)\n- [Claude Desktop Setup](docs/CLAUDE_DESKTOP_SETUP.md)\n- [API Documentation](docs/API.md)\n- [Contributing Guide](CONTRIBUTING.md)\n\n## Using with Claude Desktop\n\n### Basic Commands\n\n```\nClaude, search documentation for React hooks\n```\n\n```\nClaude, analyze this Python code and suggest improvements...\n```\n\n```\nClaude, find best practices for Next.js App Router\n```\n\n### Advanced Usage\n\n```\nClaude, search for documentation about async/await in Python 3.9\n```\n\n```\nClaude, analyze this code for security issues and suggest fixes...\n```\n\nFor more examples, see the [Usage Guide](docs/USAGE.md)\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch:\n   ```bash\n   git checkout -b feature/amazing-feature\n   ```\n3. Commit your changes:\n   ```bash\n   git commit -m 'Add amazing feature'\n   ```\n4. Push to the branch:\n   ```bash\n   git push origin feature/amazing-feature\n   ```\n5. Open a Pull Request\n\n## Testing\n\nRun the test suite:\n\n```bash\nnpm test\n```\n\nRun specific tests:\n\n```bash\nnpm test -- tests/integration/BraveSearchIntegration.test.ts\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "search",
        "documentation server",
        "document processing",
        "documentation code"
      ],
      "category": "document-processing"
    },
    "mario-andreschak--mcp-msoffice-interop-word": {
      "owner": "mario-andreschak",
      "name": "mcp-msoffice-interop-word",
      "url": "https://github.com/mario-andreschak/mcp-msoffice-interop-word",
      "imageUrl": "/freedevtools/mcp/pfp/mario-andreschak.webp",
      "description": "Interact programmatically with Microsoft Word documents, automating common Word processing tasks using a simple MCP interface. Facilitates document manipulation capabilities via COM Interop on Windows.",
      "stars": 12,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:32:25Z",
      "readme_content": "# MCP Office Interop Word Server\n\nThis project implements a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server that allows interaction with Microsoft Word documents using COM Interop on Windows.\n\nIt provides MCP tools to perform common Word processing tasks programmatically.\n\n## Features\n\n*   Wraps common Microsoft Word operations via COM Interop (`winax`).\n*   Exposes functionality as MCP tools.\n*   Supports both `stdio` and `sse` transports for MCP communication.\n*   Built with TypeScript and uses the `@modelcontextprotocol/sdk`.\n\n## Prerequisites\n\n*   Node.js (v18 or later recommended)\n*   npm\n*   Microsoft Word installed on a Windows machine.\n\n## Installation\n\n1.  Clone the repository or download the source code.\n2.  Navigate to the project directory in your terminal.\n3.  Install dependencies:\n    ```bash\n    npm install\n    ```\n\n## Building\n\nTo compile the TypeScript code to JavaScript:\n\n```bash\nnpm run build\n```\n\nThis will output the compiled files to the `dist` directory.\n\n## Running the Server\n\nThe server can run using two different MCP transports: `stdio` or `sse`.\n\n### stdio Transport\n\nThis is the default mode. It's suitable for local clients that communicate via standard input/output.\n\n```bash\nnpm start\n```\n\nor\n\n```bash\nnode dist/index.js\n```\n\nConnect your MCP client (e.g., MCP Inspector) using the stdio method, pointing to the `node dist/index.js` command.\n\n### SSE (Server-Sent Events) Transport\n\nThis mode uses HTTP and Server-Sent Events, suitable for web-based or remote clients.\n\n**PowerShell:**\n\n```powershell\n$env:MCP_TRANSPORT=\"sse\"; npm start\n```\n\n**Bash / Cmd:**\n\n```bash\nMCP_TRANSPORT=sse npm start\n```\n\nThe server will start an HTTP server, typically on port 3001 (or the port specified by the `PORT` environment variable).\n\n*   **SSE Endpoint:** `http://localhost:3001/sse`\n*   **Message Endpoint (for client POSTs):** `http://localhost:3001/messages`\n\nConnect your MCP client using the SSE method, providing the SSE endpoint URL.\n\n## Available Tools\n\nThe server exposes the following tools (tool names are prefixed with `word_`):\n\n**Document Operations:**\n\n*   `word_createDocument`: Creates a new, blank Word document.\n*   `word_openDocument`: Opens an existing document.\n    *   `filePath` (string): Absolute path to the document.\n*   `word_saveActiveDocument`: Saves the currently active document.\n*   `word_saveActiveDocumentAs`: Saves the active document to a new path/format.\n    *   `filePath` (string): Absolute path to save to.\n    *   `fileFormat` (number, optional): Numeric `WdSaveFormat` value (e.g., 16 for docx, 17 for pdf).\n*   `word_closeActiveDocument`: Closes the active document.\n    *   `saveChanges` (number, optional): `WdSaveOptions` value (0=No, -1=Yes, -2=Prompt). Default: 0.\n\n**Text Manipulation:**\n\n*   `word_insertText`: Inserts text at the selection.\n    *   `text` (string): Text to insert.\n*   `word_deleteText`: Deletes text relative to the selection.\n    *   `count` (number, optional): Number of units to delete (default: 1). Positive=forward, negative=backward.\n    *   `unit` (number, optional): `WdUnits` value (1=Char, 2=Word, etc.). Default: 1.\n*   `word_findAndReplace`: Finds and replaces text.\n    *   `findText` (string): Text to find.\n    *   `replaceText` (string): Replacement text.\n    *   `matchCase` (boolean, optional): Default: false.\n    *   `matchWholeWord` (boolean, optional): Default: false.\n    *   `replaceAll` (boolean, optional): Default: true.\n*   `word_toggleBold`: Toggles bold formatting for the selection.\n*   `word_toggleItalic`: Toggles italic formatting for the selection.\n*   `word_toggleUnderline`: Toggles underline formatting for the selection.\n    *   `underlineStyle` (number, optional): `WdUnderline` value (default: 1=Single).\n\n**Paragraph Formatting:**\n\n*   `word_setParagraphAlignment`: Sets paragraph alignment.\n    *   `alignment` (number): `WdParagraphAlignment` value (0=Left, 1=Center, 2=Right, 3=Justify).\n*   `word_setParagraphLeftIndent`: Sets left indent.\n    *   `indentPoints` (number): Indent value in points.\n*   `word_setParagraphRightIndent`: Sets right indent.\n    *   `indentPoints` (number): Indent value in points.\n*   `word_setParagraphFirstLineIndent`: Sets first line/hanging indent.\n    *   `indentPoints` (number): Indent value in points (positive=indent, negative=hanging).\n*   `word_setParagraphSpaceBefore`: Sets space before paragraphs.\n    *   `spacePoints` (number): Space value in points.\n*   `word_setParagraphSpaceAfter`: Sets space after paragraphs.\n    *   `spacePoints` (number): Space value in points.\n*   `word_setParagraphLineSpacing`: Sets line spacing.\n    *   `lineSpacingRule` (number): `WdLineSpacing` value (0=Single, 1=1.5, 2=Double, 3=AtLeast, 4=Exactly, 5=Multiple).\n    *   `lineSpacingValue` (number, optional): Value needed for rules 3, 4, 5.\n\n**Table Operations:**\n\n*   `word_addTable`: Adds a table at the selection.\n    *   `numRows` (number): Number of rows.\n    *   `numCols` (number): Number of columns.\n*   `word_setTableCellText`: Sets text in a table cell.\n    *   `tableIndex` (number): 1-based table index.\n    *   `rowIndex` (number): 1-based row index.\n    *   `colIndex` (number): 1-based column index.\n    *   `text` (string): Text to set.\n*   `word_insertTableRow`: Inserts a row into a table.\n    *   `tableIndex` (number): 1-based table index.\n    *   `beforeRowIndex` (number, optional): Insert before this 1-based row index (or at end if omitted).\n*   `word_insertTableColumn`: Inserts a column into a table.\n    *   `tableIndex` (number): 1-based table index.\n    *   `beforeColIndex` (number, optional): Insert before this 1-based column index (or at right end if omitted).\n*   `word_applyTableAutoFormat`: Applies a style to a table.\n    *   `tableIndex` (number): 1-based table index.\n    *   `formatName` (string | number): Style name or `WdTableFormat` value.\n\n**Image Operations:**\n\n*   `word_insertPicture`: Inserts an inline picture.\n    *   `filePath` (string): Absolute path to the image file.\n    *   `linkToFile` (boolean, optional): Default: false.\n    *   `saveWithDocument` (boolean, optional): Default: true.\n*   `word_setInlinePictureSize`: Resizes an inline picture.\n    *   `shapeIndex` (number): 1-based index of the inline shape.\n    *   `heightPoints` (number): Height in points (-1 or 0 to auto-size).\n    *   `widthPoints` (number): Width in points (-1 or 0 to auto-size).\n    *   `lockAspectRatio` (boolean, optional): Default: true.\n\n**Header/Footer Operations:**\n\n*   `word_setHeaderFooterText`: Sets text in a header or footer.\n    *   `text` (string): Text content.\n    *   `isHeader` (boolean): True for header, false for footer.\n    *   `sectionIndex` (number, optional): 1-based section index (default: 1).\n    *   `headerFooterType` (number, optional): `WdHeaderFooterIndex` value (1=Primary, 2=FirstPage, 3=EvenPages). Default: 1.\n\n**Page Setup Operations:**\n\n*   `word_setPageMargins`: Sets page margins.\n    *   `topPoints` (number): Top margin in points.\n    *   `bottomPoints` (number): Bottom margin in points.\n    *   `leftPoints` (number): Left margin in points.\n    *   `rightPoints` (number): Right margin in points.\n*   `word_setPageOrientation`: Sets page orientation.\n    *   `orientation` (number): `WdOrientation` value (0=Portrait, 1=Landscape).\n*   `word_setPaperSize`: Sets paper size.\n    *   `paperSize` (number): `WdPaperSize` value (e.g., 1=Letter, 8=A4).\n\n## Notes\n\n*   This server requires Microsoft Word to be installed and accessible via COM Interop on the machine where the server runs.\n*   Error handling for COM operations is basic. Robust production use might require more detailed error checking and recovery.\n*   Word object model constants (like `WdSaveFormat`, `WdUnits`, etc.) are represented by their numeric values in the tool arguments. You may need to refer to the Word VBA documentation for specific values.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "msoffice",
        "automating",
        "microsoft",
        "msoffice interop",
        "microsoft word",
        "mcp msoffice"
      ],
      "category": "document-processing"
    },
    "mawazawa--agentic-pdf-app": {
      "owner": "mawazawa",
      "name": "agentic-pdf-app",
      "url": "https://github.com/mawazawa/agentic-pdf-app",
      "imageUrl": "/freedevtools/mcp/pfp/mawazawa.webp",
      "description": "Automatically fills California court PDF forms by extracting and mapping data from donor documents through AI analysis. Features a minimalist interface and a modular microservices architecture for easy deployment using Docker.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T15:08:46Z",
      "readme_content": "# Agentic PDF Form Filler\n\nA visually stunning, minimalist system for automatically filling California court PDF forms using information extracted from donor documents.\n\n![Form Filling Process](https://mermaid.ink/img/pako:eNqtlMFO4zAQhl_F8rmVnMY-oC8A2nKACBBcuFRO4hFWc9ysYwtWqO_Ouly2Igu7XKLYmvnnn8-eiX3o2J7s1V0N4BuEe7DdnuxQKV-5mK-9cY2N0GGFyZVvQQsdrHRYIriP5F7rYzpH0XtdWUJk2wDnYGsNj7CmK6KzDhv2ELpBkDk1ELSxIURg-4YYexvNw5x8oO-4zy32hfUlX5cA2jYWIe5hXb40XPbkgFWu3fE7Hm0bSKj2rHJrPO8u3yEj-XLsMXxMBzJq_tPOH03-Ug7jzfH4qcef7qGLnYscZYezE96yK7VLPQW9UNlUJCBUB46gEkVBOEkpT3NaFIrndVmQvPg0aw_dNk1QLssTxT5FSk3JnZQiz1mqxKzKZZnTLFF5qkaVJ7OVDiPAIIx-MwNbnN6OsZGJj_jBuqiHxN4b-YcZHGJDr1t6XxvQp-dvTNZj7_DaIvjXBpuU-IXCWRN3FrwwdYWFc25pJKFsqU3_IOTC4YuIbYeXcTxJVt8wzlPGZSlTIUZMkpxTpZSk-UxykSsy40wzmRUinaU0S_MaTD9-hO25DPr0aBKGDHxpzCQUkLHoR_5mEo5CaPfvjdDnhA3Qv4h5sQ0gxFhM9h8ufVSJ?type=png)\n\n## Features\n\n- **Intelligent Document Extraction**: Uses Perplexity Sonar API or OpenAI API to extract data from various document types\n- **Smart Field Mapping**: Automatically maps extracted data to form fields\n- **Beautiful UI**: Clean, minimalist interface for easy form processing\n- **Docker-Based**: Simple deployment with Docker for both development and production\n- **Modular Design**: Microservices architecture for maintainability and scalability\n\n## Quick Start\n\n### Prerequisites\n\n- Docker and Docker Compose installed\n- API keys for Perplexity and/or OpenAI\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/agentic-pdf.git\ncd agentic-pdf\n```\n\n2. Create an `.env` file in the root directory:\n```\nPERPLEXITY_API_KEY=your_perplexity_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n3. Start the application in development mode:\n```bash\ndocker-compose up --build\n```\n\nThe services will be available at:\n- UI: http://localhost:8080\n- Orchestration Service API: http://localhost:3002\n- Other microservices: ports 3000-3005\n\n### Usage\n\n1. Open http://localhost:8080 in your browser\n2. Enter the URL of the California court form to fill (or use the default)\n3. Upload supporting documents (ID cards, previous forms, etc.)\n4. Wait for the system to process and fill the form\n5. Download the completed form\n\n## Architecture\n\nThis project follows a modern microservices architecture:\n\n- **UI Layer** (Next.js): Provides a clean, responsive interface\n- **MCP Servers** (Node.js/TypeScript): Implements Model Context Protocol servers for various services:\n  - Puppeteer Server: Downloads PDFs from URLs\n  - AI Analysis Server: Extracts form fields using AI vision\n  - Document Extraction Server: Analyzes donor documents\n  - Field Mapping Service: Maps donor data to form fields\n  - Form Filling Server: Fills the PDF with mapped data\n- **Orchestration Layer**: Coordinates the workflow between services\n\n## Development\n\n### Project Structure\n\n```\nagentic-pdf/\n├── docker-compose.yml         # Main docker-compose configuration\n├── docker-compose.override.yml # Development overrides\n├── docker-compose.prod.yml    # Production configuration\n├── nginx.conf                 # Nginx config for production\n├── src/\n│   └── mcp-servers/           # Backend services (TypeScript)\n│       ├── src/\n│       │   ├── config/        # Configuration\n│       │   ├── orchestration/ # Workflow orchestrators\n│       │   ├── servers/       # MCP server implementations\n│       │   └── services/      # Service implementations\n│       └── Dockerfile         # Multi-stage Dockerfile\n└── ui/                        # Frontend (Next.js)\n    ├── src/\n    │   ├── components/        # React components\n    │   ├── pages/             # Next.js pages\n    │   └── styles/            # CSS styles\n    └── Dockerfile             # Multi-stage Dockerfile\n```\n\n### Local Development\n\nFor local development with automatic reloading:\n\n```bash\ndocker-compose up\n```\n\n### Production Deployment\n\nFor a production deployment:\n\n```bash\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n```\n\n## Design Principles\n\nThis project adheres to several key design principles:\n\n1. **Minimalism**: Clean interface and streamlined user experience\n2. **Separation of Concerns**: Each component has a single, well-defined responsibility\n3. **Visual Harmony**: Consistent use of typography, color, and space\n4. **Progressive Disclosure**: Complexity is revealed only when needed\n5. **Error Prevention**: Validation and clear user guidance\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "microservices",
        "documents",
        "document",
        "documents ai",
        "document processing",
        "pdf app"
      ],
      "category": "document-processing"
    },
    "nCrom--readme-updater-mcp": {
      "owner": "nCrom",
      "name": "readme-updater-mcp",
      "url": "https://github.com/nCrom/readme-updater-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Enhance your README.md files effortlessly by analyzing and resolving content conflicts with Ollama. Automatically update your documentation while ensuring consistency and clarity. Streamline your project documentation process with intelligent suggestions and conflict resolution.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ncrom",
        "documentation",
        "document",
        "processing ncrom",
        "ncrom readme",
        "document processing"
      ],
      "category": "document-processing"
    },
    "nathanonn--mcp-url-fetcher": {
      "owner": "nathanonn",
      "name": "mcp-url-fetcher",
      "url": "https://github.com/nathanonn/mcp-url-fetcher",
      "imageUrl": "/freedevtools/mcp/pfp/nathanonn.webp",
      "description": "Fetch and transform web content from any URL into formats like HTML, JSON, Markdown, or plain text. This MCP server supports various input types and intelligently detects source formats for seamless content conversion.",
      "stars": 4,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-13T18:29:14Z",
      "readme_content": "# MCP URL Format Converter\n\nA Model Context Protocol (MCP) server that fetches content from any URL and converts it to your desired output format.\n\n## Overview\n\nMCP URL Format Converter provides tools for retrieving content from any web URL and transforming it into various formats (HTML, JSON, Markdown, or plain text), regardless of the original content type. It's designed to work with any MCP-compatible client, including Claude for Desktop, enabling LLMs to access, transform, and analyze web content in a consistent format.\n\n## Features\n\n- 🔄 **Format Conversion**: Transform any web content to HTML, JSON, Markdown, or plain text\n- 🌐 **Universal Input Support**: Handle websites, APIs, raw files, and more\n- 🔍 **Automatic Content Detection**: Intelligently identifies source format\n- 🧰 **Robust Library Support**: Uses industry-standard libraries:\n  - Cheerio for HTML parsing\n  - Marked for Markdown processing\n  - Fast-XML-Parser for XML handling\n  - CSVtoJSON for CSV conversion\n  - SanitizeHTML for security\n  - Turndown for HTML-to-Markdown conversion\n- 🔧 **Advanced Format Processing**:\n  - HTML parsing with metadata extraction\n  - JSON pretty-printing and structure preservation\n  - Markdown rendering with styling\n  - CSV-to-table conversion\n  - XML-to-JSON transformation\n- 📜 **History Tracking**: Maintains logs of recently fetched URLs\n- 🛡️ **Security Focus**: Content sanitization to prevent XSS attacks\n\n## Installation\n\n### Prerequisites\n\n- Node.js 16.x or higher\n- npm or yarn\n\n### Quick Start\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-url-converter.git\n   cd mcp-url-converter\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n\n   ```bash\n   npm run build\n   ```\n\n4. Run the server:\n   ```bash\n   npm start\n   ```\n\n## Integration with Claude for Desktop\n\n1. Open your Claude for Desktop configuration file:\n\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the URL converter server to your configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"url-converter\": {\n         \"command\": \"node\",\n         \"args\": [\"/absolute/path/to/mcp-url-converter/build/index.js\"]\n       }\n     }\n   }\n   ```\n\n3. Restart Claude for Desktop\n\n## Available Tools\n\n### `fetch`\n\nFetches content from any URL and automatically detects the best output format.\n\n**Parameters:**\n\n- `url` (string, required): The URL to fetch content from\n- `format` (string, optional): Format to convert to (`auto`, `html`, `json`, `markdown`, `text`). Default: `auto`\n\n**Example:**\n\n```\nCan you fetch https://example.com and choose the best format to display it?\n```\n\n### `fetch-json`\n\nFetches content from any URL and converts it to JSON format.\n\n**Parameters:**\n\n- `url` (string, required): The URL to fetch content from\n- `prettyPrint` (boolean, optional): Whether to pretty-print the JSON. Default: `true`\n\n**Example:**\n\n```\nCan you fetch https://example.com and convert it to JSON format?\n```\n\n### `fetch-html`\n\nFetches content from any URL and converts it to HTML format.\n\n**Parameters:**\n\n- `url` (string, required): The URL to fetch content from\n- `extractText` (boolean, optional): Whether to extract text content only. Default: `false`\n\n**Example:**\n\n```\nCan you fetch https://api.example.com/users and convert it to HTML?\n```\n\n### `fetch-markdown`\n\nFetches content from any URL and converts it to Markdown format.\n\n**Parameters:**\n\n- `url` (string, required): The URL to fetch content from\n\n**Example:**\n\n```\nCan you fetch https://example.com and convert it to Markdown?\n```\n\n### `fetch-text`\n\nFetches content from any URL and converts it to plain text format.\n\n**Parameters:**\n\n- `url` (string, required): The URL to fetch content from\n\n**Example:**\n\n```\nCan you fetch https://example.com and convert it to plain text?\n```\n\n### `web-search` and `deep-research`\n\nThese tools provide interfaces to Perplexity search capabilities (when supported by the MCP host).\n\n## Available Resources\n\n### `recent-urls://list`\n\nReturns a list of recently fetched URLs with timestamps and output formats.\n\n**Example:**\n\n```\nWhat URLs have I fetched recently?\n```\n\n## Security\n\nThis server implements several security measures:\n\n- HTML sanitization using `sanitize-html` to prevent XSS attacks\n- Content validation before processing\n- Error handling and safe defaults\n- Input parameter validation with Zod\n- Safe output encoding\n\n## Testing\n\nYou can test the server using the MCP Inspector:\n\n```bash\nnpm run test\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection errors**: Verify that the URL is accessible and correctly formatted\n2. **Conversion errors**: Some complex content may not convert cleanly between formats\n3. **Cross-origin issues**: Some websites may block requests from unknown sources\n\n### Debug Mode\n\nFor additional debugging information, set the `DEBUG` environment variable:\n\n```bash\nDEBUG=mcp:* npm start\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- Built with the [Model Context Protocol](https://modelcontextprotocol.io/)\n- Uses modern, actively maintained libraries with security focus\n- Sanitization approach based on OWASP recommendations\n\n---\n\nLast updated: 29 March 2025\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "formats",
        "html",
        "mcp url",
        "mcp server",
        "url formats"
      ],
      "category": "document-processing"
    },
    "ndchikin--reference-mcp": {
      "owner": "ndchikin",
      "name": "reference-mcp",
      "url": "https://github.com/ndchikin/reference-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ndchikin.webp",
      "description": "Retrieve BibTeX-formatted citation data from CiteAs and Google Scholar to streamline citation management in research applications.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-07T07:55:23Z",
      "readme_content": "# CiteAssist MCP server\n\n[![smithery badge](https://smithery.ai/badge/@ndchikin/reference-mcp)](https://smithery.ai/server/@ndchikin/reference-mcp)\n\nA Model Context Protocol server that provides BibTeX-formatted citation data from CiteAs and Google Scholar. Enhance your research workflow by integrating citation retrieval directly into your applications.\n\n## Components\n\n### Tools\n\n* `get_citeas_data` - Retrieve BibTeX-formatted citation for the specified resource from the CiteAs\n  * `resource` (string, required): DOI, URL, keyword\n* `get_scholar_data` - Retrieve BibTeX-formatted citations from the Google Scholar\n  * `query` (string, required): Search query\n  * `results` (integer, optional): Number of results (default: 2)\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nDevelopment/Unpublished Servers Configuration:\n\n```json\n\"mcpServers\": {\n  \"reference-mcp\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/path/to/project/dir\",\n      \"run\",\n      \"reference-mcp\"\n    ]\n  }\n}\n```\n\nPublished Servers Configuration:\n\n```json\n\"mcpServers\": {\n  \"reference-mcp\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"reference-mcp\"\n    ]\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install reference-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ndchikin/reference-mcp):\n\n```bash\nnpx -y @smithery/cli install @ndchikin/reference-mcp --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/project/dir run reference-mcp\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bibtex",
        "citation",
        "citeas",
        "citation management",
        "citation data",
        "streamline citation"
      ],
      "category": "document-processing"
    },
    "nfshanq--mcp-invoice": {
      "owner": "nfshanq",
      "name": "mcp-invoice",
      "url": "https://github.com/nfshanq/mcp-invoice",
      "imageUrl": "/freedevtools/mcp/pfp/nfshanq.webp",
      "description": "Advanced OCR capabilities for invoice and receipt management, enabling data extraction from various formats and document merging for efficient handling.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-06T06:07:14Z",
      "readme_content": "# MCP Invoice\n\n这是一个使用 Python 开发的 MCP (Model Context Protocol) 服务器项目，专注于发票和收据处理的工具集合。该项目利用 OCR 技术实现发票识别、数据提取和 PDF 处理功能，为 AI 助手提供发票处理能力。\n\n## 功能特点\n\n- 发票和收据 OCR 处理：\n  - 使用 macOS Vision framework 进行高精度文本识别\n  - 支持处理单个 PDF 或图片文件（JPEG、PNG）\n  - 支持批量处理目录下的所有发票和收据文件\n  - 支持中英文 OCR 识别，适用于各类发票格式\n  - 提供结构化数据提取，方便后续分析\n- PDF合并和管理功能：\n  - 支持将多个发票或收据 PDF 文件合并为一个\n  - 支持将多张发票图片（JPEG、PNG）合并为一个 PDF 文件\n  - 支持混合合并不同格式的发票和收据\n- 高效的内存处理方式，减少磁盘 I/O，提高处理速度\n\n## MCP 部署指南\n\n### 环境要求\n\n- macOS 系统（支持 Vision framework）\n- Python 3.10 或更高版本\n- uv 依赖管理工具\n\n### 从 GitHub 安装\n\n1. 确保已安装 uv：\n\n```bash\npip install uv\n```\n\n2. 从 GitHub 克隆项目并安装依赖：\n\n```bash\ngit clone https://github.com/[username]/mcp-invoice.git\ncd mcp-invoice\nuv venv\nsource .venv/bin/activate\nuv pip install -e .\n```\n\n### 安装 PDF 处理依赖\n\npdf2image 需要系统上安装 Poppler：\n\n```bash\nbrew install poppler\n```\n\n### 部署为 MCP 服务\n\n有两种方式部署 MCP 服务：\n\n#### 1. 直接运行服务器\n\n```bash\n# 标准模式\nmcp-invoice\n\n# 或启用调试模式（包含文本位置信息）\nMCP_INVOICE_DEBUG=true mcp-invoice\n```\n\n#### 2. 使用 invoice_server.py 脚本\n\n```bash\n# 标准模式\npython invoice_server.py\n\n# 或启用调试模式\nMCP_INVOICE_DEBUG=true python invoice_server.py\n```\n\n### 配置为系统服务\n\n可以将 MCP 服务配置为系统服务，使其在系统启动时自动运行：\n\n1. 创建 LaunchAgent plist 文件：\n\n```bash\nmkdir -p ~/Library/LaunchAgents\ncat > ~/Library/LaunchAgents/com.user.mcp-invoice.plist << EOF\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.user.mcp-invoice</string>\n    <key>ProgramArguments</key>\n    <array>\n        <string>$(which python)</string>\n        <string>$(pwd)/invoice_server.py</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n    <key>KeepAlive</key>\n    <true/>\n    <key>StandardOutPath</key>\n    <string>$(pwd)/logs/mcp-invoice.log</string>\n    <key>StandardErrorPath</key>\n    <string>$(pwd)/logs/mcp-invoice-error.log</string>\n    <key>WorkingDirectory</key>\n    <string>$(pwd)</string>\n    <key>EnvironmentVariables</key>\n    <dict>\n        <key>PATH</key>\n        <string>/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin</string>\n    </dict>\n</dict>\n</plist>\nEOF\n\n# 创建日志目录\nmkdir -p logs\n\n# 加载服务\nlaunchctl load ~/Library/LaunchAgents/com.user.mcp-invoice.plist\n```\n\n2. 启动服务：\n\n```bash\nlaunchctl start com.user.mcp-invoice\n```\n\n3. 停止服务：\n\n```bash\nlaunchctl stop com.user.mcp-invoice\n```\n\n## AI 编辑器集成\n\n### 在 Cursor 中配置 MCP\n\n在 Cursor 编辑器中，可以通过以下配置添加 MCP Invoice 服务：\n\n1. 打开 Cursor 的 MCP 配置文件：\n   - macOS: `$HOME/Library/Application Support/Cursor/tools/tools.json`\n   - Linux: `$HOME/.config/Cursor/tools/tools.json`\n   - Windows: `%APPDATA%\\Cursor\\tools\\tools.json`\n\n2. 在 `tools.json` 文件中添加 MCP Invoice 服务配置：\n\n```json\n{\n  \"tools\": {\n    \"invoice\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/mcp-invoice\",\n        \"run\",\n        \"invoice_server.py\"\n      ],\n      \"env\": {\n        \"MCP_INVOICE_DEBUG\": \"false\"\n      },\n      \"alwaysAllow\": [\n        \"process_file\",\n        \"process_directory\",\n        \"merge_pdfs\"\n      ]\n    }\n  }\n}\n```\n\n3. 重启 Cursor 后，你可以在 AI 助手中直接请求使用功能，例如：\n   \"请使用 OCR 工具读取文件：/path/to/document.pdf\"\n   \"请合并这些 PDF 文件：/path/to/file1.pdf, /path/to/file2.pdf\"\n\n### 在 Cline 中配置 MCP\n\nCline 与 MCP 集成方法类似于 Cursor：\n\n1. 更新 Cline 配置文件来添加此 MCP 工具\n2. 重启 Cline 客户端\n3. 使用与 Cursor 相同的方式来请求处理发票文件\n\n### 在 Roocode 中配置 MCP\n\n在 Roocode 中集成 MCP Invoice 服务：\n\n1. 按照 Roocode 文档配置外部工具\n2. 指向已部署的 MCP Invoice 服务\n3. 在 Roocode 中使用自然语言请求处理发票文件\n\n## MCP 服务调试\n\n1. 启用详细日志：\n\n```bash\nMCP_INVOICE_DEBUG=true mcp-invoice\n```\n\n2. 检查服务是否正常运行：\n\n```bash\nps aux | grep mcp-invoice\n```\n\n3. 查看日志：\n\n```bash\ntail -f logs/mcp-invoice.log\n```\n\n## 使用方法\n\nMCP 服务器提供以下主要工具：\n\n1. `process_file`: 处理单个文件并提取文本\n   - 参数：`file_path` - 文件的绝对路径\n   - 返回：包含文件路径和提取文本的字典\n\n2. `process_directory`: 处理一个目录中的所有 PDF 和图片文件\n   - 参数：`directory_path` - 目录的绝对路径\n   - 返回：包含每个文件路径和提取文本的字典列表\n\n3. `merge_pdfs`: 合并多个PDF和/或图片文件为一个PDF\n   - 参数：`file_paths` - 要合并的文件路径列表\n   - 参数：`output_path` - 输出PDF的路径\n   - 返回：合并后PDF的路径\n\n## 开发和扩展\n\n本项目使用 uv 进行依赖管理，使用 hatch 进行构建。\n\n### 开发环境设置\n\n1. 创建虚拟环境：\n\n```bash\nuv venv\nsource .venv/bin/activate\n```\n\n2. 安装开发依赖：\n\n```bash\nuv pip install -e \".[dev]\"\n```\n\n## 故障排除\n\n1. **Vision framework 错误**\n   - 确保使用 macOS 系统\n   - 确保已安装 pyobjc-framework-vision 11.0 或更高版本\n\n2. **UTF-8 编码问题**\n   - 所有 OCR 结果均使用 UTF-8 编码处理，确保多语言文本正确显示\n\n3. **PDF 转换错误**\n   - 确保已安装 poppler\n   - 检查 PDF 文件是否有权限访问和可读\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "invoice",
        "nfshanq",
        "mcp invoice",
        "ocr capabilities",
        "document processing"
      ],
      "category": "document-processing"
    },
    "nloui--paperless-mcp": {
      "owner": "nloui",
      "name": "paperless-mcp",
      "url": "https://github.com/nloui/paperless-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nloui.webp",
      "description": "Manage documents, tags, correspondents, and document types through the Paperless-NGX API. Enables efficient organization and retrieval of document-related information.",
      "stars": 88,
      "forks": 24,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T12:04:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/nloui-paperless-mcp-badge.png)](https://mseep.ai/app/nloui-paperless-mcp)\n\n# Paperless-NGX MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nloui/paperless-mcp)](https://smithery.ai/server/@nloui/paperless-mcp)\n\nAn MCP (Model Context Protocol) server for interacting with a Paperless-NGX API server. This server provides tools for managing documents, tags, correspondents, and document types in your Paperless-NGX instance.\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install Paperless NGX MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nloui/paperless-mcp):\n\n```bash\nnpx -y @smithery/cli install @nloui/paperless-mcp --client claude\n```\n\n### Manual Installation\n1. Install the MCP server:\n```bash\nnpm install -g paperless-mcp\n```\n\n2. Add it to your Claude's MCP configuration:\n\nFor VSCode extension, edit `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`:\n```json\n{\n  \"mcpServers\": {\n    \"paperless\": {\n      \"command\": \"npx\",\n      \"args\": [\"paperless-mcp\", \"http://your-paperless-instance:8000\", \"your-api-token\"]\n    }\n  }\n}\n```\n\nFor Claude desktop app, edit `~/Library/Application Support/Claude/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"paperless\": {\n      \"command\": \"npx\",\n      \"args\": [\"paperless-mcp\", \"http://your-paperless-instance:8000\", \"your-api-token\"]\n    }\n  }\n}\n```\n\n3. Get your API token:\n   1. Log into your Paperless-NGX instance\n   2. Click your username in the top right\n   3. Select \"My Profile\"\n   4. Click the circular arrow button to generate a new token\n\n4. Replace the placeholders in your MCP config:\n   - `http://your-paperless-instance:8000` with your Paperless-NGX URL\n   - `your-api-token` with the token you just generated\n\nThat's it! Now you can ask Claude to help you manage your Paperless-NGX documents.\n\n## Example Usage\n\nHere are some things you can ask Claude to do:\n\n- \"Show me all documents tagged as 'Invoice'\"\n- \"Search for documents containing 'tax return'\"\n- \"Create a new tag called 'Receipts' with color #FF0000\"\n- \"Download document #123\"\n- \"List all correspondents\"\n- \"Create a new document type called 'Bank Statement'\"\n\n## Available Tools\n\n### Document Operations\n\n#### list_documents\nGet a paginated list of all documents.\n\nParameters:\n- page (optional): Page number\n- page_size (optional): Number of documents per page\n\n```typescript\nlist_documents({\n  page: 1,\n  page_size: 25\n})\n```\n\n#### get_document\nGet a specific document by ID.\n\nParameters:\n- id: Document ID\n\n```typescript\nget_document({\n  id: 123\n})\n```\n\n#### search_documents\nFull-text search across documents.\n\nParameters:\n- query: Search query string\n\n```typescript\nsearch_documents({\n  query: \"invoice 2024\"\n})\n```\n\n#### download_document\nDownload a document file by ID.\n\nParameters:\n- id: Document ID\n- original (optional): If true, downloads original file instead of archived version\n\n```typescript\ndownload_document({\n  id: 123,\n  original: false\n})\n```\n\n#### bulk_edit_documents\nPerform bulk operations on multiple documents.\n\nParameters:\n- documents: Array of document IDs\n- method: One of:\n  - set_correspondent: Set correspondent for documents\n  - set_document_type: Set document type for documents\n  - set_storage_path: Set storage path for documents\n  - add_tag: Add a tag to documents\n  - remove_tag: Remove a tag from documents\n  - modify_tags: Add and/or remove multiple tags\n  - delete: Delete documents\n  - reprocess: Reprocess documents\n  - set_permissions: Set document permissions\n  - merge: Merge multiple documents\n  - split: Split a document into multiple documents\n  - rotate: Rotate document pages\n  - delete_pages: Delete specific pages from a document\n- Additional parameters based on method:\n  - correspondent: ID for set_correspondent\n  - document_type: ID for set_document_type\n  - storage_path: ID for set_storage_path\n  - tag: ID for add_tag/remove_tag\n  - add_tags: Array of tag IDs for modify_tags\n  - remove_tags: Array of tag IDs for modify_tags\n  - permissions: Object for set_permissions with owner, permissions, merge flag\n  - metadata_document_id: ID for merge to specify metadata source\n  - delete_originals: Boolean for merge/split\n  - pages: String for split \"[1,2-3,4,5-7]\" or delete_pages \"[2,3,4]\"\n  - degrees: Number for rotate (90, 180, or 270)\n\nExamples:\n```typescript\n// Add a tag to multiple documents\nbulk_edit_documents({\n  documents: [1, 2, 3],\n  method: \"add_tag\",\n  tag: 5\n})\n\n// Set correspondent and document type\nbulk_edit_documents({\n  documents: [4, 5],\n  method: \"set_correspondent\",\n  correspondent: 2\n})\n\n// Merge documents\nbulk_edit_documents({\n  documents: [6, 7, 8],\n  method: \"merge\",\n  metadata_document_id: 6,\n  delete_originals: true\n})\n\n// Split document into parts\nbulk_edit_documents({\n  documents: [9],\n  method: \"split\",\n  pages: \"[1-2,3-4,5]\"\n})\n\n// Modify multiple tags at once\nbulk_edit_documents({\n  documents: [10, 11],\n  method: \"modify_tags\",\n  add_tags: [1, 2],\n  remove_tags: [3, 4]\n})\n```\n\n#### post_document\nUpload a new document to Paperless-NGX.\n\nParameters:\n- file: Base64 encoded file content\n- filename: Name of the file\n- title (optional): Title for the document\n- created (optional): DateTime when the document was created (e.g. \"2024-01-19\" or \"2024-01-19 06:15:00+02:00\")\n- correspondent (optional): ID of a correspondent\n- document_type (optional): ID of a document type\n- storage_path (optional): ID of a storage path\n- tags (optional): Array of tag IDs\n- archive_serial_number (optional): Archive serial number\n- custom_fields (optional): Array of custom field IDs\n\n```typescript\npost_document({\n  file: \"base64_encoded_content\",\n  filename: \"invoice.pdf\",\n  title: \"January Invoice\",\n  created: \"2024-01-19\",\n  correspondent: 1,\n  document_type: 2,\n  tags: [1, 3],\n  archive_serial_number: \"2024-001\"\n})\n```\n\n### Tag Operations\n\n#### list_tags\nGet all tags.\n\n```typescript\nlist_tags()\n```\n\n#### create_tag\nCreate a new tag.\n\nParameters:\n- name: Tag name\n- color (optional): Hex color code (e.g. \"#ff0000\")\n- match (optional): Text pattern to match\n- matching_algorithm (optional): One of \"any\", \"all\", \"exact\", \"regular expression\", \"fuzzy\"\n\n```typescript\ncreate_tag({\n  name: \"Invoice\",\n  color: \"#ff0000\",\n  match: \"invoice\",\n  matching_algorithm: \"fuzzy\"\n})\n```\n\n### Correspondent Operations\n\n#### list_correspondents\nGet all correspondents.\n\n```typescript\nlist_correspondents()\n```\n\n#### create_correspondent\nCreate a new correspondent.\n\nParameters:\n- name: Correspondent name\n- match (optional): Text pattern to match\n- matching_algorithm (optional): One of \"any\", \"all\", \"exact\", \"regular expression\", \"fuzzy\"\n\n```typescript\ncreate_correspondent({\n  name: \"ACME Corp\",\n  match: \"ACME\",\n  matching_algorithm: \"fuzzy\"\n})\n```\n\n### Document Type Operations\n\n#### list_document_types\nGet all document types.\n\n```typescript\nlist_document_types()\n```\n\n#### create_document_type\nCreate a new document type.\n\nParameters:\n- name: Document type name\n- match (optional): Text pattern to match\n- matching_algorithm (optional): One of \"any\", \"all\", \"exact\", \"regular expression\", \"fuzzy\"\n\n```typescript\ncreate_document_type({\n  name: \"Invoice\",\n  match: \"invoice total amount due\",\n  matching_algorithm: \"any\"\n})\n```\n\n## Error Handling\n\nThe server will show clear error messages if:\n- The Paperless-NGX URL or API token is incorrect\n- The Paperless-NGX server is unreachable\n- The requested operation fails\n- The provided parameters are invalid\n\n## Development\n\nWant to contribute or modify the server? Here's what you need to know:\n\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Make your changes to server.js\n4. Test locally:\n```bash\nnode server.js http://localhost:8000 your-test-token\n```\n\nThe server is built with:\n- [litemcp](https://github.com/wong2/litemcp): A TypeScript framework for building MCP servers\n- [zod](https://github.com/colinhacks/zod): TypeScript-first schema validation\n\n## API Documentation\n\nThis MCP server implements endpoints from the Paperless-NGX REST API. For more details about the underlying API, see the [official documentation](https://docs.paperless-ngx.com/api/).\n\n## Running the MCP Server\n\nThe MCP server can be run in two modes:\n\n### 1. stdio (default)\n\nThis is the default mode. The server communicates over stdio, suitable for CLI and direct integrations.\n\n```\nnpm run start -- <baseUrl> <token>\n```\n\n### 2. HTTP (Streamable HTTP Transport)\n\nTo run the server as an HTTP service, use the `--http` flag. You can also specify the port with `--port` (default: 3000). This mode requires [Express](https://expressjs.com/) to be installed (it is included as a dependency).\n\n```\nnpm run start -- <baseUrl> <token> --http --port 3000\n```\n\n- The MCP API will be available at `POST /mcp` on the specified port.\n- Each request is handled statelessly, following the [StreamableHTTPServerTransport](https://github.com/modelcontextprotocol/typescript-sdk) pattern.\n- GET and DELETE requests to `/mcp` will return 405 Method Not Allowed.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperless",
        "document",
        "documents",
        "paperless ngx",
        "nloui paperless",
        "document processing"
      ],
      "category": "document-processing"
    },
    "oakenai--mcp-edit-file-lines": {
      "owner": "oakenai",
      "name": "mcp-edit-file-lines",
      "url": "https://github.com/oakenai/mcp-edit-file-lines",
      "imageUrl": "/freedevtools/mcp/pfp/oakenai.webp",
      "description": "Make precise line-based edits to text files using string or regex pattern matching, including the ability to replace entire lines, specific text matches, and handle multiple edits with a preview function for safety.",
      "stars": 29,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T05:55:40Z",
      "readme_content": "# Edit File Lines MCP Server\n\nA TypeScript-based MCP server that provides tools for making precise line-based edits to text files within allowed directories.\n\n## Features\n\n### Main Editing Tool\n\n#### `edit_file_lines`\nMake line-based edits to a file using string or regex pattern matching. Each edit can:\n- Replace entire lines\n- Replace specific text matches while preserving line formatting\n- Use regex patterns for complex matches\n- Handle multiple lines and multiple edits\n- Preview changes with dry run mode\n\nExample file (`src/components/App.tsx`):\n```typescript\n// Basic component with props\nconst Button = ({ color = \"blue\", size = \"md\" }) => {\n  return <button className={`btn-${color} size-${size}`}>Click me</button>;\n};\n\n// Component with multiple props and nested structure\nexport const Card = ({\n  title,\n  subtitle = \"Default subtitle\",\n  theme = \"light\",\n  size = \"lg\",\n}) => {\n  const cardClass = `card-${theme} size-${size}`;\n  \n  return (\n    <div className={cardClass}>\n      <h2>{title}</h2>\n      <p>{subtitle}</p>\n    </div>\n  );\n};\n\n// Constants and configurations\nconst THEME = {\n  light: { bg: \"#ffffff\", text: \"#000000\" },\n  dark: { bg: \"#000000\", text: \"#ffffff\" },\n};\n\nconst CONFIG = {\n  apiUrl: \"https://api.example.com\",\n  timeout: 5000,\n  retries: 3,\n};\n```\n\n### Example Use Cases\n\n1. Simple String Replacement\n```json\n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 2,\n    \"endLine\": 2,\n    \"content\": \"primary\",\n    \"strMatch\": \"blue\"\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -1,6 +1,6 @@\n // Basic component with props\n-const Button = ({ color = \"blue\", size = \"md\" }) => {\n+const Button = ({ color = \"primary\", size = \"md\" }) => {\n   return Click me;\n };\n \n // Component with multiple props and nested structure\n ```\n\nState ID: fcbf740a\nUse this ID with approve_edit to apply the changes.\n\n\n2. Multi-line Content with Preserved Structure  \n```json\n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 16,\n    \"endLine\": 19,\n    \"content\": \"    <div className={cardClass}>\\n      <h2 className=\\\"title\\\">{title}</h2>\\n      <p className=\\\"subtitle\\\">{subtitle}</p>\\n    </div>\",\n    \"regexMatch\": \"<div[^>]*>[\\\\s\\\\S]*?</div>\"\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -13,10 +13,10 @@\n   const cardClass = `card-${theme} size-${size}`;\n   \n   return (\n     <div className={cardClass}>\n-      <h2>{title}</h2>\n-      <p>{subtitle}</p>\n+      <h2 className=\"title\">{title}</h2>\n+      <p className=\"subtitle\">{subtitle}</p>\n     </div>\n   );\n };\n```\nState ID: f2ce973f\nUse this ID with approve_edit to apply the changes.\n\n\n3. Complex JSX Structure Modification\n```json\n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 7,\n    \"endLine\": 12,\n    \"content\": \"export const Card = ({\\n  title,\\n  subtitle = \\\"New default\\\",\\n  theme = \\\"modern\\\",\\n  size = \\\"responsive\\\"\\n}) => {\",\n    \"regexMatch\": \"export const Card[\\\\s\\\\S]*?\\\\) => \\\\{\"\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -5,11 +5,11 @@\n // Component with multiple props and nested structure\n export const Card = ({\n   title,\n-  subtitle = \"Default subtitle\",\n-  theme = \"light\",\n-  size = \"lg\",\n+  subtitle = \"New default\",\n+  theme = \"modern\",\n+  size = \"responsive\"\n }) => {\n   const cardClass = `card-${theme} size-${size}`;\n   \n   return (\n```   \nState ID: f1f1d27b\nUse this ID with approve_edit to apply the changes.\n\n\n4. Configuration Update with Whitespace Preservation\n```json\n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 29,\n    \"endLine\": 32,\n    \"content\": \"const CONFIG = {\\n  baseUrl: \\\"https://api.newexample.com\\\",\\n  timeout: 10000,\\n  maxRetries: 5\",\n    \"regexMatch\": \"const CONFIG[\\\\s\\\\S]*?retries: \\\\d+\"\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -26,8 +26,8 @@\n   dark: { bg: \"#000000\", text: \"#ffffff\" },\n };\n \n const CONFIG = {\n-  apiUrl: \"https://api.example.com\",\n-  timeout: 5000,\n-  retries: 3,\n+  baseUrl: \"https://api.newexample.com\",\n+  timeout: 10000,\n+  maxRetries: 5\n };\n```\nState ID: 20e93c34\nUse this ID with approve_edit to apply the changes.\n\n5. Flexible Whitespace Matching\n```json\n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 9,\n    \"endLine\": 9,\n    \"content\": \"description\",\n    \"strMatch\": \"subtitle   =   \\\"Default subtitle\\\"\"  // Extra spaces are handled\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -5,9 +5,9 @@\n // Component with multiple props and nested structure\n export const Card = ({\n   title,\n-  subtitle = \"Default subtitle\",\n+  description\n   theme = \"light\",\n   size = \"lg\",\n }) => {\n   const cardClass = `card-${theme} size-${size}`;\n```\n\n### Additional Tools\n\n#### `approve_edit`\nApply changes from a previous dry run of `edit_file_lines`. This tool provides a two-step editing process for safety. Here is an example workflow:\n\n1. First, make a dry run edit:\n```json \n{\n  \"p\": \"src/components/App.tsx\",\n  \"e\": [{\n    \"startLine\": 2,\n    \"endLine\": 2,\n    \"content\": \"primary\",\n    \"strMatch\": \"blue\"\n  }],\n  \"dryRun\": true\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -1,6 +1,6 @@\n // Basic component with props\n-const Button = ({ color = \"blue\", size = \"md\" }) => {\n+const Button = ({ color = \"primary\", size = \"md\" }) => {\n   return <button className={`btn-${color} size-${size}`}>Click me</button>;\n };\n ```\n\nState ID: fcbf740a\nUse this ID with approve_edit to apply the changes.\n\n\n2. Then, approve the changes using the state ID:\n```json\n{\n  \"stateId\": \"fcbf740a\"\n}\n```\n\nOutput:\n```diff\nIndex: src/components/App.tsx\n===================================================================\n--- src/components/App.tsx        original\n+++ src/components/App.tsx        modified\n@@ -1,6 +1,6 @@\n // Basic component with props\n-const Button = ({ color = \"blue\", size = \"md\" }) => {\n+const Button = ({ color = \"primary\", size = \"md\" }) => {\n   return <button className={`btn-${color} size-${size}`}>Click me</button>;\n };\n```\n\n3. Verify the changes:\n```json\n{\n  \"path\": \"src/components/App.tsx\",\n  \"lineNumbers\": [2],\n  \"context\": 1\n}\n```\n\nOutput:\n```\nLine 2:\n  1: // Basic component with props\n> 2: const Button = ({ color = \"primary\", size = \"md\" }) => {\n  3:   return <button className={`btn-${color} size-${size}`}>Click me</button>;\n```\n\nNote that state IDs expire after a short time for security. Attempting to use an expired or invalid state ID will result in an error:\n```json\n{\n  \"stateId\": \"invalid123\"\n}\n```\n\nOutput:\n```\nError: Invalid or expired state ID\n```\n\n#### `get_file_lines`\nInspect specific lines in a file with optional context lines. This tool is useful for verifying line content before making edits.\n\n```json\n{\n  \"path\": \"src/components/App.tsx\",\n  \"lineNumbers\": [1, 2, 3],\n  \"context\": 1\n}\n```\n\nOutput:\n```\nLine 1:\n> 1: // Basic component with props\n  2: const Button = ({ color = \"blue\", size = \"md\" }) => {\n\nLine 2:\n  1: // Basic component with props\n> 2: const Button = ({ color = \"blue\", size = \"md\" }) => {\n  3:   return Click me;\n\nLine 3:\n  2: const Button = ({ color = \"blue\", size = \"md\" }) => {\n> 3:   return Click me;\n  4: };\n```\n\n#### `search_file`\nSearch a file for text patterns or regular expressions to find specific line numbers and their surrounding context. This tool is particularly useful for locating the exact lines you want to edit with `edit_file_lines`.\n\nFeatures:\n- Simple text search with optional case sensitivity\n- Regular expression support\n- Whole word matching\n- Configurable context lines\n- Returns line numbers, content, and surrounding context with line numbers\n\nArguments:\n```typescript\n{\n  path: string;          // Path to the file to search\n  pattern: string;       // Search pattern (text or regex)\n  type?: \"text\" | \"regex\"; // Type of search (default: \"text\")\n  caseSensitive?: boolean; // Case-sensitive search (default: false)\n  contextLines?: number;   // Number of context lines (default: 2, max: 10)\n  maxMatches?: number;     // Maximum matches to return (default: 100)\n  wholeWord?: boolean;     // Match whole words only (default: false)\n  multiline?: boolean;     // Enable multiline regex mode (default: false)\n}\n```\n\nExample use cases:\n\n1. Simple text search:\n```json\n{\n  \"path\": \"src/components/App.tsx\",\n  \"pattern\": \"const\",\n  \"contextLines\": 2\n}\n```\n\nOutput:\n```\nFound 6 matches in 0.9ms:\nFile size: 0.7KB\n\nMatch 1: Line 2, Column 1\n----------------------------------------\n     1 | // Basic component with props\n>    2 | const Button = ({ color = \"blue\", size = \"md\" }) => {\n     3 |   return <button className={`btn-${color} size-${size}`}>Click me</button>;\n     4 | };\n\nMatch 2: Line 7, Column 8\n----------------------------------------\n     5 | \n     6 | // Component with multiple props and nested structure\n>    7 | export const Card = ({\n     8 |   title,\n     9 |   subtitle = \"Default subtitle\",\n\nMatch 3: Line 13, Column 3\n----------------------------------------\n    11 |   size = \"lg\",\n    12 | }) => {\n>   13 |   const cardClass = `card-${theme} size-${size}`;\n    14 |   \n    15 |   return (\n\nMatch 4: Line 23, Column 4\n----------------------------------------\n    21 | };\n    22 | \n>   23 | // Constants and configurations\n    24 | const THEME = {\n    25 |   light: { bg: \"#ffffff\", text: \"#000000\" },\n\nMatch 5: Line 24, Column 1\n----------------------------------------\n    22 | \n    23 | // Constants and configurations\n>   24 | const THEME = {\n    25 |   light: { bg: \"#ffffff\", text: \"#000000\" },\n    26 |   dark: { bg: \"#000000\", text: \"#ffffff\" },\n\nMatch 6: Line 29, Column 1\n----------------------------------------\n    27 | };\n    28 | \n>   29 | const CONFIG = {\n    30 |   apiUrl: \"https://api.example.com\",\n    31 |   timeout: 5000,\n```\n\n2. Case-sensitive whole word search:\n```json\n{\n  \"path\": \"src/components/App.tsx\",\n  \"pattern\": \"props\",\n  \"caseSensitive\": true,\n  \"wholeWord\": true,\n  \"contextLines\": 1\n}\n```\n\nOutput:\n```\nFound 2 matches in 0.7ms:\nFile size: 0.7KB\n\nMatch 1: Line 1, Column 25\n----------------------------------------\n>    1 | // Basic component with props\n     2 | const Button = ({ color = \"blue\", size = \"md\" }) => {\n\nMatch 2: Line 6, Column 28\n----------------------------------------\n     5 | \n>    6 | // Component with multiple props and nested structure\n     7 | export const Card = ({\n```\n\n3. Finding JSX components:\n```json\n{\n  \"path\": \"src/components/App.tsx\",\n  \"pattern\": \"<[A-Z]\\\\w+\\\\s\",\n  \"type\": \"regex\",\n  \"contextLines\": 1\n}\n```\n\nOutput:\n```\nFound 2 matches in 0.6ms:\nFile size: 0.7KB\n\nMatch 1: Line 3, Column 10\n----------------------------------------\n     2 | const Button = ({ color = \"blue\", size = \"md\" }) => {\n>    3 |   return <button className={`btn-${color} size-${size}`}>Click me</button>;\n     4 | };\n\nMatch 2: Line 16, Column 5\n----------------------------------------\n    15 |   return (\n>   16 |     <div className={cardClass}>\n    17 |       <h2>{title}</h2>\n```\n\nCommon workflows:\n\n1. Find then edit:\n```typescript\n// First, search for the line\n{\n  \"path\": \"src/config.ts\",\n  \"pattern\": \"API_URL\",\n  \"wholeWord\": true\n}\n\n// Then use the returned line number in edit_file_lines\n{\n  \"p\": \"src/config.ts\",\n  \"e\": [{\n    \"startLine\": 23,  // Line number from search result\n    \"endLine\": 23,\n    \"content\": \"export const API_URL = 'https://new-api.example.com';\"\n  }]\n}\n```\n\n2. Find all usages:\n```typescript\n{\n  \"path\": \"src/components/App.tsx\",\n  \"pattern\": \"\\\\buseMemo\\\\b\",\n  \"type\": \"regex\",\n  \"contextLines\": 2,\n  \"maxMatches\": 50\n}\n```\n\n3. Find specific prop patterns:\n```typescript\n{\n  \"path\": \"src/components/App.tsx\",\n  \"pattern\": \"className=['\\\"]([^'\\\"]+)['\\\"]\",\n  \"type\": \"regex\",\n  \"contextLines\": 1\n}\n```\n\n### Important Notes\n\n1. Whitespace Handling\n   - The tool intelligently handles whitespace in both string and regex matches\n   - Original indentation is preserved in replacements\n   - Multiple spaces between tokens are normalized for matching\n\n2. Pattern Matching\n   - String matches (`strMatch`) are whitespace-normalized\n   - Regex patterns (`regexMatch`) support look-ahead and look-behind\n   - Cannot use both `strMatch` and `regexMatch` in the same edit\n   - Overlapping regex patterns are detected and prevented\n\n3. Best Practices\n   - Always use dry run first to verify changes\n   - Review the diff output before approving changes\n   - Keep edit operations focused and atomic\n   - Use appropriate pattern matching for your use case\n\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Testing\n\nRun the test suite:\n```bash\nnpm run test\n```\n\nAdditional testing utilities:\n\n#### Test Tools Script\nTest the MCP tools directly against sample files:\n```bash\nnpm run test:tools\n```\n\nThis script:\n- Resets test fixtures to a known state\n- Connects to the MCP server\n- Tests each tool in sequence:\n  - `get_file_lines`\n  - `edit_file_lines` (dry run)\n  - `approve_edit`\n- Shows the output of each operation\n- Verifies changes were applied correctly\n\n#### Reset Fixtures Script\nReset test fixtures to their original state:\n```bash\nnpm run reset:fixtures\n```\n\nUse this script to:\n- Reset test files to a known state before testing\n- Clean up after failed tests\n- Ensure consistent test environment\n- Create missing fixture directories\n\n## Usage\n\nThe server requires one or more allowed directories to be specified when starting:\n\n```bash\nnode build/index.js <allowed-directory> [additional-directories...]\n```\n\nAll file operations will be restricted to these directories for security.\n\n### Environment Variables\n\n- `MCP_EDIT_STATE_TTL`: Time-to-live in milliseconds for edit states (default: 60000). Edit states will expire after this duration and must be recreated.\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"edit-file-lines\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/edit-file-lines/build/index.js\",\n        \"<allowed-directory>\"\n      ],\n      \"env\": {\n        \"MCP_EDIT_STATE_TTL\": \"300000\"  // Optional: Set custom TTL (in milliseconds)\n      }\n    }\n  }\n}\n```\n\n### Error Handling\n\nThe tool provides clear error messages for common issues:\n\n1. Match Not Found\n```\nError: No string match found for \"oldValue\" on line 5\n```\n\n2. Invalid Regex\n```\nError: Invalid regex pattern \"([\": Unterminated group\n```\n\n3. Multiple Edits on Same Line\n```\nError: Line 5 is affected by multiple edits\n```\n\n### Security Considerations\n\n- All file operations are restricted to explicitly allowed directories\n- Symlinks are validated to prevent escaping allowed directories\n- Parent directory traversal is prevented\n- Path normalization is performed for consistent security checks\n- Invalid line numbers and character positions are rejected\n- Line ending normalization ensures consistent behavior across platforms\n- Edit states expire after 60 seconds for security\n- Edit approvals require exact match of file path and edits\n\n### Debugging\n\nUse the Test Tools script to test the MCP tools directly against sample files. The [MCP Inspector](https://github.com/modelcontextprotocol/inspector) might help, but it currently does not support handing input that are not string values.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lines",
        "file",
        "regex",
        "edit file",
        "file lines",
        "edits text"
      ],
      "category": "document-processing"
    },
    "oborchers--mcp-server-docy": {
      "owner": "oborchers",
      "name": "mcp-server-docy",
      "url": "https://github.com/oborchers/mcp-server-docy",
      "imageUrl": "/freedevtools/mcp/pfp/oborchers.webp",
      "description": "Provides real-time access to technical documentation from various sources, enabling accurate coding assistance. Supports dynamic updates to documentation sources and employs caching to reduce latency while ensuring fresh content.",
      "stars": 12,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T13:25:47Z",
      "readme_content": "![Docy Logo](media/logo.png)\n\n# Docy: Documentation at Your AI's Fingertips\n\n**Supercharge your AI assistant with instant access to technical documentation.**\n\nDocy gives your AI direct access to the technical documentation it needs, right when it needs it. No more outdated information, broken links, or rate limits - just accurate, real-time documentation access for more precise coding assistance.\n\n## Why Choose Docy?\n\n- **Instant Documentation Access**: Direct access to docs from React, Python, crawl4ai, and any other tech stack you use\n- **Hot-Reload Support**: Add new documentation sources on-the-fly without restarting - just edit the .docy.urls file!\n- **Intelligent Caching**: Reduces latency and external requests while maintaining fresh content\n- **Self-Hosted Control**: Keep your documentation access within your security perimeter\n- **Seamless MCP Integration**: Works effortlessly with Claude, VS Code, and other MCP-enabled AI tools\n\n> **Note**: Claude may default to using its built-in WebFetchTool instead of Docy. To explicitly request Docy's functionality, use a callout like: \"Please use Docy to find...\"\n\n# Docy MCP Server\n\nA Model Context Protocol server that provides documentation access capabilities. This server enables LLMs to search and retrieve content from documentation websites by scraping them with crawl4ai. Built with FastMCP v2.\n\n## Using Docy\n\nHere are examples of how Docy can help with common documentation tasks:\n\n```\n# Verify implementation against documentation\nAre we implementing Crawl4Ai scrape results correctly? Let's check the documentation.\n\n# Explore API usage patterns\nWhat do the docs say about using mcp.tool? Show me examples from the documentation.\n\n# Compare implementation options\nHow should we structure our data according to the React documentation? What are the best practices?\n```\n\nWith Docy, Claude Code can directly access and analyze documentation from configured sources, making it more effective at providing accurate, documentation-based guidance.\n\nTo ensure Claude Code prioritizes Docy for documentation-related tasks, add the following guidelines to your project's `CLAUDE.md` file:\n\n```\n## Documentation Guidelines\n- When checking documentation, prefer using Docy over WebFetchTool\n- Use list_documentation_sources_tool to discover available documentation sources\n- Use fetch_documentation_page to retrieve full documentation pages\n- Use fetch_document_links to discover related documentation\n```\n\nAdding these instructions to your `CLAUDE.md` file helps Claude Code consistently use Docy instead of its built-in web fetch capabilities when working with documentation.\n\n\n### Available Tools\n\n- `list_documentation_sources_tool` - List all available documentation sources\n  - No parameters required\n\n- `fetch_documentation_page` - Fetch the content of a documentation page by URL as markdown\n  - `url` (string, required): The URL to fetch content from\n\n- `fetch_document_links` - Fetch all links from a documentation page\n  - `url` (string, required): The URL to fetch links from\n\n### Prompts\n\n- **documentation_sources**\n  - List all available documentation sources with their URLs and types\n  - No arguments required\n\n- **documentation_page**\n  - Fetch the full content of a documentation page at a specific URL as markdown\n  - Arguments:\n    - `url` (string, required): URL of the specific documentation page to get\n\n- **documentation_links**\n  - Fetch all links from a documentation page to discover related content\n  - Arguments:\n    - `url` (string, required): URL of the documentation page to get links from\n\n## Installation\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-server-docy*.\n\n### Using PIP\n\nAlternatively you can install `mcp-server-docy` via pip:\n\n```\npip install mcp-server-docy\n```\n\nAfter installation, you can run it as a script using:\n\n```\nDOCY_DOCUMENTATION_URLS=\"https://docs.crawl4ai.com/,https://react.dev/\" python -m mcp_server_docy\n```\n\n### Using Docker\n\nYou can also use the Docker image:\n\n```\ndocker pull oborchers/mcp-server-docy:latest\ndocker run -i --rm -e DOCY_DOCUMENTATION_URLS=\"https://docs.crawl4ai.com/,https://react.dev/\" oborchers/mcp-server-docy\n```\n\n### Global Server Setup\n\nFor teams or multi-project development, check out the `server/README.md` for instructions on running a persistent SSE server that can be shared across multiple projects. This setup allows you to maintain a single Docy instance with shared documentation URLs and cache.\n\n## Configuration\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"docy\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-docy\"],\n    \"env\": {\n      \"DOCY_DOCUMENTATION_URLS\": \"https://docs.crawl4ai.com/,https://react.dev/\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using docker</summary>\n\n```json\n\"mcpServers\": {\n  \"docy\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\", \"oborchers/mcp-server-docy:latest\"],\n    \"env\": {\n      \"DOCY_DOCUMENTATION_URLS\": \"https://docs.crawl4ai.com/,https://react.dev/\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using pip installation</summary>\n\n```json\n\"mcpServers\": {\n  \"docy\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_docy\"],\n    \"env\": {\n      \"DOCY_DOCUMENTATION_URLS\": \"https://docs.crawl4ai.com/,https://react.dev/\"\n    }\n  }\n}\n```\n</details>\n\n### Configure for VS Code\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is needed when using the `mcp.json` file.\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"docy\": {\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-docy\"],\n        \"env\": {\n          \"DOCY_DOCUMENTATION_URLS\": \"https://docs.crawl4ai.com/,https://react.dev/\"\n        }\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using Docker</summary>\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"docy\": {\n        \"command\": \"docker\",\n        \"args\": [\"run\", \"-i\", \"--rm\", \"oborchers/mcp-server-docy:latest\"],\n        \"env\": {\n          \"DOCY_DOCUMENTATION_URLS\": \"https://docs.crawl4ai.com/,https://react.dev/\"\n        }\n      }\n    }\n  }\n}\n```\n</details>\n\n### Configuration Options\n\nThe application can be configured using environment variables:\n\n- `DOCY_DOCUMENTATION_URLS` (string): Comma-separated list of URLs to documentation sites to include (e.g., \"https://docs.crawl4ai.com/,https://react.dev/\")\n- `DOCY_DOCUMENTATION_URLS_FILE` (string): Path to a file containing documentation URLs, one per line (default: \".docy.urls\")\n- `DOCY_CACHE_TTL` (integer): Cache time-to-live in seconds (default: 432000)\n- `DOCY_CACHE_DIRECTORY` (string): Path to the cache directory (default: \".docy.cache\")\n- `DOCY_USER_AGENT` (string): Custom User-Agent string for HTTP requests\n- `DOCY_DEBUG` (boolean): Enable debug logging (\"true\", \"1\", \"yes\", or \"y\")\n- `DOCY_SKIP_CRAWL4AI_SETUP` (boolean): Skip running the crawl4ai-setup command at startup (\"true\", \"1\", \"yes\", or \"y\")\n- `DOCY_TRANSPORT` (string): Transport protocol to use (options: \"sse\" or \"stdio\", default: \"stdio\")\n- `DOCY_HOST` (string): Host address to bind the server to (default: \"127.0.0.1\")\n- `DOCY_PORT` (integer): Port to run the server on (default: 8000)\n\nEnvironment variables can be set directly or via a `.env` file.\n\n### URL Configuration File\n\nAs an alternative to setting the `DOCY_DOCUMENTATION_URLS` environment variable, you can create a `.docy.urls` file in your project directory with one URL per line:\n\n```\nhttps://docs.crawl4ai.com/\nhttps://react.dev/\n# Lines starting with # are treated as comments\nhttps://docs.python.org/3/\n```\n\nThis approach is especially useful for:\n- Projects where you want to share documentation sources with your team\n- Repositories where storing URLs in version control is beneficial\n- Situations where you want to avoid long environment variable values\n\nThe server will first check for URLs in the `DOCY_DOCUMENTATION_URLS` environment variable, and if none are found, it will look for the `.docy.urls` file.\n\n#### Hot Reload for URL File\n\nWhen using the `.docy.urls` file for documentation sources, the server implements a hot-reload mechanism that reads the file on each request rather than caching the URLs. This means you can:\n\n1. Add, remove, or modify documentation URLs in the `.docy.urls` file while the server is running\n2. See those changes reflected immediately in subsequent calls to `list_documentation_sources_tool` or other documentation tools\n3. Avoid restarting the server when modifying your documentation sources\n\nThis is particularly useful during development or when you need to quickly add new documentation sources to a running server.\n\n### Documentation URL Best Practices\n\nThe URLs you configure should ideally point to documentation index or introduction pages that contain:\n\n- Tables of contents\n- Navigation structures\n- Collections of internal and external links\n\nThis allows the LLM to:\n1. Start at a high-level documentation page\n2. Discover relevant subpages via links\n3. Navigate to specific documentation as needed\n\nUsing documentation sites with well-structured subpages is highly recommended as it:\n- Minimizes context usage by allowing the LLM to focus on relevant sections\n- Improves navigation efficiency through documentation\n- Provides a natural way to explore and find information\n- Reduces the need to load entire documentation sets at once\n\nFor example, instead of loading an entire documentation site, the LLM can start at the index page, identify the relevant section, and then navigate to specific subpages as needed.\n\n### Caching Behavior\n\nThe MCP server automatically caches documentation content to improve performance:\n\n- At startup, the server pre-fetches and caches all configured documentation URLs from `DOCY_DOCUMENTATION_URLS`\n- The cache time-to-live (TTL) can be configured via the `DOCY_CACHE_TTL` environment variable\n- Each new site accessed is automatically loaded into cache to reduce traffic and improve response times\n- Cached content is stored in a persistent disk-based cache using the `diskcache` library\n- The cache location can be configured via the `DOCY_CACHE_DIRECTORY` environment variable (default: \".docy.cache\")\n- The cache persists between server restarts, providing better performance for frequently accessed documentation\n\n#### Exceptions to Caching\n\nWhile most content is cached for performance, there are specific exceptions:\n\n- **Documentation URL Lists**: When using the `.docy.urls` file, the list of documentation sources is never cached - instead, the file is re-read on each request to support hot-reloading of URLs\n- **Page Content**: The actual content of documentation pages is still cached according to the configured TTL\n\nThis hybrid approach offers both performance benefits for content access and flexibility for documentation source management.\n\n## Local Development\n- Run in development mode: `fastmcp dev src/mcp_server_docy/__main__.py --with-editable .`\n- Access API at: `http://127.0.0.1:6274`\n- Run with MCP inspector: `uv run --with fastmcp --with-editable /Users/oliverborchers/Desktop/Code.nosync/mcp-server-docy --with crawl4ai --with loguru --with diskcache --with pydantic-settings fastmcp run src/mcp_server_docy/__main__.py`\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For uvx installations:\n\n```\nDOCY_DOCUMENTATION_URLS=\"https://docs.crawl4ai.com/\" npx @modelcontextprotocol/inspector uvx mcp-server-docy\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```\ncd path/to/docy\nDOCY_DOCUMENTATION_URLS=\"https://docs.crawl4ai.com/\" npx @modelcontextprotocol/inspector uv run mcp-server-docy\n```\n\n### Troubleshooting: \"Tool not found\" Error in Claude Code CLI\n\nIf you encounter errors like \"ERROR Tool not found for mcp__docy__fetch_documentation_page\" in Claude Code CLI, follow these steps:\n\n1. Create a `.docy.urls` file in your current directory with your documentation URLs:\n```\nhttps://docs.crawl4ai.com/\nhttps://react.dev/\n```\n\n2. Run the server using Docker with the SSE transport protocol and mount the URLs file:\n\n```bash\ndocker run -i --rm -p 8000:8000 \\\n  -e DOCY_TRANSPORT=sse \\\n  -e DOCY_HOST=0.0.0.0 \\\n  -e DOCY_PORT=8000 \\\n  -v \"$(pwd)/.docy.urls:/app/.docy.urls\" \\\n  oborchers/mcp-server-docy\n```\n\n3. Configure your Claude Code `.mcp.json` to use the SSE endpoint:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"docy\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:8000/sse\"\n      }\n    }\n  }\n}\n```\n\nThis configuration:\n- Uses a mounted `.docy.urls` file instead of environment variables for documentation sources\n- Switches from the default stdio mode to SSE (Server-Sent Events) protocol\n- Makes the server accessible from outside the container\n- Exposes the server on port 8000 for HTTP access\n\nThe SSE transport is recommended when running the server as a standalone service that needs to be accessed over HTTP, which is particularly useful for Docker deployments.\n\n## Release Process\n\nThe project uses GitHub Actions for automated releases:\n\n1. Update the version in `pyproject.toml`\n2. Create a new tag with `git tag vX.Y.Z` (e.g., `git tag v0.1.0`)\n3. Push the tag with `git push --tags`\n\nThis will automatically:\n- Verify the version in `pyproject.toml` matches the tag\n- Run tests and lint checks\n- Build and publish to PyPI\n- Build and publish to Docker Hub as `oborchers/mcp-server-docy:latest` and `oborchers/mcp-server-docy:X.Y.Z`\n\n## Contributing\n\nWe encourage contributions to help expand and improve mcp-server-docy. Whether you want to add new features, enhance existing functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make mcp-server-docy even more powerful and useful.\n\n## License\n\nmcp-server-docy is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "docy",
        "docy provides",
        "server docy",
        "document processing"
      ],
      "category": "document-processing"
    },
    "omer-ayhan--custom-context-mcp": {
      "owner": "omer-ayhan",
      "name": "custom-context-mcp",
      "url": "https://github.com/omer-ayhan/custom-context-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/omer-ayhan.webp",
      "description": "Transforms and structures text into JSON formats by extracting key information from AI-generated text. Facilitates seamless data integration into applications by converting unstructured text into structured JSON data.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-07T20:42:45Z",
      "readme_content": "# Custom Context MCP Server\n\nThis Model Context Protocol (MCP) server provides tools for structuring and extracting data from text according to JSON templates.\n\n## Features\n\n### Text-to-JSON Transformation\n\n- Group and structure text based on JSON templates with placeholders\n- Extract information from AI-generated text into structured JSON formats\n- Support for any arbitrary JSON structure with nested placeholders\n- Intelligent extraction of key-value pairs from text\n- Process AI outputs into structured data for downstream applications\n\n## Getting Started\n\n### Installation\n\n```bash\nnpm install\n```\n\n### Running the server\n\n```bash\nnpm start\n```\n\nFor development with hot reloading:\n\n```bash\nnpm run dev:watch\n```\n\n## Usage\n\nThis MCP server provides two main tools:\n\n### 1. Group Text by JSON (`group-text-by-json`)\n\nThis tool takes a JSON template with placeholders and generates a prompt for an AI to group text according to the template's structure.\n\n```json\n{\n\t\"template\": \"{ \\\"type\\\": \\\"<type>\\\", \\\"text\\\": \\\"<text>\\\" }\"\n}\n```\n\nThe tool analyzes the template, extracts placeholder keys, and returns a prompt that guides the AI to extract information in a key-value format.\n\n### 2. Text to JSON (`text-to-json`)\n\nThis tool takes the grouped text output from the previous step and converts it into a structured JSON object based on the original template.\n\n```json\n{\n\t\"template\": \"{ \\\"type\\\": \\\"<type>\\\", \\\"text\\\": \\\"<text>\\\" }\",\n\t\"text\": \"type: pen\\ntext: This is a blue pen\"\n}\n```\n\nIt extracts key-value pairs from the text and structures them according to the template.\n\n## Example Workflow\n\n1. **Define a JSON template with placeholders:**\n\n   ```json\n   {\n   \t\"item\": {\n   \t\t\"name\": \"<name>\",\n   \t\t\"price\": \"<price>\",\n   \t\t\"description\": \"<description>\"\n   \t}\n   }\n   ```\n\n2. **Use `group-text-by-json` to create a prompt for AI:**\n\n   - The tool identifies placeholder keys: name, price, description\n   - Generates a prompt instructing the AI to group information by these keys\n\n3. **Send the prompt to an AI model and receive grouped text:**\n\n   ```\n   name: Blue Pen\n   price: $2.99\n   description: A smooth-writing ballpoint pen with blue ink\n   ```\n\n4. **Use `text-to-json` to convert the grouped text to JSON:**\n   - Result:\n   ```json\n   {\n   \t\"item\": {\n   \t\t\"name\": \"Blue Pen\",\n   \t\t\"price\": \"$2.99\",\n   \t\t\"description\": \"A smooth-writing ballpoint pen with blue ink\"\n   \t}\n   }\n   ```\n\n## Template Format\n\nTemplates can include placeholders anywhere within a valid JSON structure:\n\n- Use angle brackets to define placeholders: `<name>`, `<type>`, `<price>`, etc.\n- The template must be a valid JSON string\n- Placeholders can be at any level of nesting\n- Supports complex nested structures\n\nExample template with nested placeholders:\n\n```json\n{\n\t\"product\": {\n\t\t\"details\": {\n\t\t\t\"name\": \"<name>\",\n\t\t\t\"category\": \"<category>\"\n\t\t},\n\t\t\"pricing\": {\n\t\t\t\"amount\": \"<price>\",\n\t\t\t\"currency\": \"USD\"\n\t\t}\n\t},\n\t\"metadata\": {\n\t\t\"timestamp\": \"2023-09-01T12:00:00Z\"\n\t}\n}\n```\n\n## Implementation Details\n\nThe server works by:\n\n1. Analyzing JSON templates to extract placeholder keys\n2. Generating prompts that guide AI models to extract information by these keys\n3. Parsing AI-generated text to extract key-value pairs\n4. Reconstructing JSON objects based on the original template structure\n\n## Development\n\n### Prerequisites\n\n- Node.js v18 or higher\n- npm or yarn\n\n### Build and Run\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server\nnpm start\n\n# Development with hot reloading\nnpm run dev:watch\n```\n\n### Custom Hot Reloading\n\nThis project includes a custom hot reloading setup that combines:\n\n- **nodemon**: Watches for file changes in the src directory and rebuilds TypeScript files\n- **browser-sync**: Automatically refreshes the browser when build files change\n- **Concurrent execution**: Runs both services simultaneously with output synchronization\n\nThe setup is configured in:\n\n- `nodemon.json`: Controls TypeScript watching and rebuilding\n- `package.json`: Uses concurrently to run nodemon and browser-sync together\n\nTo use the custom hot reloading feature:\n\n```bash\nnpm run dev:watch\n```\n\nThis creates a development environment where:\n\n1. TypeScript files are automatically rebuilt when changed\n2. The MCP server restarts with the updated code\n3. Connected browsers refresh to show the latest changes\n\n### Using with MCP Inspector\n\nYou can use the MCP Inspector for debugging:\n\n```bash\nnpm run dev\n```\n\nThis runs the server with the MCP Inspector for visual debugging of requests and responses.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "json",
        "structured",
        "unstructured",
        "structured json",
        "text structured",
        "unstructured text"
      ],
      "category": "document-processing"
    },
    "orellazri--coda-mcp": {
      "owner": "orellazri",
      "name": "coda-mcp",
      "url": "https://github.com/orellazri/coda-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/orellazri.webp",
      "description": "Enable seamless interaction with Coda documents, including listing, creating, reading, updating, and duplicating pages. Provides command access to manipulate document content directly within an AI framework.",
      "stars": 28,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T07:11:35Z",
      "readme_content": "# Coda MCP Server\n\nThis project implements a Model Context Protocol (MCP) server that acts as a bridge to interact with the [Coda](https://coda.io/) API. It allows an MCP client (like an AI assistant) to perform actions on Coda pages, such as listing, creating, reading, updating, duplicating, and renaming.\n\n## Features\n\nThe server exposes the following tools to the MCP client:\n\n- **`coda_list_documents`**: Lists all documents available to the user.\n- **`coda_list_pages`**: Lists all pages within the configured Coda document with pagination support.\n- **`coda_create_page`**: Creates a new page in the document, optionally under a specified parent page (creating a subpage) and populating it with initial markdown content.\n- **`coda_get_page_content`**: Retrieves the content of a specified page (by ID or name) as markdown.\n- **`coda_replace_page_content`**: Replaces the content of a specified page with new markdown content.\n- **`coda_append_page_content`**: Appends new markdown content to the end of a specified page.\n- **`coda_duplicate_page`**: Creates a copy of an existing page with a new name.\n- **`coda_rename_page`**: Renames an existing page.\n- **`coda_peek_page`**: Peek into the beginning of a page and return a limited number of lines.\n- **`coda_resolve_link`**: Resolve metadata given a browser link to a Coda object.\n\n## Usage\n\nAdd the MCP server to Cursor/Claude Desktop/etc. like so:\n\n```json\n{\n  \"mcpServers\": {\n    \"coda\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"coda-mcp@latest\"],\n      \"env\": {\n        \"API_KEY\": \"...\"\n      }\n    }\n  }\n}\n```\n\nRequired environment variables:\n\n- `API_KEY`: Your Coda API key. You can generate one from your Coda account settings.\n\nThis MCP server is also available with Docker, like so:\n\n```json\n{\n  \"mcpServers\": {\n    \"coda\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"API_KEY\", \"reaperberri/coda-mcp:latest\"],\n      \"env\": {\n        \"API_KEY\": \"...\"\n      }\n    }\n  }\n}\n```\n\n## Local Setup\n\n1.  **Prerequisites:**\n\n    - Node.js\n    - pnpm\n\n2.  **Clone the repository:**\n\n    ```bash\n    git clone <repository-url>\n    cd coda-mcp\n    ```\n\n3.  **Install dependencies:**\n\n    ```bash\n    pnpm install\n    ```\n\n4.  **Build the project:**\n    ```bash\n    pnpm build\n    ```\n    This compiles the TypeScript code to JavaScript in the `dist/` directory.\n\n## Running the Server\n\nThe MCP server communicates over standard input/output (stdio). To run it, set the environment variables and run the compiled JavaScript file - `dist/index.js`.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "coda",
        "document",
        "processing",
        "coda documents",
        "coda mcp",
        "orellazri coda"
      ],
      "category": "document-processing"
    },
    "pbteja1998--sourcesyncai-mcp": {
      "owner": "pbteja1998",
      "name": "sourcesyncai-mcp",
      "url": "https://github.com/pbteja1998/sourcesyncai-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pbteja1998.webp",
      "description": "Integrates with a knowledge management platform to manage and organize documents, ingest content from various sources, and perform semantic and hybrid searches. Facilitates connections to external services for enhanced data retrieval and document management.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-11T16:57:56Z",
      "readme_content": "# SourceSync.ai MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@pbteja1998/sourcesyncai-mcp)](https://smithery.ai/server/@pbteja1998/sourcesyncai-mcp)\n\nA Model Context Protocol (MCP) server implementation for the [SourceSync.ai](https://sourcesync.ai) API. This server allows AI models to interact with SourceSync.ai's knowledge management platform through a standardized interface.\n\n## Features\n\n- Manage namespaces for organizing knowledge\n- Ingest content from various sources (text, URLs, websites, external services)\n- Retrieve, update, and manage documents stored in your knowledge base\n- Perform semantic and hybrid searches against your knowledge base\n- Access document content directly from parsed text URLs\n- Manage connections to external services\n- Default configuration support for seamless AI integration\n\n## Installation\n\n### Running with npx\n\n```bash\n# Install and run with your API key and tenant ID\nenv SOURCESYNC_API_KEY=your_api_key npx -y sourcesyncai-mcp\n```\n\n### Installing via Smithery\n\nTo install sourcesyncai-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pbteja1998/sourcesyncai-mcp):\n\n```bash\nnpx -y @smithery/cli install @pbteja1998/sourcesyncai-mcp --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/sourcesyncai-mcp.git\ncd sourcesyncai-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server\nnode dist/index.js\n```\n\n### Running on Cursor\n\nTo configure SourceSync.ai MCP in Cursor:\n\n1. Open Cursor Settings\n1. Go to `Features > MCP Servers`\n1. Click `+ Add New MCP Server`\n1. Enter the following:\n   - Name: `sourcesyncai-mcp` (or your preferred name)\n   - Type: `command`\n   - Command: `env SOURCESYNCAI_API_KEY=your-api-key npx -y sourcesyncai-mcp`\n\nAfter adding, you can use SourceSync.ai tools with Cursor's AI features by describing your knowledge management needs.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sourcesyncai-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"soucesyncai-mcp\"],\n      \"env\": {\n        \"SOURCESYNC_API_KEY\": \"your_api_key\",\n        \"SOURCESYNC_NAMESPACE_ID\": \"your_namespace_id\",\n        \"SOURCESYNC_TENANT_ID\": \"your_tenant_id\"\n      }\n    }\n  }\n}\n```\n\n### Running on Claude Desktop\n\nTo use this MCP server with Claude Desktop:\n\n1. Locate the Claude Desktop configuration file:\n\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\n2. Edit the configuration file to add the SourceSync.ai MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"sourcesyncai-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sourcesyncai-mcp\"],\n      \"env\": {\n        \"SOURCESYNC_API_KEY\": \"your_api_key\",\n        \"SOURCESYNC_NAMESPACE_ID\": \"your_namespace_id\",\n        \"SOURCESYNC_TENANT_ID\": \"your_tenant_id\"\n      }\n    }\n  }\n}\n```\n\n3. Save the configuration file and restart Claude Desktop\n\n## Configuration\n\n### Environment Variables\n\n#### Required\n\n- `SOURCESYNC_API_KEY`: Your SourceSync.ai API key (required)\n\n#### Optional\n\n- `SOURCESYNC_NAMESPACE_ID`: Default namespace ID to use for operations\n- `SOURCESYNC_TENANT_ID`: Your tenant ID (optional)\n\n### Configuration Examples\n\nBasic configuration with default values:\n\n```bash\nexport SOURCESYNC_API_KEY=your_api_key\nexport SOURCESYNC_TENANT_ID=your_tenant_id\nexport SOURCESYNC_NAMESPACE_ID=your_namespace_id\n```\n\n## Available Tools\n\n### Authentication\n\n- `validate_api_key`: Validate a SourceSync.ai API key\n\n```json\n{\n  \"name\": \"validate_api_key\",\n  \"arguments\": {}\n}\n```\n\n### Namespaces\n\n- `create_namespace`: Create a new namespace\n- `list_namespaces`: List all namespaces\n- `get_namespace`: Get details of a specific namespace\n- `update_namespace`: Update a namespace\n- `delete_namespace`: Delete a namespace\n\n```json\n{\n  \"name\": \"create_namespace\",\n  \"arguments\": {\n    \"name\": \"my-namespace\",\n    \"fileStorageConfig\": {\n      \"provider\": \"S3_COMPATIBLE\",\n      \"config\": {\n        \"endpoint\": \"s3.amazonaws.com\",\n        \"accessKey\": \"your_access_key\",\n        \"secretKey\": \"your_secret_key\",\n        \"bucket\": \"your_bucket\",\n        \"region\": \"us-east-1\"\n      }\n    },\n    \"vectorStorageConfig\": {\n      \"provider\": \"PINECONE\",\n      \"config\": {\n        \"apiKey\": \"your_pinecone_api_key\",\n        \"environment\": \"your_environment\",\n        \"index\": \"your_index\"\n      }\n    },\n    \"embeddingModelConfig\": {\n      \"provider\": \"OPENAI\",\n      \"config\": {\n        \"apiKey\": \"your_openai_api_key\",\n        \"model\": \"text-embedding-3-small\"\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"list_namespaces\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"update_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"name\": \"updated-namespace-name\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"delete_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Data Ingestion\n\n- `ingest_text`: Ingest text content\n- `ingest_urls`: Ingest content from URLs\n- `ingest_sitemap`: Ingest content from a sitemap\n- `ingest_website`: Ingest content from a website\n- `ingest_notion`: Ingest content from Notion\n- `ingest_google_drive`: Ingest content from Google Drive\n- `ingest_dropbox`: Ingest content from Dropbox\n- `ingest_onedrive`: Ingest content from OneDrive\n- `ingest_box`: Ingest content from Box\n- `get_ingest_job_run_status`: Get the status of an ingestion job run\n\n```json\n{\n  \"name\": \"ingest_text\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"TEXT\",\n      \"config\": {\n        \"name\": \"example-document\",\n        \"text\": \"This is an example document for ingestion.\",\n        \"metadata\": {\n          \"category\": \"example\",\n          \"author\": \"AI Assistant\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_urls\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"URLS\",\n      \"config\": {\n        \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n        \"metadata\": {\n          \"source\": \"web\",\n          \"category\": \"documentation\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_sitemap\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"SITEMAP\",\n      \"config\": {\n        \"url\": \"https://example.com/sitemap.xml\",\n        \"metadata\": {\n          \"source\": \"sitemap\",\n          \"website\": \"example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_website\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"WEBSITE\",\n      \"config\": {\n        \"url\": \"https://example.com\",\n        \"maxDepth\": 3,\n        \"maxPages\": 100,\n        \"metadata\": {\n          \"source\": \"website\",\n          \"domain\": \"example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_notion\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"NOTION\",\n      \"config\": {\n        \"connectionId\": \"your_notion_connection_id\",\n        \"metadata\": {\n          \"source\": \"notion\",\n          \"workspace\": \"My Workspace\"\n        }\n      }\n    },\n    \"tenantId\": \"your_tenant_id\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_google_drive\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"GOOGLE_DRIVE\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"google_drive\",\n          \"owner\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_dropbox\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"DROPBOX\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"dropbox\",\n          \"account\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_onedrive\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"ONEDRIVE\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"onedrive\",\n          \"account\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_box\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"BOX\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"box\",\n          \"owner\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_ingest_job_run_status\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestJobRunId\": \"ingest_job_run_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Documents\n\n- `getDocuments`: Retrieve documents with optional filters\n- `updateDocuments`: Update document metadata\n- `deleteDocuments`: Delete documents\n- `resyncDocuments`: Resync documents\n- `fetchUrlContent`: Fetch text content from document URLs\n\n```json\n{\n  \"name\": \"getDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"filterConfig\": {\n      \"documentTypes\": [\"PDF\"]\n    },\n    \"includeConfig\": {\n      \"parsedTextFileUrl\": true\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"updateDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    },\n    \"data\": {\n      \"metadata\": {\n        \"status\": \"reviewed\",\n        \"category\": \"technical\"\n      }\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"deleteDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"resyncDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"fetchUrlContent\",\n  \"arguments\": {\n    \"url\": \"https://api.sourcesync.ai/v1/documents/doc_XXX/content?format=text\",\n    \"apiKey\": \"your_api_key\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Search\n\n- `semantic_search`: Perform semantic search\n- `hybrid_search`: Perform hybrid search (semantic + keyword)\n\n```json\n{\n  \"name\": \"semantic_search\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"query\": \"example document\",\n    \"topK\": 5,\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"hybrid_search\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"query\": \"example document\",\n    \"topK\": 5,\n    \"tenantId\": \"tenant_XXX\",\n    \"hybridConfig\": {\n      \"semanticWeight\": 0.7,\n      \"keywordWeight\": 0.3\n    }\n  }\n}\n```\n\n### Connections\n\n- `create_connection`: Create a new connection to an external service\n- `list_connections`: List all connections\n- `get_connection`: Get details of a specific connection\n- `update_connection`: Update a connection\n- `revoke_connection`: Revoke a connection\n\n```json\n{\n  \"name\": \"create_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"name\": \"My Connection\",\n    \"connector\": \"GOOGLE_DRIVE\",\n    \"clientRedirectUrl\": \"https://your-app.com/callback\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"list_connections\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"update_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\",\n    \"name\": \"Updated Connection Name\",\n    \"clientRedirectUrl\": \"https://your-app.com/updated-callback\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"revoke_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\"\n  }\n}\n```\n\n## Example Prompts\n\nHere are some example prompts you can use with Claude or Cursor after configuring the MCP server:\n\n- \"Search my SourceSync knowledge base for information about machine learning.\"\n- \"Ingest this article into my SourceSync knowledge base: [URL]\"\n- \"Create a new namespace in SourceSync for my project documentation.\"\n- \"List all the documents in my SourceSync namespace.\"\n- \"Get the text content of document [document_id] from my SourceSync namespace.\"\n\n## Troubleshooting\n\n### Connection Issues\n\nIf you encounter issues connecting the SourceSync.ai MCP server:\n\n1. **Verify Paths**: Ensure all paths in your configuration are absolute paths, not relative.\n2. **Check Permissions**: Ensure the server file has execution permissions (`chmod +x dist/index.js`).\n3. **Enable Developer Mode**: In Claude Desktop, enable Developer Mode and check the MCP Log File.\n4. **Test the Server**: Run the server directly from the command line:\n\n   ```bash\n   node /path/to/sourcesyncai-mcp/dist/index.js\n   ```\n\n5. **Restart AI Client**: After making changes, completely restart Claude Desktop or Cursor.\n6. **Check Environment Variables**: Ensure all required environment variables are correctly set.\n\n### Debug Logging\n\nFor detailed logging, add the DEBUG environment variable:\n\n```\n\n```\n\n## Development\n\n### Project Structure\n\n- `src/index.ts`: Main entry point and server setup\n- `src/schemas.ts`: Schema definitions for all tools\n- `src/sourcesync.ts`: Client for interacting with SourceSync.ai API\n- `src/sourcesync.types.ts`: TypeScript type definitions\n\n### Building and Testing\n\n```bash\n# Build the project\nnpm run build\n\n# Run tests\nnpm test\n```\n\n## License\n\nMIT\n\n## Links\n\n- [SourceSync.ai Documentation](https://sourcesync.ai)\n- [SourceSync.ai API Reference](https://sourcesync.ai/api-reference/authentication)\n- [Model Context Protocol](https://modelcontextprotocol.io/introduction)\n\nDocument content retrieval workflow:\n\n1. First, use `getDocuments` with `includeConfig.parsedTextFileUrl: true` to get documents with their content URLs\n2. Extract the URL from the document response\n3. Use `fetchUrlContent` to retrieve the actual content:\n\n```json\n{\n  \"name\": \"fetchUrlContent\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "document",
        "sourcesyncai",
        "document processing",
        "document management",
        "sourcesyncai mcp"
      ],
      "category": "document-processing"
    },
    "petercat-ai--whiskerrag_toolkit": {
      "owner": "petercat-ai",
      "name": "whiskerrag_toolkit",
      "url": "https://github.com/petercat-ai/whiskerrag_toolkit",
      "imageUrl": "/freedevtools/mcp/pfp/petercat-ai.webp",
      "description": "Provides retrieval-augmented generation capabilities for applications, allowing integration of various data sources with advanced processing methods. Features a toolkit with type definitions and methods for effective RAG implementation.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-02T06:44:45Z",
      "readme_content": "# WhiskerRAG\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n[![Python Version](https://img.shields.io/pypi/pyversions/whiskerrag)](https://pypi.org/project/whiskerrag/)\n[![PyPI version](https://badge.fury.io/py/whiskerrag.svg)](https://badge.fury.io/py/whiskerrag)\n[![codecov](https://codecov.io/gh/petercat-ai/whiskerrag_toolkit/branch/main/graph/badge.svg)](https://codecov.io/gh/petercat-ai/whiskerrag_toolkit)\n\nWhiskerRAG 是为 PeterCat 和 Whisker 项目开发的 RAG（Retrieval-Augmented Generation）工具包，提供完整的 RAG 相关类型定义和方法实现。\n\n## 特性\n\n- 针对通用 RAG 的领域建模类型, 包括任务（Task）、知识（Knowledge）、分段(Chunk)、租户(Tenant)、知识库空间(Space)。\n- Whisker rag 插件接口描述。\n- Github 仓库、S3 资源管理器。\n\n## 安装\n\n使用 pip 安装：\n\n```bash\npip install whiskerrag\n```\n\n## 快速开始\n\nwhiskerrag 包含三个子模块，分别是 whiskerrag_utils、whiskerrag_client、whiskerrag_types。它们分别有不同的用途：\n\n### whiskerrag_utils\n\n包含了构建 RAG 系统的常用方法：\n\n```python\nfrom whiskerrag_utils import loader,embedding,retriever\n```\n\n### whiskerrag_client\n\n将 RAG 系统服务通过 python sdk 的形式向外暴露。\n\n```python\nfrom whiskerrag_client import APIClient\n\napi_client = APIClient(\n    base_url=\"https://api.example.com\",\n    token=\"your_token_here\"\n)\n\nknowledge_chunks = await api_client.retrieval.retrieve_knowledge_content(\n    RetrievalByKnowledgeRequest(knowledge_id=\"your knowledge uuid here\")\n)\n\nspace_chunks = await api_client.retrieval.retrieve_space_content(\n    RetrievalBySpaceRequest(space_id=\"your space id here \")\n)\n\nchunk_list = await api_client.chunk.get_chunk_list(\n    page=1,\n    size=10,\n    filters={\"status\": \"active\"}\n)\n\ntask_list = await api_client.task.get_task_list(\n    page=1,\n    size=10\n)\n\ntask_detail = await api_client.task.get_task_detail(\"task_id_here\")\n```\n\n### whiskerrag_types\n\n一些辅助开发的类型提示，接口；\n\n```python\nfrom whiskerrag_types.interface import DBPluginInterface, TaskEngineInterface\nfrom whiskerrag_types.model import Knowledge, Task, Tenant, PageParams, PageResponse\n```\n\n## 开发者指南\n\n### 环境初始化\n\n1. 克隆项目\n\n```bash\ngit clone https://github.com/petercat-ai/whiskerrag_toolkit.git\ncd whiskerrag_toolkit\n```\n\n2. 创建并激活虚拟环境\n\n```bash\n# 查看poetry配置\npoetry config --list\n\n# 修改 poetry 配置\npoetry config virtualenvs.create true\npoetry config virtualenvs.in-project true\n\npoetry env use python3.10\n\n# 激活虚拟环境\nsource .venv/bin/activate\n```\n\n3. 安装依赖\n\n```bash\n# 安装项目依赖\npoetry install\n# 安装 pre-commit 工具\npre-commit install\n```\n\n4. 运行测试\n\n```bash\n# 运行所有测试\npoetry run pytest\n# 运行指定测试文件\npoetry run pytest tests/test_loader.py\n```\n\n4. poetry 常用命令\n\n```bash\n# 安装依赖\npoetry install\n\n# 添加新依赖\npoetry add package_name\n\n# 添加新 dev 依赖\npoetry add --dev package_name\n\n# 更新依赖\npoetry update\n\n# 查看环境信息\npoetry env info\n\n# 查看已安装的包\npoetry show\n```\n\n### 开发工作流\n\n1. 创建新分支\n2. 开发新功能，补充单元测试，确保代码质量。注意，请确保单元测试覆盖率不低于 80%。\n3. 提交代码，并创建 Pull Request。\n4. 等待代码审查，并根据反馈进行修改。\n5. 合并 Pull Request。\n\n## 项目结构\n\n```\nwhiskerRAG-toolkit/\n├── src/\n│   ├── whiskerrag_utils/\n│   └── whiskerrag_types/\n│   └── whiskerrag_client/\n└── pyproject.toml\n```\n\n## 贡献指南\n\n1. Fork 本仓库\n2. 创建特性分支 (`make branch name=feature/amazing-feature`)\n3. 提交更改 (`git commit -m 'Add some amazing feature'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 开启 Pull Request\n\n## 许可证\n\n本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情\n\n## 联系方式\n\n项目维护者 - [@petercat-ai](https://github.com/petercat-ai)\n\n项目链接：[https://github.com/petercat-ai/whiskerrag_toolkit](https://github.com/your-username/whiskerrag_toolkit)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whiskerrag_toolkit",
        "toolkit",
        "retrieval",
        "ai whiskerrag_toolkit",
        "whiskerrag_toolkit provides",
        "document processing"
      ],
      "category": "document-processing"
    },
    "privetin--chroma": {
      "owner": "privetin",
      "name": "chroma",
      "url": "https://github.com/privetin/chroma",
      "imageUrl": "/freedevtools/mcp/pfp/privetin.webp",
      "description": "Provides vector database capabilities for semantic search and document management, enabling storage and retrieval of documents along with their metadata.",
      "stars": 38,
      "forks": 14,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-05T23:55:58Z",
      "readme_content": "# Chroma MCP Server\n\nA Model Context Protocol (MCP) server implementation that provides vector database capabilities through Chroma. This server enables semantic document search, metadata filtering, and document management with persistent storage.\n\n## Requirements\n\n- Python 3.8+\n- Chroma 0.4.0+\n- MCP SDK 0.1.0+\n\n## Components\n\n### Resources\nThe server provides document storage and retrieval through Chroma's vector database:\n- Stores documents with content and metadata\n- Persists data in `src/chroma/data` directory\n- Supports semantic similarity search\n\n### Tools\n\nThe server implements CRUD operations and search functionality:\n\n#### Document Management\n- `create_document`: Create a new document\n  - Required: `document_id`, `content`\n  - Optional: `metadata` (key-value pairs)\n  - Returns: Success confirmation\n  - Error: Already exists, Invalid input\n\n- `read_document`: Retrieve a document by ID\n  - Required: `document_id`\n  - Returns: Document content and metadata\n  - Error: Not found\n\n- `update_document`: Update an existing document\n  - Required: `document_id`, `content`\n  - Optional: `metadata`\n  - Returns: Success confirmation\n  - Error: Not found, Invalid input\n\n- `delete_document`: Remove a document\n  - Required: `document_id`\n  - Returns: Success confirmation\n  - Error: Not found\n\n- `list_documents`: List all documents\n  - Optional: `limit`, `offset`\n  - Returns: List of documents with content and metadata\n\n#### Search Operations\n- `search_similar`: Find semantically similar documents\n  - Required: `query`\n  - Optional: `num_results`, `metadata_filter`, `content_filter`\n  - Returns: Ranked list of similar documents with distance scores\n  - Error: Invalid filter\n\n## Features\n\n- **Semantic Search**: Find documents based on meaning using Chroma's embeddings\n- **Metadata Filtering**: Filter search results by metadata fields\n- **Content Filtering**: Additional filtering based on document content\n- **Persistent Storage**: Data persists in local directory between server restarts\n- **Error Handling**: Comprehensive error handling with clear messages\n- **Retry Logic**: Automatic retries for transient failures\n\n## Installation\n\n1. Install dependencies:\n```bash\nuv venv\nuv sync --dev --all-extras\n```\n\n## Configuration\n\n### Claude Desktop\n\nAdd the server configuration to your Claude Desktop config:\n\nWindows: `C:\\Users\\<username>\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"chroma\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:/MCP/server/community/chroma\",\n        \"run\",\n        \"chroma\"\n      ]\n    }\n  }\n}\n```\n\n### Data Storage\n\nThe server stores data in:\n- Windows: `src/chroma/data`\n- MacOS/Linux: `src/chroma/data`\n\n## Usage\n\n1. Start the server:\n```bash\nuv run chroma\n```\n\n2. Use MCP tools to interact with the server:\n\n```python\n# Create a document\ncreate_document({\n    \"document_id\": \"ml_paper1\",\n    \"content\": \"Convolutional neural networks improve image recognition accuracy.\",\n    \"metadata\": {\n        \"year\": 2020,\n        \"field\": \"computer vision\",\n        \"complexity\": \"advanced\"\n    }\n})\n\n# Search similar documents\nsearch_similar({\n    \"query\": \"machine learning models\",\n    \"num_results\": 2,\n    \"metadata_filter\": {\n        \"year\": 2020,\n        \"field\": \"computer vision\"\n    }\n})\n```\n\n## Error Handling\n\nThe server provides clear error messages for common scenarios:\n- `Document already exists [id=X]`\n- `Document not found [id=X]`\n- `Invalid input: Missing document_id or content`\n- `Invalid filter`\n- `Operation failed: [details]`\n\n## Development\n\n### Testing\n\n1. Run the MCP Inspector for interactive testing:\n```bash\nnpx @modelcontextprotocol/inspector uv --directory C:/MCP/server/community/chroma run chroma\n```\n\n2. Use the inspector's web interface to:\n   - Test CRUD operations\n   - Verify search functionality\n   - Check error handling\n   - Monitor server logs\n\n### Building\n\n1. Update dependencies:\n```bash\nuv compile pyproject.toml\n```\n\n2. Build package:\n```bash\nuv build\n```\n\n## Contributing\n\nContributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) for details on:\n- Code style\n- Testing requirements\n- Pull request process\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "document",
        "retrieval",
        "retrieval documents",
        "document processing",
        "documents metadata"
      ],
      "category": "document-processing"
    },
    "probelabs--docs-mcp": {
      "owner": "probelabs",
      "name": "docs-mcp",
      "url": "https://github.com/probelabs/docs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/probelabs.webp",
      "description": "Enables AI assistants to search and interact with documentation or codebases by pointing to a Git repository or local folder, allowing for natural language queries about the contents.",
      "stars": 60,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-21T01:52:20Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/buger-docs-mcp-badge.png)](https://mseep.ai/app/buger-docs-mcp)\n\n# Docs MCP Server\n[![smithery badge](https://smithery.ai/badge/@buger/docs-mcp)](https://smithery.ai/server/@buger/docs-mcp)\n\nThis project provides a flexible Model Context Protocol (MCP) server, powered by [Probe](https://probeai.dev/), designed to make documentation or codebases searchable by AI assistants.\n\n<a href=\"https://glama.ai/mcp/servers/@buger/docs-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@buger/docs-mcp/badge\" alt=\"Docs Server MCP server\" />\n</a>\n\nYou can chat with code or your docs, simply by pointing to git repo or a folder.\n```\nnpx -y @buger/docs-mcp@latest --gitUrl https://github.com/buger/probe\n```\n\n**Use Cases:**\n\n*   **Chat with any GitHub Repository:** Point the server to a public or private Git repository to enable natural language queries about its contents.\n*   **Search Your Documentation:** Integrate your project's documentation (from a local directory or Git) for easy searching.\n*   **Build Custom MCP Servers:** Use this project as a template to create your own official MCP servers tailored to specific documentation sets or even codebases.\n\nThe content source (documentation or code) can be **pre-built** into the package during the `npm run build` step, or configured **dynamically** at runtime using local directories or Git repositories. By default, when using a `gitUrl` without enabling auto-updates, the server downloads a `.tar.gz` archive for faster startup. Full Git cloning is used only when `autoUpdateInterval` is greater than 0.\n\n## Features\n\n- **Powered by Probe:** Leverages the [Probe](https://probeai.dev/) search engine for efficient and relevant results.\n- **Flexible Content Sources:** Include a specific local directory or clone a Git repository.\n- **Pre-build Content:** Optionally bundle documentation/code content directly into the package.\n- **Dynamic Configuration:** Configure content sources, Git settings, and MCP tool details via config file, CLI arguments, or environment variables.\n- **Automatic Git Updates:** Keep content fresh by automatically pulling changes from a Git repository at a configurable interval.\n- **Customizable MCP Tool:** Define the name and description of the search tool exposed to AI assistants.\n- **AI Integration:** Seamlessly integrates with AI assistants supporting the Model Context Protocol (MCP).\n\n## Usage\n\nThe primary way to use this server is via `npx`, which downloads and runs the package without needing a local installation. This makes it easy to integrate with AI assistants and MCP clients (like IDE extensions).\n\n### Installing via Smithery\n\nTo install Docs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@buger/docs-mcp):\n\n```bash\nnpx -y @smithery/cli install @buger/docs-mcp --client claude\n```\n\n### Integrating with MCP Clients (e.g., IDEs)\n\nYou can configure your MCP client to launch this server using `npx`. Here are examples of how you might configure a client (syntax may vary based on the specific client):\n\n**Example 1: Dynamically Searching a Git Repository (Tyk Docs)**\n\nThis configuration tells the client to run the latest `@buger/docs-mcp` package using `npx`, pointing it dynamically to the Tyk documentation repository. The `-y` argument automatically confirms the `npx` installation prompt. The `--toolName` and `--toolDescription` arguments customize how the search tool appears to the AI assistant.\n\n```json\n{\n  \"mcpServers\": {\n    \"tyk-docs-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@buger/docs-mcp@latest\",\n        \"--gitUrl\",\n        \"https://github.com/TykTechnologies/tyk-docs\",\n        \"--toolName\",\n        \"search_tyk_docs\",\n        \"--toolDescription\",\n        \"Search Tyk API Management Documentation\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\nAlternatively, some clients might allow specifying the full command directly. You could achieve the same as Example 1 using:\n\n```bash\nnpx -y @buger/docs-mcp@latest --gitUrl https://github.com/TykTechnologies/tyk-docs --toolName search_tyk_docs --toolDescription \"Search Tyk API Management Documentation\"\n```\n\n**Example 2: Using a Pre-built, Branded MCP Server (e.g., Tyk Package)**\n\nIf a team publishes a pre-built package containing specific documentation (like `@tyk-technologies/docs-mcp`), the configuration becomes simpler as the content source and tool details are baked into that package. The `-y` argument is still recommended for `npx`.\n\n```json\n{\n  \"mcpServers\": {\n    \"tyk-official-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@tyk-technologies/docs-mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\nThis approach is ideal for distributing standardized search experiences for official documentation or codebases. See the \"Creating Your Own Pre-built MCP Server\" section below.\n\nHere is example on how Tyk team have build own documentation MCP server https://github.com/TykTechnologies/docs-mcp. \n\n## Configuration\n\nCreate a `docs-mcp.config.json` file in the root directory to define the **default** content source and MCP tool details used during the build and at runtime (unless overridden by CLI arguments or environment variables).\n\n### Example 1: Using a Local Directory\n\n```json\n{\n  \"includeDir\": \"/Users/username/projects/my-project/docs\",\n  \"toolName\": \"search_my_project_docs\",\n  \"toolDescription\": \"Search the documentation for My Project.\",\n  \"ignorePatterns\": [\n    \"node_modules\",\n    \".git\",\n    \"build\",\n    \"*.log\"\n  ]\n}\n```\n\n### Example 2: Using a Git Repository\n\n```json\n{\n  \"gitUrl\": \"https://github.com/your-org/your-codebase.git\",\n  \"gitRef\": \"develop\",\n  \"autoUpdateInterval\": 15,\n  \"toolName\": \"search_codebase\",\n  \"toolDescription\": \"Search the main company codebase.\",\n  \"ignorePatterns\": [\n    \"*.test.js\",\n    \"dist/\",\n    \"__snapshots__\"\n  ]\n}\n```\n\n### Configuration Options\n\n- `includeDir`: **(Build/Runtime)** Absolute path to a local directory whose contents will be copied to the `data` directory during build, or used directly at runtime if `dataDir` is not specified. Use this OR `gitUrl`.\n- `gitUrl`: **(Build/Runtime)** URL of the Git repository. Use this OR `includeDir`.\n    - If `autoUpdateInterval` is 0 (default), the server attempts to download a `.tar.gz` archive directly (currently assumes GitHub URL structure: `https://github.com/{owner}/{repo}/archive/{ref}.tar.gz`). This is faster but doesn't support updates.\n    - If `autoUpdateInterval` > 0, the server performs a `git clone` and enables periodic updates.\n- `gitRef`: **(Build/Runtime)** The branch, tag, or commit hash to use from the `gitUrl` (default: `main`). Used for both tarball download and Git clone/pull.\n- `autoUpdateInterval`: **(Runtime)** Interval in minutes to automatically check for Git updates (default: 0, meaning disabled). Setting this to a value > 0 enables Git cloning and periodic `git pull` operations. Requires the `git` command to be available in the system path.\n- `dataDir`: **(Runtime)** Path to the directory containing the content to be searched at runtime. Overrides content sourced from `includeDir` or `gitUrl` defined in the config file or built into the package. Useful for pointing the server to live data without rebuilding.\n- `toolName`: **(Build/Runtime)** The name of the MCP tool exposed by the server (default: `search_docs`). Choose a descriptive name relevant to the content.\n- `toolDescription`: **(Build/Runtime)** The description of the MCP tool shown to AI assistants (default: \"Search documentation using the probe search engine.\").\n- `ignorePatterns`: **(Build/Runtime)** An array of glob patterns.\n- `enableBuildCleanup`: **(Build)** If `true` (default), removes common binary/media files (images, videos, archives, etc.) and files larger than 100KB from the `data` directory after the build step. Set to `false` to disable this cleanup.\n    - If using `includeDir` during build: Files matching these patterns are excluded when copying to `data`. `.gitignore` rules are also respected.\n    - If using `gitUrl` or `dataDir` at runtime: Files matching these patterns within the `data` directory are ignored by the search indexer.\n\n**Precedence:**\n\n1.  **Runtime Configuration (Highest):** CLI arguments (`--dataDir`, `--gitUrl`, etc.) and Environment Variables (`DATA_DIR`, `GIT_URL`, etc.) override all other settings. CLI arguments take precedence over Environment Variables.\n2.  **Build-time Configuration:** Settings in `docs-mcp.config.json` (`includeDir`, `gitUrl`, `toolName`, etc.) define defaults used during `npm run build` and also serve as runtime defaults if not overridden.\n3.  **Default Values (Lowest):** Internal defaults are used if no configuration is provided (e.g., `toolName: 'search_docs'`, `autoUpdateInterval: 5`).\n\nNote: If both `includeDir` and `gitUrl` are provided in the *same* configuration source (e.g., both in the config file, or both as CLI args), `gitUrl` takes precedence.\n\n## Creating Your Own Pre-built MCP Server\n\nYou can use this project as a template to create and publish your own npm package with documentation or code pre-built into it. This provides a zero-configuration experience for users (like Example 2 above).\n\n1.  **Fork/Clone this Repository:** Start with this project's code.\n2.  **Configure `docs-mcp.config.json`:** Define the `includeDir` or `gitUrl` pointing to your content source. Set the default `toolName` and `toolDescription`.\n3.  **Update `package.json`:** Change the `name` (e.g., `@my-org/my-docs-mcp`), `version`, `description`, etc.\n4.  **Build:** Run `npm run build`. This clones/copies your content into the `data` directory and makes the package ready.\n5.  **Publish:** Run `npm publish` (you'll need npm authentication configured).\n\nNow, users can run your specific documentation server easily: `npx @my-org/my-docs-mcp@latest`.\n\n*(The previous \"Running\", \"Dynamic Configuration at Runtime\", and \"Environment Variables\" sections have been removed as `npx` usage with arguments within client configurations is now the primary documented method.)*\n\n## Using with AI Assistants\n\nThis MCP server exposes a search tool to connected AI assistants via the Model Context Protocol. The tool's name and description are configurable (see Configuration section). It searches the content within the currently active `data` directory (determined by build settings, config file, CLI args, or environment variables).\n\n**Tool Parameters:**\n\n- `query`: A natural language query or keywords describing what to search for (e.g., \"how to configure the gateway\", \"database connection example\", \"user authentication\"). The server uses Probe's search capabilities to find relevant content. (Required)\n- `page`: The page number for results when dealing with many matches. Defaults to 1 if omitted. (Optional)\n\n**Example Tool Call (using `search_tyk_docs` from Usage Example 1):**\n\n```json\n{\n  \"tool_name\": \"search_tyk_docs\",\n  \"arguments\": {\n    \"query\": \"gateway rate limiting\",\n    \"page\": 1 // Requesting the first page\n  }\n}\n```\n\n**Example Tool Call (using the tool from the `@tyk/docs-mcp` package):**\n\nAssuming the pre-built package `@tyk/docs-mcp` defined its tool name as `search_tyk_official_docs`:\n\n```json\n{\n  \"tool_name\": \"search_tyk_official_docs\",\n  \"arguments\": {\n    \"query\": \"dashboard api access\",\n    \"page\": 2 // Requesting the second page\n  }\n}\n```\n\n*(The previous \"Publishing as an npm Package\" section has been replaced by the \"Creating Your Own Pre-built MCP Server\" section above.)*\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "git",
        "probelabs",
        "docs mcp",
        "documentation codebases",
        "probelabs docs"
      ],
      "category": "document-processing"
    },
    "puchunjie--doc-tools-mcp": {
      "owner": "puchunjie",
      "name": "doc-tools-mcp",
      "url": "https://github.com/puchunjie/doc-tools-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/puchunjie.webp",
      "description": "Manipulate Word documents using natural language commands for tasks such as creation, editing, and management. The server supports advanced features like table creation, layout control, and metadata management, along with real-time document state monitoring.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-19T22:50:01Z",
      "readme_content": "# Word Tools MCP Server\n\nA Model Context Protocol (MCP) server that provides AI-powered Word document manipulation capabilities. This server implements the MCP protocol to enable AI applications to create, edit, and manage Word documents through natural language interactions.\n\n[![smithery badge](https://smithery.ai/badge/@puchunjie/doc-tools)](https://smithery.ai/server/@puchunjie/doc-tools)\n\n<a href=\"https://glama.ai/mcp/servers/q9e176vq7l\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/q9e176vq7l/badge\" />\n</a>\n\n## Features\n\n- Full MCP protocol implementation\n- Word document creation and management\n- Rich text content manipulation\n- Table creation and formatting\n- Document layout control\n- Document metadata management\n- Real-time document state monitoring\n\n## Prerequisites\n\n- Node.js 14 or higher\n- Microsoft Word (optional, for advanced features)\n\n## Installation\n\n```bash\nnpx @puchunjie/doc-tools-mcp\n```\n\nOr install globally:\n\n```bash\nnpm install -g @puchunjie/doc-tools-mcp\n```\n\nFor use as a dependency in your project:\n\n```bash\nnpm install @puchunjie/doc-tools-mcp\n```\n\n## Usage\n\n1. Start the MCP server:\n\n```bash\nnpx @puchunjie/doc-tools-mcp\n```\n\n2. The server will start on port 8765 by default\n\n3. Configure your AI application (e.g., Cursor, VSCode) to use the MCP server:\n   ```\n   http://localhost:8765\n   ```\n\n## MCP Tools\n\nThe server provides the following MCP functions:\n\n- `create_document` - Create a new Word document\n  - Parameters: filePath (required), title, author\n\n- `open_document` - Open an existing Word document\n  - Parameters: filePath (required)\n\n- `add_paragraph` - Add a paragraph to the document\n  - Parameters: filePath (required), text (required), style, alignment\n\n- `add_table` - Add a table to the document\n  - Parameters: filePath (required), rows (required), cols (required), headers, data\n\n- `search_and_replace` - Find and replace text in the document\n  - Parameters: filePath (required), searchText (required), replaceText (required), matchCase\n\n- `set_page_margins` - Set document page margins\n  - Parameters: filePath (required), top, right, bottom, left\n\n- `get_document_info` - Get document metadata\n  - Parameters: filePath (required)\n\n## Integration with AI Applications\n\n### Cursor\n\n1. Open the Cursor configuration file `~/.cursor/mcp.json`\n2. Add the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"doc-tools-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@puchunjie/doc-tools-mcp\"\n      ]\n    }\n  }\n}\n\n```\n\nOr for local development version:\n```json\n{\n  \"mcpServers\": {\n    \"doc-tools-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/doc-tools-mcp/dist/mcp-server.js\"\n      ]\n    }\n  }\n}\n```\n\nAfter configuration, you can use natural language to manipulate Word documents:\n```\n\"Create a new document named report.docx\"\n\"Add a heading 'Monthly Report' to report.docx\"\n\"Insert a 4x3 table with sales data\"\n```\n\n### VSCode and Other MCP-Compatible Tools\n\nSimilar integration steps apply to other tools that support the MCP protocol. Consult your tool's documentation for specific MCP server configuration steps.\n\n## Development\n\nTo extend or modify this MCP server:\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd doc-tools-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Start in development mode:\n```bash\nnpm run start\n```\n\n4. Build for production:\n```bash\nnpm run build\n```\n\n### Adding New MCP Functions\n\n1. Add new methods in `src/services/DocumentService.ts`\n2. Register new functions in `src/mcp-server.ts`\n3. Update type definitions as needed\n\n## Configuration\n\n- Default port: 8765 (configurable)\n- Supported file types: .docx\n- All file paths should be absolute or relative to the current working directory\n\n## License\n\nMIT\n\n## Support\n\nIf you encounter any issues or have suggestions for improvements, please submit an issue on our GitHub repository. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "puchunjie",
        "documents",
        "doc tools",
        "document processing",
        "tools mcp"
      ],
      "category": "document-processing"
    },
    "puremd--puremd-mcp": {
      "owner": "puremd",
      "name": "puremd-mcp",
      "url": "https://github.com/puremd/puremd-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/puremd.webp",
      "description": "Access web content in markdown format by prefixing URLs with `pure.md/`, facilitating seamless retrieval of web pages while avoiding bot detection. It converts various formats like HTML and PDFs into markdown and globally caches responses for efficiency.",
      "stars": 41,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-23T01:32:32Z",
      "readme_content": "# pure.md MCP server\n\n[![smithery badge](https://smithery.ai/badge/@puremd/puremd-mcp)](https://smithery.ai/server/@puremd/puremd-mcp)\n\nWelcome to the Model Context Protocol (MCP) server for [pure.md](https://pure.md).\n\n![pure.md - Markdown delivery network for LLMs](https://pure.md/assets/og.png)\n\n[pure.md](https://pure.md) lets your scripts, APIs, apps, agents, etc reliably access web content in markdown format -- simply prefix any URL with `pure.md/`.\nIt avoids bot detection and renders JavaScript for SPAs, and can convert HTML, PDFs, images, and more into pure markdown. Like a CDN for markdown content, it globally caches responses for future requests to the same resource, relieving stress on origin web servers.\n\n**Without puremd-mcp, local agents may fail to fetch web content.** puremd-mcp teaches MCP clients like Cursor, Windsurf, and Claude Desktop how to adopt the functionality of pure.md, giving them web unblocking and searching capabilities.\n\npuremd-mcp comes with two tools:\n\n- `unblock-url` - Extract markdown from web pages without getting blocked\n- `search-web` - Search the web for a query and concatenate results into markdown\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction), developed by Anthropic, is an open standard that enables AI systems to seamlessly interact with an ecosystem of tooling. With it, MCP clients like Cursor, Windsurf, and Claude Desktop can learn how to use a variety of APIs and other functionality.\n\n## Authentication\n\nGenerating an API key is an optional step that unlocks higher rate limits. If you'd like to use the pure.md MCP server anonymously, simply set your `PUREMD_API_KEY` value to empty string (`\"\"`).\n\n1. Sign up for a new account at [pure.md](https://pure.md) &mdash; it's free to sign up!\n2. In the dashboard, generate a new API token\n3. Copy the token, and use it for the `PUREMD_API_KEY` value in your MCP client's configuration file (see below)\n\n## Client configuration\n\n### Cursor\n\nAdd the following to your `~/.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following to your `./codeium/windsurf/model_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Claude Desktop\n\nAdd the following to your `~/Library/Application\\ Support/Claude/claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install puremd-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@puremd/puremd-mcp):\n\n```bash\nnpx -y @smithery/cli install @puremd/puremd-mcp --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "puremd",
        "markdown",
        "formats",
        "processing puremd",
        "puremd puremd",
        "puremd mcp"
      ],
      "category": "document-processing"
    },
    "qing-turnaround--markitdown_mcp_server": {
      "owner": "qing-turnaround",
      "name": "markitdown_mcp_server",
      "url": "https://github.com/qing-turnaround/markitdown_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/qing-turnaround.webp",
      "description": "Convert various file formats to Markdown using the MarkItDown utility. Process PDFs, Office documents, images, audio files, HTML, and more into a Markdown format for streamlined content handling.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-24T07:56:26Z",
      "readme_content": "# MarkItDown MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@KorigamiK/markitdown_mcp_server)](https://smithery.ai/server/@KorigamiK/markitdown_mcp_server)\n\nA Model Context Protocol (MCP) server that converts various file formats to Markdown using the MarkItDown utility.\n\n<a href=\"https://glama.ai/mcp/servers/sbc6bljjg5\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/sbc6bljjg5/badge\" alt=\"MarkItDown Server MCP server\" /></a>\n\n## Supported Formats\n\n- PDF\n- PowerPoint\n- Word\n- Excel\n- Images (EXIF metadata and OCR)\n- Audio (EXIF metadata and speech transcription)\n- HTML\n- Text-based formats (CSV, JSON, XML)\n- ZIP files (iterates over contents)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MarkItDown MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KorigamiK/markitdown_mcp_server):\n\n```bash\nnpx -y @smithery/cli install @KorigamiK/markitdown_mcp_server --client claude\n```\n\n### Manual Installation\n\n1. Clone this repository\n2. Install dependencies:\n```bash\nuv install\n```\n\n## Usage\n\n### As MCP Server\n\nThe server can be integrated with any MCP client. Here are some examples:\n\n#### Zed Editor\n\nAdd the following to your `settings.json`:\n\n```json\n\"context_servers\": {\n  \"markitdown_mcp\": {\n    \"settings\": {},\n    \"command\": {\n      \"path\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/markitdown_mcp_server\",\n        \"run\",\n        \"markitdown\"\n      ]\n    }\n  }\n}\n```\n\n### Commands\n\nThe server responds to the following MCP commands:\n\n- `/md <file>` - Convert the specified file to Markdown\n\nExample:\n```bash\n/md document.pdf\n```\n\n## Supported MCP Clients\n\nWorks with any MCP-compliant client listed at [modelcontextprotocol.io/clients](https://modelcontextprotocol.io/clients), including:\n\n- Zed Editor\n- Any other MCP-compatible editors and tools\n\n## License\n\nMIT License. See [LICENSE](LICENSE) for details.\n\n## Acknowledgements\n\nhttps://github.com/microsoft/markitdown#readme\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markitdown",
        "markdown",
        "markitdown_mcp_server",
        "formats markdown",
        "markitdown utility",
        "markdown format"
      ],
      "category": "document-processing"
    },
    "qpd-v--mcp-ragdocs": {
      "owner": "qpd-v",
      "name": "mcp-ragdocs",
      "url": "https://github.com/qpd-v/mcp-ragdocs",
      "imageUrl": "/freedevtools/mcp/pfp/qpd-v.webp",
      "description": "Fetches and stores documentation in a vector database for semantic search and retrieval, enhancing LLM capabilities with relevant documentation context. Supports adding documentation from URLs or local files and querying with natural language.",
      "stars": 124,
      "forks": 63,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-10-03T17:26:40Z",
      "readme_content": "# MCP-Ragdocs\n\nA Model Context Protocol (MCP) server that enables semantic search and retrieval of documentation using a vector database (Qdrant). This server allows you to add documentation from URLs or local files and then search through them using natural language queries.\n\n## Quick Install Guide\n\n1. Install the package globally:\n   ```bash\n   npm install -g @qpd-v/mcp-server-ragdocs\n   ```\n\n2. Start Qdrant (using Docker):\n   ```bash\n   docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n   ```\n\n3. Ensure Ollama is running with the default embedding model:\n   ```bash\n   ollama pull nomic-embed-text\n   ```\n\n4. Add to your configuration file:\n   - For Cline: `%AppData%\\Roaming\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n   - For Roo-Code: `%AppData%\\Roaming\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\cline_mcp_settings.json`\n   - For Claude Desktop: `%AppData%\\Claude\\claude_desktop_config.json`\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"ragdocs\": {\n         \"command\": \"node\",\n         \"args\": [\"C:/Users/YOUR_USERNAME/AppData/Roaming/npm/node_modules/@qpd-v/mcp-server-ragdocs/build/index.js\"],\n         \"env\": {\n           \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n           \"EMBEDDING_PROVIDER\": \"ollama\",\n           \"OLLAMA_URL\": \"http://localhost:11434\"\n         }\n       }\n     }\n   }\n   ```\n\n5. Verify installation:\n   ```bash\n   # Check Qdrant is running\n   curl http://localhost:6333/collections\n   \n   # Check Ollama has the model\n   ollama list | grep nomic-embed-text\n   ```\n\n## Version\n\nCurrent version: 0.1.6\n\n## Features\n\n- Add documentation from URLs or local files\n- Store documentation in a vector database for semantic search\n- Search through documentation using natural language\n- List all documentation sources\n\n## Installation\n\nInstall globally using npm:\n\n```bash\nnpm install -g @qpd-v/mcp-server-ragdocs\n```\n\nThis will install the server in your global npm directory, which you'll need for the configuration steps below.\n\n## Requirements\n\n- Node.js 16 or higher\n- Qdrant (either local or cloud)\n- One of the following for embeddings:\n  - Ollama running locally (default, free)\n  - OpenAI API key (optional, paid)\n\n## Qdrant Setup Options\n\n### Option 1: Local Qdrant\n\n1. Using Docker (recommended):\n```bash\ndocker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n```\n\n2. Or download from [Qdrant's website](https://qdrant.tech/documentation/quick-start/)\n\n### Option 2: Qdrant Cloud\n\n1. Create an account at [Qdrant Cloud](https://cloud.qdrant.io/)\n2. Create a new cluster\n3. Get your cluster URL and API key from the dashboard\n4. Use these in your configuration (see Configuration section below)\n\n## Configuration\n\nThe server can be used with both Cline/Roo and Claude Desktop. Configuration differs slightly between them:\n\n### Cline Configuration\n\nAdd to your Cline settings file (`%AppData%\\Roaming\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`)\nAND/OR\nAdd to your Roo-Code settings file (`%AppData%\\Roaming\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\cline_mcp_settings.json`):\n\n1. Using npm global install (recommended):\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"ragdocs\": {\n\t\t\t\t\t\t\"command\": \"node\",\n      \"args\": [\"C:/Users/YOUR_USERNAME/AppData/Roaming/npm/node_modules/@qpd-v/mcp-server-ragdocs/build/index.js\"],\n      \"env\": {\n        \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n        \"EMBEDDING_PROVIDER\": \"ollama\",\n        \"OLLAMA_URL\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n```\n\nFor OpenAI instead of Ollama:\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"ragdocs\": {\n\t\t\t\t\t\t\"command\": \"node\",\n      \"args\": [\"C:/Users/YOUR_USERNAME/AppData/Roaming/npm/node_modules/@qpd-v/mcp-server-ragdocs/build/index.js\"],\n      \"env\": {\n        \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n        \"EMBEDDING_PROVIDER\": \"openai\",\n        \"OPENAI_API_KEY\": \"your-openai-api-key\"\n      }\n    }\n  }\n}\n```\n\n2. Using local development setup:\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"ragdocs\": {\n\t\t\t\t\t\t\"command\": \"node\",\n\t\t\t\t\t\t\"args\": [\"PATH_TO_PROJECT/mcp-ragdocs/build/index.js\"],\n\t\t\t\t\t\t\"env\": {\n\t\t\t\t\t\t\t\t\"QDRANT_URL\": \"http://127.0.0.1:6333\",\n\t\t\t\t\t\t\t\t\"EMBEDDING_PROVIDER\": \"ollama\",\n\t\t\t\t\t\t\t\t\"OLLAMA_URL\": \"http://localhost:11434\"\n\t\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n}\n```\n\n### Claude Desktop Configuration\n\nAdd to your Claude Desktop config file:\n- Windows: `%AppData%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n1. Windows Setup with Ollama (using full paths):\n```json\n{\n  \"mcpServers\": {\n    \"ragdocs\": {\n      \"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\",\n      \"args\": [\n        \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@qpd-v/mcp-server-ragdocs\\\\build\\\\index.js\"\n      ],\n      \"env\": {\n\t\t\t\t\t\t\t\t\"QDRANT_URL\": \"http://127.0.0.1:6333\",\n\t\t\t\t\t\t\t\t\"EMBEDDING_PROVIDER\": \"ollama\",\n\t\t\t\t\t\t\t\t\"OLLAMA_URL\": \"http://localhost:11434\"\n\t\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n}\n```\n\nWindows Setup with OpenAI:\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"ragdocs\": {\n\t\t\t\t\t\t\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\",\n\t\t\t\t\t\t\"args\": [\n\t\t\t\t\t\t\t\t\"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@qpd-v/mcp-server-ragdocs\\\\build\\\\index.js\"\n\t\t\t\t\t\t],\n\t\t\t\t\t\t\"env\": {\n\t\t\t\t\t\t\t\t\"QDRANT_URL\": \"http://127.0.0.1:6333\",\n\t\t\t\t\t\t\t\t\"EMBEDDING_PROVIDER\": \"openai\",\n\t\t\t\t\t\t\t\t\"OPENAI_API_KEY\": \"your-openai-api-key\"\n\t\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n}\n```\n\n2. macOS Setup with Ollama:\n```json\n{\n\t\t\"mcpServers\": {\n\t\t\t\t\"ragdocs\": {\n\t\t\t\t\t\t\"command\": \"/usr/local/bin/node\",\n\t\t\t\t\t\t\"args\": [\n\t\t\t\t\t\t\t\t\"/usr/local/lib/node_modules/@qpd-v/mcp-server-ragdocs/build/index.js\"\n\t\t\t\t\t\t],\n\t\t\t\t\t\t\"env\": {\n\t\t\t\t\t\t\t\t\"QDRANT_URL\": \"http://127.0.0.1:6333\",\n\t\t\t\t\t\t\t\t\"EMBEDDING_PROVIDER\": \"ollama\",\n\t\t\t\t\t\t\t\t\"OLLAMA_URL\": \"http://localhost:11434\"\n\t\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n}\n```\n\n### Qdrant Cloud Configuration\n\nFor either Cline or Claude Desktop, when using Qdrant Cloud, modify the env section:\n\nWith Ollama:\n```json\n{\n\t\t\"env\": {\n\t\t\t\t\"QDRANT_URL\": \"https://your-cluster-url.qdrant.tech\",\n\t\t\t\t\"QDRANT_API_KEY\": \"your-qdrant-api-key\",\n\t\t\t\t\"EMBEDDING_PROVIDER\": \"ollama\",\n\t\t\t\t\"OLLAMA_URL\": \"http://localhost:11434\"\n\t\t}\n}\n```\n\nWith OpenAI:\n```json\n{\n\t\t\"env\": {\n\t\t\t\t\"QDRANT_URL\": \"https://your-cluster-url.qdrant.tech\",\n\t\t\t\t\"QDRANT_API_KEY\": \"your-qdrant-api-key\",\n\t\t\t\t\"EMBEDDING_PROVIDER\": \"openai\",\n\t\t\t\t\"OPENAI_API_KEY\": \"your-openai-api-key\"\n\t\t}\n}\n```\n\n### Environment Variables\n\n#### Qdrant Configuration\n- `QDRANT_URL` (required): URL of your Qdrant instance\n  - For local: http://localhost:6333\n  - For cloud: https://your-cluster-url.qdrant.tech\n- `QDRANT_API_KEY` (required for cloud): Your Qdrant Cloud API key\n\n#### Embeddings Configuration\n- `EMBEDDING_PROVIDER` (optional): Choose between 'ollama' (default) or 'openai'\n- `EMBEDDING_MODEL` (optional):\n  - For Ollama: defaults to 'nomic-embed-text'\n  - For OpenAI: defaults to 'text-embedding-3-small'\n- `OLLAMA_URL` (optional): URL of your Ollama instance (defaults to http://localhost:11434)\n- `OPENAI_API_KEY` (required if using OpenAI): Your OpenAI API key\n\n## Available Tools\n\n1. `add_documentation`\n   - Add documentation from a URL to the RAG database\n   - Parameters:\n     - `url`: URL of the documentation to fetch\n\n2. `search_documentation`\n   - Search through stored documentation\n   - Parameters:\n     - `query`: Search query\n     - `limit` (optional): Maximum number of results to return (default: 5)\n\n3. `list_sources`\n   - List all documentation sources currently stored\n   - No parameters required\n\n## Example Usage\n\nIn Claude Desktop or any other MCP-compatible client:\n\n1. Add documentation:\n```\nAdd this documentation: https://docs.example.com/api\n```\n\n2. Search documentation:\n```\nSearch the documentation for information about authentication\n```\n\n3. List sources:\n```\nWhat documentation sources are available?\n```\n\n## Development\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/qpd-v/mcp-server-ragdocs.git\ncd mcp-server-ragdocs\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Run locally:\n```bash\nnpm start\n```\n\n## License\n\nMIT\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Qdrant Connection Error**\n   ```\n   Error: Failed to connect to Qdrant at http://localhost:6333\n   ```\n   - Check if Docker is running\n   - Verify Qdrant container is running: `docker ps | grep qdrant`\n   - Try restarting the container\n\n2. **Ollama Model Missing**\n   ```\n   Error: Model nomic-embed-text not found\n   ```\n   - Run: `ollama pull nomic-embed-text`\n   - Verify model is installed: `ollama list`\n\n3. **Configuration Path Issues**\n   - Windows: Replace `YOUR_USERNAME` with your actual Windows username\n   - Check file permissions\n   - Verify the paths exist\n\n4. **npm Global Install Issues**\n   - Try installing with admin privileges\n   - Check npm is in PATH: `npm -v`\n   - Verify global installation: `npm list -g @qpd-v/mcp-server-ragdocs`\n\nFor other issues, please check:\n- Docker logs: `docker logs $(docker ps -q --filter ancestor=qdrant/qdrant)`\n- Ollama status: `ollama list`\n- Node.js version: `node -v` (should be 16 or higher)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "qpd",
        "stores documentation",
        "semantic search",
        "documentation vector"
      ],
      "category": "document-processing"
    },
    "qpd-v--mcp-wordcounter": {
      "owner": "qpd-v",
      "name": "mcp-wordcounter",
      "url": "https://github.com/qpd-v/mcp-wordcounter",
      "imageUrl": "/freedevtools/mcp/pfp/qpd-v.webp",
      "description": "Analyzes text documents by providing word and character counting capabilities. It processes files directly without exposing content to language models, offering statistics on total words, characters including spaces, and characters excluding spaces.",
      "stars": 10,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-07-28T02:45:59Z",
      "readme_content": "# MCP Word Counter\n\nA Model Context Protocol server that provides tools for analyzing text documents, including counting words and characters. This server helps LLMs perform text analysis tasks by exposing simple document statistics functionality.\n\n## Features\n\n- Count words in documents\n- Count total characters (including spaces)\n- Count characters excluding spaces\n- Process files directly without exposing content to LLMs\n\n## Installation\n\n```bash\nnpm install mcp-wordcounter\n```\n\n## Usage\n\n### As a CLI tool\n\n```bash\nnpx mcp-wordcounter\n```\n\n### In Claude Desktop\n\nAdd to your Claude Desktop configuration (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-wordcounter\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-wordcounter\"],\n      \"alwaysAllow\": [\"analyze_text\"]\n    }\n  }\n}\n```\n\n### Available Tools\n\n#### analyze_text\n\nCounts words and characters in a text document.\n\nParameters:\n- `filePath` (string, required): Path to the text file to analyze\n\nReturns:\n- Word count\n- Character count (including spaces)\n- Character count (excluding spaces)\n\nExample response:\n```json\n{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"Analysis Results:\\n• Word count: 150\\n• Character count (including spaces): 842\\n• Character count (excluding spaces): 702\"\n  }]\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run in watch mode during development\nnpm run watch\n\n# Test with MCP Inspector\nnpm run inspector\n```\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wordcounter",
        "qpd",
        "document",
        "wordcounter analyzes",
        "mcp wordcounter",
        "document processing"
      ],
      "category": "document-processing"
    },
    "quillopy--quillopy-mcp": {
      "owner": "quillopy",
      "name": "quillopy-mcp",
      "url": "https://github.com/quillopy/quillopy-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/quillopy.webp",
      "description": "Retrieve relevant package documentation for programming languages and libraries through the Quillopy API, enhancing the coding experience by providing up-to-date information directly into the user's context.",
      "stars": 119,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-18T11:05:49Z",
      "readme_content": "<h3 align=\"center\">\n  <a href=\"https://quillopy.com\">🏠 Home page</a>\n  <a href=\"https://discord.gg/HuyzbYRzwu\">💬 Discord</a>\n  <a href=\"https://quillopy.com/documentation/all\">📚 Check docs</a>\n  <a href=\"https://quillopy.com/add\">➕ Add docs</a>\n</h4>\n\n# Quillopy MCP — Real Docs. Real Code. Zero Hallucination.\n\n<div align=\"center\">\n<img src=\"assets/demo.gif\">\n</div>\n\n## 🧠 Your LLM is smart. But it can’t see the latest docs.\n\n### ❌ Without Quillopy:\n\n- You get code that references functions that were deprecated two years ago\n- You spend time debugging things that were never supposed to work\n- Answers are vague, outdated, or flat-out wrong\n\n### ✅ With Quillopy:\n\nQuillopy pipes accurate documentation directly into your code assistant’s context — so it generates **real**, **working**, **up-to-date** code.\n\nNo manual uploads. No stale info. No wasted time.\n\n---\n\n### How it works:\n\n1. Ask your question in Cursor (or any assistant that supports the MCPs)\n2. Behind the scenes, Quillopy injects the right docs — automatically\n3. You get a code completion that actually runs\n\nTo explicitly activate Quillopy, just add `@quillopy` to your question — or use `@quillopy[package_name]` to specify exactly what library to pull in.\n\nNo hacks. No guessing. Just code that *works*.\n\n---\n\n### Try it with questions like:\n\n> “How to code an agent browsing the web to fetch the latest news using browser-use? @quillopy[browser-use]”\\\n> “How do I store and retrieve JSON data in Supabase? @quillopy”\\\n> “How do I secure routes with the newest NextAuth? @quillopy”\n\n---\n\n### Why devs are switching to Quillopy:\n\n✅ Zero setup — no uploads or config\\\n✅ 600+ libraries pre-indexed and updated in real time\\\n✅ Optimized for minimal context usage (perfect for LLMs)\\\n✅ Works with any library, any version, anytime\n\n\n## 🛠️ Getting Started\n\n### 1. Create an API key\n\n**Important:** You need a Quillopy API key to use this MCP server. Visit https://quillopy.com to sign up and obtain your API key (free).\n\n### 2. Install the Quillopy MCP\n\n#### Option 1: Use Smithery (Recommended)\n\nSmithery provides the easiest way to install and configure the Quillopy MCP across various AI assistant platforms.\n\n```\n# Claude\nnpx -y @smithery/cli@latest install @quillopy/quillopy-mcp --client claude\n\n# Cursor\nnpx -y @smithery/cli@latest install @quillopy/quillopy-mcp --client cursor\n\n# Windsurf\nnpx -y @smithery/cli@latest install @quillopy/quillopy-mcp --client windsurf\n```\n\nFor more information and additional integration options, visit https://smithery.ai/server/@quillopy/quillopy-mcp\n\n#### Option 2: Manual Setup\n\n##### Cursor\n\n1. Navigate to `Settings` -> `Cursor Settings` -> `MCP` -> `+ Add new global MCP server`\n2. Copy paste the following config in `~/.cursor/.mcp.json`\n   ```json\n   {\n     \"mcpServers\": {\n       \"quillopy\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@quillopy/mcp\"],\n         \"env\": {\n           \"QUILLOPY_API_KEY\": \"<your-api-key>\"\n         }\n       }\n     }\n   }\n   ```\n3. Replace `<your-api-key>` with your actual API key\n\nCheck the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more infos.\n\n##### Windsurf\nAdd this to your Windsurf MCP config file. Check the [Windsurf MCP docs](https://docs.windsurf.com/windsurf/mcp) for more infos.\n ```json\n {\n   \"mcpServers\": {\n     \"quillopy\": {\n       \"command\": \"npx\",\n       \"args\": [\"-y\", \"@quillopy/mcp\"],\n       \"env\": {\n         \"QUILLOPY_API_KEY\": \"<your-api-key>\"\n       }\n     }\n   }\n }\n ```\n\n##### Claude Desktop\n\nAdd this to your `claude_desktop_config.json`.\n\n```json\n{\n  \"mcpServers\": {\n    \"quillopy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@quillopy/mcp\"],\n      \"env\": {\n        \"QUILLOPY_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n##### Continue.dev\n\n1. Open your Continue.dev configuration file in either format:\n\n   - YAML:\n     - MacOS/Linux: `~/.continue/config.yaml`\n     - Windows: `%USERPROFILE%\\.continue\\config.yaml`\n   - JSON:\n     - Same location as above, but named `config.json`\n\n2. Add the configuration using either format:\n\n   YAML format:\n\n   ```yaml\n   experimental:\n     modelContextProtocolServers:\n       - transport:\n           type: stdio\n           command: node\n           args: [\"-y\", \"@quillopy/mcp\"]\n           env: { \"QUILLOPY_API_KEY\": \"<your-api-key>\" }\n   ```\n\n   JSON format:\n\n   ```json\n   {\n     \"experimental\": {\n       \"modelContextProtocolServers\": [\n         {\n           \"transport\": {\n             \"type\": \"stdio\",\n             \"command\": \"npx\",\n             \"args\": [\"-y\", \"@quillopy/mcp\"],\n             \"env\": { \"QUILLOPY_API_KEY\": \"<your-api-key>\" }\n           }\n         }\n       ]\n     }\n   }\n   ```\n\n3. Save the file - Continue will automatically refresh to apply the new configuration. If the changes don't take effect immediately, try restarting your IDE.\n\nCheck [Continue MCP docs](https://docs.continue.dev/customize/deep-dives/mcp) for more infos.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quillopy",
        "documentation",
        "libraries",
        "libraries quillopy",
        "quillopy api",
        "quillopy mcp"
      ],
      "category": "document-processing"
    },
    "rajnaveen344--lsp-tools-mcp": {
      "owner": "rajnaveen344",
      "name": "lsp-tools-mcp",
      "url": "https://github.com/rajnaveen344/lsp-tools-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rajnaveen344.webp",
      "description": "Enhances text analysis by providing regex matching capabilities to find positions of regex matches in files and managing directory access for secure interaction with system files.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-03-09T09:45:17Z",
      "readme_content": "# LSP Tools MCP Server\n\nA Model Context Protocol (MCP) server providing Language Server Protocol-like functionality for text analysis.\n\n## Features\n\n- **Find Regex Position**: Find the 0-indexed line and column positions of regex pattern matches in a file\n- **List Allowed Directories**: Get a list of directories the server is allowed to access\n\n## Installation\n\n```bash\nnpm install\nnpm run build\n```\n\n## Usage\n\n```bash\n# Start the server allowing access to a specific directory\nnode dist/index.js /path/to/allowed/directory\n\n# Start the server with multiple allowed directories\nnode dist/index.js /path/to/dir1 /path/to/dir2 /path/to/dir3\n```\n\n## Development\n\n### Running Tests\n\nThe project uses Jest for testing. Run the tests with:\n\n```bash\nnpm test\n```\n\nTo run tests in watch mode during development:\n\n```bash\nnpm run test:watch\n```\n\n### Linting\n\nLint the code with ESLint:\n\n```bash\nnpm run lint\n```\n\n## Tool Documentation\n\n### find_regex_position\n\nThis tool finds the 0-indexed line and column positions of regex pattern matches in a file.\n\n**Parameters:**\n- `path`: The path to the file to search in\n- `regex`: The regular expression pattern to search for\n\n**Returns:**\n- An array of matches with the following properties:\n  - `match`: The matched text\n  - `line`: The starting line (0-indexed)\n  - `column`: The starting column (0-indexed)\n  - `endLine`: The ending line (0-indexed)\n  - `endColumn`: The ending column (0-indexed, exclusive)\n\n### list_allowed_directories\n\nThis tool lists all directories that this server is allowed to access.\n\n**Parameters:**\n- None\n\n**Returns:**\n- An array of absolute paths to allowed directories\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "regex",
        "lsp",
        "files",
        "matches files",
        "lsp tools",
        "document processing"
      ],
      "category": "document-processing"
    },
    "rjadhavJT--docgen-mcp": {
      "owner": "rjadhavJT",
      "name": "docgen-mcp",
      "url": "https://github.com/rjadhavJT/docgen-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Automates the creation of standardized documentation by extracting information from various source files and applying templates. Integrates with Google Drive and GitHub to enhance documentation processes with AI-generated content and management features.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "docgen",
        "documentation",
        "document",
        "documentation extracting",
        "documentation processes",
        "enhance documentation"
      ],
      "category": "document-processing"
    },
    "samaraxmmar--Deepseek_chat_rag": {
      "owner": "samaraxmmar",
      "name": "Deepseek_chat_rag",
      "url": "https://github.com/samaraxmmar/Deepseek_chat_rag",
      "imageUrl": "/freedevtools/mcp/pfp/samaraxmmar.webp",
      "description": "Utilizes advanced retrieval-augmented generation models to answer queries based on indexed documents extracted from various file formats. Engages users by providing relevant answers from a Chroma database that stores extracted text from PDF, DOCX, TXT, and CSV files.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-14T12:03:25Z",
      "readme_content": "# DeepSeek RAG Chatbot 🤖\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V2/refs/heads/main/figures/logo.svg\" alt=\"DeepSeek RAG Chatbot Banner\" width=\"700\">\n</p>\n\n<p align=\"center\">\n    <b>An intelligent chatbot powered by Groq, LangChain, and ChromaDB to chat with your documents.</b>\n<br/><br/>\n    <a href=\"https://github.com/samaraxmmar/Deepseek_chat_rag/issues\"><img src=\"https://img.shields.io/github/issues/samaraxmmar/Deepseek_chat_rag?style=for-the-badge&color=brightgreen\" alt=\"Issues\"></a>\n    <a href=\"https://github.com/samaraxmmar/Deepseek_chat_rag/stargazers\"><img src=\"https://img.shields.io/github/stars/samaraxmmar/Deepseek_chat_rag?style=for-the-badge&color=f0c60f\" alt=\"Stars\"></a>\n    <a href=\"https://github.com/samaraxmmar/Deepseek_chat_rag/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/samaraxmmar/Deepseek_chat_rag?style=for-the-badge&color=blue\" alt=\"License\"></a>\n</p>\n\n---\n\n## 🌟 Introduction\n\n**DeepSeek RAG Chatbot** is a powerful and intuitive application that allows you to have conversations with your own documents. By leveraging the speed of the **Groq LPU Inference Engine** and the versatility of **LangChain**, this tool transforms your static files (PDFs, DOCX, TXT, CSV) into an interactive knowledge base.\n\nSimply upload your documents, and the system will automatically process, index, and prepare them for your questions. The user-friendly interface, built with **Streamlit**, makes it easy for anyone to get instant, accurate answers drawn directly from the provided content.\n\n## ✨ Key Features\n\n* **Multi-Format Document Support**: Upload and process various file types, including `.pdf`, `.docx`, `.txt`, and `.csv`.\n* **High-Speed Inferencing**: Powered by **Groq**, delivering responses at exceptional speed for a fluid, real-time conversational experience.\n* **Advanced RAG Pipeline**: Utilizes **LangChain** for robust Retrieval-Augmented Generation, ensuring answers are relevant and contextually accurate.\n* **Efficient Vector Storage**: Employs **ChromaDB** to create and manage a persistent vector database of your document embeddings for fast retrieval.\n* **User-Friendly Interface**: A clean and simple web UI built with **Streamlit** that includes real-time processing feedback and chat history.\n* **Open Source & Customizable**: Fully open-source, allowing for easy customization and integration into other projects.\n\n## ⚙️ How It Works\n\nThe application follows a sophisticated Retrieval-Augmented Generation (RAG) architecture to provide answers from your documents.\n\n<p align=\"center\">\n  <img src=\"https://www.deepchecks.com/wp-content/uploads/2024/10/img-rag-architecture-model.jpg\" alt=\"RAG Architecture Diagram\" width=\"700\">\n</p>\n\n1.  **Document Loading**: You upload your documents (PDF, DOCX, etc.) through the Streamlit interface.\n2.  **Text Splitting & Embedding**: The system loads the documents, splits them into smaller, manageable chunks, and generates vector embeddings for each chunk.\n3.  **Vector Indexing**: These embeddings are stored in a **ChromaDB** vectorstore, creating a searchable index of your document's knowledge.\n4.  **User Query**: You ask a question in the chat interface.\n5.  **Context Retrieval**: The system takes your query, embeds it, and performs a similarity search in ChromaDB to retrieve the most relevant document chunks (the \"context\").\n6.  **Response Generation**: The retrieved context and your original query are passed to the **Groq**-powered language model, which generates a human-like, accurate answer based on the provided information.\n\n## 🚀 Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n* Python 3.8+\n* A Groq API Key. You can get one for free at [GroqCloud](https://console.groq.com/keys).\n\n### 1. Clone the Repository\n\n```bash\ngit clone [https://github.com/samaraxmmar/Deepseek_chat_rag.git](https://github.com/samaraxmmar/Deepseek_chat_rag.git)\ncd Deepseek_chat_rag\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "samaraxmmar",
        "documents",
        "document processing",
        "samaraxmmar deepseek_chat_rag",
        "processing samaraxmmar"
      ],
      "category": "document-processing"
    },
    "sammcj--mcp-data-extractor": {
      "owner": "sammcj",
      "name": "mcp-data-extractor",
      "url": "https://github.com/sammcj/mcp-data-extractor",
      "imageUrl": "/freedevtools/mcp/pfp/sammcj.webp",
      "description": "Extracts embedded data such as i18n translations and configurations from TypeScript and JavaScript source code, converting them into structured JSON files while preserving the hierarchical structure and template variables.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-03T22:32:57Z",
      "readme_content": "# mcp-data-extractor MCP Server\n\nA Model Context Protocol server that extracts embedded data (such as i18n translations or key/value configurations) from TypeScript/JavaScript source code into structured JSON configuration files.\n\n[![smithery badge](https://smithery.ai/badge/mcp-data-extractor)](https://smithery.ai/server/mcp-data-extractor)\n\n<a href=\"https://glama.ai/mcp/servers/40c3iyazm5\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/40c3iyazm5/badge\" alt=\"MCP Data Extractor MCP server\" /></a>\n\n## Features\n\n- Data Extraction:\n  - Extracts string literals, template literals, and complex nested objects\n  - Preserves template variables (e.g., `Hello, {{name}}!`)\n  - Supports nested object structures and arrays\n  - Maintains hierarchical key structure using dot notation\n  - Handles both TypeScript and JavaScript files with JSX support\n  - Replaces source file content with \"MIGRATED TO <target absolute path>\" after successful extraction (configurable)\n\n- SVG Extraction:\n  - Extracts SVG components from React/TypeScript/JavaScript files\n  - Preserves SVG structure and attributes\n  - Removes React-specific code and props\n  - Creates individual .svg files named after their component\n  - Replaces source file content with \"MIGRATED TO <target absolute path>\" after successful extraction (configurable)\n\n## Usage\n\nAdd to your MCP Client configuration:\n\n```bash\n{\n  \"mcpServers\": {\n    \"data-extractor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-data-extractor\"\n      ],\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"extract_data\",\n        \"extract_svg\"\n      ]\n    }\n  }\n}\n```\n\n### Basic Usage\n\nThe server provides two tools:\n\n#### 1. Data Extraction\n\nUse `extract_data` to extract data (like i18n translations) from source files:\n\n```typescript\n<use_mcp_tool>\n<server_name>data-extractor</server_name>\n<tool_name>extract_data</tool_name>\n<arguments>\n{\n  \"sourcePath\": \"src/translations.ts\",\n  \"targetPath\": \"src/translations.json\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### 2. SVG Extraction\n\nUse `extract_svg` to extract SVG components into individual files:\n\n```typescript\n<use_mcp_tool>\n<server_name>data-extractor</server_name>\n<tool_name>extract_svg</tool_name>\n<arguments>\n{\n  \"sourcePath\": \"src/components/icons/InspectionIcon.tsx\",\n  \"targetDir\": \"src/assets/icons\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Source File Replacement\n\nBy default, after successful extraction, the server will replace the content of the source file with:\n- \"MIGRATED TO <target path>\" for data extraction\n- \"MIGRATED TO <target directory>\" for SVG extraction\n\nThis helps track which files have already been processed and prevents duplicate extraction. It also makes it easy for LLMs and developers to see where the extracted data now lives when they encounter the source file later.\n\nTo disable this behavior, set the `DISABLE_SOURCE_REPLACEMENT` environment variable to `true` in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"data-extractor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-data-extractor\"\n      ],\n      \"env\": {\n        \"DISABLE_SOURCE_REPLACEMENT\": \"true\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"extract_data\",\n        \"extract_svg\"\n      ]\n    }\n  }\n}\n```\n\n### Supported Patterns\n\n#### Data Extraction Patterns\n\nThe data extractor supports various patterns commonly used in TypeScript/JavaScript applications:\n\n1. Simple Object Exports:\n```typescript\nexport default {\n  welcome: \"Welcome to our app\",\n  greeting: \"Hello, {name}!\",\n  submit: \"Submit form\"\n};\n```\n\n2. Nested Objects:\n```typescript\nexport default {\n  header: {\n    title: \"Book Your Flight\",\n    subtitle: \"Find the best deals\"\n  },\n  footer: {\n    content: [\n      \"Please refer to {{privacyPolicyUrl}} for details\",\n      \"© {{year}} {{companyName}}\"\n    ]\n  }\n};\n```\n\n3. Complex Structures with Arrays:\n```typescript\nexport default {\n  faq: {\n    heading: \"Common questions\",\n    content: [\n      {\n        heading: \"What if I need to change my flight?\",\n        content: \"You can change your flight online if:\",\n        list: [\n          \"You have a flexible fare type\",\n          \"Your flight is more than 24 hours away\"\n        ]\n      }\n    ]\n  }\n};\n```\n\n4. Template Literals with Variables:\n```typescript\nexport default {\n  greeting: `Hello, {{username}}!`,\n  message: `Welcome to {{appName}}`\n};\n```\n\n### Output Formats\n\n#### Data Extraction Output\n\nThe extracted data is saved as a JSON file with dot notation for nested structures:\n\n```json\n{\n  \"welcome\": \"Welcome to our app\",\n  \"header.title\": \"Book Your Flight\",\n  \"footer.content.0\": \"Please refer to {{privacyPolicyUrl}} for details\",\n  \"footer.content.1\": \"© {{year}} {{companyName}}\",\n  \"faq.content.0.heading\": \"What if I need to change my flight?\"\n}\n```\n\n#### SVG Extraction Output\n\nSVG components are extracted into individual .svg files, with React-specific code removed. For example:\n\nInput (React component):\n```tsx\nconst InspectionIcon: React.FC<InspectionIconProps> = ({ title }) => (\n  <svg className=\"c-tab__icon\" width=\"40px\" id=\"Layer_1\" data-name=\"Layer 1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 32 32\">\n    <title>{title}</title>\n    <path className=\"cls-1\" d=\"M18.89,12.74a3.18,3.18,0,0,1-3.24-3.11...\" />\n  </svg>\n);\n```\n\nOutput (InspectionIcon.svg):\n```svg\n<svg width=\"40px\" id=\"Layer_1\" data-name=\"Layer 1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 32 32\">\n    <path class=\"cls-1\" d=\"M18.89,12.74a3.18,3.18,0,0,1-3.24-3.11...\" />\n</svg>\n```\n\n## Extending Supported Patterns\n\nThe extractor uses Babel to parse and traverse the AST (Abstract Syntax Tree) of your source files. You can extend the supported patterns by modifying the source code:\n\n1. **Add New Node Types**: The `extractStringValue` method in `src/index.ts` handles different types of string values. Extend it to support new node types:\n\n```typescript\nprivate extractStringValue(node: t.Node): string | null {\n  if (t.isStringLiteral(node)) {\n    return node.value;\n  } else if (t.isTemplateLiteral(node)) {\n    return node.quasis.map(quasi => quasi.value.raw).join('{{}}');\n  }\n  // Add support for new node types here\n  return null;\n}\n```\n\n2. **Custom Value Processing**: The `processValue` method handles different value types (strings, arrays, objects). Extend it to support new value types or custom processing:\n\n```typescript\nprivate processValue(value: t.Node, currentPath: string[]): void {\n  if (t.isStringLiteral(value) || t.isTemplateLiteral(value)) {\n    // Process string values\n  } else if (t.isArrayExpression(value)) {\n    // Process arrays\n  } else if (t.isObjectExpression(value)) {\n    // Process objects\n  }\n  // Add support for new value types here\n}\n```\n\n3. **Custom AST Traversal**: The server uses Babel's traverse to walk the AST. You can add new visitors to handle different node types:\n\n```typescript\ntraverse(ast, {\n  ExportDefaultDeclaration(path: NodePath<t.ExportDefaultDeclaration>) {\n    // Handle default exports\n  },\n  // Add new visitors here\n});\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sammcj",
        "extractor",
        "mcp",
        "processing sammcj",
        "sammcj mcp",
        "data extractor"
      ],
      "category": "document-processing"
    },
    "sanderkooger--mcp-server-ragdocs": {
      "owner": "sanderkooger",
      "name": "mcp-server-ragdocs",
      "url": "https://github.com/sanderkooger/mcp-server-ragdocs",
      "imageUrl": "/freedevtools/mcp/pfp/sanderkooger.webp",
      "description": "Retrieve and process documentation using vector search to enhance AI responses. Enable the creation of documentation-aware AI assistants and context-aware tooling for developers.",
      "stars": 23,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T19:11:35Z",
      "readme_content": "# MCP-server-ragdocs\n[![Node.js Package](https://github.com/sanderkooger/mcp-server-ragdocs/actions/workflows/release.yml/badge.svg)](https://github.com/sanderkooger/mcp-server-ragdocs/actions/workflows/release.yml)\n![NPM Downloads](https://img.shields.io/npm/dy/%40sanderkooger%2Fmcp-server-ragdocs)\n[![Version](https://img.shields.io/npm/v/@sanderkooger/mcp-server-ragdocs)](https://npmjs.com/package/@sanderkooger/mcp-server-ragdocs)\n[![codecov](https://codecov.io/gh/sanderkooger/mcp-server-ragdocs/branch/main/graph/badge.svg)](https://codecov.io/gh/sanderkooger/mcp-server-ragdocs)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context.\n\n## Table of Contents\n\n- [Usage](#usage)\n- [Features](#features)\n- [Configuration](#configuration)\n- [Deployment](#deployment)\n  - [Local Development](#local-development)\n  - [Cloud Deployment](#cloud-deployment)\n- [Playwright Integration](#playwright-integration)\n- [Tools](#tools)\n- [Project Structure](#project-structure)\n- [Using Ollama Embeddings](#using-ollama-embeddings)\n- [License](#license)\n- [Development Workflow](#development-workflow)\n- [Contributing](#contributing)\n- [Forkception Acknowledgments](#forkception-acknowledgments)\n\n## Usage\n\nThe RAG Documentation tool is designed for:\n\n- Enhancing AI responses with relevant documentation\n- Building documentation-aware AI assistants\n- Creating context-aware tooling for developers\n- Implementing semantic documentation search\n- Augmenting existing knowledge bases\n\n## Features\n\n- Vector-based documentation search and retrieval\n- Support for multiple documentation sources\n- Support for local (Ollama) embeddings generation or OPENAI\n- Semantic search capabilities\n- Automated documentation processing\n- Real-time context augmentation for LLMs\n\n## Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sanderkooger/mcp-server-ragdocs\"],\n      \"env\": {\n        \"EMBEDDINGS_PROVIDER\": \"ollama\",\n        \"QDRANT_URL\": \"your-qdrant-url\",\n        \"QDRANT_API_KEY\": \"your-qdrant-key\" # if applicable\n      }\n    }\n  }\n}\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n### OpenAI Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs-openai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sanderkooger/mcp-server-ragdocs\"],\n      \"env\": {\n        \"EMBEDDINGS_PROVIDER\": \"openai\",\n        \"OPENAI_API_KEY\": \"your-openai-key-here\",\n        \"QDRANT_URL\": \"your-qdrant-url\",\n        \"QDRANT_API_KEY\": \"your-qdrant-key\"\n      }\n    }\n  }\n}\n```\n\n### Ollama Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs-ollama\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sanderkooger/mcp-server-ragdocs\"],\n      \"env\": {\n        \"EMBEDDINGS_PROVIDER\": \"ollama\",\n        \"OLLAMA_BASE_URL\": \"http://localhost:11434\",\n        \"QDRANT_URL\": \"your-qdrant-url\",\n        \"QDRANT_API_KEY\": \"your-qdrant-key\"\n      }\n    }\n  }\n}\n```\n\n### Ollama run from this codebase\n```\n\"ragdocs-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/home/sander/code/mcp-server-ragdocs/build/index.js\"\n      ],\n      \"env\": {\n        \"QDRANT_URL\": \"http://127.0.0.1:6333\",\n        \"EMBEDDINGS_PROVIDER\": \"ollama\",\n        \"OLLAMA_URL\": \"http://localhost:11434\"\n      },\n      \"alwaysAllow\": [\n        \"run_queue\",\n        \"list_queue\",\n        \"list_sources\",\n        \"search_documentation\",\n        \"clear_queue\",\n        \"remove_documentation\",\n        \"extract_urls\"\n      ],\n      \"timeout\": 3600\n    }\n```\n\n\n\n## Environment Variables Reference\n\n| Variable                | Required For  | Default                  | remarks                       |\n|-------------------------|---------------|--------------------------|-------------------------------|\n| `EMBEDDINGS_PROVIDER`   | All           | `ollama`                 | \"openai\" or \"ollama\"          |\n| `OPENAI_API_KEY`        | OpenAI        | -                        | Obtain from OpenAI dashboard  |\n| `OLLAMA_BASE_URL`       | Ollama        | `http://localhost:11434` | Local Ollama server URL       |\n| `QDRANT_URL`            | All           | `http://localhost:6333`  | Qdrant endpoint URL           |\n| `QDRANT_API_KEY`        | Cloud Qdrant  | -                        | From Qdrant Cloud console     |\n| `PLAYWRIGHT_WS_ENDPOINT`| Playwright Remote | -                      | WebSocket endpoint for remote Playwright server (e.g., `ws://localhost:3000/`) |\n\n\n### Local Deployment\n\nThe repository includes Docker Compose configuration for local development:\n\n[Docker Compose Download](https://raw.githubusercontent.com/sanderkooger/mcp-server-ragdocs/main/docker-compose.yml)\n\n```bash\ndocker compose up -d\n```\n\nThis starts:\n\n- Qdrant vector database on port 6333\n- Ollama LLM service on port 11434\n\nAccess endpoints:\n\n- Qdrant: http://localhost:6333\n- Ollama: http://localhost:11434\n\n### Cloud Deployment\n\nFor production deployments:\n\n1. Use hosted Qdrant Cloud service\n2. Set these environment variables:\n\n```bash\nQDRANT_URL=your-cloud-cluster-url\nQDRANT_API_KEY=your-cloud-api-key\n```\n\n## Playwright Integration\n\nThis project supports running Playwright either locally or via a Docker container. This provides flexibility for environments where Playwright's dependencies might be challenging to install directly.\n\n### How it Works\n\nThe `src/api-client.ts` file automatically detects the presence of the `PLAYWRIGHT_WS_ENDPOINT` environment variable:\n\n- **If `PLAYWRIGHT_WS_ENDPOINT` is set**: The application will attempt to connect to a remote Playwright server at the specified WebSocket endpoint using `chromium.connect()`. This is ideal for using a containerized Playwright instance.\n- **If `PLAYWRIGHT_WS_ENDPOINT` is not set**: The application will launch a local Playwright browser instance using `chromium.launch()`.\n\n### Running Playwright in Docker\n\nA `playwright` service has been added to the `docker-compose.yml` file to facilitate running Playwright in a Docker container.\n\nTo start the Playwright server in Docker:\n\n```bash\ndocker-compose up playwright\n```\n\nThis command will pull the `mcr.microsoft.com/playwright:v1.53.0-noble` image and start a Playwright server accessible on port `3000` of your host machine.\n\nTo configure your application to use this containerized Playwright instance, set the following environment variable:\n\n```bash\nPLAYWRIGHT_WS_ENDPOINT=ws://localhost:3000/\n```\n\n## Tools\n\n### search_documentation\n\nSearch through stored documentation using natural language queries. Returns matching excerpts with context, ranked by relevance.\n\n**Inputs:**\n\n- `query` (string): The text to search for in the documentation. Can be a natural language query, specific terms, or code snippets.\n- `limit` (number, optional): Maximum number of results to return (1-20, default: 5). Higher limits provide more comprehensive results but may take longer to process.\n\n### list_sources\n\nList all documentation sources currently stored in the system. Returns a comprehensive list of all indexed documentation including source URLs, titles, and last update times. Use this to understand what documentation is available for searching or to verify if specific sources have been indexed.\n\n### extract_urls\n\nExtract and analyze all URLs from a given web page. This tool crawls the specified webpage, identifies all hyperlinks, and optionally adds them to the processing queue.\n\n**Inputs:**\n\n- `url` (string): The complete URL of the webpage to analyze (must include protocol, e.g., https://). The page must be publicly accessible.\n- `add_to_queue` (boolean, optional): If true, automatically add extracted URLs to the processing queue for later indexing. Use with caution on large sites to avoid excessive queuing.\n\n### remove_documentation\n\nRemove specific documentation sources from the system by their URLs. The removal is permanent and will affect future search results.\n\n**Inputs:**\n\n- `urls` (string[]): Array of URLs to remove from the database. Each URL must exactly match the URL used when the documentation was added.\n\n### list_queue\n\nList all URLs currently waiting in the documentation processing queue. Shows pending documentation sources that will be processed when run_queue is called. Use this to monitor queue status, verify URLs were added correctly, or check processing backlog.\n\n### run_queue\n\nProcess and index all URLs currently in the documentation queue. Each URL is processed sequentially, with proper error handling and retry logic. Progress updates are provided as processing occurs. Long-running operations will process until the queue is empty or an unrecoverable error occurs.\n\n### clear_queue\n\nRemove all pending URLs from the documentation processing queue. Use this to reset the queue when you want to start fresh, remove unwanted URLs, or cancel pending processing. This operation is immediate and permanent - URLs will need to be re-added if you want to process them later.\n\n## Project Structure\n\nThe package follows a modular architecture with clear separation between core components and MCP protocol handlers. See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed structural documentation and design decisions.\n\n## Using Ollama Embeddings without docker\n\n1. Install Ollama:\n\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n2. Download the nomic-embed-text model:\n\n```bash\nollama pull nomic-embed-text\n```\n\n3. Verify installation:\n\n```bash\nollama list\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Contributing\n\nWe welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines, but here are the basics:\n\n1. Fork the repository\n2. Install dependencies: `npm install`\n3. Create a feature branch: `git checkout -b feat/your-feature`\n4. Commit changes with npm run commit to ensure compliance with [Conventional Commits](https://www.conventionalcommits.org)\n5. Push to your fork and open a PR\n\n## Forkception Acknowledgments\n\nThis project is based on a fork of [hannesrudolph/mcp-ragdocs](https://github.com/hannesrudolph/mcp-ragdocs), which itself was forked from the original work by [qpd-v/mcp-ragdocs](https://github.com/qpd-v/mcp-ragdocs). The original project provided the foundation for this implementation.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "ai",
        "document",
        "documentation aware",
        "document processing",
        "ai assistants"
      ],
      "category": "document-processing"
    },
    "sdairs--claudekeep": {
      "owner": "sdairs",
      "name": "claudekeep",
      "url": "https://github.com/sdairs/claudekeep",
      "imageUrl": "/freedevtools/mcp/pfp/sdairs.webp",
      "description": "A server implementation that enables the saving and sharing of AI conversations from Claude Desktop, featuring both a private chat storage and a public chat display web app. This implementation utilizes the Model Context Protocol (MCP) to manage interactions with AI chat logs.",
      "stars": 10,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T20:22:05Z",
      "readme_content": "Anthropic sent a trademark take down notice for claudekeep, so this little experiment is over. The code remains available in case its useful for anyone working with MCP.\n\nIf it wasn't clear enough, Claudekeep is not affiliated in any way with, nor endorsed by, Anthropic.\n\n# ClaudeKeep\n\nClaudeKeep is an experiment with Model Context Protocol (MCP) to save and share your AI conversations from Claude Desktop.\n\nIt includes:\n- an MCP server implementation that allows you to ask Claude to save your chat\n- a web app that allows you to view your private chats and see public chats\n\n## WARNING - THIS IS AN EXPERIMENT\n\nThis is an experiment. Please do not assume that it works perfectly or is secure.\n\nWhile I have done some testing, I make no guarantees that I've caught every edge case to make sure your chats are not exposed. I suggest you don't test it with sensitive chats.\n\n## How to use it?\n\n### 1. Login and get a token\n\nGo to [https://claudekeep.com](https://claudekeep.com) and hit **Login**. This will attempt to log you in via OAuth with GitHub. At the moment, this is the only OAuth provider supported.\n\nOnce logged in, in the top right you'll see a box with a JWT token, copy it.\n\n### 2. Configure Claude Desktop to use the MCP server\n\nTo use with Claude Desktop, you need to add the server config to the following file:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nUse this config and then restart Claude Desktop (you must completely kill it CMD+Q style and then restart it):\n\n```json\n{\n  \"mcpServers\": {\n    \"claudekeep-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"claudekeep-mcp\"\n      ],\n      \"env\": {\n        \"CLAUDEKEEP_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\nClaude Desktop can be awkward with reading your PATH. See the [MCP readme](./apps/mcp/README.md) for more info if the MCP server doesn't work.\n\n### 3. Select the MCP and chat with Claude\n\nWhen you open Claude Desktop, there is a little paperclip icon. Hover over it and there will be a little plug icon. Click that and pick `default` under `claudekeep-mcp`. This will attach the default prompt to Claude. \n\nNow just chat with Claude as normal.\n\nEvery message you write will trigger the `store_message` tool. This will store the message locally in Claude Desktop.\n\nWhen you want to save your chat, just ask Claude. Claude will run the `save_chat` tool. By default chats are always stored as private. If you want to make your chat public straight away, let claude know when you ask it to save.\n\nFor example (but remember, it's an LLM, it's interpreting your langauge, so you can ask however you want and it will probably hopefully do the right thing):\n\nTo save a a private chat:\n```\nyou: save this chat\n```\n\nTo save a public chat:\n```\nyou: save this chat and make it public\n```\n\n## Need help?\n\nRaise an issue or contact me on [BlueSky](https://bsky.app/profile/alasdairb.com).\n\n## Refresh your token\n\nIf you accidentally expose your token, login and hit the little refresh icon next to the token. You'll see a warning, click the confirm button and it will generate a new token. The old token will be destroyed and is not recoverable.\n\n## Abuse\n\nI hope people are good and don't share dodgy chats, but it's the internet, so 🤷‍♂️ it'll probably happen. I'll do my best to catch it, but please nudge me on BlueSky if I miss something.\n\nNote that your chats are stored against your GitHub account, so while public chats are anonymous to other users, they're not anonymous on the server.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chat",
        "conversations",
        "document",
        "ai chat",
        "ai conversations",
        "chat storage"
      ],
      "category": "document-processing"
    },
    "seanivore--Convert-Markdown-PDF-MCP": {
      "owner": "seanivore",
      "name": "Convert-Markdown-PDF-MCP",
      "url": "https://github.com/seanivore/Convert-Markdown-PDF-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/seanivore.webp",
      "description": "Converts Markdown files into styled PDF documents using VS Code's markdown formatting and Python's ReportLab. Offers note storage with custom URI access and provides functionality to summarize all stored notes.",
      "stars": 12,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T10:44:47Z",
      "readme_content": "# md-pdf-mcp (Markdown to PDF MCP Server)\n\nConvert Markdown to gorgeously styled PDFs using VS Code's markdown styling and Python's ReportLab.\n\n## Components\n\n### Resources\n\nThe server implements a simple note storage system with:\n- Custom note:// URI scheme for accessing individual notes\n- Each note resource has a name, description and text/plain mimetype\n\n### Prompts\n\nThe server provides a single prompt:\n- summarize-notes: Creates summaries of all stored notes\n  - Optional \"style\" argument to control detail level (brief/detailed)\n  - Generates prompt combining all current notes with style preference\n\n### Tools\n\nThe server implements one tool:\n- add-note: Adds a new note to the server\n  - Takes \"name\" and \"content\" as required string arguments\n  - Updates server state and notifies clients of resource changes\n\n## Configuration\n\n[TODO: Add configuration details specific to your implementation]\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"md-pdf-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/path/to/your/local/md-pdf-mcp\",\n        \"run\",\n        \"md-pdf-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"md-pdf-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"md-pdf-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/seanivore/Development/md-pdf-mcp run md-pdf-mcp\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "document",
        "notes",
        "markdown files",
        "markdown pdf",
        "converts markdown"
      ],
      "category": "document-processing"
    },
    "seanivore--mcp-file-preview": {
      "owner": "seanivore",
      "name": "mcp-file-preview",
      "url": "https://github.com/seanivore/mcp-file-preview",
      "imageUrl": "/freedevtools/mcp/pfp/seanivore.webp",
      "description": "Enables previewing and analyzing local HTML files, including capturing full-page screenshots and examining their structural elements such as headings, paragraphs, images, and links.",
      "stars": 22,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:04Z",
      "readme_content": "# MCP File Preview Server\n\nA Model Context Protocol (MCP) server that provides HTML file preview and analysis capabilities. This server enables capturing full-page screenshots of local HTML files and analyzing their structure.\n\n## Features\n\n- **File Preview**: Capture full-page screenshots of HTML files with proper CSS styling\n- **Content Analysis**: Analyze HTML structure (headings, paragraphs, images, links)\n- **Local File Support**: Handle local file paths and resources\n- **Screenshot Management**: Save screenshots to a dedicated directory\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/your-username/mcp-file-preview.git\ncd mcp-file-preview\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Configuration\n\nAdd the server to your Claude or Cline MCP settings:\n\n### Claude Desktop App\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"file-preview\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-file-preview/build/index.js\"]\n    }\n  }\n}\n```\n\n### Cline VSCode Extension\nAdd to VSCode's MCP settings:\n```json\n{\n  \"mcpServers\": {\n    \"file-preview\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-file-preview/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides two main tools:\n\n### preview_file\nCaptures a screenshot and returns HTML content:\n```typescript\n<use_mcp_tool>\n<server_name>file-preview</server_name>\n<tool_name>preview_file</tool_name>\n<arguments>\n{\n  \"filePath\": \"/path/to/file.html\",\n  \"width\": 1024,  // optional\n  \"height\": 768   // optional\n}\n</arguments>\n</use_mcp_tool>\n```\n\nScreenshots are saved to `screenshots/` directory in the project folder.\n\n### analyze_content\nAnalyzes HTML structure:\n```typescript\n<use_mcp_tool>\n<server_name>file-preview</server_name>\n<tool_name>analyze_content</tool_name>\n<arguments>\n{\n  \"filePath\": \"/path/to/file.html\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\nReturns counts of:\n- Headings\n- Paragraphs\n- Images\n- Links\n\n## Development\n\n1. Install dependencies:\n```bash\nnpm install @modelcontextprotocol/sdk puppeteer typescript @types/node @types/puppeteer\n```\n\n2. Make changes in `src/`\n3. Build:\n```bash\nnpm run build\n```\n4. Test locally:\n```bash\nnpm run dev\n```\n\n## Implementation Details\n\nThe server uses the MCP SDK's Server class with proper initialization:\n\n```typescript\nthis.server = new Server(\n  // Metadata object\n  {\n    name: 'file-preview-server',\n    version: '0.1.0'\n  },\n  // Options object with capabilities\n  {\n    capabilities: {\n      tools: {\n        preview_file: {\n          description: 'Preview local HTML file and capture screenshot',\n          inputSchema: {\n            // ... schema definition\n          }\n        }\n      }\n    }\n  }\n);\n```\n\nKey points:\n- Server constructor takes separate metadata and options objects\n- Tools are declared in capabilities.tools\n- Each tool needs a description and inputSchema\n- Screenshots are saved to a local `screenshots/` directory\n\n## Debugging\n\n1. Use the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector\n```\n\n2. Connect with:\n   - Transport Type: STDIO\n   - Command: node\n   - Arguments: /path/to/build/index.js\n\n3. Check Claude OS logs if tools don't appear in the dropdown\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "previewing",
        "preview",
        "html",
        "file preview",
        "html files",
        "previewing analyzing"
      ],
      "category": "document-processing"
    },
    "shifusen329--doc-lib-mcp": {
      "owner": "shifusen329",
      "name": "doc-lib-mcp",
      "url": "https://github.com/shifusen329/doc-lib-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/shifusen329.webp",
      "description": "Manage and semantically search documentation by adding, ingesting, chunking, and querying notes across various document types. Create summaries tailored to specific detail levels and access individual notes through a custom URI scheme.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-19T07:06:54Z",
      "readme_content": "# doc-lib-mcp MCP server\n\nA Model Context Protocol (MCP) server for document ingestion, chunking, semantic search, and note management.\n\n## Components\n\n### Resources\n\n- Implements a simple note storage system with:\n  - Custom `note://` URI scheme for accessing individual notes\n  - Each note resource has a name, description, and `text/plain` mimetype\n\n### Prompts\n\n- Provides a prompt:\n  - **summarize-notes**: Creates summaries of all stored notes\n    - Optional \"style\" argument to control detail level (brief/detailed)\n    - Generates prompt combining all current notes with style preference\n\n### Tools\n\nThe server implements a wide range of tools:\n\n- **add-note**: Add a new note to the in-memory note store\n  - Arguments: `name` (string), `content` (string)\n- **ingest-string**: Ingest and chunk a markdown or plain text string provided via message\n  - Arguments: `content` (string, required), `source` (string, optional), `tags` (list of strings, optional)\n- **ingest-markdown**: Ingest and chunk a markdown (.md) file\n  - Arguments: `path` (string)\n- **ingest-python**: Ingest and chunk a Python (.py) file\n  - Arguments: `path` (string)\n- **ingest-openapi**: Ingest and chunk an OpenAPI JSON file\n  - Arguments: `path` (string)\n- **ingest-html**: Ingest and chunk an HTML file\n  - Arguments: `path` (string)\n- **ingest-html-url**: Ingest and chunk HTML content from a URL (optionally using Playwright for dynamic content)\n  - Arguments: `url` (string), `dynamic` (boolean, optional)\n- **smart_ingestion**: Extracts all technically relevant content from a file using Gemini, then chunks it using robust markdown logic.\n  - Arguments:\n    - `path` (string, required): File path to ingest.\n    - `prompt` (string, optional): Custom prompt to use for Gemini.\n    - `tags` (list of strings, optional): Optional list of tags for classification.\n  - Uses Gemini 2.0 Flash 001 to extract only code, configuration, markdown structure, and technical definitions (no summaries or commentary).\n  - Passes the extracted content to a mistune 3.x-based chunker that preserves both code blocks and markdown/narrative content as separate chunks.\n  - Each chunk is embedded and stored for semantic search and retrieval.\n- **search-chunks**: Semantic search over ingested content\n  - Arguments:\n    - `query` (string): The semantic search query.\n    - `top_k` (integer, optional, default 3): Number of top results to return.\n    - `type` (string, optional): Filter results by chunk type (e.g., `code`, `html`, `markdown`).\n    - `tag` (string, optional): Filter results by tag in chunk metadata.\n  - Returns the most relevant chunks for a given query, optionally filtered by type and/or tag.\n- **delete-source**: Delete all chunks from a given source\n  - Arguments: `source` (string)\n- **delete-chunk-by-id**: Delete one or more chunks by id\n  - Arguments: `id` (integer, optional), `ids` (list of integers, optional)\n  - You can delete a single chunk by specifying `id`, or delete multiple chunks at once by specifying `ids`.\n- **update-chunk-type**: Update the type attribute for a chunk by id\n  - Arguments: `id` (integer, required), `type` (string, required)\n- **ingest-batch**: Ingest and chunk multiple documentation files (markdown, OpenAPI JSON, Python) in batch\n  - Arguments: `paths` (list of strings)\n- **list-sources**: List all unique sources (file paths) that have been ingested and stored in memory, with optional filtering by tag or semantic search.\n  - Arguments:\n    - `tag` (string, optional): Filter sources by tag in chunk metadata.\n    - `query` (string, optional): Semantic search query to find relevant sources.\n    - `top_k` (integer, optional, default 10): Number of top sources to return when using query.\n- **get-context**: Retrieve relevant content chunks (content only) for use as AI context, with filtering by tag, type, and semantic similarity.\n  - Arguments:\n    - `query` (string, optional): The semantic search query.\n    - `tag` (string, optional): Filter results by a specific tag in chunk metadata.\n    - `type` (string, optional): Filter results by chunk type (e.g., 'code', 'markdown').\n    - `top_k` (integer, optional, default 5): The number of top relevant chunks to retrieve.\n- **update-chunk-metadata**: Update the metadata field for a chunk by id\n  - Arguments: `id` (integer), `metadata` (object)\n- **tag-chunks-by-source**: Adds specified tags to the metadata of all chunks associated with a given source (URL or file path). Merges with existing tags.\n  - Arguments: `source` (string), `tags` (list of strings)\n- **list-notes**: List all currently stored notes and their content.\n\n#### Chunking and Code Extraction\n\n- Markdown, Python, OpenAPI, and HTML files are split into logical chunks for efficient retrieval and search.\n- The markdown chunker uses mistune 3.x's AST API and regex to robustly split content by code blocks and narrative, preserving all original formatting.\n- Both code blocks and markdown/narrative content are preserved as separate chunks.\n- The HTML chunker uses the `readability-lxml` library to extract main content first, then extracts block code snippets from `<pre>` tags as dedicated \"code\" chunks. Inline `<code>` content remains part of the narrative chunks.\n\n#### Semantic Search\n\n- The `search-chunks` tool performs vector-based semantic search over all ingested content, returning the most relevant chunks for a given query.\n- Supports optional `type` and `tag` arguments to filter results by chunk type (e.g., `code`, `html`, `markdown`) and/or by tag in chunk metadata, before semantic ranking.\n- This enables highly targeted retrieval, such as \"all code chunks tagged with 'langfuse' relevant to 'cost and usage'\".\n\n#### Metadata Management\n\n- Chunks include a `metadata` field for categorization and tagging.\n- The `update-chunk-metadata` tool allows updating metadata for any chunk by its id.\n- The `tag-chunks-by-source` tool allows adding tags to all chunks from a specific source in one operation. Tagging merges new tags with existing ones, preserving previous tags.\n\n## Configuration\n\nThe server requires the following environment variables (can be set in a .env file):\n\n### Ollama Configuration\n- OLLAMA_HOST: Hostname for Ollama API (default: localhost)\n- OLLAMA_PORT: Port for Ollama API (default: 11434)\n- RAG_AGENT: Ollama model to use for RAG responses (default: llama3)\n- OLLAMA_MODEL: Ollama model to use for embeddings (default: nomic-embed-text-v2-moe)\n\n### Database Configuration\n- HOST: PostgreSQL database host (default: localhost)\n- DB_PORT: PostgreSQL database port (default: 5432)\n- DB_NAME: PostgreSQL database name (default: doclibdb)\n- DB_USER: PostgreSQL database user (default: doclibdb_user)\n- DB_PASSWORD: PostgreSQL database password (default: doclibdb_password)\n\n### Reranker Configuration\n- RERANKER_MODEL_PATH: Path to the reranker model (default: /srv/samba/fileshare2/AI/models/bge-reranker-v2-m3)\n- RERANKER_USE_FP16: Whether to use FP16 for reranker (default: True)\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"doc-lib-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/home/administrator/python-share/doc-lib-mcp\",\n        \"run\",\n        \"doc-lib-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"doc-lib-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"doc-lib-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /home/administrator/python-share/doc-lib-mcp run doc-lib-mcp\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "shifusen329",
        "shifusen329 doc",
        "document processing",
        "search documentation"
      ],
      "category": "document-processing"
    },
    "shineforever--Youtube-Transcript-Download": {
      "owner": "shineforever",
      "name": "Youtube-Transcript-Download",
      "url": "https://github.com/shineforever/Youtube-Transcript-Download",
      "imageUrl": "/freedevtools/mcp/pfp/shineforever.webp",
      "description": "Download subtitles from popular video platforms like YouTube, Bilibili, TED, and Coursera using the AITransDub MCP service. Supports multiple subtitle languages for easier access and processing.",
      "stars": 2,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-13T15:41:31Z",
      "readme_content": "# Youtube-Transcript-Download\n\n## Overview\n\nThe **Youtube-Transcript-Download** project allows users to download subtitles from various popular video platforms such as YouTube, Bilibili, TED, and Coursera. This tool is powered by the **AITransDub MCP** service, enabling you to retrieve subtitle data from these platforms for further processing or personal use.\n\n### Key Features\n- Download subtitles from YouTube, Bilibili, TED, and Coursera.\n- Use AITransDub MCP to retrieve subtitle data.\n- Supports multiple subtitle languages including English, Korean, and more.\n- Effortless integration with video platforms for easy extraction.\n\nFor even more powerful features, visit the official website at [AITransDub.com](https://AITransDub.com).\n\n## Installation\n\nTo get started, you need to clone the repository and install the dependencies:\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/shineforever/Youtube-Transcript-Download.git\n   cd Youtube-Transcript-Download\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "subtitles",
        "subtitle",
        "aitransdub",
        "shineforever youtube",
        "download subtitles",
        "processing shineforever"
      ],
      "category": "document-processing"
    },
    "slamdunkasaur--ru-heritage": {
      "owner": "slamdunkasaur",
      "name": "ru-heritage",
      "url": "https://github.com/slamdunkasaur/ru-heritage",
      "imageUrl": "/freedevtools/mcp/pfp/slamdunkasaur.webp",
      "description": "Downloads digitised books from e-heritage.ru and converts them into PDF format, facilitating easier access to digital heritage content.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-01-20T15:00:03Z",
      "readme_content": "# ru-heritage\nDownload digitised books from e-heritage.ru and save them as PDF\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slamdunkasaur",
        "ru",
        "heritage",
        "ru heritage",
        "heritage ru",
        "slamdunkasaur ru"
      ],
      "category": "document-processing"
    },
    "softgridinc-pte-ltd--mcp-excel-reader-server": {
      "owner": "softgridinc-pte-ltd",
      "name": "mcp-excel-reader-server",
      "url": "https://github.com/softgridinc-pte-ltd/mcp-excel-reader-server",
      "imageUrl": "/freedevtools/mcp/pfp/softgridinc-pte-ltd.webp",
      "description": "Extract data from Excel files in structured JSON format, allowing access to all sheets or specific sheets by name or index. Handles data type conversions and manages empty cells efficiently.",
      "stars": 5,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-19T14:18:36Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/softgridinc-pte-ltd-mcp-excel-reader-server-badge.png)](https://mseep.ai/app/softgridinc-pte-ltd-mcp-excel-reader-server)\n\n# Excel Reader Server\n\nA Model Context Protocol (MCP) server that provides tools for reading Excel (xlsx) files.\n\n<a href=\"https://glama.ai/mcp/servers/kniyyx0gej\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kniyyx0gej/badge\" alt=\"Excel Reader Server MCP server\" />\n</a>\n\n## Features\n\n- Read content from all sheets in an Excel file\n- Read content from a specific sheet by name\n- Read content from a specific sheet by index\n- Returns data in JSON format\n- Handles empty cells and data type conversions\n\n## Installation\n\nRequires Python 3.10 or higher.\n\n```bash\n# Using pip\npip install excel-reader-server\n\n# Using uv (recommended)\nuv pip install excel-reader-server\n```\n\n## Dependencies\n\n- mcp >= 1.2.1\n- openpyxl >= 3.1.5\n\n## Usage\n\nThe server provides three main tools:\n\n### 1. read_excel\n\nReads content from all sheets in an Excel file.\n\n```python\n{\n  \"file_path\": \"path/to/your/excel/file.xlsx\"\n}\n```\n\n### 2. read_excel_by_sheet_name\n\nReads content from a specific sheet by name. If no sheet name is provided, reads the first sheet.\n\n```python\n{\n  \"file_path\": \"path/to/your/excel/file.xlsx\",\n  \"sheet_name\": \"Sheet1\"  # optional\n}\n```\n\n### 3. read_excel_by_sheet_index\n\nReads content from a specific sheet by index. If no index is provided, reads the first sheet (index 0).\n\n```python\n{\n  \"file_path\": \"path/to/your/excel/file.xlsx\",\n  \"sheet_index\": 0  # optional\n}\n```\n\n## Response Format\n\nThe server returns data in the following JSON format:\n\n```json\n{\n  \"Sheet1\": [\n    [\"Header1\", \"Header2\", \"Header3\"],\n    [\"Value1\", \"Value2\", \"Value3\"],\n    [\"Value4\", \"Value5\", \"Value6\"]\n  ]\n}\n```\n\n- Each sheet is represented as a key in the top-level object\n- Sheet data is an array of arrays, where each inner array represents a row\n- All values are converted to strings\n- Empty cells are represented as empty strings\n\n## Error Handling\n\nThe server provides clear error messages for common issues:\n- File not found\n- Invalid sheet name\n- Index out of range\n- General Excel file reading errors\n\n## License\n\nThis project is released under the Apache 2 License. See the LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "excel",
        "softgridinc",
        "processing",
        "excel files",
        "excel reader",
        "processing softgridinc"
      ],
      "category": "document-processing"
    },
    "spacemeowx2--cargo-doc-mcp": {
      "owner": "spacemeowx2",
      "name": "cargo-doc-mcp",
      "url": "https://github.com/spacemeowx2/cargo-doc-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/spacemeowx2.webp",
      "description": "Manage and interact with Rust documentation, performing tasks such as checking, building, and searching through project documentation. Access crate documentation and symbol listings to enhance development workflows.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T00:33:54Z",
      "readme_content": "# cargo doc MCP Server\n\nA MCP server for managing Rust documentation through cargo doc commands. This server provides tools to check, build, and search Rust documentation locally.\n\n<a href=\"https://glama.ai/mcp/servers/l4augy7aft\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/l4augy7aft/badge\" alt=\"Cargo Doc Server MCP server\" />\n</a>\n\n## Features\n\n### Tools\n\n- `get_crate_doc` - Get crate's main documentation page for understanding overall concepts and usage\n\n  - Parameters:\n    - `project_path`: Path to the Rust project (must be absolute path)\n    - `crate_name`: Name of the crate to get documentation for\n\n- `list_symbols` - List all symbols (structs, enums, traits, etc.) in a crate's documentation\n\n  - Parameters:\n    - `project_path`: Path to the Rust project (must be absolute path)\n    - `crate_name`: Name of the crate to list symbols for\n\n- `search_doc` - Search within a crate's documentation\n  - Parameters:\n    - `project_path`: Path to the Rust project (must be absolute path)\n    - `crate_name`: Name of the crate to search in\n    - `query`: Search query (keyword or symbol)\n    - `limit` (optional): Maximum number of results to return (default: 10)\n\n## Requirements\n\n- Node.js 16 or later\n- Rust and Cargo installed\n\n## Installation\n\nInstall dependencies:\n\n```bash\npnpm install\n```\n\nBuild the server:\n\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm run watch\n```\n\n## Usage\n\nAdd the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"docs-rs-mcp\": {\n      \"command\": \"/absolute/path/to/docs-rs-mcp/build/index.js\"\n    }\n  }\n}\n```\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the MCP Inspector:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Cache System\n\nThe server maintains a cache of built documentation paths to improve performance. Cache entries expire after 24 hours to ensure documentation stays up-to-date.\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "document",
        "cargo",
        "rust documentation",
        "crate documentation",
        "cargo doc"
      ],
      "category": "document-processing"
    },
    "speakeasy-api--markdown-sidecar-mcp": {
      "owner": "speakeasy-api",
      "name": "markdown-sidecar-mcp",
      "url": "https://github.com/speakeasy-api/markdown-sidecar-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Access and serve markdown documentation for NPM packages, Go Modules, and PyPi packages from a local environment, enhancing the code generation process. Default support for Python help documentation is included for packages lacking markdown documentation.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "markdown",
        "packages",
        "markdown documentation",
        "documentation included",
        "documentation npm"
      ],
      "category": "document-processing"
    },
    "spences10--mcp-jinaai-reader": {
      "owner": "spences10",
      "name": "mcp-jinaai-reader",
      "url": "https://github.com/spences10/mcp-jinaai-reader",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Reader API for efficient web content extraction, enabling analysis and processing of documentation and web content.",
      "stars": 29,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:10Z",
      "readme_content": "# mcp-jinaai-reader\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's Reader\nAPI with LLMs. This server provides efficient and comprehensive web\ncontent extraction capabilities, optimized for documentation and web\ncontent analysis.\n\n<a href=\"https://glama.ai/mcp/servers/a75afsx9cx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/a75afsx9cx/badge\" />\n</a>\n\n## Features\n\n- 📚 Advanced web content extraction through Jina.ai Reader API\n- 🚀 Fast and efficient content retrieval\n- 📄 Complete text extraction with preserved structure\n- 🔄 Clean format optimized for LLMs\n- 🌐 Support for various content types including documentation\n- 🏗️ Built on the Model Context Protocol\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-reader\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-reader\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-reader\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-reader\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### read_url\n\nConvert any URL to LLM-friendly text using Jina.ai Reader.\n\nParameters:\n\n- `url` (string, required): URL to process\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `format` (string, optional): Response format (\"json\" or \"stream\").\n  Defaults to \"json\"\n- `timeout` (number, optional): Maximum time in seconds to wait for\n  webpage load\n- `target_selector` (string, optional): CSS selector to focus on\n  specific elements\n- `wait_for_selector` (string, optional): CSS selector to wait for\n  specific elements\n- `remove_selector` (string, optional): CSS selector to exclude\n  specific elements\n- `with_links_summary` (boolean, optional): Gather all links at the\n  end of response\n- `with_images_summary` (boolean, optional): Gather all images at the\n  end of response\n- `with_generated_alt` (boolean, optional): Add alt text to images\n  lacking captions\n- `with_iframe` (boolean, optional): Include iframe content in\n  response\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Run in development mode:\n\n```bash\nnpm run dev\n```\n\n### Publishing\n\n1. Update version in package.json\n2. Build the project:\n\n```bash\nnpm run build\n```\n\n3. Publish to npm:\n\n```bash\nnpm publish\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Reader API](https://jina.ai)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "extraction",
        "reader",
        "content extraction",
        "jinaai reader",
        "document processing"
      ],
      "category": "document-processing"
    },
    "srvdneat--docs": {
      "owner": "srvdneat",
      "name": "docs",
      "url": "https://github.com/srvdneat/docs",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Provides a starter kit for creating and maintaining documentation, including guide pages, navigation, customizations, and API references. Supports local previews and automatic deployment of documentation updates via integration with a GitHub app.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "srvdneat",
        "document",
        "maintaining documentation",
        "documentation including",
        "srvdneat docs"
      ],
      "category": "document-processing"
    },
    "sylphxltd--pdf-reader-mcp": {
      "owner": "sylphxltd",
      "name": "pdf-reader-mcp",
      "url": "https://github.com/sylphxltd/pdf-reader-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sylphxltd.webp",
      "description": "Enables secure reading and extraction of text, metadata, and page counts from PDF files. Processes multiple PDFs from local paths or URLs with structured JSON output for easy parsing.",
      "stars": 262,
      "forks": 34,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:07:54Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/sylphxltd-pdf-reader-mcp-badge.png)](https://mseep.ai/app/sylphxltd-pdf-reader-mcp)\n\n# PDF Reader MCP Server (@sylphlab/pdf-reader-mcp)\n\n<!-- Status Badges Area -->\n\n[![CI/CD Pipeline](https://github.com/sylphlab/pdf-reader-mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/sylphlab/pdf-reader-mcp/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/sylphlab/pdf-reader-mcp/graph/badge.svg?token=VYRQFB40UN)](https://codecov.io/gh/sylphlab/pdf-reader-mcp)\n[![npm version](https://badge.fury.io/js/%40sylphlab%2Fpdf-reader-mcp.svg)](https://badge.fury.io/js/%40sylphlab%2Fpdf-reader-mcp)\n[![Docker Pulls](https://img.shields.io/docker/pulls/sylphlab/pdf-reader-mcp.svg)](https://hub.docker.com/r/sylphlab/pdf-reader-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<!-- End Status Badges Area -->\n\nEmpower your AI agents (like Cline) with the ability to securely read and extract information (text, metadata, page count) from PDF files within your project context using a single, flexible tool.\n\n<a href=\"https://glama.ai/mcp/servers/@sylphlab/pdf-reader-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@sylphlab/pdf-reader-mcp/badge\" alt=\"PDF Reader Server MCP server\" />\n</a>\n\n## Installation\n\n### Using npm (Recommended)\n\nInstall as a dependency in your MCP host environment or project:\n\n```bash\npnpm add @sylphlab/pdf-reader-mcp # Or npm install / yarn add\n```\n\nConfigure your MCP host (e.g., `mcp_settings.json`) to use `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"pdf-reader-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"@sylphlab/pdf-reader-mcp\"],\n      \"name\": \"PDF Reader (npx)\"\n    }\n  }\n}\n```\n\n_(Ensure the host sets the correct `cwd` for the target project)_\n\n### Using Docker\n\nPull the image:\n\n```bash\ndocker pull sylphlab/pdf-reader-mcp:latest\n```\n\nConfigure your MCP host to run the container, mounting your project directory to `/app`:\n\n```json\n{\n  \"mcpServers\": {\n    \"pdf-reader-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/your/project:/app\", // Or use \"$PWD:/app\", \"%CD%:/app\", etc.\n        \"sylphlab/pdf-reader-mcp:latest\"\n      ],\n      \"name\": \"PDF Reader (Docker)\"\n    }\n  }\n}\n```\n\n### Local Build (For Development)\n\n1. Clone: `git clone https://github.com/sylphlab/pdf-reader-mcp.git`\n2. Install: `cd pdf-reader-mcp && pnpm install`\n3. Build: `pnpm run build`\n4. Configure MCP Host:\n   ```json\n   {\n     \"mcpServers\": {\n       \"pdf-reader-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/cloned/repo/pdf-reader-mcp/build/index.js\"],\n         \"name\": \"PDF Reader (Local Build)\"\n       }\n     }\n   }\n   ```\n   _(Ensure the host sets the correct `cwd` for the target project)_\n\n## Quick Start\n\nAssuming the server is running and configured in your MCP host:\n\n**MCP Request (Get metadata and page 2 text from a local PDF):**\n\n```json\n{\n  \"tool_name\": \"read_pdf\",\n  \"arguments\": {\n    \"sources\": [\n      {\n        \"path\": \"./documents/my_report.pdf\",\n        \"pages\": [2]\n      }\n    ],\n    \"include_metadata\": true,\n    \"include_page_count\": false, // Default is true, explicitly false here\n    \"include_full_text\": false // Ignored because 'pages' is specified\n  }\n}\n```\n\n**Expected Response Snippet:**\n\n```json\n{\n  \"results\": [\n    {\n      \"source\": \"./documents/my_report.pdf\",\n      \"success\": true,\n      \"data\": {\n        \"page_texts\": [\n          { \"page\": 2, \"text\": \"Text content from page 2...\" }\n        ],\n        \"info\": { ... },\n        \"metadata\": { ... }\n        // num_pages not included as requested\n      }\n    }\n  ]\n}\n```\n\n## Why Choose This Project?\n\n- **🛡️ Secure:** Confines file access strictly to the project root directory.\n- **🌐 Flexible:** Handles both local relative paths and public URLs.\n- **🧩 Consolidated:** A single `read_pdf` tool serves multiple extraction needs (full text, specific pages, metadata, page count).\n- **⚙️ Structured Output:** Returns data in a predictable JSON format, easy for agents to parse.\n- **🚀 Easy Integration:** Designed for seamless use within MCP environments via `npx` or Docker.\n- **✅ Robust:** Uses `pdfjs-dist` for reliable parsing and Zod for input validation.\n\n## Performance Advantages\n\nInitial benchmarks using Vitest on a sample PDF show efficient handling of various operations:\n\n| Scenario                         | Operations per Second (hz) | Relative Speed |\n| :------------------------------- | :------------------------- | :------------- |\n| Handle Non-Existent File         | ~12,933                    | Fastest        |\n| Get Full Text                    | ~5,575                     |                |\n| Get Specific Page (Page 1)       | ~5,329                     |                |\n| Get Specific Pages (Pages 1 & 2) | ~5,242                     |                |\n| Get Metadata & Page Count        | ~4,912                     | Slowest        |\n\n_(Higher hz indicates better performance. Results may vary based on PDF complexity and environment.)_\n\nSee the [Performance Documentation](./docs/performance/index.md) for more details and future plans.\n\n## Features\n\n- Read full text content from PDF files.\n- Read text content from specific pages or page ranges.\n- Read PDF metadata (author, title, creation date, etc.).\n- Get the total page count of a PDF.\n- Process multiple PDF sources (local paths or URLs) in a single request.\n- Securely operates within the defined project root.\n- Provides structured JSON output via MCP.\n- Available via npm and Docker Hub.\n\n## Design Philosophy\n\nThe server prioritizes security through context confinement, efficiency via structured data transfer, and simplicity for easy integration into AI agent workflows. It aims for minimal dependencies, relying on the robust `pdfjs-dist` library.\n\nSee the full [Design Philosophy](./docs/design/index.md) documentation.\n\n## Comparison with Other Solutions\n\nCompared to direct file access (often infeasible) or generic filesystem tools, this server offers PDF-specific parsing capabilities. Unlike external CLI tools (e.g., `pdftotext`), it provides a secure, integrated MCP interface with structured output, enhancing reliability and ease of use for AI agents.\n\nSee the full [Comparison](./docs/comparison/index.md) documentation.\n\n## Future Plans (Roadmap)\n\n- **Documentation:**\n  - Finalize all documentation sections (Guide, API, Design, Comparison).\n  - Resolve TypeDoc issue and generate API documentation.\n  - Add more examples and advanced usage patterns.\n  - Implement PWA support and mobile optimization for the docs site.\n  - Add share buttons and growth metrics to the docs site.\n- **Benchmarking:**\n  - Conduct comprehensive benchmarks with diverse PDF files (size, complexity).\n  - Measure memory usage.\n  - Compare URL vs. local file performance.\n- **Core Functionality:**\n  - Explore potential optimizations for very large PDF files.\n  - Investigate options for extracting images or annotations (longer term).\n- **Testing:**\n  - Increase test coverage towards 100% where practical.\n  - Add runtime tests once feasible.\n\n## Documentation\n\nFor detailed usage, API reference, and guides, please visit the **[Full Documentation Website](https://sylphlab.github.io/pdf-reader-mcp/)** (Link to be updated upon deployment).\n\n## Community & Support\n\n- **Found a bug or have a feature request?** Please open an issue on [GitHub Issues](https://github.com/sylphlab/pdf-reader-mcp/issues).\n- **Want to contribute?** We welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md).\n- **Star & Watch:** If you find this project useful, please consider starring ⭐ and watching 👀 the repository on [GitHub](https://github.com/sylphlab/pdf-reader-mcp) to show your support and stay updated!\n\n## License\n\nThis project is licensed under the [MIT License](./LICENSE).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "pdf",
        "sylphxltd",
        "pdf reader",
        "sylphxltd pdf",
        "reader mcp"
      ],
      "category": "document-processing"
    },
    "takashiishida--arxiv-latex-mcp": {
      "owner": "takashiishida",
      "name": "arxiv-latex-mcp",
      "url": "https://github.com/takashiishida/arxiv-latex-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/takashiishida.webp",
      "description": "Fetches and processes LaTeX sources of arXiv papers, enabling AI models to accurately interpret mathematical content and equations without the limitations of PDF files.",
      "stars": 66,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T06:58:54Z",
      "readme_content": "# arxiv-latex MCP Server\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub Release](https://img.shields.io/github/v/release/takashiishida/arxiv-latex-mcp)](https://github.com/takashiishida/arxiv-latex-mcp/releases)\n\n\nAn MCP server that enables [Claude Desktop](https://claude.ai/download), [Cursor](https://www.cursor.com/), or other MCP clients to directly access and process arXiv papers by fetching the LaTeX source. It uses [arxiv-to-prompt](https://github.com/takashiishida/arxiv-to-prompt) under the hood to handle downloading and processing the LaTeX.\n\nWhy use the LaTeX source instead of uploading PDFs? Many PDF chat applications often struggle with mathematical content and equation-heavy papers. By utilizing the original LaTeX source code from arXiv papers, the LLM can accurately understand and handle equations and notations. This approach is particularly valuable for fields like computer science, mathematics, and engineering where precise interpretation of mathematical expressions is crucial.\n\n## Installation\n\nIf you are using Claude Desktop and MacOS, you can utilize Desktop Extensions by double-clicking on the .dxt file to install.\nDownload the .dxt file from [here](https://github.com/takashiishida/arxiv-latex-mcp/releases/).\n\nOtherwise, you can manually add the following configuration to your config file:\n```json\n{\n  \"mcpServers\": {\n      \"arxiv-latex-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/ABSOLUTE/PATH/TO/arxiv-latex-mcp\",\n              \"run\",\n              \"server/main.py\"\n          ]\n      }\n  }\n}\n```\n\nYou may need to replace the `command` field with the full path of `uv`: check this by running `which uv` (MacOS/Linux) or `where uv` (Windows).\n\nRestart the application after saving the above.\n\nFor Claude Desktop, click on the hammer icon, and you should see `get_paper_prompt` in the list of \"Available MCP tools\".\n\n## Example\nTry asking questions about a paper from arXiv, e.g., \"Explain the first theorem in 2202.00395\"\n\n<div align=\"center\">\n  <img src=\"example.png\" alt=\"Example of using arXiv LaTeX MCP with Claude Desktop\" width=\"600\">\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "latex",
        "ai",
        "arxiv",
        "arxiv latex",
        "arxiv papers",
        "latex sources"
      ],
      "category": "document-processing"
    },
    "tatn--mcp-server-diff-python": {
      "owner": "tatn",
      "name": "mcp-server-diff-python",
      "url": "https://github.com/tatn/mcp-server-diff-python",
      "imageUrl": "/freedevtools/mcp/pfp/tatn.webp",
      "description": "Obtain text differences between two strings using Python's `difflib`, providing output in Unified diff format suitable for text comparison and version control.",
      "stars": 7,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-02T12:48:21Z",
      "readme_content": "# mcp-server-diff-python\n\nAn MCP server for obtaining text differences between two strings.\nThis server leverages Python's standard library `difflib` to efficiently generate and provide differences between two texts in Unified diff format, making it ideal for text comparison and version control purposes.\n\n<a href=\"https://glama.ai/mcp/servers/qbwsx2g4vd\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/qbwsx2g4vd/badge\" alt=\"Server Diff Python MCP server\" /></a>\n\n## Features\n\n### Tools\n\nThe server provides a single tool:\n\n- **get-unified-diff**: Get differences between two texts in Unified diff format\n  - Arguments:\n    - `string_a`: Source text for comparison (required)\n    - `string_b`: Target text to compare against (required)\n  - Return value: A string containing the differences in Unified diff format\n\n## Usage\n\n### Claude Desktop\n\nUsing with Claude Desktop\nTo use with Claude Desktop, add the server config:\n\nOn MacOS:  `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n\"mcpServers\": {\n  \"mcp-server-diff-python\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-diff-python\"\n    ]\n  }\n}\n```\n\nor Add the following configuration:\n\n```bash\ngit clone https://github.com/tatn/mcp-server-diff-python.git\ncd mcp-server-diff-python\nuv sync\nuv build\n```\n\n```json\n\"mcpServers\": {\n  \"mcp-server-diff-python\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"path\\\\to\\\\mcp-server-diff-python\",\n      \"run\",\n      \"mcp-server-diff-python\"\n    ]\n  }\n}\n```\n\n## Development\n### Debugging\n\nYou can start the MCP Inspector using [npx](https://docs.npmjs.com/cli/v11/commands/npx)with the following commands:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx mcp-server-diff-python\n```\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory path\\to\\mcp-server-diff-python run mcp-server-diff-python\n```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "difflib",
        "diff",
        "python",
        "diff python",
        "python difflib",
        "diff format"
      ],
      "category": "document-processing"
    },
    "taxihabbel--parsemypdf": {
      "owner": "taxihabbel",
      "name": "parsemypdf",
      "url": "https://github.com/taxihabbel/parsemypdf",
      "imageUrl": "/freedevtools/mcp/pfp/taxihabbel.webp",
      "description": "Extract and analyze complex PDF documents using various tools to maintain document structure and efficiently extract tables, images, and mixed content. Specialized processors are available tailored to the complexity and content type of the PDFs.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-13T22:52:33Z",
      "readme_content": "<div align=\"center\">\n<h1><a href=\"https://www.instagram.com/genieincodebottle/\"><img width=\"200\" src=\"https://github.com/genieincodebottle/generative-ai/blob/main/images/logo_genie_new.png\">&nbsp;</a></h1>\n</div>\n<div align=\"center\">\n    <a target=\"_blank\" href=\"https://www.youtube.com/@genieincodebottle\"><img src=\"https://img.shields.io/badge/YouTube-@genieincodebottle-blue\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/rajesh-srivastava\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.instagram.com/genieincodebottle/\"><img src=\"https://img.shields.io/badge/@genieincodebottle-C13584?style=flat-square&labelColor=C13584&logo=instagram&logoColor=white&link=https://www.instagram.com/eduardopiresbr/\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://github.com/genieincodebottle/generative-ai/blob/main/GenAI_Roadmap.md\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=GenAI Roadmap&logo=github&style=social\"></a>\n</div>\n\n#  \n\n# 📑 Complex PDF Parsing\n\nA comprehensive example codes for extracting content from PDFs\n\nAlso, check -> [Pdf Parsing Guide](https://github.com/genieincodebottle/parse-my-pdf/blob/main/pdf-parsing-guide.pdf)\n\n## 📌 Core Features\n\n### 📤 Content Extraction\n- Multiple extraction methods with different tools/libraries:\n  - Cloud-based: Claude 3.5 Sonnet, GPT-4 Vision, Unstructured.io\n  - Local: Llama 3.2 11B, Docling, PDFium\n  - Specialized: Camelot (tables), PDFMiner (text), PDFPlumber (mixed), PyPdf etc\n- Maintains document structure and formatting\n- Handles complex PDFs with mixed content including extracting image data\n\n\n## 📦 Implementation Options\n\n### 1. ☁️ Cloud-Based Methods\n- **Claude & Llama**: Excellent  for complex PDFs with mixed content\n- **GPT-4 Vision**: Excellent for visual content analysis\n- **Unstructured.io**: Advanced content partitioning and classification\n\n### 2. 🖥️ Local Methods\n- **Llama 3.2 11B Vision**: Image-based PDF processing\n- **Docling**: Excellent  for complex PDFs with mixed content\n- **PDFium**: High-fidelity processing using Chrome's PDF engine\n- **Camelot**: Specialized table extraction\n- **PDFMiner/PDFPlumber**: Basic text and layout extraction\n\n## 🔗 Dependencies\n\n### 📚 Core Libraries\n```bash\nlangchain_ollama\nlangchain_huggingface\nlangchain_community\nFAISS\npython-dotenv\n```\n\n### ⚙️ Implementation-Specific\n```bash\nanthropic        # Claude\nopenai           # GPT-4 Vision\ncamelot-py      # Table extraction\ndocling         # Text processing\npdf2image       # PDF conversion\npypdfium2       # PDFium processing\nboto3           # AWS Textract\n```\n\n## 🛠️ Setup\n\n1. Environment Variables\n```bash\nANTHROPIC_API_KEY=your_key_here    # For Claude\nOPENAI_API_KEY=your_key_here       # For OpenAI\nUNSTRUCTURED_API_KEY=your_key_here # For Unstructured.io\n```\n\n2. Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n3. Install Ollama & Models (for local processing)\n```bash\n# Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Pull required models\nollama pull llama3.1\nollama pull x/llama3.2-vision:11b\n```\n\n## 📈 Usage\n\n1. Place PDF files in `input/` directory\n\n## 📄 Example Complex Pdf placed in Input folder\n- **sample-1.pdf**: Standard tables\n- **sample-2.pdf**: Image-based simple tables\n- **sample-3.pdf**: Image-based complex tables\n- **sample-4.pdf**: Mixed content (text, tables, images)\n\n## 📝 Notes\n- System resources needed for local LLM operations\n- API keys required for cloud based implementations\n- Consider PDF complexity when choosing implementation\n- Ghostscript required for Camelot\n- Different processors suit different use cases\n  - Cloud: Complex documents, mixed content\n  - Local: Simple text, basic tables\n  - Specialized: Specific content types (tables, forms)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "parsemypdf",
        "pdfs",
        "pdf",
        "parsemypdf extract",
        "taxihabbel parsemypdf",
        "pdf documents"
      ],
      "category": "document-processing"
    },
    "thirdstrandstudio--mcp-xpath": {
      "owner": "thirdstrandstudio",
      "name": "mcp-xpath",
      "url": "https://github.com/thirdstrandstudio/mcp-xpath",
      "imageUrl": "/freedevtools/mcp/pfp/thirdstrandstudio.webp",
      "description": "Execute XPath queries on XML and HTML content, fetching and querying data from URLs or local files. Return structured results to enhance applications with powerful XML data manipulation capabilities.",
      "stars": 0,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-04T16:44:53Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/thirdstrandstudio-mcp-xpath-badge.png)](https://mseep.ai/app/thirdstrandstudio-mcp-xpath)\n\n# XPath MCP Server\n\n[![Third Strand Studio](https://img.shields.io/badge/Third%20Strand%20Studio-Visit%20Us-blue)](https://tss.topiray.com)\n\nMCP Server for executing XPath queries on XML content.\n\n<a href=\"https://glama.ai/mcp/servers/@thirdstrandstudio/mcp-xpath\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@thirdstrandstudio/mcp-xpath/badge\" alt=\"mcp-xpath MCP server\" />\n</a>\n\n![image](https://github.com/user-attachments/assets/369045f3-1cdb-4204-9c62-0f5f32636262)\n\n[![smithery badge](https://smithery.ai/badge/@thirdstrandstudio/mcp-xpath)](https://smithery.ai/server/@thirdstrandstudio/mcp-xpath)\n\n## Tools\n\n1. `xpath`\n   - Query XML content using XPath expressions\n   - Inputs:\n     - `xml` (string): The XML content to query\n     - `query` (string): The XPath query to execute\n     - `mimeType` (optional, string): The MIME type (e.g. text/xml, application/xml, text/html, application/xhtml+xml)\n   - Returns: The result of the XPath query as a string\n\n2. `xpathwithurl`\n   - Fetch content from a URL and query it using XPath expressions\n   - Inputs:\n     - `url` (string): The URL to fetch XML/HTML content from\n     - `query` (string): The XPath query to execute\n     - `mimeType` (optional, string): The MIME type (e.g. text/xml, application/xml, text/html, application/xhtml+xml)\n   - Returns: The result of the XPath query as a string\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-xpath for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@thirdstrandstudio/mcp-xpath):\n\n```bash\nnpx -y @smithery/cli install @thirdstrandstudio/mcp-xpath --client claude\n```\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the package\nnpm run build\n```\n\n## Setup\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n#### npx\n\n```json\n{\n  \"mcpServers\": {\n    \"xpath\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@thirdstrandstudio/mcp-xpath\"\n      ]\n    }\n  }\n}\n```\n\n#### Direct Node.js\n\n```json\n{\n  \"mcpServers\": {\n    \"xpath\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/mcp-xpath/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/path/to/mcp-xpath` with the actual path to your repository.\n\n\n## Examples\n\n### Query XML content\n\n```javascript\n// Select all <item> elements from XML\nconst result = await callTool(\"xpath\", {\n  xml: \"<root><item>value1</item><item>value2</item></root>\",\n  query: \"//item/text()\",\n  mimeType: \"text/xml\"\n});\n```\n\n### Query HTML content\n\n```javascript\n// Get all links from HTML\nconst result = await callTool(\"xpath\", {\n  xml: \"<html><body><a href='link1.html'>Link 1</a><a href='link2.html'>Link 2</a></body></html>\",\n  query: \"//a/@href\",\n  mimeType: \"text/html\"\n});\n```\n\n### Query URL content\n\n```javascript\n// Get all links from a webpage\nconst result = await callTool(\"xpathwithurl\", {\n  url: \"https://example.com\",\n  query: \"//a/@href\",\n  mimeType: \"text/html\"\n});\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Start the server in development mode\nnpm start\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xpath",
        "xml",
        "thirdstrandstudio",
        "mcp xpath",
        "xpath queries",
        "execute xpath"
      ],
      "category": "document-processing"
    },
    "tizee--mcp-unix-manual": {
      "owner": "tizee",
      "name": "mcp-unix-manual",
      "url": "https://github.com/tizee/mcp-unix-manual",
      "imageUrl": "/freedevtools/mcp/pfp/tizee.webp",
      "description": "Retrieve Unix command documentation, including help pages and version information. List common commands and check command availability within conversations.",
      "stars": 1,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-05T23:34:38Z",
      "readme_content": "# Unix Manual Server (MCP)\n\nAn MCP server that provides Unix command documentation directly within Claude conversations.\n\n## Features\n\n- **Get command documentation**: Retrieve help pages, man pages, and usage information for Unix commands\n- **List common commands**: Discover available commands on your system, categorized by function\n- **Check command existence**: Verify if a specific command is available and get its version information\n\n## Installation\n\n### Prerequisites\n\n- Python 3.13+\n- [Claude Desktop](https://claude.ai/download) or any MCP-compatible client\n\n### Setup\n\n1. Clone this repository\n2. Install the package:\n\n```bash\npip install -e .\n# or\nuv install -e .\n```\n\n3. Install the server in Claude Desktop:\n\n```bash\nmcp install unix_manual_server.py\n# uv\nuv run mcp install unix_manual_server.py\n```\n\n## Usage\n\nOnce installed, you can use the server's tools directly in Claude:\n\n### Get command documentation\n\n```\nI need help with the grep command. Can you show me the documentation?\n```\n\n### List common commands\n\n```\nWhat Unix commands are available on my system?\n```\n\n### Check if a command exists\n\n```\nIs the awk command available on my system?\n```\n\n## Development\n\nTo test the server locally without installing it in Claude:\n\n```bash\nmcp dev unix_manual_server.py\n```\n\n## Security\n\nThe server takes precautions to prevent command injection by:\n- Validating command names against a regex pattern\n- Executing commands directly without using shell\n- Setting timeouts on all command executions\n- Only checking for documentation, never executing arbitrary commands\n\n## Logging\n\nLogs are saved to `unix-manual-server.log` in the same directory as the script, useful for debugging.\n\n- use `@modelcontextprotocol/inspector` with `npx` under the hood.\n\n```zsh\nuv run mcp dev unix_manual_server.py\n```\n\n```\nnpx @modelcontextprotocol/inspector uv run unix_manual_server.py\n```\n\n## License\n\nMIT\n\n---\n\n*Created with the MCP Python SDK. For more information about MCP, visit [modelcontextprotocol.io](https://modelcontextprotocol.io).*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "commands",
        "unix",
        "documentation",
        "command documentation",
        "unix manual",
        "mcp unix"
      ],
      "category": "document-processing"
    },
    "trafflux--pdf-reader-mcp": {
      "owner": "trafflux",
      "name": "pdf-reader-mcp",
      "url": "https://github.com/trafflux/pdf-reader-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/trafflux.webp",
      "description": "Extracts text from both local and online PDF files with robust error handling and standardized output. Supports various PDF formats and includes features for auto-detection of encoding and volume mounting.",
      "stars": 31,
      "forks": 8,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T09:42:50Z",
      "readme_content": "# PDF Reader MCP Server\n\nA Model Context Protocol (MCP) server that provides tools for reading and extracting text from PDF files, supporting both local files and URLs.\n\n## Author\n\nPhilip Van de Walker  \nEmail: philip.vandewalker@gmail.com  \nGitHub: https://github.com/trafflux\n\n## Features\n\n- Read text content from local PDF files\n- Read text content from PDF URLs\n- Error handling for corrupt or invalid PDFs\n- Volume mounting for accessing local PDFs\n- Auto-detection of PDF encoding\n- Standardized JSON output format\n\n## Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/trafflux/pdf-reader-mcp.git\ncd pdf-reader-mcp\n```\n\n2. Build the Docker image:\n\n```bash\ndocker build -t mcp/pdf-reader .\n```\n\n## Usage\n\n### Running the Server\n\nTo run the server with access to local PDF files:\n\n```bash\ndocker run -i --rm -v /path/to/pdfs:/pdfs mcp/pdf-reader\n```\n\nReplace `/path/to/pdfs` with the actual path to your PDF files directory.\n\nIf not using local PDF files:\n\n```bash\ndocker run -i --rm mcp/pdf-reader\n```\n\n### MCP Configuration\n\nAdd to your MCP settings configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pdf-reader\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/pdfs:/pdfs\",\n        \"mcp/pdf-reader\"\n      ],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nWithout local file PDF files:\n\n```json\n{\n  \"mcpServers\": {\n    \"pdf-reader\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"mcp/pdf-reader\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Available Tools\n\n1. `read_local_pdf`\n\n   - Purpose: Read text content from a local PDF file\n   - Input:\n     ```json\n     {\n       \"path\": \"/pdfs/document.pdf\"\n     }\n     ```\n   - Output:\n     ```json\n     {\n       \"success\": true,\n       \"data\": {\n         \"text\": \"Extracted content...\"\n       }\n     }\n     ```\n\n2. `read_pdf_url`\n   - Purpose: Read text content from a PDF URL\n   - Input:\n     ```json\n     {\n       \"url\": \"https://example.com/document.pdf\"\n     }\n     ```\n   - Output:\n     ```json\n     {\n       \"success\": true,\n       \"data\": {\n         \"text\": \"Extracted content...\"\n       }\n     }\n     ```\n\n## Error Handling\n\nThe server handles various error cases with clear error messages:\n\n- Invalid or corrupt PDF files\n- Missing files\n- Failed URL requests\n- Permission issues\n- Network connectivity problems\n\nError responses follow the format:\n\n```json\n{\n  \"success\": false,\n  \"error\": \"Detailed error message\"\n}\n```\n\n## Dependencies\n\n- Python 3.11+\n- PyPDF2: PDF parsing and text extraction\n- requests: HTTP client for fetching PDFs from URLs\n- MCP SDK: Model Context Protocol implementation\n\n## Project Structure\n\n```\n.\n├── Dockerfile          # Container configuration\n├── README.md          # This documentation\n├── requirements.txt   # Python dependencies\n└── src/\n    ├── __init__.py    # Package initialization\n    └── server.py      # Main server implementation\n```\n\n## License\n\nCopyright 2025 Philip Van de Walker\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Contact\n\nFor questions, issues, or contributions, please contact Philip Van de Walker:\n\n- Email: philip.vandewalker@gmail.com\n- GitHub: https://github.com/trafflux\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdf",
        "trafflux",
        "formats",
        "trafflux pdf",
        "pdf reader",
        "pdf formats"
      ],
      "category": "document-processing"
    },
    "truaxki--mcp-Pdf2png": {
      "owner": "truaxki",
      "name": "mcp-Pdf2png",
      "url": "https://github.com/truaxki/mcp-Pdf2png",
      "imageUrl": "/freedevtools/mcp/pfp/truaxki.webp",
      "description": "Convert PDF documents into high-quality PNG images seamlessly, transforming each page of a PDF into a PNG file using a simple MCP tool call. Enhance document processing with efficient image generation from PDFs.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-13T02:42:13Z",
      "readme_content": "# PDF to PNG MCP Server\n\nA Model Context Protocol (MCP) server that provides PDF to PNG conversion capabilities. This server allows you to convert PDF documents into PNG images with a simple MCP tool call.\n\n## Prerequisites\n\nThis server requires the Model Context Protocol (MCP). If you're new to MCP, start by installing the SDK:\n```bash\nuv pip install mcp\n```\n\nAdditional requirements:\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- poppler (required for pdf2image)\n\n### Installing Poppler\n\n- **Windows**: Download and install from [poppler-windows](https://github.com/oschwartz10612/poppler-windows/releases/)\n- **macOS**: `brew install poppler`\n- **Linux**: `sudo apt-get install poppler-utils`\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/truaxki/mcp-Pdf2png.git\n   cd mcp-Pdf2png\n   ```\n\n2. Create and activate a virtual environment:\n   ```bash\n   uv venv\n   # Windows\n   .venv\\Scripts\\activate\n   # Unix/macOS\n   source .venv/bin/activate\n   ```\n\n3. Install the package:\n   ```bash\n   uv pip install -e .\n   ```\n\n## Usage\n\n### 1. Configure MCP Client\n\nAdd the server configuration to your `claude_desktop_config.json`. The file is typically located in:\n- Windows: `%APPDATA%\\Claude Desktop\\config\\claude_desktop_config.json`\n- macOS/Linux: `~/.config/Claude Desktop/config/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"pdf2png\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/absolute/path/to/mcp-Pdf2png\",\n        \"run\",\n        \"pdf2png\"\n      ]\n    }\n  }\n}\n```\n\nNote: Replace `/absolute/path/to/mcp-Pdf2png` with the actual path where you cloned the repository.\n\n### 2. Using the Server\n\nThe server provides a single tool `pdf2png` with these parameters:\n- `read_file_path`: Absolute path to the input PDF file\n- `write_folder_path`: Absolute path to the directory where PNG files should be saved\n\nOutput:\n- Each PDF page is converted to a PNG image\n- Files are named `page_1.png`, `page_2.png`, etc.\n- Returns a success message with the conversion count\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdf2png",
        "pdfs",
        "png",
        "mcp pdf2png",
        "pdf png",
        "pdf2png convert"
      ],
      "category": "document-processing"
    },
    "tumf--mcp-text-editor": {
      "owner": "tumf",
      "name": "mcp-text-editor",
      "url": "https://github.com/tumf/mcp-text-editor",
      "imageUrl": "/freedevtools/mcp/pfp/tumf.webp",
      "description": "Provides line-oriented text file editing capabilities through a standardized API, optimized for efficient interaction with large language models, enabling partial file access to minimize token usage.",
      "stars": 161,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T01:16:39Z",
      "readme_content": "# MCP Text Editor Server\n\n[![codecov](https://codecov.io/gh/tumf/mcp-text-editor/branch/main/graph/badge.svg?token=52D51U0ZUR)](https://codecov.io/gh/tumf/mcp-text-editor)\n[![smithery badge](https://smithery.ai/badge/mcp-text-editor)](https://smithery.ai/server/mcp-text-editor)\n[![Glama MCP Server](https://glama.ai/mcp/servers/k44dnvso10/badge)](https://glama.ai/mcp/servers/k44dnvso10)\n\nA Model Context Protocol (MCP) server that provides line-oriented text file editing capabilities through a standardized API. Optimized for LLM tools with efficient partial file access to minimize token usage.\n\n## Quick Start for Claude.app Users\n\nTo use this editor with Claude.app, add the following configuration to your prompt:\n\n```shell\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"text-editor\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-text-editor\"\n      ]\n    }\n  }\n}\n```\n\n## Overview\n\nMCP Text Editor Server is designed to facilitate safe and efficient line-based text file operations in a client-server architecture. It implements the Model Context Protocol, ensuring reliable file editing with robust conflict detection and resolution. The line-oriented approach makes it ideal for applications requiring synchronized file access, such as collaborative editing tools, automated text processing systems, or any scenario where multiple processes need to modify text files safely. The partial file access capability is particularly valuable for LLM-based tools, as it helps reduce token consumption by loading only the necessary portions of files.\n\n### Key Benefits\n\n- Line-based editing operations\n- Token-efficient partial file access with line-range specifications\n- Optimized for LLM tool integration\n- Safe concurrent editing with hash-based validation\n- Atomic multi-file operations\n- Robust error handling with custom error types\n- Comprehensive encoding support (utf-8, shift_jis, latin1, etc.)\n\n## Features\n\n- Line-oriented text file editing and reading\n- Smart partial file access to minimize token usage in LLM applications\n- Get text file contents with line range specification\n- Read multiple ranges from multiple files in a single operation\n- Line-based patch application with correct handling of line number shifts\n- Edit text file contents with conflict detection\n- Flexible character encoding support (utf-8, shift_jis, latin1, etc.)\n- Support for multiple file operations\n- Proper handling of concurrent edits with hash-based validation\n- Memory-efficient processing of large files\n\n## Requirements\n\n- Python 3.11 or higher\n- POSIX-compliant operating system (Linux, macOS, etc.) or Windows\n- Sufficient disk space for text file operations\n- File system permissions for read/write operations\n\n1. Install Python 3.11+\n\n```bash\npyenv install 3.11.6\npyenv local 3.11.6\n```\n\n2. Install uv (recommended) or pip\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n3. Create virtual environment and install dependencies\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e \".[dev]\"\n```\n\n## Requirements\n\n- Python 3.13+\n- POSIX-compliant operating system (Linux, macOS, etc.) or Windows\n- File system permissions for read/write operations\n\n## Installation\n\n### Run via uvx\n\n```bash\nuvx mcp-text-editor\n```\n\n### Installing via Smithery\n\nTo install Text Editor Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-text-editor):\n\n```bash\nnpx -y @smithery/cli install mcp-text-editor --client claude\n```\n\n### Manual Installation\n\n1. Install Python 3.13+\n\n```bash\npyenv install 3.13.0\npyenv local 3.13.0\n```\n\n2. Install uv (recommended) or pip\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n3. Create virtual environment and install dependencies\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e \".[dev]\"\n```\n\n## Usage\n\nStart the server:\n\n```bash\npython -m mcp_text_editor\n```\n\n### MCP Tools\n\nThe server provides several tools for text file manipulation:\n\n#### get_text_file_contents\n\nGet the contents of one or more text files with line range specification.\n\n**Single Range Request:**\n\n```json\n{\n  \"file_path\": \"path/to/file.txt\",\n  \"line_start\": 1,\n  \"line_end\": 10,\n  \"encoding\": \"utf-8\"  // Optional, defaults to utf-8\n}\n```\n\n**Multiple Ranges Request:**\n\n```json\n{\n  \"files\": [\n    {\n      \"file_path\": \"file1.txt\",\n      \"ranges\": [\n        {\"start\": 1, \"end\": 10},\n        {\"start\": 20, \"end\": 30}\n      ],\n      \"encoding\": \"shift_jis\"  // Optional, defaults to utf-8\n    },\n    {\n      \"file_path\": \"file2.txt\",\n      \"ranges\": [\n        {\"start\": 5, \"end\": 15}\n      ]\n    }\n  ]\n}\n```\n\nParameters:\n- `file_path`: Path to the text file\n- `line_start`/`start`: Line number to start from (1-based)\n- `line_end`/`end`: Line number to end at (inclusive, null for end of file)\n- `encoding`: File encoding (default: \"utf-8\"). Specify the encoding of the text file (e.g., \"shift_jis\", \"latin1\")\n\n**Single Range Response:**\n\n```json\n{\n  \"contents\": \"File contents\",\n  \"line_start\": 1,\n  \"line_end\": 10,\n  \"hash\": \"sha256-hash-of-contents\",\n  \"file_lines\": 50,\n  \"file_size\": 1024\n}\n```\n\n**Multiple Ranges Response:**\n\n```json\n{\n  \"file1.txt\": [\n    {\n      \"content\": \"Lines 1-10 content\",\n      \"start\": 1,\n      \"end\": 10,\n      \"hash\": \"sha256-hash-1\",\n      \"total_lines\": 50,\n      \"content_size\": 512\n    },\n    {\n      \"content\": \"Lines 20-30 content\",\n      \"start\": 20,\n      \"end\": 30,\n      \"hash\": \"sha256-hash-2\",\n      \"total_lines\": 50,\n      \"content_size\": 512\n    }\n  ],\n  \"file2.txt\": [\n    {\n      \"content\": \"Lines 5-15 content\",\n      \"start\": 5,\n      \"end\": 15,\n      \"hash\": \"sha256-hash-3\",\n      \"total_lines\": 30,\n      \"content_size\": 256\n    }\n  ]\n}\n```\n\n#### patch_text_file_contents\n\nApply patches to text files with robust error handling and conflict detection. Supports editing multiple files in a single operation.\n\n**Request Format:**\n\n```json\n{\n  \"files\": [\n    {\n      \"file_path\": \"file1.txt\",\n      \"hash\": \"sha256-hash-from-get-contents\",\n      \"encoding\": \"utf-8\",  // Optional, defaults to utf-8\n      \"patches\": [\n        {\n          \"start\": 5,\n          \"end\": 8,\n          \"range_hash\": \"sha256-hash-of-content-being-replaced\",\n          \"contents\": \"New content for lines 5-8\\n\"\n        },\n        {\n          \"start\": 15,\n          \"end\": null,  // null means end of file\n          \"range_hash\": \"sha256-hash-of-content-being-replaced\",\n          \"contents\": \"Content to append\\n\"\n        }\n      ]\n    }\n  ]\n}\n```\n\nImportant Notes:\n1. Always get the current hash and range_hash using get_text_file_contents before editing\n2. Patches are applied from bottom to top to handle line number shifts correctly\n3. Patches must not overlap within the same file\n4. Line numbers are 1-based\n5. `end: null` can be used to append content to the end of file\n6. File encoding must match the encoding used in get_text_file_contents\n\n**Success Response:**\n\n```json\n{\n  \"file1.txt\": {\n    \"result\": \"ok\",\n    \"hash\": \"sha256-hash-of-new-contents\"\n  }\n}\n```\n\n**Error Response with Hints:**\n\n```json\n{\n  \"file1.txt\": {\n    \"result\": \"error\",\n    \"reason\": \"Content hash mismatch\",\n    \"suggestion\": \"get\",  // Suggests using get_text_file_contents\n    \"hint\": \"Please run get_text_file_contents first to get current content and hashes\"\n  }\n}\n```\n\n    \"result\": \"error\",\n    \"reason\": \"Content hash mismatch - file was modified\",\n    \"hash\": \"current-hash\",\n    \"content\": \"Current file content\"\n\n  }\n}\n\n```\n\n### Common Usage Pattern\n\n1. Get current content and hash:\n\n```python\ncontents = await get_text_file_contents({\n    \"files\": [\n        {\n            \"file_path\": \"file.txt\",\n            \"ranges\": [{\"start\": 1, \"end\": null}]  # Read entire file\n        }\n    ]\n})\n```\n\n2. Edit file content:\n\n```python\nresult = await edit_text_file_contents({\n    \"files\": [\n        {\n            \"path\": \"file.txt\",\n            \"hash\": contents[\"file.txt\"][0][\"hash\"],\n            \"encoding\": \"utf-8\",  # Optional, defaults to \"utf-8\"\n            \"patches\": [\n                {\n                    \"line_start\": 5,\n                    \"line_end\": 8,\n                    \"contents\": \"New content\\n\"\n                }\n            ]\n        }\n    ]\n})\n```\n\n3. Handle conflicts:\n\n```python\nif result[\"file.txt\"][\"result\"] == \"error\":\n    if \"hash mismatch\" in result[\"file.txt\"][\"reason\"]:\n        # File was modified by another process\n        # Get new content and retry\n        pass\n```\n\n### Error Handling\n\nThe server handles various error cases:\n- File not found\n- Permission errors\n- Hash mismatches (concurrent edit detection)\n- Invalid patch ranges\n- Overlapping patches\n- Encoding errors (when file cannot be decoded with specified encoding)\n- Line number out of bounds\n\n## Security Considerations\n\n- File Path Validation: The server validates all file paths to prevent directory traversal attacks\n- Access Control: Proper file system permissions should be set to restrict access to authorized directories\n- Hash Validation: All file modifications are validated using SHA-256 hashes to prevent race conditions\n- Input Sanitization: All user inputs are properly sanitized and validated\n- Error Handling: Sensitive information is not exposed in error messages\n\n## Troubleshooting\n\n### Common Issues\n\n1. Permission Denied\n   - Check file and directory permissions\n   - Ensure the server process has necessary read/write access\n\n2. Hash Mismatch and Range Hash Errors\n   - The file was modified by another process\n   - Content being replaced has changed\n   - Run get_text_file_contents to get fresh hashes\n\n3. Encoding Issues\n   - Verify file encoding matches the specified encoding\n   - Use utf-8 for new files\n   - Check for BOM markers in files\n\n4. Connection Issues\n   - Verify the server is running and accessible\n   - Check network configuration and firewall settings\n\n5. Performance Issues\n   - Consider using smaller line ranges for large files\n   - Monitor system resources (memory, disk space)\n   - Use appropriate encoding for file type\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Create and activate a Python virtual environment\n3. Install development dependencies: `uv pip install -e \".[dev]\"`\n4. Run tests: `make all`\n\n### Code Quality Tools\n\n- Ruff for linting\n- Black for code formatting\n- isort for import sorting\n- mypy for type checking\n- pytest-cov for test coverage\n\n### Testing\n\nTests are located in the `tests` directory and can be run with pytest:\n\n```bash\n# Run all tests\npytest\n\n# Run tests with coverage report\npytest --cov=mcp_text_editor --cov-report=term-missing\n\n# Run specific test file\npytest tests/test_text_editor.py -v\n```\n\nCurrent test coverage: 90%\n\n### Project Structure\n\n```\nmcp-text-editor/\n├── mcp_text_editor/\n│   ├── __init__.py\n│   ├── __main__.py      # Entry point\n│   ├── models.py        # Data models\n│   ├── server.py        # MCP Server implementation\n│   ├── service.py       # Core service logic\n│   └── text_editor.py   # Text editor functionality\n├── tests/               # Test files\n└── pyproject.toml       # Project configuration\n```\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Run tests and code quality checks\n5. Submit a pull request\n\n### Type Hints\n\nThis project uses Python type hints throughout the codebase. Please ensure any contributions maintain this.\n\n### Error Handling\n\nAll error cases should be handled appropriately and return meaningful error messages. The server should never crash due to invalid input or file operations.\n\n### Testing\n\nNew features should include appropriate tests. Try to maintain or improve the current test coverage.\n\n### Code Style\n\nAll code should be formatted with Black and pass Ruff linting. Import sorting should be handled by isort.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "editor",
        "editing",
        "tumf",
        "file editing",
        "text editor",
        "editor provides"
      ],
      "category": "document-processing"
    },
    "u3588064--Entity-Resolution": {
      "owner": "u3588064",
      "name": "Entity-Resolution",
      "url": "https://github.com/u3588064/Entity-Resolution",
      "imageUrl": "/freedevtools/mcp/pfp/u3588064.webp",
      "description": "Compares two sets of data to determine if they originate from the same entity using text normalization and semantic analysis. It evaluates both exact and semantic equality of values, ensuring accurate data validation.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-03T13:53:49Z",
      "readme_content": "# EntityIdentification\nIdentify whether two sets of data are from the same entity. 识别两组数据是否来自同一主体\n\nThis is a MCP (Model Context Protocol) server. 这是一个支持MCP协议的服务器。\n\n\n# Data Comparison Tool\n\nThis tool provides a comprehensive way to compare two sets of data, evaluating both exact and semantic equality of their values. It leverages text normalization and a language model to determine if the data originates from the same entity.\n\n## Features\n\n- **Text Normalization**: Converts text to lowercase, removes punctuation, and normalizes whitespace.\n- **Value Comparison**: Compares values directly and semantically (ignoring order for lists).\n- **JSON Traversal**: Iterates through each key in the JSON objects and compares corresponding values.\n- **Language Model Integration**: Uses a generative language model to assess semantic similarity and provide a final judgment on whether the data comes from the same entity.\n\n## Installation\n\nTo use this tool, ensure you have the necessary dependencies installed. You can install them using pip:\n\n```bash\npip install genai\n```\n\n## Usage\n\n### Functions\n\n1. **normalize_text(text)**:\n   - Normalizes the input text by converting it to lowercase, removing punctuation, and normalizing whitespace.\n\n2. **compare_values(val1, val2)**:\n   - Compares two values both exactly and semantically.\n   - If the values are lists, it ignores the order of elements for semantic comparison.\n\n3. **compare_json(json1, json2)**:\n   - Compares two JSON objects key by key.\n   - Uses `compare_values` to evaluate each key's values.\n   - Integrates a language model to assess semantic similarity and provides a final judgment.\n\n### Example\n\n```python\nimport json\nimport genai\nimport re\n\n# Define your JSON objects\njson1 = {\n    \"name\": \"John Doe\",\n    \"address\": \"123 Main St, Anytown, USA\",\n    \"hobbies\": [\"reading\", \"hiking\", \"coding\"]\n}\n\njson2 = {\n    \"name\": \"john doe\",\n    \"address\": \"123 Main Street, Anytown, USA\",\n    \"hobbies\": [\"coding\", \"hiking\", \"reading\"]\n}\n\n# Compare the JSON objects\ncomparison_results = compare_json(json1, json2)\n\n# Generate final matching result\nmodel1 = genai.GenerativeModel(\"gemini-2.0-flash-thinking-exp\")\nresult_matching = model1.generate_content(\"综合这些信息，你认为可以判断两个数据来自同一主体吗？\"+json.dumps(comparison_results, ensure_ascii=False, indent=4))\nprint(result_matching.text)\n```\n\n## Contributing\n\nContributions are welcome! Please open an issue or submit a pull request.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n## Contact\n\nIf you have any questions or suggestions, please contact me:\n- Email: u3588064@connect.hku.hk\n- GitHub: [u3588064@connect.hku.hk](mailto:u3588064@connect.hku.hk)。\n\nWechat\n![qrcode_for_gh_643efb7db5bc_344(1)](https://github.com/u3588064/LLMemory/assets/53069671/8bb26c0f-4cab-438b-9f8c-16b1c26b3587)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "entity",
        "semantic",
        "normalization",
        "entity resolution",
        "normalization semantic",
        "document processing"
      ],
      "category": "document-processing"
    },
    "ucalyptus--prem-mcp-server": {
      "owner": "ucalyptus",
      "name": "prem-mcp-server",
      "url": "https://github.com/ucalyptus/prem-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ucalyptus.webp",
      "description": "Integrates with Prem AI's features for chat interactions and document management, supporting Retrieval-Augmented Generation with document repositories and real-time streaming responses.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-26T02:24:56Z",
      "readme_content": "# Prem MCP Server\n[![smithery badge](https://smithery.ai/badge/@ucalyptus/prem-mcp-server)](https://smithery.ai/server/@ucalyptus/prem-mcp-server)\n\nA Model Context Protocol (MCP) server implementation for [Prem AI](https://premai.io/), enabling seamless integration with Claude and other MCP-compatible clients. This server provides access to Prem AI's powerful features through the MCP interface.\n\n## Features\n\n- 🤖 **Chat Completions**: Interact with Prem AI's language models\n- 📚 **RAG Support**: Retrieval-Augmented Generation with document repository integration\n- 📝 **Document Management**: Upload and manage documents in repositories\n- 🎭 **Template System**: Use predefined prompt templates for specialized outputs\n- ⚡ **Streaming Responses**: Real-time streaming of model outputs\n- 🛡️ **Error Handling**: Robust error handling and logging\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- A Prem AI account with API key\n- A Prem project ID\n\n## Installation\n\n### Installing via Smithery\n\nTo install prem-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ucalyptus/prem-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @ucalyptus/prem-mcp-server --client claude\n```\n\n### Installing Manually\n```bash\n# Using npm\nnpm install prem-mcp-server\n\n# Using yarn\nyarn add prem-mcp-server\n\n# Using pnpm\npnpm add prem-mcp-server\n```\n\n## Configuration\n\n### 1. Environment Variables\nCreate a `.env` file in your project root:\n```env\nPREM_API_KEY=your_api_key_here\nPREM_PROJECT_ID=your_project_id_here\n```\n\n### 2. Cursor Configuration\nTo use the Prem MCP server with Cursor, add the following to your `~/.cursor/mcp.json`:\n```json\n{\n  \"mcpServers\": {\n    \"PremAI\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/your/prem-mcp/build/index.js\", \"--stdio\"],\n      \"env\": {\n        \"PREM_API_KEY\": \"your_api_key_here\",\n        \"PREM_PROJECT_ID\": \"your_project_id_here\"\n      }\n    }\n  }\n}\n```\nReplace `/path/to/your/prem-mcp` with the actual path to your project directory.\n\n### 3. Claude Desktop Configuration\nFor Claude Desktop users, add the following to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"PremAI\": {\n      \"command\": \"npx\",\n      \"args\": [\"prem-mcp-server\", \"--stdio\"],\n      \"env\": {\n        \"PREM_API_KEY\": \"your_api_key_here\",\n        \"PREM_PROJECT_ID\": \"your_project_id_here\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpx prem-mcp-server\n```\n\n### Example Prompts\n\n1. **Basic Chat**\n```\nLet's have a conversation about artificial intelligence.\n```\n\n2. **RAG with Documents**\n```\nBased on the documents in repository XYZ, what are the key points about [topic]?\n```\n\n3. **Using Templates**\n```\nUse template ABC to generate [specific type of content].\n```\n\n### Document Upload\n\nThe server supports uploading documents to Prem AI repositories for RAG operations. Supported formats:\n- `.txt`\n- `.pdf`\n- `.docx`\n\n## API Reference\n\n### Chat Completion Parameters\n\n- `query`: The input text\n- `system_prompt`: Custom system prompt\n- `model`: Model identifier\n- `temperature`: Response randomness (0-1)\n- `max_tokens`: Maximum response length\n- `repository_ids`: Array of repository IDs for RAG\n- `similarity_threshold`: Threshold for document similarity\n- `limit`: Maximum number of document chunks\n\n### Template Parameters\n\n- `template_id`: ID of the prompt template\n- `params`: Template-specific parameters\n- `temperature`: Response randomness (0-1)\n- `max_tokens`: Maximum response length\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/prem-mcp-server.git\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run tests\nnpm test\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Server Not Found**\n   - Verify the server path in `claude_desktop_config.json`\n   - Check if the server is running\n\n2. **API Key Invalid**\n   - Ensure your Prem AI API key is valid\n   - Check if the API key has the required permissions\n\n3. **Document Upload Failed**\n   - Verify file format is supported\n   - Check file permissions\n   - Ensure repository ID is correct\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Prem AI](https://prem.ai/) for their powerful AI platform\n- [Model Context Protocol](https://github.com/anthropics/anthropic-tools/tree/main/model-context-protocol) for the protocol specification\n- [Anthropic](https://www.anthropic.com/) for Claude and the MCP ecosystem\n\n## Support\n\nFor issues and feature requests, please use the GitHub Issues page.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "document",
        "chat",
        "ai",
        "features chat",
        "document processing",
        "document management"
      ],
      "category": "document-processing"
    },
    "umuthopeyildirim--markai": {
      "owner": "umuthopeyildirim",
      "name": "markai",
      "url": "https://github.com/umuthopeyildirim/markai",
      "imageUrl": "/freedevtools/mcp/pfp/umuthopeyildirim.webp",
      "description": "MarkAI is a platform that enables users to ask questions and receive answers derived from their documents, providing efficient data access. It supports various file formats and offers both public and private collaboration options.",
      "stars": 22,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-03-27T03:57:16Z",
      "readme_content": "# MarkAI - Another RAG template\n\n![License](https://img.shields.io/github/license/umuthopeyildirim/markai)\n\nMarkAI is a AI-platform that guides you on your employees to be more efficient. It is a platform that you can ask questions to your AI and get answers from your documents. We designed it to be serverless and scalable. It is a platform that you can use for your company or personal use.\n\n## Key Features 🎯\n\n-   **Fast and Efficient**: Designed with speed and efficiency at its core. MarkAI ensures rapid access to your data.\n-   **Secure**: Your data, your control. Always.\n-   **File Compatibility**: Text, Markdown, HTML\n-   **Open Source**: Freedom is beautiful, and so is MarkAI. Open source and free to use.\n-   **Public/Private**: Invite your team to collaborate, or keep your data private.\n\n![MarkAI](./docs/images/Vector.png)\n\n## Setup 🛠\n\nJust follow the instructions on the [setup.md](docs/setup.md).\n\n## Used Technologies\n\n-   [Clerk](https://clerk.com) for authentication\n-   [Supabase](https://supabase.com) for database and vector search\n-   [LangChain](https://www.langchain.com/) for Agents and Embeddings\n-   [Cloudflare](https://cloudflare.com) (Optional) for custom domain and AI Gateway(Monitoring)\n-   [Vercel](https://vercel.com) for deployment, CI/CD and serverless functions\n-   [OpenAI](https://openai.com) for AI API calls\n-   [TailwindCSS](https://tailwindcss.com) for styling\n-   [Next.js](https://nextjs.org) for frontend\n-   [NextUI](https://nextui.org) for UI components\n\n## Contributing\n\nIf you encounter a bug or have a feature request, please open an issue. If you want to contribute code, fork this repository and make a pull request.\n\n## Contributors ✨\n\nThanks go to these wonderful people:\n\n<a href=\"https://github.com/umuthopeyildirim/markai/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=umuthopeyildirim/markai\" />\n</a>\n\n## Stars History 📈\n\n<a href=\"https://star-history.com/#umuthopeyildirim/markai&Timeline\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=umuthopeyildirim/markai&type=Timeline&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=umuthopeyildirim/markai&type=Timeline\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=umuthopeyildirim/markai&type=Timeline\" />\n  </picture>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markai",
        "document",
        "documents",
        "markai platform",
        "markai markai",
        "umuthopeyildirim markai"
      ],
      "category": "document-processing"
    },
    "vgnshiyer--apple-books-mcp": {
      "owner": "vgnshiyer",
      "name": "apple-books-mcp",
      "url": "https://github.com/vgnshiyer/apple-books-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vgnshiyer.webp",
      "description": "Manage and explore your Apple Books library, summarize highlights, and receive book recommendations by harnessing Claude's capabilities.",
      "stars": 32,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-25T20:39:59Z",
      "readme_content": "# Apple Books MCP\n\nModel Context Protocol (MCP) server for Apple Books.\n\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n[![PyPI](https://img.shields.io/pypi/v/apple-books-mcp.svg)](https://pypi.org/project/apple-books-mcp/)\n[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![](https://img.shields.io/badge/Follow-vgnshiyer-0A66C2?logo=linkedin)](https://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=vgnshiyer)\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Donate-yellow.svg?logo=buymeacoffee)](https://www.buymeacoffee.com/vgnshiyer)\n\n## At a glance\n\n* Ask Claude to summarize your recent highlights\n* Ask Claude to organize books in your library by genre\n* Ask Claude to recommend similar books based on your reading history\n* Ask Claude to compare notes from different books read on the same subject\n\nhttps://github.com/user-attachments/assets/77a5a29b-bfd7-4275-a4af-8d6c51a4527e\n\nAnd much more!\n\n## Available Tools\n\n| Tool | Description | Parameters |\n|----------|-------------|------------|\n| list_collections() | List all collections | None |\n| get_collection_books(collection_id) | Get all books in a collection | collection_id: str |\n| describe_collection(collection_id) | Get details of a collection | collection_id: str |\n| list_all_books() | List all books | None |\n| get_book_annotations(book_id) | Get all annotations for a book | book_id: str |\n| describe_book(book_id) | Get details of a particular book | book_id: str |\n| list_all_annotations() | List all annotations | None |\n| get_highlights_by_color(color) | Get all highlights by color | color: str |\n| search_highlighted_text(text) | Search for highlights by highlighted text | text: str |\n| search_notes(note) | Search for notes | note: str |\n| full_text_search(text) | Search for annotations containing the given text | text: str |\n| recent_annotations() | Get 10 most recent annotations | None |\n| describe_annotation(annotation_id) | Get details of an annotation | annotation_id: str |\n\n## Installation\n\n### Using uv (recommended)\n\n[uvx](https://docs.astral.sh/uv/guides/tools/) can be used to directly run apple-books-mcp (without installing it).\n\n```bash\nbrew install uv  # for macos\nuvx apple-books-mcp\n```\n\n### Using pip\n\n```bash\npip install apple-books-mcp\n```\n\nAfter installing, you can run the server using:\n\n```bash\npython -m apple_books_mcp\n```\n\n## Configuration\n\n### Claude Desktop Setup\n\n#### Using uvx (recommended)\n\n```json\n{\n    \"mcpServers\": {\n        \"apple-books-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [ \"apple-books-mcp@latest\" ]\n        }\n    }\n}\n```\n\n#### Using python\n\n```json\n{\n    \"mcpServers\": {\n        \"apple-books-mcp\": {\n            \"command\": \"python\",\n            \"args\": [\"-m\", \"apple_books_mcp\"]\n        }\n    }\n}\n```\n\n## Upcoming Features\n\n- [ ] add docker support\n- [ ] add resources support\n- [ ] edit collections support\n- [ ] edit highlights support\n\n## Contribution\n\nThank you for considering contributing to this project!\n\n### Development\n\nIf you cloned this repository, you can test it using Claude Desktop with below configuration:\n\nUse `uv venv` to create a virtual environment and install the dependencies.\n\n```bash\nuv venv\nuv sync\n```\n\n#### Debugging\n\n**With Claude Desktop**\n\n```json\n{\n    \"mcpServers\": {\n        \"apple-books-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/apple-books-mcp/\",\n                \"run\",\n                \"apple_books_mcp\",\n                \"-v\"\n            ]\n        }\n    }\n}\n```\n\n**With inspector**\n\n```bash\nnpx @modelcontextprotocol/inspector uvx apple-books-mcp\n```\n\n### Opening Issues\nIf you encounter a bug, have a feature request, or want to discuss something related to the project, please open an issue on the GitHub repository. When opening an issue, please provide:\n\n**Bug Reports**: Describe the issue in detail. Include steps to reproduce the bug if possible, along with any error messages or screenshots.\n\n**Feature Requests**: Clearly explain the new feature you'd like to see added to the project. Provide context on why this feature would be beneficial.\n\n**General Discussions**: Feel free to start discussions on broader topics related to the project.\n\n### Contributing\n\n1️⃣ Fork the GitHub repository https://github.com/vgnshiyer/apple-books-mcp \\\n2️⃣ Create a new branch for your changes (git checkout -b feature/my-new-feature). \\\n3️⃣ Make your changes and test them thoroughly. \\\n4️⃣ Push your changes and open a Pull Request to `main`.\n\n*Please provide a clear title and description of your changes.*\n\n## License\n\nApple Books MCP is licensed under the Apache 2.0 license. See the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "books",
        "mcp",
        "library",
        "apple books",
        "books mcp",
        "vgnshiyer apple"
      ],
      "category": "document-processing"
    },
    "visheshd--docmcp": {
      "owner": "visheshd",
      "name": "docmcp",
      "url": "https://github.com/visheshd/docmcp",
      "imageUrl": "/freedevtools/mcp/pfp/visheshd.webp",
      "description": "Index and query technical documentation using AI-powered semantic search. It crawls, processes, and embeds documentation for efficient retrieval through AI IDEs with built-in MCP tools for seamless integration.",
      "stars": 6,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-24T08:17:50Z",
      "readme_content": "# DocMCP: Index the latest doc for LLMs on PostgreSQL using pgvector and expose to AI IDEs\n\nA system for crawling, processing, and querying documentation with AI-powered embedding generation and semantic search capabilities.\n\n## Features\n\n- **Documentation Crawling**: Automatically crawl documentation sites with customizable depth and rate limiting\n- **Content Processing**: Convert HTML to clean Markdown with metadata extraction\n- **Vector Embeddings**: Generate embeddings using AWS Bedrock for semantic searching\n- **Job Management**: Track and manage document processing jobs with detailed progress reporting\n- **MCP Integration**: Built-in MCP tools for AI agent integration\n\n## Roadmap\n- SPA support: currently the crawler doesn't support SPAs\n- Caching: crawled urls are directly added to the DB\n\n## Getting Started (Development Setup)\n\n### Prerequisites\n\n- Docker ([Install Guide](https://docs.docker.com/engine/install/))\n- Docker Compose ([Install Guide](https://docs.docker.com/compose/install/))\n- Node.js 16+\n- Git\n- AWS Account with Bedrock access\n- AWS CLI configured with appropriate credentials\n\n### Quick Start Steps\n\n1.  **Clone the Repository:**\n    ```bash\n    git clone https://github.com/visheshd/docmcp.git\n    cd docmcp\n    ```\n\n2.  **Configure Environment:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   **Edit the `.env` file:**\n        *   Set `DATABASE_URL` to `postgresql://postgres:postgres@localhost:5433/docmcp`\n        *   **Configure AWS Bedrock:**\n            *   Set `AWS_REGION` to your AWS region (e.g., `us-east-1`)\n            *   Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` with your AWS credentials\n            *   Or ensure your AWS CLI is configured with appropriate credentials\n        *   Adjust other settings like `LOG_LEVEL` if needed\n\n3.  **Start the Development Environment:**\n    ```bash\n    # Make the script executable\n    chmod +x dev-start.sh\n    \n    # Start the development environment\n    ./dev-start.sh\n    ```\n    This script will:\n    * Start PostgreSQL with pgvector in a Docker container\n    * Install project dependencies\n    * Run database migrations\n    * Import seed data automatically\n    * The database will be accessible on port 5433\n\n4.  **Add Documentation:**\n    Use the `add-docs` script to crawl and process documentation:\n    ```bash\n    # Basic usage\n    npm run add-docs -- --url https://example.com/docs --max-depth 3\n\n    # With additional options\n    npm run add-docs -- \\\n      --url https://example.com/docs \\\n      --max-depth 3 \\\n      --tags react,frontend \\\n      --package react \\\n      --version 18.0.0 \\\n      --wait\n    ```\n\n    Available options:\n    - `--url`: Documentation URL to crawl (required)\n    - `--max-depth`: Maximum crawl depth (default: 3)\n    - `--tags`: Comma-separated tags for categorization\n    - `--package`: Package name this documentation is for\n    - `--version`: Package version (defaults to \"latest\")\n    - `--wait`: Wait for processing to complete\n    - `--verbose`: Enable detailed logging\n    - See `npm run add-docs -- --help` for all options\n\n5.  **Query Documentation:**\n    Once documentation is added, you can query it using the MCP tools. See the \"Querying Documentation\" section below.\n\n6.  **Stop the Development Environment:**\n    ```bash\n    docker-compose -f docker-compose.dev.yml down\n    ```\n\nThis setup provides a lightweight development environment with just the required PostgreSQL database and pre-loaded seed data. For production deployments or if you prefer a fully containerized setup, see the \"Production Docker Setup\" section below.\n\n## Cursor Setup\n\nTo use DocMCP with Cursor IDE, you'll need to configure the MCP transport. Add the following configuration to your Cursor settings:\n\n```json\n{\n    \"docmcp-local-stdio\": {\n      \"transport\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\n        \"<DOCMCP_DIR>/dist/stdio-server.js\"\n      ],\n      \"clientInfo\": {\n        \"name\": \"cursor-client\",\n        \"version\": \"1.0.0\"\n      }\n    }\n}\n```\n\nReplace `<DOCMCP_DIR>` with the absolute path to your DocMCP installation directory.\n\nFor example, if DocMCP is installed in `/home/user/projects/docmcp`, your configuration would be:\n```json\n\"args\": [\"/home/user/projects/docmcp/dist/stdio-server.js\"]\n```\n\nAfter adding this configuration, restart Cursor for the changes to take effect.\n\n## Architecture\n\nThe system consists of several core services:\n\n- **CrawlerService**: Handles documentation site crawling with robots.txt support\n- **DocumentProcessorService**: Processes documents (HTML→Markdown, chunking, embedding)\n- **JobService**: Manages asynchronous processing jobs with detailed status tracking\n- **ChunkService**: Stores and retrieves document chunks with vector search capabilities\n- **MCP Tools**: Agent-friendly interface for adding and querying documentation\n\n### Document Processing Pipeline\n\nThe DocMCP system processes documentation through the following pipeline:\n\n1. **Documentation Input**\n   - User provides a URL through the `add_documentation` MCP tool\n   - System creates a Job record with \"pending\" status\n   - Job is assigned tags for categorization and future filtering\n\n2. **Web Crawling** (CrawlerService)\n   - Crawler respects robots.txt restrictions\n   - Follows links up to specified maximum depth\n   - Captures HTML content and metadata\n   - Creates Document records linked to the parent Job\n\n3. **Document Processing** (DocumentProcessorService)\n   - Cleans HTML and converts to structured Markdown\n   - Extracts metadata (package info, version, document type)\n   - Establishes parent-child relationships between documents\n   - Updates Job progress as processing continues\n\n4. **Chunking & Embedding** (ChunkService)\n   - Splits documents into semantic chunks for better retrieval\n   - Generates vector embeddings using AWS Bedrock\n   - Stores embeddings in PostgreSQL with pgvector extension\n   - Preserves chunk metadata and document references\n\n5. **Job Finalization** (JobService)\n   - Updates Job status to \"completed\"\n   - Calculates and stores document statistics\n   - Makes documents available for querying\n\n6. **Querying & Retrieval**\n   - User sends query through `query_documentation` MCP tool\n   - System converts query to vector embedding\n   - Performs similarity search to find relevant chunks\n   - Returns formatted results with source information\n   - Supports filtering by tags, status, and metadata\n\nThis pipeline enables efficient storage, processing, and retrieval of documentation with semantic understanding capabilities. All steps are tracked through the job system, allowing detailed progress monitoring and error handling.\n\n## Project Structure\n\n```\ndocmcp/\n├── prisma/                  # Database schema and migrations\n│   └── schema.prisma        # Prisma model definitions and database configuration\n├── src/\n│   ├── config/              # Application configuration\n│   │   └── database.ts      # Database connection setup\n│   ├── generated/           # Generated code (Prisma client)\n│   ├── services/            # Core service modules\n│   │   ├── crawler.service.ts     # Website crawling functionality\n│   │   ├── document.service.ts    # Document management\n│   │   ├── document-processor.service.ts # Document processing and transformation\n│   │   ├── job.service.ts         # Async job management\n│   │   ├── chunk.service.ts       # Document chunking and vector operations\n│   │   └── mcp-tools/       # MCP integration tools\n│   │       ├── add-documentation.tool.ts    # Tool for adding new documentation\n│   │       ├── get-job-status.tool.ts       # Tool for checking job status\n│   │       ├── list-documentation.tool.ts   # Tool for listing available documentation\n│   │       ├── query-documentation.tool.ts  # Tool for querying documentation\n│   │       ├── sample.tool.ts               # Example tool implementation\n│   │       └── index.ts                     # Tool registry and exports\n│   ├── types/               # TypeScript type definitions\n│   │   └── mcp.ts           # MCP tool interface definitions\n│   ├── utils/               # Utility functions\n│   │   ├── logger.ts        # Logging utilities\n│   │   └── prisma-filters.ts # Reusable Prisma filtering patterns\n│   └── __tests__/           # Test files\n│       └── utils/           # Test utilities\n│           └── testDb.ts    # Test database setup and teardown\n├── .env                     # Environment variables\n└── package.json             # Project dependencies and scripts\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "docmcp",
        "documentation",
        "search",
        "docmcp index",
        "semantic search",
        "visheshd docmcp"
      ],
      "category": "document-processing"
    },
    "w-jeon--mcp-framework": {
      "owner": "w-jeon",
      "name": "mcp-framework",
      "url": "https://github.com/w-jeon/mcp-framework",
      "imageUrl": "/freedevtools/mcp/pfp/w-jeon.webp",
      "description": "This framework enables the creation of custom tools for interaction with large language models, facilitating web content retrieval and various file handling capabilities. It automates the processing of PDF, Word, and Excel documents for enhanced productivity.",
      "stars": 3,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-12T10:49:14Z",
      "readme_content": "# MCP开发框架\n[![smithery badge](https://smithery.ai/badge/@aigo666/mcp-framework)](https://smithery.ai/server/@aigo666/mcp-framework)\n\n一个强大的MCP（Model Context Protocol）开发框架，用于创建与大语言模型交互的自定义工具。该框架提供了一套完整的工具集，可以轻松地扩展Cursor IDE的功能，实现网页内容获取、文件处理（PDF、Word、Excel）等高级功能。\n\n## 主要功能\n\n本框架提供了以下核心功能：\n\n### 1. 综合文件处理\n\n使用`file`工具可以自动识别文件类型并选择合适的处理方式，支持PDF、Word和Excel文件。\n\n- **用法**: `file /path/to/document`\n- **支持格式**: \n  - PDF文件 (.pdf)\n  - Word文档 (.doc, .docx)\n  - Excel文件 (.xls, .xlsx, .xlsm)\n- **参数**: `file_path` - 文件的本地路径\n- **返回**: 根据文件类型返回相应的处理结果\n\n### 2. PDF文档处理\n\n使用`pdf`工具可以处理PDF文档，支持两种处理模式：\n\n- **用法**: `pdf /path/to/document.pdf [mode]`\n- **参数**: \n  - `file_path` - PDF文件的本地路径\n  - `mode` - 处理模式（可选）：\n    - `quick` - 快速预览模式，仅提取文本内容\n    - `full` - 完整解析模式，提取文本和图片内容（默认）\n- **返回**: \n  - 快速预览模式：文档的文本内容\n  - 完整解析模式：文档的文本内容和图片\n- **特点**: \n  - 使用PyMuPDF提供高质量的文本提取和图像处理\n  - 自动处理大型文件\n  - 支持图片提取和保存\n\n### 3. Word文档解析\n\n使用`word`工具可以解析Word文档，提取文本、表格和图片信息。\n\n- **用法**: `word /path/to/document.docx`\n- **功能**: 解析Word文档并提取文本内容、表格和图片信息\n- **参数**: `file_path` - Word文档的本地路径\n- **返回**: 文档的文本内容、表格和图片信息\n- **特点**: 使用python-docx库提供高质量的文本和表格提取\n\n### 4. Excel文件处理\n\n使用`excel`工具可以解析Excel文件，提供完整的表格数据和结构信息。\n\n- **用法**: `excel /path/to/spreadsheet.xlsx`\n- **功能**: 解析Excel文件的所有工作表\n- **参数**: `file_path` - Excel文件的本地路径\n- **返回**: \n  - 文件基本信息（文件名、工作表数量）\n  - 每个工作表的详细信息：\n    - 行数和列数\n    - 列名列表\n    - 完整的表格数据\n- **特点**: \n  - 使用pandas和openpyxl提供高质量的表格数据处理\n  - 支持多工作表处理\n  - 自动处理数据类型转换\n\n### 5. 网页内容获取\n\n使用`url`工具可以获取任何网页的内容。\n\n- **用法**: `url https://example.com`\n- **参数**: `url` - 要获取内容的网站URL\n- **返回**: 网页的文本内容\n- **特点**: \n  - 完整的HTTP错误处理\n  - 超时管理\n  - 自动编码处理\n\n## 技术特点\n\n本框架采用了多种技术来优化文件处理性能：\n\n1. **智能文件类型识别**\n   - 自动根据文件扩展名选择合适的处理工具\n   - 提供统一的文件处理接口\n\n2. **高效的文档处理**\n   - PDF处理：支持快速预览和完整解析两种模式\n   - Word处理：精确提取文本、表格和图片\n   - Excel处理：高效处理大型表格数据\n\n3. **内存优化**\n   - 使用临时文件管理大型文件\n   - 自动清理临时资源\n   - 分块处理大型文档\n\n4. **错误处理**\n   - 完整的异常捕获和处理\n   - 详细的错误信息反馈\n   - 优雅的失败处理机制\n\n## 文档处理技术细节\n\n### PDF处理\n\n1. **多层次处理策略**:\n   - 首先尝试使用PyMuPDF（fitz）提取内容（速度快、准确度高）\n   - 如果失败，回退到PymuPDF4llm（专为大语言模型优化）\n   - 最后尝试PyPDF2作为最终备用方案\n\n2. **性能优化**:\n   - 限制处理的最大页数（完整模式: 30页，快速模式: 50页）\n   - 图片处理优化（DPI调整、大小限制）\n   - 多线程处理加速\n\n3. **错误处理**:\n   - 详细的错误信息和提示\n   - 备用处理方法，确保服务稳定性\n   - 超时保护机制（5分钟超时设置）\n\n### Word文档处理\n\n1. **文档结构解析**:\n   - 提取文档属性（标题、作者、创建时间等）\n   - 段落内容提取，保留原始格式\n   - 表格转换为Markdown格式\n\n2. **图片信息**:\n   - 提供文档中图片的数量信息\n   - 图片引用关系识别\n\n## 项目结构\n\n本框架采用模块化设计，便于扩展和维护：\n\n```\nmcp_tool/\n├── tools/\n│   ├── __init__.py        # 定义工具基类和注册器\n│   ├── loader.py          # 工具加载器，自动加载所有工具\n│   ├── file_tool.py       # 综合文件处理工具\n│   ├── pdf_tool.py        # PDF解析工具\n│   ├── word_tool.py       # Word文档解析工具\n│   ├── excel_tool.py      # Excel文件处理工具\n│   └── url_tool.py        # URL工具实现\n├── __init__.py\n├── __main__.py\n└── server.py              # MCP服务器实现\n```\n\n## 开发指南\n\n### 如何开发新工具\n\n1. 在`tools`目录下创建一个新的Python文件，如`your_tool.py`\n2. 导入必要的依赖和基类\n3. 创建一个继承自`BaseTool`的工具类\n4. 使用`@ToolRegistry.register`装饰器注册工具\n5. 实现工具的`execute`方法\n\n### 工具模板示例\n\n```python\nimport mcp.types as types\nfrom . import BaseTool, ToolRegistry\n\n@ToolRegistry.register\nclass YourTool(BaseTool):\n    \"\"\"您的工具描述\"\"\"\n    name = \"your_tool_name\"  # 工具的唯一标识符\n    description = \"您的工具描述\"  # 工具的描述信息，将显示给用户\n    input_schema = {\n        \"type\": \"object\",\n        \"required\": [\"param1\"],  # 必需的参数\n        \"properties\": {\n            \"param1\": {\n                \"type\": \"string\",\n                \"description\": \"参数1的描述\",\n            },\n            \"param2\": {\n                \"type\": \"integer\",\n                \"description\": \"参数2的描述（可选）\",\n            }\n        },\n    }\n  \n    async def execute(self, arguments: dict) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n        \"\"\"执行工具逻辑\"\"\"\n        # 参数验证\n        if \"param1\" not in arguments:\n            return [types.TextContent(\n                type=\"text\",\n                text=\"Error: Missing required argument 'param1'\"\n            )]\n          \n        # 获取参数\n        param1 = arguments[\"param1\"]\n        param2 = arguments.get(\"param2\", 0)  # 获取可选参数，提供默认值\n      \n        # 执行工具逻辑\n        result = f\"处理参数: {param1}, {param2}\"\n      \n        # 返回结果\n        return [types.TextContent(\n            type=\"text\",\n            text=result\n        )]\n```\n\n## 部署指南\n\n### Docker部署（推荐）\n\n1. 初始设置：\n```bash\n# 克隆仓库\ngit clone https://github.com/your-username/mcp-framework.git\ncd mcp-framework\n\n# 创建环境文件\ncp .env.example .env\n```\n\n2. 使用Docker Compose：\n```bash\n# 构建并启动\ndocker compose up --build -d\n\n# 查看日志\ndocker compose logs -f\n\n# 管理容器\ndocker compose ps\ndocker compose pause\ndocker compose unpause\ndocker compose down\n```\n\n3. 访问服务：\n- SSE端点: http://localhost:8000/sse\n\n4. Cursor IDE配置：\n- 设置 → 功能 → 添加MCP服务器\n- 类型: \"sse\"\n- URL: `http://localhost:8000/sse`\n\n### 传统Python部署\n\n1. 安装系统依赖：\n```bash\n# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim\n\n# macOS\nbrew install poppler tesseract tesseract-lang\n\n# Windows\n# 1. 下载并安装Tesseract: https://github.com/UB-Mannheim/tesseract/wiki\n# 2. 将Tesseract添加到系统PATH\n```\n\n2. 安装Python依赖：\n```bash\n# 创建虚拟环境\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖\npip install -r requirements.txt\n```\n\n3. 启动服务：\n```bash\npython -m mcp_tool\n```\n\n## 配置\n\n### 环境变量\n\n在`.env`文件中配置：\n\n- `MCP_SERVER_PORT`: 服务器端口（默认: 8000）\n- `MCP_SERVER_HOST`: 绑定地址（默认: 0.0.0.0）\n- `DEBUG`: 调试模式（默认: false）\n- `MCP_USER_AGENT`: 自定义User-Agent\n\n## 依赖项\n\n主要依赖：\n- `mcp`: Model Context Protocol实现\n- `PyMuPDF`: PDF文档处理\n- `python-docx`: Word文档处理\n- `pandas`和`openpyxl`: Excel文件处理\n- `httpx`: 异步HTTP客户端\n- `anyio`: 异步I/O支持\n- `click`: 命令行接口\n\n## 贡献指南\n\n1. Fork仓库\n2. 创建功能分支 (`git checkout -b feature/amazing-feature`)\n3. 提交更改 (`git commit -m 'Add some amazing feature'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 打开Pull Request\n\n## 许可证\n\n本项目采用MIT许可证 - 详情请参阅[LICENSE](LICENSE)文件。\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "framework",
        "document",
        "processing",
        "document processing",
        "mcp framework",
        "framework framework"
      ],
      "category": "document-processing"
    },
    "whiteking64--macos-ocr-mcp": {
      "owner": "whiteking64",
      "name": "macos-ocr-mcp",
      "url": "https://github.com/whiteking64/macos-ocr-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/whiteking64.webp",
      "description": "Perform Optical Character Recognition (OCR) on images with the help of macOS's Vision framework, extracting recognized text segments, confidence scores, and bounding box coordinates. Suitable for applications that require text extraction from image files.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-07T18:16:13Z",
      "readme_content": "# macOS OCR MCP Tool\n\nThis project provides a MetaCall Protocol (MCP) tool to perform Optical Character Recognition (OCR) on images using macOS's built-in Vision framework. It exposes an `ocr_image` tool that takes an image file path and returns the recognized text along with confidence scores and bounding boxes.\n\n## Project Setup\n\n### Dependencies\nThis project relies on Python 3.13+ and the following main dependencies:\n- `ocrmac`: For accessing macOS OCR capabilities. See [ocrmac](https://github.com/straussmaximilian/ocrmac).\n- `Pillow`: For image manipulation.\n- `mcp[cli]>=1.7.1`: For the MetaCall Protocol server and client.\n\n### Installation\nIt is recommended to use a virtual environment.\n\n1.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv .venv\n    source .venv/bin/activate\n    ```\n\n2.  **Install dependencies using `uv`:**\n    ```bash\n    uv sync\n    ```\n\n## Running the MCP Server\n\nTo start the MCP server, run `main.py`:\n```bash\nuv run main.py\n```\nThis will start the MCP server, making the `ocr_image` tool available.\n\n## Available MCP Tools\n\n### `ocr_image`\n-   **Description:** Conducts OCR on the provided image file using macOS's built-in capabilities. Returns recognized text segments, their confidence scores, and bounding box coordinates.\n-   **Input:** `file_path: str` - The absolute or relative path to the image file.\n-   **Output (Example Success):**\n    ```json\n    {\n      \"filename\": \"path/to/your/image.png\",\n      \"annotations\": [\n        {\n          \"text\": \"Hello World\",\n          \"confidence\": 0.95,\n          \"bounding_box\": [0.1, 0.1, 0.5, 0.05] \n        },\n        // ... more annotations\n      ]\n    }\n    ```\n-   **Output (Example Error):**\n    ```json\n    {\n      \"error\": \"OCR functionality is only available on macOS.\"\n    }\n    ```\n    or\n    ```json\n    {\n      \"error\": \"File not found: path/to/nonexistent/image.png\"\n    }\n    ```\n\n**Note:** This tool will only function correctly on a macOS system due to its reliance on the Vision framework.\n\n## Testing with MCP Inspector\n\nYou can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to connect to the running MCP server and test the tool.\n\n## Cursor MCP Configuration\n\nTo configure this MCP server in Cursor, you can add the following to your MCP JSON configuration file (e.g., `~/.cursor/mcp.json` or project-specific `.cursor/mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"ocrmac\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/macos-ocr-mcp\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\nThis configuration tells Cursor how to start your MCP server. You can then call the `ocrmac.ocr_image` tool from within Cursor.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "macos",
        "recognition",
        "macos ocr",
        "ocr mcp",
        "ocr images"
      ],
      "category": "document-processing"
    },
    "worldnine--scrapbox-cosense-mcp": {
      "owner": "worldnine",
      "name": "scrapbox-cosense-mcp",
      "url": "https://github.com/worldnine/scrapbox-cosense-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/worldnine.webp",
      "description": "Access and interact with Scrapbox project pages, facilitating content retrieval, page listing, and full-text searching across project content.",
      "stars": 31,
      "forks": 8,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-09-01T23:47:39Z",
      "readme_content": "# scrapbox-cosense-mcp\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/worldnine-scrapbox-cosense-mcp-badge.png)](https://mseep.ai/app/worldnine-scrapbox-cosense-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/8huixkwpe2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/8huixkwpe2/badge\" alt=\"Scrapbox Cosense Server MCP server\" /></a>\n\n\n[English](#english) | [日本語](#日本語)\n\n## English\n\nMCP server for [cosense/scrapbox](https://cosen.se).\n\n### Features\n\n- `get_page`\n  - Get page content from cosense/Scrapbox\n    - Input: Page title, optional project name\n    - Output: Page content, metadata, links, and editor information\n- `list_pages`\n  - Browse and list pages with flexible sorting and pagination\n    - Purpose: Discover pages by recency, popularity, or alphabetically\n    - Input: Sorting options, pagination, optional project name\n    - Output: Page metadata and first 5 lines of content\n    - Max: 1000 pages per request\n    - Sorting: updated, created, accessed, linked, views, title\n- `search_pages`\n  - Search for content within pages using keywords or phrases\n    - Purpose: Find pages containing specific keywords or phrases\n    - Input: Search query, optional project name\n    - Output: Matching pages with highlighted search terms and content snippets\n    - Max: 100 results (API limitation)\n    - Supports: basic search, AND search, exclude search, exact phrases\n- `create_page`\n  - Create a new page in the project with WebSocket API\n    - Input: Page title, optional markdown body text, optional project name, optional createActually flag\n    - Output: Creates the page immediately and returns success confirmation with URL\n    - Note: Markdown content is converted to Scrapbox format\n    - Feature: Automatically converts numbered lists to bullet lists (configurable)\n    - Authentication: Requires COSENSE_SID for actual page creation\n- `get_page_url`\n  - Generate URL for a page in the project\n    - Input: Page title, optional project name\n    - Output: Direct URL to the specified page\n- `insert_lines`\n  - Insert text after a specified line in a page\n    - Input: Page title, target line text, text to insert, optional project name\n    - Output: Success message with insertion details\n    - Behavior: If target line not found, text is appended to the end of the page\n\n### Installation\n\n```bash\ngit clone https://github.com/worldnine/scrapbox-cosense-mcp.git\ncd scrapbox-cosense-mcp\nnpm install\nnpm run build\n```\n\n### Basic Setup\n\nTo use with Claude Desktop, add the server configuration as follows:\n\nFor MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nFor Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n**Single Project Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"scrapbox-cosense-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\", // Required for private projects\n        \"COSENSE_PAGE_LIMIT\": \"25\", // Optional (default: 100)\n        \"COSENSE_SORT_METHOD\": \"created\", // Optional (default: \"updated\")\n        \"SERVICE_LABEL\": \"scrapbox(cosense)\" // Optional (default: \"cosense(scrapbox)\")\n      }\n    }\n  }\n}\n```\n\n### Usage Examples\n\nOnce configured, you can use the tools in Claude:\n\n```\n# Get a specific page\nPlease get the content of page \"Meeting Notes\" using get_page.\n\n# List recent pages  \nPlease list the 10 most recently updated pages using list_pages.\n\n# Search for content\nPlease search for pages containing \"JavaScript tutorial\" using search_pages.\n\n# Create a new page\nPlease create a new page titled \"Today's Learning\" using create_page.\n\n# Get page URL\nPlease get the URL for page \"Project Plan\" using get_page_url.\n```\n\n### Environment Variables\n\nThis server uses the following environment variables:\n\n#### Required Environment Variables\n\n- `COSENSE_PROJECT_NAME`: Project name\n- `COSENSE_SID`: Session ID for Scrapbox/Cosense authentication (required for private projects) - [See how to get this cookie](#how-to-get-cosense_sid-cookie)\n\n#### Optional Environment Variables\n\n- `API_DOMAIN`: API domain (default: \"scrapbox.io\")\n- `SERVICE_LABEL`: Service identifier (default: \"cosense (scrapbox)\")\n- `COSENSE_PAGE_LIMIT`: Initial page fetch limit (1-1000, default: 100)\n- `COSENSE_SORT_METHOD`: Initial page fetch order (updated/created/accessed/linked/views/title, default: updated)\n- `COSENSE_TOOL_SUFFIX`: Tool name suffix for multiple server instances (e.g., \"main\" creates \"get_page_main\")\n- `COSENSE_CONVERT_NUMBERED_LISTS`: Convert numbered lists to bullet lists in Markdown (true/false, default: false)\n\n#### Environment Variable Behavior\n\n- **COSENSE_PROJECT_NAME**: Required environment variable. Server will exit with an error if not set.\n- **COSENSE_SID**: Required for accessing private projects. If not set, only public projects are accessible. [See detailed instructions](#how-to-get-cosense_sid-cookie) for obtaining this cookie.\n- **API_DOMAIN**:\n  - Uses \"scrapbox.io\" if not set\n  - While unverified with domains other than \"scrapbox.io\" in the author's environment, this option exists in case some environments require \"cosen.se\"\n- **COSENSE_PAGE_LIMIT**:\n  - Uses 100 if not set\n  - Uses 100 if value is invalid (non-numeric or out of range)\n  - Valid range: 1-1000\n- **COSENSE_SORT_METHOD**:\n  - Uses 'updated' if not set\n  - Uses 'updated' if value is invalid\n  - Does not affect list_pages tool behavior (only used for initial resource fetch)\n\n### How to Get COSENSE_SID Cookie\n\nFor accessing private Scrapbox projects, you need to obtain the `connect.sid` cookie from your browser. Follow these steps:\n\n1. **Navigate to your Scrapbox project**\n   - Open your browser and go to `https://scrapbox.io/YOUR_PROJECT_NAME`\n   - Replace `YOUR_PROJECT_NAME` with your actual project name\n\n2. **Log in to Scrapbox**\n   - Make sure you're logged in to your Scrapbox account\n   - Verify you can access your private project\n\n3. **Open Developer Tools**\n   - **Windows/Linux**: Press `F12` or `Ctrl+Shift+I`\n   - **macOS**: Press `Cmd+Option+I`\n   - **Alternative**: Right-click on the page and select \"Inspect\" or \"Inspect Element\"\n\n4. **Navigate to Cookies**\n   - In the Developer Tools, look for the **\"Application\"** tab (Chrome/Edge) or **\"Storage\"** tab (Firefox)\n   - In the left sidebar, expand **\"Cookies\"**\n   - Click on `https://scrapbox.io`\n\n5. **Find and copy the connect.sid cookie**\n   - Look for a cookie named `connect.sid`\n   - Click on it to see its value\n   - **Important**: The browser displays the URL-encoded value, but you need to use the **decoded** value\n   - Browser shows: `s%3Axxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n   - You should use: `s:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` (note the `:` after `s`)\n\n6. **Set the environment variable**\n   - Use the **decoded** value (with `:` instead of `%3A`) as your `COSENSE_SID` environment variable\n   - **Correct format**: `COSENSE_SID=s:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n   - **Incorrect format**: `COSENSE_SID=s%3Axxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n\n**Important Notes:**\n- Keep your `connect.sid` cookie value secure and never share it publicly\n- The cookie may expire after some time; you'll need to obtain a new one if authentication fails\n- This cookie provides access to your private projects, so treat it like a password\n\n### Multiple Project Support (Advanced)\n\n#### Method 1: Single Server with Optional Parameters\n\nAll tools support an optional `projectName` parameter to target different projects from a single server:\n\n- **Default behavior**: Uses `COSENSE_PROJECT_NAME` environment variable when no project is specified\n- **Multi-project usage**: Specify `projectName` parameter to access different projects  \n- **Backward compatibility**: Existing configurations work unchanged\n\n**Usage Examples:**\n\n```\n# Get page from default project\nPlease get the content of page \"Meeting Notes\" using get_page.\n\n# Get page from specific project  \nPlease get the content of page \"Design Guidelines\" from project \"help-ja\" using get_page.\n\n# Search in different project\nPlease search for pages containing \"API documentation\" in project \"developer-docs\" using search_pages.\n\n# Create page in specific project\nPlease create a new page titled \"Weekly Report\" in project \"team-updates\" using create_page.\n```\n\n**Important Limitations:**\n\nThis approach works best with public projects or projects that share the same authentication. For multiple private projects with different access credentials, use Method 2 below.\n\n#### Method 2: Multiple MCP Server Instances (Recommended for Private Projects)\n\nFor best user experience with multiple private projects, run separate MCP server instances for each project:\n\n```json\n{\n  \"mcpServers\": {\n    \"main-scrapbox\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"main-project\",        // Actual project name for API calls\n        \"COSENSE_SID\": \"s:main_sid_here...\",           // Session ID for this project\n        \"COSENSE_TOOL_SUFFIX\": \"main\",                 // Creates tools like get_page_main\n        \"SERVICE_LABEL\": \"Main Scrapbox\"               // Human-readable label in tool descriptions\n      }\n    },\n    \"team-cosense\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"team-workspace\",      // Actual project name for API calls\n        \"COSENSE_SID\": \"s:team_sid_here...\",           // Session ID for this project\n        \"COSENSE_TOOL_SUFFIX\": \"team\",                 // Creates tools like get_page_team\n        \"SERVICE_LABEL\": \"Team Cosense\"                // Human-readable label in tool descriptions\n      }\n    }\n  }\n}\n```\n\n**Key Configuration Points:**\n- **COSENSE_PROJECT_NAME**: The actual project name used in API calls (e.g., `scrapbox.io/main-project`)\n- **SERVICE_LABEL**: Display name shown in tool descriptions (e.g., \"Create page on Main Scrapbox\")\n- **COSENSE_TOOL_SUFFIX**: Creates unique tool names like `get_page_main` and `get_page_team`\n- **Different service names**: Using \"Scrapbox\" and \"Cosense\" helps distinguish between projects\n\nThis creates tools like `get_page_main`, `list_pages_main`, `get_page_team`, `list_pages_team`, allowing LLMs to automatically select the appropriate project.\n\n### Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nAuto-rebuild during development:\n\n```bash\nnpm run watch\n```\n\nRun tests:\n\n```bash\nnpm test\n```\n\nRun linting:\n\n```bash\nnpm run lint\n```\n\n### Quality Management\n\nThis project includes automated quality checks to ensure code reliability:\n\n- **ESLint**: TypeScript-aware linting with console.log warnings\n- **GitHub Actions**: Automated CI/CD pipeline for pull requests\n- **Branch Protection**: Main branch requires PR and passing checks\n- **Test Suite**: 142+ tests covering all functionality\n\n#### Contributing Guidelines\n\n1. Create a feature branch from main\n2. Make your changes with appropriate tests\n3. Run `npm run lint` and `npm test` locally\n4. Create a pull request\n5. CI will automatically run quality checks\n6. Merge only after all checks pass\n\nThe quality management system prevents debug logs and broken code from reaching production.\n\n### Debugging\n\nSince MCP servers communicate via stdio, debugging can be challenging. This server includes comprehensive debug logging to help troubleshoot issues.\n\n#### Debug Logs\n\nThe server outputs detailed debug information to help identify configuration and API issues:\n\n- **Server Configuration**: Project name, tool suffix, SID presence, limits\n- **Tool Generation**: List of generated tools with their names\n- **Tool Calls**: Requested vs normalized tool names, arguments\n- **API Requests**: URLs, project names, authentication status\n- **API Errors**: Detailed error information with context\n\n#### Using MCP Inspector\n\nUsing [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is recommended for interactive debugging:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector provides a URL to access debugging tools in the browser.\n\n#### Troubleshooting Multiple Servers\n\nWhen running multiple server instances, check the debug logs for:\n\n1. **Tool Name Conflicts**: Ensure `COSENSE_TOOL_SUFFIX` is set differently for each server\n2. **API Access**: Verify SID authentication works for each project\n3. **Project Names**: Confirm project names are correctly configured\n\n## 日本語\n\n[cosense/scrapbox](https://cosen.se) 用のMCPサーバーです。\n\n## 機能\n\n- `get_page`\n  - cosense/Scrapboxからページコンテンツを取得\n    - 入力: ページタイトル、オプションのプロジェクト名\n    - 出力: ページコンテンツ、メタデータ、リンク、編集者の情報\n- `list_pages`\n  - 柔軟なソートとページネーションによるページ一覧の閲覧\n    - 用途: 最新性、人気度、アルファベット順でページを発見\n    - 入力: ソートオプション、ページネーション、オプションのプロジェクト名\n    - 出力: ページのメタデータと冒頭5行の内容\n    - 最大: 1リクエスト当たり1000件\n    - ソート: updated, created, accessed, linked, views, title\n- `search_pages`\n  - キーワードやフレーズを使用したページ内コンテンツの検索\n    - 用途: 特定のキーワードやフレーズを含むページを発見\n    - 入力: 検索クエリ、オプションのプロジェクト名\n    - 出力: マッチしたページとハイライトされた検索語句、コンテンツスニペット\n    - 最大: 100件（API制限）\n    - サポート: 基本検索、AND検索、除外検索、完全一致フレーズ\n- `create_page`\n  - WebSocket APIを使ってプロジェクトに新しいページを作成\n    - 入力: ページタイトル、オプションのマークダウン本文テキスト、オプションのプロジェクト名、オプションのcreateActuallyフラグ\n    - 出力: ページを即座に作成し、成功確認とURLを返す\n    - 注意: マークダウンコンテンツはScrapbox形式に変換されます\n    - 機能: 数字付きリストを自動的に箇条書きに変換（設定可能）\n    - 認証: 実際のページ作成にはCOSENSE_SIDが必要\n- `get_page_url`\n  - プロジェクト内のページのURLを生成\n    - 入力: ページタイトル、オプションのプロジェクト名\n    - 出力: 指定されたページへの直接URL\n- `insert_lines`\n  - ページの指定した行の後にテキストを挿入\n    - 入力: ページタイトル、対象行のテキスト、挿入するテキスト、オプションのプロジェクト名\n    - 出力: 挿入の詳細を含む成功メッセージ\n    - 動作: 対象行が見つからない場合は、ページの末尾にテキストが追加されます\n\n## インストール方法\n\n```bash\ngit clone https://github.com/worldnine/scrapbox-cosense-mcp.git\ncd scrapbox-cosense-mcp\nnpm install\nnpm run build\n```\n\n## 基本設定\n\nClaude Desktopで使用するには、以下のようにサーバー設定を追加してください:\n\nMacOSの場合: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nWindowsの場合: `%APPDATA%/Claude/claude_desktop_config.json`\n\n**単一プロジェクト設定:**\n```json\n{\n  \"mcpServers\": {\n    \"scrapbox-cosense-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\", // プライベートプロジェクトの場合は必須\n        \"COSENSE_PAGE_LIMIT\": \"25\", // オプション（デフォルト: 100）\n        \"COSENSE_SORT_METHOD\": \"created\", // オプション（デフォルト: \"updated\"）\n        \"SERVICE_LABEL\": \"scrapbox(cosense)\" // オプション（デフォルト: \"cosense(scrapbox)\"）\n      }\n    }\n  }\n}\n```\n\n## 使用例\n\n設定完了後、Claudeで以下のようにツールを使用できます：\n\n```\n# 特定のページを取得\nget_page を使用してページ「会議メモ」の内容を取得してください。\n\n# 最近のページ一覧\nlist_pages を使用して最近更新された10件のページを一覧表示してください。\n\n# コンテンツ検索\nsearch_pages を使用して「JavaScript チュートリアル」を含むページを検索してください。\n\n# 新しいページを作成\ncreate_page を使用して「今日の学び」というタイトルでページを作成してください。\n\n# ページURLを取得\nget_page_url を使用してページ「プロジェクト計画」のURLを取得してください。\n```\n\n## 環境変数\n\nこのサーバーは以下の環境変数を使用します：\n\n### 必須の環境変数\n\n- `COSENSE_PROJECT_NAME`: プロジェクト名\n- `COSENSE_SID`: Scrapbox/Cosenseの認証用セッションID（プライベートプロジェクトの場合は必須） - [Cookieの取得方法](#cosense_sid-cookieの取得方法)\n\n### オプションの環境変数\n\n- `API_DOMAIN`: APIドメイン（デフォルト: \"scrapbox.io\"）\n- `SERVICE_LABEL`: サービスの識別名（デフォルト: \"cosense (scrapbox)\"）\n- `COSENSE_PAGE_LIMIT`: 初期取得時のページ数（1-1000、デフォルト: 100）\n- `COSENSE_SORT_METHOD`: 初期取得時の取得ページ順（updated/created/accessed/linked/views/title、デフォルト: updated）\n- `COSENSE_TOOL_SUFFIX`: 複数サーバーインスタンス用のツール名サフィックス（例：\"main\"で\"get_page_main\"が作成されます）\n\n### 環境変数の挙動について\n\n- **COSENSE_PROJECT_NAME**: 必須の環境変数です。未設定の場合、サーバーは起動時にエラーで終了します。\n- **COSENSE_SID**: プライベートプロジェクトへのアクセスに必要です。未設定の場合、パブリックプロジェクトのみアクセス可能です。[詳細な取得手順](#cosense_sid-cookieの取得方法)をご確認ください。\n- **API_DOMAIN**:\n  - 未設定時は\"scrapbox.io\"を使用\n  - 作者の環境では\"scrapbox.io\"以外の値は未検証ですが、\"cosen.se\"でないと動作しない環境が存在する可能性があるため念のためのオプションです。\n- **COSENSE_PAGE_LIMIT**:\n  - 未設定時は100を使用\n  - 無効な値（数値以外や範囲外）の場合は100を使用\n  - 有効範囲: 1-1000\n- **COSENSE_SORT_METHOD**:\n  - 未設定時は'updated'を使用\n  - 無効な値の場合は'updated'を使用\n  - list_pagesツールの動作には影響しません（初期リソース取得時のみ使用）\n\n### COSENSE_SID Cookieの取得方法\n\nプライベートなScrapboxプロジェクトにアクセスするには、ブラウザから `connect.sid` Cookieを取得する必要があります。以下の手順に従ってください：\n\n1. **Scrapboxプロジェクトにアクセス**\n   - ブラウザで `https://scrapbox.io/あなたのプロジェクト名` を開きます\n   - `あなたのプロジェクト名` を実際のプロジェクト名に置き換えてください\n\n2. **Scrapboxにログイン**\n   - Scrapboxアカウントにログインしていることを確認してください\n   - プライベートプロジェクトにアクセスできることを確認してください\n\n3. **開発者ツールを開く**\n   - **Windows/Linux**: `F12` キーまたは `Ctrl+Shift+I` を押します\n   - **macOS**: `Cmd+Option+I` を押します\n   - **別の方法**: ページ上で右クリックして「検証」または「要素を調査」を選択\n\n4. **Cookieを確認**\n   - 開発者ツールで **「Application」** タブ（Chrome/Edge）または **「ストレージ」** タブ（Firefox）を探します\n   - 左側のサイドバーで **「Cookies」** を展開します\n   - `https://scrapbox.io` をクリックします\n\n5. **connect.sid Cookieを見つけてコピー**\n   - `connect.sid` という名前のCookieを探します\n   - それをクリックして値を確認します\n   - **重要**: ブラウザはURLエンコードされた値を表示しますが、実際には**デコードされた値**を使用する必要があります\n   - ブラウザ表示: `s%3Axxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n   - 使用すべき値: `s:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`（`s`の後は`:`です）\n\n6. **環境変数に設定**\n   - **デコードされた値**（`%3A`の代わりに`:`）を `COSENSE_SID` 環境変数として使用します\n   - **正しい形式**: `COSENSE_SID=s:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n   - **間違った形式**: `COSENSE_SID=s%3Axxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\n\n**重要な注意事項:**\n- `connect.sid` Cookieの値は機密情報のため、安全に管理し、公開しないでください\n- Cookieは時間が経つと期限切れになる場合があります。認証エラーが発生した場合は新しいCookieを取得してください\n- このCookieはプライベートプロジェクトへのアクセス権を提供するため、パスワードと同様に扱ってください\n\n## 複数プロジェクト対応（高度な機能）\n\n### 方法1: オプショナルパラメータを使用した単一サーバー\n\nすべてのツールで、単一サーバーから異なるプロジェクトを対象とするオプションの`projectName`パラメータをサポートしています：\n\n- **デフォルト動作**: プロジェクトが指定されていない場合は`COSENSE_PROJECT_NAME`環境変数を使用\n- **複数プロジェクト使用**: `projectName`パラメータを指定して異なるプロジェクトにアクセス\n- **後方互換性**: 既存の設定は変更なしで動作\n\n**使用例:**\n\n```\n# デフォルトプロジェクトからページを取得\nget_page を使用してページ「会議メモ」の内容を取得してください。\n\n# 特定のプロジェクトからページを取得  \nget_page を使用して、プロジェクト名「help-ja」からページ「使い方」の内容を取得してください。\n\n# 異なるプロジェクトでページを検索\nsearch_pages を使用して、プロジェクト名「developer-docs」で「API ドキュメント」を含むページを検索してください。\n\n# 特定のプロジェクトにページを作成\ncreate_page を使用して、プロジェクト名「team-updates」に「週次レポート」というタイトルでページを作成してください。\n```\n\n**重要な制限事項:**\n\nこの方法は、パブリックプロジェクトや同じ認証情報を共有するプロジェクトで最も効果的です。異なるアクセス認証情報を持つ複数のプライベートプロジェクトには、以下の方法2をご利用ください。\n\n### 方法2: 複数MCPサーバーインスタンス（プライベートプロジェクト推奨）\n\n複数のプライベートプロジェクトで最良のユーザー体験を得るには、プロジェクトごとに別々のMCPサーバーインスタンスを実行します：\n\n```json\n{\n  \"mcpServers\": {\n    \"main-scrapbox\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"main-project\",        // API呼び出しで使用される実際のプロジェクト名\n        \"COSENSE_SID\": \"s:main_sid_here...\",           // このプロジェクト用のセッションID\n        \"COSENSE_TOOL_SUFFIX\": \"main\",                 // get_page_main のようなツール名を作成\n        \"SERVICE_LABEL\": \"Main Scrapbox\"               // ツール説明で表示される人間向けの名前\n      }\n    },\n    \"team-cosense\": {\n      \"command\": \"npx\",\n      \"args\": [\"github:worldnine/scrapbox-cosense-mcp\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"team-workspace\",      // API呼び出しで使用される実際のプロジェクト名\n        \"COSENSE_SID\": \"s:team_sid_here...\",           // このプロジェクト用のセッションID\n        \"COSENSE_TOOL_SUFFIX\": \"team\",                 // get_page_team のようなツール名を作成\n        \"SERVICE_LABEL\": \"Team Cosense\"                // ツール説明で表示される人間向けの名前\n      }\n    }\n  }\n}\n```\n\n**設定のポイント:**\n- **COSENSE_PROJECT_NAME**: API呼び出しで使用される実際のプロジェクト名（例: `scrapbox.io/main-project`）\n- **SERVICE_LABEL**: ツール説明で表示される名前（例: 「Main Scrapboxでページを作成」）\n- **COSENSE_TOOL_SUFFIX**: `get_page_main` や `get_page_team` のような固有のツール名を作成\n- **サービス名の使い分け**: 「Scrapbox」と「Cosense」を使い分けることでプロジェクトを区別\n\nこれにより `get_page_main`、`list_pages_main`、`get_page_team`、`list_pages_team` のようなツールが作成され、LLMが自動的に適切なプロジェクトを選択できるようになります。\n\n## 開発方法\n\n依存関係のインストール:\n\n```bash\nnpm install\n```\n\nサーバーのビルド:\n\n```bash\nnpm run build\n```\n\n開発時の自動リビルド:\n\n```bash\nnpm run watch\n```\n\nテストの実行:\n\n```bash\nnpm test\n```\n\nリンティングの実行:\n\n```bash\nnpm run lint\n```\n\n### 品質管理\n\nこのプロジェクトでは、コードの信頼性を確保するための自動品質チェックが導入されています：\n\n- **ESLint**: TypeScript対応のリンティング、console.log使用時の警告\n- **GitHub Actions**: プルリクエスト用の自動CI/CDパイプライン\n- **ブランチ保護**: mainブランチへはPRとチェック通過が必須\n- **テストスイート**: 142+のテストで全機能をカバー\n\n#### 貢献ガイドライン\n\n1. mainから機能ブランチを作成\n2. 適切なテストと共に変更を実装\n3. ローカルで `npm run lint` と `npm test` を実行\n4. プルリクエストを作成\n5. CIが自動的に品質チェックを実行\n6. 全チェック通過後にマージ\n\nこの品質管理システムにより、デバッグログや壊れたコードの本番環境への混入を防げます。\n\n### デバッグ方法\n\nMCPサーバーはstdioを介して通信を行うため、デバッグが難しい場合があります。[MCP Inspector](https://github.com/modelcontextprotocol/inspector)の使用を推奨します。以下のコマンドで実行できます：\n\n```bash\nnpm run inspector\n```\n\nInspectorはブラウザでデバッグツールにアクセスするためのURLを提供します。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapbox",
        "retrieval",
        "pages",
        "scrapbox project",
        "scrapbox cosense",
        "interact scrapbox"
      ],
      "category": "document-processing"
    },
    "worldnine--textwell-mcp": {
      "owner": "worldnine",
      "name": "textwell-mcp",
      "url": "https://github.com/worldnine/textwell-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/worldnine.webp",
      "description": "A specialized MCP server for writing text to the Textwell application on macOS, offering modes for replacing, inserting, or appending text. It facilitates text manipulation directly within the Textwell environment.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-01-17T13:30:15Z",
      "readme_content": "# Textwell Write Tool (MCP Server)\n\nA specialized MCP server designed exclusively for writing text to the Textwell application on macOS.\n\n## Overview\n\nThis tool provides a straightforward way to write text to Textwell. \n\n## Features\n\nThe server provides a single tool: `write-text`\n\n### Write Text Tool\n\nWrites text to Textwell using specified modes:\n\n- **Replace Mode** (default)\n  - Replaces the entire content with new text\n  - Use case: Complete content replacement\n\n- **Insert Mode**\n  - Inserts text at the current cursor position\n  - Use case: Adding content within existing text\n\n- **Add Mode**\n  - Appends text to the end of current content\n  - Use case: Adding new content while preserving existing text\n\n## Limitations\n\n- Write-only operations (no read capabilities)\n\n## Development Setup\n\n### Prerequisites\n\n- Node.js v22.12.0 (managed by Volta)\n- npm v10.9.0 (managed by Volta)\n- macOS (for Textwell integration)\n\n### Installation\n\n1. Clone the repository\n```bash\ngit clone [repository-url]\ncd textwell-mcp\n```\n\n2. Install dependencies\n```bash\nnpm install\n```\n\n3. Build the server\n```bash\nnpm run build\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "textwell",
        "mcp",
        "text",
        "textwell mcp",
        "textwell application",
        "text textwell"
      ],
      "category": "document-processing"
    },
    "xiandan-erizo--ops-mcp": {
      "owner": "xiandan-erizo",
      "name": "ops-mcp",
      "url": "https://github.com/xiandan-erizo/ops-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/xiandan-erizo.webp",
      "description": "Search and retrieve Confluence documents, accessing full page content and associated metadata for efficient document management. Supports full-text search and retrieval of document details including title, space, and version information.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-14T10:53:12Z",
      "readme_content": "# Confluence MCP Server\n\n基于 Model Context Protocol (MCP) 的 Confluence 文档访问服务，提供文档搜索和内容获取功能。\n\n## 功能特性\n\n- **文档搜索**\n  - 支持标题和全文搜索\n  - 限制返回结果数量\n  - 返回匹配内容片段和文档基本信息\n\n- **文档内容获取**\n  - 获取完整的页面内容\n  - 包含元数据（标题、空间信息、版本等）\n  - 创建和修改信息\n  - 页面标签\n\n## 快速开始\n\n### 环境配置\n\n创建 `.env` 文件配置 Confluence 访问信息：\n\n```bash\nCONFLUENCE_URL=\"your-confluence-url\"\nCONFLUENCE_USERNAME=\"your-username\"\nCONFLUENCE_PASSWORD=\"your-password\"\n# 或者使用 Token 认证\nCONFLUENCE_TOKEN=\"your-api-token\"\n```\n\n### 安装依赖\n\n```bash\n# 安装项目依赖\nuv pip install -e .\n\n# 或者直接安装依赖包\nuv pip install \"mcp[cli]>=1.5.0\" \"atlassian-python-api>=3.41.4\" \"typer>=0.9.0\"\n```\n\n### 运行服务\n\n```bash\n# 使用 uvx\n/Users/hose/.local/bin/uvx --directory /Users/hose/code/ops/ops-mcp mcp main.py\n\n# 或者使用 uv run\nuv run --with mcp mcp run main.py\n```\n\n## MCP 接口说明\n\n### Tools (工具)\n\n1. search_confluence\n   ```python\n   # 搜索 Confluence 内容\n   Input:\n   - query: str       # 搜索关键词\n   - limit: int = 10  # 返回结果数量限制\n   \n   Output:\n   {\n     \"success\": true,\n     \"query\": \"搜索词\",\n     \"total\": 5,\n     \"results\": [\n       {\n         \"id\": \"12345\",\n         \"title\": \"页面标题\",\n         \"type\": \"page\",\n         \"url\": \"页面URL\",\n         \"excerpt\": \"匹配内容片段\"\n       }\n     ]\n   }\n   ```\n\n2. get_confluence_page\n   ```python\n   # 获取页面详细信息\n   Input:\n   - page_id: str  # 页面ID\n   \n   Output:\n   {\n     \"success\": true,\n     \"page\": {\n       \"id\": \"12345\",\n       \"title\": \"页面标题\",\n       \"space\": {\n         \"key\": \"SPACE\",\n         \"name\": \"空间名称\"\n       },\n       \"version\": 1,\n       \"content\": \"页面内容\",\n       \"url\": \"页面URL\",\n       \"created\": {\n         \"date\": \"创建时间\",\n         \"by\": \"创建者\"\n       },\n       \"modified\": {\n         \"date\": \"修改时间\",\n         \"by\": \"修改者\"\n       },\n       \"labels\": [\"标签1\", \"标签2\"]\n     }\n   }\n   ```\n\n### Resources (资源)\n\n1. confluence://pages/{page_id}\n   - 通过页面ID直接获取页面内容和元数据\n   - 返回 JSON 格式数据\n\n2. confluence://search/{query}\n   - 通过关键词直接搜索内容\n   - 返回 JSON 格式的搜索结果\n\n## 错误处理\n\n所有接口在出错时返回统一格式：\n\n```json\n{\n  \"success\": false,\n  \"error\": \"错误信息描述\"\n}\n```\n\n## 使用示例\n\n1. 搜索文档\n```python\nresult = await mcp.use_tool(\"search_confluence\", {\n    \"query\": \"Python\",\n    \"limit\": 5\n})\n```\n\n2. 获取页面内容\n```python\npage = await mcp.use_tool(\"get_confluence_page\", {\n    \"page_id\": \"12345\"\n})\n```\n\n3. 使用资源URI\n```python\ncontent = await mcp.access_resource(\"confluence://pages/12345\")\nsearch_results = await mcp.access_resource(\"confluence://search/Python\")\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documents",
        "document",
        "confluence",
        "retrieval document",
        "document processing",
        "confluence documents"
      ],
      "category": "document-processing"
    },
    "xraywu--mcp-pdf-extraction-server": {
      "owner": "xraywu",
      "name": "mcp-pdf-extraction-server",
      "url": "https://github.com/xraywu/mcp-pdf-extraction-server",
      "imageUrl": "/freedevtools/mcp/pfp/xraywu.webp",
      "description": "Extracts text from PDF files using advanced reading and OCR capabilities. Supports content retrieval from specified pages or entire documents for seamless integration into applications.",
      "stars": 18,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-14T15:12:13Z",
      "readme_content": "# PDF Extraction MCP Server (Claude Code Fork)\n\nMCP server to extract contents from PDF files, with fixes for Claude Code CLI installation.\n\nThis fork includes critical fixes for installing and running the server with Claude Code (the CLI version).\n\n## What's Different in This Fork\n\n1. **Added `__main__.py`** - Enables the package to be run as a module with `python -m pdf_extraction`\n2. **Claude Code specific instructions** - Clear installation steps that work with Claude Code CLI\n3. **Tested installation process** - Verified working with `claude mcp add` command\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- **extract-pdf-contents**: Extract contents from a local PDF file\n  - Takes `pdf_path` as a required string argument (local file path)\n  - Takes `pages` as an optional string argument (comma-separated page numbers, supports negative indexing like `-1` for last page)\n  - Supports both PDF text extraction and OCR for scanned documents\n\n## Installation for Claude Code CLI\n\n### Prerequisites\n\n- Python 3.11 or higher\n- pip or conda\n- Claude Code CLI installed (`claude` command)\n\n### Step 1: Clone and Install\n\n```bash\n# Clone this fork\ngit clone https://github.com/lh/mcp-pdf-extraction-server.git\ncd mcp-pdf-extraction-server\n\n# Install in development mode\npip install -e .\n```\n\n### Step 2: Find the Installed Command\n\n```bash\n# Check where pdf-extraction was installed\nwhich pdf-extraction\n# Example output: /opt/homebrew/Caskroom/miniconda/base/bin/pdf-extraction\n```\n\n### Step 3: Add to Claude Code\n\n```bash\n# Add the server using the full path from above\nclaude mcp add pdf-extraction /opt/homebrew/Caskroom/miniconda/base/bin/pdf-extraction\n\n# Verify it was added\nclaude mcp list\n```\n\n### Step 4: Use in Claude\n\n```bash\n# Start a new Claude session\nclaude\n\n# In Claude, type:\n/mcp\n\n# You should see:\n# MCP Server Status\n# • pdf-extraction: connected\n```\n\n## Usage Example\n\nOnce connected, you can ask Claude to extract PDF contents:\n\n```\n\"Can you extract the content from the PDF at /path/to/document.pdf?\"\n\n\"Extract pages 1-3 and the last page from /path/to/document.pdf\"\n```\n\n## Troubleshooting\n\n### Server Not Connecting\n\n1. Make sure you started a NEW Claude session after adding the server\n2. Verify the command path is correct: `ls -la $(which pdf-extraction)`\n3. Test the command directly (it should hang waiting for input): `pdf-extraction`\n\n### Module Not Found Errors\n\nIf you get Python import errors:\n1. Make sure you're using the same Python environment where you installed the package\n2. Try using the full Python path: `claude mcp add pdf-extraction /path/to/python -m pdf_extraction`\n\n### Installation Issues\n\nIf `pip install -e .` fails:\n1. Make sure you have Python 3.11+: `python --version`\n2. Try creating a fresh virtual environment:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   pip install -e .\n   ```\n\n## For Claude Desktop Users\n\nThis fork is specifically for Claude Code CLI. If you're using Claude Desktop (the GUI app), please refer to the [original repository](https://github.com/xraywu/mcp-pdf-extraction-server) for installation instructions.\n\n## Dependencies\n\n- mcp>=1.2.0\n- pypdf2>=3.0.1\n- pytesseract>=0.3.10 (for OCR support)\n- Pillow>=10.0.0\n- pydantic>=2.10.1,<3.0.0\n- pymupdf>=1.24.0\n\n## Contributing\n\nContributions are welcome! The main change in this fork is the addition of `__main__.py` to make the package runnable as a module.\n\n## License\n\nSame as the original repository.\n\n## Credits\n\nOriginal server by [@xraywu](https://github.com/xraywu)\nClaude Code fixes by [@lh](https://github.com/lh)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ocr",
        "pdf",
        "document",
        "pdf extraction",
        "document processing",
        "processing xraywu"
      ],
      "category": "document-processing"
    },
    "yellowgg2--mcp-bookstack": {
      "owner": "yellowgg2",
      "name": "mcp-bookstack",
      "url": "https://github.com/yellowgg2/mcp-bookstack",
      "imageUrl": "/freedevtools/mcp/pfp/yellowgg2.webp",
      "description": "Search and retrieve structured data from BookStack pages with customizable queries, pagination, and HTML-to-text conversion for enhanced reading. It includes robust error handling and validation for seamless content access.",
      "stars": 9,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T00:34:46Z",
      "readme_content": "# BookStack MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@yellowgg2/mcp-bookstack)](https://smithery.ai/server/@yellowgg2/mcp-bookstack)\n\nA Model Context Protocol (MCP) server that provides tools for searching pages from BookStack. This server interacts with the BookStack API and provides structured data for pages with clean HTML-to-text conversion.\n\n## Features\n\n- Search pages from BookStack with customizable queries\n- Get structured data including titles, URLs, and content\n- Configurable pagination (page number and count)\n- HTML-to-text conversion for clean content reading\n- Clean error handling and validation\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-bookstack for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yellowgg2/mcp-bookstack):\n\n```bash\nnpx -y @smithery/cli install @yellowgg2/mcp-bookstack --client claude\n```\n\n### Installing Manually\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/yellowgg2/mcp-bookstack.git\ncd mcp-bookstack\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Configure your environment:\n\nCreate a `.env` file with your BookStack API credentials or provide them in the MCP settings configuration file:\n\n```\nBOOKSTACK_API_TOKEN=your_token\nBOOKSTACK_API_URL=your_bookstack_url\nBOOKSTACK_API_KEY=your_api_key\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n5. Add to your MCP settings configuration file (location depends on your system):\n\nFor VSCode Claude extension:\n\n```json\n{\n  \"mcpServers\": {\n    \"bookstack\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-bookstack/build/app.js\"],\n      \"env\": {\n        \"BOOKSTACK_API_URL\": \"your_bookstack_url\",\n        \"BOOKSTACK_API_TOKEN\": \"your_token\",\n        \"BOOKSTACK_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a tool called `search_pages` that can be used to search pages from BookStack.\n\n### Tool: search_pages\n\nParameters:\n\n- `query` (string): Query to search for pages\n  - Default: \"\" (empty string)\n- `page` (number): Page number to return\n  - Range: 1-10\n  - Default: 1\n- `count` (number): Number of pages to return\n  - Range: 1-30\n  - Default: 10\n\nExample usage:\n\n```typescript\nuse_mcp_tool with:\nserver_name: \"bookstack\"\ntool_name: \"search_pages\"\narguments: {\n  \"query\": \"knowledge base\",\n  \"page\": 1,\n  \"count\": 5\n}\n```\n\nSample output:\n\n```\n# Page Title\n\nPage content in plain text format...\n\nSource: https://your-bookstack-url/page-url\n```\n\n## Integrating with Claude\n\nTo use this MCP server with Claude, you'll need to:\n\n1. Have the Claude desktop app or VSCode Claude extension installed\n2. Configure the MCP server in your settings\n3. Use Claude's natural language interface to interact with BookStack\n\n### Configuration\n\nFor the Claude desktop app, VSCode Claude extension, and Cursor, add the server configuration to:\n\n```json\n// ~/Library/Application Support/Claude/claude_desktop_config.json (macOS)\n// %APPDATA%\\Claude\\claude_desktop_config.json (Windows)\n{\n  \"mcpServers\": {\n    \"bookstack\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-bookstack/build/app.js\"],\n      \"env\": {\n        \"BOOKSTACK_API_URL\": \"your_bookstack_url\",\n        \"BOOKSTACK_API_TOKEN\": \"your_token\",\n        \"BOOKSTACK_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n\n### Example Interactions\n\nOnce configured, you can interact with Claude using natural language to search BookStack pages. Examples:\n\n- \"Search for documentation about API usage in our BookStack knowledge base\"\n- \"Find information about deployment in our internal docs\"\n- \"Look up security guidelines in BookStack\"\n\nClaude will automatically use the appropriate parameters to search for the pages you want.\n\n## Page Response Structure\n\nEach page response includes:\n\n- Title of the page\n- Full content of the page (converted from HTML to plain text)\n- Source URL to the original page\n\nThe HTML-to-text conversion handles:\n\n- HTML entity decoding\n- Line breaks and paragraph formatting\n- List items with bullet points\n- Removal of HTML tags\n- Whitespace normalization\n\n## Development\n\nThe server is built using:\n\n- TypeScript\n- Model Context Protocol SDK\n- Axios for API requests\n- Zod for data validation\n- dotenv for environment configuration\n\nTo modify the server:\n\n1. Make changes to `src/app.ts`\n2. Rebuild:\n\n```bash\nnpm run build\n```\n\n## Error Handling\n\nThe server includes robust error handling for:\n\n- API connection failures\n- Authentication issues\n- Invalid parameter values\n- Data parsing errors\n\nErrors are returned with appropriate error codes and descriptive messages.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - feel free to use this in your own projects.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bookstack",
        "pages",
        "reading",
        "bookstack search",
        "bookstack pages",
        "data bookstack"
      ],
      "category": "document-processing"
    },
    "yosider--cosense-mcp-server": {
      "owner": "yosider",
      "name": "cosense-mcp-server",
      "url": "https://github.com/yosider/cosense-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/yosider.webp",
      "description": "Access and interact with the Cosense knowledge sharing platform by retrieving, listing, and searching for pages, as well as inserting text into existing pages.",
      "stars": 8,
      "forks": 8,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-07T02:00:58Z",
      "readme_content": "# Cosense MCP Server\n\nA MCP server for [Cosense](https://cosen.se).\n\n## Tools\n\nThe following tools are available for interacting with Cosense pages:\n\n- `get_page`: Retrieves a page with the specified title\n- `list_pages`: Lists available pages in the resources\n- `search_pages`: Searches for pages containing the specified query string\n- `insert_lines`: Inserts text after a specified line in a page\n\n## MCP Client Configuration\n\nThe following environment variables are required:\n\n- `COSENSE_PROJECT_NAME`: Project name\n- `COSENSE_SID`: Session ID for authentication\n  - Required for writing to pages and reading private pages\n  - Handle with care as it contains sensitive information\n  - For more details, see [scrapboxlab/connect.sid](https://scrapbox.io/scrapboxlab/connect.sid)\n- `NODE_ENV`: Execution environment (`development` or `production`)\n  - Controls logging behavior\n  - In `development` mode, debug logs are displayed\n  - In `production` mode, debug logs are suppressed\n\n### Run from npm registry\n\n#### JSR registry configuration\n\nThis package depends on [@cosense/std](https://jsr.io/@cosense/std) and [@cosense/types](https://jsr.io/@cosense/types) which are hosted on JSR. Before using npx, you need to configure the JSR registry globally:\n\nFor Linux/macOS:\n\n```bash\necho \"@jsr:registry=https://npm.jsr.io\" >> ~/.npmrc\n```\n\nFor Windows (PowerShell):\n\n```powershell\necho \"@jsr:registry=https://npm.jsr.io\" >> $env:USERPROFILE\\.npmrc\n```\n\nOr if you prefer not to modify global settings, run from source instead (see the section below)\n\n#### Client json configuration\n\nAfter configuring JSR registry, configure your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"cosense-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@yosider/cosense-mcp-server\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\"\n      }\n    }\n  }\n}\n```\n\n### Run from source\n\n#### Clone and build\n\n```bash\ngit clone https://github.com/yosider/cosense-mcp-server.git\ncd cosense-mcp-server\nnpm install\nnpm run build\n```\n\n#### Client json configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"cosense-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"/path/to/cosense-mcp-server\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\"\n      }\n    }\n  }\n}\n```\n\nFor development debugging, add `\"NODE_ENV\": \"development\"` to the `env` section. Note that setting environment variables in a `.env` file won't work due to execution timing - use the MCP client configuration instead.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspect\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Acknowledgments\n\nThis project is forked from [funwarioisii/cosense-mcp-server](https://github.com/funwarioisii/cosense-mcp-server).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cosense",
        "pages",
        "processing",
        "cosense knowledge",
        "interact cosense",
        "cosense mcp"
      ],
      "category": "document-processing"
    },
    "zanetworker--mcp-docling": {
      "owner": "zanetworker",
      "name": "mcp-docling",
      "url": "https://github.com/zanetworker/mcp-docling",
      "imageUrl": "/freedevtools/mcp/pfp/zanetworker.webp",
      "description": "Convert documents to markdown, extract tables, and process multiple files efficiently for enhanced document processing capabilities.",
      "stars": 18,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T02:42:51Z",
      "readme_content": "# MCP Docling Server\n\nAn MCP server that provides document processing capabilities using the Docling library.\n\n## Installation\n\nYou can install the package using pip:\n\n```bash\npip install -e .\n```\n\n## Usage\n\nStart the server using either stdio (default) or SSE transport:\n\n```bash\n# Using stdio transport (default)\nmcp-server-lls\n\n# Using SSE transport on custom port\nmcp-server-lls --transport sse --port 8000\n```\n\nIf you're using uv, you can run the server directly without installing:\n\n```bash\n# Using stdio transport (default)\nuv run mcp-server-lls\n\n# Using SSE transport on custom port\nuv run mcp-server-lls --transport sse --port 8000\n```\n\n## Available Tools\n\nThe server exposes the following tools:\n\n1. **convert_document**: Convert a document from a URL or local path to markdown format\n   - `source`: URL or local file path to the document (required)\n   - `enable_ocr`: Whether to enable OCR for scanned documents (optional, default: false)\n   - `ocr_language`: List of language codes for OCR, e.g. [\"en\", \"fr\"] (optional)\n\n2. **convert_document_with_images**: Convert a document and extract embedded images\n   - `source`: URL or local file path to the document (required)\n   - `enable_ocr`: Whether to enable OCR for scanned documents (optional, default: false)\n   - `ocr_language`: List of language codes for OCR (optional)\n\n3. **extract_tables**: Extract tables from a document as structured data\n   - `source`: URL or local file path to the document (required)\n\n4. **convert_batch**: Process multiple documents in batch mode\n   - `sources`: List of URLs or file paths to documents (required)\n   - `enable_ocr`: Whether to enable OCR for scanned documents (optional, default: false)\n   - `ocr_language`: List of language codes for OCR (optional)\n\n5. **qna_from_document**: Create a Q&A document from a URL or local path to YAML format\n   - `source`: URL or local file path to the document (required)\n   - `no_of_qnas`: Number of expected Q&As (optional, default: 5)\n   - **Note**: This tool requires IBM Watson X credentials to be set as environment variables:\n     - `WATSONX_PROJECT_ID`: Your Watson X project ID\n     - `WATSONX_APIKEY`: Your IBM Cloud API key\n     - `WATSONX_URL`: The Watson X API URL (default: https://us-south.ml.cloud.ibm.com)\n\n6. **get_system_info**: Get information about system configuration and acceleration status\n\n## Example with Llama Stack\n\n\nhttps://github.com/user-attachments/assets/8ad34e50-cbf7-4ec8-aedd-71c42a5de0a1\n\n\nYou can use this server with [Llama Stack](https://github.com/meta-llama/llama-stack) to provide document processing capabilities to your LLM applications. Make sure you have a running Llama Stack server, then configure your `INFERENCE_MODEL`\n\n```python\nfrom llama_stack_client.lib.agents.agent import Agent\nfrom llama_stack_client.lib.agents.event_logger import EventLogger\nfrom llama_stack_client.types.agent_create_params import AgentConfig\nfrom llama_stack_client.types.shared_params.url import URL\nfrom llama_stack_client import LlamaStackClient\nimport os\n\n# Set your model ID\nmodel_id = os.environ[\"INFERENCE_MODEL\"]\nclient = LlamaStackClient(\n    base_url=f\"http://localhost:{os.environ.get('LLAMA_STACK_PORT', '8080')}\"\n)\n\n# Register MCP tools\nclient.toolgroups.register(\n    toolgroup_id=\"mcp::docling\",\n    provider_id=\"model-context-protocol\",\n    mcp_endpoint=URL(uri=\"http://0.0.0.0:8000/sse\"))\n\n# Define an agent with MCP toolgroup\nagent_config = AgentConfig(\n    model=model_id,\n    instructions=\"\"\"You are a helpful assistant with access to tools to manipulate documents.\nAlways use the appropriate tool when asked to process documents.\"\"\",\n    toolgroups=[\"mcp::docling\"],\n    tool_choice=\"auto\",\n    max_tool_calls=3,\n)\n\n# Create the agent\nagent = Agent(client, agent_config)\n\n# Create a session\nsession_id = agent.create_session(\"test-session\")\n\ndef _summary_and_qna(source: str):\n    # Define the prompt\n    run_turn(f\"Please convert the document at {source} to markdown and summarize its content.\")\n    run_turn(f\"Please generate a Q&A document with 3 items for source at {source} and display it in YAML format.\")\n\ndef _run_turn(prompt):\n    # Create a turn\n    response = agent.create_turn(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        session_id=session_id,\n    )\n\n    # Log the response\n    for log in EventLogger().log(response):\n        log.print()\n\n_summary_and_qna('https://arxiv.org/pdf/2004.07606')\n```\n\n## Caching\n\nThe server caches processed documents in `~/.cache/mcp-docling/` to improve performance for repeated requests.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "docling",
        "document",
        "markdown",
        "document processing",
        "docling convert",
        "convert documents"
      ],
      "category": "document-processing"
    },
    "zcaceres--markdownify-mcp": {
      "owner": "zcaceres",
      "name": "markdownify-mcp",
      "url": "https://github.com/zcaceres/markdownify-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zcaceres.webp",
      "description": "Converts various file types and web content into Markdown format, supporting multiple input types such as PDFs, images, and audio files.",
      "stars": 2168,
      "forks": 178,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T16:23:43Z",
      "readme_content": "# Markdownify MCP Server\n\n> Help! I need someone with a Windows computer to help me add support for Markdownify-MCP on Windows. PRs exist but I cannot test them. Post [here](https://github.com/zcaceres/markdownify-mcp/issues/18) if interested.\n\n![markdownify mcp logo](logo.jpg)\n\nMarkdownify is a Model Context Protocol (MCP) server that converts various file types and web content to Markdown format. It provides a set of tools to transform PDFs, images, audio files, web pages, and more into easily readable and shareable Markdown text.\n\n<a href=\"https://glama.ai/mcp/servers/bn5q4b0ett\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/bn5q4b0ett/badge\" alt=\"Markdownify Server MCP server\" /></a>\n\n## Features\n\n- Convert multiple file types to Markdown:\n  - PDF\n  - Images\n  - Audio (with transcription)\n  - DOCX\n  - XLSX\n  - PPTX\n- Convert web content to Markdown:\n  - YouTube video transcripts\n  - Bing search results\n  - General web pages\n- Retrieve existing Markdown files\n\n## Getting Started\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pnpm install\n   ```\n\nNote: this will also install `uv` and related Python depdencies.\n\n3. Build the project:\n   ```\n   pnpm run build\n   ```\n4. Start the server:\n   ```\n   pnpm start\n   ```\n\n## Development\n\n- Use `pnpm run dev` to start the TypeScript compiler in watch mode\n- Modify `src/server.ts` to customize server behavior\n- Add or modify tools in `src/tools.ts`\n\n## Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```js\n{\n  \"mcpServers\": {\n    \"markdownify\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"{ABSOLUTE PATH TO FILE HERE}/dist/index.js\"\n      ],\n      \"env\": {\n        // By default, the server will use the default install location of `uv`\n        \"UV_PATH\": \"/path/to/uv\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n- `youtube-to-markdown`: Convert YouTube videos to Markdown\n- `pdf-to-markdown`: Convert PDF files to Markdown\n- `bing-search-to-markdown`: Convert Bing search results to Markdown\n- `webpage-to-markdown`: Convert web pages to Markdown\n- `image-to-markdown`: Convert images to Markdown with metadata\n- `audio-to-markdown`: Convert audio files to Markdown with transcription\n- `docx-to-markdown`: Convert DOCX files to Markdown\n- `xlsx-to-markdown`: Convert XLSX files to Markdown\n- `pptx-to-markdown`: Convert PPTX files to Markdown\n- `get-markdown-file`: Retrieve an existing Markdown file. File extension must end with: *.md, *.markdown.\n  \n  OPTIONAL: set `MD_SHARE_DIR` env var to restrict the directory from which files can be retrieved, e.g. `MD_SHARE_DIR=[SOME_PATH] pnpm run start` \n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdownify",
        "markdown",
        "mcp",
        "markdownify mcp",
        "zcaceres markdownify",
        "content markdown"
      ],
      "category": "document-processing"
    },
    "zentala--zntl-mcp-server": {
      "owner": "zentala",
      "name": "zntl-mcp-server",
      "url": "https://github.com/zentala/zntl-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Provides AI-powered transcription and analysis functionalities via a standardized Model Context Protocol interface, enabling efficient data searching, summarizing, and retrieval. Integrates with the Transcripter project to facilitate interaction with transcription and analysis data.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcripter",
        "transcription",
        "zntl",
        "transcription analysis",
        "transcripter project",
        "integrates transcripter"
      ],
      "category": "document-processing"
    },
    "zeyangxu--local-rag": {
      "owner": "zeyangxu",
      "name": "local-rag",
      "url": "https://github.com/zeyangxu/local-rag",
      "imageUrl": "/freedevtools/mcp/pfp/zeyangxu.webp",
      "description": "Access and query information from large PDF files using a powerful retrieval-augmented generation (RAG) system that integrates with Claude. Utilize advanced document processing and vector storage to enhance data retrieval capabilities.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-02T02:13:20Z",
      "readme_content": "# PDF RAG System with MCP Server\n\nThis project implements a Retrieval-Augmented Generation (RAG) system with an MCP server that allows Claude to access and query information from large PDF files. It uses Chroma as the vector database.\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- npm (v6 or higher)\n- Python 3.9+ with ChromaDB installed\n- OpenAI API key (for embeddings)\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Install Python dependencies:\n   ```\n   python3 -m pip install chromadb\n   ```\n4. Configure environment variables by editing the `.env` file:\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   PORT=3000  # Port for the MCP server\n   ```\n\n## Usage\n\n### 1. Add PDF Files\n\nPlace your PDF files in the `data/pdfs` directory:\n\n```\ndata/\n  pdfs/\n    your-file1.pdf\n    your-file2.pdf\n```\n\n### 2. Start the Chroma Server\n\nStart the Chroma database server:\n\n```\n./start-chroma.sh\n```\n\nOr manually:\n\n```\npython3 -m chromadb.cli.cli run --path ./data/chroma_db\n```\n\nThis will start a Chroma server at http://localhost:8000.\n\n### 3. Ingest PDFs\n\nIn a new terminal, process the PDFs and create the vector store:\n\n```\nnpm run ingest\n```\n\nThis will:\n- Extract text from the PDFs\n- Split the text into chunks\n- Create embeddings\n- Store the vectors in a Chroma database\n\n### 4. Start the MCP Server\n\nIn another terminal, start the server:\n\n```\nnpm run dev\n```\n\nThe MCP server will be available at: http://localhost:3000/api/mcp/query\n\n### 5. Query the MCP Server\n\nYou can query the system using Claude or a REST client:\n\n```\nPOST http://localhost:3000/api/mcp/query\nContent-Type: application/json\n\n{\n  \"query\": \"What does the document say about...\",\n  \"topK\": 5  # Optional, number of results to return\n}\n```\n\n## Claude Integration\n\nTo use this with Claude via MCP:\n\n1. Configure Claude to use the MCP endpoint\n2. Ensure Claude has access to this server\n3. Now Claude can query the content of your PDFs through the RAG system\n\n## Project Structure\n\n- `src/`\n  - `index.ts` - Main server file\n  - `ingest.ts` - Script for processing PDFs\n  - `services/`\n    - `documentProcessor.ts` - PDF processing and Chroma database operations\n    - `mcpService.ts` - MCP service for Claude\n  - `routes/`\n    - `mcpRoutes.ts` - API routes for MCP\n  - `utils/`\n    - `env.ts` - Environment variable utilities\n- `data/`\n  - `pdfs/` - Directory for PDF files\n  - `chroma_db/` - Directory for Chroma vector database\n- `start-chroma.sh` - Script to start the Chroma server\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "rag",
        "document",
        "document processing",
        "data retrieval",
        "retrieval capabilities"
      ],
      "category": "document-processing"
    },
    "zhiwei5576--excel-mcp-server": {
      "owner": "zhiwei5576",
      "name": "excel-mcp-server",
      "url": "https://github.com/zhiwei5576/excel-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zhiwei5576.webp",
      "description": "Read, write, and analyze Excel files while seamlessly managing data through various functionalities, including accessing multiple worksheets and exporting structure information.",
      "stars": 41,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-06T02:32:45Z",
      "readme_content": "# Excel MCP Server\n[![npm](https://img.shields.io/npm/v/@zhiweixu/excel-mcp-server)](https://www.npmjs.com/package/@zhiweixu/excel-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@zhiwei5576/excel-mcp-server)](https://smithery.ai/server/@zhiwei5576/excel-mcp-server)\n[简体中文](./README_CN.md) | English\n\nExcel file processing server based on Model Context Protocol (MCP), providing functionalities for reading, writing, and analyzing Excel files.\n\n## Features\n\n- 📖 Read Excel Files\n\n  - Get worksheet list\n  - Read specific worksheet data\n  - Read all worksheets data\n\n- ✍️ Write Excel Files\n\n  - Create new Excel files\n  - Write to specific worksheet\n  - Support multiple worksheets\n\n- 🔍 Analyze Excel Structure\n\n  - Analyze worksheet structure\n  - Export structure to new file\n\n- 💾 Cache Management\n\n  - Automatic file content caching\n  - Scheduled cache cleanup\n  - Manual cache clearing\n\n- 📝 Log Management\n  - Automatic operation logging\n  - Periodic log cleanup\n\n## Installation\n\n### Installing via Smithery\n\nTo install excel-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zhiwei5576/excel-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @zhiwei5576/excel-mcp-server --client claude\n```\n\n### Installing Manually\nInstalling via NPM\nexcel-mcp-server can be automatically installed by adding the following configuration to the MCP servers configuration.\n\nWindows Platform:\n\n```bash\n{\n  \"mcpServers\": {\n    \"excel\": {\n        \"command\": \"cmd\",\n        \"args\": [\"/c\", \"npx\", \"--yes\", \"@zhiweixu/excel-mcp-server\"],\n        \"env\": {\n            \"LOG_PATH\": \"[set an accessible absolute path]\",\n            \"CACHE_MAX_AGE\": \"1\",\n            \"CACHE_CLEANUP_INTERVAL\": \"4\",\n            \"LOG_RETENTION_DAYS\": \"7\",\n            \"LOG_CLEANUP_INTERVAL\": \"24\"\n        }\n    }\n}\n```\n\nOther Platforms:\n\n```bash\n{\n  \"mcpServers\": {\n    \"excel\": {\n        \"command\": \"npx\",\n        \"args\": [\"--yes\", \"@zhiweixu/excel-mcp-server\"],\n        \"env\": {\n            \"LOG_PATH\": \"[set an accessible absolute path]\",\n            \"CACHE_MAX_AGE\": \"1\",\n            \"CACHE_CLEANUP_INTERVAL\": \"4\",\n            \"LOG_RETENTION_DAYS\": \"7\",\n            \"LOG_CLEANUP_INTERVAL\": \"24\"\n        }\n    }\n}\n```\nNote: LOG_PATH is optional. If not set, logs will be stored in the 'logs' folder under the application root directory.other arguments are optional.\n\n## API Tools\n\n### Structure Tools\n\n1. analyzeExcelStructure\n   - Function: Get Excel file structure including sheet list and column headers in JSON format\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file\n     - headerRows: Number of header rows (default: 1)\n\n2. exportExcelStructure\n   - Function: Export Excel file structure (sheets and headers) to a new Excel template file\n   - Parameters:\n     - sourceFilePath: Source Excel file path\n     - targetFilePath: Target Excel file path\n     - headerRows: Number of header rows (default: 1)\n\n### Read Tools\n\n1. readSheetNames\n   - Function: Get all sheet names from the Excel file\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file\n\n2. readDataBySheetName\n   - Function: Get data from a specific sheet in the Excel file\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file\n     - sheetName: Name of the sheet to read\n     - headerRow: Header row number (default: 1)\n     - dataStartRow: Data start row number (default: 2)\n\n3. readSheetData\n   - Function: Get data from all sheets in the Excel file\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file\n     - headerRow: Header row number (default: 1)\n     - dataStartRow: Data start row number (default: 2)\n\n### Write Tools\n\n1. writeDataBySheetName\n   - Function: Write data to a specific sheet in the Excel file (overwrites if sheet exists)\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file\n     - sheetName: Name of the sheet to write\n     - data: Array of data to write\n\n2. writeSheetData\n   - Function: Create a new Excel file with provided data\n   - Parameters:\n     - fileAbsolutePath: Absolute path for the new Excel file\n     - data: Object containing multiple sheet data\n\n### Cache Tools\n\n1. clearFileCache\n   - Function: Clear cached data for the specified Excel file\n   - Parameters:\n     - fileAbsolutePath: Absolute path of the Excel file to clear from cache\n\n## Configuration\n\n### Environment Variables\n\n- `LOG_PATH`: Log files storage path\n  - Optional\n  - Default: 'logs' folder under application root directory\n\n- `CACHE_MAX_AGE`: Cache expiration time (hours)\n  - Optional\n  - Default: 1\n\n- `CACHE_CLEANUP_INTERVAL`: Cache cleanup interval (hours)\n  - Optional\n  - Default: 4\n\n- `LOG_RETENTION_DAYS`: Log retention days\n  - Optional\n  - Default: 7\n\n- `LOG_CLEANUP_INTERVAL`: Log cleanup interval (hours)\n  - Optional\n  - Default: 24\n\n### Default Configuration\n\n- Cache Configuration\n  - Cache expiration time: 1 hour\n  - Cache cleanup interval: 4 hours\n\n- Log Configuration\n  - Log retention days: 7 days\n  - Cleanup interval: 24 hours\n\n## Dependencies\n\n- @modelcontextprotocol/sdk: ^1.7.0\n- xlsx: ^0.18.5\n- typescript: ^5.8.2\n\n## Development Dependencies\n\n- @types/node: ^22.13.10\n- nodemon: ^3.1.9\n- ts-node: ^10.9.2\n\n## License\n\nThis project is licensed under the MIT License. This means you are free to:\n\n- Use the software for commercial or non-commercial purposes\n- Modify the source code\n- Distribute original or modified code\n  Requirements:\n\n- Retain the original copyright notice\n- No liability can be claimed against the authors for software use\n  For detailed license information,please see the [LICENSE](./LICENSE) file.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "excel",
        "worksheets",
        "exporting",
        "zhiwei5576 excel",
        "excel files",
        "document processing"
      ],
      "category": "document-processing"
    },
    "zhuangmanhong--PDFMathTranslate": {
      "owner": "zhuangmanhong",
      "name": "PDFMathTranslate",
      "url": "https://github.com/zhuangmanhong/PDFMathTranslate",
      "imageUrl": "/freedevtools/mcp/pfp/zhuangmanhong.webp",
      "description": "Translate PDF scientific papers while maintaining the integrity of formulas, charts, and annotations. Supports multiple languages and various translation services through a command-line interface, interactive GUI, or Docker deployment.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2025-03-17T15:52:34Z",
      "readme_content": "<div align=\"center\">\n\nEnglish | [简体中文](docs/README_zh-CN.md) | [繁體中文](docs/README_zh-TW.md) | [日本語](docs/README_ja-JP.md) | [한국어](docs/README_ko-KR.md)\n\n<img src=\"./docs/images/banner.png\" width=\"320px\"  alt=\"PDF2ZH\"/>\n\n<h2 id=\"title\">PDFMathTranslate</h2>\n\n<p>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/pdf2zh/\">\n    <img src=\"https://img.shields.io/pypi/v/pdf2zh\"></a>\n  <a href=\"https://pepy.tech/projects/pdf2zh\">\n    <img src=\"https://static.pepy.tech/badge/pdf2zh\"></a>\n  <a href=\"https://hub.docker.com/repository/docker/byaidu/pdf2zh\">\n    <img src=\"https://img.shields.io/docker/pulls/byaidu/pdf2zh\"></a>\n  <a href=\"https://gitcode.com/Byaidu/PDFMathTranslate/overview\">\n    <img src=\"https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg\"></a>\n  <a href=\"https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D\"></a>\n  <a href=\"https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate\">\n    <img src=\"https://img.shields.io/badge/ModelScope-Demo-blue\"></a>\n  <a href=\"https://github.com/Byaidu/PDFMathTranslate/pulls\">\n    <img src=\"https://img.shields.io/badge/contributions-welcome-green\"></a>\n  <a href=\"https://t.me/+Z9_SgnxmsmA5NzBl\">\n    <img src=\"https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&logo=telegram&logoColor=white\"></a>\n  <!-- License -->\n  <a href=\"./LICENSE\">\n    <img src=\"https://img.shields.io/github/license/Byaidu/PDFMathTranslate\"></a>\n</p>\n\n<a href=\"https://trendshift.io/repositories/12424\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/12424\" alt=\"Byaidu%2FPDFMathTranslate | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\nPDF scientific paper translation and bilingual comparison.\n\n- 📊 Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.\n- 🌐 Support [multiple languages](#language), and diverse [translation services](#services).\n- 🤖 Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)\n\nFeel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).\n\nFor details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).\n\n<h2 id=\"updates\">Updates</h2>\n\n- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))\n- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))\n- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_\n- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_\n- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_\n- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_\n\n<h2 id=\"preview\">Preview</h2>\n\n<div align=\"center\">\n<img src=\"./docs/images/preview.gif\" width=\"80%\"/>\n</div>\n\n<h2 id=\"demo\">Online Service 🌟</h2>\n\nYou can try our application out using either of the following demos:\n\n- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.\n- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_\n- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)\n- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.\n\nNote that the computing resources of the demo are limited, so please avoid abusing them.\n\n<h2 id=\"install\">Installation and Usage</h2>\n\n### Methods\n\nFor different use cases, we provide distinct methods to use our program:\n\n<details open>\n  <summary>1. UV install</summary>\n\n1. Python installed (3.10 <= version <= 3.12)\n2. Install our package:\n\n   ```bash\n   pip install uv\n   uv tool install --python 3.12 pdf2zh\n   ```\n\n3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):\n\n   ```bash\n   pdf2zh document.pdf\n   ```\n\n</details>\n\n<details>\n  <summary>2. Windows exe</summary>\n\n1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)\n\n2. Unzip and double-click `pdf2zh.exe` to run.\n\n</details>\n\n<details>\n  <summary>3. Graphic user interface</summary>\n1. Python installed (3.10 <= version <= 3.12)\n2. Install our package:\n\n```bash\npip install pdf2zh\n```\n\n3. Start using in browser:\n\n   ```bash\n   pdf2zh -i\n   ```\n\n4. If your browswer has not been started automatically, goto\n\n   ```bash\n   http://localhost:7860/\n   ```\n\n   <img src=\"./docs/images/gui.gif\" width=\"500\"/>\n\nSee [documentation for GUI](./docs/README_GUI.md) for more details.\n\n</details>\n\n<details>\n  <summary>4. Docker</summary>\n\n1. Pull and run:\n\n   ```bash\n   docker pull byaidu/pdf2zh\n   docker run -d -p 7860:7860 byaidu/pdf2zh\n   ```\n\n2. Open in browser:\n\n   ```\n   http://localhost:7860/\n   ```\n\nFor docker deployment on cloud service:\n\n<div>\n<a href=\"https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate\">\n  <img src=\"https://www.herokucdn.com/deploy/button.svg\" alt=\"Deploy\" height=\"26\"></a>\n<a href=\"https://render.com/deploy\">\n  <img src=\"https://render.com/images/deploy-to-render-button.svg\" alt=\"Deploy to Koyeb\" height=\"26\"></a>\n<a href=\"https://zeabur.com/templates/5FQIGX?referralCode=reycn\">\n  <img src=\"https://zeabur.com/button.svg\" alt=\"Deploy on Zeabur\" height=\"26\"></a>\n<a href=\"https://app.koyeb.com/deploy?type=git&builder=buildpack&repository=github.com/Byaidu/PDFMathTranslate&branch=main&name=pdf-math-translate\">\n  <img src=\"https://www.koyeb.com/static/images/deploy/button.svg\" alt=\"Deploy to Koyeb\" height=\"26\"></a>\n</div>\n\n</details>\n\n<details>\n  <summary>5. Zotero Plugin</summary>\n\n\nSee [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.\n\n</details>\n\n<details>\n  <summary>6. Commandline</summary>\n\n1. Python installed (3.10 <= version <= 3.12)\n2. Install our package:\n\n   ```bash\n   pip install pdf2zh\n   ```\n\n3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):\n\n   ```bash\n   pdf2zh document.pdf\n   ```\n\n</details>\n\n> [!TIP]\n>\n> - If you're using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.\n>\n> - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).\n> ```bash\n> docker pull ghcr.io/byaidu/pdfmathtranslate\n> docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate\n> ```\n\n### Unable to install?\n\nThe present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:\n\n```shell\nset HF_ENDPOINT=https://hf-mirror.com\n```\n\nFor PowerShell user:\n\n```shell\n$env:HF_ENDPOINT = https://hf-mirror.com\n```\n\nIf the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).\n\n<h2 id=\"usage\">Advanced Options</h2>\n\nExecute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).\n\n<img src=\"./docs/images/cmd.explained.png\" width=\"580px\"  alt=\"cmd\"/>\n\nIn the following table, we list all advanced options for reference:\n\n| Option         | Function                                                                                                      | Example                                        |\n| -------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |\n| files          | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |\n| links          | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |\n| `-i`           | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |\n| `-p`           | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |\n| `-li`          | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |\n| `-lo`          | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |\n| `-s`           | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |\n| `-t`           | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |\n| `-o`           | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |\n| `-f`, `-c`     | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f \"(MS.*)\"`               |\n| `-cp`          | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |\n| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)  | `pdf2zh example.pdf --skip-subset-fonts`       |\n| `--ignore-cache` | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)       | `pdf2zh example.pdf --ignore-cache`            |\n| `--share`      | Public link                                                                                                   | `pdf2zh -i --share`                            |\n| `--authorized` | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |\n| `--prompt`     | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |\n| `--onnx`       | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |\n| `--serverport` | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |\n| `--dir`        | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |\n| `--config`     | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |\n| `--serverport` | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |\n|`--babeldoc`| Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate |`pdf2zh --babeldoc` -s openai example.pdf|\n\nFor detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.\n\n<h2 id=\"downstream\">Secondary Development (APIs)</h2>\n\nThe current pdf2zh API is temporarily deprecated. The API will be provided again after [pdf2zh 2.0](https://github.com/Byaidu/PDFMathTranslate/issues/586) is released. For users who need programmatic access, please use the `babeldoc.high_level.async_translate` function of [BabelDOC](https://github.com/funstory-ai/BabelDOC).\n\nThis API being temporarily deprecated means: the relevant code will not be removed for now, but no technical support will be provided, and no bug fixes will be made.\n<!-- For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for futher information about:\n\n- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs\n- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed -->\n\n<h2 id=\"todo\">TODOs</h2>\n\n- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)\n\n- [ ] Fix page rotation, table of contents, format of lists\n\n- [ ] Fix pixel formula in old papers\n\n- [ ] Async retry except KeyboardInterrupt\n\n- [ ] Knuth–Plass algorithm for western languages\n\n- [ ] Support non-PDF/A files\n\n- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)\n\n<h2 id=\"acknowledgement\">Acknowledgements</h2>\n\n- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)\n\n- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)\n\n- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)\n\n- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)\n\n- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)\n\n- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)\n\n- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)\n\n- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)\n\n- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)\n\n- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)\n\n<h2 id=\"contrib\">Contributors</h2>\n\n<a href=\"https://github.com/Byaidu/PDFMathTranslate/graphs/contributors\">\n  <img src=\"https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&button=false\" />\n</a>\n\n![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg \"Repobeats analytics image\")\n\n<h2 id=\"star_hist\">Star History</h2>\n\n<a href=\"https://star-history.com/#Byaidu/PDFMathTranslate&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&type=Date\"/>\n </picture>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfmathtranslate",
        "zhuangmanhong",
        "pdf",
        "zhuangmanhong pdfmathtranslate",
        "processing zhuangmanhong",
        "translate pdf"
      ],
      "category": "document-processing"
    },
    "zw459123678--tuniao-server": {
      "owner": "zw459123678",
      "name": "tuniao-server",
      "url": "https://github.com/zw459123678/tuniao-server",
      "imageUrl": "/freedevtools/mcp/pfp/zw459123678.webp",
      "description": "Provides access to TuNiao UI components documentation and listings via the Model Context Protocol. Features include retrieving component information and detailed documentation for specific components.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-18T15:47:34Z",
      "readme_content": "# Tuniao UI MCP Server\n\nA Model Context Protocol (MCP) server that provides access to TuNiao UI components documentation and listings.\n\n## Features\n\n- Component information from [Tuniao UI](https://vue2.tuniaokj.com/)\n- MCP protocol compatible\n- Easy integration with AI models\n- Markdown formatted output with clickable links\n- Comprehensive component documentation\n\n## Available Tools\n\n- `get_component_list`\n  - Gets a list of available TuNiao UI components\n  \n- `get_component_doc`\n  - Gets detailed documentation for a specific TuNiao UI component\n\n## Installation\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-tuniao\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@zw459123678/tuniao-server\"\n      ]\n    }\n  }\n}\n```\n\n### Docker \n（ Docker image not uploaded to Docker Hub, need to build it yourself. ）\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-tuniao\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"zw459123678/tuniao-server\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Watch mode\nnpm run watch\n\n# Build\nnpm run build\n\n# Test components\nnpm run test:comp\n```\n\nDocker build:\n\n```bash\ndocker build -t zw459123678/tuniao-server:latest -f Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tuniao",
        "documentation",
        "document",
        "tuniao ui",
        "tuniao server",
        "access tuniao"
      ],
      "category": "document-processing"
    }
  }
}