{
  "category": "web-search",
  "categoryDisplay": "Web Search",
  "description": "",
  "totalRepositories": 266,
  "repositories": {
    "BjornMelin--crawl4ai-mcp-server": {
      "owner": "BjornMelin",
      "name": "crawl4ai-mcp-server",
      "url": "https://github.com/BjornMelin/crawl4ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/BjornMelin.webp",
      "description": "Enables high-performance web scraping, crawling, and deep research through a Model Context Protocol server, offering structured data extraction and asynchronous crawling capabilities. Features secure authentication methods and integration with web research tools via CloudFlare Workers for AI-driven data gathering.",
      "stars": 18,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T08:01:18Z",
      "readme_content": "# ⚠️ NOTICE\n\n> **MCP SERVER CURRENTLY UNDER DEVELOPMENT**  \n> **NOT READY FOR PRODUCTION USE**  \n> **WILL UPDATE WHEN OPERATIONAL**\n\n# Crawl4AI MCP Server\n\n🚀 High-performance MCP Server for Crawl4AI - Enable AI assistants to access web scraping, crawling, and deep research via Model Context Protocol. Faster and more efficient than FireCrawl!\n\n## Overview\n\nThis project implements a custom Model Context Protocol (MCP) Server that integrates with Crawl4AI, an open-source web scraping and crawling library. The server is deployed as a remote MCP server on CloudFlare Workers, allowing AI assistants like Claude to access Crawl4AI's powerful web scraping capabilities.\n\n## Documentation\n\nFor comprehensive details about this project, please refer to the following documentation:\n\n- [Migration Plan](docs/MIGRATION_PLAN.md) - Detailed plan for migrating from Firecrawl to Crawl4AI\n- [Enhanced Architecture](docs/ENHANCED_ARCHITECTURE.md) - Multi-tenant architecture with cloud provider flexibility\n- [Implementation Guide](docs/IMPLEMENTATION_GUIDE.md) - Technical implementation details and code examples\n- [Codebase Simplification](docs/SIMPLIFICATION.md) - Details on code simplification and best practices implemented\n- [Docker Setup Guide](docs/DOCKER.md) - Instructions for Docker setup for local development and production\n\n## Features\n\n### Web Data Acquisition\n\n- 🌐 **Single Webpage Scraping**: Extract content from individual webpages\n- 🕸️ **Web Crawling**: Crawl websites with configurable depth and page limits\n- 🗺️ **URL Discovery**: Map and discover URLs from a starting point\n- 🕸️ **Asynchronous Crawling**: Crawl entire websites efficiently\n\n### Content Processing\n\n- 🔍 **Deep Research**: Conduct comprehensive research across multiple pages\n- 📊 **Structured Data Extraction**: Extract specific data using CSS selectors or LLM-based extraction\n- 🔎 **Content Search**: Search through previously crawled content\n\n### Integration & Security\n\n- 🔄 **MCP Integration**: Seamless integration with MCP clients (Claude Desktop, etc.)\n- 🔒 **OAuth Authentication**: Secure access with proper authorization\n- 🔒 **Authentication Options**: Secure access via OAuth or API key (Bearer token)\n- ⚡ **High Performance**: Optimized for speed and efficiency\n\n## Project Structure\n\n```plaintext\ncrawl4ai-mcp/\n├── src/\n│   ├── index.ts               # Main entry point with OAuth provider setup\n│   ├── auth-handler.ts        # Authentication handler\n│   ├── mcp-server.ts          # MCP server implementation\n│   ├── crawl4ai-adapter.ts    # Adapter for Crawl4AI API\n│   ├── tool-schemas/          # MCP tool schema definitions\n│   │   └── [...].ts           # Tool schemas\n│   ├── handlers/\n│   │   ├── crawl.ts           # Web crawling implementation\n│   │   ├── search.ts          # Search functionality\n│   │   └── extract.ts         # Content extraction\n│   └── utils/                 # Utility functions\n├── tests/                     # Test cases\n├── .github/                   # GitHub configuration\n├── wrangler.toml              # CloudFlare Workers configuration\n├── tsconfig.json              # TypeScript configuration\n├── package.json               # Node.js dependencies\n└── README.md                  # Project documentation\n```\n\n## Getting Started\n\n### Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [npm](https://www.npmjs.com/)\n- [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/) (CloudFlare Workers CLI)\n- A CloudFlare account\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/BjornMelin/crawl4ai-mcp-server.git\n   cd crawl4ai-mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Set up CloudFlare KV namespace:\n\n   ```bash\n   wrangler kv:namespace create CRAWL_DATA\n   ```\n\n4. Update `wrangler.toml` with the KV namespace ID:\n\n   ```toml\n   kv_namespaces = [\n     { binding = \"CRAWL_DATA\", id = \"your-namespace-id\" }\n   ]\n   ```\n\n## Development\n\n### Local Development\n\n#### Using NPM\n\n1. Start the development server:\n\n   ```bash\n   npm run dev\n   ```\n\n2. The server will be available at <http://localhost:8787>\n\n#### Using Docker\n\nYou can also use Docker for local development, which includes the Crawl4AI API and a debug UI:\n\n1. Set up environment variables:\n\n   ```bash\n   cp .env.example .env\n   # Edit .env file with your API key\n   ```\n\n2. Start the Docker development environment:\n\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Access the services:\n   - MCP Server: <http://localhost:8787>\n   - Crawl4AI UI: <http://localhost:3000>\n\nSee the [Docker Setup Guide](docs/DOCKER.md) for more details.\n\n### Testing\n\nThe project includes a comprehensive test suite using Jest. To run tests:\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests with watch mode during development\nnpm run test:watch\n\n# Run tests with coverage report\nnpm run test:coverage\n\n# Run only unit tests\nnpm run test:unit\n\n# Run only integration tests\nnpm run test:integration\n```\n\nWhen running in Docker:\n\n```bash\ndocker-compose exec mcp-server npm test\n```\n\n## Deployment\n\n1. Deploy to CloudFlare Workers:\n\n   ```bash\n   npm run deploy\n   ```\n\n2. Your server will be available at the CloudFlare Workers URL assigned to your deployed worker.\n\n## Usage with MCP Clients\n\nThis server implements the Model Context Protocol, allowing AI assistants to access its tools.\n\n### Authentication\n\n- Implement OAuth authentication with workers-oauth-provider\n- Add API key authentication using Bearer tokens\n- Create login page and token management\n\n### Connecting to an MCP Client\n\n1. Use the CloudFlare Workers URL assigned to your deployed worker\n2. In Claude Desktop or other MCP clients, add this server as a tool source\n\n### Available Tools\n\n- `crawl`: Crawl web pages from a starting URL\n- `getCrawl`: Retrieve crawl data by ID\n- `listCrawls`: List all crawls or filter by domain\n- `search`: Search indexed documents by query\n- `extract`: Extract structured content from a URL\n\n## Configuration\n\nThe server can be configured by modifying environment variables in `wrangler.toml`:\n\n- `MAX_CRAWL_DEPTH`: Maximum depth for web crawling (default: 3)\n- `MAX_CRAWL_PAGES`: Maximum pages to crawl (default: 100)\n- `API_VERSION`: API version string (default: \"v1\")\n- `OAUTH_CLIENT_ID`: OAuth client ID for authentication\n- `OAUTH_CLIENT_SECRET`: OAuth client secret for authentication\n\n## Roadmap\n\nThe project is being developed with these components in mind:\n\n1. **Project Setup and Configuration**: CloudFlare Worker setup, TypeScript configuration\n2. **MCP Server and Tool Schemas**: Implementation of MCP server with tool definitions\n3. **Crawl4AI Adapter**: Integration with the Crawl4AI functionality\n4. **OAuth Authentication**: Secure authentication implementation\n5. **Performance Optimizations**: Enhancing speed and reliability\n6. **Advanced Extraction Features**: Improving structured data extraction capabilities\n\n## Contributing\n\nContributions are welcome! Please check the open issues or create a new one before starting work on a feature or bug fix. See [Contributing Guidelines](CONTRIBUTING.md) for detailed guidelines.\n\n## Support\n\nIf you encounter issues or have questions:\n\n- Open an issue on the GitHub repository\n- Check the [Crawl4AI documentation](https://crawl4ai.com/docs)\n- Refer to the [Model Context Protocol specification](https://github.com/anthropics/model-context-protocol)\n\n## How to Cite\n\nIf you use Crawl4AI MCP Server in your research or projects, please cite it using the following BibTeX entry:\n\n```bibtex\n@software{crawl4ai_mcp_2025,\n  author = {Melin, Bjorn},\n  title = {Crawl4AI MCP Server: High-performance Web Crawling for AI Assistants},\n  url = {https://github.com/BjornMelin/crawl4ai-mcp-server},\n  version = {1.0.0},\n  year = {2025},\n  month = {5}\n}\n```\n\n## License\n\n[MIT](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawl4ai",
        "scraping",
        "crawling",
        "crawling capabilities",
        "scraping crawling",
        "crawl4ai mcp"
      ],
      "category": "web-search"
    },
    "BochaAI--bocha-search-mcp": {
      "owner": "BochaAI",
      "name": "bocha-search-mcp",
      "url": "https://github.com/BochaAI/bocha-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/BochaAI.webp",
      "description": "Provides access to high-quality world knowledge from billions of web pages and various ecological content sources through a powerful search engine interface. Supports natural language semantic search, offering rich, structured results and vertical domain cards for enhanced information retrieval across multiple domains.",
      "stars": 26,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T07:24:49Z",
      "readme_content": "# MCP Server 产品名称: 博查\n\n\n\n## 版本信息\nv1\n\n## 产品描述\n### 短描述\n博查是一个给AI用的搜索引擎，让你的AI应用从近百亿网页和生态内容源中获取高质量的世界知识，涵盖天气、新闻、百科、医疗、火车票、图片等多种领域。\n\n### 长描述\n博查是一个给AI用的搜索引擎，让你的AI应用从近百亿网页和生态内容源中获取高质量的世界知识，涵盖天气、新闻、百科、医疗、火车票、图片等多种领域。\n\n## 分类\n网页搜索\n\n## 标签\n搜索, 新闻, 天气, 百科\n\n## Tools\n### Tool1: Bocha Web Search\n#### 详细描述\n从博查搜索全网信息和网页链接，返回结果包括网页标题、网页URL、网页摘要、网站名称、网站图标、发布时间、图片链接等。\n\n#### 调试所需要的参数\n输入:\n  - query: 搜索词(必填)\n  - freshness: 搜索指定时间范围内的网页 (可选值 YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. 默认为 noLimit)\n  - count: 返回结果的条数 (1-50, 默认为 10)\n\n输出:\n  - 网页标题、网页链接、网页摘要、发布时间、网站名称\n\n### Tool2: Bocha AI Search\n#### 详细描述\n在博查网页搜索的基础上，AI识别搜索词语义并额外返回垂直领域内容的结构化模态卡，例如天气卡、日历卡、百科卡等几十种模态卡，在语义识别、搜索结果时效性、内容丰富性等方面更好。\n\n#### 调试所需要的参数\n输入:\n  - query: 搜索词(必填)\n  - freshness: 搜索指定时间范围内的网页 (可选值 YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. 默认为 noLimit)\n  - count: 返回结果的条数 (1-50, 默认为 10)\n\n输出:\n  - 网页标题、网页链接、网页摘要、发布时间、网站名称、模态卡\n\n## 可适配平台\n方舟, python, Claude, Cursor等\n\n## 服务开通链接\n您需要前往 [博查AI开放平台](https://open.bochaai.com)，登陆后获取 API KEY。\n\n## 鉴权方式\nAPI Key\n\n## 安装部署\n### 步骤一：下载代码至本地\n```bash\ngit clone git@github.com:BochaAI/bocha-search-mcp.git\n```\n\n### 步骤二: 在客户端中配置\n#### Claude Desktop\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n  \"mcpServers\": {\n    \"bocha-search-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/bocha-search-mcp\",\n        \"run\",\n        \"bocha-search-mcp\"\n      ],\n      \"env\": {\n        \"BOCHA_API_KEY\": \"sk-****\"\n      }\n    }\n  }\n  ```\n\n### 步骤三: 在客户端中使用\n\n\n### 步骤四: 调试本地服务（可选）\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/bocha-search-mcp run bocha-search-mcp\n```\n\n## 客户案例\n\n目前博查已经累计服务**3000+企业用户**和**20000+开发者用户**，并且成为**DeepSeek官方联网搜索供应方**以及**阿里、腾讯、字节官方推荐的搜索API**，目前**承接着国内60%以上AI应用的联网搜索请求**。\n\n博查的搜索内容源包括全网近百亿个网页，以及生态合作内容（含短视频、新闻、百科、天气、医疗、火车票、酒店、餐厅、景点、企业、学术等）。博查后续将会继续与各个平台在内容生态、智能体创作等方面开展共创合作，为博查用户的搜索问题提供丰富多彩的答案。\n\n## 常见问题\n\n### Bocha Web Search API服务可以提供什么样的能力?\nBocha Web Search 提供全网通用搜索能力。您可以从博查搜索全网信息和网页链接，返回结果包括网页标题、网页URL、网页摘要、网站名称、网站图标、发布时间、图片链接等，每次搜索结果返回的网页最多支持50条（count50）。\n\n传统搜索引擎使用的是关键字+竞价排名机制的搜索算法，搜索结果的目标不是直接为用户提供正确的答案，而是吸引用户点击以获得广告收入。\n\n博查是基于多模态混合搜索与语义排序技术的新一代搜索引擎，支持AI应用场景的自然语言搜索方式，同时搜索结果目标是提供干净、准确、高质量的答案。\n\n博查的语义排序技术基于Transformer架构，会根据搜索结果与用户问题的语义相关性进行排序。由于大模型同样是Transformer架构，通过判断上下文与用户问题的语义相关性进行取舍，因此最终大模型更加喜欢博查提供的搜索结果。\n\n目前博查的搜索效果是国内最接近Bing Search API的搜索引擎，由于Bing Search API数据会出海（无国内Region）、价格昂贵（15美元/千次）且不提供文本摘要（只有50-100字的snippet），国内很多企业客户都已经从Bing切换至博查。\n\n### Bocha AI Search API 服务可以提供什么样的能力？\nBocha AI Search 在博查 Web Search 的基础上，AI识别搜索词语义并额外返回垂直领域内容的结构化模态卡，例如天气卡、日历卡、百科卡等几十种模态卡，在语义识别、搜索结果时效性、内容丰富性等方面更好。\n\n目前支持的模态卡类型包括：天气、百科、医疗、万年历、火车、星座属相、贵金属、汇率、油价、手机、股票、汽车等。\n\n以股票信息为例，网页中一般无法获取到实时的股票数据，需要结构化模态卡来支撑。博查AI Search API可以在提供网页信息的基础上，额外输出股价的结构化数据模态卡，通过模态卡提供的结构化数据，可以进一步增强AI应用中用户对于时效性问题的回答准确性。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "search",
        "semantic",
        "semantic search",
        "bocha search",
        "search bochaai"
      ],
      "category": "web-search"
    },
    "BurtTheCoder--mcp-maigret": {
      "owner": "BurtTheCoder",
      "name": "mcp-maigret",
      "url": "https://github.com/BurtTheCoder/mcp-maigret",
      "imageUrl": "/freedevtools/mcp/pfp/BurtTheCoder.webp",
      "description": "MCP server for conducting OSINT username searches across various social networks and analyzing URLs to collect user account information from public sources.",
      "stars": 192,
      "forks": 28,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T03:23:35Z",
      "readme_content": "# Maigret MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-maigret)](https://smithery.ai/server/mcp-maigret)\n\nA Model Context Protocol (MCP) server for [maigret](https://github.com/soxoj/maigret), a powerful OSINT tool that collects user account information from various public sources. This server provides tools for searching usernames across social networks and analyzing URLs. It is designed to integrate seamlessly with MCP-compatible applications like [Claude Desktop](https://claude.ai).\n\n<a href=\"https://glama.ai/mcp/servers/knnpcz651x\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/knnpcz651x/badge\" alt=\"mcp-maigret MCP server\" /></a>\n\n\n## ⚠️ Warning\n\nThis tool is designed for legitimate OSINT research purposes. Please:\n- Only search for information that is publicly available\n- Respect privacy and data protection laws\n- Follow the terms of service of the platforms being searched\n- Use responsibly and ethically\n- Be aware that some sites may rate-limit or block automated searches\n\n## Requirements\n\n- Node.js (v18 or later)\n- Docker\n- macOS, Linux, or Windows with Docker Desktop installed\n- Write access to the reports directory\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install Maigret for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-maigret):\n\n```bash\nnpx -y @smithery/cli install mcp-maigret --client claude\n```\n\n### Installing Manually\n1. Install Docker:\n   - macOS: Install [Docker Desktop](https://www.docker.com/products/docker-desktop)\n   - Linux: Follow the [Docker Engine installation guide](https://docs.docker.com/engine/install/)\n\n2. Install the server globally via npm:\n```bash\nnpm install -g mcp-maigret\n```\n\n3. Create a reports directory:\n```bash\nmkdir -p /path/to/reports/directory\n```\n\n4. Add to your Claude Desktop configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"maigret\": {\n      \"command\": \"mcp-maigret\",\n      \"env\": {\n        \"MAIGRET_REPORTS_DIR\": \"/path/to/reports/directory\"\n      }\n    }\n  }\n}\n```\n\nConfiguration file location:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n5. Restart Claude Desktop\n\n## Alternative Setup (From Source)\n\nIf you prefer to run from source or need to modify the code:\n\n1. Clone and build:\n```bash\ngit clone <repository_url>\ncd mcp-maigret\nnpm install\nnpm run build\n```\n\n2. Add to your Claude Desktop configuration:\n```json\n{\n  \"mcpServers\": {\n    \"maigret\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-maigret/build/index.js\"],\n      \"env\": {\n        \"MAIGRET_REPORTS_DIR\": \"/path/to/reports/directory\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- **Username Search**: Search for a username across hundreds of social networks and websites\n- **URL Analysis**: Parse URLs to extract information and search for associated usernames\n- **Multiple Output Formats**: Support for txt, html, pdf, json, csv, and xmind formats\n- **Site Filtering**: Filter searches by site tags (e.g., photo, dating, us)\n- **Docker-based**: Reliable and consistent execution across environments\n\n## Tools\n\n### 1. Username Search Tool\n- Name: `search_username`\n- Description: Search for a username across social networks and sites\n- Parameters:\n  * `username` (required): Username to search for\n  * `format` (optional, default: \"pdf\"): Output format (txt, html, pdf, json, csv, xmind)\n  * `use_all_sites` (optional, default: false): Use all available sites instead of top 500\n  * `tags` (optional): Array of tags to filter sites (e.g., [\"photo\", \"dating\"])\n\nExample:\n```json\n{\n  \"username\": \"test_user123\",\n  \"format\": \"html\",\n  \"use_all_sites\": false,\n  \"tags\": [\"photo\"]\n}\n```\n\n### 2. URL Analysis Tool\n- Name: `parse_url`\n- Description: Parse a URL to extract information and search for associated usernames\n- Parameters:\n  * `url` (required): URL to analyze\n  * `format` (optional, default: \"pdf\"): Output format (txt, html, pdf, json, csv, xmind)\n\nExample:\n```json\n{\n  \"url\": \"https://example.com/profile\",\n  \"format\": \"txt\"\n}\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n1. Verify Docker is installed and running:\n```bash\ndocker --version\ndocker ps\n```\n\n2. Check Docker permissions:\n   - Ensure your user has permissions to run Docker commands\n   - On Linux, add your user to the docker group: `sudo usermod -aG docker $USER`\n\n### Reports Directory Issues\n\n1. Verify the reports directory:\n   - The directory specified in MAIGRET_REPORTS_DIR must exist\n   - Your user must have write permissions to this directory\n   - Check permissions: `ls -la /path/to/reports/directory`\n\n2. Common configuration mistakes:\n   - Missing MAIGRET_REPORTS_DIR environment variable\n   - Directory doesn't exist\n   - Incorrect permissions\n   - Trailing slashes in the path\n\n3. After fixing any issues:\n   - Save the configuration file\n   - Restart Claude Desktop\n\n## Error Messages\n\n- \"Docker is not installed or not running\": Install Docker and start the Docker daemon\n- \"MAIGRET_REPORTS_DIR environment variable must be set\": Add the environment variable to your configuration\n- \"Error creating reports directory\": Check directory permissions and path\n- \"Error executing maigret\": Check Docker logs and ensure the container has proper permissions\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-maigret",
      "npm_downloads": 5622,
      "keywords": [
        "searches",
        "search",
        "mcp",
        "username searches",
        "searches various",
        "web search"
      ],
      "category": "web-search"
    },
    "Cam10001110101--mcp-server-ollama-deep-researcher": {
      "owner": "Cam10001110101",
      "name": "mcp-server-ollama-deep-researcher",
      "url": "https://github.com/Cam10001110101/mcp-server-ollama-deep-researcher",
      "imageUrl": "/freedevtools/mcp/pfp/Cam10001110101.webp",
      "description": "Conduct in-depth research on any topic by generating web search queries, gathering results, summarizing findings, and providing a comprehensive markdown summary of all sources used.",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T09:03:38Z",
      "readme_content": "# Ollama Deep Researcher DXT Extension\n\n## Overview\n\n**Ollama Deep Researcher** is a Desktop Extension (DXT) that enables advanced topic research using web search and LLM synthesis, powered by a local MCP server. It supports configurable research parameters, status tracking, and resource access, and is designed for seamless integration with the DXT ecosystem.\n\n- **Research any topic** using web search APIs and LLMs (Ollama, DeepSeek, etc.)\n- **Configure** max research loops, LLM model, and search API\n- **Track status** of ongoing research\n- **Access research results** as resources via MCP protocol\n\n## Features\n\n- Implements the MCP protocol over stdio for local, secure operation\n- Defensive programming: error handling, timeouts, and validation\n- Logging and debugging via stderr\n- Compatible with DXT host environments\n\n## Directory Structure\n\n```\n.\n├── manifest.json         # DXT manifest (see MANIFEST.md for spec)\n├── src/\n│   ├── index.ts         # MCP server entrypoint (Node.js, stdio transport)\n│   └── assistant/       # Python research logic\n│       └── run_research.py\n├── README.md            # This documentation\n└── ...\n```\n\n## Installation & Setup\n\n1. **Clone the repository** and install dependencies:\n   ```sh\n   git clone <your-repo-url>\n   cd mcp-server-ollama-deep-researcher\n   npm install\n   ```\n\n2. **Install Python dependencies** for the assistant:\n   ```sh\n   cd src/assistant\n   pip install -r requirements.txt\n   # or use pyproject.toml/uv if preferred\n   ```\n\n3. **Set required environment variables** for web search APIs:\n   - For Tavily: `TAVILY_API_KEY`\n   - For Perplexity: `PERPLEXITY_API_KEY`\n   - Example:\n     ```sh\n     export TAVILY_API_KEY=your_tavily_key\n     export PERPLEXITY_API_KEY=your_perplexity_key\n     ```\n\n4. **Build the TypeScript server** (if needed):\n   ```sh\n   npm run build\n   ```\n\n5. **Run the extension locally for testing:**\n   ```sh\n   node dist/index.js\n   # Or use the DXT host to load the extension per DXT documentation\n   ```\n\n## Usage\n\n- **Research a topic:**\n  - Use the `research` tool with `{ \"topic\": \"Your subject\" }`\n- **Get research status:**\n  - Use the `get_status` tool\n- **Configure research parameters:**\n  - Use the `configure` tool with any of: `maxLoops`, `llmModel`, `searchApi`\n\n## Manifest\n\nSee `manifest.json` for the full DXT manifest, including tool schemas and resource templates. Follows [DXT MANIFEST.md](https://github.com/anthropics/dxt/blob/main/MANIFEST.md).\n\n## Logging & Debugging\n\n- All server logs and errors are output to `stderr` for debugging.\n- Research subprocesses are killed after 5 minutes to prevent hangs.\n- Invalid requests and configuration errors return clear, structured error messages.\n\n## Security & Best Practices\n\n- All tool schemas are validated before execution.\n- API keys are required for web search APIs and are never logged.\n- MCP protocol is used over stdio for local, secure communication.\n\n## Testing & Validation\n\n- Validate the extension by loading it in a DXT-compatible host.\n- Ensure all tool calls return valid, structured JSON responses.\n- Check that the manifest loads and the extension registers as a DXT.\n\n## Troubleshooting\n\n- **Missing API key:** Ensure `TAVILY_API_KEY` or `PERPLEXITY_API_KEY` is set in your environment.\n- **Python errors:** Check Python dependencies and logs in `stderr`.\n- **Timeouts:** Research subprocesses are limited to 5 minutes.\n\n## References\n\n- [DXT Architecture Overview](https://github.com/anthropics/dxt/blob/main/README.md)\n- [DXT Manifest Spec](https://github.com/anthropics/dxt/blob/main/MANIFEST.md)\n- [DXT Example Extensions](https://github.com/anthropics/dxt/tree/main/examples)\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk)\n\n---\n\n© 2025 Your Name or Organization. Licensed under MIT.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "search",
        "researcher",
        "web search",
        "deep researcher",
        "research topic"
      ],
      "category": "web-search"
    },
    "ChanMeng666--server-google-jobs": {
      "owner": "ChanMeng666",
      "name": "server-google-jobs",
      "url": "https://github.com/ChanMeng666/server-google-jobs",
      "imageUrl": "/freedevtools/mcp/pfp/ChanMeng666.webp",
      "description": "Provides capabilities to search for job listings using Google Jobs via SerpAPI integration, supporting multiple languages.",
      "stars": 13,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T20:57:09Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chanmeng666-server-google-jobs-badge.png)](https://mseep.ai/app/chanmeng666-server-google-jobs)\n\n<div align=\"center\">\n <h1><br/>Google Jobs MCP Server</h1>\n <img alt=\"TypeScript_007ACC_style_flat_logo_typescript_logoColor_white\" src=\"https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white\"/>\n <img alt=\"Node_js_43853D_style_flat_logo_node_js_logoColor_white\" src=\"https://img.shields.io/badge/Node.js-43853D?style=flat&logo=node.js&logoColor=white\"/>\n <img alt=\"MCP_Server_blue_style_flat\" src=\"https://img.shields.io/badge/MCP-Server-blue?style=flat\"/>\n <img alt=\"License_MIT_brightgreen_style_flat\" src=\"https://img.shields.io/badge/License-MIT-brightgreen?style=flat\"/>\n<a href=\"https://smithery.ai/server/@chanmeng666/google-jobs-server\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@chanmeng666/google-jobs-server\"></a>\n</div>\n\n<br/>\n\nA Model Context Protocol (MCP) server implementation that provides Google Jobs search capabilities via SerpAPI integration. Features multi-language support, flexible search parameters, and smart error handling.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/ChanMeng666/server-google-jobs)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chanmeng666-server-google-jobs-badge.png)](https://mseep.ai/app/chanmeng666-server-google-jobs)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/df679562-50ad-4615-ac3c-a509c27583f0)\n\n<a href=\"https://glama.ai/mcp/servers/bijbpfhrbx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/bijbpfhrbx/badge\" alt=\"Google Jobs Server MCP server\" />\n</a>\n\n<br/>\n\n[![👉Try It Now!👈](https://gradient-svg-generator.vercel.app/api/svg?text=%F0%9F%91%89Try%20It%20Now!%F0%9F%91%88&color=000000&height=60&gradientType=radial&duration=6s&color0=ffffff&template=pride-rainbow)](https://smithery.ai/server/@chanmeng666/google-jobs-server)\n\n<br/>\n\nhttps://github.com/user-attachments/assets/8f6739e1-7db7-4171-88b4-59c6290a4c72\n\n![屏幕截图 2024-12-31 183813](https://github.com/user-attachments/assets/fd02f916-7ba0-4d92-8970-79ccecdb1115)\n\n![屏幕截图 2024-12-31 183754](https://github.com/user-attachments/assets/22f497f5-381e-40d1-b082-d13d13239677)\n\n![屏幕截图 2024-12-31 180734](https://github.com/user-attachments/assets/19f74219-5059-4c49-95e9-3a1741d866d2)\n\n![屏幕截图 2024-12-31 182106](https://github.com/user-attachments/assets/5e88ec38-66cd-4f02-95b3-118007736dbd)\n\n\n# ✨ Features\n\n### 🌍 Multi-Language Support\nFull localization support for English, Chinese, Japanese and Korean with automatic language detection and fallback.\n\n### 🔍 Flexible Search Options\nComprehensive search parameters including:\n- Job title and keywords\n- Location with radius filtering\n- Employment type (full-time, part-time, etc.)\n- Salary range filters\n- Post date filtering\n- Results sorting\n\n### 💡 Smart Error Handling\n- Comprehensive input validation\n- Helpful error messages and suggestions\n- Automatic search refinement suggestions\n- Rate limit handling\n\n### 📊 Rich Job Details\n- Detailed job information formatting\n- Company benefits and highlights\n- Salary information when available\n- Direct application links\n- Job posting timestamps\n\n### 🔄 Advanced Features\n- Pagination support\n- Multiple sorting options\n- Geographic radius search\n- Employment type filtering\n\n# 🔑 SERP API Setup Guide\n\nBefore getting started, you'll need to obtain a SERP API key:\n\n1. Visit [SERP API website](https://serpapi.com/) and create an account\n\n2. After registration, go to your Dashboard:\n   - Locate the \"API Key\" section\n   - Copy your API key\n   - New users get 100 free API calls\n\n3. API Usage Details:\n   - Free tier: 100 searches per month\n   - Paid plans start at $50/month for 5000 searches\n   - Billing based on successful API calls\n   - Multiple payment methods: Credit Card, PayPal, etc.\n\n4. Usage Limits:\n   - Request Rate: 2 requests/second\n   - IP Restrictions: None\n   - Concurrent Requests: 5\n   - Response Cache Time: 1 hour\n\n# 👩‍🔧 Solution for MCP Servers Connection Issues with NVM/NPM\n\nClick to view my configuration solution 👉 https://github.com/modelcontextprotocol/servers/issues/76\n\n# 🚀 Quick Start\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Configure environment:\nModify your `claude_desktop_config.json` with the following content (adjust paths according to your system):\n```json\n{\n  \"google-jobs\": {\n    \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\n    \"args\": [\"D:\\\\github_repository\\\\path_to\\\\dist\\\\index.js\"],\n    \"env\": {\n      \"SERP_API_KEY\": \"your-api-key\"\n    }\n  }\n}\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Start the server:\n```bash\nnpm start\n```\n\n## Troubleshooting\n\n1. API Key Issues:\n- Verify key in configuration\n- Check key status in SERP API dashboard\n- Confirm key has remaining quota\n\n2. Search Issues:\n- Validate search parameters format\n- Check network connectivity\n- Verify country/language code support\n\n# 📦 Installation\n\n## Installing via Smithery\n\nTo install Google Jobs for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@chanmeng666/google-jobs-server):\n\n```bash\nnpx -y @smithery/cli install @chanmeng666/google-jobs-server --client claude\n```\n\n## Manual Installation\n\n<img alt=\"CB3837\" src=\"https://cdn.simpleicons.org/npm/CB3837\" height=\"14\"/> <a href=\"https://www.npmjs.com/package/@chanmeng666/google-jobs-server\">@chanmeng666/google-jobs-server</a>\n\n```bash\n# Using npm\nnpm i @chanmeng666/google-jobs-server\n# or\nnpm install @chanmeng666/google-jobs-server\n\n# Using yarn\nyarn add @chanmeng666/google-jobs-server\n\n# Using pnpm\npnpm add @chanmeng666/google-jobs-server\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/index.ts\n```\n# 💻 Tech Stack\n\n![TypeScript](https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=typescript&logoColor=white)\n![NodeJS](https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&logo=node.js&logoColor=white)\n![MCP](https://img.shields.io/badge/MCP-SDK-blue?style=for-the-badge)\n\n# 📖 API Documentation\n\nThe server implements the Model Context Protocol and exposes a job search tool with the following parameters:\n\n- `query`: Search query string (required)\n- `location`: Job location (optional)\n- `posted_age`: Post date filter (optional)\n- `employment_type`: Job type filter (optional)\n- `salary`: Salary range filter (optional)\n- `radius`: Geographic search radius (optional)\n- `hl`: Language code (optional)\n- `page`: Pagination number (optional)\n- `sort_by`: Sort order (optional)\n\n# 🔧 Development\n\n```bash\n# Run in development mode\nnpm run dev\n\n# Run type checking\nnpm run typecheck\n\n# Build for production\nnpm run build\n```\n\n# 📝 License\n\nThis project is [MIT licensed](./LICENSE).\n\n# 🙋‍♀ Author\n\nCreated and maintained by [Chan Meng](https://chanmeng.org/).\n[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=normal&logo=github&logoColor=white)](https://github.com/ChanMeng666)\n[![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=normal&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/chanmeng666/)\n\n<details>\n<summary>🤖 AI Assistant Integration & GEO Optimization</summary>\n\n## AI Assistant Usage Guide\n\nThis MCP server is optimized for AI assistant integration. AI assistants can use this tool through the following methods:\n\n### Quick Start for AI\n1. **Installation**: `npm install @chanmeng666/google-jobs-server`\n2. **Configuration**: Add server configuration to MCP client\n3. **Usage**: Call the `search_jobs` tool for job searching\n\n### Search Parameters for AI\n- `query`: Search keywords (required)\n- `location`: Job location\n- `posted_age`: Post date filter (today, 3days, week, month)\n- `employment_type`: Job type (FULLTIME, PARTTIME, CONTRACTOR, INTERN)\n- `salary`: Salary range (e.g., $50K+, $100K+)\n- `radius`: Search radius (e.g., 10mi, 20mi, 50mi)\n- `hl`: Language code (en, zh-CN, ja, ko)\n- `page`: Page number (10 results per page)\n- `sort_by`: Sort order (date, relevance, salary)\n\n### AI Use Cases\n- Help users find specific job positions\n- Provide multi-language job search capabilities\n- Filter job opportunities based on criteria\n- Retrieve detailed job information and application links\n\n### Error Handling for AI\nThe system provides intelligent error prompts and search suggestions to help optimize search parameters.\n\n## Structured Data for AI Crawlers\n\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"SoftwareApplication\",\n  \"name\": \"Google Jobs MCP Server\",\n  \"description\": \"A powerful MCP server for searching Google Jobs via SerpAPI with multi-language support\",\n  \"url\": \"https://github.com/ChanMeng666/server-google-jobs\",\n  \"applicationCategory\": \"DeveloperApplication\",\n  \"operatingSystem\": \"Node.js\",\n  \"programmingLanguage\": \"TypeScript\",\n  \"author\": {\n    \"@type\": \"Person\",\n    \"name\": \"Chan Meng\",\n    \"url\": \"https://chanmeng.org/\"\n  },\n  \"offers\": {\n    \"@type\": \"Offer\",\n    \"price\": \"0\",\n    \"priceCurrency\": \"USD\"\n  },\n  \"featureList\": [\n    \"Multi-language support (English, Chinese, Japanese, Korean)\",\n    \"Flexible search parameters\",\n    \"Smart error handling\",\n    \"Rich job details formatting\",\n    \"Pagination support\",\n    \"Geographic radius search\"\n  ],\n  \"softwareVersion\": \"1.0.0\",\n  \"datePublished\": \"2024-12-31\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"google-jobs\",\n    \"job-search\",\n    \"mcp-server\",\n    \"serpapi\",\n    \"typescript\",\n    \"careers\",\n    \"employment\"\n  ]\n}\n</script>\n\n## AI Crawler Instructions\n\n<script type=\"text/llms.txt\">\n## AI Assistant Usage Guide\n\nThis is a Google Jobs MCP Server project. AI assistants can use it in the following ways:\n\n### Quick Start\n1. Install: npm install @chanmeng666/google-jobs-server\n2. Configure: Add server configuration to MCP client\n3. Use: Call search_jobs tool for job searching\n\n### Search Parameter Guide\n- query: Search keywords (required)\n- location: Job location\n- posted_age: Post date filter (today, 3days, week, month)\n- employment_type: Job type (FULLTIME, PARTTIME, CONTRACTOR, INTERN)\n- salary: Salary range (e.g., $50K+, $100K+)\n- radius: Search radius (e.g., 10mi, 20mi, 50mi)\n- hl: Language code (en, zh-CN, ja, ko)\n- page: Page number (10 results per page)\n- sort_by: Sort order (date, relevance, salary)\n\n### Use Cases\n- Help users find specific job positions\n- Provide multi-language job search capabilities\n- Filter job opportunities based on criteria\n- Retrieve detailed job information and application links\n\n### Error Handling\nThe system provides intelligent error prompts and search suggestions to help optimize search parameters.\n</script>\n\n</details>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "google",
        "serpapi",
        "jobs",
        "google jobs",
        "jobs serpapi",
        "search job"
      ],
      "category": "web-search"
    },
    "Claw256--mcp-web-search": {
      "owner": "Claw256",
      "name": "mcp-web-search",
      "url": "https://github.com/Claw256/mcp-web-search",
      "imageUrl": "/freedevtools/mcp/pfp/Claw256.webp",
      "description": "Provides Google search capabilities with advanced filtering and allows viewing of web content, including markdown conversion. Features include rate limiting, caching, browser instance pooling, and bot detection avoidance.",
      "stars": 5,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-15T16:27:05Z",
      "readme_content": "# Web Search MCP Server\n\nAn MCP server that provides Google search capabilities and web content viewing with advanced bot detection avoidance.\n\n## Features\n\n- Google Custom Search with advanced filtering\n- Web content viewing with markdown conversion\n- Rate limiting and caching\n- Browser instance pooling\n- Bot detection avoidance using rebrowser-puppeteer\n\n## Prerequisites\n\n- Bun runtime v1.0 or higher\n- Google API credentials (API key and Search Engine ID)\n\n## Installation\n\n```bash\n# Install dependencies\nbun install\n\n# Build the TypeScript files\nbun run build\n```\n\n## Configuration\n\n### Cookie Setup\n\nFor authenticated site access, you'll need to:\n\n1. Install the [Get cookies.txt LOCALLY](https://chromewebstore.google.com/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc) Chrome extension\n2. Visit the sites you want to authenticate with and log in\n3. Use the extension to export your cookies in JSON format\n4. Store the exported cookies file in a secure location\n5. Set the `BROWSER_COOKIES_PATH` environment variable to the absolute path of your cookies file\n\n### MCP Server Configuration\n\nAdd the server configuration to your MCP settings file:\n\n- For Cline: `%APPDATA%\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\cline_mcp_settings.json`\n- For Claude Desktop:\n  - MacOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n  - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"bun\",\n      \"args\": [\n        \"run\",\n        \"/ABSOLUTE/PATH/TO/web_search_mcp/dist/index.js\"\n      ],\n      \"env\": {\n        \"GOOGLE_API_KEY\": \"your_api_key\",\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your_search_engine_id\",\n        \"MAX_CONCURRENT_BROWSERS\": \"3\",\n        \"BROWSER_TIMEOUT\": \"30000\",\n        \"RATE_LIMIT_WINDOW\": \"60000\",\n        \"RATE_LIMIT_MAX_REQUESTS\": \"60\",\n        \"SEARCH_CACHE_TTL\": \"3600\",\n        \"VIEW_URL_CACHE_TTL\": \"7200\",\n        \"MAX_CACHE_ITEMS\": \"1000\",\n        \"BROWSER_POOL_MIN\": \"1\",\n        \"BROWSER_POOL_MAX\": \"5\",\n        \"BROWSER_POOL_IDLE_TIMEOUT\": \"30000\",\n        \"REBROWSER_PATCHES_RUNTIME_FIX_MODE\": \"addBinding\",\n        \"REBROWSER_PATCHES_SOURCE_URL\": \"jquery.min.js\",\n        \"REBROWSER_PATCHES_UTILITY_WORLD_NAME\": \"util\",\n        \"REBROWSER_PATCHES_DEBUG\": \"0\",\n        \"BROWSER_COOKIES_PATH\": \"C:\\\\path\\\\to\\\\cookies.json\",\n        \"LOG_LEVEL\": \"info\",\n        \"NO_COLOR\": \"0\",\n        \"BUN_FORCE_COLOR\": \"1\",\n        \"FORCE_COLOR\": \"1\"\n      }\n    }\n  }\n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/web_search_mcp` with the absolute path to your server directory.\n\n### Logging Configuration\n\nThe following environment variables control logging behavior:\n\n- `LOG_LEVEL`: Sets the logging level (error, warn, info, debug). Default: info\n- `NO_COLOR`: Disables colored output when set to \"1\"\n- `BUN_FORCE_COLOR`: Controls colored output in Bun runtime (set to \"0\" to disable)\n- `FORCE_COLOR`: Controls colored output globally (set to \"0\" to disable)\n\n## Bot Detection Avoidance\n\nThis server uses rebrowser-puppeteer to avoid bot detection:\n\n1. Runtime.Enable Leak Prevention:\n   - Uses the addBinding technique to avoid Runtime.Enable detection\n   - Works with web workers and iframes\n   - Maintains access to the main world context\n\n2. Source URL Masking:\n   - Changes Puppeteer's sourceURL to look like a legitimate script\n   - Helps avoid detection of automation tools\n\n3. Utility World Name:\n   - Uses a generic utility world name\n   - Prevents detection through world name patterns\n\n4. Browser Launch Configuration:\n   - Disables automation flags\n   - Uses optimized Chrome arguments\n   - Configures viewport and window settings\n\n## Using with Claude Desktop\n\n1. Make sure you have Claude Desktop installed and updated to the latest version\n2. Open your Claude Desktop configuration file:\n   - MacOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the server configuration as shown in the [Configuration](#configuration) section above.\n\n4. Restart Claude Desktop\n5. Look for the hammer icon ![](https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg) to confirm the tools are available\n\n## Available Tools\n\n### 1. Search Tool\n```typescript\n{\n  name: \"search\",\n  params: {\n    query: string;\n    trustedDomains?: string[];\n    excludedDomains?: string[];\n    resultCount?: number;\n    safeSearch?: boolean;\n    dateRestrict?: string;\n  }\n}\n```\n\n### 2. View URL Tool\n```typescript\n{\n  name: \"view_url\",\n  params: {\n    url: string;\n    includeImages?: boolean;\n    includeVideos?: boolean;\n    preserveLinks?: boolean;\n    formatCode?: boolean;\n  }\n}\n```\n\n## Troubleshooting\n\n### Claude Desktop Integration Issues\n\n1. Check the logs:\n   ```bash\n   # MacOS/Linux\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type %APPDATA%\\Claude\\Logs\\mcp*.log\n   ```\n\n2. Common issues:\n   - Server not showing up: Check configuration file syntax and paths\n   - Tool calls failing: Check server logs and restart Claude Desktop\n   - Path issues: Ensure you're using absolute paths\n\nFor more detailed troubleshooting, refer to the [MCP debugging guide](https://modelcontextprotocol.io/docs/tools/debugging).\n\n## Development\n\n```bash\n# Run in development mode with watch\nbun --watch run dev\n\n# Run tests\nbun run test\n\n# Run linter\nbun run lint\n```\n\n## Important Notes\n\n1. Bot Detection:\n   - The bot detection avoidance features help prevent most common detection methods\n   - However, additional measures like proper proxies and user agents may be needed\n   - Some websites may still detect automation through other means\n\n2. Performance:\n   - Browser instances are pooled and reused\n   - Idle browsers are automatically cleaned up\n   - Resource limits prevent overloading\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp-web-search",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "search",
        "claw256",
        "mcp web",
        "web search",
        "search claw256"
      ],
      "category": "web-search"
    },
    "Cognitive-Stack--search-stock-news-mcp": {
      "owner": "Cognitive-Stack",
      "name": "search-stock-news-mcp",
      "url": "https://github.com/Cognitive-Stack/search-stock-news-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Cognitive-Stack.webp",
      "description": "Search real-time stock news with customizable queries and domain-specific filters using the Tavily API. Provides type-safe, configurable search operations for AI workflows.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T00:57:49Z",
      "readme_content": "<h1 align=\"center\">Search Stock News MCP Server 🚀</h1>\n\n<p align=\"center\">\n  \n</p>\n\n> 🔌 **Compatible with Cline, Cursor, Claude Desktop, and any other MCP Clients!**\n> \n> Search Stock News MCP works seamlessly with any MCP client\n\nThe Model Context Protocol (MCP) is an open standard that enables AI systems to interact seamlessly with various data sources and tools, facilitating secure, two-way connections.\n\nThe Search Stock News MCP server provides:\n\n* Real-time stock news search capabilities via Tavily API\n* Multiple customizable search query templates\n* Configurable search parameters and filtering\n* Domain-specific content filtering\n* Type-safe operations with TypeScript\n\n## Prerequisites 🔧\n\nBefore you begin, ensure you have:\n\n* Tavily API Key\n* Claude Desktop, Cursor, or any MCP-compatible client\n* Node.js (v16 or higher)\n* Git installed (only needed if using Git installation method)\n\n## Search Stock News MCP Server Installation ⚡\n\n### Running with NPX\n\n```bash\nnpx -y search-stock-news-mcp@latest\n```\n\n### Installing via Smithery\n\nTo install Search Stock News MCP Server for Claude Desktop automatically via Smithery:\n\n```bash\nnpx -y @smithery/cli install search-stock-news-mcp --client claude\n```\n\n## Configuring MCP Clients ⚙️\n\n### Configuring Cline 🤖\n\nThe easiest way to set up the Search Stock News MCP server in Cline is through the marketplace:\n\n1. Open Cline in VS Code\n2. Click on the Cline icon in the sidebar\n3. Navigate to the \"MCP Servers\" tab\n4. Search \"Search Stock News\" and click \"install\"\n5. When prompted, enter your Tavily API key\n\nAlternatively, manually configure the server in Cline:\n\n1. Open the Cline MCP settings file:\n```bash\n# For macOS:\ncode ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n\n# For Windows:\ncode %APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json\n```\n\n2. Add the Search Stock News server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"search-stock-news-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"search-stock-news-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configuring Cursor 🖥️\n\nTo set up the Search Stock News MCP server in Cursor:\n\n1. Open Cursor Settings\n2. Navigate to Features > MCP Servers\n3. Click on the \"+ Add New MCP Server\" button\n4. Fill out the following information:\n   * **Name**: \"search-stock-news-mcp\"\n   * **Type**: \"command\"\n   * **Command**:\n   ```bash\n   env TAVILY_API_KEY=your-api-key-here npx -y search-stock-news-mcp@latest\n   ```\n\n### Configuring Claude Desktop 🖥️\n\n#### For macOS:\n```bash\ntouch \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\nopen -e \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n```\n\n#### For Windows:\n```bash\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"search-stock-news-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"search-stock-news-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage Examples 🎯\n\n1. **Basic Stock News Search**:\n```json\n{\n  \"symbol\": \"AAPL\",\n  \"companyName\": \"Apple Inc.\",\n  \"maxResults\": 10\n}\n```\n\n2. **Advanced Search with Filters**:\n```json\n{\n  \"symbol\": \"TSLA\",\n  \"companyName\": \"Tesla Inc.\",\n  \"maxResults\": 20,\n  \"searchDepth\": \"advanced\",\n  \"minScore\": 0.6\n}\n```\n\n3. **Custom Domain Search**:\n```json\n{\n  \"symbol\": \"MSFT\",\n  \"companyName\": \"Microsoft Corporation\",\n  \"includeDomains\": [\"reuters.com\", \"bloomberg.com\"]\n}\n```\n\n## Troubleshooting 🛠️\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify npm installation\n   * Check configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Verify your Tavily API key is valid\n   * Check the API key is correctly set in config\n   * Ensure no spaces or quotes around the API key\n\n3. **Search Results Issues**\n   * Check search parameters are within valid ranges\n   * Verify domain filters are correctly formatted\n   * Ensure company name and symbol are accurate\n\n## Acknowledgments ✨\n\n* Model Context Protocol for the MCP specification\n* Anthropic for Claude Desktop\n* Tavily for the News Search API\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/search-stock-news-mcp",
      "npm_downloads": 5035,
      "keywords": [
        "search",
        "api",
        "queries",
        "search operations",
        "search stock",
        "web search"
      ],
      "category": "web-search"
    },
    "ConechoAI--openai-websearch-mcp": {
      "owner": "ConechoAI",
      "name": "openai-websearch-mcp",
      "url": "https://github.com/ConechoAI/openai-websearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ConechoAI.webp",
      "description": "Provides real-time web search capabilities for AI assistants, enhancing their responses with current information from the web. It integrates with applications like Claude.app and Zed editor for streamlined usage during conversations.",
      "stars": 65,
      "forks": 13,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:14:37Z",
      "readme_content": "# OpenAI WebSearch MCP Server 🔍\n\n[![PyPI version](https://badge.fury.io/py/openai-websearch-mcp.svg)](https://badge.fury.io/py/openai-websearch-mcp)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced MCP server that provides intelligent web search capabilities using OpenAI's reasoning models. Perfect for AI assistants that need up-to-date information with smart reasoning capabilities.\n\n## ✨ Features\n\n- **🧠 Reasoning Model Support**: Full compatibility with OpenAI's latest reasoning models (gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini)\n- **⚡ Smart Effort Control**: Intelligent `reasoning_effort` defaults based on use case\n- **🔄 Multi-Mode Search**: Fast iterations with gpt-5-mini or deep research with gpt-5\n- **🌍 Localized Results**: Support for location-based search customization\n- **📝 Rich Descriptions**: Complete parameter documentation for easy integration\n- **🔧 Flexible Configuration**: Environment variable support for easy deployment\n\n## 🚀 Quick Start\n\n### One-Click Installation for Claude Desktop\n\n```bash\nOPENAI_API_KEY=sk-xxxx uvx --with openai-websearch-mcp openai-websearch-mcp-install\n```\n\nReplace `sk-xxxx` with your OpenAI API key from the [OpenAI Platform](https://platform.openai.com/).\n\n## ⚙️ Configuration\n\n### Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd to your MCP settings in Cursor:\n\n1. Open Cursor Settings (`Cmd/Ctrl + ,`)\n2. Search for \"MCP\" or go to Extensions → MCP\n3. Add server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Claude Code\n\nClaude Code automatically detects MCP servers configured for Claude Desktop. Use the same configuration as above for Claude Desktop.\n\n### Local Development\n\nFor local testing, use the absolute path to your virtual environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"/path/to/your/project/.venv/bin/python\",\n      \"args\": [\"-m\", \"openai_websearch_mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\",\n        \"PYTHONPATH\": \"/path/to/your/project/src\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Available Tools\n\n### `openai_web_search`\n\nIntelligent web search with reasoning model support.\n\n#### Parameters\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `input` | `string` | The search query or question to search for | *Required* |\n| `model` | `string` | AI model to use. Supports gpt-4o, gpt-4o-mini, gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini | `gpt-5-mini` |\n| `reasoning_effort` | `string` | Reasoning effort level: low, medium, high, minimal | Smart default |\n| `type` | `string` | Web search API version | `web_search_preview` |\n| `search_context_size` | `string` | Context amount: low, medium, high | `medium` |\n| `user_location` | `object` | Optional location for localized results | `null` |\n\n## 💬 Usage Examples\n\nOnce configured, simply ask your AI assistant to search for information using natural language:\n\n### Quick Search\n> \"Search for the latest developments in AI reasoning models using openai_web_search\"\n\n### Deep Research  \n> \"Use openai_web_search with gpt-5 and high reasoning effort to provide a comprehensive analysis of quantum computing breakthroughs\"\n\n### Localized Search\n> \"Search for local tech meetups in San Francisco this week using openai_web_search\"\n\nThe AI assistant will automatically use the `openai_web_search` tool with appropriate parameters based on your request.\n\n## 🤖 Model Selection Guide\n\n### Quick Multi-Round Searches 🚀\n- **Recommended**: `gpt-5-mini` with `reasoning_effort: \"low\"`\n- **Use Case**: Fast iterations, real-time information, multiple quick queries\n- **Benefits**: Lower latency, cost-effective for frequent searches\n\n### Deep Research 🔬\n- **Recommended**: `gpt-5` with `reasoning_effort: \"medium\"` or `\"high\"`\n- **Use Case**: Comprehensive analysis, complex topics, detailed investigation\n- **Benefits**: Multi-round reasoned results, no need for agent iterations\n\n### Model Comparison\n\n| Model | Reasoning | Default Effort | Best For |\n|-------|-----------|----------------|----------|\n| `gpt-4o` | ❌ | N/A | Standard search |\n| `gpt-4o-mini` | ❌ | N/A | Basic queries |\n| `gpt-5-mini` | ✅ | `low` | Fast iterations |\n| `gpt-5` | ✅ | `medium` | Deep research |\n| `gpt-5-nano` | ✅ | `medium` | Balanced approach |\n| `o3` | ✅ | `medium` | Advanced reasoning |\n| `o4-mini` | ✅ | `medium` | Efficient reasoning |\n\n## 📦 Installation\n\n### Using uvx (Recommended)\n\n```bash\n# Install and run directly\nuvx openai-websearch-mcp\n\n# Or install globally\nuvx install openai-websearch-mcp\n```\n\n### Using pip\n\n```bash\n# Install from PyPI\npip install openai-websearch-mcp\n\n# Run the server\npython -m openai_websearch_mcp\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Install dependencies\nuv sync\n\n# Run in development mode\nuv run python -m openai_websearch_mcp\n```\n\n## 👩‍💻 Development\n\n### Setup Development Environment\n\n```bash\n# Clone and setup\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Create virtual environment and install dependencies\nuv sync\n\n# Run tests\nuv run python -m pytest\n\n# Install in development mode\nuv pip install -e .\n```\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OPENAI_API_KEY` | Your OpenAI API key | *Required* |\n| `OPENAI_DEFAULT_MODEL` | Default model to use | `gpt-5-mini` |\n\n## 🐛 Debugging\n\n### Using MCP Inspector\n\n```bash\n# For uvx installations\nnpx @modelcontextprotocol/inspector uvx openai-websearch-mcp\n\n# For pip installations\nnpx @modelcontextprotocol/inspector python -m openai_websearch_mcp\n```\n\n### Common Issues\n\n**Issue**: \"Unsupported parameter: 'reasoning.effort'\"\n**Solution**: This occurs when using non-reasoning models (gpt-4o, gpt-4o-mini) with reasoning_effort parameter. The server automatically handles this by only applying reasoning parameters to compatible models.\n\n**Issue**: \"No module named 'openai_websearch_mcp'\"\n**Solution**: Ensure you've installed the package correctly and your Python path includes the package location.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- 🤖 Generated with [Claude Code](https://claude.ai/code)\n- 🔥 Powered by [OpenAI's Web Search API](https://openai.com)\n- 🛠️ Built on the [Model Context Protocol](https://modelcontextprotocol.io/)\n\n---\n\n**Co-Authored-By**: Claude <noreply@anthropic.com>",
      "npm_url": "https://www.npmjs.com/package/openai-websearch-mcp",
      "npm_downloads": 821,
      "keywords": [
        "websearch",
        "openai",
        "search",
        "openai websearch",
        "websearch mcp",
        "search conechoai"
      ],
      "category": "web-search"
    },
    "DARPAI--darp_engine": {
      "owner": "DARPAI",
      "name": "darp_engine",
      "url": "https://github.com/DARPAI/darp_engine",
      "imageUrl": "/freedevtools/mcp/pfp/DARPAI.webp",
      "description": "DARPEngine provides smart search capabilities by storing metadata for MCP servers online, enabling efficient discovery and connection to relevant MCP tools. It offers a CLI and API for users to search and route requests based on user queries.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-15T09:08:32Z",
      "readme_content": "# DARPEngine\nThe MCP searchengine for DARP.\n\n[![X][x-image]][x-url]\n[![Code style: black][black-image]][black-url]\n[![Imports: reorder-python-imports][imports-image]][imports-url]\n[![Pydantic v2][pydantic-image]][pydantic-url]\n[![pre-commit][pre-commit-image]][pre-commit-url]\n[![License MIT][license-image]][license-url]\n\nDARPEngine stores metadata for MCP servers hosted online and provides smart search capabilites.\n\n## Features\n\n* Simple CLI\n* API access to search\n* MCP tool to retrieve search results for connecting manually\n* Routing MCP tool based on the server: answer any question using the tools found for the user's request\n\n### Coming soon\n\n* Support for `.well-known/mcp.json`\n* Crawler\n* Nice frontend\n* Hosted version\n* Validate different levels of SSL certificates and integrate this info smarly to make sensitive MCP servers difficult to spoof\n\n## Installation\n\n```\nexport OPENAI_API_KEY=sk-...\ndocker network create highkey_network\ndocker compose build\ndocker compose -f docker-compose.yaml -f docker-compose-debug.yaml up --build --wait\n```\n\n## Getting started\n\nYou can connect the DARPEngine to an MCP Client (e.g. Claude Desktop or Cursor) using mcp tools provided. Just select SSE mode & specify `http://localhost:4689/sse` as the endpoint.\n\n\n### Direct CLI use\n\nAnother way is to use CLI. Most of the scripts work with just standard Python libraries, but routing tool requires mcp package, you can install script requirements like this:\n\n```\nconda create -n darp 'python>=3.10'\nconda activate darp\npip install -r mcp_server/requirements.txt\n```\n\nWhen installation is over we can use the scripts.\n\nTo begin with it we need to add some MCP servers to the engine, e.g:\n\n```\n$ python scripts/darp-add.py --url http://memelabs.ai:3006/sse --name code_analysis --description \"Analyze gitlab repo for quality, topics, packages use\"\n```\n\nThen we can make the requests:\n\n```\n$ python scripts/darp-search.py \"Analyze https://github.com/BenderV/autochat\"\nFound 1 servers:\ncode_analysis\n```\n\nYou can get more useful results with the routing tool:\n\n```\n$ python scripts/darp-router.py \"Analyze https://github.com/BenderV/autochat\"\nassistant: [tool_calls]\n[tool] ...\n[tool] ...\n[tool] ...\nassistant:\n  ### Code Quality\n  The code in the AutoChat repository demonstrates a good level of quality concerning **readability**, **maintainability**, and adherence to best practices:\n\n  - **Readability**: Consistent naming conventions are evident, with adherence to Python's PEP 8 styling guidelines. Descriptive function and variable names enhance the understanding of the code's purpose.\n\n  - **Maintainability**: The code is structured to allow easy updates and modifications, with a clear separation of concerns observed through the use of classes and methods handling distinct functionalities.\n\n  - **Best Practices**: Extensive use of exception handling, type annotations, and docstrings reflect best practices in Python development. Some TODO comments suggest areas for improvement, indicating that further attention is needed.\n\n  ### Code Structure\n  The code is organized into multiple files and modules, each serving a distinct purpose:\n\n  - **Modular Design**: Various classes (e.g., `Autochat`, `Image`, `Message`) indicate a well-structured object-oriented design that promotes separation of concerns, making the code easier to navigate.\n\n  - **Logical Organization**: Files are logically separated based on functionality. For example, `chat.py` focuses on chat-related logic, while `model.py` handles message and image processing. The utility functions in `utils.py` enhance reusability.\n\n  - **Testing**: The presence of a test file (`tests/test_utils.py`) shows commitment to testing, crucial for code reliability. The use of `unittest` indicates a structured approach to testing individual components.\n\n  ### Main Functionality\n  The code appears to be part of an **AutoChat package**, providing a framework for building conversational agents. Key functionalities include:\n\n  - **Chat Management**: The `Autochat` class acts as the main interface for managing conversations, handling message history, context, and interaction limits.\n\n  - **Message Handling**: Classes like `Message` and `MessagePart` enable structured message creation and processing, accommodating different message types, including text and images.\n\n  - **Functionality Extensions**: Methods like `add_tool` and `add_function` allow dynamic addition of tools and functions, facilitating customization of the chat experience.\n\n  - **Provider Integration**: Different API provider integrations (e.g., OpenAI, Anthropic) are encapsulated within respective classes, allowing flexibility in backend communication.\n\n  - **Utilities**: Utility functions offer additional capabilities such as CSV formatting and function parsing that support main chat operations.\n\n  Overall, the codebase is well-organized and showcases a thoughtful approach to developing a conversational AI framework. There is room for further refinement and enhancement, particularly in documentation and clarity of variable names.\n\n  ### Library Usage\n  The project makes use of **AI libraries**, indicated by its functionality related to conversational agents and integration with AI service providers. This supports its ability to manage interactions with AI models efficiently.\n\n  ### Summary\n  The AutoChat project is a chat system designed for communication with various AI models, primarily through the `Autochat` class, which manages conversations and supports complex message types, including text and images. The code is moderately complex due to its integration with external APIs and its ability to handle diverse interactions through extensible methods like `add_tool` and `add_function`. The quality of code is commendable, featuring a well-structured modular design that promotes readability and maintainability, although some areas require further documentation and refinement, such as clarifying variable names and enhancing comments. The organization into separate files for models, utilities, and tests aids development, but the utility functions could benefit from better categorization for improved clarity.\n```\n\nOf course, the usefulness of the result depends on the MCP servers you connect to the engine.\n\n\n## Get help and support\n\nPlease feel free to connect with us using the [discussion section](https://github.com/hipasus/darp_engine/discussions).\n\n## Contributing\n\nFollow us on X: https://x.com/DARP_AI\n\n## License\n\nThe DARPEngine codebase is under MIT license.\n\n<br>\n\n[x-image]: https://img.shields.io/twitter/follow/DARP_AI?style=social\n[x-url]: https://x.com/DARP_AI\n[black-image]: https://img.shields.io/badge/code%20style-black-000000.svg\n[black-url]: https://github.com/psf/black\n[imports-image]: https://img.shields.io/badge/%20imports-reorder_python_imports-%231674b1?style=flat&labelColor=ef8336\n[imports-url]: https://github.com/asottile/reorder-python-imports/\n[pydantic-image]: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json\n[pydantic-url]: https://pydantic.dev\n[pre-commit-image]: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white\n[pre-commit-url]: https://github.com/pre-commit/pre-commit\n[license-image]: https://img.shields.io/github/license/DARPAI/darp_engine\n[license-url]: https://opensource.org/licenses/MIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "darp_engine",
        "darpai",
        "darpengine",
        "search darpai",
        "darp_engine darpengine",
        "darpengine provides"
      ],
      "category": "web-search"
    },
    "DaInfernalCoder--perplexity-mcp": {
      "owner": "DaInfernalCoder",
      "name": "perplexity-mcp",
      "url": "https://github.com/DaInfernalCoder/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DaInfernalCoder.webp",
      "description": "A simple notes system designed to manage and summarize text notes, incorporating an intelligent research assistant with search capabilities for handling various queries efficiently.",
      "stars": 252,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-28T20:54:17Z",
      "readme_content": "# Perplexity MCP Server\n\nAn intelligent research assistant powered by Perplexity's specialized AI models. Features automatic query complexity detection to route requests to the most appropriate model for optimal results. Unlike the Official server, it has search capabilities FOR EVERY TASK, essentially \n\n## Tools\n\n**Quick Note: The Deep Research tool is going to timeout with some tools like cline, but not with others like cursor due to implementation differences, but the reason tool makes up for it.**\n\n### 1. Search (Sonar Pro)\nQuick search for simple queries and basic information lookup. Best for straightforward questions that need concise, direct answers.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"What is the capital of France?\",\n    force_model: false // Optional: force using this model even if query seems complex\n  }\n});\n```\n\n### 2. Reason (Sonar Reasoning Pro)\nHandles complex, multi-step tasks requiring detailed analysis. Perfect for explanations, comparisons, and problem-solving.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"reason\",\n  arguments: {\n    query: \"Compare and contrast REST and GraphQL APIs, explaining their pros and cons\",\n    force_model: false // Optional: force using this model even if query seems simple\n  }\n});\n```\n\n### 3. Deep Research (Sonar Deep Research)\nConducts comprehensive research and generates detailed reports. Ideal for in-depth analysis of complex topics.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"deep_research\",\n  arguments: {\n    query: \"The impact of quantum computing on cryptography\",\n    focus_areas: [\n      \"Post-quantum cryptographic algorithms\",\n      \"Timeline for quantum threats\",\n      \"Practical mitigation strategies\"\n    ],\n    force_model: false // Optional: force using this model even if query seems simple\n  }\n});\n```\n\n## Intelligent Model Selection\n\nThe server automatically analyzes query complexity to route requests to the most appropriate model:\n\n1. **Simple Queries** → Sonar Pro\n   - Basic information lookup\n   - Straightforward questions\n   - Quick facts\n\n2. **Complex Queries** → Sonar Reasoning Pro\n   - How/why questions\n   - Comparisons\n   - Step-by-step explanations\n   - Problem-solving tasks\n\n3. **Research Queries** → Sonar Deep Research\n   - In-depth analysis\n   - Comprehensive research\n   - Detailed investigations\n   - Multi-faceted topics\n\nYou can override the automatic selection using `force_model: true` in any tool's arguments.\n\n## Setup\n\n1. **Prerequisites**\n   - Node.js (from [nodejs.org](https://nodejs.org))\n   - Perplexity API key (from [perplexity.ai/settings/api](https://www.perplexity.ai/settings/api))\n   - clone the repo somewhere\n\n2. **Configure MCP Settings**\n\nAdd to your MCP settings file (location varies by platform):\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/perplexity-server/build/index.js\"],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\nOr use NPX to not have to install it locally (recommended for macos): \n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"perplexity-mcp\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=DaInfernalCoder/perplexity-mcp&type=Timeline)](https://www.star-history.com/#DaInfernalCoder/perplexity-mcp&Timeline)\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "notes",
        "text",
        "search",
        "text notes",
        "notes incorporating",
        "assistant search"
      ],
      "category": "web-search"
    },
    "DealExpress--mcp-server": {
      "owner": "DealExpress",
      "name": "mcp-server",
      "url": "https://github.com/DealExpress/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/DealExpress.webp",
      "description": "Enables interaction with ads on the DealX platform through natural language queries, supporting search, sorting, and pagination of ads. The framework is easily extendable for future functionalities related to the DealX platform.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-01T17:41:14Z",
      "readme_content": "# @dealx/mcp-server\n\nThis is a Model Context Protocol (MCP) server for the [DealX platform](https://dealx.com.ua). It allows LLMs to interact with the DealX platform, specifically to search for ads.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Available Tools](#available-tools)\n- [Extending the Server](#extending-the-server)\n- [Development](#development)\n- [Troubleshooting](#troubleshooting)\n\n## Overview\n\nThe DealX MCP Server implements the [Model Context Protocol](https://github.com/modelcontextprotocol/typescript-sdk) to provide a standardized way for LLMs to interact with the [DealX platform](https://dealx.com.ua). Currently, it supports searching for ads, with plans to add more functionality in the future.\n\n### What is MCP?\n\nThe Model Context Protocol (MCP) is a standardized way for LLMs to interact with external systems. It provides a structured interface for LLMs to access data and perform actions in the real world. This server implements the MCP specification to allow LLMs to interact with the DealX platform.\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v20 or later)\n- npm (v11 or later)\n\n### MCP Configuration\n\nTo use this server with an LLM like Claude, you need to add it to your LLM's MCP configuration:\n\n1. Open your LLM's MCP configuration file:\n\n   - **Claude Desktop App**:\n     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n     - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n     - Linux: `~/.config/Claude/claude_desktop_config.json`\n   - **Cline (VS Code Extension)**:\n     - `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n\n2. Add the DealX MCP server to the `mcpServers` section:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"dealx\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@dealx/mcp-server\"],\n         \"env\": {\n           \"DEALX_API_URL\": \"https://dealx.com.ua\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n### Installation via npm\n\nThe easiest way to install the DealX MCP Server is via npm:\n\n```shell\nnpm install -g @dealx/mcp-server\n```\n\n### Installation for Development\n\nIf you want to modify the server or contribute to its development:\n\n1. Clone the repository:\n\n   ```shell\n   git clone <repository-url>\n   cd dealx/mcp\n   ```\n\n2. Install dependencies:\n\n   ```shell\n   npm install\n   ```\n\n3. Create a `.env` file based on the `.env.example` file:\n\n   ```shell\n   cp .env.example .env\n   ```\n\n4. Edit the `.env` file to set the appropriate values:\n\n   ```shell\n   # DealX API URL\n   DEALX_API_URL=http://localhost:3001\n\n   # Optional: Specify the port for the MCP server\n   MCP_SERVER_PORT=3100\n\n   # Optional: Log level (debug, info, warn, error)\n   LOG_LEVEL=info\n   ```\n\n5. Build the server:\n\n   ```shell\n   npm run build\n   ```\n\n## Usage\n\n### Starting the Server\n\nYou can run the server in several ways:\n\n1. If installed globally:\n\n   ```shell\n   node node_modules/@dealx/mcp-server/build/index.js\n   ```\n\n2. Using npx without installation:\n\n   ```shell\n   npx -y @dealx/mcp-server\n   ```\n\n3. With environment variables:\n\n   ```shell\n   DEALX_API_URL=https://dealx.com.ua npx -y @dealx/mcp-server\n   ```\n\n4. For development:\n\n   ```shell\n   npm start\n   ```\n\n### Using with an LLM\n\nOnce configured in your LLM's MCP settings, you can use natural language to interact with the DealX platform.\n\nExample prompts:\n\n- \"Search for ads on DealX with the query 'laptop'\"\n- \"Find the newest 5 ads for 'iPhone' on DealX\"\n- \"Search DealX for apartments in Kyiv\"\n\n## Available Tools\n\n### search_ads\n\nSearch for ads on the DealX platform.\n\n**Parameters:**\n\n- `query` (string, optional): Search query string\n- `sort` (string, optional): Sort order (e.g., \"-created\" for newest first)\n- `offset` (number, optional): Pagination offset (starts at 1, default: 1)\n- `limit` (number, optional): Number of results per page (max 100, default: 30)\n\n**Example Usage:**\n\n```json\n{\n  \"query\": \"laptop\",\n  \"sort\": \"-created\",\n  \"offset\": 1,\n  \"limit\": 10\n}\n```\n\n## Extending the Server\n\nThe server is designed to be easily extended with additional tools. Here's how to add a new tool:\n\n- Define the tool in the `TOOLS` object in `src/index.ts`:\n\n  ```typescript\n  const TOOLS = {\n    SEARCH_ADS: \"search_ads\",\n    NEW_TOOL: \"new_tool\", // Add your new tool here\n  };\n  ```\n\n- Create a new file in the `src/tools` directory for your tool implementation:\n\n  ```typescript\n  // src/tools/new-tool.ts\n  import { ErrorCode, McpError } from \"@modelcontextprotocol/sdk/types.js\";\n\n  interface NewToolParams {\n    // Define your tool parameters here\n  }\n\n  export async function newTool(params: NewToolParams) {\n    try {\n      // Implement your tool logic here\n\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: JSON.stringify(result, null, 2),\n          },\n        ],\n      };\n    } catch (error) {\n      // Handle errors\n      // ...\n    }\n  }\n  ```\n\n- Add the tool to the `ListToolsRequestSchema` handler in `src/index.ts`:\n\n  ```typescript\n  this.server.setRequestHandler(ListToolsRequestSchema, async () => ({\n    tools: [\n      // Existing tools...\n      {\n        name: TOOLS.NEW_TOOL,\n        description: \"Description of your new tool\",\n        inputSchema: {\n          type: \"object\",\n          properties: {\n            // Define your tool parameters here\n          },\n          required: [], // List required parameters\n        },\n      },\n    ],\n  }));\n  ```\n\n- Add the tool to the `CallToolRequestSchema` handler in `src/index.ts`:\n\n  ```typescript\n  this.server.setRequestHandler(CallToolRequestSchema, async (request) => {\n    const { name, arguments: args } = request.params;\n\n    switch (name) {\n      // Existing cases...\n      case TOOLS.NEW_TOOL:\n        return await newTool(args);\n      default:\n        throw new McpError(ErrorCode.MethodNotFound, `Unknown tool: ${name}`);\n    }\n  });\n  ```\n\n- Import your new tool in `src/index.ts`:\n\n  ```typescript\n  import { newTool } from \"./tools/new-tool.js\";\n  ```\n\n### Planned Future Tools\n\nThe following tools are planned for future implementation:\n\n- `create_ad`: Create a new ad on the [DealX platform](https://dealx.com.ua)\n- `edit_ad`: Edit an existing ad\n- `delete_ad`: Delete an ad\n- `get_threads`: Get discussion threads for an ad\n- `create_thread`: Create a new discussion thread\n\n## Development\n\n### Project Structure\n\n```shell\nmcp/\n├── build/              # Compiled JavaScript files\n├── src/                # TypeScript source files\n│   ├── tools/          # Tool implementations\n│   │   └── search-ads.ts\n│   └── index.ts        # Main server implementation\n├── .env                # Environment variables (not in git)\n├── .env.example        # Example environment variables\n├── package.json        # Project dependencies and scripts\n├── tsconfig.json       # TypeScript configuration\n└── README.md           # This file\n```\n\n### npm Scripts\n\n- `npm run build` - Compile TypeScript to JavaScript\n- `npm start` - Start the server using the compiled JavaScript\n- `npm run dev` - Start the server in development mode with hot reloading\n- `npm run lint` - Lint the code using ESLint\n- `npm run format` - Format the code using Prettier\n- `npm test` - Run tests\n\n## Troubleshooting\n\n### Common Issues\n\n#### Server Not Starting\n\nIf the server fails to start, check the following:\n\n- Make sure you have the correct Node.js version installed\n- Check that all dependencies are installed\n- Verify that the `.env` file exists and has the correct values\n- Check the console output for error messages\n\n#### Connection Issues\n\nIf the LLM can't connect to the server:\n\n- Make sure the server is running\n- Check that the MCP configuration in the LLM's settings is correct\n- Verify that the path to the server executable is correct\n- Check that the environment variables are set correctly\n\n#### API Connection Issues\n\nIf the server can't connect to the DealX API:\n\n- Make sure the DealX API is running\n- Check that the `DEALX_API_URL` environment variable is set correctly\n- Verify that the API endpoint is accessible from the server\n\n### Getting Help\n\nIf you encounter issues not covered here, please open an issue against this GitHub repository.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "dealexpress",
        "dealx",
        "queries",
        "search dealexpress",
        "ads dealx",
        "dealx platform"
      ],
      "category": "web-search"
    },
    "DeepSpringAI--search_mcp_server": {
      "owner": "DeepSpringAI",
      "name": "search_mcp_server",
      "url": "https://github.com/DeepSpringAI/search_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/DeepSpringAI.webp",
      "description": "Enables applications to perform web searches and extract relevant information from search results, while also allowing for similarity searches to identify related content. This functionality simplifies access to web data and enhances content discovery.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-23T14:58:59Z",
      "readme_content": "# parquet_mcp_server\n[![smithery badge](https://smithery.ai/badge/@DeepSpringAI/parquet_mcp_server)](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server)\n\nA powerful MCP (Model Control Protocol) server that provides tools for performing web searches and finding similar content. This server is designed to work with Claude Desktop and offers two main functionalities:\n\n1. **Web Search**: Perform a web search and scrape results\n2. **Similarity Search**: Extract relevant information from previous searches\n\nThis server is particularly useful for:\n- Applications requiring web search capabilities\n- Projects needing to find similar content based on search queries\n\n## Installation\n\n### Installing via Smithery\n\nTo install Parquet MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server):\n\n```bash\nnpx -y @smithery/cli install @DeepSpringAI/parquet_mcp_server --client claude\n```\n\n### Clone this repository\n\n```bash\ngit clone ...\ncd parquet_mcp_server\n```\n\n### Create and activate virtual environment\n\n```bash\nuv venv\n.venv\\Scripts\\activate  # On Windows\nsource .venv/bin/activate  # On macOS/Linux\n```\n\n### Install the package\n\n```bash\nuv pip install -e .\n```\n\n### Environment\n\nCreate a `.env` file with the following variables:\n\n```bash\nEMBEDDING_URL=http://sample-url.com/api/embed  # URL for the embedding service\nOLLAMA_URL=http://sample-url.com/  # URL for Ollama server\nEMBEDDING_MODEL=sample-model  # Model to use for generating embeddings\nSEARCHAPI_API_KEY=your_searchapi_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nVOYAGE_API_KEY=your_voyage_api_key\nAZURE_OPENAI_ENDPOINT=http://sample-url.com/azure_openai\nAZURE_OPENAI_API_KEY=your_azure_openai_api_key\n```\n\n## Usage with Claude Desktop\n\nAdd this to your Claude Desktop configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"parquet-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/home/${USER}/workspace/parquet_mcp_server/src/parquet_mcp_server\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server provides two main tools:\n\n1. **Search Web**: Perform a web search and scrape results\n   - Required parameters:\n     - `queries`: List of search queries\n   - Optional parameters:\n     - `page_number`: Page number for the search results (defaults to 1)\n\n2. **Extract Info from Search**: Extract relevant information from previous searches\n   - Required parameters:\n     - `queries`: List of search queries to merge\n\n## Example Prompts\n\nHere are some example prompts you can use with the agent:\n\n### For Web Search:\n```\n\"Please perform a web search for 'macbook' and 'laptop' and scrape the results from page 1\"\n```\n\n### For Extracting Info from Search:\n```\n\"Please extract relevant information from the previous searches for 'macbook'\"\n```\n\n## Testing the MCP Server\n\nThe project includes a comprehensive test suite in the `src/tests` directory. You can run all tests using:\n\n```bash\npython src/tests/run_tests.py\n```\n\nOr run individual tests:\n\n```bash\n# Test Web Search\npython src/tests/test_search_web.py\n\n# Test Extract Info from Search\npython src/tests/test_extract_info_from_search.py\n```\n\nYou can also test the server using the client directly:\n\n```python\nfrom parquet_mcp_server.client import (\n    perform_search_and_scrape,  # New web search function\n    find_similar_chunks  # New extract info function\n)\n\n# Perform a web search\nperform_search_and_scrape([\"macbook\", \"laptop\"], page_number=1)\n\n# Extract information from the search results\nfind_similar_chunks([\"macbook\"])\n```\n\n### Troubleshooting\n\n1. If you get SSL verification errors, make sure the SSL settings in your `.env` file are correct\n2. If embeddings are not generated, check:\n   - The Ollama server is running and accessible\n   - The model specified is available on your Ollama server\n   - The text column exists in your input Parquet file\n3. If DuckDB conversion fails, check:\n   - The input Parquet file exists and is readable\n   - You have write permissions in the output directory\n   - The Parquet file is not corrupted\n4. If PostgreSQL conversion fails, check:\n   - The PostgreSQL connection settings in your `.env` file are correct\n   - The PostgreSQL server is running and accessible\n   - You have the necessary permissions to create/modify tables\n   - The pgvector extension is installed in your database\n\n## PostgreSQL Function for Vector Similarity Search\n\nTo perform vector similarity searches in PostgreSQL, you can use the following function:\n\n```sql\n-- Create the function for vector similarity search\nCREATE OR REPLACE FUNCTION match_web_search(\n  query_embedding vector(1024),  -- Adjusted vector size\n  match_threshold float,\n  match_count int  -- User-defined limit for number of results\n)\nRETURNS TABLE (\n  id bigint,\n  metadata jsonb,\n  text TEXT,  -- Added text column to the result\n  date TIMESTAMP,  -- Using the date column instead of created_at\n  similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    web_search.id,\n    web_search.metadata,\n    web_search.text,  -- Returning the full text of the chunk\n    web_search.date,  -- Returning the date timestamp\n    1 - (web_search.embedding <=> query_embedding) as similarity\n  FROM web_search\n  WHERE 1 - (web_search.embedding <=> query_embedding) > match_threshold\n  ORDER BY web_search.date DESC,  -- Sort by date in descending order (newest first)\n           web_search.embedding <=> query_embedding  -- Sort by similarity\n  LIMIT match_count;  -- Limit the results to the match_count specified by the user\nEND;\n$$;\n```\n\nThis function allows you to perform similarity searches on vector embeddings stored in a PostgreSQL database, returning results that meet a specified similarity threshold and limiting the number of results based on user input. The results are sorted by date and similarity.\n\n\n\n## Postgres table creation\n```\nCREATE TABLE web_search (\n    id SERIAL PRIMARY KEY,\n    text TEXT,\n    metadata JSONB,\n    embedding VECTOR(1024),\n\n    -- This will be auto-updated\n    date TIMESTAMP DEFAULT NOW()\n);\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search_mcp_server",
        "searches",
        "search",
        "deepspringai search_mcp_server",
        "search deepspringai",
        "web search"
      ],
      "category": "web-search"
    },
    "DevEnterpriseSoftware--scrapi-mcp": {
      "owner": "DevEnterpriseSoftware",
      "name": "scrapi-mcp",
      "url": "https://github.com/DevEnterpriseSoftware/scrapi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DevEnterpriseSoftware.webp",
      "description": "ScrAPI MCP Server facilitates the extraction of data from any website, handling bot detection and geolocation restrictions efficiently. It provides an intuitive interface for web scraping tasks.",
      "stars": 13,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T18:02:21Z",
      "readme_content": "![ScrAPI logo](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-sdk-dotnet/master/icon_small.png)\n\n# ScrAPI MCP Server\n\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![NPM Downloads](https://img.shields.io/npm/dm/@deventerprisesoftware/scrapi-mcp)](https://www.npmjs.com/package/@deventerprisesoftware/scrapi-mcp)\n[![Docker Pulls](https://img.shields.io/docker/pulls/deventerprisesoftware/scrapi-mcp)](https://hub.docker.com/r/deventerprisesoftware/scrapi-mcp)\n[![smithery badge](https://smithery.ai/badge/@DevEnterpriseSoftware/scrapi-mcp)](https://smithery.ai/server/@DevEnterpriseSoftware/scrapi-mcp)\n\nMCP server for using [ScrAPI](https://scrapi.tech) to scrape web pages.\n\nScrAPI is your ultimate web scraping solution, offering powerful, reliable, and easy-to-use features to extract data from any website effortlessly.\n\n## Tools\n\n1. `scrape_url_html`\n   - Use a URL to scrape a website using the ScrAPI service and retrieve the result as HTML.\n     Use this for scraping website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n     The result will be in HTML which is preferable if advanced parsing is required.\n   - Input: `url` (string)\n   - Returns: HTML content of the URL\n\n2. `scrape_url_markdown`\n   - Use a URL to scrape a website using the ScrAPI service and retrieve the result as Markdown.\n     Use this for scraping website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n     The result will be in Markdown which is preferable if the text content of the webpage is important and not the structural information of the page.\n   - Input: `url` (string)\n   - Returns: Markdown content of the URL\n\n## Setup\n\n### API Key (optional)\n\nOptionally get an API key from the [ScrAPI website](https://scrapi.tech).\n\nWithout an API key you will be limited to one concurrent call and twenty free calls per day with minimal queuing capabilities.\n\n### Cloud Server\n\nThe ScrAPI MCP Server is also available in the cloud over SSE at https://api.scrapi.tech/mcp/sse and streamable HTTP at https://api.scrapi.tech/mcp\n\nCloud MCP servers are not widely supported yet but you can access this directly from your own custom clients or use [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to test it. There is currently no facility to pass through your API key when connecting to the cloud MCP server.\n\n![MCP-Inspector](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-mcp/master/img/mcp-inspector.jpg)\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"ScrAPI\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SCRAPI_API_KEY\",\n        \"deventerprisesoftware/scrapi-mcp\"\n      ],\n      \"env\": {\n        \"SCRAPI_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"ScrAPI\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@deventerprisesoftware/scrapi-mcp\"\n      ],\n      \"env\": {\n        \"SCRAPI_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n![Claude-Desktop](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-mcp/master/img/claude-desktop.jpg)\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t deventerprisesoftware/scrapi-mcp -f Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/@deventerprisesoftware/scrapi-mcp",
      "npm_downloads": 4420,
      "keywords": [
        "scraping",
        "scrapi",
        "mcp",
        "mcp scrapi",
        "scrapi mcp",
        "deventerprisesoftware scrapi"
      ],
      "category": "web-search"
    },
    "Domoteek--mcp-server-airbnb": {
      "owner": "Domoteek",
      "name": "mcp-server-airbnb",
      "url": "https://github.com/Domoteek/mcp-server-airbnb",
      "imageUrl": "/freedevtools/mcp/pfp/Domoteek.webp",
      "description": "Search for Airbnb listings based on various parameters such as location, check-in and check-out dates, and guest count. Retrieve detailed information and direct links to the listings for easy access and trip planning.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-24T15:59:47Z",
      "readme_content": "# Serveur MCP Airbnb\n[![smithery badge](https://smithery.ai/badge/@Domoteek/mcp-server-airbnb)](https://smithery.ai/server/@Domoteek/mcp-server-airbnb)\n\nServeur MCP pour rechercher des annonces Airbnb et obtenir les détails des logements. Fournit des liens directs vers les annonces Airbnb dans les résultats de recherche.\n\n## Outils\n\n1. `airbnb_search`\n   - Recherche d'annonces Airbnb\n   - Entrée requise : `location` (chaîne de caractères)\n   - Entrées optionnelles :\n     - `placeId` (chaîne de caractères)\n     - `checkin` (chaîne de caractères, AAAA-MM-JJ)\n     - `checkout` (chaîne de caractères, AAAA-MM-JJ)\n     - `adults` (nombre)\n     - `children` (nombre)\n     - `infants` (nombre)\n     - `pets` (nombre)\n     - `minPrice` (nombre)\n     - `maxPrice` (nombre)\n     - `cursor` (chaîne de caractères)\n     - `ignoreRobotsText` (booléen)\n   - Retourne : Tableau d'annonces avec des détails comme le nom, le prix, l'emplacement, etc. Chaque annonce inclut une `url` directe vers la page Airbnb.\n\n2. `airbnb_listing_details`\n   - Obtenir des informations détaillées sur une annonce Airbnb spécifique\n   - Entrée requise : `id` (chaîne de caractères)\n   - Entrées optionnelles :\n     - `checkin` (chaîne de caractères, AAAA-MM-JJ)\n     - `checkout` (chaîne de caractères, AAAA-MM-JJ)\n     - `adults` (nombre)\n     - `children` (nombre)\n     - `infants` (nombre)\n     - `pets` (nombre)\n     - `ignoreRobotsText` (booléen)\n   - Retourne : Informations détaillées sur l'annonce, y compris la description, les détails de l'hôte, les équipements, les tarifs, etc. La réponse inclut une `url` directe vers la page de l'annonce Airbnb.\n\n## Fonctionnalités\n\n- Respecte les règles du fichier robots.txt d'Airbnb\n- Utilise cheerio pour l'analyse HTML\n- Aucune clé API requise\n- Retourne des données JSON structurées\n- Réduit la charge de contexte en aplatissant et en sélectionnant les données\n- Fournit des URL directes vers les annonces Airbnb\n\n## Installation\n\n### Installation sur Claude Desktop\nAvant de commencer, assurez-vous que [Node.js](https://nodejs.org/) est installé sur votre ordinateur pour que `npx` fonctionne.\n\n1. Allez dans : Paramètres > Développeur > Modifier la configuration\n\n2. Ajoutez ce qui suit à votre fichier `claude_desktop_config.json` :\n\n```json\n{\n  \"mcpServers\": {\n    \"airbnb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@Domoteek/mcp-server-airbnb\"\n      ]\n    }\n  }\n}\n```\n\nPour ignorer le fichier robots.txt pour toutes les requêtes, utilisez cette version avec l'argument `--ignore-robots-txt` :\n\n```json\n{\n  \"mcpServers\": {\n    \"airbnb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@Domoteek/mcp-server-airbnb\",\n        \"--ignore-robots-txt\"\n      ]\n    }\n  }\n}\n```\n3. Redémarrez Claude Desktop et planifiez votre prochain voyage incluant des Airbnbs !\n\n### Autre option : Installation via Smithery\n\nPour installer mcp-server-airbnb pour Claude Desktop automatiquement via [Smithery](https://smithery.ai/server/@Domoteek/mcp-server-airbnb) :\n\n```bash\nnpx -y @smithery/cli install @Domoteek/mcp-server-airbnb --client claude\n```\n\n## Compilation (pour les développeurs)\n\n```bash\nnpm install\nnpm run build\n```\n\n## Licence\n\nCe serveur MCP est sous licence MIT.\n\n## Avertissement\n\nAirbnb est une marque déposée d'Airbnb, Inc.\nDomoteek n'est pas lié à Airbnb, Inc. ou à ses filiales\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "airbnb",
        "listings",
        "domoteek",
        "airbnb search",
        "search airbnb",
        "airbnb listings"
      ],
      "category": "web-search"
    },
    "ElfProxy--proxy-pool-mcp-server": {
      "owner": "ElfProxy",
      "name": "proxy-pool-mcp-server",
      "url": "https://github.com/ElfProxy/proxy-pool-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ElfProxy.webp",
      "description": "Provides secure access to global residential proxies with dynamic IP rotation, enabling geo-targeted web data extraction and AI-optimized web interactions. Features include adaptive rate limiting and intelligent content sanitization for enhanced AI workflows.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-04-18T10:32:08Z",
      "readme_content": "<p align=\"center\">\n  \n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  ElfProxy MCP Server\n</h1>\n<p align=\"center\">\n  <p align=\"center\">Extract agent seamlessly with ElfProxy via the Model Context Protocol</p>\n</p>\n\n[![jdk version](https://img.shields.io/badge/java%20version-%2017-blue)](http://pyrn.2.vu/1)\n[![SpringBoot version](https://img.shields.io/badge/spring%20boot-%203-blue)](http://pyrn.2.vu/1)\n\n<br/>\n\n## 📖 Overview\n\nA privacy-first infrastructure solution combining dynamic IP rotation with AI-optimized web interaction capabilities. This integration enables secure, large-scale web data access for AI systems through two synergistic components:\n\n**1. ElfProxy Dynamic IP Network**  \nGlobal residential proxy infrastructure featuring:\n- 🌐 195+ country IP pools with city-level targeting\n- 🔄 Automatic IP rotation (1s-24h adjustable intervals)\n- 🛡️ Built-in TLS fingerprint masking\n- ⚡ Multi-protocol support (HTTP/SOCKS5/WebSocket)\n- 📊 Real-time traffic analytics dashboard\n\n**2. Enhanced Model Context Protocol (MCP)**  \nAI-specific web interaction layer with:\n- 🧠 Context-aware request scheduling\n- 🖥️ Headless browser rendering cluster\n- ✂️ Intelligent content sanitization (Ads/Trackers/Boilerplate removal)\n- 🔄 3-stage failover system (IP/Header/Protocol rotation)\n- ⏱️ Adaptive rate limiting (Requests/second auto-tuning)\n\n### Key Features\n| Category              | Capabilities                                                                 |\n|-----------------------|-----------------------------------------------------------------------------|\n| **Anonymity**         | IP/UserAgent/Canvas fingerprint rotation                                   |\n| **AI Optimization**   | Content extraction → Markdown/JSON/Plaintext conversion                    |\n| **Geo-Targeting**     | Country/City/ASN-level location simulation                                 |\n| **Security**          | End-to-end HTTPS encryption + Request timestamp obfuscation                |\n| **Reliability**       | 99.99% SLA with automatic proxy blacklisting                              |\n\n## 💡 Example Queries\nWhen you've set up the MCP server with **Claude**, you can make requests like:\n\n- Please extract a proxy IP address from the United States for me\n- Accessing through a US proxy IP using `puppeteer`  ` https://www.amazon.de `\n\n## ✅ Prerequisites\n\nBefore you begin, make sure you have:\n\n- **ElfProxy Account**: Please register an account in [ElfProxy](http://pyrn.2.vu/1/) first, contact ElfProxy customers, and obtain an apiKey (there will be 200MB of data available for trial use)\n\n### 🧩 Server Configuration\n\nThe apiKey can be configured using the `application.yml` system property. \n\n### Cursor Tool Configuration\n\nTo use this MCP server in Cursor, add the following configuration to your Cursor settings:\n\n```json\n{\n  \"proxy-pool-server\": {\n    \"url\": \"http://localhost:9000/sse\",\n    \"enabled\": true\n  }\n}\n```\n\n## Building\n\n```bash\nmvn clean package\n```\n\n## Running\n\n```bash\njava -jar target/proxy-pool-server-{version}.jar\n```\n\n## API Endpoints\n\nThe server exposes the following MCP tools:\n\n- `getProxy`:Retrieve an IP address from the proxy pool and use the 'code' field to tell me which country's code you need. I will extract the IP address from the code and provide it to you\n\n\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n## About ElfProxy\n\nWith over 100M IP nodes and global service coverage (excluding China), we provide service support for multiple million level products with stable and reliable IP quality. Our comprehensive services are trustworthy\n\n[](http://pyrn.2.vu/1/)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elfproxy",
        "proxy",
        "proxies",
        "search elfproxy",
        "proxy pool",
        "elfproxy proxy"
      ],
      "category": "web-search"
    },
    "Evilran--baidu-mcp-server": {
      "owner": "Evilran",
      "name": "baidu-mcp-server",
      "url": "https://github.com/Evilran/baidu-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Evilran.webp",
      "description": "Connects to Baidu to perform web searches and retrieve webpage content, incorporating intelligent text extraction and formatted results. Features built-in rate limiting and robust error handling for efficient content management.",
      "stars": 13,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T06:14:41Z",
      "readme_content": "# Baidu Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Evilran/baidu-mcp-server)](https://smithery.ai/server/@Evilran/baidu-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Baidu, with additional features for content fetching and parsing.\n\n<a href=\"https://glama.ai/mcp/servers/phcus2gcpn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/phcus2gcpn/badge\" alt=\"Baidu Server MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Search Baidu with advanced rate limiting and result formatting\n- **Content Fetching**: Retrieve and parse webpage content with intelligent text extraction\n- **Rate Limiting**: Built-in protection against rate limits for both search and content fetching\n- **Error Handling**: Comprehensive error handling and logging\n- **LLM-Friendly Output**: Results formatted specifically for large language model consumption\n\n## Installation\n\n### Installing via Smithery\n\nTo install Baidu Search Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Evilran/baidu-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @Evilran/baidu-mcp-server --client claude\n```\n\n### Installing via `uv`\n\nInstall directly from PyPI using `uv`:\n\n```bash\nuv pip install baidu-mcp-server\n```\n\n## Usage\n\n### Running with Claude Desktop\n\n1. Download [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"baidu-search\": {\n            \"command\": \"uvx\",\n            \"args\": [\"baidu-mcp-server\"]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop\n\n### Development\n\nFor local development, you can use the MCP CLI:\n\n```bash\n# Run with the MCP Inspector\nmcp dev server.py\n\n# Install locally for testing with Claude Desktop\nmcp install server.py\n```\n## Available Tools\n\n### 1. Search Tool\n\n```python\nasync def search(query: str, max_results: int = 10) -> str\n```\n\nPerforms a web search on Baidu and returns formatted results.\n\n**Parameters:**\n- `query`: Search query string\n- `max_results`: Maximum number of results to return (default: 10)\n\n**Returns:**\nFormatted string containing search results with titles, URLs, and snippets.\n\n### 2. Content Fetching Tool\n\n```python\nasync def fetch_content(url: str) -> str\n```\n\nFetches and parses content from a webpage.\n\n**Parameters:**\n- `url`: The webpage URL to fetch content from\n\n**Returns:**\nCleaned and formatted text content from the webpage.\n\n## Features in Detail\n\n### Rate Limiting\n\n- Search: Limited to 30 requests per minute\n- Content Fetching: Limited to 20 requests per minute\n- Automatic queue management and wait times\n\n### Result Processing\n\n- Removes ads and irrelevant content\n- Cleans up Baidu redirect URLs\n- Formats results for optimal LLM consumption\n- Truncates long content appropriately\n\n### Error Handling\n\n- Comprehensive error catching and reporting\n- Detailed logging through MCP context\n- Graceful degradation on rate limits or timeouts\n\n## Contributing\n\nIssues and pull requests are welcome! Some areas for potential improvement:\n\n- Additional search parameters (region, language, etc.)\n- Enhanced content parsing options\n- Caching layer for frequently accessed content\n- Additional rate limiting strategies\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Acknowledgments\n\nThe code in this project references the following repositories:\n\n- [nickclyde/duckduckgo-mcp-server](https://github.com/nickclyde/duckduckgo-mcp-server)\n- [BaiduSpider/BaiduSpider](https://github.com/BaiduSpider/BaiduSpider)\n\nThanks to the authors and contributors of these repositories for their efforts and contributions to the open-source community.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "baidu",
        "searches",
        "webpage",
        "baidu mcp",
        "web search",
        "search evilran"
      ],
      "category": "web-search"
    },
    "ExactDoug--mcp-fetch": {
      "owner": "ExactDoug",
      "name": "mcp-fetch",
      "url": "https://github.com/ExactDoug/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/ExactDoug.webp",
      "description": "Fetches web content and converts it to markdown format for easy processing by LLMs. It retrieves content from specified URLs and allows for extraction in manageable chunks by specifying character indexes.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-05T15:41:24Z",
      "readme_content": "# Fetch MCP Server\n\nA Model Context Protocol server that provides web content fetching capabilities. This server enables LLMs to retrieve and process content from web pages, converting HTML to markdown for easier consumption.\n\nThe fetch tool will truncate the response, but by using the `start_index` argument, you can specify where to start the content extraction. This lets models read a webpage in chunks, until they find the information they need.\n\n### Available Tools\n\n- `fetch` - Fetches a URL from the internet and extracts its contents as markdown.\n    - `url` (string, required): URL to fetch\n    - `max_length` (integer, optional): Maximum number of characters to return (default: 5000)\n    - `start_index` (integer, optional): Start content from this character index (default: 0)\n    - `raw` (boolean, optional): Get raw content without markdown conversion (default: false)\n\n### Prompts\n\n- **fetch**\n    - Fetch a URL and extract its contents as markdown\n    - Arguments:\n        - `url` (string, required): URL to fetch\n\n## Installation\n\nOptionally: Install node.js, this will cause the fetch server to use a different HTML simplifier that is more robust.\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run `mcp-server-fetch`.\n\n### Using PIP\n\nAlternatively you can install `mcp-server-fetch` via pip:\n\n```\npip install mcp-server-fetch\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "fetches",
        "mcp",
        "fetch",
        "mcp fetch",
        "search exactdoug",
        "exactdoug mcp"
      ],
      "category": "web-search"
    },
    "FradSer--mcp-server-local-web-search": {
      "owner": "FradSer",
      "name": "mcp-server-local-web-search",
      "url": "https://github.com/FradSer/mcp-server-local-web-search",
      "imageUrl": "/freedevtools/mcp/pfp/FradSer.webp",
      "description": "Perform local web searches and extract relevant content from web pages, returning structured results including titles, URLs, and descriptions. The server supports customizable result limits, content truncation, domain filtering, and utilizes headless browser operations for enhanced performance.",
      "stars": 9,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T20:59:41Z",
      "readme_content": "# mcp-server-local-web-search\n\n![A_FRAD_PRODUCT_WIP_yellow](https://img.shields.io/badge/A%20FRAD%20PRODUCT-WIP-yellow) [![Twitter Follow](https://img.shields.io/twitter/follow/FradSer?style=social)](https://twitter.com/FradSer)\n\nAn MCP server for performing local web searches. This server provides tools to search and extract content from web pages through the Model Context Protocol.\n\n## Features\n\n- Perform web searches with customizable result limits\n- Extract and process content from web pages\n- Return structured results with titles, URLs, and descriptions\n- Support for content truncation and domain filtering\n- Clean content extraction using Readability\n- Headless browser operation for improved performance\n\n## Installation\n\nTo install dependencies:\n\n```bash\nbun install\n```\n\n## Setup\n\nRun the setup script to configure the MCP server:\n\n```bash\nbun run setup.ts\n```\n\nThis will add the server to your Claude MCP configuration.\n\n### Available Tools\n\n1. `local_web_search`\n   - Performs web search and returns results with title, URL and description\n   - Parameters:\n     - `query`: Search query to find relevant content (required)\n     - `excludeDomains`: List of domains to exclude from search results (default: [])\n     - `limit`: Maximum number of results to return (default: 5)\n     - `truncate`: Maximum length of content to return per result (default: 4000)\n     - `show`: Show browser window for debugging (default: false)\n     - `proxy`: Proxy server to use for requests (optional)\n\n## Requirements\n\n- [Bun](https://bun.sh) runtime\n- Node.js TypeScript support\n\n## Development\n\nThis project uses:\n\n- [Bun](https://bun.sh) as the JavaScript runtime\n- [TypeScript](https://www.typescriptlang.org/) for type safety\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk) for server implementation\n- [@egoist/local-web-search](https://github.com/egoist/local-web-search/) for web search (using playwright-core)\n- [Readability](https://github.com/mozilla/readability) for content extraction\n\n## Contributors\n- [egoist](https://github.com/egoist) - Original local web search author\n- [FradSer](https://github.com/FradSer) - Original author\n- [TheSethRose](https://github.com/TheSethRose) - Playwright integration and performance improvements\n\n## License\n\nMIT License\n\nThis project was created using `bun init` in bun v1.2.2. [Bun](https://bun.sh) is a fast all-in-one JavaScript runtime.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "web",
        "search fradser",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "Hajime-Y--deep-research-mcp": {
      "owner": "Hajime-Y",
      "name": "deep-research-mcp",
      "url": "https://github.com/Hajime-Y/deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Hajime-Y.webp",
      "description": "Provides advanced web search capabilities, document analysis, and image processing. Extracts information from various sources including PDFs and YouTube transcripts efficiently.",
      "stars": 12,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-06T18:57:01Z",
      "readme_content": "# Deep Research MCP Server\n\nDeep Research is an agent-based tool that provides web search and advanced research capabilities. It leverages HuggingFace's `smolagents` and is implemented as an MCP server.\n\nThis project is based on [HuggingFace's open_deep_research example](https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research).\n\n## Features\n\n- Web search and information gathering\n- PDF and document analysis\n- Image analysis and description\n- YouTube transcript retrieval\n- Archive site search\n\n## Requirements\n\n- Python 3.11 or higher\n- `uv` package manager\n- The following API keys:\n  - OpenAI API key\n  - HuggingFace token\n  - SerpAPI key\n\n## Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/Hajime-Y/deep-research-mcp.git\ncd deep-research-mcp\n```\n\n2. Create a virtual environment and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate # For Linux or Mac\n# .venv\\Scripts\\activate # For Windows\nuv sync\n```\n\n## Environment Variables\n\nCreate a `.env` file in the root directory of the project and set the following environment variables:\n\n```\nOPENAI_API_KEY=your_openai_api_key\nHF_TOKEN=your_huggingface_token\nSERPER_API_KEY=your_serper_api_key\n```\n\nYou can obtain a SERPER_API_KEY by signing up at [Serper.dev](https://serper.dev/signup).\n\n## Usage\n\nStart the MCP server:\n\n```bash\nuv run deep_research.py\n```\n\nThis will launch the `deep_research` agent as an MCP server.\n\n## Docker Usage\n\nYou can also run this MCP server in a Docker container:\n\n```bash\n# Build the Docker image\ndocker build -t deep-research-mcp .\n\n# Run with required API keys\ndocker run -p 8080:8080 \\\n  -e OPENAI_API_KEY=your_openai_api_key \\\n  -e HF_TOKEN=your_huggingface_token \\\n  -e SERPER_API_KEY=your_serper_api_key \\\n  deep-research-mcp\n```\n\n### Registering with MCP Clients\n\nTo register this Docker container as an MCP server in different clients:\n\n#### Claude Desktop\n\nAdd the following to your Claude Desktop configuration file (typically located at `~/.config/Claude/claude_desktop_config.json` on Linux, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS, or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \"OPENAI_API_KEY=your_openai_api_key\",\n        \"-e\", \"HF_TOKEN=your_huggingface_token\", \n        \"-e\", \"SERPER_API_KEY=your_serper_api_key\",\n        \"deep-research-mcp\"\n      ]\n    }\n  }\n}\n```\n\n#### Cursor IDE\n\nFor Cursor IDE, add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \"OPENAI_API_KEY=your_openai_api_key\",\n        \"-e\", \"HF_TOKEN=your_huggingface_token\", \n        \"-e\", \"SERPER_API_KEY=your_serper_api_key\",\n        \"deep-research-mcp\"\n      ]\n    }\n  }\n}\n```\n\n#### Using with Remote MCP Server\n\nIf you're running the MCP server on a remote machine or exposing it as a service, you can use the URL-based configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"url\": \"http://your-server-address:8080/mcp\",\n      \"type\": \"sse\"\n    }\n  }\n}\n```\n\n## Key Components\n\n- `deep_research.py`: Entry point for the MCP server\n- `create_agent.py`: Agent creation and configuration\n- `scripts/`: Various tools and utilities\n  - `text_web_browser.py`: Text-based web browser\n  - `text_inspector_tool.py`: File inspection tool\n  - `visual_qa.py`: Image analysis tool\n  - `mdconvert.py`: Converts various file formats to Markdown\n\n## License\n\nThis project is provided under the Apache License 2.0.\n\n## Acknowledgements\n\nThis project uses code from HuggingFace's `smolagents` and Microsoft's `autogen` projects.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "hajime",
        "mcp",
        "search hajime",
        "web search",
        "research mcp"
      ],
      "category": "web-search"
    },
    "IA-Programming--Youtube-MCP": {
      "owner": "IA-Programming",
      "name": "Youtube-MCP",
      "url": "https://github.com/IA-Programming/Youtube-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/IA-Programming.webp",
      "description": "Search for YouTube videos, retrieve their transcripts, and conduct semantic searches over video content to extract insights.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:22Z",
      "readme_content": "# YouTube MCP Server\n\nA Model Context Protocol (MCP) server that provides tools for searching YouTube videos, retrieving transcripts, and performing semantic search over video content.\n\n## Support Us\n\nIf you find this project helpful and would like to support future projects, consider buying us a coffee! Your support helps us continue building innovative AI solutions.\n\n<a href=\"https://www.buymeacoffee.com/blazzmocompany\"><img alt=\"text_Buy_me_a_coffee_emoji_slug_blazzmocompany_button_colour_40DCA5_font_colour_ffffff_font_family_Cookie_outline_colour_000000_coffee_colour_FFDD00\" src=\"https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=blazzmocompany&button_colour=40DCA5&font_colour=ffffff&font_family=Cookie&outline_colour=000000&coffee_colour=FFDD00\"></a>\n\nYour contributions go a long way in fueling our passion for creating intelligent and user-friendly applications.\n\n## Table of Contents\n\n- [YouTube MCP Server](#youtube-mcp-server)\n- [Features](#features)\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Running the Server](#running-the-server)\n  - [1. Direct Method](#1-direct-method)\n  - [2. Configure for Claude.app](#2-configure-for-claudeapp)\n- [Available Tools](#available-tools)\n- [Using with MCP Clients](#using-with-mcp-clients)\n  - [Example Usage](#example-usage)\n- [Debugging](#debugging)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Features\n\n- Search YouTube videos without using the official API\n- Retrieve video transcripts\n- Store video information and transcripts in a vector database\n- Perform semantic search over stored video transcripts\n\n## Prerequisites\n\n- Python 3.8+\n- Google API key for embeddings\n- uv package manager\n\n## Installation\n\n1. Clone this repository\n\n2. Create and activate a virtual environment using uv:\n```bash\nuv venv\n# On Windows:\n.venv\\Scripts\\activate\n# On Unix/MacOS:\nsource .venv/bin/activate\n```\n\n3. Install dependencies using uv:\n```bash\nuv pip install -r requirements.txt\n```\n\n4. Create a `.env` file with your Google API key:\n```\nGOOGLE_API_KEY=your_api_key_here\n```\n\n## Running the Server\n\nThere are two ways to run the MCP server:\n\n### 1. Direct Method\n\nTo start the MCP server directly:\n\n```bash\nuv run python server.py\n```\n\n### 2. Configure for Claude.app\n\nAdd to your Claude settings without using any package manager this works for windows:\n```json\n\"mcpServers\": {\n  \"youtube\": {\n    \"command\": \"C:\\\\Path\\\\To\\\\Your\\\\Project\\\\.venv\\\\Scripts\\\\python.exe\",\n    \"args\": [\"C:\\\\Path\\\\To\\\\Your\\\\Project\\\\server.py\"],\n    \"env\": {\n      \"GOOGLE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\nUsing Uv package manager this works for windows:\n\n```json\n\"mcpServers\": {\n  \"youtube\": {\n    \"command\": \"uv\",\n    \"args\": [\"--directory\", \"C:\\\\Path\\\\To\\\\Your\\\\Project\", \"run\", \"server.py\"],\n    \"env\": {\n      \"GOOGLE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server provides the following tools:\n\n1. `search-youtube`: Search for YouTube videos based on a query\n   - Parameters:\n     - query: Search query string\n     - max_results: Maximum number of results to return (default: 5)\n\n2. `get-transcript`: Get the transcript of a YouTube video\n   - Parameters:\n     - video_url: URL of the YouTube video\n\n3. `store-video-info`: Store video information and transcript in the vector database\n   - Parameters:\n     - video_url: URL of the YouTube video\n     - metadata: Optional metadata about the video\n\n4. `search-transcripts`: Search stored video transcripts using semantic search\n   - Parameters:\n     - query: Search query\n     - limit: Maximum number of results to return (default: 3)\n\n## Using with MCP Clients\n\nThis server can be used with any MCP-compatible client, such as Claude Desktop App. The tools will be automatically discovered and made available to the client.\n\n### Example Usage\n\n1. Start the server using one of the methods described above\n2. Open Claude Desktop App\n3. Look for the hammer icon to verify that the YouTube tools are available\n4. You can now use commands like:\n   - \"Search for Python tutorial videos\"\n   - \"Get the transcript of this video: [video_url]\"\n   - \"Search through stored video transcripts about machine learning\"\n\n## Debugging\n\nIf you encounter any issues:\n\n1. Make sure your Google API key is correctly set in the `.env` file\n2. Check that all dependencies are installed correctly\n3. Verify that the server is running and listening for connections\n4. Look for any error messages in the server output\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp",
      "npm_downloads": 590,
      "keywords": [
        "youtube",
        "searches",
        "search",
        "programming youtube",
        "search youtube",
        "searches video"
      ],
      "category": "web-search"
    },
    "JackKuo666--medRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "medRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/medRxiv-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Integrate with medRxiv's preprint repository to search for and access health sciences research papers and their metadata via a standardized interface. Enables efficient querying and retrieval of paper details based on custom search parameters and DOI.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-29T12:23:22Z",
      "readme_content": "# medRxiv MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/medrxiv-mcp-server)](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server)\n\n🔍 Enable AI assistants to search and access medRxiv papers through a simple MCP interface.\n\nThe medRxiv MCP Server provides a bridge between AI assistants and medRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for health sciences preprints and access their content in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query medRxiv papers with custom search strings or advanced search parameters ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers using DOI ✅\n- 📊 Research Support: Facilitate health sciences research and analysis ✅\n- 📄 Paper Access: Download and read paper content 📝\n- 📋 Paper Listing: View all downloaded papers 📝\n- 🗃️ Local Storage: Papers are saved locally for faster access 📝\n- 📝 Research Prompts: A set of specialized prompts for paper analysis 📝\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install medRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/medrxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client cline --config \"{}\"\n```\n\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install medRxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/medRxiv-MCP-Server.git\ncd medRxiv-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython medrxiv_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_key_words\", {\n    \"key_words\": \"COVID-19 vaccine efficacy\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_advanced\", {\n    \"term\": \"COVID-19\",\n    \"author1\": \"MacLachlan\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2023-12-31\",\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get metadata for a specific paper\n\n```python\nresult = await mcp.use_tool(\"get_medrxiv_metadata\", {\n    \"doi\": \"10.1101/2025.03.09.25323517\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the medRxiv MCP Server. Adjust the parameters as needed for your specific use case.\n\n## 🛠 MCP Tools\n\nThe medRxiv MCP Server provides the following tools:\n\n### search_medrxiv_key_words\n\nSearch for articles on medRxiv using key words.\n\n**Parameters:**\n- `key_words` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### search_medrxiv_advanced\n\nPerform an advanced search for articles on medRxiv.\n\n**Parameters:**\n- `term` (str, optional): General search term\n- `title` (str, optional): Search in title\n- `author1` (str, optional): First author\n- `author2` (str, optional): Second author\n- `abstract_title` (str, optional): Search in abstract and title\n- `text_abstract_title` (str, optional): Search in full text, abstract, and title\n- `section` (str, optional): Section of medRxiv\n- `start_date` (str, optional): Start date for search range (format: YYYY-MM-DD)\n- `end_date` (str, optional): End date for search range (format: YYYY-MM-DD)\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### get_medrxiv_metadata\n\nFetch metadata for a medRxiv article using its DOI.\n\n**Parameters:**\n- `doi` (str): DOI of the article\n\n**Returns:** Dictionary containing article metadata\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"medrxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"medrxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-medRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-medRxiv/medrxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Papers\n\nYou can ask Claude to search for papers using queries like:\n```\nCan you search medRxiv for recent papers about genomics?\n```\n\nThe search will return basic information about matching papers including:\n\n• Paper title\n\n• Authors\n\n• DOI\n\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the details for paper 10.1101/003541?\n```\n\nThis will return:\n\n• Full paper title\n\n• Authors\n\n• Publication date\n\n• Paper abstract\n\n• Links to available formats (PDF/HTML)\n\n\n\n## 📝 TODO\n\n### download_paper\n\nDownload a paper and save it locally.\n\n### read_paper\n\nRead the content of a downloaded paper.\n\n### list_papers\n\nList all downloaded papers.\n\n### 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n#### Paper Analysis Prompt\n\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## 📁 Project Structure\n\n- `medrxiv_server.py`: The main MCP server implementation using FastMCP\n- `medrxiv_web_search.py`: Contains the web scraping logic for searching medRxiv\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests (for web scraping, used in medrxiv_web_search.py)\n- beautifulsoup4 (for web scraping, used in medrxiv_web_search.py)\n\nYou can install the required dependencies using:\n\n```bash\npip install FastMCP requests beautifulsoup4\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## 🙏 Acknowledgements\n\nThis project is inspired by and built upon the work done in the [arxiv-mcp-server](https://github.com/blazickjp/arxiv-mcp-server) project.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect medRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "medrxiv",
        "search",
        "retrieval",
        "papers metadata",
        "retrieval paper",
        "medrxiv mcp"
      ],
      "category": "web-search"
    },
    "JackKuo666--semanticscholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "semanticscholar-MCP-Server",
      "url": "https://github.com/JackKuo666/semanticscholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Search for academic papers, retrieve detailed information about specific papers and authors, and access citations and references through the Semantic Scholar API.",
      "stars": 29,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T19:47:33Z",
      "readme_content": "# 🎓 Semantic Scholar MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/semanticscholar-mcp-server)](https://smithery.ai/server/@JackKuo666/semanticscholar-mcp-server)\n\nThis project implements a Model Context Protocol (MCP) server for interacting with the Semantic Scholar API. It provides tools for searching papers, retrieving paper and author details, and fetching citations and references.\n\n## ✨ Features\n\n- 🔍 Search for papers on Semantic Scholar\n- 📄 Retrieve detailed information about specific papers\n- 👤 Get author details\n- 🔗 Fetch citations and references for a paper\n\n## 📋 Prerequisites\n\n- 🐍 Python 3.10+\n- 📚 `semanticscholar` Python package\n- 🔧 `mcp` Python package (Model Context Protocol)\n\n## 🚀 Installation\n### Installing via Smithery\n\nTo install semanticscholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/semanticscholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/semanticscholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client cline --config \"{}\"\n```\n\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/JackKuo666/semanticscholar-MCP-Server.git\n   cd semanticscholar-mcp-server\n   ```\n\n2. Install the required packages:\n   ```\n   pip install semanticscholar mcp\n   ```\n\n## 🖥️ Usage\n\n1. Start the Semantic Scholar MCP server:\n   ```\n   python semantic_scholar_server.py\n   ```\n\n2. The server will start and listen for MCP requests.\n\n3. Use an MCP client to interact with the server and access the following tools:\n\n   - 🔍 `search_semantic_scholar`: Search for papers using a query string\n   - 📄 `get_semantic_scholar_paper_details`: Get details of a specific paper\n   - 👤 `get_semantic_scholar_author_details`: Get details of a specific author\n   - 🔗 `get_semantic_scholar_citations_and_references`: Get citations and references for a paper\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"semanticscholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\semanticscholar-MCP-Server\\\\semanticscholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/semanticscholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 📁 File Structure\n\n- 📜 `semantic_scholar_search.py`: Contains functions for interacting with the Semantic Scholar API\n- 🖥️ `semantic_scholar_server.py`: Implements the MCP server and defines the available tools\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "citations",
        "search",
        "semantic scholar",
        "scholar api",
        "search academic"
      ],
      "category": "web-search"
    },
    "JavaProgrammerLB--unsplash-mcp-server": {
      "owner": "JavaProgrammerLB",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/JavaProgrammerLB/unsplash-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/JavaProgrammerLB.webp",
      "description": "Search for high-quality images from Unsplash, enabling applications to integrate visual search capabilities. This server also serves as a learning resource for building MCP servers in Java.",
      "stars": 7,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-07-15T07:26:39Z",
      "readme_content": "# Unsplash MCP Server\n- With this mcp server, you can easily search picture from **[unsplash](https://unsplash.com/)**\n- With this project, you can easily learn how to write MCP Server with JAVA\n> **search two house picture from unsplash** and write to the document\n\n## Feature\n\n\n\n## How to use\n\n1. Clone this project\n```\ngit clone https://github.com/JavaProgrammerLB/unsplash-mcp-server.git\n```\n\n2. Build\n```\ncd unsplash-mcp-server\nmvn clean package\n```\n3. Get Unsplash Access Key\n- visit [unsplash](https://unsplash.com/developers)\n- create an unsplash application\n- find out the access key in unsplash application detail page\n\n4. Config MCP Server\n```\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-Dspring.ai.mcp.server.stdio=true\",\n        \"-Dspring.main.web-application-type=none\",\n        \"-Dlogging.pattern.console=\",\n        \"-jar\",\n        \"/ABSOLUTE/PATH/target/unsplash-mcp-server-1.0.jar\"\n      ],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"${YOUR UNSPLASH ACCESS KEY}\"\n      }\n    }\n  }\n}\n```\n\n## Thanks\n- [Python Version Unsplash MCP Server](https://github.com/hellokaton/unsplash-mcp-server)",
      "npm_url": "https://www.npmjs.com/package/unsplash-mcp-server",
      "npm_downloads": 315,
      "keywords": [
        "javaprogrammerlb",
        "java",
        "mcp",
        "search javaprogrammerlb",
        "server search",
        "javaprogrammerlb unsplash"
      ],
      "category": "web-search"
    },
    "JayArrowz--mcp-osrs": {
      "owner": "JayArrowz",
      "name": "mcp-osrs",
      "url": "https://github.com/JayArrowz/mcp-osrs",
      "imageUrl": "/freedevtools/mcp/pfp/JayArrowz.webp",
      "description": "Seamless access to Old School RuneScape Wiki content and game data through a standardized protocol. Enables searching wiki pages, retrieving detailed page information, and querying various game data files to enhance applications with OSRS context.",
      "stars": 11,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T20:23:42Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jayarrowz-mcp-osrs-badge.png)](https://mseep.ai/app/jayarrowz-mcp-osrs)\n\n# OSRS MCP Server [![smithery badge](https://smithery.ai/badge/@jayarrowz/mcp-osrs)](https://smithery.ai/server/@jayarrowz/mcp-osrs)\n\nMCP Server for interacting with the Old School RuneScape (OSRS) Wiki API and data files. This server provides tools to search the OSRS Wiki and access game data definitions through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/@JayArrowz/mcp-osrs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@JayArrowz/mcp-osrs/badge\" alt=\"OSRS Server MCP server\" />\n</a>\n\n![image](https://github.com/user-attachments/assets/da9d1f48-513d-4a1b-a65b-56f8a012fa83)\n\n![image](https://github.com/user-attachments/assets/9e7e4e07-6e47-44f9-ab0c-b3835418bd37)\n\n![image](https://github.com/user-attachments/assets/628f35e1-2e85-42f4-8693-4ef4f16591d4)\n\n\n## Tools\n\nThis server implements the following tools:\n\n### OSRS Wiki Methods\n1. `osrs_wiki_search` - Search the OSRS Wiki for pages matching a search term\n2. `osrs_wiki_get_page_info` - Get information about specific pages on the OSRS Wiki\n3. `osrs_wiki_parse_page` - Get the parsed HTML content of a specific OSRS Wiki page\n\n### Game Data Search Methods\n4. `search_varptypes` - Search the varptypes.txt file for player variables (varps) that store player state and progress\n5. `search_varbittypes` - Search the varbittypes.txt file for variable bits (varbits) that store individual bits from varps\n6. `search_iftypes` - Search the iftypes.txt file for interface definitions used in the game's UI\n7. `search_invtypes` - Search the invtypes.txt file for inventory type definitions in the game\n8. `search_loctypes` - Search the loctypes.txt file for location/object type definitions in the game world\n9. `search_npctypes` - Search the npctypes.txt file for NPC (non-player character) definitions\n10. `search_objtypes` - Search the objtypes.txt file for object/item definitions in the game\n11. `search_rowtypes` - Search the rowtypes.txt file for row definitions used in various interfaces\n12. `search_seqtypes` - Search the seqtypes.txt file for animation sequence definitions\n13. `search_soundtypes` - Search the soundtypes.txt file for sound effect definitions in the game\n14. `search_spottypes` - Search the spottypes.txt file for spot animation (graphical effect) definitions\n15. `search_spritetypes` - Search the spritetypes.txt file for sprite image definitions used in the interface\n16. `search_tabletypes` - Search the tabletypes.txt file for interface tab definitions\n\n### Generic Data File Methods\n17. `search_data_file` - Search any file in the data directory for matching entries\n18. `get_file_details` - Get details about a file in the data directory\n19. `list_data_files` - List available data files in the data directory\n\n## Installation\n\n### Installing via Smithery\nTo install mcp-osrs for Claude Desktop automatically via [Smithery](https://smithery.ai/embed/@jayarrowz/mcp-osrs):\n\n```bash\nnpx @smithery/cli@latest install @jayarrowz/mcp-osrs --client claude\n```\n\n### Prerequisites\n- Node.js (v16 or later)\n- npm or yarn\n\n### Installing the package\n```bash\n# Clone the repository\ngit clone https://github.com/jayarrowz/mcp-osrs.git\ncd mcp-osrs\n\n# Install dependencies\nnpm install\n\n# Build the package\nnpm run build\n```\n\n## Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n### Using npx\n```json\n{\n  \"mcpServers\": {\n    \"osrs\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jayarrowz/mcp-osrs\"]\n    }\n  }\n}\n```\n\n### Direct Node.js\n```json\n{\n  \"mcpServers\": {\n    \"osrs\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-osrs/dist/index.js\"]\n    }\n  }\n}\n```\n\nReplace `/path/to/mcp-osrs` with the actual path to your repository.\n\n## Examples\n\n### Search the OSRS Wiki\n```javascript\n// Search for information about the Abyssal whip\nconst result = await callTool(\"osrs_wiki_search\", { \n  search: \"Abyssal whip\" \n});\n```\n\n### Get Page Information\n```javascript\n// Get information about a specific wiki page\nconst pageInfo = await callTool(\"osrs_wiki_get_page_info\", { \n  titles: \"Abyssal_whip\" \n});\n```\n\n### Search Game Data\n```javascript\n// Search for items in the object definitions\nconst items = await callTool(\"search_objtypes\", { \n  query: \"dragon\",\n  page: 1,\n  pageSize: 10\n});\n```\n\n### List Available Data Files\n```javascript\n// Get a list of all data files\nconst files = await callTool(\"list_data_files\", {});\n```\n\n## Development\n```bash\n# Install dependencies\nnpm install\n\n# Start the server in development mode\nnpm start\n\n# Build the server\nnpm run build\n```\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/@jayarrowz/mcp-osrs",
      "npm_downloads": 3398,
      "keywords": [
        "runescape",
        "osrs",
        "wiki",
        "runescape wiki",
        "osrs context",
        "applications osrs"
      ],
      "category": "web-search"
    },
    "JeremyNixon--mcp-fetch": {
      "owner": "JeremyNixon",
      "name": "mcp-fetch",
      "url": "https://github.com/JeremyNixon/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/JeremyNixon.webp",
      "description": "Fetches web content and processes images for integration with AI models, streamlining the retrieval and handling of online content in various applications.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-15T18:56:32Z",
      "readme_content": "# MCP Fetch\n\n[![smithery badge](https://smithery.ai/badge/@kazuph/mcp-fetch)](https://smithery.ai/server/@kazuph/mcp-fetch)\n\nModel Context Protocol server for fetching web content and processing images. This allows Claude Desktop (or any MCP client) to fetch web content and handle images appropriately.\n\n## Quick Start (For Users)\n\nTo use this tool with Claude Desktop, simply add the following to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"tools\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-fetch\"]\n    }\n  }\n}\n```\n\nThis will automatically download and run the latest version of the tool when needed.\n\n### Required Setup\n\n1. Enable Accessibility for Claude:\n   - Open System Settings\n   - Go to Privacy & Security > Accessibility\n   - Click the \"+\" button\n   - Add Claude from your Applications folder\n   - Turn ON the toggle for Claude\n\nThis accessibility setting is required for automated clipboard operations (Cmd+V) to work properly.\n\n## For Developers\n\nThe following sections are for those who want to develop or modify the tool.\n\n## Prerequisites\n\n- Node.js 18+\n- macOS (for clipboard operations)\n- Claude Desktop (install from https://claude.ai/desktop)\n- tsx (install via `npm install -g tsx`)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MCP Fetch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kazuph/mcp-fetch):\n\n```bash\nnpx -y @smithery/cli install @kazuph/mcp-fetch --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/kazuph/mcp-fetch.git\ncd mcp-fetch\nnpm install\nnpm run build\n```\n\n## Image Processing Specifications\n\nWhen processing images from web content, the following limits are applied:\n\n- Maximum 6 images per group\n- Maximum height of 8000 pixels per group\n- Maximum size of 30MB per group\n\nIf content exceeds these limits, images will be automatically split into multiple groups, and you'll need to paste (Cmd+V) multiple times.\n\n## Configuration\n\n1. Make sure Claude Desktop is installed and running.\n\n2. Install tsx globally if you haven't:\n```bash\nnpm install -g tsx\n# or\npnpm add -g tsx\n```\n\n3. Modify your Claude Desktop config located at:\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n\nYou can easily find this through the Claude Desktop menu:\n1. Open Claude Desktop\n2. Click Claude on the Mac menu bar\n3. Click \"Settings\"\n4. Click \"Developer\"\n\nAdd the following to your MCP client's configuration:\n\n```json\n{\n  \"tools\": {\n    \"fetch\": {\n      \"args\": [\"tsx\", \"/path/to/mcp-fetch/index.ts\"]\n    }\n  }\n}\n```\n\n## Available Tools\n\n- `fetch`: Retrieves URLs from the Internet and extracts their content as markdown. Images are automatically processed and prepared for clipboard operations.\n\n## Notes\n\n- This tool is designed for macOS only due to its dependency on macOS-specific clipboard operations.\n- Images are processed using Sharp for optimal performance and quality.\n- When multiple images are found, they are merged vertically with consideration for size limits.\n- Animated GIFs are automatically handled by extracting their first frame.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "retrieval",
        "ai",
        "search",
        "web search",
        "ai models",
        "fetches web"
      ],
      "category": "web-search"
    },
    "JoeBuildsStuff--mcp-jina-ai": {
      "owner": "JoeBuildsStuff",
      "name": "mcp-jina-ai",
      "url": "https://github.com/JoeBuildsStuff/mcp-jina-ai",
      "imageUrl": "/freedevtools/mcp/pfp/JoeBuildsStuff.webp",
      "description": "Access Jina AI's web services for web page reading, web search, and fact checking. Extract and format content from web pages for use with LLMs.",
      "stars": 30,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T04:12:28Z",
      "readme_content": "# Jina AI MCP Server\n[![smithery badge](https://smithery.ai/badge/jina-ai-mcp-server)](https://smithery.ai/server/jina-ai-mcp)\n[![smithery badge](https://smithery.ai/badge/jina-ai-mcp-server)](https://smithery.ai/server/jina-ai-mcp-server)\n\nAn MCP server that provides access to Jina AI's powerful web services through Claude. This server implements three main tools:\n\n- Web page reading and content extraction\n- Web search\n- Fact checking/grounding\n\n<a href=\"https://glama.ai/mcp/servers/c1l6ib2j49\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/c1l6ib2j49/badge\" alt=\"mcp-jina-ai MCP server\" /></a>\n\n## Features\n\n### Tools\n\n#### `read_webpage`\n- Extract content from web pages in a format optimized for LLMs\n- Supports multiple output formats (Default, Markdown, HTML, Text, Screenshot, Pageshot)\n- Options for including links and images\n- Ability to generate alt text for images\n- Cache control options\n\n#### `search_web`\n- Search the web using Jina AI's search API\n- Configurable number of results (default: 5)\n- Support for image retention and alt text generation\n- Multiple return formats (markdown, text, html)\n- Returns structured results with titles, descriptions, and content\n\n#### `fact_check`\n- Fact-check statements using Jina AI's grounding engine\n- Provides factuality scores and supporting evidence \n- Optional deep-dive mode for more thorough analysis\n- Returns references with key quotes and supportive/contradictory classification\n\n## Setup\n\n### Prerequisites\n\nYou'll need a Jina AI API key to use this server. Get one for free at https://jina.ai/\n\n### Installation\n\nThere are two ways to use this server:\n\n#### Installing via Smithery\n\nTo install Jina AI for Claude Desktop automatically via [Smithery](https://smithery.ai/server/jina-ai-mcp-server):\n\n```bash\nnpx -y @smithery/cli install jina-ai-mcp-server --client claude\n```\n\n#### Option 1: NPX (Recommended)\nAdd this configuration to your Claude Desktop config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"jina-ai-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"jina-ai-mcp-server\"\n      ],\n      \"env\": {\n        \"JINA_API_KEY\": \"<YOUR_KEY>\"\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Local Installation\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Add this configuration to your Claude Desktop config:\n```json\n{\n  \"mcpServers\": {\n    \"jina-ai-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/jina-ai-mcp-server/dist/index.js\"\n      ],\n      \"env\": {\n        \"JINA_API_KEY\": \"<YOUR_KEY>\"\n      }\n    }\n  }\n}\n```\n\n### Config File Location\n\nOn MacOS:\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\nOn Windows:\n```bash\n%APPDATA%/Claude/claude_desktop_config.json\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## API Response Types\n\nAll tools return structured JSON responses that include:\n\n- Status codes and metadata\n- Formatted content based on the requested output type\n- Usage information (token counts)\n- When applicable: images, links, and additional metadata\n\nFor detailed schema information, see `schemas.ts`.\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts index.ts\n```",
      "npm_url": "https://www.npmjs.com/package/mcp-jina-ai",
      "npm_downloads": 480,
      "keywords": [
        "web",
        "search",
        "jina",
        "web search",
        "ai web",
        "reading web"
      ],
      "category": "web-search"
    },
    "JonaFly--RedNote-MCP": {
      "owner": "JonaFly",
      "name": "RedNote-MCP",
      "url": "https://github.com/JonaFly/RedNote-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/JonaFly.webp",
      "description": "Access and interact with Xiaohongshu (RedNote) content by authenticating, searching notes via keywords, and retrieving note content through URLs. Simplifies the integration of RedNote data into AI workflows and tools.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:31:39Z",
      "readme_content": "# RedNote MCP\n\n[![English](https://img.shields.io/badge/English-Click-yellow)](docs/README.en.md)\n[![简体中文](https://img.shields.io/badge/简体中文-点击查看-orange)](README.md)\n[![npm](https://img.shields.io/npm/v/rednote-mcp)](https://www.npmjs.com/package/rednote-mcp)\n[![smithery badge](https://smithery.ai/badge/@JonaFly/rednote-mcp)](https://smithery.ai/server/@JonaFly/rednote-mcp)\n\n小红书内容访问的MCP服务\n\nhttps://github.com/user-attachments/assets/06b2c67f-d9ed-4a30-8f1d-9743f3edaa3a\n\n## 快速开始\n\n开始前确保安装了 [playwright](https://github.com/microsoft/playwright) 环境：\n\n```bash\nnpx playwright install\n```\n\n### Installing via Smithery\n\nTo install rednote-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JonaFly/rednote-mcp):\n\n```bash\nnpx -y @smithery/cli install @JonaFly/rednote-mcp --client claude\n```\n\n### NPM 全局安装\n\n```bash\n# 全局安装\nnpm install -g rednote-mcp\n\n# 初始化登录，会自动记录cookie到 ~/.mcp/rednote/cookies.json\nrednote-mcp init\n```\n\n### 从源码安装\n\n```bash\n# 克隆项目\ngit clone https://github.com/ifuryst/rednote-mcp.git\ncd rednote-mcp\n\n# 安装依赖\nnpm install\n\n# 全局安装（可选，方便命令行调用）\nnpm install -g .\n\n# 或者直接运行，如初始化登录\nnpm run dev -- init\n```\n\n## 功能特性\n\n- 认证管理（支持 Cookie 持久化）\n- 关键词搜索笔记\n- 命令行初始化工具\n- 通过 URL 访问笔记内容\n- [ ] 通过 URL 访问评论内容\n\n## 使用说明\n\n### 1. 初始化登录\n\n首次使用需要先进行登录初始化：\n\n```bash\nrednote-mcp init\n# 或者直接从源码run\nnpm run dev -- init\n# 或者mcp-client里选择login\n```\n\n执行此命令后：\n\n1. 会自动打开浏览器窗口\n2. 跳转到小红书登录页面\n3. 请手动完成登录操作\n4. 登录成功后会自动保存 Cookie 到 `~/.mcp/rednote/cookies.json` 文件\n\n### 2. 在 Cursor 中配置 MCP Server\n\n在 Cursor 的 settings.json 中添加以下配置：\n\n```json\n{\n  \"mcpServers\": {\n    \"RedNote MCP\": {\n      \"command\": \"rednote-mcp\",\n      \"args\": [\n        \"--stdio\"\n      ]\n    }\n  }\n}\n```\n\n或者使用 npx 方式：\n\n```json\n{\n  \"mcpServers\": {\n    \"RedNote MCP\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"rednote-mcp\",\n        \"--stdio\"\n      ]\n    }\n  }\n}\n```\n\n配置说明：\n\n- `command`: 可以是全局安装后的 `rednote-mcp` 命令，或使用 `npx` 直接运行\n- `args`: 必须包含 `--stdio` 参数以支持 Cursor 的通信方式\n\n## 开发指南\n\n### 环境要求\n\n- Node.js >= 16\n- npm >= 7\n\n### 开发流程\n\n```bash\n# 安装依赖\nnpm install\n\n# 构建项目\nnpm run build\n\n# 开发模式运行\nnpm run dev\n\n# 运行测试\nnpm test\n```\n\n### 使用 MCP Inspector 进行调试\n\nMCP Inspector 是一个用于调试 MCP 服务器的工具，可以帮助开发者检查和验证 MCP 服务器的行为。使用以下命令启动：\n\n```bash\nnpx @modelcontextprotocol/inspector npx rednote-mcp --stdio\n```\n\n这个命令会：\n\n1. 启动 MCP Inspector 工具\n2. 通过 Inspector 运行 rednote-mcp 服务\n3. 提供一个交互式界面来检查请求和响应\n4. 帮助调试和验证 MCP 协议的实现\n\n## 注意事项\n\n1. 首次使用必须执行 `init` 命令进行登录\n2. Cookie 文件包含敏感信息，避免泄露\n3. 建议定期更新 Cookie，避免失效\n4. 确保已正确安装 Node.js 环境\n\n## 贡献指南\n\n1. Fork 本仓库\n2. 创建你的特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交你的改动 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 开启一个 Pull Request\n\n## 许可证\n\nMIT License - 详见 [LICENSE](LICENSE) 文件 \n",
      "npm_url": "https://www.npmjs.com/package/rednote-mcp",
      "npm_downloads": 3377,
      "keywords": [
        "rednote",
        "notes",
        "xiaohongshu",
        "rednote mcp",
        "xiaohongshu rednote",
        "rednote content"
      ],
      "category": "web-search"
    },
    "KBB99--mcp-registry-server": {
      "owner": "KBB99",
      "name": "mcp-registry-server",
      "url": "https://github.com/KBB99/mcp-registry-server",
      "imageUrl": "/freedevtools/mcp/pfp/KBB99.webp",
      "description": "Retrieve MCP Servers through semantic search capabilities, allowing users to find relevant servers efficiently based on queries.",
      "stars": 5,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-25T06:11:20Z",
      "readme_content": "# MCP Registry Server\n\n[![smithery badge](https://smithery.ai/badge/@KBB99/mcp-registry-server)](https://smithery.ai/server/@KBB99/mcp-registry-server)\n\n## Features\n\n- **MCP Retriever**: Retrieve MCP Servers using semantic search\n\n<a href=\"https://glama.ai/mcp/servers/8pg7mzcpt8\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/8pg7mzcpt8/badge\" alt=\"mcp-registry-server MCP server\" /></a>\n\n## Tools\n\n- **retrieve_mcps**\n  - Perform retrieval operations against the MCP registry.\n  - Inputs:\n    - `query` (string): The search query for retrieval.\n\n## Configuration\n\n### Installation Guide\n\n### Installing via Smithery\n\nTo install MCP Registry Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KBB99/mcp-registry-server):\n\n```bash\nnpx -y @smithery/cli install @KBB99/mcp-registry-server --client claude\n```\n\n### Usage with Claude Desktop\n\nFirst build the server:\n\n```bash\ngit clone https://github.com/KBB99/mcp-registry-server.git\ncd mcp-registry-server\nnpm install\nnpm run build\n```\n\nYou can confirm the server is working by running:\n\n```bash\nnode ./dist/index.js\n```\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-registry-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"./path/to/build/mcp-registry-server/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\nThis README assumes that your server package is named `mcp-server-retriever`. Adjust the package name and installation details if they differ in your setup. Also, ensure that your server script is correctly built and that all dependencies are properly managed in your `package.json`.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "registry",
        "mcp",
        "kbb99",
        "mcp registry",
        "mcp servers",
        "search kbb99"
      ],
      "category": "web-search"
    },
    "Karry-cpu--go-proxy-bingai": {
      "owner": "Karry-cpu",
      "name": "go-proxy-bingai",
      "url": "https://github.com/Karry-cpu/go-proxy-bingai",
      "imageUrl": "/freedevtools/mcp/pfp/Karry-cpu.webp",
      "description": "Access Microsoft Bing AI with a consistent user interface for seamless chat interactions and advanced features such as image generation. The server supports local deployment and customization while facilitating use without the need for a Microsoft account login.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-06-30T06:28:29Z",
      "readme_content": "# go-proxy-bing\n\n基于微软 New Bing 用 Vue3 和 Go 简单定制的微软 New Bing 演示站点，拥有一致的 UI 体验，支持 ChatGPT 提示词，国内可用，基本兼容微软 Bing AI 所有功能，无需登录即可畅聊。\n\n⭐ Bing 官方聊天服务器（相对较快和稳定，推荐）不可用时，可参考以下方案\n\n  > 1. 可用 ModHeader 添加 X-Forwarded-For 请求头，对应 URL 是 wss://sydney.bing.com/sydney/ChatHub，具体可参考 [issues #71](https://github.com/adams549659584/go-proxy-bingai/issues/71) 及 https://zhuanlan.zhihu.com/p/606655303\n\n  > 2. 本地部署再部署一份作为聊天中转服务，或下载 Release 直接运行，自定义聊天服务器中填入 http://localhost:8080，并选择。\n\n⭐ 聊天服务器 (暂时默认 Cloudflare Workers，请求数每天限额 100,000，撑不了多久 ，推荐自行部署，参考下面 [部署聊天服务器](#部署聊天服务器) ) 可在右上角 设置 => 服务选择 中切换\n\n⭐ 国内可用 （部署服务器需要直连 www.bing.com 不重定向 CN ，可配置 socks 连接）\n\n⭐ 支持现有开源 ChatGPT 提示词库\n\n⭐ 需要画图等高级功能时(需选更有创造力模式或右上角 设置 => 图像创建 )，可登录微软账号设置用户 Cookie 进行体验\n\n⭐ 遇到一切问题，先点左下角  试试，不行使用刷新大法（Shift + F5 或 Ctrl + Shift + R 或 右上角设置中的一键重置），最终大招就 清理浏览器缓存 及 Cookie ，比如（24 小时限制、未登录提示等等）\n\n- [go-proxy-bing](#go-proxy-bing)\n  - [网页展示](#网页展示)\n  - [侧边栏](#侧边栏)\n  - [演示站点](#演示站点)\n  - [设置用户](#设置用户)\n  - [环境变量](#环境变量)\n  - [部署](#部署)\n    - [Docker](#Docker)\n    - [Release](#Release)\n    - [Railway](#Railway)\n    - [Vercel](#Vercel)\n    - [Render](#Render)\n  - [部署聊天服务器](#部署聊天服务器)\n  - [TODO](#TODO)\n\n## 网页展示\n\n- 电脑端未登录状态\n\n\n\n- 电脑端登录\n\n\n\n\n\n\n- 电脑端画图\n\n> ⭐ 需登录，并选择 更有创造力 对话模式\n\n\n\n- 手机端未登录状态\n\n\n\n## 侧边栏\n\n- 在 Edge 浏览器可把聊天和撰写分别添加侧边栏\n\n\n\n\n\n\n\n## 演示站点\n\n### 甲骨文小鸡仔，轻虐\n\n- https://bing.vcanbb.top\n\n### Railway 搭建\n\n- https://bing-railway.vcanbb.top\n\n- https://go-proxy-bingai-production.up.railway.app\n\n### Vercel 搭建\n\n- https://bing-vercel.vcanbb.top\n\n- https://go-proxy-bingai-adams549659584.vercel.app\n\n### Render 搭建\n\n- https://bing-render.vcanbb.top\n\n- https://go-proxy-bingai.onrender.com\n\n## 设置用户\n\n- 访问 https://www.bing.com/ 或 https://cn.bing.com/ ，登录\n\n- F12 或 Ctrl + Shift + I 打开控制台\n\n- 拿到 Cookie 中 _U 的值 后，在网站设置 => 设置用户 中填入即可。\n\n\n\n## 环境变量\n\n```bash\n# 运行端口 默认 8080 可选\nPORT=8080\n# Socks 环境变量 示例 可选\nGo_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070\n# Socks 账号、密码 可选\nGo_Proxy_BingAI_SOCKS_USER=xxx\nGo_Proxy_BingAI_SOCKS_PWD=xxx\n# 默认用户 Cookie 设置，可选，不推荐使用，固定前缀 Go_Proxy_BingAI_USER_TOKEN 可设置多个，未登录用户将随机使用，多人共用将很快触发图形验证，并很快达到该账号的24小时限制\nGo_Proxy_BingAI_USER_TOKEN_1=xxx\nGo_Proxy_BingAI_USER_TOKEN_2=xxx\nGo_Proxy_BingAI_USER_TOKEN_3=xxx ...\n# 简单授权认证密码，可选\nGo_Proxy_BingAI_AUTH_KEY=xxx\n```\n\n## 部署\n\n> ⭐ 需 https 域名 (自行配置 nginx 等) (前后端都有限制 只有在HTTPS的情况下，浏览器 Accept-Encoding 才会包含 br , localhost 除外)\n\n> 支持 Linux (amd64 / arm64)、Windows (amd64 / arm64)\n\n> 国内机器部署可配置 socks 环境变量\n\n### Docker\n\n> 参考 [Dockerfile](./docker/Dockerfile) 、[docker-compose.yml](./docker/docker-compose.yml)\n\n- docker 示例\n\n```bash\n# 运行容器 监听8080 端口\ndocker run -d -p 8080:8080 --name go-proxy-bingai --restart=unless-stopped adams549659584/go-proxy-bingai\n\n# 配置 socks 环境变量\ndocker run -e Go_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070 -e Go_Proxy_BingAI_SOCKS_USER=xxx -e Go_Proxy_BingAI_SOCKS_PWD=xxx -d -p 8080:8080 --name go-proxy-bingai --restart=unless-stopped adams549659584/go-proxy-bingai\n```\n\n- docker compose 示例\n\n```yaml\nversion: '3'\n\nservices:\n  go-proxy-bingai:\n    # 镜像名称\n    image: adams549659584/go-proxy-bingai\n    # 容器名称\n    container_name: go-proxy-bingai  \n    # 自启动\n    restart: unless-stopped\n    ports:\n      - 8080:8080\n    # environment:\n    #   - Go_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070\n    #   - Go_Proxy_BingAI_SOCKS_USER=xxx\n    #   - Go_Proxy_BingAI_SOCKS_PWD=xxx\n    #   - Go_Proxy_BingAI_USER_TOKEN_1=xxx\n    #   - Go_Proxy_BingAI_USER_TOKEN_2=xxx    \n```\n\n### Release\n\n在 [GitHub Releases](https://github.com/adams549659584/go-proxy-bingai/releases) 下载适用于对应平台的压缩包，解压后可得到可执行文件 go-proxy-bingai，直接运行即可。\n\n### Railway\n\n> 主要配置 Dockerfile 路径 及 端口就可以\n\n```bash\nPORT=8080\nRAILWAY_DOCKERFILE_PATH=docker/Dockerfile\n```\n\n一键部署，点这里 => [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/uIckWS?referralCode=BBs747)\n\n\n\n自行使用 Railway 部署配置如下\n\n\n\n\n\n### Vercel\n\n> ⭐ Vercel 部署不支持 Websocket ，需选择 官方聊天服务器 或 Cloudflare\n\n一键部署，点这里 => [![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/adams549659584/go-proxy-bingai&project-name=go-proxy-bingai&repository-name=go-proxy-bingai-vercel)\n\n\n\n\n\n### Render\n\n一键部署，点这里 => [![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://render.com/deploy?repo=https://github.com/adams549659584/go-proxy-bingai)\n\n\n\n\n\n## 部署聊天服务器\n\n> 核心代码 [worker.js](./cloudflare/worker.js)\n\n> 具体部署 Cloudflare Workers 教程自行查询，大概如下\n\n- [注册 Cloudflare 账号](https://dash.cloudflare.com/sign-up)\n\n- 创建 Worker 服务，复制 [worker.js](./cloudflare/worker.js) 全部代码，粘贴至创建的服务中，保存并部署。\n\n- 触发器 中自定义访问域名。\n\n## TODO\n\n- [x] 撰写\n- [x] Vue3 重构\n- [x] 提示词\n- [x] 历史聊天\n- [x] 导出消息到本地（Markdown、图片、PDF）\n- [x] 简单访问权限控制",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bingai",
        "bing",
        "web",
        "bingai access",
        "bing ai",
        "microsoft bing"
      ],
      "category": "web-search"
    },
    "Klavis-AI--klavis": {
      "owner": "Klavis-AI",
      "name": "klavis",
      "url": "https://github.com/Klavis-AI/klavis",
      "imageUrl": "/freedevtools/mcp/pfp/Klavis-AI.webp",
      "description": "Generates visually appealing web reports based on simple search queries, integrating live web search results and storing reports in a database for easy access. Utilizes AI to synthesize information into interactive HTML formats.",
      "stars": 4534,
      "forks": 431,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T04:42:04Z",
      "readme_content": "<div align=\"center\">\n  <picture>\n    <img alt=\"klavis_ai\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" width=\"100\">\n  </picture>\n</div>\n\n<h1 align=\"center\">Klavis AI</h1>\n<p align=\"center\"><strong>📦 MCP integration layers that let AI agents use tools reliably at any scale</strong></p>\n\n<div align=\"center\">\n\n[![Documentation](https://img.shields.io/badge/Documentation-📖-green)](https://docs.klavis.ai)\n[![Website](https://img.shields.io/badge/Website-🌐-purple)](https://www.klavis.ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289DA?logo=discord&logoColor=white)](https://discord.gg/p7TuTEcssn)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n<a href=\"https://www.producthunt.com/products/strata-2?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-strata&#0045;2\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1016948&theme=light&period=daily&t=1758639605639\" alt=\"Strata - One&#0032;MCP&#0032;server&#0032;for&#0032;AI&#0032;agents&#0032;to&#0032;handle&#0032;thousands&#0032;of&#0032;tools | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n</div>\n\n## 🎯 Choose Your Solution\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\" width=\"50%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>📦 Strata</h2>\n            <p><strong>Unified MCP Router</strong></p>\n            <p>One MCP server for AI agents to use tools reliably at any scale</p>\n          </div>\n          <div>\n            <a href=\"open-strata/README.md\">\n              <img alt=\"svg_xml_base64_PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8_CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8_Cjwvc3ZnPg\" src=\"https://img.shields.io/badge/Explore-Strata-blue?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"50%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>🛠️ MCP Integrations</h2>\n            <p><strong>50+ Production MCP Servers</strong></p>\n            <p>Self-hosted or managed MCP servers with enterprise OAuth support for all major services</p>\n          </div>\n          <div>\n            <a href=\"mcp_servers/README.md\">\n              <img alt=\"svg_xml_base64_PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4\" src=\"https://img.shields.io/badge/Explore-MCP%20Servers-purple?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4=\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n    </tr>\n  </table>\n</div>\n\n## Strata\n\nStrata is one MCP server that guides your AI agents use tools reliably progressively at any scale.\n\n### Why Strata?\n\n🎯 **Scalable Tool Integration** → Beyond 40-50 tool limits  \n🚀 **Progressive Discovery** → Guides agents from intent to action, step-by-step.\n\n[📖 **Learn More** →](https://docs.klavis.ai/documentation/concepts/strata)\n\n## MCP Integrations\n\n**50+ production MCP servers. OAuth included. Deploy anywhere.**\n\nConnect your AI to GitHub, Gmail, Slack, Salesforce, and more - all with enterprise OAuth and Docker support.\n\n🔐 **Real OAuth** → Not just API keys  \n🐳 **Docker ready** → One-line deploy  \n\n[🌐 **Browse All Servers** →](https://docs.klavis.ai/documentation/mcp-server/overview)\n\n## 🚀 Quick Start\n\n### Option 1: Open Source\n\nSelf-host everything on your own infrastructure:\n\n```bash\n# Run any MCP Integration\ndocker pull ghcr.io/klavis-ai/github-mcp-server:latest\ndocker run -p 5000:5000 ghcr.io/klavis-ai/github-mcp-server:latest\n\n# Install Open Source Strata locally\npipx install strata-mcp\nstrata add --type stdio playwright npx @playwright/mcp@latest\n```\n\n### Option 2: Use Hosted Service by WebUI\n\nGet instant access without any setup:\n\n1. **Sign Up**: [Create account →](https://www.klavis.ai/auth/sign-up)\n2. **Get Started**: [Follow quickstart guide →](https://docs.klavis.ai/documentation/quickstart)\n3. **Use Strata or individual MCP servers** in Claude Code, Cursor, VSCode, etc.\n\nReady in under 2 minutes! 🚀\n\n### Option 3: SDK\n\nBuild custom applications with our SDKs:\n\n```python\n# Python SDK\nfrom klavis import Klavis\nfrom klavis.types import McpServerName\n\nklavis = Klavis(api_key=\"your-key\")\n\n# Create Strata instance\nstrata = klavis.mcp_server.create_strata_server(\n    user_id=\"user123\",\n    servers=[McpServerName.GMAIL, McpServerName.YOUTUBE],\n)\n\n# Or use individual MCP servers\ngmail = klavis.mcp_server.create_server_instance(\n    server_name=McpServerName.GMAIL,\n    user_id=\"user123\",\n)\n```\n\n```typescript\n// TypeScript SDK\nimport { KlavisClient, McpServerName } from 'klavis';\n\nconst klavis = new KlavisClient({ apiKey: 'your-api-key' });\n\n// Create Strata instance\nconst strata = await klavis.mcpServer.createStrataServer({\n    userId: \"user123\",\n    servers: [McpServerName.GMAIL, McpServerName.YOUTUBE]\n});\n\n// Or use individual MCP servers\nconst gmail = await klavis.mcpServer.createServerInstance({\n    serverName: McpServerName.GMAIL,\n    userId: \"user123\"\n});\n```\n\n### Option 4: Direct API\n\nUse REST API for any programming language:\n\n```bash\n# Create Strata server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/strata\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user123\",\n    \"servers\": [\"GMAIL\", \"YOUTUBE\"]\n  }'\n\n# Create individual MCP server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/instance\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"server_name\": \"GMAIL\",\n    \"user_id\": \"user123\"\n  }'\n```\n\n[📖 **Complete Documentation** →](https://docs.klavis.ai/documentation/quickstart)\n\n\n## 📚 Resources\n\n- 📖 [Documentation](https://docs.klavis.ai)\n- 💬 [Discord Community](https://discord.gg/p7TuTEcssn)\n- 🐛 [Report Issues](https://github.com/klavis-ai/klavis/issues)\n- 🌐 [Klavis AI Website](https://www.klavis.ai)\n\n## 📜 License\n\n- **Root Repository**: Apache 2.0 license - see [LICENSE](LICENSE)\n\n---\n\n<div align=\"center\">\n  <p><strong>Klavis AI (YC X25) 🚀 Empowering AI with Seamless Integration</strong></p>\n</div>",
      "npm_url": "https://www.npmjs.com/package/klavis",
      "npm_downloads": 16908,
      "keywords": [
        "klavis",
        "html",
        "search",
        "search klavis",
        "web search",
        "web reports"
      ],
      "category": "web-search"
    },
    "Kotelberg--playwright-mcp-server": {
      "owner": "Kotelberg",
      "name": "playwright-mcp-server",
      "url": "https://github.com/Kotelberg/playwright-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Kotelberg.webp",
      "description": "Retrieve web page content and interact with it by simulating user actions, navigating pages, and extracting data using a standardized protocol.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-15T10:13:00Z",
      "readme_content": "# Playwright MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@showfive/playwright-mcp-server)](https://smithery.ai/server/@showfive/playwright-mcp-server)\n\nEnglish | [日本語](README.ja.md)\n\nThis project is a server that provides Playwright web page content retrieval functionality using the Model Context Protocol (MCP).\n\n## Features\n\n- Page navigation\n- Full page content retrieval\n- Visible content retrieval\n- Interactive elements detection\n- Mouse operation simulation\n- Echo functionality for testing\n\n## Installation\n\n### Installing via Smithery\n\nTo install Playwright MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@showfive/playwright-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @showfive/playwright-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### MCP Tools\n\nThe following tools are available:\n\n1. `navigate`\n   - Navigate to a specified URL\n   - Arguments: `{ url: string }`\n   - Returns: Navigation result\n\n2. `get_all_content`\n   - Retrieve content from the entire page\n   - Arguments: None\n   - Returns: All text content from the page\n\n3. `get_visible_content`\n   - Retrieve currently visible content\n   - Arguments: `{ minVisiblePercentage?: number }`\n   - Returns: Visible text content\n\n4. `get_interactive_elements`\n   - Get position information of interactive elements (buttons, links, etc.) on the page\n   - Arguments: None\n   - Returns: Coordinates and boundary information of interactive elements\n\n5. `move_mouse`\n   - Move mouse cursor to specified coordinates\n   - Arguments: `{ x: number, y: number }`\n   - Returns: Operation result\n\n6. `mouse_click`\n   - Execute mouse click at specified coordinates\n   - Arguments: `{ x: number, y: number, button?: \"left\" | \"right\" | \"middle\", clickCount?: number }`\n   - Returns: Click operation result\n\n7. `mouse_wheel`\n   - Execute mouse wheel scrolling\n   - Arguments: `{ deltaY: number, deltaX?: number }`\n   - Returns: Scroll operation result\n\n8. `drag_and_drop`\n   - Execute drag and drop operation\n   - Arguments: `{ sourceX: number, sourceY: number, targetX: number, targetY: number }`\n   - Returns: Drag and drop operation result\n\n9. `echo`\n   - Echo tool for testing\n   - Arguments: `{ message: string }`\n   - Returns: Sent message\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Generate coverage report\nnpm run test:coverage\n```\n\n### Test Structure\n\n- `tools/*.test.ts`: Function tests for each tool\n- `mcp-server.test.ts`: MCP server function tests\n\n## Implementation Features\n\n1. Content Retrieval\n   - Full page content retrieval\n   - Visible content only retrieval\n   - Proper HTML parsing\n\n2. Interaction\n   - Detection and position information retrieval of interactive elements\n   - Mouse operation simulation (movement, clicks, scrolling)\n   - Drag and drop support\n\n3. Error Handling\n   - Proper navigation error handling\n   - Timeout processing\n   - Invalid URL detection\n\n4. Configuration Flexibility\n   - Headless/head mode selection\n   - Custom user agent\n   - Viewport size settings\n\n## Important Notes\n\n- Ensure necessary environment variables are set before using the MCP server\n- Follow the terms of service of target websites when retrieving web page content\n- Maintain appropriate intervals when sending multiple requests\n- When performing mouse operations, maintain appropriate intervals as they simulate actual user interactions\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/playwright-mcp-server",
      "npm_downloads": 5469,
      "keywords": [
        "web",
        "pages",
        "mcp",
        "mcp server",
        "web search",
        "retrieve web"
      ],
      "category": "web-search"
    },
    "Krieg2065--firecrawl-mcp-server": {
      "owner": "Krieg2065",
      "name": "firecrawl-mcp-server",
      "url": "https://github.com/Krieg2065/firecrawl-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Krieg2065.webp",
      "description": "Enables advanced web scraping and content extraction, supporting JavaScript rendering and batch processing for deep research. It includes features for automatic retries and efficient data integration while monitoring credit usage and handling rate limits.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-15T01:07:39Z",
      "readme_content": "# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/mendableai/firecrawl) for web scraping capabilities.\n\n> Big thanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n>\n> You can also play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server). Thanks to MCP.so for hosting and [@gstarwd](https://github.com/gstarwd) for integrating our server.\n\n## Features\n\n- Scrape, crawl, search, extract, deep research and batch scrape support\n- Web scraping with JS rendering\n- URL discovery and crawling\n- Web search with content extraction\n- Automatic retries with exponential backoff\n  - Efficient batch processing with built-in rate limiting\n- Credit usage monitoring for cloud API\n- Comprehensive logging system\n- Support for cloud and self-hosted Firecrawl instances\n- Mobile/Desktop viewport support\n- Smart content filtering with tag inclusion/exclusion\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key && npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\nResponse includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"your search query\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n### 5. Crawl Tool (`firecrawl_crawl`)\n\nStart an asynchronous crawl with advanced options.\n\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n### 6. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n#### Extract Tool Options:\n\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n\n### 7. Deep Research Tool (firecrawl_deep_research)\n\nConduct deep web research on a query using intelligent crawling, search, and LLM analysis.\n\n```json\n{\n  \"name\": \"firecrawl_deep_research\",\n  \"arguments\": {\n    \"query\": \"how does carbon capture technology work?\",\n    \"maxDepth\": 3,\n    \"timeLimit\": 120,\n    \"maxUrls\": 50\n  }\n}\n```\n\nArguments:\n\n- query (string, required): The research question or topic to explore.\n- maxDepth (number, optional): Maximum recursive depth for crawling/search (default: 3).\n- timeLimit (number, optional): Time limit in seconds for the research session (default: 120).\n- maxUrls (number, optional): Maximum number of URLs to analyze (default: 50).\n\nReturns:\n\n- Final analysis generated by an LLM based on research. (data.finalAnalysis)\n- May also include structured activities and sources used in the research process.\n\n### 8. Generate LLMs.txt Tool (firecrawl_generate_llmstxt)\n\nGenerate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact with the site.\n\n```json\n{\n  \"name\": \"firecrawl_generate_llmstxt\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"maxUrls\": 20,\n    \"showFullText\": true\n  }\n}\n```\n\nArguments:\n\n- url (string, required): The base URL of the website to analyze.\n- maxUrls (number, optional): Max number of URLs to include (default: 10).\n- showFullText (boolean, optional): Whether to include llms-full.txt contents in the response.\n\nReturns:\n\n- Generated llms.txt file contents and optionally the llms-full.txt (data.llmstxt and/or data.llmsfulltxt)\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "firecrawl",
        "search",
        "web scraping",
        "scraping content",
        "firecrawl mcp"
      ],
      "category": "web-search"
    },
    "Kryzo--mcp-bibliotheque_nationale_de_France": {
      "owner": "Kryzo",
      "name": "mcp-bibliotheque_nationale_de_France",
      "url": "https://github.com/Kryzo/mcp-bibliotheque_nationale_de_France",
      "imageUrl": "/freedevtools/mcp/pfp/Kryzo.webp",
      "description": "Access the Gallica digital library to search for documents, images, maps, and other resources, and generate structured research reports that include organized bibliographies and relevant visual content.",
      "stars": 5,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-01T16:19:01Z",
      "readme_content": "# BnF API Server\n\nUn serveur MCP (Model-Client-Protocol) pour accéder à l'API Gallica de la Bibliothèque nationale de France (BnF) et générer des rapports de recherche séquentiels.\n\n## Fonctionnalités\n\n- **Recherche dans Gallica** : Recherche de documents, images, cartes et autres ressources dans la bibliothèque numérique Gallica\n- **Génération de rapports séquentiels** : Création automatique de rapports de recherche structurés sur n'importe quel sujet\n- **Intégration de graphiques** : Inclusion d'images et de cartes pertinentes dans les rapports générés\n- **Citations formatées** : Génération automatique de bibliographies avec citations correctement formatées\n\n## Installation\n\n### Prérequis\n\n- Python 3.8 ou supérieur\n- Pip (gestionnaire de paquets Python)\n\n### Étapes d'installation\n\n1. **Cloner le dépôt**:\n   ```bash\n   git clone https://github.com/votre-nom/mcp-bnf.git\n   cd mcp-bnf\n   ```\n\n2. **Installer les dépendances**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Configuration avec Claude Desktop\n\n1. **Installer Claude Desktop** si ce n'est pas déjà fait.\n\n2. **Ouvrir la configuration de Claude Desktop**:\n   - Accéder aux paramètres de Claude Desktop\n   - Ouvrir le fichier de configuration (généralement situé à `%APPDATA%\\Claude\\claude_desktop_config.json`)\n\n```json\n{\n \"bnf\": {\n  \"command\": \"py\",\n  \"args\": [\n    \"c:\\\\chemin\\\\vers\\\\mcp-bnf\\\\bnf_server.py\"\n  ],\n  \"cwd\": \"c:\\\\chemin\\\\vers\\\\mcp-bnf\"\n},\n```\n\nRemplacez `chemin\\\\vers\\\\mcp-bnf` par le chemin réel vers votre répertoire d'installation.\n\n3. **Enregistrer le fichier de configuration** et redémarrer Claude Desktop\n\n## Outils MCP disponibles\n\nUne fois configuré, les outils suivants seront disponibles dans Claude Desktop:\n\n### Recherche dans Gallica\n\nPermet de rechercher des documents dans la bibliothèque numérique Gallica de la BnF en utilisant différents critères (titre, auteur, sujet, date, type de document).\n\n### Génération de rapports séquentiels\n\nCrée des rapports de recherche complets sur n'importe quel sujet en utilisant les sources de Gallica. Les rapports incluent:\n- Une bibliographie formatée\n- Une introduction\n- Un contexte historique\n- Une analyse\n- Une conclusion\n- Des images et cartes pertinentes (optionnel)\n\n## Structure du projet\n\n```\nmcp-bnf/\n│\n├── bnf_server.py              # Serveur MCP principal\n├── requirements.txt           # Dépendances du projet\n│\n└── bnf_api/                   # Package API BnF\n    ├── __init__.py            # Exports du package\n    ├── api.py                 # Client API Gallica BnF\n    ├── search.py              # Fonctions de recherche\n    ├── config.py              # Constantes et configuration\n    └── sequential_reporting.py # Outil de génération de rapports séquentiels\n```\n\n## Utilisation\n\nUne fois configuré avec Claude Desktop, vous pouvez demander à Claude d'utiliser les outils BnF pour:\n\n1. **Rechercher des documents**:\n   - \"Recherche des livres sur Victor Hugo dans Gallica\"\n   - \"Trouve des cartes de Paris du 19ème siècle\"\n\n2. **Générer des rapports**:\n   - \"Crée un rapport sur l'impressionnisme en France\"\n   - \"Génère un rapport sur l'histoire du Liban sous mandat français avec des images\"\n\n## Développement\n\nPour contribuer au projet:\n\n1. Forker le dépôt\n2. Créer une branche pour votre fonctionnalité (`git checkout -b feature/nouvelle-fonctionnalite`)\n3. Committer vos changements (`git commit -am 'Ajouter une nouvelle fonctionnalité'`)\n4. Pousser vers la branche (`git push origin feature/nouvelle-fonctionnalite`)\n5. Créer une Pull Request\n\n## Licence\n\nCe projet est open source.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bibliotheque_nationale_de_france",
        "bibliographies",
        "search",
        "mcp bibliotheque_nationale_de_france",
        "bibliotheque_nationale_de_france access",
        "search kryzo"
      ],
      "category": "web-search"
    },
    "KunihiroS--google-patents-mcp": {
      "owner": "KunihiroS",
      "name": "google-patents-mcp",
      "url": "https://github.com/KunihiroS/google-patents-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/KunihiroS.webp",
      "description": "Search Google Patents information using the SerpApi, providing access to comprehensive patent data for streamlined research. Facilitates quick querying and retrieval of patent-related information.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T02:34:00Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kunihiros-google-patents-mcp-badge.png)](https://mseep.ai/app/kunihiros-google-patents-mcp)\n\n# Google Patents MCP Server (`google-patents-mcp`)\n\n[![smithery badge](https://smithery.ai/badge/@KunihiroS/google-patents-mcp)](https://smithery.ai/server/@KunihiroS/google-patents-mcp)\n[![npm version](https://badge.fury.io/js/%40kunihiros%2Fgoogle-patents-mcp.svg)](https://badge.fury.io/js/%40kunihiros%2Fgoogle-patents-mcp)\n\nThis project provides a Model Context Protocol (MCP) server that allows searching Google Patents information via the [SerpApi Google Patents API](https://serpapi.com/google-patents-api).\n\n### Installing via Smithery\n\nTo install Google Patents MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KunihiroS/google-patents-mcp):\n\n```bash\nnpx -y @smithery/cli install @KunihiroS/google-patents-mcp --client claude\n```\n\n## Changelog\n\n### v0.2.0 (2025-04-17)\n\n*   **Fix:** Implemented empty handlers for `resources/list` and `prompts/list` MCP methods.\n*   **Fix:** Declared `prompts` capability in server initialization.\n*   **Chore:** Updated dependencies.\n\nThese changes aim to improve compatibility with MCP clients like Claude Desktop which may require these standard endpoints, though direct testing with Claude Desktop has not yet been performed.\n\n## Features\n\n*   Provides an MCP tool `search_patents` to search Google Patents.\n*   Uses SerpApi as the backend.\n*   Can be run directly using `npx` without local installation.\n\n## Prerequisites\n\n*   **Node.js:** Version 18 or higher is recommended.\n*   **npm:** Required to run the `npx` command.\n*   **SerpApi API Key:** You need a valid API key from [SerpApi](https://serpapi.com/) to use the Google Patents API.\n\n## Quick Start (Using npx)\n\nThe easiest way to run this server is using `npx`. This command downloads (if necessary) and runs the server directly.\n\n```bash\nnpx @kunihiros/google-patents-mcp\n```\n\n**Note:** Replace `@kunihiros/google-patents-mcp` with the actual published package name if it differs.\n\nThe server will start and listen for MCP requests on standard input/output.\n\n## Configuration\n\nThe server requires your SerpApi API key. You can provide it in one of the following ways:\n\n1.  **Environment Variable (Recommended for MCP Hosts):**\n    Set the `SERPAPI_API_KEY` environment variable when running the server. MCP Host configurations often allow setting environment variables for servers.\n\n    Example MCP Host configuration snippet (`config.json` or similar):\n    ```json\n    {\n      \"mcpServers\": {\n        \"google-patents-mcp\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\", // Skips confirmation if the package isn't installed locally\n            \"@kunihiros/google-patents-mcp\" // Use the correct package name\n          ],\n          \"env\": {\n            \"SERPAPI_API_KEY\": \"YOUR_ACTUAL_SERPAPI_KEY\"\n            // Optional: Set log level\n            // \"LOG_LEVEL\": \"debug\"\n          }\n        }\n      }\n    }\n    ```\n\n2.  **.env File:**\n    Create a `.env` file in the directory where you run the `npx` command (for local testing or if not using an MCP Host), or in your home directory (`~/.google-patents-mcp.env`), with the following content:\n\n    ```dotenv\n    SERPAPI_API_KEY=YOUR_ACTUAL_SERPAPI_KEY\n    # Optional: Set log level (e.g., debug, info, warn, error)\n    # LOG_LEVEL=debug\n    ```\n    **Note:** While using a `.env` file is convenient for local testing, for production or integration with MCP Hosts, setting the environment variable directly via the host configuration is the recommended and more secure approach. The primary intended use case is execution via `npx`, where environment variables are typically managed by the calling process or MCP Host.\n\nThe server searches for `.env` files in the following order:\n    *   `./.env` (relative to where `npx` is run)\n    *   `~/.google-patents-mcp.env` (in your home directory)\n\n## Provided MCP Tool\n\n### `search_patents`\n\nSearches Google Patents via SerpApi.\n\n**Input Schema:**\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"q\": {\n      \"type\": \"string\",\n      \"description\": \"Search query (required). Although optional in SerpApi docs, a non-empty query is practically needed. Use semicolon (;) to separate multiple terms. Advanced syntax like '(Coffee) OR (Tea);(A47J)' is supported. See 'About Google Patents' for details.\"\n    },\n    \"page\": {\n      \"type\": \"integer\",\n      \"description\": \"Page number for pagination (default: 1).\",\n      \"default\": 1\n    },\n    \"num\": {\n      \"type\": \"integer\",\n      \"description\": \"Number of results per page (default: 10). **IMPORTANT: Must be 10 or greater (up to 100).**\",\n      \"default\": 10,\n      \"minimum\": 10,\n      \"maximum\": 100\n    },\n    \"sort\": {\n      \"type\": \"string\",\n      \"enum\": [\"relevance\", \"new\", \"old\"],\n      \"description\": \"Sorting method. 'relevance' (default), 'new' (newest by filing/publication date), 'old' (oldest by filing/publication date).\",\n      \"default\": \"relevance\"\n    },\n    \"before\": {\n      \"type\": \"string\",\n      \"description\": \"Maximum date filter (e.g., 'publication:20231231', 'filing:20220101'). Format: type:YYYYMMDD where type is 'priority', 'filing', or 'publication'.\"\n    },\n    \"after\": {\n      \"type\": \"string\",\n      \"description\": \"Minimum date filter (e.g., 'publication:20230101', 'filing:20220601'). Format: type:YYYYMMDD where type is 'priority', 'filing', or 'publication'.\"\n    },\n    \"inventor\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by inventor names. Separate multiple names with a comma (,).\"\n    },\n    \"assignee\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by assignee names. Separate multiple names with a comma (,).\"\n    },\n    \"country\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by country codes (e.g., 'US', 'WO,JP'). Separate multiple codes with a comma (,).\"\n    },\n    \"language\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by language (e.g., 'ENGLISH', 'JAPANESE,GERMAN'). Separate multiple languages with a comma (,). Supported: ENGLISH, GERMAN, CHINESE, FRENCH, SPANISH, ARABIC, JAPANESE, KOREAN, PORTUGUESE, RUSSIAN, ITALIAN, DUTCH, SWEDISH, FINNISH, NORWEGIAN, DANISH.\"\n    },\n    \"status\": {\n      \"type\": \"string\",\n      \"enum\": [\"GRANT\", \"APPLICATION\"],\n      \"description\": \"Filter by patent status: 'GRANT' or 'APPLICATION'.\"\n    },\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\"PATENT\", \"DESIGN\"],\n      \"description\": \"Filter by patent type: 'PATENT' or 'DESIGN'.\"\n    },\n    \"scholar\": {\n      \"type\": \"boolean\",\n      \"description\": \"Include Google Scholar results (default: false).\",\n      \"default\": false\n    }\n  },\n  \"required\": [\"q\"]\n}\n```\n\n**Output:**\n\nReturns a JSON object containing the search results from SerpApi. The structure follows the SerpApi response format.\n\n**Example Usage (MCP Request):**\n\n```json\n{\n  \"mcp_version\": \"1.0\",\n  \"type\": \"CallToolRequest\",\n  \"id\": \"req-123\",\n  \"server_name\": \"google-patents-mcp\",\n  \"params\": {\n    \"name\": \"search_patents\",\n    \"arguments\": {\n      \"q\": \"organic light emitting diode\",\n      \"num\": 10,\n      \"language\": \"ENGLISH\",\n      \"status\": \"GRANT\",\n      \"after\": \"publication:20230101\"\n    }\n  }\n}\n```\n\n## Development\n\n1.  **Clone the repository (if needed for development):**\n    ```bash\n    # git clone <repository-url>\n    # cd google-patents-mcp\n    ```\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n3.  **Create `.env` file:**\n    Copy `.env.example` to `.env` and add your `SERPAPI_API_KEY`.\n4.  **Build:**\n    ```bash\n    npm run build\n    ```\n5.  **Run locally:**\n    ```bash\n    npm start\n    ```\n    Or for development with auto-rebuild:\n    ```bash\n    npm run dev\n    ```\n\n## Logging\n\n*   Logs are output to standard error.\n*   Log level can be controlled via the `LOG_LEVEL` environment variable (`error`, `warn`, `info`, `http`, `verbose`, `debug`, `silly`). Defaults to `info`.\n*   A log file is attempted to be created in the project root (`google-patents-server.log`), user's home directory (`~/.google-patents-server.log`), or `/tmp/google-patents-server.log`.\n\n## License\n\nMIT License (See LICENSE file)\n",
      "npm_url": "https://www.npmjs.com/package/@kunihiros/google-patents-mcp",
      "npm_downloads": 0,
      "keywords": [
        "patents",
        "patent",
        "serpapi",
        "patents information",
        "google patents",
        "patent data"
      ],
      "category": "web-search"
    },
    "L3-N0X--Minecraft-Wiki-MCP": {
      "owner": "L3-N0X",
      "name": "Minecraft-Wiki-MCP",
      "url": "https://github.com/L3-N0X/Minecraft-Wiki-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/L3-N0X.webp",
      "description": "Browse and search the official Minecraft Wiki for detailed information on structures, entities, items, and blocks. Access multi-language support and navigate through wiki categories and specific sections for comprehensive content.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T04:16:05Z",
      "readme_content": "# Minecraft Wiki MCP\n[![smithery badge](https://smithery.ai/badge/@L3-N0X/Minecraft-Wiki-MCP)](https://smithery.ai/server/@L3-N0X/Minecraft-Wiki-MCP)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/f80cbb34-35d6-4652-a302-2413ffe60cb4)\n\nA MCP Server for browsing the official Minecraft Wiki!\n\n> [!WARNING]\n> This MCP is still in development and while working most of the time, there might still be smaller issues and bugs left!\n\n<a href=\"https://glama.ai/mcp/servers/@L3-N0X/Minecraft-Wiki-MCP\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@L3-N0X/Minecraft-Wiki-MCP/badge\" alt=\"Minecraft Wiki MCP server\" />\n</a>\n\n## Features\n\n- **Wiki Search**: Find information about Minecraft structures, entities, items, and blocks\n- **Page Navigation**: Get summaries and detailed content from wiki pages\n- **Section Access**: Target specific sections within wiki pages\n- **Category Browsing**: Explore wiki categories and their member pages\n- **Multi-Language Support**: Connect to different language versions of the Minecraft Wiki\n\n## Installation\n\nCurrently, only local installation is supported, other might follow!\n\n### Installing via Smithery\n\nTo install Minecraft Wiki Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@L3-N0X/Minecraft-Wiki-MCP):\n\n```bash\nnpx -y @smithery/cli install @L3-N0X/Minecraft-Wiki-MCP --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/L3-N0X/Minecraft-Wiki-MCP.git\ncd Minecraft-Wiki-MCP\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nThen, you can use the server with this configuration in your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"minecraft-wiki\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/dist/server.js\", \n        \"--api-url\",\n        \"https://minecraft.wiki/api.php\"\n      ]\n    }\n  }\n}\n```\n\n## Configuration\n\nMake sure to update the path to the server.js file!\nBy default, this server connects to <https://minecraft.wiki/api.php> (English version). You can use a different wiki API URL by using the `api-url` option to access different language versions:\n\n```json\n{\n  \"mcpServers\": {\n    \"minecraft-wiki\": {\n      \"command\": \"node\",\n       \"args\": [\n        \"/path/to/your/dist/server.js\", \n        \"--api-url\",\n        \"https://de.minecraft.wiki/api.php\" // German version\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nThis server provides the following tools for interacting with the Minecraft Wiki:\n\n### Search and Navigation\n\n- `MinecraftWiki_searchWiki`: Search for structures, entities, items, or blocks\n- `MinecraftWiki_getPageSummary`: Get page summary and list of available sections\n- `MinecraftWiki_resolveRedirect`: Resolve redirect pages to their targets\n\n### Page Content\n\n- `MinecraftWiki_getPageContent`: Get full page content\n- `MinecraftWiki_getPageSection`: Get specific section content\n- `MinecraftWiki_getSectionsInPage`: Get overview of all sections in a page\n\n### Categories\n\n- `MinecraftWiki_listAllCategories`: List all available categories\n- `MinecraftWiki_listCategoryMembers`: List pages within a category\n- `MinecraftWiki_getCategoriesForPage`: Get categories for a specific page\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minecraft",
        "l3",
        "wiki",
        "minecraft wiki",
        "n0x minecraft",
        "official minecraft"
      ],
      "category": "web-search"
    },
    "Laksh-star--mcp-server-tmdb": {
      "owner": "Laksh-star",
      "name": "mcp-server-tmdb",
      "url": "https://github.com/Laksh-star/mcp-server-tmdb",
      "imageUrl": "/freedevtools/mcp/pfp/Laksh-star.webp",
      "description": "Integrates with The Movie Database (TMDB) API to provide access to movie information, search capabilities, and personalized recommendations.",
      "stars": 51,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T05:19:05Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/laksh-star-mcp-server-tmdb-badge.png)](https://mseep.ai/app/laksh-star-mcp-server-tmdb)\n\n# TMDB MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Laksh-star/mcp-server-tmdb)](https://smithery.ai/server/@Laksh-star/mcp-server-tmdb)\nThis MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n\n<a href=\"https://glama.ai/mcp/servers/g3nl1a0n25\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/g3nl1a0n25/badge\" alt=\"mcp-server-tmdb MCP server\" /></a>\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Laksh-star/mcp-server-tmdb)\n## Prerequisites\n\nBefore installing and running the TMDB MCP server, ensure you have the following prerequisites installed and configured:\n\n### Required software\n\n- **Node.js**\n  - Version 18.0.0 or higher\n  - Download from [Node.js official website](https://nodejs.org/)\n  - Verify installation: `node --version`\n\n- **npm (Node Package Manager)**\n  - Version 8.0.0 or higher (comes with Node.js)\n  - Verify installation: `npm --version`\n\n- **TypeScript**\n  - Will be installed as a project dependency\n  - Can be installed globally: `npm install -g typescript`\n  - Verify installation: `tsc --version`\n\n### Required accounts & API keys\n\n- **TMDB account**\n  - Free account at [TMDB](https://www.themoviedb.org/)\n  - API key from TMDB dashboard\n  - API access must be approved by TMDB\n\n- **Claude desktop application**\n  - Latest version installed\n  - Access to modify configuration files\n\n### System requirements\n\n- **Operating systems**\n  - macOS (10.15 or later)\n  - Linux (modern distributions)\n\n- **Hardware requirements**\n- Minimum 4GB RAM\n  - 1GB free disk space\n  - Stable internet connection\n\n### Development environment\n\nFor the best development experience, we recommend:\n- A code editor with TypeScript support (e.g., VS Code)\n- Terminal access\n- Git (for version control)\n\n## Features\n\n### Tools\n\n- **search_movies**\n  - Search for movies by title or keywords\n  - Input: `query` (string): Search query\n  - Returns: List of movies with titles, release years, IDs, ratings, and overviews\n  - Example: Search for movies about space exploration\n\n- **get_recommendations**\n  - Get movie recommendations based on a movie ID\n  - Input: `movieId` (string): TMDB movie ID\n  - Returns: Top 5 recommended movies with details\n  - Example: Get recommendations based on movie ID 550 (Fight Club)\n\n- **get_trending**\n  - Get trending movies for a specified time window\n  - Input: `timeWindow` (string): Either \"day\" or \"week\"\n  - Returns: Top 10 trending movies with details\n  - Example: Get today's trending movies\n\n### Resources\n\nThe server provides access to TMDB movie information:\n\n- **Movies** (`tmdb:///movie/<movie_id>`)\n  - Comprehensive movie details including:\n    - Title and release date\n    - Rating and overview\n    - Genres\n    - Poster URL\n    - Cast information (top 5 actors)\n    - Director\n    - Selected reviews\n  - All data is returned in JSON format\n\n## Getting started\n\n1. Get a TMDB API key:\n   - Sign up at [TMDB](https://www.themoviedb.org/)\n   - Go to your account settings\n   - Navigate to the API section\n   - Request an API key for developer use\n\n2. Clone and set up the project:\n   ```bash\n   git clone [repository-url]\n   cd mcp-server-tmdb\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n4. Set up your environment variable:\n   ```bash\n   export TMDB_API_KEY=your_api_key_here\n   ```\n\n### Usage with Claude Desktop\n\nTo integrate this server with Claude Desktop, add the following to your app's server configuration file (located at `~/Library/Application Support/Claude/config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"tmdb\": {\n      \"command\": \"/full/path/to/dist/index.js\",\n      \"env\": {\n        \"TMDB_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nReplace `/full/path/to` with the actual path to your project directory.\n\n### Installing via Smithery\n\nTo install TMDB Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Laksh-star/mcp-server-tmdb):\n\n```bash\nnpx -y @smithery/cli install @Laksh-star/mcp-server-tmdb --client claude\n```\n\n## Example usage\n\nOnce the server is running with Claude Desktop, you can use commands like:\n\n1. Search for movies:\n   ```\n   \"Search for movies about artificial intelligence\"\n   ```\n\n2. Get trending movies:\n   ```\n   \"What are the trending movies today?\"\n   \"Show me this week's trending movies\"\n   ```\n\n3. Get movie recommendations:\n   ```\n   \"Get movie recommendations based on movie ID 550\"\n   ```\n\n4. Get movie details:\n   ```\n   \"Tell me about the movie with ID 550\"\n   ```\n\n## Error handling\n\nThe server includes comprehensive error handling for:\n- Invalid API keys\n- Network errors\n- Invalid movie IDs\n- Malformed requests\n\nError messages will be returned in a user-friendly format through Claude Desktop.\n\n## Development\n\nTo watch for changes during development:\n```bash\nnpm run watch\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-tmdb",
      "npm_downloads": 78,
      "keywords": [
        "tmdb",
        "search",
        "database",
        "tmdb api",
        "server tmdb",
        "database tmdb"
      ],
      "category": "web-search"
    },
    "LinkupPlatform--python-mcp-server": {
      "owner": "LinkupPlatform",
      "name": "python-mcp-server",
      "url": "https://github.com/LinkupPlatform/python-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/LinkupPlatform.webp",
      "description": "Fetch real-time web search results using the LinkupClient to provide updated information for AI models, enhancing their responses with current events and trusted sources.",
      "stars": 44,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:02:32Z",
      "readme_content": "# 🌟 Linkup Python MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n\n<a href=\"https://glama.ai/mcp/servers/69qbbv8hl9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/69qbbv8hl9/badge\" alt=\"mcp-search-linkup MCP server\" /></a>\n\n## ✨ Why Linkup?\n\n- 🔍 **Advanced Web Search**: Leverage Linkup's AI-powered search engine for high-quality, relevant results\n- 💬 **Natural Language Queries**: Ask questions in plain English or your preferred language - no need for keyword optimization\n- 🚀 **Real-time Information**: Access up-to-date web content and current information\n- 📚 **Comprehensive Results**: Get detailed search results with source citations\n- 🔧 **Easy Integration**: Works with any MCP-compatible client\n\n## 🚀 Installation\n\nThe Linkup MCP server can be used with any MCP-compatible client. \n\nFor an integration with Claude Desktop or with Cursor, please follow instruction [here](https://docs.linkup.so/pages/integrations/mcp/mcp).\n\nFor other MCP-compatible clients, use these connection details:\n\n- **Command**: `python -m linkup_mcp_server`\n- **Required Environment Variables**: `LINKUP_API_KEY`\n\nConsult your MCP client's documentation for specific configuration instructions.\n\n## 💬 Example Queries\n\nThe Linkup MCP server excels at answering complex questions and finding specific information:\n\n- \"What are the latest developments in quantum computing?\"\n- \"How does the EU AI Act affect startups?\"\n- \"Find recent research on sustainable aviation fuel\"\n- \"What are the current best practices for MCP server development?\"\n\n## 🤝 Contributing\n\nPull requests are welcome! Feel free to open an issue first to discuss what you’d like to see improved.\n\n## 📚 Resources\n\n- [Linkup Documentation](https://docs.linkup.so)\n- [MCP Protocol Specification](https://modelcontextprotocol.io)\n- [Linkup API Reference](https://docs.linkup.so/api-reference)\n\n## 📣 Community & Support\n\n* Email: [support@linkup.so](mailto:support@linkup.so)\n* Discord: [Join our community](https://discord.com/invite/9q9mCYJa86)\n* X / Twitter: [@Linkup_platform](https://x.com/Linkup_platform)\n\n## 📄 License\n\nThis project is licensed under the MIT License - Innovate freely! 🚀\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkupclient",
        "search",
        "web",
        "web search",
        "search linkupplatform",
        "search results"
      ],
      "category": "web-search"
    },
    "Lorhlona--geminiserchMCP": {
      "owner": "Lorhlona",
      "name": "geminiserchMCP",
      "url": "https://github.com/Lorhlona/geminiserchMCP",
      "imageUrl": "/freedevtools/mcp/pfp/Lorhlona.webp",
      "description": "Generates responses based on the latest information using the Gemini API and Google Search, requiring integration with AI assistants like Cline for functionality.",
      "stars": 23,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-03T22:52:12Z",
      "readme_content": "# Gemini Search MCP Server\n\n[![MCP Server](https://glama.io/mcp-servers/badge/Lorhlona/geminiserchMCP)](https://glama.io/mcp-servers/Lorhlona/geminiserchMCP)\n\nAn MCP server that generates responses based on the latest information using the Gemini API and Google Search.\n\n> **Note**: This MCP server does not work standalone. It needs to be used in combination with AI assistants like [Cline](https://github.com/ClineLabs/cline). The Gemini search functionality becomes available when you load this project into an AI assistant.\n\n---\n\nGemini APIとGoogle検索を使用して、最新の情報に基づいた回答を生成するMCPサーバーです。\n\n> **注意**: このMCPサーバーは単体では動作しません。[Cline](https://github.com/ClineLabs/cline)などのAIアシスタントと組み合わせて使用する必要があります。AIアシスタントにこのプロジェクトを読み込ませることで、Gemini検索機能が利用可能になります。\n\n## Features\n\n### Tools\n- `search` - Generate answers using Gemini 2.0 and Google Search\n  - Takes a query as input and returns Gemini's response along with relevant search results\n\n## 機能\n\n### Tools\n- `search` - Gemini 2.0とGoogle検索を使用して質問に回答\n  - クエリを入力として受け取り、Geminiの回答と関連する検索結果を返します\n\n## Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build:\n```bash\nnpm run build\n```\n\n3. Set environment variables:\nCreate a `.env` file in the project root with the following content:\n```\nGEMINI_API_KEY=your_api_key_here\n```\nNote: You can get your Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey).\n\n## Development\n\nFor automatic builds during development:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the following configuration:\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n```json\n{\n  \"mcpServers\": {\n    \"gemini\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/gemini-search-server/build/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate via stdio, we recommend using [MCP Inspector](https://github.com/modelcontextprotocol/inspector) for debugging:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector provides a URL to access debugging tools in your browser.\n\n## License\n\nThe code in this project is released under the [MIT License](LICENSE).\nHowever, please note that this project uses the Google Gemini API, which is subject to [Google's Terms of Service](https://ai.google.dev/terms). When using this MCP server, you must comply with both the MIT License for our code and Google's terms for the Gemini API.\n\n---\n\n## セットアップ\n\n1. 依存関係のインストール:\n```bash\nnpm install\n```\n\n2. ビルド:\n```bash\nnpm run build\n```\n\n3. 環境変数の設定:\n`.env`ファイルをプロジェクトのルートに作成し、以下の内容を設定してください：\n```\nGEMINI_API_KEY=your_api_key_here\n```\n※ Gemini APIキーは[Google AI Studio](https://makersuite.google.com/app/apikey)から取得できます。\n\n## 開発\n\n開発時の自動ビルド:\n```bash\nnpm run watch\n```\n\n## インストール\n\nClaude Desktopで使用するには、以下の設定を追加してください：\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n```json\n{\n  \"mcpServers\": {\n    \"gemini\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/gemini-search-server/build/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### デバッグ\n\nMCPサーバーはstdioを介して通信するため、デバッグには[MCP Inspector](https://github.com/modelcontextprotocol/inspector)の使用を推奨します：\n\n```bash\nnpm run inspector\n```\n\nInspectorはブラウザでデバッグツールにアクセスするためのURLを提供します。\n\n## ライセンス\n\nこのプロジェクトのコードは[MIT License](LICENSE)の下で公開されています。\nただし、このプロジェクトはGoogle Gemini APIを使用しているため、[Googleの利用規約](https://ai.google.dev/terms)も適用されます。このMCPサーバーを使用する際は、コードのMITライセンスとGemini APIの利用規約の両方に従う必要があります。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geminiserchmcp",
        "search",
        "api",
        "gemini api",
        "lorhlona geminiserchmcp",
        "web search"
      ],
      "category": "web-search"
    },
    "MaitreyaM--WEB-SCRAPING-MCP": {
      "owner": "MaitreyaM",
      "name": "WEB-SCRAPING-MCP",
      "url": "https://github.com/MaitreyaM/WEB-SCRAPING-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MaitreyaM.webp",
      "description": "Enables AI agents to scrape web content, search for specific text snippets, and perform structured information extraction from websites using natural language instructions. Utilizes LLMs for intelligent content retrieval and provides an API for interaction.",
      "stars": 21,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T19:15:34Z",
      "readme_content": "# Crawl4AI Web Scraper MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Optional: Add license -->\n\nThis project provides an MCP (Model Context Protocol) server that uses the **[crawl4ai](https://github.com/unclecode/crawl4ai)** library to perform web scraping and intelligent content extraction tasks. It allows AI agents (like Claude, or agents built with LangChain/LangGraph) to interact with web pages, retrieve content, search for specific text, and perform LLM-based extraction based on natural language instructions.\n\nThis server uses:\n\n*   **[FastMCP](https://github.com/model-context-protocol/mcp-py/blob/main/docs/fastmcp.md):** For creating the MCP server endpoint.\n*   **[crawl4ai](https://github.com/unclecode/crawl4ai):** For the core web crawling and extraction logic.\n*   **[dotenv](https://github.com/theskumar/python-dotenv):** For managing API keys via a `.env` file.\n*   **(Optional) Docker:** For containerized deployment, bundling Python and dependencies.\n\n## Features\n\n*   Exposes MCP tools for web interaction:\n    *   `scrape_url`: Get the full content of a webpage in Markdown format.\n    *   `extract_text_by_query`: Find specific text snippets on a page based on a query.\n    *   `smart_extract`: Use an LLM (currently Google Gemini) to extract structured information based on instructions.\n*   Configurable via environment variables (API keys).\n*   Includes Docker configuration (`Dockerfile`) for easy, self-contained deployment.\n*   Communicates over Server-Sent Events (SSE) on port 8002 by default.\n\n## Exposed MCP Tools\n\n### `scrape_url`\n\nScrape a webpage and return its content in Markdown format.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to scrape.\n\n**Returns:**\n\n*   (str): The webpage content in Markdown format, or an error message.\n\n### `extract_text_by_query`\n\nExtract relevant text snippets from a webpage that contain a specific search query. Returns up to the first 5 matches found.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to search within.\n*   `query` (str, **required**): The text query to search for (case-insensitive).\n*   `context_size` (int, *optional*): The number of characters to include before and after the matched query text in each snippet. Defaults to `300`.\n\n**Returns:**\n\n*   (str): A formatted string containing the found text snippets or a message indicating no matches were found, or an error message.\n\n### `smart_extract`\n\nIntelligently extract specific information from a webpage using the configured LLM (currently requires Google Gemini API key) based on a natural language instruction.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to analyze and extract from.\n*   `instruction` (str, **required**): Natural language instruction specifying what information to extract (e.g., \"List all the speakers mentioned on this page\", \"Extract the main contact email address\", \"Summarize the key findings\").\n\n**Returns:**\n\n*   (str): The extracted information (often formatted as JSON or structured text based on the instruction) or a message indicating no relevant information was found, or an error message (including if the required API key is missing).\n\n## Setup and Running\n\nYou can run this server either locally or using the provided Docker configuration.\n\n### Option 1: Running with Docker (Recommended for Deployment)\n\nThis method bundles Python and all necessary libraries. You only need Docker installed on the host machine.\n\n1.  **Install Docker:** Download and install [Docker Desktop](https://www.docker.com/products/docker-desktop/) for your OS. Start Docker Desktop.\n2.  **Clone Repository:**\n    ```bash\n    git clone https://github.com/your-username/your-repo-name.git # Replace with your repo URL\n    cd your-repo-name\n    ```\n3.  **Create `.env` File:** Create a file named `.env` in the project root directory and add your API keys:\n    ```.env\n    # Required for the smart_extract tool\n    GOOGLE_API_KEY=your_google_ai_api_key_here\n\n    # Optional, checked by server but not currently used by tools\n    # OPENAI_API_KEY=your_openai_key_here\n    # MISTRAL_API_KEY=your_mistral_key_here\n    ```\n4.  **Build the Docker Image:**\n    ```bash\n    docker build -t crawl4ai-mcp-server .\n    ```\n5.  **Run the Container:** This starts the server, making port 8002 available on your host machine. It uses `--env-file` to securely pass the API keys from your local `.env` file into the container's environment.\n    ```bash\n    docker run -it --rm -p 8002:8002 --env-file .env crawl4ai-mcp-server\n    ```\n    *   `-it`: Runs interactively.\n    *   `--rm`: Removes container on exit.\n    *   `-p 8002:8002`: Maps host port 8002 to container port 8002.\n    *   `--env-file .env`: Loads environment variables from your local `.env` file into the container. **Crucial for API keys.**\n    *   `crawl4ai-mcp-server`: The name of the image you built.\n6.  **Server is Running:** Logs will appear, indicating the server is listening on SSE (`http://0.0.0.0:8002`).\n7.  **Connecting Client:** Configure your MCP client (e.g., LangChain agent) to connect to `http://127.0.0.1:8002/sse` with `transport: \"sse\"`.\n\n### Option 2: Running Locally\n\nThis requires Python and manual installation of dependencies on your host machine.\n\n1.  **Install Python:** Ensure Python >= 3.9 (check `crawl4ai` requirements if needed, 3.10+ recommended).\n2.  **Clone Repository:**\n    ```bash\n    git clone https://github.com/your-username/your-repo-name.git # Replace with your repo URL\n    cd your-repo-name\n    ```\n3.  **Create Virtual Environment (Recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate # Linux/macOS\n    # venv\\Scripts\\activate # Windows\n    ```\n    *(Or use Conda: `conda create --name crawl4ai-env python=3.11 -y && conda activate crawl4ai-env`)*\n4.  **Install Dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n5.  **Create `.env` File:** Create a file named `.env` in the project root directory and add your API keys (same content as in Docker setup step 3).\n6.  **Run the Server:**\n    ```bash\n    python your_server_script_name.py # e.g., python webcrawl_mcp_server.py\n    ```\n7.  **Server is Running:** It will listen on `http://127.0.0.1:8002/sse`.\n8.  **Connecting Client:** Configure your MCP client to connect to `http://127.0.0.1:8002/sse`.\n\n## Environment Variables\n\nThe server uses the following environment variables, typically loaded from an `.env` file:\n\n*   `GOOGLE_API_KEY`: **Required** for the `smart_extract` tool to function (uses Google Gemini). Get one from [Google AI Studio](https://aistudio.google.com/app/apikey).\n*   `OPENAI_API_KEY`: Checked for existence but **not currently used** by any tool in this version.\n*   `MISTRAL_API_KEY`: Checked for existence but **not currently used** by any tool in this version.\n\n## Example Agent Interaction\n\n```\n# Example using the agent CLI from the previous setup\n\nYou: scrape_url https://example.com\nAgent: Thinking...\n[Agent calls scrape_url tool]\nAgent: [Markdown content of example.com]\n------------------------------\nYou: extract text from https://en.wikipedia.org/wiki/Web_scraping using the query \"ethical considerations\"\nAgent: Thinking...\n[Agent calls extract_text_by_query tool]\nAgent: Found X matches for 'ethical considerations' on the page. Here are the relevant sections:\nMatch 1:\n... text snippet ...\n---\nMatch 2:\n... text snippet ...\n------------------------------\nYou: Use smart_extract on https://blog.google/technology/ai/google-gemini-ai/ to get the main points about Gemini models\nAgent: Thinking...\n[Agent calls smart_extract tool with Google API Key]\nAgent: Successfully extracted information based on your instruction:\n{\n  \"main_points\": [\n    \"Gemini is Google's most capable AI model family (Ultra, Pro, Nano).\",\n    \"Designed to be multimodal, understanding text, code, audio, image, video.\",\n    \"Outperforms previous models on various benchmarks.\",\n    \"Being integrated into Google products like Bard and Pixel.\"\n  ]\n}\n\n```\n\n## Files\n\n*   `your_server_script_name.py`: The main Python script for the MCP server (e.g., `webcrawl_mcp_server.py`).\n*   `Dockerfile`: Instructions for building the Docker container image.\n*   `requirements.txt`: Python dependencies.\n*   `.env.example`: (Recommended) An example environment file showing needed keys. **Do not commit your actual `.env` file.**\n*   `.gitignore`: Specifies intentionally untracked files for Git (should include `.env`).\n*   `README.md`: This file.\n\n## Contributing\n\n(Add contribution guidelines if desired)\n\n## License\n\n(Specify your license, e.g., MIT License)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "retrieval",
        "web",
        "web scraping",
        "extraction websites",
        "scraping mcp"
      ],
      "category": "web-search"
    },
    "MrunmayS--exa-mcp-server": {
      "owner": "MrunmayS",
      "name": "exa-mcp-server",
      "url": "https://github.com/MrunmayS/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/MrunmayS.webp",
      "description": "Connects AI assistants to the Exa AI Search API for real-time web searches, facilitating access to updated web information in a structured manner.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-30T12:31:07Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Handles rate limiting and error cases gracefully\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n````\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1.  Clone the repository:\n    \n\n```\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2.  Install dependencies:\n    \n\n```\nnpm install --save axios dotenv\n```\n\n3.  Build the project:\n    \n\n```\nnpm run build\n```\n\n4.  Create a global link (this makes the server executable from anywhere):\n    \n\n```\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1.  Open your Claude Desktop configuration:\n    \n\n```\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1.  Open your Claude Desktop configuration:\n    \n\n```\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n\n### 2.  Add the Exa server configuration:\n    \n\n```\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1.  Completely quit Claude Desktop (not just close the window)\n    \n2.  Start Claude Desktop again\n    \n3.  Look for the 🔌 icon to verify the Exa server is connected\n    \n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\nThe server will:\n\n1.  Process the search request\n    \n2.  Query the Exa API\n    \n3.  Return formatted results to Claude\n    \n4.  Cache the search for future reference\n    \n\n## Features ✨\n\n*   **Web Search Tool**: Enables Claude to search the web using natural language queries\n    \n*   **Error Handling**: Gracefully handles API errors and rate limits\n    \n*   **Type Safety**: Full TypeScript implementation with proper type checking\n    \n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1.  **Server Not Found**\n    \n    *   Verify the npm link is correctly set up\n        \n    *   Check Claude Desktop configuration syntax\n        \n    *   Ensure Node.js is properly installed\n        \n2.  **API Key Issues**\n    \n    *   Confirm your Exa API key is valid\n        \n    *   Check the API key is correctly set in the Claude Desktop config\n        \n    *   Verify no spaces or quotes around the API key\n        \n3.  **Connection Issues**\n    \n    *   Restart Claude Desktop completely\n        \n    *   Check Claude Desktop logs:\n        \n        ```\n        # macOS\n        tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n        ```\n        \n\n### Getting Help\n\nIf you encounter issues review the [MCP Documentation](https://modelcontextprotocol.io)\n    \n    \n\n\n## Acknowledgments 🙏\n\n*   [Exa AI](https://exa.ai) for their powerful search API\n    \n*   [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n    \n*   [Anthropic](https://anthropic.com) for Claude Desktop\n    \n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "ai",
        "search",
        "ai search",
        "ai assistants",
        "exa ai"
      ],
      "category": "web-search"
    },
    "Msparihar--mcp-server-firecrawl": {
      "owner": "Msparihar",
      "name": "mcp-server-firecrawl",
      "url": "https://github.com/Msparihar/mcp-server-firecrawl",
      "imageUrl": "/freedevtools/mcp/pfp/Msparihar.webp",
      "description": "Provides capabilities for web scraping, intelligent content searching, and site crawling using the Firecrawl API, facilitating customizable data extraction and structured output.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-02-20T06:10:43Z",
      "readme_content": "# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server for web scraping, content searching, site crawling, and data extraction using the Firecrawl API.\n\n## Features\n\n- **Web Scraping**: Extract content from any webpage with customizable options\n  - Mobile device emulation\n  - Ad and popup blocking\n  - Content filtering\n  - Structured data extraction\n  - Multiple output formats\n\n- **Content Search**: Intelligent search capabilities\n  - Multi-language support\n  - Location-based results\n  - Customizable result limits\n  - Structured output formats\n\n- **Site Crawling**: Advanced web crawling functionality\n  - Depth control\n  - Path filtering\n  - Rate limiting\n  - Progress tracking\n  - Sitemap integration\n\n- **Site Mapping**: Generate site structure maps\n  - Subdomain support\n  - Search filtering\n  - Link analysis\n  - Visual hierarchy\n\n- **Data Extraction**: Extract structured data from multiple URLs\n  - Schema validation\n  - Batch processing\n  - Web search enrichment\n  - Custom extraction prompts\n\n## Installation\n\n```bash\n# Global installation\nnpm install -g @modelcontextprotocol/mcp-server-firecrawl\n\n# Local project installation\nnpm install @modelcontextprotocol/mcp-server-firecrawl\n```\n\n## Quick Start\n\n1. Get your Firecrawl API key from the [developer portal](https://firecrawl.dev/dashboard)\n\n2. Set your API key:\n\n   **Unix/Linux/macOS (bash/zsh):**\n\n   ```bash\n   export FIRECRAWL_API_KEY=your-api-key\n   ```\n\n   **Windows (Command Prompt):**\n\n   ```cmd\n   set FIRECRAWL_API_KEY=your-api-key\n   ```\n\n   **Windows (PowerShell):**\n\n   ```powershell\n   $env:FIRECRAWL_API_KEY = \"your-api-key\"\n   ```\n\n   **Alternative: Using .env file (recommended for development):**\n\n   ```bash\n   # Install dotenv\n   npm install dotenv\n\n   # Create .env file\n   echo \"FIRECRAWL_API_KEY=your-api-key\" > .env\n   ```\n\n   Then in your code:\n\n   ```javascript\n   import dotenv from 'dotenv';\n   dotenv.config();\n   ```\n\n3. Run the server:\n\n   ```bash\n   mcp-server-firecrawl\n   ```\n\n## Integration\n\n### Claude Desktop App\n\nAdd to your MCP settings:\n\n```json\n{\n  \"firecrawl\": {\n    \"command\": \"mcp-server-firecrawl\",\n    \"env\": {\n      \"FIRECRAWL_API_KEY\": \"your-api-key\"\n    }\n  }\n}\n```\n\n### Claude VSCode Extension\n\nAdd to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"firecrawl\": {\n      \"command\": \"mcp-server-firecrawl\",\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## Usage Examples\n\n### Web Scraping\n\n```typescript\n// Basic scraping\n{\n  name: \"scrape_url\",\n  arguments: {\n    url: \"https://example.com\",\n    formats: [\"markdown\"],\n    onlyMainContent: true\n  }\n}\n\n// Advanced extraction\n{\n  name: \"scrape_url\",\n  arguments: {\n    url: \"https://example.com/blog\",\n    jsonOptions: {\n      prompt: \"Extract article content\",\n      schema: {\n        title: \"string\",\n        content: \"string\"\n      }\n    },\n    mobile: true,\n    blockAds: true\n  }\n}\n```\n\n### Site Crawling\n\n```typescript\n// Basic crawling\n{\n  name: \"crawl\",\n  arguments: {\n    url: \"https://example.com\",\n    maxDepth: 2,\n    limit: 100\n  }\n}\n\n// Advanced crawling\n{\n  name: \"crawl\",\n  arguments: {\n    url: \"https://example.com\",\n    maxDepth: 3,\n    includePaths: [\"/blog\", \"/products\"],\n    excludePaths: [\"/admin\"],\n    ignoreQueryParameters: true\n  }\n}\n```\n\n### Site Mapping\n\n```typescript\n// Generate site map\n{\n  name: \"map\",\n  arguments: {\n    url: \"https://example.com\",\n    includeSubdomains: true,\n    limit: 1000\n  }\n}\n```\n\n### Data Extraction\n\n```typescript\n// Extract structured data\n{\n  name: \"extract\",\n  arguments: {\n    urls: [\"https://example.com/product1\", \"https://example.com/product2\"],\n    prompt: \"Extract product details\",\n    schema: {\n      name: \"string\",\n      price: \"number\",\n      description: \"string\"\n    }\n  }\n}\n```\n\n## Configuration\n\nSee [configuration guide](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/docs/configuration.md) for detailed setup options.\n\n## API Documentation\n\nSee [API documentation](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/docs/api.md) for detailed endpoint specifications.\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Start in development mode\nnpm run dev\n```\n\n## Examples\n\nCheck the [examples](https://github.com/Msparihar/mcp-server-firecrawl/tree/main/examples) directory for more usage examples:\n\n- Basic scraping: [scrape.ts](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/examples/scrape.ts)\n- Crawling and mapping: [crawl-and-map.ts](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/examples/crawl-and-map.ts)\n\n## Error Handling\n\nThe server implements robust error handling:\n\n- Rate limiting with exponential backoff\n- Automatic retries\n- Detailed error messages\n- Debug logging\n\n## Security\n\n- API key protection\n- Request validation\n- Domain allowlisting\n- Rate limiting\n- Safe error messages\n\n## Contributing\n\nSee [CONTRIBUTING.md](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/CONTRIBUTING.md) for contribution guidelines.\n\n## License\n\nMIT License - see [LICENSE](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/LICENSE) for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-firecrawl",
      "npm_downloads": 9784,
      "keywords": [
        "firecrawl",
        "scraping",
        "crawling",
        "firecrawl provides",
        "server firecrawl",
        "firecrawl api"
      ],
      "category": "web-search"
    },
    "NightTrek--Serper-search-mcp": {
      "owner": "NightTrek",
      "name": "Serper-search-mcp",
      "url": "https://github.com/NightTrek/Serper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/NightTrek.webp",
      "description": "Integrates Google search capabilities through the Serper API, offering rich search data and an AI-powered Deep Research tool for thorough topic exploration and information synthesis.",
      "stars": 41,
      "forks": 15,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-16T15:27:59Z",
      "readme_content": "# Serper Search MCP Server\n\nA Model Context Protocol server that provides Google search capabilities through the Serper API, along with an AI-powered Deep Research tool. This server enables easy integration of search and research functionality into your MCP-enabled applications.\n\n## ✨ Features\n\n- 🌐 Powerful Google search integration through Serper API\n- 🔄 Rich search response data including:\n  - Knowledge Graph information\n  - Organic search results\n  - \"People Also Ask\" questions\n  - Related searches\n- 🧠 AI-powered Deep Research tool:\n  - Performs multi-step, iterative research\n  - Generates sub-queries to explore topics thoroughly\n  - Synthesizes information from multiple sources\n  - Provides citations for all information\n  - Adjustable research depth levels\n  - Built-in quality metrics for continuous improvement\n- 🛠 Configurable search parameters:\n  - Country targeting\n  - Language selection\n  - Result count customization\n  - Autocorrect options\n- 🔒 Secure API key handling\n- ⚡️ Rate limiting and caching support\n- 📝 TypeScript support with full type definitions\n- 📊 Integrated performance metrics for result optimization\n\n## 🚀 Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/serper-search-server.git\ncd serper-search-server\n```\n\n2. Install dependencies:\n```bash\npnpm install\n```\n\n3. Build the server:\n```bash\npnpm run build\n```\n\n## ⚙️ Configuration\n\n1. Get your Serper API key from [Serper.dev](https://serper.de\n\n2. Create a `.env` file in the root directory:\n```env\n# Required\nSERPER_API_KEY=your_api_key_here\n\n# Optional - Advanced Quality Metrics Configuration (pre-configured by default)\nUSAGE_METRICS_KEY=your-custom-metrics-key     # Optional\nUSAGE_PROJECT_ID=your-custom-project-id       # Optional\nMETRICS_ENDPOINT=https://your-custom-host.com # Optional\nDISABLE_METRICS=false                         # Not recommended\n```\n\nSee [TELEMETRY.md](TELEMETRY.md) for detailed information about:\n- Quality metrics collection\n- Performance monitoring\n- Usage analytics\n- Dashboard setup\n- Continuous improvement\n\n## 🔌 Integration\n\n### Claude Desktop\n\nAdd the server config to your Claude Desktop configuration:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search-server\": {\n      \"command\": \"/path/to/serper-search-server/build/index.js\",\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## 🛠 Usage\n\n### Search Tool\n\nThe server provides a powerful search tool with the following parameters:\n\n```typescript\n{\n  \"query\": string,          // Search query\n  \"numResults\"?: number,    // Number of results (default: 10, max: 100)\n  \"gl\"?: string,           // Country code (e.g., \"us\", \"uk\")\n  \"hl\"?: string,           // Language code (e.g., \"en\", \"es\")\n  \"autocorrect\"?: boolean, // Enable autocorrect (default: true)\n  \"type\"?: \"search\"        // Search type (more types coming soon)\n}\n```\n\n### Deep Research Tool\n\nFor more comprehensive research needs, the server provides a deep research tool that performs multi-step research with the following parameters:\n\n```typescript\n{\n  \"query\": string,          // Research query or question\n  \"depth\"?: \"basic\" | \"standard\" | \"deep\",  // Research depth (default: \"standard\")\n  \"maxSources\"?: number     // Maximum sources to include (default: 10)\n}\n```\n\nThe deep research tool:\n- Breaks down complex queries into focused sub-queries\n- Executes multiple searches to gather comprehensive information\n- Uses AI to synthesize information from multiple sources\n- Formats results with proper citations and references\n- Adapts its research strategy based on intermediate results\n- Collects anonymous quality metrics to improve search results\n\nDepth Levels:\n- basic: Quick overview (3-5 sources, ~5 min)\n  Good for: Simple facts, quick definitions, straightforward questions\n- standard: Comprehensive analysis (5-10 sources, ~10 min)\n  Good for: Most research needs, balanced depth and speed\n- deep: Exhaustive research (10+ sources, ~15-20 min)\n  Good for: Complex topics, academic research, thorough analysis\n\n### Search Tool Example Response\n\nThe search results include rich data:\n\n```json\n{\n  \"searchParameters\": {\n    \"q\": \"apple inc\",\n    \"gl\": \"us\",\n    \"hl\": \"en\",\n    \"autocorrect\": true,\n    \"type\": \"search\"\n  },\n  \"knowledgeGraph\": {\n    \"title\": \"Apple\",\n    \"type\": \"Technology company\",\n    \"website\": \"http://www.apple.com/\",\n    \"description\": \"Apple Inc. is an American multinational technology company...\",\n    \"attributes\": {\n      \"Headquarters\": \"Cupertino, CA\",\n      \"CEO\": \"Tim Cook (Aug 24, 2011–)\",\n      \"Founded\": \"April 1, 1976, Los Altos, CA\"\n    }\n  },\n  \"organic\": [\n    {\n      \"title\": \"Apple\",\n      \"link\": \"https://www.apple.com/\",\n      \"snippet\": \"Discover the innovative world of Apple...\",\n      \"position\": 1\n    }\n  ],\n  \"peopleAlsoAsk\": [\n    {\n      \"question\": \"What does Apple Inc mean?\",\n      \"snippet\": \"Apple Inc., formerly Apple Computer, Inc....\",\n      \"link\": \"https://www.britannica.com/topic/Apple-Inc\"\n    }\n  ],\n  \"relatedSearches\": [\n    {\n      \"query\": \"Who invented the iPhone\"\n    }\n  ]\n}\n```\n\n## 🔍 Response Types\n\n### Knowledge Graph\nContains entity information when available:\n- Title and type\n- Website URL\n- Description\n- Key attributes\n\n### Organic Results\nList of search results including:\n- Title and URL\n- Snippet (description)\n- Position in results\n- Sitelinks when available\n\n### People Also Ask\nCommon questions related to the search:\n- Question text\n- Answer snippet\n- Source link\n\n### Related Searches\nList of related search queries users often make.\n\n## 📊 Quality Metrics\n\nThe Deep Research tool includes integrated quality metrics:\n\n- Research process metrics\n- Performance monitoring\n- Issue tracking\n- Usage patterns\n- Result quality indicators\n\nSee [TELEMETRY.md](TELEMETRY.md) for detailed information about the metrics collected to improve search quality.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Serper API](https://serper.dev) for providing the Google search capabilities\n- [Model Context Protocol](https://github.com/modelcontextprotocol/mcp) for the MCP framework\n- [PostHog](https://posthog.com) for analytics capabilities\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "serper",
        "serper search",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "Ozamatash--deep-research-mcp": {
      "owner": "Ozamatash",
      "name": "deep-research-mcp",
      "url": "https://github.com/Ozamatash/deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Ozamatash.webp",
      "description": "Performs deep research on various topics by integrating search engines, web scraping, and language models to create comprehensive reports and follow-up questions.",
      "stars": 275,
      "forks": 28,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:50Z",
      "readme_content": "# Open Deep Research MCP Server\n\nAn AI-powered research assistant that performs deep, iterative research on any topic. It combines search engines, web scraping, and AI to explore topics in depth and generate comprehensive reports. Available as a Model Context Protocol (MCP) tool or standalone CLI. Look at exampleout.md to see what a report might look like.\n\n## Quick Start\n\n1. Clone and install:\n```bash\ngit clone https://github.com/Ozamatash/deep-research\ncd deep-research\nnpm install\n```\n\n2. Set up environment in `.env.local`:\n```bash\n# Copy the example environment file\ncp .env.example .env.local\n```\n\n3. Build:\n```bash\n# Build the server\nnpm run build\n```\n\n4. Run the cli version:\n```bash\nnpm run start\n```\n5. Test MCP Server with Claude Desktop:  \nFollow the guide thats at the bottom of server quickstart to add the server to Claude Desktop:  \nhttps://modelcontextprotocol.io/quickstart/server\n\nFor remote servers: Streamable HTTP\n```bash\nnpm run start:http\n```\nServer runs on `http://localhost:3000/mcp` without session management.\n\n## Features\n\n- Performs deep, iterative research by generating targeted search queries\n- Controls research scope with depth (how deep) and breadth (how wide) parameters\n- Evaluates source reliability with detailed scoring (0-1) and reasoning\n- Prioritizes high-reliability sources (≥0.7) and verifies less reliable information\n- Generates follow-up questions to better understand research needs\n- Produces detailed markdown reports with findings, sources, and reliability assessments\n- Available as a Model Context Protocol (MCP) tool for AI agents\n- For now MCP version doesn't ask follow up questions\n- Natural-language source preferences (avoid listicles, forums, affiliate reviews, specific domains)\n\n### Model Selection (OpenAI, Anthropic, Google, xAI)\n\nPick a provider and model per run.\n\n- CLI: you will be prompted for provider and model. Example: `openai` + `gpt-5`.\n- MCP/HTTP: pass `model`, e.g. `openai:gpt-5`,\n\nSet the corresponding API key in `.env.local`:\n\n```\nOPENAI_API_KEY=...\nANTHROPIC_API_KEY=...\nGOOGLE_API_KEY=...\nXAI_API_KEY=...\n```\n\n## How It Works\n\n```mermaid\nflowchart TB\n    subgraph Input\n        Q[User Query]\n        B[Breadth Parameter]\n        D[Depth Parameter]\n        FQ[Feedback Questions]\n    end\n\n    subgraph Research[Deep Research]\n        direction TB\n        SQ[Generate SERP Queries]\n        SR[Search]\n        RE[Source Reliability Evaluation]\n        PR[Process Results]\n    end\n\n    subgraph Results[Research Output]\n        direction TB\n        L((Learnings with\n        Reliability Scores))\n        SM((Source Metadata))\n        ND((Next Directions:\n        Prior Goals,\n        New Questions))\n    end\n\n    %% Main Flow\n    Q & FQ --> CQ[Combined Query]\n    CQ & B & D --> SQ\n    SQ --> SR\n    SR --> RE\n    RE --> PR\n\n    %% Results Flow\n    PR --> L\n    PR --> SM\n    PR --> ND\n\n    %% Depth Decision and Recursion\n    L & ND --> DP{depth > 0?}\n    DP -->|Yes| SQ\n    \n    %% Final Output\n    DP -->|No| MR[Markdown Report]\n\n    %% Styling\n    classDef input fill:#7bed9f,stroke:#2ed573,color:black\n    classDef process fill:#70a1ff,stroke:#1e90ff,color:black\n    classDef output fill:#ff4757,stroke:#ff6b81,color:black\n    classDef results fill:#a8e6cf,stroke:#3b7a57,color:black,width:150px,height:150px\n\n    class Q,B,D,FQ input\n    class SQ,SR,RE,PR process\n    class MR output\n    class L,SM,ND results\n```\n## Advanced Setup\n\n### Using Local Firecrawl (Free Option)\n\nInstead of using the Firecrawl API, you can run a local instance. You can use the official repo or my fork which uses searXNG as the search backend to avoid using a searchapi key:\n\n1. Set up local Firecrawl:\n```bash\ngit clone https://github.com/Ozamatash/localfirecrawl\ncd localfirecrawl\n# Follow setup in localfirecrawl README\n```\n\n2. Update `.env.local`:\n```bash\nFIRECRAWL_BASE_URL=\"http://localhost:3002\"\n```\n\n### Optional: Observability\n\nAdd observability to track research flows, queries, and results using Langfuse:\n\n```bash\n# Add to .env.local\nLANGFUSE_PUBLIC_KEY=\"your_langfuse_public_key\"\nLANGFUSE_SECRET_KEY=\"your_langfuse_secret_key\"\n```\n\nThe app works normally without observability if no Langfuse keys are provided.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "research",
        "web",
        "search engines",
        "web search",
        "deep research"
      ],
      "category": "web-search"
    },
    "PhamQuangVinh22022648--brave-search": {
      "owner": "PhamQuangVinh22022648",
      "name": "brave-search",
      "url": "https://github.com/PhamQuangVinh22022648/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/PhamQuangVinh22022648.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, allowing general web queries, news and article searches, as well as searches for local businesses and services. Supports pagination, filtering, and smart fallbacks to web results when local queries yield no results.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-05-12T05:33:10Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n#### Docker\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"BRAVE_API_KEY\",\n          \"mcp/brave-search\"\n        ],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "search",
        "brave",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "PhialsBasement--Pagespeed-MCP-Server": {
      "owner": "PhialsBasement",
      "name": "Pagespeed-MCP-Server",
      "url": "https://github.com/PhialsBasement/Pagespeed-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Connects AI models to Google's PageSpeed Insights API for detailed website performance analysis and metrics interpretation.",
      "stars": 7,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-13T23:16:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-pagespeed-mcp-server-badge.png)](https://mseep.ai/app/phialsbasement-pagespeed-mcp-server)\n\n# PageSpeed MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-pagespeed-server)](https://smithery.ai/server/mcp-pagespeed-server)\n\nA Model Context Protocol (MCP) server that extends AI assistant capabilities with PageSpeed Insights functionality. This server acts as a bridge between AI models and Google's PageSpeed Insights API, enabling detailed performance analysis of websites.\n\n## Overview\n\nThe PageSpeed MCP server is designed to enhance AI assistants' capabilities by allowing them to perform comprehensive web performance analysis. When integrated, AI models can request and interpret detailed performance metrics, Core Web Vitals, and other critical web performance data for any given URL.\n\n## Installation\n\n### Installing via Smithery\n\nTo install PageSpeed Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-pagespeed-server):\n\n```bash\nnpx -y @smithery/cli install mcp-pagespeed-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install pagespeed-mcp-server\n```\n\n## Configuration\n\nAdd the PageSpeed MCP to your AI assistant's(claude in this case) configuration file:\n\n```json\n{\n    \"pagespeed\": {\n        \"command\": \"node\",\n        \"args\": [\"path/to/mcp-pagespeed-server/dist/index.js\"],\n        \"env\": {\n           \"GOOGLE_API_KEY\": \"<YOUR-API-KEY>\"\n        }\n    }\n}\n```\n\n## Detailed Capabilities\n\n### Performance Metrics Analysis\n- First Contentful Paint (FCP)\n- Largest Contentful Paint (LCP)\n- Time to Interactive (TTI)\n- Total Blocking Time (TBT)\n- Cumulative Layout Shift (CLS)\n- Speed Index\n- Time to First Byte (TTFB)\n\n### Best Practices Assessment\n- HTTPS usage\n- JavaScript error monitoring\n- Browser console warnings\n- Deprecated API usage\n- Image aspect ratio analysis\n- Link security checks\n\n### SEO Analysis\n- Meta description validation\n- Robots.txt validation\n- Structured data validation\n- Crawlable links verification\n- Meta tags assessment\n- Mobile friendliness\n\n### Accessibility Audits\n- ARIA attribute validation\n- Color contrast checking\n- Heading hierarchy analysis\n- Alt text verification\n- Focus management assessment\n- Keyboard navigation testing\n\n### Resource Optimization\n- Image optimization suggestions\n- JavaScript bundling analysis\n- CSS optimization recommendations\n- Cache policy validation\n- Resource minification checks\n- Render-blocking resource identification\n\n## API Response Structure\n\nThe MCP server provides detailed JSON responses including:\n\n```javascript\n{\n    \"lighthouseResult\": {\n        \"categories\": {\n            \"performance\": { /* Performance metrics */ },\n            \"accessibility\": { /* Accessibility results */ },\n            \"best-practices\": { /* Best practices audit */ },\n            \"seo\": { /* SEO findings */ }\n        },\n        \"audits\": {\n            // Detailed audit results for each category\n        },\n        \"timing\": {\n            // Performance timing data\n        },\n        \"stackPacks\": {\n            // Technology-specific advice\n        }\n    }\n}\n```\n\n## Advanced Usage\n\n### Custom Configuration\nYou can customize the PageSpeed analysis by providing additional parameters:\n\n```json\n{\n    \"strategy\": \"mobile\", // or \"desktop\"\n    \"category\": [\"performance\", \"accessibility\", \"best-practices\", \"seo\"],\n    \"locale\": \"en\",\n    \"threshold\": {\n        \"performance\": 90,\n        \"accessibility\": 100,\n        \"best-practices\": 90,\n        \"seo\": 90\n    }\n}\n```\n\n### Error Handling\nThe MCP server includes robust error handling for:\n- Invalid URLs\n- Network timeouts\n- API rate limiting\n- Invalid parameters\n- Server-side errors\n\n## Requirements\n\n\n### Network Requirements\n- Stable internet connection\n- Access to Google's PageSpeed Insights API\n\n### Platform Support\n- Windows (x64, x86)\n- Linux (x64)\n- macOS (x64, arm64)\n\n## Integration Examples\n\n### Basic Integration\n```javascript\nconst PageSpeedMCP = require('pagespeed-mcp-server');\nconst mcp = new PageSpeedMCP();\n\nawait mcp.analyze('https://example.com');\n```\n\n### With Custom Options\n```javascript\nconst results = await mcp.analyze('https://example.com', {\n    strategy: 'mobile',\n    categories: ['performance', 'accessibility'],\n    locale: 'en-US'\n});\n```\n\n## Troubleshooting\n\n### Common Issues\n1. Connection Timeouts\n   - Check internet connectivity\n\n2. API Rate Limiting\n   - Use API key for higher limits\n\n3. Memory Issues\n   - Adjust Node.js memory limits\n\n## Development\n\n### Building from Source\n```bash\ngit clone https://github.com/phialsbasement/mcp-pagespeed-server\ncd mcp-pagespeed-server\nnpm install\nnpm run build\n```\n\n### Running Tests\n```bash\nnpm run test\n```\n\n### Contributing\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Support\n\n### Getting Help\n- GitHub Issues: Report bugs and feature requests\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pagespeed",
        "performance",
        "google",
        "pagespeed insights",
        "google pagespeed",
        "phialsbasement pagespeed"
      ],
      "category": "web-search"
    },
    "PhialsBasement--mcp-webresearch-stealthified": {
      "owner": "PhialsBasement",
      "name": "mcp-webresearch-stealthified",
      "url": "https://github.com/PhialsBasement/mcp-webresearch-stealthified",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Connects AI models to the web for real-time information retrieval, webpage content extraction, and research session tracking, along with the ability to capture screenshots.",
      "stars": 7,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T02:14:39Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration --- THIS FORK FIXES THIS --- NOW NO LONGER GETTING CAPTCHA BLOCKED\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [x] Linux\n- [x] Windows\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) \n",
      "npm_url": "https://www.npmjs.com/package/mcp-webresearch-stealthified",
      "npm_downloads": 328,
      "keywords": [
        "webresearch",
        "webpage",
        "web",
        "webresearch stealthified",
        "web search",
        "mcp webresearch"
      ],
      "category": "web-search"
    },
    "Prajwal-ak-0--youtube-mcp": {
      "owner": "Prajwal-ak-0",
      "name": "youtube-mcp",
      "url": "https://github.com/Prajwal-ak-0/youtube-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Prajwal-ak-0.webp",
      "description": "Extract transcripts from YouTube videos, summarize content using Gemini AI, and facilitate natural language queries about video content. Provides tools for YouTube video search and comment analysis.",
      "stars": 12,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:13Z",
      "readme_content": "# YouTube MCP\n[![smithery badge](https://smithery.ai/badge/@Prajwal-ak-0/youtube-mcp)](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp)\n\nA Model Context Protocol (MCP) server for YouTube video analysis, providing tools to get transcripts, summarize content, and query videos using Gemini AI.\n\n## Features\n\n- 📝 **Transcript Extraction**: Get detailed transcripts from YouTube videos\n- 📊 **Video Summarization**: Generate concise summaries using Gemini AI\n- ❓ **Natural Language Queries**: Ask questions about video content\n- 🔍 **YouTube Search**: Find videos matching specific queries\n- 💬 **Comment Analysis**: Retrieve and analyze video comments\n\n## Requirements\n\n- Python 3.9+\n- Google Gemini API key\n- YouTube Data API key\n\n## Running Locally\n\n### Installing via Smithery\n\nTo install youtube-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp):\n\n```bash\nnpx -y @smithery/cli install @Prajwal-ak-0/youtube-mcp --client claude\n```\n\n### Option 1: Install directly from smithery\n\n[![smithery badge](https://smithery.ai/badge/@Prajwal-ak-0/youtube-mcp)](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp)\n\n### Option 2: Local setup\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/Prajwal-ak-0/youtube-mcp\n   cd youtube-mcp\n   ```\n\n2. Create a virtual environment and install dependencies:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   pip install -e .\n   ```\n\n3. Create a `.env` file with your API keys:\n   ```\n   GEMINI_API_KEY=your_gemini_api_key\n   YOUTUBE_API_KEY=your_youtube_api_key\n   ```\n   \n4. Run MCP Server\n   ```bash\n   mcp dev main.py\n   ```\n   Navigate to [Stdio](http://localhost:5173)\n\n   OR\n\n6. Go cursor or windsurf configure with this json content:\n   ```json\n   {\n     \"youtube\": {\n       \"command\": \"uv\",\n       \"args\": [\n         \"--directory\",\n         \"/absolute/path/to/youtube-mcp\",\n         \"run\",\n         \"main.py\",\n         \"--transport\",\n         \"stdio\",\n         \"--debug\"\n       ]\n     }\n   }\n   ```\n\n## Available Tools\n\n- `youtube/get-transcript`: Get video transcript\n- `youtube/summarize`: Generate a video summary\n- `youtube/query`: Answer questions about a video\n- `youtube/search`: Search for YouTube videos\n- `youtube/get-comments`: Retrieve video comments\n- `youtube/get-likes`: Get video like count\n\n## Contributing\n\nContributions welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp",
      "npm_downloads": 590,
      "keywords": [
        "youtube",
        "prajwal",
        "search",
        "transcripts youtube",
        "youtube mcp",
        "ak youtube"
      ],
      "category": "web-search"
    },
    "PykeW--playwright-mcp-server": {
      "owner": "PykeW",
      "name": "playwright-mcp-server",
      "url": "https://github.com/PykeW/playwright-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/PykeW.webp",
      "description": "Retrieve and interact with web page content, enabling navigation, content extraction, and user action simulation on webpages using a standardized protocol.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-10T06:33:48Z",
      "readme_content": "# Playwright MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@showfive/playwright-mcp-server)](https://smithery.ai/server/@showfive/playwright-mcp-server)\n\nEnglish | [日本語](README.ja.md)\n\nThis project is a server that provides Playwright web page content retrieval functionality using the Model Context Protocol (MCP).\n\n## Features\n\n- Page navigation\n- Full page content retrieval\n- Visible content retrieval\n- Interactive elements detection\n- Mouse operation simulation\n- Echo functionality for testing\n\n## Installation\n\n### Installing via Smithery\n\nTo install Playwright MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@showfive/playwright-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @showfive/playwright-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### MCP Tools\n\nThe following tools are available:\n\n1. `navigate`\n   - Navigate to a specified URL\n   - Arguments: `{ url: string }`\n   - Returns: Navigation result\n\n2. `get_all_content`\n   - Retrieve content from the entire page\n   - Arguments: None\n   - Returns: All text content from the page\n\n3. `get_visible_content`\n   - Retrieve currently visible content\n   - Arguments: `{ minVisiblePercentage?: number }`\n   - Returns: Visible text content\n\n4. `get_interactive_elements`\n   - Get position information of interactive elements (buttons, links, etc.) on the page\n   - Arguments: None\n   - Returns: Coordinates and boundary information of interactive elements\n\n5. `move_mouse`\n   - Move mouse cursor to specified coordinates\n   - Arguments: `{ x: number, y: number }`\n   - Returns: Operation result\n\n6. `mouse_click`\n   - Execute mouse click at specified coordinates\n   - Arguments: `{ x: number, y: number, button?: \"left\" | \"right\" | \"middle\", clickCount?: number }`\n   - Returns: Click operation result\n\n7. `mouse_wheel`\n   - Execute mouse wheel scrolling\n   - Arguments: `{ deltaY: number, deltaX?: number }`\n   - Returns: Scroll operation result\n\n8. `drag_and_drop`\n   - Execute drag and drop operation\n   - Arguments: `{ sourceX: number, sourceY: number, targetX: number, targetY: number }`\n   - Returns: Drag and drop operation result\n\n9. `echo`\n   - Echo tool for testing\n   - Arguments: `{ message: string }`\n   - Returns: Sent message\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Generate coverage report\nnpm run test:coverage\n```\n\n### Test Structure\n\n- `tools/*.test.ts`: Function tests for each tool\n- `mcp-server.test.ts`: MCP server function tests\n\n## Implementation Features\n\n1. Content Retrieval\n   - Full page content retrieval\n   - Visible content only retrieval\n   - Proper HTML parsing\n\n2. Interaction\n   - Detection and position information retrieval of interactive elements\n   - Mouse operation simulation (movement, clicks, scrolling)\n   - Drag and drop support\n\n3. Error Handling\n   - Proper navigation error handling\n   - Timeout processing\n   - Invalid URL detection\n\n4. Configuration Flexibility\n   - Headless/head mode selection\n   - Custom user agent\n   - Viewport size settings\n\n## Important Notes\n\n- Ensure necessary environment variables are set before using the MCP server\n- Follow the terms of service of target websites when retrieving web page content\n- Maintain appropriate intervals when sending multiple requests\n- When performing mouse operations, maintain appropriate intervals as they simulate actual user interactions\n\n## License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webpages",
        "web",
        "page",
        "simulation webpages",
        "webpages using",
        "interact web"
      ],
      "category": "web-search"
    },
    "RamXX--mcp-tavily": {
      "owner": "RamXX",
      "name": "mcp-tavily",
      "url": "https://github.com/RamXX/mcp-tavily",
      "imageUrl": "/freedevtools/mcp/pfp/RamXX.webp",
      "description": "Provides AI-powered web search capabilities through Tavily's search API, enabling sophisticated searches, direct answers to questions, and retrieval of recent news articles with relevant content extraction.",
      "stars": 70,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:22Z",
      "readme_content": "# Tavily MCP Server\n\nA Model Context Protocol server that provides AI-powered web search capabilities using Tavily's search API. This server enables LLMs to perform sophisticated web searches, get direct answers to questions, and search recent news articles with AI-extracted relevant content.\n\n## Features\n\n### Available Tools\n\n- `tavily_web_search` - Performs comprehensive web searches with AI-powered content extraction.\n\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"basic\")\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n- `tavily_answer_search` - Performs web searches and generates direct answers with supporting evidence.\n\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"advanced\")\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n- `tavily_news_search` - Searches recent news articles with publication dates.\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `days` (integer, optional): Number of days back to search (default: 3)\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n### Prompts\n\nThe server also provides prompt templates for each search type:\n\n- **tavily_web_search** - Search the web using Tavily's AI-powered search engine\n- **tavily_answer_search** - Search the web and get an AI-generated answer with supporting evidence\n- **tavily_news_search** - Search recent news articles with Tavily's news search\n\n## Prerequisites\n\n- Python 3.11 or later\n- A Tavily API key (obtain from [Tavily's website](https://tavily.com))\n- `uv` Python package manager (recommended)\n\n## Installation\n\n### Option 1: Using pip or uv\n\n```bash\n# With pip\npip install mcp-tavily\n\n# Or with uv (recommended)\nuv add mcp-tavily\n```\n\nYou should see output similar to:\n\n```\nResolved packages: mcp-tavily, mcp, pydantic, python-dotenv, tavily-python [...]\nSuccessfully installed mcp-tavily-0.1.4 mcp-1.0.0 [...]\n```\n\n### Option 2: From source\n\n```bash\n# Clone the repository\ngit clone https://github.com/RamXX/mcp-tavily.git\ncd mcp-tavily\n\n# Create a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies and build\nuv sync  # Or: pip install -r requirements.txt\nuv build  # Or: pip install -e .\n\n# To install with test dependencies:\nuv sync --dev  # Or: pip install -r requirements-dev.txt\n```\n\nDuring installation, you should see the package being built and installed with its dependencies.\n\n### Usage with VS Code\n\nFor quick installation, use one of the one-click install buttons below:\n\n[![Install with UV in VS Code](https://img.shields.io/badge/VS_Code-UV-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Tavily%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-tavily%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with UV in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-UV-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Tavily%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-tavily%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Tavily API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"tavily\": {\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-tavily\"],\n        \"env\": {\n          \"TAVILY_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### API Key Setup\n\nThe server requires a Tavily API key, which can be provided in three ways:\n\n1. Through a `.env` file in your project directory:\n\n   ```\n   TAVILY_API_KEY=your_api_key_here\n   ```\n\n2. As an environment variable:\n\n   ```bash\n   export TAVILY_API_KEY=your_api_key_here\n   ```\n\n3. As a command-line argument:\n   ```bash\n   python -m mcp_server_tavily --api-key=your_api_key_here\n   ```\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n```json\n\"mcpServers\": {\n  \"tavily\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_tavily\"]\n  },\n  \"env\": {\n    \"TAVILY_API_KEY\": \"your_api_key_here\"\n  }\n}\n```\n\nIf you encounter issues, you may need to specify the full path to your Python interpreter. Run `which python` to find the exact path.\n\n## Usage Examples\n\nFor a regular web search:\n\n```\nTell me about Anthropic's newly released MCP protocol\n```\n\nTo generate a report with domain filtering:\n\n```\nTell me about redwood trees. Please use MLA format in markdown syntax and include the URLs in the citations. Exclude Wikipedia sources.\n```\n\nTo use answer search mode for direct answers:\n\n```\nI want a concrete answer backed by current web sources: What is the average lifespan of redwood trees?\n```\n\nFor news search:\n\n```\nGive me the top 10 AI-related news in the last 5 days\n```\n\n## Testing\n\nThe project includes a comprehensive test suite with automated dependency compatibility testing.\n\n### Running Tests\n\n1. Install test dependencies:\n\n   ```bash\n   source .venv/bin/activate  # If using a virtual environment\n   uv sync --dev  # Or: pip install -r requirements-dev.txt\n   ```\n\n2. Run the standard test suite:\n   ```bash\n   ./tests/run_tests.sh\n   # Or using Make\n   make test\n   ```\n\n### Dependency Compatibility Testing\n\nTo ensure the project works with the latest dependency versions, use these commands:\n\n```bash\n# Test with latest dependencies using Make\nmake test-deps\n\n# Full compatibility test with verbose output\nmake test-compatibility\n\n# Or use the standalone script\n./scripts/test-compatibility.sh\n```\n\nThese commands will:\n- Update all dependencies to their latest versions\n- Run the full test suite with coverage\n- Report any compatibility issues\n- Show version changes for transparency\n\n### Automated Testing\n\nThe project includes automated dependency compatibility testing through GitHub Actions:\n\n- **Weekly Testing**: Runs every Monday at 8 AM UTC\n- **Multi-Python Support**: Tests against Python 3.11, 3.12, and 3.13\n- **Issue Creation**: Automatically creates GitHub issues when tests fail\n- **Manual Trigger**: Can be triggered manually from the GitHub Actions tab\n\n### Understanding Test Results\n\n**When tests pass**: Your project is compatible with the latest dependency versions. You can safely update your requirements files.\n\n**When tests fail**: Review the test output to identify breaking changes, update your code to handle API changes, update tests if needed, or consider pinning problematic dependency versions.\n\n### Test Output Example\n\nYou should see output similar to:\n\n```\n======================================================= test session starts ========================================================\nplatform darwin -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0\nrootdir: /Users/ramirosalas/workspace/mcp-tavily\nconfigfile: pyproject.toml\nplugins: cov-6.0.0, asyncio-0.25.3, anyio-4.8.0, mock-3.14.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=function\ncollected 50 items                                                                                                                 \n\ntests/test_docker.py ..                                                                                                      [  4%]\ntests/test_integration.py .....                                                                                              [ 14%]\ntests/test_models.py .................                                                                                       [ 48%]\ntests/test_server_api.py .....................                                                                               [ 90%]\ntests/test_utils.py .....                                                                                                    [100%]\n\n---------- coverage: platform darwin, python 3.13.3-final-0 ----------\nName                                Stmts   Miss  Cover\n-------------------------------------------------------\nsrc/mcp_server_tavily/__init__.py      16      2    88%\nsrc/mcp_server_tavily/__main__.py       2      2     0%\nsrc/mcp_server_tavily/server.py       149     16    89%\n-------------------------------------------------------\nTOTAL                                 167     20    88%\n```\n\nThe test suite includes tests for data models, utility functions, integration testing, error handling, and parameter validation. It focuses on verifying that all API capabilities work correctly, including handling of domain filters and various input formats.\n\n## Release Management\n\nThe project includes tools for building and releasing with the latest dependency versions:\n\n### Building with Latest Dependencies\n\n```bash\n# Build package with latest dependency versions\nmake build-latest\n\n# Complete release workflow: test, build, and check with latest deps\nmake release-all\n\n# Prepare a release with version management\n./scripts/prepare-release.sh [new_version]\n```\n\n### Release Workflow\n\n**Recommended approach for releases with latest dependencies:**\n\n1. **Complete release preparation**: `make release-all`\n2. **Upload without downgrades**: `make upload-latest`\n\n**Alternative step-by-step approach:**\n\n1. **Test with latest dependencies**: `make test-compatibility`\n2. **Build for release**: `make release-build`\n3. **Upload without rebuilding**: `make upload-latest`\n\n**One-command release and publish:**\n```bash\nmake release-publish\n```\n\n**Important**: Use `make upload-latest` instead of `make upload` to prevent dependency downgrades during the upload process. The `upload-latest` command uses existing distribution files without reinstalling dependencies.\n\nThe release commands ensure your package is built and tested with the most recent compatible dependency versions, preventing the downgrades that can occur with traditional build chains.\n\n## Docker\n\nBuild the Docker image:\n\n```bash\nmake docker-build\n```\n\nAlternatively, build directly with Docker:\n\n```bash\ndocker build -t mcp_tavily .\n```\n\nRun a detached Docker container (default name `mcp_tavily_container`, port 8000 → 8000):\n\n```bash\nmake docker-run\n```\n\nOr manually:\n\n```bash\ndocker run -d --name mcp_tavily_container \\\n  -e TAVILY_API_KEY=your_api_key_here \\\n  -p 8000:8000 mcp_tavily\n```\n\nStop and remove the container:\n\n```bash\nmake docker-stop\n```\n\nFollow container logs:\n\n```bash\nmake docker-logs\n```\n\nYou can override defaults by setting environment variables:\n  - DOCKER_IMAGE: image name (default `mcp_tavily`)\n  - DOCKER_CONTAINER: container name (default `mcp_tavily_container`)\n  - HOST_PORT: host port to bind (default `8000`)\n  - CONTAINER_PORT: container port (default `8000`)\n\n## Debugging\n\nYou can use the MCP inspector to debug the server:\n\n```bash\n# Using npx\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n\n# For development\ncd path/to/mcp-tavily\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n```\n\n## Contributing\n\nWe welcome contributions to improve mcp-tavily! Here's how you can help:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests to ensure they pass\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\n## License\n\nmcp-tavily is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "retrieval",
        "tavily search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "Richard-Weiss--mcp-google-cse": {
      "owner": "Richard-Weiss",
      "name": "mcp-google-cse",
      "url": "https://github.com/Richard-Weiss/mcp-google-cse",
      "imageUrl": "/freedevtools/mcp/pfp/Richard-Weiss.webp",
      "description": "Provides search capabilities using a custom Google search engine, returning only search results without content extraction. Designed for integration with other tools to enable content retrieval and more extensive searching.",
      "stars": 28,
      "forks": 5,
      "license": "The Unlicense",
      "language": "Python",
      "updated_at": "2025-09-08T02:35:59Z",
      "readme_content": "# Google Custom Search Engine MCP Server\n\nA Model Context Protocol server that provides search capabilities using a CSE (custom search engine). This server enables LLMs to provide a regular google search term and returns the found search results.\n\nThe tool only returns the results itself and not the content, the tool should be combined with other servers like [mcp-server-fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) to extract the content from the search results.\nYou may also combine it with other tools to enable some kind of \"deep search\" or tool chaining in general.\n\n**The free quota is 100 searches (1 tool call == 1 search) per day, if you don't want to set up billing and this is insufficient for your use case, you should consider using another server.**\n\n<a href=\"https://glama.ai/mcp/servers/mieczol4lv\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mieczol4lv/badge\" alt=\"Google Custom Search Engine Server MCP server\" /></a>\n[![smithery badge](https://smithery.ai/badge/@Richard-Weiss/mcp-google-cse)](https://smithery.ai/server/@Richard-Weiss/mcp-google-cse)\n\n## Available Tools\n\n- `google_search` - Searches the custom search engine using the search term and returns a list of results containing the title, link and snippet of each result.\n    - `search_term` (string, required): The search term to search for, equaling the [query parameter](https://bit.ly/AllTheOperators) `q` in the usual Google search.\n\n## Environment variables\n\n- `API_KEY` (required): The API key for the custom search engine.\n- `ENGINE_ID` (required): The engine ID for the custom search engine.\n- `SERVICE_NAME` (required/optional): The name of the service, leave empty if you haven't changed the name (customsearch).\n- `COUNTRY_REGION` (optional): Restricts search results to documents originating in a particular country. See [Country Parameter Values](https://developers.google.com/custom-search/docs/json_api_reference#countryCollections) for valid values.\n- `GEOLOCATION` (optional, default \"us\"): The geolocation of the end-user performing the search. See [Geolocation Parameter Values](https://developers.google.com/custom-search/docs/json_api_reference#countryCodes) for valid values.\n- `RESULT_LANGUAGE` (optional, default \"lang_en\"): The language of the search results. See [CSE Query parameters, lr](https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list?apix=true#query-parameters) for valid values.\n- `RESULT_NUM` (optional, default 10): The number of search results to return. Range from 1-10.\n\n## CSE Setup\nCreating a custom search engine is comparatively easy, completely free and can be done in under 5 minutes.\n\n1. Go to https://console.cloud.google.com/ and create a new project. Call it \"Claude CSE\" for example.\n2. Select the project and search for \"Custom Search API\" in the search bar.\n3. Click on the search result and click on \"Enable\".\n4. Click on the Credentials tab and create a new API key.\n5. Go to https://programmablesearchengine.google.com to create a new custom search engine.\n6. Create a new search engine and give it any name, the name doesn't correlate to SERVICE_NAME.\n7. Select \"Search the entire web\" if you want a normal Google Search experience.\n8. Click on \"Create\" and copy the engine id from the js code, or hit customize and get it from the overview.\n9. You can optionally customize the search engine to your liking.\n\nWith the default quota, you will get 100 searches per day for free. A tool call only costs 1 search, even if you get 10 results for example.\n\n\n## Installation\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-google-cse*.\n\n### Using PIP\n\nAlternatively you can install `mcp-google-cse` via pip:\n\n```\npip install mcp-google-cse\n```\n\nAfter installation, you can run it as a script using:\n\n```\npython -m mcp-google-cse\n```\n\n### Installing via Smithery\n\nTo install Google Custom Search Engine for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Richard-Weiss/mcp-google-cse):\n\n```bash\nnpx -y @smithery/cli install @Richard-Weiss/mcp-google-cse --client claude\n```\n\n## Configuration\n\n### Configure for Claude app\n\nAdd to your `claude_desktop_config.json`:\n\n\n#### Using uvx (use this if you don't know which one to choose)\n```\n\"mcp-google-cse\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-google-cse\"],\n    \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n    }\n}\n```\n\n\n#### Using pip installation\n\n```\n\"mcp-google-cse\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp-google-cse\"],\n    \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n    }\n}\n```\n\n#### Running locally\n\n```\n    \"mcp-google-cse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"{{Path to the cloned repo\",\n        \"run\",\n        \"mcp-google-cse\"\n      ],\n      \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n      }\n    }\n```\n\n### Example result\ngoogle_search(\"What is MCP after:2024-11-01\")\nResult:\n```json\n[\n    {\n        \"title\": \"Can someone explain MCP to me? How are you using it? And what ...\",\n        \"link\": \"https://www.reddit.com/r/ClaudeAI/comments/1h55zxd/can_someone_explain_mcp_to_me_how_are_you_using/\",\n        \"snippet\": \"Dec 2, 2024 ... Comments Section ... MCP essentially allows you to give Claude access to various external systems. This can be files on your computer, an API, a browser, a ...\"\n    },\n    {\n        \"title\": \"Introducing the Model Context Protocol \\\\ Anthropic\",\n        \"link\": \"https://www.anthropic.com/news/model-context-protocol\",\n        \"snippet\": \"Nov 25, 2024 ... The Model Context Protocol (MCP) is an open standard for connecting AI assistants to the systems where data lives, including content repositories, ...\"\n    },\n    {\n        \"title\": \"3.5 Sonnet + MCP + Aider = Complete Game Changer : r ...\",\n        \"link\": \"https://www.reddit.com/r/ChatGPTCoding/comments/1hwn6qd/35_sonnet_mcp_aider_complete_game_changer/\",\n        \"snippet\": \"Jan 8, 2025 ... Really cool stuff. For those out of the loop here are some MCP servers. You can give your Claude chat (in the desktop version, or in a tool like Cline) ...\"\n    },\n    {\n        \"title\": \"Announcing Spring AI MCP: A Java SDK for the Model Context ...\",\n        \"link\": \"https://spring.io/blog/2024/12/11/spring-ai-mcp-announcement\",\n        \"snippet\": \"Dec 11, 2024 ... This SDK will enable Java developers to easily connect with an expanding array of AI models and tools while maintaining consistent, reliable integration ...\"\n    },\n    {\n        \"title\": \"Implementing a MCP server in Quarkus - Quarkus\",\n        \"link\": \"https://quarkus.io/blog/mcp-server/\",\n        \"snippet\": \"6 days ago ... The Model Context Protocol (MCP) is an emerging standard that enables AI models to safely interact with external tools and resources. In this tutorial, I'll ...\"\n    },\n    {\n        \"title\": \"mark3labs/mcp-go: A Go implementation of the Model ... - GitHub\",\n        \"link\": \"https://github.com/mark3labs/mcp-go\",\n        \"snippet\": \"Dec 18, 2024 ... A Go implementation of the Model Context Protocol (MCP), enabling seamless integration between LLM applications and external data sources and tools.\"\n    },\n    {\n        \"title\": \"MCP enables Claude to Build, Run and Test Web Apps by Looking ...\",\n        \"link\": \"https://wonderwhy-er.medium.com/mcp-enable-claude-to-build-run-and-test-web-apps-using-screenshots-3ae06aea6c4a\",\n        \"snippet\": \"Dec 18, 2024 ... How to Replicate My Experiment on Your Machine. If you're ready to dive into setting up MCP for Claude, follow these steps: ... 2. Download the Project: ... 3.\"\n    },\n    {\n        \"title\": \"MCP definition and meaning | Collins English Dictionary\",\n        \"link\": \"https://www.collinsdictionary.com/dictionary/english/mcp\",\n        \"snippet\": \"2 days ago ... 2 meanings: male chauvinist pig → informal, derogatory a man who exhibits male chauvinism Abbreviation: MCP.... Click for more definitions.\"\n    },\n    {\n        \"title\": \"What is Anthropic's New MCP Standard and How Can It Improve ...\",\n        \"link\": \"https://dappier.medium.com/what-is-anthropics-new-mcp-standard-and-how-can-it-improve-your-ai-agent-be6f6c72eb6a\",\n        \"snippet\": \"Nov 26, 2024 ... Anthropic has released a new protocol, MCP, for connecting AI agents to data sets. This blog explores when and why developers might use MCP to improve their ...\"\n    },\n    {\n        \"title\": \"Mostafa Gharib on LinkedIn: What is MCP and how it works\",\n        \"link\": \"https://www.linkedin.com/posts/mostafa-gharib_what-is-mcp-and-how-it-works-activity-7274301560594026497-p_yq\",\n        \"snippet\": \"Dec 15, 2024 ... ... MCP Host can use. (Bonus: SDKs in Python and TypeScript make it easy to build these servers!) 2️⃣ MCP Clients These interact with MCP Servers via the protocol.\"\n    }\n]\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "cse",
        "searching",
        "google cse",
        "web search",
        "search engine"
      ],
      "category": "web-search"
    },
    "Rickyyy1116--mcp-youtube-sheets": {
      "owner": "Rickyyy1116",
      "name": "mcp-youtube-sheets",
      "url": "https://github.com/Rickyyy1116/mcp-youtube-sheets",
      "imageUrl": "/freedevtools/mcp/pfp/Rickyyy1116.webp",
      "description": "Search YouTube videos and save results to Google Sheets with structured data including video title, URL, channel name, and publish date.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-07T05:32:35Z",
      "readme_content": "# YouTube to Google Sheets MCP Server\n\nThis MCP server provides functionality to search YouTube videos and automatically save the results to Google Sheets. It's designed to work with Claude and other AI assistants that support the Model Context Protocol.\n\n[English](README.md) | [日本語](README.ja.md)\n\n## Features\n\n- Search YouTube videos using the YouTube Data API v3\n- Save search results to Google Sheets automatically\n- Configurable search parameters (query, max results)\n- Results include video title, URL, channel name, and publish date\n\n## Installation\n\n```bash\nnpm install @rikukawa/youtube-sheets-server\n```\n\n## Prerequisites\n\n1. YouTube Data API v3 Setup:\n   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n   - Create a new project\n   - Enable YouTube Data API v3\n   - Create an API key\n\n2. Google Sheets API Setup:\n   - In the same project, enable Google Sheets API\n   - Create a service account\n   - Download the service account key (JSON format)\n   - Share your target Google Sheet with the service account email\n\n## Configuration\n\nAdd the server to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-sheets\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/youtube-sheets-server/build/index.js\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your-youtube-api-key\",\n        \"SPREADSHEET_ID\": \"your-spreadsheet-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n## Usage\n\n“Ask the AI assistant to ‘search for YouTube videos with “ChatGPT usage” and retrieve 10 videos’ and try using it in that way.”\n\n## Output Format\n\nThe tool will save the following information to your Google Sheet:\n- Video Title\n- Video URL\n- Channel Name\n- Publish Date\n\n## License\n\nMIT\n\n## Author\n\nRiku Kawashima\n\n## Repository\n\n[GitHub Repository](https://github.com/Rickyyy1116/mcp-youtube-sheets)\n\n## NPM Package\n\n[@rikukawa/youtube-sheets-server](https://www.npmjs.com/package/@rikukawa/youtube-sheets-server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rickyyy1116",
        "youtube",
        "videos",
        "mcp youtube",
        "search rickyyy1116",
        "rickyyy1116 mcp"
      ],
      "category": "web-search"
    },
    "RmMargt--searchAPI-mcp": {
      "owner": "RmMargt",
      "name": "searchAPI-mcp",
      "url": "https://github.com/RmMargt/searchAPI-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RmMargt.webp",
      "description": "Provides standardized access to various search services such as Google Maps, Google Flights, and Google Hotels, enabling efficient retrieval of information through a unified interface. It encapsulates search operations, functioning as a bridge between AI assistants and these services.",
      "stars": 59,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T21:44:42Z",
      "readme_content": "# SearchAPI MCP Server\n\n一个基于 Model Context Protocol (MCP) 的搜索 API 服务器，提供了对 Google Maps、Google Flights、Google Hotels 等服务的标准化访问接口。该服务器使 AI 助手能够通过统一的接口访问各种搜索服务。\n\nA Model Context Protocol (MCP) based search API server that provides standardized access to Google Maps, Google Flights, Google Hotels and other services. This server enables AI assistants to access various search services through a unified interface.\n\n## 概述 | Overview\n\nSearchAPI-MCP-Server 实现了 Model Context Protocol，将各种搜索操作封装为工具和资源。它作为 AI 助手和搜索服务之间的桥梁，支持地图搜索、航班查询、酒店预订等多种功能。\n\nSearchAPI-MCP-Server implements the Model Context Protocol, encapsulating various search operations as tools and resources. It serves as a bridge between AI assistants and search services, supporting map search, flight queries, hotel bookings, and more.\n\n## 功能特性 | Features\n\n### Google 搜索 | Google Search\n* 网页搜索结果 | Web search results\n* 知识图谱集成 | Knowledge graph integration\n* 相关问题推荐 | Related questions\n* 搜索建议 | Search suggestions\n* 多语言支持 | Multi-language support\n* 地区特定结果 | Region-specific results\n* 时间范围过滤 | Time range filtering\n* 安全搜索选项 | Safe search options\n\n### Google Video 搜索 | Google Video Search\n* 视频内容搜索 | Video content search\n* 视频列表获取 | Video list retrieval\n* 视频轮播支持 | Video carousel support\n* 短视频内容 | Short video content\n* 按时长筛选 | Duration filtering\n* 按来源过滤 | Source filtering\n* 按上传时间排序 | Upload time sorting\n* 高清预览支持 | HD preview support\n\n### Google Maps 搜索 | Google Maps Search\n* 搜索地点和服务 | Search places and services\n* 获取地点详细信息 | Get place details\n* 查看用户评论 | View user reviews\n* 获取位置坐标 | Get location coordinates\n\n### Google Flights 航班搜索 | Google Flights Search\n* 单程/往返航班搜索 | One-way/round-trip flight search\n* 多城市行程规划 | Multi-city itinerary planning\n* 航班价格日历 | Flight price calendar\n* 航班筛选和排序 | Flight filtering and sorting\n* 行李额度查询 | Baggage allowance query\n* 航空公司选择 | Airline selection\n\n### Google Hotels 酒店搜索 | Google Hotels Search\n* 酒店位置搜索 | Hotel location search\n* 价格和可用性查询 | Price and availability query\n* 设施和服务筛选 | Facilities and services filtering\n* 用户评分和评论 | User ratings and reviews\n* 特殊优惠查询 | Special offers query\n\n* 房型选择 | Room type selection\n\n### SearchAPI 新增功能 | Additional SearchAPI Features\n* SearchAPI Dashboard 与账号信息 | Dashboard and account management\n* 搜索历史记录查看 | View search history\n* 更多搜索引擎支持，如 Google Ads Transparency、Google Shopping、Google Images、Google News、Bing、Baidu、Naver、Yahoo、Amazon、Shein、eBay、Google Play Store、Apple App Store、DuckDuckGo、YouTube\n* 专用接口：Google Travel Explore、Google Hotels Autocomplete、Google Flights Location Search、Google Maps Photos、Google Maps Reviews、Google Maps Place\n\n## 安装说明 | Installation\n\n### 环境要求 | Requirements\n* Python 3.7 或更高版本 | Python 3.7 or higher\n* pip 包管理器 | pip package manager\n\n### 基础安装 | Basic Installation\n\n```bash\n# 克隆仓库 | Clone repository\ngit clone https://github.com/RmMargt/searchAPI-mcp.git\ncd searchAPI-mcp\n\n# 创建并激活虚拟环境 | Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或 | or\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖 | Install dependencies\npip install -r requirements.txt\n```\n\n## MCP 配置 | MCP Configuration\n\n### Claude for Desktop 配置示例 | Claude for Desktop Configuration Example\n\n在 Claude for Desktop 的配置文件中添加以下内容：\nAdd the following to your Claude for Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/path/to/searchAPI-mcp/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SEARCHAPI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n配置文件位置 | Configuration file location:\n* macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n* Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n## 许可证 | License\n\n本项目采用 MIT 许可证 - 详见 LICENSE 文件\nThis project is licensed under the MIT License - see the LICENSE file for details\n\n## 致谢 | Acknowledgments\n\n* Model Context Protocol - 协议规范 | Protocol specification\n* FastMCP - Python MCP 实现 | Python MCP implementation\n* SearchAPI.io - 搜索服务提供商 | Search service provider\n\n---\n\n_注意：本服务器会与外部 API 进行交互。在使用 MCP 客户端确认操作之前，请始终验证请求的操作是否合适。_\n_Note: This server interacts with external APIs. Always verify that requested operations are appropriate before confirming them in MCP clients._ ",
      "npm_url": "https://www.npmjs.com/package/searchapi-mcp",
      "npm_downloads": 81,
      "keywords": [
        "searchapi",
        "search",
        "retrieval",
        "searchapi mcp",
        "rmmargt searchapi",
        "search services"
      ],
      "category": "web-search"
    },
    "RmMargt--searchapi-mcp-agent": {
      "owner": "RmMargt",
      "name": "searchapi-mcp-agent",
      "url": "https://github.com/RmMargt/searchapi-mcp-agent",
      "imageUrl": "/freedevtools/mcp/pfp/RmMargt.webp",
      "description": "Integrates multiple search APIs for maps, flights, hotels, and videos through a standardized interface, enabling complex search operations via natural language queries. Offers multilingual support and customizable filtering options for enhanced search results.",
      "stars": 11,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:13:09Z",
      "readme_content": "# SearchAPI MCP Agent with A2A 支持 | SearchAPI MCP Agent with A2A Support\n\n一个基于 Agent-to-Agent (A2A) 协议的 SearchAPI 代理，通过 Model Context Protocol (MCP) 系统集成了多种搜索 API 工具。\n\nAn Agent-to-Agent (A2A) protocol based SearchAPI agent that integrates various search API tools through the Model Context Protocol (MCP) system.\n\n## 更新说明 | Update Notes\n\n**2024 更新**: \n- 修复了导入路径问题：从 samples.python.agents 导入修改为直接从当前目录导入\n- 修复了 a2a_common 导入问题：修改为从 common 模块导入\n- 移除了 a2a_common 依赖安装需求\n\n**2024 Updates**:\n- Fixed import path issues: Changed from samples.python.agents imports to direct imports from the current directory\n- Fixed a2a_common import issues: Changed to import from the common module\n- Removed a2a_common dependency installation requirements\n\n## 概述 | Overview\n\nSearchAPI-MCP-Agent 实现了 A2A 协议和 Model Context Protocol，将各种搜索操作封装为工具和资源。它作为 AI 助手和搜索服务之间的桥梁，支持地图搜索、航班查询、酒店预订等多种功能。\n\nSearchAPI-MCP-Agent implements the A2A protocol and Model Context Protocol, encapsulating various search operations as tools and resources. It serves as a bridge between AI assistants and search services, supporting map search, flight queries, hotel bookings, and more.\n\n## SearchAPI Agent 核心特性 | Core Features\n\n- **多MCP配置支持** - 作为MCP客户端，可以同时连接和配置多个MCP服务器，扩展可用的工具集\n  **Multiple MCP Configuration** - As an MCP client, can connect to and configure multiple MCP servers simultaneously, expanding the available toolset\n- **动态工具发现** - 自动发现和加载MCP服务器提供的工具列表，无需手动配置\n  **Dynamic Tool Discovery** - Automatically discovers and loads tool lists provided by MCP servers without manual configuration\n- **智能LLM路由** - 使用Gemini模型自动将自然语言查询路由到合适的工具并提取参数，确保调用成功\n  **Intelligent LLM Routing** - Uses Gemini model to automatically route natural language queries to appropriate tools and extract parameters, ensuring successful invocation\n- **实时状态反馈** - 通过A2A协议向Host Agent提供实时的工具执行状态更新和流式响应\n  **Real-time Status Feedback** - Provides real-time tool execution status updates and streaming responses to the Host Agent via A2A protocol\n- **错误处理和恢复** - 自动处理API调用错误，提供友好的错误信息和回退机制\n  **Error Handling and Recovery** - Automatically handles API call errors, providing friendly error messages and fallback mechanisms\n\n### Google 搜索 | Google Search\n* 网页搜索结果 | Web search results\n* 知识图谱集成 | Knowledge graph integration\n* 相关问题推荐 | Related questions\n* 搜索建议 | Search suggestions\n* 多语言支持 | Multi-language support\n* 地区特定结果 | Region-specific results\n* 时间范围过滤 | Time range filtering\n* 安全搜索选项 | Safe search options\n\n### Google Video 搜索 | Google Video Search\n* 视频内容搜索 | Video content search\n* 视频列表获取 | Video list retrieval\n* 视频轮播支持 | Video carousel support\n* 短视频内容 | Short video content\n* 按时长筛选 | Duration filtering\n* 按来源过滤 | Source filtering\n* 按上传时间排序 | Upload time sorting\n* 高清预览支持 | HD preview support\n\n### Google Maps 搜索 | Google Maps Search\n* 搜索地点和服务 | Search places and services\n* 获取地点详细信息 | Get place details\n* 查看用户评论 | View user reviews\n* 获取位置坐标 | Get location coordinates\n\n### Google Flights 航班搜索 | Google Flights Search\n* 单程/往返航班搜索 | One-way/round-trip flight search\n* 多城市行程规划 | Multi-city itinerary planning\n* 航班价格日历 | Flight price calendar\n* 航班筛选和排序 | Flight filtering and sorting\n* 行李额度查询 | Baggage allowance query\n* 航空公司选择 | Airline selection\n\n### Google Hotels 酒店搜索 | Google Hotels Search\n* 酒店位置搜索 | Hotel location search\n* 价格和可用性查询 | Price and availability query\n* 设施和服务筛选 | Facilities and services filtering\n* 用户评分和评论 | User ratings and reviews\n* 特殊优惠查询 | Special offers query\n* 房型选择 | Room type selection\n\n## 安装说明 | Installation\n\n### 环境要求 | Requirements\n* Python 3.9 或更高版本 | Python 3.9 or higher\n* pip 包管理器 | pip package manager\n* UV 包管理器（推荐）| UV package manager (recommended)\n\n### 基础安装 | Basic Installation\n\n```bash\n# 克隆仓库 | Clone repository\ngit clone https://github.com/RmMargt/searchapi-mcp-agent.git\ncd searchapi-mcp-agent\n\n# 创建并激活虚拟环境 | Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或 | or\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖 | Install dependencies\npip install -r requirements.txt\n```\n\n### 配置环境变量 | Configure Environment Variables\n\n创建 `.env` 文件并设置以下环境变量：\nCreate a `.env` file and set the following environment variables:\n\n```\nSEARCHAPI_API_KEY=your_searchapi_key_here\nGOOGLE_API_KEY=your_google_api_key_here\n```\n\n## 使用方法 | Usage\n\n### 启动 Google A2A 项目的 Host Agent 和 SearchAPI Agent\n\n按照以下步骤启动完整的 A2A 环境，包括 Host Agent 和 SearchAPI Agent:\n\n#### 1. 启动 SearchAPI Agent\n```bash\n# 在searchapi-mcp-agent目录下\npython -m searchapi_mcp_agent --host localhost --port 10001\n```\n\n#### 2. 启动 Host Agent (基于 Google A2A 项目)\n```bash\n# 切换到 Google A2A 样例目录\ncd path/to/A2A/samples/python\n\n# 运行 Host Agent (选择一种)\nuv run hosts/cli        # 命令行界面\n# 或\nuv run hosts/multiagent # 多代理环境\n```\n\n#### 3. 在本地浏览器中访问 Demo UI\n如果你运行的是多代理环境，可以在浏览器中访问以下地址:\n```\nhttp://localhost:12000\n```\n\n在 UI 中，点击机器人图标添加 SearchAPI Agent，使用以下地址:\n```\nhttp://localhost:10001/agent-card\n```\n\n### 直接发送请求 | Send Requests\n可以通过以下方式发送请求：\nYou can send requests in the following ways:\n\n1. **自然语言查询 | Natural Language Query**：\n   ```json\n   {\n     \"query\": \"查找从纽约到洛杉矶的航班\"\n   }\n   ```\n   Agent会使用LLM自动将查询路由到合适的工具。\n   The agent will use LLM to automatically route the query to the appropriate tool.\n\n2. **直接指定工具 | Direct Tool Specification**：\n   ```json\n   {\n     \"tool_name\": \"search_google_flights\",\n     \"parameters\": {\n       \"departure_id\": \"NYC\",\n       \"arrival_id\": \"LAX\",\n       \"outbound_date\": \"2024-12-01\"\n     }\n   }\n   ```\n\n## A2A 集成 | A2A Integration\n\n本项目已完全实现 A2A 协议，可以作为 AI 助手的服务端点。API 符合 A2A 规范，支持任务创建、状态查询和流式响应。\n\nThis project fully implements the A2A protocol and can serve as a service endpoint for AI assistants. The API complies with the A2A specification, supporting task creation, status queries, and streaming responses.\n\n### A2A 协议特性实现 | A2A Protocol Implementation\n\n- **动态工具路由** - 通过自然语言处理自动识别用户意图并选择合适的搜索工具\n  **Dynamic Tool Routing** - Automatically identifies user intent through natural language processing and selects the appropriate search tool\n- **流式响应** - 支持大型搜索结果的分块流式传输，提供实时反馈\n  **Streaming Responses** - Supports chunked streaming of large search results, providing real-time feedback\n- **任务状态更新** - 实时报告搜索任务的进度和状态变化\n  **Task Status Updates** - Reports progress and status changes of search tasks in real-time\n- **错误处理** - 优雅处理搜索API错误，提供有用的错误消息\n  **Error Handling** - Gracefully handles search API errors, providing useful error messages\n\n## MCP 配置 | MCP Configuration\n\n### Claude for Desktop 配置示例 | Claude for Desktop Configuration Example\n\n在 Claude for Desktop 的配置文件中添加以下内容：\nAdd the following to your Claude for Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/path/to/searchapi-mcp-agent/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SEARCHAPI_API_KEY\": \"your_api_key_here\",\n        \"GOOGLE_API_KEY\": \"your_google_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n配置文件位置 | Configuration file location:\n* macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n* Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n## 许可证 | License\n\n本项目采用 MIT 许可证 - 详见 LICENSE 文件\nThis project is licensed under the MIT License - see the LICENSE file for details\n\n## 致谢 | Acknowledgments\n\n* Model Context Protocol - 协议规范 | Protocol specification\n* A2A Protocol - Agent-to-Agent 协议规范 | Agent-to-Agent protocol specification \n* FastMCP - Python MCP 实现 | Python MCP implementation\n* SearchAPI.io - 搜索服务提供商 | Search service provider\n* Google A2A - Agent-to-Agent 协议参考实现 | A2A protocol reference implementation\n\n---\n\n_注意：本服务器会与外部 API 进行交互。在使用 MCP 客户端确认操作之前，请始终验证请求的操作是否合适。_\n_Note: This server interacts with external APIs. Always verify that requested operations are appropriate before confirming them in MCP clients._ \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searchapi",
        "search",
        "filtering",
        "searchapi mcp",
        "rmmargt searchapi",
        "search apis"
      ],
      "category": "web-search"
    },
    "RossH121--perplexity-mcp": {
      "owner": "RossH121",
      "name": "perplexity-mcp",
      "url": "https://github.com/RossH121/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RossH121.webp",
      "description": "Utilizes web search capabilities to efficiently retrieve information while automatically selecting the most suitable AI model based on the user’s query intent. Offers intelligent options for domain and recency filtering to enhance search results.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-07T19:26:52Z",
      "readme_content": "# Perplexity MCP Server\n\nAn MCP server that provides web search capabilities using Perplexity's API with automatic model selection based on query intent.\n\n<a href=\"https://glama.ai/mcp/servers/6qmvjay9z5\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/6qmvjay9z5/badge\" alt=\"Perplexity Server MCP server\" />\n</a>\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- A Perplexity API key (get one at <https://www.perplexity.ai/settings/api>)\n- Claude Desktop App\n\n## Installation\n\n### Installing via Git\n\n1. Clone this repository:\n\n    ```bash\n    git clone https://github.com/RossH121/perplexity-mcp.git\n    cd perplexity-mcp\n    ```\n\n2. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n3. Build the server:\n\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\n1. Get your Perplexity API key from <https://www.perplexity.ai/settings/api>\n\n2. Add the server to Claude's config file at `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/perplexity-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\",\n        \"PERPLEXITY_MODEL\": \"sonar\"\n      }\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to` with the actual path to where you cloned the repository.\n\n### Available Models\n\nThe server now supports automatic model selection based on query intent, but you can also specify a default model using the `PERPLEXITY_MODEL` environment variable. Available options:\n\n- `sonar-deep-research` - Specialized for extensive research and expert-level analysis across domains\n- `sonar-reasoning-pro` - Optimized for advanced logical reasoning and complex problem-solving\n- `sonar-reasoning` - Designed for reasoning tasks with balanced performance\n- `sonar-pro` - General-purpose model with excellent search capabilities and citation density\n- `sonar` - Fast and efficient for straightforward queries\n\nThe default model (specified in the environment variable) will be used as the baseline for automatic model selection.\n\nFor up-to-date model pricing and availability, visit: <https://docs.perplexity.ai/guides/pricing>\n\n## Usage\n\nAfter configuring the server and restarting Claude, you can simply ask Claude to search for information. For example:\n\n- \"What's the latest news about SpaceX?\"\n- \"Search for the best restaurants in Chicago\"\n- \"Find information about the history of jazz music\"\n- \"I need a deep research analysis of recent AI developments\" (uses sonar-deep-research)\n- \"Help me reason through this complex problem\" (uses sonar-reasoning-pro)\n\nClaude will automatically use the Perplexity search tool to find and return relevant information. The server will automatically select the most appropriate model based on your query's intent.\n\nIf for whatever reason it decides not to use the search tool, you can force the issue by prepending your prompt with \"Search the web\".\n\n### Intelligent Model Selection\n\nThe server automatically selects the most appropriate Perplexity model based on your query:\n\n- Use research-oriented terms like \"deep research,\" \"comprehensive,\" or \"in-depth\" to trigger sonar-deep-research\n- Use reasoning terms like \"solve,\" \"figure out,\" or \"complex problem\" to trigger sonar-reasoning-pro\n- Use simple terms like \"quick,\" \"brief,\" or \"basic\" to trigger the lightweight sonar model\n- General search terms default to sonar-pro for balanced performance\n\nEach search response includes information about which model was used and why.\n\n### Domain Filtering\n\nThis server supports domain filtering to customize your search experience. You can allow or block specific domains using these commands:\n\n- **Add an allowed domain**: \"Use the domain_filter tool to allow wikipedia.org\"\n- **Add a blocked domain**: \"Use the domain_filter tool to block pinterest.com\"\n- **View current filters**: \"Use the list_filters tool\" (shows domain and recency filters)\n- **Clear all filters**: \"Use the clear_filters tool\" (clears both domain and recency filters)\n\n**Note**: Perplexity API supports up to 3 domains total with priority given to allowed domains. Domain filtering requires a Perplexity API tier that supports this feature.\n\nExample usage flow:\n1. \"Use the domain_filter tool to allow wikipedia.org\"\n2. \"Use the domain_filter tool to allow arxiv.org\"\n3. \"Use the list_filters tool\" (to verify your settings)\n4. \"Search for quantum computing advances\" (results will prioritize wikipedia.org and arxiv.org)\n\n### Recency Filtering\n\nYou can limit search results to a specific time window using the recency filter:\n\n- **Set recency filter**: \"Use the recency_filter tool with filter=hour\" (options: hour, day, week, month)\n- **Disable recency filter**: \"Use the recency_filter tool with filter=none\"\n\nThis is particularly useful for time-sensitive queries like current events or breaking news.\n\n### Model Selection Control\n\nWhile the automatic model selection works well for most cases, you can manually control which model is used:\n\n- **View model information**: \"Use the model_info tool\"\n- **Set a specific model**: \"Use the model_info tool with model=sonar-deep-research\"\n- **Return to automatic selection**: Set the model back to the default model\n\nExample usage:\n1. \"Use the model_info tool\" (to see available models and current status)\n2. \"Use the model_info tool with model=sonar-reasoning-pro\" (to force using reasoning model)\n3. \"Search for a mathematical proof of the Pythagorean theorem\" (will use sonar-reasoning-pro)\n4. \"Use the model_info tool with model=sonar-pro\" (to return to automatic selection)\n\n## Development\n\nTo modify the server:\n\n1. Edit `src/index.ts`\n2. Rebuild with `npm run build`\n3. Restart Claude to load the changes\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "filtering",
        "mcp",
        "web search",
        "search capabilities",
        "search results"
      ],
      "category": "web-search"
    },
    "Rudra-ravi--wikipedia-mcp": {
      "owner": "Rudra-ravi",
      "name": "wikipedia-mcp",
      "url": "https://github.com/Rudra-ravi/wikipedia-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Rudra-ravi.webp",
      "description": "Provides real-time access to Wikipedia content, enabling retrieval of articles, summaries, sections, and links in multiple languages for AI applications. Facilitates grounded responses by integrating reliable Wikipedia data into Large Language Models.",
      "stars": 114,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T07:52:30Z",
      "readme_content": "# Wikipedia MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Rudra-ravi/wikipedia-mcp)](https://smithery.ai/server/@Rudra-ravi/wikipedia-mcp)\n\nA Model Context Protocol (MCP) server that retrieves information from Wikipedia to provide context to Large Language Models (LLMs). This tool helps AI assistants access factual information from Wikipedia to ground their responses in reliable sources.\n\n<a href=\"https://glama.ai/mcp/servers/@Rudra-ravi/wikipedia-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Rudra-ravi/wikipedia-mcp/badge\" alt=\"Wikipedia Server MCP server\" />\n</a>\n\n![image](https://github.com/user-attachments/assets/e41382f7-111a-4105-97f3-7851c906843e)\n\n## Overview\n\nThe Wikipedia MCP server provides real-time access to Wikipedia information through a standardized Model Context Protocol interface. This allows LLMs to retrieve accurate and up-to-date information directly from Wikipedia to enhance their responses.\n\n## Verified By\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/rudra-ravi-wikipedia-mcp-badge.png)](https://mseep.ai/app/rudra-ravi-wikipedia-mcp)\n\n## Features\n\n- **Search Wikipedia**: Find articles matching specific queries with enhanced diagnostics\n- **Retrieve Article Content**: Get full article text with all information\n- **Article Summaries**: Get concise summaries of articles\n- **Section Extraction**: Retrieve specific sections from articles\n- **Link Discovery**: Find links within articles to related topics\n- **Related Topics**: Discover topics related to a specific article\n- **Multi-language Support**: Access Wikipedia in different languages by specifying the `--language` or `-l` argument when running the server (e.g., `wikipedia-mcp --language ta` for Tamil).\n- **Country/Locale Support**: Use intuitive country codes like `--country US`, `--country China`, or `--country TW` instead of language codes. Automatically maps to appropriate Wikipedia language variants.\n- **Language Variant Support**: Support for language variants such as Chinese traditional/simplified (e.g., `zh-hans` for Simplified Chinese, `zh-tw` for Traditional Chinese), Serbian scripts (`sr-latn`, `sr-cyrl`), and other regional variants.\n- **Optional caching**: Cache API responses for improved performance using --enable-cache\n- **Google ADK Compatibility**: Fully compatible with Google ADK agents and other AI frameworks that use strict function calling schemas\n\n## Installation\n\n### Using pipx (Recommended for Claude Desktop)\n\nThe best way to install for Claude Desktop usage is with pipx, which installs the command globally:\n\n```bash\n# Install pipx if you don't have it\npip install pipx\npipx ensurepath\n\n# Install the Wikipedia MCP server\npipx install wikipedia-mcp\n```\n\nThis ensures the `wikipedia-mcp` command is available in Claude Desktop's PATH.\n\n### Installing via Smithery\n\nTo install wikipedia-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Rudra-ravi/wikipedia-mcp):\n\n```bash\nnpx -y @smithery/cli install @Rudra-ravi/wikipedia-mcp --client claude\n```\n\n### From PyPI (Alternative)\n\nYou can also install directly from PyPI:\n\n```bash\npip install wikipedia-mcp\n```\n\n**Note**: If you use this method and encounter connection issues with Claude Desktop, you may need to use the full path to the command in your configuration. See the [Configuration](#configuration-for-claude-desktop) section for details.\n\n### Using a virtual environment\n\n```bash\n# Create a virtual environment\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Install the package\npip install git+https://github.com/rudra-ravi/wikipedia-mcp.git\n```\n\n### From source\n\n```bash\n# Clone the repository\ngit clone https://github.com/rudra-ravi/wikipedia-mcp.git\ncd wikipedia-mcp\n\n# Create a virtual environment\npython3 -m venv wikipedia-mcp-env\nsource wikipedia-mcp-env/bin/activate\n\n# Install in development mode\npip install -e .\n```\n\n## Usage\n\n### Running the server\n\n```bash\n# If installed with pipx\nwikipedia-mcp\n\n# If installed in a virtual environment\nsource venv/bin/activate\nwikipedia-mcp\n\n# Specify transport protocol (default: stdio)\nwikipedia-mcp --transport stdio  # For Claude Desktop\nwikipedia-mcp --transport sse    # For HTTP streaming\n\n# Specify language (default: en for English)\nwikipedia-mcp --language ja  # Example for Japanese\nwikipedia-mcp --language zh-hans  # Example for Simplified Chinese\nwikipedia-mcp --language zh-tw    # Example for Traditional Chinese (Taiwan)\nwikipedia-mcp --language sr-latn  # Example for Serbian Latin script\n\n# Specify country/locale (alternative to language codes)\nwikipedia-mcp --country US        # English (United States)\nwikipedia-mcp --country China     # Chinese Simplified\nwikipedia-mcp --country Taiwan    # Chinese Traditional (Taiwan)  \nwikipedia-mcp --country Japan     # Japanese\nwikipedia-mcp --country Germany   # German\nwikipedia-mcp --country france    # French (case insensitive)\n\n# List all supported countries\nwikipedia-mcp --list-countries\n\n# Optional: Specify host/port for SSE (use 0.0.0.0 for containers)\nwikipedia-mcp --transport sse --host 0.0.0.0 --port 8080\n\n# Optional: Enable caching\nwikipedia-mcp --enable-cache\n\n# Optional: Use Personal Access Token to avoid rate limiting (403 errors)\nwikipedia-mcp --access-token your_wikipedia_token_here\n\n# Or set via environment variable\nexport WIKIPEDIA_ACCESS_TOKEN=your_wikipedia_token_here\nwikipedia-mcp\n\n# Security note for SSE: The transport does not define built-in endpoint authentication.\n# To restrict access, run the server behind an authenticating reverse proxy (e.g., Nginx/Traefik),\n# or expose it only on a private network/VPN and use firewall rules.\n\n# Combine options\nwikipedia-mcp --country Taiwan --enable-cache --access-token your_token --transport sse --port 8080\n\n### Docker/Kubernetes\n\nWhen running inside containers, bind the SSE server to all interfaces and map\nthe container port to the host or service:\n\n```bash\n# Build and run with Docker\ndocker build -t wikipedia-mcp .\ndocker run --rm -p 8080:8080 wikipedia-mcp --transport sse --host 0.0.0.0 --port 8080\n```\n\nKubernetes example (minimal):\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wikipedia-mcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wikipedia-mcp\n  template:\n    metadata:\n      labels:\n        app: wikipedia-mcp\n    spec:\n      containers:\n        - name: server\n          image: your-repo/wikipedia-mcp:latest\n          args: [\"--transport\", \"sse\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: wikipedia-mcp\nspec:\n  selector:\n    app: wikipedia-mcp\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n```\n```\n\n### Configuration for Claude Desktop\n\nAdd the following to your Claude Desktop configuration file:\n\n**Option 1: Using command name (requires `wikipedia-mcp` to be in PATH)**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia\": {\n      \"command\": \"wikipedia-mcp\"\n    }\n  }\n}\n```\n\n**Option 2: Using full path (recommended if you get connection errors)**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia\": {\n      \"command\": \"/full/path/to/wikipedia-mcp\"\n    }\n  }\n}\n```\n\n**Option 3: With country/language specification**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia-us\": {\n      \"command\": \"wikipedia-mcp\",\n      \"args\": [\"--country\", \"US\"]\n    },\n    \"wikipedia-taiwan\": {\n      \"command\": \"wikipedia-mcp\", \n      \"args\": [\"--country\", \"TW\"]\n    },\n    \"wikipedia-japan\": {\n      \"command\": \"wikipedia-mcp\",\n      \"args\": [\"--country\", \"Japan\"]\n    }\n  }\n}\n```\n\nTo find the full path, run: `which wikipedia-mcp`\n\n**Configuration file locations:**\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n> **Note**: If you encounter connection errors, see the [Troubleshooting](#common-issues) section for solutions.\n\n## Available MCP Tools\n\nThe Wikipedia MCP server provides the following tools for LLMs to interact with Wikipedia:\n\n### `search_wikipedia`\n\nSearch Wikipedia for articles matching a query.\n\n**Parameters:**\n- `query` (string): The search term\n- `limit` (integer, optional): Maximum number of results to return (default: 10)\n\n**Returns:**\n- A list of search results with titles, snippets, and metadata\n\n### `get_article`\n\nGet the full content of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- Article content including text, summary, sections, links, and categories\n\n### `get_summary`\n\nGet a concise summary of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A text summary of the article\n\n### `get_sections`\n\nGet the sections of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A structured list of article sections with their content\n\n### `get_links`\n\nGet the links contained within a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A list of links to other Wikipedia articles\n\n### `get_coordinates`\n\nGet the coordinates of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A dictionary containing coordinate information including:\n  - `title`: The article title\n  - `pageid`: The page ID\n  - `coordinates`: List of coordinate objects with latitude, longitude, and metadata\n  - `exists`: Whether the article exists\n  - `error`: Any error message if retrieval failed\n\n### `get_related_topics`\n\nGet topics related to a Wikipedia article based on links and categories.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `limit` (integer, optional): Maximum number of related topics (default: 10)\n\n**Returns:**\n- A list of related topics with relevance information\n\n### `summarize_article_for_query`\n\nGet a summary of a Wikipedia article tailored to a specific query.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `query` (string): The query to focus the summary on\n- `max_length` (integer, optional): Maximum length of the summary (default: 250)\n\n**Returns:**\n- A dictionary containing the title, query, and the focused summary\n\n### `summarize_article_section`\n\nGet a summary of a specific section of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `section_title` (string): The title of the section to summarize\n- `max_length` (integer, optional): Maximum length of the summary (default: 150)\n\n**Returns:**\n- A dictionary containing the title, section title, and the section summary\n\n### `extract_key_facts`\n\nExtract key facts from a Wikipedia article, optionally focused on a specific topic within the article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `topic_within_article` (string, optional): A specific topic within the article to focus fact extraction\n- `count` (integer, optional): Number of key facts to extract (default: 5)\n\n**Returns:**\n- A dictionary containing the title, topic, and a list of extracted facts\n\n## Country/Locale Support\n\nThe Wikipedia MCP server supports intuitive country and region codes as an alternative to language codes. This makes it easier to access region-specific Wikipedia content without needing to know language codes.\n\n### Supported Countries and Regions\n\nUse `--list-countries` to see all supported countries:\n\n```bash\nwikipedia-mcp --list-countries\n```\n\nThis will display countries organized by language, for example:\n\n```\nSupported Country/Locale Codes:\n========================================\n    en: US, USA, United States, UK, GB, Canada, Australia, ...\n    zh-hans: CN, China\n    zh-tw: TW, Taiwan  \n    ja: JP, Japan\n    de: DE, Germany\n    fr: FR, France\n    es: ES, Spain, MX, Mexico, AR, Argentina, ...\n    pt: PT, Portugal, BR, Brazil\n    ru: RU, Russia\n    ar: SA, Saudi Arabia, AE, UAE, EG, Egypt, ...\n```\n\n### Usage Examples\n\n```bash\n# Major countries by code\nwikipedia-mcp --country US       # United States (English)\nwikipedia-mcp --country CN       # China (Simplified Chinese)\nwikipedia-mcp --country TW       # Taiwan (Traditional Chinese)\nwikipedia-mcp --country JP       # Japan (Japanese)\nwikipedia-mcp --country DE       # Germany (German)\nwikipedia-mcp --country FR       # France (French)\nwikipedia-mcp --country BR       # Brazil (Portuguese)\nwikipedia-mcp --country RU       # Russia (Russian)\n\n# Countries by full name (case insensitive)\nwikipedia-mcp --country \"United States\"\nwikipedia-mcp --country China\nwikipedia-mcp --country Taiwan  \nwikipedia-mcp --country Japan\nwikipedia-mcp --country Germany\nwikipedia-mcp --country france    # Case insensitive\n\n# Regional variants\nwikipedia-mcp --country HK       # Hong Kong (Traditional Chinese)\nwikipedia-mcp --country SG       # Singapore (Simplified Chinese)\nwikipedia-mcp --country \"Saudi Arabia\"  # Arabic\nwikipedia-mcp --country Mexico   # Spanish\n```\n\n### Country-to-Language Mapping\n\nThe server automatically maps country codes to appropriate Wikipedia language editions:\n\n- **English-speaking**: US, UK, Canada, Australia, New Zealand, Ireland, South Africa → `en`\n- **Chinese regions**: \n  - CN, China → `zh-hans` (Simplified Chinese)\n  - TW, Taiwan → `zh-tw` (Traditional Chinese - Taiwan)\n  - HK, Hong Kong → `zh-hk` (Traditional Chinese - Hong Kong)\n  - SG, Singapore → `zh-sg` (Simplified Chinese - Singapore)\n- **Major languages**: JP→`ja`, DE→`de`, FR→`fr`, ES→`es`, IT→`it`, RU→`ru`, etc.\n- **Regional variants**: Supports 140+ countries and regions\n\n### Error Handling\n\nIf you specify an unsupported country, you'll get a helpful error message:\n\n```bash\n$ wikipedia-mcp --country INVALID\nError: Unsupported country/locale: 'INVALID'. \nSupported country codes include: US, USA, UK, GB, CA, AU, NZ, IE, ZA, CN. \nUse --language parameter for direct language codes instead.\n\nUse --list-countries to see supported country codes.\n```\n\n## Language Variants\n\nThe Wikipedia MCP server supports language variants for languages that have multiple writing systems or regional variations. This feature is particularly useful for Chinese, Serbian, Kurdish, and other languages with multiple scripts or regional differences.\n\n### Supported Language Variants\n\n#### Chinese Language Variants\n- `zh-hans` - Simplified Chinese\n- `zh-hant` - Traditional Chinese  \n- `zh-tw` - Traditional Chinese (Taiwan)\n- `zh-hk` - Traditional Chinese (Hong Kong)\n- `zh-mo` - Traditional Chinese (Macau)\n- `zh-cn` - Simplified Chinese (China)\n- `zh-sg` - Simplified Chinese (Singapore)\n- `zh-my` - Simplified Chinese (Malaysia)\n\n#### Serbian Language Variants\n- `sr-latn` - Serbian Latin script\n- `sr-cyrl` - Serbian Cyrillic script\n\n#### Kurdish Language Variants\n- `ku-latn` - Kurdish Latin script\n- `ku-arab` - Kurdish Arabic script\n\n#### Norwegian Language Variants\n- `no` - Norwegian (automatically mapped to Bokmål)\n\n### Usage Examples\n\n```bash\n# Access Simplified Chinese Wikipedia\nwikipedia-mcp --language zh-hans\n\n# Access Traditional Chinese Wikipedia (Taiwan)\nwikipedia-mcp --language zh-tw\n\n# Access Serbian Wikipedia in Latin script\nwikipedia-mcp --language sr-latn\n\n# Access Serbian Wikipedia in Cyrillic script\nwikipedia-mcp --language sr-cyrl\n```\n\n### How Language Variants Work\n\nWhen you specify a language variant like `zh-hans`, the server:\n1. Maps the variant to the base Wikipedia language (e.g., `zh` for Chinese variants)\n2. Uses the base language for API connections to the Wikipedia servers\n3. Includes the variant parameter in API requests to get content in the specific variant\n4. Returns content formatted according to the specified variant's conventions\n\nThis approach ensures optimal compatibility with Wikipedia's API while providing access to variant-specific content and formatting.\n\n## Example Prompts\n\nOnce the server is running and configured with Claude Desktop, you can use prompts like:\n\n### General Wikipedia queries:\n- \"Tell me about quantum computing using the Wikipedia information.\"\n- \"Summarize the history of artificial intelligence based on Wikipedia.\"\n- \"What does Wikipedia say about climate change?\"\n- \"Find Wikipedia articles related to machine learning.\"\n- \"Get me the introduction section of the article on neural networks from Wikipedia.\"\n- \"What are the coordinates of the Eiffel Tower?\"\n- \"Find the latitude and longitude of Mount Everest from Wikipedia.\"\n- \"Get coordinate information for famous landmarks in Paris.\"\n\n### Using country-specific Wikipedia:\n- \"Search Wikipedia China for information about the Great Wall.\" (uses Chinese Wikipedia)\n- \"Tell me about Tokyo from Japanese Wikipedia sources.\"\n- \"What does German Wikipedia say about the Berlin Wall?\"\n- \"Find information about the Eiffel Tower from French Wikipedia.\"\n- \"Get Taiwan Wikipedia's article about Taiwanese cuisine.\"\n\n### Language variant examples:\n- \"Search Traditional Chinese Wikipedia for information about Taiwan.\"\n- \"Find Simplified Chinese articles about modern China.\"\n- \"Get information from Serbian Latin Wikipedia about Belgrade.\"\n\n## MCP Resources\n\nThe server also provides MCP resources (similar to HTTP endpoints but for MCP):\n\n- `search/{query}`: Search Wikipedia for articles matching the query\n- `article/{title}`: Get the full content of a Wikipedia article\n- `summary/{title}`: Get a summary of a Wikipedia article\n- `sections/{title}`: Get the sections of a Wikipedia article\n- `links/{title}`: Get the links in a Wikipedia article\n- `coordinates/{title}`: Get the coordinates of a Wikipedia article\n- `summary/{title}/query/{query}/length/{max_length}`: Get a query-focused summary of an article\n- `summary/{title}/section/{section_title}/length/{max_length}`: Get a summary of a specific article section\n- `facts/{title}/topic/{topic_within_article}/count/{count}`: Extract key facts from an article\n\n## Development\n\n### Local Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/rudra-ravi/wikipedia-mcp.git\ncd wikipedia-mcp\n\n# Create a virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install the package in development mode\npip install -e .\n\n# Install development and test dependencies\npip install -r requirements-dev.txt\n\n# Run the server\nwikipedia-mcp\n```\n\n### Project Structure\n\n- `wikipedia_mcp/`: Main package\n  - `__main__.py`: Entry point for the package\n  - `server.py`: MCP server implementation\n  - `wikipedia_client.py`: Wikipedia API client\n  - `api/`: API implementation\n  - `core/`: Core functionality\n  - `utils/`: Utility functions\n- `tests/`: Test suite\n  - `test_basic.py`: Basic package tests\n  - `test_cli.py`: Command-line interface tests\n  - `test_server_tools.py`: Comprehensive server and tool tests\n\n## Testing\n\nThe project includes a comprehensive test suite to ensure reliability and functionality.\n\n### Test Structure\n\nThe test suite is organized in the `tests/` directory with the following test files:\n\n- **`test_basic.py`**: Basic package functionality tests\n- **`test_cli.py`**: Command-line interface and transport tests\n- **`test_server_tools.py`**: Comprehensive tests for all MCP tools and Wikipedia client functionality\n\n### Running Tests\n\n#### Run All Tests\n```bash\n# Install test dependencies\npip install -r requirements-dev.txt\n\n# Run all tests\npython -m pytest tests/ -v\n\n# Run tests with coverage\npython -m pytest tests/ --cov=wikipedia_mcp --cov-report=html\n```\n\n#### Run Specific Test Categories\n```bash\n# Run only unit tests (excludes integration tests)\npython -m pytest tests/ -v -m \"not integration\"\n\n# Run only integration tests (requires internet connection)\npython -m pytest tests/ -v -m \"integration\"\n\n# Run specific test file\npython -m pytest tests/test_server_tools.py -v\n```\n\n### Test Categories\n\n#### Unit Tests\n- **WikipediaClient Tests**: Mock-based tests for all client methods\n  - Search functionality\n  - Article retrieval\n  - Summary extraction\n  - Section parsing\n  - Link extraction\n  - Related topics discovery\n- **Server Tests**: MCP server creation and tool registration\n- **CLI Tests**: Command-line interface functionality\n\n#### Integration Tests\n- **Real API Tests**: Tests that make actual calls to Wikipedia API\n- **End-to-End Tests**: Complete workflow testing\n\n### Test Configuration\n\nThe project uses `pytest.ini` for test configuration:\n\n```ini\n[pytest]\nmarkers =\n    integration: marks tests as integration tests (may require network access)\n    slow: marks tests as slow running\n\ntestpaths = tests\naddopts = -v --tb=short\n```\n\n### Continuous Integration\n\nAll tests are designed to:\n- Run reliably in CI/CD environments\n- Handle network failures gracefully\n- Provide clear error messages\n- Cover edge cases and error conditions\n\n### Adding New Tests\n\nWhen contributing new features:\n\n1. Add unit tests for new functionality\n2. Include both success and failure scenarios\n3. Mock external dependencies (Wikipedia API)\n4. Add integration tests for end-to-end validation\n5. Follow existing test patterns and naming conventions\n\n## Troubleshooting\n\n### Common Issues\n\n#### Claude Desktop Connection Issues\n\n**Problem**: Claude Desktop shows errors like `spawn wikipedia-mcp ENOENT` or cannot find the command.\n\n**Cause**: This occurs when the `wikipedia-mcp` command is installed in a user-specific location (like `~/.local/bin/`) that's not in Claude Desktop's PATH.\n\n**Solutions**:\n\n1. **Use full path to the command** (Recommended):\n   ```json\n   {\n     \"mcpServers\": {\n       \"wikipedia\": {\n         \"command\": \"/home/username/.local/bin/wikipedia-mcp\"\n       }\n     }\n   }\n   ```\n   \n   To find your exact path, run: `which wikipedia-mcp`\n\n2. **Install with pipx for global access**:\n   ```bash\n   pipx install wikipedia-mcp\n   ```\n   Then use the standard configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"wikipedia\": {\n         \"command\": \"wikipedia-mcp\"\n       }\n     }\n   }\n   ```\n\n3. **Create a symlink to a global location**:\n   ```bash\n   sudo ln -s ~/.local/bin/wikipedia-mcp /usr/local/bin/wikipedia-mcp\n   ```\n\n#### Other Issues\n\n- **Article Not Found**: Check the exact spelling of article titles\n- **Rate Limiting**: Wikipedia API has rate limits; consider adding delays between requests\n- **Large Articles**: Some Wikipedia articles are very large and may exceed token limits\n\n## Troubleshooting Search Issues\n\nIf you're experiencing empty search results, use the new diagnostic tools:\n\n### 1. Test Connectivity\n\nUse the `test_wikipedia_connectivity` tool to check if you can reach Wikipedia's API:\n\n```json\n{\n  \"tool\": \"test_wikipedia_connectivity\"\n}\n```\n\nThis returns diagnostics including:\n- Connection status (`success` or `failed`)\n- Response time in milliseconds\n- Site/host information when successful\n- Error details when connectivity fails\n\n### 2. Enhanced Search Error Information\n\nThe `search_wikipedia` tool now returns detailed metadata:\n\n```json\n{\n  \"tool\": \"search_wikipedia\",\n  \"arguments\": {\n    \"query\": \"Ada Lovelace\",\n    \"limit\": 10\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"query\": \"Ada Lovelace\",\n  \"results\": [...],\n  \"count\": 5,\n  \"status\": \"success\",\n  \"language\": \"en\"\n}\n```\n\nWhen no results are found, you receive:\n\n```json\n{\n  \"query\": \"nonexistent\",\n  \"results\": [],\n  \"status\": \"no_results\",\n  \"count\": 0,\n  \"language\": \"en\",\n  \"message\": \"No search results found. This could indicate connectivity issues, API errors, or simply no matching articles.\"\n}\n```\n\n### 3. Common Search Issues and Solutions\n\n- **Empty results**: Run the connectivity test, verify query spelling, try broader terms.\n- **Connection errors**: Check firewall or proxy settings, ensure `*.wikipedia.org` is reachable.\n- **API limits**: Requests with `limit > 500` are automatically capped; negative values reset to the default (10).\n\n### 4. Debugging with Verbose Logging\n\nLaunch the server with debug logging for deeper insight:\n\n```bash\nwikipedia-mcp --log-level DEBUG\n```\n\nThis emits the request parameters, response status codes, and any warnings returned by the API.\n\n## Understanding the Model Context Protocol (MCP)\n\nThe Model Context Protocol (MCP) is not a traditional HTTP API but a specialized protocol for communication between LLMs and external tools. Key characteristics:\n\n- Uses stdio (standard input/output) or SSE (Server-Sent Events) for communication\n- Designed specifically for AI model interaction\n- Provides standardized formats for tools, resources, and prompts\n- Integrates directly with Claude and other MCP-compatible AI systems\n\nClaude Desktop acts as the MCP client, while this server provides the tools and resources that Claude can use to access Wikipedia information.\n\n## Contributing\n\nContributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Connect with the Author\n\n- 🌐 Portfolio: [ravikumar-dev.me](https://ravikumar-dev.me)\n- 📝 Blog: [Medium](https://medium.com/@Ravikumar-e)\n- 💼 LinkedIn: [in/ravi-kumar-e](https://linkedin.com/in/ravi-kumar-e)\n- 🐦 Twitter: [@Ravikumar_d3v](https://twitter.com/Ravikumar_d3v) ",
      "npm_url": "https://www.npmjs.com/package/wikipedia-mcp",
      "npm_downloads": 4363,
      "keywords": [
        "wikipedia",
        "retrieval",
        "search",
        "retrieval articles",
        "wikipedia data",
        "wikipedia content"
      ],
      "category": "web-search"
    },
    "Sacode--searxng-simple-mcp": {
      "owner": "Sacode",
      "name": "searxng-simple-mcp",
      "url": "https://github.com/Sacode/searxng-simple-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Sacode.webp",
      "description": "Provides web search capabilities using SearxNG, enabling AI models to efficiently search the web while maintaining user privacy. Integrates seamlessly with LLMs to deliver accurate search results without tracking.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T19:39:17Z",
      "readme_content": "# SearxNG MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities using SearxNG, allowing AI assistants like Claude to search the web.\n\n> *Created by AI with human supervision - because sometimes even artificial intelligence needs someone to tell it when to take a coffee break! 🤖☕*\n\n## Overview\n\nThis project implements an MCP server that connects to SearxNG, a privacy-respecting metasearch engine. The server provides a simple and efficient way for Large Language Models to search the web without tracking users.\n\nThe server is specifically designed for LLMs and includes only essential features to minimize context window usage. This streamlined approach ensures efficient communication between LLMs and the search engine, preserving valuable context space for more important information.\n\n### Features\n\n- Privacy-focused web search through SearxNG\n- Simple API for LLM integration\n- Compatible with Claude Desktop and other MCP-compliant clients\n- Configurable search parameters\n- Clean, formatted search results optimized for LLMs\n\n## Integration with MCP-Compatible Applications\n\n### Integration Examples\n\n#### Using pipx run (Recommended, no installation required)\n\nCreate a `.clauderc` file in your home directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"pipx\",\n      \"args\": [\n        \"run\", \"searxng-simple-mcp@latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using uvx run (No installation required)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"run\", \"searxng-simple-mcp@latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using Python with pip (requires installation)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"searxng_simple_mcp.server\"],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using with Docker (No installation required)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\", \"--network=host\",\n        \"-e\", \"SEARXNG_MCP_SEARXNG_URL=http://localhost:8080\",\n        \"ghcr.io/sacode/searxng-simple-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n**Note:** When using Docker with MCP servers:\n\n1. Environment variables must be passed directly using the `-e` flag in the `args` array, as the `env` object is not properly passed to the Docker container.\n2. If you need to access a SearxNG instance running on localhost (e.g., <http://localhost:8080>), you must use the `--network=host` flag to allow the container to access the host's network. Otherwise, \"localhost\" inside the container will refer to the container itself, not your host machine.\n3. When using `--network=host`, port mappings (`-p`) are not needed and will be ignored, as the container shares the host's network stack directly.\n\n## Configuration\n\nConfigure the server using environment variables:\n\n| Environment Variable | Description | Default Value |\n|----------------------|-------------|---------------|\n| SEARXNG_MCP_SEARXNG_URL | URL of the SearxNG instance to use | <https://paulgo.io/> |\n| SEARXNG_MCP_TIMEOUT | HTTP request timeout in seconds | 10 |\n| SEARXNG_MCP_DEFAULT_RESULT_COUNT | Default number of results to return | 10 |\n| SEARXNG_MCP_DEFAULT_LANGUAGE | Language code for results (e.g., 'en', 'ru', 'all') | all |\n| SEARXNG_MCP_DEFAULT_FORMAT | Default format for results ('text', 'json') | text |\n| SEARXNG_MCP_LOG_LEVEL | Logging level (e.g., 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL') | ERROR |\n| TRANSPORT_PROTOCOL | Transport protocol ('stdio' or 'sse') | stdio |\n\n**Note:** Setting log levels higher than ERROR (such as DEBUG or INFO) may break integration with some applications due to excessive output in the communication channel.\n\nYou can find a list of public SearxNG instances at [https://searx.space](https://searx.space) if you don't want to host your own.\n\n## Installation & Usage\n\n### Prerequisites\n\n- Python 3.10 or higher\n- A SearxNG instance (public or self-hosted)\n\n### Option 1: Run Without Installation (Recommended)\n\nThe easiest way to use this server is with pipx or uvx, which allows you to run the package without installing it permanently:\n\n```bash\n# Using pipx\npip install pipx  # Install pipx if you don't have it\npipx run searxng-simple-mcp\n\n# OR using uvx\npip install uvx  # Install uvx if you don't have it\nuvx run searxng-simple-mcp\n```\n\nYou can pass configuration options directly:\n\n```bash\n# Using pipx with custom SearxNG instance\npipx run searxng-simple-mcp --searxng-url https://your-instance.example.com\n```\n\n### Option 2: Install from PyPI or Source\n\nFor more permanent installation:\n\n```bash\n# From PyPI using pip\npip install searxng-simple-mcp\n\n# OR using uv (faster installation)\npip install uv\nuv pip install searxng-simple-mcp\n\n# OR from source\ngit clone https://github.com/Sacode/searxng-simple-mcp.git\ncd searxng-simple-mcp\npip install uv\nuv pip install -e .\n```\n\nAfter installation, you can run the server with:\n\n```bash\n# Run directly after installation\npython -m searxng_simple_mcp.server\n\n# OR with configuration options\npython -m searxng_simple_mcp.server --searxng-url https://your-instance.example.com\n```\n\n### Option 3: Docker\n\nIf you prefer using Docker:\n\n```bash\n# Pull the Docker image\ndocker pull ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run the container with default settings (stdio transport)\ndocker run --rm -i ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run with environment file for configuration\ndocker run --rm -i --env-file .env ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run with SSE transport (starts HTTP server on port 8000)\ndocker run -p 8000:8000 -e TRANSPORT_PROTOCOL=sse ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Building locally\ndocker build -t searxng-simple-mcp:local .\ndocker run --rm -i searxng-simple-mcp:local\n\n# Using Docker Compose\ndocker-compose up -d\n```\n\nFor complete Docker usage information, see the [Docker Configuration](#docker-configuration) section below.\n\n## Transport Protocols\n\nThe MCP server supports two transport protocols:\n\n- **STDIO** (default): For CLI applications and direct integration\n  - Used by default in all examples\n  - Suitable for integration with Claude Desktop and other MCP-compliant clients\n  - No HTTP server is started\n\n- **SSE** (Server-Sent Events): For web-based clients and HTTP-based integrations\n  - Starts an HTTP server that clients can connect to\n  - Useful for web applications and services that need real-time updates\n  - Requires port mapping when using Docker\n\n### Using SSE Transport\n\nTo use the SSE transport protocol:\n\n1. **With direct execution**:\n\n   ```bash\n   # Set the transport protocol to SSE\n   TRANSPORT_PROTOCOL=sse python -m searxng_simple_mcp.server\n   \n   # Or with FastMCP\n   fastmcp run src/searxng_simple_mcp/server.py --transport sse\n   ```\n\n2. **With Docker**:\n\n   ```bash\n   # Run with SSE transport protocol\n   docker run -p 8000:8000 -e TRANSPORT_PROTOCOL=sse -e SEARXNG_MCP_SEARXNG_URL=https://your-instance.example.com ghcr.io/sacode/searxng-simple-mcp:latest\n   ```\n\n3. **With Docker Compose** (from the included `docker-compose.yml`):\n\n   ```yaml\n   environment:\n     - SEARXNG_MCP_SEARXNG_URL=https://searx.info\n     - SEARXNG_MCP_TIMEOUT=10\n     - SEARXNG_MCP_MAX_RESULTS=20\n     - SEARXNG_MCP_LANGUAGE=all\n     - TRANSPORT_PROTOCOL=sse # Transport protocol: stdio or sse\n   ```\n\nWhen using SSE, the server will be accessible via HTTP at `http://localhost:8000` by default.\n\nTo connect to the SSE server from an MCP client, use a configuration like:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"url\": \"http://localhost:8000\",\n      \"transport\": \"sse\"\n    }\n  }\n}\n```\n\n**Note:** Not all applications support the SSE transport protocol. Make sure your MCP client is compatible with SSE before using this transport method.\n\n## Development\n\nFor development and testing:\n\n```bash\n# Install dependencies\nuv pip install -e .\n\n# Run linter and formatter\nruff check .\nruff check --fix .\nruff format .\n\n# Run the server directly\npython -m src.searxng_simple_mcp.server\n\n# OR using FastMCP\nfastmcp run src/searxng_simple_mcp/server.py  # Use stdio transport (default)\nfastmcp run src/searxng_simple_mcp/server.py --transport sse  # Use sse transport\n\n# Run in development mode (launches MCP Inspector)\nfastmcp dev src/searxng_simple_mcp/server.py\n```\n\n## Publishing to PyPI\n\nFor maintainers who need to publish new versions of the package to PyPI:\n\n```bash\n# Install development dependencies\nnpm run install:deps\n\n# Clean, build, and check the package\nnpm run build:package\nnpm run check:package\n\n# Publish to PyPI (requires PyPI credentials)\nnpm run publish:pypi\n\n# Alternatively, use the all-in-one commands to update version and publish\nnpm run publish:patch  # Increments patch version (1.0.1 -> 1.0.2)\nnpm run publish:minor  # Increments minor version (1.0.1 -> 1.1.0)\nnpm run publish:major  # Increments major version (1.0.1 -> 2.0.0)\n```\n\nThese commands will:\n\n1. Update the version in both package.json and pyproject.toml\n2. Clean the dist directory to remove old builds\n3. Build the package (creating wheel and source distribution)\n4. Check the package for errors\n5. Upload the package to PyPI\n\nYou'll need to have a PyPI account and be authenticated with twine. You can set up authentication by:\n\n- Creating a `.pypirc` file in your home directory\n- Using environment variables (`TWINE_USERNAME` and `TWINE_PASSWORD`)\n- Using PyPI API tokens (recommended)\n\n## Docker Configuration\n\nWhen using Docker with MCP servers, keep these points in mind:\n\n1. **Integration with MCP clients**: Use the configuration shown in the [Using with Docker](#using-with-docker-no-installation-required) section for integrating with Claude Desktop or other MCP-compliant clients.\n\n2. **Transport protocols**:\n   - By default, the Docker container uses the stdio transport protocol\n   - For SSE transport, see the [Using SSE Transport](#using-sse-transport) section\n\n3. **Configuration options**:\n   - Use an environment file (.env) to configure the server: `docker run --env-file .env ...`\n   - Pass individual environment variables with the `-e` flag: `docker run -e SEARXNG_MCP_SEARXNG_URL=https://example.com ...`\n   - See the [Configuration](#configuration) section for available environment variables\n\n4. **Networking**:\n   - Use `--network=host` when you need to access services on your host machine\n   - Use `-p 8000:8000` when exposing the SSE server to your network\n\n## Package Structure\n\n```\nsearxng-simple-mcp/\n├── src/\n│   ├── run_server.py         # Entry point script\n│   └── searxng_simple_mcp/   # Main package\n├── docker-compose.yml        # Docker Compose configuration\n├── Dockerfile                # Docker configuration\n└── pyproject.toml            # Python project configuration\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "web",
        "search web",
        "searxng simple",
        "web search"
      ],
      "category": "web-search"
    },
    "ScrapeGraphAI--scrapegraph-mcp": {
      "owner": "ScrapeGraphAI",
      "name": "scrapegraph-mcp",
      "url": "https://github.com/ScrapeGraphAI/scrapegraph-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ScrapeGraphAI.webp",
      "description": "A scraping API for structured data extraction from websites that includes features for specific webpage data extraction, web search integration, and content filtering. It also converts webpages into markdown format.",
      "stars": 39,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-22T03:15:16Z",
      "readme_content": "# ScrapeGraph MCP Server\n\n\n<a href=\"https://glama.ai/mcp/servers/37us0q2tr6\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/37us0q2tr6/badge\" alt=\"ScrapeGraph Server MCP server\" style=\"display: inline-block;\"/>\n</a>\n<a href=\"https://mseep.ai/app/scrapegraphai-scrapegraph-mcp\">\n  <img src=\"https://mseep.net/pr/scrapegraphai-scrapegraph-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" style=\"display: inline-block;\"/>\n</a>\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)\n[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)\n\n\nA production-ready [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides seamless integration with the [ScapeGraph AI](https://scrapegraphai.com) API. This server enables language models to leverage advanced AI-powered web scraping capabilities with enterprise-grade reliability.\n\n\n## Available Tools\n\nThe server provides the following enterprise-ready tools:\n\n- `markdownify(website_url: str)`: Transform any webpage into clean, structured markdown format\n- `smartscraper(user_prompt: str, website_url: str)`: Leverage AI to extract structured data from any webpage\n- `searchscraper(user_prompt: str)`: Execute AI-powered web searches with structured, actionable results\n\n## Setup Instructions\n\nTo utilize this server, you'll need a ScapeGraph API key. Follow these steps to obtain one:\n\n1. Navigate to the [ScapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n2. Create an account and generate your API key\n\n### Automated Installation via Smithery\n\nFor automated installation of the ScrapeGraph API Integration Server using [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp):\n\n```bash\nnpx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude\n```\n\n### Claude Desktop Configuration\n\nUpdate your Claude Desktop configuration file with the following settings (located on the top rigth of the Cursor page):\n\n(remember to add your API key inside the config)\n\n```json\n{\n    \"mcpServers\": {\n        \"@ScrapeGraphAI-scrapegraph-mcp\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@smithery/cli@latest\",\n                \"run\",\n                \"@ScrapeGraphAI/scrapegraph-mcp\",\n                \"--config\",\n                \"\\\"{\\\\\\\"scrapegraphApiKey\\\\\\\":\\\\\\\"YOUR-SGAI-API-KEY\\\\\\\"}\\\"\"\n            ]\n        }\n    }\n}\n```\n\nThe configuration file is located at:\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\n### Cursor Integration\n\nAdd the ScrapeGraphAI MCP server on the settings:\n\n\n\n## Example Use Cases\n\nThe server enables sophisticated queries such as:\n\n- \"Analyze and extract the main features of the ScapeGraph API\"\n- \"Generate a structured markdown version of the ScapeGraph homepage\"\n- \"Extract and analyze pricing information from the ScapeGraph website\"\n- \"Research and summarize recent developments in AI-powered web scraping\"\n- \"Create a comprehensive summary of the Python documentation website\"\n\n## Error Handling\n\nThe server implements robust error handling with detailed, actionable error messages for:\n\n- API authentication issues\n- Malformed URL structures\n- Network connectivity failures\n- Rate limiting and quota management\n\n## Common Issues\n\n### Windows-Specific Connection\n\nWhen running on Windows systems, you may need to use the following command to connect to the MCP server:\n\n```bash\nC:\\Windows\\System32\\cmd.exe /c npx -y @smithery/cli@latest run @ScrapeGraphAI/scrapegraph-mcp --config \"{\\\"scrapegraphApiKey\\\":\\\"YOUR-SGAI-API-KEY\\\"}\"\n```\n\nThis ensures proper execution in the Windows environment.\n\n## License\n\nThis project is distributed under the MIT License. For detailed terms and conditions, please refer to the LICENSE file.\n\n## Acknowledgments\n\nSpecial thanks to [tomekkorbak](https://github.com/tomekkorbak) for his implementation of [oura-mcp-server](https://github.com/tomekkorbak/oura-mcp-server), which served as starting point for this repo.\n\nMade with ❤️ by [ScrapeGraphAI](https://scrapegraphai.com) Team",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapegraph",
        "scraping",
        "scrapegraphai",
        "scrapegraph mcp",
        "mcp scraping",
        "extraction web"
      ],
      "category": "web-search"
    },
    "Scrapezy--mcp": {
      "owner": "Scrapezy",
      "name": "mcp",
      "url": "https://github.com/Scrapezy/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Scrapezy.webp",
      "description": "Extract structured data from websites using AI models by providing a URL and a clear prompt. It simplifies data extraction tasks and integrates with AI workflows for enhanced application capabilities.",
      "stars": 10,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T18:17:20Z",
      "readme_content": "# @scrapezy/mcp MCP Server\n\n<a href=\"https://glama.ai/mcp/servers/rnktqsouvy\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/rnktqsouvy/badge\" alt=\"Scrapezy MCP server\" />\n</a>\n\n[![smithery badge](https://smithery.ai/badge/@Scrapezy/mcp)](https://smithery.ai/server/@Scrapezy/mcp)\n\nA Model Context Protocol server for [Scrapezy](https://scrapezy.com) that enables AI models to extract structured data from websites.\n\n## Features\n\n### Tools\n- `extract_structured_data` - Extract structured data from a website\n  - Takes URL and prompt as required parameters\n  - Returns structured data extracted from the website based on the prompt\n  - The prompt should clearly describe what data to extract from the website\n\n## Installation\n\n### Installing via Smithery\n\nTo install Scrapezy MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Scrapezy/mcp):\n\n```bash\nnpx -y @smithery/cli install @Scrapezy/mcp --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g @scrapezy/mcp\n```\n\n## Usage\n\n### API Key Setup\n\nThere are two ways to provide your Scrapezy API key:\n\n1. **Environment Variable:**\n   ```bash\n   export SCRAPEZY_API_KEY=your_api_key\n   npx @scrapezy/mcp\n   ```\n\n2. **Command-line Argument:**\n   ```bash\n   npx @scrapezy/mcp --api-key=your_api_key\n   ```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapezy\": {\n      \"command\": \"npx @scrapezy/mcp --api-key=your_api_key\"\n    }\n  }\n}\n```\n\n### Example Usage in Claude\n\nYou can use this tool in Claude with prompts like:\n\n```\nPlease extract product information from this page: https://example.com/product\nExtract the product name, price, description, and available colors.\n```\n\nClaude will use the MCP server to extract the requested structured data from the website.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "scrapezy",
        "extract",
        "extraction",
        "search scrapezy",
        "data extraction",
        "scrapezy mcp"
      ],
      "category": "web-search"
    },
    "Selenium39--mcp-server-weibo": {
      "owner": "Selenium39",
      "name": "mcp-server-weibo",
      "url": "https://github.com/Selenium39/mcp-server-weibo",
      "imageUrl": "/freedevtools/mcp/pfp/Selenium39.webp",
      "description": "Retrieves detailed Weibo user information, feeds, and conducts user searches. Facilitates the analysis and scraping of Weibo data for integration into various applications.",
      "stars": 30,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-09T07:08:53Z",
      "readme_content": "# Weibo MCP Server (TypeScript 版本)\n\n## 安装\n\n从源码安装：\n\n```json\n{\n    \"mcpServers\": {\n        \"weibo\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"--from\",\n                \"git+https://github.com/Selenium39/mcp-server-weibo.git\",\n                \"mcp-server-weibo\"\n            ]\n        }\n    }\n}\n```\n\n从包管理器安装：\n\n```json\n{\n    \"mcpServers\": {\n        \"weibo\": {\n            \"command\": \"npx\",\n            \"args\": [\"mcp-server-weibo\"]\n        }\n    }\n}\n```\n\n## 组件\n\n### Tools\n\n- `search_users(keyword, limit)`：根据关键词搜索微博用户\n- `get_profile(uid)`：获取用户详细资料信息\n- `get_feeds(uid, limit)`：获取用户微博动态\n- `get_hot_search(limit)`：获取微博热搜榜\n- `search_content(keyword, limit, page?)`：根据关键词搜索微博内容\n\n### Resources\n\n无\n\n### Prompts\n\n无\n\n## 系统要求\n\n- Node.js >= 18.0.0\n\n## 许可证\n\nMIT License\n\n## 免责声明\n\n本项目与微博无关，仅用于学习和研究目的。\n\n## MCP Server推荐\n\n[mcp-server-tempmail](https://chat-tempmail.com/zh/mcp-server)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-weibo",
      "npm_downloads": 3944,
      "keywords": [
        "weibo",
        "scraping",
        "selenium39",
        "weibo retrieves",
        "weibo data",
        "scraping weibo"
      ],
      "category": "web-search"
    },
    "Shoofio--brave-search-mcp-sse": {
      "owner": "Shoofio",
      "name": "brave-search-mcp-sse",
      "url": "https://github.com/Shoofio/brave-search-mcp-sse",
      "imageUrl": "/freedevtools/mcp/pfp/Shoofio.webp",
      "description": "Integrate web and local search capabilities to access comprehensive search results with flexible filtering and smart fallbacks. Utilizes Server-Sent Events (SSE) to connect AI models with the Brave Search API.",
      "stars": 13,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-06T13:57:11Z",
      "readme_content": "# Brave Search MCP/SSE Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Docker Hub](https://img.shields.io/docker/v/shoofio/brave-search-mcp-sse/latest?sort=semver&label=Docker%20Hub)](https://hub.docker.com/r/shoofio/brave-search-mcp-sse)\n[![Helm Chart](https://img.shields.io/badge/Helm%20Chart-1.0.10-blue?link=https://shoofio.github.io/brave-search-mcp-sse/)](https://shoofio.github.io/brave-search-mcp-sse/)\n<!-- Add other badges here if applicable (e.g., build status, Docker pulls) -->\n\nAn implementation of the Model Context Protocol (MCP) using Server-Sent Events (SSE) that integrates the [Brave Search API](https://brave.com/search/api/), providing AI models and other clients with web and local search capabilities through a streaming interface.\n\n## Overview\n\nThis server acts as a tool provider for Large Language Models that understand the Model Context Protocol. It exposes Brave's powerful web and local search functionalities via an SSE connection, allowing for real-time streaming of search results and status updates.\n\n**Key Design Goals:**\n\n*   **Centralized Access:** Designed with centrality in mind, allowing organizations or individuals to manage a single Brave Search API key and provide controlled access to multiple internal clients or applications.\n*   **Observability:** Features robust logging to track requests, API interactions, errors, and rate limits, providing visibility into usage and aiding debugging.\n*   **Flexible Deployment:** Can be deployed privately within a network or optionally exposed publicly via methods like Kubernetes Ingress or direct Docker port mapping.\n\n## Features\n\n*   **Web Search**: Access Brave's independent web search index for general queries, news, articles, etc. Supports pagination and filtering controls.\n*   **Local Search**: Find businesses, restaurants, and services with detailed information like address, phone number, and ratings.\n*   **Smart Fallbacks**: Local search automatically falls back to a filtered web search if no specific local results are found for the query.\n*   **Server-Sent Events (SSE)**: Efficient, real-time streaming of search results and tool execution status.\n*   **Model Context Protocol (MCP)**: Adheres to the MCP standard for seamless integration with compatible clients.\n*   **Docker Support**: Includes a `Dockerfile` for easy containerization and deployment.\n*   **Helm Chart**: Provides a Helm chart for straightforward deployment to Kubernetes clusters.\n\n## Prerequisites\n\nDepending on your chosen deployment method, you will need some of the following:\n\n*   **Brave Search API Key**: Required for all deployment methods. See \"Getting Started\" below.\n*   **Docker**: Required if deploying using Docker.\n*   **kubectl & Helm**: Required if deploying to Kubernetes using Helm.\n*   **Node.js & npm**: Required *only* for local development (Node.js v22.x or later recommended).\n*   **Git**: Required for cloning the repository for local development or building custom Docker images.\n\n## Getting Started\n\n### 1. Obtain a Brave Search API Key\n\n1.  Sign up for a [Brave Search API account](https://brave.com/search/api/).\n2.  Choose a plan (a free tier is available).\n3.  Generate your API key from the [developer dashboard](https://api.search.brave.com/app/keys).\n\n### 2. Configuration\n\nThe server requires the Brave Search API key to be set via the `BRAVE_API_KEY` environment variable.\n\nOther potential environment variables (check `src/config/config.ts` for details):\n*   `PORT`: The port the server listens on (defaults to `8080`).\n*   `LOG_LEVEL`: Logging verbosity (e.g., `info`, `debug`).\n\nSet these variables in your environment or using a `.env` file in the project root for local development.\n\n## Installation & Usage\n\nChoose the deployment method that best suits your needs:\n\n### Option 1: Docker (Recommended for Deployment)\n\n**Prerequisites:** Docker installed.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Pull the Docker image:**\n    Pull the latest image from Docker Hub:\n    ```bash\n    docker pull shoofio/brave-search-mcp-sse:latest\n    ```\n    Or pull a specific version tag (e.g., `1.0.10`):\n    ```bash\n    docker pull shoofio/brave-search-mcp-sse:1.0.10\n    ```\n    *(Alternatively, you can build the image locally if needed. Clone the repository and run `docker build -t brave-search-mcp-sse:custom .`)*\n3.  **Run the Docker container:**\n    Use the tag you pulled (e.g., `latest` or `1.0.10`):\n    ```bash\n    docker run -d --rm \\\n      -p 8080:8080 \\\n      -e BRAVE_API_KEY=\"YOUR_API_KEY_HERE\" \\\n      -e PORT=\"8080\" # Optional: Define the port if needed\n      # -e LOG_LEVEL=\"info\" # Optional: Set log level\n      --name brave-search-server \\\n      shoofio/brave-search-mcp-sse:latest # Or your specific tag\n    ```\n    This runs the server in detached mode, mapping port 8080 on your host to the container.\n\n### Option 2: Helm (Kubernetes Deployment)\n\n**Prerequisites:** `kubectl` connected to your cluster, Helm installed.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Add the Helm repository:**\n    ```bash\n    helm repo add brave-search-mcp-sse https://shoofio.github.io/brave-search-mcp-sse/\n    helm repo update\n    ```\n3.  **Prepare API Key Secret (Recommended):**\n    Create a Kubernetes secret in the target namespace:\n    ```bash\n    kubectl create secret generic brave-search-secret \\\n      --from-literal=api-key='YOUR_API_KEY_HERE' \\\n      -n <your-namespace>\n    ```\n4.  **Install the Helm chart:**\n    The chart version corresponds to the application version (latest is `1.0.10`). Install using the secret:\n    ```bash\n    helm install brave-search brave-search-mcp-sse/brave-search-mcp-sse \\\n      -n <your-namespace> \\\n      --set braveSearch.existingSecret=brave-search-secret\n      # Optionally specify a version: --version 1.0.10\n    ```\n    Or provide the key directly (less secure):\n    ```bash\n    helm install brave-search brave-search-mcp-sse/brave-search-mcp-sse \\\n      -n <your-namespace> \\\n      --set braveSearch.apiKey=\"YOUR_API_KEY_HERE\"\n    ```\n5.  **Chart Configuration:**\n    You can customize the deployment by overriding default values. Create a YAML file (e.g., `dev-values.yaml`, `prod-values.yaml`) with your desired settings and use the `-f` flag during installation: `helm install ... -f dev-values.yaml`.\n\n    Refer to the chart's default [`values.yaml`](./helm/brave-search-mcp-sse/values.yaml) file to see all available configuration options and their default settings.\n\n### Option 3: Local Development\n\n**Prerequisites:** Node.js and npm (v22.x or later recommended), Git.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Clone the repository:**\n    ```bash\n    git clone <repository_url> # Replace with the actual URL\n    cd brave-search-mcp-sse\n    ```\n3.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n4.  **Set Environment Variables:**\n    Create a `.env` file in the root directory:\n    ```env\n    BRAVE_API_KEY=YOUR_API_KEY_HERE\n    PORT=8080\n    # LOG_LEVEL=debug\n    ```\n5.  **Build the TypeScript code:**\n    ```bash\n    npm run build\n    ```\n6.  **Run the server:**\n    ```bash\n    npm start\n    # Or for development with auto-reloading (if nodemon/ts-node-dev is configured)\n    # npm run dev\n    ```\n    The server will start listening on the configured port (default `8080`).\n\n## API / Protocol Interaction\n\nClients connect to this server via HTTP GET request to establish an SSE connection. The specific endpoint depends on your deployment (e.g., `http://localhost:8080/`, `http://<k8s-service-ip>:8080/`, or through an Ingress).\n\nOnce connected, the server and client communicate using MCP messages over the SSE stream.\n\n### Available Tools\n\nThe server exposes the following tools to connected clients:\n\n1.  **`brave_web_search`**\n    *   **Description**: Performs a general web search using the Brave Search API.\n    *   **Inputs**:\n        *   `query` (string, required): The search query.\n        *   `count` (number, optional): Number of results to return (1-20, default 10).\n        *   `offset` (number, optional): Pagination offset (0-9, default 0).\n        *   *(Other Brave API parameters like `search_lang`, `country`, `freshness`, `result_filter`, `safesearch` might be supported - check `src/services/braveSearchApi.ts`)*\n    *   **Output**: Streams MCP messages containing search results (title, URL, snippet, etc.).\n\n2.  **`brave_local_search`**\n    *   **Description**: Performs a search for local businesses and places using the Brave Search API. Falls back to web search if no local results are found.\n    *   **Inputs**:\n        *   `query` (string, required): The local search query (e.g., \"pizza near me\", \"cafes in downtown\").\n        *   `count` (number, optional): Maximum number of results (1-20, default 5).\n    *   **Output**: Streams MCP messages containing local business details (name, address, phone, rating, etc.).\n\n*(Example using `curl` - Note: Actual MCP interaction requires a client library)*\n```bash\n# Example: Connect to SSE endpoint (won't show MCP messages directly)\ncurl -N http://localhost:8080/ # Or your deployed endpoint\n```\n\n### Client Configuration Example (Cursor)\n\nTo use this server with an MCP client like Cursor, you need to configure the client to connect to the server's SSE endpoint.\n\nAdd the following configuration to your Cursor settings (`mcp.json` or similar configuration file), replacing the URL with the actual address and port where your `brave-search-mcp-sse` server is accessible:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8080/sse\"\n    }\n  }\n}\n```\n\n**Explanation:**\n\n*   `transport`: Must be set to `\"sse\"` for this server.\n*   `url`: This is the crucial part.\n    *   If running locally via Docker (as shown in the example), `http://localhost:8080/sse` is likely correct.\n    *   If running in Kubernetes, replace `localhost:8080` with the appropriate Kubernetes Service address/port or the Ingress hostname/path configured to reach the server's port 8080.\n    *   Ensure the URL path ends with `/sse`.\n\n*(Similar configuration steps might apply to other MCP clients that support the SSE transport, like recent versions of Claude Desktop, but refer to their specific documentation.)*\n\n## Project Structure\n\n```\n.\n├── Dockerfile             # Container build definition\n├── helm/                  # Helm chart for Kubernetes deployment\n│   └── brave-search-mcp-sse/\n├── node_modules/        # Project dependencies (ignored by git)\n├── src/                   # Source code (TypeScript)\n│   ├── config/            # Configuration loading\n│   ├── services/          # Brave API interaction logic\n│   ├── tools/             # Tool definitions for MCP\n│   ├── transport/         # SSE/MCP communication handling\n│   ├── types/             # TypeScript type definitions\n│   ├── utils/             # Utility functions\n│   └── index.ts           # Main application entry point\n├── dist/                  # Compiled JavaScript output (ignored by git)\n├── package.json           # Project metadata and dependencies\n├── tsconfig.json          # TypeScript compiler options\n├── .env.example           # Example environment file\n├── .gitignore\n└── README.md              # This file\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request with your changes. Ensure your code adheres to the existing style and includes tests where applicable. I will review PRs as time permits.\n\n## License\n\nThis MCP server is licensed under the[MIT License](./LICENSE). This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sse",
        "search",
        "web",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "Sivan22--mcp-otzaria-server": {
      "owner": "Sivan22",
      "name": "mcp-otzaria-server",
      "url": "https://github.com/Sivan22/mcp-otzaria-server",
      "imageUrl": "/freedevtools/mcp/pfp/Sivan22.webp",
      "description": "Provides powerful search capabilities for Jewish texts and literature through a standardized interface, enabling complex queries and relevance-based scoring for rich search results.",
      "stars": 19,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T21:15:26Z",
      "readme_content": "# Jewish Library MCP Server\n\nAn MCP (Model Context Protocol) server that provides powerful search capabilities for Jewish texts and literature. This server enables Large Language Models to search and reference Jewish texts through a standardized interface.\n\n## Features\n\n- Full-text search across Jewish texts and literature\n- Advanced query syntax support:\n  - Field-specific search (text:term, reference:term, topics:term)\n  - Boolean operators (AND, OR)\n  - Required/excluded terms (+term, -term)\n  - Phrase search ('exact phrase')\n  - Wildcards (?, *)\n- Relevance-based scoring\n- Rich search results including references, topics, and highlighted excerpts\n\n## Installation\n\nRequires Python 3.10 or higher.\n\n\n### Clone the repository\n```bash\ngit clone https://github.com/sivan22/mcp-otzaria-server.git\ncd mcp-otzaria-server\n```\n### Get the index\ndownload and extract the index from [here](https://drive.google.com/file/d/1lpbBCPimwcNfC0VZOlQueA4SHNGIp5_t/view?usp=drive_link)\n\n### Install dependencies\n```\npip install .\n```\n## Running the Server\n\nThe server can be run directly:\n\n```bash\nuv --directory path/to/directory run jewish_library\n```\n\nOr through an MCP client that supports the Model Context Protocol.\nfor claude desktop app and cline you should use the following config:\n```\n{\n  \"mcpServers\": {        \n      \"jewish_library\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"your/path/to/directory\",\n              \"run\",\n              \"jewish_library\"\n          ],\n          \"env\": {\n            \"PYTHONIOENCODING\": \"utf-8\" \n          }\n      }\n  }\n}\n```\n\n## Available tools\n\nThe server provides a single tool through the MCP interface:\n\n### full_text_search\n\nPerforms a full-text search across the Jewish library with advanced query capabilities.\n\nExample query formats:\n```\n# Basic search\n\"maimonides on prayer\"\n\n# Field-specific search\ntext:\"love your neighbor\" AND topics:mitzvot\n\n# Required terms\n+shabbat +candles\n\n# Phrase search with topic filter\n\"four species\" AND topics:sukkot\n\n# Wildcard search\npray* AND reference:psalms\n```\n\nSearch results include:\n- Reference information\n- Relevant topics\n- Highlighted excerpts showing query matches\n- Relevance score\n\n## Development\n\nThis project uses:\n- [MCP SDK](https://github.com/modelcontextprotocol/sdk) for server implementation\n- [Tantivy](https://github.com/quickwit-oss/tantivy) for full-text search capabilities\n\n\n\n\n\n## Requirements\n\n- Python >= 3.10\n- MCP SDK >= 1.1.1\n- Tantivy search engine\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "texts",
        "queries",
        "search results",
        "web search",
        "rich search"
      ],
      "category": "web-search"
    },
    "SpaceFrontiers--mcp": {
      "owner": "SpaceFrontiers",
      "name": "mcp",
      "url": "https://github.com/SpaceFrontiers/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SpaceFrontiers.webp",
      "description": "Interact with Space Frontiers data sources through a simple API for keyword and semantic searches across various databases. Enables seamless access to rich space-related information to enhance language model capabilities.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-27T21:39:03Z",
      "readme_content": "# Space Frontiers MCP Server\n\n## General Overview\n\nThis project implements a Model Context Protocol (MCP) server that acts as an interface to the Space Frontiers API. It allows language models to interact with Space Frontiers data sources through MCP tools. The server is built using the FastMCP library.\n\nAt a high level, the server provides LLM-accessible search and discovery across Space Frontiers sources such as scholarly literature, Telegram, and Reddit, including both query-based search and recent-item retrieval. Tools are self-describing via MCP schemas and annotations; your MCP client can list and introspect them at runtime.\n\n**Hosted option:** Space Frontiers provides a publicly hosted MCP server at `https://mcp.spacefrontiers.org`. Obtain an API key from `https://spacefrontiers.org/developers/keys` and include it via the `Authorization: Bearer <your_api_key>` header.\n\n## Environment Variables\n\nThe server utilizes the following environment variables:\n\n*   `SPACE_FRONTIERS_API_ENDPOINT`: The base URL for the Space Frontiers API.\n    *   **Default:** `https://api.spacefrontiers.org`\n*   `SPACE_FRONTIERS_API_KEY`: An optional API key for authenticating requests to the Space Frontiers API.\n    *   **Note:** Authentication can also be provided via request headers:\n        *   `Authorization: Bearer <your_api_key>`\n        *   `X-Api-Key: <your_api_key>`\n        *   Alternatively, a user ID can be provided via the `X-User-Id` header. If none of these are provided, the server will attempt to use the `SPACE_FRONTIERS_API_KEY` environment variable if set.\n        *   **Note on `X-User-Id`:** This header is intended for Space Frontiers internal usage only and cannot be exploited for external authentication.\n\n## Running the Server\n\n```bash\nuv run fastmcp run mcp_server.py\n```\n\nEnsure `SPACE_FRONTIERS_API_KEY` is set in the environment if your client does not pass authentication headers.\n\n### Example Claude Desktop App Configuration (`claude_desktop_config.json`)\n\n```json\n{\n  \"mcpServers\": {\n    \"Space Frontiers MCP server\": {\n      \"command\": \"/path/to/your/uv\",\n      \"args\": [\n        \"run\",\n        \"fastmcp\",\n        \"run\",\n        \"--with\",\n        \"izihawa-loglib\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"spacefrontiers-clients\",\n        \"/path/to/your/spacefrontiers-mcp/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SPACE_FRONTIERS_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nNote: replace placeholder paths and the API key with your actual values.",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "spacefrontiers",
        "space",
        "searches",
        "search spacefrontiers",
        "semantic searches",
        "space frontiers"
      ],
      "category": "web-search"
    },
    "The-AI-Workshops--searxng-mcp-server": {
      "owner": "The-AI-Workshops",
      "name": "searxng-mcp-server",
      "url": "https://github.com/The-AI-Workshops/searxng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/The-AI-Workshops.webp",
      "description": "Integrates with a SearXNG instance to enable AI agents to perform privacy-respecting web searches. Provides a standardized MCP server interface for seamless interaction within AI workflows.",
      "stars": 11,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T13:19:59Z",
      "readme_content": "# SearXNG MCP Server\n\nAn MCP sse implementation of the Model Context Protocol (MCP) server integrated with [SearXNG](https://github.com/searxng/searxng) for providing AI agents with powerful, privacy-respecting search capabilities.\n\n---\n\n## Overview\n\nThis project demonstrates how to build an MCP server that enables AI agents to perform web searches using a SearXNG instance. It serves as a practical template for creating your own MCP servers, using SearXNG as a backend.\n\nThe implementation follows the best practices laid out by Anthropic for building MCP servers, allowing seamless integration with any MCP-compatible client.\n\n---\n\n## Prerequisites\n\n- Python 3.9+\n- Access to a running SearXNG instance (local or remote)\n- Docker (optional, for containerized deployment)\n- [uv](https://github.com/astral-sh/uv) (optional, for fast Python dependency management)\n- [Smithery](https://github.com/The-AI-Workshops/smithery) (optional, for MCP server management)\n\n### SearXNG Server (Required)\n\nYou must have a SearXNG server running and accessible. The recommended way is via Docker:\n\n```bash\ndocker run -d --name=searxng -p 32768:8080 -v \"/root/searxng:/etc/searxng\" \\\n  -e \"BASE_URL=http://0.0.0.0:32768/\" \\\n  -e \"INSTANCE_NAME=home\" \\\n  --restart always searxng/searxng\n```\n\n- This will run SearXNG on port 32768 and persist configuration in `/root/searxng`.\n- The MCP server expects SearXNG to be available at `http://172.17.0.1:32768` by default (see `.env`).\n\n---\n\n## Installation\n\n### Using uv\n\nInstall uv if you don't have it:\n\n```bash\npip install uv\n```\n\nClone this repository:\n\n```bash\ngit clone https://github.com/The-AI-Workshops/searxng-mcp-server.git\ncd searxng-mcp-server/dev/searXNG-mcp\n```\n\nInstall dependencies:\n\n```bash\nuv pip install -r requirements.txt\n```\n\nCreate a `.env` file based on the provided example:\n\n```bash\nnano .env\n# Edit .env as needed\n```\n\nConfigure your environment variables in the `.env` file (see Configuration section).\n\n---\n\n### Using Docker (Recommended)\n\nBuild the Docker image:\n\n```bash\ndocker build -t mcp/searxng-mcp .\n```\n\nCreate a `.env` file and configure your environment variables.\n\n---\nRun the Docker image:\n\n```bash\ndocker run -d --env-file ./.env -p 32769:32769 mcp/searxng-mcp\n```\n\n---\n\n### Using Smithery\n\n[Smithery](https://github.com/The-AI-Workshops/smithery) is a command-line tool for managing AI agent tools and MCP servers.\n\nInstall Smithery if you don't have it (see Smithery documentation for various installation methods, e.g., using pipx):\n```bash\npipx install smithery\n```\n\nInstall the SearXNG MCP server using Smithery:\n```bash\nsmithery install @The-AI-Workshops/searxng-mcp-server\n```\nThis will install the server and its dependencies into a dedicated environment managed by Smithery.\n\nAfter installation, Smithery will provide you with the path to the installed server. You will need to navigate to this directory to configure it. For example, if Smithery installs tools into `~/.smithery/tools/`, the path might be `~/.smithery/tools/The-AI-Workshops/searxng-mcp-server`.\n\nCreate a `.env` file in the server's directory by copying the example:\n```bash\n# Example:\n# cd ~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\ncp .env.example .env\nnano .env\n# Edit .env as needed\n```\nConfigure your environment variables in the `.env` file (see Configuration section).\n\n---\n\n## Configuration\n\nThe following environment variables can be configured in your `.env` file:\n\n| Variable           | Description                                 | Example                                 |\n|--------------------|---------------------------------------------|-----------------------------------------|\n| SEARXNG_BASE_URL   | Base URL of your SearXNG instance           | http://172.17.0.1:32768                 |\n| HOST               | Host to bind to when using SSE transport    | 0.0.0.0                                 |\n| PORT               | Port to listen on when using SSE transport  | 32769                                   |\n| TRANSPORT          | Transport protocol (sse or stdio)           | sse                                     |\n\n---\n\n## Running the Server\n\n### Using uv\n\n**SSE Transport**\n\nSet `TRANSPORT=sse` in `.env` then:\n\n```bash\nuv run dev/searXNG-mcp/server.py\n```\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself can spin up the MCP server, so nothing to run at this point.\n\n---\n\n### Using Docker\n\n**SSE Transport**\n\n```bash\ndocker build -t mcp/searxng-mcp .\ndocker run --rm -it -p 32769:32769 --env-file dev/searXNG-mcp/.env -v $(pwd)/dev/searXNG-mcp:/app mcp/searxng-mcp\n```\n\n- The `-v $(pwd)/dev/searXNG-mcp:/app` mount allows you to live-edit the code and .env file on your host and have changes reflected in the running container.\n- The server will be available at `http://localhost:32769/sse`.\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself can spin up the MCP server container, so nothing to run at this point.\n\n---\n\n### Running with Smithery\n\n**SSE Transport**\n\nSet `TRANSPORT=sse` in `.env` in the Smithery-installed server directory.\nThen, you can typically run the server using the Python interpreter from the virtual environment Smithery created for the tool:\n```bash\n# Navigate to the server directory, e.g.,\n# cd ~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\n~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python server.py\n```\nAlternatively, if Smithery provides a direct run command for installed tools (check Smithery documentation):\n```bash\nsmithery run @The-AI-Workshops/searxng-mcp-server\n```\nThe server will be available based on your HOST and PORT settings in `.env` (e.g., `http://localhost:32769/sse`).\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself will spin up the server. The client configuration will need to point to the `server.py` script within the Smithery-managed directory, potentially using `smithery exec` or the direct path to the Python interpreter in the tool's virtual environment. See the \"Integration with MCP Clients\" section for examples.\n\n---\n\n## Integration with MCP Clients\n\n### SSE Configuration\n\nOnce you have the server running with SSE transport, you can connect to it using this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:32769/sse\"\n    }\n  }\n}\n```\n\n**Note for Windsurf users:** Use `serverUrl` instead of `url` in your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"transport\": \"sse\",\n      \"serverUrl\": \"http://localhost:32769/sse\"\n    }\n  }\n}\n```\n\n**Note for n8n users:** Use `host.docker.internal` instead of `localhost` since n8n has to reach outside of its own container to the host machine:\n\nSo the full URL in the MCP node would be: `http://host.docker.internal:32769/sse`\n\nMake sure to update the port if you are using a value other than the default 32769.\n\n---\n\n### Python with Stdio Configuration\n\nAdd this server to your MCP configuration for Claude Desktop, Windsurf, or any other MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"python\",\n      \"args\": [\"dev/searXNG-mcp/server.py\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\",\n        \"HOST\": \"0.0.0.0\",\n        \"PORT\": \"32769\"\n      }\n    }\n  }\n}\n```\n\n---\n\n### Docker with Stdio Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"--rm\", \"-i\",\n               \"-e\", \"TRANSPORT\",\n               \"-e\", \"SEARXNG_BASE_URL\",\n               \"-e\", \"HOST\",\n               \"-e\", \"PORT\",\n               \"mcp/searxng-mcp\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\",\n        \"HOST\": \"0.0.0.0\",\n        \"PORT\": \"32769\"\n      }\n    }\n  }\n}\n```\n\n---\n\n### Smithery with Stdio Configuration\n\nIf you installed the server using Smithery, you can configure your MCP client to run it via stdio. Smithery provides an `exec` command to run executables from within the tool's environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"smithery\",\n      \"args\": [\"exec\", \"@The-AI-Workshops/searxng-mcp-server\", \"--\", \"python\", \"server.py\"],\n      // \"cwd\" (current working directory) might be automatically handled by Smithery.\n      // If server.py is in a subdirectory, adjust the python script path e.g., \"python\", \"path/to/server.py\"\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\", // Adjust as needed\n        \"HOST\": \"0.0.0.0\", // Typically not used by stdio server itself but good to set\n        \"PORT\": \"32769\"  // Typically not used by stdio server itself\n      }\n    }\n  }\n}\n```\nAlternatively, you can find the path to the Python interpreter in the virtual environment created by Smithery (e.g., `~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python`) and the path to `server.py` (e.g., `~/.smithery/tools/The-AI-Workshops/searxng-mcp-server/server.py`) and use those directly:\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python\",\n      \"args\": [\"~/.smithery/tools/The-AI-Workshops/searxng-mcp-server/server.py\"],\n      // \"cwd\" should be the directory containing server.py if not using absolute paths for args,\n      // or if server.py relies on relative paths for other files (like .env).\n      // Example: \"cwd\": \"~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\",\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\"\n        // Other necessary env vars from .env can be duplicated here\n      }\n    }\n  }\n}\n```\nEnsure the paths are correct for your Smithery installation and that the `.env` file is discoverable by `server.py` (usually by setting `cwd` to the server's root directory or ensuring `server.py` loads it from an absolute path if Smithery sets one).\n\n---\n\n## Building Your Own Server\n\nThis template provides a foundation for building more complex MCP servers. To build your own:\n\n- Add your own tools by creating methods with the `@mcp.tool()` decorator\n- Create your own lifespan function to add your own dependencies (clients, database connections, etc.)\n- Add prompts and resources as well with `@mcp.resource()` and `@mcp.prompt()`\n\n---\n\n## SearXNG Search Tool Parameters\n\nThe `search` tool supports the following parameters (all optional except `q`):\n\n- `q` (required): The search query string.\n- `categories`: Comma-separated list of active search categories.\n- `engines`: Comma-separated list of active search engines.\n- `language`: Code of the language.\n- `page`: Search page number (default: 1).\n- `time_range`: [day, month, year]\n- `format`: [json, csv, rss] (default: json)\n- `results_on_new_tab`: [0, 1]\n- `image_proxy`: [true, false]\n- `autocomplete`: [google, dbpedia, duckduckgo, mwmbl, startpage, wikipedia, stract, swisscows, qwant]\n- `safesearch`: [0, 1, 2]\n- `theme`: [simple]\n- `enabled_plugins`: List of enabled plugins.\n- `disabled_plugins`: List of disabled plugins.\n- `enabled_engines`: List of enabled engines.\n- `disabled_engines`: List of disabled engines.\n\nSee the [SearXNG documentation](https://docs.searxng.org/) for more details.\n\n---\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "searches",
        "ai",
        "ai workflows",
        "search ai",
        "searxng mcp"
      ],
      "category": "web-search"
    },
    "Tomatio13--mcp-server-tavily": {
      "owner": "Tomatio13",
      "name": "mcp-server-tavily",
      "url": "https://github.com/Tomatio13/mcp-server-tavily",
      "imageUrl": "/freedevtools/mcp/pfp/Tomatio13.webp",
      "description": "Perform web searches and retrieve information using the Tavily API, returning results in text format along with AI responses, URIs, and titles.",
      "stars": 43,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-14T15:59:56Z",
      "readme_content": "# tavily-search MCP server\n\nA MCP server project\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Tomatio13/mcp-server-tavily)](https://archestra.ai/mcp-catalog/tomatio13__mcp-server-tavily)\n<a href=\"https://glama.ai/mcp/servers/s0hka6zney\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/s0hka6zney/badge\" alt=\"tavily-search MCP server\" /></a>\n\n## Components\n\nThis server uses the Tavily API to perform searches based on specified queries.\n- Search results are returned in text format.\n- Search results include AI responses, URIs, and titles of the search results.\n\n### Tools\n\nThis server implements the following tools:\n- search: Performs searches based on specified queries\n  - Required argument: \"query\"\n  - Optional argument: \"search_depth\" (basic or advanced)\n\n### Installing via Smithery\n\nTo install Tavily Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/tavily-search):\n\n```bash\nnpx -y @smithery/cli install tavily-search --client claude\n```\n\n### Install\n\n1. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n2. Open the Claude Desktop configuration file.\n```\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `C:\\Users\\[username]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n```\n\n3. Edit the configuration file as follows:\n  ```yaml\n  \"mcpServers\": {\n    \"tavily-search\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\your_path\\\\mcp-server-tavily\",\n        \"run\",\n        \"tavily-search\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n  }\n  ```\n\n4. Restart Claude Desktop.\n\n## Usage\n\nIn Claude Desktop, when you ask \"Please search for something\", you will receive search results.\n\nSearch example:\n```\nPlease search in detail for today's events in Kamakura\n```\nResponse example:\n```\nAccording to the search results, the following events start today, December 1st:\n\"Kamakura Promotion Photo Contest 2025\"\nPeriod: December 1, 2024 - January 31, 2025\nA photo contest for those who love Kamakura\nApplications start accepting from today\nAlso, as a related upcoming event:\nOn December 7th, an exhibition by 12 Kamakura artists will be held at the Seibu Press Inn Kamakura Ofuna Station East Exit Lounge.\n```\n\n## Log Storage Location\n\nLogs are stored in the following location:\n\nFor Windows:\n```\nC:\\Users\\[username]\\AppData\\Roaming\\Claude\\logs\\mcp-server-tavily-search\n```\n## Execution with Cursor\n\n1. Create a shell script (e.g., `script.sh`) as shown below:\n\n```bash\n#!/bin/bash\nTARGET_DIR=/path/to/mcp-server-tavily\ncd \"${TARGET_DIR}\"\nexport TAVILY_API_KEY=\"your-api-key\"\nexport PYTHONIOENCODING=utf-8\nuv --directory $PWD run tavily-search\n```\n\n2. Configure Cursor's MCP Server settings as follows:\n\n```\nName: tavily-search\nType: command\nCommand: /path/to/your/script.sh\n```\n\n3. Save the settings.\n\n4. Once the settings are saved, you can ask Cursor's Composer-Agent to \"search for something,\" and it will return the search results.\n\n## Running in Local Environment Using Docker Compose\n\n### Purpose\nFor operating systems other than Windows/MacOS where Claude Desktop cannot be used,\nthis section explains how to set up and run an MCP server and client in a local environment\nusing Docker compose.\n\n### Steps\n1. Install Docker.\n2. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n3. Run Docker compose.\n```bash\ndocker compose up -d\n``` \n4. Execute the client.\n```bash\ndocker exec mcp_server uv --directory /usr/src/app/mcp-server-tavily/src run client.py\n```\n5. Execution Results\n6. After searching for available tools as shown below, a query will be issued to Tavily and a response will be returned:\n```bash\n2024-12-01 11:21:56,930 - tavily-search-server - INFO - Starting Tavily search server\n2024-12-01 11:21:56,932 - tavily-search-server - INFO - Server initialized, starting main loop\n2024-12-01 11:21:56,936 - mcp.server - INFO - Processing request of type ListToolsRequest\n2024-12-01 11:21:56,936 - tavily-search-server - INFO - Listing available tools\n利用可能なツール: nextCursor=None tools=[Tool(name='search', description='Search the web using Tavily API', inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query'}, 'search_depth': {'type': 'string', 'description': 'Search depth (basic or advanced)', 'enum': ['basic', 'advanced']}}, 'required': ['query']})]\n2024-12-01 11:21:56,937 - mcp.server - INFO - Processing request of type CallToolRequest\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - TOOL_CALL_DEBUG: Tool called - name: search, arguments: {'query': '今日の東京タワーのイベントを教えて下さい'}\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - Executing search with query: '今日の東京タワーのイベントを教えて下さい'\n2024-12-01 11:22:00,243 - httpx - INFO - HTTP Request: POST https://api.tavily.com/search \"HTTP/1.1 200 OK\"\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Answer generated\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Results available\nツール実行結果: content=[TextContent(type='text', text='AI Answer:\\n今日の東京タワーのイベントは以下の通りです：\\n1. Candlelight: エド・シーランとコールドプレイのヒットメドレー - 12月01日\\n2. チームラボプラネッツ TOKYO - 12月01日から1月21日\\n\\n他にもイベントがある可能性がありますので、公式ウェブサイト等で最新情報をご確認ください。\\n\\n\\n\\nSearch Results:\\n\\n1. 東京タワー (東京): 現在のイベントとチケット | Fever\\nURL: https://feverup.com/ja/tokyo/venue/tokyo-tower\\nSummary: Summary not found\\n\\n\\n2. 東京タワー(東京都)の施設で開催するイベント一覧｜ウォーカープラス\\nURL: https://www.walkerplus.com/spot/ar0313s03867/e_list.html\\nSummary: Summary not found\\n\\n\\n3. 東京タワー - Tokyo Tower\\nURL: https://www.tokyotower.co.jp/event/\\nSummary: Summary not found\\n')] isError=False\n``` \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tomatio13",
        "searches",
        "api",
        "search tomatio13",
        "tavily api",
        "tomatio13 mcp"
      ],
      "category": "web-search"
    },
    "VinhPhamAI--brave-search": {
      "owner": "VinhPhamAI",
      "name": "brave-search",
      "url": "https://github.com/VinhPhamAI/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, including general queries, news and articles searches, and detailed local business and service lookups with pagination and filtering options.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "search",
        "api",
        "brave search",
        "search api",
        "search vinhphamai"
      ],
      "category": "web-search"
    },
    "VivekKumarNeu--MCP-Lucene-Server": {
      "owner": "VivekKumarNeu",
      "name": "MCP-Lucene-Server",
      "url": "https://github.com/VivekKumarNeu/MCP-Lucene-Server",
      "imageUrl": "/freedevtools/mcp/pfp/VivekKumarNeu.webp",
      "description": "Efficiently manage and retrieve documents using Apache Lucene with a RESTful API for complex querying and document management tasks. Supports adding, updating, deleting, and querying documents while utilizing Lucene's powerful indexing features.",
      "stars": 0,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Java",
      "updated_at": "2025-09-28T01:48:15Z",
      "readme_content": "![License](https://img.shields.io/github/license/VivekKumarNeu/MCP-Lucene-Server)\n\n\n# MCP Lucene Server\n\n## Description\n\nThe MCP Lucene Server is a Java-based implementation of the Model Context Protocol (MCP) designed to provide efficient search and retrieval capabilities using Apache Lucene. This server allows you to manage and query documents, leveraging Lucene's powerful indexing and search features. It is built using Spring Boot for easy setup and deployment.\n\n\n![lucene_mcp1](https://github.com/user-attachments/assets/5dc28224-2dda-4b42-ac90-83343c9c386d)\n\n![lucene_mcp2](https://github.com/user-attachments/assets/b5ffd0cf-87ad-4129-af34-98163690f2ba)\n\n## Features\n\n* **MCP Compliance:** Implements the core Model Context Protocol.\n\n* **Lucene-Powered:** Utilizes Apache Lucene for full-text search and indexing.\n\n* **RESTful API:** Provides a RESTful API for interacting with the server.\n\n* **Document Management:**\n\n    * **Upsert:** Add or update documents in the Lucene index.\n\n    * **Delete:** Delete documents from the Lucene index.\n\n    * **List:** Retrieve a list of documents from the index.\n\n* **Querying:**\n\n    * Supports complex queries using the Lucene query syntax.\n\n    * Filtering: Filter queries based on document metadata.\n\n* **Status:** Check the server status.\n\n* **Spring Boot:** Built with Spring Boot for easy setup and deployment.\n* **Dockerization:** Includes instructions for containerizing the application using Docker.\n\n## Table of Contents\n\n* [Description](#description)\n\n* [Features](#features)\n\n* [Getting Started](#getting-started)\n\n    * [Prerequisites](#prerequisites)\n\n    * [Installation](#installation)\n\n    * [Running the Server](#running-the-server)\n\n* [Usage](#usage)\n\n    * [API Endpoints](#api-endpoints)\n\n    * [Examples](#examples)\n\n* [Configuration](#configuration)\n\n* [License](#license)\n\n## Getting Started\n\n### Prerequisites\n\n* **Java:** Java 11 or higher.\n\n* **Maven:** Maven 3.6.0 or higher.\n* **Docker:** [Install Docker](https://docs.docker.com/get-docker/) if you plan to use the Docker image.\n\n### Installation\n\n1.  **Clone the repository:**\n\n    ```\n    git clone [https://github.com/your-username/mcp-lucene-server.git](https://github.com/your-username/mcp-lucene-server.git)\n    cd mcp-lucene-server\n    ```\n\n    (Replace `your-username` with your GitHub username)\n\n2.  **Build the project using Maven:**\n\n    ```\n    mvn clean install\n    ```\n\n### Running the Server\n\n#### Without Docker\n\n1.  **Run the Spring Boot application:**\n    ```bash\n    java -jar target/mcp-lucene-server-0.0.1-SNAPSHOT.jar\n    ```\n    (The exact name of the `.jar` file might vary slightly depending on your project version.)\n\n2.  The server will start on port `8080` by default.\n\n#### With Docker\n\n1.  **Ensure you have Docker installed:** Follow the instructions on the official Docker website: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n2.  **Build the Docker image:**\n    Navigate to the root directory of your project in your terminal and run:\n    ```bash\n    docker build -t mcp-lucene-server .\n    ```\n\n5.  **Run the Docker container:**\n    ```bash\n    docker run -p 8080:8080 mcp-lucene-server\n    ```\n    This will map port `8080` on your host machine to port `8080` inside the container.\n\n## MCP Shim for Claude Desktop\n\nThis project includes an optional MCP shim (`mcp-shim/`) that exposes the server's REST endpoints as MCP tools over STDIO so you can use them directly from Claude Desktop.\n\n### Prerequisites\n- Java 17+\n- Node.js 18+\n- Maven 3.6+\n\n### 1) Run the Spring Boot server\n```bash\nmvn spring-boot:run\n```\nThe API will be available at `http://localhost:8080/mcp/v1`.\n\n### 2) Run the MCP shim\n```bash\ncd mcp-shim\nnpm install\n# JSON + text output (default)\nLUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n# If your client cannot render JSON tool outputs, force text-only\nMCP_FORCE_TEXT=1 LUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n```\n\n### 3) Configure Claude Desktop\nUpdate `~/.claude/mcp/config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"lucene\": {\n      \"command\": \"/opt/homebrew/bin/node\",\n      \"args\": [\".../MCP-Lucene-Server/mcp-shim/server.js\"],\n      \"env\": {\n        \"LUCENE_BASE_URL\": \"http://localhost:8080/mcp/v1\",\n        \"MCP_FORCE_TEXT\": \"1\"\n      }\n    }\n  }\n}\n```\nAlternatively, use the wrapper script to capture shim logs to `/tmp/mcp-lucene-shim.stderr.log`:\n```bash\ncat > .../MCP-Lucene-Server/mcp-shim/run-shim.sh <<'SH'\n#!/usr/bin/env bash\nset -euo pipefail\nexport LUCENE_BASE_URL=\"${LUCENE_BASE_URL:-http://localhost:8080/mcp/v1}\"\nexec node .../MCP-Lucene-Server/mcp-shim/server.js \\\n  2> /tmp/mcp-lucene-shim.stderr.log\nSH\nchmod +x .../MCP-Lucene-Server/mcp-shim/run-shim.sh\n```\nThen set in `~/.claude/mcp/config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"lucene\": {\n      \"command\": \".../MCP-Lucene-Server/mcp-shim/run-shim.sh\",\n      \"env\": {\n        \"LUCENE_BASE_URL\": \"http://localhost:8080/mcp/v1\",\n        \"MCP_FORCE_TEXT\": \"1\"\n      }\n    }\n  }\n}\n```\n\n### 4) Available tools\n- `lucene_status`: Get server/index status\n- `lucene_upsert`: Upsert documents\n- `lucene_query`: Query documents (with optional metadata filters)\n- `lucene_delete`: Delete by IDs\n- `lucene_list`: List documents with pagination\n\n### 5) Example prompts for Claude Desktop\n- Run `lucene_status`\n- Run `lucene_list` with: `{ \"limit\": 10, \"offset\": 0 }`\n- Run `lucene_upsert` with: `{\"documents\":[{\"id\":\"doc-1\",\"text\":\"hello world\",\"metadata\":{\"lang\":\"en\"}}]}`\n- Run `lucene_query` with: `{\"queries\":[{\"query\":\"hello\",\"top_k\":5}]}`\n- Run `lucene_delete` with: `{ \"ids\": [\"doc-1\"] }`\n\n### 6) Troubleshooting\n- Verify the API returns JSON:\n```bash\ncurl -i http://localhost:8080/mcp/v1/status\n```\n- If Claude shows \"unsupported format\", start the shim with text-only output:\n```bash\nMCP_FORCE_TEXT=1 LUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n```\n- View shim logs (when using wrapper):\n```bash\ntail -n +1 /tmp/mcp-lucene-shim.stderr.log\n```\n- Ensure the paths in your `config.json` are absolute and correct, then restart Claude Desktop.\n\n\n### API Endpoints (for Curl)\n\nThe server provides the following API endpoints:\n\n* `GET /mcp/v1/status`\n\n    * Returns the status of the server.\n\n* `POST /mcp/v1/upsert`\n\n    * Upserts (inserts or updates) one or more documents.\n\n    * Request body:\n\n        ```json\n        {\n          \"documents\": [\n            {\n              \"id\": \"doc1\",\n              \"text\": \"This is the text of document 1.\",\n              \"metadata\": {\n                \"category\": \"example\",\n                \"language\": \"english\"\n              }\n            },\n            {\n              \"id\": \"doc2\",\n              \"text\": \"This is document 2's text.\",\n              \"metadata\": {\n                \"category\": \"sample\",\n                \"language\": \"spanish\"\n              }\n            }\n          ]\n        }\n        ```\n\n* `POST /mcp/v1/query`\n\n    * Queries the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n          \"queries\": [\n            {\n              \"query\": \"document\",\n              \"top_k\": 10,\n              \"filter\": {\n                \"language\": \"english\"\n              }\n            },\n             {\n              \"query\": \"text search\",\n              \"filter\": {\n                 \"category\": \"example\"\n               }\n             }\n          ]\n        }\n        ```\n\n    * `query`: The Lucene query string.\n\n    * `top_k`: (Optional) The maximum number of results to return (default: 10).\n\n    * `filter`: (Optional) A map of metadata fields and values to filter by.\n\n* `POST /mcp/v1/delete`\n\n    * Deletes documents from the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n            \"ids\": [\"doc1\", \"doc2\"]\n        }\n        ```\n\n* `GET /mcp/v1/list`\n\n    * Lists documents from the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n            \"ids\": [\"doc1\", \"doc2\"]\n        }\n        ```\n\n### Examples\n\n**Get server status:**\n\n```bash\ncurl http://localhost:8080/mcp/v1/status\n```\n\n**Upsert documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/upsert \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"documents\": [\n{\n\"id\": \"doc1\",\n\"text\": \"This is the text of document 1.\",\n\"metadata\": {\n\"category\": \"example\",\n\"language\": \"english\"\n}\n},\n{\n\"id\": \"doc2\",\n\"text\": \"This is document 2''s text.\",\n\"metadata\": {\n\"category\": \"sample\",\n\"language\": \"spanish\"\n}\n}\n]\n}'\n```\n\n**Query documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/query \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"queries\": [\n{\n\"query\": \"document text\",\n\"top_k\": 5,\n\"filter\": {\n\"language\": \"english\"\n}\n}\n]\n}'\n```\n\n**Delete documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/delete \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"ids\": [\"doc1\"]\n}'\n```\n\n**List documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/list \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"ids\": [\"doc1\", \"doc2\"]\n}'\n```\n\n## Configuration\n\nThe server can be configured using Spring Boot's application properties. Here are some of the key properties:\n\n* `server.port`: The port the server listens on (default: 8080).\n\n* `lucene.index.path`: The path to the Lucene index directory. This is where the indexed data is stored. If not set, a default location is used. It is highly recommended to configure this to a persistent storage location.\n\nYou can set these properties in an `application.properties` or `application.yml` file in your `src/main/resources` directory, or by using environment variables.\n\n**Example `application.properties`:**\n\n\nserver.port=8080\nlucene.index.path=/path/to/lucene/index\n\n## License\n\nThis project is licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lucene",
        "indexing",
        "search",
        "lucene restful",
        "apache lucene",
        "lucene server"
      ],
      "category": "web-search"
    },
    "WebChatAppAi--webchat": {
      "owner": "WebChatAppAi",
      "name": "webchat",
      "url": "https://github.com/WebChatAppAi/webchat",
      "imageUrl": "/freedevtools/mcp/pfp/WebChatAppAi.webp",
      "description": "Build interactive chat experiences using a responsive web chat application powered by Next.js, enabling real-time updates as modifications are made. Customize and enhance chat functionalities easily with integrated tools and data sources.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-31T01:55:59Z",
      "readme_content": "# WebChat Application\n\n[![Next.js](https://img.shields.io/badge/Next.js-14-black)](https://nextjs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)\n[![Prisma](https://img.shields.io/badge/Prisma-Latest-green)](https://www.prisma.io/)\n[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-Latest-cyan)](https://tailwindcss.com/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## 📌 Overview\n\nA modern real-time web chat application built with Next.js, Prisma, and NextAuth.js. This application provides secure authentication and real-time messaging capabilities in a sleek, responsive interface.\n\n## ✨ Features\n\n- Real-time messaging\n- Email-based authentication with NextAuth.js\n- SQLite database with Prisma ORM\n- Responsive UI with Tailwind CSS\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Node.js (v14 or later recommended)\n- npm or yarn\n- [Any other prerequisites]\n\n### Installation\n\n1. Clone the repository\n   ```bash\n   git clone https://github.com/WebChatAppAi/webchat.git\n   cd webchat\n   ```\n\n2. Install dependencies\n   ```bash\n   npm install\n   # or\n   yarn install\n   ```\n\n3. Create a `.env` file in the root directory with the following variables:\n\n   ```\n   # Prisma\n   DATABASE_URL=\"file:./dev.db\"\n\n   # NextAuth.js\n   # Generate a random string for NEXTAUTH_SECRET (e.g., using `openssl rand -base64 32` or an online generator)\n   NEXTAUTH_SECRET=\"your_nextauth_secret\"\n   NEXTAUTH_URL=\"http://localhost:3000\" # Replace with your deployment URL in production\n\n   # Email Provider (for NextAuth.js)\n   # For development, NextAuth will log email links to the console if you don't provide real email credentials\n   EMAIL_SERVER_HOST=\"smtp.your-email-provider.com\"\n   EMAIL_SERVER_PORT=\"465\"\n   EMAIL_SERVER_USER=\"your-email@example.com\"\n   EMAIL_SERVER_PASSWORD=\"your-email-password\"\n   EMAIL_FROM=\"your-email@example.com\"\n   ```\n\n4. Start the development server\n   ```bash\n   npm run dev\n   # or\n   yarn dev\n   ```\n\n## 🛠️ Technologies Used\n\n- **Frontend & Backend**: Next.js 14 (App Router)\n- **Language**: TypeScript\n- **Database**: SQLite with Prisma ORM\n- **Authentication**: NextAuth.js\n- **Styling**: TailwindCSS\n- **UI Components**: Shadcn UI\n\n## 📋 Project Structure\n\n```\nwebchat/\n├── prisma/            # Prisma schema and migrations\n├── public/            # Static assets\n├── src/               # Next.js application code\n│   ├── app/           # App router pages and components\n│   ├── components/    # Reusable UI components\n│   └── lib/           # Utility functions and configurations\n├── .env               # Environment variables (not committed)\n├── .gitignore         # Git ignore rules\n├── next.config.ts     # Next.js configuration\n└── README.md          # This file\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/webchat",
      "npm_downloads": 2620,
      "keywords": [
        "webchatappai",
        "webchat",
        "chat",
        "webchatappai webchat",
        "webchat build",
        "search webchatappai"
      ],
      "category": "web-search"
    },
    "XPE-7--MCP-Server": {
      "owner": "XPE-7",
      "name": "MCP-Server",
      "url": "https://github.com/XPE-7/MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/XPE-7.webp",
      "description": "Facilitates intelligent search and retrieval of AI/ML library documentation using Claude's reasoning capabilities, supporting multiple sources and an extendable architecture for adding new documentation easily.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-22T10:56:15Z",
      "readme_content": "# 🤖 Claude AI Documentation Assistant 📚\n\n<div align=\"center\">\n\n\n\n*A powerful MCP server that supercharges Claude with documentation search capabilities*\n\n[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)\n\n</div>\n\n## ✨ Features\n\n- 🔍 **Smart Documentation Search** - Search across multiple AI/ML library documentation\n- 🧠 **Claude Integration** - Seamless connection with Claude's advanced reasoning capabilities\n- 🌐 **Intelligent Web Search** - Leverages Serper API for targeted documentation lookup\n- 💨 **Fast Response Times** - Optimized for quick retrieval and processing\n- 🧩 **Extendable Architecture** - Easily add more documentation sources\n\n## 📋 Prerequisites\n\n- 🐍 Python 3.8 or higher\n- 🔑 Claude Pro subscription\n- 🔐 Serper API key ([Get one here](https://serper.dev))\n- 💻 Claude Desktop application\n\n## 🚀 Quick Start\n\n### 1️⃣ Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/your-username/claude-docs-assistant.git\ncd claude-docs-assistant\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### 2️⃣ Configuration\n\nCreate a `.env` file in the project root with your API keys:\n\n```\nSERPER_API_KEY=your_serper_api_key_here\n```\n\n### 3️⃣ Start the MCP Server\n\n```bash\npython main.py\n```\n\nYou should see output indicating the server is running and waiting for Claude to connect.\n\n### 4️⃣ Connect Claude Desktop App\n\n1. 📱 Open the Claude Desktop App\n2. ⚙️ Click on your profile icon and select \"Settings\"\n3. 🧰 Navigate to the \"Tools\" section\n4. ➕ Click \"Add Tool\"\n5. 🔗 Select \"Connect to a local tool\"\n6. 🖥️ Follow the prompts to connect to your running MCP server\n7. ✅ Confirm the connection is successful\n\n## 🎮 Using Your Claude Documentation Assistant\n\nOnce connected, you can start asking Claude questions that will trigger the documentation search. For example:\n\n```\nCould you explain how to use FAISS with LangChain? Please search the langchain documentation to help me.\n```\n\nClaude will automatically use your MCP server to:\n1. 🔍 Search for relevant documentation\n2. 📥 Retrieve the content\n3. 🧠 Process and explain the information\n\n## 🔧 Under the Hood\n\n### 📄 Code Structure\n\n```\nclaude-docs-assistant/\n├── main.py           # MCP server implementation\n├── requirements.txt  # Project dependencies\n├── .env              # Environment variables (API keys)\n└── README.md         # This documentation\n```\n\n### 🔌 Supported Libraries\n\nThe assistant currently supports searching documentation for:\n\n- 🦜 **LangChain**: `python.langchain.com/docs`\n- 🦙 **LlamaIndex**: `docs.llamaindex.ai/en/stable`\n- 🧠 **OpenAI**: `platform.openai.com/docs`\n\n### 🧩 How It Works\n\n1. 📡 The MCP server exposes a `get_docs` tool to Claude\n2. 🔍 When invoked, the tool searches for documentation using Serper API\n3. 📚 Results are scraped for their content\n4. 🔄 Content is returned to Claude for analysis and explanation\n\n## 🛠️ Advanced Configuration\n\n### Adding New Documentation Sources\n\nExtend the `docs_urls` dictionary in `main.py`:\n\n```python\ndocs_urls = {\n    \"langchain\": \"python.langchain.com/docs\",\n    \"llama-index\": \"docs.llamaindex.ai/en/stable\",\n    \"openai\": \"platform.openai.com/docs\",\n    \"huggingface\": \"huggingface.co/docs\",  # Add new documentation sources\n    \"tensorflow\": \"www.tensorflow.org/api_docs\",\n}\n```\n\n### Customizing Search Behavior\n\nModify the `search_web` function to adjust the number of results:\n\n```python\npayload = json.dumps({\"q\": query, \"num\": 5})  # Increase from default 2\n```\n\n## 🔍 Troubleshooting\n\n### Common Issues\n\n- **🚫 \"Connection refused\" error**: Ensure the MCP server is running before connecting Claude\n- **⏱️ Timeout errors**: Check your internet connection or increase the timeout value\n- **🔒 API key issues**: Verify your Serper API key is correct in the `.env` file\n\n### Debugging Tips\n\nAdd more detailed logging by modifying the main.py file:\n\n```python\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n```\n\n## 📈 Performance Optimization\n\n- ⚡ For faster response times, consider caching frequently accessed documentation\n- 🧠 Limit the amount of text returned to Claude to avoid token limitations\n- 🌐 Use more specific queries to get more relevant documentation\n\n## 🤝 Contributing\n\nContributions are welcome! Here's how you can help:\n\n1. 🍴 Fork the repository\n2. 🌿 Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. 💾 Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. 📤 Push to the branch (`git push origin feature/amazing-feature`)\n5. 🔍 Open a Pull Request\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🙏 Acknowledgements\n\n- [Anthropic](https://www.anthropic.com/) for creating Claude\n- [Serper.dev](https://serper.dev) for their search API\n- All the open-source libraries that make this project possible\n\n---\n\n<div align=\"center\">\n  Made with ❤️ for Claude enthusiasts\n</div>",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "search",
        "documentation",
        "xpe",
        "search xpe",
        "intelligent search",
        "retrieval ai"
      ],
      "category": "web-search"
    },
    "YeonwooSung--metasearch-mcp": {
      "owner": "YeonwooSung",
      "name": "metasearch-mcp",
      "url": "https://github.com/YeonwooSung/metasearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YeonwooSung.webp",
      "description": "Perform searches based on specified queries and receive AI responses along with URIs and titles of the search results. It enhances search capabilities within applications like Claude Desktop or Cursor, streamlining information retrieval.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-01T03:32:43Z",
      "readme_content": "# metasearch MCP server\n\nA MCP server for metasearch\n\n<a href=\"https://glama.ai/mcp/servers/xxb4uogn02\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xxb4uogn02/badge\" alt=\"tavily-search MCP server\" /></a>\n\n## Components\n\nThis server uses the Tavily API to perform searches based on specified queries.\n- Search results are returned in text format.\n- Search results include AI responses, URIs, and titles of the search results.\n\n### Tools\n\nThis server implements the following tools:\n- search: Performs searches based on specified queries\n  - Required argument: \"query\"\n  - Optional argument: \"search_depth\" (basic or advanced)\n\n### Install\n\n1. Download the repository.\n```bash\ngit clone https://github.com/YeonwooSung/metasearch-mcp.git\n``` \n\n2. Open the Claude Desktop configuration file.\n```\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `C:\\Users\\[username]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n```\n\n3. Edit the configuration file as follows:\n  ```yaml\n  \"mcpServers\": {\n    \"tavily-search\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\your_path\\\\mcp-server-tavily\",\n        \"run\",\n        \"tavily-search\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n  }\n  ```\n\n4. Restart Claude Desktop.\n\n## Usage\n\nIn Claude Desktop, when you ask \"Please search for something\", you will receive search results.\n\nSearch example:\n```\nPlease search in detail for today's events in Kamakura\n```\nResponse example:\n```\nAccording to the search results, the following events start today, December 1st:\n\"Kamakura Promotion Photo Contest 2025\"\nPeriod: December 1, 2024 - January 31, 2025\nA photo contest for those who love Kamakura\nApplications start accepting from today\nAlso, as a related upcoming event:\nOn December 7th, an exhibition by 12 Kamakura artists will be held at the Seibu Press Inn Kamakura Ofuna Station East Exit Lounge.\n```\n\n## Log Storage Location\n\nLogs are stored in the following location:\n\nFor Windows:\n```\nC:\\Users\\[username]\\AppData\\Roaming\\Claude\\logs\\mcp-server-tavily-search\n```\n## Execution with Cursor\n\n1. Create a shell script (e.g., `script.sh`) as shown below:\n\n```bash\n#!/bin/bash\nTARGET_DIR=/path/to/mcp-server-tavily\ncd \"${TARGET_DIR}\"\nexport TAVILY_API_KEY=\"your-api-key\"\nexport PYTHONIOENCODING=utf-8\nuv --directory $PWD run tavily-search\n```\n\n2. Configure Cursor's MCP Server settings as follows:\n\n```\nName: tavily-search\nType: command\nCommand: /path/to/your/script.sh\n```\n\n3. Save the settings.\n\n4. Once the settings are saved, you can ask Cursor's Composer-Agent to \"search for something,\" and it will return the search results.\n\n## Running in Local Environment Using Docker Compose\n\n### Purpose\nFor operating systems other than Windows/MacOS where Claude Desktop cannot be used,\nthis section explains how to set up and run an MCP server and client in a local environment\nusing Docker compose.\n\n### Steps\n1. Install Docker.\n2. Download the repository.\n```bash\ngit clone https://github.com/YeonwooSung/metasearch-mcp.git\n``` \n3. Run Docker compose.\n```bash\ndocker compose up -d\n``` \n4. Execute the client.\n```bash\ndocker exec mcp_server uv --directory /usr/src/app/mcp-server-tavily/src run client.py\n```\n5. Execution Results\n6. After searching for available tools as shown below, a query will be issued to Tavily and a response will be returned\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "metasearch",
        "searches",
        "search",
        "metasearch mcp",
        "yeonwoosung metasearch",
        "web search"
      ],
      "category": "web-search"
    },
    "bertGPT-feng--deepseek-free-api": {
      "owner": "bertGPT-feng",
      "name": "deepseek-free-api",
      "url": "https://github.com/bertGPT-feng/deepseek-free-api",
      "imageUrl": "/freedevtools/mcp/pfp/bertGPT-feng.webp",
      "description": "Provides access to conversational AI capabilities, enabling multi-turn dialogues and internet searches with zero configuration deployment. Fully compatible with ChatGPT interfaces for enhanced interaction.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "",
      "updated_at": "2025-02-28T12:23:21Z",
      "readme_content": "# DeepSeek V3 Free 服务\n\n<span>[ 中文 | <a href=\"README_EN.md\">English</a> ]</span>\n\n[![deepseek_free_api](https://img.shields.io/github/license/llm-red-team/deepseek-free-api.svg)](LICENSE)\n![deepseek_free_api](https://img.shields.io/github/stars/llm-red-team/deepseek-free-api.svg)\n![deepseek_free_api](https://img.shields.io/github/forks/llm-red-team/deepseek-free-api.svg)\n![deepseek_free_api](https://img.shields.io/docker/pulls/vinlic/deepseek-free-api.svg)\n\n# 风险警告\n\n## **近期，我们发现部分自媒体引导用户将本仓库源码或镜像部署至非个人使用渠道，并公开提供服务。此行为可能违反了DeepSeek的[《用户协议》](https://chat.deepseek.com/downloads/DeepSeek%20Terms%20of%20Use.html)。我们特此提醒，请相关自媒体和个人立即停止此类不当行为。若持续违规，DeepSeek官方将保留依法追究其法律责任的权利。**\n\n支持高速流式输出、支持多轮对话、支持联网搜索、支持R1深度思考和静默深度思考，零配置部署，多路token支持。\n\n与ChatGPT接口完全兼容。\n\n还有以下十个free-api欢迎关注：\n\nMoonshot AI（Kimi.ai）接口转API [kimi-free-api](https://github.com/LLM-Red-Team/kimi-free-api)\n\n智谱AI (智谱清言) 接口转API [glm-free-api](https://github.com/LLM-Red-Team/glm-free-api)\n\n阶跃星辰 (跃问StepChat) 接口转API [step-free-api](https://github.com/LLM-Red-Team/step-free-api)\n\n阿里通义 (Qwen) 接口转API [qwen-free-api](https://github.com/LLM-Red-Team/qwen-free-api)\n\n秘塔AI (Metaso) 接口转API [metaso-free-api](https://github.com/LLM-Red-Team/metaso-free-api)\n\n字节跳动（豆包）接口转API [doubao-free-api](https://github.com/LLM-Red-Team/doubao-free-api)\n\n字节跳动（即梦AI）接口转API [jimeng-free-api](https://github.com/LLM-Red-Team/jimeng-free-api)\n\n讯飞星火（Spark）接口转API [spark-free-api](https://github.com/LLM-Red-Team/spark-free-api)\n\nMiniMax（海螺AI）接口转API [hailuo-free-api](https://github.com/LLM-Red-Team/hailuo-free-api)\n\n聆心智能 (Emohaa) 接口转API [emohaa-free-api](https://github.com/LLM-Red-Team/emohaa-free-api)\n\n## 目录\n\n* [免责声明](#免责声明)\n* [效果示例](#效果示例)\n* [接入准备](#接入准备)\n  * [多账号接入](#多账号接入)\n* [Docker部署](#Docker部署)\n  * [Docker-compose部署](#Docker-compose部署)\n* [Render部署](#Render部署)\n* [Vercel部署](#Vercel部署)\n* [原生部署](#原生部署)\n* [推荐使用客户端](#推荐使用客户端)\n* [接口列表](#接口列表)\n  * [对话补全](#对话补全)\n  * [userToken存活检测](#userToken存活检测)\n* [注意事项](#注意事项)\n  * [Nginx反代优化](#Nginx反代优化)\n  * [Token统计](#Token统计)\n* [Star History](#star-history)\n  \n## 免责声明\n\n**逆向API是不稳定的，建议前往DeepSeek官方 https://platform.deepseek.com/ 付费使用API，避免封禁的风险。**\n\n**本组织和个人不接受任何资金捐助和交易，此项目是纯粹研究交流学习性质！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n## 效果示例\n\n### 验明正身Demo\n\n\n\n### 多轮对话Demo\n\n\n\n### 联网搜索Demo\n\n\n\n## 接入准备\n\n请确保您在中国境内或者拥有中国境内的个人计算设备，否则部署后可能因无法访问DeepSeek而无法使用。\n\n从 [DeepSeek](https://chat.deepseek.com/) 获取userToken value\n\n进入DeepSeek随便发起一个对话，然后F12打开开发者工具，从Application > LocalStorage中找到`userToken`中的value值，这将作为Authorization的Bearer Token值：`Authorization: Bearer TOKEN`\n\n\n\n### 多账号接入\n\n目前同个账号同时只能有*一路*输出，你可以通过提供多个账号的userToken value并使用`,`拼接提供：\n\n`Authorization: Bearer TOKEN1,TOKEN2,TOKEN3`\n\n每次请求服务会从中挑选一个。\n\n### 环境变量（可选）\n\n| 环境变量 | 是否必填 | 说明                               |\n|------|------|----------------------------------|\n|  DEEP_SEEK_CHAT_AUTHORIZATION   | 否    | 当配置了token 则使用token，未配置则需要在请求头中传递Authorization |\n\n## Docker部署\n\n拉取镜像并启动服务。\n\n```shell\ndocker run -it -d --init --name deepseek-free-api -p 8000:8000 -e TZ=Asia/Shanghai  vinlic/deepseek-free-api:latest\n# 或将token配置在环境变量\ndocker run -it -d --init --name deepseek-free-api -p 8000:8000 -e TZ=Asia/Shanghai -e DEEP_SEEK_CHAT_AUTHORIZATION=xxx  vinlic/deepseek-free-api:latest\n```\n\n查看服务实时日志\n\n```shell\ndocker logs -f deepseek-free-api\n```\n\n重启服务\n\n```shell\ndocker restart deepseek-free-api\n```\n\n停止服务\n\n```shell\ndocker stop deepseek-free-api\n```\n\n### Docker-compose部署\n\n```yaml\nversion: '3'\n\nservices:\n  deepseek-free-api:\n    container_name: deepseek-free-api\n    image: vinlic/deepseek-free-api:latest\n    restart: always\n    ports:\n      - \"8000:8000\"\n    environment:\n      - TZ=Asia/Shanghai\n```\n\n### Render部署\n\n**注意：部分部署区域可能无法连接deepseek，如容器日志出现请求超时或无法连接，请切换其他区域部署！**\n**注意：免费账户的容器实例将在一段时间不活动时自动停止运行，这会导致下次请求时遇到50秒或更长的延迟，建议查看[Render容器保活](https://github.com/LLM-Red-Team/free-api-hub/#Render%E5%AE%B9%E5%99%A8%E4%BF%9D%E6%B4%BB)**\n\n1. fork本项目到你的github账号下。\n\n2. 访问 [Render](https://dashboard.render.com/) 并登录你的github账号。\n\n3. 构建你的 Web Service（New+ -> Build and deploy from a Git repository -> Connect你fork的项目 -> 选择部署区域 -> 选择实例类型为Free -> Create Web Service）。\n\n4. 等待构建完成后，复制分配的域名并拼接URL访问即可。\n\n### Vercel部署\n\n**注意：Vercel免费账户的请求响应超时时间为10秒，但接口响应通常较久，可能会遇到Vercel返回的504超时错误！**\n\n请先确保安装了Node.js环境。\n\n```shell\nnpm i -g vercel --registry http://registry.npmmirror.com\nvercel login\ngit clone https://github.com/LLM-Red-Team/deepseek-free-api\ncd deepseek-free-api\nvercel --prod\n```\n\n## 原生部署\n\n请先安装好Node.js环境并且配置好环境变量，确认node命令可用。\n\n安装依赖\n\n```shell\nnpm i\n```\n\n安装PM2进行进程守护\n\n```shell\nnpm i -g pm2\n```\n\n编译构建，看到dist目录就是构建完成\n\n```shell\nnpm run build\n```\n\n启动服务\n\n```shell\npm2 start dist/index.js --name \"deepseek-free-api\"\n```\n\n查看服务实时日志\n\n```shell\npm2 logs deepseek-free-api\n```\n\n重启服务\n\n```shell\npm2 reload deepseek-free-api\n```\n\n停止服务\n\n```shell\npm2 stop deepseek-free-api\n```\n\n## 推荐使用客户端\n\n使用以下二次开发客户端接入free-api系列项目更快更简单，支持文档/图像上传！\n\n由 [Clivia](https://github.com/Yanyutin753/lobe-chat) 二次开发的LobeChat [https://github.com/Yanyutin753/lobe-chat](https://github.com/Yanyutin753/lobe-chat)\n\n由 [时光@](https://github.com/SuYxh) 二次开发的ChatGPT Web [https://github.com/SuYxh/chatgpt-web-sea](https://github.com/SuYxh/chatgpt-web-sea)\n\n## 接口列表\n\n目前支持与openai兼容的 `/v1/chat/completions` 接口，可自行使用与openai或其他兼容的客户端接入接口，或者使用 [dify](https://dify.ai/) 等线上服务接入使用。\n\n### 对话补全\n\n对话补全接口，与openai的 [chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) 兼容。\n\n**POST /v1/chat/completions**\n\nheader 需要设置 Authorization 头部：\n\n```\nAuthorization: Bearer [userToken value]\n```\n\n请求数据：\n```json\n{\n    // model名称\n    // 默认：deepseek\n    // 深度思考：deepseek-think 或 deepseek-r1\n    // 联网搜索：deepseek-search\n    // 深度思考+联网搜索：deepseek-r1-search 或 deepseek-think-search\n    // 静默模式（不输出思考过程或联网搜索结果）：deepseek-think-silent 或 deepseek-r1-silent 或 deepseek-search-silent\n    // 深度思考但思考过程使用<details>可折叠标签包裹（需要页面支持显示）：deepseek-think-fold 或 deepseek-r1-fold\n    \"model\": \"deepseek\",\n    // 默认多轮对话基于消息合并实现，某些场景可能导致能力下降且受单轮最大token数限制\n    // 如果您想获得原生的多轮对话体验，可以传入上一轮消息获得的id，来接续上下文\n    // \"conversation_id\": \"50207e56-747e-4800-9068-c6fd618374ee@2\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你是谁？\"\n        }\n    ],\n    // 如果使用流式响应请设置为true，默认false\n    \"stream\": false\n}\n```\n\n响应数据：\n```json\n{\n    \"id\": \"50207e56-747e-4800-9068-c6fd618374ee@2\",\n    \"model\": \"deepseek\",\n    \"object\": \"chat.completion\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \" 我是DeepSeek Chat，一个由深度求索公司开发的智能助手，旨在通过自然语言处理和机器学习技术来提供信息查询、对话交流和解答问题等服务。\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 1,\n        \"completion_tokens\": 1,\n        \"total_tokens\": 2\n    },\n    \"created\": 1715061432\n}\n```\n\n### userToken存活检测\n\n检测userToken是否存活，如果存活live为true，否则为false，请不要频繁（小于10分钟）调用此接口。\n\n**POST /token/check**\n\n请求数据：\n```json\n{\n    \"token\": \"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9...\"\n}\n```\n\n响应数据：\n```json\n{\n    \"live\": true\n}\n```\n\n## 注意事项\n\n### Nginx反代优化\n\n如果您正在使用Nginx反向代理deepseek-free-api，请添加以下配置项优化流的输出效果，优化体验感。\n\n```nginx\n# 关闭代理缓冲。当设置为off时，Nginx会立即将客户端请求发送到后端服务器，并立即将从后端服务器接收到的响应发送回客户端。\nproxy_buffering off;\n# 启用分块传输编码。分块传输编码允许服务器为动态生成的内容分块发送数据，而不需要预先知道内容的大小。\nchunked_transfer_encoding on;\n# 开启TCP_NOPUSH，这告诉Nginx在数据包发送到客户端之前，尽可能地发送数据。这通常在sendfile使用时配合使用，可以提高网络效率。\ntcp_nopush on;\n# 开启TCP_NODELAY，这告诉Nginx不延迟发送数据，立即发送小数据包。在某些情况下，这可以减少网络的延迟。\ntcp_nodelay on;\n# 设置保持连接的超时时间，这里设置为120秒。如果在这段时间内，客户端和服务器之间没有进一步的通信，连接将被关闭。\nkeepalive_timeout 120;\n```\n\n### Token统计\n\n由于推理侧不在deepseek-free-api，因此token不可统计，将以固定数字返回。\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=LLM-Red-Team/deepseek-free-api&type=Date)](https://star-history.com/#LLM-Red-Team/deepseek-free-api&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bertgpt",
        "dialogues",
        "chatgpt",
        "search bertgpt",
        "conversational ai",
        "dialogues internet"
      ],
      "category": "web-search"
    },
    "bilhasry-deriv--mcp-web-a11y": {
      "owner": "bilhasry-deriv",
      "name": "mcp-web-a11y",
      "url": "https://github.com/bilhasry-deriv/mcp-web-a11y",
      "imageUrl": "/freedevtools/mcp/pfp/bilhasry-deriv.webp",
      "description": "Analyze web accessibility of any URL and simulate color blindness using color matrices. The server provides detailed reporting based on the accessibility analysis results.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-08T14:11:33Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/bilhasry-deriv-mcp-web-a11y-badge.png)](https://mseep.ai/app/bilhasry-deriv-mcp-web-a11y)\n\n# Web Accessibility MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@bilhasry-deriv/mcp-web-a11y)](https://smithery.ai/server/@bilhasry-deriv/mcp-web-a11y)\n\nAn MCP (Model Context Protocol) server that provides web accessibility analysis capabilities using axe-core and Puppeteer.\n\n<a href=\"https://glama.ai/mcp/servers/mya2mkxy9a\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mya2mkxy9a/badge\" alt=\"Web Accessibility Server MCP server\" /></a>\n\n## Features\n\n- Analyze web accessibility of any URL using axe-core\n- Simulate color blindness (protanopia, deuteranopia, tritanopia) using color matrices\n- Detailed reporting of accessibility violations\n- Support for custom user agents and selectors\n- Debug logging for troubleshooting\n- Comprehensive accessibility checks based on WCAG guidelines\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- npm\n\n## Installation\n\n### Installing via Smithery\n\nTo install Web Accessibility MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@bilhasry-deriv/mcp-web-a11y):\n\n```bash\nnpx -y @smithery/cli install @bilhasry-deriv/mcp-web-a11y --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n```bash\ngit clone [repository-url]\ncd mcp-web-a11y\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n## Configuration\n\nAdd the server to your MCP settings file (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-a11y\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-web-a11y/build/index.js\"],\n      \"disabled\": false,\n      \"autoApprove\": [],\n      \"env\": {\n        \"MCP_OUTPUT_DIR\": \"/path/to/output/directory\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `MCP_OUTPUT_DIR`: Directory where screenshot outputs will be saved\n  - Required for the `simulate_colorblind` tool\n  - If not specified, defaults to './output' relative to the current working directory\n  - Must be an absolute path when configured in MCP settings\n\n## Usage\n\nThe server provides two tools: `check_accessibility` for analyzing web accessibility and `simulate_colorblind` for simulating color blindness.\n\n### Tool: check_accessibility\n\nChecks the accessibility of a given URL using axe-core.\n\n#### Parameters\n\n- `url` (required): The URL to analyze\n- `waitForSelector` (optional): CSS selector to wait for before analysis\n- `userAgent` (optional): Custom user agent string for the request\n\n#### Example Usage\n\n```typescript\n<use_mcp_tool>\n<server_name>mcp-web-a11y</server_name>\n<tool_name>check_accessibility</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"waitForSelector\": \".main-content\",\n  \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Tool: simulate_colorblind\n\nSimulates how a webpage appears to users with different types of color blindness using color matrix transformations.\n\n#### Color Blindness Types\n\nThe tool supports three types of color blindness simulation:\n\n1. **Protanopia** (red-blind) - Uses matrix:\n   ```\n   0.567, 0.433, 0\n   0.558, 0.442, 0\n   0, 0.242, 0.758\n   ```\n\n2. **Deuteranopia** (green-blind) - Uses matrix:\n   ```\n   0.625, 0.375, 0\n   0.7, 0.3, 0\n   0, 0.3, 0.7\n   ```\n\n3. **Tritanopia** (blue-blind) - Uses matrix:\n   ```\n   0.95, 0.05, 0\n   0, 0.433, 0.567\n   0, 0.475, 0.525\n   ```\n\n#### Parameters\n\n- `url` (required): The URL to capture\n- `type` (required): Type of color blindness to simulate ('protanopia', 'deuteranopia', or 'tritanopia')\n- `outputPath` (optional): Custom path for the screenshot output\n- `userAgent` (optional): Custom user agent string for the request\n\n#### Example Usage\n\n```typescript\n<use_mcp_tool>\n<server_name>mcp-web-a11y</server_name>\n<tool_name>simulate_colorblind</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"type\": \"deuteranopia\",\n  \"outputPath\": \"colorblind_simulation.png\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Response Format\n\n#### check_accessibility Response\n\n```json\n{\n  \"url\": \"analyzed-url\",\n  \"timestamp\": \"ISO-timestamp\",\n  \"violations\": [\n    {\n      \"impact\": \"serious|critical|moderate|minor\",\n      \"description\": \"Description of the violation\",\n      \"help\": \"Help text explaining the issue\",\n      \"helpUrl\": \"URL to detailed documentation\",\n      \"nodes\": [\n        {\n          \"html\": \"HTML of the affected element\",\n          \"failureSummary\": \"Summary of what needs to be fixed\"\n        }\n      ]\n    }\n  ],\n  \"passes\": 42,\n  \"inapplicable\": 45,\n  \"incomplete\": 3\n}\n```\n\n#### simulate_colorblind Response\n\n```json\n{\n  \"url\": \"analyzed-url\",\n  \"type\": \"colorblind-type\",\n  \"outputPath\": \"path/to/screenshot.png\",\n  \"timestamp\": \"ISO-timestamp\",\n  \"message\": \"Screenshot saved with [type] simulation\"\n}\n```\n\n### Error Handling\n\nThe server includes comprehensive error handling for common scenarios:\n\n- Network errors\n- Invalid URLs\n- Timeout issues\n- DNS resolution problems\n\nError responses will include detailed messages to help diagnose the issue.\n\n## Development\n\n### Project Structure\n\n```\nmcp-web-a11y/\n├── src/\n│   └── index.ts    # Main server implementation\n├── build/          # Compiled JavaScript\n├── output/         # Generated screenshots\n├── package.json    # Project dependencies and scripts\n└── tsconfig.json   # TypeScript configuration\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\nThis will:\n1. Compile TypeScript to JavaScript\n2. Make the output file executable\n3. Place the compiled files in the `build` directory\n\n### Debugging\n\nThe server includes detailed debug logging that can be observed in the console output. This includes:\n- Network requests and responses\n- Page loading status\n- Selector waiting status\n- Any console messages from the analyzed page\n- Color simulation progress\n\n## Common Issues and Solutions\n\n1. **Timeout Errors**\n   - Increase the timeout value in the code\n   - Check network connectivity\n   - Verify the URL is accessible\n\n2. **DNS Resolution Errors**\n   - Verify the URL is correct\n   - Check network connectivity\n   - Try using the www subdomain\n\n3. **Selector Not Found**\n   - Verify the selector exists on the page\n   - Wait for dynamic content to load\n   - Check the page source for the correct selector\n\n4. **Color Simulation Issues**\n   - Ensure the page's colors are specified in a supported format (RGB, RGBA, or HEX)\n   - Check if the page uses dynamic color changes (may require additional wait time)\n   - Verify the screenshot output directory exists and is writable\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "bilhasry",
        "blindness",
        "web accessibility",
        "accessibility url",
        "color blindness"
      ],
      "category": "web-search"
    },
    "billster45--mcp-chatgpt-responses": {
      "owner": "billster45",
      "name": "mcp-chatgpt-responses",
      "url": "https://github.com/billster45/mcp-chatgpt-responses",
      "imageUrl": "/freedevtools/mcp/pfp/billster45.webp",
      "description": "Access OpenAI's ChatGPT API for dynamic conversations with Claude Desktop, featuring customizable parameters and automatic conversation state management. Engage in real-time discussions between AI models with web search capabilities.",
      "stars": 13,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T03:17:10Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/billster45-mcp-chatgpt-responses-badge.png)](https://mseep.ai/app/billster45-mcp-chatgpt-responses)\n\n# MCP ChatGPT Server\n[![smithery badge](https://smithery.ai/badge/@billster45/mcp-chatgpt-responses)](https://smithery.ai/server/@billster45/mcp-chatgpt-responses)\n\nThis MCP server allows you to access OpenAI's ChatGPT API directly from Claude Desktop.\n\n📝 **Read about why I built this project**: [I Built an AI That Talks to Other AIs: Demystifying the MCP Hype](https://medium.com/@billcockerill/i-built-an-ai-that-talks-to-other-ais-demystifying-the-mcp-hype-88dc03520552)\n\n## Features\n\n- Call the ChatGPT API with customisable parameters\n- Aks Claude and ChatGPT to talk to each other in a long running discussion!\n- Configure model versions, temperature, and other parameters\n- Use web search to get up-to-date information from the internet\n- Uses OpenAI's Responses API for automatic conversation state management\n- Use your own OpenAI API key\n\n## Setup Instructions\n\n### Installing via Smithery\n\nTo install ChatGPT Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@billster45/mcp-chatgpt-responses):\n\n```bash\nnpx -y @smithery/cli install @billster45/mcp-chatgpt-responses --client claude\n```\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [Claude Desktop](https://claude.ai/download) application\n- [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)\n- [uv](https://github.com/astral-sh/uv) for Python package management\n\n### Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/billster45/mcp-chatgpt-responses.git\n   cd mcp-chatgpt-responses\n   ```\n\n2. Set up a virtual environment and install dependencies using uv:\n   ```bash\n   uv venv\n   ```\n\n   ```bash\n   .venv\\\\Scripts\\\\activate\n   ```\n   \n   ```bash\n   uv pip install -r requirements.txt\n   ```\n\n### Using with Claude Desktop\n\n1. Configure Claude Desktop to use this MCP server by following the instructions at:\n   [MCP Quickstart Guide](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server)\n\n2. Add the following configuration to your Claude Desktop config file (adjust paths as needed):\n   ```json\n   {\n     \"mcpServers\": {\n       \"chatgpt\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"\\\\path\\\\to\\\\mcp-chatgpt-responses\",\n           \"run\",\n           \"chatgpt_server.py\"\n         ],\n         \"env\": {\n           \"OPENAI_API_KEY\": \"your-api-key-here\",\n           \"DEFAULT_MODEL\": \"gpt-4o\",\n           \"DEFAULT_TEMPERATURE\": \"0.7\",\n           \"MAX_TOKENS\": \"1000\"\n         }\n       }\n     }\n   }\n   ```\n\n3. Restart Claude Desktop.\n\n4. You can now use the ChatGPT API through Claude by asking questions that mention ChatGPT or that Claude might not be able to answer.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n1. `ask_chatgpt(prompt, model, temperature, max_output_tokens, response_id)` - Send a prompt to ChatGPT and get a response\n\n2. `ask_chatgpt_with_web_search(prompt, model, temperature, max_output_tokens, response_id)` - Send a prompt to ChatGPT with web search enabled to get up-to-date information\n\n## Example Usage\n\n### Basic ChatGPT usage:\n\nTell Claude to ask ChatGPT a question!\n```\nUse the ask_chatgpt tool to answer: What is the best way to learn Python?\n```\n\nTell Claude to have a conversation with ChatGPT:\n```\nUse the ask_chatgpt tool to have a two way conversation between you and ChatGPT about the topic that is most important to you.\n```\nNote how in a turn taking conversation the response id allows ChatGPT to store the history of the conversation so its a genuine conversation and not just as series of API calls. This is called [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#openai-apis-for-conversation-state).\n\n### With web search:\n\nFor questions that may benefit from up-to-date information:\n```\nUse the ask_chatgpt_with_web_search tool to answer: What are the latest developments in quantum computing?\n```\n\nNow try web search in agentic way to plan your perfect day out based on the weather!\n```\nUse the ask_chatgpt_with_web_search tool to find the weather tomorrow in New York, then based on that weather and what it returns, keep using the tool to build up a great day out for someone who loves food and parks\n```\n\n## How It Works\n\nThis tool utilizes OpenAI's Responses API, which automatically maintains conversation state on OpenAI's servers. This approach:\n\n1. Simplifies code by letting OpenAI handle the conversation history\n2. Provides more reliable context tracking\n3. Improves the user experience by maintaining context across messages\n4. Allows access to the latest information from the web with the web search tool\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "conversations",
        "conversation",
        "openai chatgpt",
        "chatgpt api",
        "automatic conversation"
      ],
      "category": "web-search"
    },
    "bmorphism--marginalia-mcp-server": {
      "owner": "bmorphism",
      "name": "marginalia-mcp-server",
      "url": "https://github.com/bmorphism/marginalia-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Access the Marginalia Search API to perform searches for non-commercial content on the internet, allowing for configurable search parameters and result counts. It includes rate limiting protection and uses the MCP SDK for integration.",
      "stars": 4,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:54Z",
      "readme_content": "# Marginalia MCP Server\n\nAn MCP (Model Context Protocol) server that provides access to [Marginalia Search](https://search.marginalia.nu/), a search engine focused on finding non-commercial content and hidden gems of the internet.\n\n## Features\n\n- Search the web using Marginalia Search API\n- Configurable search parameters including index and result count\n- Rate limiting protection with helpful error messages\n- Built using the MCP SDK for seamless integration\n\n## Installation\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `MARGINALIA_API_KEY`: Your Marginalia Search API key (optional, defaults to public access)\n\nTo request a dedicated API key, contact: kontakt@marginalia.nu\n\n## Usage\n\nThe server provides the following MCP tool:\n\n### search\n\nSearch the web using Marginalia Search with the following parameters:\n\n- `query` (required): Search query string\n- `index` (optional): Search index number (corresponds to dropdown in main GUI)\n- `count` (optional): Number of results to return (1-100, default: 10)\n\nExample usage through MCP:\n\n```typescript\nconst result = await mcp.useTool(\"marginalia\", \"search\", {\n  query: \"interesting non-commercial websites\",\n  count: 5\n});\n```\n\n## Response Format\n\nThe search results are returned in the following format:\n\n```json\n{\n  \"query\": \"your search query\",\n  \"license\": \"license information\",\n  \"results\": [\n    {\n      \"url\": \"result url\",\n      \"title\": \"page title\",\n      \"description\": \"page description\"\n    }\n    // ... more results\n  ]\n}\n```\n\n## Error Handling\n\nThe server includes robust error handling for:\n- Rate limiting (503 responses)\n- Invalid requests\n- Network errors\n- API-specific errors\n\n## Adding Text-to-Speech Support\n\nTo enable text-to-speech capabilities using the say MCP server, add the following configuration:\n\n### For Cline (VSCode Extension)\n\nAdd to `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/Users/barton/worlds/servers/src/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n### For Claude Desktop\n\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/Users/barton/worlds/servers/src/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\nThe say MCP server provides text-to-speech capabilities with multiple voices and languages. Available tools:\n\n- `speak`: Read text aloud using specified voice and rate\n- `list_voices`: List all available text-to-speech voices\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "marginalia",
        "marginalia search",
        "marginalia mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "bneil--mcp-go-colly": {
      "owner": "bneil",
      "name": "mcp-go-colly",
      "url": "https://github.com/bneil/mcp-go-colly",
      "imageUrl": "/freedevtools/mcp/pfp/bneil.webp",
      "description": "Provides a framework for concurrent web crawling tailored for large language model applications, allowing for the extraction and structuring of web content with customizable settings for depth and domain restrictions. Integrates seamlessly with the Model Context Protocol to facilitate web data integration into LLM workflows with effective error handling and batch processing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-06-06T03:22:05Z",
      "readme_content": "# MCP Go Colly Crawler\n[![smithery badge](https://smithery.ai/badge/@bneil/mcp-go-colly)](https://smithery.ai/server/@bneil/mcp-go-colly)\n\n## Overview\nMCP Go Colly is a sophisticated web crawling framework that integrates the Model Context Protocol (MCP) with the powerful Colly web scraping library. This project aims to provide a flexible and extensible solution for extracting web content for large language model (LLM) applications.\n\n## Features\n- Concurrent web crawling with configurable depth and domain restrictions\n- MCP server integration for tool-based crawling\n- Graceful shutdown handling\n- Robust error handling and result formatting\n- Support for both single URL and batch URL crawling\n\n## Building from Source\n\n### Prerequisites\n- Go 1.21 or later\n- Make (for using Makefile commands)\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-go-colly.git\ncd mcp-go-colly\n```\n\n2. Install dependencies:\n```bash\nmake deps\n```\n\n### Building\n\nThe project includes a Makefile with several useful commands:\n\n```bash\n# Build the binary (outputs to bin/mcp-go-colly)\nmake build\n\n# Build for all platforms (Linux, Windows, macOS)\nmake build-all\n\n# Run tests\nmake test\n\n# Clean build artifacts\nmake clean\n\n# Format code\nmake fmt\n\n# Run linter\nmake lint\n```\n\nAll binaries will be generated in the `bin/` directory.\n\nThen you need to add the following configuration to the `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"web-scraper\": {\n      \"command\": \"<add path here>/mcp-go-colly/bin/mcp-go-colly\"\n    }\n  }\n}\n```\n\n## Usage\n\n### As an MCP Tool\n\nThe crawler is implemented as an MCP tool that can be called with the following parameters:\n\n```json\n{\n    \"urls\": [\"https://example.com\"],  // Single URL or array of URLs\n    \"max_depth\": 2                    // Optional: Maximum crawl depth (default: 2)\n}\n```\n\n### Example MCP Tool Call\n\n```go\nresult, err := crawlerTool.Call(ctx, mcp.CallToolRequest{\n    Params: struct{ Arguments map[string]interface{} }{\n        Arguments: map[string]interface{}{\n            \"urls\": []string{\"https://example.com\"},\n            \"max_depth\": 2,\n        },\n    },\n})\n```\n\n## Configuration Options\n- `max_depth`: Set maximum crawl depth (default: 2)\n- `urls`: Single URL string or array of URLs to crawl\n- Domain restrictions are automatically applied based on the provided URLs\n\n## Contributing\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\nMIT\n\n## Acknowledgments\n- Colly Web Scraping Framework\n- Mark3 Labs MCP Project\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawling",
        "search",
        "web",
        "web crawling",
        "web search",
        "concurrent web"
      ],
      "category": "web-search"
    },
    "bradleygolden--hexdocs-mcp": {
      "owner": "bradleygolden",
      "name": "hexdocs-mcp",
      "url": "https://github.com/bradleygolden/hexdocs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bradleygolden.webp",
      "description": "HexDocs MCP provides semantic search capabilities for Hex package documentation, enabling integration with AI applications to access and find relevant documentation efficiently. It includes an Elixir binary for processing documentation and a TypeScript server that implements the Model Context Protocol to facilitate searches.",
      "stars": 58,
      "forks": 3,
      "license": "MIT License",
      "language": "Elixir",
      "updated_at": "2025-09-26T07:52:49Z",
      "readme_content": "# HexDocs MCP\n\nHexDocs MCP is a project that provides semantic search capabilities for Hex package documentation, designed specifically for AI applications. It consists of two main components:\n\n1. An Elixir binary that downloads, processes, and generates embeddings from Hex package documentation\n2. A TypeScript server implementing the Model Context Protocol (MCP) that calls the Elixir binary to fetch and search documentation\n\n> [!CAUTION]\n> **This documentation reflects the current development state on the main branch.**\n> For documentation on the latest stable release, please see the [latest release page](https://github.com/bradleygolden/hexdocs-mcp/releases/latest) and the [latest release branch](https://github.com/bradleygolden/hexdocs-mcp/tree/v0.5.0).\n\n## Installation\n\n### MCP Client Configuration\n\nThe TypeScript MCP server implements the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) and is designed to be used by MCP-compatible clients such as Cursor, Claude Desktop App, Continue, and others. The server provides tools for semantic search of Hex documentation. For a complete list of MCP-compatible clients, see the [MCP Clients documentation](https://modelcontextprotocol.io/clients).\n\nAdd this to your client's MCP json config:\n\n```json\n{\n  \"mcpServers\": {\n    \"hexdocs-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"hexdocs-mcp@0.5.0\"\n      ]\n    }\n  }\n}\n```\n\nThis command will automatically download the elixir binaries to both fetch_docs and search documentation. While the server handles downloading the binaries, you still need Elixir and Mix installed on your system for the HexDocs fetching functionality to work properly.\n\n#### Smithery\n\nAlternatively, you can use [Smithery](https://smithery.ai/server/@bradleygolden/hexdocs-mcp) to automatically add the MCP server to your client config.\n\nFor example, for Cursor, you can use the following command:\n\n```bash\nnpx -y @smithery/cli@latest install @bradleygolden/hexdocs-mcp --client cursor\n```\n\n### Elixir Package\n\nAlternatively, you can add the hexdocs_mcp package to your project if you don't want to use the MCP server.\n\n```elixir\n{:hexdocs_mcp, \"~> 0.5.0\", only: :dev, runtime: false}\n```\n\nAnd if you use floki or any other dependencies that are marked as only available in\nanother environment, update them to be available in the `:dev` environment as well.\n\nFor example floki is commonly used in `:test`:\n\n```elixir\n{:floki, \">= 0.30.0\", only: :test}\n```\n\nBut you can update it to be available in the :dev environment:\n\n```elixir\n{:floki, \">= 0.30.0\", only: [:dev, :test]}\n```\n\n### Requirements\n\n- [Ollama](https://ollama.ai) - Required for generating embeddings\n  - Run `ollama pull mxbai-embed-large` to download the recommended embedding model\n  - Ensure Ollama is running before using the embedding features\n- Elixir 1.16+ and Erlang/OTP 26+ \n  - Installed automatically in CI environments\n  - Required locally for development\n- Mix - The Elixir build tool (comes with Elixir installation)\n- Node.js 22 or later (for the MCP server)\n\n### Breaking Change: Model Migration (v0.6.0+)\n\n**⚠️ IMPORTANT**: Version 0.6.0 introduces a breaking change with the default embedding model.\n\n**What changed**: \n- Default model changed from `nomic-embed-text` (384 dimensions) to `mxbai-embed-large` (1024 dimensions)\n- Existing embeddings are incompatible and will be cleared during upgrade\n\n**To upgrade**:\n1. Pull the new model:\n   ```bash\n   ollama pull mxbai-embed-large\n   ```\n\n2. Your existing embeddings will be automatically cleared when you first run any command\n\n3. Regenerate embeddings for your packages:\n   ```bash\n   mix hex.docs.mcp fetch_docs phoenix\n   ```\n\n**Why this change**: `mxbai-embed-large` provides significantly better semantic search quality and consistent dimensions across all platforms (Windows/macOS/Linux).\n\n## Configuration\n\n### Environment Variables\n\nThe following environment variables can be used to configure the tool:\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `HEXDOCS_MCP_PATH` | Path where data will be stored | `~/.hexdocs_mcp` |\n| `HEXDOCS_MCP_MIX_PROJECT_PATHS` | Comma-separated list of paths to mix.exs files | (none) |\n\n#### Examples:\n\n```bash\n# Set custom storage location\nexport HEXDOCS_MCP_PATH=/path/to/custom/directory\n\n# Configure common project paths to avoid specifying --project flag each time\nexport HEXDOCS_MCP_MIX_PROJECT_PATHS=\"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\n```\n\n### MCP Server Configuration\n\nYou can also configure environment variables in the MCP configuration for the server:\n\n```json\n{\n  \"mcpServers\": {\n    \"hexdocs-mcp\": {\n      \"command\": \"...\",\n      \"args\": [\n        \"...\"\n      ],\n      \"env\": {\n        \"HEXDOCS_MCP_PATH\": \"/path/to/custom/directory\",\n        \"HEXDOCS_MCP_MIX_PROJECT_PATHS\": \"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\n### AI Tooling\n\nThe MCP server can be used by any MCP-compatible AI tooling. The server will automatically fetch documentation when needed and store it in the configured data directory.\n\nNote that large packages make take time to download and process.\n\n### Elixir Package\n\nThe SQLite database for vector storage and retrieval is created automatically when needed.\n\nFetch documentation, process, and generate embeddings for a package:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix\n```\n\nFetch documentation for a specific version:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix 1.5.9\n```\n\nFetch documentation for a package using the version from your project:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix --project path/to/mix.exs\n```\n\nConfigure project paths to avoid specifying them every time:\n\n```bash\nexport HEXDOCS_MCP_MIX_PROJECT_PATHS=\"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\nmix hex.docs.mcp fetch_docs phoenix  # Will use the first path from HEXDOCS_MCP_MIX_PROJECT_PATHS\n```\n\nSearch in the existing embeddings:\n\n```bash\nmix hex.docs.mcp semantic_search phoenix --query \"channels\"\n```\n\nCheck if embeddings exist for a package:\n\n```bash\nmix hex.docs.mcp check_embeddings phoenix\nmix hex.docs.mcp check_embeddings phoenix 1.7.0\n```\n\n## Acknowledgements\n\n- [hex2text](https://github.com/mjrusso/hex2txt) - For the initial idea and as a reference\n\n## Development\n\nThis project uses [mise](https://mise.jdx.dev/) (formerly rtx) to manage development tools and tasks. Mise provides consistent tool versions and task automation across the project.\n\n### Setting Up Development Environment\n\n1. Install mise (if you don't have it already):\n   ```bash\n   # macOS with Homebrew\n   brew install mise\n   \n   # Using the installer script\n   curl https://mise.run | sh\n   ```\n\n2. Clone the repository and setup the development environment:\n   ```bash\n   git clone https://github.com/bradleygolden/hexdocs-mcp.git\n   cd hexdocs-mcp\n   mise install # Installs the right versions of Elixir and Node.js\n   ```\n\n3. Setup dependencies:\n   ```bash\n   mise build\n   ```\n\n### Development Tasks\n\nMise defines several useful development tasks:\n\n- `mise build` - Build both Elixir and TypeScript components\n- `mise test` - Run all tests\n- `mise mcp_inspect` - Start the MCP inspector for testing the server\n- `mise start_mcp_server` - Start the MCP server (primarily for debugging)\n\n### Without Mise\n\nIf you prefer not to use mise, you'll need:\n\n- Elixir 1.18.x\n- Node.js 22.x\n\nThen, you can run these commands directly:\n\n```bash\n# Instead of mise run setup_elixir\nmix setup\n\n# Instead of mise run setup_ts\nnpm install\n\n# Instead of mise run build\nmix compile --no-optional-deps --warnings-as-errors\nnpm run build\n\n# Instead of mise run test\nmix test\nmix format --check-formatted\nmix deps --check-unused\nmix deps.unlock --all\nmix deps.get\nmix test\n\n# Instead of mise run mcp_inspect\nMCP_INSPECTOR=true npx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## AI Assistant Integration\n\nThis project includes custom instructions for AI assistants to help optimize your workflow when working with Hex documentation.\n\n### Example Custom Instructions\n\nYou can find sample custom instructions in the repository:\n- [Cursor rules](.cursor/rules/hexdocs-mcp.mdc) - Custom rules for Cursor editor\n- [GitHub Copilot](.github/copilot/instructions.md) - Custom instructions for GitHub Copilot\n\n### Suggested Content\n\n```\nWhen working with Elixir projects that use Hex packages:\n\n## HexDocs MCP Workflow\n\n1. Use `search` to find relevant documentation\n2. Use `fetch` to fetch documentation for a package\n```\n\n## Release Guidelines\n\nWhen preparing a new release, please follow these guidelines to ensure consistency:\n\n### Version Management\n\n1. **SemVer Compliance**: Follow [Semantic Versioning](https://semver.org/) strictly:\n   - MAJOR: incompatible API changes\n   - MINOR: backward-compatible functionality\n   - PATCH: backward-compatible bug fixes\n\n2. **Version Synchronization**:\n   - Hex package version (in `mix.exs`) and npm package version (in `package.json`) MUST be identical\n   - Update both files when changing the version\n\n### Code Style\n\n1. **Formatting and Comments**:\n   - Follow the Elixir formatter rules defined in .formatter.exs\n   - Do not add comments to code unless strictly necessary for context\n   - Self-documenting code with clear function names is preferred\n   - Use module and function documentation (@moduledoc and @doc) instead of inline comments\n\n### Changelog Management\n\n1. **Update CHANGELOG.md**:\n   - Document all changes under the appropriate heading (Added, Changed, Fixed, etc.)\n   - Include the new version number and date\n   - Keep an [Unreleased] section for tracking current changes\n   - Follow the [Keep a Changelog](https://keepachangelog.com/) format\n\n2. **Entry Format**:\n   - Use present tense, imperative style (e.g., \"Add feature\" not \"Added feature\")\n   - Include issue/PR numbers where applicable\n   - Group related changes\n\n### Release Process\n\n1. **Before Release**:\n   - Run `mix test` to ensure all tests pass\n   - Run `mix format` to ensure code is properly formatted\n   - Verify CHANGELOG.md is updated\n\n2. **Release Commits**:\n   - Create a version bump commit that updates:\n     - mix.exs\n     - package.json\n     - CHANGELOG.md (move [Unreleased] to new version)\n   - Tag the commit with the version number (v0.1.0 format)\n\n3. **After Release**:\n   - Add a new [Unreleased] section to CHANGELOG.md\n   - Update version links at the bottom of CHANGELOG.md\n\nThese guidelines apply to both human contributors and AI assistants working on this project.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\nThis project is licensed under MIT - see the [LICENSE](https://github.com/bradleygolden/hexdocs-mcp/blob/main/LICENSE) file for details.",
      "npm_url": "https://www.npmjs.com/package/hexdocs-mcp",
      "npm_downloads": 2723,
      "keywords": [
        "hexdocs",
        "searches",
        "documentation",
        "mcp hexdocs",
        "hexdocs mcp",
        "semantic search"
      ],
      "category": "web-search"
    },
    "brightdata--brightdata-mcp": {
      "owner": "brightdata",
      "name": "brightdata-mcp",
      "url": "https://github.com/brightdata/brightdata-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/brightdata.webp",
      "description": "Access and extract structured data from public web sources without captcha challenges, enabling real-time market information retrieval and web searches. Supports ethical web scraping techniques for various applications like weather forecasting and decision-making.",
      "stars": 1387,
      "forks": 187,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T08:44:14Z",
      "readme_content": "<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/mcp-server\">\n    <img src=\"https://github.com/user-attachments/assets/c21b3f7b-7ff1-40c3-b3d8-66706913d62f\" alt=\"Bright Data Logo\">\n  </a>\n\n  <h1>The Web MCP</h1>\n  \n  <p>\n    <strong>🌐 Give your AI real-time web superpowers</strong><br/>\n    <i>Seamlessly connect LLMs to the live web without getting blocked</i>\n  </p>\n\n  <p>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/v/@brightdata/mcp?style=for-the-badge&color=blue\" alt=\"npm version\"/>\n    </a>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/dw/@brightdata/mcp?style=for-the-badge&color=green\" alt=\"npm downloads\"/>\n    </a>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/LICENSE\">\n      <img src=\"https://img.shields.io/badge/license-MIT-purple?style=for-the-badge\" alt=\"License\"/>\n    </a>\n  </p>\n\n  <p>\n    <a href=\"#-quick-start\">Quick Start</a> •\n    <a href=\"#-features\">Features</a> •\n    <a href=\"#-pricing--modes\">Pricing</a> •\n    <a href=\"#-demos\">Demos</a> •\n    <a href=\"#-documentation\">Docs</a> •\n    <a href=\"#-support\">Support</a>\n  </p>\n\n  <div>\n    <h3>🎉 <strong>Free Tier Available!</strong> 🎉</h3>\n    <p><strong>5,000 requests/month FREE</strong> <br/>\n    <sub>Perfect for prototyping and everyday AI workflows</sub></p>\n  </div>\n</div>\n\n---\n\n## 🌟 Overview\n\n**The Web MCP** is your gateway to giving AI assistants true web capabilities. No more outdated responses, no more \"I can't access real-time information\" - just seamless, reliable web access that actually works.\n\nBuilt by [Bright Data](https://brightdata.com), the world's #1 web data platform, this MCP server ensures your AI never gets blocked, rate-limited, or served CAPTCHAs.\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">✅ <strong>Works with Any LLM</strong><br/><sub>Claude, GPT, Gemini, Llama</sub></td>\n      <td align=\"center\">🛡️ <strong>Never Gets Blocked</strong><br/><sub>Enterprise-grade unblocking</sub></td>\n      <td align=\"center\">🚀 <strong>5,000 Free Requests</strong><br/><sub>Monthly</sub></td>\n      <td align=\"center\">⚡ <strong>Zero Config</strong><br/><sub>Works out of the box</sub></td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🎯 Perfect For\n\n- 🔍 **Real-time Research** - Get current prices, news, and live data\n- 🛍️ **E-commerce Intelligence** - Monitor products, prices, and availability  \n- 📊 **Market Analysis** - Track competitors and industry trends\n- 🤖 **AI Agents** - Build agents that can actually browse the web\n- 📝 **Content Creation** - Access up-to-date information for writing\n- 🎓 **Academic Research** - Gather data from multiple sources efficiently\n\n---\n\n## ⚡ Quick Start\n\n\n<summary><b>📡 Use our hosted server - No installation needed!</b></summary>\n\nPerfect for users who want zero setup. Just add this URL to your MCP client:\n\n```\nhttps://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN_HERE\n```\n\n**Setup in Claude Desktop:**\n1. Go to: Settings → Connectors → Add custom connector\n2. Name: `Bright Data Web`\n3. URL: `https://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN`\n4. Click \"Add\" and you're done! ✨\n\n\n<summary><b>Run locally on your machine</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"<your-api-token-here>\"\n      }\n    }\n  }\n}\n```\n\n\n---\n\n## 🚀 Pricing & Modes\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th width=\"33%\">⚡ Rapid Mode (Free tier)</th>\n      <th width=\"33%\">💎 Pro Mode</th>\n    </tr>\n    <tr>\n      <td align=\"center\">\n        <h3>$0/month</h3>\n        <p><strong>5,000 requests</strong></p>\n        <hr/>\n        <p>✅ Web Search<br/>\n        ✅ Scraping with Web unlocker<br/>\n        ❌ Browser Automation<br/>\n        ❌ Web data tools</p>\n        <br/>\n        <code>Default Mode</code>\n      </td>\n      <td align=\"center\">\n        <h3>Pay-as-you-go</h3>\n        <p><strong>Every thing in rapid and 60+ Advanced Tools</strong></p>\n        <hr/>\n        <p>✅ Browser Control<br/>\n        ✅ Web Data APIs<br/>\n        <br/>\n        <br/>\n        <br/>\n        <code>PRO_MODE=true</code>\n      </td>\n    </tr>\n  </table>\n</div>\n\n> **💡 Note:** Pro mode is **not included** in the free tier and incurs additional charges based on usage.\n\n---\n\n## ✨ Features\n\n### 🔥 Core Capabilities\n\n<table>\n  <tr>\n    <td>🔍 <b>Smart Web Search</b><br/>Google-quality results optimized for AI</td>\n    <td>📄 <b>Clean Markdown</b><br/>AI-ready content extraction</td>\n  </tr>\n  <tr>\n    <td>🌍 <b>Global Access</b><br/>Bypass geo-restrictions automatically</td>\n    <td>🛡️ <b>Anti-Bot Protection</b><br/>Never get blocked or rate-limited</td>\n  </tr>\n  <tr>\n    <td>🤖 <b>Browser Automation</b><br/>Control real browsers remotely (Pro)</td>\n    <td>⚡ <b>Lightning Fast</b><br/>Optimized for minimal latency</td>\n  </tr>\n</table>\n\n### 🎯 Example Queries That Just Work\n\n```yaml\n✅ \"What's Tesla's current stock price?\"\n✅ \"Find the best-rated restaurants in Tokyo right now\"\n✅ \"Get today's weather forecast for New York\"\n✅ \"What movies are releasing this week?\"\n✅ \"What are the trending topics on Twitter today?\"\n```\n\n---\n\n## 🎬 Demos\n\n> **Note:** These videos show earlier versions. New demos coming soon! 🎥\n\n<details>\n<summary><b>View Demo Videos</b></summary>\n\n### Basic Web Search Demo\nhttps://github.com/user-attachments/assets/59f6ebba-801a-49ab-8278-1b2120912e33\n\n### Advanced Scraping Demo\nhttps://github.com/user-attachments/assets/61ab0bee-fdfa-4d50-b0de-5fab96b4b91d\n\n[📺 More tutorials on YouTube →](https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md)\n\n</details>\n\n---\n\n## 🔧 Available Tools\n\n### ⚡ Rapid Mode Tools (Default - Free)\n\n| Tool | Description | Use Case |\n|------|-------------|----------|\n| 🔍 `search_engine` | Web search with AI-optimized results | Research, fact-checking, current events |\n| 📄 `scrape_as_markdown` | Convert any webpage to clean markdown | Content extraction, documentation |\n\n### 💎 Pro Mode Tools (60+ Tools)\n\n<details>\n<summary><b>Click to see all Pro tools</b></summary>\n\n| Category | Tools | Description |\n|----------|-------|-------------|\n| **Browser Control** | `scraping_browser.*` | Full browser automation |\n| **Web Data APIs** | `web_data_*` | Structured data extraction |\n| **E-commerce** | Product scrapers | Amazon, eBay, Walmart data |\n| **Social Media** | Social scrapers | Twitter, LinkedIn, Instagram |\n| **Maps & Local** | Location tools | Google Maps, business data |\n\n[📚 View complete tool documentation →](https://github.com/brightdata-com/brightdata-mcp/blob/main/assets/Tools.md)\n\n</details>\n\n---\n\n## 🎮 Try It Now!\n\n### 🧪 Online Playground\nTry the Web MCP without any setup:\n\n<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/playground-chat\">\n    <img src=\"https://img.shields.io/badge/Try_on-Playground-00C7B7?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMyA3VjE3TDEyIDIyTDIxIDE3VjdMMTIgMloiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8L3N2Zz4=\" alt=\"Playground\"/>\n  </a>\n</div>\n\n---\n\n## 🔧 Configuration\n\n### Basic Setup\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Configuration\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\",\n        \"PRO_MODE\": \"true\",              // Enable all 60+ tools\n        \"RATE_LIMIT\": \"100/1h\",          // Custom rate limiting\n        \"WEB_UNLOCKER_ZONE\": \"custom\",   // Custom unlocker zone\n        \"BROWSER_ZONE\": \"custom_browser\" // Custom browser zone\n      }\n    }\n  }\n}\n```\n\n---\n\n## 📚 Documentation\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <img src=\"https://img.shields.io/badge/📖-API_Docs-blue?style=for-the-badge\" alt=\"API Docs\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/examples\">\n          <img src=\"https://img.shields.io/badge/💡-Examples-green?style=for-the-badge\" alt=\"Examples\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/CHANGELOG.md\">\n          <img src=\"https://img.shields.io/badge/📝-Changelog-orange?style=for-the-badge\" alt=\"Changelog\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://brightdata.com/blog/ai/web-scraping-with-mcp\">\n          <img src=\"https://img.shields.io/badge/📚-Tutorial-purple?style=for-the-badge\" alt=\"Tutorial\"/>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🚨 Common Issues & Solutions\n\n<details>\n<summary><b>🔧 Troubleshooting Guide</b></summary>\n\n### ❌ \"spawn npx ENOENT\" Error\n**Solution:** Install Node.js or use the full path to node:\n```json\n\"command\": \"/usr/local/bin/node\"  // macOS/Linux\n\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"  // Windows\n```\n\n### ⏱️ Timeouts on Complex Sites\n**Solution:** Increase timeout in your client settings to 180s\n\n### 🔑 Authentication Issues\n**Solution:** Ensure your API token is valid and has proper permissions\n\n### 📡 Remote Server Connection\n**Solution:** Check your internet connection and firewall settings\n\n[More troubleshooting →](https://github.com/brightdata-com/brightdata-mcp#troubleshooting)\n\n</details>\n\n---\n\n## 🤝 Contributing\n\nWe love contributions! Here's how you can help:\n\n- 🐛 [Report bugs](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 💡 [Suggest features](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 🔧 [Submit PRs](https://github.com/brightdata-com/brightdata-mcp/pulls)\n- ⭐ Star this repo!\n\nPlease follow [Bright Data's coding standards](https://brightdata.com/dna/js_code).\n\n---\n\n## 📞 Support\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/issues\">\n          <strong>🐛 GitHub Issues</strong><br/>\n          <sub>Report bugs & features</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <strong>📚 Documentation</strong><br/>\n          <sub>Complete guides</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"mailto:support@brightdata.com\">\n          <strong>✉️ Email</strong><br/>\n          <sub>support@brightdata.com</sub>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 📜 License\n\nMIT © [Bright Data Ltd.](https://brightdata.com)\n\n---\n\n<div align=\"center\">\n  <p>\n    <strong>Built with ❤️ by</strong><br/>\n    <a href=\"https://brightdata.com\">\n      <img src=\"https://idsai.net.technion.ac.il/files/2022/01/Logo-600.png\" alt=\"Bright Data\" height=\"30\"/>\n    </a>\n  </p>\n  <p>\n    <sub>The world's #1 web data platform</sub>\n  </p>\n  \n  <br/>\n  \n  <p>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp\">⭐ Star us on GitHub</a> • \n    <a href=\"https://brightdata.com/blog\">Read our Blog</a>\n  </p>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "brightdata",
        "scraping",
        "web",
        "web scraping",
        "search brightdata",
        "ethical web"
      ],
      "category": "web-search"
    },
    "bsmi021--mcp-server-webscan": {
      "owner": "bsmi021",
      "name": "mcp-server-webscan",
      "url": "https://github.com/bsmi021/mcp-server-webscan",
      "imageUrl": "/freedevtools/mcp/pfp/bsmi021.webp",
      "description": "Scan and analyze web content, fetching pages in Markdown format, extracting links, and generating sitemaps for comprehensive site analysis.",
      "stars": 11,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T22:33:16Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/bsmi021-mcp-server-webscan-badge.png)](https://mseep.ai/app/bsmi021-mcp-server-webscan)\n\n# MCP Webscan Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-webscan)](https://smithery.ai/server/mcp-server-webscan)\n\nA Model Context Protocol (MCP) server for web content scanning and analysis. This server provides tools for fetching, analyzing, and extracting information from web pages.\n\n<a href=\"https://glama.ai/mcp/servers/u0tna3hemh\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u0tna3hemh/badge\" alt=\"Webscan Server MCP server\" /></a>\n\n## Features\n\n- **Page Fetching**: Convert web pages to Markdown for easy analysis\n- **Link Extraction**: Extract and analyze links from web pages\n- **Site Crawling**: Recursively crawl websites to discover content\n- **Link Checking**: Identify broken links on web pages\n- **Pattern Matching**: Find URLs matching specific patterns\n- **Sitemap Generation**: Generate XML sitemaps for websites\n\n## Installation\n\n### Installing via Smithery\n\nTo install Webscan for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-webscan):\n\n```bash\nnpx -y @smithery/cli install mcp-server-webscan --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd mcp-server-webscan\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm start\n```\n\nThe server runs on stdio transport, making it compatible with MCP clients like Claude Desktop.\n\n### Available Tools\n\n1. `fetch-page`\n   - Fetches a web page and converts it to Markdown.\n   - Parameters:\n     - `url` (required): URL of the page to fetch.\n     - `selector` (optional): CSS selector to target specific content.\n\n2. `extract-links`\n   - Extracts all links from a web page with their text.\n   - Parameters:\n     - `url` (required): URL of the page to analyze.\n     - `baseUrl` (optional): Base URL to filter links.\n     - `limit` (optional, default: 100): Maximum number of links to return.\n\n3. `crawl-site`\n   - Recursively crawls a website up to a specified depth.\n   - Parameters:\n     - `url` (required): Starting URL to crawl.\n     - `maxDepth` (optional, default: 2): Maximum crawl depth (0-5).\n\n4. `check-links`\n   - Checks for broken links on a page.\n   - Parameters:\n     - `url` (required): URL to check links for.\n\n5. `find-patterns`\n   - Finds URLs matching a specific pattern.\n   - Parameters:\n     - `url` (required): URL to search in.\n     - `pattern` (required): JavaScript-compatible regex pattern to match URLs against.\n\n6. `generate-site-map`\n   - Generates a simple XML sitemap by crawling.\n   - Parameters:\n     - `url` (required): Root URL for sitemap crawl.\n     - `maxDepth` (optional, default: 2): Maximum crawl depth for discovering URLs (0-5).\n     - `limit` (optional, default: 1000): Maximum number of URLs to include in the sitemap.\n\n## Example Usage with Claude Desktop\n\n1. Configure the server in your Claude Desktop settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"webscan\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-server-webscan/build/index.js\"], // Corrected path\n      \"env\": {\n        \"NODE_ENV\": \"development\",\n        \"LOG_LEVEL\": \"info\" // Example: Set log level via env var\n      }\n    }\n  }\n}\n```\n\n2. Use the tools in your conversations:\n\n```\nCould you fetch the content from https://example.com and convert it to Markdown?\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- npm\n\n### Project Structure (Post-Refactor)\n\n```\nmcp-server-webscan/\n├── src/\n│   ├── config/\n│   │   └── ConfigurationManager.ts\n│   ├── services/\n│   │   ├── CheckLinksService.ts\n│   │   ├── CrawlSiteService.ts\n│   │   ├── ExtractLinksService.ts\n│   │   ├── FetchPageService.ts\n│   │   ├── FindPatternsService.ts\n│   │   ├── GenerateSitemapService.ts\n│   │   └── index.ts\n│   ├── tools/\n│   │   ├── checkLinksTool.ts\n│   │   ├── checkLinksToolParams.ts\n│   │   ├── crawlSiteTool.ts\n│   │   ├── crawlSiteToolParams.ts\n│   │   ├── extractLinksTool.ts\n│   │   ├── extractLinksToolParams.ts\n│   │   ├── fetchPageTool.ts\n│   │   ├── fetchPageToolParams.ts\n│   │   ├── findPatterns.ts\n│   │   ├── findPatternsToolParams.ts\n│   │   ├── generateSitemapTool.ts\n│   │   ├── generateSitemapToolParams.ts\n│   │   └── index.ts\n│   ├── types/\n│   │   ├── checkLinksTypes.ts\n│   │   ├── crawlSiteTypes.ts\n│   │   ├── extractLinksTypes.ts\n│   │   ├── fetchPageTypes.ts\n│   │   ├── findPatternsTypes.ts\n│   │   ├── generateSitemapTypes.ts\n│   │   └── index.ts\n│   ├── utils/\n│   │   ├── errors.ts\n│   │   ├── index.ts\n│   │   ├── logger.ts\n│   │   ├── markdownConverter.ts\n│   │   └── webUtils.ts\n│   ├── initialize.ts\n│   └── index.ts    # Main server entry point\n├── build/          # Compiled JavaScript (Corrected)\n├── node_modules/\n├── .clinerules\n├── .gitignore\n├── Dockerfile\n├── LICENSE\n├── mcp-consistant-servers-guide.md\n├── package.json\n├── package-lock.json\n├── README.md\n├── RFC-2025-001-Refactor.md\n├── smithery.yaml\n└── tsconfig.json\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Development Mode\n\n```bash\nnpm run dev\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/tools/extractLinksTool.ts\n```\n## Error Handling\n\nThe server implements comprehensive error handling:\n\n- Invalid parameters\n- Network errors\n- Content parsing errors\n- URL validation\n\nAll errors are properly formatted according to the MCP specification.\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nMIT License - see the LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webscan",
        "sitemaps",
        "web",
        "webscan scan",
        "server webscan",
        "site analysis"
      ],
      "category": "web-search"
    },
    "btwiuse--npm-search-mcp-server": {
      "owner": "btwiuse",
      "name": "npm-search-mcp-server",
      "url": "https://github.com/btwiuse/npm-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/btwiuse.webp",
      "description": "Search for npm packages using the `npm search` command to find relevant libraries and tools from the npm registry.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T14:17:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/btwiuse-npm-search-mcp-server-badge.png)](https://mseep.ai/app/btwiuse-npm-search-mcp-server)\n\n# npm-search MCP Server\n[![smithery badge](https://smithery.ai/badge/npm-search-mcp-server)](https://smithery.ai/server/npm-search-mcp-server)\n\nA Model Context Protocol server that allows you to search for npm packages by calling the `npm search` command.\n\n<a href=\"https://glama.ai/mcp/servers/yeb3luefvf\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/yeb3luefvf/badge\" alt=\"npm-search-mcp-server MCP server\" /></a>\n\n### Available Tools\n\n- `search_npm_packages` - Search for npm packages.\n  - Required arguments:\n    - `query` (string): The search query.\n\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install npm-search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/npm-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install npm-search-mcp-server --client claude\n```\n\n### Using NPM (recommended)\n\nAlternatively you can install `npm-search-mcp-server` via npm:\n\n```bash\nnpm install -g npm-search-mcp-server\n```\n\nAfter installation, you can run it as a command using:\n\n```bash\nnpm-search-mcp-server\n```\n\n### Using uv\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *npm-search-mcp-server*.\n\n## Configuration\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n<details>\n<summary>Using npm installation</summary>\n\n```json\n\"mcpServers\": {\n  \"npm-search\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"npm-search-mcp-server\"]\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"npm-search\": {\n    \"command\": \"uvx\",\n    \"args\": [\"npm-search-mcp-server\"]\n  }\n}\n```\n</details>\n\n### Configure for Zed\n\nAdd to your Zed settings.json:\n\n<details>\n<summary>Using npm installation</summary>\n\n```json\n\"context_servers\": {\n  \"npm-search-mcp-server\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"npm-search-mcp-server\"]\n  }\n},\n```\n</details>\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"npm-search-mcp-server\": {\n    \"command\": \"uvx\",\n    \"args\": [\"npm-search-mcp-server\"]\n  }\n],\n```\n</details>\n\n## Example Interactions\n\n1. Search for npm packages:\n```json\n{\n  \"name\": \"search_npm_packages\",\n  \"arguments\": {\n    \"query\": \"express\"\n  }\n}\n```\nResponse:\n```json\n{\n  \"results\": [\n    {\n      \"name\": \"express\",\n      \"description\": \"Fast, unopinionated, minimalist web framework\",\n      \"version\": \"4.17.1\",\n      \"author\": \"TJ Holowaychuk\",\n      \"license\": \"MIT\"\n    },\n    ...\n  ]\n}\n```\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For uvx installations:\n\n```bash\nnpx @modelcontextprotocol/inspector npx -y npm-search-mcp-server\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```bash\ncd path/to/servers/src/npm-search\nnpx @modelcontextprotocol/inspector uv run npm-search-mcp-server\n```\n\n## Examples of Questions for Claude\n\n1. \"Search for express package on npm\"\n2. \"Find packages related to react\"\n3. \"Show me npm packages for web development\"\n\n## Build\n\nDocker build:\n\n```bash\ncd src/npm-search\ndocker build -t mcp/npm-search .\n```\n\n## Contributing\n\nWe encourage contributions to help expand and improve npm-search-mcp-server. Whether you want to add new npm-related tools, enhance existing functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make npm-search-mcp-server even more powerful and useful.\n\n## License\n\nnpm-search-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/npm-search-mcp-server",
      "npm_downloads": 6100,
      "keywords": [
        "npm",
        "packages",
        "search",
        "npm packages",
        "search npm",
        "npm search"
      ],
      "category": "web-search"
    },
    "cabrit0--mcp_server_reuneMacacada": {
      "owner": "cabrit0",
      "name": "mcp_server_reuneMacacada",
      "url": "https://github.com/cabrit0/mcp_server_reuneMacacada",
      "imageUrl": "/freedevtools/mcp/pfp/cabrit0.webp",
      "description": "Generates structured learning paths by aggregating and organizing resources from the web based on specified topics, with support for web search and scraping. The server returns a standardized JSON structure for easy integration with client applications.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:32Z",
      "readme_content": "# MCP Server\n\nA server that generates Master Content Plans (MCPs) based on topics. The server aggregates resources from the web and organizes them into structured learning paths.\n\n## Features\n\n- Generate learning paths for any topic (not just technology topics)\n- Find relevant resources using web search and scraping\n- Organize resources into a logical sequence with customizable number of nodes\n- Support for multiple languages with focus on Portuguese\n- Performance optimizations for Render's free tier\n- Caching system for faster responses\n- Return a standardized JSON structure for consumption by client applications\n- **NEW**: TF-IDF based resource relevance filtering to ensure resources match the requested topic\n- **NEW**: Strategic quiz distribution across learning trees for balanced learning experiences\n- **NEW**: YouTube integration to include relevant videos in learning paths\n- **NEW**: Category system to generate more specific content for different types of topics\n- **NEW**: Asynchronous task system with real-time progress feedback to improve user experience and avoid timeouts\n- **NEW**: Enhanced caching system for improved performance and faster response times\n- **NEW**: Optimized web scraping techniques for better resource utilization\n- **NEW**: Adaptive scraping system that automatically chooses the most efficient method for each website\n- **NEW**: Puppeteer instance pool for efficient browser reuse and reduced memory usage\n\n## Tech Stack\n\n- Python 3.9+\n- FastAPI\n- Pyppeteer for JavaScript-heavy web scraping\n- Pyppeteer-stealth for avoiding detection during web scraping\n- Puppeteer instance pool for efficient browser reuse\n- DuckDuckGo Search API\n- BeautifulSoup for HTML parsing\n- scikit-learn for TF-IDF based resource relevance filtering\n- yt-dlp for YouTube video search and metadata extraction\n- Redis (optional) for distributed caching\n- msgpack for efficient data serialization\n- cachetools for advanced in-memory caching\n\n## Installation\n\n1. Clone the repository:\n\n   ```\n   git clone https://github.com/yourusername/mcp_server.git\n   cd mcp_server\n   ```\n\n2. Create a virtual environment:\n\n   ```\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install Python dependencies:\n\n   ```\n   pip install -r requirements.txt\n   ```\n\n4. Install Node.js dependencies (for the optimized scraping system):\n\n   ```\n   npm install\n   ```\n\n5. Install Chrome/Chromium for Pyppeteer (if not already installed)\n\n## Usage\n\n### Running Locally\n\n1. Start the server using the provided batch file (Windows):\n\n   ```\n   run_local.bat\n   ```\n\n   Or manually with uvicorn:\n\n   ```\n   uvicorn main:app --reload --host 0.0.0.0 --port 8000\n   ```\n\n2. Access the API at `http://localhost:8000`\n\n3. Generate an MCP by making a GET request to:\n\n   ```\n   GET /generate_mcp?topic=your_topic\n   ```\n\n4. Check the API documentation at `http://localhost:8000/docs`\n\n### Production URL\n\nThe production server is available at:\n\n```\nhttps://reunemacacada.onrender.com\n```\n\nAll endpoints documented in this README are available at both the local and production URLs.\n\n### Testing the Caching System\n\n1. Make a first request to generate an MCP (this will populate the cache):\n\n   ```\n   GET /generate_mcp?topic=python&num_nodes=15&language=pt\n   ```\n\n2. Make a second request with the same parameters (this should use the cache):\n\n   ```\n   GET /generate_mcp?topic=python&num_nodes=15&language=pt\n   ```\n\n   The second request should be significantly faster as the result will be retrieved from the cache.\n\n## Documentation\n\nDetailed documentation is available in the `docs` folder:\n\n- [API Reference](docs/api_reference.md) - Detailed API documentation\n- [Endpoints Reference](docs/endpoints_reference.md) - Complete reference of all endpoints\n- [Flutter Integration](docs/flutter_integration.md) - Guide for integrating with Flutter apps\n- [Async Tasks System](docs/async_tasks_system.md) - Documentation for the asynchronous task system\n- [Performance Improvements](docs/performance_improvements.md) - Overview of performance optimizations\n- [Caching System](docs/caching_system.md) - Documentation for the caching system\n- [Web Scraping Optimization](docs/web_scraping_optimization.md) - Details on web scraping optimizations\n\n## API Endpoints\n\n- `GET /health` - Health check endpoint\n- `GET /generate_mcp?topic={topic}&max_resources={max_resources}&num_nodes={num_nodes}&min_width={min_width}&max_width={max_width}&min_height={min_height}&max_height={max_height}&language={language}&category={category}` - Generate an MCP for the specified topic synchronously\n  - `topic` (required): The topic to generate an MCP for (minimum 3 characters)\n  - `max_resources` (optional): Maximum number of resources to include (default: 15, min: 5, max: 30)\n  - `num_nodes` (optional): Number of nodes to include in the learning path (default: 15, min: 10, max: 30)\n  - `min_width` (optional): Minimum width of the tree (nodes at first level) (default: 3, min: 2, max: 10)\n  - `max_width` (optional): Maximum width at any level of the tree (default: 5, min: 3, max: 15)\n  - `min_height` (optional): Minimum height of the tree (depth) (default: 3, min: 2, max: 8)\n  - `max_height` (optional): Maximum height of the tree (depth) (default: 7, min: 3, max: 12)\n  - `language` (optional): Language for resources (default: \"pt\")\n  - `category` (optional): Category for the topic (e.g., \"technology\", \"finance\", \"health\"). If not provided, it will be detected automatically.\n- `POST /generate_mcp_async?topic={topic}&max_resources={max_resources}&num_nodes={num_nodes}&min_width={min_width}&max_width={max_width}&min_height={min_height}&max_height={max_height}&language={language}&category={category}` - Start asynchronous generation of an MCP\n- `GET /status/{task_id}` - Check the status of an asynchronous task\n- `GET /tasks` - List all tasks\n- `POST /clear_cache?pattern={pattern}&clear_domain_cache={clear_domain_cache}` - Clear the cache based on a pattern\n  - `pattern` (optional): Pattern to match cache keys (default: \"\\*\" for all)\n  - `clear_domain_cache` (optional): Whether to also clear the domain method cache (default: false)\n- `GET /cache_stats` - Get statistics about the cache, including information about the domain method cache\n\n## Examples\n\n### Basic usage (Portuguese)\n\n```\nGET /generate_mcp?topic=python\n```\n\n### Custom number of nodes\n\n```\nGET /generate_mcp?topic=machine+learning&num_nodes=20\n```\n\n### English language\n\n```\nGET /generate_mcp?topic=javascript&language=en\n```\n\n### Specify category manually\n\n```\nGET /generate_mcp?topic=python&category=technology\n```\n\n### Full customization\n\n```\nGET /generate_mcp?topic=história+do+brasil&max_resources=20&num_nodes=25&min_width=4&max_width=8&min_height=4&max_height=8&language=pt\n```\n\n### Control tree structure\n\n```\nGET /generate_mcp?topic=machine+learning&min_width=2&max_width=4&min_height=5&max_height=10\n```\n\n### Asynchronous generation\n\n```\nPOST /generate_mcp_async?topic=inteligência+artificial&category=technology\n```\n\n### Check task status\n\n```\nGET /status/550e8400-e29b-41d4-a716-446655440000\n```\n\n### Clear cache\n\n```\nPOST /clear_cache\n```\n\n### Clear specific cache\n\n```\nPOST /clear_cache?pattern=mcp:*\n```\n\n## Performance Improvements\n\nThe MCP Server includes several performance optimizations:\n\n- **Caching System**: Results are cached to improve response times for repeated queries\n- **Asynchronous Task System**: Long-running operations are handled asynchronously\n- **Resource Filtering**: TF-IDF based filtering to select the most relevant resources\n- **Optimized Web Scraping**: Efficient web scraping techniques to reduce resource usage\n- **Adaptive Scraping System**: Automatically chooses the most efficient scraping method for each website\n- **Puppeteer Instance Pool**: Reuses browser instances to reduce memory usage and startup time\n- **Domain Method Cache**: Remembers which scraping method works best for each domain\n- **Resource Blocking**: Blocks unnecessary resources (images, stylesheets, fonts) during scraping\n\n### Performance Gains\n\n- **60-80% reduction in response time** for topics already in cache\n- **30-50% reduction in response time** for new topics\n- **40-60% reduction in memory usage** during web scraping\n- **3-5x increase in throughput** for simultaneous requests\n\n## Deployment\n\nThe server can be deployed to various platforms:\n\n### Using Docker\n\n```\ndocker build -t mcp-server .\ndocker run -p 8080:8080 mcp-server\n```\n\n### Deploying to Render, Fly.io, or other platforms\n\nFollow the platform-specific instructions for deploying a Docker container or a Python application.\n\n## License\n\n**Proprietary Software - All Rights Reserved**\n\nThis software is proprietary and confidential. Unauthorized copying, distribution, modification, public display, or public performance of this software is strictly prohibited. This software is intended for use under a paid subscription model only.\n\n© 2024 ReuneMacacada. All rights reserved.\n\nLast commit: v1.1.2 - Correção de problemas com DuckDuckGo rate limit e Puppeteer\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_server_reunemacacada",
        "search",
        "web",
        "web search",
        "learning paths",
        "resources web"
      ],
      "category": "web-search"
    },
    "cancelei--mcp_test_booking": {
      "owner": "cancelei",
      "name": "mcp_test_booking",
      "url": "https://github.com/cancelei/mcp_test_booking",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Fetch and parse content from websites using advanced web scraping techniques, including the ability to extract structured data and bypass bot detection. Supports customizable content extraction through CSS selectors and direct HTTP requests.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "mcp_test_booking",
        "search",
        "web scraping",
        "scraping techniques",
        "content extraction"
      ],
      "category": "web-search"
    },
    "captainChaozi--search-intent-mcp": {
      "owner": "captainChaozi",
      "name": "search-intent-mcp",
      "url": "https://github.com/captainChaozi/search-intent-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/captainChaozi.webp",
      "description": "Analyzes user search keywords to reveal intent and provides insights for enhancing SEO strategies. Delivers possible categories, reasoning, and related references to improve search effectiveness.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-02T07:55:22Z",
      "readme_content": "# Search Intent MCP\n\n请通过 https://aisearchintent.com 获取 API key\n\n这是一个基于 MCP (Model Context Protocol) 的搜索意图分析服务。它可以帮助分析用户搜索关键词的意图，为 SEO 分析提供支持。\n\n## 功能特点\n\n- 分析搜索关键词的意图\n- 提供可能的分类\n- 提供推理过程\n- 提供相关参考链接\n- 提供搜索建议\n\n## 使用方法\n\n### claude 安装\n\n```json\n{\n  \"mcpServers\": {\n    \"search_intent\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@search-intent/mcp\"],\n      \"env\": {\n        \"SEARCH_INTENT_API_KEY\": \"xxx\"\n      }\n    }\n  }\n}\n```\n\n## 开发\n\n```bash\n# 克隆仓库\ngit clone\n\n# 安装依赖\npnpm install\n\n# 设置环境变量\nexport SEARCH_INTENT_API_KEY=your_api_key\n```\n\n## API 使用示例\n\n服务提供了一个名为 `search_intent_analysis` 的工具，可以这样使用：\n\n```json\n{\n  \"name\": \"search_intent_analysis\",\n  \"arguments\": {\n    \"query\": \"grok3\"\n  }\n}\n```\n\n返回结果示例：\n\n```json\n{\n  \"query\": \"grok3\",\n  \"intent\": \"Information Lookup about xAI's Grok 3...\",\n  \"possibleCategories\": [\n    \"AI Model\",\n    \"Technology\",\n    \"Chatbot\",\n    \"Product Information\"\n  ],\n  \"reasoning\": \"The user is likely trying to understand...\",\n  \"references\": [\n    {\n      \"url\": \"https://example.com\",\n      \"title\": \"Example Title\"\n    }\n  ],\n  \"searchSuggestions\": [\n    \"grok3 meaning\",\n    \"grok3 search intent\",\n    \"grok3 categories\",\n    \"grok3\"\n  ]\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "seo",
        "search",
        "keywords",
        "search intent",
        "search effectiveness",
        "search keywords"
      ],
      "category": "web-search"
    },
    "chat-prompt--gpters-search-mcp-server": {
      "owner": "chat-prompt",
      "name": "gpters-search-mcp-server",
      "url": "https://github.com/chat-prompt/gpters-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/chat-prompt.webp",
      "description": "Access knowledge from the GPTers AI study community efficiently through an MCP interface, enabling targeted searches based on creation date, author, and board categories. The server integrates with various MCP clients to deliver relevant insights from community content.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-19T01:18:20Z",
      "readme_content": "# GPTers Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chat-prompt/gpters-search-mcp-server)](https://smithery.ai/server/@chat-prompt/gpters-search-mcp-server)\n\n본 MCP 서버는 GPTers AI 스터디 커뮤니티의 지식을 검색할 수 있는 MCP 서버입니다.\n\n## 소개\n\nGPTers Search MCP Server는 GPTers AI 스터디 커뮤니티의 지식을 검색하고, 이를 MCP를 통해 Claude Desktop이나 Cursor와 같은 MCP Client(Host)에게 제공합니다.  \n해당 서버는 GPTers 커뮤니티의 지식을 기반으로 한 다양한 응용 서비스에 활용될 수 있습니다.\n\n## 주요 기능\n\n- GPTers AI 스터디 커뮤니티 검색 API 연동\n- MCP 형식의 검색 결과 제공\n- 작성일시, 작성자, 게시판 기준 필터링 지원\n    - 작성일시 기준으로 검색 가능\n        - 프롬프트 예 : gpters에서 ~~를 찾아주고, 최근 1년 내의 사례 글에서 정리해줘.\n    - 작성자 기준으로 검색 가능\n        - 프롬프트 예 : gpters에서 작성자가 김태현이고 ~~ 관련한 게시물을 찾아서 정리해줘.\n    - 게시판 이름으로 검색 가능\n        - 프롬프트 예 : gpters에서 \"이미지 / 음악 / 영상\" 게시판에서 검색해서 ~~ 관련한 게시물을 찾아서 정리해줘.\n\n## 설치 방법\n\n### Smithery를 통한 설치\n\n[Smithery](https://smithery.ai/server/@chat-prompt/gpters-search-mcp-server)를 통해 GPTers Search MCP 서버를 자동으로 설치하려면 AI 클라이언트에 따라 다음 명령 중 하나를 사용하세요:\n\n> **Note**: API 키는 GPTers 커뮤니티 관리자에게 요청하여 발급받을 수 있습니다.\n\nClaude Desktop:\n\n```bash\nnpx -y @smithery/cli@latest install @chat-prompt/gpters-search-mcp-server --client claude --config '\"{\\\"apiSecretKey\\\":\\\"your_api_key\\\"}\"'\n```\n\nCursor:\n\n```bash\nnpx -y @smithery/cli@latest install @chat-prompt/gpters-search-mcp-server --client cursor --config '\"{\\\"apiSecretKey\\\":\\\"your_api_key\\\"}\"'\n```\n\n## 사용 예제\n\n### 기본 검색\n\n\n\n\n### 작성일시 기준 검색",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "community",
        "search chat",
        "gpters search",
        "searches based"
      ],
      "category": "web-search"
    },
    "chenmingkong--bilibili-mcp-server": {
      "owner": "chenmingkong",
      "name": "bilibili-mcp-server",
      "url": "https://github.com/chenmingkong/bilibili-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/chenmingkong.webp",
      "description": "Seamlessly access Bilibili's API, enabling advanced searching and data retrieval operations for users, videos, live streams, and articles, as well as fetching video comments (danmaku). Offers functionalities for general searches and precise filtering of results across various content types.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-09-07T15:31:21Z",
      "readme_content": "# Bilibili API MCP Server\r\n\r\n用于哔哩哔哩 API 的 MCP（模型上下文协议）服务器，支持多种操作。\r\n\r\n## 环境要求\r\n\r\n- [uv](https://docs.astral.sh/uv/) - 一个项目管理工具，可以很方便管理依赖。\r\n\r\n## 使用方法\r\n\r\n1. clone 本项目\r\n\r\n2. 使用 uv 安装依赖\r\n\r\n```bash\r\nuv sync\r\n```\r\n\r\n3. 在任意 mcp client 中配置本 Server\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"bilibili\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"/your-project-path/bilibili-mcp-server\",\r\n        \"run\",\r\n        \"bilibili.py\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n4. 在 client 中使用\r\n\r\n## 支持的操作\r\n\r\n支持以下操作：\r\n\r\n1. `general_search`: 基础搜索功能，使用关键词在哔哩哔哩进行搜索。\r\n2. `search_user`: 专门用于搜索哔哩哔哩用户的功能，可以按照粉丝数排序。\r\n3. `get_precise_results`: 精确搜索功能，可以过滤掉不必要的信息，支持多种搜索类型：\r\n   - 用户搜索 (`user`)：精确匹配用户名，只返回完全匹配的结果。例如搜索\"双雷\"只会返回用户名为\"双雷\"的账号信息，不会返回其他相关用户\r\n   - 视频搜索 (`video`)\r\n   - 直播搜索 (`live`)\r\n   - 专栏搜索 (`article`)\r\n返回结果包含 `exact_match` 字段，标识是否找到精确匹配的结果。\r\n4. `get_video_danmaku·`: 获取视频弹幕信息。\r\n\r\n## 如何为本项目做贡献\r\n\r\n1. Fork 本项目\r\n2. 新建分支，并在新的分支上做改动\r\n3. 提交 PR\r\n\r\n## License\r\n\r\nMIT\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "searching",
        "search",
        "web search",
        "search chenmingkong",
        "advanced searching"
      ],
      "category": "web-search"
    },
    "chenxilol--perplexity-mcp-go": {
      "owner": "chenxilol",
      "name": "perplexity-mcp-go",
      "url": "https://github.com/chenxilol/perplexity-mcp-go",
      "imageUrl": "/freedevtools/mcp/pfp/chenxilol.webp",
      "description": "Connect AI models to the Perplexity search API for real-time web searches, retrieving results with citations. Access updated information and related questions to enhance application capabilities.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-03-29T15:54:54Z",
      "readme_content": "# Perplexity Search MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Go implementation of a Perplexity Search MCP server that allows large language models (LLMs) to access the Perplexity search API through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).\n\n## Features\n\n- **perplexity_search**: Perform web searches and return results, including citations\n  - **Parameters**:\n    - `query` (string, required): The search query\n    - `search_recency_filter` (string, optional): Filter results by time (`month`, `week`, `day`, `hour`)\n    - `max_tokens` (integer, optional): Maximum number of tokens to return\n    - `temperature` (number, optional, default: 0.2): Controls randomness in response\n    - `top_p` (number, optional, default: 0.9): Nucleus sampling threshold\n    - `search_domain_filter` (array, optional): List of domains to limit search results\n    - `return_images` (boolean, optional): Include image links in results\n    - `return_related_questions` (boolean, optional): Include related questions\n    - `top_k` (number, optional, default: 0): Number of tokens for top-k filtering\n    - `stream` (boolean, optional): Stream response incrementally\n    - `presence_penalty` (number, optional, default: 0): Adjust likelihood of new topics\n    - `frequency_penalty` (number, optional, default: 1): Reduce repetition\n    - `web_search_options` (object, optional): Configuration options for web search\n\n## Setup & Usage\n\n### Installing via Smithery\n\nTo install Perplexity Search Golang for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@chenxilol/perplexity-mcp-go):\n\n```bash\nnpx -y @smithery/cli install @chenxilol/perplexity-mcp-go --client claude\n```\n\n### Prerequisites\n\n- Go 1.23 or higher\n- Perplexity API key\n\n### Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/chenxilol/perplexity-mcp-go.git\ncd perplexity-mcp-go\n```\n\n2. Build the application:\n\n```bash\ngo build -o perplexity-search-mcp\n```\n\n### Running Locally\n\n1. Set your Perplexity API key:\n\n```bash\nexport PERPLEXITY_API_KEY=\"your-api-key-here\"\n```\n\n2. Run the server:\n\n```bash\n./perplexity-search-mcp\n```\n\n### Integrating with Claude\n\n1. Copy the provided `claude_desktop_config.json` to your Claude configuration directory:\n   - Windows: `%USERPROFILE%\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Edit the configuration file to include your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-search\": {\n      \"command\": \"/path/to/perplexity-search-mcp\",\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Docker Support\n\n1. Build the Docker image:\n\n```bash\ndocker build -t perplexity-search-mcp:latest .\n```\n\n2. Run the container:\n\n```bash\ndocker run -i --rm -e PERPLEXITY_API_KEY=your-api-key-here perplexity-search-mcp:latest\n```\n\n## Example Usage\n\nOnce configured, Claude can use the perplexity_search tool via MCP to perform real-time web searches.\n\nExample search with parameters:\n```json\n{\n  \"query\": \"latest AI research developments\",\n  \"search_recency_filter\": \"week\",\n  \"temperature\": 0.5,\n  \"return_related_questions\": true,\n  \"web_search_options\": {\n    \"search_context_size\": \"high\"\n  }\n}\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n1. Verify your API key is correctly set\n2. Check network connectivity\n3. Examine stderr logs for error messages\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the MCP specification\n- [MCP-Go](https://github.com/mark3labs/mcp-go) for the Go MCP implementation\n- [Perplexity](https://www.perplexity.ai/) for their search API \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "ai",
        "perplexity search",
        "web search",
        "search api"
      ],
      "category": "web-search"
    },
    "chrishayuk--chuk-mcp-time-server": {
      "owner": "chrishayuk",
      "name": "chuk-mcp-time-server",
      "url": "https://github.com/chrishayuk/chuk-mcp-time-server",
      "imageUrl": "/freedevtools/mcp/pfp/chrishayuk.webp",
      "description": "Retrieve current times and convert between various global timezones using advanced utilities. Provides accurate and reliable time data through a microservice built with Python and asyncio.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-25T03:08:56Z",
      "readme_content": "# MCP Time Server\n\n## Overview\n\nThe MCP Time Server is a sophisticated Python-based microservice designed to provide advanced time-related utilities across different timezones. It offers robust functionality for retrieving current times and converting times between various global timezones.\n\n## Project Details\n\n- **Version**: 0.1.1\n- **Python Compatibility**: Python 3.11+\n\n## Features\n\n- **Current Time Retrieval**: Get the current time for any IANA timezone\n- **Time Zone Conversion**: Convert times between different time zones\n- **Comprehensive Validation**: Robust input validation using Pydantic models\n- **Async Server Architecture**: Built with asyncio for efficient performance\n- **Flexible Configuration**: Configurable through environment variables and config files\n\n## Dependencies\n\nCore dependencies:\n- mcp (>=1.6.0)\n- pydantic (>=2.11.2)\n- PyYAML (>=6.0.2)\n- pyz (>=0.4.3)\n\nDevelopment dependencies:\n- pytest (>=8.3.5)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.11 or higher\n- pip\n- (Optional) Virtual environment recommended\n\n### Install from PyPI\n\n```bash\npip install chuk-mcp-artifact-server\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd chuk-mcp-artifact-server\n```\n\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n```\n\n3. Install the package:\n```bash\npip install .  # Installs the package in editable mode\n```\n\n### Development Installation\n\nTo set up for development:\n```bash\npip install .[dev]  # Installs package with development dependencies\n```\n\n## Running the Server\n\n### Command-Line Interface\n\n```bash\nchuk-mcp-artifact-server\n```\n\n### Programmatic Usage\n\n```python\nfrom chuk_mcp_artifact_server.main import main\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Environment Variables\n\n- `NO_BOOTSTRAP`: Set to disable component bootstrapping\n- Other configuration options can be set in the configuration files\n\n## Available Tools\n\n### 1. Get Current Time\n\n**Input**:\n- `timezone`: IANA timezone name (e.g., 'America/New_York')\n\n**Example**:\n```python\nget_current_time('Europe/London')\n```\n\n**Returns**:\n- Current time in the specified timezone\n- Timezone details\n- Daylight Saving Time (DST) status\n\n### 2. Convert Time\n\n**Input**:\n- `source_timezone`: Source timezone (IANA format)\n- `time`: Time in HH:MM (24-hour) format\n- `target_timezone`: Target timezone (IANA format)\n\n**Example**:\n```python\nconvert_time('America/New_York', '14:30', 'Europe/Paris')\n```\n\n**Returns**:\n- Source time details\n- Target time details\n- Time difference between zones\n\n## Development\n\n### Code Formatting\n\n- Black is used for code formatting\n- isort is used for import sorting\n- Line length is set to 88 characters\n\n### Running Tests\n\n```bash\npytest\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Ensure code passes formatting and testing\n4. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n5. Push to the branch (`git push origin feature/AmazingFeature`)\n6. Open a Pull Request\n\n## License\n\n[MIT License](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "timezones",
        "microservice",
        "mcp",
        "timezones using",
        "global timezones",
        "time server"
      ],
      "category": "web-search"
    },
    "chuanmingliu--mcp-webresearch": {
      "owner": "chuanmingliu",
      "name": "mcp-webresearch",
      "url": "https://github.com/chuanmingliu/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/chuanmingliu.webp",
      "description": "Integrates real-time web research capabilities into workflows by enabling Google search, webpage content extraction, research session tracking, and screenshot capture.",
      "stars": 16,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-31T08:24:56Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webresearch",
        "web",
        "webpage",
        "mcp webresearch",
        "web research",
        "webresearch integrates"
      ],
      "category": "web-search"
    },
    "clssck--researcher-mcp": {
      "owner": "clssck",
      "name": "researcher-mcp",
      "url": "https://github.com/clssck/researcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/clssck.webp",
      "description": "Integrates with intelligent search and documentation retrieval to assist coding by discovering APIs and modernizing code. Maintains context across queries using a database, while employing advanced query processing and intelligent rate management for effective responses.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T09:03:43Z",
      "readme_content": "# MCP-researcher Server\n\nA powerful research assistant that integrates with Cline and Claude Desktop! Leverages Perplexity AI for intelligent search, documentation retrieval, API discovery, and code modernization assistance - all while you code.\n\n## Features\n\n- **Seamless Context Tracking**: Maintains conversation history in SQLite database to provide coherent responses across multiple queries\n- **Advanced Query Processing**: Uses Perplexity's Sonar models for sophisticated reasoning and detailed answers to complex questions\n- **Intelligent Rate Management**: Implements adaptive rate limiting with exponential backoff to maximize API usage without hitting limits\n- **High Performance Networking**: Optimizes API calls with connection pooling and automatic retry logic for reliable operation\n\n## Tools\n\n### 1. Search\n\nPerforms general search queries to get comprehensive information on any topic. The example shows how to use different detail levels (brief, normal, detailed) to get tailored responses.\n\n### 2. Get Documentation\n\nRetrieves documentation and usage examples for specific technologies, libraries, or APIs. The example demonstrates getting comprehensive documentation for React hooks, including best practices and common pitfalls.\n\n### 3. Find APIs\n\nDiscovers and evaluates APIs that could be integrated into a project. The example shows finding payment processing APIs with detailed analysis of features, pricing, and integration complexity.\n\n### 4. Check Deprecated Code\n\nAnalyzes code for deprecated features or patterns, providing migration guidance. The example demonstrates checking React class components and lifecycle methods for modern alternatives.\n\n## Installation\n\n### paste this part into claude directly if you want to, the ai can install it for you\n\n1. First install Node.js if not already installed (from nodejs.org)\n\n2. Clone the repo\n\n3. Install dependencies and build\n\n4. Get a Perplexity API key from [https://www.perplexity.ai/settings/api](https://www.perplexity.ai/settings/api)\n\n5. Create the MCP settings file in the appropriate location for your OS:\n\n6. To use with Claude Desktop, add the server config:\n\n7. To use with Cline, add into mcpServers:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\"[path/to/researcher-mcp/build/index.js]\"],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"pplx-...\",\n        \"PERPLEXITY_MODEL\": \"sonar-reasoning\" // you can use different models\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [],\n      \"autoApprove\": [\n        \"search\",\n        \"get_documentation\",\n        \"find_apis\",\n        \"check_deprecated_code\",\n        \"get_request_status\"\n      ]\n    }\n  }\n}\n```\n\n8. Build the server:\n   npm run build\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "retrieval",
        "queries",
        "search documentation",
        "search clssck",
        "intelligent search"
      ],
      "category": "web-search"
    },
    "code-yeongyu--perplexity-advanced-mcp": {
      "owner": "code-yeongyu",
      "name": "perplexity-advanced-mcp",
      "url": "https://github.com/code-yeongyu/perplexity-advanced-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/code-yeongyu.webp",
      "description": "Enhances query processing capabilities with advanced integration for AI models, enabling seamless connectivity between AI systems and various tools or data sources.",
      "stars": 24,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-12T19:32:55Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/code-yeongyu-perplexity-advanced-mcp-badge.png)](https://mseep.ai/app/code-yeongyu-perplexity-advanced-mcp)\n\n<div align=\"center\">\n\n# Perplexity Advanced MCP\n\n[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/code-yeongyu/perplexity-advanced-mcp)\n[![PyPI](https://img.shields.io/badge/pypi-3775A9?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/perplexity-advanced-mcp)\n[![smithery badge](https://smithery.ai/badge/@code-yeongyu/perplexity-advanced-mcp)](https://smithery.ai/server/@code-yeongyu/perplexity-advanced-mcp)\n\n[한국어](README-ko.md)\n\n</div>\n\n---\n\n## Overview\n\nPerplexity Advanced MCP is an advanced integration package that leverages the [OpenRouter](https://openrouter.ai/) and [Perplexity](https://docs.perplexity.ai/home) APIs to provide enhanced query processing capabilities. With an intuitive command-line interface and a robust API client, this package facilitates seamless interactions with AI models for both simple and complex queries.\n\n## Comparison with [perplexity-mcp](https://github.com/jsonallen/perplexity-mcp)\n\nWhile [perplexity-mcp](https://github.com/jsonallen/perplexity-mcp) provides basic web search functionality using [Perplexity](https://docs.perplexity.ai/home) AI's API, Perplexity Advanced MCP offers several additional features:\n\n- **Multi-vendor Support:** Supports both [Perplexity](https://docs.perplexity.ai/home) and [OpenRouter](https://openrouter.ai/) APIs, giving you flexibility in choosing your provider\n- **Query Type Optimization:** Distinguishes between simple and complex queries, optimizing for cost and performance\n- **File Attachment Support:** Allows including file contents as context in your queries, enabling more precise and contextual responses\n- **Enhanced Retry Logic:** Implements robust retry mechanisms for improved reliability\n\nOverall, this is the most suitable MCP for handling codebases when integrated with editors like [Cline](https://cline.bot/) or [Cursor](https://www.cursor.com/).\n\n\n## Features\n\n- **Unified API Client:** Supports both [OpenRouter](https://openrouter.ai/) and [Perplexity](https://docs.perplexity.ai/home) APIs with configurable models for handling simple and complex queries.\n- **Command-Line Interface (CLI):** Manage API key configuration and run the MCP server using [Typer](https://typer.tiangolo.com/).\n- **Advanced Query Processing:** Incorporates file attachment processing, allowing you to include contextual data in your queries.\n- **Robust Retry Mechanism:** Utilizes Tenacity for retry logic to ensure consistent and reliable API communications.\n- **Customizable Logging:** Flexible logging configuration for detailed debugging and runtime monitoring.\n\n## Optimal AI Configuration\n\nFor the best experience with AI assistants (e.g., [Cursor](https://www.cursor.com/), [Claude for Desktop](https://claude.ai/download)), I recommend adding the following configuration to your project instructions or AI rules:\n\n```xml\n<perplexity-advanced-mcp>\n    <description>\n        Perplexity is an LLM that can search the internet, gather information, and answer users' queries.\n\n        For example, let's suppose we want to find out the latest version of Python.\n        1. You would search on Google.\n        2. Then read the top two or three results directly to verify.\n\n        Perplexity does that work for you.\n\n        To answer a user's query, Perplexity searches, opens the top search results, finds information on those websites, and then provides the answer.\n\n        Perplexity can be used with two types of queries: simple and complex. Choosing the right query type to fulfill the user's request is most important.\n    </description>\n    <simple-query>\n        <description>\n            It's cheap and fast. However, it's not suitable for complex queries. On average, it's more than 10 times cheaper and 3 times faster than complex queries.\n            Use it for simple questions such as \"What is the latest version of Python?\"\n        </description>\n        <pricing>\n            $1/M input tokens\n            $1/M output tokens\n        </pricing>\n    </simple-query>\n\n    <complex-query>\n        <description>\n            It's slower and more expensive. Compared to simple queries, it's on average more than 10 times more expensive and 3 times slower.\n            Use it for more complex requests like \"Analyze the attached code to examine the current status of a specific library and create a migration plan.\"\n        </description>\n        <pricing>\n            $1/M input tokens\n            $5/M output tokens\n        </pricing>\n    </complex-query>\n\n    <instruction>\n        When reviewing the user's request, if you find anything unexpected, uncertain, or questionable, **and you think you can get answer from the internet**, do not hesitate to use the \"ask_perplexity\" tool to consult Perplexity. However, if the internet is not required to satisfy users' request, it's meaningless to ask to perplexity.\n        Since Perplexity is also an LLM, prompt engineering techniques are paramount.\n        Remember the basics of prompt engineering, such as providing clear instructions, sufficient context, and examples\n        Include as much context and relevant files as possible to smoothly fulfill the user's request. When adding files as attachments, make sure they are absolute paths.\n    </instruction>\n</perplexity-advanced-mcp>\n```\n\nThis configuration helps AI assistants better understand when and how to use the Perplexity search functionality, optimizing for both cost and performance.\n\n## Usage\n\n### Installing via Smithery\n\nTo install Perplexity Advanced MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@code-yeongyu/perplexity-advanced-mcp):\n\n```bash\nnpx -y @smithery/cli install @code-yeongyu/perplexity-advanced-mcp --client claude\n```\n\n### Quick Start with [uvx](https://docs.astral.sh/uv/guides/tools/)\n\nThe easiest way to run the MCP server is using [uvx](https://docs.astral.sh/uv/guides/tools/):\n\n```sh\nuvx perplexity-advanced-mcp -o <openrouter_api_key> # or -p <perplexity_api_key>\n```\n\nYou can also configure the API keys using environment variables:\n\n```sh\nexport OPENROUTER_API_KEY=\"your_key_here\"\n# or\nexport PERPLEXITY_API_KEY=\"your_key_here\"\n\nuvx perplexity-advanced-mcp\n```\n\nNote:\n- Providing both OpenRouter and Perplexity API keys simultaneously will result in an error\n- When both CLI arguments and environment variables are provided, CLI arguments take precedence\n\nThe CLI is built with [Typer](https://typer.tiangolo.com/), ensuring a user-friendly command-line experience.\n\n### MCP Search Tool\n\nThe package includes an MCP search tool integrated via the `ask_perplexity` function. It supports both simple and complex queries and processes file attachments to provide additional context.\n\n- **Simple Queries:** Provides fast, efficient responses.\n- **Complex Queries:** Engages in detailed reasoning and supports file attachments formatted as XML.\n\n## Configuration\n\n- **API Keys:** Configure either the `OPENROUTER_API_KEY` or `PERPLEXITY_API_KEY` through command-line options or environment variables.\n- **Model Selection:** The configuration (in `src/perplexity_advanced_mcp/config.py`) maps query types to specific models:\n  - **[OpenRouter](https://openrouter.ai/):**\n    - Simple Queries: `perplexity/sonar`\n    - Complex Queries: `perplexity/sonar-reasoning`\n  - **[Perplexity](https://docs.perplexity.ai/home):**\n    - Simple Queries: `sonar-pro`\n    - Complex Queries: `sonar-reasoning-pro`\n\n## Development Background & Philosophy\n\nThis project emerged from my personal curiosity and experimentation. Following the recent [\"vibe coding\"](https://x.com/karpathy/status/1886192184808149383) trend, over 95% of the code was written through [Cline](https://cline.bot/) + [Cursor](https://www.cursor.com/) IDE. They say \"talk is cheap, show me the code\" - well, with [Wispr Flow](https://wisprflow.ai/)'s speech-to-text magic, I literally just talked and the code showed up! Most of the development was done by me saying things like \"Write me the code for x y z, fix the bug here x y z.\" and pressing enter. Remarkably, creating this fully functional project took less than a few hours.\n\nFrom project scaffolding to file structure, everything was written and reviewed through LLM. Even the GitHub Actions workflow for PyPI publishing and the release approval process were handled through Cursor. As a human developer, my role was to:\n\n- Starting and stopping the MCP server to help AI conduct proper testing\n- Copying and providing error logs when issues occurred\n- Finding and providing [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk) documentation and examples from the internet\n- Requesting modifications for code that didn't seem correct\n\nIn today's world where many things can be automated and replaced, I hope this MCP can help developers like you who use it to discover value beyond just writing code. May this tool assist you in becoming a new era developer who can make higher-level decisions and considerations.\n\n## Development\n\nTo contribute or modify this package:\n\n### 1. **Clone the Repository:**\n\n```sh\ngh repo clone code-yeongyu/perplexity-advanced-mcp\n```\n\n### 2. **Install Dependencies:**\n\n```sh\nuv sync\n```\n\n### 3. **Contribute:**\n\nContributions are welcome! Please follow the existing code style and commit guidelines.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "ai",
        "web",
        "search code",
        "web search",
        "query processing"
      ],
      "category": "web-search"
    },
    "codyde--mcp-firecrawl-tool": {
      "owner": "codyde",
      "name": "mcp-firecrawl-tool",
      "url": "https://github.com/codyde/mcp-firecrawl-tool",
      "imageUrl": "/freedevtools/mcp/pfp/codyde.webp",
      "description": "Scrape websites to extract structured data using Firecrawl's APIs, with features for error tracking and performance monitoring. Supports gathering content in various formats and custom schema data extraction.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-19T14:56:36Z",
      "readme_content": "# MCP Firecrawl Server\n\nThis is a simple MCP server that provides tools to scrape websites and extract structured data using Firecrawl's APIs.\n\n## Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Create a `.env` file in the root directory with the following variables:\n```\nFIRECRAWL_API_TOKEN=your_token_here\nSENTRY_DSN=your_sentry_dsn_here\n```\n\n- `FIRECRAWL_API_TOKEN` (required): Your Firecrawl API token\n- `SENTRY_DSN` (optional): Sentry DSN for error tracking and performance monitoring\n\n3. Start the server:\n```bash\nnpm start\n```\n\nAlternatively, you can set environment variables directly when running the server:\n```bash\nFIRECRAWL_API_TOKEN=your_token_here npm start\n```\n\n## Features\n\n- **Website Scraping**: Extract content from websites in various formats\n- **Structured Data Extraction**: Extract specific data points based on custom schemas\n- **Error Tracking**: Integrated with Sentry for error tracking and performance monitoring\n\n## Usage\n\nThe server exposes two tools:\n1. `scrape-website`: Basic website scraping with multiple format options\n2. `extract-data`: Structured data extraction based on prompts and schemas\n\n### Tool: scrape-website\n\nThis tool scrapes a website and returns its content in the requested formats.\n\nParameters:\n- `url` (string, required): The URL of the website to scrape\n- `formats` (array of strings, optional): Array of desired output formats. Supported formats are:\n  - `\"markdown\"` (default)\n  - `\"html\"`\n  - `\"text\"`\n\nExample usage with MCP Inspector:\n```bash\n# Basic usage (defaults to markdown)\nmcp-inspector --tool scrape-website --args '{\n  \"url\": \"https://example.com\"\n}'\n\n# Multiple formats\nmcp-inspector --tool scrape-website --args '{\n  \"url\": \"https://example.com\",\n  \"formats\": [\"markdown\", \"html\", \"text\"]\n}'\n```\n\n### Tool: extract-data\n\nThis tool extracts structured data from websites based on a provided prompt and schema.\n\nParameters:\n- `urls` (array of strings, required): Array of URLs to extract data from\n- `prompt` (string, required): The prompt describing what data to extract\n- `schema` (object, required): Schema definition for the data to extract\n\nThe schema definition should be an object where keys are field names and values are types. Supported types are:\n- `\"string\"`: For text fields\n- `\"boolean\"`: For true/false fields\n- `\"number\"`: For numeric fields\n- Arrays: Specified as `[\"type\"]` where type is one of the above\n- Objects: Nested objects with their own type definitions\n\nExample usage with MCP Inspector:\n```bash\n# Basic example extracting company information\nmcp-inspector --tool extract-data --args '{\n  \"urls\": [\"https://example.com\"],\n  \"prompt\": \"Extract the company mission, whether it supports SSO, and whether it is open source.\",\n  \"schema\": {\n    \"company_mission\": \"string\",\n    \"supports_sso\": \"boolean\",\n    \"is_open_source\": \"boolean\"\n  }\n}'\n\n# Complex example with nested data\nmcp-inspector --tool extract-data --args '{\n  \"urls\": [\"https://example.com/products\", \"https://example.com/pricing\"],\n  \"prompt\": \"Extract product information including name, price, and features.\",\n  \"schema\": {\n    \"products\": [{\n      \"name\": \"string\",\n      \"price\": \"number\",\n      \"features\": [\"string\"]\n    }]\n  }\n}'\n```\n\nBoth tools will return appropriate error messages if the scraping or extraction fails and automatically log errors to Sentry if configured.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Verify your Firecrawl API token is valid\n2. Check that the URLs you're trying to scrape are accessible\n3. For complex schemas, ensure they follow the supported format\n4. Review Sentry logs for detailed error information (if configured) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firecrawl",
        "scrape",
        "apis",
        "firecrawl apis",
        "firecrawl tool",
        "scrape websites"
      ],
      "category": "web-search"
    },
    "coyaSONG--youtube-mcp-server": {
      "owner": "coyaSONG",
      "name": "youtube-mcp-server",
      "url": "https://github.com/coyaSONG/youtube-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/coyaSONG.webp",
      "description": "Interact with YouTube data to search for videos, channels, and comments, and analyze performance metrics. Retrieve transcripts and generate summaries of video content.",
      "stars": 7,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T09:03:57Z",
      "readme_content": "# YouTube MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@coyaSONG/youtube-mcp-server)](https://smithery.ai/server/@coyaSONG/youtube-mcp-server)\n\nA Model Context Protocol (MCP) server for interacting with YouTube data. This server provides resources and tools to query YouTube videos, channels, comments, and transcripts through a stdio interface.\n\n## Features\n\n- Search for YouTube videos with advanced filtering options\n- Get detailed information about specific videos and channels\n- Compare statistics across multiple videos\n- Discover trending videos by region and category\n- Analyze channel performance and video statistics\n- Retrieve video comments and transcripts/captions\n- Generate video analysis and transcript summaries\n\n## Prerequisites\n\n- Node.js (v16+)\n- YouTube Data API key\n\n## Installation\n\n### Installing via Smithery\n\nTo install YouTube MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@coyaSONG/youtube-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @coyaSONG/youtube-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/coyaSONG/youtube-mcp-server.git\n   cd youtube-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file in the root directory:\n   ```\n   YOUTUBE_API_KEY=your_youtube_api_key_here\n   PORT=3000\n   ```\n\n## Usage\n\n### Building and Running\n\n1. Build the project:\n   ```bash\n   npm run build\n   ```\n\n2. Run the server (HTTP transport):\n   ```bash\n   npm start\n   ```\n   The server will listen on port 3000 (or PORT environment variable) and accept MCP requests at `/mcp` endpoint.\n\n3. Run in development mode:\n   ```bash\n   npm run dev\n   ```\n\n4. Clean build artifacts:\n   ```bash\n   npm run clean\n   ```\n\n### HTTP Transport Migration\n\n**Migration Status**: ✅ **Complete** - Successfully migrated from STDIO to Streamable HTTP transport\n\nThis server has been updated to use the modern Streamable HTTP transport as required by Smithery hosting platform. The migration includes:\n\n- **Modern Protocol**: Uses Streamable HTTP transport (protocol version 2025-03-26)\n- **Express.js Framework**: Built on Express.js for robust HTTP handling\n- **Session Management**: Supports stateful operations with proper session tracking\n- **MCP Endpoint**: All requests handled at `/mcp` endpoint\n- **Backwards Compatibility**: Maintains full compatibility with all existing tools and resources\n- **Enhanced Performance**: Improved scalability and better error handling\n\n### Testing the Migration\n\n**Local Testing**:\n```bash\n# Start the server\nnpm start\n\n# Test with MCP Inspector\nnpx @modelcontextprotocol/inspector\n# Connect to: http://localhost:3000/mcp\n```\n\n**Smithery Integration**:\n- The server is fully compatible with Smithery's new hosting requirements\n- All existing Claude Desktop integrations will continue to work seamlessly\n- No changes required for end users\n\n## Docker Deployment\n\nThe project includes a Dockerfile for containerized deployment:\n\n```bash\n# Build the Docker image\ndocker build -t youtube-mcp-server .\n\n# Run the container with HTTP transport\ndocker run -p 3000:3000 --env-file .env youtube-mcp-server\n```\n\n**Important**: The container now exposes port 3000 for HTTP-based MCP communication instead of STDIO.\n\n## API Reference\n\n### Resources\n\n- `youtube://video/{videoId}` - Get detailed information about a specific video\n- `youtube://channel/{channelId}` - Get information about a specific channel\n- `youtube://transcript/{videoId}` - Get transcript for a specific video\n  - Optional query parameter: `?language=LANGUAGE_CODE` (e.g., `en`, `ko`, `ja`)\n\n### Tools\n\n#### Basic Tools\n- `search-videos` - Search for YouTube videos with advanced filtering options\n- `get-video-comments` - Get comments for a specific video\n- `get-video-transcript` - Get transcript for a specific video with optional language\n- `enhanced-transcript` - Advanced transcript extraction with filtering, search, and multi-video capabilities\n- `get-key-moments` - Extract key moments with timestamps from a video transcript for easier navigation\n- `get-segmented-transcript` - Divide a video transcript into segments for easier analysis\n\n#### Statistical Tools\n- `get-video-stats` - Get statistical information for a specific video\n- `get-channel-stats` - Get subscriber count, view count, and other channel statistics\n- `compare-videos` - Compare statistics across multiple videos\n\n#### Discovery Tools\n- `get-trending-videos` - Retrieve trending videos by region and category\n- `get-video-categories` - Get available video categories for a specific region\n\n#### Analysis Tools\n- `analyze-channel-videos` - Analyze performance trends of videos from a specific channel\n\n### Prompts\n\n- `video-analysis` - Generate an analysis of a YouTube video\n- `transcript-summary` - Generate a summary of a video based on its transcript with customizable length and keywords extraction\n- `segment-by-segment-analysis` - Provide detailed breakdown of content by analyzing each segment of the video\n\n## Examples\n\n### Accessing a Video Transcript\n\n```\nyoutube://transcript/dQw4w9WgXcQ\n```\n\n### Getting a Transcript in a Specific Language\n\n```\nyoutube://transcript/dQw4w9WgXcQ?language=en\n```\n\n### Using the Statistical Tools\n\n```javascript\n// Get video statistics\n{\n  \"type\": \"tool\",\n  \"name\": \"get-video-stats\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\"\n  }\n}\n\n// Compare multiple videos\n{\n  \"type\": \"tool\",\n  \"name\": \"compare-videos\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\", \"9bZkp7q19f0\"]\n  }\n}\n```\n\n### Using the Transcript Summary Prompt\n\n```javascript\n{\n  \"type\": \"prompt\",\n  \"name\": \"transcript-summary\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"language\": \"en\"\n  }\n}\n```\n\n### Using the Enhanced Transcript Tool\n\n```javascript\n// Basic multi-video transcript extraction\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\", \"9bZkp7q19f0\"],\n    \"format\": \"timestamped\"\n  }\n}\n\n// With search and time filtering\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\"],\n    \"filters\": {\n      \"timeRange\": {\n        \"start\": 60,  // Start at 60 seconds\n        \"end\": 180    // End at 180 seconds\n      },\n      \"search\": {\n        \"query\": \"never gonna\",\n        \"contextLines\": 2\n      }\n    },\n    \"format\": \"merged\"\n  }\n}\n\n// With smart segmentation for easier analysis\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\"],\n    \"filters\": {\n      \"segment\": {\n        \"count\": 5,\n        \"method\": \"smart\"  // Breaks at natural pauses\n      }\n    },\n    \"format\": \"timestamped\",\n    \"language\": \"en\"\n  }\n}\n```\n\n### Using the Enhanced Transcript Analysis Features\n\n```javascript\n// Get key moments from a video\n{\n  \"type\": \"tool\",\n  \"name\": \"get-key-moments\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"maxMoments\": \"5\"\n  }\n}\n\n// Get a segmented transcript\n{\n  \"type\": \"tool\",\n  \"name\": \"get-segmented-transcript\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"segmentCount\": \"4\"\n  }\n}\n\n// Get a segment-by-segment analysis\n{\n  \"type\": \"prompt\",\n  \"name\": \"segment-by-segment-analysis\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"segmentCount\": \"4\"\n  }\n}\n\n// Get customized transcript summary\n{\n  \"type\": \"prompt\",\n  \"name\": \"transcript-summary\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"language\": \"en\",\n    \"summaryLength\": \"detailed\",\n    \"includeKeywords\": \"true\"\n  }\n}\n```\n\n## Error Handling\n\nThe server handles various error conditions, including:\n\n- Invalid API key\n- Video or channel not found\n- Transcript not available\n- Network issues\n\n## License\n\nMIT\n\n## Acknowledgements\n\n- [Model Context Protocol TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [YouTube Data API](https://developers.google.com/youtube/v3)\n- [YouTube Captions Scraper](https://github.com/algolia/youtube-captions-scraper)",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp-server",
      "npm_downloads": 258,
      "keywords": [
        "youtube",
        "coyasong",
        "search",
        "coyasong youtube",
        "youtube data",
        "youtube mcp"
      ],
      "category": "web-search"
    },
    "cr7258--higress-ai-search-mcp-server": {
      "owner": "cr7258",
      "name": "higress-ai-search-mcp-server",
      "url": "https://github.com/cr7258/higress-ai-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cr7258.webp",
      "description": "Enhances AI model responses through real-time search results from various online search engines and integrates internal knowledge bases for improved accuracy and relevance in responses.",
      "stars": 5,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-25T06:21:06Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cr7258-higress-ai-search-mcp-server-badge.png)](https://mseep.ai/app/cr7258-higress-ai-search-mcp-server)\n\n# Higress AI-Search MCP Server\n\n## Overview\n\nA Model Context Protocol (MCP) server that provides an AI search tool to enhance AI model responses with real-time search results from various search engines through [Higress](https://higress.cn/) [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) feature.\n\n<a href=\"https://glama.ai/mcp/servers/gk0xde4wbp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/gk0xde4wbp/badge\" alt=\"Higress AI-Search Server MCP server\" />\n</a>\n\n## Demo\n\n### Cline\n\nhttps://github.com/user-attachments/assets/60a06d99-a46c-40fc-b156-793e395542bb\n\n### Claude Desktop\n\nhttps://github.com/user-attachments/assets/5c9e639f-c21c-4738-ad71-1a88cc0bcb46\n\n## Features\n\n- **Internet Search**: Google, Bing, Quark - for general web information\n- **Academic Search**: Arxiv - for scientific papers and research\n- **Internal Knowledge Search**\n\n## Prerequisites\n\n- [uv](https://github.com/astral-sh/uv) for package installation.\n- Config Higress with [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) plugin and [ai-proxy](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-proxy/README.md) plugin.\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `HIGRESS_URL`(optional): URL for the Higress service (default: `http://localhost:8080/v1/chat/completions`).\n- `MODEL`(required): LLM model to use for generating responses.\n- `INTERNAL_KNOWLEDGE_BASES`(optional): Description of internal knowledge bases.\n\n### Option 1: Using uvx\n\nUsing uvx will automatically install the package from PyPI, no need to clone the repository locally.\n\n```bash\n{\n  \"mcpServers\": {\n    \"higress-ai-search-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"higress-ai-search-mcp-server\"\n      ],\n      \"env\": {\n        \"HIGRESS_URL\": \"http://localhost:8080/v1/chat/completions\",\n        \"MODEL\": \"qwen-turbo\",\n        \"INTERNAL_KNOWLEDGE_BASES\": \"Employee handbook, company policies, internal process documents\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Using uv with local development\n\nUsing uv requires cloning the repository locally and specifying the path to the source code.\n\n```bash\n{\n  \"mcpServers\": {\n    \"higress-ai-search-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/src/higress-ai-search-mcp-server\",\n        \"run\",\n        \"higress-ai-search-mcp-server\"\n      ],\n      \"env\": {\n        \"HIGRESS_URL\": \"http://localhost:8080/v1/chat/completions\",\n        \"MODEL\": \"qwen-turbo\",\n        \"INTERNAL_KNOWLEDGE_BASES\": \"Employee handbook, company policies, internal process documents\"\n      }\n    }\n  }\n}\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "search",
        "web",
        "ai search",
        "search results",
        "ai model"
      ],
      "category": "web-search"
    },
    "cyberchitta--scrapling-fetch-mcp": {
      "owner": "cyberchitta",
      "name": "scrapling-fetch-mcp",
      "url": "https://github.com/cyberchitta/scrapling-fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/cyberchitta.webp",
      "description": "Accesses text content from bot-protected websites to retrieve documentation and reference materials. Optimized for low-volume retrieval, this server facilitates the extraction of text and HTML content that is otherwise inaccessible to AI assistants.",
      "stars": 53,
      "forks": 8,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T18:34:27Z",
      "readme_content": "# scrapling-fetch-mcp\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI version](https://img.shields.io/pypi/v/scrapling-fetch-mcp.svg)](https://pypi.org/project/scrapling-fetch-mcp/)\n\nAn MCP server that helps AI assistants access text content from websites that implement bot detection, bridging the gap between what you can see in your browser and what the AI can access.\n\n## Intended Use\n\nThis tool is optimized for low-volume retrieval of documentation and reference materials (text/HTML only) from websites that implement bot detection. It has not been designed or tested for general-purpose site scraping or data harvesting.\n\n> **Note**: This project was developed in collaboration with Claude Sonnets 3.7 and 4.5, using [LLM Context](https://github.com/cyberchitta/llm-context.py).\n\n## Installation\n\n### Requirements\n\n- Python 3.10+\n- [uv](https://github.com/astral-sh/uv) package manager\n\n### Install\n\n```bash\n# Install scrapling-fetch-mcp\nuv tool install scrapling-fetch-mcp\n\n# Install browser binaries (REQUIRED - large downloads)\nuvx --from scrapling-fetch-mcp scrapling install\n```\n\n**Important**: The browser installation downloads hundreds of MB of data and must complete before first use. If the MCP server times out on first use, the browsers may still be installing in the background. Wait a few minutes and try again.\n\n## Setup with Claude Desktop\n\nAdd this configuration to your Claude Desktop MCP settings:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapling-fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"scrapling-fetch-mcp\"]\n    }\n  }\n}\n```\n\nAfter updating the config, restart Claude Desktop.\n\n## What It Does\n\nThis MCP server provides two tools that Claude can use automatically when you ask it to fetch web content:\n\n- **Page fetching**: Retrieves complete web pages with support for pagination\n- **Pattern extraction**: Finds and extracts specific content using regex patterns\n\nThe AI decides which tool to use based on your request. You just ask naturally:\n\n```\n\"Can you fetch the docs at https://example.com/api\"\n\"Find all mentions of 'authentication' on that page\"\n\"Get me the installation instructions from their homepage\"\n```\n\n## Protection Modes\n\nThe tools support three levels of bot detection bypass:\n\n- **basic**: Fast (1-2s), works for most sites\n- **stealth**: Moderate (3-8s), handles more protection\n- **max-stealth**: Maximum (10+s), for heavily protected sites\n\nClaude automatically starts with `basic` mode and escalates if needed.\n\n## Tips for Best Results\n\n- Just ask naturally - Claude handles the technical details\n- For large pages, Claude can page through content automatically\n- For specific searches, mention what you're looking for and Claude will use pattern matching\n- The metadata returned helps Claude decide whether to page or search\n\n## Limitations\n\n- Designed for text content only (documentation, articles, references)\n- Not for high-volume scraping or data harvesting\n- May not work with sites requiring authentication\n- Performance varies by site complexity and protection level\n\nBuilt with [Scrapling](https://github.com/D4Vinci/Scrapling) for web scraping with bot detection bypass.\n\n## License\n\nApache 2.0\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cyberchitta",
        "retrieval",
        "search",
        "search cyberchitta",
        "cyberchitta scrapling",
        "content bot"
      ],
      "category": "web-search"
    },
    "dabidstudio--youtubeinsights-mcp-server": {
      "owner": "dabidstudio",
      "name": "youtubeinsights-mcp-server",
      "url": "https://github.com/dabidstudio/youtubeinsights-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dabidstudio.webp",
      "description": "Extract insights from YouTube videos, analyze transcripts, and retrieve metadata and channel information. Enable keyword-based video discovery to support content strategy development.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-07T05:32:51Z",
      "readme_content": "# YouTube Insights MCP Server\n\n\nA Model Context Protocol (MCP) server that enables insight extraction from YouTube videos, including subtitle parsing, keyword-based video discovery, and channel info retrieval.\n\n## Features\n\n- Extract transcripts from YouTube videos (multi-language)\n- Search videos by keyword and fetch metadata (views, likes, thumbnails, etc.)\n- Retrieve channel info and latest videos from any YouTube video URL\n- FastMCP-based server integration for easy deployment\n- MCP Tools for seamless agent workflows\n\n\n### Example usecases\n\n\n<details>\n<summary>Finding Trending Videos and Summarizing</summary>\n<img alt=\"60a97619_13cf_4aba_807e_0fad0a4f3b42\" src=\"https://github.com/user-attachments/assets/60a97619-13cf-4aba-807e-0fad0a4f3b42\" width=\"480\"/>\n</details>\n \n<details>\n<summary>Analyzing a Channel's Recent Performance</summary>\n<img alt=\"4f35a716_0c92_4368_8ba5_0b564613aae0\" src=\"https://github.com/user-attachments/assets/4f35a716-0c92-4368-8ba5-0b564613aae0\" width=\"480\"/>\n</details>\n\n\n\n## Installation\n\n\n### Installing via Smithery\n\nTo install youtubeinsights-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dabidstudio/youtubeinsights-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @dabidstudio/youtubeinsights-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `claude_desktop_config.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"youtubeinsights\": {\n      \"command\": \"uvx\",\n      \"args\": [\"youtubeinsights-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your-api-key\",\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n\n2. Copy `.env.example` to `.env` and fill in your youtube data api credentials\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtubeinsights\": {\n          \"command\": \"uv\",\n          \"args\": [\n            \"--directory\",\n            \"path/to/youtubeinsights-mcp-server\",\n            \"run\",\n            \"youtubeinsights-mcp-server\"\n          ],\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"your-api-key\",\n          }\n        }\n      }\n    }\n    ```\n\n## Available MCP Tools\n\n- `get_youtube_transcript`: Extract full transcript (subtitles) from a YouTube video URL (supports `ko`, `en`)\n- `search_youtube_videos`: Search for videos on YouTube by keyword and retrieve key metadata\n- `get_channel_info`: Get channel metadata and recent uploads based on any YouTube video URL\n\n## Sample MCP Tool Descriptions\n\n```json\n{\n  \"tool\": \"get_youtube_transcript\",\n  \"description\": \"Extract subtitles from a given YouTube video URL.\"\n}\n```\n\n```json\n{\n  \"tool\": \"search_youtube_videos\",\n  \"description\": \"Search videos by keyword and return metadata including views, likes, and thumbnails.\"\n}\n```\n\n```json\n{\n  \"tool\": \"get_channel_info\",\n  \"description\": \"Retrieve channel info (title, subscriber count, latest uploads) based on a video URL.\"\n}\n```\n\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtubeinsights",
        "youtube",
        "videos",
        "youtubeinsights mcp",
        "dabidstudio youtubeinsights",
        "insights youtube"
      ],
      "category": "web-search"
    },
    "ddkang1--ddg-mcp": {
      "owner": "ddkang1",
      "name": "ddg-mcp",
      "url": "https://github.com/ddkang1/ddg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ddkang1.webp",
      "description": "Provides access to DuckDuckGo's search capabilities, including text, image, news, and video search. Summarizes search results and facilitates engaging conversations through AI chat.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T02:02:28Z",
      "readme_content": "# ddg-mcp MCP server\n\nDuckDuckGo search API MCP - A server that provides DuckDuckGo search capabilities through the Model Context Protocol.\n\n## Components\n\n### Prompts\n\nThe server provides the following prompts:\n- **search-results-summary**: Creates a summary of DuckDuckGo search results\n  - Required \"query\" argument for the search term\n  - Optional \"style\" argument to control detail level (brief/detailed)\n\n### Tools\n\nThe server implements the following DuckDuckGo search tools:\n\n- **ddg-text-search**: Search the web for text results using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-image-search**: Search the web for images using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"size\", \"color\", \"type_image\", \"layout\", \"license_image\", \"max_results\"\n  \n- **ddg-news-search**: Search for news articles using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-video-search**: Search for videos using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"resolution\", \"duration\", \"license_videos\", \"max_results\"\n  \n- **ddg-ai-chat**: Chat with DuckDuckGo AI\n  - Required: \"keywords\" - Message or question to send to the AI\n  - Optional: \"model\" - AI model to use (options: \"gpt-4o-mini\", \"llama-3.3-70b\", \"claude-3-haiku\", \"o3-mini\", \"mistral-small-3\")\n\n## Installation\n\n### Prerequisites\n\n- Python 3.9 or higher\n- [uv](https://github.com/astral-sh/uv) (recommended) or pip\n\n### Install from PyPI\n\n```bash\n# Using uv\nuv install ddg-mcp\n\n# Using pip\npip install ddg-mcp\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/misanthropic-ai/ddg-mcp.git\ncd ddg-mcp\n```\n\n2. Install the package:\n```bash\n# Using uv\nuv install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration\n\n### Required Dependencies\n\nThe server requires the `duckduckgo-search` package, which will be installed automatically when you install `ddg-mcp`.\n\nIf you need to install it manually:\n```bash\nuv install duckduckgo-search\n# or\npip install duckduckgo-search\n```\n\n## DuckDuckGo Search Parameters\n\n### Common Parameters\n\nThese parameters are available for most search types:\n\n- **region**: Region code for localized results (default: \"wt-wt\")\n  - Examples: \"us-en\" (US English), \"uk-en\" (UK English), \"ru-ru\" (Russian)\n  - See [DuckDuckGo regions](https://duckduckgo.com/params) for more options\n\n- **safesearch**: Content filtering level (default: \"moderate\")\n  - \"on\": Strict filtering\n  - \"moderate\": Moderate filtering\n  - \"off\": No filtering\n\n- **timelimit**: Time range for results\n  - \"d\": Last day\n  - \"w\": Last week\n  - \"m\": Last month\n  - \"y\": Last year (not available for news/videos)\n\n- **max_results**: Maximum number of results to return (default: 10)\n\n### Search Operators\n\nYou can use these operators in your search keywords:\n\n- `cats dogs`: Results about cats or dogs\n- `\"cats and dogs\"`: Results for exact term \"cats and dogs\"\n- `cats -dogs`: Fewer dogs in results\n- `cats +dogs`: More dogs in results\n- `cats filetype:pdf`: PDFs about cats (supported: pdf, doc(x), xls(x), ppt(x), html)\n- `dogs site:example.com`: Pages about dogs from example.com\n- `cats -site:example.com`: Pages about cats, excluding example.com\n- `intitle:dogs`: Page title includes the word \"dogs\"\n- `inurl:cats`: Page URL includes the word \"cats\"\n\n### Image Search Specific Parameters\n\n- **size**: \"Small\", \"Medium\", \"Large\", \"Wallpaper\"\n- **color**: \"color\", \"Monochrome\", \"Red\", \"Orange\", \"Yellow\", \"Green\", \"Blue\", \"Purple\", \"Pink\", \"Brown\", \"Black\", \"Gray\", \"Teal\", \"White\"\n- **type_image**: \"photo\", \"clipart\", \"gif\", \"transparent\", \"line\"\n- **layout**: \"Square\", \"Tall\", \"Wide\"\n- **license_image**: \"any\", \"Public\", \"Share\", \"ShareCommercially\", \"Modify\", \"ModifyCommercially\"\n\n### Video Search Specific Parameters\n\n- **resolution**: \"high\", \"standard\"\n- **duration**: \"short\", \"medium\", \"long\"\n- **license_videos**: \"creativeCommon\", \"youtube\"\n\n### AI Chat Models\n\n- **gpt-4o-mini**: OpenAI's GPT-4o mini model\n- **llama-3.3-70b**: Meta's Llama 3.3 70B model\n- **claude-3-haiku**: Anthropic's Claude 3 Haiku model\n- **o3-mini**: OpenAI's O3 mini model\n- **mistral-small-3**: Mistral AI's small model\n\n## Quickstart\n\n### Running the Server\n\nYou can run the server directly:\n\n```bash\nddg-mcp\n```\n\nBy default, it uses streamable-http transport. To use stdio transport:\n\n```bash\nddg-mcp --transport stdio\n```\n\nTo use streamable-http transport:\n\n```bash\nddg-mcp --transport streamable-http\n```\n\nYou can also specify host and port for SSE and streamable-http transports:\n\n```bash\nddg-mcp --host 0.0.0.0 --port 3001\n```\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/shannon/Workspace/artivus/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Usage Examples\n\n### Text Search\n\n```\nUse the ddg-text-search tool to search for \"climate change solutions\"\n```\n\nAdvanced example:\n```\nUse the ddg-text-search tool to search for \"renewable energy filetype:pdf site:edu\" with region \"us-en\", safesearch \"off\", timelimit \"y\", and max_results 20\n```\n\n### Image Search\n\n```\nUse the ddg-image-search tool to find images of \"renewable energy\" with color set to \"Green\"\n```\n\nAdvanced example:\n```\nUse the ddg-image-search tool to find images of \"mountain landscape\" with size \"Large\", color \"Blue\", type_image \"photo\", layout \"Wide\", and license_image \"Public\"\n```\n\n### News Search\n\n```\nUse the ddg-news-search tool to find recent news about \"artificial intelligence\" from the last day\n```\n\nAdvanced example:\n```\nUse the ddg-news-search tool to search for \"space exploration\" with region \"uk-en\", timelimit \"w\", and max_results 15\n```\n\n### Video Search\n\n```\nUse the ddg-video-search tool to find videos about \"machine learning tutorials\" with duration set to \"medium\"\n```\n\nAdvanced example:\n```\nUse the ddg-video-search tool to search for \"cooking recipes\" with resolution \"high\", duration \"short\", license_videos \"creativeCommon\", and max_results 10\n```\n\n### AI Chat\n\n```\nUse the ddg-ai-chat tool to ask \"What are the latest developments in quantum computing?\" using the claude-3-haiku model\n```\n\n### Search Results Summary\n\n```\nUse the search-results-summary prompt with query \"space exploration\" and style \"detailed\"\n```\n\n## Claude config\n\"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/INSTALLATION/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n  },\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Automated Publishing with GitHub Actions\n\nThis repository includes a GitHub Actions workflow for automated publishing to PyPI. The workflow is triggered when:\n\n1. A new GitHub Release is created\n2. The workflow is manually triggered via the GitHub Actions interface\n\nTo set up automated publishing:\n\n1. Generate a PyPI API token:\n   - Go to https://pypi.org/manage/account/token/\n   - Create a new token with scope limited to the `ddg-mcp` project\n   - Copy the token value (you'll only see it once)\n\n2. Add the token to your GitHub repository secrets:\n   - Go to your repository on GitHub\n   - Navigate to Settings > Secrets and variables > Actions\n   - Click \"New repository secret\"\n   - Name: `PYPI_API_TOKEN`\n   - Value: Paste your PyPI token\n   - Click \"Add secret\"\n\n3. To publish a new version:\n   - Update the version number in `pyproject.toml`\n   - Create a new release on GitHub or manually trigger the workflow\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/your/ddg-mcp run ddg-mcp --transport stdio\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "https://www.npmjs.com/package/ddg-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "ddkang1",
        "search",
        "duckduckgo search",
        "search ddkang1",
        "access duckduckgo"
      ],
      "category": "web-search"
    },
    "deepkl--mcp-searxng": {
      "owner": "deepkl",
      "name": "mcp-searxng",
      "url": "https://github.com/deepkl/mcp-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/deepkl.webp",
      "description": "Integrates web search capabilities with support for executing general queries, news, and articles, allowing for pagination to tailor results. Enhances applications with robust search functionalities that can control result sizes and counts.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-08T07:33:15Z",
      "readme_content": "# SearXNG MCP Server\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) implementation that integrates the SearxNG API, providing web search capabilities.\n\n<a href=\"https://glama.ai/mcp/servers/0j7jjyt7m9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0j7jjyt7m9/badge\" alt=\"SearXNG Server MCP server\" /></a>\n\n[![smithery badge](https://smithery.ai/badge/@ihor-sokoliuk/server-searxng)](https://smithery.ai/server/@ihor-sokoliuk/server-searxng)\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination.\n- **Pagination**: Control return size and result counts options.\n\n## Tools\n\n- **searxng_web_search**\n  - Execute web searches with pagination\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (default 20)\n    - `offset` (number, optional): Pagination offset (default 0)\n\n## Configuration\n\n### Setting the SEARXNG_URL\n\n1. Choose a SearxNG instance from the [list of public instances](https://searx.space/) or use your local environment.\n2. Set the `SEARXNG_URL` environment variable to the instance URL.\n3. The default `SEARXNG_URL` value is `http://localhost:8080`.\n\n### Usage with Claude Desktop\n\n### Installing via Smithery\n\nTo install SearxNG Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ihor-sokoliuk/server-searxng):\n\n```bash\nnpx -y @smithery/cli install @ihor-sokoliuk/server-searxng --client claude\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\"\n        \"<full path to mcp-searxng repo>/\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n### Docker\n\n#### Build\n\n```bash\ndocker build -t mcp-server-searxng:latest -f Dockerfile .\n```\n\n#### Use\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SEARXNG_URL\",\n        \"mcp-server-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-searxng",
      "npm_downloads": 27918,
      "keywords": [
        "searxng",
        "search",
        "pagination",
        "search deepkl",
        "web search",
        "queries news"
      ],
      "category": "web-search"
    },
    "delorenj--mcp-server-ticketmaster": {
      "owner": "delorenj",
      "name": "mcp-server-ticketmaster",
      "url": "https://github.com/delorenj/mcp-server-ticketmaster",
      "imageUrl": "/freedevtools/mcp/pfp/delorenj.webp",
      "description": "Provides tools for discovering events, venues, and attractions via the Ticketmaster Discovery API, enabling flexible searches based on keywords, dates, and locations, with structured outputs for programmatic access or human-readable formats.",
      "stars": 22,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T15:06:18Z",
      "readme_content": "# MCP Server for Ticketmaster\n[![smithery badge](https://smithery.ai/badge/mcp-server-ticketmaster)](https://smithery.ai/server/mcp-server-ticketmaster)\n\nA Model Context Protocol server that provides tools for discovering events, venues, and attractions through the Ticketmaster Discovery API.\n\n<a href=\"https://glama.ai/mcp/servers/u91gv8f3on\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u91gv8f3on/badge\" alt=\"Server for Ticketmaster Events MCP server\" /></a>\n\n## Features\n\n- Search for events, venues, and attractions with flexible filtering:\n  - Keyword search\n  - Date range for events\n  - Location (city, state, country)\n  - Venue-specific searches\n  - Attraction-specific searches\n  - Event classifications/categories\n- Output formats:\n  - Structured JSON data for programmatic use\n  - Human-readable text for direct consumption\n- Comprehensive data including:\n  - Names and IDs\n  - Dates and times (for events)\n  - Price ranges (for events)\n  - URLs\n  - Images\n  - Locations and addresses (for venues)\n  - Classifications (for attractions)\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-ticketmaster for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-ticketmaster):\n\n```bash\nnpx -y @smithery/cli install mcp-server-ticketmaster --client claude\n```\n\n### Manual Installation\n```bash\nnpx -y install @delorenj/mcp-server-ticketmaster\n```\n\n## Configuration\n\nThe server requires a Ticketmaster API key. You can get one by:\n1. Going to https://developer.ticketmaster.com/\n2. Creating an account or signing in\n3. Going to \"My Apps\" in your account\n4. Creating a new app to get your API key\n\nSet your API key in your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"ticketmaster\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@delorenj/mcp-server-ticketmaster\"],\n      \"env\": {\n        \"TICKETMASTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a tool called `search_ticketmaster` that accepts:\n\n### Required Parameters\n- `type`: Type of search ('event', 'venue', or 'attraction')\n\n### Optional Parameters\n- `keyword`: Search term\n- `startDate`: Start date in YYYY-MM-DD format (for events)\n- `endDate`: End date in YYYY-MM-DD format (for events)\n- `city`: City name\n- `stateCode`: State code (e.g., 'NY')\n- `countryCode`: Country code (e.g., 'US')\n- `venueId`: Specific venue ID\n- `attractionId`: Specific attraction ID\n- `classificationName`: Event category (e.g., 'Sports', 'Music')\n- `format`: Output format ('json' or 'text', defaults to 'json')\n\n### Examples\n\n#### Structured JSON Output (Default)\n```\n<use_mcp_tool>\n<server_name>ticketmaster</server_name>\n<tool_name>search_ticketmaster</tool_name>\n<arguments>\n{\n  \"type\": \"event\",\n  \"keyword\": \"concert\",\n  \"startDate\": \"2025-02-01\",\n  \"endDate\": \"2025-02-28\",\n  \"city\": \"New York\",\n  \"stateCode\": \"NY\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### Human-Readable Text Output\n```\n<use_mcp_tool>\n<server_name>ticketmaster</server_name>\n<tool_name>search_ticketmaster</tool_name>\n<arguments>\n{\n  \"type\": \"event\",\n  \"keyword\": \"concert\",\n  \"startDate\": \"2025-02-01\",\n  \"endDate\": \"2025-02-28\",\n  \"city\": \"New York\",\n  \"stateCode\": \"NY\",\n  \"format\": \"text\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Development\n\n1. Clone the repository\n2. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n3. Add your Ticketmaster API key to `.env`\n4. Install dependencies:\n   ```bash\n   npm install\n   ```\n5. Build the project:\n   ```bash\n   npm run build\n   ```\n6. Test with the inspector:\n   ```bash\n   npm run inspector\n   ```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-ticketmaster",
      "npm_downloads": 393,
      "keywords": [
        "ticketmaster",
        "searches",
        "search",
        "ticketmaster discovery",
        "server ticketmaster",
        "ticketmaster provides"
      ],
      "category": "web-search"
    },
    "devabdultech--hn-mcp": {
      "owner": "devabdultech",
      "name": "hn-mcp",
      "url": "https://github.com/devabdultech/hn-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/devabdultech.webp",
      "description": "Access real-time stories, comments, and user profiles from Hacker News, allowing search and retrieval of data through a robust integration with the Model Context Protocol.",
      "stars": 15,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-17T17:33:20Z",
      "readme_content": "# Hacker News MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@devabdultech/hn-mcp)](https://smithery.ai/server/@devabdultech/hn-mcp)\nOfficial Hacker News MCP Server - Adds powerful Hacker News integration to Cursor, Claude, and any other LLM clients. Access stories, comments, user profiles, and search functionality through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/73uji99mwg\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/73uji99mwg/badge\" alt=\"Hacker News Server MCP server\" />\n</a>\n\n## Features\n\n- Search stories and comments using Algolia's HN Search API\n- Get stories by type (top, new, best, ask, show, job)\n- Get individual stories with comments\n- Get comment trees and user discussions\n- Get user profiles and submissions\n- Real-time access to Hacker News data\n\n## Set Up\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"hackernews\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@devabdultech/hn-mcp-server\"]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install Hacker News MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@devabdultech/hn-mcp):\n\n```bash\nnpx -y @smithery/cli install @devabdultech/hn-mcp --client claude\n```\n\n## Tools\n\n1. `search`\n   * Search for stories and comments on Hacker News using Algolia's search API\n   * Inputs:\n         * `query` (string): Search query\n         * `type` (optional string): Filter by type ('story' or 'comment')\n         * `page` (optional number): Page number for pagination\n         * `hitsPerPage` (optional number): Results per page (max 100)\n   * Returns: Search results with stories and comments\n\n2. `getStories`\n   * Get multiple stories by type (top, new, best, ask, show, job)\n   * Inputs:\n         * `type` (string): Type of stories to fetch ('top', 'new', 'best', 'ask', 'show', 'job')\n         * `limit` (optional number): Number of stories to fetch (max 100)\n   * Returns: Array of story objects\n\n3. `getStoryWithComments`\n   * Get a story along with its comment thread\n   * Inputs:\n         * `id` (number): Story ID\n   * Returns: Story details with nested comments\n\n4. `getCommentTree`\n   * Get the full comment tree for a story\n   * Inputs:\n         * `storyId` (number): ID of the story\n   * Returns: Hierarchical comment tree structure\n\n5. `getUser`\n   * Get a user's profile information\n   * Inputs:\n         * `id` (string): Username\n   * Returns: User profile details including karma, created date, and about text\n\n6. `getUserSubmissions`\n   * Get a user's submissions (stories and comments)\n   * Inputs:\n         * `id` (string): Username\n   * Returns: Array of user's submitted stories and comments\n\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a new Pull Request\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## About\n\nThis MCP server is built and maintained by [devabdultech](https://github.com/devabdultech). It uses the official Hacker News API and Algolia Search API to provide comprehensive access to Hacker News data through the Model Context Protocol.\n",
      "npm_url": "https://www.npmjs.com/package/hn-mcp",
      "npm_downloads": 221,
      "keywords": [
        "search",
        "hacker",
        "web",
        "hacker news",
        "web search",
        "search retrieval"
      ],
      "category": "web-search"
    },
    "divslingerx--mcp-server": {
      "owner": "divslingerx",
      "name": "mcp-server",
      "url": "https://github.com/divslingerx/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/divslingerx.webp",
      "description": "Provides web search capabilities using Puppeteer, returning structured JSON results from Google searches in a lightweight and stateless design.",
      "stars": 0,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-01-20T18:03:10Z",
      "readme_content": "# Memory Store MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities using Puppeteer.\n\n## Features\n\n- Web search functionality via Google\n- Structured JSON results\n- Lightweight and stateless design\n- Easy integration with MCP-enabled systems\n\n## Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-server.git\n   cd mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nCreate a `.env` file in the project root with the following environment variables:\n\n```env\n# Puppeteer configuration\nPUPPETEER_EXECUTABLE_PATH=/path/to/chrome\nPUPPETEER_HEADLESS=true\n\n# Server settings\nPORT=3000\n```\n\n## Usage\n\nStart the server:\n\n```bash\nnpm start\n```\n\nThe server will be available to MCP clients. Example usage through MCP:\n\n```json\n{\n  \"tool\": \"search_web\",\n  \"arguments\": {\n    \"query\": \"example search\"\n  }\n}\n```\n\n## Development\n\n### Building the Project\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n### Linting\n\n```bash\nnpm run lint\n```\n\n### Formatting\n\n```bash\nnpm run format\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "puppeteer",
        "searches",
        "search",
        "using puppeteer",
        "search divslingerx",
        "searches lightweight"
      ],
      "category": "web-search"
    },
    "doronaviguy--mpc-0x": {
      "owner": "doronaviguy",
      "name": "mpc-0x",
      "url": "https://github.com/doronaviguy/mpc-0x",
      "imageUrl": "/freedevtools/mcp/pfp/doronaviguy.webp",
      "description": "Provides real-time information about Ethereum addresses across multiple chains and fetches address data seamlessly through an easy-to-use API.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-16T08:01:48Z",
      "readme_content": "# MCP Ethereum Address Info Server\n\nThis server provides information about Ethereum addresses across multiple chains using the Model Context Protocol (MCP). It includes a Server-Sent Events (SSE) endpoint for real-time updates.\n\n## Table of Contents\n\n- [Setup](#setup)\n- [Running the Server](#running-the-server)\n- [Available Endpoints](#available-endpoints)\n- [Using the SSE Endpoint](#using-the-sse-endpoint)\n- [Testing with Curl](#testing-with-curl)\n- [Example Workflow](#example-workflow)\n\n## Setup\n\n1. Clone the repository:\n   ```bash\n   git clone <repository-url>\n   cd mcp-0x-address\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file with the following variables:\n   ```\n   MCP_PORT=3002\n   ```\n\n## Running the Server\n\nStart the HTTP MCP server:\n\n```bash\nnpm run start:http\n```\n\nThis will start the server on port 3002 (or the port specified in your `.env` file).\n\n## Available Endpoints\n\nThe server provides the following endpoints:\n\n- `GET /health` - Server health check\n- `POST /mcp` - MCP endpoint for tool calls\n- `GET /sse` - Server-Sent Events endpoint for real-time updates\n- `GET /sse/clients` - Get information about connected SSE clients\n- `POST /sse/subscribe/:clientId` - Subscribe to address updates\n- `POST /sse/unsubscribe/:clientId` - Unsubscribe from address updates\n\n## Using the SSE Endpoint\n\nThe SSE endpoint allows clients to receive real-time updates from the server. Here's how to use it:\n\n1. Connect to the SSE endpoint\n2. Get your client ID from the connection response\n3. Subscribe to specific addresses\n4. Receive real-time updates for those addresses\n\n## Testing with Curl\n\n### 1. Connect to the SSE Endpoint\n\n```bash\ncurl -N http://localhost:3002/sse\n```\n\nThis will establish a connection to the SSE endpoint and start receiving events. The connection will remain open until you manually terminate it.\n\n### 2. Check Connected Clients\n\n```bash\ncurl http://localhost:3002/sse/clients\n```\n\n### 3. Subscribe to Address Updates\n\nAfter connecting to the SSE endpoint, you'll receive a client ID. Use that ID to subscribe to address updates:\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/sse/subscribe/YOUR_CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\", \"0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2\"]}'\n```\n\nReplace `YOUR_CLIENT_ID` with the client ID you received when connecting to the SSE endpoint.\n\n### 4. Unsubscribe from Address Updates\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/sse/unsubscribe/YOUR_CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n```\n\n### 5. Trigger an Address Update\n\nTo trigger an address update (which will be sent to subscribed clients), call the `get-address-info` tool:\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"get-address-info\",\n      \"arguments\": {\n        \"address\": \"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"\n      }\n    }\n  }'\n```\n\n### 6. Check Server Health\n\n```bash\ncurl http://localhost:3002/health\n```\n\n### 7. Test the Ping Tool\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"ping\",\n      \"arguments\": {}\n    }\n  }'\n```\n\n## Example Workflow\n\nHere's a complete workflow for testing the SSE functionality:\n\n1. Start the server:\n   ```bash\n   npm run start:http\n   ```\n\n2. In a new terminal, connect to the SSE endpoint:\n   ```bash\n   curl -N http://localhost:3002/sse\n   ```\n\n   You'll receive a response like:\n   ```\n   data: {\"type\":\"connection\",\"clientId\":\"client-1234567890abcdef\",\"message\":\"Connected to MCP SSE endpoint\",\"timestamp\":\"2023-01-01T00:00:00.000Z\"}\n   ```\n\n3. Note the `clientId` from the response.\n\n4. In another terminal, subscribe to address updates:\n   ```bash\n   curl -X POST \\\n     http://localhost:3002/sse/subscribe/client-1234567890abcdef \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n   ```\n\n5. Trigger an address update:\n   ```bash\n   curl -X POST \\\n     http://localhost:3002/mcp \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"jsonrpc\": \"2.0\",\n       \"id\": 1,\n       \"method\": \"tools/call\",\n       \"params\": {\n         \"name\": \"get-address-info\",\n         \"arguments\": {\n           \"address\": \"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"\n         }\n       }\n     }'\n   ```\n\n6. In the terminal where you're connected to the SSE endpoint, you'll see updates for the address.\n\n## Automated Testing Script\n\nFor a more automated test, you can use this bash script:\n\n```bash\n#!/bin/bash\n\n# Start SSE connection in the background and capture the output\ncurl -N http://localhost:3002/sse > sse_output.txt &\nSSE_PID=$!\n\n# Wait a moment for the connection to establish\nsleep 2\n\n# Extract the client ID from the output\nCLIENT_ID=$(grep -o '\"clientId\":\"[^\"]*\"' sse_output.txt | head -1 | cut -d'\"' -f4)\n\nif [ -z \"$CLIENT_ID\" ]; then\n  echo \"Failed to get client ID\"\n  kill $SSE_PID\n  exit 1\nfi\n\necho \"Connected with client ID: $CLIENT_ID\"\n\n# Subscribe to an address\ncurl -X POST \\\n  http://localhost:3002/sse/subscribe/$CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n\necho \"Subscribed to address. Waiting for updates...\"\necho \"Press Ctrl+C to stop\"\n\n# Keep the script running to see updates\ntail -f sse_output.txt\n\n# Clean up on exit\ntrap \"kill $SSE_PID; rm sse_output.txt\" EXIT\n```\n\nSave this as `test_sse.sh`, make it executable with `chmod +x test_sse.sh`, and run it with `./test_sse.sh`.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "addresses",
        "mpc",
        "address",
        "address data",
        "ethereum addresses",
        "doronaviguy mpc"
      ],
      "category": "web-search"
    },
    "enemyrr--mcp-server-pagespeed": {
      "owner": "enemyrr",
      "name": "mcp-server-pagespeed",
      "url": "https://github.com/enemyrr/mcp-server-pagespeed",
      "imageUrl": "/freedevtools/mcp/pfp/enemyrr.webp",
      "description": "Analyze webpage performance using Google PageSpeed Insights to provide metrics and suggestions for optimization.",
      "stars": 8,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-09T09:05:24Z",
      "readme_content": "# @enemyrr/mcp-server-pagespeed\n\nA Model Context Protocol server that provides Google PageSpeed Insights analysis. This server enables AI models to analyze webpage performance through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/wes81w8il2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/wes81w8il2/badge\" alt=\"Server Pagespeed MCP server\" /></a>\n\n## Installation & Setup for Cursor IDE\n\n1. Clone and build the project:\n```bash\ngit clone https://github.com/enemyrr/mcp-server-pagespeed.git\ncd mcp-server-pagespeed\nnpm install\nnpm run build\n```\n\n2. Add the server in Cursor IDE settings:\n   - Open Command Palette (Cmd/Ctrl + Shift + P)\n   - Search for \"MCP: Add Server\"\n   - Fill in the fields:\n     - Name: `pagespeed`\n     - Type: `command`\n     - Command: `node /absolute/path/to/mcp-server-pagespeed/build/index.js`\n\n> **Note**: Replace `/absolute/path/to/` with the actual path where you cloned and built the project.\n\n## Command-line Usage\n\nJust run:\n\n```bash\nnpx mcp-server-pagespeed\n```\n\n## Available Tools\n\n### analyze_pagespeed\nAnalyze a webpage using Google PageSpeed Insights API.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"pagespeed\",\n  tool_name: \"analyze_pagespeed\",\n  arguments: {\n    url: \"https://example.com\"\n  }\n});\n```\n\nThe tool returns:\n- Overall performance score (0-100)\n- Loading experience metrics\n  - First Contentful Paint\n  - First Input Delay\n- Top 5 improvement suggestions with:\n  - Title\n  - Description\n  - Potential impact\n  - Current value\n\n## Features\n\n- Real-time webpage performance analysis\n- Detailed loading experience metrics\n- Prioritized improvement suggestions\n- Comprehensive error handling\n- TypeScript support\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Invalid URLs\n- API request failures\n- Connection issues\n- Invalid tool calls\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/enemyrr/mcp-server-pagespeed\n\n## License\n\nMIT \n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-pagespeed",
      "npm_downloads": 240,
      "keywords": [
        "pagespeed",
        "performance",
        "webpage",
        "pagespeed insights",
        "pagespeed analyze",
        "google pagespeed"
      ],
      "category": "web-search"
    },
    "falahgs--Brave-Gemini-Research-MCP-Server": {
      "owner": "falahgs",
      "name": "Brave-Gemini-Research-MCP-Server",
      "url": "https://github.com/falahgs/Brave-Gemini-Research-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Enables AI assistants to conduct web searches and analyze research papers using Brave Search and Google's Gemini model. Supports general and local web search functionalities alongside academic analysis capabilities.",
      "stars": 5,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-28T01:16:07Z",
      "readme_content": "# Brave-Gemini Research MCP Server\r\n\r\nA modern MCP (Model Context Protocol) server implementation that provides AI assistants with web search capabilities via the Brave Search API and advanced research paper analysis with Google's Gemini model.\r\n\r\n## Overview\r\n\r\nThis project enables AI assistants like Claude to perform web searches and analyze research papers directly through a standardized API interface. The MCP server exposes three main tools:\r\n\r\n1. **Web Search** - For general internet searches and information retrieval\r\n2. **Local Search** - For finding businesses, locations, and places of interest\r\n3. **Research Paper Analysis** - For in-depth analysis of academic papers using Google's Gemini model\r\n\r\n## Features\r\n\r\n- 🔍 **Web Search API** - Find information across the web\r\n- 🏢 **Local Search API** - Discover businesses and places\r\n- 📑 **Research Paper Analysis** - Analyze academic papers with Gemini AI\r\n- 🤖 **Claude Integration** - Seamless connection with Claude Desktop\r\n- 🛠️ **Extensible Design** - Easy to add new tools and capabilities\r\n\r\n## Setup and Installation\r\n\r\n### Prerequisites\r\n\r\n- Node.js v18+ recommended\r\n- Brave Search API key ([Get one here](https://brave.com/search/api/))\r\n- Google API key for Gemini integration (required for research paper analysis)\r\n- Claude Desktop for AI assistant integration (optional)\r\n\r\n### Installation\r\n\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/falahgs/brave-gemini-research-mcp.git\r\n   cd brave-gemini-research-mcp\r\n   ```\r\n\r\n2. Install dependencies:\r\n   ```bash\r\n   npm install\r\n   ```\r\n\r\n3. Create a `.env` file with your API keys:\r\n   ```\r\n   BRAVE_API_KEY=your_brave_api_key\r\n   GOOGLE_API_KEY=your_google_api_key\r\n   ```\r\n\r\n### Building\r\n\r\nCompile the TypeScript code to JavaScript:\r\n\r\n```bash\r\nnpm run build\r\n# or manually\r\nnpx tsc\r\n```\r\n\r\n### Running the Server\r\n\r\nSet environment variables and start the server:\r\n\r\n**PowerShell:**\r\n```powershell\r\n$env:BRAVE_API_KEY=\"your_brave_api_key\"\r\n$env:GOOGLE_API_KEY=\"your_google_api_key\"\r\nnode dist/index.js\r\n```\r\n\r\n**Command Prompt:**\r\n```\r\nSET BRAVE_API_KEY=your_brave_api_key\r\nSET GOOGLE_API_KEY=your_google_api_key\r\nnode dist/index.js\r\n```\r\n\r\n**Bash/Linux/macOS:**\r\n```bash\r\nBRAVE_API_KEY=your_brave_api_key GOOGLE_API_KEY=your_google_api_key node dist/index.js\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nFollow these steps to integrate the MCP server with Claude Desktop:\r\n\r\n1. Ensure you have Claude Desktop installed ([Download here](https://claude.ai/desktop))\r\n\r\n2. Locate your Claude Desktop configuration file:\r\n   - Windows: `C:\\Users\\<username>\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\r\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\r\n\r\n3. Add the Brave-Gemini Research MCP configuration:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"Brave-Gemini Research\": {\r\n      \"command\": \"node\",\r\n      \"args\": [\"G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp\\\\dist\\\\index.js\"],\r\n      \"cwd\": \"G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp\",\r\n      \"timeoutMs\": 120000,\r\n      \"env\": {\r\n        \"BRAVE_API_KEY\": \"your_brave_api_key\",\r\n        \"GOOGLE_API_KEY\": \"your_google_api_key\",\r\n        \"NODE_ENV\": \"production\",\r\n        \"DEBUG\": \"mcp:*\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n4. Important notes:\r\n   - Use **absolute paths** with double backslashes (Windows) in the `args` and `cwd` fields\r\n   - Replace `G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp` with the actual path to your project\r\n   - Replace `your_brave_api_key` and `your_google_api_key` with your actual API keys\r\n   - The `timeoutMs` setting helps prevent timeout issues during initialization\r\n\r\n5. Save the file and restart Claude Desktop\r\n\r\n### Using with Claude\r\n\r\nAfter configuration, you can ask Claude to search the web or analyze research papers with prompts like:\r\n\r\n- \"Search the web for the latest AI research papers\"\r\n- \"Find coffee shops in San Francisco\"\r\n- \"Analyze this research paper on quantum computing: [paper content]\"\r\n\r\nClaude will use the MCP server to perform these searches and analyses, returning the results directly in your conversation.\r\n\r\n## Tool Capabilities\r\n\r\n### Web Search Tool\r\n\r\nThe web search tool enables general internet searches:\r\n\r\n- **Function**: `brave_web_search`\r\n- **Parameters**:\r\n  - `query` (required): Search query (max 400 chars)\r\n  - `count` (optional): Number of results (1-20, default 10)\r\n  - `offset` (optional): Pagination offset (max 9, default 0)\r\n\r\n### Local Search Tool\r\n\r\nThe local search tool finds businesses and locations:\r\n\r\n- **Function**: `brave_local_search`\r\n- **Parameters**:\r\n  - `query` (required): Local search query (e.g., \"pizza near Central Park\")\r\n  - `count` (optional): Number of results (1-20, default 5)\r\n\r\n### Research Paper Analysis Tool\r\n\r\nThe research paper analysis tool provides in-depth analysis of academic papers using Google's Gemini model:\r\n\r\n- **Function**: `gemini_research_paper_analysis`\r\n- **Parameters**:\r\n  - `paperContent` (required): The full text of the research paper to analyze\r\n  - `analysisType` (optional): Type of analysis to perform\r\n    - Options: \"summary\", \"critique\", \"literature review\", \"key findings\", \"comprehensive\" (default)\r\n  - `additionalContext` (optional): Specific questions or context to guide the analysis\r\n\r\n**Analysis Types:**\r\n- **Summary**: Comprehensive overview including research question, methodology, key findings, and conclusions\r\n- **Critique**: Critical evaluation of methodology, validity, limitations, and suggestions for improvement\r\n- **Literature Review**: Analysis of how the paper fits within the broader research landscape\r\n- **Key Findings**: Extraction and explanation of the most significant findings and implications\r\n- **Comprehensive**: Complete analysis covering all aspects (default)\r\n\r\n### Example Analysis Result\r\n\r\nWhen using the Research Paper Analysis tool with Gemini, you'll receive a structured, comprehensive analysis depending on the analysis type selected. For example, with a \"comprehensive\" analysis, you might get:\r\n\r\n```\r\n## Research Paper Analysis: Comprehensive\r\n\r\n### Overview\r\n[Summary of paper's main topic and research objectives]\r\n\r\n### Methodology Assessment\r\n[Evaluation of the research methods and design]\r\n\r\n### Key Findings\r\n[Breakdown of the most significant discoveries and results]\r\n\r\n### Limitations\r\n[Analysis of constraints and weaknesses in the research]\r\n\r\n### Significance & Implications\r\n[Discussion of the paper's importance to the field]\r\n\r\n### Recommendations\r\n[Suggestions for future research or applications]\r\n```\r\n\r\nThe Gemini model provides expert-level analysis that helps researchers, students, and professionals quickly understand and evaluate complex academic content.\r\n\r\n## Troubleshooting\r\n\r\n### Common Issues\r\n\r\n1. **Module Not Found Errors**:\r\n   - Ensure all imports include `.js` extensions in TypeScript files\r\n   - Run `npx tsc` to recompile after fixing imports\r\n   - Check the generated `dist` directory structure\r\n\r\n2. **Timeout Errors**:\r\n   - Increase the `timeoutMs` in Claude Desktop configuration (120000 ms recommended)\r\n   - Check that environment variables are properly set\r\n\r\n3. **API Key Issues**:\r\n   - Verify your API keys are correctly set in the environment\r\n   - Check for rate limiting or usage restrictions\r\n\r\n4. **Gemini Model Issues**:\r\n   - Ensure your Google API key has access to Gemini models\r\n   - Check if the paper content exceeds token limits (try shorter excerpts)\r\n   - Verify the analysis type is one of the supported options\r\n\r\n5. **Windows-Specific Issues**:\r\n   - Use PowerShell for more reliable environment variable handling\r\n   - For Windows paths in JSON config, use double backslashes (e.g., `G:\\\\path\\\\to\\\\file`)\r\n   - Consider using absolute paths if relative paths aren't working\r\n\r\n### Debugging\r\n\r\nFor detailed debugging output:\r\n\r\n```bash\r\n# Set environment variables\r\nDEBUG=mcp:* NODE_ENV=development node dist/index.js\r\n```\r\n\r\n## Testing Your Setup\r\n\r\nTo verify your MCP server is working correctly:\r\n\r\n1. **Manual Test**:\r\n   - Run the server using the command line instructions above\r\n   - Check the console output for \"Brave-Gemini Research MCP Server running on stdio\"\r\n   - No error messages should appear\r\n\r\n2. **Claude Desktop Test**:\r\n   - After configuring Claude Desktop, open a new conversation\r\n   - Ask Claude to \"Search for latest developments in AI\"\r\n   - Claude should respond with search results from Brave Search\r\n   - Ask Claude to analyze a research paper\r\n   - Claude should respond with a detailed analysis from Gemini\r\n\r\n## Technical Details\r\n\r\n### MCP Protocol\r\n\r\nThe Model Context Protocol allows AI models to access external tools through a standardized interface. Key components include:\r\n\r\n- **Tools**: Functions with defined schemas\r\n- **Transports**: Communication channels between clients and servers\r\n- **Handlers**: Logic to process requests and return responses\r\n\r\n### Project Structure\r\n\r\n```\r\n├── dist/               # Compiled JavaScript files\r\n├── src/\r\n│   ├── config.ts       # Server configuration\r\n│   ├── server.ts       # MCP server implementation\r\n│   ├── tools/          # Tool definitions and handlers\r\n│   └── utils/          # Utility functions and API clients\r\n├── index.ts            # Server entry point\r\n├── tsconfig.json       # TypeScript configuration\r\n└── package.json        # Project dependencies\r\n```\r\n\r\n## Citation\r\n\r\nIf you use this tool in your research or project, please cite it as:\r\n\r\n```\r\nSalieh, F. G. (2025). Brave-Gemini Research MCP Server: A tool for AI assistants to search the web and analyze research papers. \r\nhttps://github.com/yourusername/brave-gemini-research-mcp\r\n```\r\n\r\n## License\r\n\r\nMIT\r\n\r\n## Copyright\r\n\r\n© 2025 Falah G. Salieh, Baghdad, Iraq. All rights reserved.\r\n\r\n---\r\n\r\nMade with ❤️ for enhancing AI capabilities\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "google",
        "brave search",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "falcosan--mcp": {
      "owner": "falcosan",
      "name": "mcp",
      "url": "https://github.com/falcosan/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/falcosan.webp",
      "description": "Bridge between AI models and the Meilisearch search engine, enabling advanced search functionalities and real-time communication. Facilitates AI-driven data retrieval and management through seamless access to Meilisearch's powerful indexing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T17:26:40Z",
      "readme_content": "# MCP Meilisearch API Server\n\nA Model Context Protocol (MCP) server implementation that provides a bridge between AI models and the Meilisearch search engine.\n\n## Overview\n\n- **MCP Server**: Exposes Meilisearch APIs as tools using the Model Context Protocol.\n- **Web Client Demo**: A demo interface showcasing search functionalities.\n- **AI Inference**: Intelligent tool selection based on user queries.\n\n## Key Features\n\n- **Multiple Transport Options**: Supports both STDIO and StreamableHTTP transports.\n- **Meilisearch API Support**: Full access to Meilisearch functionalities.\n- **Web Client Demo**: Updated interface showcasing search capabilities and features.\n- **AI Inference**: Leverages LLMs from providers such as OpenAI, HuggingFace, OpenRouter, and Ollama to intelligently determine and utilize the most suitable tool for user queries.\n\n## Getting Started\n\n### Prerequisites\n\n- Node.js v20 or higher.\n- A running Meilisearch instance (local or remote).\n- API key for Meilisearch (if required).\n- AI provider API key (if using AI inference).\n\n### Installation\n\nInstall the package:\n\n```bash\n# Using npm\nnpm install mcp-meilisearch\n\n# Using yarn\nyarn add mcp-meilisearch\n\n# Using pnpm\npnpm add mcp-meilisearch\n```\n\n### Options\n\n#### Meilisearch Connection Options\n\n- `meilisearchHost`: URL of the Meilisearch instance (Default: \"http://localhost:7700\")\n- `meilisearchApiKey`: API key for authenticating with Meilisearch (Default: \"\")\n\n#### MCP Server Options\n\n- `transport`: Transport type for MCP server (\"http\" | \"stdio\") (Default: \"http\")\n- `httpPort`: HTTP port for MCP server (Default: 4995)\n- `mcpEndpoint`: MCP endpoint path (Default: \"/mcp\")\n\n#### Session Options\n\n- `sessionTimeout`: Session timeout in milliseconds (Default: 3600000)\n- `sessionCleanupInterval`: Session cleanup interval in milliseconds (Default: 60000)\n\n#### AI Inference Options\n\n- `aiProviderName`: Name of the AI provider (\"openai\" | \"huggingface\" | \"openrouter\" | \"ollama\") (Default: \"openai\")\n- `aiProviderApiKey`: AI provider API key for AI inference\n- `llmModel`: AI model to use (Default: \"gpt-3.5-turbo\")\n\nPlease be aware that not all models support function calling, which is required for proper AI inference in this package. Make sure to select a model that supports the tools parameter.\n\n| Provider    | Supported Models                                                                              |\n| ----------- | --------------------------------------------------------------------------------------------- |\n| OpenAI      | [List of supported models](https://platform.openai.com/docs/models)                           |\n| OpenRouter  | [List of supported models](https://openrouter.ai/models?fmt=cards&supported_parameters=tools) |\n| HuggingFace | [List of supported models](https://huggingface.co/models?other=function+calling)              |\n| Ollama      | [List of supported models](https://ollama.com/search?c=tools)                                 |\n\n#### Example server setup\n\n```typescript\nawait mcpMeilisearchServer({\n  meilisearchHost: \"http://localhost:7700\",\n  aiProviderName: \"openrouter\",\n  aiProviderApiKey: \"your_openrouter_api_key\",\n  llmModel: \"anthropic/claude-3-opus\", // Make sure to use a model that supports function calling\n});\n```\n\n### Using the MCPClient\n\nThe package exports the MCPClient class for client-side integration:\n\n```typescript\nimport { MCPClient } from \"mcp-meilisearch/client\";\n\nconst client = new MCPClient(\"mcp-meilisearch-client\");\n\nawait client.connectToServer(\"http://localhost:4995/mcp\");\n\nconst result = await client.callTool(\"global-search\", {\n  q: \"search kiosco antonio\",\n});\n```\n\n#### Client Methods\n\n##### `callTool(name, args)`\n\nCalls a specific tool on the MCP server with optional arguments.\n\n**Parameters:**\n\n- `name`: String - The name of the tool to call\n- `args`: Object (Optional) - Arguments to pass to the tool\n\n##### `processSummary(query)`\n\nProcesses data using AI to generate a human-readable summary.\n\n**Parameters:**\n\n- `query`: Any - The data to be summarized\n\n##### `callToolWithAI(query, options)`\n\nProcesses a user query through AI to determine and execute the most appropriate tool.\n\n**Parameters:**\n\n- `query`: String - The user's query or request to be processed\n- `options`: Object (Optional) - Configuration options\n  - `specificTools`: String[] (Optional) - Restricts tool selection to this list of tool names\n  - `justReasoning`: Boolean (Optional) - When set to `true`, returns only the AI's reasoning without executing the selected tool\n  - `provideSummary`: Boolean (Optional) - When set to `true`, generates a concise summary of the search results along with the regular response\n\n### Starting the Server\n\nYou can start the server programmatically:\n\n```typescript\nimport mcpMeilisearchServer from \"mcp-meilisearch\";\n\nawait mcpMeilisearchServer({\n  meilisearchHost: \"http://localhost:7700\",\n  meilisearchApiKey: \"your_meilisearch_api_key\",\n  aiProviderName: \"openai\",\n  aiProviderApiKey: \"your_ai_provider_api_key\",\n  llmModel: \"gpt-4\",\n});\n```\n\n## Tools\n\nThe MCP server exposes various tools that allow you to interact with Meilisearch functionalities. Each tool corresponds to a specific Meilisearch API endpoint, enabling you to perform operations such as searching, indexing, and managing documents.\n\n### Tool Categories\n\n1. **System Tools**: Health checks, version info, server stats.\n2. **Index Tools**: Manage indexes (create, update, delete, list).\n3. **Document Tools**: Add, update, delete, and retrieve documents.\n4. **Search Tools**: Advanced search, including vector search.\n5. **Settings Tools**: Configure index settings.\n6. **Task Tools**: Manage asynchronous tasks.\n7. **Vector Tools**: Experimental vector search capabilities.\n\n### System Tools\n\n#### health\n\n- **Description**: Check if the Meilisearch server is healthy.\n\n#### version\n\n- **Description**: Get the version information of the Meilisearch server.\n\n#### info\n\n- **Description**: Get the system information of the Meilisearch server.\n\n#### stats\n\n- **Description**: Get statistics about all indexes or a specific index.\n- **Parameters**:\n  - `indexUid` (string, optional): Unique identifier of the index.\n\n#### get-tasks\n\n- **Description**: Get information about tasks with optional filtering.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of tasks to return.\n  - `from` (number, optional): Task uid from which to start fetching.\n  - `status` (string, optional): Status of tasks to return.\n  - `type` (string, optional): Type of tasks to return.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks were performed.\n\n#### delete-tasks\n\n- **Description**: Delete tasks based on provided filters.\n- **Parameters**:\n  - `statuses` (string[], optional): Statuses of tasks to delete.\n  - `types` (string[], optional): Types of tasks to delete.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks to delete were performed.\n  - `uids` (number[], optional): UIDs of the tasks to delete.\n  - `canceledBy` (number[], optional): UIDs of the tasks that canceled tasks to delete.\n  - `beforeUid` (number, optional): Delete tasks whose uid is before this value.\n  - `beforeStartedAt` (string, optional): Delete tasks that started processing before this date (ISO 8601 format).\n  - `beforeFinishedAt` (string, optional): Delete tasks that finished processing before this date (ISO 8601 format).\n\n### Index Tools\n\n#### list-indexes\n\n- **Description**: List all indexes in the Meilisearch instance.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of indexes to return.\n  - `offset` (number, optional): Number of indexes to skip.\n\n#### get-index\n\n- **Description**: Get information about a specific Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### create-index\n\n- **Description**: Create a new Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier for the new index.\n  - `primaryKey` (string, optional): Primary key for the index.\n\n#### update-index\n\n- **Description**: Update a Meilisearch index (currently only supports updating the primary key).\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `primaryKey` (string, required): New primary key for the index.\n\n#### delete-index\n\n- **Description**: Delete a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index to delete.\n\n#### swap-indexes\n\n- **Description**: Swap two or more indexes in Meilisearch.\n- **Parameters**:\n  - `indexes` (string, required): JSON array of index pairs to swap, e.g. [[\"movies\", \"movies_new\"]].\n\n### Document Tools\n\n#### get-documents\n\n- **Description**: Get documents from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `limit` (number, optional): Maximum number of documents to return (default: 20).\n  - `offset` (number, optional): Number of documents to skip (default: 0).\n  - `fields` (string[], optional): Fields to return in the documents.\n  - `filter` (string, optional): Filter query to apply.\n\n#### get-document\n\n- **Description**: Get a document by its ID from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentId` (string, required): ID of the document to retrieve.\n  - `fields` (string[], optional): Fields to return in the document.\n\n#### add-documents\n\n- **Description**: Add documents to a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documents` (string, required): JSON array of documents to add.\n  - `primaryKey` (string, optional): Primary key for the documents.\n\n#### update-documents\n\n- **Description**: Update documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documents` (string, required): JSON array of documents to update.\n  - `primaryKey` (string, optional): Primary key for the documents.\n\n#### delete-document\n\n- **Description**: Delete a document by its ID from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentId` (string, required): ID of the document to delete.\n\n#### delete-documents\n\n- **Description**: Delete multiple documents by their IDs from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentIds` (string, required): JSON array of document IDs to delete.\n\n#### delete-all-documents\n\n- **Description**: Delete all documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n### Search Tools\n\n#### search\n\n- **Description**: Search for documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `q` (string, required): Search query.\n  - `limit` (number, optional): Maximum number of results to return (default: 20).\n  - `offset` (number, optional): Number of results to skip (default: 0).\n  - `filter` (string, optional): Filter query to apply.\n  - `sort` (string[], optional): Attributes to sort by, e.g. [\"price:asc\"].\n  - `facets` (string[], optional): Facets to return.\n  - `attributesToRetrieve` (string[], optional): Attributes to include in results.\n  - `attributesToCrop` (string[], optional): Attributes to crop.\n  - `cropLength` (number, optional): Length at which to crop cropped attributes.\n  - `attributesToHighlight` (string[], optional): Attributes to highlight.\n  - `highlightPreTag` (string, optional): Tag to insert before highlighted text.\n  - `highlightPostTag` (string, optional): Tag to insert after highlighted text.\n  - `showMatchesPosition` (boolean, optional): Whether to include match positions in results.\n  - `matchingStrategy` (string, optional): Matching strategy: 'all' or 'last'.\n\n#### multi-search\n\n- **Description**: Perform multiple searches in one request.\n- **Parameters**:\n  - `queries` (string, required): JSON array of search queries, each containing the same parameters as the `search` tool.\n\n#### global-search\n\n- **Description**: Search for a term across all available Meilisearch indexes and return combined results.\n- **Parameters**:\n  - `q` (string, required): Search query.\n  - `limit` (number, optional): Maximum number of results to return per index (default: 20).\n  - `attributesToRetrieve` (string[], optional): Attributes to include in results.\n\n#### facet-search\n\n- **Description**: Search for facet values matching specific criteria.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `facetName` (string, required): Name of the facet to search.\n  - `facetQuery` (string, optional): Query to match against facet values.\n  - `filter` (string, optional): Filter to apply to the base search.\n\n### Settings Tools\n\n#### get-settings\n\n- **Description**: Get all settings for a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### update-settings\n\n- **Description**: Update settings for a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `settings` (string, required): JSON object containing settings to update.\n\n#### reset-settings\n\n- **Description**: Reset all settings for a Meilisearch index to their default values.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### Get Settings Tools\n\nThe following tools retrieve specific settings for a Meilisearch index:\n\n- **get-displayed-attributes**\n- **get-searchable-attributes**\n- **get-filterable-attributes**\n- **get-sortable-attributes**\n- **get-ranking-rules**\n- **get-stop-words**\n- **get-synonyms**\n- **get-typo-tolerance**\n- **get-pagination**\n- **get-faceting**\n- **get-dictionary**\n- **get-proximity-precision**\n- **get-separator-tokens**\n- **get-non-separator-tokens**\n- **get-word-dictionary**\n\nAll these tools have the same parameter:\n\n- `indexUid` (string, required): Unique identifier of the index.\n\n#### Update Settings Tools\n\nThe following tools update specific settings for a Meilisearch index:\n\n- **update-displayed-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `displayedAttributes` (string, required): JSON array of attributes to display, e.g. [\"title\", \"description\"].\n\n- **update-searchable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `searchableAttributes` (string, required): JSON array of attributes that can be searched, e.g. [\"title\", \"description\"].\n\n- **update-filterable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `filterableAttributes` (string, required): JSON array of attributes that can be used as filters, e.g. [\"genre\", \"director\"].\n\n- **update-sortable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `sortableAttributes` (string, required): JSON array of attributes that can be used for sorting, e.g. [\"price\", \"date\"].\n\n- **update-ranking-rules**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `rankingRules` (string, required): JSON array of ranking rules, e.g. [\"typo\", \"words\", \"proximity\", \"attribute\", \"sort\", \"exactness\"].\n\n- **update-stop-words**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `stopWords` (string, required): JSON array of words to ignore in search queries, e.g. [\"the\", \"a\", \"an\"].\n\n- **update-synonyms**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `synonyms` (string, required): JSON object mapping words to their synonyms, e.g. {\"movie\": [\"film\"]}.\n\n- **update-typo-tolerance**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `typoTolerance` (string, required): JSON object with typo tolerance configuration, e.g. {\"enabled\": true, \"minWordSizeForTypos\": {\"oneTypo\": 5, \"twoTypos\": 9}}.\n\n- **update-pagination**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `pagination` (string, required): JSON object with pagination configuration, e.g. {\"maxTotalHits\": 1000}.\n\n- **update-faceting**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `faceting` (string, required): JSON object with faceting configuration, e.g. {\"maxValuesPerFacet\": 100}.\n\n- **update-dictionary**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `dictionary` (string, required): JSON array of words to consider as a single word, e.g. [\"San Francisco\", \"New York\"].\n\n- **update-proximity-precision**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `proximityPrecision` (string, required): String with proximity precision value, can be 'byWord' or 'byAttribute'.\n\n- **update-separator-tokens**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `separatorTokens` (string, required): JSON array of tokens that should be considered as word separators, e.g. [\"-\", \"_\"].\n\n- **update-non-separator-tokens**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `nonSeparatorTokens` (string, required): JSON array of tokens that should not be considered as word separators, e.g. [\"@\", \".\"].\n\n- **update-word-dictionary**\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `wordDictionary` (string, required): JSON array of custom words to add to the dictionary, e.g. [\"cbuilder\", \"meilisearch\"].\n\n#### Reset Settings Tools\n\nThe following tools reset specific settings for a Meilisearch index to their default values:\n\n- **reset-displayed-attributes**\n- **reset-searchable-attributes**\n- **reset-filterable-attributes**\n- **reset-sortable-attributes**\n- **reset-ranking-rules**\n- **reset-stop-words**\n- **reset-synonyms**\n- **reset-typo-tolerance**\n- **reset-pagination**\n- **reset-faceting**\n- **reset-dictionary**\n- **reset-proximity-precision**\n- **reset-separator-tokens**\n- **reset-non-separator-tokens**\n- **reset-word-dictionary**\n\nAll these reset tools have the same parameter:\n\n- `indexUid` (string, required): Unique identifier of the index.\n\n### Task Tools\n\n#### list-tasks\n\n- **Description**: List tasks with optional filtering.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of tasks to return.\n  - `from` (number, optional): Task uid from which to start fetching.\n  - `statuses` (string[], optional): Statuses of tasks to return.\n  - `types` (string[], optional): Types of tasks to return.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks were performed.\n  - `uids` (number[], optional): UIDs of specific tasks to return.\n\n#### get-task\n\n- **Description**: Get information about a specific task.\n- **Parameters**:\n  - `taskUid` (number, required): Unique identifier of the task.\n\n#### cancel-tasks\n\n- **Description**: Cancel tasks based on provided filters.\n- **Parameters**:\n  - `statuses` (string[], optional): Statuses of tasks to cancel.\n  - `types` (string[], optional): Types of tasks to cancel.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks to cancel were performed.\n  - `uids` (number[], optional): UIDs of the tasks to cancel.\n\n#### wait-for-task\n\n- **Description**: Wait for a specific task to complete.\n- **Parameters**:\n  - `taskUid` (number, required): Unique identifier of the task to wait for.\n  - `timeoutMs` (number, optional): Maximum time to wait in milliseconds (default: 5000).\n  - `intervalMs` (number, optional): Polling interval in milliseconds (default: 500).\n\n### Vector Tools\n\n#### enable-vector-search\n\n- **Description**: Enable the vector search experimental feature in Meilisearch.\n\n#### get-experimental-features\n\n- **Description**: Get the status of experimental features in Meilisearch.\n\n#### update-embedders\n\n- **Description**: Configure embedders for vector search.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `embedders` (string, required): JSON object containing embedder configurations.\n\n#### get-embedders\n\n- **Description**: Get the embedders configuration for an index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### reset-embedders\n\n- **Description**: Reset the embedders configuration for an index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### vector-search\n\n- **Description**: Perform a vector search in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `vector` (string, required): JSON array representing the vector to search for.\n  - `limit` (number, optional): Maximum number of results to return (default: 20).\n  - `offset` (number, optional): Number of results to skip (default: 0).\n  - `filter` (string, optional): Filter to apply (e.g., 'genre = horror AND year > 2020').\n  - `embedder` (string, optional): Name of the embedder to use (if omitted, a 'vector' must be provided).\n  - `attributes` (string[], optional): Attributes to include in the vector search.\n  - `query` (string, optional): Text query to search for (if using 'embedder' instead of 'vector').\n  - `hybrid` (boolean, optional): Whether to perform a hybrid search (combining vector and text search).\n  - `hybridRatio` (number, optional): Ratio of vector vs text search in hybrid search (0-1, default: 0.5).\n",
      "npm_url": "https://www.npmjs.com/package/mcp-mcp",
      "npm_downloads": 83,
      "keywords": [
        "indexing",
        "search",
        "retrieval",
        "meilisearch search",
        "search engine",
        "advanced search"
      ],
      "category": "web-search"
    },
    "fefergrgrgrg--smileyCoinDev": {
      "owner": "fefergrgrgrg",
      "name": "smileyCoinDev",
      "url": "https://github.com/fefergrgrgrg/smileyCoinDev",
      "imageUrl": "/freedevtools/mcp/pfp/fefergrgrgrg.webp",
      "description": "Provides web development tools to capture screenshots and programmatically list available screens, helping to streamline development tasks and automate operations.",
      "stars": 15,
      "forks": 1,
      "license": "MIT License",
      "language": "C++",
      "updated_at": "2025-05-16T13:54:28Z",
      "readme_content": "Litecoin Core integration/staging tree\n=====================================\n\n[![Build Status](https://travis-ci.org/litecoin-project/litecoin.svg?branch=master)](https://travis-ci.org/litecoin-project/litecoin)\n\nhttps://litecoin.org\n\nWhat is Litecoin?\n----------------\n\nLitecoin is an experimental digital currency that enables instant payments to\nanyone, anywhere in the world. Litecoin uses peer-to-peer technology to operate\nwith no central authority: managing transactions and issuing money are carried\nout collectively by the network. Litecoin Core is the name of open source\nsoftware which enables the use of this currency.\n\nFor more information, as well as an immediately useable, binary version of\nthe Litecoin Core software, see [https://litecoin.org](https://litecoin.org).\n\nLicense\n-------\n\nLitecoin Core is released under the terms of the MIT license. See [COPYING](COPYING) for more\ninformation or see https://opensource.org/licenses/MIT.\n\nDevelopment Process\n-------------------\n\nThe `master` branch is regularly built and tested, but is not guaranteed to be\ncompletely stable. [Tags](https://github.com/litecoin-project/litecoin/tags) are created\nregularly to indicate new official, stable release versions of Litecoin Core.\n\nThe contribution workflow is described in [CONTRIBUTING.md](CONTRIBUTING.md)\nand useful hints for developers can be found in [doc/developer-notes.md](doc/developer-notes.md).\n\nThe developer [mailing list](https://groups.google.com/forum/#!forum/litecoin-dev)\nshould be used to discuss complicated or controversial changes before working\non a patch set.\n\nDeveloper IRC can be found on Freenode at #litecoin-dev.\n\nTesting\n-------\n\nTesting and code review is the bottleneck for development; we get more pull\nrequests than we can review and test on short notice. Please be patient and help out by testing\nother people's pull requests, and remember this is a security-critical project where any mistake might cost people\nlots of money.\n\n### Automated Testing\n\nDevelopers are strongly encouraged to write [unit tests](src/test/README.md) for new code, and to\nsubmit new unit tests for old code. Unit tests can be compiled and run\n(assuming they weren't disabled in configure) with: `make check`. Further details on running\nand extending unit tests can be found in [/src/test/README.md](/src/test/README.md).\n\nThere are also [regression and integration tests](/test), written\nin Python, that are run automatically on the build server.\nThese tests can be run (if the [test dependencies](/test) are installed) with: `test/functional/test_runner.py`\n\nThe Travis CI system makes sure that every pull request is built for Windows, Linux, and macOS, and that unit/sanity tests are run automatically.\n\n### Manual Quality Assurance (QA) Testing\n\nChanges should be tested by somebody other than the developer who wrote the\ncode. This is especially important for large or high-risk changes. It is useful\nto add a test plan to the pull request description if testing the changes is\nnot straightforward.\n\nTranslations\n------------\n\nWe only accept translation fixes that are submitted through [Bitcoin Core's Transifex page](https://www.transifex.com/projects/p/bitcoin/).\nTranslations are converted to Litecoin periodically.\n\nTranslations are periodically pulled from Transifex and merged into the git repository. See the\n[translation process](doc/translation_process.md) for details on how this works.\n\n**Important**: We do not accept translation changes as GitHub pull requests because the next\npull from Transifex would automatically overwrite them again.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "smileycoindev",
        "fefergrgrgrg",
        "screenshots",
        "fefergrgrgrg smileycoindev",
        "smileycoindev provides",
        "capture screenshots"
      ],
      "category": "web-search"
    },
    "fengin--search-server": {
      "owner": "fengin",
      "name": "search-server",
      "url": "https://github.com/fengin/search-server",
      "imageUrl": "/freedevtools/mcp/pfp/fengin.webp",
      "description": "Provides online and local search capabilities with support for multiple search engines including Brave Search, Metaso Search, and Bocha Search. Designed for integration with AI models like Cursor and Claude Desktop, enabling enhanced content retrieval.",
      "stars": 73,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-19T03:59:51Z",
      "readme_content": "# Search MCP Server\n\n\n\n一个基于MCP协议的搜索服务实现，提供多种搜索引擎支持，Cursor和Claude Desktop能与之无缝集成。\n\n使用Python开发，支持异步处理和高并发请求，目前支持三种搜索引擎选择：\n\n- Brave Search ：国外一家专业提供搜索接口服务产品\n\n- 秘塔（Metaso）搜索：秘塔AI搜索的逆向实现接口，非官方接口\n\n- 博查（bocha）搜索：国内Search API市场占有率最高的搜索API产品\n\n更多MCP知识，见AI全书([ 一文看懂什么是MCP(大模型上下文)？用来干什么的？怎么用它？](https://aibook.ren/archives/mcp-course))\n\n**作者**： 凌封（微信fengin）\n\n**网址**：[https://aibook.ren]()（AI全书）\n\n## 使用示例\n\n\n\n## 功能特点\n\n- **多搜索引擎支持**: \n  - Brave Search: 提供网络搜索和位置搜索\n  - Metaso搜索: 提供网络搜索和学术搜索，支持简洁和深入两种模式\n  - 博查搜索: 提供网络搜索，支持时间范围过滤、详细摘要和图片搜索\n- **适用场景**: Claude Desktop或者Cursor无缝集成使用，大大扩展工具的内容获取能力\n- **模块化设计**: 每个搜索引擎都是独立的模块，也可以单独拷出去其他地方使用\n\n## 三种搜索的选择\n\n<mark>运行时只能生效一种搜索引擎</mark>，为了方便大家选择配置哪个上线，我列了下大致的对比：\n\n| 搜索引擎   | 国内/外 | 需魔法 | 自带总结 | 质量  | 免费    | 官方  | 速度      | 注册门槛 |\n| ------ | ---- | --- | ---- | --- | ----- | --- | ------- | ---- |\n| Brave  | 国外   | 是   | 否    | 高   | 是(限量) | 是   | 中       | 很高   |\n| Metaso | 国内   | 否   | 是    | 中   | 是     | 否   | 慢(AI总结) | 低    |\n| Bocha  | 国内   | 否   | 否    | 高   | 否     | 是   | 极快      | 低    |\n\n## 安装和使用\n\n### 1. 环境要求\n\n- Python 3.10+\n- uv 0.24.0+\n- node.js v20.15.0\n- cursor >=0.45.10 (低于该版本mcp server配置老是连不上)\n- 科学上网（仅使用Brave Search需要）\n\n#### 1.1 安装浏览器驱动(仅Metaso需要)\n\n```\n# 安装Playwright框架\npip install playwright>=1.35.0\n# 安装浏览器驱动,仅安装chromium\nplaywright install chromium\n```\n\n\n\n### 2.下载代码\n\n```bash\ngit clone https://github.com/fengin/search-server.git\n```\n\n### 3. 启用你要的搜索引擎\n\n打开项目根目录，修改server.py以下代码选择启用类型：\n\n```python\n# 搜索引擎配置\nSEARCH_ENGINE = os.getenv(\"SEARCH_ENGINE\", \"bocha\")\n```\n\n其中值分别对应有brave、metaso、bocha，也可以通过配置环境变量SEARCH_ENGIN\n\n### 4\\. 配置对应的搜索模块\n\n以下三个模块目录下都对应有一个config.py文件：\n\n- src\\search\\proxy\\brave\n\n- src\\search\\proxy\\metaso\n\n- src\\search\\proxy\\bocha\n\n根据你的选择，修改对应的config.py文件配置\n\n#### 4.1 brave search配置\n\n```python\n# 检查API密钥\nBRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")\nif not BRAVE_API_KEY:\n    BRAVE_API_KEY = \"你申请的 brave_api_key\"\n```\n\n在Claude Desktop里面使用的话，也可以在Claude Desktop里配置通过环境变量传这个参数，但是Cursor目前不支持环境变量，只能在这文件里修改\n\nAPI KEY 申请地址：[Brave Search - API](https://api-dashboard.search.brave.com/login)\n\n<mark>申请门槛比较高</mark>，要求：\n\n- 魔法（使用时也需要）\n\n- 邮箱验证\n\n- 信用卡（可以用虚拟的：[https://cardgenerator.org/](https://cardgenerator.org/)）\n\n#### 4.2 秘塔(metaso)配置\n\n```python\n# 认证信息\nMETASO_UID = os.getenv(\"METASO_UID\")\nMETASO_SID = os.getenv(\"METASO_SID\")\nif not METASO_UID or not METASO_SID:\n    METASO_UID = \"你获取的 metaso_uid\"\n    METASO_SID = \"你获取的 metaso_sid\"\n```\n\n同样Claude Desktop使用可以通过 MCP Servers配置里环境变量；\n\n**uid和sid获取方式：**\n\n进入秘塔AI搜索，登录账号（<mark>建议登录账号，否则可能遭遇奇怪的限制</mark>），然后F12打开开发者工具，从Application > Cookies中找到`uid`和`sid`的值。\n\n\n\n**多账号接入**\n\n<mark>注意：目前怀疑秘塔对IP地址的总搜索次数有限制，建议加入IP轮换</mark>\n\n你可以通过提供多个账号的uid-sid并使用`,`修改下相关的使用代码，每次请求服务会从中挑选一个，本人后续再考虑。\n\n#### 4.3 博查(bocha)配置\n\n```python\nBOCHA_API_KEY = os.getenv(\"BOCHA_API_KEY\", \"\")\nif not BOCHA_API_KEY:\n    BOCHA_API_KEY=\"你申请的 bocha_api_key\"\n```\n\n注册申请地址：https://open.bochaai.com/\n\n调用按次数收费，不便宜，但是搜索质量确实比较好，我这有少量的免费试用码，有需要的微信联系我；\n\n### 5. AI工具配置\n\n#### 5.1 Cursor里配置\n\n\n\n- name:  search\n\n- type:  cmd\n\n- command: uv --directory D:\\\\code\\\\search-server run search\n\n其中 “D:\\code\\search-server” 就是你拉下来源代码目录\n\n#### 5.2 Claude Desktop配置\n\n找到配置文件\n\n**方法一**\n\n```\n# widnows\nC:\\Users\\{用户}\\AppData\\Roaming\\Claude\\claude_desktop_config.json\n# mac/linux 应该在用户家目录下找\n```\n\n**方法二**\n\n打开Claude Desktop应用进入查看：\nClaude Desktop—>菜单—>Settings—>Developer—>Edit Config\n\n编辑增加以下MCP Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"search\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"D:\\\\code\\\\search-server\",\n                \"run\",\n                \"search\"\n            ],\n            \"env\": {\n                \"BRAVE_API_KEY\": \"你申请的API KEY\"\n            }\n        }\n  }\n}\n```\n\n环境变量看你需要，如果代码改了这就没必要配置了\n\n<mark>Cursor会弹出一个黑窗口，不要关闭，不要关闭</mark>，这是启动的MCP Server进程，目前没办法解决不弹出来。\n\nClaude Desktop配置完一定要重启应用才生效。\n\n#### 5.4 问题排错\n\n配置Cursor后很多人遇到，MCP Servers里面配置完，还是显示状态红点，Tools Not Found，使用时也不会去调用，这是因为根本没配置好。\n\n大部分可能出现的情况是：\n\n1. 环境没准备好，包括需要的软件及版本要求，详见环境章节\n\n2. 准备的环境不对，像windows有cmd终端、powershell终端、还有可能装了gitbash终端，你打开cmd终端（cursor一般是这个）检查下环境，直接运行下uv --directory D:\\code\\search-server run search\n\n3. 配置路径/命令不对，可以打开终端运行命令看下：uv --directory D:\\code\\search-server run search\n\n4. 把黑窗口关闭了，再次开启需要重启Cursor\n\n5. Cursor版本太旧\n\n6. 运行时报以下错，原因是没有安装chromium，解决办法见环境准备1.1章节\n   \n   ```shell\n   错误:搜索执行错误:BrowserType.launch persistent context:Executable doesn't exist atC:\\Users\\fengi\\AppDatalLocal\\ms-playwright\\chromium headless shell-1155\\chrome-winlheadless shell.exe\n   ```\n\n### 6. 使用\n\n<mark>在你的Claude Desktop 或者 Cursor 里直接你正常的工作就行了，必要时它会自动调用搜索接口获取内容</mark>，比如说，你整理下网络上2025年技术发展方向作为软件什么内容，它就会去调用搜索工具获取网络信息：\n\n- 配置好工具后，它的信息里面就知道有这个工具\n\n- 根据你的要求，它自动会分析判断需要用到搜索工具\n\n- 根据需求，提取关键词，调用搜索工具\n\n- 根据搜索返回内容，组织你要的结果\n\n有一点需要注意的，<mark>在Cursor里面，必须启用composer的agent模式工作才会生效</mark>，调用工具时，也需要你点下执行；\n\n## 技术内幕\n\n### 项目结构\n\n```shell\nsearch/\n├── __init__.py\n├── server.py              # MCP服务器实现\n└── proxy/                 # 搜索引擎代理\n    ├── brave/             # Brave搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── metaso/            # Metaso搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── bocha/             # 博查搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── brave_search.py    # Brave MCP工具实现\n    ├── metaso_search.py   # Metaso MCP工具实现\n    └── bocha_search.py    # 博查搜索MCP工具实现\n```\n\n### 接口参数\n\n#### Brave Search引擎\n\n- **search**\n  \n  - 执行网络搜索，支持分页和过滤\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `count` (number, 可选): 每页结果数量(最大20)\n    - `offset` (number, 可选): 分页偏移量(最大9)\n\n- **location_search**\n  \n  - 搜索地理位置相关信息（商家、餐厅等）\n  - 输入参数:\n    - `query` (string): 位置搜索关键词\n    - `count` (number, 可选): 结果数量(最大20)\n  - 无相关结果时自动切换到网络搜索\n\n#### Metaso搜索引擎\n\n- **search**\n  \n  - 执行网络搜索，支持多种模式\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `mode` (string, 可选): 搜索模式\n      - `concise`: 简洁模式，回答简短精炼\n      - `detail`: 深入模式，回答详细全面（默认）\n      - `research`: 研究模式，回答深度分析（<mark>目前暂不支持，逆向没有成功</mark>）\n\n- **scholar_search**\n  \n  - 执行学术搜索，专门用于查找学术资源\n  - 输入参数:\n    - `query` (string): 学术搜索关键词\n    - `mode` (string, 可选): 搜索模式，同上\n\n#### 博查搜索引擎\n\n- **search**\n  - 执行网络搜索，支持时间范围过滤和详细摘要\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `count` (number, 可选): 结果数量(1-10，默认10)\n    - `page` (number, 可选): 页码，从1开始\n    - `freshness` (string, 可选): 时间范围\n      - `noLimit`: 不限时间（默认）\n      - `oneDay`: 一天内\n      - `oneWeek`: 一周内\n      - `oneMonth`: 一月内\n      - `oneYear`: 一年内\n    - `summary` (boolean, 可选): 是否显示详细摘要，默认false\n  - 返回内容:\n    - 搜索统计信息（总结果数、当前页/总页数、本页结果数）\n    - 网页搜索结果（标题、URL、来源、摘要、发布时间）\n    - 相关图片信息（尺寸、来源、URL）\n\n##",
      "npm_url": "https://www.npmjs.com/package/mcp-search-server",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "fengin",
        "retrieval",
        "search fengin",
        "fengin search",
        "search server"
      ],
      "category": "web-search"
    },
    "fr0ziii--perplexity-mcp-server": {
      "owner": "fr0ziii",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/fr0ziii/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrate with Perplexity AI for chatting, searching, and retrieving documentation from various sources. Provides a standardized access method to Perplexity AI's capabilities for MCP-based systems.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "search",
        "searching",
        "perplexity",
        "perplexity ai",
        "perplexity mcp",
        "chatting searching"
      ],
      "category": "web-search"
    },
    "funwarioisii--cosense-mcp-server": {
      "owner": "funwarioisii",
      "name": "cosense-mcp-server",
      "url": "https://github.com/funwarioisii/cosense-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/funwarioisii.webp",
      "description": "Retrieve page data efficiently and integrate with Claude Desktop for streamlined information management. The server enhances application capabilities by providing real-time context through an MCP interface.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-29T13:47:55Z",
      "readme_content": "# cosense-mcp-server MCP Server\n\nMCP server for [cosense](https://cosen.se)\n\n## Features\n\n- Get Page\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Installation\n\n```bash\ngit clone https://github.com/funwarioisii/cosense-mcp-server.git\ncd cosense-mcp-server\nnpm run install\nnpm run build\n```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"cosense-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/cosense-mcp-server/build/index.js\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\"\n      }\n    }\n  }\n}\n```\n\n`COSENSE_SID` is optional.\nIf you want to use this server towards a private project, you need to set `COSENSE_SID`.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "search",
        "page",
        "web search",
        "mcp server",
        "page data"
      ],
      "category": "web-search"
    },
    "gabrimatic--mcp-web-search-tool": {
      "owner": "gabrimatic",
      "name": "mcp-web-search-tool",
      "url": "https://github.com/gabrimatic/mcp-web-search-tool",
      "imageUrl": "/freedevtools/mcp/pfp/gabrimatic.webp",
      "description": "Provides real-time web search capabilities, enabling AI assistants to retrieve current information from various sources through a modular architecture. Returns structured JSON results for enhanced context-aware responses in areas like weather, news, sports, and finance.",
      "stars": 13,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-20T16:43:48Z",
      "readme_content": "# MCP Web Search Tool\n\nA powerful Model Context Protocol (MCP) server providing real-time web search capabilities through pluggable search providers. Currently integrated with the [Brave Search API](https://api-dashboard.search.brave.com/app/documentation/web-search/get-started).\n\n\n\n## ✨ Features\n\n- **Real-Time Information Access**: Enables AI assistants to retrieve up-to-date information from the web\n- **Pluggable Search Providers**: Modular architecture allows for easy switching between different search engines\n- **Structured Output Format**: Returns search results in a clean, consistent JSON format\n- **Smart Query Handling**: Automatically categorizes queries and provides AI assistants with context-aware guidance\n\n## 📋 Requirements\n\n- **Node.js**: v16.x or newer\n- **npm**: v7.x or newer\n- **Brave Search API Key**: Required for accessing the Brave Search API\n\n## 🚀 Installation\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/gabrimatic/mcp-web-search-tool.git\n   cd mcp-web-search-tool\n   ```\n\n2. **Install Dependencies**:\n   ```bash\n   npm install\n   ```\n\n3. **Configure Environment Variables**:\n   Create a `.env` file in the project root:\n   ```\n   BRAVE_API_KEY=your_api_key_here\n   MAX_RESULTS=10 # Optional: Default is 10\n   REQUEST_TIMEOUT=10000 # Optional: Default is 10000ms\n   ```\n\n4. **Build the Project**:\n   ```bash\n   npm run build\n   ```\n\n## 💻 Usage\n\n### Starting the Server\n```bash\nnpm start\n```\n\n### Testing the Server\n```bash\nnode test-server.js\n```\n\n## Integration with Claude Desktop App _(optional)_\n\nOne of the most exciting aspects of this project is its seamless integration with the Claude Desktop app. This integration allows users to ask Claude questions that require real-time information, and Claude will automatically use the web search tool to provide up-to-date answers.\n\n### Configuration\n\n1. Create a `claude_desktop_config.json` file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-web-search\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/mcp-web-search-tool/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n\n2. Launch Claude Desktop with your configuration file.\n\n3. Ask Claude questions requiring real-time information, and it will automatically use the web search tool.\n\n###  🎥 YouTube Video\n\nWatch Claude in action using the MCP Web Search Tool for real-time AI browsing!\n\n[📺 Claude + MCP Web Search – Live Demo](https://youtu.be/6jAnjJSCL30?si=4n0-NtTyG_3SVaFh)\n\n## Example Queries:\n- \"What are analysts saying about the MVP race after tonight’s NBA games?\"\n- \"What are the latest news about artificial intelligence?\"\n- \"What's the weather like in New York today?\"\n- \"How is the stock market performing right now?\"\n\n## 🛠️ Available Tools\n\n#### Web Search\n- **Tool Name**: `web_search`\n- **Description**: Search the web for REAL-TIME information\n- **Necessarily For**: Weather, current events, sports scores, stock market updates\n- **Parameters**:\n  - `search_term` (string): The search term to look up\n  - `provider` (string, optional): Search provider (defaults to Brave)\n\n### Query Categories\nThe tool automatically categorizes queries into:\n- Weather information\n- Current events and news\n- Sports scores and results\n- Stock market and financial data\n- Time-sensitive information\n- General information seeking\n\n## 📜 License\n\n[MIT License](LICENSE)\n\n## 👨‍💻 Developer\nBy [Hossein Yousefpour](https://gabrimatic.info \"Hossein Yousefpour\")\n\n&copy; All rights reserved.\n\n## 📝 Medium Article\n\nRead more about the MCP Web Search Tool, its capabilities, and how it enhances AI-driven web search in our detailed Medium article:\n📖 [Deep Dive into MCP Web Search Tool](https://medium.com/@gabrimatic/introducing-mcp-web-search-tool-bridging-ai-assistants-to-real-time-web-information-5df9ab92ad02)\n\n\n## ☕ Support\n<a href=\"https://www.buymeacoffee.com/gabrimatic\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Book\" style=\"height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;\" ></a>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "web",
        "gabrimatic",
        "search gabrimatic",
        "web search",
        "mcp web"
      ],
      "category": "web-search"
    },
    "galihfr09--quran_cloud_mcp_server": {
      "owner": "galihfr09",
      "name": "quran_cloud_mcp_server",
      "url": "https://github.com/galihfr09/quran_cloud_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/galihfr09.webp",
      "description": "Connects AI models to the Quran API for precise access to Quranic text, minimizing hallucination by retrieving only necessary data upon request. Enables seamless integration with applications such as Claude desktop for enhanced reliability when handling sensitive religious content.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-07T07:39:09Z",
      "readme_content": "# quran_cloud_mcp_server\nMCP server to help LLMs to get access to Quran API (https://alquran.cloud/api) to prevent the hallucination with Quran text.\n\nhallucination is a big problem specially when you are working on sensitive data that each character is important.\n\none way of reducing the hallucination is by providing the context to your LLM but of course with large chunk of text like the holy Quran it's not efficient if you put all text in each request.\n\nSo, in this repo I have created an MCP server that's connect your LLM to a free API https://alquran.cloud/api that enables your model to retrieve only the data he needs.\n\nAlso, I will show to you how we can connect this MCP server to Claude desktop application.\n\n## Example of Claude the original response\n![Claude original response](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/original_claude_response.png?raw=true)\n\n## Example of Claude the new response after connecting to Search-Quran MCP server \n![Claude New response](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/new_claude_response.png?raw=true)\n\n## Installation\nmake sure you have python 3.13 & pip\n\nOpen your terminal and write these commands\n\n### Cloning\n```\ngit clone https://github.com/marwanWaly/quran_cloud_mcp_server.git\n```\n\n### Move to project directory\n```\ncd quran_cloud_mcp_server\n```\n\n### Create virtual environment\n```\npython -m venv .venv\n```\n\n### Activate venv\nOn Windows\n```\n.\\venv\\Scripts\\activate\n```\n\nOn Mac or linux\n```\nsource .venv/bin/activate\n```\n\n### Python packages installation\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install requirements.txt.\n```\npip install -r requirements.txt\n```\n\n### Create .env file\n```\nOPENAI_API_KEY=Your-secret-key\n```\n\n### Run in terminal\n```\npython client.py\n```\n\nnow you can directly chat with GPT4o in your terminal\n\n![server running in terminal](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/run_server_in_terminal.PNG)\n\n## Connect the server to Claude Desktop\nDownload [Claude](https://claude.ai/download) desktop and open it\n\n## Step 1\nSelect setting from the file menu\n\n![step 1](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-1.png)\n\n## Step 2\nClick on `Developer` then `Edit Config`\n\n![step 2](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-2.png)\n\n## Step 3\nOpen `claude_desktop_config.json`\n\n![step 3](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-3.png)\n\n## Step 4\nWrite this configuration in the file\n\n```\n{\n  \"mcpServers\": {\n    \"Search-Quran\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"PROJECT_PATH_ON_YOUR_PC\\\\server.py\"\n      ],\n      \"host\": \"127.0.0.1\",\n      \"port\": 8080,\n      \"timeout\": 30000\n    }\n  }\n}\n```\n\nDon't forget to replace `PROJECT_PATH_ON_YOUR_PC` with the absolute path to your project server\n\n![step 4](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-4.png)\n\n## Step 5\nRestart Claude app (make sure it's completely closed from your taskbar by right click on Claude icon and select `Quit`)\n\nCheck if the new MCP has been added\n\n![step 5](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-5.png)\n\nClick on tools icon \n\n![step 6](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-6.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quran_cloud_mcp_server",
        "quran",
        "quranic",
        "quran api",
        "access quranic",
        "galihfr09 quran_cloud_mcp_server"
      ],
      "category": "web-search"
    },
    "garylab--serper-mcp-server": {
      "owner": "garylab",
      "name": "serper-mcp-server",
      "url": "https://github.com/garylab/serper-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/garylab.webp",
      "description": "Provides Google search results to LLMs through Serper integration, enabling dynamic retrieval of up-to-date search information.",
      "stars": 15,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T17:00:37Z",
      "readme_content": "# Serper MCP Server\n[![smithery badge](https://smithery.ai/badge/@garylab/serper-mcp-server)](https://smithery.ai/server/@garylab/serper-mcp-server)\n\nA Model Context Protocol server that provides **Google Search via Serper**. This server enables LLMs to get search result information from Google.\n\n## Available Tools\n\n- `google_search` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_images` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_videos` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_places` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `google_search_maps` - Set [all the parameters](src/serper_mcp_server/schemas.py#L24)\n- `google_search_reviews` - Set [all the parameters](src/serper_mcp_server/schemas.py#L34)\n- `google_search_news` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_shopping` - Set [all the parameters](src/serper_mcp_server/schemas.py#L45)\n- `google_search_lens` - Set [all the parameters](src/serper_mcp_server/schemas.py#L50)\n- `google_search_scholar` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `google_search_parents` - Set [all the parameters](src/serper_mcp_server/schemas.py#L56)\n- `google_search_autocomplete` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `webpage_scrape` - Set [all the parameters](src/serper_mcp_server/schemas.py#L62)\n\n\n## Usage\n\n### Installing via Smithery\n\nTo install Serper MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@garylab/serper-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @garylab/serper-mcp-server --client claude\n```\n\n### Using `uv` (recommended)\n\n1. Make sure you had installed [`uv`](https://docs.astral.sh/uv/) on your os system.\n\n2. In your MCP client code configuration or **Claude** settings (file `claude_desktop_config.json`) add `serper` mcp server:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"uvx\",\n                \"args\": [\"serper-mcp-server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n    `uv` will download mcp server automatically using `uvx` from [pypi.org](https://pypi.org/project/serper-mcp-server/) and apply to your MCP client.\n\n### Using `pip` for project\n1. Add `serper-mcp-server` to your MCP client code `requirements.txt` file.\n    ```txt\n    serper-mcp-server\n    ```\n\n2. Install the dependencies.\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n3. Add the configuration for you client:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"python3\",\n                \"args\": [\"-m\", \"serper_mcp_server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n\n\n### Using `pip` for globally usage\n\n1. Make sure the `pip` or `pip3` is in your os system.\n    ```bash\n    pip install serper-mcp-server\n    # or\n    pip3 install serper-mcp-server\n    ```\n\n2. MCP client code configuration or **Claude** settings, add `serper` mcp server:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"python3\",\n                \"args\": [\"serper-mcp-server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For `uvx` installations:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx serper-mcp-server\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```bash\ngit clone https://github.com/garylab/serper-mcp-server.git\ncd serper-mcp-server\nnpx @modelcontextprotocol/inspector uv run serper-mcp-server -e SERPER_API_KEY=<the key>\n```\n\n\n## License\n\nserper-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "serper",
        "search",
        "google",
        "llms serper",
        "web search",
        "search results"
      ],
      "category": "web-search"
    },
    "geezerrrr--exa-mcp-server": {
      "owner": "geezerrrr",
      "name": "exa-mcp-server",
      "url": "https://github.com/geezerrrr/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/geezerrrr.webp",
      "description": "Provides real-time web search capabilities using Exa's robust search API, enabling AI models to retrieve and cache web information in a secure manner while offering customizable search parameters.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-22T08:47:51Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Caches recent searches as resources for reference\n- Handles rate limiting and error cases gracefully\n- Supports real-time web crawling for fresh content\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n```\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Create a global link (this makes the server executable from anywhere):\n\n```bash\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1. Open your Claude Desktop configuration:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1. Open your Claude Desktop configuration:\n\n```powershell\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### 2. Add the Exa server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1. Completely quit Claude Desktop (not just close the window)\n2. Start Claude Desktop again\n3. Look for the 🔌 icon to verify the Exa server is connected\n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\n```\nSearch for today's breaking news about tech.\n```\n\n```\nSearch for the top 10 AI research papers from 2023, and only use live crawling as a fallback.\n```\n\n```\nSearch for electric vehicles and return 3 results, always using live crawling.\n```\n\nThe server will:\n\n1. Process the search request\n2. Query the Exa API with optimal settings (including live crawling)\n3. Return formatted results to Claude\n4. Cache the search for future reference\n\n## Features ✨\n\n* **Simplified Web Search Tool**: Enables Claude to search the web with just a query parameter\n* **Customizable Search Parameters**: Control the number of results and live crawling strategy\n* **Automatic Live Crawling**: Uses real-time crawling based on specified strategy\n* **Preset Optimal Parameters**: Uses best defaults for result count and character limits\n* **Search Caching**: Saves recent searches as resources for reference\n* **Error Handling**: Gracefully handles API errors and rate limits\n* **Type Safety**: Full TypeScript implementation with Zod validation\n* **MCP Compliance**: Fully implements the latest MCP protocol specification\n\n## Testing with MCP Inspector 🔍\n\nYou can test the server directly using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\nThis opens an interactive interface where you can explore the server's capabilities, execute search queries, and view cached search results.\n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify the npm link is correctly set up\n   * Check Claude Desktop configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Confirm your EXA_API_KEY is valid\n   * Check the EXA_API_KEY is correctly set in the Claude Desktop config\n   * Verify no spaces or quotes around the API key\n\n3. **Connection Issues**\n   * Restart Claude Desktop completely\n   * Check Claude Desktop logs:\n   \n   ```bash\n   # macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type \"%APPDATA%\\Claude\\logs\\mcp*.log\"\n   ```\n\n### Getting Help\n\nIf you encounter issues, review the [MCP Documentation](https://modelcontextprotocol.io) or visit the [GitHub discussions](https://github.com/orgs/modelcontextprotocol/discussions) for community support.\n\n## Acknowledgments 🙏\n\n* [Exa AI](https://exa.ai) for their powerful search API\n* [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n* [Anthropic](https://anthropic.com) for Claude Desktop\n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "search",
        "exa",
        "web",
        "search geezerrrr",
        "geezerrrr exa",
        "web search"
      ],
      "category": "web-search"
    },
    "gianlucamazza--mcp-duckduckgo": {
      "owner": "gianlucamazza",
      "name": "mcp-duckduckgo",
      "url": "https://github.com/gianlucamazza/mcp-duckduckgo",
      "imageUrl": "/freedevtools/mcp/pfp/gianlucamazza.webp",
      "description": "Provides web search functionality using DuckDuckGo with advanced navigation and content exploration features, allowing users to retrieve detailed search results and related queries.",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T13:15:10Z",
      "readme_content": "# MCP DuckDuckGo Search Plugin\n\nA DuckDuckGo search plugin for Model Context Protocol (MCP), compatible with Claude Code. Provides web search functionality with advanced navigation and content exploration features.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub](https://img.shields.io/github/stars/gianlucamazza/mcp-duckduckgo?style=social)](https://github.com/gianlucamazza/mcp-duckduckgo)\n\n## Description\n\nThis project implements a Model Context Protocol (MCP) server that provides web search functionality using DuckDuckGo. The plugin is designed to work seamlessly with Claude Code or any other client that supports MCP, offering not just basic search capabilities but also advanced navigation and result exploration features.\n\n## Features\n\n- **Web Search Tool**: Perform web searches using DuckDuckGo\n- **Detailed Results**: Get detailed information about specific search results\n- **Related Searches**: Discover related search queries based on your original search\n- **Pagination Support**: Navigate through multiple pages of search results\n- **Domain Extraction**: View domain information for each search result\n- **Advanced Filtering**: Filter results by site and time period\n- **Enhanced Content Extraction**: Extract rich content from webpages including metadata, structure, and snippets\n- **Basic Web Spidering**: Follow links from search results to explore related content (configurable depth)\n- **Metadata Extraction**: Extract titles, authors, keywords, publication dates, and more\n- **Social Media Detection**: Identify and extract social media links from webpages\n- **Content Structure Analysis**: Extract headings and sections to understand webpage structure\n- **Search Documentation**: Access comprehensive documentation about the search functionality\n- **Search Assistant**: Get help formulating effective search queries\n- **Parameterized Resource**: Retrieve formatted search results for specific queries\n\n## Requirements\n\n- Python 3.9 or higher\n- Package manager: `uv` (recommended) or `pip`\n- Python packages listed in `pyproject.toml`\n\n## Installation\n\n### From PyPI\n\n*Note: This package is not yet published to PyPI. Please install from source below.*\n\nIn the future, once published, you'll be able to install with:\n\n```bash\npip install mcp-duckduckgo\n```\n\n### From Source\n\n#### Using uv (Recommended)\n\n[uv](https://github.com/astral-sh/uv) is a fast Python package manager that provides better dependency resolution and faster installs.\n\n1. Install uv if you haven't already:\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. Clone and install as a tool:\n   ```bash\n   git clone https://github.com/gianlucamazza/mcp-duckduckgo.git\n   cd mcp-duckduckgo\n   uv tool install .\n   ```\n\n   Or install directly from the repository:\n   ```bash\n   uv tool install git+https://github.com/gianlucamazza/mcp-duckduckgo.git\n   ```\n\n#### Using pip\n\n1. Clone this repository:\n\n   ```bash\n   git clone https://github.com/gianlucamazza/mcp-duckduckgo.git\n   cd mcp-duckduckgo\n   ```\n\n2. Install the package in development mode:\n\n   ```bash\n   pip install -e .\n   ```\n\n   Or use the provided script:\n\n   ```bash\n   ./scripts/install_dev.sh\n   ```\n\n   Or use Make:\n\n   ```bash\n   make install\n   ```\n\n## Usage\n\n### Starting the Server Manually\n\nTo start the MCP server:\n\n```bash\nmcp-duckduckgo\n```\n\nOr with custom parameters:\n\n```bash\nmcp-duckduckgo --port 8000\n```\n\nOr use the provided script for development:\n\n```bash\n./scripts/run.sh\n```\n\nOr use Make:\n\n```bash\nmake run\n```\n\n### Environment Variables\n\nThe MCP server can be configured using environment variables:\n\n- `MCP_PORT`: Set the port number for the server (default: 3000)\n\nExample usage:\n\n```bash\n# Set port via environment variable\nexport MCP_PORT=8080\nmcp-duckduckgo\n\n# Or set it inline\nMCP_PORT=8080 mcp-duckduckgo\n```\n\nNote: The `--port` command-line argument takes precedence over the `MCP_PORT` environment variable.\n\n### Using with Claude Code\n\n1. Install the package from source as described above.\n\n2. Configure Claude Code to use the plugin:\n\n   ```bash\n   claude mcp add duckduckgo-search -- mcp-duckduckgo\n   ```\n\n3. For global configuration (available in all projects):\n\n   ```bash\n   claude mcp add duckduckgo-search --scope user -- mcp-duckduckgo\n   ```\n\n4. Start Claude Code:\n\n   ```bash\n   claude\n   ```\n\n5. Now you can use the DuckDuckGo search functionality within Claude Code.\n\n## Available Endpoints\n\nThe plugin provides the following endpoints:\n\n### Tool: `duckduckgo_web_search`\n\nPerforms a web search using DuckDuckGo with the following parameters:\n\n- `query` (required): The search query (max 400 characters, 50 words)\n- `count` (optional, default: 10): Number of results per page (1-20)\n- `page` (optional, default: 1): Page number for pagination\n- `site` (optional): Limit results to a specific site (e.g., 'example.com')\n- `time_period` (optional): Filter results by time period ('day', 'week', 'month', 'year')\n\nExample usage in Claude Code:\n\n```text\nSearch for \"artificial intelligence latest developments\"\n```\n\n### Tool: `duckduckgo_get_details`\n\nRetrieves detailed information about a specific search result:\n\n- `url` (required): URL of the result to get details for\n\nExample usage in Claude Code:\n\n```text\nGet details for \"https://example.com/article\"\n```\n\n### Tool: `duckduckgo_related_searches`\n\nSuggests related search queries based on the original query:\n\n- `query` (required): Original search query (max 400 characters)\n- `count` (optional, default: 5): Number of related searches to return (1-10)\n\nExample usage in Claude Code:\n\n```text\nFind related searches for \"renewable energy\"\n```\n\n### Resource: `docs://search`\n\nProvides comprehensive documentation about the search functionality.\n\nExample usage in Claude Code:\n\n```text\nShow me the documentation for the DuckDuckGo search\n```\n\n### Prompt: `search_assistant`\n\nHelps formulate effective search queries.\n\nExample usage in Claude Code:\n\n```text\nHelp me formulate a search query about climate change solutions\n```\n\n### Resource: `search://{query}`\n\nRetrieves formatted search results for a specific query.\n\nExample usage in Claude Code:\n\n```text\nGet search results for \"quantum computing breakthroughs\"\n```\n\n## Using the Navigation Features\n\nThe plugin provides several features to help navigate and explore search results:\n\n### Pagination\n\nTo navigate through multiple pages of search results:\n\n```text\nSearch for \"climate change solutions\" with 5 results per page, page 2\n```\n\n### Filtering Results\n\nTo filter results by specific site:\n\n```text\nSearch for \"machine learning tutorials\" on \"tensorflow.org\"\n```\n\nTo filter results by time period:\n\n```text\nSearch for \"latest news\" from the past week\n```\n\n### Exploring Result Details\n\nTo get more information about a specific search result:\n\n```text\nGet details for \"https://example.com/article-found-in-search\"\n```\n\n### Finding Related Searches\n\nTo discover related search queries:\n\n```text\nFind related searches for \"electric vehicles\"\n```\n\nThese navigation features can be combined with Claude's natural language capabilities to create a powerful search and exploration experience. For example:\n\n```text\nSearch for \"python machine learning libraries\", then get details on the top result, and finally show me related search terms\n```\n\n## Implementation Notes\n\nThis implementation uses DuckDuckGo's public web interface and parses the HTML response to extract results. This approach is used for demonstration purposes, as DuckDuckGo does not offer an official search API. In a production environment, it's recommended to use a search service with an official API.\n\n## Enhanced Content Extraction\n\nThe DuckDuckGo plugin includes advanced content extraction capabilities that go beyond simple search results:\n\n### Content Extraction Features\n\n- **Full Webpage Analysis**: Extract and parse HTML content from search result URLs\n- **Intelligent Content Targeting**: Identify and extract main content areas from different types of websites\n- **Rich Metadata Extraction**: Extract titles, descriptions, authors, keywords, and publication dates\n- **Image Detection**: Identify and extract main images and media from webpages\n- **Social Media Integration**: Detect and extract links to social media profiles\n- **Content Structure Analysis**: Extract headings and sections to understand webpage organization\n- **Official Source Detection**: Identify whether a source is official based on domain and content signals\n\n### Web Spidering Capabilities\n\nThe plugin includes basic web spidering functionality:\n\n- **Configurable Depth**: Follow links from 0 to 3 levels deep from the original URL\n- **Link Limitation**: Control the maximum number of links to follow per page (1-5)\n- **Domain Restriction**: Option to only follow links within the same domain\n- **Related Content Discovery**: Find and analyze content related to the original search\n\n### Using Enhanced Content Extraction\n\nTo use the enhanced content extraction features:\n\n```text\nGet details for \"https://example.com/article\" with spider depth 1\n```\n\nTo control spidering behavior:\n\n```text\nGet details for \"https://example.com/article\" with spider depth 2, max links 3, same domain only\n```\n\n## Development\n\nThe project includes several utility scripts in the `scripts` directory to help with development:\n\n- `install_dev.sh`: Sets up the development environment\n- `run.sh`: Runs the MCP server with development settings\n- `test.sh`: Runs tests with coverage reporting\n- `lint.sh`: Runs linting and code formatting\n- `publish.sh`: Builds and publishes the package to PyPI\n\nFor convenience, a Makefile is also provided with the following targets:\n\n```bash\nmake install  # Install the package in development mode\nmake test     # Run tests with coverage\nmake lint     # Run linting and code formatting\nmake run      # Run the MCP server\nmake publish  # Build and publish the package to PyPI\nmake clean    # Clean build artifacts\nmake all      # Run install, lint, and test (default)\nmake help     # Show help message\n```\n\n### Testing\n\nThe project includes a comprehensive test suite covering all major functionality. Tests are located in the `tests/` directory.\n\n#### Installing Test Dependencies\n\nBefore running the tests, install the test dependencies:\n\n```bash\npip install -e \".[test]\"\n```\n\n#### Running Tests\n\nYou can run all tests with:\n\n```bash\npytest\n```\n\nTo run tests with coverage reporting:\n\n```bash\npytest --cov=mcp_duckduckgo\n```\n\nTo run a specific test file:\n\n```bash\npytest tests/test_models.py\n```\n\nTo run tests with verbose output:\n\n```bash\npytest -v\n```\n\nOr use the provided script:\n\n```bash\n./scripts/test.sh\n```\n\nOr use Make:\n\n```bash\nmake test\n```\n\n#### Test Structure\n\nThe test suite is organized as follows:\n\n- `conftest.py` - Shared fixtures and configurations for tests\n- `test_models.py` - Tests for data models\n- `test_search.py` - Tests for search functionality\n- `test_tools.py` - Tests for MCP tools\n- `test_resources.py` - Tests for MCP resources\n- `test_integration.py` - End-to-end integration tests\n- `test_server.py` - Server lifecycle tests\n\nFor more details about testing, see the [tests/README.md](tests/README.md) file.\n\n### Code Formatting and Linting\n\n```bash\nblack mcp_duckduckgo\nisort mcp_duckduckgo\nmypy mcp_duckduckgo\n```\n\nOr use the provided script:\n\n```bash\n./scripts/lint.sh\n```\n\nOr use Make:\n\n```bash\nmake lint\n```\n\n### Publishing to PyPI\n\nIf you want to publish the package to PyPI:\n\n1. Update the version in `pyproject.toml`\n2. Ensure you have the necessary credentials and tools:\n   ```bash\n   pip install build twine\n   ```\n3. Build and publish:\n   ```bash\n   python -m build\n   twine upload dist/*\n   ```\n\nOr use the provided script if available:\n\n```bash\n./scripts/publish.sh\n```\n\nOr use Make:\n\n```bash\nmake publish\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\n[MIT](LICENSE)\n\n## Repository\n\n[GitHub Repository](https://github.com/gianlucamazza/mcp-duckduckgo)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "search",
        "gianlucamazza",
        "duckduckgo provides",
        "using duckduckgo",
        "duckduckgo advanced"
      ],
      "category": "web-search"
    },
    "gradusnikov--google-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "google-search-mcp-server",
      "url": "https://github.com/gradusnikov/google-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Integrate Google Custom Search to retrieve and display search results directly in applications. Enhance applications with the ability to access vast amounts of online information efficiently.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T10:02:29Z",
      "readme_content": "# Google Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/google-search-mcp-server)](https://smithery.ai/server/@gradusnikov/google-search-mcp-server)\n\nA Model Context Protocol (MCP) server that provides Google Custom Search functionality.\n\n## Installation\n\n### Installing via Smithery\n\nTo install google-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/google-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/google-search-mcp-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n```bash\ngit clone https://github.com/gradusnikov/google-search-mpc-server.git\ncd google-search-mpc-server\n```\n\n2. Install dependencies:\n```bash\npip install fastmcp google-api-python-client python-dotenv\n```\n\n## Configuration\n\nCreate a `.env` file in the project root with the following variables:\n\n```\nGOOGLE_API_KEY=your_google_api_key\nGOOGLE_CSE_ID=your_custom_search_engine_id\n```\n\nTo obtain these credentials:\n1. Create a Google Cloud project and enable the Custom Search API\n2. Generate an API key from the Google Cloud Console\n3. Create a Custom Search Engine at https://cse.google.com/cse/all and get its ID\n\n## Usage\n\nStart the server using MCP:\n\n```bash\nmcp run google_search_mcp_server.py\n```\n\nor add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n```\n\"google-search\": {\n            \"command\": \"wsl.exe\",\n            \"args\": [\n                \"bash\",\n                \"-c\",\n                \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && /home/[user]/anaconda3/bin/mcp run /home/[user]/google-search-mpc-server/google_search_mcp_server.py\"\n            ]\n        },\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "web",
        "search results",
        "custom search",
        "web search"
      ],
      "category": "web-search"
    },
    "gregkop--sketchfab-mcp-server": {
      "owner": "gregkop",
      "name": "sketchfab-mcp-server",
      "url": "https://github.com/gregkop/sketchfab-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gregkop.webp",
      "description": "Search, view details, and download 3D models from Sketchfab, utilizing keywords, tags, and categories to find models and obtain comprehensive information about them. Supports downloading in multiple formats for creative projects.",
      "stars": 28,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:12:28Z",
      "readme_content": "# Sketchfab MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Sketchfab's 3D model platform. This MCP allows you to search, view details, and download 3D models from Sketchfab directly through Claude or Cursor.\n\n## Features\n\n- **Search for 3D Models**: Find models on Sketchfab using keywords, tags, and categories\n- **View Model Details**: Get comprehensive information about specific models\n- **Download Models**: Download models in various formats (gltf, glb, usdz, source)\n\n## Prerequisites\n\n- Node.js 18 or higher\n- A Sketchfab API key (for authentication)\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Build the project:\n   ```\n   npm run build\n   ```\n\n## Usage\n\n### Running the MCP Server\n\n```\nnpm start\n```\n\nTo provide your Sketchfab API key, use the `--api-key` parameter:\n\n```\nnode build/index.js --api-key YOUR_API_KEY\n```\n\nAlternatively, you can set the `SKETCHFAB_API_KEY` environment variable:\n\n```\nexport SKETCHFAB_API_KEY=YOUR_API_KEY\nnpm start\n```\n\n### Available Tools\n\n#### 1. sketchfab-search\n\nSearch for 3D models on Sketchfab based on keywords and filters.\n\nParameters:\n- `query` (optional): Text search query (e.g., \"car\", \"house\", \"character\")\n- `tags` (optional): Filter by specific tags (e.g., [\"animated\", \"rigged\", \"pbr\"])\n- `categories` (optional): Filter by categories (e.g., [\"characters\", \"architecture\", \"vehicles\"])\n- `downloadable` (optional): Set to true to show only downloadable models\n- `limit` (optional): Maximum number of results to return (1-24, default: 10)\n\n#### 2. sketchfab-model-details\n\nGet detailed information about a specific Sketchfab model.\n\nParameters:\n- `modelId`: The unique ID of the Sketchfab model\n\n#### 3. sketchfab-download\n\nDownload a 3D model from Sketchfab.\n\nParameters:\n- `modelId`: The unique ID of the Sketchfab model to download\n- `format` (optional): Preferred format to download the model in (gltf, glb, usdz, source)\n- `outputPath` (optional): Local directory or file path to save the downloaded file\n\n## Using with Cursor\n\n1. Go to Cursor Settings -> MCP -> Add new MCP server\n2. Configure your MCP:\n   - Name: Sketchfab MCP\n   - Type: command\n   - Command: `node /path/to/build/index.js --api-key YOUR_API_KEY`\n\n## Using with Claude Desktop\n\nAdd the following MCP config to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"sketchfab\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/build/index.js\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n## Environment Variables\n\nYou can set the following environment variables:\n\n- `SKETCHFAB_API_KEY`: Your Sketchfab API key (alternative to passing it with the --api-key parameter)\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/sketchfab-mcp-server",
      "npm_downloads": 3623,
      "keywords": [
        "sketchfab",
        "3d",
        "models",
        "models sketchfab",
        "sketchfab mcp",
        "sketchfab utilizing"
      ],
      "category": "web-search"
    },
    "happyhackerbird--jobspy-mcp-server": {
      "owner": "happyhackerbird",
      "name": "jobspy-mcp-server",
      "url": "https://github.com/happyhackerbird/jobspy-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/happyhackerbird.webp",
      "description": "Facilitates job searching on LinkedIn by utilizing the unofficial LinkedIn API for credential management and job scraping. Matches job opportunities using JobSpy for enhanced search capabilities.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-24T14:09:31Z",
      "readme_content": "# MCP Server for Job Searching\n## Overview\nA Model Context Protocol (MCP) server designed to facilitate seamless job searching on LinkedIn, leveraging the unofficial LinkedIn API for client credential management. Based off [JobSpy](https://github.com/cullenwatson/JobSpy). \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkedin",
        "jobspy",
        "happyhackerbird",
        "happyhackerbird jobspy",
        "searching linkedin",
        "linkedin api"
      ],
      "category": "web-search"
    },
    "hesreallyhim--mcp-server-isitdown": {
      "owner": "hesreallyhim",
      "name": "mcp-server-isitdown",
      "url": "https://github.com/hesreallyhim/mcp-server-isitdown",
      "imageUrl": "/freedevtools/mcp/pfp/hesreallyhim.webp",
      "description": "Checks the status of websites to determine if they are currently down and provides information on recent downtime events.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-05T10:25:53Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hesreallyhim-mcp-server-isitdown-badge.png)](https://mseep.ai/app/hesreallyhim-mcp-server-isitdown)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/24e83a9a-3b58-48c5-a490-04d6a631ed1e)\n# mcp-server-isitdown\n\nAn MCP server that checks if a website is currently down by querying [www.isitdownrightnow.com](https://www.isitdownrightnow.com).\n\n[![smithery badge](https://smithery.ai/badge/@hesreallyhim/mcp-server-isitdown)](https://smithery.ai/server/@hesreallyhim/mcp-server-isitdown)\n<a href=\"https://glama.ai/mcp/servers/1wx4z4amkm\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/1wx4z4amkm/badge\" alt=\"IsItDown Server MCP server\" />\n</a>\n\n## Overview\n\nThis MCP server provides a simple tool to check if a website is experiencing downtime, and can provide some information about recent downtime events.\n\n## Tools\n\nThe following tools are implemented:\n\n* **`get_website_status`**: Checks if a website is currently down or not.\n  * **`Input`**: `root_domain` (string): The root domain of the website to check (e.g., \"example.com\")\n  * **`Output`**: A string message indicating whether the website is up or down, with the last recorded downtime information\n\n## Installation\n\n### Installing via Smithery\n\nTo install IsItDown Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hesreallyhim/mcp-server-isitdown):\n\n```bash\nnpx -y @smithery/cli install @hesreallyhim/mcp-server-isitdown --client claude\n```\n\n> **Note**: This package is not currently published to a public registry. Installation is only available from source.\n\n### From Source\n\n```bash\n# Clone the repository \ngit clone https://github.com/yourusername/mcp-server-isitdown.git\ncd mcp-server-isitdown\n\n# Using uv (recommended)\nuv pip install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration for Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json` file:\n\n```json\n\"isitdown\": {\n  \"command\": \"/path/to/uv\",\n  \"args\": [\n    \"--directory\",\n    \"/path/to/cloned/repo/src\",\n    \"run\",\n    \"mcp_server_isitdown\"\n  ]\n}\n```\n\n## Usage\n\n### Run as a standalone MCP server\n\n```bash\n# Using the installed script\nmcp-server-isitdown\n\n# Using the Python module\npython -m mcp_server_isitdown\n```\n\n### Example usage with Claude for Dekstop:\n\n* \"Is wikipedia down right now?\"\n* \"When was the last time reddit was down?\"\n\n### Use as a library\n\n```python\nfrom mcp_server_isitdown.server import get_website_status\n\n# Check if a website is down (async function)\nasync def check_website():\n    result = await get_website_status(\"example.com\")\n    print(result)  # Prints status message with up/down status\n```\n\n## Development\n\n```bash\n# Type checking\nuvx mypy .\n\n# Run all pre-commit hooks\nuv pre-commit run --all-files\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n\n# Run the Inspector\nmcp dev src/mcp_server_isitdown/server.py\n```\n\n## Build\n\n```bash\n# Build the package\nuv build\n\n# Install the built package\nuv pip install dist/mcp_isitdown_service-*.whl\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "isitdown",
        "downtime",
        "mcp",
        "server isitdown",
        "isitdown checks",
        "downtime events"
      ],
      "category": "web-search"
    },
    "highlight-ing--highlight-youtube-mcp": {
      "owner": "highlight-ing",
      "name": "highlight-youtube-mcp",
      "url": "https://github.com/highlight-ing/highlight-youtube-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/highlight-ing.webp",
      "description": "Extract transcripts from YouTube videos by providing a video URL. The server supports multiple URL formats and returns the transcript text in a structured array format.",
      "stars": 1,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-10T02:12:46Z",
      "readme_content": "# YouTube Integration\n\nThe YouTube MCP server provides functionality to extract transcripts from YouTube videos.\n\n## Available Tools\n\n### get_youtube_transcript\nRetrieves the transcript text from a YouTube video.\n\n**Parameters**:\n- `videoUrl`: Full YouTube video URL (supports standard, shortened, and embed URLs)\n\n**Returns**: Object containing:\n- `content`: Array with transcript text\n\n## URL Support\n\nHandles multiple YouTube URL formats:\n- Standard: `https://www.youtube.com/watch?v=VIDEO_ID`\n- Shortened: `https://youtu.be/VIDEO_ID`\n- Embed: `https://www.youtube.com/embed/VIDEO_ID`\n\n## Error Handling\n\nThe server implements standard error handling:\n- Invalid URLs return `ErrorCode.InvalidParams`\n- Missing URL returns `ErrorCode.InvalidParams`\n- Failed transcript fetches return formatted error messages\n- Graceful shutdown on SIGINT\n\n## Technical Details\n\n- Built using the Highlight AI MCP SDK\n- Uses youtube-transcript library\n- Input validation via Zod\n- Runs as a stdio-based MCP server\n- Supports Node.js >=18.0.0\n\n## Limitations\n\n- Only works with videos that have captions enabled\n- Currently only returns English transcripts\n- Rate limits depend on YouTube's API restrictions\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcript",
        "transcripts",
        "youtube",
        "transcripts youtube",
        "highlight youtube",
        "youtube mcp"
      ],
      "category": "web-search"
    },
    "iamadk--reference-servers": {
      "owner": "iamadk",
      "name": "reference-servers",
      "url": "https://github.com/iamadk/reference-servers",
      "imageUrl": "/freedevtools/mcp/pfp/iamadk.webp",
      "description": "Retrieve and process web content by converting HTML to markdown, enabling easier extraction of information in manageable chunks. Supports starting extraction from a specified character index for better navigation of web content.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-12T12:21:35Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the Typescript and Python SDK.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[cognee-mcp](https://github.com/topoteretes/cognee-mcp-server)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, and plain text, with other formats like PDF, csv and docx in development.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's RAG Web Browser Actor to perform web searches, scrape URLs, and return content in Markdown.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers.\n\n* [EasyMCP](https://github.com/zcaceres/easy-mcp/) (TypeScript)\n* [FastMCP](https://github.com/punkpeye/fastmcp) (TypeScript)\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "html",
        "search",
        "search iamadk",
        "web search",
        "web content"
      ],
      "category": "web-search"
    },
    "ibrooksSDX--mcp-server-opensearch": {
      "owner": "ibrooksSDX",
      "name": "mcp-server-opensearch",
      "url": "https://github.com/ibrooksSDX/mcp-server-opensearch",
      "imageUrl": "/freedevtools/mcp/pfp/ibrooksSDX.webp",
      "description": "Create a semantic memory layer on top of OpenSearch to facilitate LLM applications by integrating external data sources and tools.",
      "stars": 4,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-06-05T06:01:06Z",
      "readme_content": "# mcp-server-opensearch: An OpenSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@ibrooksSDX/mcp-server-opensearch)](https://smithery.ai/server/@ibrooksSDX/mcp-server-opensearch)\n\n> The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis repository is an example of how to create a MCP server for [OpenSearch](https://opensearch.org/), a distributed search and analytics engine.\n\n# Under Contruction \n\n\n\n\n\n## Current Blocker - Async Client from OpenSearch isn't installing\n\n[Open Search Async Client Docs](https://github.com/opensearch-project/opensearch-py/blob/main/guides/async.m) \n\n```shell\npip install opensearch-py[async]\nzsh: no matches found: opensearch-py[async]\n```\n\n## Overview \n\nA basic Model Context Protocol server for keeping and retrieving memories in the OpenSearch engine.\nIt acts as a semantic memory layer on top of the OpenSearch database.\n\n## Components\n\n### Tools\n\n1. `search-openSearch`\n   - Store a memory in the OpenSearch database\n   - Input:\n     - `query` (json): prepared json query message\n   - Returns: Confirmation message\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-opensearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ibrooksSDX/mcp-server-opensearch):\n\n```bash\nnpx -y @smithery/cli install @ibrooksSDX/mcp-server-opensearch --client claude\n```\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed to directly run *mcp-server-opensearch*.\n\n```shell\nuv run mcp-server-opensearch \\\n  --opensearch-url \"http://localhost:9200\" \\\n  --index-name \"my_index\" \\\n```\nor \n\n```shell\nuv run fastmcp run demo.py:main\n```\n\n## Testing - Local Open Search Client\n\n\n\n```shell\nuv run python src/mcp-server-opensearch/test_opensearch.py\n```\n## Testing - MCP Server Connection to Open Search Client\n\n\n\n\n```shell\ncd src/mcp-server-opensearch\nuv run fastmcp dev demo.py\n```\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"opensearch\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-opensearch\",\n      \"--opensearch-url\",\n      \"http://localhost:9200\",\n      \"--opensearch-api-key\",\n      \"your_api_key\",\n      \"--index-name\",\n      \"your_index_name\"\n    ]\n  }, \"Demo\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"opensearch-py\",\n        \"fastmcp\",\n        \"run\",\n        \"/Users/ibrooks/Documents/GitHub/mcp-server-opensearch/src/mcp-server-opensearch/demo.py\"\n      ]\n    }\n}\n```\n\nOr use the FastMCP UI to install the server to Claude\n\n```shell\nuv run fastmcp install demo.py\n```\n\n## Environment Variables\n\nThe configuration of the server can be also done using environment variables:\n\n- `OPENSEARCH_HOST`: URL of the OpenSearch server, e.g. `http://localhost`\n- `OPENSEARCH_HOSTPORT`: Port of the host of the OpenSearch server `9200`\n- `INDEX_NAME`: Name of the index to use",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opensearch",
        "search",
        "llm",
        "opensearch facilitate",
        "server opensearch",
        "layer opensearch"
      ],
      "category": "web-search"
    },
    "icraft2170--youtube-data-mcp-server": {
      "owner": "icraft2170",
      "name": "youtube-data-mcp-server",
      "url": "https://github.com/icraft2170/youtube-data-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/icraft2170.webp",
      "description": "Interact with YouTube content to retrieve video details, manage captions, and analyze trends using the YouTube Data API for real-time insights.",
      "stars": 47,
      "forks": 23,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T05:15:44Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/icraft2170-youtube-data-mcp-server-badge.png)](https://mseep.ai/app/icraft2170-youtube-data-mcp-server)\n\n# YouTube MCP Server\n[![smithery badge](https://smithery.ai/badge/@icraft2170/youtube-data-mcp-server)](https://smithery.ai/server/@icraft2170/youtube-data-mcp-server)\n\nA Model Context Protocol (MCP) server implementation utilizing the YouTube Data API. It allows AI language models to interact with YouTube content through a standardized interface.\n\n## Key Features\n\n### Video Information\n* Retrieve detailed video information (title, description, duration, statistics)\n* Search for videos by keywords\n* Get related videos based on a specific video\n* Calculate and analyze video engagement ratios\n\n### Transcript/Caption Management\n* Retrieve video captions with multi-language support\n* Specify language preferences for transcripts\n* Access time-stamped captions for precise content reference\n\n### Channel Analysis\n* View detailed channel statistics (subscribers, views, video count)\n* Get top-performing videos from a channel\n* Analyze channel growth and engagement metrics\n\n### Trend Analysis\n* View trending videos by region and category\n* Compare performance metrics across multiple videos\n* Discover popular content in specific categories\n\n## Available Tools\n\nThe server provides the following MCP tools:\n\n| Tool Name | Description | Required Parameters |\n|-----------|-------------|---------------------|\n| `getVideoDetails` | Get detailed information about multiple YouTube videos including metadata, statistics, and content details | `videoIds` (array) |\n| `searchVideos` | Search for videos based on a query string | `query`, `maxResults` (optional) |\n| `getTranscripts` | Retrieve transcripts for multiple videos | `videoIds` (array), `lang` (optional) |\n| `getRelatedVideos` | Get videos related to a specific video based on YouTube's recommendation algorithm | `videoId`, `maxResults` (optional) |\n| `getChannelStatistics` | Retrieve detailed metrics for multiple channels including subscriber count, view count, and video count | `channelIds` (array) |\n| `getChannelTopVideos` | Get the most viewed videos from a specific channel | `channelId`, `maxResults` (optional) |\n| `getVideoEngagementRatio` | Calculate engagement metrics for multiple videos (views, likes, comments, and engagement ratio) | `videoIds` (array) |\n| `getTrendingVideos` | Get currently popular videos by region and category | `regionCode` (optional), `categoryId` (optional), `maxResults` (optional) |\n| `compareVideos` | Compare statistics across multiple videos | `videoIds` (array) |\n\n## Installation\n\n### Automatic Installation via Smithery\n\nAutomatically install YouTube MCP Server for Claude Desktop via [Smithery](https://smithery.ai/server/@icraft2170/youtube-data-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @icraft2170/youtube-data-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\n# Install from npm\nnpm install youtube-data-mcp-server\n\n# Or clone repository\ngit clone https://github.com/icraft2170/youtube-data-mcp-server.git\ncd youtube-data-mcp-server\nnpm install\n```\n\n## Environment Configuration\nSet the following environment variables:\n* `YOUTUBE_API_KEY`: YouTube Data API key (required)\n* `YOUTUBE_TRANSCRIPT_LANG`: Default caption language (optional, default: 'ko')\n\n## MCP Client Configuration\nAdd the following to your Claude Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"youtube-data-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"YOUTUBE_TRANSCRIPT_LANG\": \"ko\"\n      }\n    }\n  }\n}\n```\n\n## YouTube API Setup\n1. Access Google Cloud Console\n2. Create a new project or select an existing one\n3. Enable YouTube Data API v3\n4. Create API credentials (API key)\n5. Use the generated API key in your environment configuration\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build\nnpm run build\n```\n\n## Network Configuration\n\nThe server exposes the following ports for communication:\n- HTTP: 3000\n- gRPC: 3001\n\n## System Requirements\n- Node.js 18.0.0 or higher\n\n## Security Considerations\n- Always keep your API key secure and never commit it to version control systems\n- Manage your API key through environment variables or configuration files\n- Set usage limits for your API key to prevent unauthorized use\n\n## License\nThis project is licensed under the MIT License. See the LICENSE file for details. \n",
      "npm_url": "https://www.npmjs.com/package/youtube-data-mcp-server",
      "npm_downloads": 41607,
      "keywords": [
        "youtube",
        "icraft2170",
        "data",
        "youtube data",
        "icraft2170 youtube",
        "youtube content"
      ],
      "category": "web-search"
    },
    "ilyazub--serpapi-mcp-server": {
      "owner": "ilyazub",
      "name": "serpapi-mcp-server",
      "url": "https://github.com/ilyazub/serpapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ilyazub.webp",
      "description": "Retrieve and parse search engine results from multiple platforms such as Google and Bing, allowing access to both live and archived data. Integrate with MCP clients for enhanced search functionalities.",
      "stars": 16,
      "forks": 10,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-01T07:17:53Z",
      "readme_content": "# SerpApi MCP Server\n\n[![Build](https://github.com/ilyazub/serpapi-mcp-server/actions/workflows/python-package.yml/badge.svg)](https://github.com/ilyazub/serpapi-mcp-server/actions/workflows/python-package.yml)\n\nBuild an MCP server that:\n\n- Get parsed search engines results pages via SerpApi using an API key, *fast*\n\nThis MCP (Model Context Protocol) server integrates with [SerpApi](https://serpapi.com) to perform searches across various search engines and retrieve both live and archived results. It exposes tools and resources for seamless interaction with MCP clients or hosts, such as Grok or Claude for Desktop.\n\n---\n\n## Installation\n\nTo set up the SerpApi MCP server, install the required Python libraries:\n\n```bash\npip install mcp serpapi python-dotenv\n```\n\nYou’ll also need a [SerpApi API key](https://serpapi.com/manage-api-key). Sign up at SerpApi to get one.\n\n## Quick Start\n\n1. Save the Server Code: Place the server code in a file, e.g., server.py.\n\n2. Configure the API Key: Create a .env file in the same directory with your SerpApi API key:\n```plaintext\nSERPAPI_API_KEY=your_api_key_here\n```\n\n3. Run the Server: Start the server with:\n\n```bash\npython server.py\n```\n\n4. Integrate with an MCP Client: Connect the server to an MCP client or host (e.g., Claude for Desktop). For Claude, update Claude_desktop_config.json:\n\n```json\n{\n  \"mcpServers\": {\n    \"serpapi\": {\n      \"command\": \"python\",\n      \"args\": [\"path/to/server.py\"]\n    }\n  }\n}\n```\n\nRestart the client to load the server.\n\n## Features\n- Supported Engines: Google, Google Light, Bing, Walmart, Yahoo, eBay, YouTube, DuckDuckGo, Yandex, Baidu\n\n- **Tools**:\n* search: Perform a search on a specified engine with a query and optional parameters.\n\n\n- **Resources**:\n* locations: Find Google Locations.\n\n## Usage Examples\n\nThese examples assume an MCP client (e.g., written in Python using the MCP client SDK) is connected to the server.\nListing Supported Engines\nRetrieve the list of supported search engines:\n\n```python\n\nengines = await session.read_resource(\"locations\")\nprint(engines)\n```\n\nPerforming a Search\nSearch for \"coffee\" on Google with a location filter:\n\n```python\n\nresult = await session.call_tool(\"search\", {\n    \"query\": \"coffee\",\n    \"engine\": \"google\",\n    \"location\": \"Austin, TX\"\n})\n```\nprint(result)\n\n## Configuration\nAPI Key: Set your SerpApi API key in the `.env` file as `SERPAPI_API_KEY`.\n\n### Running the Server\n\nProduction Mode: Launch the server with:\n```bash\n\npython server.py\n```\n\nDevelopment Mode: Use the MCP Inspector for debugging:\n\n```bash\nmcp dev server.py\n```\n\n## Testing\n\nTest the server using the MCP Inspector or an MCP client. For Claude for Desktop, configure the server in `Claude_desktop_config.json`, restart the app, and use the hammer icon to explore and test available tools.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "search",
        "serpapi",
        "search engine",
        "google bing",
        "web search"
      ],
      "category": "web-search"
    },
    "instructa--mcp-youtube-music": {
      "owner": "instructa",
      "name": "mcp-youtube-music",
      "url": "https://github.com/instructa/mcp-youtube-music",
      "imageUrl": "/freedevtools/mcp/pfp/instructa.webp",
      "description": "Search for and play tracks on YouTube Music, enabling quick access to music via simple commands. Integrates music capabilities seamlessly into workflows.",
      "stars": 9,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T14:00:34Z",
      "readme_content": "# YouTube Music MCP 🎵\n\nThis is a simple MCP server that allows you to search for and play tracks on YouTube Music directly from your AI assistant like Cursor or Claude Desktop.\n\nBuilt with:\n\n- [YouTube Music](https://music.youtube.com/)\n- [Anthropic MCP](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)\n- [Cursor](https://cursor.so/)\n\n## Available Tools\n\n- `searchTrack`: Search for tracks on YouTube Music by name.\n- `playTrack`: Play tracks directly by searching and opening them in your default browser.\n\n\n# Installation\n\n## 1. Get a key\n\nTo make this work you need a valid [Google Youtube API Key](https://console.cloud.google.com/marketplace/product/google/youtube.googleapis.com)\n\n## 2. Add to cursor\n\nAdd the following MCP configuration to your Cursor `.cursor/mcp.json` settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-music-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@instructa/mcp-youtube-music\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"<INSERT_API_KEY_HERE>\"\n      }\n    }\n  }\n}\n```\n\n**Develop**\n\nThis MCP is typically run directly using `npx` and doesn't require local installation or building unless you intend to modify the source code. If you want to develop it locally, you would typically clone the source repository (if available) and follow its specific contribution guidelines.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n---\n\n## Links\n\n- X/Twitter: [@kregenrek](https://x.com/kregenrek)\n- Bluesky: [@kevinkern.dev](https://bsky.app/profile/kevinkern.dev)\n\n## Courses\n- Learn Cursor AI: [Ultimate Cursor Course](https://www.instructa.ai/en/cursor-ai)\n- Learn to build software with AI: [instructa.ai](https://www.instructa.ai)\n\n## See my other projects:\n\n* [AI Prompts](https://github.com/instructa/ai-prompts/blob/main/README.md) - Curated AI Prompts for Cursor AI, Cline, Windsurf and Github Copilot\n* [codefetch](https://github.com/regenrek/codefetch) - Turn code into Markdown for LLMs with one simple terminal command\n* [aidex](https://github.com/regenrek/aidex) A CLI tool that provides detailed information about AI language models, helping developers choose the right model for their needs.\n* [codetie](https://github.com/codetie-ai/codetie) - XCode CLI",
      "npm_url": "https://www.npmjs.com/package/@instructa/mcp-youtube-music",
      "npm_downloads": 445,
      "keywords": [
        "music",
        "youtube",
        "mcp",
        "youtube music",
        "music search",
        "mcp youtube"
      ],
      "category": "web-search"
    },
    "isnow890--naver-search-mcp": {
      "owner": "isnow890",
      "name": "naver-search-mcp",
      "url": "https://github.com/isnow890/naver-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/isnow890.webp",
      "description": "Connects to the Naver Search API for searching diverse content types such as news, blogs, and shopping. Analyzes search and shopping trends using the DataLab API, offering insights into consumer behavior patterns by category, device, gender, and age group.",
      "stars": 40,
      "forks": 15,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T12:55:28Z",
      "readme_content": "# Naver Search MCP Server\n\n[![한국어](https://img.shields.io/badge/한국어-README-yellow)](README-ko.md)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/isnow890/naver-search-mcp)](https://archestra.ai/mcp-catalog/isnow890__naver-search-mcp)\n[![smithery badge](https://smithery.ai/badge/@isnow890/naver-search-mcp)](https://smithery.ai/server/@isnow890/naver-search-mcp)\n[![MCP.so](https://img.shields.io/badge/MCP.so-Naver%20Search%20MCP-blue)](https://mcp.so/server/naver-search-mcp/isnow890)\n\n#### Version History\n\n###### 1.0.45 (2025-09-28)\n\n- Smithery compatibility issues resolved - Now available through Smithery with latest features\n- Excel compatibility issues fixed in category search - Replaced with JSON functionality\n- Web Korean search (`search_webkr`) functionality restored\n- Full compatibility with Smithery platform installations\n\n###### 1.0.44 (2025-08-31)\n\n- `get_current_korean_time` tool added - Essential time context tool for Korean timezone\n- Enhanced all existing tool descriptions to reference time tool for temporal queries\n- Improved temporal context handling for \"today\", \"now\", \"current\" searches\n- Comprehensive Korean time formatting with multiple output formats\n\n###### 1.0.4 (2025-08-21)\n\n- `find_category` tool added - with fuzzy matching and ranking system support\n- Enhanced parameter validation with Zod schema\n- Improved category search workflow\n\n###### 1.0.30 (2025-08-04)\n\n- MCP SDK upgraded to 1.17.1\n- Fixed compatibility issues with Smithery specification changes\n- Added comprehensive DataLab shopping category code documentation\n\n###### 1.0.2 (2025-04-26)\n\n- README updated: cafe article search tool and version history section improved\n\n###### 1.0.1 (2025-04-26)\n\n- Cafe article search feature added\n- Shopping category info added to zod\n- Source code refactored\n\n###### 1.0.0 (2025-04-08)\n\n- Initial release\n\n#### Prerequisites\n\n- Naver Developers API Key (Client ID and Secret)\n- Node.js 18 or higher\n- NPM 8 or higher\n- Docker (optional, for container deployment)\n\n#### Getting API Keys\n\n1. Visit [Naver Developers](https://developers.naver.com/apps/#/register)\n2. Click \"Register Application\"\n3. Enter application name and select ALL of the following APIs:\n   - Search (for blog, news, book search, etc.)\n   - DataLab (Search Trends)\n   - DataLab (Shopping Insight)\n4. Set the obtained Client ID and Client Secret as environment variables\n\n## Tool Details\n\n### Available tools:\n\n#### 🕐 Time & Context Tools\n\n- **get_current_korean_time**: Get current Korean time (KST) with comprehensive date/time information. Essential for understanding \"today\", \"now\", or \"current\" context in Korean timezone. Always use this tool when temporal context is needed for searches or analysis.\n\n#### 🆕 Category Search\n\n- **find_category**: Category search tool - No more need to manually check category numbers via URL for trend and shopping insight searches. The LLM will find it out as you say.\n\n#### Search Tools\n\n- **search_webkr**: Search Naver web documents\n- **search_news**: Search Naver news\n- **search_blog**: Search Naver blogs\n- **search_cafearticle**: Search Naver cafe articles\n- **search_shop**: Search Naver shopping\n- **search_image**: Search Naver images\n- **search_kin**: Search Naver KnowledgeiN\n- **search_book**: Search Naver books\n- **search_encyc**: Search Naver encyclopedia\n- **search_academic**: Search Naver academic papers\n- **search_local**: Search Naver local places\n\n#### DataLab Tools\n\n- **datalab_search**: Analyze search term trends\n- **datalab_shopping_category**: Analyze shopping category trends\n- **datalab_shopping_by_device**: Analyze shopping trends by device\n- **datalab_shopping_by_gender**: Analyze shopping trends by gender\n- **datalab_shopping_by_age**: Analyze shopping trends by age group\n- **datalab_shopping_keywords**: Analyze shopping keyword trends\n- **datalab_shopping_keyword_by_device**: Analyze shopping keyword trends by device\n- **datalab_shopping_keyword_by_gender**: Analyze shopping keyword trends by gender\n- **datalab_shopping_keyword_by_age**: Analyze shopping keyword trends by age group\n\n#### Complete Category List:\n\nFor a complete list of category codes, you can download from Naver Shopping Partner Center or extract them by browsing Naver Shopping categories.\n\n### 🎯 Business Use Cases & Scenarios\n\n#### 🛍️ E-commerce Market Research\n\n```javascript\n// Fashion trend discovery\nfind_category(\"fashion\") → Check top fashion categories and codes\ndatalab_shopping_category → Analyze seasonal fashion trends\ndatalab_shopping_age → Identify fashion target demographics\ndatalab_shopping_keywords → Compare \"dress\" vs \"jacket\" vs \"coat\"\n```\n\n#### 📱 Digital Marketing Strategy\n\n```javascript\n// Beauty industry analysis\nfind_category(\"cosmetics\") → Find beauty categories\ndatalab_shopping_gender → 95% female vs 5% male shoppers\ndatalab_shopping_device → Mobile dominance in beauty shopping\ndatalab_shopping_keywords → \"tint\" vs \"lipstick\" keyword performance\n```\n\n#### 🏢 Business Intelligence & Competitive Analysis\n\n```javascript\n// Tech product insights\nfind_category(\"smartphone\") → Check electronics categories\ndatalab_shopping_category → Track iPhone vs Galaxy trends\ndatalab_shopping_age → 20-30s as main smartphone buyers\ndatalab_shopping_device → PC vs mobile shopping behavior\n```\n\n#### 📊 Seasonal Business Planning\n\n```javascript\n// Holiday shopping analysis\nfind_category(\"gift\") → Gift categories\ndatalab_shopping_category → Black Friday, Christmas trends\ndatalab_shopping_keywords → \"Mother's Day gift\" vs \"birthday gift\"\ndatalab_shopping_age → Age-based gift purchasing patterns\n```\n\n#### 🎯 Customer Persona Development\n\n```javascript\n// Fitness market analysis\nfind_category(\"exercise\") → Sports/fitness categories\ndatalab_shopping_gender → Male vs female fitness spending\ndatalab_shopping_age → Primary fitness demographics (20-40s)\ndatalab_shopping_keywords → \"home workout\" vs \"gym\" trend analysis\n```\n\n### 📈 Advanced Analysis Scenarios\n\n#### Market Entry Strategy\n\n1. **Category Discovery**: Use `find_category` to explore market segments\n2. **Trend Analysis**: Identify growing vs declining categories\n3. **Demographic Targeting**: Age/gender analysis for customer targeting\n4. **Competitive Intelligence**: Keyword performance comparison\n5. **Device Strategy**: Mobile vs PC shopping optimization\n\n#### Product Launch Planning\n\n1. **Market Validation**: Category growth trends and seasonality\n2. **Target Customers**: Demographic analysis for product positioning\n3. **Marketing Channels**: Device preferences for advertising strategy\n4. **Competitive Landscape**: Keyword competition and opportunities\n5. **Pricing Strategy**: Category performance and price correlation\n\n#### Performance Monitoring\n\n1. **Category Health**: Monitor product category trends\n2. **Keyword Tracking**: Track brand and product keyword performance\n3. **Demographic Shifts**: Monitor changing customer demographics\n4. **Seasonal Patterns**: Plan inventory and marketing campaigns\n5. **Competitive Benchmarking**: Compare performance against category averages\n\n### Quick Reference: Popular Category Codes\n\n| Category            | Code     | Korean        |\n| ------------------- | -------- | ------------- |\n| Fashion/Clothing    | 50000000 | 패션의류      |\n| Cosmetics/Beauty    | 50000002 | 화장품/미용   |\n| Digital/Electronics | 50000003 | 디지털/가전   |\n| Sports/Leisure      | 50000004 | 스포츠/레저   |\n| Food/Beverages      | 50000008 | 식품/음료     |\n| Health/Medical      | 50000009 | 건강/의료용품 |\n\n💡 **Tip**: Use `find_category` with fuzzy searches like \"beauty\", \"fashion\", \"electronics\" to easily find categories.\n\n## Installation\n\n### Method 1: NPX Installation (Recommended)\n\nThe most reliable way to use this MCP server is through direct NPX installation. For detailed package information, see the [NPM package page](https://www.npmjs.com/package/@isnow890/naver-search-mcp).\n\n#### Claude Desktop Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS/Linux):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor AI Configuration\n\nAdd to `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n### Method 2: Smithery Installation (Alternative - Known Issues)\n\n⚠️ **Important Notice**: Smithery installation may experience connection timeouts and hanging issues due to WebSocket relay infrastructure problems. This is a known Smithery platform issue, not a problem with this MCP server code. **NPX installation (Method 1) is strongly recommended for reliable operation.**\n\n#### Known Issues with Smithery:\n- Server initialization hangs or times out\n- `Error -32001: Request timed out`\n- WebSocket connection drops after handshake\n- Server shuts down unexpectedly before processing requests\n\n#### If you still want to try Smithery:\n\n##### For Claude Desktop:\n\n```bash\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client claude\n```\n\n##### For other AI clients:\n\n```bash\n# Cursor\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cursor\n\n# Windsurf\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client windsurf\n\n# Cline\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cline\n```\n\n**If you experience timeout issues with Smithery, please switch to Method 1 (NPX) for stable operation.**\n\n### Method 3: Local Installation\n\nFor local development or custom modifications:\n\n#### Step 1: Download and Build Source Code\n\n##### Clone with Git\n\n```bash\ngit clone https://github.com/isnow890/naver-search-mcp.git\ncd naver-search-mcp\nnpm install\nnpm run build\n```\n\n##### Or Download ZIP File\n\n1. Download the latest version from [GitHub Releases](https://github.com/isnow890/naver-search-mcp/releases)\n2. Extract the ZIP file to your desired location\n3. Navigate to the extracted folder in terminal:\n\n```bash\ncd /path/to/naver-search-mcp\nnpm install\nnpm run build\n```\n\n⚠️ **Important**: You must run `npm run build` after installation to generate the `dist` folder that contains the compiled JavaScript files.\n\n#### Step 2: Claude Desktop Configuration\n\nAfter building, you'll need the following information:\n\n- **NAVER_CLIENT_ID**: Client ID from Naver Developers\n- **NAVER_CLIENT_SECRET**: Client Secret from Naver Developers\n- **Installation Path**: Absolute path to the downloaded folder\n\n##### Windows Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"node\",\n        \"C:\\\\path\\\\to\\\\naver-search-mcp\\\\dist\\\\src\\\\index.js\"\n      ],\n      \"cwd\": \"C:\\\\path\\\\to\\\\naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### macOS/Linux Configuration\n\nAdd to Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/naver-search-mcp/dist/src/index.js\"],\n      \"cwd\": \"/path/to/naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### Path Configuration Important Notes\n\n⚠️ **Important**: You must change the following paths in the above configuration to your actual installation paths:\n\n- **Windows**: Change `C:\\\\path\\\\to\\\\naver-search-mcp` to your actual downloaded folder path\n- **macOS/Linux**: Change `/path/to/naver-search-mcp` to your actual downloaded folder path\n- **Build Path**: Make sure the path points to `dist/src/index.js` (not just `index.js`)\n\nFinding your path:\n\n```bash\n# Check current location\npwd\n\n# Absolute path examples\n# Windows: C:\\Users\\username\\Downloads\\naver-search-mcp\n# macOS: /Users/username/Downloads/naver-search-mcp\n# Linux: /home/username/Downloads/naver-search-mcp\n```\n\n#### Step 3: Restart Claude Desktop\n\nAfter completing the configuration, completely close and restart Claude Desktop to activate the Naver Search MCP server.\n\n---\n\n## Alternative Installation Methods\n\n### Method 4: Docker Installation\n\nFor containerized deployment:\n\n```bash\ndocker run -i --rm \\\n  -e NAVER_CLIENT_ID=your_client_id \\\n  -e NAVER_CLIENT_SECRET=your_client_secret \\\n  mcp/naver-search\n```\n\nDocker configuration for Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"NAVER_CLIENT_ID=your_client_id\",\n        \"-e\",\n        \"NAVER_CLIENT_SECRET=your_client_secret\",\n        \"mcp/naver-search\"\n      ]\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/naver-search .\n```\n\n## License\n\nMIT License\n",
      "npm_url": "https://www.npmjs.com/package/naver-search-mcp",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "search",
        "naver",
        "naver search",
        "search shopping",
        "search api"
      ],
      "category": "web-search"
    },
    "it-beard--tavily-server": {
      "owner": "it-beard",
      "name": "tavily-server",
      "url": "https://github.com/it-beard/tavily-server",
      "imageUrl": "/freedevtools/mcp/pfp/it-beard.webp",
      "description": "Facilitates comprehensive web searches and retrieves up-to-date information through AI-powered search capabilities, with features like result scoring, caching, and search history storage. Provides access to rich search results including titles, URLs, content snippets, and AI-generated summaries.",
      "stars": 6,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-09T18:09:08Z",
      "readme_content": "# Tavily MCP Server\n\nA Model Context Protocol (MCP) server that provides AI-powered search capabilities using the Tavily API. This server enables AI assistants to perform comprehensive web searches and retrieve relevant, up-to-date information.\n\n## Features\n\n- AI-powered search functionality\n- Support for basic and advanced search depths\n- Rich search results including titles, URLs, and content snippets\n- AI-generated summaries of search results\n- Result scoring and response time tracking\n- Comprehensive search history storage with caching\n- MCP Resources for flexible data access\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm (Node Package Manager)\n- Tavily API key (Get one at [Tavily's website](https://tavily.com))\n- An MCP client (e.g., Cline, Claude Desktop, or your own implementation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/it-beard/tavily-server.git\ncd tavily-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Configuration\n\nThis server can be used with any MCP client. Below are configuration instructions for popular clients:\n\n### Cline Configuration\n\nIf you're using Cline (the VSCode extension for Claude), create or modify the MCP settings file at:\n- macOS: `~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n- Windows: `%APPDATA%\\Cursor\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n- Linux: `~/.config/Cursor/User/globalStorage/saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n\nAdd the following configuration (replace paths and API key with your own):\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/tavily-server/build/index.js\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Claude Desktop Configuration\n\nIf you're using the Claude Desktop app, modify the configuration file at:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\nUse the same configuration format as shown above.\n\n### Other MCP Clients\n\nFor other MCP clients, consult their documentation for the correct configuration file location and format. The server configuration should include:\n1. Command to run the server (typically `node`)\n2. Path to the compiled server file\n3. Environment variables including the Tavily API key\n\n## Usage\n\n### Tools\n\nThe server provides a single tool named `search` with the following parameters:\n\n#### Required Parameters\n- `query` (string): The search query to execute\n\n#### Optional Parameters\n- `search_depth` (string): Either \"basic\" (faster) or \"advanced\" (more comprehensive)\n\n#### Example Usage\n\n```typescript\n// Example using the MCP SDK\nconst result = await mcpClient.callTool(\"tavily\", \"search\", {\n  query: \"latest developments in artificial intelligence\",\n  search_depth: \"basic\"\n});\n```\n\n### Resources\n\nThe server provides both static and dynamic resources for flexible data access:\n\n#### Static Resources\n- `tavily://last-search/result`: Returns the results of the most recent search query\n  - Persisted to disk in the data directory\n  - Survives server restarts\n  - Returns a 'No search has been performed yet' error if no search has been done\n\n#### Dynamic Resources (Resource Templates)\n- `tavily://search/{query}`: Access search results for any query\n  - Replace {query} with your URL-encoded search term\n  - Example: `tavily://search/artificial%20intelligence`\n  - Returns cached results if the query was previously made\n  - Performs and stores new search if query hasn't been searched before\n  - Returns the same format as the search tool but through a resource interface\n\nResources in MCP provide an alternative way to access data compared to tools:\n- Tools are for executing operations (like performing a new search)\n- Resources are for accessing data (like retrieving existing search results)\n- Resource URIs can be stored and accessed later\n- Resources support both static (fixed) and dynamic (templated) access patterns\n\n#### Response Format\n\n```typescript\ninterface SearchResponse {\n  query: string;\n  answer: string;\n  results: Array<{\n    title: string;\n    url: string;\n    content: string;\n    score: number;\n  }>;\n  response_time: number;\n}\n```\n\n### Persistent Storage\n\nThe server implements comprehensive persistent storage for search results:\n\n#### Storage Location\n- Data is stored in the `data` directory\n- `data/searches.json` contains all historical search results\n- Data persists between server restarts\n- Storage is automatically initialized on server start\n\n#### Storage Features\n- Stores complete search history\n- Caches all search results for quick retrieval\n- Automatic saving of new search results\n- Disk-based persistence\n- JSON format for easy debugging\n- Error handling for storage operations\n- Automatic directory creation\n\n#### Caching Behavior\n- All search results are cached automatically\n- Subsequent requests for the same query return cached results\n- Caching improves response time and reduces API calls\n- Cache persists between server restarts\n- Last search is tracked for quick access\n\n## Development\n\n### Project Structure\n\n```\ntavily-server/\n├── src/\n│   └── index.ts    # Main server implementation\n├── data/           # Persistent storage directory\n│   └── searches.json  # Search history and cache storage\n├── build/          # Compiled JavaScript files\n├── package.json    # Project dependencies and scripts\n└── tsconfig.json   # TypeScript configuration\n```\n\n### Available Scripts\n\n- `npm run build`: Compile TypeScript and make the output executable\n- `npm run start`: Start the MCP server (after building)\n- `npm run dev`: Run the server in development mode\n\n## Error Handling\n\nThe server provides detailed error messages for common issues:\n- Invalid API key\n- Network errors\n- Invalid search parameters\n- API rate limiting\n- Resource not found\n- Invalid resource URIs\n- Storage read/write errors\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/protocol) for the server framework\n- [Tavily API](https://tavily.com) for providing the search capabilities\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "web",
        "web search",
        "web searches",
        "search results"
      ],
      "category": "web-search"
    },
    "jaacob--perplexity-mcp": {
      "owner": "jaacob",
      "name": "perplexity-mcp",
      "url": "https://github.com/jaacob/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jaacob.webp",
      "description": "Provides web search capabilities through the Perplexity API, allowing access to real-time information from the web.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-31T12:55:27Z",
      "readme_content": "# Perplexity MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jaacob/perplexity-mcp)](https://smithery.ai/server/@jaacob/perplexity-mcp)\n\nAn MCP server that provides web search capabilities using Perplexity's API.\n\n<a href=\"https://glama.ai/mcp/servers/97nsl3drhq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/97nsl3drhq/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- A Perplexity API key (get one at <https://www.perplexity.ai/settings/api>)\n- Claude Desktop App\n\n## Installation\n\n### Installing via Smithery\n\nTo install Perplexity Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jaacob/perplexity-mcp):\n\n```bash\nnpx -y @smithery/cli install @jaacob/perplexity-mcp --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n\n    ```bash\n    git clone https://github.com/jaacob/perplexity-mcp\n    cd perplexity-mcp\n    ```\n\n2. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n3. Build the server:\n\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\n1. Get your Perplexity API key from <https://www.perplexity.ai/settings/api>\n\n2. Add the server to Claude's config file at `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/perplexity-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\",\n        \"PERPLEXITY_MODEL\": \"sonar\"\n      }\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to` with the actual path to where you cloned the repository.\n\n### Available Models\n\nYou can specify which model to use by setting the `PERPLEXITY_MODEL` environment variable. Available options:\n\n- `sonar-reasoning-pro` - Most capable model with enhanced reasoning\n- `sonar-reasoning` - Enhanced reasoning capabilities\n- `sonar-pro` - Faster response times\n- `sonar` - Default model (used if no model is specified)\n\nFor up-to-date model pricing and availability, visit: <https://docs.perplexity.ai/guides/pricing>\n\n## Usage\n\nAfter configuring the server and restarting Claude, you can simply ask Claude to search for information. For example:\n\n- \"What's the latest news about SpaceX?\"\n- \"Search for the best restaurants in Chicago\"\n- \"Find information about the history of jazz music\"\n\nClaude will automatically use the Perplexity search tool to find and return relevant information.\n\nIf for whatever reason it decides not to, you can force the issue by prepending your prompt with \"Search the web\".\n\n## Development\n\nTo modify the server:\n\n1. Edit `src/index.ts`\n2. Rebuild with `npm run build`\n3. Restart Claude to load the changes\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "mcp",
        "perplexity",
        "perplexity api",
        "search jaacob",
        "perplexity mcp"
      ],
      "category": "web-search"
    },
    "jae-jae--g-search-mcp": {
      "owner": "jae-jae",
      "name": "g-search-mcp",
      "url": "https://github.com/jae-jae/g-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jae-jae.webp",
      "description": "Enables efficient parallel searching on Google using multiple keywords, automatically handling verification challenges and returning structured search results in JSON format for analysis.",
      "stars": 210,
      "forks": 22,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T14:10:50Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://github.com/jae-jae/g-search-mcp/raw/main/icon.svg\" width=\"120\" height=\"120\" alt=\"g-search-mcp Logo\" />\n</div>\n\n# G-Search MCP\n\nA powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously.\n\n> This project is modified from [google-search](https://github.com/web-agent-master/google-search).\n\n> 🌟 **Recommended**: [OllaMan](https://ollaman.com/) - Powerful Ollama AI Model Manager.\n\n## Advantages\n\n- **Parallel Searching**: Supports searching with multiple keywords on Google simultaneously, improving search efficiency\n- **Browser Optimization**: Opens multiple tabs in a single browser instance for efficient parallel searching\n- **Automatic Verification Handling**: Intelligently detects CAPTCHA and enables visible browser mode for user verification when needed\n- **User Behavior Simulation**: Simulates real user browsing patterns to reduce the possibility of detection by search engines\n- **Structured Data**: Returns structured search results in JSON format for easy processing and analysis\n- **Configurable Parameters**: Supports various parameter configurations such as search result limits, timeout settings, locale settings, etc.\n\n## Quick Start\n\nRun directly with npx:\n\n```bash\nnpx -y g-search-mcp\n```\n\nFirst time setup - install the required browser by running the following command in your terminal:\n\n```bash\nnpx playwright install chromium\n```\n\n### Debug Mode\n\nUse the `--debug` option to run in debug mode (showing browser window):\n\n```bash\nnpx -y g-search-mcp --debug\n```\n\n## Configure MCP\n\nConfigure this MCP server in Claude Desktop:\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"g-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"g-search-mcp\"]\n    }\n  }\n}\n```\n\n## Features\n\n- `search` - Execute Google searches with multiple keywords and return results\n  - Uses Playwright browser to perform searches\n  - Supports the following parameters:\n    - `queries`: Array of search queries to execute (required parameter)\n    - `limit`: Maximum number of results to return per query, default is 10\n    - `timeout`: Page loading timeout in milliseconds, default is 60000 (60 seconds)\n    - `noSaveState`: Whether to avoid saving browser state, default is false\n    - `locale`: Locale setting for search results, default is en-US\n    - `debug`: Whether to enable debug mode (showing browser window), overrides the --debug flag in command line\n\n**Example usage**:\n\n```\nUse the search tool to search for \"machine learning\" and \"artificial intelligence\" on Google\n```\n\n**Example response**:\n\n```json\n{\n  \"searches\": [\n    {\n      \"query\": \"machine learning\",\n      \"results\": [\n        {\n          \"title\": \"What is Machine Learning? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/machine-learning\",\n          \"snippet\": \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\"\n        },\n        ...\n      ]\n    },\n    {\n      \"query\": \"artificial intelligence\",\n      \"results\": [\n        {\n          \"title\": \"What is Artificial Intelligence (AI)? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/artificial-intelligence\",\n          \"snippet\": \"Artificial intelligence leverages computers and machines to mimic the problem-solving and decision-making capabilities of the human mind.\"\n        },\n        ...\n      ]\n    }\n  ]\n}\n```\n\n## Usage Tips\n\n### Handling Special Website Scenarios\n\n#### Adjusting Search Parameters\n\n- **Search Result Quantity**: For more search results:\n\n  ```\n  Please return the top 20 search results for each keyword\n  ```\n\n  This will set the `limit: 20` parameter.\n\n- **Increase Timeout Duration**: For slow loading situations:\n  ```\n  Please set the page loading timeout to 120 seconds\n  ```\n  This will adjust the `timeout` parameter to 120000 milliseconds.\n\n#### Locale Settings Adjustment\n\n- **Change Search Region**: Specify a different locale setting:\n  ```\n  Please use Chinese locale (zh-CN) for searching\n  ```\n  This will set the `locale: \"zh-CN\"` parameter.\n\n### Debugging and Troubleshooting\n\n#### Enable Debug Mode\n\n- **Dynamic Debug Activation**: To display the browser window during a specific search operation:\n  ```\n  Please enable debug mode for this search operation\n  ```\n  This sets `debug: true` even if the server was started without the `--debug` flag.\n\n## Installation\n\n### Prerequisites\n\n- Node.js 18 or higher\n- NPM or Yarn\n\n### Install from Source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/jae-jae/g-search-mcp.git\ncd g-search-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Install Playwright browser:\n\n```bash\nnpm run install-browser\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n## Development\n\n### Auto Rebuild (Development Mode)\n\n```bash\nnpm run watch\n```\n\n### Using MCP Inspector for Debugging\n\n```bash\nnpm run inspector\n```\n\n## Related Projects\n\n- [fetcher-mcp](https://github.com/jae-jae/fetcher-mcp): A powerful MCP server for fetching web page content using Playwright headless browser. Features intelligent content extraction, parallel processing, resource optimization, and more, making it an ideal tool for web content scraping.\n\n## License\n\nLicensed under the [MIT License](https://choosealicense.com/licenses/mit/)\n",
      "npm_url": "https://www.npmjs.com/package/g-search-mcp",
      "npm_downloads": 12269,
      "keywords": [
        "searching",
        "search",
        "google",
        "parallel searching",
        "searching google",
        "web search"
      ],
      "category": "web-search"
    },
    "jasonjmcghee--WebMCP": {
      "owner": "jasonjmcghee",
      "name": "WebMCP",
      "url": "https://github.com/jasonjmcghee/WebMCP",
      "imageUrl": "/freedevtools/mcp/pfp/jasonjmcghee.webp",
      "description": "A protocol for websites to share tools, resources, and prompts with client-side LLMs, enhancing user interaction without the need for API keys. It allows multiple WebMCP-enabled sites to be connected for a richer user experience.",
      "stars": 59,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T15:25:20Z",
      "readme_content": "# WebMCP\n\nA proposal and code for websites to support client side LLMs\n\n[![NPM Version](https://img.shields.io/npm/v/%40jason.today%2Fwebmcp)](https://www.npmjs.com/package/@jason.today/webmcp) [![MIT licensed](https://img.shields.io/npm/l/%40jason.today%2Fwebmcp)](./LICENSE)\n\nWebMCP allows websites to share tools, resources, prompts, etc. to LLMs. In other words, WebMCP allows a website to be an MCP server. No sharing API Keys. Use any model you want.\n\n[Here's a simple website I built that is WebMCP-enabled](https://webmcp.dev)\n\nIt comes in the form of a widget that a website owner can put on their site and expose tools to give client-side LLMs what they need to provide a great UX for the user or agent.\n\n_The look, feel, how it's used, and security are all absolutely open for contribution / constructive criticism. MCP Clients directly building WebMCP functionality seems like an ideal outcome._\n\nAn end-user can connect to any number of websites at a time - and tools are \"scoped\" (by name) based on the domain to simplify organization.\n\n### Super Quick Demo (20 seconds, Sound on 🔊)\n\nhttps://github.com/user-attachments/assets/61229470-1242-401e-a7d9-c0d762d7b519\n\n## Getting started (using your LLM with websites using WebMCP)\n\n#### Installation\n\nJust specify your MCP client (`claude`, `cursor`, `cline`, `windsurf`, or a path to json)\n\n```bash\nnpx -y @jason.today/webmcp@latest --config claude\n```\n\n_If you're interested in setting it up manually, use the command `npx -y @jason.today/webmcp@latest --mcp`._\n\n_Auto-install was inspired by Smithery, but their code is AGPL so I wrote this myself. If it doesn't work for you or you don't see your mcp client, please file an issue._\n\n#### Using WebMCP\n\nWhen you're ready to connect to a website, you can ask your model to generate you an mcp token.\n\nCopy the token and paste it to the website's input. As soon as the website registers with it, it's thrown away and cannot be used for subsequent registrations or anything else. The website will receive its own session token for making requests.\n\nIf you'd rather your model / service never see the token, you can manually execute `npx @jason.today/webmcp --new` instead.\n\nSome MCP clients, including Claude Desktop, need to be restarted to get access to new tools. (at least at time of writing)\n\nTo disconnect, you can close the browser tab, click \"disconnect\", or shut down the server with `npx @jason.today/webmcp -q`.\n\nAll configuration files are stored in `~/.webmcp` directory.\n\n## Getting started (adding WebMCP to your website)\n\nTo use WebMCP, simply include [`webmcp.js`](https://github.com/jasonjmcghee/WebMCP/releases) on your page (via src or directly):\n\n```\n<script src=\"webmcp.js\"></script>\n```\n\nThe WebMCP widget will automatically initialize and appear in the bottom right corner of your page. Clicking on it will ask for a webmcp token which the end-user will generate.\n\n### Full Demo (3 minutes)\n\nhttps://github.com/user-attachments/assets/43ad160a-846d-48ad-9af9-f6d537e78473\n\n## More Info About How It Works\n\nThe bridge between the MCP client and the website is a localhost-only (not accessible to requests outside your computer) websocket server. Because it is configured to allow requests from your local web browser, authentication / token exchange is required, in case you visit a website attempting to abuse this.\n\n_Ideally the web browser itself would have an explicit permission for this, like webcam or microphone use._\n\n1. The MCP client connects to the `/mcp` path using the server token from `.env` (auto-generated)\n2. The server generates a registration token (instigated via the built-in mcp tool by a model or the `--new` command)\n3. Web clients connect to the `/register` endpoint with this token and its domain.\n4. Web pages connect to their assigned channel based on their domain.\n5. When an LLM wants to use a tool / resource / prompt, the request flows from:\n   - MCP Client → MCP Server → WebSocket Server → Web Page with the tool / resource / prompt\n   - (similar for requesting a list of tools / resources / prompts)\n6. The web page performs the request (e.g. call tool) and sends the result back through the same path\n7. Multiple web pages can be connected simultaneously, each with their own set of tools and tokens\n8. The MCP client sees all tools as a unified list, with channel prefixes to avoid name collisions\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MCP as MCP Client\n    participant Server as MCP Server\n    participant WS as WebSocket Server\n    participant Web as Website\n    \n    %% Initial connection\n    MCP->>Server: Connect to /mcp with internal server token\n    \n    %% Website registration token\n    User->>MCP: Request registration token\n    MCP->>Server: Request registration token\n    Server-->>MCP: Return registration token\n    MCP-->>User: Display registration token\n    \n    %% Website registration\n    User->>Web: Paste registration token\n    Web->>WS: Connect to /register with token & domain (registration token deleted)\n    WS-->>Web: Assign channel & session token\n    Web->>WS: Connect to assigned channel\n    \n    %% Tool interaction\n    MCP->>Server: Request tools list\n    Server->>WS: Forward request\n    WS->>Web: Request tools\n    Web-->>WS: Return tools list\n    WS-->>Server: Forward tools list\n    Server-->>MCP: Return tools list\n    \n    %% Tool execution\n    MCP->>Server: Tool request\n    Server->>WS: Forward request\n    WS->>Web: Execute tool\n    Web-->>WS: Return result\n    WS-->>Server: Forward result\n    Server-->>MCP: Return result\n    \n    %% Disconnection\n    User->>Web: Disconnect\n    Web->>WS: Close connection\n```\n\n## Security\n\nThis is a super early project. I'm very interested in hardening security to prevent malicious extensions etc. from being\nable to perform prompt injection attacks and similar. If you have constructive ideas, please reach out or open an issue.\n\n## Built in tools\n\n- Token generator (for connecting to WebMCP websites)\n- MCP Tool Definer (to simplify building the schema of a tool for use with MCP)\n  - You can ask for the javascript (if relevant) in a follow-up message for use with WebMCP\n\n## Docker\n\nThere is a `Dockerfile` specifically for Smithery deployment.\n\nIf you'd like to use docker to run the websocket server, I've added a `docker-compose.yml` for demonstration purposes.\n\nIf `--docker` is provided to the mcp client config alongside `--mcp`, it will assume the server is running. This will allow you to dockerize the main process (websocket server), and your mcp client will connect to your docker container via websocket. Similarly, websites will communicate with your docker container.\n",
      "npm_url": "https://www.npmjs.com/package/webmcp",
      "npm_downloads": 126,
      "keywords": [
        "webmcp",
        "web",
        "client",
        "webmcp protocol",
        "webmcp enabled",
        "jasonjmcghee webmcp"
      ],
      "category": "web-search"
    },
    "jbdamask--mcp-nih-reporter": {
      "owner": "jbdamask",
      "name": "mcp-nih-reporter",
      "url": "https://github.com/jbdamask/mcp-nih-reporter",
      "imageUrl": "/freedevtools/mcp/pfp/jbdamask.webp",
      "description": "Search for NIH-funded research projects and publications using various criteria such as fiscal years, investigator names, organization details, and funding information. Retrieve detailed project information, including abstracts, in a conversational format.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-19T10:49:39Z",
      "readme_content": "# NIH RePORTER MCP\n\nA Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server for chatting with [NIH RePORTER](https://reporter.nih.gov/). Search for NIH-funded research projects and publications in a conversational manner.\nAccompanying blog post [here](https://open.substack.com/pub/johndamask/p/building-an-mcp-server-over-nihs?r=2ee1b&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true).\n\n\n\n\n## Features\n\n- Search NIH-funded research projects with various criteria:\n  - Fiscal years\n  - Principal Investigator names\n  - Organization details (name, state, city, type, department)\n  - Funding amounts\n  - COVID-19 response status\n  - Funding mechanism\n  - Institute/Center codes\n  - RCDC terms\n  - Date ranges\n- Search publications associated with NIH projects\n- Combined search functionality for both projects and publications\n- Detailed project and publication information including abstracts\n- Configurable result limits\n\n## Prerequisites\n\n- Python 3.12 or higher\n- UV package manager (recommended for faster dependency installation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd mcp-nih-reporter\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # On Windows, use `.venv\\Scripts\\activate`\n```\n\n3. Install dependencies using UV:\n```bash\nuv pip install -e .\n```\n## Usage\n\nThis MCP server provides access to the NIH RePORTER API through several tools:\n\n- `search_projects`: Search for NIH-funded research projects\n- `search_publications`: Search for publications associated with NIH projects\n- `search_combined`: Combined search for both projects and publications\n- `test_connection`: Test the API connection\n\nYou can use this MCP with any MCP-compatible client, such as:\n- Claude Desktop\n- Cursor\n- Other MCP-enabled tools\n\n### Example claude_desktop_config.json\n```\n{\n  \"mcpServers\": {\n\t \"nih-reporter\": {\n\t      \"command\": \"<fully qualified path to>/uv\",\n\t      \"args\": [\n\t        \"run\",\n\t        \"--with\",\n\t        \"mcp[cli]\",\n\t        \"mcp\",\n\t        \"run\",\n\t        \"<fully qualified path to>/mcp-nih-reporter/mcp-nih-reporter.py\"\n\t      ]\n\t    }\n  }\n}\n```\n\nThe search results will be returned in a structured format containing project details including:\n- Project title and abstract\n- Principal Investigator information\n- Organization details\n- Funding information\n- Project dates and status\n\n## Debugging\n\nA log file will be created in the root folder when the MCP attempts to run in a client (e.g. Claude Desktop). Check there if you're having trouble.\n\n## Development\n\nThe project uses:\n- `httpx` for async HTTP requests\n- `mcp` for the Mission Control Protocol implementation\n- `python-dotenv` for environment variable management\n- `uv` for dependency management\n\n## Logging\n\nLogs are written to `mcp-nih-reporter.log` in the project root directory. The logging level is set to INFO by default.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and follow the existing code style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "projects",
        "publications",
        "nih funded",
        "funded research",
        "search nih"
      ],
      "category": "web-search"
    },
    "jedrazb--elastic-semantic-search-mcp-server": {
      "owner": "jedrazb",
      "name": "elastic-semantic-search-mcp-server",
      "url": "https://github.com/jedrazb/elastic-semantic-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jedrazb.webp",
      "description": "Enables semantic search capabilities for blog posts indexed in Elasticsearch, facilitating efficient content crawling and indexing to enhance search functionality and retrieval of relevant information.",
      "stars": 3,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-08T19:17:13Z",
      "readme_content": "# MCP Server: Elasticsearch semantic search tool\n\nDemo repo for: https://j.blaszyk.me/tech-blog/mcp-server-elasticsearch-semantic-search/\n\n## Table of Contents\n- [Overview](#overview)\n- [Running the MCP Server](#running-the-mcp-server)\n- [Integrating with Claude Desktop](#integrating-with-claude-desktop)\n- [Crawling Search Labs Blog Posts](#crawling-search-labs-blog-posts)\n  - [1. Verify Crawler Setup](#1-verify-crawler-setup)\n  - [2. Configure Elasticsearch](#2-configure-elasticsearch)\n  - [3. Update Index Mapping for Semantic Search](#3-update-index-mapping-for-semantic-search)\n  - [4. Start Crawling](#4-start-crawling)\n  - [5. Verify Indexed Documents](#5-verify-indexed-documents)\n\n---\n\n## Overview\nThis repository provides a **Python implementation of an MCP server** for **semantic search** through **Search Labs blog posts** indexed in **Elasticsearch**.\n\nIt assumes you've crawled the blog posts and stored them in the `search-labs-posts` index using **Elastic Open Crawler**.\n\n---\n\n## Running the MCP Server\n\nAdd `ES_URL` and `ES_AP_KEY` into `.env` file, (take a look [here](#2-configure-elasticsearch) for generating api key with minimum permissions)\n\nStart the server in **MCP Inspector**:\n\n```sh\nmake dev\n```\n\nOnce running, access the MCP Inspector at: [http://localhost:5173](http://localhost:5173)\n\n---\n\n## Integrating with Claude Desktop\n\nTo add the MCP server to **Claude Desktop**:\n\n```sh\nmake install-claude-config\n```\n\nThis updates `claude_desktop_config.json` in your home directory. On the next restart, the Claude app will detect the server and load the declared tool.\n\n---\n\n## Crawling Search Labs Blog Posts\n\n### 1. Verify Crawler Setup\nTo check if the **Elastic Open Crawler** works, run:\n\n```sh\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  -v \"$(pwd)/crawler-config:/app/config\" \\\n  --network host \\\n  docker.elastic.co/integrations/crawler:latest \\\n  -c \"bin/crawler crawl config/test-crawler.yml\"\n```\n\nThis should print crawled content from a **single page**.\n\n---\n\n### 2. Configure Elasticsearch\nSet up **Elasticsearch URL and API Key**.\n\nGenerate an API key with **minimum crawler permissions**:\n\n```sh\nPOST /_security/api_key\n{\n  \"name\": \"crawler-search-labs\",\n  \"role_descriptors\": {\n    \"crawler-search-labs-role\": {\n      \"cluster\": [\"monitor\"],\n      \"indices\": [\n        {\n          \"names\": [\"search-labs-posts\"],\n          \"privileges\": [\"all\"]\n        }\n      ]\n    }\n  },\n  \"metadata\": {\n    \"application\": \"crawler\"\n  }\n}\n```\n\nCopy the `encoded` value from the response and set it as `API_KEY`.\n\n---\n\n### 3. Update Index Mapping for Semantic Search\n\nEnsure the `search-labs-posts` index exists. If not, create it:\n\n```sh\nPUT search-labs-posts\n```\n\nUpdate the **mapping** to enable **semantic search**:\n\n```sh\nPUT search-labs-posts/_mappings\n{\n  \"properties\": {\n    \"body\": {\n      \"type\": \"text\",\n      \"copy_to\": \"semantic_body\"\n    },\n    \"semantic_body\": {\n      \"type\": \"semantic_text\",\n      \"inference_id\": \".elser-2-elasticsearch\"\n    }\n  }\n}\n```\n\nThe `body` field is indexed as **semantic text** using **Elasticsearch’s ELSER model**.\n\n---\n\n### 4. Start Crawling\n\nRun the crawler to populate the index:\n\n```sh\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  -v \"$(pwd)/crawler-config:/app/config\" \\\n  --network host \\\n  docker.elastic.co/integrations/crawler:latest \\\n  -c \"bin/crawler crawl config/elastic-search-labs-crawler.yml\"\n```\n> [!TIP]\n> **If using a fresh Elasticsearch cluster**, wait for the **ELSER model** to start before indexing.\n\n---\n\n### 5. Verify Indexed Documents\nCheck if the documents were indexed:\n\n```sh\nGET search-labs-posts/_count\n```\n\nThis will return the total document count in the index. You can also verify in **Kibana**.\n\n---\n\n **Done!** You can now perform **semantic searches** on **Search Labs blog posts**\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elasticsearch",
        "blog",
        "indexing",
        "semantic search",
        "indexed elasticsearch",
        "elasticsearch facilitating"
      ],
      "category": "web-search"
    },
    "jeong-sik--kakao-api-mcp-server": {
      "owner": "jeong-sik",
      "name": "kakao-api-mcp-server",
      "url": "https://github.com/jeong-sik/kakao-api-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jeong-sik.webp",
      "description": "Provides access to Kakao Map and Daum Search APIs for fetching location data, performing place searches, converting coordinates to addresses, and finding routes. Enables integration of various search functionalities and mapping features through a standardized interface.",
      "stars": 14,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T05:55:11Z",
      "readme_content": "# 카카오 API MCP 서버\n\n카카오맵 API 및 Daum 검색 API를 [Model Context Protocol](https://github.com/anthropics/model-context-protocol)(MCP)을 통해 활용할 수 있는 서버입니다. 이 서버를 통해 AI 모델이 카카오맵의 지도 관련 기능과 Daum의 다양한 검색 기능을 활용할 수 있습니다.\n\n## 주의 사항\n\n*   **카카오 로그인, 카카오톡 메시지 보내기 등 사용자 계정 관련 기능은 포함되어 있지 않습니다.** 이 서버는 공개된 카카오 및 Daum의 Open API만을 사용합니다.\n*   본 서버를 사용하기 위해서는 유효한 **카카오 REST API 키**가 필요합니다. [카카오 디벨로퍼스](https://developers.kakao.com/)에서 앱을 생성하고 REST API 키를 발급받으세요.\n\n## 주요 기능\n\n### 카카오맵 API\n\n1. **장소 검색 (`mcp_kakao_map_search_places`)**\n   - 키워드로 카카오맵에서 장소를 검색\n   - 위치, 카테고리, 연락처 정보 제공\n\n2. **좌표-주소 변환 (`mcp_kakao_map_coord_to_address`)**\n   - 경위도 좌표를 실제 주소로 변환\n   - 도로명 주소 및 지번 주소 정보 제공\n\n3. **길찾기 (`mcp_kakao_map_find_route`)**\n   - 출발지에서 목적지까지의 경로 검색\n   - 거리, 소요 시간, 예상 택시 요금 등 제공\n   - 교통 상황 정보 포함 (선택적)\n\n### Daum 검색 API\n\n1. **웹 문서 검색 (`mcp_kakao_map_search_web`)**\n   - 키워드로 다음 웹 문서 검색\n   - 페이지 정렬 및 검색 결과 개수 조정 가능\n\n2. **이미지 검색 (`mcp_kakao_map_search_image`)**\n   - 키워드로 다음 이미지 검색\n   - 이미지 URL 및 관련 정보 제공\n\n3. **블로그 검색 (`mcp_kakao_map_search_blog`)**\n   - 키워드로 다음 블로그 글 검색\n   - 블로그 이름, 포스트 제목, 내용 요약 제공\n\n4. **카페 검색 (`mcp_kakao_map_search_cafe`)**\n   - 키워드로 다음 카페 글 검색\n   - 카페 이름, 게시물 제목, 내용 요약 제공\n\n## 도구 사용 예시 (MCP)\n\n아래는 MCP 클라이언트(예: AI 모델)가 이 서버의 도구를 호출하는 방법과 예상되는 응답 형식입니다.\n\n### 카카오맵 API\n\n#### 1. 장소 검색 (`mcp_kakao_map_search_places`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_places\",\n  \"parameters\": {\n    \"keyword\": \"판교역 현대백화점\"\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_places\",\n  \"result\": \"장소 검색 결과 (결과 수: 15, 총 18개):\\n이름: 현대백화점 판교점\\n주소: 경기 성남시 분당구 백현동 541\\n카테고리: 쇼핑,유통 > 백화점 > 현대백화점\\n전화번호: 031-5170-2233\\n상세정보: http://place.map.kakao.com/18757447\\n---\\n... (추가 결과)\"\n}\n```\n\n#### 2. 좌표-주소 변환 (`mcp_kakao_map_coord_to_address`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_coord_to_address\",\n  \"parameters\": {\n    \"x\": 127.1120278,\n    \"y\": 37.3955833\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_coord_to_address\",\n  \"result\": \"주소 변환 결과:\\n도로명: 경기 성남시 분당구 판교역로146번길 20\\n지번: 경기 성남시 분당구 백현동 535\"\n}\n```\n\n#### 3. 길찾기 (`mcp_kakao_map_find_route`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_find_route\",\n  \"parameters\": {\n    \"origin\": \"판교역\",\n    \"destination\": \"정자역\",\n    \"transportation_type\": \"car\",\n    \"traffic_info\": true\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_find_route\",\n  \"result\": \"🗺️ 길찾기 결과\\n\\n출발지: 판교역 신분당선 (경기 성남시 분당구 삼평동)\\n\\n목적지: 정자역 신분당선 (경기 성남시 분당구 정자동)\\n\\n이동 수단: 자동차\\n\\n총 거리: 3.6km\\n예상 소요 시간: 10분\\n예상 택시 요금: 5,600원\\n\\n📊 교통 상황 요약:\\n\\n카카오맵에서 보기: https://map.kakao.com/?sName=%ED%8C%90%EA%B5%90%EC%97%AD&eName=%EC%A0%95%EC%9E%90%EC%97%AD\\n\"\n}\n```\n\n### Daum 검색 API\n\n#### 1. 웹 문서 검색 (`mcp_kakao_map_search_web`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_web\",\n  \"parameters\": {\n    \"query\": \"카카오브레인 칼로\",\n    \"size\": 2\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_web\",\n  \"result\": \"웹 문서 검색 결과 (총 2083개 중 2개 표시):\\n\\n1. 카카오브레인 | 칼로 Karlo\\n   내용: 카카오브레인의 이미지 생성 모델 Karlo는 사용자가 입력한 문장(Text)을 이해하여, 세상에 단 하나뿐인 이미지를 만들어내는 인공지능 화가입니다. 수백만 장 규모의...\\n   URL: https://kakaobrain.com/karlo\\n   날짜: 2024. 1. 1.\\n\\n2. 카카오브레인, AI 아티스트 '칼로 2.0' 공개 - 테크레시피\\n   내용: 카카오브레인이 초거대 인공지능(AI) 이미지 생성 모델 '칼로(Karlo) 2.0'을 공개했다고 11일 밝혔다. 칼로 2.0은 약 3억 장 규모의 텍스트-이미지 데이터셋을 학습한 모델이...\\n   URL: https://techrecipe.co.kr/posts/56513\\n   날짜: 2023. 7. 11.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 2. 이미지 검색 (`mcp_kakao_map_search_image`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_image\",\n  \"parameters\": {\n    \"query\": \"고양이\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_image\",\n  \"result\": \"이미지 검색 결과 (총 8715385개 중 1개 표시):\\n\\n1. 컬렉션 이름: Daum 백과\\n   문서 URL: http://100.daum.net/encyclopedia/view/172XX61300001\\n   이미지 URL: https://t1.daumcdn.net/thumb/R1024x0/?fname=http%3A%2F%2Ft1.daumcdn.net%2Fencyclop%2F172%2F613%2F172XX61300001\\n   썸네일 URL: https://search1.kakaocdn.net/thumb/R100x100/?fname=http%3A%2F%2Ft1.daumcdn.net%2Fencyclop%2F172%2F613%2F172XX61300001&token=1579057346066cfd0b2e0c671d07c433\\n   크기: 가로 1024px, 세로 682px\\n   표시 URL: 100.daum.net\\n   날짜: 2014. 11. 6.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 3. 블로그 검색 (`mcp_kakao_map_search_blog`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_blog\",\n  \"parameters\": {\n    \"query\": \"판교 맛집\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_blog\",\n  \"result\": \"블로그 검색 결과 (총 215893개 중 1개 표시):\\n\\n1. 블로그명: 짱돌의 일상다반사\\n   제목: 판교 맛집 추천 | 유스페이스몰 가성비 좋은 점심 맛집\\n   내용: 판교테크노밸리 유스페이스몰은 늘 점심시간마다 직장인들로 인산인해를 이루는 곳이다. 오늘은 판교 점심 맛집으로 괜찮은 곳 두 군데를 소개해 본다. 1.... \\n   URL: http://jdcamping.tistory.com/1374\\n   썸네일: https://search2.kakaocdn.net/thumb/R180x180/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcQv0tX%2FbtrOfR4oUu3%2FdKQGkK0kY6kKk40f4kYkYK%2Fimg.jpg&token=1c251bb24ae4bb01657303012e2641ac\\n   날짜: 2024. 12. 17.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 4. 카페 검색 (`mcp_kakao_map_search_cafe`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_cafe\",\n  \"parameters\": {\n    \"query\": \"코딩 스터디\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_cafe\",\n  \"result\": \"카페 검색 결과 (총 18335개 중 1개 표시):\\n\\n1. 카페명: 독취사-취업,대학생,대기업,공기업,NCS,인적성,취업카페\\n   제목: [스터디] 웹개발/코딩 기초 스터디 구해요\\n   내용: 안녕하세요! 웹개발 및 코딩 기초를 함께 공부할 스터디원을 모집합니다. 현재 2명이며, 최대 4명까지 생각하고 있습니다. 장소는 주로 강남/사당에서 진행하고, 온라...\\n   URL: http://cafe.daum.net/breakjob/DldL/12345\\n   썸네일: https://search1.kakaocdn.net/thumb/P180x180/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafe_image%2F%2Fconfig%2Fimg_default_profile%3Fver%3D1&token=de43b9d06222d0a2192f9f70fcb0f134\\n   날짜: 2025. 3. 28.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n## 설치 및 설정\n\n1. **저장소 복제 및 종속성 설치:**\n```bash\ngit clone https://github.com/yousleepwhen/kakao-api-mcp-server.git # 저장소 URL을 실제 URL로 변경해주세요\ncd kakao-api-mcp-server\nyarn install\n```\n*   이 프로젝트는 `yarn` 패키지 매니저 사용을 권장합니다.\n\n2. **카카오 REST API 키 설정:**\n   - 프로젝트 루트 디렉토리에 `.env` 파일을 생성합니다.\n   - `.env` 파일 안에 다음과 같이 카카오 디벨로퍼스에서 발급받은 REST API 키를 입력합니다:\n     ```\n     KAKAO_REST_API_KEY=여기에_카카오_REST_API_키_입력\n     ```\n   - 또는, 서버 실행 시 `--kakao-api-key` 인자를 통해 직접 전달할 수도 있습니다.\n\n## 실행 방법\n\n서버를 실행하기 전에 코드를 빌드해야 합니다. `start` 관련 스크립트에 빌드 과정이 포함되어 있으므로 별도로 `yarn build`를 실행할 필요는 없습니다.\n\n### HTTP 모드 (기본)\n\n다른 서비스나 도구와 HTTP를 통해 통신할 때 사용합니다.\n\n```bash\nyarn start\n```\n\n기본적으로 3000번 포트를 사용합니다. 포트를 변경하려면 `--port` 인자를 사용하세요:\n\n```bash\nyarn start --port 8080\n```\n\n### stdio 모드\n\n터미널의 표준 입출력(stdin/stdout)을 통해 MCP 메시지를 주고받을 때 사용합니다.\n\n```bash\nyarn start:stdio\n```\n\n### 개발 모드\n\n개발 중 코드 변경 시 자동으로 빌드하고 서버를 재시작하려면 (nodemon 등 별도 설정 필요) `dev` 스크립트를 활용할 수 있습니다. 현재 `dev` 스크립트는 `start`와 동일하게 동작합니다.\n\n```bash\nyarn dev\n```\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스 하에 배포됩니다.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kakao",
        "searches",
        "search",
        "kakao api",
        "kakao map",
        "search apis"
      ],
      "category": "web-search"
    },
    "jerpint--paperpal": {
      "owner": "jerpint",
      "name": "paperpal",
      "url": "https://github.com/jerpint/paperpal",
      "imageUrl": "/freedevtools/mcp/pfp/jerpint.webp",
      "description": "Enhance literature review processes by enabling searches and discussions about machine learning papers from arXiv and Hugging Face. Facilitate natural conversations with language models to discover new research and organize ideas effectively.",
      "stars": 10,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T07:29:54Z",
      "readme_content": "🚨 Development has moved to https://github.com/milatechtransfer/paperpal\n\n# paperpal\n\nMCP Extension to aid you in searching and writing literature reviews\n\n> Check out this [conversation with Claude](https://claude.ai/share/0572fbd9-3ba2-4143-9f7f-5cae205c6d0d) to see what it can do\n\n## How it works\n\n`paperpal` gives your LLMs access to [arxiv](https://www.arxiv.org) and [Hugging Face papers](https://huggingface.co/papers).\nYou can then have a natural conversation with your favourite LLMs (e.g. Claude) and have it guide you.\n\nYou can:\n\n* Discuss papers\n* Look for new papers\n* Organize ideas for liteature reviews\n* etc.\n\nOf course, this tool is as good as the sum of its parts. LLMs can still hallucinate, and semantic search is never perfect.\n\n## Quickstart\n\nThere are many different ways with which you can interact with an MCP server.\n\n### Claude Desktop App\n\n> If this is your first time using an MCP server for Claude Desktop App, see https://modelcontextprotocol.io/quickstart/user\n\nFirst, clone this repository locally:\n\n    git clone https://github.com/jerpint/paperpal\n\nNext, add the extension to your app. Open your configuration file (on macOS this should be `~/Library/Application Support/Claude/claude_desktop_config.json`) and and add the following to the extension:\n\nFor example on MacOS:\n\n```python\n{\n  \"mcpServers\": {\n    \"paperpal\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/<username>/paperpal\",\n        \"run\",\n        \"paperpal.py\"\n      ]\n    }\n  }\n}\n```\n\nRestart your Claude Desktop App and you should see it appear.\n\n\n### Cursor\n\n> If this is your first time using an MCP server for Cursor, see https://docs.cursor.com/context/model-context-protocol#remote-development\n\nFirst, clone this repository locally:\n\n    git clone https://github.com/jerpint/paperpal\n\n\nAdd this to the root of the project in a `.cursor/mcp.json` file:\n\n```\n{\n  \"mcpServers\": {\n    \"paperpal\": {\n      \"command\": \"/Users/jeremypinto/.cargo/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/jeremypinto/paperpal\",\n        \"run\",\n        \"paperpal.py\"\n      ]\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperpal",
        "searches",
        "research",
        "searches discussions",
        "learning papers",
        "jerpint paperpal"
      ],
      "category": "web-search"
    },
    "jikime--py-mcp-naver-search": {
      "owner": "jikime",
      "name": "py-mcp-naver-search",
      "url": "https://github.com/jikime/py-mcp-naver-search",
      "imageUrl": "/freedevtools/mcp/pfp/jikime.webp",
      "description": "Access diverse Naver search APIs for retrieving structured data across multiple categories including blogs, news, books, images, and shopping items. The server facilitates pagination, adult content filtering, and keyboard input error correction for enhanced search functionality.",
      "stars": 3,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-16T05:41:13Z",
      "readme_content": "# Naver Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jikime/py-mcp-naver-search)](https://smithery.ai/server/@jikime/py-mcp-naver-search) ![](https://badge.mcpx.dev?type=server 'MCP Server') ![Version](https://img.shields.io/badge/version-1.1.10-green) ![License](https://img.shields.io/badge/license-MIT-blue)\n\nThis MCP (Multi-platform Communication Protocol) server provides access to Naver Search APIs, allowing AI agents to search for various types of content on Naver.\n\n## Overview\n\n- Search for blogs, news, books, images, shopping items, and more\n- Multiple search categories with pagination support\n- Structured text responses optimized for LLM consumption\n- Check for adult content\n- Convert keyboard input errors (errata)\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configure MCP Settings](#configure-mcp-settings)\n- [API Reference](#api-reference)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Setup\n\n### Prerequisites\n\n- Python 3.12+\n- Naver Developer API credentials\n  - You can obtain these credentials by signing up at the [Naver Developers](https://developers.naver.com/apps/#/register) portal.\n  - And You can check my blog [Naver Search API MCP Server](https://devway.tistory.com/55), too.\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/jikime/py-mcp-naver-search.git\ncd py-mcp-naver-search\n```\n\n2. uv installation\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n3. Create a virtual environment and install dependencies:\n```bash\nuv venv -p 3.12\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n4. Create a `.env` file with your Naver API credentials:\n```\ncp env.example .env\nvi .env\n\nNAVER_CLIENT_ID=your_client_id_here\nNAVER_CLIENT_SECRET=your_client_secret_here\n```\n\n\n#### Using Docker\n\n1. Build the Docker image:\n```bash\ndocker build -t py-mcp-naver-search .\n```\n\n2. Run the container:\n```bash\ndocker run py-mcp-naver-search\n```\n\n#### Using Local\n\n1. Run the server:\n```bash\nmcp run server.py\n```\n2. Run the MCP Inspector\n```bash\nmcp dev server.py\n```\n\n## Configure MCP Settings\nAdd the server configuration to your MCP settings file:\n\n#### Claude desktop app \n1. To install automatically via [Smithery](https://smithery.ai/server/@jikime/py-mcp-naver-search):\n\n```bash\nnpx -y @smithery/cli install @jikime/py-mcp-naver-search --client claude\n```\n\n2. To install manually\nopen `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-naver-search\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n#### Cursor IDE \nopen `~/.cursor/mcp.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-naver-search\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n#### for Docker\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"py-mcp-naver-search\"\n      ]\n    }\n  }\n}\n```\n\n### Using the Client\n\nThe repository includes a client script for testing:\n\n```bash\n# Basic search\nuv run client.py blog \"Python programming\" display=5 page=1\n\n# News search with sorting\nuv run client.py news \"AI\" display=10 page=1 sort=date\n\n# Image search with filtering\nuv run client.py image \"cat\" display=10 filter=large\n\n# Check for adult content\nuv run client.py adult \"your query\"\n\n# Errata correction\nuv run client.py errata \"spdlqj\"\n```\n\n## Available Search Categories\n\nThe server supports the following search categories:\n\n1. `blog` - Blog posts\n2. `news` - News articles\n3. `book` - Books\n4. `adult` - Adult content check\n5. `encyc` - Encyclopedia entries\n6. `cafe_article` - Cafe articles\n7. `kin` - Knowledge iN Q&A\n8. `local` - Local business information\n9. `errata` - Keyboard input error correction\n10. `shop` - Shopping items\n11. `doc` - Academic papers and documents\n12. `image` - Images\n13. `webkr` - Web documents\n\n## API Reference\n\n### Tools\n\n#### Search Blog\n```\nsearch_blog(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for blogs on Naver using the given keyword.\n\n#### Search News\n```\nsearch_news(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for news on Naver using the given keyword.\n\n#### Search Book\n```\nsearch_book(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for book information on Naver using the given keyword.\n\n#### Check Adult Query\n```\ncheck_adult_query(query: str) -> str\n```\nDetermines if the input query is an adult search term.\n\n#### Search Encyclopedia\n```\nsearch_encyclopedia(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for encyclopedia information on Naver using the given keyword.\n\n#### Search Cafe Article\n```\nsearch_cafe_article(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for cafe articles on Naver using the given keyword.\n\n#### Search KnowledgeiN\n```\nsearch_kin(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for Knowledge iN Q&A on Naver using the given keyword.\n\n#### Search Local\n```\nsearch_local(query: str, display: int = 5, page: int = 1, sort: str = \"random\") -> str\n```\nSearches for local business information using the given keyword.\n\n#### Correct Errata\n```\ncorrect_errata(query: str) -> str\n```\nConverts Korean/English keyboard input errors.\n\n#### Search Shop\n```\nsearch_shop(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for shopping product information on Naver using the given keyword.\n\n#### Search Document\n```\nsearch_doc(query: str, display: int = 10, page: int = 1) -> str\n```\nSearches for academic papers, reports, etc. using the given keyword.\n\n#### Search Image\n```\nsearch_image(query: str, display: int = 10, page: int = 1, sort: str = \"sim\", filter: str = \"all\") -> str\n```\nSearches for images using the given keyword.\n\n#### Search Web Document\n```\nsearch_webkr(query: str, display: int = 10, page: int = 1) -> str\n```\nSearches for web documents using the given keyword.\n\n### Resources\n\n#### Available Search Categories\n```\nGET naver://available-search-categories\n```\nReturns a list of Naver search categories available on this MCP server.\n\n## Response Format\n\nAll tools return responses in structured text format, optimized for LLM processing:\n\n```\nNaver Blog search results (total 12,345 of 1~10):\n\n### Result 1\nTitle(title): Sample Blog Post\nLink(link): https://blog.example.com/post1\nDescription(description): This is a sample blog post about...\nBlogger name(bloggername): John Doe\nBlogger link(bloggerlink): https://blog.example.com\nPost date(postdate): 20250429\n\n### Result 2\n...\n```\n\n## Acknowledgements\n- [Naver Search API MCP Server Blog](https://devway.tistory.com/55)\n- [Naver Open API](https://developers.naver.com/docs/search/blog/)\n- [MCP Protocol](https://github.com/mcp-foundation/mcp-spec)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "apis",
        "naver",
        "search apis",
        "naver search",
        "search functionality"
      ],
      "category": "web-search"
    },
    "jikime--py-mcp-youtube-toolbox": {
      "owner": "jikime",
      "name": "py-mcp-youtube-toolbox",
      "url": "https://github.com/jikime/py-mcp-youtube-toolbox",
      "imageUrl": "/freedevtools/mcp/pfp/jikime.webp",
      "description": "Interact with YouTube to perform video searches, extract transcripts, retrieve comments, and obtain detailed information about videos and channels. Provides advanced filtering options and access to trending videos and transcript analysis.",
      "stars": 20,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T09:20:54Z",
      "readme_content": "# py-mcp-youtube-toolbox\n[![smithery badge](https://smithery.ai/badge/@jikime/py-mcp-youtube-toolbox)](https://smithery.ai/server/@jikime/py-mcp-youtube-toolbox) ![](https://badge.mcpx.dev?type=server 'MCP Server') ![Version](https://img.shields.io/badge/version-1.0.0-green) ![License](https://img.shields.io/badge/license-MIT-blue)\n\nAn MCP server that provides AI assistants with powerful tools to interact with YouTube, including video searching, transcript extraction, comment retrieval, and more.\n\n<a href=\"https://glama.ai/mcp/servers/@jikime/py-mcp-youtube-toolbox\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@jikime/py-mcp-youtube-toolbox/badge\" alt=\"YouTube Toolbox MCP server\" />\n</a>\n\n## Overview\n\npy-mcp-youtube-toolbox provides the following YouTube-related functionalities:\n\n- Search YouTube videos with advanced filtering options\n- Get detailed information about videos and channels\n- Retrieve video comments with sorting options\n- Extract video transcripts and captions in multiple languages\n- Find related videos for a given video\n- Get trending videos by region\n- Generate summaries of video content based on transcripts\n- Advanced transcript analysis with filtering, searching, and multi-video capabilities\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configure MCP Settings](#configure-mcp-settings)\n- [Tools Documentation](#tools-documentation)\n  - [Video Tools](#video-tools)\n  - [Channel Tools](#channel-tools)\n  - [Transcript Tools](#transcript-tools)\n  - [Prompt Tools](#prompt-tools)\n  - [Resource Tools](#resource-tools)\n- [Development](#development)\n- [License](#license)\n\n## Prerequisites\n1. **Python**: Install Python 3.12 or higher\n2. **YouTube API Key**:\n   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n   - Create a new project or select an existing one\n   - Enable the YouTube Data API v3:\n     1. Go to \"APIs & Services\" > \"Library\"\n     2. Search for and enable \"YouTube Data API v3\"\n   - Create credentials:\n     1. Go to \"APIs & Services\" > \"Credentials\"\n     2. Click \"Create Credentials\" > \"API key\"\n     3. Note down your API key\n\n## Installation\n#### Git Clone\n```bash\ngit clone https://github.com/jikime/py-mcp-youtube-toolbox.git\ncd py-mcp-youtube-toolbox\n```\n\n#### Configuration \n1. Install UV package manager:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Create and activate virtual environment:\n```bash\nuv venv -p 3.12\nsource .venv/bin/activate  # On MacOS/Linux\n# or\n.venv\\Scripts\\activate  # On Windows\n```\n\n3. Install dependencies:\n```bash\nuv pip install -r requirements.txt\n```\n\n4. Environment variables:\n```bash\ncp env.example .env\nvi .env\n# Update with your YouTube API key\nYOUTUBE_API_KEY=your_youtube_api_key\n```\n\n#### Using Docker\n\n1. Build the Docker image:\n```bash\ndocker build -t py-mcp-youtube-toolbox .\n```\n\n2. Run the container:\n```bash\ndocker run -e YOUTUBE_API_KEY=your_youtube_api_key py-mcp-youtube-toolbox\n```\n\n#### Using Local\n\n1. Run the server:\n```bash\nmcp run server.py\n```\n\n2. Run the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## Configure MCP Settings\nAdd the server configuration to your MCP settings file:\n\n#### Claude desktop app \n1. To install automatically via [Smithery](https://smithery.ai/server/@jikime/py-mcp-youtube-toolbox):\n\n```bash\nnpx -y @smithery/cli install @jikime/py-mcp-youtube-toolbox --client claude\n```\n\n2. To install manually\nopen `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-youtube-toolbox\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor IDE \nopen `~/.cursor/mcp.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-youtube-toolbox\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\"\n      }\n    }\n  }\n}\n```\n\n#### for Docker\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"YOUTUBE_API_KEY=your_youtube_api_key\",\n        \"py-mcp-youtube-toolbox\"\n      ]\n    }\n  }\n}\n```\n\n## Tools Documentation\n\n### Video Tools\n\n- `search_videos`: Search for YouTube videos with advanced filtering options (channel, duration, region, etc.)\n- `get_video_details`: Get detailed information about a specific YouTube video (title, channel, views, likes, etc.)\n- `get_video_comments`: Retrieve comments from a YouTube video with sorting options\n- `get_related_videos`: Find videos related to a specific YouTube video\n- `get_trending_videos`: Get trending videos on YouTube by region\n\n### Channel Tools\n\n- `get_channel_details`: Get detailed information about a YouTube channel (name, subscribers, views, etc.)\n\n### Transcript Tools\n\n- `get_video_transcript`: Extract transcripts/captions from YouTube videos in specified languages\n- `get_video_enhanced_transcript`: Advanced transcript extraction with filtering, search, and multi-video capabilities\n\n### Prompt Tools\n\n- `transcript_summary`: Generate summaries of YouTube video content based on transcripts with customizable options\n\n### Resource Tools\n\n- `youtube://available-youtube-tools`: Get a list of all available YouTube tools\n- `youtube://video/{video_id}`: Get detailed information about a specific video\n- `youtube://channel/{channel_id}`: Get information about a specific channel\n- `youtube://transcript/{video_id}?language={language}`: Get transcript for a specific video\n\n## Development\n\nFor local testing, you can use the included client script:\n\n```bash\n# Example: Search videos\nuv run client.py search_videos query=\"MCP\" max_results=5\n\n# Example: Get video details\nuv run client.py get_video_details video_id=zRgAEIoZEVQ\n\n# Example: Get channel details\nuv run client.py get_channel_details channel_id=UCRpOIr-NJpK9S483ge20Pgw\n\n# Example: Get video comments\nuv run client.py get_video_comments video_id=zRgAEIoZEVQ max_results=10 order=time\n\n# Example: Get video transcript\nuv run client.py get_video_transcript video_id=zRgAEIoZEVQ language=ko\n\n# Example: Get related videos\nuv run client.py get_related_videos video_id=zRgAEIoZEVQ max_results=5\n\n# Example: Get trending videos\nuv run client.py get_trending_videos region_code=ko max_results=10\n\n# Example: Advanced transcript extraction\nuv run client.py get_video_enhanced_transcript video_ids=zRgAEIoZEVQ language=ko format=timestamped include_metadata=true start_time=100 end_time=200 query=에이전트 case_sensitive=true segment_method=equal segment_count=2\n\n# Example: \n```\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube",
        "jikime",
        "py",
        "youtube toolbox",
        "jikime py",
        "mcp youtube"
      ],
      "category": "web-search"
    },
    "jlgrimes--ptcg-mcp": {
      "owner": "jlgrimes",
      "name": "ptcg-mcp",
      "url": "https://github.com/jlgrimes/ptcg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jlgrimes.webp",
      "description": "Search and display detailed information about Pokemon Trading Card Game cards, including attributes, legality, and statistics. Access high-resolution images to enhance gameplay effectively.",
      "stars": 8,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-01T18:03:13Z",
      "readme_content": "# Pokemon TCG Card Search MCP\n\nThis Model Context Protocol (MCP) server allows Claude to search and display Pokemon Trading Card Game cards.\n\n## Setup Instructions\n\n1. Update your Claude configuration file:\n\n   - Open `/Users/ABSOLUTE_PATH_HERE/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the following configuration (remove any existing MCP configurations):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"ptcg-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\"ABSOLUTE_PATH_HERE/dist/index.js\"]\n       }\n     }\n   }\n   ```\n\n2. Quit Claude:\n\n   - Open Task Manager\n   - Find and quit Claude completely\n\n3. Restart Claude:\n   - The Pokemon TCG Card Search MCP will be automatically loaded\n   - You can now ask Claude questions about Pokemon cards\n\n## Usage\n\nOnce configured, you can ask Claude questions about Pokemon cards such as:\n\n- \"Show me standard-legal basic Pokemon with free retreat\"\n- \"Find water-type Pokemon with more than 120 HP\"\n- \"Search for Pikachu cards\"\n\nClaude will display the matching cards with their images and relevant information.\n\n## Features\n\n- Search cards by name, type, subtype, legality, and more\n- View high-resolution card images\n- Filter by various card attributes:\n  - Name (supports exact matching with `!` and wildcards with `*`)\n  - Subtypes (e.g., Basic, EX, GX, V, VMAX, etc.)\n  - Legalities (Standard, Expanded, Unlimited)\n  - Types (Water, Fire, Grass, etc.)\n  - Retreat cost\n  - HP\n  - National Pokedex numbers\n  - And more!\n\n## Example Queries\n\nHere are some example queries you can try:\n\n- \"Show me standard-legal basic Pokemon with free retreat\"\n- \"Find water-type Pokemon with more than 120 HP\"\n- \"Search for cards with 'char\\*' in their name\"\n- \"Show me banned cards in Standard format\"\n- \"Find EX Pokemon that evolve from Charmander\"\n\n## Query Syntax\n\n### Name Search\n\n- Regular search: `name:pikachu`\n- Exact match: `!name:pikachu`\n- Wildcard: `name:char*`\n- Preserve hyphens: `name:chien-pao`\n\n### Filters\n\n- Types: `types:water` or `-types:water` (exclude)\n- Subtypes: `subtypes:basic`\n- Legalities: `legalities.standard:legal`\n- HP: `hp:[100 TO 200]`\n- Retreat Cost: `convertedRetreatCost:0`\n\n### Range Queries\n\nUse `[` and `]` for inclusive ranges, `{` and `}` for exclusive ranges:\n\n- `hp:[100 TO 200]` - HP between 100 and 200 (inclusive)\n- `hp:{100 TO 200}` - HP between 100 and 200 (exclusive)\n- `hp:[* TO 100]` - HP up to 100\n- `hp:[100 TO *]` - HP 100 or higher\n\n## Response Format\n\nThe MCP returns card information including:\n\n- Card name\n- Set name\n- High-resolution card image\n- Card legalities\n- Other card details as requested\n\n## Notes\n\n- The MCP uses the Pokemon TCG API to fetch card data\n- Images are displayed directly from the Pokemon TCG API's CDN\n- All queries are case-insensitive\n- Multiple filters can be combined in a single query\n",
      "npm_url": "https://www.npmjs.com/package/ptcg-mcp",
      "npm_downloads": 429,
      "keywords": [
        "ptcg",
        "cards",
        "card",
        "pokemon trading",
        "trading card",
        "game cards"
      ],
      "category": "web-search"
    },
    "jmh108--MCP-server-readability-python": {
      "owner": "jmh108",
      "name": "MCP-server-readability-python",
      "url": "https://github.com/jmh108/MCP-server-readability-python",
      "imageUrl": "/freedevtools/mcp/pfp/jmh108.webp",
      "description": "Extracts and transforms webpage content into clean, LLM-optimized Markdown, removing ads and non-essential elements for improved readability and processing by language models.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T05:37:20Z",
      "readme_content": "# MCP Server Readability Parser (Python / FastMCP)\n\n## Credits/Reference\nThis project is based on the original [server-moz-readability](https://github.com/emzimmer/server-moz-readability) implementation of [emzimmer](https://github.com/emzimmer). (For the original README documentation, please refer to the [original README.md](https://github.com/emzimmer/server-moz-readability/blob/main/readme.md).)\n\nThis Python implementation adapts the original concept to run as python based MCP using [FastMCP](https://github.com/jlowin/fastmcp)\n\n\n\n# Mozilla Readability Parser MCP Server\n\nA Python implementation of the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) server that extracts and transforms webpage content into clean, LLM-optimized Markdown.\n\n## Table of Contents\n- [Features](#features)\n- [Why Not Just Fetch?](#why-not-just-fetch)\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Tool Reference](#tool-reference)\n- [Dependencies](#dependencies)\n- [License](#license)\n\n## Features\n- Removes ads, navigation, footers and other non-essential content\n- Converts clean HTML into well-formatted Markdown\n- Handles errors gracefully\n- Optimized for LLM processing\n- Lightweight and fast\n\n## Why Not Just Fetch?\nUnlike simple fetch requests, this server:\n- Extracts only relevant content using Readability algorithm\n- Eliminates noise like ads, popups, and navigation menus\n- Reduces token usage by removing unnecessary HTML/CSS\n- Provides consistent Markdown formatting for better LLM processing\n- Handles complex web pages with dynamic content\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/jmh108/MCP-server-readability-python.git\ncd MCP-server-readability-python\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Quick Start\n\n1. Start the server:\n```bash\nfastmcp run server.py\n```\n\n2. Example request:\n```bash\ncurl -X POST http://localhost:8000/tools/extract_content \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com/article\"}'\n```\n\n## Tool Reference\n\n### `extract_content`\nFetches and transforms webpage content into clean Markdown.\n\n**Arguments:**\n```json\n{\n  \"url\": {\n    \"type\": \"string\",\n    \"description\": \"The website URL to parse\",\n    \"required\": true\n  }\n}\n```\n\n**Returns:**\n```json\n{\n  \"content\": \"Markdown content...\"\n}\n```\n\n## MCP Server Configuration\n\nTo configure the MCP server, add the following to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"readability\": {\n      \"command\": \"fastmcp\",\n      \"args\": [\"run\", \"server.py\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThe server can then be started using the MCP protocol and accessed via the `parse` tool.\n\n## Dependencies\n- [readability-lxml](https://github.com/buriy/python-readability) - Content extraction\n- [html2text](https://github.com/Alir3z4/html2text) - HTML to Markdown conversion\n- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/) - DOM parsing\n- [requests](https://docs.python-requests.org/) - HTTP requests\n\n## License\nMIT License - See [LICENSE](LICENSE) for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "web",
        "webpage",
        "search jmh108",
        "webpage content",
        "jmh108 mcp"
      ],
      "category": "web-search"
    },
    "jmh108--md-webcrawl-mcp": {
      "owner": "jmh108",
      "name": "md-webcrawl-mcp",
      "url": "https://github.com/jmh108/md-webcrawl-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jmh108.webp",
      "description": "Extracts website content and saves it as markdown files while mapping website structures and links efficiently, enabling batch processing of multiple URLs.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-19T18:57:32Z",
      "readme_content": "# MD MCP Webcrawler Project\n\nA Python-based MCP (https://modelcontextprotocol.io/introduction) web crawler for extracting and saving website content. \n\n## Features\n- Extract website content and save as markdown files\n- Map website structure and links\n- Batch processing of multiple URLs\n- Configurable output directory\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/webcrawler.git\ncd webcrawler\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Optional: Configure environment variables:\n```bash\nexport OUTPUT_PATH=./output  # Set your preferred output directory\n```\n\n## Output\nCrawled content is saved in markdown format in the specified output directory.\n\n## Configuration\nThe server can be configured through environment variables:\n\n- `OUTPUT_PATH`: Default output directory for saved files\n- `MAX_CONCURRENT_REQUESTS`: Maximum parallel requests (default: 5)\n- `REQUEST_TIMEOUT`: Request timeout in seconds (default: 30)\n\n## Claude Set-Up\nInstall with FastMCP \n``` fastmcp install server.py ```\n\nor user custom settings to run with fastmcp directly\n\n````\n\"Crawl Server\": {\n      \"command\": \"fastmcp\",\n      \"args\": [\n        \"run\",\n        \"/Users/mm22/Dev_Projekte/servers-main/src/Webcrawler/server.py\"\n      ],\n      \"env\": {\n        \"OUTPUT_PATH\": \"/Users/user/Webcrawl\"\n      }\n```` \n\n\n\n## Development\n\n### Live Development\n```bash\nfastmcp dev server.py --with-editable .\n```\n### Debug \nIt helps to use https://modelcontextprotocol.io/docs/tools/inspector for debugging\n\n## Examples\n\n### Example 1: Extract and Save Content\n```bash\nmcp call extract_content --url \"https://example.com\" --output_path \"example.md\"\n```\n\n### Example 2: Create Content Index\n```bash\nmcp call scan_linked_content --url \"https://example.com\" | \\\n  mcp call create_index --content_map - --output_path \"index.md\"\n```\n\n## Contributing\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n## Requirements\n\n- Python 3.7+\n- FastMCP (uv pip install fastmcp)\n- Dependencies listed in requirements.txt\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcrawl",
        "markdown",
        "urls",
        "md webcrawl",
        "webcrawl mcp",
        "search jmh108"
      ],
      "category": "web-search"
    },
    "jobsonlook--xhs-mcp": {
      "owner": "jobsonlook",
      "name": "xhs-mcp",
      "url": "https://github.com/jobsonlook/xhs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jobsonlook.webp",
      "description": "Facilitates access to Xiaohongshu's API using JavaScript reverse engineering to bypass the need for heavyweight tools like Playwright. Supports searching notes, retrieving note content, fetching comments, and posting comments.",
      "stars": 334,
      "forks": 79,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T02:51:21Z",
      "readme_content": "# 小红书MCP服务\n[![smithery badge](https://smithery.ai/badge/@jobsonlook/xhs-mcp)](https://smithery.ai/server/@jobsonlook/xhs-mcp)\n[![PyPI version](https://badge.fury.io/py/jobson-xhs-mcp.svg)](https://badge.fury.io/py/jobson-xhs-mcp)\n\n一个用于小红书API的MCP（Model Context Protocol）服务器，支持搜索笔记、获取内容、查看评论和发表评论等功能。\n## 特点\n- [x] 采用js逆向出x-s,x-t,直接请求http接口,无须笨重的playwright\n- [x] 搜索笔记\n- [x] 获取笔记内容\n- [x] 获取笔记的评论\n- [x] 发表评论\n\n![特性](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/feature.png)\n\n## 快速开始\n\n### 方法一：使用uvx（推荐）\n\n#### 1. 环境要求\n- Python 3.12+\n- uv (安装方法: `pip install uv`)\n\n#### 2. 获取小红书的cookie\n[打开web小红书](https://www.xiaohongshu.com/explore)\n登录后，获取cookie，将cookie配置到下一步的 XHS_COOKIE 环境变量中\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n#### 3. 配置MCP服务器\n\n在你的MCP客户端配置文件中添加以下配置：\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"jobson-xhs-mcp\",\n                \"xhs-mcp\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"你的小红书cookie\"\n            }\n        }\n    }\n}\n```\n\n#### 4. 测试运行\n```bash\n# 设置环境变量\nexport XHS_COOKIE=\"你的小红书cookie\"\n\n# 直接运行测试\nuvx --from jobson-xhs-mcp xhs-mcp --help\n```\n\n### 方法二：从源码安装\n\n#### 1. 环境要求\n- node\n- python 3.12\n- uv (pip install uv)\n\n#### 2. 克隆并安装\n```sh\ngit clone git@github.com:jobsonlook/xhs-mcp.git\ncd xhs-mcp\nuv sync\n```\n\n#### 3. 获取小红书的cookie\n[打开web小红书](https://www.xiaohongshu.com/explore)\n登录后，获取cookie，将cookie配置到下一步的 XHS_COOKIE 环境变量中\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n#### 4. 配置MCP服务器\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/xhs-mcp\",\n                \"run\",\n                \"xhs_mcp/__main__.py\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"你的小红书cookie\"\n            }\n        }\n    }\n}\n```\n\n## 可用工具\n\n本MCP服务器提供以下工具：\n\n- `check_cookie()` - 检测cookie是否失效\n- `home_feed()` - 获取首页推荐笔记\n- `search_notes(keywords)` - 根据关键词搜索笔记\n- `get_note_content(url)` - 获取笔记内容（需要带xsec_token的完整URL）\n- `get_note_comments(url)` - 获取笔记评论（需要带xsec_token的完整URL）\n- `post_comment(comment, note_id)` - 发布评论到指定笔记\n\n## 使用示例\n\n### 在Claude Desktop中使用\n\n1. 打开Claude Desktop的设置\n2. 找到MCP服务器配置\n3. 添加上述JSON配置\n4. 重启Claude Desktop\n5. 现在你可以在对话中使用小红书相关功能了\n\n### 常见问题\n\n**Q: Cookie如何获取？**\nA: 在浏览器中登录小红书网页版，打开开发者工具，在Network标签页中找到任意请求，复制Cookie头的值。\n\n**Q: 为什么提示cookie失效？**\nA: 小红书的cookie有时效性，需要定期更新。重新登录网页版获取新的cookie即可。\n\n**Q: uvx命令找不到？**\nA: 请先安装uv：`pip install uv`，然后确保PATH环境变量包含uv的安装路径。\n\n## 免责声明\n本项目仅用于学习交流，禁止用于其他用途，任何涉及商业盈利目的均不得使用，否则风险自负。\n\n",
      "npm_url": "https://www.npmjs.com/package/xhs-mcp",
      "npm_downloads": 1311,
      "keywords": [
        "xiaohongshu",
        "jobsonlook",
        "xhs",
        "xiaohongshu api",
        "jobsonlook xhs",
        "search jobsonlook"
      ],
      "category": "web-search"
    },
    "josemartinrodriguezmortaloni--webSearch-Tools": {
      "owner": "josemartinrodriguezmortaloni",
      "name": "webSearch-Tools",
      "url": "https://github.com/josemartinrodriguezmortaloni/webSearch-Tools",
      "imageUrl": "/freedevtools/mcp/pfp/josemartinrodriguezmortaloni.webp",
      "description": "Perform intelligent web searches and extract specific information from web pages using natural language prompts. Utilize advanced web scraping and crawling capabilities for enhanced content analysis and retrieval tasks.",
      "stars": 1,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-18T05:59:54Z",
      "readme_content": "# WebSearch - Advanced Web Search and Content Extraction Tool\n\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)\n![Firecrawl](https://img.shields.io/badge/firecrawl-latest-green)\n![uv](https://img.shields.io/badge/uv-latest-purple)\n\nA powerful web search and content extraction tool built with Python, leveraging the Firecrawl API for advanced web scraping, searching, and content analysis capabilities.\n\n## 🚀 Features\n\n- **Advanced Web Search**: Perform intelligent web searches with customizable parameters\n- **Content Extraction**: Extract specific information from web pages using natural language prompts\n- **Web Crawling**: Crawl websites with configurable depth and limits\n- **Web Scraping**: Scrape web pages with support for various output formats\n- **MCP Integration**: Built as a Model Context Protocol (MCP) server for seamless integration\n\n## 📋 Prerequisites\n\n- Python 3.8 or higher\n- uv package manager\n- Firecrawl API key\n- OpenAI API key (optional, for enhanced features)\n- Tavily API key (optional, for additional search capabilities)\n\n## 🛠️ Installation\n\n1. Install uv:\n\n```bash\n# On Windows (using pip)\npip install uv\n\n# On Unix/MacOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH (Unix/MacOS)\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Add uv to PATH (Windows - add to Environment Variables)\n# Add: %USERPROFILE%\\.local\\bin\n```\n\n2. Clone the repository:\n\n```bash\ngit clone https://github.com/yourusername/websearch.git\ncd websearch\n```\n\n3. Create and activate a virtual environment with uv:\n\n```bash\n# Create virtual environment\nuv venv\n\n# Activate on Windows\n.\\.venv\\Scripts\\activate.ps1\n\n# Activate on Unix/MacOS\nsource .venv/bin/activate\n```\n\n4. Install dependencies with uv:\n\n```bash\n# Install from requirements.txt\nuv sync\n```\n\n5. Set up environment variables:\n\n```bash\n# Create .env file\ntouch .env\n\n# Add your API keys\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n## 🎯 Usage\n\n### Setting Up With Claude for Desktop\n\nInstead of running the server directly, you can configure Claude for Desktop to access the WebSearch tools:\n\n1. Locate or create your Claude for Desktop configuration file:\n\n   - Windows: `%env:AppData%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n2. Add the WebSearch server configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"websearch\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"D:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\WebSearch\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n3. Make sure to replace the directory path with the absolute path to your WebSearch project folder.\n\n4. Save the configuration file and restart Claude for Desktop.\n\n5. Once configured, the WebSearch tools will appear in the tools menu (hammer icon) in Claude for Desktop.\n\n### Available Tools\n\n1. **Search**\n\n2. **Extract Information**\n\n3. **Crawl Websites**\n\n4. **Scrape Content**\n\n## 📚 API Reference\n\n### Search\n\n- `query` (str): The search query\n- Returns: Search results in JSON format\n\n### Extract\n\n- `urls` (List[str]): List of URLs to extract information from\n- `prompt` (str): Instructions for extraction\n- `enableWebSearch` (bool): Enable supplementary web search\n- `showSources` (bool): Include source references\n- Returns: Extracted information in specified format\n\n### Crawl\n\n- `url` (str): Starting URL\n- `maxDepth` (int): Maximum crawl depth\n- `limit` (int): Maximum pages to crawl\n- Returns: Crawled content in markdown/HTML format\n\n### Scrape\n\n- `url` (str): Target URL\n- Returns: Scraped content with optional screenshots\n\n## 🔧 Configuration\n\n### Environment Variables\n\nThe tool requires certain API keys to function. We provide a `.env.example` file that you can use as a template:\n\n1. Copy the example file:\n\n```bash\n# On Unix/MacOS\ncp .env.example .env\n\n# On Windows\ncopy .env.example .env\n```\n\n2. Edit the `.env` file with your API keys:\n\n```env\n# OpenAI API key - Required for AI-powered features\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Firecrawl API key - Required for web scraping and searching\nFIRECRAWL_API_KEY=your_firecrawl_api_key_here\n```\n\n### Getting the API Keys\n\n1. **OpenAI API Key**:\n\n   - Visit [OpenAI's platform](https://platform.openai.com/)\n   - Sign up or log in\n   - Navigate to API keys section\n   - Create a new secret key\n\n2. **Firecrawl API Key**:\n   - Visit [Firecrawl's website](https://docs.firecrawl.dev/)\n   - Create an account\n   - Navigate to your dashboard\n   - Generate a new API key\n\nIf everything is configured correctly, you should receive a JSON response with search results.\n\n### Troubleshooting\n\nIf you encounter errors:\n\n1. Ensure all required API keys are set in your `.env` file\n2. Verify the API keys are valid and have not expired\n3. Check that the `.env` file is in the root directory of the project\n4. Make sure the environment variables are being loaded correctly\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Firecrawl](https://docs.firecrawl.dev/) for their powerful web scraping API\n- [OpenAI](https://openai.com/) for AI capabilities\n- [MCP](https://modelcontextprotocol.io/introduction)The MCP community for the protocol specification\n\n## 📬 Contact\n\nJosé Martín Rodriguez Mortaloni - [@m4s1t425](https://x.com/m4s1t425) - jmrodriguezm13@gmail.com\n\n---\n\nMade with ❤️ using Python and Firecrawl\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websearch",
        "scraping",
        "searches",
        "josemartinrodriguezmortaloni websearch",
        "websearch tools",
        "web scraping"
      ],
      "category": "web-search"
    },
    "jsonallen--perplexity-mcp": {
      "owner": "jsonallen",
      "name": "perplexity-mcp",
      "url": "https://github.com/jsonallen/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jsonallen.webp",
      "description": "Provides web search functionality leveraging Perplexity AI's API, allowing users to search for recent information on various topics.",
      "stars": 254,
      "forks": 33,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:32Z",
      "readme_content": "# perplexity-mcp MCP server\n\n[![smithery badge](https://smithery.ai/badge/perplexity-mcp)](https://smithery.ai/server/perplexity-mcp)\n\nA Model Context Protocol (MCP) server that provides web search functionality using [Perplexity AI's](https://www.perplexity.ai/) API. Works with the [Anthropic](https://www.anthropic.com/news/model-context-protocol) Claude desktop client.\n\n## Example\n\nLet's you use prompts like, \"Search the web to find out what's new at Anthropic in the past week.\"\n\n## Glama Scores\n\n<a href=\"https://glama.ai/mcp/servers/ebg0za4hn9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ebg0za4hn9/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Components\n\n### Prompts\n\nThe server provides a single prompt:\n\n- perplexity_search_web: Search the web using Perplexity AI\n  - Required \"query\" argument for the search query\n  - Optional \"recency\" argument to filter results by time period:\n    - 'day': last 24 hours\n    - 'week': last 7 days\n    - 'month': last 30 days (default)\n    - 'year': last 365 days\n  - Uses Perplexity's API to perform web searches\n\n### Tools\n\nThe server implements one tool:\n\n- perplexity_search_web: Search the web using Perplexity AI\n  - Takes \"query\" as a required string argument\n  - Optional \"recency\" parameter to filter results (day/week/month/year)\n  - Returns search results from Perplexity's API\n\n## Installation\n\n### Installing via Smithery\n\nTo install Perplexity MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/perplexity-mcp):\n\n```bash\nnpx -y @smithery/cli install perplexity-mcp --client claude\n```\n\n### Requires [UV](https://github.com/astral-sh/uv) (Fast Python package and project manager)\n\nIf uv isn't installed.\n\n```bash\n# Using Homebrew on macOS\nbrew install uv\n```\n\nor\n\n```bash\n# On macOS and Linux.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows.\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n### Environment Variables\n\nThe following environment variable is required in your claude_desktop_config.json. You can obtain an API key from [Perplexity](https://perplexity.ai)\n\n- `PERPLEXITY_API_KEY`: Your Perplexity AI API key\n\nOptional environment variables:\n\n- `PERPLEXITY_MODEL`: The Perplexity model to use (defaults to \"sonar\" if not specified)\n\n  Available models:\n\n  - `sonar-deep-research`: 128k context - Enhanced research capabilities\n  - `sonar-reasoning-pro`: 128k context - Advanced reasoning with professional focus\n  - `sonar-reasoning`: 128k context - Enhanced reasoning capabilities\n  - `sonar-pro`: 200k context - Professional grade model\n  - `sonar`: 128k context - Default model\n  - `r1-1776`: 128k context - Alternative architecture\n\nAnd updated list of models is avaiable (here)[https://docs.perplexity.ai/guides/model-cards]\n\n### Cursor & Claude Desktop Installation\n\nAdd this tool as a mcp server by editing the Cursor/Claude config file.\n\n```json\n  \"perplexity-mcp\": {\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"XXXXXXXXXXXXXXXXXXXX\",\n      \"PERPLEXITY_MODEL\": \"sonar\"\n    },\n    \"command\": \"uvx\",\n    \"args\": [\n      \"perplexity-mcp\"\n    ]\n  }\n```\n\n#### Cursor\n- On MacOS: `/Users/your-username/.cursor/mcp.json`\n- On Windows: `C:\\Users\\your-username\\.cursor\\mcp.json`\n\nIf everything is working correctly, you should now be able to call the tool from Cursor.\n\n\n#### Claude Desktop\n- On MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nTo verify the server is working. Open the Claude client and use a prompt like \"search the web for news about openai in the past week\". You should see an alert box open to confirm tool usage. Click \"Allow for this chat\".\n\n  <img width=\"600\" alt=\"mcp_screenshot\" src=\"https://github.com/user-attachments/assets/922d8f6a-8c9a-4978-8be6-788e70b4d049\" />",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "perplexity",
        "jsonallen",
        "search jsonallen",
        "leveraging perplexity",
        "jsonallen perplexity"
      ],
      "category": "web-search"
    },
    "jtucker--mcp-untappd-server": {
      "owner": "jtucker",
      "name": "mcp-untappd-server",
      "url": "https://github.com/jtucker/mcp-untappd-server",
      "imageUrl": "/freedevtools/mcp/pfp/jtucker.webp",
      "description": "Query the Untappd API to find beers and retrieve detailed information about them. Supports beer searches and accessing specific beer details using their unique identifiers.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-02-28T01:19:19Z",
      "readme_content": "# Untappd Model Context Protocol Server\n\nThis is a simple MCP server written in `node` to query the [Untappd API](https://untappd.com/api/docs). Unfortunately they are no longer accepting registrations for new API keys. \n\nCurrently only supports 3 tools:\n\n- `search_beer`: This will search Untappd for a beer\n- `get_beer_info`: This get's the detailed beer info via the `beer_id` returned from a search.\n- `get_user_checkins`: _currently does not work_ This will return a users checkin's. \n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"untappd-server\": {\n      \"command\": \"/path/to/untappd-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "retrieve",
        "beer searches",
        "search jtucker",
        "api beers"
      ],
      "category": "web-search"
    },
    "junmer--mcp-server-lottiefiles": {
      "owner": "junmer",
      "name": "mcp-server-lottiefiles",
      "url": "https://github.com/junmer/mcp-server-lottiefiles",
      "imageUrl": "/freedevtools/mcp/pfp/junmer.webp",
      "description": "Search and retrieve Lottie animations from LottieFiles, access detailed animation information, and discover popular animations for projects.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T10:15:46Z",
      "readme_content": "# LottieFiles MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-server-lottiefiles)](https://smithery.ai/server/mcp-server-lottiefiles)\n\nA Model Context Protocol (MCP) server for searching and retrieving Lottie animations from LottieFiles.\n\n## Features\n\n- Search Lottie animations\n- Get animation details\n- Get popular animations list\n\n## Installation\n\n### Installing via Smithery\n\nTo install LottieFiles Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-lottiefiles):\n\n```bash\nnpx -y smithery install mcp-server-lottiefiles --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n1. Start the server:\n\n```bash\nnpm start\n```\n\n2. Connect using an MCP client\n\n## API Tools\n\n### Search Animations\n\nSearch for Lottie animations by keywords.\n\nParameters:\n- `query`: Search keywords\n- `page`: Page number (optional, default: 1)\n- `limit`: Items per page (optional, default: 20)\n\n### Get Animation Details\n\nGet detailed information about a specific Lottie animation.\n\nParameters:\n- `id`: Unique identifier of the animation\n\n### Get Popular Animations\n\nGet a list of currently popular Lottie animations.\n\nParameters:\n- `page`: Page number (optional, default: 1)\n- `limit`: Items per page (optional, default: 20)\n\n## Development\n\n```bash\n# Build\nnpm run build\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-lottiefiles",
      "npm_downloads": 587,
      "keywords": [
        "lottiefiles",
        "lottie",
        "animations",
        "animations lottiefiles",
        "lottie animations",
        "lottiefiles search"
      ],
      "category": "web-search"
    },
    "jzhang17--prospect-research-mcp": {
      "owner": "jzhang17",
      "name": "prospect-research-mcp",
      "url": "https://github.com/jzhang17/prospect-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jzhang17.webp",
      "description": "Provides tools for conducting prospect research, including semantic search, webpage scraping, and batch processing of search queries. Enhances research tasks with a focus on understanding context and intent behind queries.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-18T20:44:16Z",
      "readme_content": "\n# Prospect Research MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jzhang17/prospect-research-mcp)](https://smithery.ai/server/@jzhang17/prospect-research-mcp)\n\nA Model Context Protocol (MCP) server implementation focused on prospect research tools, deployed on Smithery Web infrastructure.\n\n## Features\n\n- **Semantic Search**: Contextual search that understands meaning and intent behind queries\n- **Webpage Scraping**: Extract and process content from multiple web pages\n- **Batch Search Processing**: Execute multiple search queries in parallel\n- **Comprehensive Coverage**: Combine different search approaches for thorough research\n\n## Tools\n\n- **web-search**\n  - A semantic search engine (Tavily) that understands the contextual meaning and intent behind queries\n  - Inputs:\n    - `query` (string): The search query to look up\n\n- **scrape-webpages**\n  - Scrape the provided web pages for detailed information\n  - Inputs:\n    - `links` (array): A list of URLs to scrape (optimally less than 10)\n  - Processes content to remove images and returns combined content from provided URLs\n\n- **batch-web-search**\n  - Traditional keyword-based search (Google via Search1API) that processes multiple queries simultaneously\n  - Inputs:\n    - `queries` (array): List of search queries to process in parallel (optimally less than 30)\n  - Executes multiple distinct search queries in parallel\n\n## Prompts\n\n- `simple-assist` - A basic prompt for general queries\n- `research` - A prompt for detailed research questions\n- `review-code` - A prompt for code review\n\n## Configuration\n\n### Required API Keys\nThis server requires the following API keys:\n- `TAVILY_API_KEY` - For semantic web search functionality\n- `JINA_API_KEY` - For webpage scraping\n- `SEARCH1API_KEY` - For batch web search\n\nThese are configured in the Smithery Web environment for the deployed version.\n\n### Usage with Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"prospect-research\": {\n      \"transport\": \"sse\",\n      \"url\": \"https://smithery.ai/server/@jzhang17/prospect-research-mcp\",\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"JINA_API_KEY\": \"YOUR_JINA_API_KEY\",\n        \"SEARCH1API_KEY\": \"YOUR_SEARCH1API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### For Other MCP Clients\nConfigure your client to connect to the server using the SSE transport type and the Smithery-hosted URL.\n\n\n## Structure\n\n- `/src/index.ts` - Main server entrypoint\n- `/src/tools/` - MCP tool implementations (web search, webpage scraping, batch search)\n- `/src/prompts/` - MCP prompt implementations\n- `/src/types/` - TypeScript type definitions\n\n## Deployment\n\nThis server is deployed to Smithery Web platform. To access the deployed server:\n\n1. Visit [Smithery.ai](https://smithery.ai/server/@jzhang17/prospect-research-mcp)\n2. The server is available at the URL provided by Smithery Web\n\n## References\n\n- [Model Context Protocol](https://modelcontextprotocol.io)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [Smithery AI Platform](https://smithery.ai)\n- [MCP Client List](https://modelcontextprotocol.io/clients)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "mcp",
        "prospect",
        "semantic search",
        "prospect research",
        "web search"
      ],
      "category": "web-search"
    },
    "kagisearch--kagimcp": {
      "owner": "kagisearch",
      "name": "kagimcp",
      "url": "https://github.com/kagisearch/kagimcp",
      "imageUrl": "/freedevtools/mcp/pfp/kagisearch.webp",
      "description": "Access Kagi Search API for retrieving information from the web, allowing users to perform web searches and gather data from online sources.",
      "stars": 198,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T17:10:45Z",
      "readme_content": "# Kagi MCP server\n\n[![smithery badge](https://smithery.ai/badge/kagimcp)](https://smithery.ai/server/kagimcp)\n\n<a href=\"https://glama.ai/mcp/servers/xabrrs4bka\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xabrrs4bka/badge\" alt=\"Kagi Server MCP server\" />\n</a>\n\n## Setup Intructions\n> Before anything, unless you are just using non-search tools, ensure you have access to the search API. It is currently in closed beta and available upon request. Please reach out to support@kagi.com for an invite.\n\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n### Installing via Smithery\n\nAlternatively, you can install Kagi for Claude Desktop via [Smithery](https://smithery.ai/server/kagimcp):\n\n```bash\nnpx -y @smithery/cli install kagimcp --client claude\n```\n\n### Setup with Claude\n#### Claude Desktop\n```json\n// claude_desktop_config.json\n// Can find location through:\n// Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uvx\",\n      \"args\": [\"kagimcp\"],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n#### Claude Code\nAdd the Kagi mcp server with the following command (setting summarizer engine optional):\n\n```bash\nclaude mcp add kagi -e KAGI_API_KEY=\"YOUR_API_KEY_HERE\" KAGI_SUMMARIZER_ENGINE=\"YOUR_ENGINE_CHOICE_HERE\" -- uvx kagimcp\n```\n\nNow claude code can use the Kagi mcp server. However, claude code comes with its own web search functionality by default, which may conflict with Kagi. You can disable claude's web search functionality with the following in your claude code settings file (`~/.claude/settings.json`):\n\n```json\n{\n  \"permissions\": {\n    \"deny\": [\n      \"WebSearch\"\n    ]\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\nnpx @modelcontextprotocol/inspector uvx kagimcp\n```\n\n## Local/Dev Setup Instructions\n\n### Clone repo\n`git clone https://github.com/kagisearch/kagimcp.git`\n\n### Install dependencies\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen install MCP server dependencies:\n```bash\ncd kagimcp\n\n# Create virtual environment and activate it\nuv venv\n\nsource .venv/bin/activate # MacOS/Linux\n# OR\n.venv/Scripts/activate # Windows\n\n# Install dependencies\nuv sync\n```\n### Setup with Claude Desktop\n\n#### Using MCP CLI SDK\n```bash\n# `pip install mcp[cli]` if you haven't\nmcp install /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py -v \"KAGI_API_KEY=API_KEY_HERE\"\n```\n\n#### Manually\n```json\n# claude_desktop_config.json\n# Can find location through:\n# Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp\",\n        \"run\",\n        \"kagimcp\"\n      ],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\n# If mcp cli installed (`pip install mcp[cli]`)\nmcp dev /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py\n\n# If not\nnpx @modelcontextprotocol/inspector \\\n      uv \\\n      --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp \\\n      run \\\n      kagimcp\n```\nThen access MCP Inspector at `http://localhost:5173`. You may need to add your Kagi API key in the environment variables in the inspector under `KAGI_API_KEY`.\n\n# Advanced Configuration\n- Level of logging is adjustable through the `FASTMCP_LOG_LEVEL` environment variable (e.g. `FASTMCP_LOG_LEVEL=\"ERROR\"`)\n  - Relevant issue: https://github.com/kagisearch/kagimcp/issues/4\n- Summarizer engine can be customized using the `KAGI_SUMMARIZER_ENGINE` environment variable (e.g. `KAGI_SUMMARIZER_ENGINE=\"daphne\"`)\n  - Learn about the different summarization engines [here](https://help.kagi.com/kagi/api/summarizer.html#summarization-engines)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kagisearch",
        "kagi",
        "searches",
        "kagi search",
        "search kagisearch",
        "kagisearch kagimcp"
      ],
      "category": "web-search"
    },
    "kazuph--mcp-fetch": {
      "owner": "kazuph",
      "name": "mcp-fetch",
      "url": "https://github.com/kazuph/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Fetch web content and process images to facilitate efficient interaction with online resources. Supports integration with MCP clients like Claude Desktop for seamless content management.",
      "stars": 30,
      "forks": 18,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T16:56:16Z",
      "readme_content": "# MCP Fetch\n\nModel Context Protocol server for fetching web content and processing images. This allows Claude Desktop (or any MCP client) to fetch web content and handle images appropriately.\n\n<a href=\"https://glama.ai/mcp/servers/5mknfdhyrg\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5mknfdhyrg/badge\" alt=\"@kazuph/mcp-fetch MCP server\" /></a>\n\n## Quick Start (For Users)\n\nTo use this tool with Claude Desktop, simply add the following to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"tools\": {\n    \"imageFetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-fetch\"]\n    }\n  }\n}\n```\n\nThis will automatically download and run the latest version of the tool when needed.\n\n### Required Setup\n\n1. Enable Accessibility for Claude:\n   - Open System Settings\n   - Go to Privacy & Security > Accessibility\n   - Click the \"+\" button\n   - Add Claude from your Applications folder\n   - Turn ON the toggle for Claude\n\nThis accessibility setting is required for automated clipboard operations (Cmd+V) to work properly.\n\n## Features\n\n- **Web Content Extraction**: Automatically extracts and formats web content as markdown\n- **Article Title Extraction**: Extracts and displays the title of the article\n- **Image Processing**: Optional processing of images from web pages with optimization (disabled by default, enable with `enableFetchImages: true`)\n- **File Saving**: Images are automatically saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` directory when processed\n- **Dual Output**: Both file saving and optional Base64 encoding for AI display\n- **Pagination Support**: Supports pagination for both text and images\n- **JPEG Optimization**: Automatically optimizes images as JPEG for better performance\n- **GIF Support**: Extracts first frame from animated GIFs\n\n## For Developers\n\nThe following sections are for those who want to develop or modify the tool.\n\n## Prerequisites\n\n- Node.js 18+\n- macOS (for clipboard operations)\n- Claude Desktop (install from https://claude.ai/desktop)\n- tsx (install via `npm install -g tsx`)\n\n## Installation\n\n```bash\ngit clone https://github.com/kazuph/mcp-fetch.git\ncd mcp-fetch\nnpm install\nnpm run build\n```\n\n## Image Processing Specifications\n\nWhen processing images from web content, the following optimizations are applied:\n\n- Images are converted to JPEG format with quality control\n- Maximum width limited to 1200px by default\n- Maximum height limited to 1600px by default\n- Chroma subsampling (4:2:0) for better compression\n- MozJPEG optimization for smaller file sizes\n\n## Configuration\n\n1. Make sure Claude Desktop is installed and running.\n\n2. Install tsx globally if you haven't:\n```bash\nnpm install -g tsx\n# or\npnpm add -g tsx\n```\n\n3. Modify your Claude Desktop config located at:\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n\nYou can easily find this through the Claude Desktop menu:\n1. Open Claude Desktop\n2. Click Claude on the Mac menu bar\n3. Click \"Settings\"\n4. Click \"Developer\"\n\nAdd the following to your MCP client's configuration:\n\n```json\n{\n  \"tools\": {\n    \"imageFetch\": {\n      \"args\": [\"tsx\", \"/path/to/mcp-fetch/index.ts\"]\n    }\n  }\n}\n```\n\n## Available Parameters\n\n- `url` (required): The URL to fetch\n- `maxLength` (default: 20000): Maximum length of content to return\n- `startIndex` (default: 0): Starting position in content\n- `imageStartIndex` (default: 0): Starting position for image collection\n- `raw` (default: false): Return raw content instead of processed markdown\n- `imageMaxCount` (default: 3): Maximum number of images to process per request\n- `imageMaxHeight` (default: 4000): Maximum height of merged image\n- `imageMaxWidth` (default: 1000): Maximum width of merged image\n- `imageQuality` (default: 80): JPEG quality (1-100)\n- `enableFetchImages` (default: false): Enable fetching and processing of images. FOR CLAUDE USERS ONLY: YOU ARE ALLOWED TO ENABLE THIS FEATURE!!! CLAUDE CAN FETCH AND PROCESS IMAGES!!! ENJOY THIS CAPABILITY!!!!!\n- `allowCrossOriginImages` (default: true): When true, also fetch images hosted on different origins from the page\n- `saveImages` (default: true): Save processed images to local files\n- `returnBase64` (default: false): Return base64 encoded images for AI display\n- `ignoreRobotsTxt` (default: false): Ignore robots.txt restrictions\n\n### Security Hardening (v1.5.1)\n\n- Only `http://` and `https://` URLs are allowed for page and image fetches\n- Blocks private/loopback/link-local IPs and local hostnames (e.g., `localhost`, `.local`)\n- Manual redirect handling with validation (max 3 hops)\n- Request timeouts (default 12s, configurable via `MCP_FETCH_TIMEOUT_MS`)\n- Response size limits: HTML up to 2MB, images up to 10MB (tunable via env)\n\nEnvironment variables:\n\n- `MCP_FETCH_TIMEOUT_MS` (default: 12000)\n- `MCP_FETCH_MAX_REDIRECTS` (default: 3)\n- `MCP_FETCH_MAX_HTML_BYTES` (default: 2000000)\n- `MCP_FETCH_MAX_IMAGE_BYTES` (default: 10000000)\n\n## Examples\n\n### Basic Content Fetching (No Images)\n```json\n{\n  \"url\": \"https://example.com\"\n}\n```\n\n### Fetching with Images (File Saving Only)\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"imageMaxCount\": 3\n}\n```\n\n### Fetching with Images for AI Display\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"returnBase64\": true,\n  \"imageMaxCount\": 3\n}\n```\n\n### Paginating Through Images\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"imageStartIndex\": 3,\n  \"imageMaxCount\": 3\n}\n```\n\n## Notes\n\n- This tool is designed for macOS only due to its dependency on macOS-specific clipboard operations.\n- Images are processed using Sharp for optimal performance and quality.\n- When multiple images are found, they are merged vertically with consideration for size limits.\n- Animated GIFs are automatically handled by extracting their first frame.\n- **File Saving**: Images are automatically saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` with filename format `hostname_HHMMSS_index.jpg`\n- **Tool Name**: The tool name has been changed from `fetch` to `imageFetch` to avoid conflicts with native fetch functions.\n\n## Changelog\n\n### v1.2.0\n- **BREAKING CHANGE**: Tool name changed from `fetch` to `imageFetch` to avoid conflicts\n- **NEW**: Automatic file saving - Images are now saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` by default\n- **NEW**: Added `saveImages` parameter (default: true) to control file saving\n- **NEW**: Added `returnBase64` parameter (default: false) for AI image display\n- **BEHAVIOR CHANGE**: Default behavior now saves files instead of only returning base64\n- Improved AI assistant integration with clear instructions for base64 option\n- Enhanced file organization with date-based directories and structured naming\n\n### v1.1.3\n- Changed default behavior: Images are not fetched by default (`enableFetchImages: false`)\n- Removed `disableImages` in favor of `enableFetchImages` parameter\n\n### v1.1.0\n- Added article title extraction feature\n- Improved response formatting to include article titles\n- Fixed type issues with MCP response content\n\n### v1.0.0\n- Initial release\n- Web content extraction\n- Image processing and optimization\n- Pagination support\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "kazuph",
        "fetch",
        "web",
        "mcp fetch",
        "kazuph mcp",
        "content management"
      ],
      "category": "web-search"
    },
    "kbyk004-diy--playwright-lighthouse-mcp": {
      "owner": "kbyk004-diy",
      "name": "playwright-lighthouse-mcp",
      "url": "https://github.com/kbyk004-diy/playwright-lighthouse-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kbyk004-diy.webp",
      "description": "Analyze website performance and capture screenshots using Playwright and Lighthouse. Gain insights into web application performance metrics with real-time analysis and visual feedback.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-01T17:15:32Z",
      "readme_content": "# Playwright-Lighthouse MCP Server\n\nA MCP server that analyzes web site performance using Playwright and Lighthouse. Through the Model Context Protocol (MCP), LLMs can perform web site performance analysis.\n\n## Features\n\n- Performance analysis with Lighthouse\n- Screenshot capture\n\n## Setup\n\n### Prerequisites\n\n- Node.js 18 or higher\n- npm\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/kbyk004/playwright-lighthouse-mcp.git\ncd playwright-lighthouse-mcp\n\n# Install dependencies\nnpm install\nnpx playwright install\n\n# Build\nnpm run build\n```\n\n## Usage\n\n### Debugging MCP Server\n\n```bash\nnpm run inspector\n```\n\n### Integration with MCP Clients\n\nThis server is designed to be used with clients that support the Model Context Protocol (MCP). For example, it can be integrated with Claude for Desktop.\n\n#### Configuration Example for Claude for Desktop\n\nAdd the following to the Claude for Desktop configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"playwright-lighthouse\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path-to/playwright-lighthouse-mcp/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. run-lighthouse\n\nRuns a Lighthouse performance analysis on the currently open page.\n\nParameters:\n- `url`: The URL of the website you want to analyze\n- `categories`: Array of categories to analyze (default: [\"performance\"])\n  - Available categories: \"performance\", \"accessibility\", \"best-practices\", \"seo\", \"pwa\"\n- `maxItems`: Maximum number of improvement items to display for each category (default: 3, max: 5)\n\n### 2. take-screenshot\n\nTakes a screenshot of the currently open page.\n\nParameters:\n- `url`: The URL of the website you want to capture\n- `fullPage`: If true, captures a screenshot of the entire page (default: false)\n\n## Output Format\n\nThe analysis results include:\n\n- Overall scores for each selected category with color indicators\n- Key improvement areas grouped by category\n- Path to the saved report file\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "performance",
        "playwright",
        "web",
        "website performance",
        "playwright lighthouse",
        "analyze website"
      ],
      "category": "web-search"
    },
    "kevinwatt--mcp-server-searxng": {
      "owner": "kevinwatt",
      "name": "mcp-server-searxng",
      "url": "https://github.com/kevinwatt/mcp-server-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/kevinwatt.webp",
      "description": "Integrates with SearXNG to provide privacy-focused meta search capabilities, combining results from multiple search engines without tracking or profiling users. Supports various categories, languages, and filtering options, ensuring reliable and safe search experiences.",
      "stars": 21,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T22:59:13Z",
      "readme_content": "# SearXNG MCP Server\n[![smithery badge](https://smithery.ai/badge/@kevinwatt/mcp-server-searxng)](https://smithery.ai/server/@kevinwatt/mcp-server-searxng)\n\nAn MCP server implementation that integrates with SearXNG, providing privacy-focused meta search capabilities.\n\n## Features\n\n- **Meta Search**: Combines results from multiple search engines\n- **Privacy-Focused**: No tracking, no user profiling\n- **Multiple Categories**: Support for general, news, science, files, images, videos, and more\n- **Language Support**: Search in specific languages or all languages\n- **Time Range Filtering**: Filter results by day, week, month, or year\n- **Safe Search**: Three levels of safe search filtering\n- **Fallback Support**: Multiple SearXNG instances for reliability\n\n## Installation\n\n### Installing via Smithery\n\nTo install SearXNG MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kevinwatt/mcp-server-searxng):\n\n```bash\nnpx -y @smithery/cli install @kevinwatt/mcp-server-searxng --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g @kevinwatt/mcp-server-searxng\n```\n\n## Usage\n\n### Direct Run\n\n```bash\nmcp-server-searxng\n```\n\n### With [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n\n1. Click \"+ Add MCP Server\" in Dive Desktop\n2. Copy and paste this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/mcp-server-searxng\"\n      ]\n    }\n  }\n}\n```\n\n3. Click \"Save\" to install the MCP server\n\n## Tool Documentation\n\n- **web_search**\n  - Execute meta searches across multiple engines\n  - Inputs:\n    - `query` (string): Search terms\n    - `page` (number, optional): Page number (default: 1)\n    - `language` (string, optional): Language code (e.g., 'en', 'all', default: 'all')\n    - `categories` (array, optional): Search categories (default: ['general'])\n      - Available: \"general\", \"news\", \"science\", \"files\", \"images\", \"videos\", \"music\", \"social media\", \"it\"\n    - `time_range` (string, optional): Time filter (day/week/month/year)\n    - `safesearch` (number, optional): Safe search level (0: None, 1: Moderate, 2: Strict, default: 1)\n\n## Development\n\n```bash\ngit clone https://github.com/kevinwatt/mcp-server-searxng.git\ncd mcp-server-searxng\nnpm install\nnpm run build\nnpm start\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## Prerequisites\n\nYou need a local SearXNG instance running. To set it up:\n\n# Run SearXNG with Docker\n\n## Quick Start\n\n```bash\n# Create config directory\nmkdir -p searxng\n\n# Create config file\ntee searxng/settings.yml << EOF\nuse_default_settings: true\n\nserver:\n  bind_address: \"0.0.0.0\"\n  secret_key: \"CHANGE_THIS_TO_SOMETHING_SECURE\"  # Generate a random key\n  port: 8080\n\nsearch:\n  safe_search: 0\n  formats:\n    - html\n    - json\n\nengines:\n  - name: google\n    engine: google\n    shortcut: g\n\n  - name: duckduckgo\n    engine: duckduckgo\n    shortcut: d\n\n  - name: bing\n    engine: bing\n    shortcut: b\n\nserver.limiter: false\nEOF\n\n# Start container\ndocker run -d \\\n  --name searxng \\\n  -p 8080:8080 \\\n  -v \"$(pwd)/searxng:/etc/searxng\" \\\n  searxng/searxng\n```\n\n## Test Search Function\n\n```bash\n# Test JSON API with curl\ncurl -v 'http://localhost:8080/search?q=test&format=json'\n\n# Or visit in browser\nhttp://localhost:8080/search?q=test\n```\n\n## Container Management\n\n```bash\n# Stop container\ndocker stop searxng\n\n# Remove container\ndocker rm searxng\n\n# View container logs\ndocker logs searxng\n\n# Enable auto-start on boot\ndocker update --restart always searxng\n```\n\nThe `--restart always` flag ensures that:\n- Container starts automatically when Docker daemon starts\n- Container restarts automatically if it crashes\n- Container restarts automatically if it is stopped unless explicitly stopped by user\n\n## Custom Configuration\n\nEdit `searxng/settings.yml` to:\n- Modify search engine list\n- Adjust security settings\n- Configure UI language\n- Change API limits\n\nFor detailed configuration options, see [SearXNG Documentation](https://docs.searxng.org/)\n\n## Environment Variables\n\n- `SEARXNG_INSTANCES`: Comma-separated list of SearXNG instances URLs\n  Default: `http://localhost:8080`\n\n- `SEARXNG_USER_AGENT`: Custom User-Agent header for requests\n  Default: `MCP-SearXNG/1.0`\n\n- `NODE_TLS_REJECT_UNAUTHORIZED`: Set to '0' to bypass SSL certificate verification (for development with self-signed certificates)\n  Default: undefined (SSL verification enabled)\n\nExample configuration with all options:\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"name\": \"searxng\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/mcp-server-searxng\"\n      ],\n      \"env\": {\n        \"SEARXNG_INSTANCES\": \"http://localhost:8080,https://searx.example.com\",\n        \"SEARXNG_USER_AGENT\": \"CustomBot/1.0\",\n        \"NODE_TLS_REJECT_UNAUTHORIZED\": \"0\"\n      }\n    }\n  }\n}\n```\n\n> ⚠️ Warning: Disabling SSL certificate verification is not recommended in production environments.\n",
      "npm_url": "https://www.npmjs.com/package/@kevinwatt/mcp-server-searxng",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "web",
        "server searxng",
        "searxng provide",
        "integrates searxng"
      ],
      "category": "web-search"
    },
    "kouui--web-search-duckduckgo": {
      "owner": "kouui",
      "name": "web-search-duckduckgo",
      "url": "https://github.com/kouui/web-search-duckduckgo",
      "imageUrl": "/freedevtools/mcp/pfp/kouui.webp",
      "description": "Search the web using DuckDuckGo to retrieve relevant search results, including titles, URLs, and snippets. Optionally fetch and convert webpage content into markdown format with support for concurrent URL fetching and error handling.",
      "stars": 3,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-29T04:23:05Z",
      "readme_content": "# DuckDuckGo Web Search MCP Server\n\nThis project provides an MCP (Model Context Protocol) server that allows you to search the web using the DuckDuckGo search engine and optionally fetch and summarize the content of the found URLs.\n\n## Features\n\n*   **Web Search:** Search the web using DuckDuckGo.\n*   **Result Extraction:** Extracts titles, URLs, and snippets from search results.\n*   **Content Fetching (Optional):** Fetches the content of the URLs found in the search results and converts it to markdown format using jina api.\n*   **Parallel Fetching:** Fetches multiple URLs concurrently for faster processing.\n*   **Error Handling:** Gracefully handles timeouts and other potential errors during search and fetching.\n*   **Configurable:** Allows you to set the maximum number of search results to return.\n* **Jina API**: using jina api to convert html to markdown.\n* **MCP Compliant**: This server is designed to be used with any MCP-compatible client.\n\n## Usage\n\n1.  **Prerequisites:**\n    *   `uvx` package manager\n\n\n2. **Claude Desktop Configuration**\n    * If you are using Claude Desktop, you can add the server to the `claude_desktop_config.json` file.\n    ```json\n    {\n        \"mcpServers\": {\n            \"web-search-duckduckgo\": {\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"--from\",\n                    \"git+https://github.com/kouui/web-search-duckduckgo.git@main\",\n                    \"main.py\"\n                ]\n            }\n        }\n    }\n    ```\n    \n    **the above configuration is not working, you might need to clone the repository to local pc and use the following configuration**\n\n    ```json\n    {\n        \"mcpServers\": {\n            \"web-search-duckduckgo\": {\n                \"command\": \"uv\",\n                \"args\": [\n                    \"--directory\",\n                    \"/path/to/web-search-duckduckgo\",\n                    \"run\",\n                    \"main.py\"\n                ]\n            }\n        }\n    }\n    ```\n\n3. **Tool**\n    *   In your MCP client (e.g., Claude), you can now use the following tools:\n\n    *   **`search_and_fetch`:** Search the web and fetch the content of the URLs.\n\n        *   `query`: The search query string.\n        *   `limit`: The maximum number of results to return (default: 3, maximum: 10).\n\n\n    *   **`fetch`:** Fetch the content of a specific URL.\n\n        *   `url`: The URL to fetch.\n\n\n## License\n\nThis project is licensed under the MIT License. (Add a license file if you want to specify a license).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "kouui",
        "search",
        "duckduckgo search",
        "kouui web",
        "search duckduckgo"
      ],
      "category": "web-search"
    },
    "kxkaloo--mcp": {
      "owner": "kxkaloo",
      "name": "mcp",
      "url": "https://github.com/kxkaloo/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Extract structured data from websites by providing a URL and a clear prompt describing the data to extract. Retrieve detailed product information, pricing, descriptions, and more from web pages seamlessly.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 0,
      "keywords": [
        "kxkaloo",
        "search",
        "extract",
        "search kxkaloo",
        "kxkaloo mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "laulauland--bluesky-context-server": {
      "owner": "laulauland",
      "name": "bluesky-context-server",
      "url": "https://github.com/laulauland/bluesky-context-server",
      "imageUrl": "/freedevtools/mcp/pfp/laulauland.webp",
      "description": "Enables querying of Bluesky instances through an MCP protocol, facilitating data interaction and retrieval.",
      "stars": 27,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T01:09:00Z",
      "readme_content": "# Bluesky Context Server\n[![smithery badge](https://smithery.ai/badge/bluesky-context-server)](https://smithery.ai/server/bluesky-context-server)\n\nA Model Context Protocol (MCP) server that enables MCP clients like Claude Desktop to interact with Bluesky. Query your profile, search posts, get your timeline, and more directly from your AI assistant.\n\n## Prerequisites\n\n- **Runtime**: Either [Bun](https://bun.sh/) or [Node.js](https://nodejs.org/) v22.6.0+\n- A Bluesky account\n\n## Setup\n\n### 1. Get Your Bluesky Credentials\n\nYou'll need two pieces of information from your Bluesky account:\n\n#### BLUESKY_IDENTIFIER\nThis is your Bluesky handle (username). It can be in either format:\n- `username.bsky.social` (e.g., `alice.bsky.social`)\n- `@username.bsky.social` (e.g., `@alice.bsky.social`)\n\n#### BLUESKY_APP_KEY\nThis is an App Password, which is different from your regular Bluesky password. To create one:\n\n1. Go to [Bluesky Settings](https://bsky.app/settings)\n2. Navigate to \"Privacy and Security\" → \"App Passwords\"\n3. Click \"Add App Password\"\n4. Give it a name (e.g., \"MCP Server\")\n5. Copy the generated password (it looks like `xxxx-xxxx-xxxx-xxxx`)\n\n⚠️ **Important**: Use the App Password, not your regular account password!\n\n### 2. Installation\n\n#### Option A: Installing via Smithery (Recommended)\n\nTo install Bluesky Context Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@laulauland/bluesky-context-server):\n\n```bash\nnpx -y @smithery/cli install @laulauland/bluesky-context-server --client claude\n```\n\nThen add your credentials to the generated configuration.\n\n#### Option B: Manual Installation\n\n1. Clone or download this repository\n2. Configure your Claude Desktop app to use the MCP server:\n\n**Using Bun:**\n```json\n// ~/Library/Application Support/Claude/config.json (macOS)\n// %APPDATA%/Claude/config.json (Windows)\n{\n\t\"mcpServers\": {\n\t\t\"bluesky\": {\n\t\t\t\"command\": \"/Users/your-username/.bun/bin/bun\",\n\t\t\t\"args\": [\n\t\t\t\t\"/path/to/bluesky-context-server/packages/server/bin/index.ts\"\n\t\t\t],\n\t\t\t\"env\": {\n\t\t\t\t\"BLUESKY_APP_KEY\": \"your-app-password-here\",\n\t\t\t\t\"BLUESKY_IDENTIFIER\": \"your-handle.bsky.social\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n**Using Node.js:**\n```json\n// ~/Library/Application Support/Claude/config.json (macOS)\n// %APPDATA%/Claude/config.json (Windows)\n{\n\t\"mcpServers\": {\n\t\t\"bluesky\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\n\t\t\t\t\"--experimental-strip-types\",\n\t\t\t\t\"/path/to/bluesky-context-server/packages/server/bin/index.ts\"\n\t\t\t],\n\t\t\t\"env\": {\n\t\t\t\t\"BLUESKY_APP_KEY\": \"your-app-password-here\",\n\t\t\t\t\"BLUESKY_IDENTIFIER\": \"your-handle.bsky.social\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n3. Restart Claude Desktop\n\n### 3. Testing the Connection\n\nAfter setup, you can test the connection by asking Claude something like:\n- \"Can you get my Bluesky profile?\"\n- \"Show me my recent posts on Bluesky\"\n- \"Search for posts about AI on Bluesky\"\n\n## Available MCP Tools\n\nThis server provides the following tools that Claude can use:\n\n### Profile & Account Tools\n\n#### `bluesky_get_profile`\nGet your Bluesky profile information including display name, bio, follower count, etc.\n- **Parameters**: None\n- **Returns**: Complete profile data\n\n#### `bluesky_get_follows`\nGet a list of accounts you follow.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: List of followed accounts with profile info\n\n#### `bluesky_get_followers`\nGet a list of accounts following you.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: List of followers with profile info\n\n### Post & Feed Tools\n\n#### `bluesky_get_posts`\nGet your recent posts.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Your recent posts with engagement data\n\n#### `bluesky_get_personal_feed`\nGet your personalized Bluesky timeline/feed.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts from your personalized feed\n\n#### `bluesky_get_liked_posts`\nGet posts you've liked.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts you've liked\n\n### Search Tools\n\n#### `bluesky_search_posts`\nSearch for posts across Bluesky.\n- **Parameters**: \n  - `query` (required): Search query string\n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts matching your search query\n\n#### `bluesky_search_profiles`\nSearch for Bluesky user profiles.\n- **Parameters**: \n  - `query` (required): Search query string\n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: User profiles matching your search query\n\n## Example Usage\n\nOnce configured, you can ask Claude things like:\n\n- \"What's in my Bluesky feed today?\"\n- \"Search for posts about TypeScript on Bluesky\"\n- \"Who are my most recent followers?\"\n- \"Show me posts I've liked recently\"\n- \"Find Bluesky users interested in AI\"\n\n## Troubleshooting\n\n### Common Issues\n\n1. **\"Authentication failed\"**: Double-check your `BLUESKY_APP_KEY` and `BLUESKY_IDENTIFIER`\n2. **\"Server not responding\"**: Ensure Bun is installed and the path to the server is correct\n3. **\"Permission denied\"**: Make sure the server file has execute permissions\n\n### Getting Help\n\nIf you encounter issues:\n1. Check that your Bluesky credentials are correct\n2. Verify Bun is installed: `bun --version`\n3. Test the server manually: `cd packages/server && bun start`\n4. Check Claude Desktop's logs for error messages\n",
      "npm_url": "https://www.npmjs.com/package/@laulauland/bluesky-context-server",
      "npm_downloads": 76,
      "keywords": [
        "bluesky",
        "retrieval",
        "search",
        "querying bluesky",
        "bluesky instances",
        "context server"
      ],
      "category": "web-search"
    },
    "leehanchung--bing-search-mcp": {
      "owner": "leehanchung",
      "name": "bing-search-mcp",
      "url": "https://github.com/leehanchung/bing-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/leehanchung.webp",
      "description": "Connect to the Microsoft Bing Search API to perform comprehensive web, news, and image searches. Retrieve timely information and visual content efficiently, enhancing AI applications with search capabilities.",
      "stars": 56,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T10:07:53Z",
      "readme_content": "# Bing Search MCP Server\n\nA Model Context Protocol (MCP) server for Microsoft Bing Search API integration, allowing AI assistants to perform web, news, and image searches.\n\n[![smithery badge](https://smithery.ai/badge/@leehanchung/bing-search-mcp)](https://smithery.ai/server/@leehanchung/bing-search-mcp)\n\n\n\n\n## Features\n\n- Web search for general information\n- News search for recent events and timely information\n- Image search for visual content\n- Rate limiting to prevent API abuse\n- Comprehensive error handling\n\n## Requirements\n\n- Python 3.10 or higher\n- Microsoft Bing Search API key\n- MCP-compatible client (e.g., Claude Desktop, Cursor)\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   uv pip install -e .\n   ```\n\n## Configuration\n\nSet the required environment variables:\n\n```bash\nexport BING_API_KEY=\"your-bing-api-key\"\nexport BING_API_URL=\"https://api.bing.microsoft.com/\"  # Optional\n```\n\nFor Windows:\n```cmd\nset BING_API_KEY=your-bing-api-key\nset BING_API_URL=https://api.bing.microsoft.com/\n```\n\n## Usage\n\n### Running the server\n\n```\nuvx bing-search-mcp\n```\n\n### Configuring with Claude for Desktop\n\nAdd the following to your Claude Desktop configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"bing-search\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"/path/to/your/bing-search-mcp\"\n      ],\n      \"env\": {\n        \"BING_API_KEY\": \"your-bing-api-key\"\n      }\n    }\n  }\n}\n```\n\n\n## Available Tools\n\n### 1. bing_web_search\nGeneral web search for information, websites, and content.\n\n```python\nbing_web_search(query: str, count: int = 10, offset: int = 0, market: str = \"en-US\")\n```\n\n### 2. bing_news_search\nSearch for news articles and current events.\n\n```python\nbing_news_search(query: str, count: int = 10, market: str = \"en-US\", freshness: str = \"Day\")\n```\n\n### 3. bing_image_search\nSearch for images.\n\n```python\nbing_image_search(query: str, count: int = 10, market: str = \"en-US\")\n```\n\n## Getting a Bing API Key\n\n1. Visit [Microsoft Azure Portal](https://portal.azure.com/)\n2. Create or sign in to your Azure account\n3. Create a new Bing Search resource\n4. Go to the resource and find your API key in the \"Keys and Endpoint\" section\n\n## License\n\n[MIT License](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "searches",
        "search",
        "bing search",
        "microsoft bing",
        "leehanchung bing"
      ],
      "category": "web-search"
    },
    "letsbuildagent--perplexity-tool": {
      "owner": "letsbuildagent",
      "name": "perplexity-tool",
      "url": "https://github.com/letsbuildagent/perplexity-tool",
      "imageUrl": "/freedevtools/mcp/pfp/letsbuildagent.webp",
      "description": "Integrate web-based research capabilities using Perplexity AI's API to provide accurate answers with citations, enhancing the information retrieval process.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-30T12:01:23Z",
      "readme_content": "# Perplexity Tool for Claude Desktop\n\nA custom MCP tool that integrates Perplexity AI's API with Claude Desktop, allowing Claude to perform web-based research and provide answers with citations.\n\n## Prerequisites Installation\n\n1. Install Git:\n   - For Mac: \n     - Install [Homebrew](https://brew.sh/) first by pasting this in Terminal:\n     ```bash\n     /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n     ```\n     - Then install Git:\n     ```bash\n     brew install git\n     ```\n   - For Windows:\n     - Download Git from [git-scm.com](https://git-scm.com/downloads)\n     - Run the installer\n\n2. Install Node.js:\n   - For Mac: \n     ```bash\n     brew install node\n     ```\n   - For Windows:\n     - Download from [nodejs.org](https://nodejs.org/)\n     - Run the installer\n\n3. Verify installations by running:\n```bash\ngit --version\nnode --version\n```\n\n## Tool Installation\n\n1. Clone the repository\n```bash\ngit clone https://github.com/letsbuildagent/perplexity-tool\ncd perplexity-tool\n```\n\n2. Install dependencies\n```bash\nnpm install\n```\n\n3. Set up your API Key\n\nYou have two options:\n\nOption 1 (Quick setup):\n- Open `server.js`\n- Find this line:\n```javascript\nconst PERPLEXITY_API_KEY = \"YOUR-API-KEY-HERE\";\n```\n- Replace with your Perplexity API key\n\nOption 2 (Best practice):\n- Create a .env file:\n  ```bash\n  # On Mac/Linux:\n  touch .env\n  open .env\n  \n  # On Windows:\n  notepad .env\n  ```\n  Or simply create a new file named `.env` in your text editor\n- Add your API key to the .env file:\n  ```\n  PERPLEXITY_API_KEY=your-api-key-here\n  ```\n- Install dotenv:\n  ```bash\n  npm install dotenv\n  ```\n- Update server.js:\n  ```javascript\n  import 'dotenv/config'\n  const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;\n  ```\n\n4. Configure Claude Desktop\n- Open `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Add this configuration:\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-tool\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/full/path/to/perplexity-tool/server.js\"\n      ]\n    }\n  }\n}\n```\nReplace `/full/path/to` with the actual path where you cloned the repository.\n\n5. Restart Claude Desktop\n\n## Usage\n\nOnce installed, you can use the tool through Claude with commands like:\n\n- \"Ask Perplexity about recent developments in AI\"\n- \"Use Perplexity to research the history of quantum computing\"\n- \"Search Perplexity for information about climate change, focusing on the last month\"\n\n### Advanced Options\n\nYou can specify additional parameters:\n- `temperature`: Controls response randomness (0-2)\n- `max_tokens`: Limits response length\n- `search_domain_filter`: Restricts search to specific domains\n- `search_recency_filter`: Filters by time period (day/week/month/year)\n\n## Troubleshooting\n\n1. Git not found:\n   - Make sure you've installed Git correctly\n   - Try restarting your terminal\n   - On Mac, make sure Homebrew is in your PATH\n\n2. Node.js errors:\n   - Verify Node.js installation with `node --version`\n   - Try reinstalling Node.js\n\n3. API Key issues:\n   - Make sure you've correctly copied your API key\n   - Check that there are no extra spaces in your .env file\n   - If using Option 2, verify dotenv is installed\n\n4. Tool not appearing in Claude:\n   - Check the path in claude_desktop_config.json\n   - Make sure the path points to your server.js file\n   - Restart Claude Desktop\n   - Check the console for any error messages\n\n## License\n\nMIT\n\n## Security Note\n\nIf you're planning to share your code or make it public:\n- Don't commit your API key to Git\n- Use the .env method (Option 2)\n- Add .env to your .gitignore file\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "citations",
        "search",
        "research",
        "perplexity tool",
        "perplexity ai",
        "web search"
      ],
      "category": "web-search"
    },
    "lieyanqzu--ygocdb-mcp": {
      "owner": "lieyanqzu",
      "name": "ygocdb-mcp",
      "url": "https://github.com/lieyanqzu/ygocdb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/lieyanqzu.webp",
      "description": "Provides access to Chinese Yu-Gi-Oh! card information and images via keyword searches and ID queries, allowing for detailed retrieval of card data and visuals from the game.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-09T05:51:51Z",
      "readme_content": "# 百鸽(ygocdb.com) MCP Server\n\n[English](README/README.en.md) | 中文\n\n一个基于 [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) 的服务端，用于与 [百鸽(ygocdb.com)](https://ygocdb.com/)等 API 交互。提供了一系列工具来查询游戏王中文卡牌信息。\n\n[![smithery badge](https://smithery.ai/badge/@lieyanqzu/ygocdb-mcp)](https://smithery.ai/server/@lieyanqzu/ygocdb-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/@lieyanqzu/ygocdb-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@lieyanqzu/ygocdb-mcp/badge\" />\n</a>\n\n## API 文档\n\n本服务端基于游戏王卡牌数据库的公开 API。\n\n- 卡牌搜索: `https://ygocdb.com/api/v0/?search=关键字`\n- 卡牌图片: `https://cdn.233.momobako.com/ygopro/pics/<id>.jpg`\n\n## 使用示例\n\n\n\n## 功能特性\n\n- **search_cards**  \n  通过关键字搜索游戏王卡牌，可以搜索卡牌名称、效果文本等。\n  \n- **get_card_by_id**  \n  通过卡牌ID获取单张游戏王卡牌的详细信息。\n  \n- **get_card_image**  \n  通过卡牌ID获取游戏王卡牌的图片。\n\n## 使用方法\n\n### 通过 NPM 包使用\n\n```bash\n# 全局安装\nnpm install -g ygocdb-mcp-server\n\n# 或直接运行（推荐）\nnpx ygocdb-mcp-server\n```\n\n### 本地开发\n\n```bash\n# 克隆项目\ngit clone <repository-url>\ncd ygocdb-mcp\n\n# 安装依赖\nnpm install\n\n# 构建项目\nnpm run build\n\n# 运行 STDIO 模式\nnpm run start:stdio\n\n# 运行 HTTP 模式\nnpm run start:http\n```\n\n### 运行模式\n\n服务端支持两种运行模式：\n\n#### STDIO 模式（默认）\n用于与 Claude Desktop 等 MCP 客户端直接集成：\n\n```bash\nnpm run start:stdio\n```\n\n#### HTTP 模式\n用于容器部署或 HTTP 客户端访问：\n\n```bash\nnpm run start:http\n```\n\nHTTP 服务器将在端口 8081 上启动，端点为 `http://localhost:8081/mcp`\n\n### 在 Claude Desktop 中集成\n\n在 `claude_desktop_config.json` 中添加配置：\n\n#### 使用 NPX（推荐）\n```json\n{\n  \"mcpServers\": {\n    \"ygocdb\": {\n      \"command\": \"npx\",\n      \"args\": [\"ygocdb-mcp-server\"]\n    }\n  }\n}\n```\n\n#### 使用本地构建\n```json\n{\n  \"mcpServers\": {\n    \"ygocdb\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/ygocdb-mcp/dist/index.js\"],\n      \"cwd\": \"path/to/ygocdb-mcp\"\n    }\n  }\n}\n```\n\n### Docker 部署\n\n```bash\n# 构建镜像\ndocker build -t ygocdb-mcp .\n\n# 运行 STDIO 模式（用于集成）\ndocker run -i --rm ygocdb-mcp\n\n# 运行 HTTP 模式（用于服务）\ndocker run -p 8081:8081 ygocdb-mcp\n```\n\n### 跨平台支持\n\n项目使用 `cross-env` 确保在所有平台上正确设置环境变量：\n\n- **Windows**: `npm run start:http` 或 `npm run start:stdio`\n- **macOS/Linux**: `npm run start:http` 或 `npm run start:stdio`\n- **Docker**: 自动使用 HTTP 模式",
      "npm_url": "https://www.npmjs.com/package/ygocdb-mcp-server",
      "npm_downloads": 282,
      "keywords": [
        "card",
        "ygocdb",
        "searches",
        "retrieval card",
        "card information",
        "ygocdb mcp"
      ],
      "category": "web-search"
    },
    "limklister--mcp-google-custom-search-server": {
      "owner": "limklister",
      "name": "mcp-google-custom-search-server",
      "url": "https://github.com/limklister/mcp-google-custom-search-server",
      "imageUrl": "/freedevtools/mcp/pfp/limklister.webp",
      "description": "Enables language models to perform web searches using Google's Custom Search API, providing formatted results such as titles, URLs, and descriptions. Supports real-time information retrieval while adhering to the Model Context Protocol (MCP).",
      "stars": 33,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T14:50:30Z",
      "readme_content": "\n# MCP Google Custom Search Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Google's Custom Search API. This server enables Language Learning Models (LLMs) to perform web searches using a standardized interface.\n\n\n\n## 🌟 Features\n\n- Seamless integration with Google Custom Search API\n- Model Context Protocol (MCP) compliant server implementation\n- Type-safe implementation using TypeScript\n- Environment variable configuration\n- Input validation using Zod\n- Configurable search results (up to 10 per query)\n- Formatted search results including titles, URLs, and descriptions\n- Error handling and validation\n- Compatible with Claude Desktop and other MCP clients\n\n## 📋 Prerequisites\n\nBefore you begin, ensure you have:\n\n1. A Google Cloud Project with Custom Search API enabled\n\n   - Visit [Google Cloud Console](https://console.cloud.google.com)\n   - Enable the Custom Search API\n   - Create API credentials\n\n2. A Custom Search Engine ID\n\n   - Visit [Programmable Search Engine](https://programmablesearchengine.google.com/)\n   - Create a new search engine\n   - Get your Search Engine ID\n\n3. Local development requirements:\n   - Node.js (v18 or higher)\n   - npm (comes with Node.js)\n\n## 🚀 Quick Start\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/limklister/mcp-google-custom-search-server.git\n   cd mcp-google-custom-search-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Create a .env file:\n\n   ```bash\n   GOOGLE_API_KEY=your-api-key\n   GOOGLE_SEARCH_ENGINE_ID=your-search-engine-id\n   ```\n\n4. Build the server:\n\n   ```bash\n   npm run build\n   ```\n\n5. Start the server:\n   ```bash\n   npm start\n   ```\n\n## 🔧 Configuration\n\n### Environment Variables\n\n| Variable                | Description                       | Required |\n| ----------------------- | --------------------------------- | -------- |\n| GOOGLE_API_KEY          | Your Google Custom Search API key | Yes      |\n| GOOGLE_SEARCH_ENGINE_ID | Your Custom Search Engine ID      | Yes      |\n\n### Claude Desktop Integration\n\nAdd this configuration to your Claude Desktop config file (typically located at `~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-search\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mcp-google-custom-search-server/build/index.js\"\n      ],\n      \"env\": {\n        \"GOOGLE_API_KEY\": \"your-api-key\",\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your-search-engine-id\"\n      }\n    }\n  }\n}\n```\n\n## 📖 API Reference\n\n### Available Tools\n\n#### search\n\nPerforms a web search using Google Custom Search API.\n\n**Parameters:**\n\n- `query` (string, required): The search query to execute\n- `numResults` (number, optional): Number of results to return\n  - Default: 5\n  - Maximum: 10\n\n**Example Response:**\n\n```\nResult 1:\nTitle: Example Search Result\nURL: https://example.com\nDescription: This is an example search result description\n---\n\nResult 2:\n...\n```\n\n## 🛠️ Development\n\n### Project Structure\n\n```\nmcp-google-custom-search-server/\n├── src/\n│   └── index.ts          # Main server implementation\n├── build/                # Compiled JavaScript output\n├── .env                  # Environment variables\n├── package.json          # Project dependencies and scripts\n├── tsconfig.json         # TypeScript configuration\n└── README.md            # Project documentation\n```\n\n### Available Scripts\n\n- `npm run build`: Compile TypeScript to JavaScript\n- `npm start`: Start the MCP server\n- `npm run dev`: Watch mode for development\n\n### Testing\n\n1. Using MCP Inspector:\n\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n2. Manual testing with example queries:\n   ```bash\n   # After starting the server\n   {\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"callTool\",\"params\":{\"name\":\"search\",\"arguments\":{\"query\":\"example search\"}}}\n   ```\n\n<a href=\"https://glama.ai/mcp/servers/y1s99uqqq6\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/y1s99uqqq6/badge\" alt=\"Google Custom Search Server MCP server\" />\n</a>\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/limklister-mcp-google-custom-search-server-badge.png)](https://mseep.ai/app/limklister-mcp-google-custom-search-server)\n\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🙏 Acknowledgments\n\n- Built with [Model Context Protocol (MCP)](https://github.com/anthropics/model-context-protocol)\n- Uses Google's Custom Search API\n- Inspired by the need for better search capabilities in LLM applications\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "google",
        "limklister",
        "mcp google",
        "search limklister",
        "web search"
      ],
      "category": "web-search"
    },
    "lixiaotiancai--exa-mcp-server": {
      "owner": "lixiaotiancai",
      "name": "exa-mcp-server",
      "url": "https://github.com/lixiaotiancai/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Enables AI assistants to perform web searches using Exa's search API, providing real-time web information with structured search results. It also caches recent searches for future access in a safe and controlled manner.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "search",
        "exa",
        "exa search",
        "search lixiaotiancai",
        "web search"
      ],
      "category": "web-search"
    },
    "lmcc-dev--mult-fetch-mcp-server": {
      "owner": "lmcc-dev",
      "name": "mult-fetch-mcp-server",
      "url": "https://github.com/lmcc-dev/mult-fetch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/lmcc-dev.webp",
      "description": "Fetch web content in various formats such as HTML and JSON using intelligent scraping techniques while providing bilingual support for English and Chinese.",
      "stars": 13,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T08:03:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/lmcc-dev-mult-fetch-mcp-server)\n\n# mult-fetch-mcp-server\n\n[![npm version](https://img.shields.io/npm/v/@lmcc-dev/mult-fetch-mcp-server.svg)](https://www.npmjs.com/package/@lmcc-dev/mult-fetch-mcp-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Node.js Version](https://img.shields.io/node/v/@lmcc-dev/mult-fetch-mcp-server)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-blue.svg)](https://www.typescriptlang.org/)\n[![MCP SDK](https://img.shields.io/badge/MCP%20SDK-1.7.0-brightgreen.svg)](https://github.com/modelcontextprotocol/typescript-sdk)\n[![GitHub Stars](https://img.shields.io/github/stars/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/issues)\n[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/pulls)\n[![npm downloads](https://img.shields.io/badge/downloads-coming%20soon-lightgrey)](https://www.npmjs.com/package/@lmcc-dev/mult-fetch-mcp-server)\n[![GitHub last commit](https://img.shields.io/github/last-commit/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/graphs/contributors)\n[![smithery badge](https://smithery.ai/badge/@lmcc-dev/mult-fetch-mcp-server)](https://smithery.ai/server/@lmcc-dev/mult-fetch-mcp-server)\n[![codecov](https://codecov.io/gh/lmcc-dev/mult-fetch-mcp-server/branch/main/graph/badge.svg)](https://codecov.io/gh/lmcc-dev/mult-fetch-mcp-server)\n[![CodeFactor](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server/badge)](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server)\n\n\n\n<!-- Future badges to consider:\n[![CodeFactor](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server/badge)](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server)\n[![Maintainability](https://api.codeclimate.com/v1/badges/a99a88d28ad37a79dbf6/maintainability)](https://codeclimate.com/github/lmcc-dev/mult-fetch-mcp-server/maintainability)\n-->\n\nThis project implements an MCP-compliant client and server for communication between AI assistants and external tools.\n\n[English](./README.md) | [中文文档](./README.zh.md)\n\n## Project Structure\n\n```\nfetch-mcp/\n├── src/                         # Source code directory\n│   ├── lib/                     # Library files\n│   │   ├── fetchers/            # Web fetching implementation\n│   │   │   ├── browser/         # Browser-based fetching\n│   │   │   │   ├── BrowserFetcher.ts      # Browser fetcher implementation\n│   │   │   │   ├── BrowserInstance.ts     # Browser instance management\n│   │   │   │   └── PageOperations.ts      # Page interaction operations\n│   │   │   ├── node/            # Node.js-based fetching\n│   │   │   └── common/          # Shared fetching utilities\n│   │   ├── utils/               # Utility modules\n│   │   │   ├── ChunkManager.ts        # Content chunking\n│   │   │   ├── ContentProcessor.ts    # HTML to text conversion\n│   │   │   ├── ContentExtractor.ts    # Intelligent content extraction\n│   │   │   ├── ContentSizeManager.ts  # Content size limiting\n│   │   │   └── ErrorHandler.ts        # Error handling\n│   │   ├── server/              # Server-related modules\n│   │   │   ├── index.ts         # Server entry\n│   │   │   ├── browser.ts       # Browser management\n│   │   │   ├── fetcher.ts       # Web fetching logic\n│   │   │   ├── tools.ts         # Tool registration and handling\n│   │   │   ├── resources.ts     # Resource handling\n│   │   │   ├── prompts.ts       # Prompt templates\n│   │   │   └── types.ts         # Server type definitions\n│   │   ├── i18n/                # Internationalization support\n│   │   └── types.ts             # Common type definitions\n│   ├── client.ts                # MCP client implementation\n│   └── mcp-server.ts            # MCP server main entry\n├── index.ts                     # Server entry point\n├── tests/                       # Test files\n└── dist/                        # Compiled files\n```\n\n## MCP Specification\n\nThe Model Context Protocol (MCP) defines two main transport methods:\n\n1. **Standard Input/Output (Stdio)**: The client starts the MCP server as a child process, and they communicate through standard input (stdin) and standard output (stdout).\n2. **Server-Sent Events (SSE)**: Used to pass messages between client and server.\n\nThis project implements the Standard Input/Output (Stdio) transport method.\n\n## Features\n\n- Implementation based on the official MCP SDK\n- Support for Standard Input/Output (Stdio) transport\n- Multiple web scraping methods (HTML, JSON, text, Markdown, plain text conversion)\n- Intelligent mode switching: automatic switching between standard requests and browser mode\n- Content size management: automatically splits large content into manageable chunks to solve AI model context size limitations\n- Chunked content retrieval: ability to request specific chunks of large content while maintaining context continuity\n- Detailed debug logging to stderr\n- Bilingual internationalization (English and Chinese)\n- Modular design for easy maintenance and extension\n- **Intelligent Content Extraction**: Based on Mozilla's Readability library, capable of extracting meaningful content from web pages while filtering out advertisements and navigation elements\n- **Metadata Support**: Ability to extract webpage metadata such as title, author, publication date, and site information\n- **Smart Content Detection**: Automatically detects if a page contains meaningful content, filtering out login pages, error pages, and other pages without substantial content\n- **Browser Automation Enhancements**: Support for page scrolling, cookie management, selector waiting, and other advanced browser interactions\n\n## Installation\n\n### Installing via Smithery\n\nTo install Mult Fetch MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@lmcc-dev/mult-fetch-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @lmcc-dev/mult-fetch-mcp-server --client claude\n```\n\n### Local Installation\n\n```bash\npnpm install\n```\n\n### Global Installation\n\n```bash\npnpm add -g @lmcc-dev/mult-fetch-mcp-server\n```\n\nOr run directly with npx (no installation required):\n\n```bash\nnpx @lmcc-dev/mult-fetch-mcp-server\n```\n\n## Integration with Claude\n\nTo integrate this tool with Claude desktop, you need to add server configuration:\n\n### Configuration File Location\n\n- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Configuration Examples\n\n#### Method 1: Using npx (Recommended)\n\nThis method is the simplest, doesn't require specifying the full path, and is suitable for global installation or direct use with npx:\n\n```json\n{\n  \"mcpServers\": {\n    \"mult-fetch-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"@lmcc-dev/mult-fetch-mcp-server\"],\n      \"env\": {\n        \"MCP_LANG\": \"en\"  // Set language to English, options: \"zh\" or \"en\"\n      }\n    }\n  }\n}\n```\n\n#### Method 2: Specifying Full Path\n\nIf you need to use a specific installation location, you can specify the full path:\n\n```json\n{\n  \"mcpServers\": {\n    \"mult-fetch-mcp-server\": {\n      \"command\": \"path-to/bin/node\",\n      \"args\": [\"path-to/@lmcc-dev/mult-fetch-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"MCP_LANG\": \"en\"  // Set language to English, options: \"zh\" or \"en\"\n      }\n    }\n  }\n}\n```\n\nPlease replace `path-to/bin/node` with the path to the Node.js executable on your system, and replace `path-to/@lmcc-dev/mult-fetch-mcp-server` with the actual path to this project.\n\n### Usage Examples\n\nBelow is an example of using this tool in Claude desktop client:\n\n\n\nThe image shows how Claude can use the fetch tools to retrieve web content and process it according to your instructions.\n\n### Usage\n\nAfter configuration, restart Claude desktop, and you can use the following tools in your conversation:\n\n- `fetch_html`: Get HTML content of a webpage\n- `fetch_json`: Get JSON data\n- `fetch_txt`: Get plain text content\n- `fetch_markdown`: Get Markdown formatted content\n- `fetch_plaintext`: Get plain text content converted from HTML (strips HTML tags)\n\n## Build\n\n```bash\npnpm run build\n```\n\n## Run Server\n\n```bash\npnpm run server\n# or\nnode dist/index.js\n# if globally installed, you can run directly\n@lmcc-dev/mult-fetch-mcp-server\n# or use npx\nnpx @lmcc-dev/mult-fetch-mcp-server\n```\n\n## Client Demo Tools\n\n> **Note**: The following client.js functionality is provided for demonstration and testing purposes only. When used with Claude or other AI assistants, the MCP server is driven by the AI, which manages the chunking process automatically.\n\n### Command Line Client\n\nThe project includes a command-line client for testing and development purposes:\n\n```bash\npnpm run client <method> <params_json>\n# example\npnpm run client fetch_html '{\"url\": \"https://example.com\", \"debug\": true}'\n```\n\n### Demo Client Chunk Control Parameters\n\nWhen testing with the command-line client, you can use these parameters to demonstrate content chunking capabilities:\n\n- `--all-chunks`: Command line flag to automatically fetch all chunks in sequence (demonstration purpose only)\n- `--max-chunks`: Command line flag to limit the maximum number of chunks to fetch (optional, default is 10)\n\n#### Real-time Output Demo\n\nThe client.js demo tool provides real-time output capabilities:\n\n```bash\nnode dist/src/client.js fetch_html '{\"url\":\"https://example.com\", \"startCursor\": 0, \"contentSizeLimit\": 500}' --all-chunks --debug\n```\n\nThe demo client will automatically fetch all chunks in sequence and display them immediately, showcasing how large content can be processed in real-time.\n\n## Run Tests\n\n```bash\n# Run MCP functionality tests\nnpm run test:mcp\n\n# Run mini4k.com website tests\nnpm run test:mini4k\n\n# Run direct client call tests\nnpm run test:direct\n```\n\n## Language Settings\n\nThis project supports Chinese and English bilingual internationalization. You can set the language using environment variables:\n\n### Using Environment Variables\n\nSet the `MCP_LANG` environment variable to control the language:\n\n```bash\n# Set to English\nexport MCP_LANG=en\nnpm run server\n\n# Set to Chinese\nexport MCP_LANG=zh\nnpm run server\n\n# Windows system\nset MCP_LANG=zh\nnpm run server\n```\n\nUsing environment variables ensures that all related processes (including the MCP server) use the same language settings.\n\n### Default Language\n\nBy default, the system will choose a language according to the following priority:\n1. `MCP_LANG` environment variable\n2. Operating system language (if it starts with \"zh\", use Chinese)\n3. English (as the final fallback option)\n\n## Debugging\n\nThis project follows the MCP protocol specification and does not output any logs by default to avoid interfering with JSON-RPC communication. Debug information is controlled through call parameters:\n\n### Using the debug Parameter\n\nSet the `debug: true` parameter when calling a tool:\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"debug\": true\n}\n```\n\nDebug messages are sent to the standard error stream (stderr) using the following format:\n\n```\n[MCP-SERVER] MCP server starting...\n[CLIENT] Fetching URL: https://example.com\n```\n\n### Debug Log File\n\nWhen debug mode is enabled, all debug messages are also written to a log file located at:\n\n```\n~/.mult-fetch-mcp-server/debug.log\n```\n\nThis log file can be accessed through the MCP resources API:\n\n```typescript\n// Access the debug log file\nconst result = await client.readResource({ uri: \"file:///logs/debug\" });\nconsole.log(result.contents[0].text);\n\n// Clear the debug log file\nconst clearResult = await client.readResource({ uri: \"file:///logs/clear\" });\nconsole.log(clearResult.contents[0].text);\n```\n\n## Proxy Settings\n\nThis tool supports various methods to configure proxy settings:\n\n### 1. Using the `proxy` Parameter\n\nThe most direct way is to specify the proxy in the request parameters:\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"proxy\": \"http://your-proxy-server:port\",\n  \"debug\": true\n}\n```\n\n### 2. Using Environment Variables\n\nThe tool will automatically detect and use proxy settings from standard environment variables:\n\n```bash\n# Set proxy environment variables\nexport HTTP_PROXY=http://your-proxy-server:port\nexport HTTPS_PROXY=http://your-proxy-server:port\n\n# Run the server\nnpm run server\n```\n\n### 3. System Proxy Detection\n\nThe tool attempts to detect system proxy settings based on your operating system:\n\n- **Windows**: Reads proxy settings from environment variables using the `set` command\n- **macOS/Linux**: Reads proxy settings from environment variables using the `env` command\n\n### 4. Proxy Troubleshooting\n\nIf you're having issues with proxy detection:\n\n1. Use the `debug: true` parameter to see detailed logs about proxy detection\n2. Explicitly specify the proxy using the `proxy` parameter\n3. Ensure your proxy URL is in the correct format: `http://host:port` or `https://host:port`\n4. For websites that require browser capabilities, set `useBrowser: true` to use browser mode\n\n### 5. Browser Mode and Proxies\n\nWhen using browser mode (`useBrowser: true`), the tool will:\n\n1. First try to use the explicitly specified proxy (if provided)\n2. Then try to use system proxy settings\n3. Finally, proceed without a proxy if none is found\n\nBrowser mode is particularly useful for websites that implement anti-scraping measures or require JavaScript execution.\n\n## Parameter Handling\n\nThis project handles parameters in the following ways:\n\n- **debug**: Passed through call parameters, each request can individually control whether to enable debug output\n- **MCP_LANG**: Retrieved from environment variables, controls the language settings of the entire server\n\n## Usage\n\n### Creating a Client\n\n```typescript\nimport { Client } from '@modelcontextprotocol/sdk/client/index.js';\nimport { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\nimport path from 'path';\nimport { fileURLToPath } from 'url';\n\n// Get the directory path of the current file\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = path.dirname(__filename);\n\n// Create client transport layer\nconst transport = new StdioClientTransport({\n  command: 'node',\n  args: [path.resolve(__dirname, 'dist/index.js')],\n  stderr: 'inherit',\n  env: {\n    ...process.env  // Pass all environment variables, including MCP_LANG\n  }\n});\n\n// Create client\nconst client = new Client({\n  name: \"example-client\",\n  version: \"1.0.0\"\n});\n\n// Connect to transport layer\nawait client.connect(transport);\n\n// Use client\nconst result = await client.callTool({\n  name: 'fetch_html',\n  arguments: {\n    url: 'https://example.com',\n    debug: true  // Control debug output through parameters\n  }\n});\n\nif (result.isError) {\n  console.error('Fetch failed:', result.content[0].text);\n} else {\n  console.log('Fetch successful!');\n  console.log('Content preview:', result.content[0].text.substring(0, 500));\n}\n```\n\n### Supported Tools\n\n- `fetch_html`: Get HTML content of a webpage\n- `fetch_json`: Get JSON data\n- `fetch_txt`: Get plain text content\n- `fetch_markdown`: Get Markdown formatted content\n- `fetch_plaintext`: Get plain text content converted from HTML (strips HTML tags)\n\n### Resources Support\n\nThe server includes support for the resources/list and resources/read methods, but currently no resources are defined in the implementation. The resource system is designed to provide access to project files and documentation, but this feature is not fully implemented yet.\n\n#### Resource Usage Example\n\n```typescript\n// Example: List available resources\nconst resourcesResult = await client.listResources({});\nconsole.log('Available resources:', resourcesResult);\n\n// Note: Currently this will return empty lists for resources and resourceTemplates\n```\n\n### Supported Prompt Templates\n\nThe server provides the following prompt templates:\n\n- `fetch-website`: Get website content, supporting different formats and browser mode\n- `extract-content`: Extract specific content from a website, supporting CSS selectors and data type specification\n- `debug-fetch`: Debug website fetching issues, analyze possible causes and provide solutions\n\n#### Prompt Template Usage\n\n1. Use `prompts/list` to get a list of available prompt templates\n2. Use `prompts/get` to get specific prompt template content\n\n```typescript\n// Example: List available prompt templates\nconst promptsResult = await client.listPrompts({});\nconsole.log('Available prompts:', promptsResult);\n\n// Example: Get website content prompt\nconst fetchPrompt = await client.getPrompt({\n  name: \"fetch-website\",\n  arguments: {\n    url: \"https://example.com\",\n    format: \"html\",\n    useBrowser: \"false\"\n  }\n});\nconsole.log('Fetch website prompt:', fetchPrompt);\n\n// Example: Debug website fetching issues\nconst debugPrompt = await client.getPrompt({\n  name: \"debug-fetch\",\n  arguments: {\n    url: \"https://example.com\",\n    error: \"Connection timeout\"\n  }\n});\nconsole.log('Debug fetch prompt:', debugPrompt);\n```\n\n### Parameter Options\n\nEach tool supports the following parameters:\n\n#### Basic Parameters\n- `url`: URL to fetch (required)\n- `headers`: Custom request headers (optional, default {})\n- `proxy`: Proxy server URL in the format http://host:port or https://host:port (optional)\n\n#### Network Control Parameters\n- `timeout`: Timeout in milliseconds (optional, default is 30000)\n- `maxRedirects`: Maximum number of redirects to follow (optional, default is 10)\n- `noDelay`: Whether to disable random delay between requests (optional, default is false)\n- `useSystemProxy`: Whether to use system proxy (optional, default is true)\n\n#### Content Size Control Parameters\n- `enableContentSplitting`: Whether to split large content into chunks (optional, default is true)\n- `contentSizeLimit`: Maximum content size in bytes before splitting (optional, default is 50000)\n- `startCursor`: Starting cursor position in bytes for retrieving content from a specific position (optional, default is 0)\n\nThese parameters help manage large content that would exceed AI model context size limits, allowing you to retrieve web content in manageable chunks while maintaining the ability to process the complete information.\n\n#### Chunk Management\n- `chunkId`: Unique identifier for a chunk set when content is split (used for requesting subsequent chunks)\n\nWhen content is split into chunks, the response includes metadata that allows the AI to request subsequent chunks using the `chunkId` and `startCursor` parameters. The system uses byte-level chunk management to provide precise control over content retrieval, enabling seamless processing of content from any position.\n\n#### Mode Control Parameters\n- `useBrowser`: Whether to use browser mode (optional, default is false)\n- `useNodeFetch`: Whether to force using Node.js mode (optional, default is false, mutually exclusive with `useBrowser`)\n- `autoDetectMode`: Whether to automatically detect and switch to browser mode if standard mode fails with 403/Forbidden errors (optional, default is true). Set to false to strictly use the specified mode without automatic switching.\n\n#### Browser Mode Specific Parameters\n- `waitForSelector`: Selector to wait for in browser mode (optional, default is 'body')\n- `waitForTimeout`: Timeout to wait in browser mode in milliseconds (optional, default is 5000)\n- `scrollToBottom`: Whether to scroll to the bottom of the page in browser mode (optional, default is false)\n- `saveCookies`: Whether to save cookies in browser mode (optional, default is true)\n- `closeBrowser`: Whether to close the browser instance (optional, default is false)\n\n#### Content Extraction Parameters\n- `extractContent`: Whether to use the Readability algorithm to extract main content (optional, default false)\n- `includeMetadata`: Whether to include metadata in the extracted content (optional, default false, only works when `extractContent` is true)\n- `fallbackToOriginal`: Whether to fall back to the original content when extraction fails (optional, default true, only works when `extractContent` is true)\n\n#### Debug Parameters\n- `debug`: Whether to enable debug output (optional, default false)\n\n### Content Extraction Feature\n\nUse the content extraction feature to get the core content of a webpage, filtering out navigation bars, advertisements, sidebars, and other distracting elements:\n\n```json\n{\n  \"url\": \"https://example.com/article\",\n  \"extractContent\": true,\n  \"includeMetadata\": true\n}\n```\n\nThe extracted content will include the following metadata (if available):\n- Title\n- Byline (author)\n- Site name\n- Excerpt\n- Content length\n- Readability flag (isReaderable)\n\n### Special Usage\n\n#### Content Extraction Examples\n\nTo extract only the meaningful content from an article webpage:\n\n```json\n{\n  \"url\": \"https://example.com/news/article\",\n  \"extractContent\": true,\n  \"includeMetadata\": true\n}\n```\n\nFor websites where content extraction might fail, you can use `fallbackToOriginal` to ensure you get some content:\n\n```json\n{\n  \"url\": \"https://example.com/complex-layout\",\n  \"extractContent\": true,\n  \"fallbackToOriginal\": true\n}\n```\n\n#### Closing Browser Without Fetching\nTo close the browser instance without performing any fetch operation:\n```json\n{\n  \"url\": \"about:blank\",\n  \"closeBrowser\": true\n}\n```\n\n#### Proxy Priority\nThe proxy is determined in the following order:\n1. Command line specified proxy\n2. `proxy` parameter in the request\n3. Environment variables (if `useSystemProxy` is true)\n4. Git configuration (if `useSystemProxy` is true)\n\nIf `proxy` is set, `useSystemProxy` will be automatically set to false.\n\n### Debug Output\n\nWhen `debug: true` is set, logs will be output to stderr with the following prefixes:\n- `[MCP-SERVER]`: Logs from the MCP server\n- `[NODE-FETCH]`: Logs from the Node.js fetcher\n- `[BROWSER-FETCH]`: Logs from the browser fetcher\n- `[CLIENT]`: Logs from the client\n- `[TOOLS]`: Logs from the tool implementation\n- `[FETCHER]`: Logs from the main fetcher interface\n- `[CONTENT]`: Logs related to content handling\n- `[CONTENT-PROCESSOR]`: Logs from the HTML content processor\n- `[CONTENT-SIZE]`: Logs related to content size management\n- `[CHUNK-MANAGER]`: Logs related to content chunking operations\n- `[ERROR-HANDLER]`: Logs related to error handling\n- `[BROWSER-MANAGER]`: Logs from the browser instance manager\n- `[CONTENT-EXTRACTOR]`: Logs from the content extractor\n\n\n## License\n\nMIT\n\n---\n\nUpdated by lmcc-dev",
      "npm_url": "https://www.npmjs.com/package/mult-fetch-mcp-server",
      "npm_downloads": 347,
      "keywords": [
        "scraping",
        "fetch",
        "web",
        "scraping techniques",
        "search lmcc",
        "intelligent scraping"
      ],
      "category": "web-search"
    },
    "longyi1207--glean-mcp-server": {
      "owner": "longyi1207",
      "name": "glean-mcp-server",
      "url": "https://github.com/longyi1207/glean-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/longyi1207.webp",
      "description": "Integrates the Glean API to provide functionalities for searching and interacting with a chatbot. Offers capabilities for retrieving search results based on queries and conducting question-and-answer sessions with a chatbot interface.",
      "stars": 8,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-08T00:46:09Z",
      "readme_content": "# Glean\n\nAn MCP server implementation that integrates the Glean API, providing the Search and Chat functions.\n\n## Tools\n- **Search**: List of search results given a query\n- **Chat**: Q&A with Chatbot\n\n### Usage with Claude Desktop\nBuild the docker image:\n```bash\ndocker build -t glean-server:latest -f src/glean/Dockerfile .\n```\n\nThen add this to your `claude_desktop_config.json`:\n```\n{\n  \"mcpServers\": {\n    \"glean-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GLEAN_API_KEY\",\n        \"-e\",\n        \"GLEAN_DOMAIN\",\n        \"glean-server\"\n      ],\n      \"env\": {\n        \"GLEAN_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"GLEAN_DOMAIN\": \"YOUR_DOMAIN_HERE\"\n      }\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/glean-mcp-server",
      "npm_downloads": 5,
      "keywords": [
        "chatbot",
        "searching",
        "search",
        "chatbot interface",
        "interacting chatbot",
        "glean api"
      ],
      "category": "web-search"
    },
    "luebken--playlist-mcp": {
      "owner": "luebken",
      "name": "playlist-mcp",
      "url": "https://github.com/luebken/playlist-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/luebken.webp",
      "description": "Provides access to transcripts of YouTube playlists, enabling retrieval and querying of video transcripts for enhanced application functionalities. It can preload specific playlists or be customized by specifying a different playlist URL.",
      "stars": 1,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-04-16T10:31:43Z",
      "readme_content": "# Playlist-MCP\n\nThis is an experimental MCP server, which makes the transcripts of a Youtube Playlist available.\n\nIt currently is preloaded with the KubeCon London 2025 transcripts. But you can change this by changing the URL.\n\n\n\n## Install\n\n```sh\n# Clone this repo\ngit clone git@github.com:luebken/playlist-mcp.git; cd playlist-mcp\n\n# Install python dependencies\nuv venv\nsource .venv/bin/activate\nuv pip install -e .\n\n# Fill the transcript cache and vector db.\nuv run server.py https://www.youtube.com/playlist?list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc\n```\n\n## Setup for Claude Desktop\n\n```sh\n# Configure for Claude > Settings > Developer > Edit Config\n# /Users/YOUR_USERNAME/Library/Application Support/Claude/claude_desktop_config.json\n{\n  \"mcpServers\": {\n      \"playlist-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/PATH/TO/PARENT/playlist-mcp/\",\n              \"run\",\n              \"server.py\",\n              \"https://www.youtube.com/playlist?list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc\"\n          ]\n      }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "playlists",
        "playlist",
        "youtube",
        "luebken playlist",
        "youtube playlists",
        "transcripts youtube"
      ],
      "category": "web-search"
    },
    "maccam912--searxng-mcp-server": {
      "owner": "maccam912",
      "name": "searxng-mcp-server",
      "url": "https://github.com/maccam912/searxng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/maccam912.webp",
      "description": "Interact with SearXNG instances to perform search operations across multiple engines and sources.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-19T19:19:25Z",
      "readme_content": "# SearXNG MCP Server\n\nA [Model Control Protocol (MCP)](https://github.com/microsoft/modelcontrol) server for [SearXNG](https://github.com/searxng/searxng), allowing AI assistants to search the web through a SearXNG instance.\n\n## Features\n\n- Search the web using SearXNG via simple API calls\n- Get information about available search engines\n- Configure search parameters including categories, languages, and safe search settings\n\n## Usage in MCP Configuration\n\n```json\n \"searxng-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"https://raw.githubusercontent.com/maccam912/searxng-mcp-server/refs/heads/main/server.py\",\n        \"--url\",\n        \"https://searxng.example.com\"\n      ]\n    }\n```\n\n## Docker Usage\n\nYou can run this MCP server using Docker:\n\n```bash\n# Build the Docker image\ndocker build -t searxng-mcp-server .\n\n# Run the container\n# Replace https://searxng.example.com with your actual SearXNG instance URL\ndocker run -p 8080:8080 searxng-mcp-server\n```\n\n## API Tools\n\n### Search\n\n```python\ndef search(query: str, categories: Optional[str] = None, engines: Optional[str] = None, \n           language: Optional[str] = None, page: int = 1, time_range: Optional[str] = None,\n           safe_search: int = 1)\n```\n\n### Get Available Engines\n\n```python\ndef get_available_engines()\n```\n\n## Local Development\n\nRequirements:\n- Python 3.11+\n- uv (Python package manager)\n\n```bash\n# Install dependencies\nuv sync\n\n# Run the server\nuv run server.py --url https://searxng.example.com\n```\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "maccam912",
        "search",
        "searxng instances",
        "searxng mcp",
        "maccam912 searxng"
      ],
      "category": "web-search"
    },
    "madarco--ragrabbit": {
      "owner": "madarco",
      "name": "ragrabbit",
      "url": "https://github.com/madarco/ragrabbit",
      "imageUrl": "/freedevtools/mcp/pfp/madarco.webp",
      "description": "Crawl websites to create AI-powered search capabilities that enhance content discoverability and interaction with AI language models.",
      "stars": 124,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T07:17:57Z",
      "readme_content": "<h1 style=\"font-weight:normal\">\n  <a href=\"https://ragrabbit.com\">\n    \n  </a>\n  &nbsp;RagRabbit&nbsp;\n  <a href=\"https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas\"><img alt=\"deploy_on_vercel_black\" src=\"https://img.shields.io/badge/deploy%20on-vercel-black.svg\"></a>\n  <a href=\"https://github.com/madarco/ragrabbit/blob/master/license.md\"><img src=https://img.shields.io/github/license/madarco/ragrabbit.svg?colorB=ff0000></a>\n  <a href=\"https://www.npmjs.com/package/@ragrabbit/mcp\"><img alt=\"ragrabbit_mcp_label_npm\" src=\"https://img.shields.io/npm/d18m/%40ragrabbit%2Fmcp?label=npm\" /></a>\n  <img alt=\"ragrabbit\" src=\"https://img.shields.io/github/stars/madarco/ragrabbit\" />\n</h1>\n\nSelf Hosted Site AI Search, LLMs.txt, MCP Server that crawls your content. 1-Click Deploy on Vercel.\n<br>\n\n<p align=\"center\">\n  \n\n</p>\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas)\n\n## How it works\n\n[RagRabbit](https://github.com/madarco/ragrabbit) is a [Next.js](https://nextjs.org/) [Turborepo](https://turbo.build/repo) app that uses [Llamaindex](https://github.com/run-llama/LlamaIndexTS) with [pgVector](https://github.com/pgvector/pgvector).\n\nFeatures\n\n- 💬 Chat Widget: Embeddable AI Chat agent and instant Search\n- 🕸️ Website Crawler: scrapes and index pages with pgVector and PostgreSQL\n- 📄 LLMs.txt Generation: fully customizable wiht ToC reorder\n- 🔌 [MCP Server](./packages//mcp-server/README.md): `npx @ragrabbit/mcp` to access your docs from Claude Desktop and Cursor IDE\n- 🛠️ Flexible: Authentication, Open Source, API Keys access\n- 🚀 Easy Deployment: One-click setup on Vercel\n\nIntegrations:\n\n- [Fumadocs](#fumadocs)\n\n### Demo\n\nView [RagRabbit Demo Page](https://ragrabbit.vercel.app/widget/demo)\n\n\n\n## Install\n\nTo install on Vercel:\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas)\n\nRequirements:\n\n- Node.js 20.x\n- PostgreSQL w/ pgVector\n- OpenAI API Key\n- (Optional) Trigger.dev API Key\n\n### Configuration\n\nSet the following environment variables:\n\n- OPENAI_API_KEY\n\nFor username/password login:\n\n- ADMIN_USER\n- ADMIN_PASSWORD\n\nFor email login:\n\n- RESEND_AUTH=true\n- To restrict access to those emails: RESEND_ALLOWED_EMAILS=\"test@test.com,foo@bar.com\"\n- To not send emails but logs the login link instead (in Vercel logs): SIMULATE_EMAILS=true\n\nSee [.env.example](./apps/saas/.env.example) for the complete list.\n\n## How to use\n\nUse the Indexing section to add a new url/website to index, either a single url or a website to crawl recursively:\n\n\n\n\nThen start the Job Runner (keep the tab open until it finish)\n\n\n\nIn the LLM.txt section you can preview the generated LLM.txt file:\n\n\n\nYou can then embed the widget in your site with the following snippet:\n\n### Chat Button\n\nEmbed a button at the bottom of your page:\n\n```\n<script src=\"https://<your deployed app>/widget.js\"></script>\n```\n\n\n\n### Chat Widget\n\nInsert a search input anwhere in your page:\n\n\n\n```\n<script src=\"https://ragrabbit.com/widget.js?type=search\"></script>\n<ragrabbit-search></ragrabbit-search>\n```\n\n### To use with React.js\n\n```typescript\n\"use client\";\n\nimport Script from \"next/script\";\n\nexport function RagRabbitSearch() {\n  return (\n    <>\n      <Script src=\"/widget.js?type=search\" strategy=\"lazyOnload\" />\n      <style>{`\n        ragrabbit-search .ragrabbit-search-input {\n            padding: 6px 12px;\n        }\n      `}</style>\n      <div className=\"ml-auto min-w-[300px] flex-1 sm:flex-initial\">\n        {/* @ts-ignore - Custom element will be mounted by external script */}\n        <ragrabbit-search></ragrabbit-search>\n      </div>\n    </>\n  );\n}\n```\n\n### MPC Server\n\nThe MCP Server allows any supported AI Clients to retrieve pages from your documentation using semantic search.\n\n### Claude Desktop\n\nAdd a custom mcp server with the name of your product, so that Claude AI can use it when looking for info about it.\n\nin `claude_desktop_config.json` (Claude -> Settings -> Developer -> Edit Config)\n\n```\n{\n  \"mcpServers\": {\n    \"<name_of_your_documentation_no_spaces>\": {\n      \"command\": \"npx\",\n      \"args\": [\"@ragrabbit/mcp\", \"http://<RagRabbit install>/\", \"<name of your documentation>\"]\n    }\n  }\n}\n```\n\n### In Cursor IDE\n\nGo to Cursor -> Settings -> Cursor Settings -> MCP\n\nAnd add a new MCP of type `command` with the command:\n\n```\nnpx @ragrabbit/mcp\", \"http://<RagRabbit install>/\", \"<name of your documentation>\"\n```\n\nArguments:\n\n- `ragrabbit-url`: (Required) The base URL of your RagRabbit instance, eg https://my-ragrabbit.vercel.com/\n- `name`: (Required) Custom name for the documentation search service (defaults to \"RagRabbit\") so that AI will know to use it when looking for info\n\n## Configuration Options\n\n### Chat button\n\nYou can configure the chat button by adding the following parameters to the widget.js script tag:\n\n#### buttonText\n\n```\n<script src=\"https://ragrabbit.com/widget.js?buttonText=Ask%20AI\"></script>\n```\n\n### Search widget\n\nYou can configure the search widget by adding the following parameters and use the mountSearch call:\n\n#### searchPlaceholder\n\n```\n<div id=\"search-container\"></div>\n<script>\n  window.mountSearch(\"search-container\", { searchPlaceholder: \"Search documentation...\" });\n</script>\n```\n\n## Integrations\n\n### Fumadocs\n\nCreate a component to replace the Search Dialog:\n\n```bash\npnpm add @ragrabbit/search-react\n```\n\n```typescript\n\"use client\";\nimport type { SharedProps } from \"fumadocs-ui/components/dialog/search\";\nimport { RagRabbitModal } from \"@ragrabbit/search-react\";\n\nexport default function SearchDialog({ open, onOpenChange }: SharedProps) {\n  return <RagRabbitModal\n    domain=\"http://localhost:3000/\"\n    open={open}\n    onOpenChange={onOpenChange}\n    />;\n}\n```\n\nThen set it in the `layout.tsx`:\n\n```tsx\n<RootProvider\n  search={{\n    SearchDialog,\n  }}\n>\n  ...\n</RootProvider>\n```\n\nOptionally add the Floating Chat button:\n\n```typescript\n\"use client\";\nimport { RagRabbitChatButton } from \"@ragrabbit/search-react\";\n\nexport default function ChatButton() {\n  return <RagRabbitChatButton domain=\"http://localhost:3000/\" />;\n}\n```\n\nAnd add it to the `layout.tsx`:\n\n```tsx\n<body className=\"flex flex-col min-h-screen\">\n  <ChatButton />\n  ...\n```\n\n## Development\n\n```bash\n# Start the db (Docker needed)\npnpm dev:utils # Starts postgresql with pgvector, Storybook and Drizzle ORM Studio\n\n# Start the app\ncd apps/saas\npnpm dev\n```\n\n### Directory structure:\n\nRagRabbit is a monorepo with Turborepo a Next.js app and a modular design with separate packages.\n\n```\napps/\n├── docs -> the documentation site\n├── saas -> the main application\n└── web -> the web site\npackages/\n├── db -> the database with Drizzle ORM\n├── auth -> the authentication with Auth.js\n├── core -> shared utils\n├── design -> the design system\n├── rag -> the LLM and RAG package with LlamaIndexTS\n├── jobs -> job runner with Trigger.dev\n└── storybook -> a Next.js Storybook app\n.cursorrules -> Fine tuned Cursor rules with all the locations to work with the monorepo\n```\n\n# Author\n\n[Marco D'Alia](https://www.madarco.net) - [@madarco](https://x.com/madarco) - [Linkedin](https://www.linkedin.com/in/marcodalia/)\n\n# License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "ai",
        "crawl",
        "search capabilities",
        "powered search",
        "web search"
      ],
      "category": "web-search"
    },
    "manimohans--farcaster-mcp": {
      "owner": "manimohans",
      "name": "farcaster-mcp",
      "url": "https://github.com/manimohans/farcaster-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/manimohans.webp",
      "description": "Interact with the Farcaster Network to fetch casts, search channels, and analyze content related to users and channels. Retrieve user-specific casts through FID or username, as well as casts from specific channels.",
      "stars": 2,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-09T22:50:39Z",
      "readme_content": "# Farcaster MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@manimohans/farcaster-mcp)](https://smithery.ai/server/@manimohans/farcaster-mcp)\n\nAn MCP server that provides tools to interact with the Farcaster network ([farcaster.xyz](https://www.farcaster.xyz)), allowing AI models to fetch casts, search channels, and analyze content.\n\n<a href=\"https://glama.ai/mcp/servers/koo5epnlc7\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/koo5epnlc7/badge\" alt=\"Farcaster Server MCP server\" />\n</a>\n\n## Features\n\n- **Get User Casts**: Retrieve casts from a specific Farcaster user by FID\n- **Get Username Casts**: Retrieve casts from a specific Farcaster user by username\n- **Get Channel Casts**: Retrieve casts from a specific Farcaster channel\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/manimohans/farcaster-mcp.git\ncd farcaster-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### Running the server\n\n```bash\nnpm start\n```\n\n### Using with MCP Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\n### Using with Claude for Desktop\n\n1. Install [Claude for Desktop](https://claude.ai/download)\n2. Open your Claude for Desktop App configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"farcaster\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/farcaster-mcp/build/index.js\"]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n\n### Using with Smithery\n\nThis project includes Smithery configuration files for easy deployment:\n\n```bash\n# Install Smithery CLI\nnpm install -g @smithery/cli\n\n# Deploy to Smithery (specify the client, e.g., claude, cline, windsurf, etc.)\nnpx @smithery/cli install @manimohans/farcaster-mcp --client claude\n```\n\nAvailable client options: claude, cline, windsurf, roo-cline, witsy, enconvo\n\n### Available Tools\n\n#### get-user-casts\n\nRetrieves casts from a specific Farcaster user by their FID (Farcaster ID).\n\nParameters:\n- `fid`: Farcaster user ID (number)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from FID 6846.\"\n\n#### get-username-casts\n\nRetrieves casts from a specific Farcaster user by their username.\n\nParameters:\n- `username`: Farcaster username (string)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from username 'mani'.\"\n\n#### get-channel-casts\n\nRetrieves casts from a specific Farcaster channel.\n\nParameters:\n- `channel`: Channel name or URL (string)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from the 'aichannel' channel.\"\n\n## Smithery Configuration\n\nThis repository includes the necessary configuration files for Smithery:\n\n- `smithery.yaml`: YAML configuration for Smithery deployment\n- `smithery.json`: JSON configuration for Smithery capabilities\n- `Dockerfile`: Container configuration for Smithery deployment\n\n## API Details\n\nThis implementation uses the Farcaster Hubble API to fetch data.\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "farcaster",
        "search",
        "web",
        "farcaster mcp",
        "farcaster network",
        "search channels"
      ],
      "category": "web-search"
    },
    "marcopesani--mcp-server-serper": {
      "owner": "marcopesani",
      "name": "mcp-server-serper",
      "url": "https://github.com/marcopesani/mcp-server-serper",
      "imageUrl": "/freedevtools/mcp/pfp/marcopesani.webp",
      "description": "Provides web search capabilities and webpage scraping functions utilizing the Serper API, enabling content extraction and rich search result retrieval.",
      "stars": 117,
      "forks": 19,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T06:28:07Z",
      "readme_content": "# Serper Search and Scrape MCP Server\n[![smithery badge](https://smithery.ai/badge/@marcopesani/mcp-server-serper)](https://smithery.ai/server/@marcopesani/mcp-server-serper)\n\nA TypeScript-based MCP server that provides web search and webpage scraping capabilities using the Serper API. This server integrates with Claude Desktop to enable powerful web search and content extraction features.\n\n<a href=\"https://glama.ai/mcp/servers/5zk327i0pj\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5zk327i0pj/badge\" alt=\"serper-search-scrape-mcp-server MCP server\" />\n</a>\n\n## Features\n\n### Tools\n\n- `google_search` - Perform web searches via Serper API\n  - Rich search results including organic results, knowledge graph, \"people also ask\", and related searches\n  - Supports region and language targeting\n  - Optional parameters for location, pagination, time filters, and autocorrection\n  - Supports advanced search operators:\n    - `site`: Limit results to specific domain\n    - `filetype`: Limit to specific file types (e.g., 'pdf', 'doc')\n    - `inurl`: Search for pages with word in URL\n    - `intitle`: Search for pages with word in title\n    - `related`: Find similar websites\n    - `cache`: View Google's cached version of a specific URL\n    - `before`: Date before in YYYY-MM-DD format\n    - `after`: Date after in YYYY-MM-DD format\n    - `exact`: Exact phrase match\n    - `exclude`: Terms to exclude from search results\n    - `or`: Alternative terms (OR operator)\n  \n- `scrape` - Extract content from web pages\n  - Get plain text and optional markdown content\n  - Includes JSON-LD and head metadata\n  - Preserves document structure\n\n## Requirements\n\n- Node.js >= 18\n- Serper API key (set as `SERPER_API_KEY` environment variable)\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\nRun tests:\n```bash\nnpm test                  # Run all tests\nnpm run test:watch       # Run tests in watch mode\nnpm run test:coverage    # Run tests with coverage\nnpm run test:integration # Run integration tests\n```\n\n### Environment Variables\n\nCreate a `.env` file in the root directory:\n\n```\nSERPER_API_KEY=your_api_key_here\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Serper Search and Scrape for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@marcopesani/mcp-server-serper):\n\n```bash\nnpx -y @smithery/cli install @marcopesani/mcp-server-serper --client claude\n```\n\n### Claude Desktop\n\nAdd the server config at:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Cline\n\n1. Open the Cline extension settings\n2. Open \"MCP Servers\" tab\n3. Click on \"Configure MCP Servers\"\n4. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/marcopesani/mcp-server-serper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\"google_search\", \"scrape\"]\n    }\n  }\n}\n```\n\nAdditional Cline configuration options:\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of tools that don't require explicit approval for each use\n\n### Cursor\n\n1. Open the Cursor settings\n2. Open \"Features\" settings\n3. In the \"MCP Servers\" section, click on \"Add new MCP Server\"\n4. Choose a name, and select \"command\" as \"Type\"\n5. In the \"Command\" field, enter the following:\n\n```\nenv SERPER_API_KEY=your_api_key_here npx -y serper-search-scrape-mcp-server\n```\n\n### Docker\n\nYou can also run the server using Docker. First, build the image:\n\n```bash\ndocker build -t mcp-server-serper .\n```\n\nThen run the container with your Serper API key:\n\n```bash\ndocker run -e SERPER_API_KEY=your_api_key_here mcp-server-serper\n```\n\nAlternatively, if you have your environment variables in a `.env` file:\n\n```bash\ndocker run --env-file .env mcp-server-serper\n```\n\nFor development, you might want to mount your source code as a volume:\n\n```bash\ndocker run -v $(pwd):/app --env-file .env mcp-server-serper\n```\n\nNote: Make sure to replace `your_api_key_here` with your actual Serper API key.",
      "npm_url": "https://www.npmjs.com/package/mcp-server-serper",
      "npm_downloads": 487,
      "keywords": [
        "scraping",
        "serper",
        "search",
        "webpage scraping",
        "serper api",
        "serper provides"
      ],
      "category": "web-search"
    },
    "mcollina--mcp-node-fetch": {
      "owner": "mcollina",
      "name": "mcp-node-fetch",
      "url": "https://github.com/mcollina/mcp-node-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/mcollina.webp",
      "description": "Fetch web content from any URL using various HTTP methods, handle headers and request bodies, and support multiple response formats. Configurable timeout and redirect settings streamline reliable content retrieval.",
      "stars": 11,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T12:44:21Z",
      "readme_content": "# MCP Node Fetch\n\nAn MCP server that enables fetching web content using the Node.js [undici](https://github.com/nodejs/undici) library.\n\n## Features\n\n- Fetch content from any URL using various HTTP methods\n- Support for headers and request body\n- Return content in various formats (text, JSON, binary, HTML fragments)\n- Handle errors gracefully\n- Configure timeout and redirect behavior\n\n## MCP Tools\n\nThis server provides the following MCP tools:\n\n### `fetch-url`\n\nFetches content from a URL and returns it.\n\nParameters:\n- `url` (string, required): The URL to fetch\n- `method` (string, optional): HTTP method (default: \"GET\")\n- `headers` (object, optional): HTTP headers to include\n- `body` (string, optional): Request body for POST/PUT requests\n- `timeout` (number, optional): Request timeout in milliseconds\n- `responseType` (string, optional): How to parse the response (\"text\", \"json\", \"binary\", \"html-fragment\")\n- `fragmentSelector` (string, optional): CSS selector to extract specific HTML fragments (when responseType is \"html-fragment\")\n- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)\n\n### `extract-html-fragment`\n\nExtracts specific HTML content from a webpage using CSS selectors and optionally navigates to anchor points.\n\nParameters:\n- `url` (string, required): The URL to fetch\n- `selector` (string, required): CSS selector for the HTML fragment to extract\n- `anchorId` (string, optional): Optional anchor ID to locate a specific fragment\n- `method` (string, optional): HTTP method (default: \"GET\")\n- `headers` (object, optional): HTTP headers to include\n- `body` (string, optional): Request body for POST requests\n- `timeout` (number, optional): Request timeout in milliseconds\n- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)\n\n### `check-status`\n\nChecks if a URL is accessible without downloading the full content.\n\nParameters:\n- `url` (string, required): The URL to check\n- `timeout` (number, optional): Request timeout in milliseconds\n\n\n## Claude for Desktop Configuration\n\nTo use with Claude for Desktop, add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"node-fetch\": {\n      \"command\": \"node\",\n      \"args\": [\"dist/index.js\"]\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-node-fetch",
      "npm_downloads": 3116,
      "keywords": [
        "mcollina",
        "fetch",
        "http",
        "search mcollina",
        "fetch web",
        "web search"
      ],
      "category": "web-search"
    },
    "mcp2everything--mcp2brave": {
      "owner": "mcp2everything",
      "name": "mcp2brave",
      "url": "https://github.com/mcp2everything/mcp2brave",
      "imageUrl": "/freedevtools/mcp/pfp/mcp2everything.webp",
      "description": "Connects to the Brave API to perform web searches, enabling access to search results within the MCP framework.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-29T18:54:20Z",
      "readme_content": "# mcp2brave\n\n一个基于MCP协议的服务器，使用Brave API实现网络搜索功能。\n\n## 系统要求\n\n- Python 3.11+\n- UV包管理器\n- Brave API密钥\n\n## 安装步骤\n\n1. 克隆项目\n```bash\ngit clone <仓库地址>\ncd mcp2brave\n```\n\n2. 创建并编辑环境变量文件\n```bash\n# 添加Brave API密钥到.env文件\n.env\n```\n\n3. 使用UV创建虚拟环境\n```bash\n# 创建并激活虚拟环境\nuv venv\n# Windows系统使用:\n.venv\\Scripts\\activate\n# Linux/Mac系统使用:\nsource .venv/bin/activate\n```\n\n4. 安装依赖\n```bash\nuv sync\n```\n\n## 使用方法\n\n### 安装为Claude扩展\n```bash\nfastmcp install mcp2brave.py\n```\n\n### 开发模式与MCP检查器\n要使用MCP检查器测试功能：\n\n```bash\nfastmcp dev mcp2brave.py\n```\n\n运行后，可以在浏览器访问MCP检查器：http://localhost:5173\n\n## 可用工具\n\n- `search_web(query: str)`: 使用Brave API搜索网络\n- `search_web_info(query: str)`: 同上，带中文描述\n\n## 环境变量\n\n- `Brave_API_KEY`: 你的Brave API密钥（必需）\n\n## 注意事项\n\n- 确保在使用前已正确设置API密钥\n- 虚拟环境激活后才能运行相关命令\n- 如遇到编码问题，请确保系统使用UTF-8编码\n\n## 手动添加Cline Continue Claude\n打开Cline Continue Claude的MCP服务器配置文件，加入以下信息\n```json\n\n\"mcp2brave\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"python-dotenv\",\n        \"--with\",\n        \"beautifulsoup4\",\n        \"--with\",\n        \"requests\",\n        \"fastmcp\",\n        \"run\",\n        \"C:\\\\Users\\\\你的真实路径\\\\mcp2brave.py\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"API密钥\"\n      }\n    }\n```\n\n\n## 详细操作步骤\n### Cline集成\n\n\n### Cline集成\n\n### Cline集成\n\n\n### 使用示例",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp2brave",
        "mcp2everything",
        "searches",
        "search mcp2everything",
        "mcp2brave connects",
        "mcp2everything mcp2brave"
      ],
      "category": "web-search"
    },
    "mcp2everything--mcp2tavily": {
      "owner": "mcp2everything",
      "name": "mcp2tavily",
      "url": "https://github.com/mcp2everything/mcp2tavily",
      "imageUrl": "/freedevtools/mcp/pfp/mcp2everything.webp",
      "description": "This server implements web search functionality using the Tavily API, providing real-time search capabilities for applications. It allows users to retrieve information from the web quickly and easily.",
      "stars": 4,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T06:51:45Z",
      "readme_content": "# MCP2Tavily\n\nA MCP protocol server that implements web search functionality using the Tavily API.\n\n## Prerequisites\n\n- Python 3.11+\n- UV package manager\n- Tavily API key\n\n## Installation\n\n1. Clone the repository\n```bash\ngit clone <repository-url>\ncd mcp2tavily\n```\n\n2. Create and edit the `.env` file\n```bash\n# Create .env file\ntouch .env\n# Add your Tavily API key to .env\necho \"TAVILY_API_KEY=your_api_key_here\" > .env\n```\n\n3. Set up virtual environment with UV\n```bash\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate\n```\n\n4. Install dependencies\n```bash\nuv sync\n```\n\n## Usage\n\n### Install as Claude extension\n```bash\nfastmcp install mcp2tavily.py\n```\n\n### Development mode with MCP Inspector\nTo test the functionality using MCP Inspector:\n\n```bash\nfastmcp dev mcp2tavily.py\n```\n\nOnce running, you can access the MCP Inspector at: http://localhost:5173\n\n## Available Tools\n\n- `search_web(query: str)`: Search the web using Tavily API\n- `search_web_info(query: str)`: Same as above, with Chinese description\n\n## Environment Variables\n\n- `TAVILY_API_KEY`: Your Tavily API key (required)\n\n## Step-by-Step Guide\n\n## 手动添加Cline Continue Claude\nCline Continue Claude的MCP JSON FILE\n```json\n\n\"mcp2tavily\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"python-dotenv\",\n        \"--with\",\n        \"tavily-python\",\n        \"fastmcp\",\n        \"run\",\n        \"C:\\\\Users\\\\你的真实路径\\\\mcp2tavily.py\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"API密钥\"\n      }\n    }\n```\n\n\n\n### Cline\n\n\n### Cline\n\n### Cline\n\n\n### EXAMPLE",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp2tavily",
        "mcp2everything",
        "search",
        "search mcp2everything",
        "mcp2tavily server",
        "web search"
      ],
      "category": "web-search"
    },
    "meicanhong--exa-mcp-server": {
      "owner": "meicanhong",
      "name": "exa-mcp-server",
      "url": "https://github.com/meicanhong/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "Enables AI assistants to perform real-time web searches using Exa's search API, providing structured results while managing caching and rate limits. Facilitates access to fresh web content in a secure manner.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-07T11:00:11Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Caches recent searches as resources for reference\n- Handles rate limiting and error cases gracefully\n- Supports real-time web crawling for fresh content\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n```\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Create a global link (this makes the server executable from anywhere):\n\n```bash\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1. Open your Claude Desktop configuration:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1. Open your Claude Desktop configuration:\n\n```powershell\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### 2. Add the Exa server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Available Tools & Tool Selection\n\nThe Exa MCP server includes the following tools:\n\n- **web_search**: Performs real-time web searches with optimized results and content extraction.\n- **research_paper_search**: Specialized search focused on academic papers and research content.\n- **twitter_search**: Dedicated Twitter/X.com search that finds tweets, profiles, and conversations.\n\nYou can choose which tools to enable by adding the `--tools` parameter to your Claude Desktop configuration:\n\n#### Specify which tools to enable:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"/path/to/exa-mcp-server/build/index.js\",\n        \"--tools=twitter_search\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nFor enabling multiple tools, use a comma-separated list:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"/path/to/exa-mcp-server/build/index.js\",\n        \"--tools=web_search,research_paper_search,twitter_search\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nIf you don't specify any tools, all tools enabled by default will be used.\n\n### 4. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1. Completely quit Claude Desktop (not just close the window)\n2. Start Claude Desktop again\n3. Look for the 🔌 icon to verify the Exa server is connected\n\n## Using via NPX\n\nIf you prefer to run the server directly, you can use npx:\n\n```bash\n# Run with all tools enabled by default\nnpx exa-mcp-server\n\n# Enable specific tools only\nnpx exa-mcp-server --tools=web_search\n\n# Enable multiple tools\nnpx exa-mcp-server --tools=web_search,research_paper_search\n\n# List all available tools\nnpx exa-mcp-server --list-tools\n```\n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\n```\nSearch Twitter for posts from @elonmusk about SpaceX.\n```\n\n```\nFind tweets from @samaltman that were published in the last week about AI safety.\n```\n\nThe server will:\n\n1. Process the search request\n2. Query the Exa API with optimal settings (including live crawling)\n3. Return formatted results to Claude\n4. Cache the search for future reference\n\n\n## Testing with MCP Inspector 🔍\n\nYou can test the server directly using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\nThis opens an interactive interface where you can explore the server's capabilities, execute search queries, and view cached search results.\n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify the npm link is correctly set up\n   * Check Claude Desktop configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Confirm your EXA_API_KEY is valid\n   * Check the EXA_API_KEY is correctly set in the Claude Desktop config\n   * Verify no spaces or quotes around the API key\n\n3. **Connection Issues**\n   * Restart Claude Desktop completely\n   * Check Claude Desktop logs:\n   \n   ```bash\n   # macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type \"%APPDATA%\\Claude\\logs\\mcp*.log\"\n   ```\n\n## Acknowledgments 🙏\n\n* [Exa AI](https://exa.ai) for their powerful search API\n* [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n* [Anthropic](https://anthropic.com) for Claude Desktop\n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "search",
        "exa",
        "exa search",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "meicanhong--video-search": {
      "owner": "meicanhong",
      "name": "video-search",
      "url": "https://github.com/meicanhong/video-search",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "Search for YouTube videos and analyze their content by generating summaries, retrieving subtitles, and pinpointing specific video segments. The tool supports keyword-based searches and provides detailed video metadata, intelligent subtitle selection, and content analysis powered by GPT.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-01-01T04:48:12Z",
      "readme_content": "# YouTube 视频搜索与分析工具\n\n基于 FastAPI 和 React 的 YouTube 视频搜索和内容分析工具，支持视频搜索、字幕分析和内容总结。\n\n## 功能特点\n\n- **视频搜索**：\n  - 基于关键词搜索 YouTube 视频\n  - 支持视频元数据获取（标题、时长、观看次数等）\n  - 自动生成搜索结果总结\n\n- **字幕处理**：\n  - 自动获取视频字幕\n  - 支持多语言字幕\n  - 智能选择最佳字幕源\n\n- **内容分析**：\n  - 基于 GPT 的内容理解和总结\n  - 精确定位相关视频片段\n  - 提供带时间戳的直达链接\n\n- **用户界面**：\n  - 现代化的 React 前端界面\n  - 响应式设计\n  - 实时内容分析\n  - 优雅的过渡动画\n\n## 技术栈\n\n### 后端\n- Python 3.8+\n- FastAPI\n- YouTube Data API v3\n- OpenAI GPT API\n- structlog 日志系统\n\n### 前端\n- React 18\n- TypeScript\n- Vite\n- Ant Design\n- TailwindCSS\n- React Query\n\n### 开发工具\n- Docker & Docker Compose\n- Rye 包管理\n- Pre-commit hooks\n- ESLint & Prettier\n\n## 快速开始\n\n1. 克隆项目\n```bash\ngit clone https://github.com/yourusername/video-search.git\ncd video-search\n```\n\n2. 配置环境变量\n```bash\n# 创建 .env 文件\ncp .env.example .env\n\n# 编辑 .env 文件，添加必要的 API 密钥\nYOUTUBE_API_KEY=your_youtube_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n3. 使用 Docker 启动服务\n```bash\n# 构建并启动服务\ndocker-compose up --build\n```\n\n服务将在以下地址启动：\n- 前端：http://localhost:3000\n- 后端：http://localhost:8000\n\n## 项目结构\n\n```\nvideo-search/\n├── src/                      # 后端源代码\n│   └── youtube_search/\n│       ├── client.py         # YouTube API 客户端\n│       ├── models.py         # 数据模型\n│       ├── service.py        # 业务逻辑\n│       ├── web.py           # Web API\n│       ├── subtitle.py      # 字幕处理\n│       ├── session.py       # 会话管理\n│       ├── openai_client.py # OpenAI API 客户端\n│       └── utils.py         # 工具函数\n├── frontend/                 # 前端源代码\n│   ├── src/\n│   │   ├── components/      # React 组件\n│   │   ├── hooks/          # 自定义 Hooks\n│   │   ├── pages/          # 页面组件\n│   │   ├── services/       # API 服务\n│   │   ├── types/          # TypeScript 类型\n│   │   └── utils/          # 工具函数\n│   ├── vite.config.ts      # Vite 配置\n│   └── tailwind.config.js  # Tailwind 配置\n├── docs/                    # 项目文档\n├── docker-compose.yml       # Docker 编排配置\n└── Dockerfile              # 后端 Docker 配置\n```\n\n## API 接口\n\n### 搜索视频\n```http\nPOST /search\nContent-Type: application/json\n\n{\n    \"keyword\": \"搜索关键词\",\n    \"max_results\": 5\n}\n```\n\n### 分析内容\n```http\nPOST /sessions/{session_id}/analyze\nContent-Type: application/json\n\n{\n    \"query\": \"用户问题\"\n}\n```\n\n详细的 API 文档请参考 `docs/api_reference.md`。\n\n## 开发指南\n\n### 后端开发\n```bash\n# 安装依赖\njust install\n\n# 启动开发服务器\njust dev\n\n# 格式化代码\njust format\n```\n\n### 前端开发\n```bash\ncd frontend\n\n# 安装依赖\nnpm install\n\n# 启动开发服务器\nnpm run dev\n\n# 构建生产版本\nnpm run build\n```\n\n## 注意事项\n\n1. API 密钥安全\n   - 请妥善保管 API 密钥\n   - 不要将 .env 文件提交到版本控制\n   - 建议设置 API 密钥使用限制\n\n2. 资源限制\n   - YouTube API 有每日配额限制\n   - OpenAI API 按使用量计费\n   - 建议实现缓存机制\n\n3. 开发建议\n   - 遵循代码规范和提交规范\n   - 编写单元测试\n   - 使用 TypeScript 类型检查\n\n## 贡献指南\n\n1. Fork 项目\n2. 创建功能分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 提交 Pull Request\n\n## 许可证\n\nMIT License",
      "npm_url": "https://www.npmjs.com/package/video-search",
      "npm_downloads": 92,
      "keywords": [
        "searches",
        "search",
        "youtube",
        "video search",
        "meicanhong video",
        "search youtube"
      ],
      "category": "web-search"
    },
    "mgsrevolver--seo-inspector-mcp": {
      "owner": "mgsrevolver",
      "name": "seo-inspector-mcp",
      "url": "https://github.com/mgsrevolver/seo-inspector-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mgsrevolver.webp",
      "description": "Analyzes HTML files and web pages to identify SEO issues and validate structured data schemas. Provides actionable recommendations for improving SEO quality directly through integrated tools without the need for a browser extension.",
      "stars": 4,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-14T17:34:26Z",
      "readme_content": "// README.md - Instructions for setup and use\n\n# SEO Inspector & Schema Validator MCP\n\nA Model Context Protocol (MCP) server for Cursor that analyzes web pages for SEO issues and validates structured data schemas.\n\n## Features\n\n- Analyze HTML files in a codebase for SEO issues\n- Validate JSON-LD structured data\n- Get recommendations to improve SEO\n- No browser extension required - works directly with your codebase\n\n## Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/yourusername/seo-inspector-mcp.git\n   cd seo-inspector-mcp\n   ```\n\n2. Install dependencies:\n\n   ```\n   npm install\n   ```\n\n3. Configure Cursor to use this MCP server:\n   - Go to Settings > Features > MCP in Cursor\n   - Add a new MCP server with:\n     - Name: SEO Inspector\n     - Type: sse\n     - URL: http://localhost:8767/sse\n\n## Usage\n\n1. Start the MCP server:\n\n   ```\n   ./run-mcp.sh\n   ```\n\n2. In Cursor, you can now use the SEO Inspector tools:\n   - `seo.analyze-codebase` - Analyze HTML files in a directory\n   - `seo.analyze-html` - Analyze a specific HTML string\n\n## Prioritized SEO Components\n\nThe tool checks for these key SEO elements (in order of importance):\n\n### Critical\n\n- Page Title\n- Meta Description\n- H1 Heading\n- Canonical URL\n\n### Important\n\n- Heading Structure (H2-H6)\n- Image Alt Text\n- Structured Data (JSON-LD)\n- Robots Directives\n\n### Recommended\n\n- Open Graph Tags\n- Twitter Cards\n- Internal Linking\n- URL Structure\n- Mobile Friendliness\n\n## Schema Validation\n\nThe tool validates the following schema types:\n\n- Organization\n- LocalBusiness\n- Product\n- Article\n- WebPage\n- FAQPage\n- Event\n- Recipe\n- Review\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "seo",
        "mgsrevolver",
        "html",
        "mgsrevolver seo",
        "seo inspector",
        "search mgsrevolver"
      ],
      "category": "web-search"
    },
    "micahman33--VonageAICodeAssist": {
      "owner": "micahman33",
      "name": "VonageAICodeAssist",
      "url": "https://github.com/micahman33/VonageAICodeAssist",
      "imageUrl": "/freedevtools/mcp/pfp/micahman33.webp",
      "description": "Provides AI-assisted access to Vonage API documentation for targeted search and content extraction, facilitating quick access to relevant information about communication APIs. Integrates with external web search APIs to enhance documentation retrieval.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-31T13:18:11Z",
      "readme_content": "# Vonage AI Code Assist MCP Server\n\n## Overview\n\nVonage AI Code Assist is a Model Context Protocol (MCP) server designed to help developers integrate Vonage API capabilities into their applications. The server leverages FastMCP to provide AI-assisted access to Vonage documentation, enabling developers to quickly find relevant information about Vonage's communication APIs.\n\n## How It Works\n\nThe Vonage Assist MCP server operates as follows:\n\n1. **Documentation Search**: The server provides a specialized tool called \"Vonage-Assist\" that searches through Vonage's official documentation.\n\n2. **Web Search Integration**: Using the Google Serper API, the tool performs targeted searches within the Vonage developer documentation domain (`developer.vonage.com/en/documentation`).\n\n3. **Content Extraction**: When a search query is submitted, the server:\n   - Formulates a site-specific search query\n   - Sends the query to Google Serper API\n   - Receives search results with relevant documentation links\n   - Fetches the content from these links\n   - Returns the extracted text content to the user\n\n4. **MCP Tool Integration**: The server is compatible with Claude and other AI assistants that support the MCP protocol, allowing these AI systems to directly utilize Vonage documentation in their responses.\n\n## Setup & Requirements\n\nTo run the Vonage Assist MCP server:\n\n1. Ensure Python 3.13+ is installed.\n\n2. Set up the required environment variables:\n   - `SERPER_API_KEY`: API key for Google Serper (required for web searches)\n\n3. Install dependencies:\n   ```bash\n   uv install\n   ```\n\n4. Run the server:\n   ```bash\n   python main.py\n   ```\n\n## Usage\n\nOnce running, the MCP server exposes the `Vonage-Assist` tool with the following parameters:\n\n- `query`: The search query (e.g., \"number verification\", \"SMS API\")\n- `library`: The documentation library to search (\"vonage\" is currently the only supported option)\n\nExample tool usage (via an MCP-compatible AI):\n```\nUse the Vonage-Assist tool to find information about implementing two-factor authentication with Vonage APIs.\n```\n\n## Technical Implementation\n\nThe server is built using:\n- FastMCP for the MCP server framework\n- httpx for asynchronous HTTP requests\n- BeautifulSoup for HTML parsing and text extraction\n- python-dotenv for environment variable management\n\nThe core functionality is implemented through several key functions:\n- `search_web()`: Performs API requests to Google Serper\n- `fetch_url()`: Retrieves and extracts content from web pages\n- `vonage_docs()`: The main tool function that orchestrates the search and content retrieval process\n\n## Future Considerations\n\nTop potential enhancements for the Vonage Assist MCP server:\n\n1. **Code Generation Tool**: Add capabilities to generate sample code snippets for common Vonage API integrations (SMS, Voice, Verify, Video) in multiple programming languages, helping developers quickly implement Vonage features with proper syntax and best practices.\n\n2. **API Parameter Helper**: Develop a tool that helps developers construct valid API requests by suggesting parameters, validating inputs, and explaining required vs. optional fields for different Vonage API endpoints.\n\n3. **Troubleshooting Assistant**: Implement functionality to diagnose common integration issues by analyzing error codes and providing actionable solutions based on KB articles and documentation - significantly reducing debugging time.\n\n4. **Webhook Configuration Helper**: Create a tool to assist with setting up and testing webhook endpoints for Vonage services, guiding developers through the process of handling callbacks and events.\n\n5. **Best Practices Advisor**: Add a capability to provide context-specific best practices for performance, security, and resilience when implementing Vonage APIs, helping developers build more robust applications.\n\n6. **Rate Limit & Pricing Estimator**: Help developers estimate costs and understand rate limits for their specific use cases.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vonageaicodeassist",
        "vonage",
        "api",
        "vonageaicodeassist provides",
        "micahman33 vonageaicodeassist",
        "vonage api"
      ],
      "category": "web-search"
    },
    "mikechao--artic-mcp": {
      "owner": "mikechao",
      "name": "artic-mcp",
      "url": "https://github.com/mikechao/artic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikechao.webp",
      "description": "Connects to the Art Institute of Chicago's art collection, enabling searches of artworks by title, full text, or artist, and retrieves detailed information including images.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-26T08:24:57Z",
      "readme_content": "[![artic-logo](https://raw.githubusercontent.com/Art-Institute-of-Chicago/template/main/aic-logo.gif)](https://www.artic.edu/)\n<br/>\n[![smithery badge](https://smithery.ai/badge/@mikechao/artic-mcp)](https://smithery.ai/server/@mikechao/artic-mcp)\n<br/>\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mikechao-artic-mcp-badge.png)](https://mseep.ai/app/mikechao-artic-mcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/d10632ad-6dcb-4f97-a3ca-eae970713956)\n<br/>\n<a href=\"https://glama.ai/mcp/servers/@mikechao/artic-mcp\">\n<img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/artic-mcp/badge\" alt=\"artic-mcp MCP server\" />\n</a>\n\n# Art Institute of Chicago MCP Server\n\nA Model Context Protocol (MCP) server that provides access to the [Art Institute of Chicago Collection](https://www.artic.edu/collection) through natural language interactions. This server allows AI models to search the art collection at the Art Institute of Chicago and have artworks available as a Resource.\n\n## Features\n\nThis server provides AI models with the following tools for interacting with the art collection.\nThe server also includes a prompt, **art-gallery** that will generate an interactive art gallery for a specific artist.\n\n### 1. Search By Title (search-by-title)\n\nSearch for artworks by title in the Art Institute of Chicago\n\n- Inputs:\n  - `title` (string) The title of the artwork to search for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Nighthawks\n  Artwork ID: 111628\n  Thumbnail alt text: Scene in a diner, viewed through wrap-around glass windows, at night on an empty urban street. A light-skinned man and woman, he in a suit and she in a red dress, sit together at a triangular wood bar, eyes downcast. At left sits another man, his back to the viewer. Behind the counter is a light-skinned man in a white uniform. The interior lights cast a yellow glow that spills onto the street in pale green. Above the diner a sign reads, \"Phillies.\"\n  Score: 1055.3839\n\n  -----\n  Title: Nighthawks\n  Artwork ID: 118165\n  Thumbnail alt text: A work made of chromogenic print.\n  Score: 57.28689\n\n  Pagination Info\n  Total: 2\n  Total Pages: 1\n  Current Page: 1\n  ```\n\n### 2. Get a specific artwork (get-artwork-by-id)\n\nGets additional information, including the image if available, for a piece of art based on it's id.\n\n- Inputs:\n  - `id` (number) The ID of the artwork to retrieve.\n  - `includeImage` (boolean, optional, default true) Whether to include the artwork image in the response.\n- Outputs:\n\n  ```\n  Title: Nighthawks\n  Artist: Edward Hopper (American, 1882–1967)\n  Artist ID: 34996\n  Description: <p>About <em>Nighthawks</em> Edward Hopper recollected, “unconsciously, probably, I was painting the loneliness of a large city.” In an all-night diner, three customers sit at the counter opposite a server, each appear to be lost in thought and disengaged from one another. The composition is tightly organized and spare in details: there is no entrance to the establishment, no debris on the streets. Through harmonious geometric forms and the glow of the diner’s electric lighting, Hopper created a serene, beautiful, yet enigmatic scene. Although inspired by a restaurant Hopper had seen on Greenwich Avenue in New York, the painting is not a realistic transcription of an actual place. As viewers, we are left to wonder about the figures, their relationships, and this imagined world.</p>\n\n  Image ID: 831a05de-d3f6-f4fa-a460-23008dd58dda\n  Place of Origin: United States\n  Dimensions: 84.1 × 152.4 cm (33 1/8 × 60 in.)\n  Medium: Oil on canvas\n  Credit Line: Friends of American Art Collection\n  Department: Arts of the Americas\n  Is On View: Yes\n  Main Reference Number: 1942.51\n  Has not been viewed much: No\n  Date Start: 1942\n  Date End: 1942\n  Date: 1942\n  Fiscal Year: 1942\n  Is Public Domain: No\n  Gallery: Gallery 262\n  Artwork Type: Painting\n  Artist Title: Edward Hopper\n  Artist Titles: Edward Hopper\n  Style Title: Modernism\n\n  ```\n\n  ```\n  **image encoded in base64 if available and includeImage is true\n  ```\n\n### 3. Full text search (full-text-search)\n\nPerforms a full text search of artworks whose metadata contains the search query.\n\n- Inputs:\n  - `query` (string) The term to search the metadata for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Untitled\n  Artwork ID: 62290\n  Thumbnail alt text: A work made of oil and enamel on paper, mounted on composition board.\n  Score: 108.70728\n\n  -----\n  ...\n  ...\n  -----\n  Title: Homage to the Square: Light Passage\n  Artwork ID: 5569\n  Thumbnail alt text: Painting of overlapping squares in grey, yellow, gold, and orange.\n  Score: 104.18398\n\n  Pagination Info\n  Total: 8399\n  Total Pages: 840\n  Current Page: 1\n  ```\n\n### 4. Artist search (search-for-artist)\n\nSearch for a specific artist\n\n- Inputs:\n  - `name` (string) The name of the artist to search for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Vincent van Gogh\n  Artist ID: 40610\n  Score: 55.865852\n\n  -----\n  Title: Imitator of Vincent van Gogh\n  Artist ID: 47301\n  Score: 48.782307\n\n  Pagination Info\n  Total: 2\n  Total Pages: 1\n  Current Page: 1\n  ```\n\n### 5. Find artwork by artist (get-artwork-by-artist)\n\nFind works of art by an artist\n\n- Inputs:\n  - `id` (number) The id of the artist to search for artworks. Should be the Artist ID of the `search-for-artist` tool.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: The Bedroom\n  Artwork ID: 28560\n  Thumbnail alt text: Painting of bedroom, blue walls, green window, tan bed, red bedding.\n  Score: 11473.843\n\n  -----\n  .\n  .\n  .\n  -----\n  Title: Weeping Tree\n  Artwork ID: 52733\n  Thumbnail alt text: A work made of reed pen and black-brown ink, with black chalk on off-white wove paper.\n  Score: 11.8061\n\n  Pagination Info\n  Total: 18\n  Total Pages: 2\n  Current Page: 1\n  ```\n\n### 6. Find artwork by medium (search-by-medium)\n\nSearch for artworks by medium in the Art Institute of Chicago\n\n- Inputs\n  - `medium` (string) The medium to search for (e.g., \"oil on canvas\", \"acrylic on panel\", \"watercolor on paper\").\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: The Bedroom\n  Artwork ID: 28560\n  Thumbnail alt text: Painting of bedroom, blue walls, green window, tan bed, red bedding.\n  Score: 56644.137\n\n  -----\n  Title: Starry Night and the Astronauts\n  Artwork ID: 129884\n  Thumbnail alt text: Abstract painting composed of small vertical dabs of multiple shades of blue with a small area of similar strokes of red, orange, and yellow in the upper right.\n  Score: 44797.906\n\n  -----\n  ```\n\n### 7. Art Gallery Prompt (art-gallery)\n\nIn the Claude Desktop app, use this prompt—along with your chosen artist’s name—to generate an interactive HTML art gallery showcasing their work.\n\nImages are not displayed in the Claude Desktop app due to security reasons. You can download the HTML from Claude Desktop and view the images in the browser.\n\nClaude Desktop Example\n\n<a href=\"https://mikechao.github.io/images/gallery-prompt.webp\" target=\"_blank\" rel=\"noopener noreferrer\">\n<img width=\"380\" height=\"200\" src=\"https://mikechao.github.io/images/gallery-prompt.webp\" alt=\"claude desktop example\" />\n</a>\n\nThe Generated Gallery in a browser\n\n<a href=\"https://mikechao.github.io/images/gallery-prompt-browser.webp\" target=\"_blank\" rel=\"noopener noreferrer\">\n<img width=\"380\" height=\"200\" src=\"https://mikechao.github.io/images/gallery-prompt-browser.webp\" alt=\"claude desktop example\" />\n</a>\n\n## Usage\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"artic-museum\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"artic-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Installing via Smithery\n\nTo install artic-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mikechao/artic-mcp):\n\n```bash\nnpx -y @smithery/cli install @mikechao/artic-mcp --client claude\n```\n\n## Example queries\n\nHere some questions you can ask the AI model when this server in connected:\n\n```\nCan you show me the painting titled \"Nighthawks\"?\nCan you find art done by Vincent van Gogh in 1890 that is on display?\nCan you find art by the artist Jackson Pollock?\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Disclaimer\n\nThis MCP Server is not officially associated with The Art Institute of Chicago. It is a third-party implementation of the [The Art Institute of Chicago's API](https://api.artic.edu/docs/) with a MCP Server.\n",
      "npm_url": "https://www.npmjs.com/package/artic-mcp",
      "npm_downloads": 382,
      "keywords": [
        "mikechao",
        "mcp",
        "searches",
        "search mikechao",
        "searches artworks",
        "mikechao artic"
      ],
      "category": "web-search"
    },
    "mikechao--brave-search-mcp": {
      "owner": "mikechao",
      "name": "brave-search-mcp",
      "url": "https://github.com/mikechao/brave-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikechao.webp",
      "description": "Integrates the Brave Search API to provide comprehensive search functionalities, including web, image, news, video, and local points of interest searches. Enables retrieval of relevant results for various search queries.",
      "stars": 80,
      "forks": 15,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-18T13:13:13Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP Server implementation that integrates the [Brave Search API](https://brave.com/search/api/), providing, Web Search, Local Points of Interest Search, Video Search, Image Search and News Search capabilities\n\n<a href=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp/badge\" alt=\"Brave Search MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Perform a regular search on the web\n- **Image Search**: Search the web for images. Image search results will be available as a Resource\n- **News Search**: Search the web for news\n- **Video Search**: Search the web for videos\n- **Local Points of Interest Search**: Search for local physical locations, businesses, restaurants, services, etc\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches using Brave's API\n  - Inputs:\n    - `query` (string): The term to search the internet for\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `offset` (number, optional, default 0): The offset for pagination\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_image_search**\n\n  - Get images from the web relevant to the query\n  - Inputs:\n    - `query` (string): The term to search the internet for images of\n    - `count` (number, optional): The number of images to return (max 3, default 1)\n\n- **brave_news_search**\n\n  - Searches the web for news\n  - Inputs:\n    - `query` (string): The term to search the internet for news articles, trending topics, or recent events\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_local_search**\n\n  - Search for local businesses, services and points of interest\n  - **REQUIRES** subscription to the Pro api plan for location results\n  - Falls back to brave_web_search if no location results are found\n  - Inputs:\n    - `query` (string): Local search term\n    - `count` (number, optional): The number of results to return (max 20, default 5)\n\n- **brave_video_search**\n\n  - Search the web for videos\n  - Inputs:\n    - `query`: (string): The term to search for videos\n    - `count`: (number, optional): The number of videos to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api.search.brave.com/app/keys)\n\n### Usage with Claude Code\n\nFor [Claude Code](https://claude.ai/code) users, run this command:\n\n**Windows:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"cmd\",\"args\":[\"/c\",\"npx\",\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\n**Linux/macOS:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"npx\",\"args\":[\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\nReplace `YOUR_API_KEY_HERE` with your actual Brave Search API key.\n\n### Usage with Claude Desktop\n\n## Desktop Extension (DXT)\n\n1. Download the `dxt` file from the [Releases](https://github.com/mikechao/brave-search-mcp/releases)\n2. Open it with Claude Desktop\n   or\n   Go to File -> Settings -> Extensions and drag the .DXT file to the window to install it\n\n## Docker\n\n1. Clone the repo\n2. Docker build\n\n```bash\ndocker build -t brave-search-mcp:latest -f ./Dockerfile .\n```\n\n3. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with LibreChat\n\nAdd this to librechat.yaml\n\n```yaml\nbrave-search:\n  command: sh\n  args:\n    - -c\n    - BRAVE_API_KEY=API KEY npx -y brave-search-mcp\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Desktop Extensions (DXT)\n\nAnthropic recently released [Desktop Extensions](https://github.com/anthropics/dxt) allowing installation of local MCP Servers with one click.\n\nInstall the CLI tool to help generate both `manifest.json` and final `.dxt` file.\n\n```sh\nnpm install -g @anthropic-ai/dxt\n```\n\n### Creating the manifest.json file\n\n1. In this folder/directory which contains the local MCP Server, run `dxt init`. The command will start an interactive CLI to help create the `manifest.json`.\n\n### Creating the `dxt` file\n\n1. First install dev dependencies and build\n\n```sh\nnpm install\nnpm run build\n```\n\n2. Then install only the production dependencies, generate a smaller nodule_modules directory\n\n```sh\nnpm install --omit=dev\n```\n\n3. Run `dxt pack` to create a `dxt` file. This will also validate the manifest.json that was created. The `dxt` is essentially a zip file and will contain everything in this directory.\n\n## Disclaimer\n\nThis library is not officially associated with Brave Software. It is a third-party implementation of the Brave Search API with a MCP Server.\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/brave-search-mcp",
      "npm_downloads": 12525,
      "keywords": [
        "searches",
        "search",
        "retrieval",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "mingdedi--InternetSearch-mcp-server": {
      "owner": "mingdedi",
      "name": "InternetSearch-mcp-server",
      "url": "https://github.com/mingdedi/InternetSearch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mingdedi.webp",
      "description": "Facilitates real-time internet searches using the Bocha AI search API, enabling applications to access and retrieve web-based information efficiently. Requires an API key for integration and supports setup through a specific configuration format.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-12T02:46:13Z",
      "readme_content": "# Internetsearch-mcp-server\n\n- [English README](README.en.md)\n\n一个用于联网搜索的MCP服务器\n基于博查搜索API的MCP服务器  \n需要使用博查AI的搜索服务密钥，具体文档请查阅[博查开发文档](https://bocha-ai.feishu.cn/wiki/HmtOw1z6vik14Fkdu5uc9VaInBb).\n\n**如何使用**  \n1、将仓库git clone\n```bash\ngit clone https://github.com/mingdedi/Internetsearch-mcp-server.git\n```\n2、使用uv重建环境\n```bash\ncd Internetsearch-mcp-server\npip install uv\nuv venv\n./.venv/Scripts/activate.bat\nuv sync\n```\n3、在配置文件中添加，类似格式如下\n```json\n{\n  \"mcpServers\": {\n    \"Internetsearch-mcp-server\": {\n      \"description\": \"Internetsearch-mcp-server\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/Internetsearch-mcp-server\",\n        \"run\",\n        \"Internet_search.py\"\n      ],\n      \"env\": {\n        \"BOCHAAI_API_KEY\": \"sk-123456789412345678312323456789e\"\n      }\n    }\n  }\n}\n```\n\n**注意**  \n这里的BOCHAAI_API_KEY中的密钥仅仅是一个示例。  \n如果想要获取一个实际可用的密钥请访问[博查AI](https://bochaai.com/)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "internetsearch",
        "searches",
        "search",
        "internetsearch mcp",
        "mingdedi internetsearch",
        "web search"
      ],
      "category": "web-search"
    },
    "misanthropic-ai--ddg-mcp": {
      "owner": "misanthropic-ai",
      "name": "ddg-mcp",
      "url": "https://github.com/misanthropic-ai/ddg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/misanthropic-ai.webp",
      "description": "Leverage DuckDuckGo's capabilities to perform text and image searches while ensuring user privacy. The server provides an interface to obtain summaries of search results and conduct web searches for various media types.",
      "stars": 9,
      "forks": 8,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T10:40:07Z",
      "readme_content": "# ddg-mcp MCP server\n\nDuckDuckGo search API MCP - A server that provides DuckDuckGo search capabilities through the Model Context Protocol.\n\n## Components\n\n### Prompts\n\nThe server provides the following prompts:\n- **search-results-summary**: Creates a summary of DuckDuckGo search results\n  - Required \"query\" argument for the search term\n  - Optional \"style\" argument to control detail level (brief/detailed)\n\n### Tools\n\nThe server implements the following DuckDuckGo search tools:\n\n- **ddg-text-search**: Search the web for text results using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-image-search**: Search the web for images using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"size\", \"color\", \"type_image\", \"layout\", \"license_image\", \"max_results\"\n  \n- **ddg-news-search**: Search for news articles using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-video-search**: Search for videos using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"resolution\", \"duration\", \"license_videos\", \"max_results\"\n  \n- **ddg-ai-chat**: Chat with DuckDuckGo AI\n  - Required: \"keywords\" - Message or question to send to the AI\n  - Optional: \"model\" - AI model to use (options: \"gpt-4o-mini\", \"llama-3.3-70b\", \"claude-3-haiku\", \"o3-mini\", \"mistral-small-3\")\n\n## Installation\n\n### Prerequisites\n\n- Python 3.9 or higher\n- [uv](https://github.com/astral-sh/uv) (recommended) or pip\n\n### Install from PyPI\n\n```bash\n# Using uv\nuv install ddg-mcp\n\n# Using pip\npip install ddg-mcp\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/misanthropic-ai/ddg-mcp.git\ncd ddg-mcp\n```\n\n2. Install the package:\n```bash\n# Using uv\nuv install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration\n\n### Required Dependencies\n\nThe server requires the `duckduckgo-search` package, which will be installed automatically when you install `ddg-mcp`.\n\nIf you need to install it manually:\n```bash\nuv install duckduckgo-search\n# or\npip install duckduckgo-search\n```\n\n## DuckDuckGo Search Parameters\n\n### Common Parameters\n\nThese parameters are available for most search types:\n\n- **region**: Region code for localized results (default: \"wt-wt\")\n  - Examples: \"us-en\" (US English), \"uk-en\" (UK English), \"ru-ru\" (Russian)\n  - See [DuckDuckGo regions](https://duckduckgo.com/params) for more options\n\n- **safesearch**: Content filtering level (default: \"moderate\")\n  - \"on\": Strict filtering\n  - \"moderate\": Moderate filtering\n  - \"off\": No filtering\n\n- **timelimit**: Time range for results\n  - \"d\": Last day\n  - \"w\": Last week\n  - \"m\": Last month\n  - \"y\": Last year (not available for news/videos)\n\n- **max_results**: Maximum number of results to return (default: 10)\n\n### Search Operators\n\nYou can use these operators in your search keywords:\n\n- `cats dogs`: Results about cats or dogs\n- `\"cats and dogs\"`: Results for exact term \"cats and dogs\"\n- `cats -dogs`: Fewer dogs in results\n- `cats +dogs`: More dogs in results\n- `cats filetype:pdf`: PDFs about cats (supported: pdf, doc(x), xls(x), ppt(x), html)\n- `dogs site:example.com`: Pages about dogs from example.com\n- `cats -site:example.com`: Pages about cats, excluding example.com\n- `intitle:dogs`: Page title includes the word \"dogs\"\n- `inurl:cats`: Page URL includes the word \"cats\"\n\n### Image Search Specific Parameters\n\n- **size**: \"Small\", \"Medium\", \"Large\", \"Wallpaper\"\n- **color**: \"color\", \"Monochrome\", \"Red\", \"Orange\", \"Yellow\", \"Green\", \"Blue\", \"Purple\", \"Pink\", \"Brown\", \"Black\", \"Gray\", \"Teal\", \"White\"\n- **type_image**: \"photo\", \"clipart\", \"gif\", \"transparent\", \"line\"\n- **layout**: \"Square\", \"Tall\", \"Wide\"\n- **license_image**: \"any\", \"Public\", \"Share\", \"ShareCommercially\", \"Modify\", \"ModifyCommercially\"\n\n### Video Search Specific Parameters\n\n- **resolution**: \"high\", \"standard\"\n- **duration**: \"short\", \"medium\", \"long\"\n- **license_videos**: \"creativeCommon\", \"youtube\"\n\n### AI Chat Models\n\n- **gpt-4o-mini**: OpenAI's GPT-4o mini model\n- **llama-3.3-70b**: Meta's Llama 3.3 70B model\n- **claude-3-haiku**: Anthropic's Claude 3 Haiku model\n- **o3-mini**: OpenAI's O3 mini model\n- **mistral-small-3**: Mistral AI's small model\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/shannon/Workspace/artivus/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ddg-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Usage Examples\n\n### Text Search\n\n```\nUse the ddg-text-search tool to search for \"climate change solutions\"\n```\n\nAdvanced example:\n```\nUse the ddg-text-search tool to search for \"renewable energy filetype:pdf site:edu\" with region \"us-en\", safesearch \"off\", timelimit \"y\", and max_results 20\n```\n\n### Image Search\n\n```\nUse the ddg-image-search tool to find images of \"renewable energy\" with color set to \"Green\"\n```\n\nAdvanced example:\n```\nUse the ddg-image-search tool to find images of \"mountain landscape\" with size \"Large\", color \"Blue\", type_image \"photo\", layout \"Wide\", and license_image \"Public\"\n```\n\n### News Search\n\n```\nUse the ddg-news-search tool to find recent news about \"artificial intelligence\" from the last day\n```\n\nAdvanced example:\n```\nUse the ddg-news-search tool to search for \"space exploration\" with region \"uk-en\", timelimit \"w\", and max_results 15\n```\n\n### Video Search\n\n```\nUse the ddg-video-search tool to find videos about \"machine learning tutorials\" with duration set to \"medium\"\n```\n\nAdvanced example:\n```\nUse the ddg-video-search tool to search for \"cooking recipes\" with resolution \"high\", duration \"short\", license_videos \"creativeCommon\", and max_results 10\n```\n\n### AI Chat\n\n```\nUse the ddg-ai-chat tool to ask \"What are the latest developments in quantum computing?\" using the claude-3-haiku model\n```\n\n### Search Results Summary\n\n```\nUse the search-results-summary prompt with query \"space exploration\" and style \"detailed\"\n```\n\n## Claude config\n\"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/INSTALLATION/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\"\n      ]\n  },\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Automated Publishing with GitHub Actions\n\nThis repository includes a GitHub Actions workflow for automated publishing to PyPI. The workflow is triggered when:\n\n1. A new GitHub Release is created\n2. The workflow is manually triggered via the GitHub Actions interface\n\nTo set up automated publishing:\n\n1. Generate a PyPI API token:\n   - Go to https://pypi.org/manage/account/token/\n   - Create a new token with scope limited to the `ddg-mcp` project\n   - Copy the token value (you'll only see it once)\n\n2. Add the token to your GitHub repository secrets:\n   - Go to your repository on GitHub\n   - Navigate to Settings > Secrets and variables > Actions\n   - Click \"New repository secret\"\n   - Name: `PYPI_API_TOKEN`\n   - Value: Paste your PyPI token\n   - Click \"Add secret\"\n\n3. To publish a new version:\n   - Update the version number in `pyproject.toml`\n   - Create a new release on GitHub or manually trigger the workflow\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/your/ddg-mcp run ddg-mcp\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "https://www.npmjs.com/package/ddg-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "searches",
        "search",
        "duckduckgo capabilities",
        "image searches",
        "web search"
      ],
      "category": "web-search"
    },
    "mixelpixx--Google-Search-MCP-Server": {
      "owner": "mixelpixx",
      "name": "Google-Search-MCP-Server",
      "url": "https://github.com/mixelpixx/Google-Search-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/mixelpixx.webp",
      "description": "Integrates Google Custom Search to retrieve search results and perform detailed analysis of webpage content. Supports multiple output formats and provides features for categorization, summarization, and optimized responses.",
      "stars": 147,
      "forks": 46,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:23:23Z",
      "readme_content": "# Version 2.0 is here\r\n\r\n# Google Search MCP Server\r\nAn MCP (Model Context Protocol) server that provides Google search capabilities and webpage content analysis tools. This server enables AI models to perform Google searches and analyze webpage content programmatically.\r\n\r\n## Features\r\n\r\n- Google Custom Search integration\r\n- Advanced search features (filters, sorting, pagination, categorization)\r\n- Webpage content analysis in multiple formats (markdown, HTML, plain text)\r\n- Batch webpage analysis\r\n- Result categorization and classification\r\n- Content summarization\r\n- Optimized, human-readable responses\r\n- MCP-compliant interface\r\n\r\n## Prerequisites\r\n\r\n- Node.js (v16 or higher)\r\n- Google Cloud Platform account\r\n- Custom Search Engine ID\r\n- Google API Key\r\n\r\n## Installation\r\n\r\n1. Clone the repository\r\n2. Install Node.js dependencies:\r\n```bash\r\nnpm install\r\n```\r\n3. Build the TypeScript code:\r\n```bash\r\nnpm run build\r\n```\r\n\r\n## Configuration\r\n\r\n1. Set up environment variables for your Google API credentials:\r\n\r\nYou can either set these as system environment variables or configure them in your MCP settings file.\r\n\r\nRequired environment variables:\r\n- `GOOGLE_API_KEY`: Your Google API key\r\n- `GOOGLE_SEARCH_ENGINE_ID`: Your Custom Search Engine ID\r\n\r\n2. Add the server configuration to your MCP settings file (typically located at `%APPDATA%/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"google-search\": {\r\n      \"autoApprove\": [\r\n        \"google_search\",\r\n        \"extract_webpage_content\",\r\n        \"extract_multiple_webpages\"\r\n      ],\r\n      \"disabled\": false,\r\n      \"timeout\": 60,\r\n      \"command\": \"node\",\r\n      \"args\": [\r\n        \"/path/to/google-search-mcp-server/dist/google-search.js\"\r\n      ],\r\n      \"env\": {\r\n        \"GOOGLE_API_KEY\": \"your-google-api-key\",\r\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your-custom-search-engine-id\"\r\n      },\r\n      \"transportType\": \"stdio\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Running\r\n\r\nStart the MCP server:\r\n```bash\r\nnpm run start\r\n```\r\n\r\n## Available Tools\r\n\r\n### 1. google_search\r\nSearch Google and return relevant results from the web. This tool finds web pages, articles, and information on specific topics using Google's search engine.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"google_search\",\r\n  \"arguments\": {\r\n    \"query\": \"your search query\",\r\n    \"num_results\": 5, // optional, default: 5\r\n    \"site\": \"example.com\", // optional, limit results to specific website\r\n    \"language\": \"en\", // optional, filter by language (ISO 639-1 code)\r\n    \"dateRestrict\": \"m6\", // optional, filter by date (e.g., \"m6\" for last 6 months)\r\n    \"exactTerms\": \"exact phrase\", // optional, search for exact phrase\r\n    \"resultType\": \"news\", // optional, specify type (news, images, videos)\r\n    \"page\": 2, // optional, page number for pagination (starts at 1)\r\n    \"resultsPerPage\": 10, // optional, results per page (max: 10)\r\n    \"sort\": \"date\" // optional, sort by \"date\" or \"relevance\" (default)\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Search results with title, link, snippet in a readable format\r\n- Pagination information (current page, total results, etc.)\r\n- Categories of results (automatically detected)\r\n- Navigation hints for pagination\r\n\r\n### 2. extract_webpage_content\r\nExtract and analyze content from a webpage, converting it to readable text. This tool fetches the main content while removing ads, navigation elements, and other clutter.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"extract_webpage_content\",\r\n  \"arguments\": {\r\n    \"url\": \"https://example.com\",\r\n    \"format\": \"markdown\" // optional, format options: \"markdown\" (default), \"html\", or \"text\"\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Title and description of the webpage\r\n- Content statistics (word count, character count)\r\n- Content summary\r\n- Content preview (first 500 characters)\r\n\r\n### 3. extract_multiple_webpages\r\nExtract and analyze content from multiple webpages in a single request. Ideal for comparing information across different sources or gathering comprehensive information on a topic.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"extract_multiple_webpages\",\r\n  \"arguments\": {\r\n    \"urls\": [\r\n      \"https://example1.com\",\r\n      \"https://example2.com\"\r\n    ],\r\n    \"format\": \"html\" // optional, format options: \"markdown\" (default), \"html\", or \"text\"\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Title and description of each webpage\r\n- Content statistics for each webpage\r\n- Content summary for each webpage\r\n- Content preview for each webpage (first 150 characters)\r\n\r\n## Getting Google API Credentials\r\n\r\n1. Go to the [Google Cloud Console](https://console.cloud.google.com/)\r\n2. Create a new project or select an existing one\r\n3. Enable the Custom Search API\r\n4. Create API credentials (API Key)\r\n5. Go to the [Custom Search Engine](https://programmablesearchengine.google.com/about/) page\r\n6. Create a new search engine and get your Search Engine ID\r\n7. Add these credentials to your MCP settings file or set them as environment variables\r\n\r\n## Error Handling\r\n\r\nThe server provides detailed error messages for:\r\n- Missing or invalid API credentials\r\n- Failed search requests\r\n- Invalid webpage URLs\r\n- Network connectivity issues\r\n\r\n## Architecture\r\n\r\nThe server is built with TypeScript and uses the MCP SDK to provide a standardized interface for AI models to interact with Google Search and webpage content analysis tools. It consists of two main services:\r\n\r\n1. **GoogleSearchService**: Handles Google API interactions for search functionality\r\n2. **ContentExtractor**: Manages webpage content analysis and extraction\r\n\r\nThe server uses caching mechanisms to improve performance and reduce API calls.\r\n\r\n## Distributing the Built Version\r\n\r\nIf you prefer to distribute only the built version of this tool rather than the source code, you can follow these steps:\r\n\r\n1. Build the TypeScript code:\r\n```bash\r\nnpm run build\r\n```\r\n\r\n2. Create a distribution package with only the necessary files:\r\n```bash\r\n# Create a distribution directory\r\nmkdir -p dist-package\r\n\r\n# Copy the compiled JavaScript files\r\ncp -r dist dist-package/\r\n\r\n# Copy package files (without dev dependencies)\r\ncp package.json dist-package/\r\ncp README.md dist-package/\r\n\r\n# Create a simplified package.json for distribution\r\nnode -e \"const pkg = require('./package.json'); delete pkg.devDependencies; delete pkg.scripts.build; delete pkg.scripts.dev; pkg.scripts.start = 'node dist/google-search.js'; require('fs').writeFileSync('dist-package/package.json', JSON.stringify(pkg, null, 2));\"\r\n```\r\n\r\n3. Users can then install and run the built version:\r\n```bash\r\n# Install production dependencies only\r\nnpm install --production\r\n\r\n# Start the server\r\nnpm start\r\n```\r\n\r\nThis approach allows you to distribute the compiled JavaScript files without exposing the TypeScript source code. Users will still need to:\r\n\r\n1. Configure their Google API credentials as environment variables\r\n2. Add the server configuration to their MCP settings file\r\n3. Install the production dependencies\r\n\r\nNote that the package.json in the distribution will only include production dependencies and a simplified set of scripts.\r\n\r\n## License\r\n\r\nMIT\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "google",
        "search",
        "webpage",
        "web search",
        "search mixelpixx",
        "search results"
      ],
      "category": "web-search"
    },
    "mkusaka--mcp-server-perplexity": {
      "owner": "mkusaka",
      "name": "mcp-server-perplexity",
      "url": "https://github.com/mkusaka/mcp-server-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/mkusaka.webp",
      "description": "Integrates Perplexity AI's API for advanced search capabilities, supporting multiple models with configurable result counts and detailed error handling.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-02-11T07:12:56Z",
      "readme_content": "# Perplexity AI MCP Server\n\nAn MCP server implementation that integrates Perplexity AI's API, providing advanced search capabilities with multiple model options.\n\n## Features\n\n- Search using Perplexity AI's models\n- Support for all official Sonar models\n- Configurable result count\n- Detailed error handling and logging\n- MCP Inspector compatible\n\n## Available Models\n\n- sonar-reasoning-pro (127k context)\n- sonar-reasoning (127k context)\n- sonar-pro (200k context)\n- sonar (127k context)\n\n## Installation\n\n```bash\npnpm install\npnpm build\n```\n\n## Configuration\n\n### API Key\n1. Sign up for a [Perplexity AI account](https://www.perplexity.ai)\n2. Get your API key from the dashboard\n3. Set the environment variable:\n```bash\nexport PERPLEXITY_API_KEY=your_api_key_here\n```\n\n## Usage\n\n### Direct Execution\n```bash\nnode dist/index.js\n# or if you made it executable\n./dist/index.js\n```\n\n### Development\n```bash\npnpm dev\n```\n\n### Testing with MCP Inspector\n```bash\npnpm inspect\n```\n\n## Tool Reference\n\n### perplexity_search\n\nPerforms a search using Perplexity AI's models.\n\nParameters:\n- `query` (string, required): The search query\n- `model` (string, optional): Model to use (default: \"sonar\")\n  - Available options: sonar-reasoning-pro, sonar-reasoning, sonar-pro, sonar\n- `count` (number, optional): Number of results (1-10, default: 5)\n\n## Development\n\n### Project Structure\n```\nsrc/perplexity/\n├── index.ts        # Main server implementation\n├── lib/\n│   └── logger.ts   # Logging configuration\n└── adr.md         # Architectural decisions\n```\n\n### Logging\nLogs are written to `perplexity-mcp.log` in the project root directory.\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a new Pull Request\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-perplexity",
      "npm_downloads": 934,
      "keywords": [
        "search",
        "perplexity",
        "ai",
        "perplexity ai",
        "search capabilities",
        "ai api"
      ],
      "category": "web-search"
    },
    "mmmaaatttttt--mcp-live-events": {
      "owner": "mmmaaatttttt",
      "name": "mcp-live-events",
      "url": "https://github.com/mmmaaatttttt/mcp-live-events",
      "imageUrl": "/freedevtools/mcp/pfp/mmmaaatttttt.webp",
      "description": "Fetch real-time event data from Ticketmaster's API and format responses for easy interpretation by AI language models. Provides details on concerts and events based on user queries.",
      "stars": 2,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-23T21:08:29Z",
      "readme_content": "# MCP Live Events Server\n\n`mcp-live-events` is a Model Context Protocol (MCP) server that integrates with\nthe Ticketmaster API to provide real-time event data. It allows AI agents to\nfetch concert and event details dynamically.\n\n## Features\n\n- 🎟️ Integrates with the Ticketmaster API to search for events\n- 🗣️ Formats API responses for ease of LLM interpretation\n\n## Setup\n\n### Prerequisites\n\nEnsure you have the following installed:\n\n- [uv](https://github.com/astral-sh/uv) (used for package management)\n- Python 3.13+\n- A [Ticketmaster](https://developer.ticketmaster.com/explore/) API key (free to\n  use, but rate limited)\n\n### Installation\n\n1. Clone the repository:\n\n    ```sh\n    git clone https://github.com/mmmaaatttttt/mcp-live-events.git\n    cd mcp-live-events\n    ```\n\n2. Install dependencies:\n\n    ```sh\n    uv venv\n    uv sync\n    ```\n\n3. Set up your environment variables, i.e. the Ticketmaster API key. This can\n   either be placed in a `.env` file in this repository, following the pattern\n   of the `.env.example` file, or it can be placed in an \"env\" section of this\n   server's configuration in your MCP client.\n\n   Note that on the Ticketmaster developer portal, the API key is named\n   \"Consumer Key.\"\n\n### Running the server\n\n```sh\nuv run mcp-live-events\n```\n\nIf it's successful, you should see `MCP Live Event server is running!` print to\nyour terminal.\n\n## Resources\n\n- [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol)\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io/introduction)\n- [MCP Server Demo Quickstart](https://modelcontextprotocol.io/quickstart/server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ticketmaster",
        "event",
        "events",
        "live events",
        "ticketmaster api",
        "events fetch"
      ],
      "category": "web-search"
    },
    "mnhlt--WebSearch-MCP": {
      "owner": "mnhlt",
      "name": "WebSearch-MCP",
      "url": "https://github.com/mnhlt/WebSearch-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/mnhlt.webp",
      "description": "Provides real-time web search capabilities to retrieve up-to-date information from the internet using the WebSearch Crawler API, enabling AI assistants to access the latest web content.",
      "stars": 17,
      "forks": 6,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-08T19:26:30Z",
      "readme_content": "# WebSearch-MCP\n\n[![smithery badge](https://smithery.ai/badge/@mnhlt/WebSearch-MCP)](https://smithery.ai/server/@mnhlt/WebSearch-MCP)\n\nA Model Context Protocol (MCP) server implementation that provides a web search capability over stdio transport. This server integrates with a WebSearch Crawler API to retrieve search results.\n\n## Table of Contents\n\n- [About](#about)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Setup & Integration](#setup--integration)\n  - [Setting Up the Crawler Service](#setting-up-the-crawler-service)\n    - [Prerequisites](#prerequisites)\n    - [Starting the Crawler Service](#starting-the-crawler-service)\n    - [Testing the Crawler API](#testing-the-crawler-api)\n    - [Custom Configuration](#custom-configuration)\n  - [Integrating with MCP Clients](#integrating-with-mcp-clients)\n    - [Quick Reference: MCP Configuration](#quick-reference-mcp-configuration)\n    - [Claude Desktop](#claude-desktop)\n    - [Cursor IDE](#cursor-ide)\n    - [Cline](#cline-command-line-interface-for-claude)\n- [Usage](#usage)\n  - [Parameters](#parameters)\n  - [Example Search Response](#example-search-response)\n  - [Testing Locally](#testing-locally)\n  - [As a Library](#as-a-library)\n- [Troubleshooting](#troubleshooting)\n  - [Crawler Service Issues](#crawler-service-issues)\n  - [MCP Server Issues](#mcp-server-issues)\n- [Development](#development)\n  - [Project Structure](#project-structure)\n  - [Publishing to npm](#publishing-to-npm)\n- [Contributing](#contributing)\n- [License](#license)\n\n## About\n\nWebSearch-MCP is a Model Context Protocol server that provides web search capabilities to AI assistants that support MCP. It allows AI models like Claude to search the web in real-time, retrieving up-to-date information about any topic.\n\nThe server integrates with a Crawler API service that handles the actual web searches, and communicates with AI assistants using the standardized Model Context Protocol.\n\n## Installation\n\n### Installing via Smithery\n\nTo install WebSearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mnhlt/WebSearch-MCP):\n\n```bash\nnpx -y @smithery/cli install @mnhlt/WebSearch-MCP --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g websearch-mcp\n```\n\nOr use without installing:\n\n```bash\nnpx websearch-mcp\n```\n\n## Configuration\n\nThe WebSearch MCP server can be configured using environment variables:\n\n- `API_URL`: The URL of the WebSearch Crawler API (default: `http://localhost:3001`)\n- `MAX_SEARCH_RESULT`: Maximum number of search results to return when not specified in the request (default: `5`)\n\nExamples:\n```bash\n# Configure API URL\nAPI_URL=https://crawler.example.com npx websearch-mcp\n\n# Configure maximum search results\nMAX_SEARCH_RESULT=10 npx websearch-mcp\n\n# Configure both\nAPI_URL=https://crawler.example.com MAX_SEARCH_RESULT=10 npx websearch-mcp\n```\n\n## Setup & Integration\n\nSetting up WebSearch-MCP involves two main parts: configuring the crawler service that performs the actual web searches, and integrating the MCP server with your AI client applications.\n\n### Setting Up the Crawler Service\n\nThe WebSearch MCP server requires a crawler service to perform the actual web searches. You can easily set up the crawler service using Docker Compose.\n\n### Prerequisites\n\n- [Docker](https://www.docker.com/get-started) and [Docker Compose](https://docs.docker.com/compose/install/)\n\n### Starting the Crawler Service\n\n1. Create a file named `docker-compose.yml` with the following content:\n\n```yaml\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\nworkaround for Mac Apple Silicon\n```\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    platform: \"linux/amd64\"\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    platform: \"linux/arm64\"\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\n\n2. Start the services:\n\n```bash\ndocker-compose up -d\n```\n\n3. Verify that the services are running:\n\n```bash\ndocker-compose ps\n```\n\n4. Test the crawler API health endpoint:\n\n```bash\ncurl http://localhost:3001/health\n```\n\nExpected response:\n```json\n{\n  \"status\": \"ok\",\n  \"details\": {\n    \"status\": \"ok\",\n    \"flaresolverr\": true,\n    \"google\": true,\n    \"message\": null\n  }\n}\n```\n\nThe crawler API will be available at `http://localhost:3001`.\n\n### Testing the Crawler API\n\nYou can test the crawler API directly using curl:\n\n```bash\ncurl -X POST http://localhost:3001/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"typescript best practices\",\n    \"numResults\": 2,\n    \"language\": \"en\",\n    \"filters\": {\n      \"excludeDomains\": [\"youtube.com\"],\n      \"resultType\": \"all\" \n    }\n  }'\n```\n\n### Custom Configuration\n\nYou can customize the crawler service by modifying the environment variables in the `docker-compose.yml` file:\n\n- `PORT`: The port on which the crawler API listens (default: 3001)\n- `LOG_LEVEL`: Logging level (options: debug, info, warn, error)\n- `FLARESOLVERR_URL`: URL of the FlareSolverr service (for bypassing Cloudflare protection)\n\n## Integrating with MCP Clients\n\n### Quick Reference: MCP Configuration\n\nHere's a quick reference for MCP configuration across different clients:\n\n```json\n{\n    \"mcpServers\": {\n        \"websearch\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"5\" // reduce to save your tokens, increase for wider information gain\n            }\n        }\n    }\n}\n```\n\nWorkaround for Windows, due to [Issue](https://github.com/smithery-ai/mcp-obsidian/issues/19)\n```\n{\n\t\"mcpServers\": {\n\t  \"websearch\": {\n            \"command\": \"cmd\",\n            \"args\": [\n\t\t\t\t\"/c\",\n\t\t\t\t\"npx\",\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"1\"\n            }\n        }\n\t}\n  }\n```\n\n## Usage\n\nThis package implements an MCP server using stdio transport that exposes a `web_search` tool with the following parameters:\n\n### Parameters\n\n- `query` (required): The search query to look up\n- `numResults` (optional): Number of results to return (default: 5)\n- `language` (optional): Language code for search results (e.g., 'en')\n- `region` (optional): Region code for search results (e.g., 'us')\n- `excludeDomains` (optional): Domains to exclude from results\n- `includeDomains` (optional): Only include these domains in results\n- `excludeTerms` (optional): Terms to exclude from results\n- `resultType` (optional): Type of results to return ('all', 'news', or 'blogs')\n\n### Example Search Response\n\nHere's an example of a search response:\n\n```json\n{\n  \"query\": \"machine learning trends\",\n  \"results\": [\n    {\n      \"title\": \"Top Machine Learning Trends in 2025\",\n      \"snippet\": \"The key machine learning trends for 2025 include multimodal AI, generative models, and quantum machine learning applications in enterprise...\",\n      \"url\": \"https://example.com/machine-learning-trends-2025\",\n      \"siteName\": \"AI Research Today\",\n      \"byline\": \"Dr. Jane Smith\"\n    },\n    {\n      \"title\": \"The Evolution of Machine Learning: 2020-2025\",\n      \"snippet\": \"Over the past five years, machine learning has evolved from primarily supervised learning approaches to more sophisticated self-supervised and reinforcement learning paradigms...\",\n      \"url\": \"https://example.com/ml-evolution\",\n      \"siteName\": \"Tech Insights\",\n      \"byline\": \"John Doe\"\n    }\n  ]\n}\n```\n\n### Testing Locally\n\nTo test the WebSearch MCP server locally, you can use the included test client:\n\n```bash\nnpm run test-client\n```\n\nThis will start the MCP server and a simple command-line interface that allows you to enter search queries and see the results.\n\nYou can also configure the API_URL for the test client:\n\n```bash\nAPI_URL=https://crawler.example.com npm run test-client\n```\n\n### As a Library\n\nYou can use this package programmatically:\n\n```typescript\nimport { createMCPClient } from '@modelcontextprotocol/sdk';\n\n// Create an MCP client\nconst client = createMCPClient({\n  transport: { type: 'subprocess', command: 'npx websearch-mcp' }\n});\n\n// Execute a web search\nconst response = await client.request({\n  method: 'call_tool',\n  params: {\n    name: 'web_search',\n    arguments: {\n      query: 'your search query',\n      numResults: 5,\n      language: 'en'\n    }\n  }\n});\n\nconsole.log(response.result);\n```\n\n## Troubleshooting\n\n### Crawler Service Issues\n\n- **API Unreachable**: Ensure that the crawler service is running and accessible at the configured API_URL.\n- **Search Results Not Available**: Check the logs of the crawler service to see if there are any errors:\n  ```bash\n  docker-compose logs crawler\n  ```\n- **FlareSolverr Issues**: Some websites use Cloudflare protection. If you see errors related to this, check if FlareSolverr is working:\n  ```bash\n  docker-compose logs flaresolverr\n  ```\n\n### MCP Server Issues\n\n- **Import Errors**: Ensure you have the latest version of the MCP SDK:\n  ```bash\n  npm install -g @modelcontextprotocol/sdk@latest\n  ```\n- **Connection Issues**: Make sure the stdio transport is properly configured for your client.\n\n## Development\n\nTo work on this project:\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the project: `npm run build`\n4. Run in development mode: `npm run dev`\n\nThe server expects a WebSearch Crawler API as defined in the included swagger.json file. Make sure the API is running at the configured API_URL.\n\n### Project Structure\n\n- `.gitignore`: Specifies files that Git should ignore (node_modules, dist, logs, etc.)\n- `.npmignore`: Specifies files that shouldn't be included when publishing to npm\n- `package.json`: Project metadata and dependencies\n- `src/`: Source TypeScript files\n- `dist/`: Compiled JavaScript files (generated when building)\n\n### Publishing to npm\n\nTo publish this package to npm:\n\n1. Make sure you have an npm account and are logged in (`npm login`)\n2. Update the version in package.json (`npm version patch|minor|major`)\n3. Run `npm publish`\n\nThe `.npmignore` file ensures that only the necessary files are included in the published package:\n- The compiled code in `dist/`\n- README.md and LICENSE files\n- package.json\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/websearch-mcp",
      "npm_downloads": 5853,
      "keywords": [
        "websearch",
        "search",
        "web",
        "websearch mcp",
        "mnhlt websearch",
        "web search"
      ],
      "category": "web-search"
    },
    "modelcontextprotocol-servers--google-search-mcp": {
      "owner": "modelcontextprotocol-servers",
      "name": "google-search-mcp",
      "url": "https://github.com/modelcontextprotocol-servers/google-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/modelcontextprotocol-servers.webp",
      "description": "Perform real-time Google searches while bypassing anti-bot mechanisms and handling CAPTCHAs automatically. Extract structured search results in various languages and regions to support AI assistants.",
      "stars": 2,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T19:34:05Z",
      "readme_content": "# Google Search MCP\n\n[Model Context Protocol](https://modelcontextprotocol.wiki) server for google search.\nA Playwright-based Model Context Protocol (MCP) tool that bypasses search engine anti-bot mechanisms, performs Google searches, and extracts results, providing real-time search capabilities for AI assistants like Claude and Cursor.\n\n## Features\n\n- **Anti-Bot Bypass**: Uses browser fingerprint spoofing and real user behavior simulation to avoid detection\n- **Automatic CAPTCHA Handling**: Switches to headed mode when encountering CAPTCHAs, allowing users to complete verification\n- **State Persistence**: Saves browser session state to reduce the need for repeated verification\n- **Adaptability**: Uses multiple selector combinations to adapt to changes in Google search pages\n- **MCP Integration**: Implements the Model Context Protocol for easy integration with AI assistants\n- **Multi-language Support**: Supports search results in different languages and regions\n\n\n## Using with Cursor\n\n**Installation - Globally**\n\nRun the MCP server using npx:\n\n```bash\nnpx -y @mcp-server/google-search-mcp@latest\n```\n\nIn your Cursor IDE\n\n1. Go to `Cursor Settings` > `MCP`\n2. Click `+ Add New MCP Server`\n3. Fill in the form:\n   - Name: `google-search` (or any name you prefer)\n   - Type: `command`\n   - Command: `npx -y @mcp-server/google-search-mcp@latest`\n\n\n**Installation - Project-specific**\n\nAdd an `.cursor/mcp.json` file to your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"google-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@mcp-server/google-search-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n```bash\nyarn install\n```\n\n## Build the project\n\n```bash\nyarn build\n```\n\n## Usage\n\n### Running as an MCP Server\n\n```bash\nyarn start\n```\n\n### Using with MCP Inspector\n\nTo debug the server, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\n# First build the project\nyarn build\n\n# Start the MCP Inspector and server\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## Parameters\n\nThe search tool accepts the following parameters:\n\n- `query` (required): Search query string\n- `limit` (optional): Number of search results to return, default is 10\n- `timeout` (optional): Search operation timeout in milliseconds, default is 60000\n- `language` (optional): Language for search results, e.g., zh-CN, en-US, default is zh-CN\n- `region` (optional): Region for search results, e.g., cn, com, co.jp, default is cn\n\n## How It Works\n\n1. The tool uses Playwright to control a Chromium browser to perform Google searches\n2. It avoids bot detection through browser fingerprint spoofing and real user behavior simulation\n3. When encountering CAPTCHA verification, it automatically switches to headed mode for user completion\n4. It extracts search results and returns them in a structured format\n5. It saves browser state for reuse in subsequent searches\n\n## Advanced Configuration\n\n### Browser State File\n\nBy default, the browser state is saved in the `.google-search-browser-state.json` file in the user's home directory. You can modify this path through parameters.\n\n### Language and Region Settings\n\nYou can specify the language and region for search results through parameters:\n\n```\n// English (US) search results\n\"language\": \"en-US\", \"region\": \"com\"\n\n// Japanese search results\n\"language\": \"ja-JP\", \"region\": \"co.jp\"\n\n// Chinese (Simplified) search results\n\"language\": \"zh-CN\", \"region\": \"cn\"\n```\n\n## Notes\n\n- On first use, if you encounter CAPTCHA verification, the system will automatically switch to headed mode for you to complete the verification\n- After verification, the system will save the state file, making subsequent searches smoother\n- Overly frequent search requests may trigger Google's rate limiting mechanisms\n- This tool is for learning and research purposes only, please comply with Google's terms of service\n\n## License\n\nMIT\n\n## Disclaimer\n\nThis tool is for learning and research purposes only. When using this tool to access Google or other search engines, please comply with relevant terms of service and legal regulations. The author is not responsible for any issues resulting from the use of this tool.",
      "npm_url": "https://www.npmjs.com/package/google-search-mcp",
      "npm_downloads": 1098,
      "keywords": [
        "searches",
        "search",
        "captchas",
        "ai assistants",
        "web search",
        "google searches"
      ],
      "category": "web-search"
    },
    "mondweep--youtube-music-mcp-server": {
      "owner": "mondweep",
      "name": "youtube-music-mcp-server",
      "url": "https://github.com/mondweep/youtube-music-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mondweep.webp",
      "description": "Control YouTube Music playback by searching for and playing songs via voice commands or text input in Google Chrome. The server enables interaction between AI models and YouTube Music to manage music playback effectively.",
      "stars": 14,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T04:22:42Z",
      "readme_content": "# YouTube Music MCP Server\n\n## Overview\nThis project implements a Model Context Protocol (MCP) server that enables AI models to control YouTube Music playback through Google Chrome. It bridges the gap between AI assistants and music playback, allowing AI to search for and play songs based on song names and artist names.\n\n## What is MCP?\nThe Model Context Protocol (MCP) is a standardized way for AI models to interact with external tools and services. It provides a structured communication protocol that allows AI assistants to:\n- Discover available tools\n- Understand tool capabilities\n- Execute actions through these tools\n- Handle responses and errors consistently\n\nLearn more about MCP:\n- [MCP Documentation](https://github.com/modelcontextprotocol/protocol)\n- [MCP SDK](https://github.com/modelcontextprotocol/sdk)\n\n## Features\n- Search YouTube Music for songs\n- Play songs directly in Google Chrome\n- Support for song name and artist name search\n- Error handling and logging\n- Cross-platform support (focused on macOS for Chrome automation)\n\n## Architecture\n\n### High-Level Overview\n\n# youtube-music-server MCP Server\n\nA Model Context Protocol server\n\nThis is a TypeScript-based MCP server that implements a simple notes system. It demonstrates core MCP concepts by providing:\n\n- Resources representing text notes with URIs and metadata\n- Tools for creating new notes\n- Prompts for generating summaries of notes\n\n## Features\n\n### Resources\n- List and access notes via `note://` URIs\n- Each note has a title, content and metadata\n- Plain text mime type for simple content access\n\n### Tools\n- `create_note` - Create new text notes\n  - Takes title and content as required parameters\n  - Stores note in server state\n\n### Prompts\n- `summarize_notes` - Generate a summary of all stored notes\n  - Includes all note contents as embedded resources\n  - Returns structured prompt for LLM summarization\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-music-server\": {\n      \"command\": \"/path/to/youtube-music-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "playback",
        "youtube",
        "chrome",
        "mondweep youtube",
        "youtube music",
        "control youtube"
      ],
      "category": "web-search"
    },
    "mrgoonie--searchapi-mcp-server": {
      "owner": "mrgoonie",
      "name": "searchapi-mcp-server",
      "url": "https://github.com/mrgoonie/searchapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mrgoonie.webp",
      "description": "Connects AI assistants to external search data sources such as Google and Bing, enabling web, image, and YouTube searches through a standardized MCP interface.",
      "stars": 10,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-13T05:57:04Z",
      "readme_content": "# SearchAPI.site - MCP Server\n\nThis project provides a Model Context Protocol (MCP) server that connects AI assistants to external data sources (Google, Bing, etc.) via [SearchAPI.site](https://searchapi.site). \n\n**Author:** Claude\n\n- [Glama](https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server)\n- [Github](https://github.com/mrgoonie/searchapi-mcp-server)\n- [NPM](https://www.npmjs.com/package/searchapi-mcp-server)\n\n<a href=\"https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server/badge\" alt=\"SearchAPI Server MCP server\" />\n</a>\n\n### Available platforms\n- [x] Google - Web Search\n- [x] Google - Image Search\n- [x] Google - YouTube Search\n- [ ] Google - Maps Search\n- [x] Bing - Web Search\n- [ ] Bing - Image Search\n- [ ] Reddit\n- [ ] X/Twitter\n- [ ] Facebook Search\n- [ ] Facebook Group Search\n- [ ] Instagram\n- [ ] TikTok\n\n## SearchAPI.site\n\n- [Website](https://searchapi.site)\n- [API Docs](https://searchapi.site/api-docs)\n- [Swagger UI Config](https://searchapi.site/api-docs/swagger-ui-init.js)\n- Create Search API key [here](https://searchapi.site/profile)\n- [GitHub](https://github.com/mrgoonie/searchapi)\n\n## Supported Transports\n\n- [x] [\"stdio\"](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#stdio) transport - Default transport for CLI usage\n- [x] [\"Streamable HTTP\"](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) transport - For web-based clients\n  - [ ] Implement auth (\"Authorization\" headers with `Bearer <token>`)\n- [x] ~~\"sse\" transport~~ **[(Deprecated)](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#backwards-compatibility)**\n- [ ] Write tests\n\n## How to use\n\n### CLI\n\n```bash\n# Google search via CLI\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\"\n\n# Google image search via CLI\nnpm run dev:cli -- search-google-images --query \"your search query\" --api-key \"your-api-key\"\n\n# YouTube search via CLI\nnpm run dev:cli -- search-youtube --query \"your search query\" --api-key \"your-api-key\" --max-results 5\n```\n\n### MCP Setup\n\n**For local configuration with stdio transport:**\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/searchapi-mcp-server/dist/index.js\"],\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n**For remote HTTP configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"type\": \"http\",\n      \"url\": \"http://mcp.searchapi.site/mcp\"\n    }\n  }\n}\n```\n\n**Environment Variables for HTTP Transport:**\n\nYou can configure the HTTP server using these environment variables:\n\n- `MCP_HTTP_HOST`: The host to bind to (default: `127.0.0.1`)\n- `MCP_HTTP_PORT`: The port to listen on (default: `8080`)\n- `MCP_HTTP_PATH`: The endpoint path (default: `/mcp`)\n\n---\n\n# Source Code Overview\n\n## What is MCP?\n\nModel Context Protocol (MCP) is an open standard that allows AI systems to securely and contextually connect with external tools and data sources.\n\nThis boilerplate implements the MCP specification with a clean, layered architecture that can be extended to build custom MCP servers for any API or data source.\n\n## Why Use This Boilerplate?\n\n- **Production-Ready Architecture**: Follows the same pattern used in published MCP servers, with clear separation between CLI, tools, controllers, and services.\n\n- **Type Safety**: Built with TypeScript for improved developer experience, code quality, and maintainability.\n\n- **Working Example**: Includes a fully implemented IP lookup tool demonstrating the complete pattern from CLI to API integration.\n\n- **Testing Framework**: Comes with testing infrastructure for both unit and CLI integration tests, including coverage reporting.\n\n- **Development Tooling**: Includes ESLint, Prettier, TypeScript, and other quality tools preconfigured for MCP server development.\n\n---\n\n# Getting Started\n\n## Prerequisites\n\n- **Node.js** (>=18.x): [Download](https://nodejs.org/)\n- **Git**: For version control\n\n---\n\n## Step 1: Clone and Install\n\n```bash\n# Clone the repository\ngit clone https://github.com/mrgoonie/searchapi-mcp-server.git\ncd searchapi-mcp-server\n\n# Install dependencies\nnpm install\n```\n\n---\n\n## Step 2: Run Development Server\n\nStart the server in development mode with stdio transport (default):\n\n```bash\nnpm run dev:server\n```\n\nOr with the Streamable HTTP transport:\n\n```bash\nnpm run dev:server:http\n```\n\nThis starts the MCP server with hot-reloading and enables the MCP Inspector at http://localhost:5173.\n\n⚙️ Proxy server listening on port 6277\n🔍 MCP Inspector is up and running at http://127.0.0.1:6274\n\nWhen using HTTP transport, the server will be available at http://127.0.0.1:8080/mcp by default.\n\n---\n\n## Step 3: Test the Example Tool\n\nRun the example IP lookup tool from the CLI:\n\n```bash\n# Using CLI in development mode\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\"\n\n# Or with a specific IP\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\" --limit 10 --offset 0 --sort \"date:d\" --from_date \"2023-01-01\" --to_date \"2023-12-31\"\n```\n\n---\n\n# Architecture\n\nThis boilerplate follows a clean, layered architecture pattern that separates concerns and promotes maintainability.\n\n## Project Structure\n\n```\nsrc/\n├── cli/              # Command-line interfaces\n├── controllers/      # Business logic\n├── resources/        # MCP resources: expose data and content from your servers to LLMs\n├── services/         # External API interactions\n├── tools/            # MCP tool definitions\n├── types/            # Type definitions\n├── utils/            # Shared utilities\n└── index.ts          # Entry point\n```\n\n## Layers and Responsibilities\n\n### CLI Layer (`src/cli/*.cli.ts`)\n\n- **Purpose**: Define command-line interfaces that parse arguments and call controllers\n- **Naming**: Files should be named `<feature>.cli.ts`\n- **Testing**: CLI integration tests in `<feature>.cli.test.ts`\n\n### Tools Layer (`src/tools/*.tool.ts`)\n\n- **Purpose**: Define MCP tools with schemas and descriptions for AI assistants\n- **Naming**: Files should be named `<feature>.tool.ts` with types in `<feature>.types.ts`\n- **Pattern**: Each tool should use zod for argument validation\n\n### Controllers Layer (`src/controllers/*.controller.ts`)\n\n- **Purpose**: Implement business logic, handle errors, and format responses\n- **Naming**: Files should be named `<feature>.controller.ts`\n- **Pattern**: Should return standardized `ControllerResponse` objects\n\n### Services Layer (`src/services/*.service.ts`)\n\n- **Purpose**: Interact with external APIs or data sources\n- **Naming**: Files should be named `<feature>.service.ts`\n- **Pattern**: Pure API interactions with minimal logic\n\n### Utils Layer (`src/utils/*.util.ts`)\n\n- **Purpose**: Provide shared functionality across the application\n- **Key Utils**:\n    - `logger.util.ts`: Structured logging\n    - `error.util.ts`: Error handling and standardization\n    - `formatter.util.ts`: Markdown formatting helpers\n\n---\n\n# Development Guide\n\n## Development Scripts\n\n```bash\n# Start server in development mode (hot-reload & inspector)\nnpm run dev:server\n\n# Run CLI in development mode\nnpm run dev:cli -- [command] [args]\n\n# Build the project\nnpm run build\n\n# Start server in production mode\nnpm run start:server\n\n# Run CLI in production mode\nnpm run start:cli -- [command] [args]\n```\n\n## Testing\n\n```bash\n# Run all tests\nnpm test\n\n# Run specific tests\nnpm test -- src/path/to/test.ts\n\n# Generate test coverage report\nnpm run test:coverage\n```\n\n##  evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/tools/searchapi.tool.ts\n```\n\n## Code Quality\n\n```bash\n# Lint code\nnpm run lint\n\n# Format code with Prettier\nnpm run format\n\n# Check types\nnpm run typecheck\n```\n\n---\n\n# Building Custom Tools\n\nFollow these steps to add your own tools to the server:\n\n## 1. Define Service Layer\n\nCreate a new service in `src/services/` to interact with your external API:\n\n```typescript\n// src/services/example.service.ts\nimport { Logger } from '../utils/logger.util.js';\n\nconst logger = Logger.forContext('services/example.service.ts');\n\nexport async function getData(param: string): Promise<any> {\n\tlogger.debug('Getting data', { param });\n\t// API interaction code here\n\treturn { result: 'example data' };\n}\n```\n\n## 2. Create Controller\n\nAdd a controller in `src/controllers/` to handle business logic:\n\n```typescript\n// src/controllers/example.controller.ts\nimport { Logger } from '../utils/logger.util.js';\nimport * as exampleService from '../services/example.service.js';\nimport { formatMarkdown } from '../utils/formatter.util.js';\nimport { handleControllerError } from '../utils/error-handler.util.js';\nimport { ControllerResponse } from '../types/common.types.js';\n\nconst logger = Logger.forContext('controllers/example.controller.ts');\n\nexport interface GetDataOptions {\n\tparam?: string;\n}\n\nexport async function getData(\n\toptions: GetDataOptions = {},\n): Promise<ControllerResponse> {\n\ttry {\n\t\tlogger.debug('Getting data with options', options);\n\n\t\tconst data = await exampleService.getData(options.param || 'default');\n\n\t\tconst content = formatMarkdown(data);\n\n\t\treturn { content };\n\t} catch (error) {\n\t\tthrow handleControllerError(error, {\n\t\t\tentityType: 'ExampleData',\n\t\t\toperation: 'getData',\n\t\t\tsource: 'controllers/example.controller.ts',\n\t\t});\n\t}\n}\n```\n\n## 3. Implement MCP Tool\n\nCreate a tool definition in `src/tools/`:\n\n```typescript\n// src/tools/example.tool.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { z } from 'zod';\nimport { Logger } from '../utils/logger.util.js';\nimport { formatErrorForMcpTool } from '../utils/error.util.js';\nimport * as exampleController from '../controllers/example.controller.js';\n\nconst logger = Logger.forContext('tools/example.tool.ts');\n\nconst GetDataArgs = z.object({\n\tparam: z.string().optional().describe('Optional parameter'),\n});\n\ntype GetDataArgsType = z.infer<typeof GetDataArgs>;\n\nasync function handleGetData(args: GetDataArgsType) {\n\ttry {\n\t\tlogger.debug('Tool get_data called', args);\n\n\t\tconst result = await exampleController.getData({\n\t\t\tparam: args.param,\n\t\t});\n\n\t\treturn {\n\t\t\tcontent: [{ type: 'text' as const, text: result.content }],\n\t\t};\n\t} catch (error) {\n\t\tlogger.error('Tool get_data failed', error);\n\t\treturn formatErrorForMcpTool(error);\n\t}\n}\n\nexport function register(server: McpServer) {\n\tserver.tool(\n\t\t'get_data',\n\t\t`Gets data from the example API, optionally using \\`param\\`.\nUse this to fetch example data. Returns formatted data as Markdown.`,\n\t\tGetDataArgs.shape,\n\t\thandleGetData,\n\t);\n}\n```\n\n## 4. Add CLI Support\n\nCreate a CLI command in `src/cli/`:\n\n```typescript\n// src/cli/example.cli.ts\nimport { program } from 'commander';\nimport { Logger } from '../utils/logger.util.js';\nimport * as exampleController from '../controllers/example.controller.js';\nimport { handleCliError } from '../utils/error-handler.util.js';\n\nconst logger = Logger.forContext('cli/example.cli.ts');\n\nprogram\n\t.command('get-data')\n\t.description('Get example data')\n\t.option('--param <value>', 'Optional parameter')\n\t.action(async (options) => {\n\t\ttry {\n\t\t\tlogger.debug('CLI get-data called', options);\n\n\t\t\tconst result = await exampleController.getData({\n\t\t\t\tparam: options.param,\n\t\t\t});\n\n\t\t\tconsole.log(result.content);\n\t\t} catch (error) {\n\t\t\thandleCliError(error);\n\t\t}\n\t});\n```\n\n## 5. Register Components\n\nUpdate the entry points to register your new components:\n\n```typescript\n// In src/cli/index.ts\nimport '../cli/example.cli.js';\n\n// In src/index.ts (for the tool)\nimport exampleTool from './tools/example.tool.js';\n// Then in registerTools function:\nexampleTool.register(server);\n```\n\n---\n\n# Debugging Tools\n\n## MCP Inspector\n\nAccess the visual MCP Inspector to test your tools and view request/response details:\n\n1. Run `npm run dev:server`\n2. Open http://localhost:5173 in your browser\n3. Test your tools and view logs directly in the UI\n\n## Server Logs\n\nEnable debug logs for development:\n\n```bash\n# Set environment variable\nDEBUG=true npm run dev:server\n\n# Or configure in ~/.mcp/configs.json\n```\n\n---\n\n# Publishing Your MCP Server\n\nWhen ready to publish your custom MCP server:\n\n1. Update package.json with your details\n2. Update README.md with your tool documentation\n3. Build the project: `npm run build`\n4. Test the production build: `npm run start:server`\n5. Publish to npm: `npm publish`\n\n---\n\n# License\n\n[ISC License](https://opensource.org/licenses/ISC)\n\n```json\n{\n\t\"searchapi\": {\n\t\t\"environments\": {\n\t\t\t\"DEBUG\": \"true\",\n\t\t\t\"SEARCHAPI_API_KEY\": \"value\"\n\t\t}\n\t}\n}\n```\n\n**Note:** For backward compatibility, the server will also recognize configurations under the full package name (`searchapi-mcp-server`) or the unscoped package name (`searchapi-mcp-server`) if the `searchapi` key is not found. However, using the short `searchapi` key is recommended for new configurations.\n\n## Co-Authors\n\n- Claude Code (Claude AI Assistant)\n- Goon",
      "npm_url": "https://www.npmjs.com/package/searchapi-mcp-server",
      "npm_downloads": 154,
      "keywords": [
        "searchapi",
        "searches",
        "bing",
        "searchapi mcp",
        "mrgoonie searchapi",
        "ai assistants"
      ],
      "category": "web-search"
    },
    "mshk--mcp-rss-crawler": {
      "owner": "mshk",
      "name": "mcp-rss-crawler",
      "url": "https://github.com/mshk/mcp-rss-crawler",
      "imageUrl": "/freedevtools/mcp/pfp/mshk.webp",
      "description": "Fetch and manage RSS feeds with caching and filtering capabilities. Integrates seamlessly with LLMs to provide access to the latest articles from various sources.",
      "stars": 15,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T15:38:33Z",
      "readme_content": "# MCP-RSS-Crawler\n\nAn MCP (Message Chain Protocol) server that fetches RSS feeds and shares them with LLMs.\n\n## Features\n\n- Fetching and caching of RSS feeds (SQLite database)\n- MCP protocol implementation for seamless LLM integration\n- Support for filtering feeds by category, source, or keywords\n- Comprehensive API endpoints for feed management\n  - Add, update, and delete feeds\n- Support for fetching articles from Firecrawl\n\n## Requirements\n\n- Bun\n- Firecrawl API key\n- Claude Desktop or other MCP client\n\n## Setup as MCP Server\n\n1. Clone this repository\n2. Create a `claude_desktop_config.json` file based on `claude_desktop_config.json.example` with your configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"rss-crawler\": {\n      \"command\": \"/path/to/bun\",\n      \"args\": [\"run\", \"/path/to/mcp-rss-crawler/apps/mcp-server/src/mcp-cli.ts\"],\n      \"cwd\": \"/path/to/mcp-rss-crawler\",\n      \"env\": {\n        \"PORT\": \"5556\",\n        \"DB_DIR\": \"/path/to/mcp-rss-crawler\",\n        \"FIRECRAWL_API_KEY\": \"fc-<YOUR_FIRECRAWL_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n3. Install dependencies:\n   ```\n   bun install\n   ```\n4. Start Claude Desktop:\n\n## MCP Protocol\n\nThe server implements the Message Chain Protocol (MCP) which allows LLMs to access your latest RSS feeds. The MCP endpoint accepts POST requests with a JSON body containing a messages array and returns a response with the latest feed items.\n\nExample request:\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What are the latest news from my RSS feeds?\"\n    }\n  ]\n}\n```\n\nExample response:\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Here are the latest articles from your RSS feeds:\",\n      \"name\": \"rss-mcp\"\n    },\n    {\n      \"role\": \"tool\",\n      \"content\": \"[{\\\"title\\\":\\\"Article Title\\\",\\\"summary\\\":\\\"Article summary...\\\",\\\"published\\\":\\\"2025-03-16T04:30:00.000Z\\\",\\\"origin\\\":\\\"Feed Name\\\",\\\"link\\\":\\\"https://example.com/article\\\"}]\",\n      \"name\": \"rss-feeds\"\n    }\n  ]\n}\n```\n\n## Configuration Options\n\nThe server can be configured through environment variables or a `.env` file:\n\n- `PORT` - Server port (default: 5556)\n- `FIRECRAWL_API_KEY` - Firecrawl API key\n- `DB_DIR` - Database directory (default: `~/.mcp-rss-crawler`)\n\n## Troubleshooting\n\n- For connection issues, check your network settings and firewall configuration\n- Logs are available in the console and can be used to diagnose problems\n- For more detailed logging, set the `DEBUG=mcp-rss:*` environment variable\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rss",
        "feeds",
        "search",
        "rss crawler",
        "rss feeds",
        "manage rss"
      ],
      "category": "web-search"
    },
    "mshojaei77--ReActMCP": {
      "owner": "mshojaei77",
      "name": "ReActMCP",
      "url": "https://github.com/mshojaei77/ReActMCP",
      "imageUrl": "/freedevtools/mcp/pfp/mshojaei77.webp",
      "description": "Integrates real-time web search capabilities into AI assistants, performing basic and advanced searches and returning markdown-formatted results including titles, URLs, publication dates, and content summaries.",
      "stars": 141,
      "forks": 23,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T22:54:50Z",
      "readme_content": "# ReActMCP Web Search\n\nReActMCP Web Search is an MCP (Model Context Protocol) server that integrates web search capabilities into your AI assistant framework. It leverages the Exa API to perform both basic and advanced web searches, returning real-time, markdown-formatted results including titles, URLs, publication dates, and content summaries.\n\nThis repository is part of the broader ReActMCP project that connects various MCP tools and servers to empower your AI assistant with a wide range of capabilities.\n\n---\n\n## Table of Contents\n\n- [Features](#features)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Configuration](#configuration)\n  - [Environment Variables](#environment-variables)\n  - [MCP Configuration](#mcp-configuration)\n  - [System Prompt](#system-prompt)\n- [Usage](#usage)\n  - [Running the Web Search Server](#running-the-web-search-server)\n  - [Testing the Tools](#testing-the-tools)\n- [Troubleshooting](#troubleshooting)\n- [License](#license)\n- [Contributing](#contributing)\n\n---\n\n## Features\n\n- **Basic Web Search**: Perform simple searches using the Exa API.\n- **Advanced Web Search**: Use additional filtering options such as domain restrictions, text inclusion requirements, and date filters.\n- **Markdown Output**: Format search results in Markdown to easily incorporate titles, URLs, and summaries.\n- **MCP Integration**: Easily add this tool into your MCP server ecosystem for multi-tool AI assistance.\n\n---\n## Requirements\n\n- **Python 3.8+**\n- [python-dotenv](https://pypi.org/project/python-dotenv/)\n- [exa_py](https://github.com/your-org/exa_py) (Exa API client)\n- Other dependencies that may be required by your MCP framework\n\n---\n\n## Installation\n\n1. **Clone the Repository**\n\n   ```bash\n   git clone https://github.com/mshojaei77/ReActMCP.git\n   cd ReActMCP\n   ```\n\n2. **Create a Virtual Environment (Optional but recommended)**\n\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n   ```\n\n3. **Install Dependencies**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n---\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file in the project root directory with at least the following variable:\n\n```env\nEXA_API_KEY=your_exa_api_key_here\nOPENAI_API_KEY=...\n```\n\nThis key is required by the Exa API for performing web searches.\n\n### MCP Configuration\n\nThe MCP configuration file `mcp_config.json` defines the settings and tools available to your MCP server. An example configuration is provided:\n\n```json\n{\n  \"websearch\": {\n    \"script\": \"web_search.py\",\n    \"encoding_error_handler\": \"ignore\",\n    \"description\": \"Web search capability using Exa API that provides real-time internet search results. Supports both basic and advanced search with filtering options including domain restrictions, text inclusion requirements, and date filtering. Returns formatted results with titles, URLs, publication dates, and content summaries.\",\n    \"required_env_vars\": [\"EXA_API_KEY\"],\n    \"active\": true\n  },\n  \"settings\": {\n    \"model\": \"gpt-4o\",\n    \"system_prompt_path\": \"system_prompt.txt\"\n  }\n}\n```\n\nYou can personalize or extend this configuration by modifying parameters such as default number of results or adding new MCP tools.\n\n### System Prompt\n\nThe `system_prompt.txt` file configures the behavior and tone of your AI assistant. It guides responses to be friendly, engaging, and informative, with the inclusion of emojis. An example prompt is provided:\n\n```text\nYou are a helpful, knowledgeable AI assistant with web search capabilities. Your goal is to provide accurate, comprehensive, and up-to-date information to users.\nUse lots of emojis and make your responses fun and engaging.\n\n## Available Search Tools\n\n- `search_web`: Basic web search that returns results based on a query\n- `advanced_search_web`: Advanced search with filtering options for domains, required text, and date ranges\n\n## Guidelines for Responding to Questions\n\n1. For current information or facts that might have changed since your training data, use the appropriate search tool to find the most recent and relevant information.\n\n2. Use `search_web` for general queries and `advanced_search_web` with appropriate filters for more specific needs.\n\n3. Formulate precise search queries to maximize result relevance.\n\n4. For recent information, use the `max_age_days` parameter in advanced search to limit results to recent publications.\n\n5. When targeting specific sources, use the `include_domains` parameter to focus your search.\n\n6. Cite sources by including URLs from search results.\n\n7. For insufficient or contradictory results, acknowledge limitations and explain findings.\n\n8. Break down complex topics into organized sections.\n\n9. Provide balanced perspectives on controversial topics.\n\n10. Be transparent about uncertainty rather than making up information.\n\n11. Maintain a helpful, informative, and conversational tone.\n\n## Response Quality Standards\n\nYour responses should be well-structured, factually accurate, and tailored to the user's level of understanding on the topic. Use the web search capabilities as your primary tools for accessing current information before responding to time-sensitive or factual queries.\n```\n\nFeel free to adjust the system prompt to align with your desired assistant behavior.\n\n---\n\n## Usage\n\n### Running the Web Search Server\n\nThe MCP servers is implemented in `servers` directory. To run a server, simply execute it :\n\n```bash\npython servers/web_search.py\n```\n\nThis command will start the MCP server which listens for requests and exposes the following tools:\n\n- **search_web**: Perform basic web searches.\n- **advanced_search_web**: Perform advanced web searches with filtering options.\n\n### Testing the Tools\n\nWithin `web_search.py`, a test function `test_search()` is provided (currently commented out) to demonstrate basic usage of the search capabilities. You can run this test by uncommenting the test execution block and using Python's asyncio runner:\n\n```python\nif __name__ == \"__main__\":\n    import asyncio\n    # Uncomment the following line to perform a test search\n    # asyncio.run(test_search())\n    mcp.run()\n```\n\nThis will print search results for sample queries and help you verify that the tool is functioning as expected.\n\n---\n## Claude Desktop Configuration:\nConfigure Claude Desktop to use this server by adding the following to your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"websearch\": {\n         \"command\": \"python\",\n         \"args\": [\"path/to/servers/exa_web_search.py\"]\n       }\n  }\n}\n```\n\n## Troubleshooting\n\n- **Missing EXA_API_KEY:** Ensure that the `.env` file is properly set up with your valid Exa API key.\n- **Dependency Issues:** Verify that all necessary Python packages are installed (check your `requirements.txt` file). Reinstall packages if needed.\n- **API Errors:** If you encounter errors during web searches, check your network connection and verify the Exa API status.\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---\n\n## Contributing\n\nContributions are welcome! If you have suggestions, bug fixes, or improvements, please open an issue or submit a pull request.\n\nHappy coding and enjoy building your personalized, multi-tool AI assistant with ReActMCP Web Search! 🚀😊\n\n## Star History\n\n<a href=\"https://www.star-history.com/#mshojaei77/ReActMCP&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date\" />\n </picture>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "reactmcp",
        "searches",
        "search",
        "mshojaei77 reactmcp",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "mugoosse--sitemap-mcp-server": {
      "owner": "mugoosse",
      "name": "sitemap-mcp-server",
      "url": "https://github.com/mugoosse/sitemap-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mugoosse.webp",
      "description": "Fetch and analyze website sitemaps to provide insights into site structure, uncover hidden pages, and extract organized hierarchies. Supports standard sitemap formats and includes prompt templates for tasks like health checks and content gap identification.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-30T06:28:14Z",
      "readme_content": "# Sitemap MCP Server\n\nDiscover website architecture and analyze site structure by fetching, parsing, and visualizing sitemaps from any URL. Uncover hidden pages and extract organized hierarchies without manual exploration.\n\nIncludes ready-to-use prompt templates for Claude Desktop that let you analyze websites, check sitemap health, extract URLs, find missing content, and create visualizations with just a URL input.\n\n![License](https://img.shields.io/github/license/mugoosse/sitemap-mcp-server)\n![PyPI](https://img.shields.io/pypi/v/sitemap-mcp-server)\n![Python Version](https://img.shields.io/badge/python-3.11+-blue)\n![Status](https://img.shields.io/badge/status-active-brightgreen.svg)\n[![smithery badge](https://smithery.ai/badge/@mugoosse/sitemap)](https://smithery.ai/server/@mugoosse/sitemap)\n\n<a href=\"https://glama.ai/mcp/servers/@mugoosse/sitemap-mcp-server\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mugoosse/sitemap-mcp-server/badge\" />\n</a>\n\n## Demo\n\nGet answers to questions about any website leveraging the power of sitemaps.\n\n<details><summary>Cursor: how many pages does a modelcontextprotocol.io have?</summary>\n\n<br/>\n\n<img width=\"1541\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f234b35f-ccb2-44c6-8ce5-71a6d2531e43\" />\n\n</details>\n\n<details><summary>Claude + prompt: visualize the sitemap in a diagram of windsurf.com</summary>\n\n<br/>\n\nClick on the \"attach\" button next to the tools button:\n\n![image](https://github.com/user-attachments/assets/e5b558c7-85fa-4b8f-b108-6d66d3b20719)\n\nThen select `visualize_sitemap`:\n\n<img width=\"558\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2106e720-52c3-48a3-aa64-fd9c0a6ab075\" />\n\nNow we enter windsurf.com:\n\n![image](https://github.com/user-attachments/assets/ceddad25-3549-4a2f-a053-f54c1154912b)\n\nAnd we get a visualization of teh sitemap:\n\n<img width=\"1470\" alt=\"image\" src=\"https://github.com/user-attachments/assets/04464315-e619-4df5-8082-a981e6437da9\" />\n\n</details>\n\n## Installation\n\nMake sure [uv](https://docs.astral.sh/uv/getting-started/installation/) is installed.\n\n### Installing in Claude Desktop, Cursor or Windsurf\n\nAdd this entry to your [claude_desktop_config.json](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server), Cursor settings, etc.:\n\n```json\n{\n  \"mcpServers\": {\n    \"sitemap\": {\n      \"command\": \"uvx\",\n      \"args\": [\"sitemap-mcp-server\"],\n      \"env\": { \"TRANSPORT\": \"stdio\" }\n    }\n  }\n}\n```\n\nRestart Claude if it's running. For Cursor simply press refresh and/or enable the MCP Server in the settings.\n\n### Installing via Smithery\n\nTo install sitemap for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mugoosse/sitemap):\n\n```bash\nnpx -y @smithery/cli install @mugoosse/sitemap --client claude\n```\n\n### MCP Inspector\n\n<details><summary>uv + stdio transport</summary>\n\n```bash\nnpx @modelcontextprotocol/inspector env TRANSPORT=stdio uvx sitemap-mcp-server\n```\n\nOpen the MCP Inspector at http://127.0.0.1:6274, select `stdio` transport, and connect to the MCP server.\n\n</details>\n\n<details><summary>uv + sse transport</summary>\n\n```bash\n# Start the server\nuvx sitemap-mcp-server\n\n# Start the MCP Inspector in a separate terminal\nnpx @modelcontextprotocol/inspector connect http://127.0.0.1:8050\n```\n\nOpen the MCP Inspector at http://127.0.0.1:6274, select `sse` transport, and connect to the MCP server.\n\n</details>\n\n### SSE Transport\n\nIf you want to use the SSE transport, follow these steps:\n\n1. Start the server:\n\n```bash\nuvx sitemap-mcp-server\n```\n\n2. Configure your MCP Client, e.g. Cursor:\n\n```json\n{\n  \"mcpServers\": {\n    \"sitemap\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8050/sse\"\n    }\n  }\n}\n```\n\n### Local Development\n\nFor instructions on building and running the project from source, please refer to the [DEVELOPERS.md](DEVELOPERS.md) guide.\n\n## Usage\n\n### Tools\n\nThe following tools are available via the MCP server:\n\n- **get_sitemap_tree** - Fetch and parse the sitemap tree from a website URL\n\n  - Arguments: `url` (website URL), `include_pages` (optional, boolean)\n  - Returns: JSON representation of the sitemap tree structure\n\n- **get_sitemap_pages** - Get all pages from a website's sitemap with filtering options\n\n  - Arguments: `url` (website URL), `limit` (optional), `include_metadata` (optional), `route` (optional), `sitemap_url` (optional), `cursor` (optional)\n  - Returns: JSON list of pages with pagination metadata\n\n- **get_sitemap_stats** - Get statistics about a website's sitemap\n\n  - Arguments: `url` (website URL)\n  - Returns: JSON object with sitemap statistics including page counts, modification dates, and subsitemap details\n\n- **parse_sitemap_content** - Parse a sitemap directly from its XML or text content\n  - Arguments: `content` (sitemap XML content), `include_pages` (optional, boolean)\n  - Returns: JSON representation of the parsed sitemap\n\n### Prompts\n\nThe server includes ready-to-use prompts that appear as templates in Claude Desktop. After installing the server, you'll see these templates in the \"Templates\" menu (click the + icon next to the message input):\n\n- **Analyze Sitemap**: Provides comprehensive structure analysis of a website's sitemap\n- **Check Sitemap Health**: Evaluates SEO and health metrics of a sitemap\n- **Extract URLs from Sitemap**: Extracts and filters specific URLs from a sitemap\n- **Find Missing Content in Sitemap**: Identifies content gaps in a website's sitemap\n- **Visualize Sitemap Structure**: Creates a Mermaid.js diagram visualization of sitemap structure\n\nTo use these prompts:\n\n1. Click the + icon next to the message input in Claude Desktop\n2. Select the desired template from the list\n3. Fill in the website URL when prompted\n4. Claude will execute the appropriate sitemap analysis\n\n### Examples\n\n#### Fetch a Complete Sitemap\n\n```json\n{\n  \"name\": \"get_sitemap_tree\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"include_pages\": true\n  }\n}\n```\n\n#### Get Pages with Filtering and Pagination\n\n##### Filter by Route\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 100,\n    \"include_metadata\": true,\n    \"route\": \"/blog/\"\n  }\n}\n```\n\n##### Filter by Specific Subsitemap\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 100,\n    \"include_metadata\": true,\n    \"sitemap_url\": \"https://example.com/blog-sitemap.xml\"\n  }\n}\n```\n\n##### Cursor-Based Pagination\n\nThe server implements MCP cursor-based pagination to handle large sitemaps efficiently:\n\n**Initial Request:**\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 50\n  }\n}\n```\n\n**Response with Pagination:**\n\n```json\n{\n  \"base_url\": \"https://example.com\",\n  \"pages\": [...],  // First batch of pages\n  \"limit\": 50,\n  \"nextCursor\": \"eyJwYWdlIjoxfQ==\"\n}\n```\n\n**Subsequent Request with Cursor:**\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 50,\n    \"cursor\": \"eyJwYWdlIjoxfQ==\"\n  }\n}\n```\n\nWhen there are no more results, the `nextCursor` field will be absent from the response.\n\n#### Get Sitemap Statistics\n\n```json\n{\n  \"name\": \"get_sitemap_stats\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\nThe response includes both total statistics and detailed stats for each subsitemap:\n\n```json\n{\n  \"total\": {\n    \"url\": \"https://example.com\",\n    \"page_count\": 150,\n    \"sitemap_count\": 3,\n    \"sitemap_types\": [\"WebsiteSitemap\", \"NewsSitemap\"],\n    \"priority_stats\": {\n      \"min\": 0.1,\n      \"max\": 1.0,\n      \"avg\": 0.65\n    },\n    \"last_modified_count\": 120\n  },\n  \"subsitemaps\": [\n    {\n      \"url\": \"https://example.com/sitemap.xml\",\n      \"type\": \"WebsiteSitemap\",\n      \"page_count\": 100,\n      \"priority_stats\": {\n        \"min\": 0.3,\n        \"max\": 1.0,\n        \"avg\": 0.7\n      },\n      \"last_modified_count\": 80\n    },\n    {\n      \"url\": \"https://example.com/blog/sitemap.xml\",\n      \"type\": \"WebsiteSitemap\",\n      \"page_count\": 50,\n      \"priority_stats\": {\n        \"min\": 0.1,\n        \"max\": 0.9,\n        \"avg\": 0.5\n      },\n      \"last_modified_count\": 40\n    }\n  ]\n}\n```\n\nThis allows MCP clients to understand which subsitemaps might be of interest for further investigation. You can then use the `sitemap_url` parameter in `get_sitemap_pages` to filter pages from a specific subsitemap.\n\n#### Parse Sitemap Content Directly\n\n```json\n{\n  \"name\": \"parse_sitemap_content\",\n  \"arguments\": {\n    \"content\": \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><urlset xmlns=\\\"http://www.sitemaps.org/schemas/sitemap/0.9\\\"><url><loc>https://example.com/</loc></url></urlset>\",\n    \"include_pages\": true\n  }\n}\n```\n\n## Acknowledgements\n\n- This MCP Server leverages the [ultimate-sitemap-parser](https://github.com/GateNLP/ultimate-sitemap-parser) library\n- Built using the [Model Context Protocol](https://modelcontextprotocol.io) Python SDK\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sitemaps",
        "sitemap",
        "mugoosse",
        "mugoosse sitemap",
        "website sitemaps",
        "sitemaps provide"
      ],
      "category": "web-search"
    },
    "mzxrai--mcp-webresearch": {
      "owner": "mzxrai",
      "name": "mcp-webresearch",
      "url": "https://github.com/mzxrai/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/mzxrai.webp",
      "description": "Integrates real-time web research capabilities by utilizing Google search for obtaining information and extracting content from webpages. It allows for tracking research sessions, including visited pages and search queries, while also enabling screenshot captures.",
      "stars": 281,
      "forks": 69,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T21:03:39Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "https://www.npmjs.com/package/@mzxrai/mcp-webresearch",
      "npm_downloads": 40957,
      "keywords": [
        "webresearch",
        "mzxrai",
        "search",
        "mcp webresearch",
        "webresearch integrates",
        "web search"
      ],
      "category": "web-search"
    },
    "nachoal--perplexity-mcp": {
      "owner": "nachoal",
      "name": "perplexity-mcp",
      "url": "https://github.com/nachoal/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nachoal.webp",
      "description": "Web search functionality using the Perplexity API to retrieve up-to-date information with comprehensive results, including sources and citations, while allowing filtering by time periods.",
      "stars": 8,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-16T10:12:32Z",
      "readme_content": "# Perplexity Web Search MCP Server\n\nA simple MCP (Model Context Protocol) server that provides web search functionality using the Perplexity API. This server allows Claude or other MCP-compatible AI assistants to search the web and get up-to-date information.\n\n## Features\n\n- Search the web with Perplexity's powerful search capabilities\n- Get comprehensive search results with sources and citations\n- Filter results by time period (day, week, month, year)\n- Includes a ready-to-use prompt template for web searches\n- Supports loading API key from environment variables or .env file\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pip install -e .\n   ```\n   or\n   ```\n   uv pip install -e .\n   ```\n\n## Configuration\n\nYou can set the `PERPLEXITY_API_KEY` environment variable with your Perplexity API key:\n\n```bash\nexport PERPLEXITY_API_KEY=\"your-api-key-here\"\n```\n\nAlternatively, you can create a `.env` file in the project root with the following content:\n\n```\nPERPLEXITY_API_KEY=your-api-key-here\n```\n\nA sample `.env.example` file is provided for reference.\n\nTo get a Perplexity API key:\n1. Visit [Perplexity API Settings](https://www.perplexity.ai/settings/api)\n2. Create an account if you don't have one\n3. Generate an API key\n\n## Usage\n\n### Running the server\n\n```bash\npython server.py\n```\n\n### Testing the server\n\nYou can test the server functionality without running the full MCP server using the included test script:\n\n```bash\npython test_server.py \"your search query here\" --recency month\n```\n\nOptions for `--recency` are: day, week, month (default), year\n\n### Using with Claude Desktop\n\n1. Edit your Claude Desktop configuration file:\n   - On macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following configuration:\n   ```json\n   {\n     \"perplexity-mcp\": {\n       \"env\": {\n         \"PERPLEXITY_API_KEY\": \"your-api-key-here\"\n       },\n       \"command\": \"python\",\n       \"args\": [\n         \"/path/to/server.py\"\n       ]\n     }\n   }\n   ```\n\n3. Restart Claude Desktop\n\n### Example Prompts for Claude\n\n- \"Search the web for the latest news about artificial intelligence\"\n- \"Use Perplexity to find information about climate change published in the past week\"\n- \"Search for recent research papers on quantum computing from the past month\"\n\n## API Reference\n\n### Tool: `search_web(query: str, recency: str = \"month\") -> str`\n\nSearch the web using Perplexity API and return results.\n\n**Parameters:**\n- `query`: The search query string\n- `recency`: Filter results by time period - 'day', 'week', 'month' (default), or 'year'\n\n**Returns:**\nA comprehensive text response containing:\n1. A detailed summary of the search results\n2. Key facts and information found\n3. Sources with URLs for verification\n4. Any conflicting information if present\n\n### Prompt: `web_search_prompt(query: str, recency: str = \"month\") -> str`\n\nCreates a prompt template for searching the web with Perplexity.\n\n**Parameters:**\n- `query`: The search query\n- `recency`: Time period filter - 'day', 'week', 'month' (default), or 'year'\n\n**Returns:**\nA formatted prompt string that instructs the AI to:\n1. Search for the specified query\n2. Focus on results from the specified time period\n3. Summarize key findings\n4. Highlight important facts\n5. Mention conflicting information\n6. Cite sources with links\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "perplexity",
        "comprehensive",
        "perplexity api",
        "web search",
        "search functionality"
      ],
      "category": "web-search"
    },
    "narphorium--mcp-memex": {
      "owner": "narphorium",
      "name": "mcp-memex",
      "url": "https://github.com/narphorium/mcp-memex",
      "imageUrl": "/freedevtools/mcp/pfp/narphorium.webp",
      "description": "Analyze web content and enhance your knowledge base by extracting information from URLs, storing it as Markdown files for easy access. Integrates seamlessly with Obsidian to facilitate questioning and retrieval of insights from the curated content.",
      "stars": 9,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-05T23:05:46Z",
      "readme_content": "# Memex for Model Context Protocol\n\nMemex is a tool for Model Context Protocol (MCP) that allows you to analyze web content and add it to your knowledge base.\n\nThe tool was inspired by the [Memex](docs/as_we_may_think.pdf) project by [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush).\n\n## Requirements\n\nYou will need API keys for the following services:\n\n- [Claude API](https://www.anthropic.com/en/claude)\n- [FireCrawl API](https://www.firecrawl.com/)\n- [Voyage API](https://voyageai.com/)\n\nThe knowledge base produced by this tool is stored as Markdown files so they can be viewed with any Markdown viewer but [Obsidian](https://obsidian.md/) is recommended.\n\n## Installation\n\n```bash\npip install mcp-memex\n```\n\nAdd the following to your `claude_desktop_config.json` and replace the placeholders with the actual paths and API keys:\n\n```json\n{\n  \"mcpServers\": {\n    \"memex\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"PATH_TO_LOCAL_MEMEX_REPO\",\n        \"run\",\n        \"mcp-memex\",\n        \"--index\",\n        \"PATH_TO_MEMEX_INDEX\",\n        \"--workspace\",\n        \"PATH_TO_OBSIDIAN_VAULT\"\n      ],\n      \"env\": {\n        \"ANTHROPIC_API_KEY\": \"YOUR-API-KEY\",\n        \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\",\n        \"VOYAGE_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nStart by asking Claude a question with a list of URLs to reference.\n\n```\nWhat is the capital of France? \"https://en.wikipedia.org/wiki/France\"\n```\n\nOnce Claude has finished analyzing the content, you will see the results in your Obsidian vault. You can then ask questions about the content and Memex will use the knowledge base to answer your questions.\n\n```\nWhat is the capital of France?\n```\n\n## Development\n\nTo run the tool locally, you can use the following command:\n\n```bash\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory PATH_TO_LOCAL_MEMEX_REPO \\\n  run \\\n  mcp-memex \\\n  --index PATH_TO_MEMEX_INDEX \\\n  --workspace PATH_TO_OBSIDIAN_VAULT\n```\n\nThen open the inspector and connect to the server.\n\nhttp://localhost:5173?timeout=30000",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memex",
        "narphorium",
        "retrieval",
        "search narphorium",
        "analyze web",
        "memex analyze"
      ],
      "category": "web-search"
    },
    "neoparad--serp-api-server": {
      "owner": "neoparad",
      "name": "serp-api-server",
      "url": "https://github.com/neoparad/serp-api-server",
      "imageUrl": "/freedevtools/mcp/pfp/neoparad.webp",
      "description": "Access real-time search engine results and data through a straightforward API. Retrieve and analyze search data from various search engines to enhance applications with powerful search capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-21T22:39:45Z",
      "readme_content": "# serp-api-server",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "api",
        "search",
        "serp",
        "serp api",
        "search engine",
        "search neoparad"
      ],
      "category": "web-search"
    },
    "nickclyde--duckduckgo-mcp-server": {
      "owner": "nickclyde",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/nickclyde/duckduckgo-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/nickclyde.webp",
      "description": "Facilitates web search capabilities via DuckDuckGo, offering content fetching and parsing with a focus on formatting results for large language model integration. Includes advanced rate limiting features for both search requests and content retrieval.",
      "stars": 510,
      "forks": 110,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T17:09:47Z",
      "readme_content": "# DuckDuckGo Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nickclyde/duckduckgo-mcp-server)](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search capabilities through DuckDuckGo, with additional features for content fetching and parsing.\n\n<a href=\"https://glama.ai/mcp/servers/phcus2gcpn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/phcus2gcpn/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Search DuckDuckGo with advanced rate limiting and result formatting\n- **Content Fetching**: Retrieve and parse webpage content with intelligent text extraction\n- **Rate Limiting**: Built-in protection against rate limits for both search and content fetching\n- **Error Handling**: Comprehensive error handling and logging\n- **LLM-Friendly Output**: Results formatted specifically for large language model consumption\n\n## Installation\n\n### Installing via Smithery\n\nTo install DuckDuckGo Search Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @nickclyde/duckduckgo-mcp-server --client claude\n```\n\n### Installing via `uv`\n\nInstall directly from PyPI using `uv`:\n\n```bash\nuv pip install duckduckgo-mcp-server\n```\n\n## Usage\n\n### Running with Claude Desktop\n\n1. Download [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"ddg-search\": {\n            \"command\": \"uvx\",\n            \"args\": [\"duckduckgo-mcp-server\"]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop\n\n### Development\n\nFor local development, you can use the MCP CLI:\n\n```bash\n# Run with the MCP Inspector\nmcp dev server.py\n\n# Install locally for testing with Claude Desktop\nmcp install server.py\n```\n## Available Tools\n\n### 1. Search Tool\n\n```python\nasync def search(query: str, max_results: int = 10) -> str\n```\n\nPerforms a web search on DuckDuckGo and returns formatted results.\n\n**Parameters:**\n- `query`: Search query string\n- `max_results`: Maximum number of results to return (default: 10)\n\n**Returns:**\nFormatted string containing search results with titles, URLs, and snippets.\n\n### 2. Content Fetching Tool\n\n```python\nasync def fetch_content(url: str) -> str\n```\n\nFetches and parses content from a webpage.\n\n**Parameters:**\n- `url`: The webpage URL to fetch content from\n\n**Returns:**\nCleaned and formatted text content from the webpage.\n\n## Features in Detail\n\n### Rate Limiting\n\n- Search: Limited to 30 requests per minute\n- Content Fetching: Limited to 20 requests per minute\n- Automatic queue management and wait times\n\n### Result Processing\n\n- Removes ads and irrelevant content\n- Cleans up DuckDuckGo redirect URLs\n- Formats results for optimal LLM consumption\n- Truncates long content appropriately\n\n### Error Handling\n\n- Comprehensive error catching and reporting\n- Detailed logging through MCP context\n- Graceful degradation on rate limits or timeouts\n\n## Contributing\n\nIssues and pull requests are welcome! Some areas for potential improvement:\n\n- Additional search parameters (region, language, etc.)\n- Enhanced content parsing options\n- Caching layer for frequently accessed content\n- Additional rate limiting strategies\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "https://www.npmjs.com/package/duckduckgo-mcp-server",
      "npm_downloads": 10136,
      "keywords": [
        "duckduckgo",
        "retrieval",
        "search",
        "web search",
        "content retrieval",
        "duckduckgo mcp"
      ],
      "category": "web-search"
    },
    "nitish-raj--searxng-mcp-bridge": {
      "owner": "nitish-raj",
      "name": "searxng-mcp-bridge",
      "url": "https://github.com/nitish-raj/searxng-mcp-bridge",
      "imageUrl": "/freedevtools/mcp/pfp/nitish-raj.webp",
      "description": "Connect to a SearXNG instance to perform search queries via the Model Context Protocol (MCP), facilitating integration with compatible AI clients.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T05:03:38Z",
      "readme_content": "# SearXNG MCP Bridge Server\n\n[![Release](https://github.com/nitish-raj/searxng-mcp-bridge/actions/workflows/release.yml/badge.svg)](https://github.com/nitish-raj/searxng-mcp-bridge/actions/workflows/release.yml)\n[![smithery badge](https://smithery.ai/badge/@nitish-raj/searxng-mcp-bridge)](https://smithery.ai/server/@nitish-raj/searxng-mcp-bridge)\n\nThis is a Model Context Protocol (MCP) server that acts as a bridge to a [SearXNG](https://github.com/searxng/searxng) instance. It allows compatible clients to perform searches using a configured SearXNG instance via MCP tools.\n\n## Quick Start (Using from npm)\n\n1. **Set up a SearXNG instance**:\n   ```bash\n   # Using Docker\n   docker run -d -p 8888:8080 --name searxng searxng/searxng\n   ```\n\n2. **Install and run the MCP bridge**\n\n   Default (STDIO, unchanged):\n   ```bash\n   # Run directly with npx (default - stdio transport)\n   npx -y @nitish-raj/searxng-mcp-bridge\n   ```\n\n   Optional: Run as an HTTP server (new, opt-in)\n   ```bash\n    # Using env variables (recommended)\n     TRANSPORT=http PORT=3002 HOST=127.0.0.1 SEARXNG_INSTANCE_URL=http://localhost:8080 npx -y @nitish-raj/searxng-mcp-bridge\n\n   # Or use the CLI flag form\n   npx -y @nitish-raj/searxng-mcp-bridge --transport=http\n\n   # Or run the built bundle\n   TRANSPORT=http node build/index.js\n   ```\n\n3. **Configure in your MCP settings file** (stdio / legacy clients)\n   Add to your MCP settings file (e.g., `~/.vscode-server/.../mcp_settings.json`):\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxng-bridge\": {\n         \"command\": \"npx\",\n         \"args\": [\n          \"-y\",\n          \"@nitish-raj/searxng-mcp-bridge\"\n          ],\n         \"env\": {\n           \"SEARXNG_INSTANCE_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n         },\n         \"disabled\": false\n       }\n     }\n   }\n   ```\n\n**Smithery Configuration**: When using Smithery, you can set `transport: \"http\"` in the Smithery config to run the bridge over HTTP instead of stdio. Smithery will set `TRANSPORT` in the process environment when launching the bridge.\n\n## Features\n\n* **Search Tool**: Perform web searches using SearXNG with configurable parameters\n* **Health Check**: Monitor SearXNG instance connectivity and performance\n* **Dual Transport**: Supports both STDIO (default) and HTTP transports\n* **Session Management**: HTTP transport includes session-based connections\n* **CORS Support**: Proper cross-origin headers for web client integration\n* **Rate Limiting**: Built-in protection against excessive requests (HTTP mode)\n\n## Configuration\n\n- `SEARXNG_INSTANCE_URL` — REQUIRED. The full URL of the SearXNG instance (e.g., `http://localhost:8080`).\n - `TRANSPORT` — Transport protocol: `stdio` (default) or `http`\n - `PORT` — HTTP server port. Default: `3000` (use `3002` for development, Smithery uses `8081`)\n - `HOST` — Server bind address. Default: `127.0.0.1` (use `0.0.0.0` for containers)\n - `CORS_ORIGIN` — Comma-separated list of allowed origins for CORS. Default: localhost:3002 (development) or `*` (production for Smithery)\n - `MCP_HTTP_BEARER` — Optional bearer token for HTTP authentication\n **HTTP Transport Features**:\n- Session management with `mcp-session-id` headers\n- Secure CORS with origin whitelist validation\n- Rate limiting (100 requests/minute per IP)\n- Optional bearer authentication via `MCP_HTTP_BEARER`\n- DNS rebinding protection\n\n**Security Notes**:\n- CORS uses secure whitelist in development (localhost:3002 only)\n- Production reflects specific origins for credentialed requests (CORS-compliant)\n- Set `CORS_ORIGIN` to customize allowed origins for your use case\n- Set `TRANSPORT=stdio` to revert to stdio mode\n\n## HTTP Transport\n\nThe HTTP transport implements the MCP Streamable HTTP specification (2025-03-26) with the following endpoints:\n\n**MCP Endpoints**:\n- `POST /mcp` - Send MCP requests\n- `GET /mcp` - Server-Sent Events for notifications  \n- `DELETE /mcp` - Terminate sessions\n- `OPTIONS /mcp` - CORS preflight requests\n\n**System Endpoints**:\n- `GET /healthz` - Health check and status\n\n**Test HTTP endpoint**:\n```bash\ncurl -X POST http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}'\n```\n\nThis returns a JSON-RPC response with the list of available tools (`search` and `health_check`).\n\n## Using Smithery\n\nSmithery provides container deployment with automatic HTTP transport configuration. Install via Smithery:\n\n```bash\nnpx -y @smithery/cli install @nitish-raj/searxng-mcp-bridge --client claude\n```\n\nSmithery automatically handles:\n- Container deployment with HTTP transport\n- Port configuration (uses 8081 by default)\n- Environment variable management\n- Secure CORS configuration for web clients\n\n## Docker\n\nThe Dockerfile exposes port `8081` for Smithery compatibility (HTTP transport). To run the container and allow HTTP access:\n```bash\n# Build (example)\ndocker build -t searxng-mcp-bridge .\n\n# Run mapping port 8081 (Smithery default)\n docker run -d -p 8081:8081 --env SEARXNG_INSTANCE_URL=http://localhost:8080 --name searxng-mcp-bridge searxng-mcp-bridge\n\n# To run HTTP transport inside container:\n docker run -d -p 8081:8081 -e TRANSPORT=http -e PORT=8081 -e SEARXNG_INSTANCE_URL=http://localhost:8080 searxng-mcp-bridge\n```\n\nNote: when containerized set `HOST=0.0.0.0` or rely on the default exposed port mapping. Port 8081 is used for Smithery deployment compatibility.\n\n## Usage\n\n**STDIO Clients**: Use the tool unchanged - no configuration changes required.\n\n**HTTP Clients**: Connect to `http://localhost:3002/mcp` (development port) and send MCP JSON-RPC requests.\n\n**Smithery**: Smithery handles all configuration automatically.\n\n## Development\n\n* `npm install`: Install dependencies.\n* `npm run build`: Compile TypeScript to JavaScript.\n* `npm run watch`: Watch for changes and rebuild automatically.\n* `npm run inspector`: Run the MCP inspector to test the server.\n\n## Migration & Compatibility\n\n**Backward Compatibility**: \n- STDIO remains the default transport - existing users need no changes\n- All tool names, parameters, and responses remain unchanged\n- Configuration is opt-in via environment variables\n\n**Migration to HTTP**:\n- Set `TRANSPORT=http` to enable HTTP transport\n- Configure `PORT` and `HOST` as needed\n- Update client to use HTTP endpoint instead of stdio\n\n**Rollback**:\n- Set `TRANSPORT=stdio` or omit the variable to return to stdio\n",
      "npm_url": "https://www.npmjs.com/package/@nitish-raj/searxng-mcp-bridge",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "mcp",
        "searxng mcp",
        "connect searxng",
        "searxng instance"
      ],
      "category": "web-search"
    },
    "noobnooc--webhook-mcp": {
      "owner": "noobnooc",
      "name": "webhook-mcp",
      "url": "https://github.com/noobnooc/webhook-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/noobnooc.webp",
      "description": "Integrates webhook-based services with large language model applications, facilitating dynamic data retrieval and action handling. Supports connections to external webhooks for enhanced AI workflows.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-23T16:23:28Z",
      "readme_content": "# Webhook MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%20%22npx%22%2C%22args%22%3A%20%5B%22-y%22%2C%20%22webhook-mcp%22%5D%2C%22env%22%3A%20%7B%20%22WEBHOOK_URL%22%3A%20%22%24%7Binput%3Anotification_webhook_url%7D%22%20%7D%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%20%22npx%22%2C%22args%22%3A%20%5B%22-y%22%2C%20%22webhook-mcp%22%5D%2C%22env%22%3A%20%7B%20%22WEBHOOK_URL%22%3A%20%22%24%7Binput%3Anotification_webhook_url%7D%22%20%7D%7D&quality=insiders)\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22WEBHOOK_URL%22%2C%22noobnooc%2Fwebhook-mcp%22%5D%2C%22env%22%3A%7B%22WEBHOOK_URL%22%3A%22%24%7Binput%3Anotification_webhook_url%7D%22%7D%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22WEBHOOK_URL%22%2C%22noobnooc%2Fwebhook-mcp%22%5D%2C%22env%22%3A%7B%22WEBHOOK_URL%22%3A%22%24%7Binput%3Anotification_webhook_url%7D%22%7D%7D&quality=insiders)\n[![smithery badge](https://smithery.ai/badge/@noobnooc/webhook-mcp)](https://smithery.ai/server/@noobnooc/webhook-mcp)\n\nA Model Context Protocol (MCP) server that sends webhook notifications when called.\n\nYou can use this server with webhook notification services like [Echobell](https://echobell.one) to get notified when long-running tasks are completed. Simply configure the server with your Echobell webhook URL (or another service's URL) and instruct your AI assistant to \"send me a notification when it's done\" within your task prompts.\n\n## Configuration\n\nThere are several ways to configure the MCP server:\n\n### Installing via Smithery\n\nTo install Webhook MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@noobnooc/webhook-mcp):\n\n```bash\nnpx -y @smithery/cli install @noobnooc/webhook-mcp --client claude\n```\n\n### Claude & Cursor & Windsurf\n\nConfigure Claude, Cursor or Windsurf to use the MCP server by adding this to your configuration:\n\n- With NPM:\n\n```json\n{\n  \"mcpServers\": {\n    \"notification\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webhook-mcp\"],\n      \"env\": {\n        \"WEBHOOK_URL\": \"your-webhook-url-here\"\n      }\n    }\n  }\n}\n```\n\n- With Docker:\n\n```json\n{\n  \"mcpServers\": {\n    \"notification\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"WEBHOOK_URL\",\n        \"noobnooc/webhook-mcp\"\n      ],\n      \"env\": {\n        \"WEBHOOK_URL\": \"<your-webhook-url-here>\"\n      }\n    }\n  }\n}\n```\n\n### VS Code\n\nAdd the following configuration to your VS Code `settings.json`:\n\n- With NPM:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"notification\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"webhook-mcp\"],\n        \"env\": {\n          \"WEBHOOK_URL\": \"your-webhook-url-here\"\n        }\n      }\n    }\n  }\n}\n```\n\n- With Docker:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"notification\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"WEBHOOK_URL\",\n          \"noobnooc/webhook-mcp\"\n        ],\n        \"env\": {\n          \"WEBHOOK_URL\": \"<your-webhook-url-here>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n- `WEBHOOK_URL` (required): The URL where webhook notifications will be sent\n\n## Parameters\n\nThe webhook can be called with optional parameters:\n\n- `message`: Custom message to include in the webhook payload\n- `url`: External URL to include in the webhook payload as `externalLink`\n\n## Development\n\nTo build the project:\n\n```bash\nnpm install\nnpm run build\n```\n\nTo run with the MCP inspector for debugging:\n\n```bash\nnpm run inspector\n```\n\n## Publishing\n\nTo build and publish the package:\n\n```bash\nnpm run publish\n```\n",
      "npm_url": "https://www.npmjs.com/package/webhook-mcp",
      "npm_downloads": 431,
      "keywords": [
        "webhook",
        "webhooks",
        "noobnooc",
        "webhook mcp",
        "noobnooc webhook",
        "webhook based"
      ],
      "category": "web-search"
    },
    "oleggolimbievsky88--bmr-nextjs": {
      "owner": "oleggolimbievsky88",
      "name": "bmr-nextjs",
      "url": "https://github.com/oleggolimbievsky88/bmr-nextjs",
      "imageUrl": "/freedevtools/mcp/pfp/oleggolimbievsky88.webp",
      "description": "A Next.js project that facilitates the development and deployment of modern web applications by providing a framework optimized for performance and user experience. It features hot reloading for rapid development and built-in support for dynamic and responsive page creation.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-12T19:04:36Z",
      "readme_content": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\r\n\r\n## Getting Started\r\n\r\nFirst, run the development server:\r\n\r\n```bash\r\nnpm run dev\r\n# or\r\nyarn dev\r\n# or\r\npnpm dev\r\n# or\r\nbun dev\r\n```\r\n\r\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\r\n\r\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\r\n\r\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\r\n\r\n## Learn More\r\n\r\nTo learn more about Next.js, take a look at the following resources:\r\n\r\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\r\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\r\n\r\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\r\n\r\n## Deploy on Vercel\r\n\r\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\r\n\r\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nextjs",
        "js",
        "web",
        "bmr nextjs",
        "nextjs js",
        "js project"
      ],
      "category": "web-search"
    },
    "olostep--olostep-mcp-server": {
      "owner": "olostep",
      "name": "olostep-mcp-server",
      "url": "https://github.com/olostep/olostep-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/olostep.webp",
      "description": "Integrates with Olostep for extracting web page content, structured Google search results, and mapping websites. Supports geo-targeted content retrieval and error handling for enhanced web interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T08:11:12Z",
      "readme_content": "# Olostep MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Olostep](https://olostep.com) for web scraping, content extraction, and search capabilities.\nTo set up Olostep MCP Server, you need to have an API key. You can get the API key by signing up on the [Olostep website](https://olostep.com/auth).\n\n\n## Features\n\n- Web page content extraction with clean markdown formatting\n- Google search results with structured data extraction\n- Website URL discovery and mapping\n- Country-specific request routing for geo-targeted content\n- Configurable wait times for JavaScript-heavy websites\n- Comprehensive error handling and reporting\n- Simple API key configuration\n\n## Installation\n\n### Running with npx\n\n```bash\nenv OLOSTEP_API_KEY=your-api-key npx -y olostep-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g olostep-mcp\n```\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-olostep\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"olostep-mcp\"],\n      \"env\": {\n        \"OLOSTEP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nOr for a more straightforward way you can install via the Smithery CLI by running the following code in your device terminal\n\n```\nnpx -y @smithery/cli install @olostep/olostep-mcp-server --client claude\n```\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-olostep\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"olostep-mcp\"],\n      \"env\": {\n        \"OLOSTEP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Running on Cursor\n\nTo configure Olostep MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers \n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"olostep-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env OLOSTEP_API_KEY=your-api-key npx -y olostep-mcp`\n\nReplace `your-api-key` with your Olostep API key.\n\n## Configuration\n\n### Environment Variables\n\n- `OLOSTEP_API_KEY`: Your Olostep API key (required)\n- `ORBIT_KEY`: An optional key for using Orbit to route requests.\n\n## Available Tools\n\n### 1. Get Webpage Content (`get_webpage_content`)\n\nRetrieves webpage content in clean markdown format with support for JavaScript rendering.\n\n```json\n{\n  \"name\": \"get_webpage_content\",\n  \"arguments\": {\n    \"url_to_scrape\": \"https://example.com\",\n    \"wait_before_scraping\": 1000,\n    \"country\": \"US\"\n  }\n}\n```\n\n#### Parameters:\n\n- `url_to_scrape`: The URL of the webpage to scrape (required)\n- `wait_before_scraping`: Time to wait in milliseconds before starting the scrape (default: 0)\n- `country`: Residential country to load the request from (e.g., US, CA, GB) (optional)\n\n#### Response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"# Example Website\\n\\nThis is the markdown content of the webpage...\"\n    }\n  ]\n}\n```\n\n### 2. Get Website URLs (`get_website_urls`)\n\nSearch and retrieve relevant URLs from a website, sorted by relevance to your query.\n\n```json\n{\n  \"name\": \"get_website_urls\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"search_query\": \"your search term\"\n  }\n}\n```\n\n#### Parameters:\n\n- `url`: The URL of the website to map (required)\n- `search_query`: The search query to sort URLs by (required)\n\n#### Response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Found 42 URLs matching your query:\\n\\nhttps://example.com/page1\\nhttps://example.com/page2\\n...\"\n    }\n  ]\n}\n```\n\n### 3. Google Search (`google_search`)\n\nRetrieve structured data from Google search results.\n\n```json\n{\n  \"name\": \"google_search\",\n  \"arguments\": {\n    \"query\": \"your search query\",\n    \"country\": \"US\"\n  }\n}\n```\n\n#### Parameters:\n\n- `query`: The search query to perform (required)\n- `country`: Country code for localized results (e.g., US, GB) (default: \"US\")\n\n#### Response includes:\n\n- Organic search results with titles, links, and snippets\n- Knowledge graph data when available\n- Related questions (People Also Ask)\n- Related searches\n- Rich snippets and other structured data\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages for API issues\n- Network error reporting\n- Authentication failure handling\n- Rate limit information\n\nExample error response:\n\n```json\n{\n  \"isError\": true,\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Olostep API Error: 401 Unauthorized. Details: {\\\"error\\\":\\\"Invalid API key\\\"}\"\n    }\n  ]\n}\n```\n\n\n## License\n\nISC License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "olostep",
        "web",
        "google",
        "search olostep",
        "olostep mcp",
        "olostep extracting"
      ],
      "category": "web-search"
    },
    "oneshot-engineering--mcp-webresearch": {
      "owner": "oneshot-engineering",
      "name": "mcp-webresearch",
      "url": "https://github.com/oneshot-engineering/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/oneshot-engineering.webp",
      "description": "Fetch real-time information from the web, extract content from webpages, and track research sessions with the ability to capture screenshots for better insights.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-28T23:55:21Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webresearch",
        "webpages",
        "web",
        "mcp webresearch",
        "capture screenshots",
        "search oneshot"
      ],
      "category": "web-search"
    },
    "onurpolat05--n8n-Assistant": {
      "owner": "onurpolat05",
      "name": "n8n-Assistant",
      "url": "https://github.com/onurpolat05/n8n-Assistant",
      "imageUrl": "/freedevtools/mcp/pfp/onurpolat05.webp",
      "description": "Search for n8n documentation, example workflows, and community forums efficiently, while providing relevant information and resources. It employs asynchronous processing for improved response times and effective content retrieval.",
      "stars": 13,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T18:50:24Z",
      "readme_content": "# n8n Assistant\n[![smithery badge](https://smithery.ai/badge/@onurpolat05/n8n-assistant)](https://smithery.ai/server/@onurpolat05/n8n-assistant)\n\nThis project contains a Multi-Channel Platform (MCP) server used to create an assistant integrated with n8n. The assistant can be used to search for n8n documentation, example workflows, and community forums.\n\n<a href=\"https://glama.ai/mcp/servers/@onurpolat05/n8n-Assistant\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@onurpolat05/n8n-Assistant/badge\" alt=\"n8n-asistans MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Searches n8n documentation, workflows, and community forums based on a specific query.\n- **HTML Content Fetching**: Uses BeautifulSoup to extract the main content from search results.\n- **Asynchronous Processing**: Performs HTTP requests asynchronously, providing faster response times.\n\n## Requirements\n\n- Python 3.7 or higher\n- `httpx` library\n- `beautifulsoup4` library\n- `python-dotenv` library\n\n## Installation\n\n### Installing via Smithery\n\nTo install n8n-assistant for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@onurpolat05/n8n-assistant):\n\n```bash\nnpx -y @smithery/cli install @onurpolat05/n8n-assistant --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd <repository-directory>\n   ```\n\n2. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Create a `.env` file and add the necessary API keys:\n   ```plaintext\n   SERPER_API_KEY=your_api_key_here\n   ```\n\n## Usage\n\nTo start the assistant, run the following command:\n```bash\nuvicorn main:app --reload\n```\n\nThen, you can query the assistant for information related to n8n like this:\n```python\nawait get_n8n_info(\"HTTP Request node\", \"docs\")\n```\n\n## MCP Server\n\nThis project uses the `n8n-asistans` MCP server. The server is started with the following command:\n```json\n{\n    \"mcpServers\": {\n        \"n8n-asistans\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/n8n-assistant\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\":{\n                \"SERPER_API_KEY\": \"*********\"\n            }\n        }\n    }\n}\n```\n\n## Contributing\n\nIf you would like to contribute, please create a pull request or report issues.\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "retrieval",
        "n8n",
        "search n8n",
        "n8n documentation",
        "search onurpolat05"
      ],
      "category": "web-search"
    },
    "p1scess--mcp-servers": {
      "owner": "p1scess",
      "name": "mcp-servers",
      "url": "https://github.com/p1scess/mcp-servers",
      "imageUrl": "/freedevtools/mcp/pfp/p1scess.webp",
      "description": "Retrieve and process web content from URLs, converting HTML to markdown for easier information extraction. Enables chunked content retrieval for efficient parsing by LLMs.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-27T13:42:30Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the TypeScript and Python SDKs.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Redis](src/redis)** - Interact with Redis key-value stores\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg\" alt=\"Adfin Logo\" /> **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).\n- <img height=\"12\" width=\"12\" src=\"https://www.agentql.com/favicon/favicon.png\" alt=\"AgentQL Logo\" /> **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).\n- <img height=\"12\" width=\"12\" src=\"https://agentrpc.com/favicon.ico\" alt=\"AgentRPC Logo\" /> **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).\n- <img height=\"12\" width=\"12\" src=\"https://aiven.io/favicon.ico\" alt=\"Aiven Logo\" /> **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQL®, Apache Kafka®, ClickHouse® and OpenSearch® services\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png\" alt=\"APIMatic Logo\" /> **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic’s API.\n- <img height=\"12\" width=\"12\" src=\"https://resources.audiense.com/hubfs/favicon-1.png\" alt=\"Audiense Logo\" /> **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://www.bankless.com/favicon.ico\" alt=\"Bankless Logo\" /> **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.\n- <img height=\"12\" width=\"12\" src=\"https://www.box.com/favicon.ico\" alt=\"Box Logo\" /> **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.chargebee.com/static/resources/brand/favicon.png\" /> **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).\n- <img height=\"12\" width=\"12\" src=\"https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg\" /> **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database\n- <img height=\"12\" width=\"12\" src=\"https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico\" alt=\"Chronulus AI Logo\" /> **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.\n- <img height=\"12\" width=\"12\" src=\"https://clickhouse.com/favicon.ico\" alt=\"ClickHouse Logo\" /> **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://www.comet.com/favicon.ico\" alt=\"Comet Logo\" /> **[Comet Opik](https://github.com/comet-ml/opik-mcp)** - Query and analyze your [Opik](https://github.com/comet-ml/opik) logs, traces, prompts and all other telemtry data from your LLMs in natural language.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.convex.dev/favicon.ico\" /> **[Convex](https://stack.convex.dev/convex-mcp-server)** - Introspect and query your apps deployed to Convex.\n- <img height=\"12\" width=\"12\" src=\"http://app.itsdart.com/static/img/favicon.png\" alt=\"Dart Logo\" /> **[Dart](https://github.com/its-dart/dart-mcp-server)** - Interact with task, doc, and project data in [Dart](https://itsdart.com), an AI-native project management tool\n- <img height=\"12\" width=\"12\" src=\"https://www.devhub.com/img/upload/favicon-196x196-dh.png\" alt=\"DevHub Logo\" /> **[DevHub](https://github.com/devhub/devhub-cms-mcp)** - Manage and utilize website content within the [DevHub](https://www.devhub.com) CMS platform\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://static.edubase.net/media/brand/favicon/favicon-32x32.png\" alt=\"EduBase Logo\" /> **[EduBase](https://github.com/EduBase/MCP)** - Interact with [EduBase](https://www.edubase.net), a comprehensive e-learning platform with advanced quizzing, exam management, and content organization capabilities\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fewsats.com/favicon.svg\" alt=\"Fewsats Logo\" /> **[Fewsats](https://github.com/Fewsats/fewsats-mcp)** - Enable AI Agents to purchase anything in a secure way using [Fewsats](https://fewsats.com)\n- <img height=\"12\" width=\"12\" src=\"https://fibery.io/favicon.svg\" alt=\"Fibery Logo\" /> **[Fibery](https://github.com/Fibery-inc/fibery-mcp-server)** - Perform queries and entity operations in your [Fibery](https://fibery.io) workspace.\n- <img height=\"12\" width=\"12\" src=\"https://financialdatasets.ai/favicon.ico\" alt=\"Financial Datasets Logo\" /> **[Financial Datasets](https://github.com/financial-datasets/mcp-server)** - Stock market API made for AI agents\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/mendableai/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://gitee.com/favicon.ico\" alt=\"Gitee Logo\" /> **[Gitee](https://github.com/oschina/mcp-gitee)** - Gitee API integration, repository, issue, and pull request management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6605a2979ff17b2cd1939cd4/6605a460de47e7596ed84f06_icon256.png\" alt=\"gotoHuman Logo\" /> **[gotoHuman](https://github.com/gotohuman/gotohuman-mcp-server)** - Human-in-the-loop platform - Allow AI agents and automations to send requests for approval to your [gotoHuman](https://www.gotohuman.com) inbox.\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/KCOWBYLKunDff1Dr452y6EfjiU.png\" alt=\"Graphlit Logo\" /> **[Graphlit](https://github.com/graphlit/graphlit-mcp-server)** - Ingest anything from Slack to Gmail to podcast feeds, in addition to web crawling, into a searchable [Graphlit](https://www.graphlit.com) project.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN01d9qrry1i6lTNa2BRa_!!6000000004364-2-tps-218-200.png\" alt=\"Hologres Logo\" /> **[Hologres](https://github.com/aliyun/alibabacloud-hologres-mcp-server)** - Connect to a [Hologres](https://www.alibabacloud.com/en/product/hologres) instance, get table metadata, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://hyperbrowser-assets-bucket.s3.us-east-1.amazonaws.com/Hyperbrowser-logo.png\" alt=\"Hyperbrowsers23 Logo\" /> **[Hyperbrowser](https://github.com/hyperbrowserai/mcp)** - [Hyperbrowser](https://www.hyperbrowser.ai/) is the next-generation platform empowering AI agents and enabling effortless, scalable browser automation.\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://forevervm.com/icon.png\" alt=\"ForeverVM Logo\" /> **[ForeverVM](https://github.com/jamsocket/forevervm/tree/main/javascript/mcp-server)** - Run Python in a code sandbox.\n- <img height=\"12\" width=\"12\" src=\"https://www.getinboxzero.com/icon.png\" alt=\"Inbox Zero Logo\" /> **[Inbox Zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server)** - AI personal assistant for email [Inbox Zero](https://www.getinboxzero.com)\n-  **[Inkeep](https://github.com/inkeep/mcp-server-python)** - RAG Search over your content powered by [Inkeep](https://inkeep.com)\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers.\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://logfire.pydantic.dev/favicon.ico\" alt=\"Logfire Logo\" /> **[Logfire](https://github.com/pydantic/logfire-mcp)** - Provides access to OpenTelemetry traces and metrics through Logfire.\n- <img height=\"12\" width=\"12\" src=\"https://langfuse.com/favicon.ico\" alt=\"Langfuse Logo\" /> **[Langfuse Prompt Management](https://github.com/langfuse/mcp-server-langfuse)** - Open-source tool for collaborative editing, versioning, evaluating, and releasing prompts.\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://www.mailgun.com/favicon.ico\" alt=\"Mailgun Logo\" /> **[Mailgun](https://github.com/mailgun/mailgun-mcp-server)** - Interact with Mailgun API.\n- <img height=\"12\" width=\"12\" src=\"https://www.make.com/favicon.ico\" alt=\"Make Logo\" /> **[Make](https://github.com/integromat/make-mcp-server)** - Turn your [Make](https://www.make.com/) scenarios into callable tools for AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img alt=\"favicon_32x32\" height=\"12\" width=\"12\" src=\"https://milvus.io/favicon-32x32.png\" /> **[Milvus](https://github.com/zilliztech/mcp-server-milvus)** - Search, Query and interact with data in your Milvus Vector Database.\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/183852044?s=48&v=4\" alt=\"Neon Logo\" /> **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/82347605?s=48&v=4\" alt=\"OceanBase Logo\" /> **[OceanBase](https://github.com/oceanbase/mcp-oceanbase)** - MCP Server for OceanBase database and its tools\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[Octagon](https://github.com/OctagonAI/octagon-mcp-server)** - Deliver real-time investment research with extensive private and public market data.\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img height=\"12\" width=\"12\" src=\"https://www.perplexity.ai/favicon.ico\" alt=\"Perplexity Logo\" /> **[Perplexity](https://github.com/ppl-ai/modelcontextprotocol)** - An MCP server that connects to Perplexity's Sonar API, enabling real-time web-wide research in conversational AI.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.ramp.com/favicon.ico\" /> **[Ramp](https://github.com/ramp-public/ramp-mcp)** - Interact with [Ramp](https://ramp.com)'s Developer API to run analysis on your spend and gain insights leveraging LLMs\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://www.rember.com/favicon.ico\" alt=\"Rember Logo\" /> **[Rember](https://github.com/rember/rember-mcp)** - Create spaced repetition flashcards in [Rember](https://rember.com) to remember anything you learn in your chats\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://screenshotone.com/favicon.ico\" alt=\"ScreenshotOne Logo\" /> **[ScreenshotOne](https://github.com/screenshotone/mcp/)** - Render website screenshots with [ScreenshotOne](https://screenshotone.com/)\n- <img height=\"12\" width=\"12\" src=\"https://semgrep.dev/favicon.ico\" alt=\"Semgrep Logo\" /> **[Semgrep](https://github.com/semgrep/mcp)** - Enable AI agents to secure code with [Semgrep](https://semgrep.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://www.starrocks.io/favicon.ico\" alt=\"StarRocks Logo\" /> **[StarRocks](https://github.com/StarRocks/mcp-server-starrocks)** - Interact with [StarRocks](https://www.starrocks.io/)\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://thirdweb.com/favicon.ico\" alt=\"Thirdweb Logo\" /> **[Thirdweb](https://github.com/thirdweb-dev/ai/tree/main/python/thirdweb-mcp)** - Read/write to over 2k blockchains, enabling data querying, contract analysis/deployment, and transaction execution, powered by [Thirdweb](https://thirdweb.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://unifai.network/favicon.ico\" alt=\"UnifAI Logo\" /> **[UnifAI](https://github.com/unifai-network/unifai-mcp-server)** - Dynamically search and call tools using [UnifAI Network](https://unifai.network)\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/plcQevjrOYnyriuGw90NfQBPoQ.jpg\" alt=\"Unstructured Logo\" /> **[Unstructured](https://github.com/Unstructured-IO/UNS-MCP)** - Set up and interact with your unstructured data processing workflows in [Unstructured Platform](https://unstructured.io)\n- **[Vectorize](https://github.com/vectorize-io/vectorize-mcp-server/)** - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n- <img height=\"12\" width=\"12\" src=\"https://www.veyrax.com/favicon.ico\" alt=\"VeyraX Logo\" /> **[VeyraX](https://github.com/VeyraX/veyrax-mcp)** - Single tool to control all 100+ API integrations, and UI components\n- <img height=\"12\" width=\"12\" src=\"https://www.xero.com/favicon.ico\" alt=\"Xero Logo\" /> **[Xero](https://github.com/XeroAPI/xero-mcp-server)** - Interact with the accounting data in your business using our official MCP server\n- **[ZenML](https://github.com/zenml-io/mcp-zenml)** - Interact with your MLOps and LLMOps pipelines through your [ZenML](https://www.zenml.io) MCP server\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n- **[Ableton Live](https://github.com/Simon-Kansara/ableton-live-mcp-server)** - an MCP server to control Ableton Live.\n- **[Airbnb](https://github.com/openbnb-org/mcp-server-airbnb)** - Provides tools to search Airbnb and get listing details.\n- **[Algorand](https://github.com/GoPlausible/algorand-mcp)** - A comprehensive MCP server for tooling interactions (40+) and resource accessibility (60+) plus many useful prompts for interacting with the Algorand blockchain.\n- **[Airflow](https://github.com/yangkyeongmo/mcp-server-apache-airflow)** - A MCP Server that connects to [Apache Airflow](https://airflow.apache.org/) using official python client.\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Apple Calendar](https://github.com/Omar-v2/mcp-ical)** - An MCP server that allows you to interact with your MacOS Calendar through natural language, including features such as event creation, modification, schedule listing, finding free time slots etc.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[Arduino](https://github.com/vishalmysore/choturobo)** - MCP Server that enables AI-powered robotics using Claude AI and Arduino (ESP32) for real-world automation and interaction with robots.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM.\n- **[AWS Athena](https://github.com/lishenxydlgzs/aws-athena-mcp)** - A MCP server for AWS Athena to run SQL queries on Glue Catalog.\n- **[AWS Cost Explorer](https://github.com/aarora79/aws-cost-explorer-mcp-server)** - Optimize your AWS spend (including Amazon Bedrock spend) with this MCP server by examining spend across regions, services, instance types and foundation models ([demo video](https://www.youtube.com/watch?v=WuVOmYLRFmI&feature=youtu.be)).\n- **[AWS Resources Operations](https://github.com/baryhuang/mcp-server-aws-resources-python)** - Run generated python code to securely query or modify any AWS resources supported by boto3.\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents.\n- **[Azure ADX](https://github.com/pab1it0/adx-mcp-server)** - Query and analyze Azure Data Explorer databases.\n- **[Azure DevOps](https://github.com/Vortiago/mcp-azure-devops)** - An MCP server that provides a bridge to Azure DevOps services, enabling AI assistants to query and manage work items.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n* **[Basic Memory](https://github.com/basicmachines-co/basic-memory)** - Local-first knowledge management system that builds a semantic graph from Markdown files, enabling persistent memory across conversations with LLMs.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Bing Web Search API](https://github.com/leehanchung/bing-search-mcp)** (by hanchunglee) - Server implementation for Microsoft Bing Web Search API.\n- **[Bitable MCP](https://github.com/lloydzhou/bitable-mcp)** (by lloydzhou) - MCP server provides access to Lark Bitable through the Model Context Protocol. It allows users to interact with Bitable tables using predefined tools.\n- **[Blender](https://github.com/ahujasid/blender-mcp)** (by ahujasid) - Blender integration allowing prompt enabled 3D scene creation, modeling and manipulation.\n- **[Bsc-mcp](https://github.com/TermiX-official/bsc-mcp)** The first MCP server that serves as the bridge between AI and BNB Chain, enabling AI agents to execute complex on-chain operations through seamless integration with the BNB Chain, including transfer, swap, launch, security check on any token and even more.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[code-assistant](https://github.com/stippi/code-assistant)** - A coding assistant MCP server that allows to explore a code-base and make changes to code. Should be used with trusted repos only (insufficient protection against prompt injections).\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[crypto-feargreed-mcp](https://github.com/kukapay/crypto-feargreed-mcp)**  -  Providing real-time and historical Crypto Fear & Greed Index data.\n- **[cryptopanic-mcp-server](https://github.com/kukapay/cryptopanic-mcp-server)** - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.\n- **[Dappier](https://github.com/DappierAI/dappier-mcp)** - Connect LLMs to real-time, rights-cleared, proprietary data from trusted sources. Access specialized models for Real-Time Web Search, News, Sports, Financial Data, Crypto, and premium publisher content. Explore data models at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n- **[Databricks](https://github.com/JordiNeil/mcp-databricks-server)** - Allows LLMs to run SQL queries, list and get details of jobs executions in a Databricks account.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DBHub](https://github.com/bytebase/dbhub/)** - Universal database MCP server connecting to MySQL, PostgreSQL, SQLite, DuckDB and etc.\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[deepseek-thinker-mcp](https://github.com/ruixingshi/deepseek-thinker-mcp)** - A MCP (Model Context Protocol) provider Deepseek reasoning content to MCP-enabled AI Clients, like Claude Desktop. Supports access to Deepseek's thought processes from the Deepseek API service or from a local Ollama server.\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dicom](https://github.com/ChristianHinge/dicom-mcp)** - An MCP server to query and retrieve medical images and for parsing and reading dicom-encapsulated documents (pdf etc.). \n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discord](https://github.com/v-3/discordmcp)** - A MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Discord](https://github.com/SaseQ/discord-mcp)** - A MCP server, which connects to Discord through a bot, and provides comprehensive integration with Discord.\n- **[Discourse](https://github.com/AshDevFr/discourse-mcp-server)** - A MCP server to search Discourse posts on a Discourse forum.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[dune-analytics-mcp](https://github.com/kukapay/dune-analytics-mcp)** -  A mcp server that bridges Dune Analytics data to AI agents.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Ergo Blockchain MCP](https://github.com/marctheshark3/ergo-mcp)** -An MCP server to integrate Ergo Blockchain Node and Explorer APIs for checking address balances, analyzing transactions, viewing transaction history, performing forensic analysis of addresses, searching for tokens, and monitoring network status.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[EVM MCP Server](https://github.com/mcpdotdirect/evm-mcp-server)** - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[Excel](https://github.com/haris-musa/excel-mcp-server)** - Excel manipulation including data reading/writing, worksheet management, formatting, charts, and pivot table.\n- **[Fantasy PL](https://github.com/rishijatia/fantasy-pl-mcp)** - Give your coding agent direct access to up-to date Fantasy Premier League data\n- **[fastn.ai – Unified API MCP Server](https://github.com/fastnai/mcp-fastn)** - A remote, dynamic MCP server with a unified API that connects to 1,000+ tools, actions, and workflows, featuring built-in authentication and monitoring.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[Fingertip](https://github.com/fingertip-com/fingertip-mcp)** - MCP server for Fingertip.com to search and create new sites.\n- **[Figma](https://github.com/GLips/Figma-Context-MCP)** - Give your coding agent direct access to Figma file data, helping it one-shot design implementation.\n- **[Firebase](https://github.com/gannonh/firebase-mcp)** - Server to interact with Firebase services including Firebase Authentication, Firestore, and Firebase Storage.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Ghost](https://github.com/MFYDev/ghost-mcp)** - A Model Context Protocol (MCP) server for interacting with Ghost CMS through LLM interfaces like Claude.\n- **[Github Actions](https://github.com/ko1ynnky/github-actions-mcp-server)** - A Model Context Protocol (MCP) server for interacting with Github Actions.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Gmail Headless](https://github.com/baryhuang/mcp-headless-gmail)** - Remote hostable MCP server that can get and send Gmail messages without local credential or file system setup.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[GOAT](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol)** - Run more than +200 onchain actions on any blockchain including Ethereum, Solana and Base.\n- **[Godot](https://github.com/Coding-Solo/godot-mcp)** - A MCP server providing comprehensive Godot engine integration for project editing, debugging, and scene management.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Goodnews](https://github.com/VectorInstitute/mcp-goodnews)** - A simple MCP server that delivers curated positive and uplifting news stories.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[GraphQL Schema](https://github.com/hannesj/mcp-graphql-schema)** - Allow LLMs to explore large GraphQL schemas without bloating the context.\n- **[HDW LinkedIn](https://github.com/horizondatawave/hdw-mcp-server)** - Access to profile data and management of user account with [HorizonDataWave.ai](https://horizondatawave.ai/).\n- **[Heurist Mesh Agent](https://github.com/heurist-network/heurist-mesh-mcp-server)** - Access specialized web3 AI agents for blockchain analysis, smart contract security, token metrics, and blockchain interactions through the [Heurist Mesh network](https://github.com/heurist-network/heurist-agent-framework/tree/main/mesh).\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[Home Assistant](https://github.com/voska/hass-mcp)** - Docker-ready MCP server for Home Assistant with entity management, domain summaries, automation support, and guided conversations. Includes pre-built container images for easy installation.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Hyperliquid](https://github.com/mektigboy/server-hyperliquid)** - An MCP server implementation that integrates the Hyperliquid SDK for exchange data.\n- **[Image Generation](https://github.com/GongRzhe/Image-Generation-MCP-Server)** - This MCP server provides image generation capabilities using the Replicate Flux model.\n- **[InfluxDB](https://github.com/idoru/influxdb-mcp-server)** - Run queries against InfluxDB OSS API v2.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Intercom](https://github.com/raoulbia-ai/mcp-server-for-intercom)** - An MCP-compliant server for retrieving customer support tickets from Intercom. This tool enables AI assistants like Claude Desktop and Cline to access and analyze your Intercom support tickets.\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[JavaFX](https://github.com/mcpso/mcp-server-javafx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, sqllite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[KiCad MCP](https://github.com/lamaalrajih/kicad-mcp)** - MCP server for KiCad on Mac, Windows, and Linux.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Kubernetes and OpenShift](https://github.com/manusa/kubernetes-mcp-server)** - A powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- **[Langflow-DOC-QA-SERVER](https://github.com/GongRzhe/Langflow-DOC-QA-SERVER)** - A Model Context Protocol server for document Q&A powered by Langflow. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[Linear (Go)](https://github.com/geropl/linear-mcp-go)** - Allows LLM to interact with Linear's API via a single static binary.\n- **[LINE](https://github.com/amornpan/py-mcp-line)** (by amornpan) - Implementation for LINE Bot integration that enables Language Models to read and analyze LINE conversations through a standardized interface. Features asynchronous operation, comprehensive logging, webhook event handling, and support for various message types.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[mac-messages-mcp](https://github.com/carterlasalle/mac_messages_mcp)** - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- **[MariaDB](https://github.com/abel9851/mcp-server-mariadb)** - MariaDB database integration with configurable access controls in Python.\n- **[Maton](https://github.com/maton-ai/agent-toolkit/tree/main/modelcontextprotocol)** - Connect to your SaaS tools like HubSpot, Salesforce, and more.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Create](https://github.com/tesla0225/mcp-create)** - A dynamic MCP server management service that creates, runs, and manages Model Context Protocol servers on-the-fly.\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-local-rag](https://github.com/nkapila6/mcp-local-rag)** - \"primitive\" RAG-like web search model context protocol (MCP) server that runs locally using Google's MediaPipe Text Embedder and DuckDuckGo Search. ✨ no APIs required ✨.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[mem0-mcp](https://github.com/mem0ai/mem0-mcp)** - A Model Context Protocol server for Mem0, which helps with managing coding preferences.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[MSSQL-MCP](https://github.com/daobataotie/mssql-mcp)** (by daobataotie) - MSSQL MCP that refer to the official website's SQLite MCP for modifications to adapt to MSSQL\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Mindmap](https://github.com/YuChenSSR/mindmap-mcp-server)** (by YuChenSSR) - A server that generates mindmaps from input containing markdown code.\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MongoDB Lens](https://github.com/furey/mongodb-lens)** - Full Featured MCP Server for MongoDB Databases.\n- **[Monday.com](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[Multicluster-MCP-Sever](https://github.com/yanmxa/multicluster-mcp-server)** - The gateway for GenAI systems to interact with multiple Kubernetes clusters.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[n8n](https://github.com/leonardsellem/n8n-mcp-server)** - This MCP server provides tools and resources for AI assistants to manage n8n workflows and executions, including listing, creating, updating, and deleting workflows, as well as monitoring their execution status.\n- **[NASA](https://github.com/ProgramComputer/NASA-MCP-server)** (by ProgramComputer) - Access to a unified gateway of NASA's data sources including but not limited to APOD, NEO, EPIC, GIBS.\n- **[National Parks](https://github.com/KyrieTangSheng/mcp-server-nationalparks)** - The server provides latest information of park details, alerts, visitor centers, campgrounds, hiking trails, and events for U.S. National Parks.\n- **[NAVER](https://github.com/pfldy2850/py-mcp-naver)** (by pfldy2850) - This MCP server provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp)** (by teddyzxcv) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OceanBase](https://github.com/yuanoOo/oceanbase_mcp_server)** - (by yuanoOo) A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases.\n- **[Okta](https://github.com/kapilduraphe/okta-mcp-server)** - Interact with Okta API.\n- **[OneNote](https://github.com/rajvirtual/MCP-Servers/tree/master/onenote)** - (by Rajesh Vijay) An MCP server that connects to Microsoft OneNote using the Microsoft Graph API. Reading notebooks, sections, and pages from OneNote,Creating new notebooks, sections, and pages in OneNote.\n- **[OpenAI WebSearch MCP](https://github.com/ConechoAI/openai-websearch-mcp)** - This is a Python-based MCP server that provides OpenAI `web_search` build-in tool.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenAPI AnyApi](https://github.com/baryhuang/mcp-server-any-openapi)** - Interact with large [OpenAPI](https://www.openapis.org/) docs using built-in semantic search for endpoints. Allows for customizing the MCP server prefix.\n- **[OpenAPI Schema](https://github.com/hannesj/mcp-openapi-schema)** - Allow LLMs to explore large [OpenAPI](https://www.openapis.org/) schemas without bloating the context.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenDota](https://github.com/asusevski/opendota-mcp-server)** - Interact with OpenDota API to retrieve Dota 2 match data, player statistics, and more.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Productboard](https://github.com/kenjihikmatullah/productboard-mcp)** - Integrate the Productboard API into agentic workflows via MCP.\n- **[Prometheus](https://github.com/pab1it0/prometheus-mcp-server)** - Query and analyze Prometheus - open-source monitoring system.\n- **[Pulumi](https://github.com/dogukanakkaya/pulumi-mcp-server)** - MCP Server to Interact with Pulumi API, creates and lists Stacks\n- **[Pushover](https://github.com/ashiknesin/pushover-mcp)** - Send instant notifications to your devices using [Pushover.net](https://pushover.net/)\n- **[QGIS](https://github.com/jjsantos01/qgis_mcp)** - connects QGIS to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.\n- **[QuickChart](https://github.com/GongRzhe/Quickchart-MCP-Server)** - A Model Context Protocol server for generating charts using QuickChart.io\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Rquest](https://github.com/xxxbrian/mcp-rquest)** - An MCP server providing realistic browser-like HTTP request capabilities with accurate TLS/JA3/JA4 fingerprints for bypassing anti-bot measures.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[scrapling-fetch](https://github.com/cyberchitta/scrapling-fetch-mcp)** - Access text content from bot-protected websites. Fetches HTML/markdown from sites with anti-automation measures using Scrapling.\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[ServiceNow](https://github.com/osomai/servicenow-mcp)** - A MCP server to interact with a ServiceNow instance\n- **[Siri Shortcuts](https://github.com/dvcrn/mcp-server-siri-shortcuts)** - MCP to interact with Siri Shortcuts on macOS. Exposes all Shortcuts as MCP tools.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server)** - This MCP server enables LLMs to interact with the Solana blockchain with help of Solana Agent Kit by SendAI, allowing for 40+ protcool actions and growing\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Starwind UI](https://github.com/Boston343/starwind-ui-mcp/)** - This MCP provides relevant commands, documentation, and other information to allow LLMs to take full advantage of Starwind UI's open source Astro components.\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Telegram](https://github.com/chigwell/telegram-mcp)** - An MCP server that provides paginated chat reading, message retrieval, and message sending capabilities for Telegram through Telethon integration.\n- **[Terminal-Control](https://github.com/GongRzhe/terminal-controller-mcp)** - A MCP server that enables secure terminal command execution, directory navigation, and file system operations through a standardized interface.\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Typesense](https://github.com/suhail-ak-s/mcp-typesense-server)** - A Model Context Protocol (MCP) server implementation that provides AI models with access to Typesense search capabilities. This server enables LLMs to discover, search, and analyze data stored in Typesense collections.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Unity Catalog](https://github.com/ognis1205/mcp-server-unitycatalog)** - An MCP server that enables LLMs to interact with Unity Catalog AI, supporting CRUD operations on Unity Catalog Functions and executing them as MCP tools.\n- **[Unity3d Game Engine](https://github.com/CoderGamester/mcp-unity)** - An MCP server that enables LLMs to interact with Unity3d Game Engine, supporting access to a variety of the Unit's Editor engine tools (e.g. Console Logs, Test Runner logs, Editor functions, hierarchy state, etc) and executing them as MCP tools or gather them as resources.\n- **[Unity Integration (Advanced)](https://github.com/quazaai/UnityMCPIntegration)** - Advanced Unity3d Game Engine MCP which supports ,Execution of Any Editor Related Code Directly Inside of Unity, Fetch Logs, Get Editor State and Allow File Access of the Project making it much more useful in Script Editing or asset creation.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[Virtual location (Google Street View,etc.)](https://github.com/mfukushim/map-traveler-mcp)** - Integrates Google Map, Google Street View, PixAI, Stability.ai, ComfyUI API and Bluesky to provide a virtual location simulation in LLM (written in Effect.ts)\n- **[VolcEngine TOS](https://github.com/dinghuazhou/sample-mcp-server-tos)** - A sample MCP server for VolcEngine TOS that flexibly get objects from TOS.\n- **[Wanaku MCP Router](https://github.com/wanaku-ai/wanaku/)** - The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.\n- **[Webflow](https://github.com/kapilduraphe/webflow-mcp-server)** - Interfact with the Webflow APIs\n- **[whale-tracker-mcp](https://github.com/kukapay/whale-tracker-mcp)**  -  A mcp server for tracking cryptocurrency whale transactions. \n- **[Whois MCP](https://github.com/bharathvaj-ganesan/whois-mcp)** - MCP server that performs whois lookup against domain, IP, ASN and TLD. \n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[xcodebuild](https://github.com/ShenghaiWang/xcodebuild)**  - 🍎 Build iOS Xcode workspace/project and feed back errors to llm.\n- **[Xero-mcp-server](https://github.com/john-zhang-dev/xero-mcp)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[XiYan](https://github.com/XGenerationLab/xiyan_mcp_server)** - 🗄️ An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n- **[FastAPI to MCP auto generator](https://github.com/tadata-org/fastapi_mcp)** – A zero-configuration tool for automatically exposing FastAPI endpoints as MCP tools by **[Tadata](https://tadata.com/)**\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[MCP-Framework](https://mcp-framework.com)** Build MCP servers with elegance and speed in Typescript. Comes with a CLI to create your project with `mcp create app`. Get started with your first server in under 5 minutes by **[Alex Andru](https://github.com/QuantGeekDev)**\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n* **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Discord Server (ModelContextProtocol)](https://discord.gg/jHEGxQu2a5)** – Connect with developers, share insights, and collaborate on projects in an active Discord community dedicated to the Model Context Protocol by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-guardian](https://github.com/eqtylab/mcp-guardian)** - GUI application + tools for proxying / managing control of MCP servers by **[EQTY Lab](https://eqtylab.io)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[mcp-dockmaster](https://mcp-dockmaster.com)** - An Open-Sourced UI to install and manage MCP servers for Windows, Linux and MacOS.\n- <img height=\"12\" width=\"12\" src=\"https://mkinf.io/favicon-lilac.png\" alt=\"mkinf Logo\" /> **[mkinf](https://mkinf.io)** - An Open Source registry of hosted MCP Servers to accelerate AI agent workflows.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[r/modelcontextprotocol](https://www.reddit.com/r/modelcontextprotocol)** – A Model Context Protocol community Reddit page - discuss ideas, get answers to your questions, network with like-minded people, and showcase your projects! by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "https://www.npmjs.com/package/mcp-servers",
      "npm_downloads": 854,
      "keywords": [
        "p1scess",
        "parsing",
        "markdown",
        "search p1scess",
        "p1scess mcp",
        "parsing llms"
      ],
      "category": "web-search"
    },
    "perplexityai--modelcontextprotocol": {
      "owner": "perplexityai",
      "name": "modelcontextprotocol",
      "url": "https://github.com/perplexityai/modelcontextprotocol",
      "imageUrl": "/freedevtools/mcp/pfp/perplexityai.webp",
      "description": "Integrates the Sonar API for live web searches, enabling conversations that leverage real-time web-wide research capabilities. Facilitates instant information retrieval directly from the web during interactions.",
      "stars": 1643,
      "forks": 229,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T16:20:54Z",
      "readme_content": "# Perplexity Ask MCP Server\n\nAn MCP server implementation that integrates the Sonar API to provide Claude with unparalleled real-time, web-wide research.\n\nPlease refer to the official [DeepWiki page](https://deepwiki.com/ppl-ai/modelcontextprotocol) for assistance with implementation. \n\n# High-level System Architecture\n\n*Credits: DeepWiki powered by Devin*\n\n\n\n\n\n\n\n\n\n\n## Tools\n\n- **perplexity_ask**\n  - Engage in a conversation with the Sonar API for live web searches.\n  - **Inputs:**\n    - `messages` (array): An array of conversation messages.\n      - Each message must include:\n        - `role` (string): The role of the message (e.g., `system`, `user`, `assistant`).\n        - `content` (string): The content of the message.\n\n## Configuration\n\n### Step 1: \n\nClone this repository:\n\n```bash\ngit clone git@github.com:ppl-ai/modelcontextprotocol.git\n```\n\nNavigate to the `perplexity-ask` directory and install the necessary dependencies:\n\n```bash\ncd modelcontextprotocol/perplexity-ask && npm install\n```\n\n### Step 2: Get a Sonar API Key\n\n1. Sign up for a [Sonar API account](https://docs.perplexity.ai/guides/getting-started).\n2. Follow the account setup instructions and generate your API key from the developer dashboard.\n3. Set the API key in your environment as `PERPLEXITY_API_KEY`.\n\n### Step 3: Configure Claude Desktop\n\n1. Download Claude desktop [here](https://claude.ai/download). \n\n2. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-ask\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"PERPLEXITY_API_KEY\",\n        \"mcp/perplexity-ask\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-ask\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"server-perplexity-ask\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nYou can access the file using:\n\n```bash\nvim ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n### Step 4: Build the Docker Image\n\nDocker build:\n\n```bash\ndocker build -t mcp/perplexity-ask:latest -f Dockerfile .\n```\n\n### Step 5: Testing\n\nLet's make sure Claude for Desktop is picking up the two tools we've exposed in our `perplexity-ask` server. You can do this by looking for the hammer icon:\n\n\n\nAfter clicking on the hammer icon, you should see the tools that come with the Filesystem MCP Server:\n\n\n\nIf you see both of these this means that the integration is active. Congratulations! This means Claude can now ask Perplexity. You can then simply use it as you would use the Perplexity web app.  \n\n### Step 6: Advanced parameters\n\nCurrently, the search parameters used are the default ones. You can modify any search parameter in the API call directly in the `index.ts` script. For this, please refer to the official [API documentation](https://docs.perplexity.ai/api-reference/chat-completions).\n\n### Troubleshooting \n\nThe Claude documentation provides an excellent [troubleshooting guide](https://modelcontextprotocol.io/docs/tools/debugging) you can refer to. However, you can still reach out to us at api@perplexity.ai for any additional support or [file a bug](https://github.com/ppl-ai/api-discussion/issues). \n\n\n# Cursor integration\n\nYou can also use our MCP with Cursor (or any other app that supports this). To use Sonar with Cursor, you can follow the following steps. \n\n### Step 1: Navigate to your Cursor settings:\n\n\n\n### Step 2: Navigate to the MCP directory\n\nAnd click on `Add new global MCP server`\n\n\n\n\n### Step 3: Insert the MCP Server Configuration from above \n\nThis is the same configuration you would use for any other application that supports MCP. \n\nYou should then see the application being part of your available tools like this:\n\n\n\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "sonar",
        "web",
        "web search",
        "web searches",
        "sonar api"
      ],
      "category": "web-search"
    },
    "pfldy2850--py-mcp-naver": {
      "owner": "pfldy2850",
      "name": "py-mcp-naver",
      "url": "https://github.com/pfldy2850/py-mcp-naver",
      "imageUrl": "/freedevtools/mcp/pfp/pfldy2850.webp",
      "description": "Interact with various Naver services to search for blogs, news, books, and more using the Naver OpenAPI. Facilitate powerful search capabilities and content validation for applications.",
      "stars": 111,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-09T10:50:00Z",
      "readme_content": "# Naver MCP Server\n\nA server implementation for Naver OpenAPI using the Model Context Protocol (MCP). This project provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n\n\n## Pre-requisite\nTo use the Naver MCP server, you need to apply for access to the Naver Open API. \nYou can apply for Open API access at the link below:\n\nhttps://developers.naver.com/apps/#/register=datalab\n\n\n## Installation\n\n### from PyPi (Claude Desktop)\n\nInstall it to Claude Desktop with (uv):\n```sh\nuv pip install mcp-naver\n\nuv run python -m mcp-naver.hosts.claude_desktop \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\nInstall it to Claude Desktop with:\n```sh\npip install mcp-naver\n\npython -m mcp-naver.hosts.claude_desktop \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\n\n### from PyPi (Cursor)\n\nInstall it to Cursor with (uv):\n```sh\nuv pip install mcp-naver\n\nuv run python -m mcp-naver.hosts.cursor \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\n\n### from source\n\n```sh\n# Clone the repository\ngit clone https://github.com/pfldy2850/py-mcp-naver.git\n\n# Navigate into the project directory\ncd py-mcp-naver\n\n# Synchronize dependencies\nuv sync --dev --all-extras\n```\n\nRun it with:\n```sh\n# Start the server (Using FastMCP CLI)\nfastmcp install mcp_naver/server.py -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\nThe tool sets up everything you need to create an MCP server integrated with Naver OpenAPI.\n\n## Features\n\nThis server provides the following tools for interacting with Naver OpenAPI:\n\n- **Blog Search**: Search blog posts on Naver.\n- **News Search**: Search news articles on Naver.\n- **Book Search**: Search books and advanced book information.\n- **Adult Content Check**: Check if a search term is adult content.\n- **Encyclopedia Search**: Search encyclopedia entries.\n- **Cafe Article Search**: Search articles in Naver cafes.\n- **Q&A Search**: Search questions and answers on Naver.\n- **Local Search**: Search local information.\n- **Spelling Correction**: Correct spelling errors in text.\n- **Web Search**: Search web pages.\n- **Image Search**: Search images with filters.\n- **Shopping Search**: Search shopping items with filters.\n- **Document Search**: Search documents.\n\n\n\n## Naver MCP Tools\n\nThe following tools are implemented in the server:\n\n### Blog Search\nSearch blog posts on Naver.\n```python\nsearch_blog(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### News Search\nSearch news articles on Naver.\n```python\nsearch_news(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Book Search\nSearch books on Naver.\n```python\nsearch_book(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Advanced Book Search\nGet detailed book information using title or ISBN.\n```python\nget_book_adv(query: str = None, d_titl: str = None, d_isbn: str = None, ...)\n```\n\n### Adult Content Check\nCheck if a search term is adult content.\n```python\nadult_check(query: str)\n```\n\n### Encyclopedia Search\nSearch encyclopedia entries on Naver.\n```python\nsearch_encyc(query: str, display: int = 10, start: int = 1)\n```\n\n### Cafe Article Search\nSearch articles in Naver cafes.\n```python\nsearch_cafe_article(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Q&A Search\nSearch questions and answers on Naver.\n```python\nsearch_kin(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Local Search\nSearch local information on Naver.\n```python\nsearch_local(query: str, display: int = 10, start: int = 1, sort: str = \"random\")\n```\n\n### Spelling Correction\nCorrect spelling errors in a given text.\n```python\nfix_spelling(query: str)\n```\n\n### Web Search\nSearch web pages on Naver.\n```python\nsearch_webkr(query: str, display: int = 10, start: int = 1)\n```\n\n### Image Search\nSearch images on Naver with filters.\n```python\nsearch_image(query: str, display: int = 10, start: int = 1, sort: str = \"sim\", filter: str = \"all\")\n```\n\n### Shopping Search\nSearch shopping items on Naver with filters.\n```python\nsearch_shop(query: str, display: int = 10, start: int = 1, sort: str = \"sim\", filter: str = None, exclude: str = None)\n```\n\n### Document Search\nSearch documents on Naver.\n```python\nsearch_doc(query: str, display: int = 10, start: int = 1)\n```\n\n## License\n\nThis project is open source software [licensed as MIT](https://opensource.org/licenses/MIT).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "naver",
        "openapi",
        "search",
        "naver openapi",
        "naver services",
        "using naver"
      ],
      "category": "web-search"
    },
    "phpmac--fetch_mcp": {
      "owner": "phpmac",
      "name": "fetch_mcp",
      "url": "https://github.com/phpmac/fetch_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/phpmac.webp",
      "description": "Fetch web content in various formats like HTML, JSON, plain text, and Markdown. Retrieve and transform website data on demand with customizable headers.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-14T07:20:07Z",
      "readme_content": "# Fetch MCP Server\n\n[English](./README.md) | [中文](./README_ZH.md)\n\nThis MCP server provides functionality to fetch web content in various formats, including HTML, JSON, plain text, and Markdown.\n\n### Tools\n\n- **fetch_html**\n\n  - Fetch website content and return as HTML\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the raw HTML content of the webpage\n\n- **fetch_json**\n\n  - Fetch JSON file from URL\n  - Input parameters:\n    - `url` (string, required): URL of the JSON to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the parsed JSON content\n\n- **fetch_txt**\n\n  - Fetch website content and return as plain text (no HTML)\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the text content of the webpage with HTML tags, scripts, and styles removed\n\n- **fetch_markdown**\n  - Fetch website content and return as Markdown\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the webpage content converted to Markdown format\n\n### 2 Ways to Start\n\n1. bun\n\n```bash\nbun i\nbun start\n```\n\n2. docker\n\n```bash\ndocker compose up --build -d\n```\n\n### Usage\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch-mcp\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:3000/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-token-here\",\n        \"X-Custom-Header\": \"custom-value\"\n      },\n      \"useNodeEventSource\": true\n    }\n  }\n}\n```\n\n### Resources\n\nThis server does not provide any persistent resources. It is designed to fetch and transform web content on demand.\n\n### References\n\n- [Original Repository zcaceres/fetch-mcp](https://github.com/zcaceres/fetch-mcp)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "phpmac",
        "fetch_mcp",
        "fetch",
        "search phpmac",
        "phpmac fetch_mcp",
        "fetch web"
      ],
      "category": "web-search"
    },
    "pinkpixel-dev--prysm-mcp-server": {
      "owner": "pinkpixel-dev",
      "name": "prysm-mcp-server",
      "url": "https://github.com/pinkpixel-dev/prysm-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/pinkpixel-dev.webp",
      "description": "A server designed for web scraping that offers various modes for optimized content retrieval, URL analysis for efficient scraping strategies, and output formatting in markdown, HTML, or JSON. Additionally, it supports image extraction and provides functionalities for saving results to a specified directory.",
      "stars": 2,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T01:40:09Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/pinkpixel-dev-prysm-mcp-server-badge.png)](https://mseep.ai/app/pinkpixel-dev-prysm-mcp-server)\n\n# 🔍 Prysm MCP Server\n\nThe Prysm MCP (Model Context Protocol) Server enables AI assistants like Claude and others to scrape web content with high accuracy and flexibility.\n\n## ✨ Features\n\n- 🎯 **Multiple Scraping Modes**: Choose from focused (speed), balanced (default), or deep (thorough) modes\n- 🧠 **Content Analysis**: Analyze URLs to determine the best scraping approach\n- 📄 **Format Flexibility**: Format results as markdown, HTML, or JSON\n- 🖼️ **Image Support**: Optionally extract and even download images\n- 🔍 **Smart Scrolling**: Configure scroll behavior for single-page applications\n- 📱 **Responsive**: Adapts to different website layouts and structures\n- 💾 **File Output**: Save formatted results to your preferred directory\n\n## 🚀 Quick Start\n\n### Installation\n\n```bash\n# Recommended: Install the LLM-optimized version\nnpm install -g @pinkpixel/prysm-mcp\n\n# Or install the standard version\nnpm install -g prysm-mcp\n\n# Or clone and build\ngit clone https://github.com/pinkpixel-dev/prysm-mcp.git\ncd prysm-mcp\nnpm install\nnpm run build\n```\n\n### Integration Guides\n\nWe provide detailed integration guides for popular MCP-compatible applications:\n\n- [Cursor Integration Guide](./docs/cursor-mcp-integration.md)\n- [Claude Desktop Integration Guide](./docs/claude-desktop-mcp-integration.md)\n- [Windsurf Integration Guide](./docs/windsurf-mcp-integration.md)\n- [Cline Integration Guide](./docs/cline-mcp-integration.md)\n- [Roo Code Integration Guide](./docs/roo-code-mcp-integration.md)\n- [Open WebUI Integration Guide](./docs/openwebui-mcp-integration.md)\n\n### Usage\n\nThere are multiple ways to set up Prysm MCP Server:\n\n#### Using mcp.json Configuration\n\nCreate a `mcp.json` file in the appropriate location according to the above guides. \n\n```json\n{\n  \"mcpServers\": {\n    \"prysm-scraper\": {\n      \"description\": \"Prysm web scraper with custom output directories\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/prysm-mcp\"\n      ],\n      \"env\": {\n        \"PRYSM_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results\",\n        \"PRYSM_IMAGE_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results/images\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Tools\n\nThe server provides the following tools:\n\n### `scrapeFocused`\n\nFast web scraping optimized for speed (fewer scrolls, main content only).\n\n```\nPlease scrape https://example.com using the focused mode\n```\n\n**Available Parameters:**\n- `url` (required): URL to scrape\n- `maxScrolls` (optional): Maximum number of scroll attempts (default: 5)\n- `scrollDelay` (optional): Delay between scrolls in ms (default: 1000)\n- `scrapeImages` (optional): Whether to include images in results\n- `downloadImages` (optional): Whether to download images locally\n- `maxImages` (optional): Maximum images to extract\n- `output` (optional): Output directory for downloaded images\n\n### `scrapeBalanced`\n\nBalanced web scraping approach with good coverage and reasonable speed.\n\n```\nPlease scrape https://example.com using the balanced mode\n```\n\n**Available Parameters:**\n- Same as `scrapeFocused` with different defaults\n- `maxScrolls` default: 10\n- `scrollDelay` default: 2000\n- Adds `timeout` parameter to limit total scraping time (default: 30000ms)\n\n### `scrapeDeep`\n\nMaximum extraction web scraping (slower but thorough).\n\n```\nPlease scrape https://example.com using the deep mode with maximum scrolls\n```\n\n**Available Parameters:**\n- Same as `scrapeFocused` with different defaults\n- `maxScrolls` default: 20\n- `scrollDelay` default: 3000\n- `maxImages` default: 100\n\n### `formatResult`\n\nFormat scraped data into different structured formats (markdown, HTML, JSON).\n\n```\nFormat the scraped data as markdown\n```\n\n**Available Parameters:**\n- `data` (required): The scraped data to format\n- `format` (required): Output format - \"markdown\", \"html\", or \"json\"\n- `includeImages` (optional): Whether to include images in output (default: true)\n- `output` (optional): File path to save the formatted result\n\nYou can also save formatted results to a file by specifying an output path:\n\n```\nFormat the scraped data as markdown and save it to \"my-results/output.md\"\n```\n\n## ⚙️ Configuration\n\n### Output Directory\n\nBy default, when saving formatted results, files will be saved to `~/prysm-mcp/output/`. You can customize this in two ways:\n\n1. **Environment Variables**: Set environment variables to your preferred directories:\n\n```bash\n# Linux/macOS\nexport PRYSM_OUTPUT_DIR=\"/path/to/custom/directory\"\nexport PRYSM_IMAGE_OUTPUT_DIR=\"/path/to/custom/image/directory\"\n\n# Windows (Command Prompt)\nset PRYSM_OUTPUT_DIR=C:\\path\\to\\custom\\directory\nset PRYSM_IMAGE_OUTPUT_DIR=C:\\path\\to\\custom\\image\\directory\n\n# Windows (PowerShell)\n$env:PRYSM_OUTPUT_DIR=\"C:\\path\\to\\custom\\directory\"\n$env:PRYSM_IMAGE_OUTPUT_DIR=\"C:\\path\\to\\custom\\image\\directory\"\n```\n\n2. **Tool Parameter**: Specify output paths directly when calling the tools:\n\n```\n# For general results\nFormat the scraped data as markdown and save it to \"/absolute/path/to/file.md\"\n\n# For image downloads when scraping\nPlease scrape https://example.com and download images to \"/absolute/path/to/images\"\n```\n\n3. **MCP Configuration**: In your MCP configuration file (e.g., `.cursor/mcp.json`), you can set these environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"prysm-scraper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@pinkpixel/prysm-mcp\"],\n      \"env\": {\n        \"PRYSM_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results\",\n        \"PRYSM_IMAGE_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results/images\"\n      }\n    }\n  }\n}\n```\n\nIf `PRYSM_IMAGE_OUTPUT_DIR` is not specified, it will default to a subfolder named `images` inside the `PRYSM_OUTPUT_DIR`.\n\nIf you provide only a relative path or filename, it will be saved relative to the configured output directory.\n\n### Path Handling Rules\n\nThe `formatResult` tool handles paths in the following ways:\n\n- **Absolute paths**: Used exactly as provided (`/home/user/file.md`)\n- **Relative paths**: Saved relative to the configured output directory (`subfolder/file.md`)\n- **Filename only**: Saved in the configured output directory (`output.md`)\n- **Directory path**: If the path points to a directory, a filename is auto-generated based on content and timestamp\n\n## 🏗️ Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server locally\nnode bin/prysm-mcp\n\n# Debug MCP communication\nDEBUG=mcp:* node bin/prysm-mcp\n\n# Set custom output directories\nPRYSM_OUTPUT_DIR=./my-output PRYSM_IMAGE_OUTPUT_DIR=./my-output/images node bin/prysm-mcp\n```\n\n### Running via npx\n\nYou can run the server directly with npx without installing:\n\n```bash\n# Run with default settings\nnpx @pinkpixel/prysm-mcp\n\n# Run with custom output directories\nPRYSM_OUTPUT_DIR=./my-output PRYSM_IMAGE_OUTPUT_DIR=./my-output/images npx @pinkpixel/prysm-mcp\n```\n\n## 📋 License\n\nMIT\n\n## 🙏 Credits\n\nDeveloped by [Pink Pixel](https://pinkpixel.dev)\n\nPowered by the [Model Context Protocol](https://modelcontextprotocol.io) and [Puppeteer](https://pptr.dev/) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "pinkpixel",
        "web",
        "search pinkpixel",
        "web scraping",
        "efficient scraping"
      ],
      "category": "web-search"
    },
    "pjookim--mcp-visit-korea": {
      "owner": "pjookim",
      "name": "mcp-visit-korea",
      "url": "https://github.com/pjookim/mcp-visit-korea",
      "imageUrl": "/freedevtools/mcp/pfp/pjookim.webp",
      "description": "Provides information related to tourism in Korea, including attractions, festivals, and accommodations. Supports various search methods based on area codes, keywords, or locations for detailed tourism content.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T06:03:50Z",
      "readme_content": "# Korea Tour MCP Server\n[![smithery badge](https://smithery.ai/badge/@pjookim/mcp-visit-korea)](https://smithery.ai/server/@pjookim/mcp-visit-korea)\nMCP (Model Context Protocol) server providing Korean tourism information.\n\n## Features\n- Area code lookup: Query metropolitan cities/provinces and sub-regions\n- Tourism information search: Support for region-based, keyword-based, and location-based searches\n- Tourism content detail lookup: Provides detailed information about attractions, festivals, accommodations, etc.\n\n\n### Installing via Smithery\n\nTo install mcp-visit-korea for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pjookim/mcp-visit-korea):\n\n```bash\nnpx -y @smithery/cli install @pjookim/mcp-visit-korea --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pjookim",
        "mcp",
        "korea",
        "tourism korea",
        "visit korea",
        "mcp visit"
      ],
      "category": "web-search"
    },
    "pl728--substack-fetcher-mcp": {
      "owner": "pl728",
      "name": "substack-fetcher-mcp",
      "url": "https://github.com/pl728/substack-fetcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pl728.webp",
      "description": "Fetch and read articles from Trade Companion by Adam Mancini on Substack, providing access to subscriber-only content in a clean, readable format. It integrates with Claude AI for an enhanced reading experience.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-06T07:42:32Z",
      "readme_content": "# Substack Reader\n\nA tool to fetch and read articles from Trade Companion by Adam Mancini on Substack.\n\n## Setup\n\n### Prerequisites\n\n1. Python 3.8+\n2. uv package manager for Python\n3. Claude AI assistant\n\n### Installation\n\n1. Install uv package manager if you don't have it already:\n   ```bash\n   curl -sSf https://install.ultraviolet.dev | sh\n   ```\n\n2. Create and activate a virtual environment:\n   ```bash\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n3. Install dependencies using the pyproject.toml file:\n   ```bash\n   uv pip install -e .\n   ```\n\n### Setting up Substack Authentication\n\nTo access subscriber-only content, you'll need to provide your Substack cookies:\n\n1. Install the Cookie-Editor extension for your browser:\n   - [Chrome Web Store](https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm)\n   - [Firefox Add-ons](https://addons.mozilla.org/en-US/firefox/addon/cookie-editor/)\n\n2. Log in to your Substack account at [tradecompanion.substack.com](https://tradecompanion.substack.com)\n\n3. Click on the Cookie-Editor extension icon\n\n4. Click \"Export\" and select \"Export as JSON\" (This copies the cookies to your clipboard)\n\n5. Create a file named `substack_cookies.json` in the root directory of this project\n\n6. Paste the copied cookies into this file and save\n\n## Usage with Claude\n\nThis tool is designed to be used with Claude AI assistant. To set it up:\n\n1. Configure Claude to use this MCP server by adding the following to your Claude config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"substack_reader\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/substack_reader\",\n        \"run\",\n        \"substack_reader.py\"\n      ]\n    }\n  },\n  \"globalShortcut\": \"Ctrl+Space\"\n}\n```\n\nReplace `/path/to/substack_reader` with the actual path to your substack_reader directory.\n\n2. When properly configured, Claude will automatically connect to this MCP server when launched.\n\n3. You can then ask Claude to fetch the latest Trade Companion article.\n\n## Features\n\n- Fetches the latest Trade Companion articles by Adam Mancini\n- Extracts article content in plain text format\n- Preserves headings, paragraphs, and list items\n- Excludes the \"My Trade Methodology Fundamentals\" article\n\n## Privacy Note\n\nYour Substack cookies are stored locally in the `substack_cookies.json` file and are only used to authenticate requests to Substack. They are not sent anywhere else or exposed in any way.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "substack",
        "fetcher",
        "fetch",
        "substack fetcher",
        "pl728 substack",
        "fetch read"
      ],
      "category": "web-search"
    },
    "podaac--cmr-mcp": {
      "owner": "podaac",
      "name": "cmr-mcp",
      "url": "https://github.com/podaac/cmr-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/podaac.webp",
      "description": "Integrate AI with NASA's Catalog of datasets through Earthdata, enabling seamless searches for Earthdata metadata. Enhance data discovery with intelligent search capabilities by accessing the common metadata repository (CMR).",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-10T14:48:04Z",
      "readme_content": "# Model Context Protocol (MCP) for NASA Earthdata Search (CMR)\n\nThis module is a [model context protocol](https://modelcontextprotocol.io/introduction) (MCP) for NASA's earthdata common metedata repository (CMR). The goal of this MCP server is to integrate AI retrievals with NASA Catalog of datasets by way of Earthaccess.\n\n## Dependencies\nuv -  a rust based python package manager\na LLM client, such as Claude desktop or chatGPT desktop (for consuming the MCP)\n\n## Install and Run\n\nClone the repository to your local environment, or where your LLM client is running.\n\n```\ngit clone https://github.com/podaac/cmr-mcp.git\ncd cmr-mcp\n```\n\n\n### Install uv \n\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n\n```\nuv venv\nsource .venv/bin/activate\n```\n\n###  Install packages with uv\n```\nuv sync\n```\n\nuse the outputs of `which uv` (UV_LIB) and `PWD` (CMR_MCP_INSTALL) to update the following configuration.\n\n\n## Adding to AI Framework\n\nIn this example we'll use Claude desktop.\n\nUpdate the `claude_desktop_config.json` file (sometimes this must be created). On a mac, this is often found in `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nAdd the following configuration, filling in the values of UV_LIB and CMR_MCP_INSTALL - don't use environment variables here.\n\n```\n{\n    \"mcpServers\": {\n        \"cmr\": {\n            \"command\": \"$UV_LIB$\",\n            \"args\": [\n                \"--directory\",\n                \"$CMR_MCP_INSTALL$\",\n                \"run\",\n                \"cmr-search.py\"\n            ]\n        }\n    }\n}\n```\n\n## Use the MCP Server\n\nSimply prompt your agent to `search cmr for...` data. Below is a simple example of this in action.\n\n\n\nOther prompts that can work:\n\n1. Search CMR for datasets from 2024 to 2025\n2. Search CMR for PO.DAAC datasets from 2020 to 2024 with keyword Climate",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthdata",
        "metadata",
        "nasa",
        "searches earthdata",
        "earthdata metadata",
        "metadata repository"
      ],
      "category": "web-search"
    },
    "pragmar--mcp-server-webcrawl": {
      "owner": "pragmar",
      "name": "mcp-server-webcrawl",
      "url": "https://github.com/pragmar/mcp-server-webcrawl",
      "imageUrl": "/freedevtools/mcp/pfp/pragmar.webp",
      "description": "Integrate web crawl data with AI language models for content filtering and analysis. Offers a full-text search interface and supports multiple crawlers for enhanced data insights.",
      "stars": 26,
      "forks": 6,
      "license": "Other",
      "language": "HTML",
      "updated_at": "2025-09-24T20:10:05Z",
      "readme_content": "<p align=\"center\">\r\n  \r\n</p>\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://pragmar.com/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Website</a> |\r\n  <a href=\"https://github.com/pragmar/mcp-server-webcrawl\" style=\"margin: 0 10px;\">GitHub</a> |\r\n  <a href=\"https://pragmar.github.io/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Docs</a> |\r\n  <a href=\"https://pypi.org/project/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">PyPi</a>\r\n</p>\r\n\r\n# mcp-server-webcrawl\r\n\r\nAdvanced search and retrieval for web crawler data. With **mcp-server-webcrawl**, your AI client filters and analyzes web content under your direction or autonomously. The server includes a fulltext search interface with boolean support, and resource filtering by type, HTTP status, and more.\r\n\r\n**mcp-server-webcrawl** provides the LLM a complete menu with which to search, and works with a variety of web crawlers:\r\n\r\n| Crawler/Format | Description | Platforms | Setup Guide |\r\n|---|---|---|---|\r\n| [**ArchiveBox**][1] | Web archiving tool | macOS/Linux | [Setup Guide][8] |\r\n| [**HTTrack**][2] | GUI mirroring tool | macOS/Windows/Linux | [Setup Guide][9] |\r\n| [**InterroBot**][3] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][10] |\r\n| [**Katana**][4] | CLI security-focused crawler | macOS/Windows/Linux | [Setup Guide][11] |\r\n| [**SiteOne**][5] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][12] |\r\n| [**WARC**][6] | Standard web archive format | varies by client | [Setup Guide][13] |\r\n| [**wget**][7] | CLI website mirroring tool | macOS/Linux | [Setup Guide][14] |\r\n\r\n[1]: https://archivebox.io\r\n[2]: https://github.com/xroche/httrack\r\n[3]: https://interro.bot\r\n[4]: https://github.com/projectdiscovery/katana\r\n[5]: https://crawler.siteone.io\r\n[6]: https://en.wikipedia.org/wiki/WARC_(file_format)\r\n[7]: https://en.wikipedia.org/wiki/Wget\r\n[8]: https://pragmar.github.io/mcp-server-webcrawl/guides/archivebox.html\r\n[9]: https://pragmar.github.io/mcp-server-webcrawl/guides/httrack.html\r\n[10]: https://pragmar.github.io/mcp-server-webcrawl/guides/interrobot.html\r\n[11]: https://pragmar.github.io/mcp-server-webcrawl/guides/katana.html\r\n[12]: https://pragmar.github.io/mcp-server-webcrawl/guides/siteone.html\r\n[13]: https://pragmar.github.io/mcp-server-webcrawl/guides/warc.html\r\n[14]: https://pragmar.github.io/mcp-server-webcrawl/guides/wget.html\r\n\r\n**mcp-server-webcrawl** is free and open source, and requires Claude Desktop and Python (>=3.10). It is installed on the command line, via pip install:\r\n\r\n```bash\r\npip install mcp-server-webcrawl\r\n```\r\n\r\nFor step-by-step MCP server setup, refer to the [Setup Guides](https://pragmar.github.io/mcp-server-webcrawl/guides.html).\r\n\r\n## Features\r\n\r\n* Claude Desktop ready\r\n* Multi-crawler compatible\r\n* Filter by type, status, and more\r\n* Boolean search support\r\n* Support for Markdown and snippets\r\n* Roll your own website knowledgebase\r\n\r\n## Prompt Routines\r\n\r\n**mcp-server-webcrawl** provides the toolkit necessary to search web crawl data freestyle, figuring it out as you go, reacting to each query. This is what it was designed for.\r\n\r\nIt is also capable of running routines (as prompts). You can write these yourself, or use the ones provided. These prompts are **copy and paste**, and used as raw Markdown. They are enabled by the advanced search provided to the LLM; queries and logic can be embedded in a procedural set of instructions, or even an input loop as is the case with Gopher Service.\r\n\r\n| Prompt | Download | Category | Description |\r\n|--------|----------|----------|-------------|\r\n|🔍 **SEO Audit** | [`auditseo.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditseo.md) | audit | Technical SEO (search engine optimization) analysis. Covers the basics, with options to dive deeper. |\r\n|🔗 **404 Audit** | [`audit404.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/audit404.md) | audit | Broken link detection and pattern analysis. Not only finds issues, but suggests fixes. |\r\n|⚡&nbsp;**Performance&nbsp;Audit** | [`auditperf.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditperf.md) | audit | Website speed and optimization analysis. Real talk. |\r\n|📁 **File Audit** | [`auditfiles.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditfiles.md) | audit | File organization and asset analysis. Discover the composition of your website. |\r\n|🌐 **Gopher Interface** | [`gopher.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/gopher.md) | interface | An old-fashioned search interface inspired by the Gopher clients of yesteryear. |\r\n|⚙️ **Search Test** | [`testsearch.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/testsearch.md) | self-test | A battery of tests to check for Boolean logical inconsistencies in the search query parser and subsequent FTS5 conversion. |\r\n\r\nIf you want to shortcut the site selection (one less query), paste the markdown and in the same request, type \"run pasted for [site name or URL].\" It will figure it out. When pasted without additional context, you should be prompted to select from a list of crawled sites.\r\n\r\n## Boolean Search Syntax\r\n\r\nThe query engine supports field-specific (`field: value`) searches and complex boolean expressions. Fulltext is supported as a combination of the url, content, and headers fields.\r\n\r\nWhile the API interface is designed to be consumed by the LLM directly, it can be helpful to familiarize yourself with the search syntax. Searches generated by the LLM are inspectable, but generally collapsed in the UI. If you need to see the query, expand the MCP collapsible.\r\n\r\n**Example Queries**\r\n\r\n| Query Example | Description |\r\n|--------------|-------------|\r\n| privacy | fulltext single keyword match |\r\n| \"privacy policy\" | fulltext match exact phrase |\r\n| boundar* | fulltext wildcard matches results starting with *boundar* (boundary, boundaries) |\r\n| id: 12345 | id field matches a specific resource by ID |\r\n| url: example.com/somedir | url field matches results with URL containing example.com/somedir |\r\n| type: html | type field matches for HTML pages only |\r\n| status: 200 | status field matches specific HTTP status codes (equal to 200) |\r\n| status: >=400 | status field matches specific HTTP status code (greater than or equal to 400) |\r\n| content: h1 | content field matches content (HTTP response body, often, but not always HTML) |\r\n| headers: text/xml | headers field matches HTTP response headers |\r\n| privacy AND policy | fulltext matches both |\r\n| privacy OR policy | fulltext matches either |\r\n| policy NOT privacy | fulltext matches policies not containing privacy |\r\n| (login OR signin) AND form | fulltext matches fulltext login or signin with form |\r\n| type: html AND status: 200 | fulltext matches only HTML pages with HTTP success |\r\n\r\n## Field Search Definitions\r\n\r\nField search provides search precision, allowing you to specify which columns of the search index to filter. Rather than searching the entire content, you can restrict your query to specific attributes like URLs, headers, or content body. This approach improves efficiency when looking for specific attributes or patterns within crawl data.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | database ID |\r\n| url | resource URL |\r\n| type | enumerated list of types (see types table) |\r\n| size | file size in bytes |\r\n| status | HTTP response codes |\r\n| headers | HTTP response headers |\r\n| content | HTTP body—HTML, CSS, JS, and more |\r\n\r\n## Field Content\r\n\r\nA subset of fields can be independently requested with results, while core fields are always on. Use of headers and content can consume tokens quickly. Use judiciously, or use extras to crunch more results into the context window. Fields are a top level argument, independent of any field searching taking place in the query.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | always available |\r\n| url | always available |\r\n| type | always available |\r\n| status | always available |\r\n| created | on request |\r\n| modified | on request |\r\n| size | on request |\r\n| headers | on request |\r\n| content | on request |\r\n\r\n## Content Types\r\n\r\nCrawls contain resource types beyond HTML pages. The `type:` field search allows filtering by broad content type groups, particularly useful when filtering images without complex extension queries. For example, you might search for `type: html NOT content: login` to find pages without \"login,\" or `type: img` to analyze image resources. The table below lists all supported content types in the search system.\r\n\r\n| Type | Description |\r\n|------|-------------|\r\n| html | webpages |\r\n| iframe | iframes |\r\n| img | web images |\r\n| audio | web audio files |\r\n| video | web video files |\r\n| font | web font files |\r\n| style | CSS stylesheets |\r\n| script | JavaScript files |\r\n| rss | RSS syndication feeds |\r\n| text | plain text content |\r\n| pdf | PDF files |\r\n| doc | MS Word documents |\r\n| other | uncategorized |\r\n\r\n## Extras\r\n\r\nThe `extras` parameter provides additional processing options, transforming HTTP data (markdown, snippets, regex, xpath), or connecting the LLM to external data (thumbnails). These options can be combined as needed to achieve the desired result format.\r\n\r\n| Extra | Description |\r\n|-------|-------------|\r\n| thumbnails | Generates base64 encoded images to be viewed and analyzed by AI models. Enables image description, content analysis, and visual understanding while keeping token output minimal. Works with images, which can be filtered using `type: img` in queries. SVG is not supported. |\r\n| markdown | Provides the HTML content field as concise Markdown, reducing token usage and improving readability for LLMs. Works with HTML, which can be filtered using `type: html` in queries. |\r\n| regex | Extracts regular expression matches from crawled files such as HTML, CSS, JavaScript, etc. Not as precise a tool as XPath for HTML, but supports any text file as a data source. One or more regex patterns can be requested, using the `extrasRegex` argument. |\r\n| snippets | Matches fulltext queries to contextual keyword usage within the content. When used without requesting the content field (or markdown extra), it can provide an efficient means of refining a search without pulling down the complete page contents. Also great for rendering old school hit-highlighted results as a list, like Google search in 1999. Works with HTML, CSS, JS, or any text-based, crawled file. |\r\n| xpath | Extracts XPath selector data, used in scraping HTML content. Use XPath's text() selector for text-only, element selectors return outerHTML. Only supported with `type: html`, other types will be ignored. One or more XPath selectors (//h1, count(//h1), etc.) can be requested, using the `extrasXpath` argument. |\r\n\r\nExtras provide a means of producing token-efficient HTTP content responses. Markdown produces roughly 1/3 the bytes of the source HTML, snippets are generally 500 or so bytes per result, and XPath can be as specific or broad as you choose. The more focused your requests, the more results you can fit into your LLM session.\r\n\r\nThe idea, of course, is that the LLM takes care of this for you. If you notice your LLM developing an affinity to the \"content\" field (full HTML), a nudge in chat to budget tokens using the extras feature should be all that is needed.\r\n\r\n## Interactive Mode\r\n\r\n**No AI, just classic Boolean search of your web-archives in a terminal.**\r\n\r\nmcp-server-webcrawl can double as a terminal search for your web archives. You can run it against your local archives, but it gets more interesting when you realize you can ssh into any remote host and view archives sitting on that host. No downloads, syncs, multifactor logins, or other common drudgery required. With interactive mode, you can be in and searching a crawl sitting on a remote server in no time at all.\r\n\r\nLaunch with --crawler and --datasource to load into search immediately, or use setup datasrc and crawler in-app.\r\n\r\n```bash\r\nmcp-server-webcrawl --crawler wget --datasrc /path/to/datasrc --interactive\r\n# or manually enter crawler and datasrc in the UI\r\nmcp-server-webcrawl --interactive\r\n```\r\n\r\nInteractive mode is a way to search through tranches of crawled data, whenever, whereever... in a terminal.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcrawl",
        "search",
        "crawl",
        "web crawl",
        "crawl data",
        "web search"
      ],
      "category": "web-search"
    },
    "priyankark--a11y-mcp": {
      "owner": "priyankark",
      "name": "a11y-mcp",
      "url": "https://github.com/priyankark/a11y-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/priyankark.webp",
      "description": "Perform accessibility audits on webpages and generate detailed reports and summaries that comply with WCAG standards. Integrate the findings into an AI-assisted workflow to fix accessibility issues efficiently.",
      "stars": 27,
      "forks": 7,
      "license": "Mozilla Public License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-29T08:04:09Z",
      "readme_content": "# A11y MCP Server\n\nAn MCP (Model Context Protocol) server for performing accessibility audits on webpages using axe-core. Use the results in an agentic loop with your favorite AI assistants (Cline/Cursor/GH Copilot) and let them fix a11y issues for you!\n\n<a href=\"https://glama.ai/mcp/servers/@priyankark/a11y-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@priyankark/a11y-mcp/badge\" alt=\"a11y-mcp MCP server\" />\n</a>\n\n## Features\n\n- Perform detailed accessibility audits on any webpage\n- Get a summary of accessibility issues\n- Filter audits by specific WCAG criteria\n- Include HTML snippets in the results for easier debugging\n\n## Installation\n\n```bash\n# Install globally\nnpm install -g a11y-mcp\n\n# Or use directly with npx\nnpx a11y-mcp\n```\n\n## Configuration\n\nTo use this MCP server with Cline, you need to add it to your MCP settings configuration file.\n\n### MCP configuration \n\nAdd the following to the `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"a11y\": {\n      \"command\": \"npx\",\n      \"args\": [\"a11y-mcp\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Available Tools\n\n### audit_webpage\n\nPerforms a detailed accessibility audit on a webpage.\n\n**Parameters:**\n- `url` (required): URL of the webpage to audit\n- `includeHtml` (optional): Whether to include HTML snippets in the results (default: false)\n- `tags` (optional): Array of specific accessibility tags to check (e.g., wcag2a, wcag2aa, wcag21a, best-practice)\n\n**Example:**\n```\nUse the a11y MCP server to audit example.com for accessibility issues\n```\n\n### get_summary\n\nGets a summary of accessibility issues for a webpage.\n\n**Parameters:**\n- `url` (required): URL of the webpage to audit\n\n**Example:**\n```\nGive me an accessibility summary of example.com\n```\n\n## Example Usage\n\nOnce configured, you can ask Claude to use the MCP server to perform accessibility audits:\n\n1. \"Can you check example.com for accessibility issues?\"\n2. \"Audit my website at https://mywebsite.com for WCAG 2.1 AA compliance\"\n3. \"Give me a summary of accessibility issues on https://example.com\"\n4. \"Check if my local development server at http://localhost:3000 has any critical accessibility problems\"\n\n## Development\n\nTo run the server locally for development:\n\n```bash\nnpm start\n```\n\n## Releasing\n\nThis project includes a release script to help with versioning and publishing to npm. The script handles version bumping, running tests, git tagging, and npm publishing.\n\nTo release a new version:\n\n```bash\n# Make sure the script is executable\nchmod +x release.sh\n\n# Release a patch version (default)\n./release.sh\n\n# Release a minor version\n./release.sh --minor\n\n# Release a major version\n./release.sh --major\n\n# Release a specific version\n./release.sh --version=1.2.3\n\n# Skip git operations\n./release.sh --skip-git\n\n# Dry run (no changes will be made)\n./release.sh --dry-run\n\n# Force release even with uncommitted changes\n./release.sh --force\n```\n\nFor more information, run:\n\n```bash\n./release.sh --help\n```\n\n## License\nMPL 2.0\n\n## Credits\nThis project builds atop the awesome work done by [axe-core](https://github.com/dequelabs/axe-core)",
      "npm_url": "https://www.npmjs.com/package/a11y-mcp",
      "npm_downloads": 4946,
      "keywords": [
        "accessibility",
        "webpages",
        "search",
        "accessibility audits",
        "perform accessibility",
        "fix accessibility"
      ],
      "category": "web-search"
    },
    "priyankark--lighthouse-mcp": {
      "owner": "priyankark",
      "name": "lighthouse-mcp",
      "url": "https://github.com/priyankark/lighthouse-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/priyankark.webp",
      "description": "Run comprehensive audits on web pages to measure performance metrics, simulate different device environments, and apply network conditions. Provides detailed insights and scores to help optimize website performance.",
      "stars": 45,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T00:44:25Z",
      "readme_content": "# Lighthouse MCP Server\n\nAn MCP server that wraps around Google's Lighthouse tool to help measure various performance metrics for web pages.\n\n## Features\n\n- Run comprehensive Lighthouse audits on any URL\n- Get performance scores and metrics\n- Configure device emulation (mobile/desktop)\n- Control network throttling\n- Select specific audit categories\n\n## Installation\n\n### Option 1: From MCP Registry (Recommended)\n\nThis server is available in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io/servers/io.github.priyankark/lighthouse-mcp). Install it using your MCP client or Claude Desktop.\n\n### Option 2: Using npx\n\nYou can run the tool directly using npx without installation:\n\n```bash\nnpx lighthouse-mcp\n```\n\n### Option 3: Global Installation\n\nInstall the package globally from npm:\n\n```bash\nnpm install -g lighthouse-mcp\n```\n\nThen run it:\n\n```bash\nlighthouse-mcp\n```\n\n### Option 4: Local Development\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n4. Run the server:\n   ```bash\n   npm start\n   ```\n\n## MCP Configuration\n\n### When installed via npm (global or npx)\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"lighthouse\": {\n      \"command\": \"npx\",\n      \"args\": [\"lighthouse-mcp\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### When using local development version\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"lighthouse\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/lighthouse-mcp/build/index.js\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to/lighthouse-mcp` with the actual path to this project.\n\n## Available Tools\n\n### run_audit\n\nRun a comprehensive Lighthouse audit on a URL.\n\n**Parameters:**\n- `url` (required): The URL to audit\n- `categories` (optional): Array of categories to audit (defaults to all)\n  - Options: \"performance\", \"accessibility\", \"best-practices\", \"seo\", \"pwa\"\n- `device` (optional): Device to emulate (defaults to \"mobile\")\n  - Options: \"mobile\", \"desktop\"\n- `throttling` (optional): Whether to apply network throttling (defaults to true)\n\n**Example:**\n```json\n{\n  \"url\": \"https://example.com\",\n  \"categories\": [\"performance\", \"accessibility\"],\n  \"device\": \"desktop\",\n  \"throttling\": false\n}\n```\n\n### get_performance_score\n\nGet just the performance score for a URL.\n\n**Parameters:**\n- `url` (required): The URL to audit\n- `device` (optional): Device to emulate (defaults to \"mobile\")\n  - Options: \"mobile\", \"desktop\"\n\n**Example:**\n```json\n{\n  \"url\": \"https://example.com\",\n  \"device\": \"mobile\"\n}\n```\n\n## Example Usage\n\nOnce the MCP server is configured, you can use it with Claude:\n\n```\nWhat's the performance score for example.com?\n```\n\nClaude will use the `get_performance_score` tool to analyze the website and return the results.\n\n## Requirements\n\n- Node.js 16+\n- Chrome/Chromium browser (for Lighthouse)\n\n## Endorsements\n<a href=\"https://glama.ai/mcp/servers/@priyankark/lighthouse-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@priyankark/lighthouse-mcp/badge\" />\n</a>\n",
      "npm_url": "https://www.npmjs.com/package/lighthouse-mcp",
      "npm_downloads": 6218,
      "keywords": [
        "performance",
        "web",
        "lighthouse",
        "website performance",
        "performance metrics",
        "priyankark lighthouse"
      ],
      "category": "web-search"
    },
    "pskill9--web-search": {
      "owner": "pskill9",
      "name": "web-search",
      "url": "https://github.com/pskill9/web-search",
      "imageUrl": "/freedevtools/mcp/pfp/pskill9.webp",
      "description": "Enables free web searching using Google search results without requiring API keys. Returns structured results including titles, URLs, and descriptions, configurable by the number of results returned per search.",
      "stars": 293,
      "forks": 116,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T11:28:26Z",
      "readme_content": "# Web Search MCP Server\n\nA Model Context Protocol (MCP) server that enables free web searching using Google search results, with no API keys required.\n\n## Features\n\n- Search the web using Google search results\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n\n## Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\nFor VSCode (Claude Dev Extension):\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `search` that accepts the following parameters:\n\n```typescript\n{\n  \"query\": string,    // The search query\n  \"limit\": number     // Optional: Number of results to return (default: 5, max: 10)\n}\n```\n\nExample usage:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"your search query\",\n    limit: 3  // optional\n  }\n})\n```\n\nExample response:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description of the search result...\"\n  }\n]\n```\n\n## Limitations\n\nSince this tool uses web scraping of Google search results, there are some important limitations to be aware of:\n\n1. **Rate Limiting**: Google may temporarily block requests if too many searches are performed in a short time. To avoid this:\n   - Keep searches to a reasonable frequency\n   - Use the limit parameter judiciously\n   - Consider implementing delays between searches if needed\n\n2. **Result Accuracy**: \n   - The tool relies on Google's HTML structure, which may change\n   - Some results might be missing descriptions or other metadata\n   - Complex search operators may not work as expected\n\n3. **Legal Considerations**:\n   - This tool is intended for personal use\n   - Respect Google's terms of service\n   - Consider implementing appropriate rate limiting for your use case\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n",
      "npm_url": "https://www.npmjs.com/package/web-search",
      "npm_downloads": 629,
      "keywords": [
        "pskill9",
        "search",
        "google",
        "search pskill9",
        "pskill9 web",
        "web search"
      ],
      "category": "web-search"
    },
    "pskill9--website-downloader": {
      "owner": "pskill9",
      "name": "website-downloader",
      "url": "https://github.com/pskill9/website-downloader",
      "imageUrl": "/freedevtools/mcp/pfp/pskill9.webp",
      "description": "Downloads entire websites for offline access, preserving the website structure and converting links for local use.",
      "stars": 138,
      "forks": 26,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T02:50:05Z",
      "readme_content": "# Website Downloader MCP Server\n\nThis MCP server provides a tool to download entire websites using wget. It preserves the website structure and converts links to work locally.\n\n<a href=\"https://glama.ai/mcp/servers/egcwr79vu2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/egcwr79vu2/badge\" alt=\"Google Workspace Server MCP server\" /></a>\n\n## Prerequisites\n\nThe server requires `wget` to be installed on your system.\n\n### Installing wget\n\n#### macOS\nUsing Homebrew:\n```bash\nbrew install wget\n```\n\n#### Linux (Debian/Ubuntu)\n```bash\nsudo apt-get update\nsudo apt-get install wget\n```\n\n#### Linux (Red Hat/Fedora)\n```bash\nsudo dnf install wget\n```\n\n#### Windows\n1. Using [Chocolatey](https://chocolatey.org/):\n```bash\nchoco install wget\n```\n\n2. Or download the binary from: https://eternallybored.org/misc/wget/\n   - Download the latest wget.exe\n   - Place it in a directory that's in your PATH (e.g., C:\\Windows\\System32)\n\n## Usage\n\nThe server provides a tool called `download_website` with the following parameters:\n\n- `url` (required): The URL of the website to download\n- `outputPath` (optional): The directory where the website should be downloaded. Defaults to the current directory.\n- `depth` (optional): Maximum depth level for recursive downloading. Defaults to infinite. Set to 0 for just the specified page, 1 for direct links, etc.\n\n### Example\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"outputPath\": \"/path/to/output\",\n  \"depth\": 2  // Optional: Download up to 2 levels deep\n}\n```\n\n## Features\n\nThe website downloader:\n- Downloads recursively with infinite depth\n- Includes all page requisites (CSS, images, etc.)\n- Converts links to work locally\n- Adds appropriate extensions to files\n- Restricts downloads to the same domain\n- Preserves the website structure\n\n## Installation\n\n1. Build the server:\n```bash\nnpm install\nnpm run build\n```\n\n2. Add to MCP settings:\n```json\n{\n  \"mcpServers\": {\n    \"website-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/website-downloader/build/index.js\"]\n    }\n  }\n}\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pskill9",
        "downloader",
        "downloads",
        "pskill9 website",
        "website downloader",
        "search pskill9"
      ],
      "category": "web-search"
    },
    "qpd-v--mcp-DEEPwebresearch": {
      "owner": "qpd-v",
      "name": "mcp-DEEPwebresearch",
      "url": "https://github.com/qpd-v/mcp-DEEPwebresearch",
      "imageUrl": "/freedevtools/mcp/pfp/qpd-v.webp",
      "description": "Provides real-time web research capabilities by enabling direct extraction of content from webpages. Optimized for performance within MCP timeout limits, facilitating efficient access to web data.",
      "stars": 77,
      "forks": 21,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:37Z",
      "readme_content": "# MCP Deep Web Research Server (v0.3.0)\n\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18-brightgreen.svg)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Model Context Protocol (MCP) server for advanced web research.\n\n<a href=\"https://glama.ai/mcp/servers/5afpizjl6x\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5afpizjl6x/badge\" alt=\"Web Research Server MCP server\" /></a>\n\n## Latest Changes\n\n- Added visit_page tool for direct webpage content extraction\n- Optimized performance to work within MCP timeout limits\n  * Reduced default maxDepth and maxBranching parameters\n  * Improved page loading efficiency\n  * Added timeout checks throughout the process\n  * Enhanced error handling for timeouts\n\n> This project is a fork of [mcp-webresearch](https://github.com/mzxrai/mcp-webresearch) by [mzxrai](https://github.com/mzxrai), enhanced with additional features for deep web research capabilities. We're grateful to the original creators for their foundational work.\n\nBring real-time info into Claude with intelligent search queuing, enhanced content extraction, and deep research capabilities.\n\n## Features\n\n- Intelligent Search Queue System\n  - Batch search operations with rate limiting\n  - Queue management with progress tracking\n  - Error recovery and automatic retries\n  - Search result deduplication\n\n- Enhanced Content Extraction\n  - TF-IDF based relevance scoring\n  - Keyword proximity analysis\n  - Content section weighting\n  - Readability scoring\n  - Improved HTML structure parsing\n  - Structured data extraction\n  - Better content cleaning and formatting\n\n- Core Features\n  - Google search integration\n  - Webpage content extraction\n  - Research session tracking\n  - Markdown conversion with improved formatting\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\n### Global Installation (Recommended)\n\n```bash\n# Install globally using npm\nnpm install -g mcp-deepwebresearch\n\n# Or using yarn\nyarn global add mcp-deepwebresearch\n\n# Or using pnpm\npnpm add -g mcp-deepwebresearch\n```\n\n### Local Project Installation\n\n```bash\n# Using npm\nnpm install mcp-deepwebresearch\n\n# Using yarn\nyarn add mcp-deepwebresearch\n\n# Using pnpm\npnpm add mcp-deepwebresearch\n```\n\n### Claude Desktop Integration\n\nAfter installing the package, add this entry to your `claude_desktop_config.json`:\n\n#### Windows\n```json\n{\n  \"mcpServers\": {\n    \"deepwebresearch\": {\n      \"command\": \"mcp-deepwebresearch\",\n      \"args\": []\n    }\n  }\n}\n```\nLocation: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### macOS\n```json\n{\n  \"mcpServers\": {\n    \"deepwebresearch\": {\n      \"command\": \"mcp-deepwebresearch\",\n      \"args\": []\n    }\n  }\n}\n```\nLocation: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n### First-time Setup\n\nAfter installation, run this command to install required browser dependencies:\n```bash\nnpx playwright install chromium\n```\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `deepwebresearch` → `agentic-research`.\n\n### Tools\n\n1. `deep_research`\n   - Performs comprehensive research with content analysis\n   - Arguments:\n     ```typescript\n     {\n       topic: string;\n       maxDepth?: number;      // default: 2\n       maxBranching?: number;  // default: 3\n       timeout?: number;       // default: 55000 (55 seconds)\n       minRelevanceScore?: number;  // default: 0.7\n     }\n     ```\n   - Returns:\n     ```typescript\n     {\n       findings: {\n         mainTopics: Array<{name: string, importance: number}>;\n         keyInsights: Array<{text: string, confidence: number}>;\n         sources: Array<{url: string, credibilityScore: number}>;\n       };\n       progress: {\n         completedSteps: number;\n         totalSteps: number;\n         processedUrls: number;\n       };\n       timing: {\n         started: string;\n         completed?: string;\n         duration?: number;\n         operations?: {\n           parallelSearch?: number;\n           deduplication?: number;\n           topResultsProcessing?: number;\n           remainingResultsProcessing?: number;\n           total?: number;\n         };\n       };\n     }\n     ```\n\n2. `parallel_search`\n   - Performs multiple Google searches in parallel with intelligent queuing\n   - Arguments: `{ queries: string[], maxParallel?: number }`\n   - Note: maxParallel is limited to 5 to ensure reliable performance\n\n3. `visit_page`\n   - Visit a webpage and extract its content\n   - Arguments: `{ url: string }`\n   - Returns:\n     ```typescript\n     {\n       url: string;\n       title: string;\n       content: string;  // Markdown formatted content\n     }\n     ```\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n## Configuration Options\n\nThe server can be configured through environment variables:\n\n- `MAX_PARALLEL_SEARCHES`: Maximum number of concurrent searches (default: 5)\n- `SEARCH_DELAY_MS`: Delay between searches in milliseconds (default: 200)\n- `MAX_RETRIES`: Number of retry attempts for failed requests (default: 3)\n- `TIMEOUT_MS`: Request timeout in milliseconds (default: 55000)\n- `LOG_LEVEL`: Logging level (default: 'info')\n\n## Error Handling\n\n### Common Issues\n\n1. Rate Limiting\n   - Symptom: \"Too many requests\" error\n   - Solution: Increase `SEARCH_DELAY_MS` or decrease `MAX_PARALLEL_SEARCHES`\n\n2. Network Timeouts\n   - Symptom: \"Request timed out\" error\n   - Solution: Ensure requests complete within the 60-second MCP timeout\n\n3. Browser Issues\n   - Symptom: \"Browser failed to launch\" error\n   - Solution: Ensure Playwright is properly installed (`npx playwright install`)\n\n### Debugging\n\nThis is beta software. If you run into issues:\n\n1. Check Claude Desktop's MCP logs:\n   ```bash\n   # On macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # On Windows\n   Get-Content -Path \"$env:APPDATA\\Claude\\logs\\mcp*.log\" -Tail 20 -Wait\n   ```\n\n2. Enable debug logging:\n   ```bash\n   export LOG_LEVEL=debug\n   ```\n\n## Development\n\n### Setup\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n### Testing\n\n```bash\n# Run all tests\npnpm test\n\n# Run tests in watch mode\npnpm test:watch\n\n# Run tests with coverage\npnpm test:coverage\n```\n\n### Code Quality\n\n```bash\n# Run linter\npnpm lint\n\n# Fix linting issues\npnpm lint:fix\n\n# Type check\npnpm type-check\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Coding Standards\n\n- Follow TypeScript best practices\n- Maintain test coverage above 80%\n- Document new features and APIs\n- Update CHANGELOG.md for significant changes\n- Follow semantic versioning\n\n### Performance Considerations\n\n- Use batch operations where possible\n- Implement proper error handling and retries\n- Consider memory usage with large datasets\n- Cache results when appropriate\n- Use streaming for large content\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [x] Windows\n- [ ] Linux\n\n## License\n\nMIT\n\n## Credits\n\nThis project builds upon the excellent work of [mcp-webresearch](https://github.com/mzxrai/mcp-webresearch) by [mzxrai](https://github.com/mzxrai). The original codebase provided the foundation for our enhanced features and capabilities.\n\n## Author\n\n[qpd-v](https://github.com/qpd-v)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-deepwebresearch",
      "npm_downloads": 1114,
      "keywords": [
        "deepwebresearch",
        "webpages",
        "web",
        "mcp deepwebresearch",
        "deepwebresearch provides",
        "web search"
      ],
      "category": "web-search"
    },
    "qwang07--duck-duck-mcp": {
      "owner": "qwang07",
      "name": "duck-duck-mcp",
      "url": "https://github.com/qwang07/duck-duck-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/qwang07.webp",
      "description": "Enables web search using the DuckDuckGo search engine, providing structured search results along with metadata and smart content classification.",
      "stars": 2,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-07T07:59:37Z",
      "readme_content": "# Duck Duck MCP\n[![smithery badge](https://smithery.ai/badge/duck-duck-mcp)](https://smithery.ai/server/duck-duck-mcp)\n\n一个基于 DuckDuckGo 搜索引擎的 Model Context Protocol (MCP) 服务器实现。\n\n<a href=\"https://glama.ai/mcp/servers/c0qz8cvfpi\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/c0qz8cvfpi/badge\" alt=\"Duck Duck MCP server\" /></a>\n\n## 功能特性\n\n- 使用 DuckDuckGo 进行网络搜索\n- 支持自定义搜索结果数量（默认 50 条）\n- 支持区域设置（默认 zh-cn）\n- 支持安全搜索级别设置（OFF/MODERATE/STRICT）\n- 提供结构化的搜索结果，包含元数据\n- 智能内容分类（文档/文章/社交媒体）\n- 自动语言检测\n- 主题标签识别\n\n## 安装\n\n### 安装通过 Smithery\n\n通过 [Smithery](https://smithery.ai/server/duck-duck-mcp) 将 DuckDuckGo 搜索自动安装到 Claude 桌面端：\n\n```bash\nnpx -y @smithery/cli install duck-duck-mcp --client claude\n```\n\n不需要安装，可以直接通过 npx 运行：\n```bash\nnpx -y @smithery/cli install duck-duck-mcp --client claude\n```\n\n### 使用方法\n\n这是一个 MCP 服务器实现，主要用于与支持 MCP 协议的 AI 客户端（如 Claude）集成。\n\n### 启动服务器：\n```bash\n# 使用 npx（推荐）\nnpx duck-duck-mcp\n\n# 或者如果已经全局安装\nmcp-server-search\n```\n\n### 搜索参数示例：\n```json\n{\n  \"query\": \"搜索关键词\",\n  \"options\": {\n    \"region\": \"zh-cn\",\n    \"safeSearch\": \"MODERATE\",\n    \"numResults\": 50\n  }\n}\n```\n\n### 返回结果格式：\n```json\n{\n  \"type\": \"search_results\",\n  \"data\": [\n    {\n      \"title\": \"标题\",\n      \"url\": \"网址\",\n      \"description\": \"描述\",\n      \"metadata\": {\n        \"type\": \"article|documentation|social|other\",\n        \"source\": \"域名\"\n      }\n    }\n  ],\n  \"metadata\": {\n    \"query\": \"搜索关键词\",\n    \"timestamp\": \"时间戳\",\n    \"resultCount\": 50,\n    \"searchContext\": {\n      \"region\": \"zh-cn\",\n      \"safeSearch\": \"MODERATE\"\n    },\n    \"queryAnalysis\": {\n      \"language\": \"zh-cn|en\",\n      \"topics\": [\"technology\", \"documentation\"]\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/duck-duck-mcp",
      "npm_downloads": 3851,
      "keywords": [
        "duckduckgo",
        "search",
        "duck",
        "duckduckgo search",
        "using duckduckgo",
        "search engine"
      ],
      "category": "web-search"
    },
    "ravinahp--flights-mcp": {
      "owner": "ravinahp",
      "name": "flights-mcp",
      "url": "https://github.com/ravinahp/flights-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ravinahp.webp",
      "description": "Search and retrieve flight information leveraging the Duffel API, particularly adept at managing complex travel plans with contextual memory of previous searches.",
      "stars": 137,
      "forks": 31,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T21:51:29Z",
      "readme_content": "# Find Flights MCP Server\nMCP server for searching and retrieving flight information using Duffel API.\n\n## How it Works\n![Flight](https://github.com/user-attachments/assets/3ee342a4-c2da-4d4e-a43c-79ae4590d893)\n\n## Video Demo\nhttps://github.com/user-attachments/assets/c111aa4c-9559-4d74-a2f6-60e322c273d4\n\n## Why This is Helpful\nWhile tools like Google Flights work great for simple trips, this tool shines when dealing with complex travel plans. Here's why:\n\n- **Contextual Memory**: Claude remembers all your previous flight searches in the chat, so you don't need to keep multiple tabs open to compare prices\n- **Flexible Date Search**: Easily search across multiple days to find the best prices without manually checking each date\n- **Complex Itineraries**: Perfect for multi-city trips, one-stop flights, or when you need to compare different route options you can just ask!\n- **Natural Conversation**: Just describe what you're looking for - no more clicking through calendar interfaces or juggling search parameters down to parsing city names, dates, and times.\n\nThink of it as having a travel agent in your chat who remembers everything you've discussed and can instantly search across dates and routes.\n\n## Features\n- Search for flights between multiple destinations\n- Support for one-way, round-trip, and multi-city flight queries\n- Detailed flight offer information\n- Flexible search parameters (departure times, cabin class, number of passengers)\n- Automatic handling of flight connections\n- Search for flights within multiple days to find the best flight for your trip (slower)\n## Prerequisites\n- Python 3.x\n- Duffel API Live Key\n\n## Getting Your Duffel API Key\nDuffel requires account verification and payment information setup, but this MCP server only uses the API for searching flights - no actual bookings or charges will be made to your account.\n\nTry using duffel_test first to see the power of this tool. If you end up liking it, you can go through the verification process below to use the live key.\n\n### Test Mode First (Recommended)\nYou can start with a test API key (`duffel_test`) to try out the functionality with simulated data before going through the full verification process:\n1. Visit [Duffel's registration page](https://app.duffel.com/join)\n2. Create an account (you can select \"Personal Use\" for Company Name)\n3. Navigate to More > Developer to find your test API key (one is already provided)\n\n### Getting a Live API Key\nTo access real flight data, follow these steps:\n1. In the Duffel dashboard, toggle \"Test Mode\" off in the top left corner\n2. The verification process requires multiple steps - you'll need to toggle test mode off repeatedly:\n   - First toggle: Verify your email address\n   - Toggle again: Complete company information (Personal Use is fine)\n   - Toggle again: Add payment information (required by Duffel but NO CHARGES will be made by this MCP server)\n   - Toggle again: Complete any remaining verification steps\n   - Final toggle: Access live mode after clicking \"Agree and Submit\"\n3. Once fully verified, go to More > Developer > Create Live Token\n4. Copy your live API key\n\n💡 TIP: Each time you complete a verification step, you'll need to toggle test mode off again to proceed to the next step. Keep toggling until you've completed all requirements.\n\n⚠️ IMPORTANT NOTES:\n- Your payment information is handled directly by Duffel and is not accessed or stored by the MCP server\n- This MCP server is READ-ONLY - it can only search for flights, not book them\n- No charges will be made to your payment method through this integration\n- All sensitive information (including API keys) stays local to your machine\n- You can start with the test API key (`duffel_test`) to evaluate the functionality\n- The verification process may take some time - this is a standard Duffel requirement\n\n### Security Note\nThis MCP server only uses Duffel's search endpoints and cannot make bookings or charges. Your payment information is solely for Duffel's verification process and is never accessed by or shared with the MCP server.\n\n### Note on API Usage Limits\n- Check Duffel's current pricing and usage limits\n- Different tiers available based on your requirements\n- Recommended to review current pricing on their website\n\n## Installation\n\n### Installing via Smithery\n\nTo install Find Flights for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ravinahp/travel-mcp):\n\n```bash\nnpx -y @smithery/cli install @ravinahp/travel-mcp --client claude\n```\n\n### Manual Installation\nClone the repository:\n```bash\ngit clone https://github.com/ravinahp/flights-mcp\ncd flights-mcp\n```\n\nInstall dependencies using uv:\n```bash\nuv sync\n```\nNote: We use uv instead of pip since the project uses pyproject.toml for dependency management.\n\n## Configure as MCP Server\nTo add this tool as an MCP server, modify your Claude desktop configuration file.\n\nConfiguration file locations:\n- MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nAdd the following configuration to your JSON file:\n```json\n{\n    \"flights-mcp\": {\n        \"command\": \"uv\",\n        \"args\": [\n            \"--directory\",\n            \"/Users/YOUR_USERNAME/Code/flights-mcp\",\n            \"run\",\n            \"flights-mcp\"\n        ],\n        \"env\": {\n            \"DUFFEL_API_KEY_LIVE\": \"your_duffel_live_api_key_here\"\n        }\n    }\n}\n```\n\n⚠️ IMPORTANT:\n- Replace `YOUR_USERNAME` with your actual system username\n- Replace `your_duffel_live_api_key_here` with your actual Duffel Live API key\n- Ensure the directory path matches your local installation\n\n## Deployment\n### Building\nPrepare the package:\n```bash\n# Sync dependencies and update lockfile\nuv sync\n\n# Build package\nuv build\n```\nThis will create distributions in the `dist/` directory.\n\n## Debugging\nFor the best debugging experience, use the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/find-flights-mcp run flights-mcp\n```\n\nThe Inspector provides:\n- Real-time request/response monitoring\n- Input/output validation\n- Error tracking\n- Performance metrics\n\n## Available Tools\n\n### 1. Search Flights\n```python\n@mcp.tool()\nasync def search_flights(params: FlightSearch) -> str:\n    \"\"\"Search for flights based on parameters.\"\"\"\n```\nSupports three flight types:\n- One-way flights\n- Round-trip flights\n- Multi-city flights\n\nParameters include:\n- `type`: Flight type ('one_way', 'round_trip', 'multi_city')\n- `origin`: Origin airport code\n- `destination`: Destination airport code\n- `departure_date`: Departure date (YYYY-MM-DD)\n- Optional parameters:\n  - `return_date`: Return date for round-trips\n  - `adults`: Number of adult passengers\n  - `cabin_class`: Preferred cabin class\n  - `departure_time`: Specific departure time range\n  - `arrival_time`: Specific arrival time range\n  - `max_connections`: Maximum number of connections\n\n### 2. Get Offer Details\n```python\n@mcp.tool()\nasync def get_offer_details(params: OfferDetails) -> str:\n    \"\"\"Get detailed information about a specific flight offer.\"\"\"\n```\nRetrieves comprehensive details for a specific flight offer using its unique ID.\n\n### 3. Search Multi-City Flights\n```python\n@mcp.tool(name=\"search_multi_city\")\nasync def search_multi_city(params: MultiCityRequest) -> str:\n    \"\"\"Search for multi-city flights.\"\"\"\n```\nSpecialized tool for complex multi-city flight itineraries.\n\nParameters include:\n- `segments`: List of flight segments\n- `adults`: Number of adult passengers\n- `cabin_class`: Preferred cabin class\n- `max_connections`: Maximum number of connections\n\n## Use Cases\n### Some Example (But try it out yourself!)\nYou can use these tools to find flights with various complexities:\n- \"Find a one-way flight from SFO to NYC on Jan 7 for 2 adults in business class\"\n- \"Search for a round-trip flight from LAX to London, departing Jan 8 and returning Jan 15\"\n- \"Plan a multi-city trip from New York to Paris on Jan 7, then to Rome on Jan 10, and back to New York on Jan 15\"\n- \"What is the cheapest flight from SFO to LAX from Jan 7 to Jan 15 for 2 adults in economy class?\"\n- You can even search for flights within multiple days to find the best flight for your trip. Right now, the reccomendation is to only search for one-way or round-trip flights this way. Example: \"Find the cheapest flight from SFO to LAX from Jan 7 to Jan 10 for 2 adults in economy class\"\n\n## Response Format\nThe tools return JSON-formatted responses with:\n- Flight offer details\n- Pricing information\n- Slice (route) details\n- Carrier information\n- Connection details\n\n## Error Handling\nThe service includes robust error handling for:\n- API request failures\n- Invalid airport codes\n- Missing or invalid API keys\n- Network timeouts\n- Invalid search parameters\n\n## Contributing\n[Add guidelines for contribution, if applicable]\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Performance Notes\n- Searches are limited to 50 offers for one-way/round-trip flights\n- Multi-city searches are limited to 10 offers\n- Supplier timeout is set to 15-30 seconds depending on the search type\n\n### Cabin Classes\nAvailable cabin classes:\n- `economy`: Standard economy class\n- `premium_economy`: Premium economy class\n- `business`: Business class\n- `first`: First class\n\nExample request with cabin class:\n```json\n{\n  \"params\": {\n    \"type\": \"one_way\",\n    \"adults\": 1,\n    \"origin\": \"SFO\",\n    \"destination\": \"LAX\",\n    \"departure_date\": \"2025-01-12\",\n    \"cabin_class\": \"business\"  // Specify desired cabin class\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flights",
        "searches",
        "flight",
        "flight information",
        "retrieve flight",
        "travel plans"
      ],
      "category": "web-search"
    },
    "regenrek--deepwiki-mcp": {
      "owner": "regenrek",
      "name": "deepwiki-mcp",
      "url": "https://github.com/regenrek/deepwiki-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/regenrek.webp",
      "description": "Crawls Deepwiki.com documentation, converting it into Markdown format by removing unnecessary HTML elements and adjusting links for better readability. Supports fetching multiple pages and offers structured output formats for knowledge retrieval.",
      "stars": 1030,
      "forks": 59,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T02:47:13Z",
      "readme_content": "# Deepwiki MCP Server\n\n> ⚠️ **IMPORTANT NOTICE**: This server is currently not working since DeepWiki has cut off the possibility to scrape it. We recommend using the official DeepWiki MCP server at https://docs.devin.ai/work-with-devin/deepwiki-mcp for the time being.\n\nThis is an **unofficial Deepwiki MCP Server**\n\nIt takes a Deepwiki URL via MCP, crawls all relevant pages, converts them to Markdown, and returns either one document or a list by page.\n\n## Features\n\n- 🔒 **Domain Safety**: Only processes URLs from deepwiki.com\n- 🧹 **HTML Sanitization**: Strips headers, footers, navigation, scripts, and ads\n- 🔗 **Link Rewriting**: Adjusts links to work in Markdown\n- 📄 **Multiple Output Formats**: Get one document or structured pages\n- 🚀 **Performance**: Fast crawling with adjustable concurrency and depth\n- **NLP**: It's to search just for the library name\n\n## Usage\n\nPrompts you can use:\n\n```\ndeepwiki fetch how can i use gpt-image-1 with \"vercel ai\" sdk\n```\n\n```\ndeepwiki fetch how can i create new blocks in shadcn?\n```\n\n```\ndeepwiki fetch i want to understand how X works\n```\n\nFetch complete Documentation (Default)\n```\nuse deepwiki https://deepwiki.com/shadcn-ui/ui\nuse deepwiki multiple pages https://deepwiki.com/shadcn-ui/ui\n```\n\nSingle Page\n```\nuse deepwiki fetch single page https://deepwiki.com/tailwindlabs/tailwindcss/2.2-theme-system\n```\n\nGet by shortform\n```\nuse deepwiki fetch tailwindlabs/tailwindcss\n```\n\n```\ndeepwiki fetch library\n\ndeepwiki fetch url\ndeepwiki fetch <name>/<repo>\n\ndeepwiki multiple pages ...\ndeepwiki single page url ...\n```\n\n## Cursor\n\nAdd this to `.cursor/mcp.json` file.\n\n```\n{\n  \"mcpServers\": {\n    \"mcp-deepwiki\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-deepwiki@latest\"]\n    }\n  }\n}\n```\n\n\n\n### MCP Tool Integration\n\nThe package registers a tool named `deepwiki_fetch` that you can use with any MCP-compatible client:\n\n```json\n{\n  \"action\": \"deepwiki_fetch\",\n  \"params\": {\n    \"url\": \"https://deepwiki.com/user/repo\",\n    \"mode\": \"aggregate\",\n    \"maxDepth\": \"1\"\n  }\n}\n```\n\n#### Parameters\n\n- `url` (required): The starting URL of the Deepwiki repository\n- `mode` (optional): Output mode, either \"aggregate\" for a single Markdown document (default) or \"pages\" for structured page data\n- `maxDepth` (optional): Maximum depth of pages to crawl (default: 10)\n\n### Response Format\n\n#### Success Response (Aggregate Mode)\n\n```json\n{\n  \"status\": \"ok\",\n  \"data\": \"# Page Title\\n\\nPage content...\\n\\n---\\n\\n# Another Page\\n\\nMore content...\",\n  \"totalPages\": 5,\n  \"totalBytes\": 25000,\n  \"elapsedMs\": 1200\n}\n```\n\n#### Success Response (Pages Mode)\n\n```json\n{\n  \"status\": \"ok\",\n  \"data\": [\n    {\n      \"path\": \"index\",\n      \"markdown\": \"# Home Page\\n\\nWelcome to the repository.\"\n    },\n    {\n      \"path\": \"section/page1\",\n      \"markdown\": \"# First Page\\n\\nThis is the first page content.\"\n    }\n  ],\n  \"totalPages\": 2,\n  \"totalBytes\": 12000,\n  \"elapsedMs\": 800\n}\n```\n\n#### Error Response\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": \"DOMAIN_NOT_ALLOWED\",\n  \"message\": \"Only deepwiki.com domains are allowed\"\n}\n```\n\n#### Partial Success Response\n\n```json\n{\n  \"status\": \"partial\",\n  \"data\": \"# Page Title\\n\\nPage content...\",\n  \"errors\": [\n    {\n      \"url\": \"https://deepwiki.com/user/repo/page2\",\n      \"reason\": \"HTTP error: 404\"\n    }\n  ],\n  \"totalPages\": 1,\n  \"totalBytes\": 5000,\n  \"elapsedMs\": 950\n}\n```\n\n### Progress Events\n\nWhen using the tool, you'll receive progress events during crawling:\n\n```\nFetched https://deepwiki.com/user/repo: 12500 bytes in 450ms (status: 200)\nFetched https://deepwiki.com/user/repo/page1: 8750 bytes in 320ms (status: 200)\nFetched https://deepwiki.com/user/repo/page2: 6200 bytes in 280ms (status: 200)\n```\n\n## Local Development - Installation\n\n### Local Usage\n\n```\n{\n  \"mcpServers\": {\n    \"mcp-deepwiki\": {\n      \"command\": \"node\",\n      \"args\": [\"./bin/cli.mjs\"]\n    }\n  }\n}\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/regenrek/deepwiki-mcp.git\ncd deepwiki-mcp\n\n# Install dependencies\nnpm install\n\n# Build the package\nnpm run build\n```\n\n#### Direct API Calls\n\nFor HTTP transport, you can make direct API calls:\n\n```bash\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"id\": \"req-1\",\n    \"action\": \"deepwiki_fetch\",\n    \"params\": {\n      \"url\": \"https://deepwiki.com/user/repo\",\n      \"mode\": \"aggregate\"\n    }\n  }'\n```\n\n## Configuration\n\n### Environment Variables\n\n- `DEEPWIKI_MAX_CONCURRENCY`: Maximum concurrent requests (default: 5)\n- `DEEPWIKI_REQUEST_TIMEOUT`: Request timeout in milliseconds (default: 30000)\n- `DEEPWIKI_MAX_RETRIES`: Maximum retry attempts for failed requests (default: 3)\n- `DEEPWIKI_RETRY_DELAY`: Base delay for retry backoff in milliseconds (default: 250)\n\nTo configure these, create a `.env` file in the project root:\n\n```\nDEEPWIKI_MAX_CONCURRENCY=10\nDEEPWIKI_REQUEST_TIMEOUT=60000\nDEEPWIKI_MAX_RETRIES=5\nDEEPWIKI_RETRY_DELAY=500\n```\n\n## Docker Deployment (Untested)\n\nBuild and run the Docker image:\n\n```bash\n# Build the image\ndocker build -t mcp-deepwiki .\n\n# Run with stdio transport (for development)\ndocker run -it --rm mcp-deepwiki\n\n# Run with HTTP transport (for production)\ndocker run -d -p 3000:3000 mcp-deepwiki --http --port 3000\n\n# Run with environment variables\ndocker run -d -p 3000:3000 \\\n  -e DEEPWIKI_MAX_CONCURRENCY=10 \\\n  -e DEEPWIKI_REQUEST_TIMEOUT=60000 \\\n  mcp-deepwiki --http --port 3000\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Run in development mode with stdio\npnpm run dev-stdio\n\n# Run tests\npnpm test\n\n# Run linter\npnpm run lint\n\n# Build the package\npnpm run build\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Permission Denied**: If you get EACCES errors when running the CLI, make sure to make the binary executable:\n   ```bash\n   chmod +x ./node_modules/.bin/mcp-deepwiki\n   ```\n\n2. **Connection Refused**: Make sure the port is available and not blocked by a firewall:\n   ```bash\n   # Check if port is in use\n   lsof -i :3000\n   ```\n\n3. **Timeout Errors**: For large repositories, consider increasing the timeout and concurrency:\n   ```\n   DEEPWIKI_REQUEST_TIMEOUT=60000 DEEPWIKI_MAX_CONCURRENCY=10 npx mcp-deepwiki\n   ```\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## License\n\nMIT\n\n## Links\n\n- X/Twitter: [@kregenrek](https://x.com/kregenrek)\n- Bluesky: [@kevinkern.dev](https://bsky.app/profile/kevinkern.dev)\n\n## Courses\n- Learn Cursor AI: [Ultimate Cursor Course](https://www.instructa.ai/en/cursor-ai)\n- Learn to build software with AI: [instructa.ai](https://www.instructa.ai)\n\n## See my other projects:\n\n* [AI Prompts](https://github.com/instructa/ai-prompts/blob/main/README.md) - Curated AI Prompts for Cursor AI, Cline, Windsurf and Github Copilot\n* [codefetch](https://github.com/regenrek/codefetch) - Turn code into Markdown for LLMs with one simple terminal command\n* [aidex](https://github.com/regenrek/aidex) A CLI tool that provides detailed information about AI language models, helping developers choose the right model for their needs.# tool-starter",
      "npm_url": "https://www.npmjs.com/package/deepwiki-mcp",
      "npm_downloads": 706,
      "keywords": [
        "deepwiki",
        "markdown",
        "documentation",
        "crawls deepwiki",
        "deepwiki com",
        "regenrek deepwiki"
      ],
      "category": "web-search"
    },
    "rileyedwards77--perplexity-mcp-server": {
      "owner": "rileyedwards77",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/rileyedwards77/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/rileyedwards77.webp",
      "description": "Integrates with Perplexity AI for chatting, searching, and retrieving documentation. Facilitates access to various functionalities of the Perplexity AI API through MCP-based systems.",
      "stars": 1,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T05:26:53Z",
      "readme_content": "# Perplexity AI MCP Server\n\nThis repository contains the source code for a Model Context Protocol (MCP) server that provides access to the Perplexity AI API.  This server allows users to interact with Perplexity AI through various tools, including chatting, searching, and retrieving documentation.\n\n## Purpose\n\nThis server simplifies the integration of Perplexity AI into MCP-based systems.  It provides a convenient and standardized way to access Perplexity AI's capabilities.\n\n## Setup\n\n1.  **Install Node.js and npm:** Ensure you have Node.js and npm installed on your system.\n2.  **Clone the repository:** Clone this repository to your local machine.\n3.  **Install dependencies:** Navigate to the project directory and run `npm install`.\n4.  **Configure API Key:** Set the `PERPLEXITY_API_KEY` environment variable to your Perplexity API key.\n5.  **Run the server:** Run `npm start` to start the server.\n\n## Usage\n\nThe server exposes several tools that can be accessed through the MCP system.  Refer to the MCP documentation for details on how to use these tools.\n\n## Technologies Used\n\n*   TypeScript\n*   @modelcontextprotocol/sdk\n*   axios\n\n## Known Issues\n\n*   The Perplexity API may be unreliable.  Error handling is included to gracefully handle API failures.\n\n## Contributing\n\nContributions are welcome!  Please open an issue or submit a pull request.\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "searching",
        "search",
        "perplexity",
        "perplexity ai",
        "ai chatting",
        "chatting searching"
      ],
      "category": "web-search"
    },
    "rockerritesh--scraper-mcp-smithery": {
      "owner": "rockerritesh",
      "name": "scraper-mcp-smithery",
      "url": "https://github.com/rockerritesh/scraper-mcp-smithery",
      "imageUrl": "/freedevtools/mcp/pfp/rockerritesh.webp",
      "description": "Enhance web scraping capabilities with tools to efficiently extract and manipulate data. Automate data collection through asynchronous web search operations and integrate smoothly with existing applications using a FastMCP server foundation.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-07T03:03:11Z",
      "readme_content": "# Web Scraper MCP for Smithery\r\n\r\nA robust MCP (Model Context Protocol) server for web scraping operations, deployed on [Smithery](https://smithery.ai/) - the orchestration layer for AI agents. This extension converts any website into clean, structured markdown format with automatic ChromeDriver management.\r\n\r\n## 🌟 Available on Smithery\r\n\r\nThis MCP server is part of [Smithery's marketplace](https://smithery.ai/) with **7953+ skills and extensions** built by the community. Deploy instantly to integrate web scraping capabilities into your AI agents.\r\n\r\n## ✨ Features\r\n\r\n- **🚀 High Performance**: Direct function integration with uv package manager for optimal speed\r\n- **🔄 Zero Configuration**: Automatic ChromeDriver management with version compatibility\r\n- **🌐 Smart URL Processing**: Auto-adds HTTPS protocol and validates URLs\r\n- **📝 Markdown Conversion**: Converts web content to clean, structured markdown\r\n- **⚡ Async Operations**: Non-blocking web scraping with proper async/await\r\n- **🛡️ Production Ready**: Comprehensive error handling and graceful fallbacks\r\n- **🐳 Smithery Optimized**: Containerized deployment with security best practices\r\n\r\n## 📋 Prerequisites\r\n\r\n- **Smithery Account** - [Sign up at smithery.ai](https://smithery.ai/)\r\n- **Python 3.12+** (for local development)\r\n- **UV** package manager\r\n- **Google Chrome** (automatically managed in deployment)\r\n\r\n## 🚀 Smithery Deployment\r\n\r\n### Deploy to Smithery Platform\r\n\r\n1. **Visit** [Smithery Web Scraper MCP](https://smithery.ai/)\r\n2. **Click \"Deploy Server\"** to add to your agent\r\n3. **Configure** with your preferred settings\r\n4. **Start scraping** websites instantly!\r\n\r\n### Local Development\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/rockerritesh/scraper-mcp-smithery.git\r\ncd scraper-mcp-smithery\r\n\r\n# Install dependencies with uv\r\nuv sync\r\n\r\n# Run the MCP development server\r\nuv run mcp dev server.py\r\n```\r\n\r\n\r\n### Direct Python Usage (Development)\r\n\r\n```python\r\nfrom scraper_doc import scrape_website\r\n\r\n# Scrape a website\r\ncontent = scrape_website(\"https://example.com\")\r\nprint(content)  # Returns markdown formatted content\r\n```\r\n\r\n### URL Format Requirements\r\n\r\n- **✅ Supported**: `https://example.com`, `http://example.com`\r\n- **✅ Auto-fixed**: `example.com` → `https://example.com`\r\n- **❌ Invalid**: Malformed URLs return descriptive error messages\r\n\r\n## 🏗️ Smithery Architecture\r\n\r\n### Integration Flow\r\n\r\n```\r\nSmithery Agent → MCP Protocol → search_web_tool → Chrome/Selenium → Markdown Output\r\n```\r\n\r\n### Platform Benefits\r\n\r\n- **🎯 Zero Setup**: Deploy instantly without infrastructure management\r\n- **📊 Monitoring**: Built-in health checks and performance metrics\r\n- **🔗 Agent Integration**: Seamless connection to Smithery's AI orchestration\r\n- **📈 Scalability**: Automatic scaling based on usage patterns\r\n\r\n### Key Improvements\r\n\r\n- **❌ Old**: Subprocess calls with performance overhead\r\n- **✅ New**: Direct function imports with async execution\r\n- **🎯 Result**: ~3x faster performance on Smithery platform\r\n\r\n## 🛠️ Development & Testing\r\n\r\n### Local Testing\r\n\r\n```bash\r\n# Test the scraper directly\r\nuv run python scraper_doc.py https://example.com\r\n\r\n# Test with output directory\r\nuv run python scraper_doc.py https://example.com ./output\r\n\r\n# Run MCP development server\r\nuv run mcp dev server.py\r\n```\r\n\r\n### Debug Mode\r\n\r\n```bash\r\nMCP_DEBUG=1 uv run mcp dev server.py\r\n```\r\n\r\n### Dependencies (Managed by UV)\r\n\r\n- **mcp[cli]** - Model Context Protocol framework\r\n- **selenium** - Web browser automation\r\n- **webdriver-manager** - Automatic ChromeDriver management\r\n- **requests** - HTTP client for image downloads\r\n- **python-dotenv** - Environment variable management\r\n\r\n## 🐛 Troubleshooting\r\n\r\n### Common Smithery Issues\r\n\r\n- **Deployment Timeout**: Usually resolves automatically; check Smithery status\r\n- **Tool Not Found**: Ensure proper MCP tool registration in server.py\r\n- **Memory Limits**: Large pages may require optimization (handled automatically)\r\n\r\n### ChromeDriver Issues\r\n\r\nAutomatically resolved by webdriver-manager, but for local development:\r\n\r\n```bash\r\n# Clear webdriver cache if needed\r\nrm -rf ~/.wdm/\r\n\r\n# Verify Chrome installation\r\ngoogle-chrome --version\r\n```\r\n\r\n## 📊 Performance on Smithery\r\n\r\n- **🚀 Scraping Speed**: 2-5 seconds per page\r\n- **💾 Memory Usage**: ~50-100MB per operation\r\n- **⚡ Concurrent Support**: Multiple async operations\r\n- **🔄 Auto-scaling**: Handled by Smithery platform\r\n\r\n## 🔐 Security Features\r\n\r\n- **🛡️ Sandboxed Execution**: Chrome runs with security flags\r\n- **👤 Non-root User**: Enhanced container security\r\n- **🔒 URL Validation**: Prevents malicious URL processing\r\n- **📊 Audit Logging**: Smithery platform monitoring\r\n\r\n## 🌐 Smithery Integration Examples\r\n\r\n### In Chat Agents\r\n\r\n```\r\nAgent: \"Can you scrape the latest news from example.com?\"\r\nWeb Scraper MCP: *Scrapes and returns structured content*\r\nAgent: \"Here's the latest news in markdown format...\"\r\n```\r\n\r\n### In Automation Workflows\r\n\r\n```\r\nTrigger → Smithery Agent → Web Scraper MCP → Content Analysis → Action\r\n```\r\n\r\n## 📚 Resources\r\n\r\n- **[Smithery Platform](https://smithery.ai/)** - Deploy and manage MCP servers\r\n- **[Smithery Documentation](https://smithery.ai/docs)** - Platform guides and API reference\r\n- **[MCP Specification](https://github.com/modelcontextprotocol/specification)** - Protocol documentation\r\n- **[Community Discord](https://smithery.ai/discord)** - Get help and share ideas\r\n\r\n## 📜 License\r\n\r\nMIT License - see [LICENSE](LICENSE) file for details.\r\n\r\n## 🤝 Contributing to Smithery Ecosystem\r\n\r\n1. Fork this repository\r\n2. Create a feature branch\r\n3. Test on Smithery platform\r\n4. Submit a pull request\r\n5. Share in [Smithery community](https://smithery.ai/)\r\n\r\n---\r\n\r\n**🚀 Deployed on [Smithery](https://smithery.ai/) | Built with FastMCP, Selenium, and UV | Part of 7953+ community extensions**\r\n```\r\n\r\nThis README provides clear setup instructions while highlighting the tool's async capabilities and Smithery integration. The structure follows best practices for developer tools documentation.\r\n\r\n---\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "scraper",
        "fastmcp",
        "web scraping",
        "scraper mcp",
        "scraping capabilities"
      ],
      "category": "web-search"
    },
    "rogeriolembo--mcp-api": {
      "owner": "rogeriolembo",
      "name": "mcp-api",
      "url": "https://github.com/rogeriolembo/mcp-api",
      "imageUrl": "/freedevtools/mcp/pfp/rogeriolembo.webp",
      "description": "Integrate with the Sonar API to conduct real-time web searches through conversational interactions, enabling enhanced research capabilities with live data.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-31T14:11:12Z",
      "readme_content": "# sysauto Ask MCP Server\n\nAn MCP server implementation that integrates the Sonar API to provide Claude with unparalleled real-time, web-wide research.\n\n\n\n\n## Tools\n\n- **sysauto_ask**\n  - Engage in a conversation with the Sonar API for live web searches.\n  - **Inputs:**\n    - `messages` (array): An array of conversation messages.\n      - Each message must include:\n        - `role` (string): The role of the message (e.g., `system`, `user`, `assistant`).\n        - `content` (string): The content of the message.\n\n## Configuration\n\n### Step 1: \n\nClone this repository:\n\n```bash\ngit clone git@github.com:rogeriolembo/mcp-api.git\n```\n\nNavigate to the `sysauto-ask` directory and install the necessary dependencies:\n\n```bash\ncd mcp-api/sysauto-ask && npm install\n```\n\n### Step 2: Get a Sonar API Key\n\n1. Sign up for a [Sonar API account](https://docs.sysauto.ai/guides/getting-started).\n2. Follow the account setup instructions and generate your API key from the developer dashboard.\n3. Set the API key in your environment as `sysauto_API_KEY`.\n\n### Step 3: Configure Claude Desktop\n\n1. Download Claude desktop [here](https://claude.ai/download). \n\n2. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sysauto-ask\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SYSAUTO_API_KEY\",\n        \"mcp/sysauto-ask\"\n      ],\n      \"env\": {\n        \"SYSAUTO_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"sysauto-ask\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@rogeriolembo/server-sysauto-ask\"\n      ],\n      \"env\": {\n        \"SYSAUTO_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nnpm publish --access public\nnpm publish",
      "npm_url": "https://www.npmjs.com/package/mcp-api",
      "npm_downloads": 83,
      "keywords": [
        "sonar",
        "searches",
        "web",
        "sonar api",
        "web search",
        "integrate sonar"
      ],
      "category": "web-search"
    },
    "ronantakizawa--a11ymcp": {
      "owner": "ronantakizawa",
      "name": "a11ymcp",
      "url": "https://github.com/ronantakizawa/a11ymcp",
      "imageUrl": "/freedevtools/mcp/pfp/ronantakizawa.webp",
      "description": "Run WCAG compliance tests and accessibility assessments using the axe-core API. Users can test raw HTML or URLs for accessibility issues and color contrast evaluation.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T06:06:07Z",
      "readme_content": "# Web Accessibility-Testing MCP Server (A11y MCP)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ronantakizawa-a11ymcp-badge.png)](https://mseep.ai/app/ronantakizawa-a11ymcp)\n[![smithery badge](https://smithery.ai/badge/@ronantakizawa/a11ymcp)](https://smithery.ai/server/@ronantakizawa/a11ymcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/01361aeb-0dce-45d6-80fb-76ff443dbfc8)\n\n<a href=\"https://glama.ai/mcp/servers/@ronantakizawa/a11ymcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ronantakizawa/a11ymcp/badge\" />\n</a>\n\n![a11ymcpwide](https://github.com/user-attachments/assets/a37c4a9e-da5e-49ac-9705-0ef87a1d5c17)\n\n\nA11y MCP is an MCP (Model Context Protocol) server that gives LLMs access to web accessibility testing APIs. \n\nThis server uses the Deque Axe-core API and Puppeteer to allow LLMs to analyze web content for WCAG compliance and identify accessibility issues.\n\nNOTE: This is not an official MCP server from Deque Labs.\n\nLeave a star if you enjoyed the project! 🌟\n\n## Features\n\n- **Test web pages**: Test any public URL for accessibility issues\n- **Test HTML snippets**: Test raw HTML strings for accessibility issues\n- **WCAG compliance testing**: Check content against various WCAG standards (2.0, 2.1, 2.2)\n- **Customizable tests**: Specify which accessibility tags/standards to test against\n- **Rule exploration**: Get information about available accessibility rules\n- **Color contrast analysis**: Check color combinations for WCAG compliance\n- **ARIA validation**: Test proper usage of ARIA attributes\n- **Orientation lock detection**: Identify content that forces specific screen orientations\n\n## Installation\nTo use this server with Claude Desktop, you need to configure it in the MCP settings:\n\n**For macOS:**\nEdit the file at `'~/Library/Application Support/Claude/claude_desktop_config.json'`\n\n```\n{\n  \"mcpServers\": {\n    \"a11y-accessibility\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"a11y-mcp-server\"\n    ]\n   }\n  }\n}\n```\n\n**For Windows:**\nEdit the file at `%APPDATA%\\Claude\\settings\\claude_mcp_settings.json`\n\n**For Linux:**\nEdit the file at `~/.config/Claude/settings/claude_mcp_settings.json`\nReplace `/path/to/axe-mcp-server/build/index.js` with the actual path to your compiled server file.\n\n\n## Available Tools\n\n### test_accessibility\n\nTests a URL for accessibility issues.\n\n**Parameters:**\n- `url` (required): The URL of the web page to test\n- `tags` (optional): Array of WCAG tags to test against (e.g., [\"wcag2aa\"])\n\nExample\n\n```\n{\n \"url\": \"https://example.com\",\n \"tags\": [\"wcag2aa\"]\n}\n```\n### test_html_string\n\nTests an HTML string for accessibility issues.\nParameters:\n\n* html (required): The HTML content to test\n* tags (optional): Array of WCAG tags to test against (e.g., [\"wcag2aa\"])\n\nExample\n\n```\n{\n  \"html\": \"<div><img src='image.jpg'></div>\",\n  \"tags\": [\"wcag2aa\"]\n}\n```\n\n### get_rules\n\nGet information about available accessibility rules with optional filtering.\n\n### check_color_contrast\n\nCheck if a foreground and background color combination meets WCAG contrast requirements.\n\n**Parameters:**\n\n- `foreground` (required): Foreground color in hex format (e.g., \"#000000\")\n- `background` (required): Background color in hex format (e.g., \"#FFFFFF\")\n- `fontSize` (optional): Font size in pixels (default: 16)\n- `isBold` (optional): Whether the text is bold (default: false)\n\nExample\n\n```\n{\n  \"foreground\": \"#777777\",\n  \"background\": \"#EEEEEE\",\n  \"fontSize\": 16,\n  \"isBold\": false\n}\n```\n\n### check_color_contrast\n\nCheck if ARIA attributes are used correctly in HTML.\n\n**Parameters:**\n\n- `html` (required): HTML content to test for ARIA attribute usage\n\nExample\n\n```\n{\n  \"html\": \"<div role='button' aria-pressed='false'>Click me</div>\"\n}\n```\n\n### check_orientation_lock\n\nCheck if content forces a specific orientation.\n\n**Parameters:**\n\n- `html` (required): HTML content to test for orientation lock issues\n\nExample\n\n```\n{\n  \"html\": \"<html><head><meta name='viewport' content='width=device-width, orientation=portrait'></head><body>Content</body></html>\"\n}\n```\n\n## Response Format\nThe server returns accessibility test results in a structured JSON format:\n```\n{\n  \"violations\": [\n    {\n      \"id\": \"color-contrast\",\n      \"impact\": \"serious\",\n      \"description\": \"Ensure the contrast between foreground and background colors meets WCAG 2 AA minimum contrast ratio thresholds\",\n      \"help\": \"Elements must meet minimum color contrast ratio thresholds\",\n      \"helpUrl\": \"https://dequeuniversity.com/rules/axe/4.10/color-contrast\",\n      \"affectedNodes\": [\n        {\n          \"html\": \"<div style=\\\"color: #aaa; background-color: #eee;\\\">Low contrast text</div>\",\n          \"target\": [\"div\"],\n          \"failureSummary\": \"Fix any of the following: Element has insufficient color contrast of 1.98 (foreground color: #aaa, background color: #eee, font size: 12.0pt, font weight: normal)\"\n        }\n      ]\n    }\n  ],\n  \"passes\": 1,\n  \"incomplete\": 0,\n  \"inapplicable\": 2,\n  \"timestamp\": \"2025-04-25T16:45:33.655Z\",\n  \"url\": \"about:blank\",\n  \"testEngine\": {\n    \"name\": \"axe-core\",\n    \"version\": \"4.10.3\"\n  },\n  \"testRunner\": {\n    \"name\": \"axe\"\n  },\n  \"testEnvironment\": {\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/135.0.0.0 Safari/537.36\",\n    \"windowWidth\": 800,\n    \"windowHeight\": 600,\n    \"orientationAngle\": 0,\n    \"orientationType\": \"portrait-primary\"\n  }\n}\n```\n\n## Dependencies\n\n- @modelcontextprotocol/sdk\n- puppeteer\n- @axe-core/puppeteer\n- axe-core\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "wcag",
        "tests",
        "tests accessibility",
        "accessibility assessments",
        "wcag compliance"
      ],
      "category": "web-search"
    },
    "ropon--mcp_demo": {
      "owner": "ropon",
      "name": "mcp_demo",
      "url": "https://github.com/ropon/mcp_demo",
      "imageUrl": "/freedevtools/mcp/pfp/ropon.webp",
      "description": "Query IP addresses to retrieve related information and insights in real-time. This server provides essential data for applications that require IP data analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-04-07T01:54:49Z",
      "readme_content": "# mcp_demo\nmcp demo ip query\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ip",
        "ropon",
        "mcp_demo",
        "query ip",
        "ip data",
        "search ropon"
      ],
      "category": "web-search"
    },
    "rvydhya--youtube_transcriptor": {
      "owner": "rvydhya",
      "name": "youtube_transcriptor",
      "url": "https://github.com/rvydhya/youtube_transcriptor",
      "imageUrl": "/freedevtools/mcp/pfp/rvydhya.webp",
      "description": "Transcribes YouTube videos by extracting transcripts, including both manual and autogenerated captions, using the provided video URL. Supports integration with MCP clients for enhanced workflows involving video transcription.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-23T13:38:39Z",
      "readme_content": "# YouTube Transcriptor MCP Tool\n\nThis is a Model Context Protocol (MCP) tool for transcribing YouTube videos using the `youtube-transcript-api`.\n\n## Features\n- Extracts and transcribes from YouTube videos (manual or autogenerated). Enables retrieval of transcripts from YouTube videos.\n- Exposes a single tool: `transcribe_video(video: str)`.\n\n## Usage\n\n### 1. Prerequisites\n- Python 3.12+\n- Install dependencies:\n  ```sh\n  pip install -r requirements.txt\n  ```\n\n### 2. Running the Tool\nYou can run the tool directly:\n```sh\npython youtube.py\n```\n\n### 3. Using with VS Code (Manual MCP Config)\nTo use this tool as an MCP server in VS Code, add the following to your `.vscode/settings.json` or your MCP client configuration:\n\n```json\n\"youtube_transcriptor\": {\n  \"type\": \"stdio\",\n  \"command\": \"python\",\n  \"args\": [\"PATH\\\\transcription\\\\youtube.py\"]\n}\n```\nReplace `PATH` with the absolute path to your workspace root.\n\n### 4. Using the Tool\nOnce configured, you can call the `transcribe_video` tool from your MCP client or compatible VS Code extension, passing a YouTube video URL as the argument.\n\n---\n\n**Example:**\n```python\nresult = transcribe_video(\"https://www.youtube.com/watch?v=VIDEO_ID\")\nprint(result)\n```\n\n---\n\n## License\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube_transcriptor",
        "transcription",
        "transcripts",
        "rvydhya youtube_transcriptor",
        "youtube_transcriptor transcribes",
        "transcribes youtube"
      ],
      "category": "web-search"
    },
    "sagacious-satadru--Documentation-MCP": {
      "owner": "sagacious-satadru",
      "name": "Documentation-MCP",
      "url": "https://github.com/sagacious-satadru/Documentation-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/sagacious-satadru.webp",
      "description": "Search and access documentation from popular AI libraries such as LangChain and LlamaIndex. Enhance conversations with contextual knowledge by leveraging intelligent documentation extraction.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-18T09:03:58Z",
      "readme_content": "# Documentation MCP Server 📚🔍\n\nA Model Context Protocol (MCP) server that enables Claude to search and access documentation from popular libraries like LangChain, LlamaIndex, and OpenAI directly within conversations.\n\n## What is MCP? 🤔\n\nMCP (Model Context Protocol) is an open protocol that standardizes how applications provide context to Large Language Models. Think of it as a universal connector that lets AI assistants like Claude access external data sources and tools.\n\n\n\n\n\n\n## Features ✨\n\n- **Documentation Search Tool**: Search through documentation of popular AI libraries\n- **Supported Libraries**:\n  - [LangChain](https://python.langchain.com/docs) 🔗\n  - [LlamaIndex](https://docs.llamaindex.ai/en/stable) 🦙\n  - [OpenAI](https://platform.openai.com/docs) 🤖\n- **Smart Extraction**: Intelligently parses HTML content to extract the most relevant information\n- **Configurable Results**: Limit the amount of text returned based on your needs\n\n## How It Works 🛠️\n\n1. The server uses the Serper API to perform Google searches with site-specific queries\n2. It fetches the content from the search results\n3. BeautifulSoup extracts the most relevant text from main content areas\n4. Claude can access this information through the `get_docs` tool\n\n## System Requirements 🖥️\n\n- Python 3.11 or higher\n- `uv` package manager\n- A Serper API key\n\n## Setup Instructions 🚀\n\n### 1. Install uv Package Manager\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### 2. Clone and Set Up the Project\n\n```bash\n# Clone or download the project\ncd documentation\n\n# Create and activate virtual environment\nuv venv\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -e .\n```\n\n### 3. Configure the Serper API Key\n\nCreate a `.env` file in the project directory with your Serper API key:\n\n```\nSERPER_API_KEY=your_serper_api_key_here\n```\n\nYou can get a Serper API key by signing up at [serper.dev](https://serper.dev).\n\n### 4. Configure Claude Desktop\n\nEdit your Claude Desktop configuration file at:\n- Windows: `/C:/Users/[Your Username]/AppData/Roaming/Claude/claude_desktop_config.json`\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd the following to the `mcpServers` section:\n\n```json\n\"documentation\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"--directory\",\n    \"/ABSOLUTE/PATH/TO/YOUR/documentation\",\n    \"run\",\n    \"main.py\"\n  ]\n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/YOUR/documentation` with the absolute path to your project directory.\n\n### 5. Restart Claude Desktop\n\nClose and reopen Claude Desktop to apply the new configuration.\n\n## Using the Documentation Tool 🧩\n\nOnce connected, you can ask Claude to use the documentation tool:\n\n> \"Can you look up information about vector stores in LangChain documentation?\"\n\nClaude will use the `get_docs` tool to search for relevant information and provide you with documentation excerpts.\n\n## Tool Parameters 📋\n\nThe `get_docs` tool accepts the following parameters:\n\n- `query`: The search term (e.g., \"vector stores\", \"embedding models\")\n- `library`: Which library to search (langchain, llama-index, or openai)\n- `max_chars`: Maximum characters to return (default: 1000)\n\n## Troubleshooting 🛠️\n\n- **Claude can't find the server**: Verify the path in `/C:/Users/fcbsa/AppData/Roaming/Claude/claude_desktop_config.json` is correct\n- **Search returns no results**: Check your Serper API key and internet connection\n- **Timeout errors**: The server might be experiencing connectivity issues or rate limits\n\n## License 📜\n\nThis project is provided as an educational example of MCP server implementation.\n\n## Acknowledgements 🙏\n\n- Built using the [MCP SDK](https://github.com/modelcontextprotocol)\n- Powered by [Serper API](https://serper.dev) for Google search integration\n- Uses [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) for HTML parsing\n- Inspired by the growing MCP community\n\n---\n\n*This MCP server enhances Claude's capabilities by providing direct access to documentation resources. Explore, learn, and build better AI applications with contextual knowledge from the docs!*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "documentation",
        "ai",
        "intelligent documentation",
        "documentation extraction",
        "ai libraries"
      ],
      "category": "web-search"
    },
    "sanjoy1234--multi-search-mcp": {
      "owner": "sanjoy1234",
      "name": "multi-search-mcp",
      "url": "https://github.com/sanjoy1234/multi-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sanjoy1234.webp",
      "description": "Enables multi-source search capabilities by unifying multiple search engines through a simple API. Facilitates the retrieval of information from Google, Brave News, and DuckDuckGo, presenting results in a consolidated format.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-21T18:40:33Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@sanjoy1234/multi-search-mcp)](https://smithery.ai/server/@sanjoy1234/multi-search-mcp)\n\n# Multi‑Search MCP Server\n\nA lightweight MCP server that **unifies multiple search engines** under one simple stdio API. Query Google, Brave News, and DuckDuckGo—without juggling three different SDKs or payload formats.\n\n---\n\n## 🔍 What It Does\n\n- **Google Search** via SerpApi (JSON‑first, optional key)\n- **Brave News** retrieval through Brave’s News API (key required)\n- **DuckDuckGo Instant Answers** via public API (no key needed)\n- Exposes each as a distinct MCP “tool” for chatbots, agents, or local scripts\n\n---\n\n## 🚀 Why It Matters\n\n- **One API to rule them all**  \n  Call `google_search()`, `brave_search()`, or `duck_search()`, and get back ready‑to‑consume JSON without provider boilerplate.\n\n- **Rapid tool development**  \n  Spin up new search‑powered agents in minutes, not days.\n\n- **Minimal key management**  \n  Only Brave needs a key; DuckDuckGo works out‑of‑the‑box; Google key is optional.\n\n- **Easy deployment**  \n  Deploy via Docker or Smithery.ai with a single click.\n\n---\n\nFor feedback or issues, join us on the Smithery Discord or open an issue in this repo.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "retrieval",
        "multi search",
        "multiple search",
        "search engines"
      ],
      "category": "web-search"
    },
    "scrapeless-ai--scrapeless-mcp-server": {
      "owner": "scrapeless-ai",
      "name": "scrapeless-mcp-server",
      "url": "https://github.com/scrapeless-ai/scrapeless-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/scrapeless-ai.webp",
      "description": "Retrieve and summarize data from Google Search results, including various Google services, to enhance AI decision-making capabilities. Integrate real-time search context into AI applications for improved interaction and research tools.",
      "stars": 53,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T09:42:40Z",
      "readme_content": "# Scrapeless MCP Server\n\n**Welcome to the official Scrapeless Model Context Protocol (MCP) Server** — a powerful integration layer that empowers LLMs, AI Agents, and AI applications to interact with the web in real time.\n\nBuilt on the open MCP standard, Scrapeless MCP Server seamlessly connects models like **ChatGPT**, **Claude**, and tools like **Cursor** and **Windsurf** to a wide range of external capabilities, including:\n\n- **Google services integration** (Search, Trends)\n- **Browser automation** for page-level navigation and interaction\n- **Scrape** dynamic, JS-heavy sites—export as HTML, Markdown, or screenshots\n\nWhether you're building an AI research assistant, a coding copilot, or autonomous web agents, this server provides the dynamic context and real-world data your workflows need—**without getting blocked**.\n\n## Usage Examples\n\n1. Automated Web Interaction and Data Extraction with Claude\n\nUsing Scrapeless MCP Browser, Claude can perform complex tasks such as web navigation, clicking, scrolling, and scraping through conversational commands, with real-time preview of web interaction results via `live sessions`.\n\n\n\n2. Bypassing Cloudflare to Retrieve Target Page Content\n\nUsing the Scrapeless MCP Browser service, the Cloudflare page is automatically accessed, and after the process is completed, the page content is extracted and returned in Markdown format.\n\n\n\n3. Extracting Dynamically Rendered Page Content and Writing to File\n\nUsing the Scrapeless MCP Universal API, the JavaScript-rendered content of the target page above is scraped, exported in Markdown format, and finally written to a local file named **`text.md`**.\n\n\n\n4. Automated SERP Scraping\n\nUsing the Scrapeless MCP Server, query the keyword “web scraping” on Google Search, retrieve the first 10 search results (including title, link, and summary), and write the content to the file named `serp.text`.\n\n\n\nHere are some additional examples of how to use these servers:\n\n| Example                                                                                                                           |\n| --------------------------------------------------------------------------------------------------------------------------------- |\n| Search scrapeless by Google search.                                                                                               |\n| Find the search interest for \"AI\" over the last year.                                                                             |\n| Use a browser to visit [chatgpt.com](http://chatgpt.com), search for \"What's the weather like today?\", and summarize the results. |\n| Scrape the HTML content of [scrapeless.com](http://scrapeless.com) page.                                                          |\n| Scrape the Markdown content of [scrapeless.com](http://scrapeless.com) page.                                                      |\n| Get screenshots of [scrapeless.com](http://scrapeless.com).                                                                       |\n\n## Setup Guide\n\n1. Get Scrapeless Key\n\n- [Log in](https://app.scrapeless.com/passport/login?utm_source=github&utm_medium=github-mcp&utm_campaign=mcp) to the Scrapeless Dashboard（Free trial available）\n- Then click \"**Setting**\" on the left -> select \"**API Key Management**\" -> click \"**Create API Key**\". Finally, click the API Key you created to **copy** it.\n\n\n\n2. Configure Your MCP Client\n\nScrapeless MCP Server supports both **Stdio** and **Streamable HTTP** transport modes.\n\n🖥️ Stdio (Local Execution)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"scrapeless-mcp-server\"],\n      \"env\": {\n        \"SCRAPELESS_KEY\": \"YOUR_SCRAPELESS_KEY\"\n      }\n    }\n  }\n}\n```\n\n🌐 Streamable HTTP (Hosted API Mode)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.scrapeless.com/mcp\",\n      \"headers\": {\n        \"x-api-token\": \"YOUR_SCRAPELESS_KEY\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n#### Advanced Options\n\nCustomize browser session behavior with optional parameters. These can be set via environment variables (for Stdio) or HTTP headers (for Streamable HTTP):\n\n| Stdio (Env Var)         | Streamable HTTP (HTTP Header) | Description                                                                                                                  |\n| ----------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n| BROWSER_PROFILE_ID      | x-browser-profile-id          | Specifies a reusable browser profile ID for session continuity.                                                              |\n| BROWSER_PROFILE_PERSIST | x-browser-profile-persist     | Enables persistent storage for cookies, local storage, etc.                                                                  |\n| BROWSER_SESSION_TTL     | x-browser-session-ttl         | Defines the **maximum session timeout** in seconds. The session will automatically expire after this duration of inactivity. |\n\n## Integration with Claude Desktop\n\n1. Open **Claude Desktop**\n2. Navigate to: `Settings` → `Tools` → `MCP Servers`\n3. Click **\"Add MCP Server\"**\n4. Paste either the `Stdio` or `Streamable HTTP` config above\n5. Save and enable the server\n6. Claude will now be able to issue web queries, extract content, and interact with pages using Scrapeless\n\n## Integration with Cursor IDE\n\n1. Open **Cursor**\n2. Press `Cmd + Shift + P` and search for: `Configure MCP Servers`\n3. Add the Scrapeless MCP config using the format above\n4. Save the file and restart Cursor (if needed)\n5. Now you can ask Cursor things like:\n   1. `\"Search StackOverflow for a solution to this error\"`\n   2. `\"Scrape the HTML from this page\"`\n6. And it will use Scrapeless in the background.\n\n## Supported MCP Tools\n\n| Name               | Description                                                    |\n| ------------------ | -------------------------------------------------------------- |\n| google_search      | Universal information search engine.                           |\n| google_trends      | Get trending search data from Google Trends.                   |\n| browser_create     | Create or reuse a cloud browser session using Scrapeless.      |\n| browser_close      | Closes the current session by disconnecting the cloud browser. |\n| browser_goto       | Navigate browser to a specified URL.                           |\n| browser_go_back    | Go back one step in browser history.                           |\n| browser_go_forward | Go forward one step in browser history.                        |\n| browser_click      | Click a specific element on the page.                          |\n| browser_type       | Type text into a specified input field.                        |\n| browser_press_key  | Simulate a key press.                                          |\n| browser_wait_for   | Wait for a specific page element to appear.                    |\n| browser_wait       | Pause execution for a fixed duration.                          |\n| browser_screenshot | Capture a screenshot of the current page.                      |\n| browser_get_html   | Get the full HTML of the current page.                         |\n| browser_get_text   | Get all visible text from the current page.                    |\n| browser_scroll     | Scroll to the bottom of the page.                              |\n| browser_scroll_to  | Scroll a specific element into view.                           |\n| scrape_html        | Scrape a URL and return its full HTML content.                 |\n| scrape_markdown    | Scrape a URL and return its content as Markdown.               |\n| scrape_screenshot  | Capture a high-quality screenshot of any webpage.              |\n\n## Security Best Practices\n\nWhen using Scrapeless MCP Server with LLMs (like ChatGPT, Claude, or Cursor), it's critical to handle all scraped or extracted web content with care. **Web data is untrusted by default**, and improper handling may expose your application to prompt injection or other security vulnerabilities.\n\n#### ✅ Recommended Practices\n\n- **Never pass raw scraped content directly into LLM prompts.** Raw HTML, JavaScript, or user-generated text may contain hidden injection payloads.\n- **Sanitize and validate all extracted content.** Strip or escape potentially harmful tags and scripts before using content in downstream logic or AI models.\n- **Prefer structured extraction over free-form text.** Use tools like `scrape_html`, `scrape_markdown`, or targeted `browser_get_text` with known-safe selectors to extract only the content you trust.\n- **Apply domain or selector whitelisting** when scraping dynamically generated pages, to restrict data flow to known and trusted sources.\n- **Log and monitor all outbound requests** made via browser or scraping tools, especially if you're handling sensitive data, tokens, or internal network access.\n\n#### 🚫 Avoid\n\n- Injecting scraped HTML directly into prompts\n- Letting users specify arbitrary URLs or CSS selectors without validation\n- Storing unfiltered scraped content for future prompt usage\n\n## Community\n\n- [MCP Server Discord](https://backend.scrapeless.com/app/api/v1/public/links/discord)\n\n## Contact Us\n\nFor questions, suggestions, or collaboration inquiries, feel free to contact us via:\n\n- Email: [market@scrapeless.com](mailto:market@scrapeless.com)\n- Official Website: [https://www.scrapeless.com](https://www.scrapeless.com/)\n- Community Forum: https://discord.gg/Np4CAHxB9a",
      "npm_url": "https://www.npmjs.com/package/scrapeless-mcp-server",
      "npm_downloads": 7193,
      "keywords": [
        "google",
        "search",
        "ai",
        "scrapeless ai",
        "ai scrapeless",
        "search scrapeless"
      ],
      "category": "web-search"
    },
    "sengokudaikon--opendeepsearch_mcp": {
      "owner": "sengokudaikon",
      "name": "opendeepsearch_mcp",
      "url": "https://github.com/sengokudaikon/opendeepsearch_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sengokudaikon.webp",
      "description": "Interact with OpenDeepSearch's search functionalities through a standardized interface, enabling LLM applications to leverage advanced web search capabilities. Integrates seamlessly with MCP-compatible clients for enhanced search integration.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T09:05:06Z",
      "readme_content": "# OpenDeepSearch MCP Server\n\nThis is a Model Context Protocol (MCP) server for OpenDeepSearch that allows LLM applications to interact with OpenDeepSearch's search capabilities.\n\n## Features\n\n- Exposes OpenDeepSearch's search functionality as MCP tools\n- Integrates with Claude Desktop and other MCP-compatible clients\n- Provides a standardized interface for LLM applications to access web search capabilities\n\n## Setup\n\nThis project uses `uv` for dependency management.\n\n1.  **Install `uv`**: Follow the instructions [here](https://docs.astral.sh/uv/install/).\n2.  **Sync Dependencies**: Navigate to the `mcp_server` directory and run:\n    ```bash\n    uv sync\n    ```\n    This will install dependencies based on `pyproject.toml` and `uv.lock`.\n\n## Configuration\n\nThe server requires certain environment variables to function correctly, especially API keys for the underlying services. These can be set directly in your environment or passed via the MCP client configuration (e.g., using Smithery CLI).\n\n| Variable               | Description                                                                      | Required | Default | Notes                                                                                         |\n|:-----------------------|:---------------------------------------------------------------------------------|:---------|:--------|:----------------------------------------------------------------------------------------------|\n| **LLM Providers**      | **(Provide at least one)**                                                       |          |         |                                                                                               |\n| `OPENAI_API_KEY`       | API key for OpenAI LLM.                                                          | Optional | None    | Needed if using OpenAI models.                                                                |\n| `OPENAI_BASE_URL`      | Custom base URL for OpenAI compatible endpoints.                                 | Optional | None    |                                                                                               |\n| `ANTHROPIC_API_KEY`    | API key for Anthropic LLM.                                                       | Optional | None    | Needed if using Anthropic models.                                                             |\n| `OPENROUTER_API_KEY`   | API key for OpenRouter.                                                          | Optional | None    | Needed if using OpenRouter models.                                                            |\n| `FIREWORKS_API_KEY`    | API key for Fireworks AI.                                                        | Optional | None    | Needed if using Fireworks models.                                                             |\n| `GEMINI_API_KEY`       | API key for Google Gemini.                                                       | Optional | None    | Needed if using Gemini models.                                                                |\n| `AZURE_API_KEY`        | API key for Azure OpenAI Service.                                                | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_API_BASE`       | API base URL for Azure OpenAI Service.                                           | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_API_VERSION`    | API version for Azure OpenAI Service.                                            | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_DEPLOYMENT_ID`  | Deployment ID for Azure OpenAI Service.                                          | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `DEEPSEEK_API_KEY`     | API key for DeepSeek.                                                            | Optional | None    | Needed if using DeepSeek models.                                                              |\n| **Search Providers**   |                                                                                  |          |         |                                                                                               |\n| `SERPER_API_KEY`       | API key for Serper search provider.                                              | Optional | None    | Required if `search_provider` is set to `'serper'` (either by default or via tool argument).  |\n| `SEARXNG_INSTANCE_URL` | URL of your SearXNG instance.                                                    | Optional | None    | Required if `search_provider` is set to `'searxng'` (either by default or via tool argument). |\n| `SEARXNG_API_KEY`      | API key for your SearXNG instance (if required by the instance).                 | Optional | None    | Used if `search_provider` is set to `'searxng'`.                                              |\n| **Rerankers**          |                                                                                  |          |         |                                                                                               |\n| `JINA_API_KEY`         | API key for Jina AI Reranker.                                                    | Optional | None    | Required if `reranker` is set to `'jina'` (either by default or via tool argument).           |\n| **Other Tools**        |                                                                                  |          |         |                                                                                               |\n| `WOLFRAM_ALPHA_APP_ID` | App ID for WolframAlpha tool integration (if enabled in the agent).              | Optional | None    |                                                                                               |\n| **Server Behavior**    |                                                                                  |          |         |                                                                                               |\n| `LOG_LEVEL`            | Controls the server's logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL). | Optional | INFO    | Can also be set via the `--log-level` CLI argument passed by `smithery.yaml`.                 |\n\n**Note:** API keys passed directly as arguments to the `perform_search` tool (`serper_api_key`, `searxng_api_key`, `jina_api_key`) will temporarily override the environment variables for that specific call.\n\n## Usage with Smithery CLI\n\nYou can run this server using the Smithery CLI and the provided `smithery.yaml` configuration file. This allows you to easily manage the required environment variables.\n\n```bash\n# Example: Run with OpenRouter key and Serper key\nnpx -y @smithery/cli@latest run . --config '{\"openrouterApiKey\":\"sk-or-...\", \"serperApiKey\":\"your-serper-key\"}'\n\n# Example: Run with OpenAI key and SearXNG\nnpx -y @smithery/cli@latest run . --config '{\"openaiApiKey\":\"sk-...\", \"searxngInstanceUrl\":\"https://your-searxng-instance.com\"}'\n\n# Example: Run with Gemini key\nnpx -y @smithery/cli@latest run . --config '{\"geminiApiKey\":\"...\"}'\n\n# Example: Run with Azure keys\nnpx -y @smithery/cli@latest run . --config '{\"azureApiKey\":\"...\", \"azureApiBase\":\"https://your-azure.openai.azure.com/\", \"azureApiVersion\":\"2024-02-01\", \"azureDeploymentId\":\"your-deployment\"}'\n```\n\nThe `smithery.yaml` file defines the necessary configuration schema. Refer to it for all available options.\n\n## Development\n\nThis package follows the MCP specification and provides tools for search functionality through OpenDeepSearch.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opendeepsearch_mcp",
        "search",
        "opendeepsearch",
        "opendeepsearch search",
        "search integration",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "shalomeir--daily": {
      "owner": "shalomeir",
      "name": "daily",
      "url": "https://github.com/shalomeir/daily",
      "imageUrl": "/freedevtools/mcp/pfp/shalomeir.webp",
      "description": "Provides a personalized news feed for developers, connects them to communities, and enables search functionalities to facilitate learning and collaboration in the developer ecosystem.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2025-04-16T04:32:42Z",
      "readme_content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n\n\n\n<br>\n\n# Welcome to the daily.dev repository\n\nWe know how hard it is to be a developer. It doesn't have to be. <br/>\ndaily.dev is the homepage every developer deserves. <br/>\n Personalized news feed, dev communities and search, much better than what’s out there. Maybe ;)\n\n[Product Docs][product-docs-link] · [Changelog][changelog-link] · [Report a Bug][report-bug-link] · [Request a Feature][github-discussions-link] · [Swag Store][swag-store-link] · [Brand Assets][brand-book-link]\n\n<!-- SHIELD GROUP -->\n\n[![][chrome-users-shield]][chrome-users-link]\n[![][extension-rating-shield]][extension-rating-link]\n[![][latest-version-shield]][latest-version-link]<br/>\n[![][github-stars-shield]][github-stars-link]\n[![][github-license-shield]][github-license-link]<br/>\n\n**Help more developers suffer less by sharing daily.dev**\n\n[![][share-x-shield]][share-x-link]\n[![][share-telegram-shield]][share-telegram-link]\n[![][share-whatsapp-shield]][share-whatsapp-link]\n[![][share-reddit-shield]][share-reddit-link]\n[![][share-mastodon-shield]][share-mastodon-link]\n[![][share-linkedin-shield]][share-linkedin-link]\n\n<sup>Want to contribute? Get started with Gitpod by clicking the button below</sup>\n\n<p align=\"center\">\n  <a href=\"https://gitpod.io/#https://github.com/dailydotdev/apps/\">\n    <img src=\"https://gitpod.io/button/open-in-gitpod.svg\" alt=\"Open in Gitpod\">\n  </a>\n</p>\n\n\n</br>\n<a href=\"https://youtu.be/igZCEr3HwCg\"><strong>👀 Watch it in action → </strong></a>\n\n</div>\n\n## 💜 About daily.dev\n\n> daily.dev is a professional network for developers to learn, collaborate, and grow together\n 👩‍💻\n\nWith daily.dev you can discover a wide variety of professional knowledge, create groups where you can collaborate with other developers you appreciate, and discuss the latest trends in the developer ecosystem. It works offline and is available both as a browser extension and as a Progressive Web App (PWA). \n\n> \\[!IMPORTANT]\n>\n> Star us to show your support and love for daily.dev ⭐️\n\n\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=dailydotdev%2Fdaily&theme=light&type=Date\">\n    <img alt=\"svg_repos_dailydotdev_daily_type_Date\" width=\"100%\" src=\"https://api.star-history.com/svg?repos=dailydotdev%2Fdaily&type=Date\">\n  </picture>\n</details>\n\n## 📌 Get daily.dev\n\ndaily.dev is currently available for Google Chrome and Microsoft Edge. There's also a PWA for mobile devices. \n\nGet it now on:\n\n<p align=\"center\">\n    <a href=\"https://r.daily.dev/chrome\">\n    <img src=\"https://img.shields.io/badge/%20-Chrome-red?logo=google-chrome&logoColor=white\" alt=\"Download for Chrome\" />\n    </a>\n    <a href=\"https://microsoftedge.microsoft.com/addons/detail/dailydev-news-for-busy/cbdhgldgiancdheindpekpcbkccpjaeb\">\n    <img src=\"https://img.shields.io/badge/%20-Edge-blue?logo=microsoft-edge&logoColor=white\" alt=\"Download for Edge\" />\n    </a>\n    <a href=\"http://go.daily.dev/\">\n    <img src=\"https://img.shields.io/badge/%20-Mobile-502ab0\" alt=\"Download for Mobile\" />\n    </a>\n</p>\n\n## 📯 Philosophy\n\nWe recognize that developers today have the greatest power as a professional group to drive change and affect lives of billions. There are many platforms that provide developers with tools that serve to their success, or the goals of their workplace, but daily.dev is by-design for developers themselves. \n\nWe, as developers, know how challenging it is to grow professionally with so much going on, and that's why we built daily.dev - to make it easy for us to navigate abundance of content and discover all the knowledge they need with zero effort.\n\nYou can use daily.dev to:\n\n* 👨‍💻 Learn and stay up-to-date\n* 🙌 Interact based on the latest trends in our ecosystem\n* 🚀 Collaborate with other developers you know and appreciate\n\n## 🌲 daily.dev projects\n\ndaily.dev might look simple on the surface but actually, it is powered by a complex and robust system of different applications. It contains several services, some are big, others are micro and easy to maintain. Below is the list of different projects that we maintain under daily.dev.\n\n### 🙌 Community & Docs\n\n*  [daily](https://github.com/dailydotdev/daily) - This is the repository you are currently at. It serves as a central place for all the projects. It contains documentation, community ideas, suggestions, and whatnot.\n\n*  [docs](https://github.com/dailydotdev/docs) - This is the repository for official product documentation of daily.dev.\n\n\n### 🎨 Frontend\n\n* [apps](https://github.com/dailydotdev/apps) - Monorepo with all the frontend related projects since daily.dev 3.0. This includes both the extension and the webapp.\n\n### 🎈 Others\n\n* [action-devcard](https://github.com/dailydotdev/action-devcard) - GitHub Actions for adding daily DevCard to GitHub profile. Read [the guide](https://daily.dev/blog/adding-the-daily-devcard-to-your-github-profile) to set it up.\n\n## 🚀 Running daily.dev locally\n\nTo spin up a local environment, we suggest using GitPod. Everything is already configured and should work out of the box. We have a GitPod button above, click on it and let's roll!\n\n> Note that currently, not all services are ready (or needed) for local environment so Daily Redirector and Daily Monetization and Authentication services are not available for you.\n\n## 🙌 Want to contribute?\n\nWe are open to all kinds of contributions. If you want to:\n* 🤔 Suggest a feature\n* 🐛 Report an issue\n* 📖 Improve documentation\n* 👨‍💻 Contribute to the code\n\nYou are more than welcome. Before contributing, kindly check our [guidelines](https://github.com/dailydotdev/.github/blob/master/CONTRIBUTING.md).\n\n## 💬 What do you think of daily.dev?\n\n<div align=\"left\">\n    <p><a href=\"https://twitter.com/dailydotdev/\"><img alt=\"Twitter @dailydotdev\" align=\"center\" src=\"https://img.shields.io/badge/twitter-%231DA1F2.svg?&style=for-the-badge&logo=twitter&logoColor=white\" /></a>&nbsp; Tweet us @dailydotdev to share your thoughts and stay up-to-date. </p>\n    <p><a href=\"https://www.producthunt.com/products/daily-dev\"><img alt=\"daily.dev at ProductHunt\" align=\"center\" src=\"https://img.shields.io/badge/producthunt-%23DA552F.svg?&style=for-the-badge&logo=product-hunt&logoColor=white\" /></a>&nbsp; Check out our Product Hunt page.</p>\n    <p><a href=\"https://daily.dev\"><img alt=\"daily.dev Website\" align=\"center\" src=\"https://img.shields.io/badge/Daily Website-%233693F3.svg?&style=for-the-badge&logo=icloud&logoColor=white\" /></a>&nbsp; Visit our home for a bunch of useful links.</p>\n    <p><a href=\"https://chrome.google.com/webstore/detail/daily-20-source-for-busy/jlmpjdjjbgclbocgajdjefcidcncaied\"><img alt=\"daily.dev at ChomeStore\" align=\"center\" src=\"https://img.shields.io/badge/Chrome Web Store-%234285F4.svg?&style=for-the-badge&logo=google-chrome&logoColor=white\" /></a>&nbsp; See our Chrome Store page to grab the extension or share your feedback.</p>\n    <p><a href=\"https://microsoftedge.microsoft.com/addons/detail/dailydev-news-for-busy/cbdhgldgiancdheindpekpcbkccpjaeb\"><img alt=\"daily.dev at EdgeAddons\" align=\"center\" src=\"https://img.shields.io/badge/Edge Addons-%230078D7.svg?&style=for-the-badge&logo=microsoft-edge&logoColor=white\" /></a>&nbsp; Check us out on Microsoft Edge Addons and let us know your thoughts.</p>\n\n## 📑 License\nLicensed under [AGPL-3.0](https://github.com/dailydotdev/daily/blob/master/LICENSE).\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n<!-- LINK GROUP -->\n\n[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-151515?style=flat-square\n[product-docs-link]: https://docs.daily.dev/docs/intro\n[changelog-link]: https://app.daily.dev/sources/daily_updates\n[report-bug-link]: https://github.com/dailydotdev/daily/issues/new?assignees=&labels=Type%3A+Bug&projects=&template=---bug-report.yml&title=%F0%9F%90%9B+BUG%3A+\n[github-discussions-link]: https://github.com/dailydotdev/daily/discussions/new?category=feature-requests\n[swag-store-link]: https://store.daily.dev/\n[brand-book-link]: https://brand.daily.dev/\n\n<!-- SHIELDS GROUP -->\n\n[chrome-users-shield]: https://img.shields.io/chrome-web-store/users/jlmpjdjjbgclbocgajdjefcidcncaied?style=flat-square&logo=googlechrome&logoColor=white&label=chrome%20active%20users&labelColor=black&color=9E15D9\n[chrome-users-link]: https://chromewebstore.google.com/detail/dailydev-the-homepage-dev/jlmpjdjjbgclbocgajdjefcidcncaied\n[extension-rating-shield]: https://img.shields.io/amo/rating/daily?style=flat-square&labelColor=black&color=0FC54F\n[extension-rating-link]: https://api.daily.dev/get\n[latest-version-shield]: https://img.shields.io/chrome-web-store/v/jlmpjdjjbgclbocgajdjefcidcncaied?style=flat-square&label=latest%20version&labelColor=black&color=0FC54F\n[latest-version-link]: https://api.daily.dev/get\n[github-stars-shield]: https://img.shields.io/github/stars/dailydotdev/daily?style=flat-square&logo=github&labelColor=black&color=508CF9\n[github-stars-link]: https://github.com/dailydotdev/daily/stargazers\n[github-license-shield]: https://img.shields.io/github/license/dailydotdev/daily?style=flat-square&logo=github&labelColor=black&color=508CF9\n[github-license-link]: https://github.com/dailydotdev/daily/issues\n\n<!-- SHARE BUTTONS GROUP -->\n\n[share-linkedin-link]: https://www.linkedin.com/shareArticle?mini=true&url=https%3A//daily.dev\n[share-linkedin-shield]: https://img.shields.io/badge/-share%20on%20linkedin-black?labelColor=black&logo=linkedin&logoColor=white&style=flat-square\n[share-mastodon-link]: https://mastodon.social/share?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.&url=\n[share-mastodon-shield]: https://img.shields.io/badge/-share%20on%20mastodon-black?labelColor=black&logo=mastodon&logoColor=white&style=flat-square\n[share-reddit-link]: http://www.reddit.com/submit?url=https%3A%2F%2Fdaily.dev&title=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.\n[share-reddit-shield]: https://img.shields.io/badge/-share%20on%20reddit-black?labelColor=black&logo=reddit&logoColor=white&style=flat-square\n[share-telegram-link]: https://t.me/share/url?url=https%3A//daily.dev&text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.\n[share-telegram-shield]: https://img.shields.io/badge/-share%20on%20telegram-black?labelColor=black&logo=telegram&logoColor=white&style=flat-square\n[share-whatsapp-link]: https://api.whatsapp.com/send?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.%20https%3A%2F%2Fdaily.dev\n[share-whatsapp-shield]: https://img.shields.io/badge/-share%20on%20whatsapp-black?labelColor=black&logo=whatsapp&logoColor=white&style=flat-square\n[share-x-link]: https://twitter.com/intent/tweet?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.&url=\n[share-x-shield]: https://img.shields.io/badge/-share%20on%20x-black?labelColor=black&logo=x&logoColor=white&style=flat-square",
      "npm_url": "https://www.npmjs.com/package/daily",
      "npm_downloads": 580,
      "keywords": [
        "shalomeir",
        "search",
        "news",
        "search shalomeir",
        "shalomeir daily",
        "web search"
      ],
      "category": "web-search"
    },
    "shariqriazz--vertex-ai-mcp-server": {
      "owner": "shariqriazz",
      "name": "vertex-ai-mcp-server",
      "url": "https://github.com/shariqriazz/vertex-ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/shariqriazz.webp",
      "description": "Provides seamless access to Google Cloud's Vertex AI Gemini models for coding assistance and general query answering, with enhanced query responses grounded in web search and direct knowledge.",
      "stars": 85,
      "forks": 16,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-15T07:47:01Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/shariqriazz-vertex-ai-mcp-server-badge.png)](https://mseep.ai/app/shariqriazz-vertex-ai-mcp-server)\n\n# Vertex AI MCP Server\n[![smithery badge](https://smithery.ai/badge/@shariqriazz/vertex-ai-mcp-server)](https://smithery.ai/server/@shariqriazz/vertex-ai-mcp-server)\n\nThis project implements a Model Context Protocol (MCP) server that provides a comprehensive suite of tools for interacting with Google Cloud's Vertex AI Gemini models, focusing on coding assistance and general query answering.\n\n<a href=\"https://glama.ai/mcp/servers/@shariqriazz/vertex-ai-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@shariqriazz/vertex-ai-mcp-server/badge\" alt=\"Vertex AI Server MCP server\" />\n</a>\n\n## Features\n\n*   Provides access to Vertex AI Gemini models via numerous MCP tools.\n*   Supports web search grounding (`answer_query_websearch`) and direct knowledge answering (`answer_query_direct`).\n*   Configurable model ID, temperature, streaming behavior, max output tokens, and retry settings via environment variables.\n*   Uses streaming API by default for potentially better responsiveness.\n*   Includes basic retry logic for transient API errors.\n*   Minimal safety filters applied (`BLOCK_NONE`) to reduce potential blocking (use with caution).\n\n## Tools Provided\n\n### Query & Generation (AI Focused)\n*   `answer_query_websearch`: Answers a natural language query using the configured Vertex AI model enhanced with Google Search results.\n*   `answer_query_direct`: Answers a natural language query using only the internal knowledge of the configured Vertex AI model.\n*   `explain_topic_with_docs`: Provides a detailed explanation for a query about a specific software topic by synthesizing information primarily from official documentation found via web search.\n*   `get_doc_snippets`: Provides precise, authoritative code snippets or concise answers for technical queries by searching official documentation.\n*   `generate_project_guidelines`: Generates a structured project guidelines document (Markdown) based on a specified list of technologies (optionally with versions), using web search for best practices.\n\n### Research & Analysis Tools\n*   `code_analysis_with_docs`: Analyzes code snippets by comparing them with best practices from official documentation, identifying potential bugs, performance issues, and security vulnerabilities.\n*   `technical_comparison`: Compares multiple technologies, frameworks, or libraries based on specific criteria, providing detailed comparison tables with pros/cons and use cases.\n*   `architecture_pattern_recommendation`: Suggests architecture patterns for specific use cases based on industry best practices, with implementation examples and considerations.\n*   `dependency_vulnerability_scan`: Analyzes project dependencies for known security vulnerabilities, providing detailed information and mitigation strategies.\n*   `database_schema_analyzer`: Reviews database schemas for normalization, indexing, and performance issues, suggesting improvements based on database-specific best practices.\n*   `security_best_practices_advisor`: Provides security recommendations for specific technologies or scenarios, with code examples for implementing secure practices.\n*   `testing_strategy_generator`: Creates comprehensive testing strategies for applications or features, suggesting appropriate testing types with coverage goals.\n*   `regulatory_compliance_advisor`: Provides guidance on regulatory requirements for specific industries (GDPR, HIPAA, etc.), with implementation approaches for compliance.\n*   `microservice_design_assistant`: Helps design microservice architectures for specific domains, with service boundary recommendations and communication patterns.\n*   `documentation_generator`: Creates comprehensive documentation for code, APIs, or systems, following industry best practices for technical documentation.\n\n### Filesystem Operations\n*   `read_file_content`: Read the complete contents of one or more files. Provide a single path string or an array of path strings.\n*   `write_file_content`: Create new files or completely overwrite existing files. The 'writes' argument accepts a single object (`{path, content}`) or an array of such objects.\n*   `edit_file_content`: Makes line-based edits to a text file, returning a diff preview or applying changes.\n*   `list_directory_contents`: Lists files and directories directly within a specified path (non-recursive).\n*   `get_directory_tree`: Gets a recursive tree view of files and directories as JSON.\n*   `move_file_or_directory`: Moves or renames files and directories.\n*   `search_filesystem`: Recursively searches for files/directories matching a name pattern, with optional exclusions.\n*   `get_filesystem_info`: Retrieves detailed metadata (size, dates, type, permissions) about a file or directory.\n*   `execute_terminal_command`: Execute a shell command, optionally specifying `cwd` and `timeout`. Returns stdout/stderr.\n\n### Combined AI + Filesystem Operations\n*   `save_generate_project_guidelines`: Generates project guidelines based on a tech stack and saves the result to a specified file path.\n*   `save_doc_snippet`: Finds code snippets from documentation and saves the result to a specified file path.\n*   `save_topic_explanation`: Generates a detailed explanation of a topic based on documentation and saves the result to a specified file path.\n*   `save_answer_query_direct`: Answers a query using only internal knowledge and saves the answer to a specified file path.\n*   `save_answer_query_websearch`: Answers a query using web search results and saves the answer to a specified file path.\n\n*(Note: Input/output schemas for each tool are defined in their respective files within `src/tools/` and exposed via the MCP server.)*\n\n## Prerequisites\n\n*   Node.js (v18+)\n*   Bun (`npm install -g bun`)\n*   Google Cloud Project with Billing enabled.\n*   Vertex AI API enabled in the GCP project.\n*   Google Cloud Authentication configured in your environment (Application Default Credentials via `gcloud auth application-default login` is recommended, or a Service Account Key).\n\n## Setup & Installation\n\n1.  **Clone/Place Project:** Ensure the project files are in your desired location.\n2.  **Install Dependencies:**\n    ```bash\n    bun install\n    ```\n3.  **Configure Environment:**\n    *   Create a `.env` file in the project root (copy `.env.example`).\n    *   Set the required and optional environment variables as described in `.env.example`.\n        *   Set `AI_PROVIDER` to either `\"vertex\"` or `\"gemini\"`.\n        *   If `AI_PROVIDER=\"vertex\"`, `GOOGLE_CLOUD_PROJECT` is required.\n        *   If `AI_PROVIDER=\"gemini\"`, `GEMINI_API_KEY` is required.\n4.  **Build the Server:**\n    ```bash\n    bun run build\n    ```\n    This compiles the TypeScript code to `build/index.js`.\n\n## Usage (Standalone / NPX)\n\nOnce published to npm, you can run this server directly using `npx`:\n\n```bash\n# Ensure required environment variables are set (e.g., GOOGLE_CLOUD_PROJECT)\nbunx vertex-ai-mcp-server\n```\n\nAlternatively, install it globally:\n\n```bash\nbun install -g vertex-ai-mcp-server\n# Then run:\nvertex-ai-mcp-server\n```\n\n**Note:** Running standalone requires setting necessary environment variables (like `GOOGLE_CLOUD_PROJECT`, `GOOGLE_CLOUD_LOCATION`, authentication credentials if not using ADC) in your shell environment before executing the command.\n\n### Installing via Smithery\n\nTo install Vertex AI Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@shariqriazz/vertex-ai-mcp-server):\n\n```bash\nbunx -y @smithery/cli install @shariqriazz/vertex-ai-mcp-server --client claude\n```\n\n## Running with Cline\n\n1.  **Configure MCP Settings:** Add/update the configuration in your Cline MCP settings file (e.g., `.roo/mcp.json`). You have two primary ways to configure the command:\n\n    **Option A: Using Node (Direct Path - Recommended for Development)**\n\n    This method uses `node` to run the compiled script directly. It's useful during development when you have the code cloned locally.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"vertex-ai-mcp-server\": {\n          \"command\": \"node\",\n          \"args\": [\n            \"/full/path/to/your/vertex-ai-mcp-server/build/index.js\" // Use absolute path or ensure it's relative to where Cline runs node\n          ],\n          \"env\": {\n            // --- General AI Configuration ---\n            \"AI_PROVIDER\": \"vertex\", // \"vertex\" or \"gemini\"\n            // --- Required (Conditional) ---\n            \"GOOGLE_CLOUD_PROJECT\": \"YOUR_GCP_PROJECT_ID\", // Required if AI_PROVIDER=\"vertex\"\n            // \"GEMINI_API_KEY\": \"YOUR_GEMINI_API_KEY\", // Required if AI_PROVIDER=\"gemini\"\n            // --- Optional Model Selection ---\n            \"VERTEX_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"vertex\" (Example override)\n            \"GEMINI_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"gemini\"\n            // --- Optional AI Parameters ---\n            \"GOOGLE_CLOUD_LOCATION\": \"us-central1\", // Specific to Vertex AI\n            \"AI_TEMPERATURE\": \"0.0\",\n            \"AI_USE_STREAMING\": \"true\",\n            \"AI_MAX_OUTPUT_TOKENS\": \"65536\", // Default from .env.example\n            \"AI_MAX_RETRIES\": \"3\",\n            \"AI_RETRY_DELAY_MS\": \"1000\",\n            // --- Optional Vertex Authentication ---\n            // \"GOOGLE_APPLICATION_CREDENTIALS\": \"/path/to/your/service-account-key.json\" // If using Service Account Key for Vertex\n          },\n          \"disabled\": false,\n          \"alwaysAllow\": [\n             // Add tool names here if you don't want confirmation prompts\n             // e.g., \"answer_query_websearch\"\n          ],\n          \"timeout\": 3600 // Optional: Timeout in seconds\n        }\n        // Add other servers here...\n      }\n    }\n    ```\n    *   **Important:** Ensure the `args` path points correctly to the `build/index.js` file. Using an absolute path might be more reliable.\n\n    **Option B: Using NPX (Requires Package Published to npm)**\n\n    This method uses `npx` to automatically download and run the server package from the npm registry. This is convenient if you don't want to clone the repository.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"vertex-ai-mcp-server\": {\n          \"command\": \"bunx\", // Use bunx\n          \"args\": [\n            \"-y\", // Auto-confirm installation\n            \"vertex-ai-mcp-server\" // The npm package name\n          ],\n          \"env\": {\n            // --- General AI Configuration ---\n            \"AI_PROVIDER\": \"vertex\", // \"vertex\" or \"gemini\"\n            // --- Required (Conditional) ---\n            \"GOOGLE_CLOUD_PROJECT\": \"YOUR_GCP_PROJECT_ID\", // Required if AI_PROVIDER=\"vertex\"\n            // \"GEMINI_API_KEY\": \"YOUR_GEMINI_API_KEY\", // Required if AI_PROVIDER=\"gemini\"\n            // --- Optional Model Selection ---\n            \"VERTEX_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"vertex\" (Example override)\n            \"GEMINI_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"gemini\"\n            // --- Optional AI Parameters ---\n            \"GOOGLE_CLOUD_LOCATION\": \"us-central1\", // Specific to Vertex AI\n            \"AI_TEMPERATURE\": \"0.0\",\n            \"AI_USE_STREAMING\": \"true\",\n            \"AI_MAX_OUTPUT_TOKENS\": \"65536\", // Default from .env.example\n            \"AI_MAX_RETRIES\": \"3\",\n            \"AI_RETRY_DELAY_MS\": \"1000\",\n            // --- Optional Vertex Authentication ---\n            // \"GOOGLE_APPLICATION_CREDENTIALS\": \"/path/to/your/service-account-key.json\" // If using Service Account Key for Vertex\n          },\n          \"disabled\": false,\n          \"alwaysAllow\": [\n             // Add tool names here if you don't want confirmation prompts\n             // e.g., \"answer_query_websearch\"\n          ],\n          \"timeout\": 3600 // Optional: Timeout in seconds\n        }\n        // Add other servers here...\n      }\n    }\n    ```\n    *   Ensure the environment variables in the `env` block are correctly set, either matching `.env` or explicitly defined here. Remove comments from the actual JSON file.\n\n2.  **Restart/Reload Cline:** Cline should detect the configuration change and start the server.\n\n3.  **Use Tools:** You can now use the extensive list of tools via Cline.\n\n## Development\n\n*   **Watch Mode:** `bun run watch`\n*   **Linting:** `bun run lint`\n*   **Formatting:** `bun run format`\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/vertex-ai-mcp-server",
      "npm_downloads": 1981,
      "keywords": [
        "search",
        "shariqriazz",
        "google",
        "search shariqriazz",
        "shariqriazz vertex",
        "cloud vertex"
      ],
      "category": "web-search"
    },
    "shiquda--mediawiki-mcp-server": {
      "owner": "shiquda",
      "name": "mediawiki-mcp-server",
      "url": "https://github.com/shiquda/mediawiki-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/shiquda.webp",
      "description": "Interact with Wikipedia's API to search and retrieve content efficiently, providing access to rich data and insights from various wiki sites.",
      "stars": 15,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T13:24:50Z",
      "readme_content": "<b>Outdated. Please use https://github.com/ProfessionalWiki/MediaWiki-MCP-Server instead!</b>\n\n# MediaWiki MCP Server 🚀\n\n[![smithery badge](https://smithery.ai/badge/@shiquda/mediawiki-mcp-server)](https://smithery.ai/server/@shiquda/mediawiki-mcp-server) ![Python_3_13_informational_logo_style_flat_logoColor_00bfff_color_005566_labelColor_00bfe6](https://img.shields.io/badge/Python-3.13-informational?logo=&style=flat&logoColor=00bfff&color=005566&labelColor=00bfe6) ![build_with_uv_informational_logo_style_flat_logoColor_333333_color_622867_labelColor_de5fe9](https://img.shields.io/badge/build%20with-uv-informational?logo=&style=flat&logoColor=333333&color=622867&labelColor=de5fe9) \n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/shiquda-mediawiki-mcp-server-badge.png)](https://mseep.ai/app/shiquda-mediawiki-mcp-server)\n\nA MCP server that provides seamless interaction with Wikipedia's API. This tool allows you to search and retrieve Wikipedia content with LLMs 🤖!\n\n<https://github.com/user-attachments/assets/b5d9c5f3-a60e-48ea-8b4b-f1a7524d4fbb>\n\n## Features ✨\n\n- 🔍 Search wiki pages with customizable wiki site. e.g. wikipedia.org, fandom.com, wiki.gg and more!\n- 📖 Retrieve detailed page content\n\n## Usage 💻\n\n1. Ensure that uv is installed on your device.\n2. Configure in your client:\n\nThe server defaults to using <https://en.wikipedia.org/>. Also, you can make the server search other wiki sites!\n\nTo see if a wiki site works with this server, check if it uses MediaWiki software (usually shown by an icon at the bottom of the site).\n\nTo check further and find the endpoint (usually the website's domain, like <https://mediawiki.org/>), check by going to base-url/rest.php/v1/page in a browser (like <https://noita.wiki.gg/rest.php/v1/page>) and see if the output looks right. If not, add '/w' to the base URL and try again.\n\nThen, set this endpoint as --base-url:\n\n\n\nAvailable transports: stdio (default), streamable-http (http://localhost/mcp), and SSE (http://localhost/sse). See -h for all options.\n\n```bash\nuvx mediawiki-mcp-server --h\nusage: main.py [-h] [--base-url BASE_URL] [--http] [--sse] [--port PORT]\n\nMediaWiki MCP Server\n\noptions:\n  -h, --help           show this help message and exit\n  --base-url BASE_URL  Base URL for the MediaWiki API (default: https://en.wikipedia.org/w/``)\n  --http               Run server as streamable-http (instead of stdio)\n  --sse                Run server as sse-http (instead of stdio)\n  --port PORT          Default port for http transport (default: 8000)\n\n```\n\nExample JSON configurations:\n\n```json\n{\n  \"mcpServers\": {\n    \"mediawiki-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mediawiki-mcp-server\",\n        \"--base-url\", \"https://example.com/\"\n      ],\n      \"env\": {\n        \"HTTP_PROXY\": \"http://example.com:port\"\n      }\n    }\n  }\n}\n```\n\nOr, if you want to run this server from source:\n\n```json\n{\n  \"mcpServers\": {\n    \"mediawiki-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \n        \"mediawiki-mcp-server\",\n        \"path/to/project/src/mediawiki_mcp_server\",\n        \"--base-url\", \"https://example.com/\"\n      ],\n      \"env\": {\n        \"HTTP_PROXY\": \"http://example.com:port\"\n      }\n    }\n  }\n}\n```\n\n## Supported Tools 🛠\n\n### Search\n\n- `query`: Search term (preferably short and focused)\n- `limit`: Maximum number of results to return (default: 5)\n\n### Get Page\n\n- `title`: The exact title of the Wikipedia page to retrieve\n\n## Development 👨‍💻\n\n```bash\nnpx @modelcontextprotocol/inspector uv run mediawiki-mcp-server\n```\n\nHere are some documents that might help:\n\n- <https://www.mediawiki.org/api/rest_v1/>\n\n## Contributing 🤝\n\nThis server is under development. Contributions are welcome! Feel free to submit issues and pull requests.\n\n## Related Projects ♥️\n\n- [Cherry Studio](https://github.com/CherryHQ/cherry-studio): A desktop client that supports for multiple LLM providers. MCP is supported.",
      "npm_url": "https://www.npmjs.com/package/mediawiki-mcp-server",
      "npm_downloads": 320,
      "keywords": [
        "mediawiki",
        "wiki",
        "wikipedia",
        "wikipedia api",
        "shiquda mediawiki",
        "interact wikipedia"
      ],
      "category": "web-search"
    },
    "skrapeai--skrape-mcp": {
      "owner": "skrapeai",
      "name": "skrape-mcp",
      "url": "https://github.com/skrapeai/skrape-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/skrapeai.webp",
      "description": "Convert web pages into clean, structured Markdown suitable for large language model (LLM) consumption, streamlining the process of feeding web content into AI applications.",
      "stars": 12,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T23:24:30Z",
      "readme_content": "# Skrape MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@skrapeai/skrape-mcp)](https://smithery.ai/server/@skrapeai/skrape-mcp)\n\nConvert webpages into clean, LLM-ready Markdown using [skrape.ai](https://skrape.ai). An MCP server that seamlessly integrates web scraping with Claude Desktop and other MCP-compatible applications.\n\n## Key Features\n\n- **Clean Output**: Removes ads, navigation, and irrelevant content\n- **JavaScript Support**: Handles dynamic content rendering\n- **LLM-Optimized**: Structured Markdown perfect for AI consumption\n- **Consistent Format**: Uniform structure regardless of source\n\n## Features\n\n### Tools\n\n- `get_markdown` - Convert any webpage to LLM-ready Markdown\n  - Takes any input URL and optional parameters\n  - Returns clean, structured Markdown optimized for LLM consumption\n  - Supports JavaScript rendering for dynamic content\n  - Optional JSON response format for advanced integrations\n\n## Installation\n\n### Installing via Smithery\n\nTo install Skrape MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@skrapeai/skrape-mcp):\n\n```bash\nnpx -y @smithery/cli install @skrapeai/skrape-mcp --client claude\n```\n\n### Manual Installation\n\n1. Get your API key from [skrape.ai](https://skrape.ai)\n\n1. Install dependencies:\n\n```bash\nnpm install\n```\n\n1. Build the server:\n\n```bash\nnpm run build\n```\n\n1. Add the server config to Claude Desktop:\n\nOn MacOS:\n\n```bash\nnano ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\nOn Windows:\n\n```bash\nnotepad %APPDATA%/Claude/claude_desktop_config.json\n```\n\nAdd this configuration (replace paths and API key with your values):\n\n```json\n{\n  \"mcpServers\": {\n    \"skrape\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/skrape-mcp/build/index.js\"],\n      \"env\": {\n        \"SKRAPE_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Using with LLMs\n\nHere's how to use the server with Claude or other LLM models:\n\n1. First, ensure the server is properly configured in your LLM application\n2. Then, you can ask the ALLMI to fetch and process any webpage:\n\n```\nConvert this webpage to markdown: https://example.com\n\nClaude will use the MCP tool like this:\n<use_mcp_tool>\n<server_name>skrape</server_name>\n<tool_name>get_markdown</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"options\": {\n    \"renderJs\": true\n  }\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe resulting Markdown will be clean, structured, and ready for LLM processing.\n\n### Advanced Options\n\nThe `get_markdown` tool accepts these parameters:\n\n- `url` (required): Any webpage URL to convert\n- `returnJson` (optional): Set to `true` to get the full JSON response instead of just markdown\n- `options` (optional): Additional scraping options\n  - `renderJs`: Whether to render JavaScript before scraping (default: true)\n\nExample with all options:\n\n```\n<use_mcp_tool>\n<server_name>skrape</server_name>\n<tool_name>get_markdown</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"returnJson\": true,\n  \"options\": {\n    \"renderJs\": false\n  }\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Development\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n---\n\n<a href=\"https://glama.ai/mcp/servers/7i81qzgkzd\">\n<img alt=\"badge\" width=\"190\" height=\"100\" src=\"https://glama.ai/mcp/servers/7i81qzgkzd/badge\" />\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "web",
        "pages",
        "markdown suitable",
        "structured markdown",
        "web content"
      ],
      "category": "web-search"
    },
    "spences10--mcp-duckduckgo-search": {
      "owner": "spences10",
      "name": "mcp-duckduckgo-search",
      "url": "https://github.com/spences10/mcp-duckduckgo-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates DuckDuckGo search capabilities with large language models, providing comprehensive web search functionalities with various result types and filtering options.",
      "stars": 3,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T19:12:36Z",
      "readme_content": "# mcp-duckduckgo-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating DuckDuckGo\nsearch capabilities with LLMs. This server provides comprehensive web\nsearch functionality with support for various result types and\nfiltering options.\n\n<a href=\"https://glama.ai/mcp/servers/v99lwtriyk\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v99lwtriyk/badge\" />\n</a>\n\n## Features\n\n- 🔍 Comprehensive web search using DuckDuckGo's search engine\n- 📊 Rich result types including:\n  - Knowledge graph information\n  - Organic search results\n  - News articles\n  - Video content\n  - Image results\n  - Related searches\n- 🌍 Region-specific search support\n- 🛡️ Configurable safe search levels\n- 📅 Date-based filtering options\n- 📄 Pagination support\n- 💾 Built-in result caching\n- 🔒 Safe search options (off, moderate, strict)\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-duckduckgo-search\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-duckduckgo-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"SERPAPI_KEY\": \"your-serpapi-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-duckduckgo-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && SERPAPI_KEY=your-serpapi-api-key /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-duckduckgo-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `SERPAPI_KEY`: Your SerpAPI key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### ddg_search\n\nPerform web searches using the DuckDuckGo search engine.\n\nParameters:\n\n- `query` (string, required): Search query\n- `region` (string, optional): Region code (e.g., us-en, uk-en)\n  (default: us-en)\n- `safe_search` (string, optional): Safe search level (off, moderate,\n  strict) (default: moderate)\n- `date_filter` (string, optional): Filter results by date:\n  - 'd': past day\n  - 'w': past week\n  - 'm': past month\n  - 'y': past year\n  - Custom range: '2023-01-01..2023-12-31'\n- `start` (number, optional): Result offset for pagination\n- `no_cache` (boolean, optional): Bypass cache for fresh results\n  (default: false)\n\nResponse includes:\n\n- Knowledge graph data when available\n- Organic search results\n- News articles\n- Video content\n- Image results\n- Related searches\n- Search metadata\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm build\n```\n\n4. Run in development mode:\n\n```bash\npnpm dev\n```\n\n### Publishing\n\nThe project uses changesets for version management. To publish:\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [DuckDuckGo](https://duckduckgo.com) through\n  [SerpAPI](https://serpapi.com)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-duckduckgo-search",
      "npm_downloads": 879,
      "keywords": [
        "search",
        "duckduckgo",
        "filtering",
        "duckduckgo search",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "spences10--mcp-jinaai-grounding": {
      "owner": "spences10",
      "name": "mcp-jinaai-grounding",
      "url": "https://github.com/spences10/mcp-jinaai-grounding",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Grounding API with language models to provide real-time web content for enhancing responses. It offers efficient web content grounding capabilities optimized for generating factual information in LLM outputs.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:14:19Z",
      "readme_content": "# mcp-jinaai-grounding\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's\nGrounding API with LLMs. This server provides efficient and\ncomprehensive web content grounding capabilities, optimized for\nenhancing LLM responses with factual, real-time web content.\n\n<a href=\"https://glama.ai/mcp/servers/urkuhet67l\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/urkuhet67l/badge\" />\n</a>\n\n## Features\n\n- 🌐 Advanced web content grounding through Jina.ai Grounding API\n- 🚀 Real-time content verification and fact-checking\n- 📚 Comprehensive web content analysis\n- 🔄 Clean format optimized for LLMs\n- 🎯 Precise content relevance scoring\n- 🏗️ Built on the Model Context Protocol\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-grounding\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-grounding\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-grounding\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-grounding\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements MCP tools for grounding LLM responses with web\ncontent:\n\n### ground_content\n\nGround LLM responses with real-time web content using Jina.ai\nGrounding.\n\nParameters:\n\n- `query` (string, required): The text to ground with web content\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `format` (string, optional): Response format (\"json\" or \"text\").\n  Defaults to \"text\"\n- `token_budget` (number, optional): Maximum number of tokens for this\n  request\n- `browser_locale` (string, optional): Browser locale for rendering\n  content\n- `stream` (boolean, optional): Enable stream mode for large pages.\n  Defaults to false\n- `gather_links` (boolean, optional): Gather all links at the end of\n  response. Defaults to false\n- `gather_images` (boolean, optional): Gather all images at the end of\n  response. Defaults to false\n- `image_caption` (boolean, optional): Caption images in the content.\n  Defaults to false\n- `enable_iframe` (boolean, optional): Extract content from iframes.\n  Defaults to false\n- `enable_shadow_dom` (boolean, optional): Extract content from shadow\n  DOM. Defaults to false\n- `resolve_redirects` (boolean, optional): Follow redirect chains to\n  final URL. Defaults to true\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm run build\n```\n\n4. Run in development mode:\n\n```bash\npnpm run dev\n```\n\n### Publishing\n\n1. Update version in package.json\n2. Build the project:\n\n```bash\npnpm run build\n```\n\n3. Publish to npm:\n\n```bash\npnpm run release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Grounding API](https://jina.ai)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-jinaai-grounding",
      "npm_downloads": 4123,
      "keywords": [
        "grounding",
        "web",
        "api",
        "grounding api",
        "content grounding",
        "jinaai grounding"
      ],
      "category": "web-search"
    },
    "spences10--mcp-jinaai-search": {
      "owner": "spences10",
      "name": "mcp-jinaai-search",
      "url": "https://github.com/spences10/mcp-jinaai-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Search API with language models for efficient web search capabilities, optimized for retrieving clean and relevant content. Provides advanced search functionality for accessing comprehensive web information.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:15:03Z",
      "readme_content": "# mcp-jinaai-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's Search\nAPI with LLMs. This server provides efficient and comprehensive web\nsearch capabilities, optimised for retrieving clean, LLM-friendly\ncontent from the web.\n\n<a href=\"https://glama.ai/mcp/servers/u6603w196t\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u6603w196t/badge\" />\n</a>\n\n## Features\n\n- 🔍 Advanced web search through Jina.ai Search API\n- 🚀 Fast and efficient content retrieval\n- 📄 Clean text extraction with preserved structure\n- 🧠 Content optimised for LLMs\n- 🌐 Support for various content types including documentation\n- 🏗️ Built on the Model Context Protocol\n- 🔄 Configurable caching for performance\n- 🖼️ Optional image and link gathering\n- 🌍 Localisation support through browser locale\n- 🎯 Token budget control for response size\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-search\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### search\n\nSearch the web and get clean, LLM-friendly content using Jina.ai\nReader. Returns top 5 results with URLs and clean content.\n\nParameters:\n\n- `query` (string, required): Search query\n- `format` (string, optional): Response format (\"json\" or \"text\").\n  Defaults to \"text\"\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `token_budget` (number, optional): Maximum number of tokens for this\n  request\n- `browser_locale` (string, optional): Browser locale for rendering\n  content\n- `stream` (boolean, optional): Enable stream mode for large pages.\n  Defaults to false\n- `gather_links` (boolean, optional): Gather all links at the end of\n  response. Defaults to false\n- `gather_images` (boolean, optional): Gather all images at the end of\n  response. Defaults to false\n- `image_caption` (boolean, optional): Caption images in the content.\n  Defaults to false\n- `enable_iframe` (boolean, optional): Extract content from iframes.\n  Defaults to false\n- `enable_shadow_dom` (boolean, optional): Extract content from shadow\n  DOM. Defaults to false\n- `resolve_redirects` (boolean, optional): Follow redirect chains to\n  final URL. Defaults to true\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm run build\n```\n\n4. Run in development mode:\n\n```bash\npnpm run dev\n```\n\n### Publishing\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm version\n```\n\n3. Build and publish:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Search API](https://jina.ai)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-jinaai-search",
      "npm_downloads": 4113,
      "keywords": [
        "search",
        "jinaai",
        "jina",
        "jinaai search",
        "web search",
        "search api"
      ],
      "category": "web-search"
    },
    "spences10--mcp-perplexity-search": {
      "owner": "spences10",
      "name": "mcp-perplexity-search",
      "url": "https://github.com/spences10/mcp-perplexity-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Perplexity's AI API with large language models for enhanced chat completion capabilities, featuring specialized prompt templates for a variety of use cases.",
      "stars": 8,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:13:02Z",
      "readme_content": "# mcp-perplexity-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Perplexity's AI\nAPI with LLMs. This server provides advanced chat completion\ncapabilities with specialized prompt templates for various use cases.\n\n<a href=\"https://glama.ai/mcp/servers/zlqdizpsr9\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/zlqdizpsr9/badge\" />\n</a>\n\n## Features\n\n- 🤖 Advanced chat completion using Perplexity's AI models\n- 📝 Predefined prompt templates for common scenarios:\n  - Technical documentation generation\n  - Security best practices analysis\n  - Code review and improvements\n  - API documentation in structured format\n- 🎯 Custom template support for specialized use cases\n- 📊 Multiple output formats (text, markdown, JSON)\n- 🔍 Optional source URL inclusion in responses\n- ⚙️ Configurable model parameters (temperature, max tokens)\n- 🚀 Support for various Perplexity models including Sonar and LLaMA\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-perplexity-search\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-perplexity-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"PERPLEXITY_API_KEY\": \"your-perplexity-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-perplexity-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && PERPLEXITY_API_KEY=your-perplexity-api-key /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-perplexity-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `PERPLEXITY_API_KEY`: Your Perplexity API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### chat_completion\n\nGenerate chat completions using the Perplexity API with support for\nspecialized prompt templates.\n\nParameters:\n\n- `messages` (array, required): Array of message objects with:\n  - `role` (string): 'system', 'user', or 'assistant'\n  - `content` (string): The message content\n- `prompt_template` (string, optional): Predefined template to use:\n  - `technical_docs`: Technical documentation with code examples\n  - `security_practices`: Security implementation guidelines\n  - `code_review`: Code analysis and improvements\n  - `api_docs`: API documentation in JSON format\n- `custom_template` (object, optional): Custom prompt template with:\n  - `system` (string): System message for assistant behaviour\n  - `format` (string): Output format preference\n  - `include_sources` (boolean): Whether to include sources\n- `format` (string, optional): 'text', 'markdown', or 'json' (default:\n  'text')\n- `include_sources` (boolean, optional): Include source URLs (default:\n  false)\n- `model` (string, optional): Perplexity model to use (default:\n  'sonar')\n- `temperature` (number, optional): Output randomness (0-1, default:\n  0.7)\n- `max_tokens` (number, optional): Maximum response length\n  (default: 1024)\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm build\n```\n\n4. Run in development mode:\n\n```bash\npnpm dev\n```\n\n### Publishing\n\nThe project uses changesets for version management. To publish:\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by\n  [Perplexity SONAR](https://docs.perplexity.ai/api-reference/chat-completions)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-perplexity-search",
      "npm_downloads": 5987,
      "keywords": [
        "search",
        "completion",
        "ai",
        "chat completion",
        "perplexity ai",
        "perplexity search"
      ],
      "category": "web-search"
    },
    "spragginsdesigns--perplexity-mcp-server": {
      "owner": "spragginsdesigns",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/spragginsdesigns/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/spragginsdesigns.webp",
      "description": "Enables web searching capabilities by utilizing the Perplexity AI API to fetch and present web search results. It serves as an interface for retrieving information from the internet based on user queries.",
      "stars": 3,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-20T01:37:15Z",
      "readme_content": "# Perplexity MCP Server\n\nThis is a simple MCP server that allows you to search the web using Perplexity AI.\n\n## \n\nMCP Review Certified: https://mcpreview.com/mcp-servers/spragginsdesigns/perplexity-mcp-server\n\n## Installation\n\n### Prerequisites\n- Node.js 18+ (Download from [nodejs.org](https://nodejs.org/))\n- Git (Download from [git-scm.com](https://git-scm.com/download/win))\n- A Perplexity AI API key\n\n### Steps for Windows\n\n1. Clone the repository:\n```bash\ngit clone [repository-url]\ncd perplexity-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create environment file:\n   - Create a new file named `.env` in the root directory\n   - Add your Perplexity AI API key:\n   ```env\n   PERPLEXITY_API_KEY=your_api_key_here\n   ```\n\n## Usage on Windows\n\n### Development Mode\n```bash\nnpm run dev\n```\n\n### Production Mode\n1. Build the project:\n```bash\nnpm run build\n```\n\n2. Start the server:\n```bash\nnpm start\n```\n\n### Using the Windows Batch File (Recommended)\nTo avoid environment variable issues on Windows, use the included batch file:\n\n1. Build the project first:\n```bash\nnpm run build\n```\n\n2. Run the batch file:\n```bash\n.\\run-perplexity-server.bat\n```\n\nAlternatively, double-click the `run-perplexity-server.bat` file in Windows Explorer.\n\nFor detailed instructions, see the [Windows Setup Guide](./docs/windows-setup.md).\n\n## Troubleshooting Windows Issues\n\n### Common Issues\n\n1. **Permission Errors**\n   - Run Command Prompt or PowerShell as Administrator\n   - Check file permissions in the project directory\n\n2. **Environment Variables**\n   - Ensure `.env` file is in the root directory\n   - No spaces around the `=` sign in `.env` file\n   - Restart terminal after making changes to environment variables\n   - Use the provided batch file to avoid environment variable issues\n\n3. **Node.js Issues**\n   - Verify Node.js installation: `node --version`\n   - Ensure npm is installed: `npm --version`\n\n### Error Messages\n\nIf you see `Error: ENOENT: no such file or directory`, ensure:\n- All paths use correct Windows-style separators\n- You're in the correct directory\n- Required files exist\n\n## Project Structure\n```\nperplexity-mcp-server/\n├── src/\n│   └── index.ts          # Main server implementation\n├── dist/                 # Compiled output\n├── .env                  # Environment variables\n├── package.json          # Project configuration\n├── run-perplexity-server.bat # Windows batch file\n└── tsconfig.json         # TypeScript configuration\n```\n\n## API Usage\n\nThe server provides a single tool `perplexity_search` for web searches:\n\n```json\n{\n  \"name\": \"perplexity_search\",\n  \"arguments\": {\n    \"query\": \"your search query here\"\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "perplexity",
        "searching",
        "search",
        "utilizing perplexity",
        "search spragginsdesigns",
        "web searching"
      ],
      "category": "web-search"
    },
    "stickerdaniel--linkedin-mcp-server": {
      "owner": "stickerdaniel",
      "name": "linkedin-mcp-server",
      "url": "https://github.com/stickerdaniel/linkedin-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/stickerdaniel.webp",
      "description": "Interact with LinkedIn to scrape profiles, analyze companies, and search for job listings. Provides insights and recommendations based on LinkedIn data.",
      "stars": 436,
      "forks": 76,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T04:13:07Z",
      "readme_content": "# LinkedIn MCP Server\n\n<p align=\"left\">\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/ci.yml\" target=\"_blank\"><img src=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/ci.yml/badge.svg?branch=main\" alt=\"CI Status\"></a>\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/release.yml\" target=\"_blank\"><img src=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/release.yml/badge.svg?branch=main\" alt=\"Release\"></a>\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/blob/main/LICENSE\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-brightgreen?labelColor=32383f\" alt=\"License\"></a>\n</p>\n\nThrough this LinkedIn MCP server, AI assistants like Claude can connect to your LinkedIn. Give access to profiles and companies, get your recommended jobs, or search for keywords. All from a Docker container on your machine.\n\n## Installation Methods\n\n[![Docker](https://img.shields.io/badge/Docker-Universal_MCP-008fe2?style=for-the-badge&logo=docker&logoColor=008fe2)](#-docker-setup-recommended---universal)\n[![Install DXT Extension](https://img.shields.io/badge/Claude_Desktop_DXT-d97757?style=for-the-badge&logo=anthropic)](#-claude-desktop-dxt-extension)\n[![uvx](https://img.shields.io/badge/uvx-Quick_Install-de5fe9?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDEiIGhlaWdodD0iNDEiIHZpZXdCb3g9IjAgMCA0MSA0MSIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTS01LjI4NjE5ZS0wNiAwLjE2ODYyOUwwLjA4NDMwOTggMjAuMTY4NUwwLjE1MTc2MiAzNi4xNjgzQzAuMTYxMDc1IDM4LjM3NzQgMS45NTk0NyA0MC4xNjA3IDQuMTY4NTkgNDAuMTUxNEwyMC4xNjg0IDQwLjA4NEwzMC4xNjg0IDQwLjA0MThMMzEuMTg1MiA0MC4wMzc1QzMzLjM4NzcgNDAuMDI4MiAzNS4xNjgzIDM4LjIwMjYgMzUuMTY4MyAzNlYzNkwzNy4wMDAzIDM2TDM3LjAwMDMgMzkuOTk5Mkw0MC4xNjgzIDM5Ljk5OTZMMzkuOTk5NiAtOS45NDY1M2UtMDdMMjEuNTk5OCAwLjA3NzU2ODlMMjEuNjc3NCAxNi4wMTg1TDIxLjY3NzQgMjUuOTk5OEwyMC4wNzc0IDI1Ljk5OThMMTguMzk5OCAyNS45OTk4TDE4LjQ3NzQgMTYuMDMyTDE4LjM5OTggMC4wOTEwNTkzTC01LjI4NjE5ZS0wNiAwLjE2ODYyOVoiIGZpbGw9IiNERTVGRTkiLz4KPC9zdmc+Cg==)](#-uvx-setup-quick-install---universal)\n[![Development](https://img.shields.io/badge/Development-Local-ffdc53?style=for-the-badge&logo=python&logoColor=ffdc53)](#-local-setup-develop--contribute)\n\nhttps://github.com/user-attachments/assets/eb84419a-6eaf-47bd-ac52-37bc59c83680\n\n## Usage Examples\n```\nWhat are my recommended jobs I can apply to?\n```\n```\nResearch the background of this candidate https://www.linkedin.com/in/stickerdaniel/\n```\n```\nGet this company profile for partnership discussions https://www.linkedin.com/company/inframs/\n```\n```\nSuggest improvements for my CV to target this job posting https://www.linkedin.com/jobs/view/4252026496\n```\n\n## Features & Tool Status\n> [!TIP]\n> - **Profile Scraping** (`get_person_profile`): Get detailed information from a LinkedIn profile including work history, education, skills, and connections\n> - **Company Analysis** (`get_company_profile`): Extract comprehensive company information from a LinkedIn company profile name\n> - **Job Details** (`get_job_details`): Retrieve specific job posting details using LinkedIn job IDs\n> - **Job Search** (`search_jobs`): Search for jobs with filters like keywords and location\n> - **Recommended Jobs** (`get_recommended_jobs`): Get personalized job recommendations based on your profile\n> - **Session Management** (`close_session`): Properly close browser session and clean up resources\n\n> [!NOTE]\n> July 2025: All tools are currently functional and actively maintained. If you encounter any issues, please report them in the [GitHub issues](https://github.com/stickerdaniel/linkedin-mcp-server/issues).\n\n<br/>\n<br/>\n\n## 🐳 Docker Setup (Recommended - Universal)\n\n**Prerequisites:** Make sure you have [Docker](https://www.docker.com/get-started/) installed and running.\n\n### Installation\n\n**Client Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"-e\", \"LINKEDIN_COOKIE\",\n        \"stickerdaniel/linkedin-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"LINKEDIN_COOKIE\": \"li_at=YOUR_COOKIE_VALUE\"\n      }\n    }\n  }\n}\n```\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n<details>\n<summary><b>🐳 Docker get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\ndocker run -it --rm \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### Docker Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**Transport Modes:**\n- **Default (stdio)**: Standard communication for local MCP servers\n- **Streamable HTTP**: For a web-based MCP server\n\n**CLI Options:**\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--get-cookie` - Attempt to login with email and password and extract the LinkedIn cookie\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\ndocker run -it --rm \\\n  -e LINKEDIN_COOKIE=\"li_at=YOUR_COOKIE_VALUE\" \\\n  -p 8080:8080 \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --transport streamable-http --host 0.0.0.0 --port 8080 --path /mcp\n```\n\n**Test with mcp inspector:**\n1. Install and run mcp inspector ```bunx @modelcontextprotocol/inspector```\n2. Click pre-filled token url to open the inspector in your browser\n3. Select `Streamable HTTP` as `Transport Type`\n4. Set `URL` to `http://localhost:8080/mcp`\n5. Connect\n6. Test tools\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Docker issues:**\n- Make sure [Docker](https://www.docker.com/get-started/) is installed\n- Check if Docker is running: `docker ps`\n\n**Login issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error.\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period of time, then try again later or follow the [local setup instructions](#-local-setup-develop--contribute) to run the server manually in --no-headless mode where you can debug the login process (solve captcha manually)\n</details>\n\n<br/>\n<br/>\n\n## 📦 Claude Desktop (DXT Extension)\n\n**Prerequisites:** [Claude Desktop](https://claude.ai/download) and [Docker](https://www.docker.com/get-started/) installed\n\n**One-click installation** for Claude Desktop users:\n1. Download the [DXT extension](https://github.com/stickerdaniel/linkedin-mcp-server/releases/latest)\n2. Double-click to install into Claude Desktop\n3. Set your LinkedIn cookie in the extension settings\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n<details>\n<summary><b>🐳 Docker get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\ndocker run -it --rm \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### DXT Extension Setup Help\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Docker issues:**\n- Make sure [Docker](https://www.docker.com/get-started/) is installed\n- Check if Docker is running: `docker ps`\n\n**Login issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error.\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period of time, then try again later or follow the [local setup instructions](#-local-setup-develop--contribute) to run the server manually in --no-headless mode where you can debug the login process (solve captcha manually)\n</details>\n\n<br/>\n<br/>\n\n## 🚀 uvx Setup (Quick Install - Universal)\n\n**Prerequisites:** Make sure you have [uv](https://docs.astral.sh/uv/) installed.\n\n### Installation\n\nRun directly from GitHub without cloning:\n\n```bash\n# Run directly from GitHub (latest version)\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --help\n\n# Run with your LinkedIn cookie\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --cookie \"li_at=YOUR_COOKIE_VALUE\"\n```\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n\n<details>\n<summary><b>🚀 uvx get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server \\\n  linkedin-mcp-server --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### uvx Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**Client Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/stickerdaniel/linkedin-mcp-server\",\n        \"linkedin-mcp-server\"\n      ],\n      \"env\": {\n        \"LINKEDIN_COOKIE\": \"li_at=YOUR_COOKIE_VALUE\"\n      }\n    }\n  }\n}\n```\n\n**Transport Modes:**\n- **Default (stdio)**: Standard communication for local MCP servers\n- **Streamable HTTP**: For web-based MCP server\n\n**CLI Options:**\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--get-cookie` - Attempt to login with email and password and extract the LinkedIn cookie\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n\n**Basic Usage Examples:**\n```bash\n# Run with cookie from environment variable\nLINKEDIN_COOKIE=\"YOUR_COOKIE_VALUE\" uvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server\n\n# Run with cookie via flag\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --cookie \"YOUR_COOKIE_VALUE\"\n\n# Run with debug logging\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --log-level DEBUG\n\n# Extract cookie with credentials\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --get-cookie\n```\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server \\\n  --transport streamable-http --host 127.0.0.1 --port 8080 --path /mcp\n```\n\n**Test with mcp inspector:**\n1. Install and run mcp inspector ```bunx @modelcontextprotocol/inspector```\n2. Click pre-filled token url to open the inspector in your browser\n3. Select `Streamable HTTP` as `Transport Type`\n4. Set `URL` to `http://localhost:8080/mcp`\n5. Connect\n6. Test tools\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Installation issues:**\n- Ensure you have uv installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n- Check uv version: `uv --version` (should be 0.4.0 or higher)\n\n**Cookie issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Cookie can be passed via `--cookie` flag or `LINKEDIN_COOKIE` environment variable\n- Make sure you have only one active LinkedIn session per cookie at a time\n\n**Login issues:**\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period\n</details>\n\n<br/>\n<br/>\n\n## 🐍 Local Setup (Develop & Contribute)\n\n**Prerequisites:** [Chrome browser](https://www.google.com/chrome/) and [Git](https://git-scm.com/downloads) installed\n\n**ChromeDriver Setup:**\n1. **Check Chrome version**: Chrome → menu (⋮) → Help → About Google Chrome\n2. **Download matching ChromeDriver**: [Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)\n3. **Make it accessible**:\n   - Place ChromeDriver in PATH (`/usr/local/bin` on macOS/Linux)\n   - Or set: `export CHROMEDRIVER_PATH=/path/to/chromedriver`\n   - if no CHROMEDRIVER_PATH is set, the server will try to find it automatically by checking common locations\n\n### Installation\n\n```bash\n# 1. Clone repository\ngit clone https://github.com/stickerdaniel/linkedin-mcp-server\ncd linkedin-mcp-server\n\n# 2. Install UV package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv python # install python if you don't have it\n\n# 3. Install dependencies and dev dependencies\nuv sync\nuv sync --group dev\n\n# 4. Install pre-commit hooks\nuv run pre-commit install\n\n# 5. Start the server once manually\n# You will be prompted to enter your LinkedIn credentials, and they will be securely stored in your OS keychain\n# Once logged in, your cookie will be stored in your OS keychain and used for subsequent runs until it expires\nuv run -m linkedin_mcp_server --no-headless --no-lazy-init\n```\n\n### Local Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**CLI Options:**\n- `--no-headless` - Show browser window (debugging)\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--get-cookie` - Login with email and password and extract the LinkedIn cookie\n- `--clear-keychain` - Clear all stored LinkedIn credentials and cookies from system keychain\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--help` - Show help\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\nuv run -m linkedin_mcp_server --transport streamable-http --host 127.0.0.1 --port 8000 --path /mcp\n```\n\n**Claude Desktop:**\n```**json**\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/linkedin-mcp-server\", \"run\", \"-m\", \"linkedin_mcp_server\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Login/Scraping issues:**\n- Use `--no-headless` to see browser actions (captcha challenge, LinkedIn mobile app 2fa, ...)\n- Add `--no-lazy-init` to attempt to login to LinkedIn immediately instead of waiting for the first tool call\n- Add `--log-level DEBUG` to see more detailed logging\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error. E.g. if you have a logged in browser session with a docker container, you can't use the same cookie to login with the local setup while the docker container is running / session is not closed.\n\n**ChromeDriver issues:**\n- Ensure Chrome and ChromeDriver versions match\n- Check ChromeDriver is in PATH or set `CHROMEDRIVER_PATH` in your env\n\n**Python issues:**\n- Check Python version: `uv python --version` (should be 3.12+)\n- Reinstall dependencies: `uv sync --reinstall`\n\n</details>\n\nFeel free to open an [issue](https://github.com/stickerdaniel/linkedin-mcp-server/issues) or [PR](https://github.com/stickerdaniel/linkedin-mcp-server/pulls)!\n\n\n<br/>\n<br/>\n\n\n## Acknowledgements\nBuilt with [LinkedIn Scraper](https://github.com/joeyism/linkedin_scraper) by [@joeyism](https://github.com/joeyism) and [FastMCP](https://gofastmcp.com/).\n\n⚠️ Use in accordance with [LinkedIn's Terms of Service](https://www.linkedin.com/legal/user-agreement). Web scraping may violate LinkedIn's terms. This tool is for personal use only.\n\n## Star History\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date&theme=dark\" />\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date\" />\n  <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date\" />\n</picture>\n\n\n## License\n\nThis project is licensed under the Apache 2.0 license.\n\n<br>\n",
      "npm_url": "https://www.npmjs.com/package/linkedin-mcp-server",
      "npm_downloads": 543,
      "keywords": [
        "linkedin",
        "web",
        "listings",
        "linkedin mcp",
        "based linkedin",
        "linkedin scrape"
      ],
      "category": "web-search"
    },
    "surya-madhav--MCP": {
      "owner": "surya-madhav",
      "name": "MCP",
      "url": "https://github.com/surya-madhav/MCP",
      "imageUrl": "/freedevtools/mcp/pfp/surya-madhav.webp",
      "description": "Interact with web content through standardized tools to fetch and summarize information from any website.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-19T09:09:35Z",
      "readme_content": "# MCP Web Tools Server\n\nA Model Context Protocol (MCP) server that provides tools for web-related operations. This server allows LLMs to interact with web content through standardized tools.\n\n## Current Tools\n\n- **web_scrape**: Converts a URL to use r.jina.ai as a prefix and returns the markdown content\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd MCP\n   ```\n\n2. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n   Alternatively, you can use [uv](https://github.com/astral-sh/uv) for faster installation:\n   ```bash\n   uv pip install -r requirements.txt\n   ```\n\n## Running the Server and UI\n\nThis repository includes convenient scripts to run either the MCP server or the Streamlit UI.\n\n### Using the Run Scripts\n\nOn macOS/Linux:\n```bash\n# Run the server with stdio transport (default)\n./run.sh server\n\n# Run the server with SSE transport\n./run.sh server --transport sse --host localhost --port 5000\n\n# Run the Streamlit UI\n./run.sh ui\n```\n\nOn Windows:\n```cmd\n# Run the server with stdio transport (default)\nrun.bat server\n\n# Run the server with SSE transport\nrun.bat server --transport sse --host localhost --port 5000\n\n# Run the Streamlit UI\nrun.bat ui\n```\n\n### Running Manually\n\nAlternatively, you can run the server directly:\n\n#### Using stdio (default)\n\n```bash\npython server.py\n```\n\n#### Using SSE\n\n```bash\npython server.py --transport sse --host localhost --port 5000\n```\n\nThis will start an HTTP server on `localhost:5000` that accepts MCP connections.\n\nAnd to run the Streamlit UI manually:\n\n```bash\nstreamlit run streamlit_app.py\n```\n\n## Testing with MCP Inspector\n\nThe MCP Inspector is a tool for testing and debugging MCP servers. You can use it to interact with your server:\n\n1. Install the MCP Inspector:\n   ```bash\n   npm install -g @modelcontextprotocol/inspector\n   ```\n\n2. Run the Inspector with your server:\n   ```bash\n   npx @modelcontextprotocol/inspector python server.py\n   ```\n\n3. Use the Inspector interface to test the `web_scrape` tool by providing a URL like `example.com` and viewing the returned markdown content.\n\n## Integrating with Claude for Desktop\n\nTo use this server with Claude for Desktop:\n\n1. Make sure you have [Claude for Desktop](https://claude.ai/download) installed.\n\n2. Open the Claude for Desktop configuration file:\n   - Mac: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration (adjust the path as needed):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-tools\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/absolute/path/to/MCP/server.py\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop.\n\n5. You should now see the web_scrape tool available in Claude's interface. You can ask Claude to fetch content from a website, and it will use the tool.\n\n## Example Usage\n\nOnce integrated with Claude, you can ask questions like:\n\n- \"What's on the homepage of example.com?\"\n- \"Can you fetch and summarize the content from mozilla.org?\"\n- \"Get the content from wikipedia.org/wiki/Model_Context_Protocol and explain it to me.\"\n\nClaude will use the web_scrape tool to fetch the content and provide it in its response.\n\n## Adding More Tools\n\nTo add more tools to this server:\n\n1. Create a new Python file in the `tools/` directory, e.g., `tools/new_tool.py`.\n\n2. Implement your tool function, following a similar pattern to the existing tools.\n\n3. Import your tool in `server.py` and register it with the MCP server:\n\n```python\n# Import your new tool\nfrom tools.new_tool import new_tool_function\n\n# Register the tool with the MCP server\n@mcp.tool()\nasync def new_tool(param1: str, param2: int) -> str:\n    \"\"\"\n    Description of what your tool does.\n    \n    Args:\n        param1: Description of param1\n        param2: Description of param2\n        \n    Returns:\n        Description of return value\n    \"\"\"\n    return await new_tool_function(param1, param2)\n```\n\n4. Restart the server to apply the changes.\n\n## Streamlit UI\n\nThis repository includes a Streamlit application that allows you to connect to and test all your MCP servers configured in Claude for Desktop.\n\n### Running the Streamlit UI\n\n```bash\nstreamlit run streamlit_app.py\n```\n\nThis will start the Streamlit server and open a web browser with the UI.\n\n### Features\n\n- Load and parse your Claude for Desktop configuration file\n- View all configured MCP servers\n- Connect to any server and view its available tools\n- Test tools by providing input parameters and viewing results\n- See available resources and prompts\n\n### Usage\n\n1. Start the Streamlit app\n2. Enter the path to your Claude for Desktop configuration file (default path is pre-filled)\n3. Click \"Load Servers\" to see all available MCP servers\n4. Select a server tab and click \"Connect\" to load its tools\n5. Select a tool and provide the required parameters\n6. Click \"Execute\" to run the tool and see the results\n\n## Troubleshooting\n\n- **Missing dependencies**: Make sure all dependencies in `requirements.txt` are installed.\n- **Connection issues**: Check that the server is running and the configuration in Claude for Desktop points to the correct path.\n- **Tool execution errors**: Look for error messages in the server output.\n- **Streamlit UI issues**: Make sure Streamlit is properly installed and the configuration file path is correct.\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "web",
        "surya",
        "search",
        "web search",
        "web content",
        "search surya"
      ],
      "category": "web-search"
    },
    "suthio--brave-deep-research-mcp": {
      "owner": "suthio",
      "name": "brave-deep-research-mcp",
      "url": "https://github.com/suthio/brave-deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/suthio.webp",
      "description": "Connects to Brave Search for web searches and uses Puppeteer for extracting full content from webpages, enabling in-depth research beyond basic search results. Retrieves detailed insights and processes linked information for comprehensive analysis.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-05T13:31:15Z",
      "readme_content": "# @suthio/brave-deep-research-mcp\n\nA Model Context Protocol (MCP) server that combines Brave Search with Puppeteer-powered content extraction for deep research capabilities. This server allows AI assistants to perform comprehensive web searches by not only retrieving search results but also visiting the pages to extract full content and explore linked pages.\n\n## Comparison with Standard Brave Search MCP Server\n\n### Standard Brave Search MCP Server:\n- **Search Capability**: Uses the Brave Search API to perform basic web searches\n- **Data Retrieval**: Returns only the search results (title, URL, and snippet) provided by the API\n- **Content Depth**: No access to full webpage content beyond the search snippets\n- **Page Exploration**: No ability to visit pages or follow links\n- **Information Scope**: Limited to the brief information available in search results\n- **Content Processing**: No content extraction or cleaning capabilities\n- **Customization**: Limited to basic search parameters (query, count, offset)\n- **Use Case**: Best for quick searches where only an overview is needed\n\n### Brave Deep Research MCP Server (this project):\n- **Search Capability**: Uses Brave Search API for initial results, then enhances with web scraping\n- **Data Retrieval**: Extracts complete page content from each search result\n- **Content Depth**: Provides full webpage content with main text extraction\n- **Page Exploration**: Can traverse links to explore related content at configurable depths\n- **Information Scope**: Accesses comprehensive information across multiple related pages\n- **Content Processing**: Intelligently identifies and extracts main content, filtering out navigation, ads, footers, etc.\n- **Customization**: Configurable depth of exploration, result count, headless mode, and timeouts\n- **Use Case**: Ideal for in-depth research requiring detailed information and context\n\n### Practical Differences in an Example Query\n\nFor a query like \"climate change mitigation technologies\":\n\n**Standard Brave Search MCP**:\n```\nTitle: \"Latest Climate Change Mitigation Technologies - Example Site\"\nURL: \"https://example.com/climate-tech\"\nSnippet: \"Various technologies are being developed to mitigate climate change, including carbon capture...\"\n```\n(Limited to just these search result snippets)\n\n**Brave Deep Research MCP**:\n```\n# Latest Climate Change Mitigation Technologies - Example Site\nURL: https://example.com/climate-tech\n\n## Content\nCarbon capture and storage (CCS) technology has advanced significantly in recent years. The latest direct air capture facilities can now remove CO2 at a cost of $250 per ton, down from $600 just five years ago. Implementation challenges remain, including...\n\n[Followed by several pages of detailed content from the original page and linked pages]\n```\n\n## Features\n\n- **Deep Search**: Go beyond search results to extract complete page content\n- **Configurable Depth**: Specify how many levels of links to follow from initial results\n- **Content Extraction**: Intelligently identify and extract main content from pages\n- **Metadata Extraction**: Get titles, descriptions, and structured content\n- **Debug Mode**: Configurable logging for troubleshooting\n- **Headless Mode Toggle**: Run browser in visible or headless mode\n\n## Installation\n\n```bash\n# Install from npm\nnpm install -g @suthio/brave-deep-research-mcp\n\n# Or clone the repository\ngit clone https://github.com/suthio/brave-deep-research-mcp.git\ncd brave-deep-research-mcp\nnpm install\nnpm run build\n```\n\n## Configuration\n\nCreate a `.env` file based on the provided `.env.example`:\n\n```bash\n# Copy the example env file\ncp .env.example .env\n\n# Edit the file to add your Brave API key and other settings\nnano .env\n```\n\n### Environment Variables\n\n- `BRAVE_API_KEY`: Your Brave Search API key (required)\n- `PUPPETEER_HEADLESS`: Whether to run Puppeteer in headless mode (default: true)\n- `PAGE_TIMEOUT`: Timeout for page loading in milliseconds (default: 30000)\n- `DEBUG_MODE`: Enable detailed debug logging (default: false)\n\n## Usage\n\n### Running from command line\n\n```bash\n# If installed globally via npm\nbrave-deep-research-mcp\n\n# Or run directly from the package\nnpx @suthio/brave-deep-research-mcp\n\n# Or run locally after cloning\nnpm start\n```\n\n### Using with Claude for Desktop\n\nTo use this server with Claude for Desktop:\n\n1. Install the package:\n```bash\nnpm install -g @suthio/brave-deep-research-mcp\n```\n\n2. Edit the Claude for Desktop configuration file:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following to the `mcpServers` section:\n```json\n{\n  \"mcpServers\": {\n    \"brave-deep-research\": {\n      \"command\": \"npx\",\n      \"args\": [\"@suthio/brave-deep-research-mcp\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"your_brave_api_key_here\",\n        \"PUPPETEER_HEADLESS\": \"true\"\n      }\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n5. You can now use the deep-search tool in your conversations\n\n### Example Queries\n\n- \"Use deep-search to research the latest developments in quantum computing\"\n- \"Perform a deep search on climate change mitigation strategies with depth 2\"\n- \"Deep search for information about sustainable architecture, with 5 results\"\n\n## Tool Parameters\n\nThe `deep-search` tool accepts the following parameters:\n\n- `query` (required): The search query\n- `results` (optional): Number of search results to process (default: 3, max: 10)\n- `depth` (optional): Depth of link traversal for each result (default: 1, max: 3)\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/suthio/brave-deep-research-mcp.git\ncd brave-deep-research-mcp\n\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build the project\nnpm run build\n```\n\n## How It Works\n\n1. The tool first performs a search using the Brave Search API to get initial results\n2. For each search result, it launches a Puppeteer browser to visit the page\n3. It extracts the main content, metadata, and links from each page\n4. If depth > 1, it follows links on the page and repeats the process\n5. All extracted content is formatted and returned to the AI assistant\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/@suthio/brave-deep-research-mcp",
      "npm_downloads": 1662,
      "keywords": [
        "puppeteer",
        "searches",
        "webpages",
        "brave search",
        "uses puppeteer",
        "search web"
      ],
      "category": "web-search"
    },
    "tahaswx--mcp-server-serper": {
      "owner": "tahaswx",
      "name": "mcp-server-serper",
      "url": "https://github.com/tahaswx/mcp-server-serper",
      "imageUrl": "/freedevtools/mcp/pfp/tahaswx.webp",
      "description": "Provides web search functionality and webpage content extraction using the Serper API, enabling real-time data retrieval for applications.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-17T21:57:42Z",
      "readme_content": "# Serper Search and Scrape MCP Server\n[![smithery badge](https://smithery.ai/badge/@marcopesani/mcp-server-serper)](https://smithery.ai/server/@marcopesani/mcp-server-serper)\n\nA TypeScript-based MCP server that provides web search and webpage scraping capabilities using the Serper API. This server integrates with Claude Desktop to enable powerful web search and content extraction features.\n\n<a href=\"https://glama.ai/mcp/servers/5zk327i0pj\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5zk327i0pj/badge\" alt=\"serper-search-scrape-mcp-server MCP server\" />\n</a>\n\n## Features\n\n### Tools\n\n- `google_search` - Perform web searches via Serper API\n  - Rich search results including organic results, knowledge graph, \"people also ask\", and related searches\n  - Supports region and language targeting\n  - Optional parameters for location, pagination, time filters, and autocorrection\n  - Supports advanced search operators:\n    - `site`: Limit results to specific domain\n    - `filetype`: Limit to specific file types (e.g., 'pdf', 'doc')\n    - `inurl`: Search for pages with word in URL\n    - `intitle`: Search for pages with word in title\n    - `related`: Find similar websites\n    - `cache`: View Google's cached version of a specific URL\n    - `before`: Date before in YYYY-MM-DD format\n    - `after`: Date after in YYYY-MM-DD format\n    - `exact`: Exact phrase match\n    - `exclude`: Terms to exclude from search results\n    - `or`: Alternative terms (OR operator)\n  \n- `scrape` - Extract content from web pages\n  - Get plain text and optional markdown content\n  - Includes JSON-LD and head metadata\n  - Preserves document structure\n\n## Requirements\n\n- Node.js >= 18\n- Serper API key (set as `SERPER_API_KEY` environment variable)\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\nRun tests:\n```bash\nnpm test                  # Run all tests\nnpm run test:watch       # Run tests in watch mode\nnpm run test:coverage    # Run tests with coverage\nnpm run test:integration # Run integration tests\n```\n\n### Environment Variables\n\nCreate a `.env` file in the root directory:\n\n```\nSERPER_API_KEY=your_api_key_here\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Serper Search and Scrape for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@marcopesani/mcp-server-serper):\n\n```bash\nnpx -y @smithery/cli install @marcopesani/mcp-server-serper --client claude\n```\n\n### Claude Desktop\n\nAdd the server config at:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Cline\n\n1. Open the Cline extension settings\n2. Open \"MCP Servers\" tab\n3. Click on \"Configure MCP Servers\"\n4. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/marcopesani/mcp-server-serper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\"google_search\", \"scrape\"]\n    }\n  }\n}\n```\n\nAdditional Cline configuration options:\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of tools that don't require explicit approval for each use\n\n### Cursor\n\n1. Open the Cursor settings\n2. Open \"Features\" settings\n3. In the \"MCP Servers\" section, click on \"Add new MCP Server\"\n4. Choose a name, and select \"command\" as \"Type\"\n5. In the \"Command\" field, enter the following:\n\n```\nenv SERPER_API_KEY=your_api_key_here npx -y serper-search-scrape-mcp-server\n```\n\n### Docker\n\nYou can also run the server using Docker. First, build the image:\n\n```bash\ndocker build -t mcp-server-serper .\n```\n\nThen run the container with your Serper API key:\n\n```bash\ndocker run -e SERPER_API_KEY=your_api_key_here mcp-server-serper\n```\n\nAlternatively, if you have your environment variables in a `.env` file:\n\n```bash\ndocker run --env-file .env mcp-server-serper\n```\n\nFor development, you might want to mount your source code as a volume:\n\n```bash\ndocker run -v $(pwd):/app --env-file .env mcp-server-serper\n```\n\nNote: Make sure to replace `your_api_key_here` with your actual Serper API key.",
      "npm_url": "https://www.npmjs.com/package/mcp-server-serper",
      "npm_downloads": 487,
      "keywords": [
        "serper",
        "retrieval",
        "webpage",
        "using serper",
        "server serper",
        "serper api"
      ],
      "category": "web-search"
    },
    "tanevanwifferen--usescraper-mcp-server": {
      "owner": "tanevanwifferen",
      "name": "usescraper-mcp-server",
      "url": "https://github.com/tanevanwifferen/usescraper-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tanevanwifferen.webp",
      "description": "Provides web scraping capabilities using the UseScraper API to extract content from web pages in various formats.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-21T18:43:04Z",
      "readme_content": "# UseScraper MCP Server\n\n[![smithery badge](https://smithery.ai/badge/usescraper-server)](https://smithery.ai/server/usescraper-server)\nThis is a TypeScript-based MCP server that provides web scraping capabilities using the UseScraper API. It exposes a single tool 'scrape' that can extract content from web pages in various formats.\n\n<a href=\"https://glama.ai/mcp/servers/oqq8he02cy\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/oqq8he02cy/badge\" alt=\"UseScraper Server MCP server\" /></a>\n\n\n## Features\n\n### Tools\n- `scrape` - Extract content from a webpage\n  - **Parameters**:\n    - `url` (required): The URL of the webpage to scrape\n    - `format` (optional): The format to save the content (text, html, markdown). Default: markdown\n    - `advanced_proxy` (optional): Use advanced proxy to circumvent bot detection. Default: false\n    - `extract_object` (optional): Object specifying data to extract\n\n## Installation\n\n### Installing via Smithery\n\nTo install UseScraper for Claude Desktop automatically via [Smithery](https://smithery.ai/server/usescraper-server):\n\n```bash\nnpx -y @smithery/cli install usescraper-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-repo/usescraper-server.git\n   cd usescraper-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"usescraper-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/usescraper-server/build/index.js\"],\n      \"env\": {\n        \"USESCRAPER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/usescraper-server` with the actual path to the server and `your-api-key-here` with your UseScraper API key.\n\n## Usage\n\nOnce configured, you can use the 'scrape' tool through the MCP interface. Example usage:\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\"\n  }\n}\n```\n\n## Development\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "usescraper",
        "extract",
        "web scraping",
        "scraping capabilities",
        "extract content"
      ],
      "category": "web-search"
    },
    "tatn--mcp-server-fetch-typescript": {
      "owner": "tatn",
      "name": "mcp-server-fetch-typescript",
      "url": "https://github.com/tatn/mcp-server-fetch-typescript",
      "imageUrl": "/freedevtools/mcp/pfp/tatn.webp",
      "description": "Retrieves and converts web content using various formats and rendering methods, suitable for both data extraction and web scraping tasks. It allows access to text-based resources and provides raw text content from specified URLs without additional processing.",
      "stars": 3,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-19T14:03:40Z",
      "readme_content": "# mcp-server-fetch-typescript MCP Server\n\nA Model Context Protocol server that provides web content fetching and conversion capabilities. This server implements a comprehensive web content retrieval system with support for various formats and rendering methods, making it ideal for tasks ranging from simple data extraction to sophisticated web scraping.\n\n<a href=\"https://glama.ai/mcp/servers/iyfpvfkgyx\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/iyfpvfkgyx/badge\" alt=\"Server Fetch TypeScript MCP server\" /></a>\n\n## Features\n\n### Tools\n\n- `get_raw_text` - Retrieve raw text content directly from URLs\n  - Takes `url` as a required parameter pointing to text-based resources\n  - Returns unprocessed text content without browser rendering\n  - Ideal for JSON, XML, CSV, TSV, or plain text files\n  - Best used when fast, direct access to source content is needed\n\n- `get_rendered_html` - Fetch fully rendered HTML content\n  - Takes `url` as a required parameter\n  - Returns complete HTML content after JavaScript execution\n  - Uses Playwright for headless browser rendering\n  - Essential for modern web applications and SPAs\n\n- `get_markdown` - Convert web content to Markdown format\n  - Takes `url` as a required parameter\n  - Returns well-formatted Markdown preserving structural elements\n  - Supports tables and definition lists\n  - Recommended for content archiving and documentation\n\n- `get_markdown_summary` - Extract and convert main content\n  - Takes `url` as a required parameter\n  - Returns clean Markdown focusing on main content\n  - Automatically removes navigation, headers, footers\n  - Perfect for article and blog post extraction\n\n## Installation\n\n### As a Global Package\n\n```bash\nnpm install -g mcp-server-fetch-typescript\n```\n\n### As a Project Dependency\n\n```bash\nnpm install mcp-server-fetch-typescript\n```\n\n## Usage\n\n### Using with Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-typescript\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"mcp-server-fetch-typescript\"\n    ]\n  }\n}\n```\n\nor Add the following configuration:\n\n```bash\ngit clone https://github.com/tatn/mcp-server-fetch-typescript.git\ncd mcp-server-fetch-typescript\nnpm install\nnpm run build\n```\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-typescript\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"/path/to/mcp-server-fetch-typescript/build/index.js\"\n    ]\n  }\n}\n```\n\n### Debugging\n\nTo debug the MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector npx -y mcp-server-fetch-typescript\n```\n\n```bash\nnpx @modelcontextprotocol/inspector node /path/to/mcp-server-fetch-typescript/build/index.js\n```\n\n\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-fetch-typescript",
      "npm_downloads": 4154,
      "keywords": [
        "typescript",
        "fetch",
        "scraping",
        "extraction web",
        "fetch typescript",
        "typescript retrieves"
      ],
      "category": "web-search"
    },
    "tcpipuk--mcp-server": {
      "owner": "tcpipuk",
      "name": "mcp-server",
      "url": "https://github.com/tcpipuk/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tcpipuk.webp",
      "description": "Empowers AI assistants to safely access external tools and websites, facilitating better problem-solving capabilities. It provides clear feedback and helpful error messages during operations.",
      "stars": 3,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-08-18T22:24:29Z",
      "readme_content": "# MCP Server\n\nGive your AI assistants the power to help you more effectively. This server lets them safely access\nwebsites and search the web - with clear feedback about what's happening and helpful error messages\nwhen things go wrong.\n\n- [🛠️ What tools does this server offer?](#️-what-tools-does-this-server-offer)\n- [🏎️ How can I run it?](#️-how-can-i-run-it)\n  - [🐋 Using Docker (recommended)](#-using-docker-recommended)\n  - [💻 Running locally](#-running-locally)\n- [🔌 How to connect](#-how-to-connect)\n- [📚 Learn more about MCP](#-learn-more-about-mcp)\n- [📄 License](#-license)\n\n## 🛠️ What tools does this server offer?\n\nThe server provides two powerful tools that help AI assistants solve real-world problems:\n\n| Tool               | What it can do                                                              |\n| ------------------ | --------------------------------------------------------------------------- |\n| [Search](docs/search.md) | Search the web via SearXNG for current information, specific resources, or to perform calculations. |\n| [Web](docs/web.md) | Access websites and process their content. Can convert pages to markdown for easy reading, get the raw content, or extract links. |\n\n## 🏎️ How can I run it?\n\n### 🐋 Using Docker (recommended)\n\nThe server runs in Docker containers to keep things safe and simple. Here's how to get started:\n\n1. [Install Docker](https://docs.docker.com/engine/install/) if you haven't already\n2. Create a file called `docker-compose.yml` with:\n\n   ```yaml:docker-compose.yml\n   services:\n     mcp-server:\n       environment:\n         # Required: URL for your SearXNG instance's Search API\n         - SEARXNG_QUERY_URL=http://searxng:8080\n         # Optional: Configure network mode (SSE) for LibreChat etc.\n         - SSE_HOST=0.0.0.0\n         - SSE_PORT=8080\n         # Optional: Set a custom User-Agent for web requests\n         - USER_AGENT=MCP-Server/1.0 (github.com/tcpipuk/mcp-server)\n       image: ghcr.io/tcpipuk/mcp-server/server:latest\n       ports: # Only needed if using SSE_HOST/SSE_PORT\n         - \"8080:8080\" # Expose port 8080 on host\n       restart: unless-stopped\n       stop_grace_period: 1s\n\n     # Example SearXNG service (optional, adapt as needed)\n     # searxng:\n     #   environment:\n     #     - SEARXNG_BASE_URL=http://searxng:8080 # Ensure SearXNG knows its own URL\n     #   image: searxng/searxng:latest\n     #   restart: unless-stopped\n     #   volumes:\n     #     - ./searxng:/etc/searxng:rw\n   ```\n\n   > **Important**: You *must* provide the `SEARXNG_QUERY_URL` environment variable, pointing to\n   > the Search API endpoint of your SearXNG instance (usually ending in `/` or `/search`).\n   >\n   > Setting `SSE_HOST` and `SSE_PORT` enables network mode (Server-Sent Events), recommended for\n   > multi-container setups like LibreChat. If omitted, the server uses standard I/O.\n\n3. Run `docker compose up -d` to start the server container (and optionally SearXNG).\n\nMost people use this with either:\n\n- [Claude Desktop](https://modelcontextprotocol.io/quickstart/user) - connects directly via stdio\n  (omit `SSE_HOST`/`SSE_PORT` in `docker-compose.yml`).\n- [LibreChat](https://www.librechat.ai/docs/local) - connects over the network via SSE.\n\nFor LibreChat, add this to your `librechat.yaml` (assuming `SSE_PORT=8080`):\n\n```yaml:librechat.yaml\nmcpServers:\n  mcp-server:\n    iconPath: \"/path/to/icon.png\" # Optional: Custom icon\n    label: \"MCP Web/Search\" # Optional: Custom label shown in UI\n    type: sse\n    url: http://mcp-server:8080/sse # Adjust host/port if needed\n```\n\n### 💻 Running locally\n\n1. Install `uv` (requires Python 3.13+):\n\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n   > **Note:** If you already have `uv` installed, update it with `uv self update`.\n\n2. Create and activate a virtual environment:\n\n   ```bash\n   uv venv\n   source .venv/bin/activate  # Linux/macOS\n   # or\n   .venv\\Scripts\\activate     # Windows\n   ```\n\n3. Install dependencies from the lockfile:\n\n   ```bash\n   uv sync\n   ```\n\n4. Set required environment variables:\n\n   ```bash\n   # Required: URL for your SearXNG instance's Search API\n   export SEARXNG_QUERY_URL=\"http://your-searxng-instance.local:8080\"\n   # Optional: Custom User-Agent\n   export USER_AGENT=\"CustomAgent/1.0\"\n   ```\n\n5. Run the server:\n\n   ```bash\n   # For network (SSE) mode (e.g., for LibreChat)\n   mcp-server --sse-host 0.0.0.0 --sse-port 3001\n\n   # For direct stdio mode (e.g., for Claude Desktop)\n   mcp-server\n   ```\n\nAvailable arguments:\n\n- `--sse-host`: SSE listening address (e.g., `0.0.0.0`). Enables SSE mode.\n- `--sse-port`: SSE listening port (e.g., `3001`). Enables SSE mode.\n- `--user-agent`: Custom User-Agent string (overrides `USER_AGENT` env var).\n\n> **Note**: If neither `--sse-host` nor `--sse-port` are provided (and `SSE_HOST`/`SSE_PORT` env\n> vars are not set), the server defaults to `stdio` mode. The `SEARXNG_QUERY_URL` environment\n> variable is *always* required.\n\n## 🔌 How to connect\n\nYou can connect to the server in two ways:\n\n| Method                    | What it means                                           | When to use it                                  |\n| ------------------------- | ------------------------------------------------------- | ----------------------------------------------- |\n| Network connection (SSE)  | The server listens on a network port for connections.   | Best for LibreChat or other networked clients.  |\n| Direct connection (stdio) | The server communicates directly via standard input/out. | Useful for local testing or Claude Desktop. |\n\n## 📚 Learn more about MCP\n\nHere are a few resources to get you started:\n\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n- [MCP Example Servers](https://github.com/modelcontextprotocol/servers)\n\n## 📄 License\n\nThis project is licensed under the GPLv3. See the [LICENSE](LICENSE) file for full details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "tcpipuk",
        "mcp",
        "ai",
        "tcpipuk mcp",
        "search tcpipuk",
        "mcp server"
      ],
      "category": "web-search"
    },
    "techkwon--mcp-gemini": {
      "owner": "techkwon",
      "name": "mcp-gemini",
      "url": "https://github.com/techkwon/mcp-gemini",
      "imageUrl": "/freedevtools/mcp/pfp/techkwon.webp",
      "description": "Leverages Google's Gemini API to generate text, create and analyze images, perform video analysis on YouTube content, and conduct web searches. Provides a range of advanced AI functionalities for various applications.",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-23T14:36:23Z",
      "readme_content": "# MCP Gemini API 서버\n\nCursor와 Claude를 위한 Google Gemini API 서버입니다. 텍스트 생성, 이미지 분석, 비디오 분석 등 Gemini의 다양한 기능을 제공합니다.\n\n## 주요 기능\n\n- 텍스트 생성 (gemini-2.0-flash 모델 사용)\n- 이미지 생성 및 분석\n- YouTube 비디오 분석\n- 웹 검색\n\n## 시작하기\n\n### 필수 요구사항\n\n- Node.js 18.0.0 이상\n- npm 또는 yarn\n- Google API 키 (Gemini API 접근용)\n\n### 설치\n\n```bash\n# 저장소 클론\ngit clone https://github.com/techkwon/mcp-gemini.git\ncd mcp-gemini\n\n# 의존성 설치\nnpm install\n```\n\n### 환경 설정\n\n1. `config.ts` 파일에 Google API 키 설정:\n\n```typescript\nexport default {\n  googleApiKey: \"your_api_key_here\",\n  // 기타 설정...\n};\n```\n\n### 빌드 및 실행\n\n```bash\n# TypeScript 빌드\nnpm run build\n\n# 서버 시작 (PM2 사용)\nnpm start\n\n# 개발 모드로 실행\nnpm run dev\n```\n\n### PM2 서버 관리\n\n서버는 PM2를 통해 자동으로 관리됩니다. 다음 명령어로 서버를 관리할 수 있습니다:\n\n```bash\n# 서버 상태 확인\nnpm run status\n\n# 서버 로그 확인\nnpm run logs\n\n# 서버 중지\nnpm run stop\n\n# 서버 재시작\nnpm run restart\n\n# 시스템 재시작 시 자동 실행 설정\npm2 startup\npm2 save\n```\n\n## Cursor/Claude 연동\n\n### MCP 설정\n\n`~/.cursor/mcp.json` 파일에 다음 설정을 추가하세요:\n\n```json\n{\n  \"github.com/techkwon/mcp-gemini\": {\n    \"command\": \"npm\",\n    \"args\": [\"start\"],\n    \"cwd\": \"<프로젝트_경로>\",\n    \"env\": {\n      \"NODE_ENV\": \"production\"\n    },\n    \"disabled\": false,\n    \"autoStart\": true,\n    \"autoApprove\": [\n      \"gem-generate\",\n      \"gem-generate-image\",\n      \"gem-analyze-video\",\n      \"gem-search\"\n    ]\n  }\n}\n```\n\n### API 엔드포인트\n\n- `/gem-generate`: 텍스트 생성\n- `/gem-generate-image`: 이미지 생성/분석\n- `/gem-analyze-video`: YouTube 비디오 분석\n- `/gem-search`: 웹 검색\n\n## 주요 업데이트\n\n### 최신 버전 (2024-03)\n- PM2를 통한 서버 자동화 구현\n- gemini-2.0-flash 모델로 통일\n- 자동 재시작 및 오류 복구 기능 추가\n- 환경 설정 개선\n\n### 이전 버전\n- YouTube 비디오 분석 기능 추가\n- 이미지 생성/분석 기능 개선\n- 웹 검색 기능 추가\n\n## 문제 해결\n\n### 일반적인 문제\n\n1. **서버가 시작되지 않는 경우**\n   ```bash\n   # PM2 로그 확인\n   npm run logs\n   \n   # PM2 프로세스 상태 확인\n   npm run status\n   ```\n\n2. **API 키 오류**\n   - `config.ts` 파일에서 API 키가 올바르게 설정되었는지 확인\n   - Gemini API 할당량 및 권한 확인\n\n3. **메모리 사용량 문제**\n   - `ecosystem.config.js`에서 메모리 제한 설정 확인\n   - PM2 모니터링으로 메모리 사용량 추적\n\n## 기여하기\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스를 따릅니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.\n\n## 연락처\n\n프로젝트 관리자: techkwon\n이메일: techkwon@example.com\n프로젝트 링크: [https://github.com/techkwon/mcp-gemini](https://github.com/techkwon/mcp-gemini)\n\n## 주요 의존성\n\n- @google/generative-ai: ^0.1.3 (Gemini API SDK)\n- @fastify/cors: ^8.5.0 (CORS 지원)\n- fastify: ^4.29.0 (웹 서버 프레임워크)\n- googleapis: ^148.0.0 (Google API 지원)\n- typescript: ^5.0.0\n- zod: ^3.24.2 (데이터 검증)\n- pino: ^8.21.0 (로깅)\n\n## Claude 데스크톱 앱 통합 가이드\n\n### 설정 파일 위치\nClaude 데스크톱 앱의 설정 파일은 다음 경로에 위치합니다:\n- Windows: `%APPDATA%/Claude/config.json`\n- macOS: `~/Library/Application Support/Claude/config.json`\n\n### JSON 설정 예시\n\n```json\n{\n  \"apis\": [\n    {\n      \"name\": \"MCP Gemini\",\n      \"url\": \"http://localhost:8000\",\n      \"methods\": [\n        {\n          \"name\": \"텍스트 생성\",\n          \"method\": \"gem-generate\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-generate\",\n            \"params\": {\n              \"prompt\": \"{input}\"\n            }\n          }\n        },\n        {\n          \"name\": \"이미지 생성\",\n          \"method\": \"gem-generate-image\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-generate-image\",\n            \"params\": {\n              \"prompt\": \"{input}\"\n            }\n          }\n        },\n        {\n          \"name\": \"비디오 분석\",\n          \"method\": \"gem-analyze-video\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-analyze-video\",\n            \"params\": {\n              \"videoUrl\": \"{input}\",\n              \"query\": \"이 영상의 주요 내용을 요약해주세요\"\n            }\n          }\n        },\n        {\n          \"name\": \"웹 검색\",\n          \"method\": \"gem-search\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-search\",\n            \"params\": {\n              \"query\": \"{input}\"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n### 변수 설명\n\n- `{uuid}`: 자동으로 생성되는 고유 요청 ID\n- `{input}`: Claude 채팅창에 입력한 텍스트\n\n### 사용 방법\n\n1. Claude 데스크톱 앱의 설정 파일을 엽니다.\n2. 위의 JSON 설정을 기존 설정에 추가합니다.\n3. Claude 데스크톱 앱을 재시작합니다.\n4. 채팅창에서 다음과 같이 사용할 수 있습니다:\n\n```\n@MCP Gemini.텍스트 생성 한국의 전통 음식에 대해 설명해주세요\n@MCP Gemini.이미지 생성 한옥마을의 아름다운 풍경\n@MCP Gemini.비디오 분석 https://youtube.com/watch?v=VIDEO_ID\n@MCP Gemini.웹 검색 최신 인공지능 기술 동향\n```\n\n### 응답 형식\n\n모든 API 응답은 다음 형식을 따릅니다:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"요청에서 보낸 ID\",\n  \"result\": {\n    \"content\": \"응답 내용\"\n  }\n}\n```\n\n### 오류 응답\n\n오류가 발생한 경우 다음 형식으로 응답합니다:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"요청에서 보낸 ID\",\n  \"error\": {\n    \"code\": 오류코드,\n    \"message\": \"오류 메시지\",\n    \"data\": {\n      \"details\": \"상세 오류 정보\"\n    }\n  }\n}\n```\n\n## 오류 처리\n\n서버는 다음과 같은 상황에서 적절한 오류 응답을 반환합니다:\n\n- 400: 잘못된 요청 형식\n- 401: 인증 오류 (API 키 관련)\n- 500: 서버 내부 오류\n\n## 보안 고려사항\n\n- API 키는 반드시 환경 변수로 관리하세요\n- 프로덕션 환경에서는 적절한 보안 설정을 추가하세요\n- 민감한 정보는 로그에 기록하지 않도록 주의하세요\n\n## 문제 해결\n\n### 포트 충돌\n이미 8000번 포트가 사용 중인 경우:\n```bash\n# 기존 Node.js 프로세스 종료\npkill -f \"node\"\n```\n\n### 서버 안정성\n서버가 예기치 않게 종료되는 경우:\n- PM2나 다른 프로세스 관리자 사용을 고려하세요\n- 로그를 확인하여 종료 원인을 파악하세요\n\n## 개발 가이드\n\n### 로깅\n- Pino 로거를 사용하여 구조화된 로깅을 구현했습니다\n- 개발 환경에서는 pino-pretty를 통해 가독성 있는 로그가 출력됩니다\n\n### 타입 안정성\n- TypeScript와 Zod를 사용하여 런타임 타입 안정성을 보장합니다\n- API 요청/응답에 대한 스키마 검증이 구현되어 있습니다\n\n## CLINE MCP 마켓플레이스 등록 가이드\n\n### 사전 준비사항\n\n1. GitHub 저장소가 공개되어 있어야 합니다\n2. README.md 파일에 명확한 설치 및 설정 방법이 포함되어 있어야 합니다\n3. (선택사항) `llms-install.md` 파일을 통해 AI 에이전트를 위한 추가 설치 가이드를 제공할 수 있습니다\n\n### 등록 절차\n\n1. [CLINE MCP 마켓플레이스 저장소](https://github.com/cline/mcp-marketplace)에 새로운 이슈를 생성합니다\n\n2. 이슈에 다음 정보를 포함합니다:\n   - **GitHub 저장소 URL:** https://github.com/techkwon/mcp-gemini\n   - **로고 이미지:** 400×400 크기의 PNG 파일\n   - **추가 이유:** 이 MCP 서버가 CLINE 사용자들에게 제공할 수 있는 가치\n   예시:\n   ```markdown\n   ## MCP Gemini 서버 등록 요청\n   \n   ### GitHub 저장소\n   https://github.com/techkwon/mcp-gemini\n   \n   ### 주요 기능\n   - Gemini API를 활용한 텍스트 생성\n   - 이미지 생성 및 편집 (gemini-2.0-flash-exp 모델 사용)\n   - YouTube 비디오 콘텐츠 분석\n   - 웹 검색 기능\n   \n   ### 사용자 이점\n   - 최신 Gemini 모델을 MCP 프로토콜을 통해 쉽게 활용\n   - 다양한 미디어 형식(텍스트, 이미지, 비디오) 처리 가능\n   - 명확한 JSON-RPC 인터페이스로 쉬운 통합\n   - 상세한 문서화와 예제 제공\n   ```\n\n3. CLINE이 README.md만으로 서버를 성공적으로 설치할 수 있는지 테스트합니다\n\n### 승인 절차\n\n1. CLINE 팀이 제출된 MCP 서버를 검토합니다\n2. 보안 및 안정성 검증을 진행합니다\n3. 승인되면 마켓플레이스에 등록되어 모든 CLINE 사용자가 접근할 수 있게 됩니다\n\n### 설치 가이드 최적화\n\n`llms-install.md` 파일을 생성하여 AI 에이전트를 위한 추가 설치 가이드를 제공할 수 있습니다:\n\n```markdown\n# MCP Gemini 서버 설치 가이드 (AI 에이전트용)\n\n## 환경 요구사항\n- Node.js 18.0.0 이상\n- npm 또는 yarn\n- Google AI Studio API 키\n\n## 설치 단계\n1. 저장소 클론\n2. 의존성 설치: `npm install`\n3. 환경 변수 설정: GOOGLE_API_KEY 추가\n4. 빌드: `npm run build`\n5. 서버 실행: `npm run start`\n\n## 설정 검증\n- 8000번 포트 사용 가능 여부 확인\n- API 키 유효성 검증\n- CORS 설정 확인\n\n## 문제 해결\n- 포트 충돌 시 해결 방법\n- API 키 오류 해결 방법\n- 일반적인 설치 문제 해결 가이드\n``` ",
      "npm_url": "https://www.npmjs.com/package/mcp-gemini",
      "npm_downloads": 176,
      "keywords": [
        "google",
        "gemini",
        "ai",
        "google gemini",
        "gemini api",
        "mcp gemini"
      ],
      "category": "web-search"
    },
    "terryso--mcp-pinterest": {
      "owner": "terryso",
      "name": "mcp-pinterest",
      "url": "https://github.com/terryso/mcp-pinterest",
      "imageUrl": "/freedevtools/mcp/pfp/terryso.webp",
      "description": "Search for images on Pinterest using keywords and retrieve detailed information about those images.",
      "stars": 16,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-14T02:40:14Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/terryso-mcp-pinterest-badge.png)](https://mseep.ai/app/terryso-mcp-pinterest)\n\n# Pinterest MCP Server\n\n[![npm version](https://badge.fury.io/js/pinterest-mcp-server.svg)](https://badge.fury.io/js/pinterest-mcp-server)\n![NPM Downloads](https://img.shields.io/npm/dw/pinterest-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@terryso/mcp-pinterest)](https://smithery.ai/server/@terryso/mcp-pinterest)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/23da55f4-6d6a-4dc9-af31-cd6073eb1b27)\n\nA Model Context Protocol (MCP) server for Pinterest image search and information retrieval.\n\n## Features\n\n- Search for images on Pinterest by keywords\n- Retrieve detailed information about Pinterest images\n- Seamless integration with Cursor IDE through MCP\n- Support for headless browser mode\n- Limit control for search results\n- Search and download images from Pinterest\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Cursor IDE](https://cursor.sh/) for MCP integration\n\n## Installation\n\n### Using NPX (Recommended)\n\nThe easiest way to use Pinterest MCP Server is via npx:\n\n```bash\nnpx pinterest-mcp-server\n```\n\nYou can configure the server with command-line options:\n\n```bash\n# Specify download directory\nnpx pinterest-mcp-server --downloadDir /path/to/downloads\n\n# Specify filename template\nnpx pinterest-mcp-server --filenameTemplate \"pinterest_{id}\"\n\n# Specify both options\nnpx pinterest-mcp-server --downloadDir ./images --filenameTemplate \"pinterest_{id}\"\n```\n\n### Global Installation\n\nTo install the package globally and use it directly from the command line:\n\n```bash\nnpm install -g pinterest-mcp-server\n```\n\nAfter installation, you can run the server with:\n\n```bash\npinterest-mcp-server\n```\n\nWith the same command line options as the NPX version:\n\n```bash\npinterest-mcp-server --downloadDir /path/to/downloads --filenameTemplate \"pinterest_{id}\"\n```\n\n### Installing via Smithery\n\nTo install mcp-pinterest for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-pinterest):\n\n```bash\nnpx -y @smithery/cli install mcp-pinterest --client claude\n```\n\n### Manual Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/terryso/mcp-pinterest.git pinterest-mcp-server\n   cd pinterest-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n4. Run the server:\n   ```bash\n   npm start\n   ```\n\n## Configuring as MCP Server in Cursor\n\n1. Open Cursor IDE\n2. Go to Settings (⚙️) > Extensions > MCP\n3. Click \"Add Server\"\n4. Enter the following details:\n   - Name: Pinterest MCP\n   - Type: Command\n   - Command: `node`\n   - Args: `[\"/path/to/mcp-pinterest/dist/pinterest-mcp-server.js\"]`\n\n   或者直接编辑Cursor的MCP配置文件（通常位于`~/.cursor/mcp.json`），添加以下内容：\n   ```json\n   \"pinterest\": {\n     \"command\": \"node\",\n     \"args\": [\"/path/to/mcp-pinterest/dist/pinterest-mcp-server.js\"]\n   }\n   ```\n5. Click \"Save\"\n\n### Alternative: Using NPX for Cursor Configuration\n\nYou can also configure Cursor to use the npx version of the server:\n\n1. Open Cursor IDE\n2. Go to Settings (⚙️) > Extensions > MCP\n3. Click \"Add Server\"\n4. Enter the following details:\n   - Name: Pinterest MCP\n   - Type: Command\n   - Command: `npx`\n   - Args: `[\"pinterest-mcp-server\"]`\n5. Click \"Save\"\n\n### Complete Configuration Example with Environment Variables\n\nFor the most flexibility, you can configure the server with environment variables in your Cursor MCP configuration:\n\n```json\n\"pinterest\": {\n  \"command\": \"npx\",\n  \"env\": {\n    \"MCP_PINTEREST_DOWNLOAD_DIR\": \"/Users/xxx/Desktop/Images\",\n    \"MCP_PINTEREST_FILENAME_TEMPLATE\": \"pin_{imageId}_{timestamp}.{fileExtension}\",\n    \"MCP_PINTEREST_PROXY_SERVER\": \"http://127.0.0.1:7890\"\n  },\n  \"args\": [\"pinterest-mcp-server\"]\n}\n```\n\nThis configuration:\n- Uses npx to run the server\n- Sets a custom download directory on your desktop\n- Uses a custom filename template with both image ID and timestamp\n- Configures a proxy server for users in regions where Pinterest might be blocked\n\nAdd this to your `~/.cursor/mcp.json` file or set up through the Cursor IDE interface.\n\n## Available MCP Functions\n\nThe server exposes the following MCP functions:\n\n- `pinterest_search`: Search for images on Pinterest by keyword\n  - Parameters:\n    - `keyword`: Search term (required)\n    - `limit`: Number of images to return (default: 10)\n    - `headless`: Whether to use headless browser mode (default: true)\n\n- `pinterest_get_image_info`: Get detailed information about a Pinterest image\n  - Parameters:\n    - `image_url`: URL of the Pinterest image (required)\n\n- `pinterest_search_and_download`: Search and download images from Pinterest\n  - Parameters:\n    - `keyword`: Search term (required)\n    - `limit`: Number of images to return (default: 10)\n    - `headless`: Whether to use headless browser mode (default: true)\n\n## Example Usage in Cursor\n\nOnce configured, you can use the Pinterest MCP functions directly in Cursor's AI chat:\n\n```\nSearch for robot images on Pinterest\n```\n\nThe AI will use the MCP server to search Pinterest and display the results.\n\n### Example Screenshot\n\n\n\n*Screenshot showing a search for 20 images of 三上悠亚 with all images successfully downloaded.*\n\n## Development\n\n### Project Structure\n\n- `pinterest-mcp-server.ts`: Main server file\n- `dist/pinterest-mcp-server.js`: Built JavaScript file for production\n- `package.json`: Project configuration and dependencies\n\n### Adding New Features\n\nTo add new MCP functions:\n\n1. Modify `pinterest-mcp-server.ts`\n2. Register new functions using the MCP SDK\n3. Implement the function logic\n4. Rebuild with `npm run build`\n\n## Troubleshooting\n\n- If the server fails to start, check if the port is already in use\n- Ensure all dependencies are correctly installed with `npm install`\n- Make sure TypeScript is properly configured with a `tsconfig.json` file\n- If you encounter build errors, try running `npm install -D typescript @types/node`\n- Verify network connectivity for Pinterest access\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Configuration Options\n\n### Command Line Options (NPX Mode)\n\nWhen using the server via npx, you can configure it using the following command line options:\n\n- `--downloadDir`: Specifies the root directory for downloading images\n  ```bash\n  npx pinterest-mcp-server --downloadDir /path/to/downloads\n  ```\n\n- `--filenameTemplate`: Specifies the filename template for downloaded images\n  ```bash\n  npx pinterest-mcp-server --filenameTemplate \"pin_{imageId}_{timestamp}\"\n  ```\n\n- `--port`: Specifies the port for the server to listen on (default: 3000)\n  ```bash\n  npx pinterest-mcp-server --port 8080\n  ```\n\n- `--proxyServer`: Specifies the proxy server to use for connecting to Pinterest\n  ```bash\n  npx pinterest-mcp-server --proxyServer \"http://127.0.0.1:7890\"\n  ```\n\nYou can combine multiple options:\n```bash\nnpx pinterest-mcp-server --downloadDir ./images --filenameTemplate \"pinterest_{id}\" --port 8080 --proxyServer \"http://127.0.0.1:7890\"\n```\n\n### Environment Variables\n\nThe server also supports the following environment variables for configuration:\n\n- `MCP_PINTEREST_DOWNLOAD_DIR`: Specifies the root directory for downloading images. If not set, the default is the `../downloads` directory relative to the server script.\n- `MCP_PINTEREST_FILENAME_TEMPLATE`: Specifies the filename template for downloaded images. If not set, the default is `pinterest_{imageId}.{fileExtension}`.\n- `MCP_PINTEREST_PROXY_SERVER`: Specifies the proxy server to use for connecting to Pinterest. Format should be `protocol://host:port`, for example `http://127.0.0.1:7890` or `socks5://127.0.0.1:1080`.\n\nThese environment variables can be set in several ways:\n1. Directly in your terminal (as shown in the examples below)\n2. In your Cursor MCP configuration through the `env` field (see [Complete Configuration Example](#complete-configuration-example-with-environment-variables))\n3. In a `.env` file in the project root directory\n4. Through command line options with npx (as shown in the [Command Line Options](#command-line-options-npx-mode) section)\n\n### Usage\n\n#### Setting Download Directory\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --downloadDir /path/to/your/download/directory\n```\n\n2. Set the download directory using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_DOWNLOAD_DIR=/path/to/your/download/directory\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_DOWNLOAD_DIR=C:\\path\\to\\your\\download\\directory\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_DOWNLOAD_DIR=\"C:\\path\\to\\your\\download\\directory\"\nnpx pinterest-mcp-server\n```\n\n3. If the environment variable is not set, the server will use the default download directory (relative to the server script's `../downloads`).\n\n#### Setting Filename Template\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --filenameTemplate \"pin_{imageId}_{timestamp}.{fileExtension}\"\n```\n\n2. Using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n```\n\nThe template supports the following variables:\n- `{imageId}`: The unique ID of the Pinterest image\n- `{fileExtension}`: The file extension (e.g., jpg, png)\n- `{timestamp}`: Current UTC timestamp in YYYYMMDDHHMMSS format\n- `{index}`: The index number when downloading multiple images (starts from 1)\n\nExample templates:\n- `pinterest_{imageId}.{fileExtension}` (default)\n- `pin_{timestamp}_{imageId}.{fileExtension}`\n- `pinterest_image_{index}_{imageId}.{fileExtension}`\n- `{timestamp}_pinterest.{fileExtension}`\n\nIf the template is invalid (e.g., contains unsupported variables or has mismatched brackets), the server will log a warning and use the default template.\n\n#### Setting Proxy Server\n\nIf you need to use a proxy to access Pinterest (especially in regions where Pinterest might be restricted), you can set the proxy configuration:\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --proxyServer \"http://127.0.0.1:7890\"\n```\n\n2. Using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_PROXY_SERVER=\"http://127.0.0.1:7890\"\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_PROXY_SERVER=http://127.0.0.1:7890\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_PROXY_SERVER=\"http://127.0.0.1:7890\"\nnpx pinterest-mcp-server\n```\n\nSupported proxy protocols:\n- HTTP: `http://host:port`\n- HTTPS: `https://host:port`\n- SOCKS4: `socks4://host:port`\n- SOCKS5: `socks5://host:port`\n\nThe proxy configuration affects both the browser used for searching and the image downloading process.\n\n#### Notes\n\n- The server will verify the existence and writability of the download directory when starting. If the directory does not exist, it will attempt to create it; if it cannot be created or written to, the server will exit.\n- Clients should not specify download paths or filename templates through parameters when calling download-related tools, as all downloads will use the server's environment variable configuration or defaults.\n- The server automatically sanitizes filenames by replacing illegal characters (such as `/`, `\\`, `:`, `*`, `?`, `\"`, `<`, `>`, `|`) with underscores.\n\n#### Interface Description\n\nThe server provides the following MCP tools:\n\n1. `pinterest_search`: Search for Pinterest images by keyword\n2. `pinterest_get_image_info`: Get detailed information about a Pinterest image\n3. `pinterest_search_and_download`: Search and download Pinterest images\n\nFor detailed interface parameter references, please refer to the MCP tool definitions. \n\n## ⭐ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=terryso/mcp-pinterest&type=Date)](https://www.star-history.com/#terryso/mcp-pinterest&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pinterest",
        "images",
        "search",
        "images pinterest",
        "search images",
        "mcp pinterest"
      ],
      "category": "web-search"
    },
    "terryso--meta_tag_genie": {
      "owner": "terryso",
      "name": "meta_tag_genie",
      "url": "https://github.com/terryso/meta_tag_genie",
      "imageUrl": "/freedevtools/mcp/pfp/terryso.webp",
      "description": "Manage image metadata to improve macOS Spotlight search functionality by adding tags, descriptions, people, and location data to JPG, PNG, and HEIC images through a standard MCP interface over Stdio.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-11T16:58:49Z",
      "readme_content": "# MetaTag Genie\n\n[![npm version](https://badge.fury.io/js/metatag-genie.svg)](https://badge.fury.io/js/metatag-genie)\n![NPM Downloads](https://img.shields.io/npm/dw/metatag-genie)\n[![Node.js Version](https://img.shields.io/node/v/metatag-genie)](https://nodejs.org)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/terryso/metatag-genie/pulls)\n[![smithery badge](https://smithery.ai/badge/@terryso/meta_tag_genie)](https://smithery.ai/server/@terryso/meta_tag_genie)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-项目文档-blue)](https://deepwiki.com/terryso/metatag-genie)\n\n## 简介\n\nMetaTag Genie 是一个 macOS Stdio MCP 服务，专为写入图片元数据以增强 Spotlight 搜索而设计。该服务可被 AI 代理或其他需要本地管理图片元数据的应用程序调用，通过标准输入输出（Stdio）与客户端通信，提供符合 MCP (Machine Comprehension Protocol) 规范的接口。\n\n## 功能特性\n\n- 通过 Stdio 暴露符合 MCP 规范的服务\n- 提供 `writeImageMetadata` MCP Tool\n- 支持向 JPG, PNG, HEIC 图片写入元数据\n- 支持的元数据类型：\n  - 标签 (Tags)\n  - 描述 (Description)\n  - 人物 (People - 作为关键词)\n  - 地点 (Location - 文本)\n- 写入的元数据可被 macOS Spotlight 搜索\n\n## 系统要求\n\n- **Node.js 版本**：Node.js 22.x LTS 或更高版本\n- **隐含依赖**：本项目使用 `exiftool-vendored`，它会自动管理 ExifTool 的依赖\n\n## 安装\n\n### 安装 Smithery\n\n要自动将 MetaTag Genie 安装到 Claude Desktop，请使用 [Smithery](https://smithery.ai/server/@terryso/meta_tag_genie)：\n\n```bash\nnpx -y @smithery/cli install @terryso/metatag_genie --client claude\n```\n\n### 通过NPM安装（推荐）\n\n```bash\n# 全局安装\nnpm install -g metatag-genie\n```\n\n使用全局安装后，可以通过以下命令直接运行：\n\n```bash\nmetatag-genie\n```\n\n或者，您可以不安装直接使用npx运行：\n\n```bash\nnpx metatag-genie\n```\n\n这种方式不需要全局安装，也是AI代理等客户端调用的推荐方式。\n\n### 通过源码安装\n\n```bash\n# 1. 克隆仓库\ngit clone <repository-url>\ncd metatag-genie\n\n# 2. 安装依赖\nnpm install\n# 或者: yarn install\n\n# 3. 编译 TypeScript 代码\nnpm run build\n# 或者: yarn build\n\n# 4. 本地链接（可选，用于测试npx和命令行调用）\nnpm link\n```\n\n## 运行服务\n\n### 通过NPX运行（推荐）\n\n安装包后，可以直接通过npx运行：\n\n```bash\nnpx metatag-genie\n```\n\n这种方式不需要全局安装包，适合作为AI代理或其他客户端的调用方式。\n\n### 开发模式\n\n开发过程中，可以直接使用TypeScript源码运行服务：\n\n```bash\nnpm run start:dev\n```\n\n此命令使用`ts-node`直接运行TypeScript代码，无需预先编译，适合快速开发和调试。\n\n### 生产模式\n\n构建后，可以通过以下命令运行编译好的JavaScript代码：\n\n```bash\nnpm start\n# 或者直接: node dist/main.js\n```\n\n服务启动后会监听标准输入输出（Stdio），等待MCP客户端连接并发送JSON-RPC消息。\n\n### 与MCP客户端集成\n\nAI代理（如Cursor）或其他客户端需要在其配置中指定命令的完整路径：\n\n- 基于Node.js：`/usr/local/bin/node /path/to/metatag-genie/dist/main.js`\n- 使用npx：`/usr/local/bin/npx metatag-genie`\n\n注意：本服务不监听网络端口，仅通过标准输入输出通信。\n\n### Cursor MCP集成\n\n在Cursor编辑器中，可以通过以下步骤集成MetaTag Genie：\n\n1. 在项目根目录创建`.cursor/mcp.json`文件\n2. 添加以下配置内容：\n\n```json\n{\n  \"mcpServers\": {\n    \"MetaTagGenie\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"metatag-genie\"\n      ]\n    }\n  }\n}\n```\n\n配置完成后，Cursor中的AI助手将能够使用MetaTag Genie提供的writeImageMetadata工具，直接为图片添加元数据，增强Spotlight搜索体验。\n\n## MCP 交互协议\n\n### 基础\n\n通信基于 Stdio 上的 JSON-RPC 2.0 协议。\n\n### 初始化流程\n\n客户端需要先发送 `initialize` 请求，服务器响应 `InitializeResult`，然后客户端发送 `initialized` 通知完成握手。\n\n### Tool 调用\n\n#### 示例：writeImageMetadata 调用\n\n**JSON-RPC 请求示例**:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id-123\",\n  \"method\": \"writeImageMetadata\",\n  \"params\": {\n    \"filePath\": \"/Users/username/Pictures/photo.jpg\",\n    \"metadata\": {\n      \"tags\": [\"Vacation\", \"Beach\"],\n      \"description\": \"Sunset view from the hotel.\",\n      \"people\": [\"Alice\", \"Bob\"],\n      \"location\": \"Hawaii, USA\"\n    },\n    \"overwrite\": true\n  }\n}\n```\n\n**JSON-RPC 成功响应示例**:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id-123\",\n  \"result\": {\n    \"success\": true,\n    \"filePath\": \"/Users/username/Pictures/photo.jpg\",\n    \"message\": \"Metadata successfully written.\"\n  }\n}\n```\n\n**注意**：实际使用时 `filePath` 需要是有效的绝对路径。\n\n完整的 Tool 参数、返回值和错误代码定义，请参阅 [MCP Tools 定义文档](./docs/mcp-tools-definition.md)。\n\n## 运行测试\n\n```bash\n# 运行所有单元测试和集成测试\nnpm test\n# 或者: yarn test\n\n# 在监视模式下运行测试\nnpm run test:watch\n# 或者: yarn test:watch\n\n# 运行测试并生成覆盖率报告 (输出到 coverage/ 目录)\nnpm run test:cov\n# 或者: yarn test:cov\n```\n\n关于不同测试层级的更多信息，请参阅 [测试策略文档](./docs/testing-strategy.md)。\n\n## 开发与贡献\n\n### 代码风格\n\n项目使用 ESLint 和 Prettier 强制代码风格，详细规范请参阅 [编码规范文档](./docs/coding-standards.md)。\n\n请在提交代码前运行以下命令：\n\n```bash\n# 检查代码风格\nnpm run lint\n\n# 自动格式化代码\nnpm run format\n```\n\n### 分支策略\n\n- `master` 分支用于发布稳定版本\n- 开发新功能或修复 Bug 时，从 `master` 创建特性分支（例如 `feature/add-png-support` 或 `fix/handle-exiftool-error`）\n- 完成后提交 Pull Request (PR) 到 `master` 分支\n\n### Pull Request (PR) 流程\n\n- PR 应包含清晰的描述，说明变更内容和原因\n- PR 需要通过所有 CI 检查 (Linting, Tests, Build)\n- 鼓励进行代码审查\n\n### 报告问题\n\n如需报告 Bug 或提出功能建议，请在项目的 Issue Tracker 中创建新的 Issue，并提供尽可能详细的信息。\n\n### 持续集成与自动发布\n\n本项目使用GitHub Actions进行持续集成和自动发布：\n\n- 每次Push和PR会自动运行测试和构建\n- 创建Release后会自动发布到NPM\n\n提交PR时请确保通过所有CI检查。如需发布新版本：\n\n1. 更新`package.json`中的版本号\n2. 创建一个新的GitHub Release\n3. GitHub Actions会自动将包发布到NPM\n\n## 文档链接\n\n- [架构文档](./docs/architecture.md)\n- [技术栈](./docs/tech-stack.md)\n- [项目结构](./docs/project-structure.md)\n- [MCP Tool 定义](./docs/mcp-tools-definition.md)\n- [测试策略](./docs/testing-strategy.md)\n- [编码规范](./docs/coding-standards.md)\n- [元数据字段映射](./docs/metadata-field-mapping.md)\n- [手动 Spotlight 测试计划](./docs/manual-spotlight-tests.md)\n\n## 许可证\n\n本项目采用 MIT 许可证。详情请参阅项目根目录下的 LICENSE 文件。 \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "meta_tag_genie",
        "metadata",
        "spotlight",
        "image metadata",
        "macos spotlight",
        "meta_tag_genie manage"
      ],
      "category": "web-search"
    },
    "the0807--GeekNews-MCP-Server": {
      "owner": "the0807",
      "name": "GeekNews-MCP-Server",
      "url": "https://github.com/the0807/GeekNews-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/the0807.webp",
      "description": "Fetch articles from GeekNews using web scraping capabilities, providing access to the latest news stories along with various metadata like titles, URLs, points, authors, and comments. The server caches data to optimize performance and includes tools for retrieving both current articles and weekly summaries.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-01T05:44:01Z",
      "readme_content": "# GeekNews MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@the0807/geeknews-mcp-server)](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n이 프로젝트는 [GeekNews](https://news.hada.io)에서 아티클을 가져오는 Model Context Protocol(MCP) 서버입니다. Python으로 구현되었으며, BeautifulSoup을 사용하여 웹 스크래핑을 수행합니다. 서버 부하를 줄이기 위해 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장하는 기능을 제공합니다.\n\n## 기능\n|    |    |\n|:-------------------------:|:-------------------------:|\n|||\n\n### 도구 (Tools)\n\n- `get_articles` 도구: GeekNews에서 아티클을 가져오는 기능\n  - 아티클 유형(top, new, ask, show)과 반환할 아티클 수를 지정할 수 있음\n  - 각 응답에는 제목, URL, 포인트, 작성자, 시간, 댓글 수, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n- `get_weekly_news` 도구: GeekNews에서 주간 뉴스를 가져오는 기능\n  - 특정 주간 뉴스 ID를 지정하거나 가장 최근 주간 뉴스를 가져올 수 있음\n  - 주간 뉴스의 제목, 번호, ID, 내용, URL, 아이템 목록 등의 정보를 제공\n  - 각 아이템에는 제목, URL, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n### 캐시 기능\n\n- 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장\n- 캐시된 데이터가 유효한 경우 GeekNews 서버에 요청하지 않고 캐시된 데이터 사용\n- 캐시 데이터는 24시간 동안 유효하며, 이후 자동으로 갱신\n- 스케줄러가 주기적으로 캐시 유효성을 검사하고 필요시 갱신\n\n## 사용법\n\n- Smithery를 이용한 설치\n\n   🚀 [geeknews-mcp-server](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n- MCP 설정 파일에 서버 정보를 추가\n\n   ```json\n   {\n   \"mcpServers\": {\n      \"geeknews-mcp-server\": {\n         \"command\": \"npx\",\n         \"args\": [\n         \"-y\",\n         \"@smithery/cli@latest\",\n         \"run\",\n         \"@the0807/geeknews-mcp-server\",\n         \"--key\",\n         \"smithery에서 발급 받은 키\"\n         ]\n      }\n   }\n   }\n   ```\n\n## 로컬 설치 방법\n\n1. Git Clone\n\n   ```bash\n   git clone https://github.com/the0807/GeekNews-MCP-Server\n   cd GeekNews-MCP-Server\n   ```\n\n2. uv로 환경 세팅\n\n   ```bash\n   uv sync\n   ```\n\n3. 가상환경 실행\n\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. MCP Inspector로 서버 테스트\n\n   ```bash\n   uv run mcp\n   mcp dev main.py\n\n   # 터미널에 나오는 URL(MCP Inspector)로 접속하여 서버 테스트\n   ```\n\n## 코드 구조\n\n- `src/models.py`: 아티클 정보를 저장하는 데이터 클래스 정의\n- `src/parser.py`: GeekNews 웹사이트의 HTML을 파싱하여 아티클 정보를 추출\n- `src/client.py`: GeekNews 웹사이트에서 데이터를 가져오는 HTTP 클라이언트\n- `src/config.py`: 설정과 상수 정의\n- `src/cache.py`: 캐시 관리 기능 제공\n- `src/scheduler.py`: 주기적인 데이터 갱신 스케줄러\n- `src/server.py`: MCP 서버 구현\n- `main.py`: 서버 실행 진입점\n\n> [!Note]\n> - 이 서버는 GeekNews 웹사이트의 HTML 구조에 의존합니다. 웹사이트 구조가 변경되면 파싱 로직을 업데이트해야 할 수 있습니다.\n> - 캐시 데이터는 사용자의 홈 디렉토리 아래 `.cache/geeknews-mcp` 폴더에 저장됩니다.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geeknews",
        "scraping",
        "articles",
        "geeknews using",
        "articles geeknews",
        "geeknews mcp"
      ],
      "category": "web-search"
    },
    "thomasvan--mcp-brave-search": {
      "owner": "thomasvan",
      "name": "mcp-brave-search",
      "url": "https://github.com/thomasvan/mcp-brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/thomasvan.webp",
      "description": "Utilizes Brave Search to perform web searches and retrieve local business information, facilitating seamless integration of search capabilities into MCP-compatible AI workflows.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-05T03:32:48Z",
      "readme_content": "# Brave Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@thomasvan/mcp-brave-search)](https://smithery.ai/server/@thomasvan/mcp-brave-search)\n\nThis project implements a Model Context Protocol (MCP) server for Brave Search, allowing integration with AI assistants like Claude.\n\n## Prerequisites\n\n- Python 3.11+\n- [uv](https://github.com/astral-sh/uv) - A fast Python package installer and resolver\n\n## Installation\n\n### Installing via Smithery\n\nTo install Brave Search MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@thomasvan/mcp-brave-search):\n\n```bash\nnpx -y @smithery/cli install @thomasvan/mcp-brave-search --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```\n   git clone https://github.com/thomasvan/mcp-brave-search.git\n   cd mcp-brave-search\n   ```\n\n2. Create a virtual environment and install dependencies using uv:\n   ```\n   uv venv\n   source .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\n   uv pip install -r requirements.txt\n   ```\n\n3. Set up your Brave Search API key:\n   ```\n   export BRAVE_API_KEY=your_api_key_here\n   ```\n   On Windows, use: `set BRAVE_API_KEY=your_api_key_here`\n\n## Usage\n\n1. Configure your MCP settings file (e.g., `claude_desktop_config.json`) to include the Brave Search MCP server:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"brave-search\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"path-to\\\\mcp-python\\\\mcp-brave-search\\\\src\",\n           \"run\",\n           \"server.py\"\n         ],\n         \"env\": {\n           \"BRAVE_API_KEY\": \"YOUR_BRAVE_API_KEY_HERE\"\n         }\n       }\n     }\n   }\n   ```\n\n   Replace `YOUR_BRAVE_API_KEY_HERE` with your actual Brave API key.\n\n2. Start the Brave Search MCP server by running your MCP-compatible AI assistant with the updated configuration.\n\n3. The server will now be running and ready to accept requests from MCP clients.\n\n4. You can now use the Brave Search functionality in your MCP-compatible AI assistant (like Claude) by invoking the available tools.\n\n## Available Tools\n\nThe server provides two main tools:\n\n1. `brave_web_search`: Performs a web search using the Brave Search API.\n2. `brave_local_search`: Searches for local businesses and places.\n\nRefer to the tool docstrings in `src/server.py` for detailed usage information.\n\n## Development\n\nTo make changes to the project:\n\n1. Modify the code in the `src` directory as needed.\n2. Update the `requirements.txt` file if you add or remove dependencies:\n   ```\n   uv pip freeze > requirements.txt\n   ```\n3. Restart the server to apply changes.\n\n## Testing\n\nThe project includes both unit tests and integration tests:\n\n### Installing Test Dependencies\n\n```bash\nuv pip install pytest pytest-asyncio pytest-cov\n```\n\n### Running Unit Tests\n\nUnit tests can be run without an API key and use mocks to simulate API responses:\n\n```bash\n# Run all unit tests\npython -m pytest tests/unit/\n\n# Run with verbose output\npython -m pytest tests/unit/ -v\n```\n\n### Running Integration Tests\n\nIntegration tests require a valid Brave API key and make real API calls:\n\n```bash\n# Run integration tests with your API key\nBRAVE_API_KEY_INTEGRATION=\"your_api_key_here\" python -m pytest tests/integration/ -v\n```\n\n### Test Coverage\n\nTo check test coverage:\n\n```bash\npython -m pytest --cov=src/mcp_brave_search\n```\n\n## Troubleshooting\n\nIf you encounter any issues:\n\n1. Ensure your Brave API key is correctly set.\n2. Check that all dependencies are installed.\n3. Verify that you're using a compatible Python version.\n4. If you make changes to the code, make sure to restart the server.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "mcp",
        "brave search",
        "search utilizes",
        "web search"
      ],
      "category": "web-search"
    },
    "tisDDM--searxng-mcp": {
      "owner": "tisDDM",
      "name": "searxng-mcp",
      "url": "https://github.com/tisDDM/searxng-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tisDDM.webp",
      "description": "Connect AI assistants to SearXNG, a privacy-focused metasearch engine that supports both public and private instances. Customize search parameters for flexible web searching with no initial configuration required.",
      "stars": 22,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T16:50:54Z",
      "readme_content": "# SearXNG MCP Server\n\n<p align=\"center\">\n  \n</p>\n\nA Model Context Protocol (MCP) server that enables AI assistants to perform web searches using [SearXNG](https://github.com/searxng/searxng), a privacy-respecting metasearch engine. Works out-of-the-box with zero additional deployment by automatically selecting a random instance from [SearX.space](https://searx.space/), while also supporting private instances with basic authentication.\n\n## Features\n\n- **Zero-configuration setup**: Works immediately by using a random public instance from [SearX.space](https://searx.space/)\n- **Private instance support**: Connect to your own SearXNG instance with optional basic authentication\n- Perform web searches with customizable parameters\n- Support for multiple search engines\n- Privacy-focused search results\n- Markdown-formatted search results\n- Sensible default values for all parameters\n\n**CAVEAT - Public Instances might be unavailabe for this purpose and return \"Request failed with status code 429\"**\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v16 or higher)\n- npm (v7 or higher)\n- Access to a SearXNG instance (self-hosted or public)\n\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/tisDDM/searxng-mcp.git\ncd searxng-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\nThe SearXNG MCP server can be configured with the following environment variables:\n\n- `SEARXNG_URL` (optional): The URL of your SearXNG instance (e.g., `https://searx.example.com`). If not provided, a random public instance from [SearX.space](https://searx.space/) will be automatically selected, making the server usable with zero additional deployment.\n- `USE_RANDOM_INSTANCE` (optional): Set to \"false\" to disable random instance selection when no URL is provided. Default is \"true\".\n- `SEARXNG_USERNAME` (optional): Username for basic authentication when connecting to a private instance\n- `SEARXNG_PASSWORD` (optional): Password for basic authentication when connecting to a private instance\n\nYou can set these environment variables in a `.env` file in the root directory of the project:\n\n```\nSEARXNG_URL=https://searx.example.com\nSEARXNG_USERNAME=your_username\nSEARXNG_PASSWORD=your_password\n```\n\n## Usage\n\n### Running the server\n\n```bash\n# If installed globally\nsearxngmcp\n\n# If installed from source\nnode build/index.js\n```\n\n### Integrating with Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings > MCP Servers\n3. Add a new MCP server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxngmcp\": {\n         \"command\": \"searxngmcp\",\n         \"env\": {\n           // Optional: If not provided, a random public instance will be used\n           \"SEARXNG_URL\": \"https://searx.example.com\",\n           // Optional: Only needed for private instances with authentication\n           \"SEARXNG_USERNAME\": \"your_username\",\n           \"SEARXNG_PASSWORD\": \"your_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n### Integrating with Claude in VSCode\n\n1. Open VSCode\n2. Go to Settings > Extensions > Claude > MCP Settings\n3. Add a new MCP server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxngmcp\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/searxng-mcp/build/index.js\"],\n         \"env\": {\n           // Optional: If not provided, a random public instance will be used\n           \"SEARXNG_URL\": \"https://searx.example.com\",\n           // Optional: Only needed for private instances with authentication\n           \"SEARXNG_USERNAME\": \"your_username\",\n           \"SEARXNG_PASSWORD\": \"your_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n## Usage with Smolagents\n\nSearXNG MCP can be easily integrated with Smolagents, a lightweight framework for building AI agents. This allows you to create powerful research agents that can search the web and process the results:\n\n```python\nfrom smolagents import CodeAgent, LiteLLMModel, ToolCollection\nfrom mcp import StdioServerParameters\n\n# Configure the SearXNG MCP server\nserver_parameters = StdioServerParameters(\n    command=\"node\",\n    args=[\"path/to/searxng-mcp/build/index.js\"],\n    env={\n        \"SEARXNG_URL\": \"https://your-searxng-instance.com\",\n        \"SEARXNG_USERNAME\": \"your_username\",  # Optional\n        \"SEARXNG_PASSWORD\": \"your_password\"   # Optional\n    }\n)\n\n# Create a tool collection from the MCP server\nwith ToolCollection.from_mcp(server_parameters) as tool_collection:\n    # Initialize your LLM model\n    model = LiteLLMModel(\n        model_id=\"your-model-id\",\n        api_key=\"your-api-key\",\n        temperature=0.7\n    )\n    \n    # Create an agent with the search tools\n    search_agent = CodeAgent(\n        name=\"search_agent\",\n        tools=tool_collection.tools,\n        model=model\n    )\n    \n    # Run the agent with a search prompt\n    result = search_agent.run(\n        \"Perform a search about: 'climate change solutions' and summarize the top 5 results.\"\n    )\n    \n    print(result)\n```\n\n## Available Tools\n\n### searxngsearch\n\nPerform web searches using SearXNG, a privacy-respecting metasearch engine. Returns relevant web content with customizable parameters.\n\n#### Parameters\n\n| Parameter   | Type             | Description                                                                      | Default     | Required |\n|-------------|------------------|----------------------------------------------------------------------------------|-------------|---------|\n| query       | string           | Search query                                                                     | -           | Yes      |\n| language    | string           | Language code for search results (e.g., 'en', 'de', 'fr')                        | 'en'        | No       |\n| time_range  | string           | Time range for search results. Options: 'day', 'week', 'month', 'year'           | null        | No       |\n| categories  | array of strings | Categories to search in (e.g., 'general', 'images', 'news')                      | null        | No       |\n| engines     | array of strings | Specific search engines to use                                                   | null        | No       |\n| safesearch  | number           | Safe search level: 0 (off), 1 (moderate), 2 (strict)                             | 1           | No       |\n| pageno      | number           | Page number for results. Must be minimum 1                                       | 1           | No       |\n| max_results | number           | Maximum number of search results to return. Range: 1-50                          | 10          | No       |\n\n#### Example\n\n```javascript\n// Example request\nconst result = await client.callTool('searxngsearch', {\n  query: 'climate change solutions',\n  language: 'en',\n  time_range: 'year',\n  categories: ['general', 'news'],\n  safesearch: 1,\n  max_results: 5\n});\n```\n\n## Development\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/tisDDM/searxng-mcp.git\ncd searxng-mcp\n\n# Install dependencies\nnpm install\n```\n\n### Build\n\n```bash\nnpm run build\n```\n\n### Watch mode (for development)\n\n```bash\nnpm run watch\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpm run inspector\n```\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/searxng-mcp",
      "npm_downloads": 264,
      "keywords": [
        "metasearch",
        "searxng",
        "searching",
        "searxng privacy",
        "metasearch engine",
        "tisddm searxng"
      ],
      "category": "web-search"
    },
    "tizee--mcp-server-ietf": {
      "owner": "tizee",
      "name": "mcp-server-ietf",
      "url": "https://github.com/tizee/mcp-server-ietf",
      "imageUrl": "/freedevtools/mcp/pfp/tizee.webp",
      "description": "Access and retrieve IETF RFC documents, enabling search by keywords and management of document pagination. Provides standardized access to essential specifications for Large Language Models.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T16:37:54Z",
      "readme_content": "# MCP-Server-IETF\n\nA Model Context Protocol server for fetching IETF documents (RFCs) for Large Language Models.\n\n## Overview\n\nThis project implements a [Model Context Protocol (MCP)](https://modelcontextprotocol.github.io/) server that provides access to IETF RFC documents. It enables Large Language Models to access RFC specifications through a standardized interface.\n\nKey features:\n- Download and cache RFC index and documents\n- Search RFCs by keyword in titles\n- Access RFC documents with pagination support\n- Extract metadata like page numbers from documents\n\n## Installation\n\n### Requirements\n- Python 3.11 or higher\n- Dependencies as listed in `pyproject.toml`\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/tizee/mcp-server-ietf\ncd mcp-server-ietf\n\n# Install with pip\npip install -e .\n```\n\n## Usage\n\n### Starting the server\n\n```bash\n# Start the server\nmcp-server-ietf\n```\n\nOr use it with the MCP inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run mcp-server-ietf\n```\n\n### Available Tools\n\nWhen connected to the server, the following tools are available:\n\n#### `list_docs_number`\nGet the total number of RFC documents available in the index.\n\n#### `get_doc`\nGet an RFC document by its number with pagination support.\n\nParameters:\n- `number`: The RFC number (e.g., \"1234\")\n- `start_line`: The line number to start from (default: 1)\n- `max_lines`: Maximum number of lines to return (default: 200)\n\n#### `search_rfc_by_keyword`\nSearch for RFC documents by keyword in their titles.\n\nParameters:\n- `keyword`: The search term to look for in RFC titles\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Install development dependencies\nuv install -e .[dev]\n```\n\nRun inspector with Makefile:\n\n```\nmake dev\n```\n\n### Running Tests\n\n```bash\n# Run tests\nuv run pytest\n```\n\nOr using the Makefile:\n\n```bash\nmake test\n```\n\n### Cache Location\n\nBy default, the server caches RFC documents and the index at `~/.cache/ietf-doc-server`.\n\n### Environment Variables\n\n- `LOG_LEVEL`: Set the logging level (default: \"DEBUG\")\n\n## License\n\nMIT License - See `LICENSE` file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rfc",
        "search",
        "pagination",
        "ietf rfc",
        "rfc documents",
        "retrieve ietf"
      ],
      "category": "web-search"
    },
    "tokenizin-agency--mcp-npx-fetch": {
      "owner": "tokenizin-agency",
      "name": "mcp-npx-fetch",
      "url": "https://github.com/tokenizin-agency/mcp-npx-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/tokenizin-agency.webp",
      "description": "Fetches and processes web content, transforming it into multiple formats such as HTML, JSON, Markdown, and plain text. It operates using the Model Context Protocol for seamless integration with AI models.",
      "stars": 37,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T22:50:35Z",
      "readme_content": "# MCP NPX Fetch\n\n<div align=\"center\">\n\n[![npm version](https://img.shields.io/npm/v/@tokenizin/mcp-npx-fetch.svg)](https://www.npmjs.com/package/@tokenizin/mcp-npx-fetch)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-blue.svg)](https://www.typescriptlang.org/)\n[![Model Context Protocol](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://github.com/modelcontextprotocol)\n\nA powerful MCP server for fetching and transforming web content into various formats (HTML, JSON, Markdown, Plain Text) with ease.\n\n[Installation](#installation) •\n[Features](#features) •\n[Usage](#usage) •\n[Documentation](#documentation) •\n[Contributing](#contributing)\n\n</div>\n\n<a href=\"https://glama.ai/mcp/servers/m2a0ue08n2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/m2a0ue08n2/badge\" alt=\"NPX Fetch MCP server\" /></a>\n\n---\n\n## 🚀 Features\n\n- 🌐 **Universal Content Fetching**: Supports HTML, JSON, plain text, and Markdown formats\n- 🔒 **Custom Headers Support**: Add authentication and custom headers to your requests\n- 🛠 **Built-in Transformations**: Automatic conversion between formats\n- ⚡ **High Performance**: Built with modern JavaScript features and optimized for speed\n- 🔌 **MCP Compatible**: Seamlessly integrates with Claude Desktop and other MCP clients\n- 🎯 **Type-Safe**: Written in TypeScript with full type definitions\n\n## 📦 Installation\n\n### NPM Global Installation\n\n```bash\nnpm install -g @tokenizin/mcp-npx-fetch\n\n```\n\n### Direct Usage with NPX\n\n```bash\nnpx @tokenizin/mcp-npx-fetch\n```\n\n## 📚 Documentation\n\n### Available Tools\n\n#### `fetch_html`\n\nFetches and returns raw HTML content from any URL.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_json`\n\nFetches and parses JSON data from any URL.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_txt`\n\nFetches and returns clean plain text content, removing HTML tags and scripts.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_markdown`\n\nFetches content and converts it to well-formatted Markdown.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n## 🔧 Usage\n\n### CLI Usage\n\nStart the MCP server directly:\n\n```bash\nmcp-npx-fetch\n```\n\nOr via npx:\n\n```bash\nnpx @tokenizin/mcp-npx-fetch\n```\n\n### Claude Desktop Integration\n\n1. Locate your Claude Desktop configuration file:\n\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the following configuration to your `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@tokenizin/mcp-npx-fetch\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## 💻 Local Development\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/tokenizin-agency/mcp-npx-fetch.git\ncd mcp-npx-fetch\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Start development mode:\n\n```bash\nnpm run dev\n```\n\n4. Run tests:\n\n```bash\nnpm test\n```\n\n## 🛠 Technical Stack\n\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk) - Core MCP functionality\n- [JSDOM](https://github.com/jsdom/jsdom) - HTML parsing and manipulation\n- [Turndown](https://github.com/mixmark-io/turndown) - HTML to Markdown conversion\n- [TypeScript](https://www.typescriptlang.org/) - Type safety and modern JavaScript features\n- [Zod](https://github.com/colinhacks/zod) - Runtime type validation\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\nMade with ❤️ by <a href=\"https://github.com/tokenizin-agency\">PT Tokenizin Technology Agency</a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "npx",
        "tokenizin",
        "search tokenizin",
        "web search",
        "mcp npx"
      ],
      "category": "web-search"
    },
    "traylinx--traylinx-search-engine-mcp-server": {
      "owner": "traylinx",
      "name": "traylinx-search-engine-mcp-server",
      "url": "https://github.com/traylinx/traylinx-search-engine-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/traylinx.webp",
      "description": "Connect to the deployed Agentic Search API to perform web searches that yield detailed and contextually relevant results, including text summaries, HTML, and media content. Integrate seamlessly with MCP clients for enhanced search functionality.",
      "stars": 0,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-03T15:04:08Z",
      "readme_content": "# Traylinx Search Engine MCP Server\n\n[![smithery badge](https://smithery.ai/badge/traylinx/traylinx-search-engine-mcp-server)](https://smithery.ai/server/traylinx/traylinx-search-engine-mcp-server)\n\nA Model Context Protocol (MCP) server that acts as a bridge to the deployed **Agentic Search API**. It allows MCP clients like Claude Desktop and Cursor to utilize intelligent search capabilities with both text summaries and structured data (HTML, images, and more).\n\n## Tools\n\n### `search`\nPerform a web search using Traylinx's API, which provides detailed and contextually relevant results with citations. By default, no time filtering is applied to search results.\n\n**Inputs:**\n- `query` (string): The search query to perform.\n- `search_recency_filter` (string, optional): Filter search results by recency. Options: \"month\", \"week\", \"day\", \"hour\". If not specified, no time filtering is applied.\n\n## How it Works\n\n1. You configure this MCP server with your Agentic Search API URL and API Key (via environment variables passed by the client config).\n2. An MCP client (e.g., Claude) sends a tool call to this server with a search query and optional recency filter.\n3. This MCP server makes a request to the Agentic Search API with the query and authorization header.\n4. It parses the rich response (text, HTML, search results, media, news) and returns structured content to the MCP client.\n\n## Installation\n\n### Prerequisites\n\n* Node.js >= 18.0.0\n* An API Key from Traylinx.com\n\n### Step 1: Get an API Key from Traylinx\n\n1. Visit [traylinx.com](https://traylinx.com) and sign up for an account\n2. Navigate to the developer dashboard/API section\n3. Generate your API key for the Agentic Search API\n4. Keep this key secure - you'll need it for configuration\n\n### Step 2: Set Up the MCP Server\n\n```bash\n# Clone the repository\ngit clone https://github.com/traylinx/traylinx-search-engine-mcp-server.git\ncd traylinx-search-engine-mcp-server\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n### Step 3: Configure Your MCP Client\n\n#### For Claude Desktop\n\nEdit your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"traylinx-search-engine-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/traylinx-search-engine-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"AGENTIC_SEARCH_API_KEY\": \"sk-lf-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n        \"AGENTIC_SEARCH_API_URL\": \"https://agentic-search-engines-n3n7u.ondigitalocean.app\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\nYou can access this file at:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### For Cursor\n\nEdit your `mcp.json` file:\n\n```json\n{\n  \"traylinx-search-engine-mcp-server\": {\n    \"env\": {\n      \"AGENTIC_SEARCH_API_KEY\": \"sk-lf-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n      \"AGENTIC_SEARCH_API_URL\": \"https://agentic-search-engines-n3n7u.ondigitalocean.app\",\n      \"LOG_LEVEL\": \"INFO\"\n    },\n    \"command\": \"node\",\n    \"args\": [\"path/to/traylinx-search-engine-mcp-server/dist/index.js\"]\n  }\n}\n```\n\n**IMPORTANT: Replace the placeholder API key with your actual key from Traylinx.com**\n\n## Verification\n\n1. After configuring your MCP client, restart it completely.\n2. Start a new chat and instruct it to use the tool:\n   - \"Use the search tool to find information about quantum computing.\"\n   - \"Search for the latest news about artificial intelligence and filter by last week.\"\n   - \"Extract text and HTML from the URL https://traylinx.com\"\n3. When the client requests permission, grant it.\n4. You should receive a response containing both text content and potentially structured data.\n\n## Advanced Usage\n\nThe Traylinx Search Engine MCP Server supports multiple response types:\n\n* **Text Content**: Standard markdown text summarizing the search results\n* **Embedded HTML**: For URL extractions, the server can return the scraped HTML\n* **Search Items**: Structured search results with title, URL, and snippet\n* **Media Items**: Images, videos, and other media found during the search\n* **News Articles**: Recent news with thumbnails and metadata\n* **Raw API Response**: Complete response data for advanced use cases\n\n### Using the Recency Filter\n\nTo filter search results by recency:\n\n```\n// Example from Claude Desktop\nUse the search tool to find recent news about SpaceX with results from the last day only.\n\n// Example from a custom client\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"SpaceX launches\",\n    \"search_recency_filter\": \"week\"\n  }\n}\n```\n\n## Features\n\n* **Rich Content Types**: Returns multiple content types beyond just text\n* **Time Filtering**: Filter results by recency (month, week, day, hour)\n* **Secure API Key Handling**: API key stays in environment variables\n* **Configurable Endpoint**: Easily switch between API endpoints if needed\n* **Full MCP Compliance**: Implements all required MCP server methods\n\n## Deployment\n\n### Smithery.ai Deployment\n\nThis MCP server can be deployed to [Smithery.ai](https://smithery.ai):\n\n1. Create/login to your Smithery account\n2. Click \"Deploy a New MCP Server\"\n3. Enter ID: `traylinx-search-engine-mcp-server`  \n4. Use base directory: `.` (dot for root)\n5. Click \"Create\"\n\nOnce deployed, you can reference this server in Claude's web interface by using:\n```\nUse the traylinx-search-engine-mcp-server to search for [your query]\n```\n\n**Note:** You'll need to provide your `AGENTIC_SEARCH_API_KEY` as an environment variable in the Smithery deployment settings.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check your API key is correctly set in the configuration\n2. Ensure the MCP client has been fully restarted after configuration\n3. Verify network connectivity to the Agentic Search API\n4. Set `LOG_LEVEL` to `DEBUG` for more detailed logs\n\nFor additional support, contact the API provider at support@traylinx.com\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "agentic",
        "agentic search",
        "search traylinx",
        "traylinx search"
      ],
      "category": "web-search"
    },
    "trilogy-group--youtube-summarizer-mcp": {
      "owner": "trilogy-group",
      "name": "youtube-summarizer-mcp",
      "url": "https://github.com/trilogy-group/youtube-summarizer-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/trilogy-group.webp",
      "description": "Integrates Youtube summarization capabilities into AI applications, providing access to APIs that summarize videos directly from YouTube.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:49Z",
      "readme_content": "# MCP Server\n\nMCP Server is created on top of all the APIs from the Youtube-Summarizer. All APIs are exposed as tools in the MCP protocol and available for any AI application to integrate with.\n\n**Note:** Currently MCP only supports local connections, so it doesn't support remote use of these tools.\n\n## Setup\n\n### Docker Setup\nBuild the Docker image:\n```bash\ndocker build -t youtube-summarizer-mcp .\n```\n\nRun the MCP server using Docker:\n```bash\ndocker run -i --rm youtube-summarizer-mcp\n```\n\n### Using the Inspector\nYou can use the MCP Inspector to explore available tools and test them:\n```bash\n./inspector.sh\n```\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-summarizer\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"youtube-summarizer-mcp\"\n      ]\n    }\n  }\n}\n```\n\nNow you can use the added mcp tools from server.py in claude desktop\n\n### MCP Client Sample (Without Claude Desktop)\nRun the MCP client locally to try out the Social Toolkit using natural language:\n\n### Setup\n```bash\n./setup.sh\n```\n\n### Run\n```bash\n./run.sh\n```\n\nIt will run both MCP server and client, connected to each other. The terminal will prompt for natural language queries from the user, which then will be translated into MCP tool calls to answer the user query.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "summarizer",
        "summarization",
        "summarize",
        "youtube summarizer",
        "youtube summarization",
        "summarize videos"
      ],
      "category": "web-search"
    },
    "tulong66--mcp-tavily-proxy": {
      "owner": "tulong66",
      "name": "mcp-tavily-proxy",
      "url": "https://github.com/tulong66/mcp-tavily-proxy",
      "imageUrl": "/freedevtools/mcp/pfp/tulong66.webp",
      "description": "Enables AI-powered web searches through proxy servers, providing direct answers and access to recent news articles with AI-extracted relevant content. It supports HTTP/HTTPS proxy configurations and offers robust logging and error handling for proxy issues.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-06T09:57:43Z",
      "readme_content": "# Tavily MCP Server with Proxy Support\n\nA Model Context Protocol server that provides AI-powered web search capabilities using Tavily's search API, with added support for HTTP/HTTPS proxy configurations. This server enables LLMs to perform sophisticated web searches through proxy servers, get direct answers to questions, and search recent news articles with AI-extracted relevant content.\n\n## Features\n\n- All original Tavily MCP Server features\n- HTTP/HTTPS proxy support through environment variables\n- Enhanced logging for proxy configurations\n- Robust error handling for proxy-related issues\n\n## Available Tools\n\n- `tavily_web_search` - Performs comprehensive web searches with AI-powered content extraction.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"basic\")\n\n- `tavily_answer_search` - Performs web searches and generates direct answers with supporting evidence.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"advanced\")\n\n- `tavily_news_search` - Searches recent news articles with publication dates.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `days` (integer, optional): Number of days back to search (default: 3)\n\n## Installation\n\n### Use `pip`\n\n```bash\npip install mcp-tavily-proxy\n```\n\nor if you have `uv` installed:\n\n```bash\nuv pip install mcp-tavily-proxy\n```\n\n### Build from Source\n\nClone this repository and build and install the program:\n\n```bash\ngit clone https://github.com/tulong66/mcp-tavily-proxy.git\ncd mcp-tavily-proxy\nuv build\nuv pip install .\n```\n\n## Configuration\n\n### API Key and Proxy Settings\n\nThe server requires a Tavily API key and supports proxy configuration through environment variables:\n\n1. Set required environment variables:\n```bash\n# Tavily API Key\nexport TAVILY_API_KEY=your_api_key_here\n\n# Proxy Settings (if needed)\nexport HTTP_PROXY=http://your-proxy:port\nexport HTTPS_PROXY=http://your-proxy:port\n```\n\n2. Or provide API key as a command-line argument:\n```bash\npython -m mcp_server_tavily --api-key=your_api_key_here\n```\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_server_tavily\"]\n    },\n    \"env\": {\n      \"TAVILY_API_KEY\": \"your_api_key_here\",\n      \"HTTP_PROXY\": \"http://your-proxy:port\",\n      \"HTTPS_PROXY\": \"http://your-proxy:port\"\n    }\n  }\n}\n```\n\n## Examples\n\nFor a regular search:\n```\nTell me about Anthropic's newly released MCP protocol\n```\n\nTo generate a report with explicit exclusions:\n```\nTell me about redwood trees. Please use MLA format in markdown syntax and include the URLs in the citations. Exclude Wikipedia sources.\n```\n\nFor news search:\n```\nGive me the top 10 AI-related news in the last 5 days\n```\n\n## Debugging\n\nEnable debug logging to see detailed proxy configuration information:\n\n```bash\nexport TAVILY_LOG_LEVEL=DEBUG\npython -m mcp_server_tavily\n```\n\nYou can also use the MCP inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit pull requests to help improve the proxy support or add new features.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\nThis project is based on the original [mcp-tavily](https://github.com/RamXX/mcp-tavily) with added proxy support functionality.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proxy",
        "searches",
        "http",
        "searches proxy",
        "tavily proxy",
        "proxy enables"
      ],
      "category": "web-search"
    },
    "vinhphamai23--brave-search": {
      "owner": "vinhphamai23",
      "name": "brave-search",
      "url": "https://github.com/vinhphamai23/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/vinhphamai23.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, enabling general web queries, news searches, and local business searches with detailed results and filtering options.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-22T03:56:01Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n#### Docker\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"BRAVE_API_KEY\",\n          \"mcp/brave-search\"\n        ],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "api",
        "search",
        "search api",
        "brave search",
        "web search"
      ],
      "category": "web-search"
    },
    "w-jeon--mcp-brave-search": {
      "owner": "w-jeon",
      "name": "mcp-brave-search",
      "url": "https://github.com/w-jeon/mcp-brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/w-jeon.webp",
      "description": "Integrates the Brave Search API for web and local search capabilities, enabling detailed queries and results filtering. Offers real-time processing with smart fallbacks between local and web search.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-12T03:34:08Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n\n## Configuration\n\n### Getting an API Key\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "api",
        "web",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "weidwonder--crawl4ai-mcp-server": {
      "owner": "weidwonder",
      "name": "crawl4ai-mcp-server",
      "url": "https://github.com/weidwonder/crawl4ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/weidwonder.webp",
      "description": "Provides powerful search capabilities optimized for LLMs to understand and extract valuable content from web pages using multiple search engines. Efficiently filters non-core content and formats outputs for AI processing.",
      "stars": 118,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T04:12:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/weidwonder-crawl4ai-mcp-server-badge.png)](https://mseep.ai/app/weidwonder-crawl4ai-mcp-server)\n\n# Crawl4AI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@weidwonder/crawl4ai-mcp-server)](https://smithery.ai/server/@weidwonder/crawl4ai-mcp-server)\n\n这是一个基于MCP (Model Context Protocol)的智能信息获取服务器,为AI助手系统提供强大的搜索能力和面向LLM优化的网页内容理解功能。通过多引擎搜索和智能内容提取,帮助AI系统高效获取和理解互联网信息,将网页内容转换为最适合LLM处理的格式。\n\n## 特性\n\n- 🔍 强大的多引擎搜索能力,支持DuckDuckGo和Google\n- 📚 面向LLM优化的网页内容提取,智能过滤非核心内容\n- 🎯 专注信息价值,自动识别和保留关键内容\n- 📝 多种输出格式,支持引用溯源\n- 🚀 基于FastMCP的高性能异步设计\n\n## 安装\n\n### 方式1: 大部分的安装场景\n\n1. 确保您的系统满足以下要求:\n   - Python >= 3.9\n   - 建议使用专门的虚拟环境\n\n2. 克隆仓库:\n```bash\ngit clone https://github.com/yourusername/crawl4ai-mcp-server.git\ncd crawl4ai-mcp-server\n```\n\n3. 创建并激活虚拟环境:\n```bash\npython -m venv crawl4ai_env\nsource crawl4ai_env/bin/activate  # Linux/Mac\n# 或\n.\\crawl4ai_env\\Scripts\\activate  # Windows\n```\n\n4. 安装依赖:\n```bash\npip install -r requirements.txt\n```\n\n5. 安装playwright浏览器:\n```bash\nplaywright install\n```\n\n### 方式2: 安装到Claude桌面客户端 via Smithery\n\n通过 [Smithery](https://smithery.ai/server/@weidwonder/crawl4ai-mcp-server) 将 Crawl4AI MCP 的 Claude 桌面端服务安装自动配置至您本地的 `Claude 伸展中心`:\n\n```bash\nnpx -y @smithery/cli install @weidwonder/crawl4ai-mcp-server --client claude\n```\n\n## 使用方法\n\n服务器提供以下工具:\n\n### search\n强大的网络搜索工具,支持多个搜索引擎:\n\n- DuckDuckGo搜索(默认): 无需API密钥,全面处理AbstractText、Results和RelatedTopics\n- Google搜索: 需要配置API密钥,提供精准搜索结果\n- 支持同时使用多个引擎获取更全面的结果\n\n参数说明:\n- `query`: 搜索查询字符串\n- `num_results`: 返回结果数量(默认10)\n- `engine`: 搜索引擎选择\n  - \"duckduckgo\": DuckDuckGo搜索(默认)\n  - \"google\": Google搜索(需要API密钥)\n  - \"all\": 同时使用所有可用的搜索引擎\n\n示例:\n```python\n# DuckDuckGo搜索(默认)\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5\n}\n\n# 使用所有可用引擎\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5,\n    \"engine\": \"all\"\n}\n```\n\n### read_url\n面向LLM优化的网页内容理解工具,提供智能内容提取和格式转换:\n\n- `markdown_with_citations`: 包含内联引用的Markdown(默认),保持信息溯源\n- `fit_markdown`: 经过LLM优化的精简内容,去除冗余信息\n- `raw_markdown`: 基础HTML→Markdown转换\n- `references_markdown`: 单独的引用/参考文献部分\n- `fit_html`: 生成fit_markdown的过滤后HTML\n- `markdown`: 默认Markdown格式\n\n示例:\n```python\n{\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown_with_citations\"\n}\n```\n\n示例:\n```python\n# DuckDuckGo搜索(默认)\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5\n}\n\n# Google搜索\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5,\n    \"engine\": \"google\"\n}\n```\n\n如需使用Google搜索,需要在config.json中配置API密钥:\n```json\n{\n    \"google\": {\n        \"api_key\": \"your-api-key\",\n        \"cse_id\": \"your-cse-id\"\n    }\n}\n```\n\n## LLM内容优化\n\n服务器采用了一系列针对LLM的内容优化策略:\n\n- 智能内容识别: 自动识别并保留文章主体、关键信息段落\n- 噪音过滤: 自动过滤导航栏、广告、页脚等对理解无帮助的内容\n- 信息完整性: 保留URL引用,支持信息溯源\n- 长度优化: 使用最小词数阈值(10)过滤无效片段\n- 格式优化: 默认输出markdown_with_citations格式,便于LLM理解和引用\n\n## 开发说明\n\n项目结构:\n```\ncrawl4ai_mcp_server/\n├── src/\n│   ├── index.py      # 服务器主实现\n│   └── search.py     # 搜索功能实现\n├── config_demo.json  # 配置文件示例\n├── pyproject.toml    # 项目配置\n├── requirements.txt  # 依赖列表\n└── README.md        # 项目文档\n```\n\n## 配置说明\n\n1. 复制配置示例文件:\n```bash\ncp config_demo.json config.json\n```\n\n2. 如需使用Google搜索,在config.json中配置API密钥:\n```json\n{\n    \"google\": {\n        \"api_key\": \"your-google-api-key\",\n        \"cse_id\": \"your-google-cse-id\"\n    }\n}\n```\n\n## 更新日志\n\n- 2025.02.08: 添加搜索功能,支持DuckDuckGo(默认)和Google搜索\n- 2025.02.07: 重构项目结构,使用FastMCP实现,优化依赖管理\n- 2025.02.07: 优化内容过滤配置,提高token效率并保持URL完整性\n\n## 许可证\n\nMIT License\n\n## 贡献\n\n欢迎提交Issue和Pull Request!\n\n## 作者\n\n- Owner: weidwonder  \n- Coder: Claude Sonnet 3.5 \n    - 100% Code wrote by Claude. Cost: $9 ($2 for code writing, $7 cost for Debuging😭)\n    - 3 hours time cost. 0.5 hours for code writing, 0.5 hours for env preparing, 2 hours for debuging.😭\n\n## 致谢\n\n感谢所有为项目做出贡献的开发者!\n\n特别感谢:\n- [Crawl4ai](https://github.com/crawl4ai/crawl4ai) 项目提供的优秀网页内容提取技术支持",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "crawl4ai",
        "web",
        "search engines",
        "crawl4ai mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "wendellbigato--mcp-cep": {
      "owner": "wendellbigato",
      "name": "mcp-cep",
      "url": "https://github.com/wendellbigato/mcp-cep",
      "imageUrl": "/freedevtools/mcp/pfp/wendellbigato.webp",
      "description": "Provides postal code (CEP) address lookups using the ViaCEP API. Integrates with command-line tools for efficient address retrieval tasks.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T12:12:49Z",
      "readme_content": "# MCP-CEP\n\nServidor MCP para consulta de CEPs usando a API pública do [ViaCEP](https://viacep.com.br).  \nCompatível com [Goose](https://block.github.io/goose/) como extensão de linha de comando (Command-line Extension).\n\n---\n\n### 👤 Autor\n\n**Wendell Barreto**  \n[https://github.com/wendellbigato](https://github.com/wendellbigato)  \n\n\n---\n\n## 🚀 Instalação\n\n### 1. Clone este repositório\n\n```bash\ngit clone https://github.com/wendellbigato/mcp-cep.git\ncd mcp-cep\n````\n\n> Altere a URL acima para a real quando publicar.\n\n---\n\n### 2. Crie e ative o ambiente virtual\n\n```bash\npython3.11 -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# ou\n.venv\\Scripts\\activate.bat  # Windows\n```\n\n---\n\n### 3. Instale as dependências com `uv` ou `pip`\n\nUsando [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv pip install -e \".[cli]\"\n```\n\nOu com pip normal:\n\n```bash\npip install -e \".[cli]\"\n```\n\n---\n\n## 🧩 Configurando como extensão no Goose\n\n1. Execute:\n\n```bash\ngoose configure\n```\n\n2. Selecione `Add Extension`\n\n3. Escolha `Command-line Extension`\n\n4. Preencha os campos:\n\n| Campo              | Valor                                                                     |\n| ------------------ | ------------------------------------------------------------------------- |\n| **Extension name** | `mcp-cep`                                                                 |\n| **Command to run** | `/caminho/completo/para/uv --directory /caminho/para/mcp-cep run main.py` |\n| **Timeout**        | `300`                                                                     |\n| **Environment**    | *(em branco, ou personalize se necessário)*                               |\n\nUse `which uv` e `pwd` para encontrar os caminhos corretos.\n\n---\n\n## ✅ Como testar no Goose\n\nApós configurar, inicie:\n\n```bash\ngoose \n```\n\nE envie comandos como:\n\n```\nRepita: Olá!\nQual o endereço do CEP 01001000?\n```\n\n---\n\n## 🧰 Ferramentas disponíveis\n\n* `echo(texto: str)`: repete o texto enviado.\n* `buscar_cep(cep: str)`: consulta informações de endereço via ViaCEP.\n\n---\n\n## 📄 Licença\n\nEste projeto está licenciado sob a licença MIT.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postal",
        "address",
        "lookups",
        "address lookups",
        "cep address",
        "address retrieval"
      ],
      "category": "web-search"
    },
    "williamvd4--web-search": {
      "owner": "williamvd4",
      "name": "web-search",
      "url": "https://github.com/williamvd4/web-search",
      "imageUrl": "/freedevtools/mcp/pfp/williamvd4.webp",
      "description": "Seamlessly search the web and retrieve structured results including titles, URLs, and descriptions from Google search without requiring API keys. The server is configurable for the number of results returned per search.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "",
      "updated_at": "2025-08-23T08:40:13Z",
      "readme_content": "# Web Search MCP Server\n\nA Model Context Protocol (MCP) server that enables free web searching using Google search results, with no API keys required.\n\n## Features\n\n- Search the web using Google search results\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n\n## Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\nFor VSCode (Claude Dev Extension):\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `search` that accepts the following parameters:\n\n```typescript\n{\n  \"query\": string,    // The search query\n  \"limit\": number     // Optional: Number of results to return (default: 5, max: 10)\n}\n```\n\nExample usage:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"your search query\",\n    limit: 3  // optional\n  }\n})\n```\n\nExample response:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description of the search result...\"\n  }\n]\n```\n\n## Limitations\n\nSince this tool uses web scraping of Google search results, there are some important limitations to be aware of:\n\n1. **Rate Limiting**: Google may temporarily block requests if too many searches are performed in a short time. To avoid this:\n   - Keep searches to a reasonable frequency\n   - Use the limit parameter judiciously\n   - Consider implementing delays between searches if needed\n\n2. **Result Accuracy**: \n   - The tool relies on Google's HTML structure, which may change\n   - Some results might be missing descriptions or other metadata\n   - Complex search operators may not work as expected\n\n3. **Legal Considerations**:\n   - This tool is intended for personal use\n   - Respect Google's terms of service\n   - Consider implementing appropriate rate limiting for your use case\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n",
      "npm_url": "https://www.npmjs.com/package/web-search",
      "npm_downloads": 629,
      "keywords": [
        "search",
        "williamvd4",
        "web",
        "search web",
        "williamvd4 web",
        "web search"
      ],
      "category": "web-search"
    },
    "wllcnm--mcp-reddit": {
      "owner": "wllcnm",
      "name": "mcp-reddit",
      "url": "https://github.com/wllcnm/mcp-reddit",
      "imageUrl": "/freedevtools/mcp/pfp/wllcnm.webp",
      "description": "Interact with Reddit content through AI assistance, enabling search, analysis, and retrieval of posts and comments from specific subreddits. Users can browse and analyze popular posts efficiently.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T12:25:33Z",
      "readme_content": "# MCP Reddit Server\n\n[English](README-en.md) | [中文说明](README.md)\n\n<h2 id=\"chinese\">中文说明</h2>\n\n## 1. 项目介绍\n\n这是一个基于MCP（Model Context Protocol）协议的Reddit服务器，专门设计用于与Claude等大语言模型进行交互。通过这个服务，你可以让AI助手帮助你浏览和分析Reddit上的内容。\n\n### 1.1 主要功能\n\n- 搜索特定subreddit中的帖子\n- 获取帖子详细信息和评论\n- 浏览subreddit中的热门帖子\n\n### 1.2 项目结构\n\n```\nnangeAGICode/reddit_chat_claude/\n├── .github/\n│   └── workflows/\n│       └── docker.yml # GitHub Actions工作流配置\n├── src/\n│   ├── init.py\n│   └── server.py # MCP服务器核心代码\n├── .gitignore # Git忽略文件配置\n├── Dockerfile # Docker构建文件\n├── LICENSE # MIT许可证\n├── README.md # 项目说明文档\n└── requirements.txt # Python依赖包列表\n```\n\n## 2. 安装和配置\n\n### 2.1 前提条件\n\n- Docker（必须）\n- Python 3.12+（用于本地开发）\n- Reddit API凭证（必需）\n\n### 2.2 获取Reddit API凭证\n\n1. 访问 https://www.reddit.com/prefs/apps\n2. 点击\"create another app...\"\n3. 选择\"script\"\n4. 填写必要信息\n5. 获取client_id和client_secret\n\n### 2.3 环境变量配置\n\n需要设置以下环境变量：\n\n```bash\nREDDIT_CLIENT_ID=你的client_id\nREDDIT_CLIENT_SECRET=你的client_secret\nREDDIT_USER_AGENT=你的user_agent\n```\n### 2.4 Docker安装\nMAC用户：\n1.访问 Docker 官网：https://www.docker.com/products/docker-desktop\n2.点击 \"Download for Mac\"\n3.选择对应你的 Mac 芯片的版本（Apple Silicon 或 Intel）\n4.下载并安装 .dmg 文件\n\n5.验证安装：\n```bash\n# 检查 Docker 版本\ndocker --version\n\n# 运行测试容器\ndocker run hello-world\n```\n6.确保 Docker 服务正在运行：\n```bash\n# 检查 Docker 服务状态\ndocker ps\n```\n\n\n\n\n\n\n\n## 3. 使用方法\n\n### 3.1 在Claude桌面客户端中使用\n\n在你的`claude_desktop_config.json`中添加以下配置：\n\n```json\n{\n  \"mcpServers\": {\n    \"reddit\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"REDDIT_CLIENT_ID=你的client_id\",\n        \"-e\", \"REDDIT_CLIENT_SECRET=你的client_secret\",\n        \"-e\", \"REDDIT_USER_AGENT=你的user_agent\",\n        \"ghcr.io/nangeplus/mcp-reddit:latest\"\n      ]\n    }\n  }\n}\n```\n\n### 3.2 可用工具\n\n1. **search_subreddit**: 搜索特定subreddit中的帖子\n   - 参数：\n     - `subreddit`: subreddit名称\n     - `query`: 搜索关键词\n     - `limit`: 最大返回结果数（默认：5）\n\n2. **get_post_details**: 获取特定帖子的详细信息\n   - 参数：\n     - `post_id`: Reddit帖子ID\n     - `comment_limit`: 获取评论的最大数量（默认：10）\n\n3. **get_subreddit_hot**: 获取subreddit中的热门帖子\n   - 参数：\n     - `subreddit`: subreddit名称\n     - `limit`: 返回帖子的最大数量（默认：5）\n\n### 3.3 使用示例\n\n在Claude中，你可以这样使用工具：\n\n```json\n{\n  \"tool\": \"get_subreddit_hot\",\n  \"arguments\": {\n    \"subreddit\": \"Python\",\n    \"limit\": 3\n  }\n}\n```\n\n**示例对话：**\n\n用户：帮我查看Python subreddit中最热门的3个帖子。\n\nClaude：好的，我来帮你查看。我将使用`get_subreddit_hot`工具：\n\n```json\n{\n  \"tool\": \"get_subreddit_hot\",\n  \"arguments\": {\n    \"subreddit\": \"Python\",\n    \"limit\": 3\n  }\n}\n```\n\n[Claude会返回帖子信息]\n\n用户：帮我查看第一个帖子的详细内容和评论。\n\nClaude：我将使用`get_post_details`工具：\n\n```json\n{\n  \"tool\": \"get_post_details\",\n  \"arguments\": {\n    \"post_id\": \"返回的帖子ID\"\n  }\n}\n```\n\n## 4. 本地开发\n\n### 4.1 克隆仓库\n\n```bash\ngit clone https://github.com/nangeplus/mcp-reddit.git\ncd mcp-reddit\n```\n\n### 4.2 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n### 4.3 运行服务器\n\n```bash\npython src/server.py\n```\n\n### 4.4 Docker构建\n\n```bash\ndocker build -t mcp-reddit .\ndocker run -i --rm \\\n  -e REDDIT_CLIENT_ID=你的client_id \\\n  -e REDDIT_CLIENT_SECRET=你的client_secret \\\n  -e REDDIT_USER_AGENT=你的user_agent \\\n  mcp-reddit\n```\n\n## 5. 注意事项\n\n1. **安全性**\n   - 请妥善保管你的Reddit API凭证\n   - 不要在公共场合分享你的配置文件\n   - 建议使用环境变量而不是硬编码凭证\n\n2. **使用限制**\n   - Reddit API有调用频率限制\n   - 默认返回的评论数量限制为10条\n   - 搜索结果默认限制为5条\n\n3. **故障排除**\n   - 检查API凭证是否正确\n   - 确保网络连接正常\n   - 查看日志输出了解详细错误信息\n\n## 6. 贡献指南\n\n1. Fork 项目\n2. 创建特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 开启 Pull Request\n\n## 7. 许可证\n\n本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情\n\n---\n",
      "npm_url": "https://www.npmjs.com/package/mcp-reddit-server",
      "npm_downloads": 90,
      "keywords": [
        "search",
        "retrieval",
        "web",
        "content ai",
        "web search",
        "search wllcnm"
      ],
      "category": "web-search"
    },
    "wolfyy970--docs-fetch-mcp": {
      "owner": "wolfyy970",
      "name": "docs-fetch-mcp",
      "url": "https://github.com/wolfyy970/docs-fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wolfyy970.webp",
      "description": "Fetch and explore web content autonomously by navigating through documentation and web pages to extract relevant information. It supports recursive exploration and filters navigation links for content-rich pages.",
      "stars": 7,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-30T22:42:56Z",
      "readme_content": "# Docs Fetch MCP Server\n\nA Model Context Protocol (MCP) server for fetching web content with recursive exploration capabilities. This server enables LLMs to autonomously explore web pages and documentation to learn about specific topics.\n\n## Overview\n\nThe Docs Fetch MCP Server provides a simple but powerful way for LLMs to retrieve and explore web content. It enables:\n\n- Fetching clean, readable content from any web page\n- Recursive exploration of linked pages up to a specified depth\n- Same-domain link traversal to gather comprehensive information\n- Smart filtering of navigation links to focus on content-rich pages\n\nThis tool is particularly useful when users want an LLM to learn about a specific topic by exploring documentation or web content.\n\n## Features\n\n- **Content Extraction**: Cleanly extracts the main content from web pages, removing distractions like navigation, ads, and irrelevant elements\n- **Link Analysis**: Identifies and extracts links from the page, assessing their relevance\n- **Recursive Exploration**: Follows links to related content within the same domain, up to a specified depth\n- **Parallel Processing**: Efficiently crawls content with concurrent requests and proper error handling\n- **Robust Error Handling**: Gracefully handles network issues, timeouts, and malformed pages\n- **Dual-Strategy Approach**: Uses fast axios requests first with puppeteer as a fallback for more complex pages\n- **Timeout Prevention**: Implements global timeout handling to ensure reliable operation within MCP time limits\n- **Partial Results**: Returns available content even when some pages fail to load completely\n\n## Usage\n\nThe server exposes a single MCP tool:\n\n### `fetch_doc_content`\n\nFetches web page content with the ability to explore linked pages up to a specified depth.\n\n**Parameters:**\n- `url` (string, required): URL of the web page to fetch\n- `depth` (number, optional, default: 1): Maximum depth of directory/link exploration (1-5)\n\n**Returns:**\n```json\n{\n  \"rootUrl\": \"https://example.com/docs\",\n  \"explorationDepth\": 2,\n  \"pagesExplored\": 5,\n  \"content\": [\n    {\n      \"url\": \"https://example.com/docs\",\n      \"title\": \"Documentation\",\n      \"content\": \"Main page content...\",\n      \"links\": [\n        {\n          \"url\": \"https://example.com/docs/topic1\",\n          \"text\": \"Topic 1\"\n        },\n        ...\n      ]\n    },\n    ...\n  ]\n}\n```\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/wolfyy970/docs-fetch-mcp.git\ncd docs-fetch-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Configure your MCP settings in your Claude Client:\n```json\n{\n  \"mcpServers\": {\n    \"docs-fetch\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/docs-fetch-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"pipe\"\n      }\n    }\n  }\n}\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: MCP server SDK\n- `puppeteer`: Headless browser for web page interaction\n- `axios`: HTTP client for making requests\n\n## Development\n\nTo run the server in development mode:\n\n```bash\nnpm run dev\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "web",
        "fetch",
        "web search",
        "explore web",
        "docs fetch"
      ],
      "category": "web-search"
    },
    "wong2--mcp-jina-reader": {
      "owner": "wong2",
      "name": "mcp-jina-reader",
      "url": "https://github.com/wong2/mcp-jina-reader",
      "imageUrl": "/freedevtools/mcp/pfp/wong2.webp",
      "description": "Fetches the content of a remote URL and converts it into Markdown format using Jina Reader.",
      "stars": 45,
      "forks": 10,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-15T11:38:55Z",
      "readme_content": "# Jina Reader MCP Server\n\nFetch the content of a remote URL as Markdown with Jina Reader\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "jina",
        "web",
        "jina reader",
        "mcp jina",
        "using jina"
      ],
      "category": "web-search"
    },
    "xiyuefox--firecrawl": {
      "owner": "xiyuefox",
      "name": "firecrawl",
      "url": "https://github.com/xiyuefox/firecrawl",
      "imageUrl": "/freedevtools/mcp/pfp/xiyuefox.webp",
      "description": "Provides advanced scraping, crawling, and data extraction capabilities to convert web content into structured formats. Facilitates the collection of clean data from any website for AI applications.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2025-03-21T03:52:51Z",
      "readme_content": "<h3 align=\"center\">\n  <a name=\"readme-top\"></a>\n  <img alt=\"firecrawl_logo\"\n    src=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png\"\n    height=\"200\"\n  >\n</h3>\n<div align=\"center\">\n    <a href=\"https://github.com/mendableai/firecrawl/blob/main/LICENSE\">\n  <img src=\"https://img.shields.io/github/license/mendableai/firecrawl\" alt=\"License\">\n</a>\n    <a href=\"https://pepy.tech/project/firecrawl-py\">\n  <img src=\"https://static.pepy.tech/badge/firecrawl-py\" alt=\"Downloads\">\n</a>\n<a href=\"https://GitHub.com/mendableai/firecrawl/graphs/contributors\">\n  <img src=\"https://img.shields.io/github/contributors/mendableai/firecrawl.svg\" alt=\"GitHub Contributors\">\n</a>\n<a href=\"https://firecrawl.dev\">\n  <img src=\"https://img.shields.io/badge/Visit-firecrawl.dev-orange\" alt=\"Visit firecrawl.dev\">\n</a>\n</div>\n<div>\n  <p align=\"center\">\n    <a href=\"https://twitter.com/firecrawl_dev\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Follow on X\" />\n    </a>\n    <a href=\"https://www.linkedin.com/company/104100957\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Follow on LinkedIn\" />\n    </a>\n    <a href=\"https://discord.com/invite/gSmWdAkdwd\">\n      <img src=\"https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join our Discord\" />\n    </a>\n  </p>\n</div>\n\n# 🔥 Firecrawl\n\nEmpower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.\n\n_This repository is in development, and we’re still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally._\n\n## What is Firecrawl?\n\n[Firecrawl](https://firecrawl.dev?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our [documentation](https://docs.firecrawl.dev).\n\n_Pst. hey, you, join our stargazers :)_\n\n<a href=\"https://github.com/mendableai/firecrawl\">\n  <img src=\"https://img.shields.io/github/stars/mendableai/firecrawl.svg?style=social&label=Star&maxAge=2592000\" alt=\"GitHub stars\">\n</a>\n\n## How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you'd like.\n\nCheck out the following resources to get started:\n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.\n\nTo run locally, refer to guide [here](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md).\n\n### API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.\n\n### Features\n\n- [**Scrape**](#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](#llm-extraction-beta), screenshot, html)\n- [**Crawl**](#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n- [**Map**](#map-alpha): input a website and get all the website urls - extremely fast\n- [**Extract**](#extract): get structured data from single page, multiple pages or entire websites with AI.\n\n### Powerful Capabilities\n- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...\n- **Media parsing**: pdfs, docx, images\n- **Reliability first**: designed to get the data you need - no matter how hard it is\n- **Actions**: click, scroll, input, wait and more before extracting data\n- **Batching (New)**: scrape thousands of URLs at the same time with a new async endpoint.\n\nYou can find all of Firecrawl's capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n\n### Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n```\n\nReturns a crawl job id and the url to check the status of the crawl.\n\n```json\n{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n```\n\n### Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n```bash\ncurl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n```\n\n```json\n{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\">...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n```\n\n### Scraping\n\nUsed to scrape a URL and get its content in the specified formats.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release 🚀](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[💥 Get 2 months free...\",\n    \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n```\n\n### Map (Alpha)\n\nUsed to map a URL and get urls of the website. This returns most links present on the website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n```\n\n#### Map with search\n\nMap with `search` param allows you to search for specific urls inside a website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n```\n\nResponse will be an ordered list from the most relevant to the least relevant.\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n```\n\n### Extract\n\nGet structured data from entire websites with a prompt and/or a schema.\n\nYou can extract structured data from one or multiple URLs, including wildcards:\n\nSingle Page:\nExample: https://firecrawl.dev/some-page\n\nMultiple Pages / Full Domain\nExample: https://firecrawl.dev/*\n\nWhen you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n```\n\nIf you are using the sdks, it will auto pull the response for you:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n```\n\n### LLM Extraction (Beta)\n\nUsed to extract structured data from scraped pages.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n```\n\n### Extracting without a schema (New)\n\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n```\n\n### Interacting with the page with Actions (Cloud-only)\n\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\n\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n```\n\n### Batch Scraping Multiple URLs (New)\n\nYou can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\n### Search\n\nThe search endpoint combines web search with Firecrawl’s scraping capabilities to return full page content for any query.\n\nInclude `scrapeOptions` with `formats: [\"markdown\"]` to get complete markdown content for each search result otherwise it defaults to getting SERP results (url, title, description).\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"query\": \"What is Mendable?\"\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://mendable.ai\",\n      \"title\": \"Mendable | AI for CX and Sales\",\n      \"description\": \"AI for CX and Sales\"\n    }\n  ]\n}\n```\n\n## Using Python SDK\n\n### Installing Python SDK\n\n```bash\npip install firecrawl-py\n```\n\n### Crawl a website\n\n```python\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  params={'formats': ['markdown', 'html']}\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev', \n  params={\n    'limit': 100, \n    'scrapeOptions': {'formats': ['markdown', 'html']}\n  },\n  poll_interval=30\n)\nprint(crawl_status)\n```\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\n\n```python\n\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n    'formats': ['json'],\n    'jsonOptions': {\n        'schema': TopArticlesSchema.model_json_schema()\n    }\n})\nprint(data[\"json\"])\n```\n\n## Using the Node SDK\n\n### Installation\n\nTo install the Firecrawl Node SDK, you can use npm:\n\n```bash\nnpm install @mendable/firecrawl-js\n```\n\n### Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.\n\n```js\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n```\n\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:\n\n```js\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n  jsonOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"json\"]);\n```\n\n## Open Source vs Cloud Offering\n\nFirecrawl is open source available under the AGPL-3.0 license. \n\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\n\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev) and offers a range of features that are not available in the open source version:\n\n![Open Source vs Cloud Offering](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/open-source-cloud.png)\n\n\n## Contributing\n\nWe love contributions! Please read our [contributing guide](CONTRIBUTING.md) before submitting a pull request. If you'd like to self-host, refer to the [self-hosting guide](SELF_HOST.md).\n\n_It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions._\n\n## Contributors\n\n<a href=\"https://github.com/mendableai/firecrawl/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=mendableai/firecrawl\"/>\n</a>\n\n## License Disclaimer\n\nThis project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n\nPlease note:\n\n- The AGPL-3.0 license applies to all parts of the project unless otherwise specified.\n- The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n- When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.\n\nFor more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.\n\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n    <a href=\"#readme-top\" style=\"text-decoration: none; color: #007bff; font-weight: bold;\">\n        ↑ Back to Top ↑\n    </a>\n</p>\n",
      "npm_url": "https://www.npmjs.com/package/firecrawl",
      "npm_downloads": 84977,
      "keywords": [
        "scraping",
        "firecrawl",
        "xiyuefox",
        "advanced scraping",
        "scraping crawling",
        "crawling data"
      ],
      "category": "web-search"
    },
    "y7ut--mcp-tavily-search": {
      "owner": "y7ut",
      "name": "mcp-tavily-search",
      "url": "https://github.com/y7ut/mcp-tavily-search",
      "imageUrl": "/freedevtools/mcp/pfp/y7ut.webp",
      "description": "This server provides search capabilities utilizing the Tavily platform, allowing users to perform search queries within the specified context. It integrates with the Model Context Protocol to facilitate search operations through a command-line interface or Docker.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-04-09T06:10:03Z",
      "readme_content": "# MCP TAVILY SEARCH\n\nA Model Context Protocol (MCP) server that provide search by tavily.\n\n## Quick start\n\ninstall\n\n```sh\ngo install github.com/y7ut/mcp-tavily-search@latest\n```\n\nadd config to mcp config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"mcp-tavily-search\",\n      \"args\": [\n        \"run\",\n        \"tvly-*******************\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"docker.ijiwei.com/mcp/mcp-tavily-search:latest\",\n        \"run\",\n        \"tvly-*******************\"\n      ]\n    }\n  }\n}\n```\n\nor debug\n\n```sh\nnpx @modelcontextprotocol/inspector mcp-tavily-search run tvly-xxxxxxxxxx\n\nnpx --no-cache @modelcontextprotocol/inspector docker run --rm -i mcp-tavily-search:latest run tvly-xxxxx\n```\n\n## Tools\n\n### search_news\n\n| **Parameter**   | **Type**   | **Default Value** | **Description**                                                                                                                                           | **Required** |\n|------------------|------------|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| `keyword`        | `string`   | N/A               | The keyword to search for.                                                                                                                                | Yes          |\n| `days`           | `number`   | `7`               | Number of days to search within. Default is 7 days.                                                                                                       | No           |\n| `limit`          | `number`   | `5`               | Number of news articles to return. Default is 5.                                                                                                          | No           |\n| `search_depth`   | `string`   | `\"basic\"`         | The depth of the search. It can be `\"basic\"` or `\"advanced\"`. Default is `\"basic\"`.                                                                       | No           |\n| `topic`          | `string`   | `\"news\"`          | The topic of the search. Options are `\"general\"` (unprocessed pages) or `\"news\"` (high-quality news). Default is `\"news\"`.                                 | No           |\n",
      "npm_url": "https://www.npmjs.com/package/mcp-tavily-search",
      "npm_downloads": 5604,
      "keywords": [
        "search",
        "docker",
        "tavily",
        "tavily search",
        "search server",
        "provides search"
      ],
      "category": "web-search"
    },
    "yiye--MCP-Finder-Server": {
      "owner": "yiye",
      "name": "MCP-Finder-Server",
      "url": "https://github.com/yiye/MCP-Finder-Server",
      "imageUrl": "/freedevtools/mcp/pfp/yiye.webp",
      "description": "Enables searching for MCP server implementations through the filtering of results by name, description, or tags. Facilitates streamlined exploration of available MCP servers by allowing users to limit search results.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-18T01:41:30Z",
      "readme_content": "# MCP Finder Server\n\n[![smithery badge](https://smithery.ai/badge/@yiye/mcp-finder-server)](https://smithery.ai/server/@yiye/mcp-finder-server)\n\nA Model Context Protocol (MCP) server that allows AI models to search for other MCP server implementations listed in the [awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers) repository.\n\n## Features\n\n- Search for MCP servers by name, description, or tags\n- Filter MCP servers by specific tags (e.g., programming language, OS compatibility)\n- Limit the number of results returned\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-finder-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yiye/mcp-finder-server):\n\n```bash\nnpx -y @smithery/cli install @yiye/mcp-finder-server --client claude\n```\n\n### Manual Installation\n1. Clone this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n\n## Usage\n\n### Build the server\n```bash\nnpm run build\n```\n\n### Run the server\n```bash\nnpm start\n```\n\n### Development mode\n```bash\nnpm run dev\n```\n\n## Tool: find-mcp-servers\n\nThe server provides a single tool called `find-mcp-servers` with the following parameters:\n\n- `search` (optional): Text to search for in server names, descriptions, or tags\n- `tag` (optional): Filter servers by tag (e.g., 'Databases', '🐍', '🏠')\n- `limit` (optional, default 10): Maximum number of results to return\n\n## Tag Meanings\n\n- 🐍 – Python codebase\n- 📇 – TypeScript codebase\n- 🏎️ – Go codebase\n- 🦀 – Rust codebase\n- #️⃣ - C# Codebase\n- ☕ - Java codebase\n- ☁️ - Cloud Service\n- 🏠 - Local Service\n- 📟 - Embedded Systems\n- 🍎 – For macOS\n- 🪟 – For Windows\n- 🐧 - For Linux\n\n## Example Response\n\n```\nFound 3 MCP servers:\n\nName: firebase/genkit\nURL: https://github.com/firebase/genkit\nTags: 📇, Frameworks\nDescription: – Provides integration between [Genkit](https://github.com/firebase/genkit/tree/main) and the Model Context Protocol (MCP).\n---\nName: lastmile-ai/mcp-agent\nURL: https://github.com/lastmile-ai/mcp-agent\nTags: 🤖, 🔌, Frameworks\nDescription: - Build effective agents with MCP servers using simple, composable patterns.\n---\nName: LiteMCP\nURL: https://github.com/wong2/litemcp\nTags: 📇, Frameworks\nDescription: - A high-level framework for building MCP servers in JavaScript/TypeScript\n---\n```\n\n## License\n\nISC \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "search",
        "mcp",
        "searching mcp",
        "mcp finder",
        "mcp servers"
      ],
      "category": "web-search"
    },
    "yokingma--one-search-mcp": {
      "owner": "yokingma",
      "name": "one-search-mcp",
      "url": "https://github.com/yokingma/one-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/yokingma.webp",
      "description": "Enables web search, scraping, and content extraction from various websites using multiple search engines and scrapers. It supports local browser searches and integrates with tools like SearXNG, DuckDuckGo, and Bing for enhanced data retrieval.",
      "stars": 58,
      "forks": 12,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T15:37:12Z",
      "readme_content": "# 🚀 OneSearch MCP Server: Web Search & Crawl & Scraper & Extract\n\nA Model Context Protocol (MCP) server implementation that integrates with Searxng/Tavily/DuckDuckGo/Bing for web search, local browser search, and scraping capabilities with Firecrawl.\n\n## Features\n\n- Web Search, scrape, crawl and extract content from websites.\n- Support multiple search engines and web scrapers: **SearXNG**, **Firecrawl**, **Tavily**, **DuckDuckGo**, **Bing**, etc.\n- **Local web search** (browser search), support multiple search engines: **Bing**, **Google**, **Baidu**, **Sogou**, etc.\n  - Use `puppeteer-core` to scrape content from websites.\n  - You should have a local browser installed, such as `Chromium`, `Google Chrome`, `Google Chrome Canary`, etc.\n  - Free, no keys required.\n- **Enabled tools:** `one_search`, `one_scrape`, `one_map`\n- Support for self-hosted: SearXNG, Firecrawl, etc. (see [Deploy](./deploy/README.md))\n\n## Installation\n\n### Installing via Smithery\n\nTo install OneSearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yokingma/one-search):\n\n```bash\nnpx -y @smithery/cli install @yokingma/one-search --client claude\n```\n\n### Manual Installation\n\n```shell\n# Manually install (Optional)\nnpm install -g one-search-mcp\n```\n\n```shell\n# using npx\nenv SEARCH_API_URL=http://127.0.0.1:8080 FIRECRAWL_API_URL=http://127.0.0.1:3002 npx -y one-search-mcp\n```\n\n## Environment Variables\n\n**Search Engine:**\n\n- **SEARCH_PROVIDER** (Optional): The search provider to use, supports `searxng`, `duckduckgo`, `bing`, `tavily`, `local`, default is `local`.\n- **SEARCH_API_URL** (Optional): The URL of the SearxNG API, required for `searxng`.\n- **SEARCH_API_KEY** (Optional): The API key for the search provider, required for `tavily`, `bing`.\n\n```ts\n// supported search providers\nexport type SearchProvider = 'searxng' | 'duckduckgo' | 'bing' | 'tavily' | 'local';\n```\n\n**Firecrawl:**\n\n- FIRECRAWL_API_URL (Optional): The URL of the Firecrawl API, required for `firecrawl`.\n- FIRECRAWL_API_KEY (Optional): The API key for the Firecrawl API, required for `firecrawl` if using cloud service.\n\n## Running on Cursor\n\nYour `mcp.json` file will look like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"one-search-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"one-search-mcp\"],\n      \"env\": {\n        \"SEARCH_PROVIDER\": \"searxng\",\n        \"SEARCH_API_URL\": \"http://127.0.0.1:8080\",\n        \"SEARCH_API_KEY\": \"YOUR_API_KEY\",\n        \"FIRECRAWL_API_URL\": \"http://127.0.0.1:3002\",\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"one-search-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"one-search-mcp\"],\n      \"env\": {\n        \"SEARCH_PROVIDER\": \"searxng\",\n        \"SEARCH_API_URL\": \"http://127.0.0.1:8080\",\n        \"SEARCH_API_KEY\": \"YOUR_API_KEY\",\n        \"FIRECRAWL_API_URL\": \"http://127.0.0.1:3002\",\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Self-host\n\nLocal deployment of SearXNG and Firecrawl, please refer to [Deploy](./deploy/README.md)\n\n## Troubleshooting\n\n- [ReferenceError]: __name is not defined: This is because Puppeteer has problems with `tsx`, [esbuild#1031](https://github.com/evanw/esbuild/issues/1031)\n\n## License\n\nMIT License - see [LICENSE](./LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/one-search-mcp",
      "npm_downloads": 8857,
      "keywords": [
        "searches",
        "scraping",
        "search",
        "search scraping",
        "web search",
        "search engines"
      ],
      "category": "web-search"
    },
    "yutakobayashidev--webforai-mcp-server": {
      "owner": "yutakobayashidev",
      "name": "webforai-mcp-server",
      "url": "https://github.com/yutakobayashidev/webforai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/yutakobayashidev.webp",
      "description": "Extract plain text from any web page URL and convert it to clean, well-formatted Markdown output. Provides robust error handling and is optimized for AI consumption through seamless integration with MCP clients.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-03T08:09:50Z",
      "readme_content": "# WebforAI Text Extractor - MCP Server\n\nA Cloudflare Workers-based Model Context Protocol (MCP) server that extracts plain text from web pages using [WebforAI](https://webforai.dev/).\n\n## 🌟 What is WebforAI?\n\n[WebforAI](https://webforai.dev/) is a powerful library designed to make web content accessible to AI models. It provides tools to:\n\n- Convert HTML to clean, structured Markdown\n- Extract meaningful content from web pages\n- Process tables, links, and images intelligently\n- Prepare web content for AI consumption\n\nThis MCP server leverages WebforAI's capabilities to extract plain text from any web page URL, making it easy to feed web content into AI models through the Model Context Protocol.\n\n## 📋 Features\n\n- **Simple API**: Extract text from any web page with a single API call\n- **Clean Output**: Receive well-formatted Markdown text without HTML noise\n- **Error Handling**: Robust error handling for failed requests\n- **Cloudflare Workers**: Serverless deployment with global distribution\n- **MCP Compatible**: Works with any MCP client like Claude Desktop or Cloudflare AI Playground\n\n## 🚀 Getting Started\n\n### Deploy to Cloudflare Workers\n\n[![Deploy to Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/yutakobayashidev/webforai-mcp-server)\n\nThis will deploy your MCP server to a URL like: `webforai-mcp-server.<your-account>.workers.dev/sse`\n\n### Local Development\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yutakobayashidev/webforai-mcp-server.git\n   cd webforai-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n\n3. Start the development server:\n   ```bash\n   pnpm dev\n   ```\n\n4. Your server will be available at `http://localhost:8787`\n\n## 🔧 Using the Text Extraction Tool\n\nThe `extractWebPageText` tool accepts a URL to a web page and returns the extracted text content in markdown format:\n\n```json\n{\n  \"url\": \"https://example.com/page\"\n}\n```\n\nThe response will contain the extracted text in Markdown format, with:\n- Links converted to plain text\n- Tables converted to plain text\n- Images hidden\n\n## 🔌 Connecting to MCP Clients\n\n### Cloudflare AI Playground\n\n1. Go to [Cloudflare AI Playground](https://playground.ai.cloudflare.com/)\n2. Enter your deployed MCP server URL (`webforai-mcp-server.<your-account>.workers.dev/sse`)\n3. You can now use your text extraction tool directly from the playground!\n\n### Claude Desktop\n\nTo connect to your MCP server from Claude Desktop:\n\n1. Follow [Anthropic's Quickstart](https://modelcontextprotocol.io/quickstart/user)\n2. In Claude Desktop go to Settings > Developer > Edit Config\n3. Update with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"webforaiExtractor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://localhost:8787/sse\"  // or webforai-mcp-server.your-account.workers.dev/sse\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude and you should see the text extraction tool become available\n\n## 📚 Learn More\n\n- [WebforAI Documentation](https://webforai.dev/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Cloudflare Workers](https://developers.cloudflare.com/workers/)\n- [Cloudflare AI](https://developers.cloudflare.com/ai/)\n\n## 📄 License\n\nMIT       \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webforai",
        "yutakobayashidev",
        "markdown",
        "webforai mcp",
        "yutakobayashidev webforai",
        "search yutakobayashidev"
      ],
      "category": "web-search"
    },
    "zbkm--mmnt-mcp-server": {
      "owner": "zbkm",
      "name": "mmnt-mcp-server",
      "url": "https://github.com/zbkm/mmnt-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zbkm.webp",
      "description": "Utilizes the Mamont search engine for executing search queries and retrieving cached web pages. Provides a standardized interface for accessing real-time search results and text-based content efficiently.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-03T15:37:47Z",
      "readme_content": "# mmnt-mcp-server\nMCP server for the [Mamont](https://www.mmnt.ru/) search engine.\n\n### Tools:\n- mmnt_search\n    * search query on search engine\n    * inputs\n        * query - query string\n        * page - page number\n- mmnt_cache\n    * retrieve page from search page cache\n    * inputs\n        * id - unique cache id\n        * onlyText - should the result be text only (no html)\n\n### Install\n```json\n{\n  \"mcpServers\": {\n    \"mmnt\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mmnt-mcp-server\"]\n    }\n  }\n}\n```\n",
      "npm_url": "https://www.npmjs.com/package/mmnt-mcp-server",
      "npm_downloads": 351,
      "keywords": [
        "search",
        "mamont",
        "mmnt",
        "mamont search",
        "web search",
        "search engine"
      ],
      "category": "web-search"
    },
    "zcaceres--fetch-mcp": {
      "owner": "zcaceres",
      "name": "fetch-mcp",
      "url": "https://github.com/zcaceres/fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zcaceres.webp",
      "description": "Fetches web content in various formats such as HTML, JSON, plain text, and Markdown from specified URLs. It can return raw HTML content and JSON data from various online sources.",
      "stars": 611,
      "forks": 100,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T17:28:25Z",
      "readme_content": "# Fetch MCP Server\n\n\n\nThis MCP server provides functionality to fetch web content in various formats, including HTML, JSON, plain text, and Markdown.\n\n[Available on NPM](https://www.npmjs.com/package/mcp-fetch-server)\n\n<a href=\"https://glama.ai/mcp/servers/nu09wf23ao\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/nu09wf23ao/badge\" alt=\"Fetch Server MCP server\" />\n</a>\n\n## Components\n\n### Tools\n\n- **fetch_html**\n  - Fetch a website and return the content as HTML\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the raw HTML content of the webpage\n\n- **fetch_json**\n  - Fetch a JSON file from a URL\n  - Input:\n    - `url` (string, required): URL of the JSON to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the parsed JSON content\n\n- **fetch_txt**\n  - Fetch a website and return the content as plain text (no HTML)\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the text content of the webpage with HTML tags, scripts, and styles removed\n\n- **fetch_markdown**\n  - Fetch a website and return the content as Markdown\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the content of the webpage converted to Markdown format\n\n### Resources\n\nThis server does not provide any persistent resources. It's designed to fetch and transform web content on demand.\n\n## Getting started\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the server: `npm run build`\n\n### Usage\n\nTo use the server, you can run it directly:\n\n```bash\nnpm start\n```\n\nThis will start the Fetch MCP Server running on stdio.\n\n### Environment variables\n\n- **DEFAULT_LIMIT** - sets the default size limit for the fetch (0 = no limit)\n\n### Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-fetch-server\"\n      ], \n      \"env\": {\n        \"DEFAULT_LIMIT\": \"50000\" // optionally change default limit\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- Fetches web content using modern fetch API\n- Supports custom headers for requests\n- Provides content in multiple formats: HTML, JSON, plain text, and Markdown\n- Uses JSDOM for HTML parsing and text extraction\n- Uses TurndownService for HTML to Markdown conversion\n\n## Development\n\n- Run `npm run dev` to start the TypeScript compiler in watch mode\n- Use `npm test` to run the test suite\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "https://www.npmjs.com/package/fetch-mcp",
      "npm_downloads": 36810,
      "keywords": [
        "zcaceres",
        "fetches",
        "fetch",
        "zcaceres fetch",
        "search zcaceres",
        "fetches web"
      ],
      "category": "web-search"
    },
    "zhsama--duckduckgo-mcp-server": {
      "owner": "zhsama",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/zhsama/duckduckgo-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zhsama.webp",
      "description": "Provides DuckDuckGo search functionality with a simple interface for performing web searches. Supports rate limiting and error handling while integrating with the DuckDuckGo API.",
      "stars": 63,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T01:13:04Z",
      "readme_content": "# duckduckgo-search MCP Server\n\nEnglish | [中文](README_zh.md)\n\nA Model Context Protocol server for DuckDuckGo Search\n\nThis is a TypeScript-based MCP server that provides DuckDuckGo search functionality. It demonstrates core MCP concepts through:\n\n- Integration with DuckDuckGo Search\n- Easy-to-use search tool interface\n- Rate limiting and error handling support\n\n<a href=\"https://glama.ai/mcp/servers/34fhy9xb9w\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/34fhy9xb9w/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n### Search Tool\n\n- `duckduckgo_search` - Perform web searches using DuckDuckGo API\n  - Required parameter: `query` (search query, max 400 characters)\n  - Optional parameter: `count` (number of results, 1-20, default 10)\n  - Optional parameter: `safeSearch` (safety level: strict/moderate/off, default moderate)\n  - Returns formatted Markdown search results\n\n### Rate Limits\n\n- Maximum 1 request per second\n- Maximum 15000 requests per month\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- pnpm >= 8.0.0\n\n### Installation\n\n```bash\n# Install pnpm if not already installed\nnpm install -g pnpm\n\n# Install project dependencies\npnpm install\n```\n\n### Build and Run\n\nBuild the server:\n\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm run watch\n```\n\n## Setup in Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n# online\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"duckduckgo-mcp-server\"\n        ]\n    }\n  }\n}\n\n# local\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/duckduckgo-search/build/index.js\"\n      ]\n    }\n  }\n}\n```\n![image](https://github.com/user-attachments/assets/6906e280-9dbb-4bb5-a537-d9e45e666084)\n![image](https://github.com/user-attachments/assets/867a70ae-082f-45ab-a623-869bfd6c31eb)\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.",
      "npm_url": "https://www.npmjs.com/package/duckduckgo-mcp-server",
      "npm_downloads": 10136,
      "keywords": [
        "duckduckgo",
        "searches",
        "search",
        "duckduckgo search",
        "duckduckgo api",
        "provides duckduckgo"
      ],
      "category": "web-search"
    },
    "zoomeye-ai--mcp_zoomeye": {
      "owner": "zoomeye-ai",
      "name": "mcp_zoomeye",
      "url": "https://github.com/zoomeye-ai/mcp_zoomeye",
      "imageUrl": "/freedevtools/mcp/pfp/zoomeye-ai.webp",
      "description": "Query network asset information using ZoomEye dorks, while benefiting from a caching mechanism and advanced error handling to enhance data retrieval capabilities.",
      "stars": 48,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T03:44:56Z",
      "readme_content": "# 🚀 ZoomEye MCP Server\n\nA Model Context Protocol (MCP) server that provides network asset information based on query conditions. This server allows Large Language Models (LLMs) to obtain network asset information by querying ZoomEye using dorks and other search parameters.\n\n## 🔔 Announcement\n\n🎉 We are excited to announce the official open-source release of **ZoomEye MCP Server** — a powerful Model Context Protocol (MCP) server that brings real-time cyber asset intelligence to AI assistants and development environments.\n\n🚀 Free Trial: 7-Day FREE Access to ZoomEye MCP!\nExperience ZoomEye MCP — the AI-powered cyberspace asset search engine — absolutely free for 7 days!\n\n🔍 Search global internet assets, track real-time changes, and unlock AI-driven insights — all in one place.\n\n👉 How to claim:\n\n1. Follow us on Twitter: [@zoomeye_team](https://x.com/zoomeye_team)\n2. DM us \"MCP\" and your MCP setup screenshot\n3. Get instant access to your 7-day membership\n\n🎁 Limited-time free trial — explore the power of AI asset search today!\n\n💡 Provide insightful feedback that gets officially adopted, and you'll unlock **even more rewards**!\n\n🔧 Fully compatible with leading MCP environments:\n\n- Claude Desktop\n- Cursor\n- Windsurf\n- Cline\n- Continue\n- Zed\n- Cherry Studio\n- Chatbox\n\n🔗 Explore ZoomEye MCP Server on:\n\n- GitHub: [zoomeye-ai/mcp_zoomeye](https://github.com/zoomeye-ai/mcp_zoomeye)\n- MCP.so: [mcp.so/server/mcp_zoomeye](https://mcp.so/server/mcp_zoomeye/zoomeye-ai)\n- Smithery: [smithery.ai/server/@zoomeye-ai/mcp_zoomeye](https://smithery.ai/server/@zoomeye-ai/mcp_zoomeye)\n- Cursor Directory: [cursor.directory/mcp/zoomeye](https://cursor.directory/mcp/zoomeye)\n- Pulse MCP: [pulsemcp.com/servers/zoomeye](https://www.pulsemcp.com/servers/zoomeye)\n- Glama MCP: [glama.ai/mcp/servers](https://glama.ai/mcp/servers)\n\nWe welcome everyone to use, explore, and contribute!\n\n## 🔑 How can I get a ZoomEye API key?\n\nTo use this MCP server, you’ll need a ZoomEye API key.\n\n1. Go to https://www.zoomeye.ai\n2. Register or log in\n3. Click your avatar → **Profile**\n4. Copy your **API-KEY**\n5. Set the environment variable:\n   \n   `export ZOOMEYE_API_KEY=\"your_api_key_here\"`\n\n\n\n\n\n## Features\n\n- Query ZoomEye for network asset information using dorks\n- Caching mechanism to improve performance and reduce API calls\n- Automatic retry mechanism for failed API requests\n- Comprehensive error handling and logging\n\n## Available Tools\n\n- `zoomeye_search` - Get network asset information based on query conditions.\n  - Required parameters:\n    - `qbase64` (string): Base64 encoded query string for ZoomEye search\n  - Optional parameters:\n    - `page` (integer): View asset page number, default is 1\n    - `pagesize` (integer): Number of records per page, default is 10, maximum is 1000\n    - `fields` (string): The fields to return, separated by commas\n    - `sub_type` (string): Data type, supports v4, v6, and web. Default is v4\n    - `facets` (string): Statistical items, separated by commas if there are multiple\n    - `ignore_cache` (boolean): Whether to ignore the cache\n\n## Usage Guide\n\n### Basic Usage\n\nOnce the server is running, you can interact with it through your AI assistant or development environment. Here's how to use it:\n\n1. **Start the server** using one of the installation methods above\n2. **Configure your AI assistant** (Claude Desktop, Cursor, Windsurf, Cline, Continue, Zed, etc.) to use the server\n3. **Query network information** using natural language\n\n\n\n### Search Syntax Guide\n\n- Search Scope covers devices (IPv4, IPv6) and websites (domains).\n- When entering a search string, the system will match keywords in \"global\" mode, including content from various\n  protocols such as HTTP, SSH, FTP, etc. (e.g., HTTP/HTTPS protocol headers, body, SSL, title, and other protocol\n  banners).\n- Search strings are case-insensitive and will be segmented for matching (the search results page provides a \"\n  segmentation\" test feature). When using == for search, it enforces exact case-sensitive matching with strict syntax.\n- Please use quotes for search strings (e.g., \"Cisco System\" or 'Cisco System'). If the search string contains quotes,\n  use the escape character, e.g.,\"a\\\"b\". If the search string contains parentheses, use the escape character, e.g.,\n  portinfo\\(\\).\n\nYou can see more detailed search syntax rules in [prompts.py](./src/mcp_server_zoomeye/prompts.py).\n\nFor more information on the ZoomEye Search API, refer to the [ZoomEye API v2 documentation](https://www.zoomeye.ai/doc).\n\n## Getting Started\n\n### Prerequisites\n\n1. **ZoomEye API Key**\n   \n   - Register for an account at [ZoomEye](https://www.zoomeye.ai/)\n   - Obtain your API key from your account settings\n   - The API key will be used to authenticate your requests to the ZoomEye API\n2. **Python Environment**\n   \n   - Python 3.10 or higher is required\n   - Alternatively, you can use Docker to run the server without installing Python\n\n## Installation\n\n### Using PIP\n\nAlternatively, you can install `mcp-server-zoomeye` via pip:\n\n```bash\npip install mcp-server-zoomeye\n```\n\nAfter installation, you can run it as a script using the following command:\n\n```bash\npython -m mcp_server_zoomeye\n```\n\n### Using Docker\n\nYou can also run the ZoomEye MCP server using Docker:\n\n#### Pull from Docker Hub\n\n```bash\n# Pull the latest image\ndocker pull zoomeyeteam/mcp-server-zoomeye:latest\n\n# Run the container with your API key\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n```\n\n> **Note**: We provide multi-architecture Docker images that support `linux/amd64` and `linux/arm64` platforms and can run on Intel/AMD and ARM (such as Apple Silicon) processors.\n\n#### Build from Source\n\nAlternatively, you can build the Docker image from source:\n\n```bash\n# Clone the repository\ngit clone https://github.com/zoomeye-ai/mcp_zoomeye.git\ncd mcp_zoomeye\n\n# Build the Docker image\ndocker build -t zoomeyeteam/mcp-server-zoomeye:local .\n\n# Run the container\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:local\n```\n\n### Using uv\n\n[`uv`](https://docs.astral.sh/uv/) is a fast Python package installer and resolver written in Rust. It's a modern alternative to pip that offers significant performance improvements.\n\n#### Installation of uv\n\n```bash\n# Install uv using curl (macOS/Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or using PowerShell (Windows)\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Or using Homebrew (macOS)\nbrew install uv\n```\n\n#### Using uvx to run mcp-server-zoomeye\n\nNo specific installation is required when using [`uvx`](https://docs.astral.sh/uv/guides/tools/), which allows you to run Python packages directly:\n\n#### Installing with uv\n\nAlternatively, you can install the package using uv:\n\n```bash\n# Install in the current environment\nuv pip install mcp-server-zoomeye\n\n# Or create and install in a new virtual environment\nuv venv\nuv pip install mcp-server-zoomeye\n```\n\n## Configuration\n\n### Environment Variables\n\nThe ZoomEye MCP server requires the following environment variable:\n\n- `ZOOMEYE_API_KEY`: Your ZoomEye API key for authentication\n\nYou can set this environment variable in several ways:\n\n1. **Export in your shell session**:\n   \n   ```bash\n   export ZOOMEYE_API_KEY=\"your_api_key_here\"\n   ```\n2. **Pass directly when running the container** (for Docker):\n   \n   ```bash\n   docker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n   ```\n\n### Configure Claude.app\n\nAdd the following in Claude settings:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Using docker</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"ZOOMEYE_API_KEY=your_api_key_here\", \"zoomeyeteam/mcp-server-zoomeye:latest\"],\n    \"env\": {\n      \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n### Configure Zed\n\nAdd the following in Zed's settings.json:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"mcp-server-zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n],\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"context_servers\": {\n  \"mcp-server-zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n},\n```\n\n</details>\n\n## Example Interactions\n\n### Example 1: Retrieve global Apache Tomcat assets\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache Tomcat\\\"\"\n  }\n}\n```\n\nResponse:\n\n```json\n{\n  \"code\": 60000,\n  \"message\": \"success\",\n  \"total\": 163139107,\n  \"query\": \"app=\\\"Apache Tomcat\\\"\",\n  \"data\": [\n    {\n      \"url\": \"https://1.1.1.1:443\",\n      \"ssl.jarm\": \"29d29d15d29d29d00029d29d29d29dea0f89a2e5fb09e4d8e099befed92cfa\",\n      \"ssl.ja3s\": \"45094d08156d110d8ee97b204143db14\",\n      \"iconhash_md5\": \"f3418a443e7d841097c714d69ec4bcb8\",\n      \"robots_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"security_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"ip\": \"1.1.1.1\",\n      \"domain\": \"www.google.com\",\n      \"hostname\": \"SPACEX\",\n      \"os\": \"windows\",\n      \"port\": 443,\n      \"service\": \"https\",\n      \"title\": [\"GoogleGoogle appsGoogle Search\"],\n      \"version\": \"1.1.0\",\n      \"device\": \"webcam\",\n      \"rdns\": \"c01031-001.cust.wallcloud.ch\",\n      \"product\": \"OpenSSD\",\n      \"header\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"header_hash\": \"27f9973fe57298c3b63919259877a84d\",\n      \"body\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"body_hash\": \"84a18166fde3ee7e7c974b8d1e7e21b4\",\n      \"banner\": \"SSH-2.0-OpenSSH_7.6p1 Ubuntu-4ubuntu0.3\",\n      \"update_time\": \"2024-07-03T14:34:10\",\n      \"header.server.name\": \"nginx\",\n      \"header.server.version\": \"1.8.1\",\n      \"continent.name\": \"Europe\",\n      \"country.name\": \"Germany\",\n      \"province.name\": \"Hesse\",\n      \"city.name\": \"Frankfurt\",\n      \"lon\": \"118.753262\",\n      \"lat\": \"32.064838\",\n      \"isp.name\": \"aviel.ru\",\n      \"organization.name\": \"SERVISFIRST BANK\",\n      \"zipcode\": \"210003\",\n      \"idc\": 0,\n      \"honeypot\": 0,\n      \"asn\": 4837,\n      \"protocol\": \"tcp\",\n      \"ssl\": \"SSL Certificate Version: TLS 1.2 CipherSuit: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256...\",\n      \"primary_industry\": \"Finance\",\n      \"sub_industry\": \"bank\",\n      \"rank\": 60\n    }\n  ]\n}\n```\n\n## Debugging and Troubleshooting\n\n### Using MCP Inspector\n\nThe Model Context Protocol Inspector is a tool that helps debug MCP servers by simulating client interactions. You can use it to test your ZoomEye MCP server:\n\n```bash\n# For uvx installation\nnpx @modelcontextprotocol/inspector uvx mcp-server-zoomeye\n\n# If developing locally\ncd path/to/servers/src/mcp_server_zoomeye\nnpx @modelcontextprotocol/inspector uv run mcp-server-zoomeye\n```\n\n### Common Issues\n\n1. **Authentication Errors**\n   \n   - Ensure your ZoomEye API key is correct and properly set as an environment variable\n   - Check that your API key has not expired or been revoked\n2. **Connection Issues**\n   \n   - Verify your internet connection\n   - Check if the ZoomEye API is experiencing downtime\n3. **No Results**\n   \n   - Your query might be too specific or contain syntax errors\n   - Try simplifying your query or using different search terms\n4. **Rate Limiting**\n   \n   - ZoomEye API has rate limits based on your account type\n   - Space out your requests or upgrade your account for higher limits\n\n## Advanced Usage\n\n### Caching\n\nThe ZoomEye MCP server implements caching to improve performance and reduce API calls:\n\n- Responses are cached based on the query parameters\n- Cache duration is configurable (default: 1 hour)\n- You can bypass the cache by setting `ignore_cache` to `true` in your query\n\n### Custom Fields\n\nYou can request specific fields in your query results by using the `fields` parameter:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"fields\": \"ip,port,domain,service,os,country,city\"\n  }\n}\n```\n\n### Pagination\n\nFor queries that return many results, you can paginate through them:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"page\": 2,\n    \"pagesize\": 20\n  }\n}\n```\n\n## Contributing\n\nWe encourage contributions to mcp-server-zoomeye to help expand and improve its functionality. Whether it's adding new related tools, enhancing existing features, or improving documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make mcp-server-zoomeye more robust and practical.\n\n## License\n\nmcp-server-zoomeye is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more information, see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zoomeye",
        "mcp_zoomeye",
        "retrieval",
        "search zoomeye",
        "mcp_zoomeye query",
        "ai mcp_zoomeye"
      ],
      "category": "web-search"
    },
    "zueai--webdev-mcp": {
      "owner": "zueai",
      "name": "webdev-mcp",
      "url": "https://github.com/zueai/webdev-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zueai.webp",
      "description": "Provides web development tools for capturing screenshots and listing available screens as base64 encoded strings for further processing or analysis.",
      "stars": 10,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T06:57:23Z",
      "readme_content": "# webdev-mcp\n\nAn MCP server that provides useful web development tools.\n\n## Usage\n\n### Cursor\n\n- To install in a project, add the MCP server to your `.cursor/mcp.json`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"webdev\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"webdev-mcp\"],\n\n\t\t}\n\t}\n}\n```\n\n- To install globally, add this command to your Cursor settings:\n\n```bash\nnpx webdev-mcp\n```\n\n### Windsurf\n\n- Add the MCP server to your `~/.codeium/windsurf/mcp_config.json` file:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"webdev\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"webdev-mcp\"]\n\t\t}\n\t}\n}\n```\n\n## Tools\n\nCurrently, the only 2 tools are `takeScreenshot` and `listScreens`. Your agent can use the list screens tool to get the screen id of the screen it wants to screenshot.\n\nThe tool will return the screenshot as a base64 encoded string.\n\n\n\n## Tips\n\nMake sure YOLO mode is on and MCP tools protection is off in your Cursor settings for the best experience. You might have to allow Cursor to record your screen on MacOS.",
      "npm_url": "https://www.npmjs.com/package/webdev-mcp",
      "npm_downloads": 3530,
      "keywords": [
        "webdev",
        "zueai",
        "screenshots",
        "zueai webdev",
        "webdev mcp",
        "capturing screenshots"
      ],
      "category": "web-search"
    },
    "zxsimple--mcp-sample": {
      "owner": "zxsimple",
      "name": "mcp-sample",
      "url": "https://github.com/zxsimple/mcp-sample",
      "imageUrl": "/freedevtools/mcp/pfp/zxsimple.webp",
      "description": "Fetch website content effortlessly for data retrieval in applications. Connect with an LLM client to automate web interactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-02T02:45:50Z",
      "readme_content": "# MCP Python SDK\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Specification][spec-badge]][spec-url]\n[![GitHub Discussions][discussions-badge]][discussions-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [MCP Python SDK](#mcp-python-sdk)\n  - [Overview](#overview)\n  - [Installation](#installation)\n    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)\n    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)\n  - [Quickstart](#quickstart)\n  - [What is MCP?](#what-is-mcp)\n  - [Core Concepts](#core-concepts)\n    - [Server](#server)\n    - [Resources](#resources)\n    - [Tools](#tools)\n    - [Prompts](#prompts)\n    - [Images](#images)\n    - [Context](#context)\n  - [Running Your Server](#running-your-server)\n    - [Development Mode](#development-mode)\n    - [Claude Desktop Integration](#claude-desktop-integration)\n    - [Direct Execution](#direct-execution)\n    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)\n  - [Examples](#examples)\n    - [Echo Server](#echo-server)\n    - [SQLite Explorer](#sqlite-explorer)\n  - [Advanced Usage](#advanced-usage)\n    - [Low-Level Server](#low-level-server)\n    - [Writing MCP Clients](#writing-mcp-clients)\n    - [MCP Primitives](#mcp-primitives)\n    - [Server Capabilities](#server-capabilities)\n  - [Documentation](#documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg\n[pypi-url]: https://pypi.org/project/mcp/\n[mit-badge]: https://img.shields.io/pypi/l/mcp.svg\n[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE\n[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg\n[python-url]: https://www.python.org/downloads/\n[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg\n[docs-url]: https://modelcontextprotocol.io\n[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg\n[spec-url]: https://spec.modelcontextprotocol.io\n[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk\n[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions\n\n## Overview\n\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\n\n- Build MCP clients that can connect to any MCP server\n- Create MCP servers that expose resources, prompts and tools\n- Use standard transports like stdio and SSE\n- Handle all MCP protocol messages and lifecycle events\n\n## Installation\n\n### Adding MCP to your python project\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects. In a uv managed python project, add mcp to dependencies by:\n\n```bash\nuv add \"mcp[cli]\"\n```\n\nAlternatively, for projects using pip for dependencies:\n```bash\npip install \"mcp[cli]\"\n```\n\n### Running the standalone MCP development tools\n\nTo run the mcp command with uv:\n\n```bash\nuv run mcp\n```\n\n## Quickstart\n\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\n```python\n# server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n```\n\nYou can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:\n```bash\nmcp install server.py\n```\n\nAlternatively, you can test it with the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## What is MCP?\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\n\n- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)\n- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\n- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)\n- And more!\n\n## Core Concepts\n\n### Server\n\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\n```python\n# Add lifespan support for startup/shutdown with strong typing\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server.fastmcp import Context, FastMCP\n\n# Create a named server\nmcp = FastMCP(\"My App\")\n\n# Specify dependencies for deployment and development\nmcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n\n\n@dataclass\nclass AppContext:\n    db: Database\n\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n    # Initialize on startup\n    db = await Database.connect()\n    try:\n        yield AppContext(db=db)\n    finally:\n        # Cleanup on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n\n# Access type-safe lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context) -> str:\n    \"\"\"Tool that uses initialized resources\"\"\"\n    db = ctx.request_context.lifespan_context[\"db\"]\n    return db.query()\n```\n\n### Resources\n\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.resource(\"config://app\")\ndef get_config() -> str:\n    \"\"\"Static configuration data\"\"\"\n    return \"App configuration here\"\n\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -> str:\n    \"\"\"Dynamic user data\"\"\"\n    return f\"Profile data for user {user_id}\"\n```\n\n### Tools\n\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n\n```python\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n    return weight_kg / (height_m**2)\n\n\n@mcp.tool()\nasync def fetch_weather(city: str) -> str:\n    \"\"\"Fetch current weather for a city\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        return response.text\n```\n\n### Prompts\n\nPrompts are reusable templates that help LLMs interact with your server effectively:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.prompt()\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n\n@mcp.prompt()\ndef debug_error(error: str) -> list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n```\n\n### Images\n\nFastMCP provides an `Image` class that automatically handles image data:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Image\nfrom PIL import Image as PILImage\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef create_thumbnail(image_path: str) -> Image:\n    \"\"\"Create a thumbnail from an image\"\"\"\n    img = PILImage.open(image_path)\n    img.thumbnail((100, 100))\n    return Image(data=img.tobytes(), format=\"png\")\n```\n\n### Context\n\nThe Context object gives your tools and resources access to MCP capabilities:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\nasync def long_task(files: list[str], ctx: Context) -> str:\n    \"\"\"Process multiple files with progress tracking\"\"\"\n    for i, file in enumerate(files):\n        ctx.info(f\"Processing {file}\")\n        await ctx.report_progress(i, len(files))\n        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n    return \"Processing complete\"\n```\n\n## Running Your Server\n\n### Development Mode\n\nThe fastest way to test and debug your server is with the MCP Inspector:\n\n```bash\nmcp dev server.py\n\n# Add dependencies\nmcp dev server.py --with pandas --with numpy\n\n# Mount local code\nmcp dev server.py --with-editable .\n```\n\n### Claude Desktop Integration\n\nOnce your server is ready, install it in Claude Desktop:\n\n```bash\nmcp install server.py\n\n# Custom name\nmcp install server.py --name \"My Analytics Server\"\n\n# Environment variables\nmcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nmcp install server.py -f .env\n```\n\n### Direct Execution\n\nFor advanced scenarios like custom deployments:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\nRun it with:\n```bash\npython server.py\n# or\nmcp run server.py\n```\n\n### Mounting to an Existing ASGI Server\n\nYou can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount, Host\nfrom mcp.server.fastmcp import FastMCP\n\n\nmcp = FastMCP(\"My App\")\n\n# Mount the SSE server to the existing ASGI server\napp = Starlette(\n    routes=[\n        Mount('/', app=mcp.sse_app()),\n    ]\n)\n\n# or dynamically mount as host\napp.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))\n```\n\nFor more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).\n\n## Examples\n\n### Echo Server\n\nA simple server demonstrating resources, tools, and prompts:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo\")\n\n\n@mcp.resource(\"echo://{message}\")\ndef echo_resource(message: str) -> str:\n    \"\"\"Echo a message as a resource\"\"\"\n    return f\"Resource echo: {message}\"\n\n\n@mcp.tool()\ndef echo_tool(message: str) -> str:\n    \"\"\"Echo a message as a tool\"\"\"\n    return f\"Tool echo: {message}\"\n\n\n@mcp.prompt()\ndef echo_prompt(message: str) -> str:\n    \"\"\"Create an echo prompt\"\"\"\n    return f\"Please process this message: {message}\"\n```\n\n### SQLite Explorer\n\nA more complex example showing database integration:\n\n```python\nimport sqlite3\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"SQLite Explorer\")\n\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -> str:\n    \"\"\"Provide the database schema as a resource\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    schema = conn.execute(\"SELECT sql FROM sqlite_master WHERE type='table'\").fetchall()\n    return \"\\n\".join(sql[0] for sql in schema if sql[0])\n\n\n@mcp.tool()\ndef query_data(sql: str) -> str:\n    \"\"\"Execute SQL queries safely\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    try:\n        result = conn.execute(sql).fetchall()\n        return \"\\n\".join(str(row) for row in result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n## Advanced Usage\n\n### Low-Level Server\n\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server import Server\n\n\n@asynccontextmanager\nasync def server_lifespan(server: Server) -> AsyncIterator[dict]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    # Initialize resources on startup\n    db = await Database.connect()\n    try:\n        yield {\"db\": db}\n    finally:\n        # Clean up on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nserver = Server(\"example-server\", lifespan=server_lifespan)\n\n\n# Access lifespan context in handlers\n@server.call_tool()\nasync def query_db(name: str, arguments: dict) -> list:\n    ctx = server.request_context\n    db = ctx.lifespan_context[\"db\"]\n    return await db.query(arguments[\"query\"])\n```\n\nThe lifespan API provides:\n- A way to initialize resources when the server starts and clean them up when it stops\n- Access to initialized resources through the request context in handlers\n- Type-safe context passing between lifespan and request handlers\n\n```python\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\n# Create a server instance\nserver = Server(\"example-server\")\n\n\n@server.list_prompts()\nasync def handle_list_prompts() -> list[types.Prompt]:\n    return [\n        types.Prompt(\n            name=\"example-prompt\",\n            description=\"An example prompt template\",\n            arguments=[\n                types.PromptArgument(\n                    name=\"arg1\", description=\"Example argument\", required=True\n                )\n            ],\n        )\n    ]\n\n\n@server.get_prompt()\nasync def handle_get_prompt(\n    name: str, arguments: dict[str, str] | None\n) -> types.GetPromptResult:\n    if name != \"example-prompt\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    return types.GetPromptResult(\n        description=\"Example prompt\",\n        messages=[\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(type=\"text\", text=\"Example prompt text\"),\n            )\n        ],\n    )\n\n\nasync def run():\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n### Writing MCP Clients\n\nThe SDK provides a high-level client interface for connecting to MCP servers:\n\n```python\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"python\",  # Executable\n    args=[\"example_server.py\"],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n\n\n# Optional: create a sampling callback\nasync def handle_sampling_message(\n    message: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult:\n    return types.CreateMessageResult(\n        role=\"assistant\",\n        content=types.TextContent(\n            type=\"text\",\n            text=\"Hello, world! from model\",\n        ),\n        model=\"gpt-3.5-turbo\",\n        stopReason=\"endTurn\",\n    )\n\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(\n            read, write, sampling_callback=handle_sampling_message\n        ) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n\n            # Get a prompt\n            prompt = await session.get_prompt(\n                \"example-prompt\", arguments={\"arg1\": \"value\"}\n            )\n\n            # List available resources\n            resources = await session.list_resources()\n\n            # List available tools\n            tools = await session.list_tools()\n\n            # Read a resource\n            content, mime_type = await session.read_resource(\"file://some/path\")\n\n            # Call a tool\n            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n### MCP Primitives\n\nThe MCP protocol defines three core primitives that servers can implement:\n\n| Primitive | Control               | Description                                         | Example Use                  |\n|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n\n### Server Capabilities\n\nMCP servers declare capabilities during initialization:\n\n| Capability  | Feature Flag                 | Description                        |\n|-------------|------------------------------|------------------------------------|\n| `prompts`   | `listChanged`                | Prompt template management         |\n| `resources` | `subscribe`<br/>`listChanged`| Resource exposure and updates      |\n| `tools`     | `listChanged`                | Tool discovery and execution       |\n| `logging`   | -                            | Server logging configuration       |\n| `completion`| -                            | Argument completion suggestions    |\n\n## Documentation\n\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Model Context Protocol specification](https://spec.modelcontextprotocol.io)\n- [Officially supported servers](https://github.com/modelcontextprotocol/servers)\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-sample-server",
      "npm_downloads": 77,
      "keywords": [
        "retrieval",
        "web",
        "search",
        "web search",
        "automate web",
        "data retrieval"
      ],
      "category": "web-search"
    },
    "zxsimple--reference-servers": {
      "owner": "zxsimple",
      "name": "reference-servers",
      "url": "https://github.com/zxsimple/reference-servers",
      "imageUrl": "/freedevtools/mcp/pfp/zxsimple.webp",
      "description": "Retrieve and process web content from URLs, converting HTML to markdown format for easier extraction and consumption by language models. It supports content truncation and allows for customized starting points for content extraction.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-02T02:58:42Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the Typescript and Python SDK.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[cognee-mcp](https://github.com/topoteretes/cognee-mcp-server)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, and plain text, with other formats like PDF, csv and docx in development.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's RAG Web Browser Actor to perform web searches, scrape URLs, and return content in Markdown.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers.\n\n* [EasyMCP](https://github.com/zcaceres/easy-mcp/) (TypeScript)\n* [FastMCP](https://github.com/punkpeye/fastmcp) (TypeScript)\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "zxsimple",
        "html",
        "search zxsimple",
        "web content",
        "content extraction"
      ],
      "category": "web-search"
    }
  }
}