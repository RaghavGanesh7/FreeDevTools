{
  "category": "search--data-extraction",
  "categoryDisplay": "Search & Data Extraction",
  "description": "",
  "totalRepositories": 44,
  "repositories": {
    "0xdaef0f--job-searchoor": {
      "owner": "0xdaef0f",
      "name": "job-searchoor",
      "url": "https://github.com/0xDAEF0F/job-searchoor",
      "imageUrl": "",
      "description": "An MCP server for searching job listings with filters for date, keywords, remote work options, and more.",
      "stars": 47,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:29:56Z",
      "readme_content": "# Job Searchoor MCP Server\n[![Twitter Follow](https://img.shields.io/twitter/follow/Alex?style=social)](https://x.com/0xdaef0f)\n\nAn MCP server implementation that provides job search functionality.\n\n![mc-demo](https://github.com/user-attachments/assets/87159634-5e4c-41af-ad54-4c5ef19bf9d0)\n\n## Tools\n\nget_jobs\n\nGet available jobs with filtering options\nInputs:\n\nsinceWhen (string): Since when to get available jobs. e.g., '1d' or '1w' (only days and weeks are supported)\nkeywords (string[], optional): Keywords to filter jobs by\nexcludeKeywords (string[], optional): Keywords to exclude from the jobs\nisRemote (boolean, optional): Whether to filter jobs by remote work\n\n## Usage with Claude Desktop\n\nAdd this to your claude_desktop_config.json:\n\n```json{\n\"mcpServers\": {\n    \"job-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"job-searchoor\"]\n    }\n}\n```\n\nLicense\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "searchoor",
        "search",
        "job searchoor",
        "searching job",
        "job listings"
      ],
      "category": "search--data-extraction"
    },
    "Aas-ee--open-webSearch": {
      "owner": "Aas-ee",
      "name": "open-webSearch",
      "url": "https://github.com/Aas-ee/open-webSearch",
      "imageUrl": "",
      "description": "Web search using free multi-engine search (NO API KEYS REQUIRED) — Supports Bing, Baidu, DuckDuckGo, Brave, Exa, and CSDN.",
      "stars": 360,
      "forks": 63,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-04T06:39:48Z",
      "readme_content": "<div align=\"center\">\n\n# Open-WebSearch MCP Server\n\n[![ModelScope](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/Aas-ee/3af09e0f4c7821fb2e9acb96483a5ff0/raw/badge.json&color=%23de5a16)](https://www.modelscope.cn/mcp/servers/Aasee1/open-webSearch)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Aas-ee/open-webSearch)](https://archestra.ai/mcp-catalog/aas-ee__open-websearch)\n[![smithery badge](https://smithery.ai/badge/@Aas-ee/open-websearch)](https://smithery.ai/server/@Aas-ee/open-websearch)\n![Version](https://img.shields.io/github/v/release/Aas-ee/open-websearch)\n![License](https://img.shields.io/github/license/Aas-ee/open-websearch)\n![Issues](https://img.shields.io/github/issues/Aas-ee/open-websearch)\n\n**[🇨🇳 中文](./README-zh.md) | 🇺🇸 English**\n\n</div>\n\nA Model Context Protocol (MCP) server based on multi-engine search results, supporting free web search without API keys.\n\n## Features\n\n- Web search using multi-engine results\n    - bing\n    - baidu\n    - ~~linux.do~~ temporarily unsupported\n    - csdn\n    - duckduckgo\n    - exa\n    - brave\n    - juejin\n- HTTP proxy configuration support for accessing restricted resources\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n- Customizable default search engine\n- Support for fetching individual article content\n    - csdn\n    - github (README files)\n\n## TODO\n- Support for ~~Bing~~ (already supported), ~~DuckDuckGo~~ (already supported), ~~Exa~~ (already supported), ~~Brave~~ (already supported), Google and other search engines\n- Support for more blogs, forums, and social platforms\n- Optimize article content extraction, add support for more sites\n- ~~Support for GitHub README fetching~~ (already supported)\n\n## Installation Guide\n\n### NPX Quick Start (Recommended)\n\nThe fastest way to get started:\n\n```bash\n# Basic usage\nnpx open-websearch@latest\n\n# With environment variables (Linux/macOS)\nDEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true npx open-websearch@latest\n\n# Windows PowerShell\n$env:DEFAULT_SEARCH_ENGINE=\"duckduckgo\"; $env:ENABLE_CORS=\"true\"; npx open-websearch@latest\n\n# Windows CMD\nset MODE=stdio && set DEFAULT_SEARCH_ENGINE=duckduckgo && npx open-websearch@latest\n\n# Cross-platform (requires cross-env, Used for local development)\nnpm install -g open-websearch\nnpx cross-env DEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true open-websearch\n```\n\n**Environment Variables:**\n\n| Variable | Default                 | Options | Description |\n|----------|-------------------------|---------|-------------|\n| `ENABLE_CORS` | `false`                 | `true`, `false` | Enable CORS |\n| `CORS_ORIGIN` | `*`                     | Any valid origin | CORS origin configuration |\n| `DEFAULT_SEARCH_ENGINE` | `bing`                  | `bing`, `duckduckgo`, `exa`, `brave`, `baidu`, `csdn`, `juejin` | Default search engine |\n| `USE_PROXY` | `false`                 | `true`, `false` | Enable HTTP proxy |\n| `PROXY_URL` | `http://127.0.0.1:7890` | Any valid URL | Proxy server URL |\n| `MODE` | `both`                  | `both`, `http`, `stdio` | Server mode: both HTTP+STDIO, HTTP only, or STDIO only |\n| `PORT` | `3000`                  | 1-65535 | Server port |\n| `ALLOWED_SEARCH_ENGINES` | empty (all available) | Comma-separated engine names | Limit which search engines can be used; if the default engine is not in this list, the first allowed engine becomes the default |\n\n**Common configurations:**\n```bash\n# Enable proxy for restricted regions\nUSE_PROXY=true PROXY_URL=http://127.0.0.1:7890 npx open-websearch@latest\n\n# Full configuration\nDEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true USE_PROXY=true PROXY_URL=http://127.0.0.1:7890 PORT=8080 npx open-websearch@latest\n```\n\n### Local Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\n**Cherry Studio:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"name\": \"Web Search MCP\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"Multi-engine web search with article fetching\",\n      \"isActive\": true,\n      \"baseUrl\": \"http://localhost:3000/mcp\"\n    }\n  }\n}\n```\n\n**VSCode (Claude Dev Extension):**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"transport\": {\n        \"type\": \"streamableHttp\",\n        \"url\": \"http://localhost:3000/mcp\"\n      }\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"transport\": {\n        \"type\": \"streamableHttp\",\n        \"url\": \"http://localhost:3000/mcp\"\n      }\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n**NPX Command Line Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"args\": [\n        \"open-websearch@latest\"\n      ],\n      \"command\": \"npx\",\n      \"env\": {\n        \"MODE\": \"stdio\",\n        \"DEFAULT_SEARCH_ENGINE\": \"duckduckgo\",\n        \"ALLOWED_SEARCH_ENGINES\": \"duckduckgo,bing,exa\"\n      }\n    }\n  }\n}\n```\n\n**Local STDIO Configuration for Cherry Studio (Windows):**\n```json\n{\n  \"mcpServers\": {\n    \"open-websearch-local\": {\n      \"command\": \"node\",\n      \"args\": [\"C:/path/to/your/project/build/index.js\"],\n      \"env\": {\n        \"MODE\": \"stdio\",\n        \"DEFAULT_SEARCH_ENGINE\": \"duckduckgo\",\n        \"ALLOWED_SEARCH_ENGINES\": \"duckduckgo,bing,exa\"\n      }\n    }\n  }\n}\n```\n\n### Docker Deployment\n\nQuick deployment using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\nOr use Docker directly:\n```bash\ndocker run -d --name web-search -p 3000:3000 -e ENABLE_CORS=true -e CORS_ORIGIN=* ghcr.io/aas-ee/open-web-search:latest\n```\n\nEnvironment variable configuration:\n\n| Variable | Default                 | Options | Description |\n|----------|-------------------------|---------|-------------|\n| `ENABLE_CORS` | `false`                 | `true`, `false` | Enable CORS |\n| `CORS_ORIGIN` | `*`                     | Any valid origin | CORS origin configuration |\n| `DEFAULT_SEARCH_ENGINE` | `bing`                  | `bing`, `duckduckgo`, `exa`, `brave` | Default search engine |\n| `USE_PROXY` | `false`                 | `true`, `false` | Enable HTTP proxy |\n| `PROXY_URL` | `http://127.0.0.1:7890` | Any valid URL | Proxy server URL |\n| `PORT` | `3000`                  | 1-65535 | Server port |\n\nThen configure in your MCP client:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"name\": \"Web Search MCP\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"Multi-engine web search with article fetching\",\n      \"isActive\": true,\n      \"baseUrl\": \"http://localhost:3000/mcp\"\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"name\": \"Web Search MCP\",\n        \"type\": \"sse\",\n        \"description\": \"Multi-engine web search with article fetching\",\n        \"isActive\": true,\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n## Usage Guide\n\nThe server provides four tools: `search`, `fetchLinuxDoArticle`, `fetchCsdnArticle`, and `fetchGithubReadme`.\n\n### search Tool Usage\n\n```typescript\n{\n  \"query\": string,        // Search query\n  \"limit\": number,        // Optional: Number of results to return (default: 10)\n  \"engines\": string[]     // Optional: Engines to use (bing,baidu,linuxdo,csdn,duckduckgo,exa,brave,juejin) default bing\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"search content\",\n    limit: 3,  // Optional parameter\n    engines: [\"bing\", \"csdn\", \"duckduckgo\", \"exa\", \"brave\", \"juejin\"] // Optional parameter, supports multi-engine combined search\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description text of the search result...\",\n    \"source\": \"Source\",\n    \"engine\": \"Engine used\"\n  }\n]\n```\n\n### fetchCsdnArticle Tool Usage\n\nUsed to fetch complete content of CSDN blog articles.\n\n```typescript\n{\n  \"url\": string    // URL from CSDN search results using the search tool\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchCsdnArticle\",\n  arguments: {\n    url: \"https://blog.csdn.net/xxx/article/details/xxx\"\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"Example search result\"\n  }\n]\n```\n\n### fetchLinuxDoArticle Tool Usage\n\nUsed to fetch complete content of Linux.do forum articles.\n\n```typescript\n{\n  \"url\": string    // URL from linuxdo search results using the search tool\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchLinuxDoArticle\",\n  arguments: {\n    url: \"https://xxxx.json\"\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"Example search result\"\n  }\n]\n```\n\n### fetchGithubReadme Tool Usage\n\nUsed to fetch README content from GitHub repositories.\n\n```typescript\n{\n  \"url\": string    // GitHub repository URL (supports HTTPS, SSH formats)\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchGithubReadme\",\n  arguments: {\n    url: \"https://github.com/Aas-ee/open-webSearch\"\n  }\n})\n```\n\nSupported URL formats:\n- HTTPS: `https://github.com/owner/repo`\n- HTTPS with .git: `https://github.com/owner/repo.git`\n- SSH: `git@github.com:owner/repo.git`\n- URLs with parameters: `https://github.com/owner/repo?tab=readme`\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"<div align=\\\"center\\\">\\n\\n# Open-WebSearch MCP Server...\"\n  }\n]\n```\n\n### fetchJuejinArticle Tool Usage\n\nUsed to fetch complete content of Juejin articles.\n\n```typescript\n{\n  \"url\": string    // Juejin article URL from search results\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchJuejinArticle\",\n  arguments: {\n    url: \"https://juejin.cn/post/7520959840199360563\"\n  }\n})\n```\n\nSupported URL format:\n- `https://juejin.cn/post/{article_id}`\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"🚀 开源 AI 联网搜索工具：Open-WebSearch MCP 全新升级，支持多引擎 + 流式响应...\"\n  }\n]\n```\n\n## Usage Limitations\n\nSince this tool works by scraping multi-engine search results, please note the following important limitations:\n\n1. **Rate Limiting**:\n    - Too many searches in a short time may cause the used engines to temporarily block requests\n    - Recommendations:\n        - Maintain reasonable search frequency\n        - Use the limit parameter judiciously\n        - Add delays between searches when necessary\n\n2. **Result Accuracy**:\n    - Depends on the HTML structure of corresponding engines, may fail when engines update\n    - Some results may lack metadata like descriptions\n    - Complex search operators may not work as expected\n\n3. **Legal Terms**:\n    - This tool is for personal use only\n    - Please comply with the terms of service of corresponding engines\n    - Implement appropriate rate limiting based on your actual use case\n\n4. **Search Engine Configuration**:\n   - Default search engine can be set via the `DEFAULT_SEARCH_ENGINE` environment variable\n   - Supported engines: bing, duckduckgo, exa, brave\n   - The default engine is used when searching specific websites\n\n5. **Proxy Configuration**:\n   - HTTP proxy can be configured when certain search engines are unavailable in specific regions\n   - Enable proxy with environment variable `USE_PROXY=true`\n   - Configure proxy server address with `PROXY_URL`\n\n## Contributing\n\nWelcome to submit issue reports and feature improvement suggestions!\n\n### Contributor Guide\n\nIf you want to fork this repository and publish your own Docker image, you need to make the following configurations:\n\n#### GitHub Secrets Configuration\n\nTo enable automatic Docker image building and publishing, please add the following secrets in your GitHub repository settings (Settings → Secrets and variables → Actions):\n\n**Required Secrets:**\n- `GITHUB_TOKEN`: Automatically provided by GitHub (no setup needed)\n\n**Optional Secrets (for Alibaba Cloud ACR):**\n- `ACR_REGISTRY`: Your Alibaba Cloud Container Registry URL (e.g., `registry.cn-hangzhou.aliyuncs.com`)\n- `ACR_USERNAME`: Your Alibaba Cloud ACR username\n- `ACR_PASSWORD`: Your Alibaba Cloud ACR password\n- `ACR_IMAGE_NAME`: Your image name in ACR (e.g., `your-namespace/open-web-search`)\n\n#### CI/CD Workflow\n\nThe repository includes a GitHub Actions workflow (`.github/workflows/docker.yml`) that automatically:\n\n1. **Trigger Conditions**:\n    - Push to `main` branch\n    - Push version tags (`v*`)\n    - Manual workflow trigger\n\n2. **Build and Push to**:\n    - GitHub Container Registry (ghcr.io) - always enabled\n    - Alibaba Cloud Container Registry - only enabled when ACR secrets are configured\n\n3. **Image Tags**:\n    - `ghcr.io/your-username/open-web-search:latest`\n    - `your-acr-address/your-image-name:latest` (if ACR is configured)\n\n#### Fork and Publish Steps:\n\n1. **Fork the repository** to your GitHub account\n2. **Configure secrets** (if you need ACR publishing):\n    - Go to Settings → Secrets and variables → Actions in your forked repository\n    - Add the ACR-related secrets listed above\n3. **Push changes** to the `main` branch or create version tags\n4. **GitHub Actions will automatically build and push** your Docker image\n5. **Use your image**, update the Docker command:\n   ```bash\n   docker run -d --name web-search -p 3000:3000 -e ENABLE_CORS=true -e CORS_ORIGIN=* ghcr.io/your-username/open-web-search:latest\n   ```\n\n#### Notes:\n- If you don't configure ACR secrets, the workflow will only publish to GitHub Container Registry\n- Make sure your GitHub repository has Actions enabled\n- The workflow will use your GitHub username (converted to lowercase) as the GHCR image name\n\n<div align=\"center\">\n\n## Star History\nIf you find this project helpful, please consider giving it a ⭐ Star!\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Aas-ee/open-webSearch&type=Date)](https://www.star-history.com/#Aas-ee/open-webSearch&Date)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websearch",
        "bing",
        "search",
        "engine search",
        "websearch web",
        "web search"
      ],
      "category": "search--data-extraction"
    },
    "Bigsy--Clojars-MCP-Server": {
      "owner": "Bigsy",
      "name": "Clojars-MCP-Server",
      "url": "https://github.com/Bigsy/Clojars-MCP-Server",
      "imageUrl": "",
      "description": "Clojars MCP Server for upto date dependency information of Clojure libraries",
      "stars": 5,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-06-25T16:21:55Z",
      "readme_content": "# Clojars MCP Server\n\n[![npm version](https://img.shields.io/npm/v/clojars-deps-server.svg)](https://www.npmjs.com/package/clojars-deps-server)\n\nA [Model Context Protocol (MCP)](https://github.com/ModelContext/protocol) server that provides tools for fetching dependency information from [Clojars](https://clojars.org/), the Clojure community's artifact repository for Cline, Roo Code, Cody, Claude Desktop etc.\n\n<a href=\"https://glama.ai/mcp/servers/i37857er6w\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/i37857er6w/badge\" alt=\"Clojars-MCP-Server MCP server\" /></a>\n\n## Installation\n\n### Installing via npx\n\nThe quickest way to use the Clojars MCP Server is to run it directly with npx:\n\n```bash\nnpx clojars-deps-server\n```\n\nYou can also install it globally:\n\n```bash\nnpm install -g clojars-deps-server\n```\n\n### Installing via Smithery\n\nTo install Clojars Dependency Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/clojars-deps-server):\n\n```bash\nnpx -y @smithery/cli install clojars-deps-server --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n```bash\ngit clone https://github.com/yourusername/clojars-deps-server.git\ncd clojars-deps-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Add the server to your Claude configuration:\n\nFor VSCode Claude extension, add to `cline_mcp_settings.json` (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/` on macOS):\n```json\n{\n  \"mcpServers\": {\n    \"clojars-deps-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/clojars-deps-server/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude desktop app, add to `claude_desktop_config.json` (typically located at `~/Library/Application Support/Claude/` on macOS):\n```json\n{\n  \"mcpServers\": {\n    \"clojars-deps-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/clojars-deps-server/build/index.js\"]\n    }\n  }\n}\n```\n\nAfter adding the server configuration, Claude will automatically detect and connect to the server on startup. The server's capabilities will be listed in Claude's system prompt under \"Connected MCP Servers\", making them available for use.\n\n\n## Features\n\n- Get the latest version of any Clojars dependency\n- Check if a specific version of a dependency exists\n- Get version history of dependencies with configurable limits\n- Simple, focused responses\n- Easy integration with Claude through MCP\n\n## How It Works\n\nWhen this MCP server is configured in Claude's settings, it automatically becomes available in Claude's system prompt under the \"Connected MCP Servers\" section. This makes Claude aware of the server's capabilities and allows it to use the provided tools through the `use_mcp_tool` command.\n\nThe server exposes three tools:\n\n### get_clojars_latest_version\n```json\n{\n  \"name\": \"get_clojars_latest_version\",\n  \"description\": \"Get the latest version of a Clojars dependency (Maven artifact)\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      }\n    },\n    \"required\": [\"dependency\"]\n  }\n}\n```\n\n### check_clojars_version_exists\n```json\n{\n  \"name\": \"check_clojars_version_exists\",\n  \"description\": \"Check if a specific version of a Clojars dependency exists\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      },\n      \"version\": {\n        \"type\": \"string\",\n        \"description\": \"Version to check (e.g. \\\"0.7.2\\\")\"\n      }\n    },\n    \"required\": [\"dependency\", \"version\"]\n  }\n}\n```\n\n### get_clojars_history\n```json\n{\n  \"name\": \"get_clojars_history\",\n  \"description\": \"Get version history of a Clojars dependency\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      },\n      \"limit\": {\n        \"type\": \"number\",\n        \"description\": \"Number of versions to return (default: 15, max: 100)\",\n        \"minimum\": 1,\n        \"maximum\": 100\n      }\n    },\n    \"required\": [\"dependency\"]\n  }\n}\n```\n\nThe tool names and descriptions are specifically designed to help Claude understand that these tools are for retrieving version information from Clojars. When users ask about Clojars dependencies, Claude can recognize that these tools are appropriate for the task based on:\n- The tool names explicitly indicate their purpose\n- The descriptions specify they're for \"Clojars dependency (Maven artifact)\"\n- The example formats show typical Clojars dependency patterns\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "clojure",
        "clojars",
        "mcp",
        "clojure libraries",
        "information clojure",
        "server clojars"
      ],
      "category": "search--data-extraction"
    },
    "Dumpling-AI--mcp-server-dumplingai": {
      "owner": "Dumpling-AI",
      "name": "mcp-server-dumplingai",
      "url": "https://github.com/Dumpling-AI/mcp-server-dumplingai",
      "imageUrl": "",
      "description": "Access data, web scraping, and document conversion APIs by [Dumpling AI](https://www.dumplingai.com/)",
      "stars": 27,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T08:58:27Z",
      "readme_content": "# Dumpling AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with Dumpling AI for data scraping, content processing, knowledge management, AI agents, and code execution capabilities.\n\n[![smithery badge](https://smithery.ai/badge/@Dumpling-AI/mcp-server-dumplingai)](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai)\n\n## Features\n\n- Complete integration with all Dumpling AI API endpoints\n- Data APIs for YouTube transcripts, search, autocomplete, maps, places, news, and reviews\n- Web scraping with support for scraping, crawling, screenshots, and structured data extraction\n- Document conversion tools for text extraction, PDF operations, video processing\n- Extract data from documents, images, audio, and video\n- AI capabilities including agent completions, knowledge base management, and image generation\n- Developer tools for running JavaScript and Python code in a secure environment\n- Automatic error handling and detailed response formatting\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-dumplingai for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai):\n\n```bash\nnpx -y @smithery/cli install @Dumpling-AI/mcp-server-dumplingai --client claude\n```\n\n### Running with npx\n\n```bash\nenv DUMPLING_API_KEY=your_api_key npx -y mcp-server-dumplingai\n```\n\n### Manual Installation\n\n```bash\nnpm install -g mcp-server-dumplingai\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️ Note: Requires Cursor version 0.45.6+\n\nTo configure Dumpling AI MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n\n```\n{\n  \"mcpServers\": {\n    \"dumplingai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-server-dumplingai\"],\n      \"env\": {\n        \"DUMPLING_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set DUMPLING_API_KEY=your-api-key && npx -y mcp-server-dumplingai\"`\n\nReplace `your-api-key` with your Dumpling AI API key.\n\n## Configuration\n\n### Environment Variables\n\n- `DUMPLING_API_KEY`: Your Dumpling AI API key (required)\n\n## Available Tools\n\n### Data APIs\n\n#### 1. Get YouTube Transcript (`get-youtube-transcript`)\n\nExtract transcripts from YouTube videos with optional timestamps.\n\n```json\n{\n  \"name\": \"get-youtube-transcript\",\n  \"arguments\": {\n    \"videoUrl\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"includeTimestamps\": true,\n    \"timestampsToCombine\": 3,\n    \"preferredLanguage\": \"en\"\n  }\n}\n```\n\n#### 2. Search (`search`)\n\nPerform Google web searches and optionally scrape content from results.\n\n```json\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"machine learning basics\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastMonth\",\n    \"scrapeResults\": true,\n    \"numResultsToScrape\": 3,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true\n    }\n  }\n}\n```\n\n#### 3. Get Autocomplete (`get-autocomplete`)\n\nGet Google search autocomplete suggestions for a query.\n\n```json\n{\n  \"name\": \"get-autocomplete\",\n  \"arguments\": {\n    \"query\": \"how to learn\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"location\": \"New York\"\n  }\n}\n```\n\n#### 4. Search Maps (`search-maps`)\n\nSearch Google Maps for locations and businesses.\n\n```json\n{\n  \"name\": \"search-maps\",\n  \"arguments\": {\n    \"query\": \"coffee shops\",\n    \"gpsPositionZoom\": \"37.7749,-122.4194,14z\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 5. Search Places (`search-places`)\n\nSearch for places with more detailed information.\n\n```json\n{\n  \"name\": \"search-places\",\n  \"arguments\": {\n    \"query\": \"hotels in paris\",\n    \"country\": \"fr\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 6. Search News (`search-news`)\n\nSearch for news articles with customizable parameters.\n\n```json\n{\n  \"name\": \"search-news\",\n  \"arguments\": {\n    \"query\": \"climate change\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastWeek\"\n  }\n}\n```\n\n#### 7. Get Google Reviews (`get-google-reviews`)\n\nRetrieve Google reviews for businesses or places.\n\n```json\n{\n  \"name\": \"get-google-reviews\",\n  \"arguments\": {\n    \"businessName\": \"Eiffel Tower\",\n    \"location\": \"Paris, France\",\n    \"limit\": 10,\n    \"sortBy\": \"relevance\"\n  }\n}\n```\n\n### Web Scraping\n\n#### 8. Scrape (`scrape`)\n\nExtract content from a web page with formatting options.\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\",\n    \"cleaned\": true,\n    \"renderJs\": true\n  }\n}\n```\n\n#### 9. Crawl (`crawl`)\n\nRecursively crawl websites and extract content with customizable parameters.\n\n```json\n{\n  \"name\": \"crawl\",\n  \"arguments\": {\n    \"baseUrl\": \"https://example.com\",\n    \"maxPages\": 10,\n    \"crawlBeyondBaseUrl\": false,\n    \"depth\": 2,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true,\n      \"renderJs\": true\n    }\n  }\n}\n```\n\n#### 10. Screenshot (`screenshot`)\n\nCapture screenshots of web pages with customizable viewport and format options.\n\n```json\n{\n  \"name\": \"screenshot\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"width\": 1280,\n    \"height\": 800,\n    \"fullPage\": true,\n    \"format\": \"png\",\n    \"waitFor\": 1000\n  }\n}\n```\n\n#### 11. Extract (`extract`)\n\nExtract structured data from web pages using AI-powered instructions.\n\n```json\n{\n  \"name\": \"extract\",\n  \"arguments\": {\n    \"url\": \"https://example.com/products\",\n    \"instructions\": \"Extract all product names, prices, and descriptions from this page\",\n    \"schema\": {\n      \"products\": [\n        {\n          \"name\": \"string\",\n          \"price\": \"number\",\n          \"description\": \"string\"\n        }\n      ]\n    },\n    \"renderJs\": true\n  }\n}\n```\n\n### Document Conversion\n\n#### 12. Doc to Text (`doc-to-text`)\n\nConvert documents to plaintext with optional OCR.\n\n```json\n{\n  \"name\": \"doc-to-text\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n#### 13. Convert to PDF (`convert-to-pdf`)\n\nConvert various file formats to PDF.\n\n```json\n{\n  \"name\": \"convert-to-pdf\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.docx\",\n    \"format\": \"docx\",\n    \"options\": {\n      \"quality\": 90,\n      \"pageSize\": \"A4\",\n      \"margin\": 10\n    }\n  }\n}\n```\n\n#### 14. Merge PDFs (`merge-pdfs`)\n\nCombine multiple PDFs into a single document.\n\n```json\n{\n  \"name\": \"merge-pdfs\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/doc1.pdf\", \"https://example.com/doc2.pdf\"],\n    \"options\": {\n      \"addPageNumbers\": true,\n      \"addTableOfContents\": true\n    }\n  }\n}\n```\n\n#### 15. Trim Video (`trim-video`)\n\nExtract a specific clip from a video.\n\n```json\n{\n  \"name\": \"trim-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"startTime\": 30,\n    \"endTime\": 60,\n    \"output\": \"mp4\",\n    \"options\": {\n      \"quality\": 720,\n      \"fps\": 30\n    }\n  }\n}\n```\n\n#### 16. Extract Document (`extract-document`)\n\nExtract specific content from documents in various formats.\n\n```json\n{\n  \"name\": \"extract-document\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"format\": \"structured\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\",\n      \"includeMetadata\": true\n    }\n  }\n}\n```\n\n#### 17. Extract Image (`extract-image`)\n\nExtract text and information from images.\n\n```json\n{\n  \"name\": \"extract-image\",\n  \"arguments\": {\n    \"url\": \"https://example.com/image.jpg\",\n    \"extractionType\": \"text\",\n    \"options\": {\n      \"language\": \"en\",\n      \"detectOrientation\": true\n    }\n  }\n}\n```\n\n#### 18. Extract Audio (`extract-audio`)\n\nTranscribe and extract information from audio files.\n\n```json\n{\n  \"name\": \"extract-audio\",\n  \"arguments\": {\n    \"url\": \"https://example.com/audio.mp3\",\n    \"language\": \"en\",\n    \"options\": {\n      \"model\": \"enhanced\",\n      \"speakerDiarization\": true,\n      \"wordTimestamps\": true\n    }\n  }\n}\n```\n\n#### 19. Extract Video (`extract-video`)\n\nExtract content from videos including transcripts, scenes, and objects.\n\n```json\n{\n  \"name\": \"extract-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"extractionType\": \"transcript\",\n    \"options\": {\n      \"language\": \"en\",\n      \"speakerDiarization\": true\n    }\n  }\n}\n```\n\n#### 20. Read PDF Metadata (`read-pdf-metadata`)\n\nExtract metadata from PDF files.\n\n```json\n{\n  \"name\": \"read-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"includeExtended\": true\n  }\n}\n```\n\n#### 21. Write PDF Metadata (`write-pdf-metadata`)\n\nUpdate metadata in PDF files.\n\n```json\n{\n  \"name\": \"write-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"metadata\": {\n      \"title\": \"New Title\",\n      \"author\": \"John Doe\",\n      \"keywords\": [\"keyword1\", \"keyword2\"]\n    }\n  }\n}\n```\n\n### AI\n\n#### 22. Generate Agent Completion (`generate-agent-completion`)\n\nGet AI agent completions with optional tool definitions.\n\n```json\n{\n  \"name\": \"generate-agent-completion\",\n  \"arguments\": {\n    \"prompt\": \"How can I improve my website's SEO?\",\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"maxTokens\": 500,\n    \"context\": [\"The website is an e-commerce store selling handmade crafts.\"]\n  }\n}\n```\n\n#### 23. Search Knowledge Base (`search-knowledge-base`)\n\nSearch a knowledge base for relevant information.\n\n```json\n{\n  \"name\": \"search-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"query\": \"How to optimize database performance\",\n    \"limit\": 5,\n    \"similarityThreshold\": 0.7\n  }\n}\n```\n\n#### 24. Add to Knowledge Base (`add-to-knowledge-base`)\n\nAdd entries to a knowledge base.\n\n```json\n{\n  \"name\": \"add-to-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"entries\": [\n      {\n        \"text\": \"MongoDB is a document-based NoSQL database.\",\n        \"metadata\": {\n          \"source\": \"MongoDB documentation\",\n          \"category\": \"databases\"\n        }\n      }\n    ],\n    \"upsert\": true\n  }\n}\n```\n\n#### 25. Generate AI Image (`generate-ai-image`)\n\nGenerate images using AI models.\n\n```json\n{\n  \"name\": \"generate-ai-image\",\n  \"arguments\": {\n    \"prompt\": \"A futuristic city with flying cars and neon lights\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1,\n    \"quality\": \"hd\",\n    \"style\": \"photorealistic\"\n  }\n}\n```\n\n#### 26. Generate Image (`generate-image`)\n\nGenerate images using various AI providers.\n\n```json\n{\n  \"name\": \"generate-image\",\n  \"arguments\": {\n    \"prompt\": \"A golden retriever in a meadow of wildflowers\",\n    \"provider\": \"dalle\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1\n  }\n}\n```\n\n### Developer Tools\n\n#### 27. Run JavaScript Code (`run-js-code`)\n\nExecute JavaScript code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-js-code\",\n  \"arguments\": {\n    \"code\": \"const result = [1, 2, 3, 4].reduce((sum, num) => sum + num, 0); console.log(`Sum: ${result}`); return result;\",\n    \"dependencies\": {\n      \"lodash\": \"^4.17.21\"\n    },\n    \"timeout\": 5000\n  }\n}\n```\n\n#### 28. Run Python Code (`run-python-code`)\n\nExecute Python code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-python-code\",\n  \"arguments\": {\n    \"code\": \"import numpy as np\\narr = np.array([1, 2, 3, 4, 5])\\nmean = np.mean(arr)\\nprint(f'Mean: {mean}')\\nreturn mean\",\n    \"dependencies\": [\"numpy\", \"pandas\"],\n    \"timeout\": 10000,\n    \"saveOutputFiles\": true\n  }\n}\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages with HTTP status codes\n- API key validation\n- Input validation using Zod schemas\n- Network error handling with descriptive messages\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Failed to fetch YouTube transcript: 404 Not Found\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n```\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "apis",
        "search",
        "server dumplingai",
        "apis dumpling",
        "dumplingai access"
      ],
      "category": "search--data-extraction"
    },
    "Himalayas-App--himalayas-mcp": {
      "owner": "Himalayas-App",
      "name": "himalayas-mcp",
      "url": "https://github.com/Himalayas-App/himalayas-mcp",
      "imageUrl": "",
      "description": "Access tens of thousands of remote job listings and company information. This public MCP server provides real-time access to Himalayas' remote jobs database.",
      "stars": 9,
      "forks": 2,
      "license": "No License",
      "language": "",
      "updated_at": "2025-09-20T10:52:19Z",
      "readme_content": "# Himalayas Remote Jobs MCP Server\n\nAccess thousands of remote job listings and company information directly from your AI coding assistant! This public MCP server provides real-time access to Himalayas.app's remote job database.\n\n🌐 **Public Server URL:** `https://mcp.himalayas.app/sse`\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=himalayas&config=eyJjb21tYW5kIjoibnB4IG1jcC1yZW1vdGUgaHR0cHM6Ly9tY3AuaGltYWxheWFzLmFwcC9zc2UifQ%3D%3D)\n\n## Available Tools\n\n### 🔍 Job Search Tools\n\n#### `search_jobs`\nSearch for specific jobs using keywords with advanced filtering.\n\n**Parameters:**\n- `keyword` (string, optional): Search term (e.g., 'Python', 'React', 'Product Manager', 'Data Scientist')\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter jobs by country\n- `worldwide` (boolean, optional): Show ONLY 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Search for Python developer jobs\"\n- \"Find React jobs in the United States\"\n- \"Look for product manager positions worldwide\"\n\n#### `get_jobs`\nRetrieve the latest remote job listings with optional filtering.\n\n**Parameters:**\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter jobs by country (e.g., 'Canada', 'United States', 'UK', 'Germany')\n- `worldwide` (boolean, optional): Show ONLY 100% remote jobs available worldwide (default: false)\n\n**Example usage:**\n- \"Get remote jobs in Canada\"\n- \"Show me worldwide remote opportunities\"\n- \"Find jobs on page 2\"\n\n### 🏢 Company Search Tools\n\n#### `search_companies`\nSearch for specific companies using keywords.\n\n**Parameters:**\n- `keyword` (string, optional): Search term (e.g., 'startup', 'fintech', 'AI', company name)\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter companies by country\n- `worldwide` (boolean, optional): Show only companies with 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Search for AI startups\"\n- \"Find fintech companies with remote jobs\"\n- \"Look for companies named 'Stripe'\"\n\n#### `get_companies`\nBrowse remote-friendly companies with optional filtering.\n\n**Parameters:**\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter companies by country\n- `worldwide` (boolean, optional): Show only companies with 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Show me remote companies in Europe\"\n- \"Find companies with worldwide remote jobs\"\n\n## Setup Instructions\n\n### 🖥️ Claude Desktop\n\n1. Open Claude Desktop and navigate to **Settings → Developer → Edit Config**\n2. Replace the content with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"himalayas\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.himalayas.app/sse\"]\n    }\n  }\n}\n```\n\n3. Save the file and restart Claude Desktop (Cmd/Ctrl + R)\n4. When Claude restarts, a browser window will open for OAuth login - complete the authorization\n5. You'll see the Himalayas tools available by clicking the tools icon (🔨) in the bottom right\n\n### ⚡ Cursor\n\n1. Open Cursor and go to **Settings → Features → Rules for AI**\n2. Choose **Type: \"Command\"**\n3. In the **Command** field, enter:\n```bash\nnpx mcp-remote https://mcp.himalayas.app/sse\n```\n4. Save the configuration and restart Cursor\n5. Complete the OAuth flow when prompted\n\n### 🌊 Windsurf\n\n1. Edit your `mcp_config.json` file\n2. Add this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"himalayas\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.himalayas.app/sse\"]\n    }\n  }\n}\n```\n\n3. Save the file and restart Windsurf\n4. Complete the OAuth authorization when prompted\n\n## Example Conversations\n\nOnce connected, you can have natural conversations with your AI assistant:\n\n### Job Searching\n- *\"I'm looking for remote Python developer jobs in Europe\"*\n- *\"Show me the latest remote data science positions\"*\n- *\"Find part-time remote marketing jobs worldwide\"*\n- *\"Search for senior software engineer roles in Canada\"*\n\n### Company Research\n- *\"Find AI startups that offer remote work\"*\n- *\"Show me fintech companies with remote opportunities\"*\n- *\"Look for companies in Germany that hire remotely\"*\n- *\"Find verified companies with 4-day work weeks\"*\n\n### Advanced Queries\n- *\"Compare remote job opportunities between the US and UK\"*\n- *\"Find companies that offer both remote work and competitive salaries\"*\n- *\"Show me the tech stack used by remote-first companies\"*\n\n## What You'll Get\n\nEach response includes rich, formatted information:\n\n### Job Listings\n- 🚀 Job title and company name\n- 💼 Employment type (Full-time, Part-time, Contract)\n- 🌍 Location restrictions or worldwide availability\n- 💰 Salary ranges and currency\n- 🛠️ Required skills and technologies\n- 🔗 Direct application links\n- 🏢 Company verification status\n\n### Company Profiles\n- 🏢 Company name and verification status\n- 👥 Company size and founding year\n- 🌍 Locations and remote work policies\n- 🔥 Number of open positions\n- 🎁 Benefits and perks information\n- ⚡ Technology stacks used\n- 🌐 Company website and social links\n\n## Troubleshooting\n\n### Connection Issues\n- Ensure you have Node.js installed for the `npx` command\n- Try restarting your AI assistant after configuration changes\n- Clear authentication cache if needed: `rm -rf ~/.mcp-auth`\n\n### Authentication Problems\n- Complete the OAuth flow in the browser window that opens\n- Make sure you're using the correct server URL: `https://mcp.himalayas.app/sse`\n- Check that your internet connection is stable\n\n### Tool Not Appearing\n- Verify the configuration file syntax is correct (valid JSON)\n- Restart your AI assistant completely\n- Check the tools icon/menu in your AI assistant's interface\n\n## Support\n\nIf you encounter any issues:\n1. Check the troubleshooting section above\n2. Verify your configuration matches the examples exactly\n3. Try the connection with the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) first\n\n---\n\n**Powered by [Himalayas.app](https://himalayas.app)** - The best place to find remote jobs and companies 🏔️\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "himalayas",
        "jobs",
        "listings",
        "access himalayas",
        "himalayas mcp",
        "himalayas app"
      ],
      "category": "search--data-extraction"
    },
    "Ihor-Sokoliuk--MCP-SearXNG": {
      "owner": "Ihor-Sokoliuk",
      "name": "MCP-SearXNG",
      "url": "https://github.com/ihor-sokoliuk/mcp-searxng",
      "imageUrl": "",
      "description": "A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)",
      "stars": 247,
      "forks": 48,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:28:35Z",
      "readme_content": "# SearXNG MCP Server\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) implementation that integrates the [SearXNG](https://docs.searxng.org) API, providing web search capabilities.\n\n[![https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true](https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true)](https://www.npmjs.com/package/mcp-searxng)\n\n[![https://badgen.net/docker/pulls/isokoliuk/mcp-searxng](https://badgen.net/docker/pulls/isokoliuk/mcp-searxng)](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n<a href=\"https://glama.ai/mcp/servers/0j7jjyt7m9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0j7jjyt7m9/badge\" alt=\"SearXNG Server MCP server\" /></a>\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination.\n- **URL Content Reading**: Advanced content extraction with pagination, section filtering, and heading extraction.\n- **Intelligent Caching**: URL content is cached with TTL (Time-To-Live) to improve performance and reduce redundant requests.\n- **Pagination**: Control which page of results to retrieve.\n- **Time Filtering**: Filter results by time range (day, month, year).\n- **Language Selection**: Filter results by preferred language.\n- **Safe Search**: Control content filtering level for search results.\n\n## Tools\n\n- **searxng_web_search**\n  - Execute web searches with pagination\n  - Inputs:\n    - `query` (string): The search query. This string is passed to external search services.\n    - `pageno` (number, optional): Search page number, starts at 1 (default 1)\n    - `time_range` (string, optional): Filter results by time range - one of: \"day\", \"month\", \"year\" (default: none)\n    - `language` (string, optional): Language code for results (e.g., \"en\", \"fr\", \"de\") or \"all\" (default: \"all\")\n    - `safesearch` (number, optional): Safe search filter level (0: None, 1: Moderate, 2: Strict) (default: instance setting)\n\n- **web_url_read**\n  - Read and convert the content from a URL to markdown with advanced content extraction options\n  - Inputs:\n    - `url` (string): The URL to fetch and process\n    - `startChar` (number, optional): Starting character position for content extraction (default: 0)\n    - `maxLength` (number, optional): Maximum number of characters to return\n    - `section` (string, optional): Extract content under a specific heading (searches for heading text)\n    - `paragraphRange` (string, optional): Return specific paragraph ranges (e.g., '1-5', '3', '10-')\n    - `readHeadings` (boolean, optional): Return only a list of headings instead of full content\n\n## Configuration\n\n### Setting the SEARXNG_URL\n\nThe `SEARXNG_URL` environment variable defines which SearxNG instance to connect to.\n\n#### Environment Variable Format\n```bash\nSEARXNG_URL=<protocol>://<hostname>[:<port>]\n```\n\n#### Examples\n```bash\n# Local development (default)\nSEARXNG_URL=http://localhost:8080\n\n# Public instance\nSEARXNG_URL=https://search.example.com\n\n# Custom port\nSEARXNG_URL=http://my-searxng.local:3000\n```\n\n#### Setup Instructions\n1. Choose a SearxNG instance from the [list of public instances](https://searx.space/) or use your local environment\n2. Set the `SEARXNG_URL` environment variable to the complete instance URL\n3. If not specified, the default value `http://localhost:8080` will be used\n\n### Using Authentication (Optional)\n\nIf you are using a password protected SearxNG instance you can set a username and password for HTTP Basic Auth:\n\n- Set the `AUTH_USERNAME` environment variable to your username\n- Set the `AUTH_PASSWORD` environment variable to your password\n\n**Note:** Authentication is only required for password-protected SearxNG instances. See the usage examples below for how to configure authentication with different installation methods.\n\n### Proxy Support (Optional)\n\nThe server supports HTTP and HTTPS proxies through environment variables. This is useful when running behind corporate firewalls or when you need to route traffic through a specific proxy server.\n\n#### Proxy Environment Variables\n\nSet one or more of these environment variables to configure proxy support:\n\n- `HTTP_PROXY`: Proxy URL for HTTP requests\n- `HTTPS_PROXY`: Proxy URL for HTTPS requests  \n- `http_proxy`: Alternative lowercase version for HTTP requests\n- `https_proxy`: Alternative lowercase version for HTTPS requests\n\n#### Proxy URL Formats\n\nThe proxy URL can be in any of these formats:\n\n```bash\n# Basic proxy\nexport HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\n\n# Proxy with authentication\nexport HTTP_PROXY=http://username:password@proxy.company.com:8080\nexport HTTPS_PROXY=http://username:password@proxy.company.com:8080\n```\n\n**Note:** If no proxy environment variables are set, the server will make direct connections as normal. See the usage examples below for how to configure proxy settings with different installation methods.\n\n### [NPX](https://www.npmjs.com/package/mcp-searxng)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPX Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### [NPM](https://www.npmjs.com/package/mcp-searxng)\n\n```bash\nnpm install -g mcp-searxng\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPM Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### Docker\n\n#### Using [Pre-built Image from Docker Hub](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n```bash\ndocker pull isokoliuk/mcp-searxng:latest\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Build Locally\n\n```bash\ndocker build -t mcp-searxng:latest -f Dockerfile .\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Build Locally Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Docker Compose\n\nCreate a `docker-compose.yml` file:\n\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\nThen configure your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker-compose\",\n      \"args\": [\"run\", \"--rm\", \"mcp-searxng\"]\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Compose Configuration Options</summary>\n\n#### With Authentication\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n```\n\n#### With Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### With Authentication and Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### Using Local Build\n```yaml\nservices:\n  mcp-searxng:\n    build: .\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\n</details>\n\n### HTTP Transport (Optional)\n\nThe server supports both STDIO (default) and HTTP transports:\n\n#### STDIO Transport (Default)\n- **Best for**: Claude Desktop and most MCP clients\n- **Usage**: Automatic - no additional configuration needed\n\n#### HTTP Transport  \n- **Best for**: Web-based applications and remote MCP clients\n- **Usage**: Set the `MCP_HTTP_PORT` environment variable\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional HTTP Transport Configuration Options</summary>\n\n#### HTTP Server with Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n**HTTP Endpoints:**\n- **MCP Protocol**: `POST/GET/DELETE /mcp` \n- **Health Check**: `GET /health`\n- **CORS**: Enabled for web clients\n\n**Testing HTTP Server:**\n```bash\n# Start HTTP server\nMCP_HTTP_PORT=3000 SEARXNG_URL=http://localhost:8080 mcp-searxng\n\n# Check health\ncurl http://localhost:3000/health\n```\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the src/index.ts file, so there is no need to rebuild between tests. You can see the full documentation [here](https://www.mcpevals.io/docs).\n\n```bash\nSEARXNG_URL=SEARXNG_URL OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n```\n\n## For Developers\n\n### Contributing to the Project\n\nWe welcome contributions! Here's how to get started:\n\n#### 0. Coding Guidelines\n\n- Use TypeScript for type safety\n- Follow existing error handling patterns\n- Keep error messages concise but informative\n- Write unit tests for new functionality\n- Ensure all tests pass before submitting PRs\n- Maintain test coverage above 90%\n- Test changes with the MCP inspector\n- Run evals before submitting PRs\n\n#### 1. Fork and Clone\n\n```bash\n# Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/mcp-searxng.git\ncd mcp-searxng\n\n# Add the original repository as upstream\ngit remote add upstream https://github.com/ihor-sokoliuk/mcp-searxng.git\n```\n\n#### 2. Development Setup\n\n```bash\n# Install dependencies\nnpm install\n\n# Start development with file watching\nnpm run watch\n```\n\n#### 3. Development Workflow\n\n1. **Create a feature branch:**\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make your changes** in `src/` directory\n   - Main server logic: `src/index.ts`\n   - Error handling: `src/error-handler.ts`\n\n3. **Build and test:**\n   ```bash\n   npm run build               # Build the project\n   npm test                     # Run unit tests\n   npm run test:coverage       # Run tests with coverage report\n   npm run inspector            # Run MCP inspector\n   ```\n\n4. **Run evals to ensure functionality:**\n   ```bash\n   SEARXNG_URL=http://localhost:8080 OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n   ```\n\n#### 4. Submitting Changes\n\n```bash\n# Commit your changes\ngit add .\ngit commit -m \"feat: description of your changes\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n\n# Create a Pull Request on GitHub\n```\n\n### Testing\n\nThe project includes comprehensive unit tests with excellent coverage.\n\n#### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run with coverage reporting\nnpm run test:coverage\n\n# Watch mode for development\nnpm run test:watch\n```\n\n#### Test Statistics\n- **Unit tests** covering all core modules\n- **100% success rate** with dynamic coverage reporting via c8\n- **HTML coverage reports** generated in `coverage/` directory\n\n#### What's Tested\n- Error handling (network, server, configuration errors)\n- Type validation and schema guards\n- Proxy configurations and environment variables\n- Resource generation and logging functionality\n- All module imports and function availability\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "protocol",
        "search",
        "searxng model",
        "server searxng",
        "mcp searxng"
      ],
      "category": "search--data-extraction"
    },
    "Linked-API--linkedapi-mcp": {
      "owner": "Linked-API",
      "name": "linkedapi-mcp",
      "url": "https://github.com/Linked-API/linkedapi-mcp",
      "imageUrl": "",
      "description": "MCP server that lets AI assistants control LinkedIn accounts and retrieve real-time data.",
      "stars": 11,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-27T12:16:10Z",
      "readme_content": "Linked API MCP server connects your LinkedIn account to AI assistants like Claude, Cursor, and VS Code. Ask them to search for leads, send messages, analyze profiles, and much more – they'll handle it through our cloud browser, safely and automatically.\n\n## Use cases\n- **Sales automation assistant**. Ask your AI to find leads, check their profiles, and draft personalized outreach. It can search for \"software engineers at companies with 50-200 employees in San Francisco\", analyze their backgrounds, and suggest connection messages that actually make sense.\n- **Recruitment assistant**. Let your assistant search for candidates with specific skills, review their experience, and send initial outreach. It handles the time-consuming parts while you focus on actually talking to people.\n- **Conversation assistant**. Your AI can read your existing LinkedIn conversations and help you respond naturally. It understands the context of your chats, suggests relevant replies, and can even send follow-up messages.\n- **Market research assistant**. Need competitor analysis? Your assistant can gather data about companies, their employees, and recent activities. Get insights about industry trends without spending hours on LinkedIn.\n\n## Get started\nTo start using Linked API MCP, spend 2 minutes reading these essential guides:\n\n1. [Installation](https://linkedapi.io/mcp/installation/) – set up MCP in Claude, Cursor, VS Code, or Windsurf.\n2. [Available tools](https://linkedapi.io/mcp/available-tools/) – explore all the LinkedIn tools your assistant can call.\n3. [Usage examples](https://linkedapi.io/mcp/usage-examples/) – see real-world examples to get you started quickly.\n\n## License\nThis project is licensed under the MIT – see the [LICENSE](https://github.com/Linked-API/linkedapi-mcp/blob/main/LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkedapi",
        "linkedin",
        "api",
        "linkedapi mcp",
        "api linkedapi",
        "linked api"
      ],
      "category": "search--data-extraction"
    },
    "OctagonAI--octagon-deep-research-mcp": {
      "owner": "OctagonAI",
      "name": "octagon-deep-research-mcp",
      "url": "https://github.com/OctagonAI/octagon-deep-research-mcp",
      "imageUrl": "",
      "description": "Lightning-Fast, High-Accuracy Deep Research Agent",
      "stars": 61,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T03:20:57Z",
      "readme_content": "# Octagon Deep Research MCP\n\n![Favicon](https://docs.octagonagents.com/logo.svg) The Octagon Deep Research MCP server provides specialized AI-powered comprehensive research and analysis capabilities by integrating with advanced deep research agents. No rate limits, faster than ChatGPT Deep Research, more thorough than Grok DeepSearch or Perplexity Deep Research. Add unlimited deep research functionality to any MCP client including Claude Desktop, Cursor, and other popular MCP-enabled applications.\n\n**Powered by [Octagon AI](https://docs.octagonagents.com)** - Learn more about the Deep Research Agent at [docs.octagonagents.com](https://docs.octagonagents.com/guide/agents/deep-research-agent.html)\n\n[![Demo](https://img.youtube.com/vi/yh1cyrm9aus/0.jpg)](https://youtu.be/yh1cyrm9aus)\n\n## 🏆 Why Teams Choose Octagon's Enterprise-Grade Deep Research API\n\n👉 **8–10x faster** than the leading incumbent—complex analyses complete in seconds, not minutes  \n👉 **Greater depth & accuracy** —pulls data from 3x more high-quality sources and cross-checks every figure  \n👉 **Unlimited parallel runs**—no rate caps, so your analysts can launch as many deep-dive tasks as they need (unlike ChatGPT Pro's 125-task monthly limit)  \n\n## 🚀 Core Differentiators\n\n✅ **No Rate Limits** - Execute unlimited deep research queries without restrictions (vs ChatGPT Pro's 125-task monthly limit)  \n✅ **Superior Performance** - Faster than ChatGPT Deep Research, more thorough than Grok DeepSearch or Perplexity Deep Research  \n✅ **Enterprise-Grade Speed** - 8-10x faster than leading incumbents, with 3x more source coverage  \n✅ **Universal MCP Integration** - Add deep research functionality to any MCP client  \n✅ **Multi-Domain Expertise** - Comprehensive research across any topic or industry  \n✅ **Advanced Data Synthesis** - Multi-source aggregation with cross-verification of every figure  \n\n## Features\n\n✅ **Comprehensive Research Capabilities**\n   - Multi-source data aggregation and synthesis\n   - Academic research and literature review\n   - Competitive landscape analysis\n   - Market intelligence and trend analysis\n   - Technical and scientific research\n   - Policy and regulatory research\n   - Real-time web scraping and data extraction\n     \n✅ **Universal Domain Coverage**\n   - Technology and AI research\n   - Healthcare and medical research\n   - Environmental and sustainability studies\n   - Economic and business analysis\n   - Scientific and engineering research\n   - Social and cultural studies\n   - Political and policy analysis\n     \n✅ **Advanced Analysis Tools**\n   - Comprehensive report generation\n   - Cross-source verification\n   - Trend identification and forecasting\n   - Comparative analysis frameworks\n\n## Get Your Octagon API Key\n\nTo use Octagon Deep Research MCP, you need to:\n\n1. Sign up for a free account at [Octagon](https://app.octagonai.co/signup/?redirectToAfterSignup=https://app.octagonai.co/api-keys)\n2. After logging in, from left menu, navigate to **API Keys** \n3. Generate a new API key\n4. Use this API key in your configuration as the `OCTAGON_API_KEY` value\n\n## Prerequisites\n\nBefore installing or running Octagon Deep Research MCP, you need to have `npx` (which comes with Node.js and npm) installed on your system.\n\n### Mac (macOS)\n\n1. **Install Homebrew** (if you don't have it):\n   ```bash\n   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n   ```\n2. **Install Node.js (includes npm and npx):**\n   ```bash\n   brew install node\n   ```\n   This will install the latest version of Node.js, npm, and npx.\n\n3. **Verify installation:**\n   ```bash\n   node -v\n   npm -v\n   npx -v\n   ```\n\n### Windows\n\n1. **Download the Node.js installer:**\n   - Go to [https://nodejs.org/](https://nodejs.org/) and download the LTS version for Windows.\n2. **Run the installer** and follow the prompts. This will install Node.js, npm, and npx.\n3. **Verify installation:**\n   Open Command Prompt and run:\n   ```cmd\n   node -v\n   npm -v\n   npx -v\n   ```\n\nIf you see version numbers for all three, you are ready to proceed with the installation steps below.\n\n## Installation\n\n### Running on Claude Desktop\n\nTo configure Octagon Deep Research MCP for Claude Desktop:\n\n1. Open Claude Desktop\n2. Go to Settings > Developer > Edit Config\n3. Add the following to your `claude_desktop_config.json` (Replace `your-octagon-api-key` with your Octagon API key):\n```json\n{\n  \"mcpServers\": {\n    \"octagon-deep-research-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"octagon-deep-research-mcp@latest\"],\n      \"env\": {\n        \"OCTAGON_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n4. Restart Claude for the changes to take effect\n\n### Running on Cursor\n\nConfiguring Cursor Desktop 🖥️\nNote: Requires Cursor version 0.45.6+\n\nTo configure Octagon Deep Research MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers \n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"octagon-deep-research-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env OCTAGON_API_KEY=your-octagon-api-key npx -y octagon-deep-research-mcp`\n\n> If you are using Windows and are running into issues, try `cmd /c \"set OCTAGON_API_KEY=your-octagon-api-key && npx -y octagon-deep-research-mcp\"`\n\nReplace `your-octagon-api-key` with your Octagon API key.\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Octagon Deep Research MCP when appropriate, but you can explicitly request it by describing your research needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"octagon-deep-research-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"octagon-deep-research-mcp@latest\"],\n      \"env\": {\n        \"OCTAGON_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Running with npx\n\n```bash\nenv OCTAGON_API_KEY=your_octagon_api_key npx -y octagon-deep-research-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g octagon-deep-research-mcp\n```\n\n## Documentation\n\nFor comprehensive documentation on using Deep Research capabilities, please visit our official documentation at:\n[https://docs.octagonagents.com](https://docs.octagonagents.com)\n\nSpecifically for the Deep Research Agent: [Deep Research Agent Guide](https://docs.octagonagents.com/guide/agents/deep-research-agent.html)\n\nThe documentation includes:\n- Detailed API references\n- Research methodology guidelines\n- Examples and use cases\n- Best practices for comprehensive research\n- Advanced features and capabilities\n\n## Available Tool\n\n### octagon-deep-research-agent\nComprehensive deep research and analysis across any topic or domain.\n\nThe tool uses a single `prompt` parameter that accepts a natural language query. Include all relevant details in your prompt for optimal results.\n\n## 📚 Example Research Queries\n\n### Technology & AI Research\n- \"Research the current state of quantum computing development and commercial applications across major tech companies\"\n- \"Analyze the competitive landscape in large language models, comparing capabilities, limitations, and market positioning\"\n- \"Investigate recent developments in autonomous vehicle technology and regulatory challenges\"\n- \"Study the evolution of edge computing architectures and their impact on IoT deployment\"\n\n### Healthcare & Medical Research\n- \"Research breakthrough medical treatments for Alzheimer's disease developed in the last 3 years\"\n- \"Analyze the effectiveness of different COVID-19 vaccine technologies and their global distribution\"\n- \"Investigate the current state of gene therapy research for rare diseases\"\n- \"Study mental health treatment innovations and their accessibility across different demographics\"\n\n### Environmental & Sustainability\n- \"Research sustainable agriculture practices and their adoption rates globally\"\n- \"Analyze renewable energy adoption trends and policy drivers across different countries\"\n- \"Investigate the environmental impact of cryptocurrency mining and proposed solutions\"\n- \"Study carbon capture technologies and their commercial viability\"\n\n### Business & Economics\n- \"Analyze the gig economy's impact on traditional employment models and worker protections\"\n- \"Research the evolution of remote work policies post-pandemic and their effectiveness on productivity\"\n- \"Investigate supply chain resilience strategies adopted after global disruptions\"\n- \"Study the impact of digital transformation on traditional retail businesses\"\n\n### Social & Cultural Studies\n- \"Research the impact of social media algorithms on information consumption patterns and political polarization\"\n- \"Analyze changing demographics in urban areas and their impact on city planning\"\n- \"Investigate the effectiveness of different approaches to digital literacy education\"\n- \"Study the cultural impact of streaming services on traditional media consumption\"\n\n### Science & Engineering\n- \"Research advances in materials science for semiconductor manufacturing\"\n- \"Analyze the development of fusion energy technologies and timeline to commercialization\"\n- \"Investigate innovations in water purification technologies for developing regions\"\n- \"Study the engineering challenges and solutions for space exploration missions\"\n\n### Policy & Governance\n- \"Investigate recent developments in AI regulation across different countries and their potential impact\"\n- \"Research privacy legislation trends and their effects on technology companies\"\n- \"Analyze different approaches to cryptocurrency regulation globally\"\n- \"Study the effectiveness of various climate policy mechanisms\"\n\n### Cybersecurity & Privacy\n- \"Investigate cybersecurity threats in IoT devices and enterprise mitigation strategies\"\n- \"Research the evolution of ransomware attacks and defensive technologies\"\n- \"Analyze privacy-preserving technologies and their adoption in consumer applications\"\n- \"Study the security implications of quantum computing on current encryption methods\"\n\n### Education & Learning\n- \"Research the effectiveness of different online learning platforms and methodologies\"\n- \"Analyze the impact of AI tools on academic research and education\"\n- \"Investigate innovative approaches to STEM education in underserved communities\"\n- \"Study the future of skills-based learning and certification programs\"\n\n## 🔍 Research Capabilities\n\n- **Multi-Source Analysis**: Aggregates information from academic papers, industry reports, news sources, and expert opinions\n- **Real-Time Data**: Accesses current information and recent developments\n- **Cross-Verification**: Validates findings across multiple reliable sources\n- **Trend Analysis**: Identifies patterns and forecasts future developments\n- **Competitive Intelligence**: Comprehensive competitive landscape analysis\n- **Technical Deep-Dives**: Detailed analysis of complex technical topics\n- **Policy Impact Assessment**: Analysis of regulatory and policy implications\n- **Market Dynamics**: Understanding of market forces and business implications\n\n## Troubleshooting\n\n1. **API Key Issues**: Ensure your Octagon API key is correctly set in the environment or config file.\n2. **Connection Issues**: Make sure the connectivity to the Octagon API is working properly.\n3. **Rate Limiting**: No rate limits apply to Deep Research MCP - execute unlimited queries.\n\n## License\n\nMIT \n\n---\n\n⭐ Star this repo if you find it helpful for your research needs!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "octagon",
        "octagonai",
        "deep",
        "octagon deep",
        "deep research",
        "extraction octagonai"
      ],
      "category": "search--data-extraction"
    },
    "Pearch-ai--mcp_pearch": {
      "owner": "Pearch-ai",
      "name": "mcp_pearch",
      "url": "https://github.com/Pearch-ai/mcp_pearch",
      "imageUrl": "",
      "description": "Best people search engine that reduces the time spent on talent discovery",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T17:17:25Z",
      "readme_content": "# Pearch.ai MCP\n\n[![smithery badge](https://smithery.ai/badge/@Pearch-ai/mcp_pearch)](https://smithery.ai/server/@Pearch-ai/mcp_pearch)\n\nOur people search API and MCP deliver the most precise results on the market. You simply ask in natural language, and we provide top-quality candidates. Designed for seamless integration with any ATS or hiring platform, our solution is backed by scientific methods, trusted by recruiters, and consistently rated the highest-quality sourcing tool.\n\n[Evaluating AI Recruitment Sourcing Tools by Human Preference](https://arxiv.org/abs/2504.02463v1)\n\n\n## Prerequisites\n\n- Python 3.7 or newer\n- Pearch.ai API key\n- FastMCP package\n\n## API Key Setup\n\n1. Visit [Pearch.ai Dashboard](https://s.pearch.ai/settings) to obtain your API key\n2. Set your API key as an environment variable:\n   ```bash\n   export PEARCH_API_KEY='your-api-key-here'\n   ```\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp_pearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Pearch-ai/mcp_pearch):\n\n```bash\nnpx -y @smithery/cli install @Pearch-ai/mcp_pearch --client claude\n```\n\n### Option 1: macOS[uv] \n\n```bash\n# Install Python and uv\nbrew install python\nbrew install uv\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install FastMCP\nuv pip install fastmcp\n```\n\n### Option 2: Linux[pip] \n\n```bash\n# Install system dependencies\nsudo apt update\nsudo apt install python3 python3-venv python3-pip\n\n# Create and activate virtual environment\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Install FastMCP\npip install fastmcp\n```\n\n## Usage\n\n### Standard Installation\n\n```bash\nfastmcp install pearch_mcp.py --name \"Pearch.ai\" --env-var PEARCH_API_KEY=pearch_mcp_key\n```\n\n### Development Mode\n\nFor local development and testing:\n\n```bash\n# Set your API key\nexport PEARCH_API_KEY='your-api-key-here'\n\n# Start development server\nfastmcp dev pearch_mcp.py\n```\n\n## Support\n\nIf you encounter any issues or have questions:\n- Open an issue in the repository\n- Contact support at [f@pearch.ai](mailto:f@pearch.ai)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "talent",
        "mcp_pearch",
        "talent discovery",
        "search engine",
        "people search"
      ],
      "category": "search--data-extraction"
    },
    "QuentinCody--catalysishub-mcp-server": {
      "owner": "QuentinCody",
      "name": "catalysishub-mcp-server",
      "url": "https://github.com/QuentinCody/catalysishub-mcp-server",
      "imageUrl": "",
      "description": "Unofficial MCP server for searching and retrieving scientific data from the Catalysis Hub database, providing access to computational catalysis research and surface reaction data.",
      "stars": 1,
      "forks": 3,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-05-20T14:47:46Z",
      "readme_content": "# Catalysis Hub MCP Server\n\nA Model Context Protocol (MCP) server interface to Catalysis Hub's GraphQL API, enabling programmatic access to catalysis research data through flexible GraphQL queries.\n\n## Key Features\n\n- **Direct GraphQL Access**: Execute any valid GraphQL query against Catalysis Hub's API\n- **Comprehensive Data Access**:\n  - Catalytic reactions (equations, conditions, catalysts)\n  - Material systems (structures, properties, descriptors)\n  - Research publications (titles, DOIs, authors)\n  - Surface reaction data (adsorption energies, binding sites)\n- **MCP Standard Compliance**: Implements the Model Context Protocol for AI-agent interoperability\n- **Flexible Query Support**: Execute complex queries with variables parameterization\n- **Error Handling**: Robust error reporting for API connectivity and query execution\n\n## License and Citation\n\nThis project is available under the MIT License with an Academic Citation Requirement. This means you can freely use, modify, and distribute the code, but any academic or scientific publication that uses this software must provide appropriate attribution.\n\n### For academic/research use:\nIf you use this software in a research project that leads to a publication, presentation, or report, you **must** cite this work according to the format provided in [CITATION.md](CITATION.md).\n\n### For commercial/non-academic use:\nCommercial and non-academic use follows the standard MIT License terms without the citation requirement.\n\nBy using this software, you agree to these terms. See [LICENSE.md](LICENSE.md) for the complete license text.\n\n## Implementation Details\n\n- **Server Configuration** (matches `claude_desktop_config.json`):\n  ```json\n  {\n    \"command\": \"/Users/quentincody/.env/bin/python3\",\n    \"args\": [\"/Users/quentincody/catalysishub-mcp-server/catalysishub_mcp_server.py\"],\n    \"options\": {\n      \"cwd\": \"/Users/quentincody/catalysishub-mcp-server\"\n    }\n  }\n  ```\n- **Core Dependency**: `httpx` for asynchronous HTTP requests\n- **Transport**: Standard input/output communication following MCP specifications\n\n## Setup & Installation\n\n1. **Clone the repository**:\n   ```bash\n   git clone <repository_url>\n   cd catalysishub-mcp-server\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Verify installation**:\n   ```bash\n   python3 catalysishub_mcp_server.py --version\n   # Should output: catalysishub-mcp-server 0.1.0\n   ```\n\n## Usage Examples\n\n### Basic Query Execution\n```python\nfrom mcp.client import MCPClient\n\nasync with MCPClient(\"catalysishub\") as hub:\n    result = await hub.catalysishub_graphql(\n        query=\"\"\"{\n            reactions(first: 5) {\n                edges {\n                    node {\n                        id\n                        Equation\n                        Temperature\n                    }\n                }\n            }\n        }\"\"\"\n    )\n    print(json.loads(result))\n```\n\n### Parameterized Query with Variables\n```python\nvariables = {\n    \"materialId\": \"mp-1234\",\n    \"firstResults\": 5\n}\n\nresponse = await hub.catalysishub_graphql(\n    query=\"\"\"query GetMaterial($materialId: String!, $firstResults: Int!) {\n        systems(uniqueId: $materialId) {\n            edges {\n                node {\n                    energy\n                    Cifdata\n                    relatedReactions(first: $firstResults) {\n                        edges {\n                            node {\n                                id\n                                Equation\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\"\"\",\n    variables=variables\n)\n```\n\n## Query Optimization Tips\n\n1. **Use GraphQL Fragments**:\n   ```graphql\n   fragment ReactionDetails on Reaction {\n       id\n       Equation\n       ActivationEnergy\n       Catalyst {\n           formula\n           surface\n       }\n   }\n   \n   query {\n       reactions(first: 10) {\n           edges {\n               node {\n                   ...ReactionDetails\n               }\n           }\n       }\n   }\n   ```\n\n2. **Batch Related Queries**:\n   ```graphql\n   query BatchQuery {\n       reactions: reactions(first: 5) { edges { node { id Equation } } }\n       materials: systems(first: 5) { edges { node { formula energy } } }\n   }\n   ```\n\n## Response Structure\n\nSuccessful responses follow this structure:\n```json\n{\n    \"data\": { /* Query results */ },\n    \"extensions\": {\n        \"responseMetadata\": {\n            \"requestDuration\": 145,\n            \"apiVersion\": \"2024-06\"\n        }\n    }\n}\n```\n\nError responses include detailed diagnostics:\n```json\n{\n    \"errors\": [{\n        \"message\": \"Cannot query field 'invalidField' on type 'Reaction'\",\n        \"locations\": [{\"line\": 5, \"column\": 21}],\n        \"path\": [\"query\", \"reactions\", \"edges\", \"node\", \"invalidField\"]\n    }]\n}\n```\n\n## Troubleshooting\n\n**Common Issues**:\n- `HTTP Request Error`: Verify network connectivity to `api.catalysis-hub.org`\n- `JSON Decode Error`: Check query syntax using Catalysis Hub's [GraphQL Playground](https://www.catalysis-hub.org/api/graphql)\n- `Timeout Errors`: Add `timeout` parameter to complex queries\n\n## Acknowledgements\n\nThis project builds on the Model Context Protocol (MCP) framework and is designed to interface with the Catalysis Hub database, a comprehensive resource for catalysis research data.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "catalysishub",
        "catalysis",
        "search",
        "catalysishub mcp",
        "data catalysis",
        "catalysis research"
      ],
      "category": "search--data-extraction"
    },
    "SecretiveShell--MCP-searxng": {
      "owner": "SecretiveShell",
      "name": "MCP-searxng",
      "url": "https://github.com/SecretiveShell/MCP-searxng",
      "imageUrl": "",
      "description": "An MCP Server to connect to searXNG instances",
      "stars": 103,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:01Z",
      "readme_content": "# MCP-searxng\n\nAn MCP server for connecting agentic systems to search systems via [searXNG](https://docs.searxng.org/).\n\n<p align=\"center\">\n  <a href=\"https://glama.ai/mcp/servers/sl2zl8vaz8\">\n    <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/sl2zl8vaz8/badge\" alt=\"MCP SearxNG Badge\"/>\n  </a>\n</p>\n\n## Tools\n\nSearch the web with SearXNG\n\n## Prompts\n\n```python\nsearch(query: str) -> f\"Searching for {query} using searXNG\"\n```\n\n## Usage\n\n### via uvx\n\n1) configure your client JSON like\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uvx\", \n      \"args\": [\n        \"mcp-searxng\"\n      ]\n    }\n  }\n}\n```\n\n### via git clone\n\n1) Add the server to claude desktop (the entrypoint is main.py)\n\nClone the repo and add this JSON to claude desktop\n\nyou can run this server with `uvx mcp-searxng`, or use a local copy of the repo\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uv\", \n      \"args\": [\n        \"--project\",\n        \"/absoloute/path/to/MCP-searxng/\",\n        \"run\",\n        \"/absoloute/path/to/MCP-searxng/mcp-searxng/main.py\"\n      ]\n    }\n  }\n}\n```\n\nyou will need to change the paths to match your environment\n\n### Custom SearXNG URL\n\n2) set the environment variable `SEARXNG_URL` to the URL of the searxng server (default is `http://localhost:8080`)\n\n3) run your MCP client and you should be able to search the web with searxng\n\nNote: if you are using claude desktop make sure to kill the process (task manager or equivalent) before running the server again\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "mcp",
        "search",
        "searxng mcp",
        "mcp searxng",
        "searxng instances"
      ],
      "category": "search--data-extraction"
    },
    "adawalli--nexus": {
      "owner": "adawalli",
      "name": "nexus",
      "url": "https://github.com/adawalli/nexus",
      "imageUrl": "",
      "description": "AI-powered web search server using Perplexity Sonar models with source citations. Zero-install setup via NPX.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T12:26:18Z",
      "readme_content": "<!-- markdownlint-disable MD033 MD041 -->\n\n<div align=\"center\">\n\n# 🔍 Nexus MCP Server\n\n**AI integration without the complexity**\n\n[![npm version](https://img.shields.io/npm/v/nexus-mcp.svg)](https://www.npmjs.com/package/nexus-mcp)\n![NPM Downloads](https://img.shields.io/npm/dt/nexus-mcp?style=flat-square&logo=npm&label=downloads)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-Ready-blue.svg)](https://www.typescriptlang.org/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n[![CodeRabbit Pull Request Reviews](https://img.shields.io/coderabbit/prs/github/adawalli/nexus)](https://coderabbit.ai)\n\n_Intelligent AI model search and discovery with zero-install simplicity_\n\n[Quick Start](#-quick-start) • [Features](#-features) • [Documentation](#-documentation) • [Contributing](#-contributing)\n\n</div>\n\n---\n\n## 🚀 What is Nexus?\n\nNexus is a production-ready **Model Context Protocol (MCP) server** that brings AI-powered web search directly into your development environment. Get intelligent search results with proper citations in **Claude Desktop**, **Cursor**, or any MCP-compatible client - all with a single command.\n\n### Why Nexus?\n\n- **🎯 Zero Setup**: Ready in 30 seconds with `npx` - no installation, no configuration\n- **🧠 AI-Powered**: Uses Perplexity Sonar models for intelligent, current web search\n- **📚 Source Citations**: Get authoritative sources with every search result\n- **🔧 Developer-First**: Built for developers who want AI capabilities without complexity\n- **⚡ Production-Ready**: Enterprise-grade reliability with comprehensive error handling\n\n## ✨ Features\n\n<table>\n<tr>\n<td width=\"50%\">\n\n### 🚀 **Zero-Install Simplicity**\n\n- Ready in 30 seconds with `npx`\n- No dependencies or build steps\n- Cross-platform compatibility\n- Always up-to-date\n\n### 🧠 **AI-Powered Intelligence**\n\n- Perplexity Sonar model integration\n- Real-time web content search\n- Context-aware result ranking\n- Multiple model options\n\n</td>\n<td width=\"50%\">\n\n### 📚 **Professional Quality**\n\n- Source citations and metadata\n- Comprehensive error handling\n- Production-grade reliability\n- TypeScript implementation\n\n### 🔧 **Developer Experience**\n\n- MCP protocol compliance\n- Extensive documentation\n- Configurable parameters\n- Community support\n\n</td>\n</tr>\n</table>\n\n## 🏃‍♂️ Quick Start\n\n**🚀 Zero-install setup - Ready in 30 seconds!**\n\n### Prerequisites\n\n- Node.js 16 or higher\n- An OpenRouter API key (get one at [OpenRouter](https://openrouter.ai))\n\n### Zero-Config Installation\n\nNo build steps, no dependencies, no setup required:\n\n```bash\n# Set your OpenRouter API key\nexport OPENROUTER_API_KEY=your-api-key-here\n\n# Run the server instantly\nnpx nexus-mcp\n```\n\nThat's it! The server is now running and ready for MCP client connections.\n\n### Testing the NPX Installation\n\n```bash\n# Test the CLI help\nnpx nexus-mcp --help\n\n# Test the version\nnpx nexus-mcp --version\n\n# Run with your API key\nOPENROUTER_API_KEY=your-key npx nexus-mcp\n```\n\n## Alternative: Local Development Installation\n\nFor local development or customization:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/adawalli/nexus.git\ncd nexus\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the server:\n\n```bash\nnpm run build\n```\n\n4. Configure your OpenRouter API key:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n\n# Edit .env and add your actual API key\n# OPENROUTER_API_KEY=your-api-key-here\n```\n\n5. Test the server:\n\n```bash\nnpm start\n```\n\n## Integration with MCP Clients\n\n### 🚀 Quick Setup with NPX (Recommended)\n\nThe easiest way to integrate with any MCP client is using NPX:\n\n### Claude Code\n\nAdd this server to your Claude Code MCP settings:\n\n1. Open your MCP settings file (usually `~/.claude/mcp_settings.json`)\n\n2. Add the server configuration using NPX:\n\n```json\n{\n  \"mcpServers\": {\n    \"nexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"nexus-mcp\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n3. Restart Claude Code\n\n**That's it!** No installation, no build steps, no path configuration required.\n\n### Cursor\n\nConfigure the server in Cursor's MCP settings:\n\n1. Open Cursor settings and navigate to MCP servers\n\n2. Add a new server with:\n\n   - **Name**: `nexus`\n   - **Command**: `npx`\n   - **Args**: `[\"nexus-mcp\"]`\n   - **Environment Variables**:\n     - `OPENROUTER_API_KEY`: `your-api-key-here`\n\n3. Restart Cursor\n\n### Other MCP Clients\n\nFor any MCP-compatible client, use these connection details:\n\n- **Transport**: stdio\n- **Command**: `npx`\n- **Args**: `[\"nexus-mcp\"]`\n- **Environment Variables**: `OPENROUTER_API_KEY=your-api-key-here`\n\n### Alternative: Local Installation\n\nIf you prefer using a local installation (after following the local development setup):\n\n```json\n{\n  \"mcpServers\": {\n    \"nexus\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/nexus-mcp/dist/cli.js\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nOnce integrated, you can use the search tool in your MCP client:\n\n### Basic Search\n\n```\nUse the search tool to find information about \"latest developments in AI\"\n```\n\n### Advanced Search with Parameters\n\n```\nSearch for \"climate change solutions\" using:\n- Model: perplexity/sonar\n- Max tokens: 2000\n- Temperature: 0.3\n```\n\n## Available Tools\n\n### `search`\n\nThe main search tool that provides AI-powered web search capabilities.\n\n**Parameters:**\n\n- `query` (required): Search query (1-2000 characters)\n- `model` (optional): Perplexity model to use (default: \"perplexity/sonar\")\n- `maxTokens` (optional): Maximum response tokens (1-4000, default: 1000)\n- `temperature` (optional): Response randomness (0-2, default: 0.7)\n\n**Example Response:**\n\n```\nBased on current information, here are the latest developments in AI...\n\n[Detailed AI-generated response with current information]\n\n---\n**Search Metadata:**\n- Model: perplexity/sonar\n- Response time: 1250ms\n- Tokens used: 850\n- Sources: 5 found\n```\n\n## Configuration\n\n### Environment Variables\n\n- `OPENROUTER_API_KEY` (required): Your OpenRouter API key\n- `NODE_ENV` (optional): Environment setting (development, production, test)\n- `LOG_LEVEL` (optional): Logging level (debug, info, warn, error)\n\n### Advanced Configuration\n\nThe server supports additional configuration through environment variables:\n\n- `OPENROUTER_TIMEOUT_MS`: Request timeout in milliseconds (default: 30000)\n- `OPENROUTER_MAX_RETRIES`: Maximum retry attempts (default: 3)\n- `OPENROUTER_BASE_URL`: Custom OpenRouter API base URL\n\n## Resources\n\nThe server provides a configuration status resource at `config://status` that shows:\n\n- Server health status\n- Configuration information (with masked API key)\n- Search tool availability\n- Server uptime and version\n\n## Troubleshooting\n\n### NPX-Specific Issues\n\n**\"npx: command not found\"**\n\n- Ensure Node.js 16+ is installed: `node --version`\n- Update npm: `npm install -g npm@latest`\n\n**\"Cannot find package 'nexus-mcp'\"**\n\n- The package may not be published yet. Use local installation instead\n- Verify network connectivity for npm registry access\n\n**NPX takes a long time to start**\n\n- This is normal on first run as NPX downloads the package\n- Subsequent runs will be faster due to caching\n- For faster startup, use local installation instead\n\n**\"Permission denied\" errors with NPX**\n\n- Try: `npx --yes nexus-mcp --stdio`\n- Or set npm permissions: `npm config set user 0 && npm config set unsafe-perm true`\n\n### Common Issues\n\n**\"Search functionality is not available\"**\n\n- Ensure `OPENROUTER_API_KEY` environment variable is set\n- Verify your API key is valid at [OpenRouter](https://openrouter.ai)\n- Check the server logs for initialization errors\n\n**\"Authentication failed: Invalid API key\"**\n\n- Double-check your API key format and validity\n- Ensure the key has sufficient credits/permissions\n- Test the key directly at OpenRouter dashboard\n\n**\"Rate limit exceeded\"**\n\n- Wait for the rate limit to reset (usually 1 minute)\n- Consider upgrading your OpenRouter plan for higher limits\n- Monitor usage in your OpenRouter dashboard\n\n**Connection timeouts**\n\n- Check your internet connection\n- The server will automatically retry failed requests\n- Increase timeout if needed: `OPENROUTER_TIMEOUT_MS=60000`\n\n**MCP client can't connect to server**\n\n- Verify your MCP configuration uses the correct command and arguments\n- Check that Node.js 16+ is available in your MCP client's environment\n- Ensure the API key is properly set in the environment variables\n\n### Debug Logging\n\nEnable debug logging by:\n\n**For local development:** Add `LOG_LEVEL=debug` to your `.env` file\n\n**For MCP clients:** Add `LOG_LEVEL: \"debug\"` to the `env` section of your MCP configuration\n\nThis will provide detailed information about:\n\n- Configuration loading\n- API requests and responses\n- Error details and stack traces\n- Performance metrics\n\n### Testing Connection\n\nYou can test if the server is working by checking the configuration status resource in your MCP client, or by running a simple search query.\n\n## Development\n\nFor developers working on this server:\n\n```bash\n# Development with hot reload\nnpm run dev\n\n# Run tests\nnpm test\n\n# Run tests with coverage\nnpm run test:coverage\n\n# Lint code\nnpm run lint\n\n# Format code\nnpm run format\n```\n\n## 💰 API Credits and Costs\n\nThis server uses OpenRouter's API, which charges based on token usage:\n\n- **Perplexity Sonar models**: Check current pricing at [OpenRouter Models](https://openrouter.ai/models)\n- **Usage monitoring**: Track consumption through the OpenRouter dashboard\n- **Cost control**: Set usage limits in your OpenRouter account\n- **Optimization**: Nexus includes built-in rate limiting and intelligent caching\n\n## 📚 Documentation\n\n<div align=\"center\">\n\n| 📖 **Guide**        | 🔗 **Link**                                 | 📝 **Description**               |\n| ------------------- | ------------------------------------------- | -------------------------------- |\n| **Quick Start**     | [Getting Started](#-quick-start)            | Zero-install setup in 30 seconds |\n| **API Reference**   | [MCP Tools](CLAUDE.md#development-commands) | Complete command reference       |\n| **Configuration**   | [Environment Setup](#configuration)         | Advanced configuration options   |\n| **Contributing**    | [Contributing Guide](CONTRIBUTING.md)       | Join our open source community   |\n| **Troubleshooting** | [Common Issues](#troubleshooting)           | Solutions to common problems     |\n\n</div>\n\n## 🤝 Contributing\n\nWe welcome contributions from developers of all experience levels!\n\n<table>\n<tr>\n<td width=\"33%\">\n\n### 🚀 **Get Started**\n\n- Fork the repository\n- Read our [Contributing Guide](CONTRIBUTING.md)\n- Check out [good first issues](https://github.com/search?q=repo%3Anexus-mcp+label%3A%22good+first+issue%22&type=issues)\n\n</td>\n<td width=\"33%\">\n\n### 🐛 **Report Issues**\n\n- [Bug Reports](https://github.com/adawalli/nexus/issues/new)\n- [Feature Requests](https://github.com/adawalli/nexus/issues/new)\n- [Ask Questions](https://github.com/adawalli/nexus/issues/new)\n\n</td>\n<td width=\"33%\">\n\n### 💬 **Join Community**\n\n- [GitHub Discussions](https://github.com/adawalli/nexus/discussions)\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Roadmap & Project Board](https://github.com/adawalli/nexus/projects)\n\n</td>\n</tr>\n</table>\n\n### 🌟 Recognition\n\nContributors are recognized in our:\n\n- [Contributors list](https://github.com/adawalli/nexus/graphs/contributors)\n- Release notes for significant contributions\n- Community spotlights and testimonials\n\n## 🔗 Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io) - The standard we implement\n- [OpenRouter](https://openrouter.ai) - Our AI model provider\n- [Claude Desktop](https://claude.ai) - Primary MCP client\n- [Cursor](https://cursor.sh) - AI-powered code editor with MCP support\n\n## 📞 Support & Community\n\n<div align=\"center\">\n\n| 💬 **Need Help?**    | 🔗 **Resource**                                                                                      |\n| -------------------- | ---------------------------------------------------------------------------------------------------- |\n| **Quick Questions**  | [GitHub Discussions](https://github.com/adawalli/nexus/discussions)                                  |\n| **Bug Reports**      | [GitHub Issues](https://github.com/adawalli/nexus/issues)                                            |\n| **Documentation**    | [OpenRouter Docs](https://openrouter.ai/docs) • [MCP Specification](https://modelcontextprotocol.io) |\n| **Feature Requests** | [Enhancement Proposals](https://github.com/adawalli/nexus/issues/new)                                |\n\n</div>\n\n## 📄 License\n\n**MIT License** - see [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n\n**Made with ❤️ by the open source community**\n\n[⭐ Star us on GitHub](https://github.com/adawalli/nexus) • [📦 View on NPM](https://www.npmjs.com/package/nexus-mcp) • [📚 Read the Docs](CLAUDE.md)\n\n_Nexus: AI integration without the complexity_\n\n[![Star History Chart](https://api.star-history.com/svg?repos=adawalli/nexus&type=Date)](https://star-history.com/#adawalli/nexus&Date)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sonar",
        "search",
        "citations",
        "search server",
        "web search",
        "extraction adawalli"
      ],
      "category": "search--data-extraction"
    },
    "ananddtyagi--webpage-screenshot-mcp": {
      "owner": "ananddtyagi",
      "name": "webpage-screenshot-mcp",
      "url": "https://github.com/ananddtyagi/webpage-screenshot-mcp",
      "imageUrl": "",
      "description": "A MCP server for taking screenshots of webpages to use as feedback during UI developement.",
      "stars": 37,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:42Z",
      "readme_content": "# Webpage Screenshot MCP Server\n\nAn MCP (Model Context Protocol) server that captures screenshots of web pages using Puppeteer. This server allows AI agents to visually verify web applications and see their progress when generating web apps.\n\n![Screen Recording May 27 2025 (2)](https://github.com/user-attachments/assets/9f186ec4-5a5c-449b-9a30-a5ec0cdba695)\n\n\n## Features\n\n- **Full page screenshots**: Capture entire web pages or just the viewport\n- **Element screenshots**: Target specific elements using CSS selectors\n- **Multiple formats**: Support for PNG, JPEG, and WebP formats\n- **Customizable options**: Set viewport size, image quality, wait conditions, and delays\n- **Base64 encoding**: Returns screenshots as base64 encoded images for easy integration\n- **Authentication support**: Manual login and cookie persistence\n- **Default browser integration**: Use your system's default browser for a more natural experience\n- **Session persistence**: Keep browser sessions open for multi-step workflows\n\n## Installation\n\n### Quick Start (Claude Desktop Extension)\n\nDrag and drop the generated `screenshot-webpage-mcp.dxt` file into Claude Desktop for automatic installation!\n\n### Manual Installation\n\nTo install and build the MCP from source:\n\n```bash\n# Clone the repository (if you haven't already)\ngit clone https://github.com/ananddtyagi/webpage-screenshot-mcp.git\ncd webpage-screenshot-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nThe MCP server is built using TypeScript and compiled to JavaScript. The `dist` folder contains the compiled JavaScript files. \n\n### Adding to Claude or Cursor\n\nTo add this MCP to Claude Desktop or Cursor:\n\n1. **Claude Desktop**:\n   - Go to Settings > Developer\n   - Click \"Edit Config\"\n   - Add the following:\n\n   ```json\n    \"webpage-screenshot\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"~/path/to/webpage-screenshot-mcp/dist/index.js\"\n      ]\n    }\n   ```\n   - Save and reload Claude\n\n2. **Cursor**:\n   - Open Cursor and go to Cursor Settings > MCP\n   - Click \"Add new global MCP server\"\n   - Add the following:\n  \n  ```json\n    \"webpage-screenshot\": {\n      \"command\": \"node\",\n      \"args\": [\"~/path/to/webpage-screenshot-mcp/dist/index.js\"]\n    }\n   ```\n\n   - Save and reload Cursor\n\n## Usage\n\n### Tools\n\nThis MCP server provides several tools:\n\n#### 1. login-and-wait\n\nOpens a webpage in a visible browser window for manual login, waits for user to complete login, then saves cookies.\n\n```json\n{\n  \"url\": \"https://example.com/login\",\n  \"waitMinutes\": 5,\n  \"successIndicator\": \".dashboard-welcome\",\n  \"useDefaultBrowser\": true\n}\n```\n\n- `url` (required): The URL of the login page\n- `waitMinutes` (optional): Maximum minutes to wait for login (default: 5)\n- `successIndicator` (optional): CSS selector or URL pattern that indicates successful login\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: true)\n\n#### 2. screenshot-page\n\nCaptures a screenshot of a given URL and returns it as base64 encoded image.\n\n```json\n{\n  \"url\": \"https://example.com/dashboard\",\n  \"fullPage\": true,\n  \"width\": 1920,\n  \"height\": 1080,\n  \"format\": \"png\",\n  \"quality\": 80,\n  \"waitFor\": \"networkidle2\",\n  \"delay\": 500,\n  \"useSavedAuth\": true,\n  \"reuseAuthPage\": true,\n  \"useDefaultBrowser\": true,\n  \"visibleBrowser\": true\n}\n```\n\n- `url` (required): The URL of the webpage to screenshot\n- `fullPage` (optional): Whether to capture the full page or just the viewport (default: true)\n- `width` (optional): Viewport width in pixels (default: 1920)\n- `height` (optional): Viewport height in pixels (default: 1080)\n- `format` (optional): Image format - \"png\", \"jpeg\", or \"webp\" (default: \"png\")\n- `quality` (optional): Quality of the image (0-100), only applicable for jpeg and webp\n- `waitFor` (optional): When to consider page loaded - \"load\", \"domcontentloaded\", \"networkidle0\", or \"networkidle2\" (default: \"networkidle2\")\n- `delay` (optional): Additional delay in milliseconds after page load (default: 0)\n- `useSavedAuth` (optional): Whether to use saved cookies from previous login (default: true)\n- `reuseAuthPage` (optional): Whether to use the existing authenticated page (default: false)\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: false)\n- `visibleBrowser` (optional): Whether to show the browser window (default: false)\n\n#### 3. screenshot-element\n\nCaptures a screenshot of a specific element on a webpage using a CSS selector.\n\n```json\n{\n  \"url\": \"https://example.com/dashboard\",\n  \"selector\": \".user-profile\",\n  \"waitForSelector\": true,\n  \"format\": \"png\",\n  \"quality\": 80,\n  \"padding\": 10,\n  \"useSavedAuth\": true,\n  \"useDefaultBrowser\": true,\n  \"visibleBrowser\": true\n}\n```\n\n- `url` (required): The URL of the webpage\n- `selector` (required): CSS selector for the element to screenshot\n- `waitForSelector` (optional): Whether to wait for the selector to appear (default: true)\n- `format` (optional): Image format - \"png\", \"jpeg\", or \"webp\" (default: \"png\")\n- `quality` (optional): Quality of the image (0-100), only applicable for jpeg and webp\n- `padding` (optional): Padding around the element in pixels (default: 0)\n- `useSavedAuth` (optional): Whether to use saved cookies from previous login (default: true)\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: false)\n- `visibleBrowser` (optional): Whether to show the browser window (default: false)\n\n#### 4. clear-auth-cookies\n\nClears saved authentication cookies for a specific domain or all domains.\n\n```json\n{\n  \"url\": \"https://example.com\"\n}\n```\n\n- `url` (optional): URL of the domain to clear cookies for. If not provided, clears all cookies.\n\n## Default Browser Mode\n\nThe default browser mode allows you to use your system's regular browser (Chrome, Edge, etc.) instead of Puppeteer's bundled Chromium. This is useful for:\n\n1. Using your existing browser sessions and extensions\n2. Manually logging in to websites with your saved credentials\n3. Having a more natural browsing experience for multi-step workflows\n4. Testing with the same browser environment as your users\n\nTo enable default browser mode, set `useDefaultBrowser: true` and `visibleBrowser: true` in your tool parameters.\n\n### How Default Browser Mode Works\n\nWhen you enable default browser mode:\n\n1. The tool will attempt to locate your system's default browser (Chrome, Edge, etc.)\n2. It launches your browser with remote debugging enabled on a random port\n3. Puppeteer connects to this browser instance instead of launching its own\n4. Your existing profiles, extensions, and cookies are available during the session\n5. The browser window remains visible so you can interact with it manually\n\nThis mode is particularly useful for workflows that require authentication or complex user interactions.\n\n## Browser Persistence\n\nThe MCP server can maintain a persistent browser session across multiple tool calls:\n\n1. When you use `login-and-wait`, the browser session is kept open\n2. Subsequent calls to `screenshot-page` or `screenshot-element` with `reuseAuthPage: true` will use the same page\n3. This allows for multi-step workflows without having to re-authenticate\n\n## Cookie Management\n\nCookies are automatically saved for each domain you visit:\n\n1. After using `login-and-wait`, cookies are saved to the `.mcp-screenshot-cookies` directory in your home folder\n2. These cookies are automatically loaded when visiting the same domain again with `useSavedAuth: true`\n3. You can clear cookies using the `clear-auth-cookies` tool\n\n## Example Workflow: Protected Page Screenshots\n\nHere's an example workflow for taking screenshots of pages that require authentication:\n\n1. **Manual Login Phase**\n\n```json\n{\n  \"name\": \"login-and-wait\",\n  \"parameters\": {\n    \"url\": \"https://example.com/login\",\n    \"waitMinutes\": 3,\n    \"successIndicator\": \".dashboard-welcome\",\n    \"useDefaultBrowser\": true\n  }\n}\n```\n\nThis will open your default browser with the login page. You can manually log in, and once complete (either by detecting the success indicator or after navigating away from the login page), the session cookies will be saved.\n\n2. **Take Screenshots Using Saved Session**\n\n```json\n{\n  \"name\": \"screenshot-page\",\n  \"parameters\": {\n    \"url\": \"https://example.com/account\",\n    \"fullPage\": true,\n    \"useSavedAuth\": true,\n    \"reuseAuthPage\": true,\n    \"useDefaultBrowser\": true,\n    \"visibleBrowser\": true\n  }\n}\n```\n\nThis will take a screenshot of the account page using your saved authentication cookies in the same browser window.\n\n3. **Take Screenshots of Specific Elements**\n\n```json\n{\n  \"name\": \"screenshot-element\",\n  \"parameters\": {\n    \"url\": \"https://example.com/dashboard\",\n    \"selector\": \".user-profile-section\",\n    \"useSavedAuth\": true,\n    \"useDefaultBrowser\": true,\n    \"visibleBrowser\": true\n  }\n}\n```\n\n4. **Clear Cookies When Done**\n\n```json\n{\n  \"name\": \"clear-auth-cookies\",\n  \"parameters\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\nThis workflow allows you to interact with protected pages as if you were a regular user, completing the full authentication flow in your default browser.\n\n## Headless vs. Visible Mode\n\n- **Headless mode** (`visibleBrowser: false`): Faster and more suitable for automated workflows where no user interaction is needed.\n- **Visible mode** (`visibleBrowser: true`): Shows the browser window, allowing for user interaction and manual verification. Required for `useDefaultBrowser: true`.\n\n## Platform Support\n\nThe default browser detection works on:\n\n- **macOS**: Detects Chrome, Edge, and Safari\n- **Windows**: Detects Chrome and Edge via registry or common installation paths\n- **Linux**: Detects Chrome and Chromium via system commands\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Default browser not found**: If the system can't find your default browser, it will fall back to Puppeteer's bundled Chromium.\n2. **Connection issues**: If there are problems connecting to the browser's debugging port, check if another instance is already using that port.\n3. **Cookie issues**: If authentication isn't working, try clearing cookies with the `clear-auth-cookies` tool.\n\n### Debugging\n\nThe MCP server logs helpful error messages to the console when issues occur. Check these messages for troubleshooting information.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "screenshots",
        "webpages",
        "webpage",
        "screenshot mcp",
        "screenshots webpages",
        "webpage screenshot"
      ],
      "category": "search--data-extraction"
    },
    "chanmeng--google-news-mcp-server": {
      "owner": "chanmeng",
      "name": "google-news-mcp-server",
      "url": "https://github.com/ChanMeng666/server-google-news",
      "imageUrl": "",
      "description": "Google News integration with automatic topic categorization, multi-language support, and comprehensive search capabilities including headlines, stories, and related topics through [SerpAPI](https://serpapi.com/).",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "serpapi",
        "news",
        "google",
        "google news",
        "news integration",
        "topics serpapi"
      ],
      "category": "search--data-extraction"
    },
    "deadletterq--mcp-opennutrition": {
      "owner": "deadletterq",
      "name": "mcp-opennutrition",
      "url": "https://github.com/deadletterq/mcp-opennutrition",
      "imageUrl": "",
      "description": "Local MCP server for searching 300,000+ foods, nutrition facts, and barcodes from the OpenNutrition database.",
      "stars": 114,
      "forks": 9,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-23T13:03:29Z",
      "readme_content": "# MCP OpenNutrition\n\nA Model Context Protocol (MCP) server providing access to the comprehensive OpenNutrition food database with 300,000+ food items, nutritional data, and barcode lookups.\n\nOpenNutrition addresses the longstanding issues with fragmented and unreliable nutritional data by combining authoritative public sources (USDA, CNF, FRIDA, AUSNUT). Unlike other databases that suffer from inconsistent user-generated content or restrictive commercial licensing, OpenNutrition provides transparent, comprehensive, and accurate nutritional data that's freely accessible to developers and researchers.\n\n## Tools\n\n- **Search by Name**: Find foods by name, brand, or partial matches\n- **Browse Foods**: Get paginated lists of all available foods\n- **Get by ID**: Retrieve detailed nutritional information using food IDs\n- **Barcode Lookup**: Find foods using EAN-13 barcodes\n\n## Installation\n\n1. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n2. Build the project:\n   ```bash\n   npm run build\n   ```\n\n3. Add to your Claude/Cline MCP configuration (set the same version of node that you used to build the project):\n   ```json\n   \"mcp-opennutrition\": {\n       \"command\": \"/Users/YOUR.USERNAME/.nvm/versions/node/v20.19.0/bin/node\",\n       \"args\": [\n           \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-opennutrition/build/index.js\"\n       ]\n   }\n   ```\n\n## Data Source\n\nThis server uses the [OpenNutrition dataset](https://www.opennutrition.app/).\n\nThe dataset provides comprehensive nutritional profiles including macronutrients, vitamins, and minerals.\n\n## Usage\n\nOnce configured, the MCP server runs fully locally on your machine and automatically provides food and nutrition query capabilities to Claude/Cline. All data processing and queries happen locally with no external API calls, ensuring privacy and fast response times.\n\n## Example\n\nHere is an example of how Claude uses the tool for a brownie recipe:",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opennutrition",
        "nutrition",
        "foods",
        "opennutrition database",
        "mcp opennutrition",
        "opennutrition local"
      ],
      "category": "search--data-extraction"
    },
    "dealx--mcp-server": {
      "owner": "dealx",
      "name": "mcp-server",
      "url": "https://github.com/DealExpress/mcp-server",
      "imageUrl": "",
      "description": "MCP Server for DealX platform",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dealx",
        "mcp",
        "search",
        "dealx mcp",
        "server dealx",
        "server mcp"
      ],
      "category": "search--data-extraction"
    },
    "devflowinc--trieve": {
      "owner": "devflowinc",
      "name": "trieve",
      "url": "https://github.com/devflowinc/trieve/tree/main/clients/mcp-server",
      "imageUrl": "",
      "description": "Crawl, embed, chunk, search, and retrieve information from datasets through [Trieve](https://trieve.ai)",
      "stars": 2510,
      "forks": 224,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-10-04T09:42:16Z",
      "readme_content": "<p align=\"center\">\n  <img height=\"100\" src=\"https://cdn.trieve.ai/trieve-logo.png\" alt=\"Trieve Logo\">\n</p>\n<p align=\"center\">\n<strong><a href=\"https://dashboard.trieve.ai\">Sign Up (1k chunks free)</a> | <a href=\"https://pdf2md.trieve.ai\">PDF2MD</a> | <a href=\"https://docs.trieve.ai\">Hacker News Search Engine</a> | <a href=\"https://docs.trieve.ai\">Documentation</a> | <a href=\"https://cal.com/nick.k/meet\">Meet a Maintainer</a> | <a href=\"https://discord.gg/eBJXXZDB8z\">Discord</a> | <a href=\"https://matrix.to/#/#trieve-general:trieve.ai\">Matrix</a>\n</strong>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/devflowinc/trieve/stargazers\">\n        <img src=\"https://img.shields.io/github/stars/devflowinc/trieve.svg?style=flat&color=yellow\" alt=\"Github stars\"/>\n    </a>\n    <a href=\"https://discord.gg/CuJVfgZf54\">\n        <img src=\"https://img.shields.io/discord/1130153053056684123.svg?label=Discord&logo=Discord&colorB=7289da&style=flat\" alt=\"Join Discord\"/>\n    </a>\n    <a href=\"https://matrix.to/#/#trieve-general:trieve.ai\">\n        <img src=\"https://img.shields.io/badge/matrix-join-purple?style=flat&logo=matrix&logocolor=white\" alt=\"Join Matrix\"/>\n    </a>\n    <a href=\"https://smithery.ai/server/trieve-mcp-server\">\n        <img src=\"https://smithery.ai/badge/trieve-mcp-server\" alt=\"smithery badge\"/>\n    </a>\n    <a href=\"https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522trieve-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522more%2520args...%2522%255D%257D\">\n        <img src=\"https://img.shields.io/badge/vscode-mcp-install?style=flat&logoColor=%230078d4&label=vscode-mcp&labelColor=%230078d4&link=https%3A%2F%2Finsiders.vscode.dev%2Fredirect%3Furl%3Dvscode%253Amcp%252Finstall%253F%25257B%252522name%252522%25253A%252522trieve-mcp-server%252522%25252C%252522command%252522%25253A%252522npx%252522%25252C%252522args%252522%25253A%25255B%252522more%252520args...%252522%25255D%25257D\" alt=\"vscode mcp install badge\"/>\n    </a>\n</p>\n\n<h2 align=\"center\">\n    <b>All-in-one solution for search, recommendations, and RAG</b>\n</h2>\n\n<a href=\"https://trieve.ai\">\n  <img alt=\"light_api\" src=\"https://cdn.trieve.ai/landing-tabs/light-api.webp\">\n</a>\n\n## Quick Links\n\n- [API Reference + Docs](https://docs.trieve.ai/api-reference)\n- [OpenAPI specification](https://api.trieve.ai/redoc)\n- [Typescript SDK](https://ts-sdk.trieve.ai/)\n- [Python SDK](https://pypi.org/project/trieve-py-client/)\n\n## Features\n\n- **🔒 Self-Hosting in your VPC or on-prem**: We have full self-hosting guides for AWS, GCP, Kubernetes generally, and docker compose available on our [documentation page here](https://docs.trieve.ai/self-hosting/docker-compose).\n- **🧠 Semantic Dense Vector Search**: Integrates with OpenAI or Jina embedding models and [Qdrant](https://qdrant.tech) to provide semantic vector search.\n- **🔍 Typo Tolerant Full-Text/Neural Search**: Every uploaded chunk is vector'ized with [naver/efficient-splade-VI-BT-large-query](https://huggingface.co/naver/efficient-splade-VI-BT-large-query) for typo tolerant, quality neural sparse-vector search.\n- **🖊️ Sub-Sentence Highlighting**: Highlight the matching words or sentences within a chunk and bold them on search to enhance UX for your users. Shout out to the [simsearch](https://github.com/smartdatalake/simsearch) crate!\n- **🌟 Recommendations**: Find similar chunks (or files if using grouping) with the recommendation API. Very helpful if you have a platform where users' favorite, bookmark, or upvote content.\n- **🤖 Convenient RAG API Routes**: We integrate with OpenRouter to provide you with access to any LLM you would like for RAG. Try our routes for [fully-managed RAG with topic-based memory management](https://api.trieve.ai/redoc#tag/message/operation/create_message_completion_handler) or [select your own context RAG](https://api.trieve.ai/redoc#tag/chunk/operation/generate_off_chunks).\n- **💼 Bring Your Own Models**: If you'd like, you can bring your own text-embedding, SPLADE, cross-encoder re-ranking, and/or large-language model (LLM) and plug it into our infrastructure.\n- **🔄 Hybrid Search with cross-encoder re-ranking**: For the best results, use hybrid search with [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) re-rank optimization.\n- **📆 Recency Biasing**: Easily bias search results for what was most recent to prevent staleness\n- **🛠️ Tunable Merchandizing**: Adjust relevance using signals like clicks, add-to-carts, or citations\n- **🕳️ Filtering**: Date-range, substring match, tag, numeric, and other filter types are supported.\n- **👥 Grouping**: Mark multiple chunks as being part of the same file and search on the file-level such that the same top-level result never appears twice\n\n**Are we missing a feature that your use case would need?** - call us at [628-222-4090](mailto:+16282224090), make a [Github issue](https://github.com/devflowinc/trieve/issues), or join the [Matrix community](https://matrix.to/#/#trieve-general:trieve.ai) and tell us! We are a small company who is still very hands-on and eager to build what you need; professional services are available.\n\n## Local development with Linux\n\n### Installing via Smithery\n\nTo install Trieve for Claude Desktop automatically via [Smithery](https://smithery.ai/server/trieve-mcp-server):\n\n```bash\nnpx -y @smithery/cli install trieve-mcp-server --client claude\n```\n\n### Debian/Ubuntu Packages needed packages\n\n```sh\nsudo apt install curl \\\ngcc \\\ng++ \\\nmake \\\npkg-config \\\npython3 \\\npython3-pip \\\nlibpq-dev \\\nlibssl-dev \\\nopenssl\n```\n\n### Arch Packages needed\n\n```sh\nsudo pacman -S base-devel postgresql-libs\n```\n\n### Install NodeJS and Yarn\n\nYou can install [NVM](https://github.com/nvm-sh/nvm) using its install script.\n\n```\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash\n```\n\nYou should restart the terminal to update bash profile with NVM. Then, you can install NodeJS LTS release and Yarn.\n\n```\nnvm install --lts\nnpm install -g yarn\n```\n\n### Make server tmp dir\n\n```\nmkdir server/tmp\n```\n\n### Install rust\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n### Install cargo-watch\n\n```\ncargo install cargo-watch\n```\n\n### Setup env's\n\nYou might need to create the `analytics` directory in ./frontends\n\n```\ncp .env.analytics ./frontends/analytics/.env\ncp .env.chat ./frontends/chat/.env\ncp .env.search ./frontends/search/.env\ncp .env.example ./server/.env\ncp .env.dashboard ./frontends/dashboard/.env\n```\n\n### Add your `LLM_API_KEY` to `./server/.env`\n\n[Here is a guide for acquiring that](https://blog.streamlit.io/beginners-guide-to-openai-api/#get-your-own-openai-api-key).\n\n#### Steps once you have the key\n\n1. Open the `./server/.env` file\n2. Replace the value for `LLM_API_KEY` to be your own OpenAI API key.\n3. Replace the value for `OPENAI_API_KEY` to be your own OpenAI API key.\n\n### Export the following keys in your terminal for local dev\n\nThe PAGEFIND_CDN_BASE_URL and S3_SECRET_KEY_CSVJSONL could be set to a random list of strings.\n\n```\nexport OPENAI_API_KEY=\"your_OpenAI_api_key\" \\\nLLM_API_KEY=\"your_OpenAI_api_key\" \\\nPAGEFIND_CDN_BASE_URL=\"lZP8X4h0Q5Sj2ZmV,aAmu1W92T6DbFUkJ,DZ5pMvz8P1kKNH0r,QAqwvKh8rI5sPmuW,YMwgsBz7jLfN0oX8\" \\\nS3_SECRET_KEY_CSVJSONL=\"Gq6wzS3mjC5kL7i4KwexnL3gP8Z1a5Xv,V2c4ZnL0uHqBzFvR2NcN8Pb1g6CjmX9J,TfA1h8LgI5zYkH9A9p7NvWlL0sZzF9p8N,pKr81pLq5n6MkNzT1X09R7Qb0Vn5cFr0d,DzYwz82FQiW6T3u9A4z9h7HLOlJb7L2V1\" \\\nGROQ_API_KEY=\"GROQ_API_KEY_if_applicable\"\n\n```\n\n### Start docker container services needed for local dev\n\n```\ncat .env.chat .env.search .env.server .env.docker-compose > .env\n\n./convenience.sh -l\n```\n\n### Install front-end packages for local dev\n\n```\ncd frontends\nyarn\n```\n`cd ..`\n\n```\ncd clients/ts-sdk\nyarn build\n```\n`cd ../..`\n\n### Start services for local dev\n\nIt is recommend to manage services through [tmuxp, see the guide here](https://gist.github.com/skeptrunedev/101c7a13bb9b9242999830655470efac) or terminal tabs.\n\n```\ncd frontends\nyarn\nyarn dev\n```\n\n```\ncd server\ncargo watch -x run\n```\n\n```\ncd server\ncargo run --bin ingestion-worker\n```\n\n```\ncd server\ncargo run --bin file-worker\n```\n\n```\ncd server\ncargo run --bin delete-worker\n```\n\n```\ncd search\nyarn\nyarn dev\n```\n\n### Verify Working Setup\n\nAfter the cargo build has finished (after the `tmuxp load trieve`):\n- check that you can see redoc with the OpenAPI reference at [localhost:8090/redoc](http://localhost:8090/redoc)\n- make an account create a dataset with test data at [localhost:5173](http://localhost:5173)\n- search that dataset with test data at [localhost:5174](http://localhost:5174)\n\n### Additional Instructions for testing cross encoder reranking models\n\nTo test the Cross Encoder rerankers in local dev, \n- click on the dataset, go to the Dataset Settings -> Dataset Options -> Additional Options and uncheck the `Fulltext Enabled` option.\n- in the Embedding Settings, select your reranker model and enter the respective key in the adjacent textbox, and hit save.\n- in the search playground, set Type -> Semantic and select Rerank By -> Cross Encoder\n- if AIMon Reranker is selected in the Embedding Settings, you can enter an optional Task Definition in the search playground to specify the domain of context documents to the AIMon reranker.\n\n\n### Debugging issues with local dev\n\nReach out to us on [discord](https://discord.gg/E9sPRZqpDT) for assistance. We are available and more than happy to assist.\n\n## Debug diesel by getting the exact generated SQL\n\n`diesel::debug_query(&query).to_string();`\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts clients/mcp-server/src/index.ts\n```\n## Local Setup for Testing Stripe Features\n\nInstall Stripe CLI.\n\n1. `stripe login`\n2. `stripe listen --forward-to localhost:8090/api/stripe/webhook`\n3. set the `STRIPE_WEBHOOK_SECRET` in the `server/.env` to the resulting webhook signing secret\n4. `stripe products create --name trieve --default-price-data.unit-amount 1200 --default-price-data.currency usd`\n5. `stripe plans create --amount=1200 --currency=usd --interval=month --product={id from response of step 3}`\n\n## Contributors\n\n<a href=\"https://github.com/devflowinc/trieve/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=devflowinc/trieve\"/>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datasets",
        "trieve",
        "devflowinc",
        "datasets trieve",
        "extraction devflowinc",
        "devflowinc trieve"
      ],
      "category": "search--data-extraction"
    },
    "emicklei--melrose-mcp": {
      "owner": "emicklei",
      "name": "melrose-mcp",
      "url": "https://github.com/emicklei/melrose-mcp",
      "imageUrl": "",
      "description": "Plays [Melrōse](https://melrōse.org) music expressions as MIDI",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-28T09:15:35Z",
      "readme_content": "## melrose-mcp \n\n[melrōse](https://melrōse.org) is a tool to create and play music by programming melodies. It uses a custom language to compose notes and create loops and tracks to play.\n\n`melrose-mcp` is an [MCP](http://modelcontextprotocol.io/) server (STDIO) that uses the packages from the `melrōse` tool.\n\n## installation\n\nThis mcp server requires the [Go SDK](https://go.dev) for installation.\nTo produce music, you need to setup a MIDI receiver such as a DAW or a hardware synthesizer.\n\n    go install github.com/emicklei/melrose-mcp@latest\n\n## tools\n\n- melrose_play\n- melrose_bpm\n- melrose_devices\n- melrose_change_output_device\n\n## session\n\n>   change the tempo in melrose to a lower value\n\n`melrose_bmp` request:\n```javascript\n{\n  `bpm`: `80`\n}\n```\n\n`llm` response:\n```\nI've reduced the tempo to 80 beats per minute from the default of 120 BPM. This creates a more relaxed and slower pace for any musical expressions we play.\n```\n\n>   play a tune from Claude Debussy\n\n`llm` response:\n```\nI'll create a small segment inspired by Claude Debussy's impressionist style using the Melrose language at our current tempo of 80 BPM.\n```\n\n`melrose_play` request:\n```javascript\n{\n  `expression`: `sequence('(F4 A4 D5) 4(F4 A4 D5) (G4 B4 E5) 4(G4 B4 E5) (A4 C5 F5) 4(A4 C5 F5) (G4 B4 E5) 4(G4 B4 E5) (F4 A4 D5) 8= 8(E4 G4 C5) 4(F4 A4 D5)')`\n}\n```\n`melrose_play` response:\n```javascript\n12.749963417s\n```\n\n## configuration in your client\n\nBelow an example to configure it in Claude Desktop.\n\n    {\n        \"mcpServers\": {\n            \"melrōse\": {\n              \"command\": \"<YOUR DIRECTORY TO THE BINARY>/melrose-mcp\",\n              \"args\": []\n            }\n        }\n    }\n\n## example prompts\n\n- play the music sequence  C D E\n- play the first bar of fur elise\n- play the famous tune from Mike Oldfield called Tubular Bells\n- play the notes of Frere Jacques\n- play the C# chord\n\nTo help your LLM understand the Melrōse language better, you can use this [context](resources/melrose_llm_context.txt).\n\n### Contributions\n\nFixes, suggestions, documentation improvements are all welcome.\nFork this project and submit small Pull requests. \nDiscuss larger ones in the Issues list.\nYou can also sponsor Melrōse via [Github Sponsors](https://github.com/sponsors/emicklei).\n\nSoftware is licensed under [MIT](LICENSE).\n&copy; 2025 [ernestmicklei.com](http://ernestmicklei.com)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "midi",
        "music",
        "search",
        "expressions midi",
        "music expressions",
        "org music"
      ],
      "category": "search--data-extraction"
    },
    "fatwang2--search1api-mcp": {
      "owner": "fatwang2",
      "name": "search1api-mcp",
      "url": "https://github.com/fatwang2/search1api-mcp",
      "imageUrl": "",
      "description": "Search via search1api (requires paid API key)",
      "stars": 156,
      "forks": 36,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T08:43:28Z",
      "readme_content": "# Search1API MCP Server\n\n[中文文档](./README_zh.md)\n\nA Model Context Protocol (MCP) server that provides search and crawl functionality using Search1API.\n\n## Prerequisites\n\n- Node.js >= 18.0.0\n- A valid Search1API API key (See **Setup Guide** below on how to obtain and configure)\n\n## Installation (Standalone / General)\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    cd search1api-mcp\n    ```\n\n2.  **Configure API Key:** Before building, you need to provide your Search1API key. See the **Setup Guide** section below for different methods (e.g., using a `.env` file or environment variables).\n\n3.  **Install dependencies and build:**\n    ```bash\n    npm install\n    npm run build\n    ```\n    *Note: If using the project's `.env` file method for the API key, ensure it exists before this step.*\n\n## Usage (Standalone / General)\n\nEnsure your API key is configured (see **Setup Guide**).\n\nStart the server:\n```bash\nnpm start\n```\n\nThe server will then be ready to accept connections from MCP clients.\n\n## Setup Guide\n\n### 1. Get Search1API Key\n\n1.  Register at [Search1API](https://www.search1api.com/?utm_source=mcp)\n2.  Get your API key from your dashboard.\n\n### 2. Configure API Key\n\nYou need to make your API key available to the server. Choose **one** of the following methods:\n\n**Method A: Project `.env` File (Recommended for Standalone or LibreChat)**\n\nThis method is required if integrating with the current version of LibreChat (see specific section below).\n\n1.  In the `search1api-mcp` project root directory, create a file named `.env`:\n    ```bash\n    # In the search1api-mcp directory\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    ```\n2.  Replace `your_api_key_here` with your actual key.\n3.  Make sure this file exists **before** running `npm install && npm run build`.\n\n**Method B: Environment Variable (Standalone Only)**\n\nSet the `SEARCH1API_KEY` environment variable before starting the server.\n\n```bash\nexport SEARCH1API_KEY=\"your_api_key_here\"\nnpm start\n```\n\n**Method C: MCP Client Configuration (Advanced)**\n\nSome MCP clients allow specifying environment variables directly in their configuration. This is useful for clients like Cursor, VS Code extensions, etc.\n\n```json\n{\n  \"mcpServers\": {\n    \"search1api\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"search1api-mcp\"\n      ],\n      \"env\": {\n        \"SEARCH1API_KEY\": \"YOUR_SEARCH1API_KEY\"\n      }\n    }\n  }\n}\n```\n\n**Note for LibreChat Users:** Due to current limitations in LibreChat, Method A (Project `.env` File) is the **required** method. See the dedicated integration section below for full instructions.\n\n## Integration with LibreChat (Docker)\n\nThis section details the required steps for integrating with LibreChat via Docker.\n\n**Overview:**\n\n1.  Clone this server's repository into a location accessible by your LibreChat `docker-compose.yml`.\n2.  Configure the required API key using the **Project `.env` File method** within this server's directory.\n3.  Build this server.\n4.  Tell LibreChat how to run this server by editing `librechat.yaml`.\n5.  Make sure the built server code is available inside the LibreChat container via a Docker volume bind.\n6.  Restart LibreChat.\n\n**Step-by-Step:**\n\n1.  **Clone the Repository:**\n    Navigate to the directory on your host machine where you manage external services for LibreChat (this is often alongside your `docker-compose.yml`). A common location is a dedicated `mcp-server` directory.\n    ```bash\n    # Example: Navigate to where docker-compose.yml lives, then into mcp-server\n    cd /path/to/your/librechat/setup/mcp-server\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    ```\n\n2.  **Navigate into the Server Directory:**\n    ```bash\n    cd search1api-mcp\n    ```\n\n3.  **Configure API Key (Project `.env` File Method - Required for LibreChat):**\n    ```bash\n    # Create the .env file\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    # IMPORTANT: Replace 'your_api_key_here' with your actual Search1API key\n    ```\n\n4.  **Install Dependencies and Build:**\n    This step compiles the server code into the `build` directory.\n    ```bash\n    npm install\n    npm run build\n    ```\n\n5.  **Configure `librechat.yaml`:**\n    Edit your main `librechat.yaml` file to tell LibreChat how to execute this MCP server. Add an entry under `mcp_servers`:\n    ```yaml\n    # In your main librechat.yaml\n    mcp_servers:\n      # You can add other MCP servers here too\n      search1api:\n        # Optional: Display name for the server in LibreChat UI\n        # name: Search1API Tools\n\n        # Command tells LibreChat to use 'node'\n        command: node\n\n        # Args specify the script for 'node' to run *inside the container*\n        args:\n          - /app/mcp-server/search1api-mcp/build/index.js\n    ```\n    *   The `args` path (`/app/...`) is the location *inside* the LibreChat API container where the built server will be accessed (thanks to the volume bind in the next step).\n\n6.  **Configure Docker Volume Bind:**\n    Edit your `docker-compose.yml` (or more likely, your `docker-compose.override.yml`) to map the `search1api-mcp` directory from your host machine into the LibreChat API container. Find the `volumes:` section for the `api:` service:\n    ```yaml\n    # In your docker-compose.yml or docker-compose.override.yml\n    services:\n      api:\n        # ... other service config ...\n        volumes:\n          # ... other volumes likely exist here ...\n\n          # Add this volume bind:\n          - ./mcp-server/search1api-mcp:/app/mcp-server/search1api-mcp\n    ```\n    *   **Host Path (`./mcp-server/search1api-mcp`):** This is the path on your host machine *relative* to where your `docker-compose.yml` file is located. Adjust it if you cloned the repo elsewhere.\n    *   **Container Path (`:/app/mcp-server/search1api-mcp`):** This is the path *inside* the container. It **must match** the directory structure used in the `librechat.yaml` `args` path.\n\n7.  **Restart LibreChat:**\n    Apply the changes by rebuilding (if you modified `docker-compose.yml`) and restarting your LibreChat stack.\n    ```bash\n    docker compose down && docker compose up -d --build\n    # Or: docker compose restart api (if only librechat.yaml changed)\n    ```\n\nNow, the Search1API server should be available as a tool provider within LibreChat.\n\n## Features\n\n- Web search functionality\n- News search functionality\n- Web page content extraction\n- Website sitemap extraction\n- Deep thinking and complex problem solving with DeepSeek R1\n- Seamless integration with Claude Desktop, Cursor, Windsurf, Cline and other MCP clients\n\n## Tools\n\n### 1. Search Tool\n- Name: `search`\n- Description: Search the web using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"google\"): Search service to use (google, bing, duckduckgo, yahoo, x, reddit, github, youtube, arxiv, wechat, bilibili, imdb, wikipedia)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 2. News Tool\n- Name: `news`\n- Description: Search for news articles using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"bing\"): Search service to use (google, bing, duckduckgo, yahoo, hackernews)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 3. Crawl Tool\n- Name: `crawl`\n- Description: Extract content from a URL using Search1API\n- Parameters:\n  * `url` (required): URL to crawl\n\n### 4. Sitemap Tool\n- Name: `sitemap`\n- Description: Get all related links from a URL\n- Parameters:\n  * `url` (required): URL to get sitemap\n\n### 5. Reasoning Tool\n- Name: `reasoning`\n- Description: A tool for deep thinking and complex problem solving with fast deepseek r1 model and web search ability(You can change to any other model in search1api website but the speed is not guaranteed)\n- Parameters:\n  * `content` (required): The question or problem that needs deep thinking\n\n### 6. Trending Tool\n- Name: `trending`\n- Description: Get trending topics from popular platforms\n- Parameters:\n  * `search_service` (required): Specify the platform to get trending topics from (github, hackernews)\n  * `max_results` (optional, default: 10): Maximum number of trending items to return\n\n## Version History\n\n- v0.2.0: Added fallback `.env` support for LibreChat integration and updated dependencies.\n- v0.1.8: Added X(Twitter) and Reddit search services\n- v0.1.7: Added Trending tool for GitHub and Hacker News\n- v0.1.6: Added Wikipedia search service\n- v0.1.5: Added new search parameters (include_sites, exclude_sites, time_range) and new search services (arxiv, wechat, bilibili, imdb)\n- v0.1.4: Added reasoning tool with deepseek r1 and updated the Cursor and Windsurf configuration guide\n- v0.1.3: Added news search functionality\n- v0.1.2: Added sitemap functionality\n- v0.1.1: Added web crawling functionality\n- v0.1.0: Initial release with search functionality\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search1api",
        "fatwang2",
        "search",
        "search1api mcp",
        "fatwang2 search1api",
        "search search1api"
      ],
      "category": "search--data-extraction"
    },
    "format37--youtube_mcp": {
      "owner": "format37",
      "name": "youtube_mcp",
      "url": "https://github.com/format37/youtube_mcp",
      "imageUrl": "",
      "description": "dlp to download audio and OpenAI's Whisper-1 for more precise transcription than youtube captions. Provide a YouTube URL and get back the full transcript splitted by chunks for long videos.",
      "stars": 23,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-23T10:24:54Z",
      "readme_content": "# youtube_mcp\nYoutube transcribation MCP server\n\n## Demo Video\n\n[![YouTube MCP Demo](https://img.youtube.com/vi/bS5vKuehzEE/maxresdefault.jpg)](https://youtu.be/bS5vKuehzEE)\n\n*Click the image above to watch the demo video*\n\n## Requirements:\n* OpenAI API key\n* Cookies\n\n## Server installation\n* Clone the repo:\n```\ngit clone https://github.com/format37/youtube_mcp.git\ncd youtube_mcp\nnano .env\n```\n* Extract your cookies. See [cookies.md](./cookies.md)  \nPlace cookies.txt in the ./mcp/ folder.\n* Generate MCP_KEY:\n```\npython token_generator.py\n```\n* Define .env:\n```\nCONTAINER_NAME=youtube_mcp_main\nPORT=7001\nMCP_KEY=YOUR-MCP-KEY\nOPENAI_API_KEY=YOUR-OPENAI-KRY\n```\n* Provide run access\n```\nsudo chmod +x compose.sh\nsudo chmod +x logs.sh\nsudo chmod +x update.sh\n```\n* Run\n```\n./compose.sh\n```\n* Check that port is opened for incoming connections.\n\n## Client configuration\n3. Add Bybit server to the Claude desktop config:\nExample:\n```\n{\n    \"mcpServers\": {\n      \"youtube\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"mcp-remote\",\n          \"http://localhost:7001/sse\",\n          \"--header\",\n          \"Authorization:YOUR-TOKEN\",\n          \"--allow-http\"\n        ],\n        \"disabled\": false\n      }\n    }\n}\n```\n## Client side\n```\n4. Restart Claude desktop\n5. Check that tws tools are listed in the tools list. Ask Claude to check ibkr account",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube_mcp",
        "transcript",
        "transcription",
        "format37 youtube_mcp",
        "youtube_mcp dlp",
        "transcription youtube"
      ],
      "category": "search--data-extraction"
    },
    "genomoncology--biomcp": {
      "owner": "genomoncology",
      "name": "biomcp",
      "url": "https://github.com/genomoncology/biomcp",
      "imageUrl": "",
      "description": "Biomedical research server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.",
      "stars": 317,
      "forks": 54,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T07:02:52Z",
      "readme_content": "# BioMCP: Biomedical Model Context Protocol\n\nBioMCP is an open source (MIT License) toolkit that empowers AI assistants and\nagents with specialized biomedical knowledge. Built following the Model Context\nProtocol (MCP), it connects AI systems to authoritative biomedical data\nsources, enabling them to answer questions about clinical trials, scientific\nliterature, and genomic variants with precision and depth.\n\n[](https://www.youtube.com/watch?v=bKxOWrWUUhM)\n\n## MCPHub Certification\n\nBioMCP is certified by [MCPHub](https://mcphub.com/mcp-servers/genomoncology/biomcp). This certification ensures that BioMCP follows best practices for Model Context Protocol implementation and provides reliable biomedical data access.\n\n## Why BioMCP?\n\nWhile Large Language Models have broad general knowledge, they often lack\nspecialized domain-specific information or access to up-to-date resources.\nBioMCP bridges this gap for biomedicine by:\n\n- Providing **structured access** to clinical trials, biomedical literature,\n  and genomic variants\n- Enabling **natural language queries** to specialized databases without\n  requiring knowledge of their specific syntax\n- Supporting **biomedical research** workflows through a consistent interface\n- Functioning as an **MCP server** for AI assistants and agents\n\n## Biomedical Data Sources\n\nBioMCP integrates with multiple biomedical data sources:\n\n### Literature Sources\n\n- **PubTator3/PubMed** - Peer-reviewed biomedical literature with entity annotations\n- **bioRxiv/medRxiv** - Preprint servers for biology and health sciences\n- **Europe PMC** - Open science platform including preprints\n\n### Clinical & Genomic Sources\n\n- **ClinicalTrials.gov** - Clinical trial registry and results database\n- **NCI Clinical Trials Search API** - National Cancer Institute's curated cancer trials database\n  - Advanced search filters (biomarkers, prior therapies, brain metastases)\n  - Organization and intervention databases\n  - Disease vocabulary with synonyms\n- **BioThings Suite** - Comprehensive biomedical data APIs:\n  - **MyVariant.info** - Consolidated genetic variant annotation\n  - **MyGene.info** - Real-time gene annotations and information\n  - **MyDisease.info** - Disease ontology and synonym information\n  - **MyChem.info** - Drug/chemical annotations and properties\n- **TCGA/GDC** - The Cancer Genome Atlas for cancer variant data\n- **1000 Genomes** - Population frequency data via Ensembl\n- **cBioPortal** - Cancer genomics portal with mutation occurrence data\n\n### Regulatory & Safety Sources\n\n- **OpenFDA** - FDA regulatory and safety data:\n  - **Drug Adverse Events (FAERS)** - Post-market drug safety reports\n  - **Drug Labels (SPL)** - Official prescribing information\n  - **Device Events (MAUDE)** - Medical device adverse events, with genomic device filtering\n\n## Available MCP Tools\n\nBioMCP provides 24 specialized tools for biomedical research:\n\n### Core Tools (3)\n\n#### 1. Think Tool (ALWAYS USE FIRST!)\n\n**CRITICAL**: The `think` tool MUST be your first step for ANY biomedical research task.\n\n```python\n# Start analysis with sequential thinking\nthink(\n    thought=\"Breaking down the query about BRAF mutations in melanoma...\",\n    thoughtNumber=1,\n    totalThoughts=3,\n    nextThoughtNeeded=True\n)\n```\n\nThe sequential thinking tool helps:\n\n- Break down complex biomedical problems systematically\n- Plan multi-step research approaches\n- Track reasoning progress\n- Ensure comprehensive analysis\n\n#### 2. Search Tool\n\nThe search tool supports two modes:\n\n##### Unified Query Language (Recommended)\n\nUse the `query` parameter with structured field syntax for powerful cross-domain searches:\n\n```python\n# Simple natural language\nsearch(query=\"BRAF melanoma\")\n\n# Field-specific search\nsearch(query=\"gene:BRAF AND trials.condition:melanoma\")\n\n# Complex queries\nsearch(query=\"gene:BRAF AND variants.significance:pathogenic AND articles.date:>2023\")\n\n# Get searchable fields schema\nsearch(get_schema=True)\n\n# Explain how a query is parsed\nsearch(query=\"gene:BRAF\", explain_query=True)\n```\n\n**Supported Fields:**\n\n- **Cross-domain**: `gene:`, `variant:`, `disease:`\n- **Trials**: `trials.condition:`, `trials.phase:`, `trials.status:`, `trials.intervention:`\n- **Articles**: `articles.author:`, `articles.journal:`, `articles.date:`\n- **Variants**: `variants.significance:`, `variants.rsid:`, `variants.frequency:`\n\n##### Domain-Based Search\n\nUse the `domain` parameter with specific filters:\n\n```python\n# Search articles (includes automatic cBioPortal integration)\nsearch(domain=\"article\", genes=[\"BRAF\"], diseases=[\"melanoma\"])\n\n# Search with mutation-specific cBioPortal data\nsearch(domain=\"article\", genes=[\"BRAF\"], keywords=[\"V600E\"])\nsearch(domain=\"article\", genes=[\"SRSF2\"], keywords=[\"F57*\"])  # Wildcard patterns\n\n# Search trials\nsearch(domain=\"trial\", conditions=[\"lung cancer\"], phase=\"3\")\n\n# Search variants\nsearch(domain=\"variant\", gene=\"TP53\", significance=\"pathogenic\")\n```\n\n**Note**: When searching articles with a gene parameter, cBioPortal data is automatically included:\n\n- Gene-level summaries show mutation frequency across cancer studies\n- Mutation-specific searches (e.g., \"V600E\") show study-level occurrence data\n- Cancer types are dynamically resolved from cBioPortal API\n\n#### 3. Fetch Tool\n\nRetrieve full details for a single article, trial, or variant:\n\n```python\n# Fetch article details (supports both PMID and DOI)\nfetch(domain=\"article\", id=\"34567890\")  # PMID\nfetch(domain=\"article\", id=\"10.1101/2024.01.20.23288905\")  # DOI\n\n# Fetch trial with all sections\nfetch(domain=\"trial\", id=\"NCT04280705\", detail=\"all\")\n\n# Fetch variant details\nfetch(domain=\"variant\", id=\"rs113488022\")\n```\n\n**Domain-specific options:**\n\n- **Articles**: `detail=\"full\"` retrieves full text if available\n- **Trials**: `detail` can be \"protocol\", \"locations\", \"outcomes\", \"references\", or \"all\"\n- **Variants**: Always returns full details\n\n### Individual Tools (21)\n\nFor users who prefer direct access to specific functionality, BioMCP also provides 21 individual tools:\n\n#### Article Tools (2)\n\n- **article_searcher**: Search PubMed/PubTator3 and preprints\n- **article_getter**: Fetch detailed article information (supports PMID and DOI)\n\n#### Trial Tools (5)\n\n- **trial_searcher**: Search ClinicalTrials.gov or NCI CTS API (via source parameter)\n- **trial_getter**: Fetch all trial details from either source\n- **trial_protocol_getter**: Fetch protocol information only (ClinicalTrials.gov)\n- **trial_references_getter**: Fetch trial publications (ClinicalTrials.gov)\n- **trial_outcomes_getter**: Fetch outcome measures and results (ClinicalTrials.gov)\n- **trial_locations_getter**: Fetch site locations and contacts (ClinicalTrials.gov)\n\n#### Variant Tools (2)\n\n- **variant_searcher**: Search MyVariant.info database\n- **variant_getter**: Fetch comprehensive variant details\n\n#### NCI-Specific Tools (6)\n\n- **nci_organization_searcher**: Search NCI's organization database\n- **nci_organization_getter**: Get organization details by ID\n- **nci_intervention_searcher**: Search NCI's intervention database (drugs, devices, procedures)\n- **nci_intervention_getter**: Get intervention details by ID\n- **nci_biomarker_searcher**: Search biomarkers used in trial eligibility criteria\n- **nci_disease_searcher**: Search NCI's controlled vocabulary of cancer conditions\n\n#### Gene, Disease & Drug Tools (3)\n\n- **gene_getter**: Get real-time gene information from MyGene.info\n- **disease_getter**: Get disease definitions and synonyms from MyDisease.info\n- **drug_getter**: Get drug/chemical information from MyChem.info\n\n**Note**: All individual tools that search by gene automatically include cBioPortal summaries when the `include_cbioportal` parameter is True (default). Trial searches can expand disease conditions with synonyms when `expand_synonyms` is True (default).\n\n## Quick Start\n\n### For Claude Desktop Users\n\n1. **Install `uv`** if you don't have it (recommended):\n\n   ```bash\n   # MacOS\n   brew install uv\n\n   # Windows/Linux\n   pip install uv\n   ```\n\n2. **Configure Claude Desktop**:\n   - Open Claude Desktop settings\n   - Navigate to Developer section\n   - Click \"Edit Config\" and add:\n   ```json\n   {\n     \"mcpServers\": {\n       \"biomcp\": {\n         \"command\": \"uv\",\n         \"args\": [\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"]\n       }\n     }\n   }\n   ```\n   - Restart Claude Desktop and start chatting about biomedical topics!\n\n### Python Package Installation\n\n```bash\n# Using pip\npip install biomcp-python\n\n# Using uv (recommended for faster installation)\nuv pip install biomcp-python\n\n# Run directly without installation\nuv run --with biomcp-python biomcp trial search --condition \"lung cancer\"\n```\n\n## Configuration\n\n### Environment Variables\n\nBioMCP supports optional environment variables for enhanced functionality:\n\n```bash\n# cBioPortal API authentication (optional)\nexport CBIO_TOKEN=\"your-api-token\"  # For authenticated access\nexport CBIO_BASE_URL=\"https://www.cbioportal.org/api\"  # Custom API endpoint\n\n# Performance tuning\nexport BIOMCP_USE_CONNECTION_POOL=\"true\"  # Enable HTTP connection pooling (default: true)\nexport BIOMCP_METRICS_ENABLED=\"false\"     # Enable performance metrics (default: false)\n```\n\n## Running BioMCP Server\n\nBioMCP supports multiple transport protocols to suit different deployment scenarios:\n\n### Local Development (STDIO)\n\nFor direct integration with Claude Desktop or local MCP clients:\n\n```bash\n# Default STDIO mode for local development\nbiomcp run\n\n# Or explicitly specify STDIO\nbiomcp run --mode stdio\n```\n\n### HTTP Server Mode\n\nBioMCP supports multiple HTTP transport protocols:\n\n#### Legacy SSE Transport (Worker Mode)\n\nFor backward compatibility with existing SSE clients:\n\n```bash\nbiomcp run --mode worker\n# Server available at http://localhost:8000/sse\n```\n\n#### Streamable HTTP Transport (Recommended)\n\nThe new MCP-compliant Streamable HTTP transport provides optimal performance and standards compliance:\n\n```bash\nbiomcp run --mode streamable_http\n\n# Custom host and port\nbiomcp run --mode streamable_http --host 127.0.0.1 --port 8080\n```\n\nFeatures of Streamable HTTP transport:\n\n- Single `/mcp` endpoint for all operations\n- Dynamic response mode (JSON for quick operations, SSE for long-running)\n- Session management support (future)\n- Full MCP specification compliance (2025-03-26)\n- Better scalability for cloud deployments\n\n### Deployment Options\n\n#### Docker\n\n```bash\n# Build the Docker image locally\ndocker build -t biomcp:latest .\n\n# Run the container\ndocker run -p 8000:8000 biomcp:latest biomcp run --mode streamable_http\n```\n\n#### Cloudflare Workers\n\nThe worker mode can be deployed to Cloudflare Workers for global edge deployment.\n\nNote: All APIs work without authentication, but tokens may provide higher rate limits.\n\n## Command Line Interface\n\nBioMCP provides a comprehensive CLI for direct database interaction:\n\n```bash\n# Get help\nbiomcp --help\n\n# Run the MCP server\nbiomcp run\n\n# Article search examples\nbiomcp article search --gene BRAF --disease Melanoma  # Includes preprints by default\nbiomcp article search --gene BRAF --no-preprints      # Exclude preprints\nbiomcp article get 21717063 --full\n\n# Clinical trial examples\nbiomcp trial search --condition \"Lung Cancer\" --phase PHASE3\nbiomcp trial search --condition melanoma --source nci --api-key YOUR_KEY  # Use NCI API\nbiomcp trial get NCT04280705 Protocol\nbiomcp trial get NCT04280705 --source nci --api-key YOUR_KEY  # Get from NCI\n\n# Variant examples with external annotations\nbiomcp variant search --gene TP53 --significance pathogenic\nbiomcp variant get rs113488022  # Includes TCGA, 1000 Genomes, and cBioPortal data by default\nbiomcp variant get rs113488022 --no-external  # Core annotations only\n\n# NCI-specific examples (requires NCI API key)\nbiomcp organization search \"MD Anderson\" --api-key YOUR_KEY\nbiomcp organization get ORG123456 --api-key YOUR_KEY\nbiomcp intervention search pembrolizumab --api-key YOUR_KEY\nbiomcp intervention search --type Device --api-key YOUR_KEY\nbiomcp biomarker search \"PD-L1\" --api-key YOUR_KEY\nbiomcp disease search melanoma --source nci --api-key YOUR_KEY\n```\n\n## Testing & Verification\n\nTest your BioMCP setup with the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run --with biomcp-python biomcp run\n```\n\nThis opens a web interface where you can explore and test all available tools.\n\n## Enterprise Version: OncoMCP\n\nOncoMCP extends BioMCP with GenomOncology's enterprise-grade precision oncology\nplatform (POP), providing:\n\n- **HIPAA-Compliant Deployment**: Secure on-premise options\n- **Real-Time Trial Matching**: Up-to-date status and arm-level matching\n- **Healthcare Integration**: Seamless EHR and data warehouse connectivity\n- **Curated Knowledge Base**: 15,000+ trials and FDA approvals\n- **Sophisticated Patient Matching**: Using integrated clinical and molecular\n  profiles\n- **Advanced NLP**: Structured extraction from unstructured text\n- **Comprehensive Biomarker Processing**: Mutation and rule processing\n\nLearn more: [GenomOncology](https://genomoncology.com/)\n\n## MCP Registries\n\n[![smithery badge](https://smithery.ai/badge/@genomoncology/biomcp)](https://smithery.ai/server/@genomoncology/biomcp)\n\n<a href=\"https://glama.ai/mcp/servers/@genomoncology/biomcp\">\n<img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@genomoncology/biomcp/badge\" />\n</a>\n\n## Example Use Cases\n\n### Gene Information Retrieval\n\n```python\n# Get comprehensive gene information\ngene_getter(gene_id_or_symbol=\"TP53\")\n# Returns: Official name, summary, aliases, links to databases\n```\n\n### Disease Synonym Expansion\n\n```python\n# Get disease information with synonyms\ndisease_getter(disease_id_or_name=\"GIST\")\n# Returns: \"gastrointestinal stromal tumor\" and other synonyms\n\n# Search trials with automatic synonym expansion\ntrial_searcher(conditions=[\"GIST\"], expand_synonyms=True)\n# Searches for: GIST OR \"gastrointestinal stromal tumor\" OR \"GI stromal tumor\"\n```\n\n### Integrated Biomedical Research\n\n```python\n# 1. Always start with thinking\nthink(thought=\"Analyzing BRAF V600E in melanoma treatment\", thoughtNumber=1)\n\n# 2. Get gene context\ngene_getter(\"BRAF\")\n\n# 3. Search for pathogenic variants\nvariant_searcher(gene=\"BRAF\", hgvsp=\"V600E\", significance=\"pathogenic\")\n\n# 4. Find relevant clinical trials with disease expansion\ntrial_searcher(conditions=[\"melanoma\"], interventions=[\"BRAF inhibitor\"])\n```\n\n## Documentation\n\nFor comprehensive documentation, visit [https://biomcp.org](https://biomcp.org)\n\n### Developer Guides\n\n- [HTTP Client Guide](./docs/http-client-guide.md) - Using the centralized HTTP client\n- [Migration Examples](./docs/migration-examples.md) - Migrating from direct HTTP usage\n- [Error Handling Guide](./docs/error-handling.md) - Comprehensive error handling patterns\n- [Integration Testing Guide](./docs/integration-testing.md) - Best practices for reliable integration tests\n- [Third-Party Endpoints](./THIRD_PARTY_ENDPOINTS.md) - Complete list of external APIs used\n- [Testing Guide](./docs/development/testing.md) - Running tests and understanding test categories\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests (including integration tests)\nmake test\n\n# Run only unit tests (excluding integration tests)\nuv run python -m pytest tests -m \"not integration\"\n\n# Run only integration tests\nuv run python -m pytest tests -m \"integration\"\n```\n\n**Note**: Integration tests make real API calls and may fail due to network issues or rate limiting.\nIn CI/CD, integration tests are run separately and allowed to fail without blocking the build.\n\n## BioMCP Examples Repo\n\nLooking to see BioMCP in action?\n\nCheck out the companion repository:\n👉 **[biomcp-examples](https://github.com/genomoncology/biomcp-examples)**\n\nIt contains real prompts, AI-generated research briefs, and evaluation runs across different models.\nUse it to explore capabilities, compare outputs, or benchmark your own setup.\n\nHave a cool example of your own?\n**We’d love for you to contribute!** Just fork the repo and submit a PR with your experiment.\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomcp",
        "biomedical",
        "genomoncology",
        "genomoncology biomcp",
        "biomcp biomedical",
        "extraction genomoncology"
      ],
      "category": "search--data-extraction"
    },
    "hellokaton--unsplash-mcp-server": {
      "owner": "hellokaton",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/hellokaton/unsplash-mcp-server",
      "imageUrl": "",
      "description": "A MCP server for Unsplash image search.",
      "stars": 174,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:30:58Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hellokaton-unsplash-mcp-server-badge.png)](https://mseep.ai/app/hellokaton-unsplash-mcp-server)\n\n# Unsplash MCP Server\n\nEnglish | [简体中文](README_zh.md)\n\n> A simple MCP server for seamless Unsplash image integration and search capabilities.\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@hellokaton/unsplash-mcp-server)](https://smithery.ai/server/@hellokaton/unsplash-mcp-server)\n\n## 📋 Overview\n\nUnsplash MCP Server is used for searching rich, high-quality images. It's ideal for developers who want to integrate Unsplash functionality into their own applications.\n\n## ✨ Features\n\n- **Advanced Image Search**: Search Unsplash's extensive photo library with filters for:\n  - Keyword relevance\n  - Color schemes\n  - Orientation options\n  - Custom sorting and pagination\n\n## 🔑 Obtaining Unsplash Access Key\n\nBefore installing this server, you'll need to obtain an Unsplash API Access Key:\n\n1. Create a developer account at [Unsplash](https://unsplash.com/developers)\n2. Register a new application\n3. Get your Access Key from the application details page\n4. Use this key in the configuration steps below\n\nFor more details, refer to the [official Unsplash API documentation](https://unsplash.com/documentation).\n\n## 🚀 Installation\n\nTo install Unsplash Image Integration Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hellokaton/unsplash-mcp-server):\n\n### IDE Setup\n\n**Cursor IDE**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cursor --key 7558c683-****-****\n```\n\n**Windsurf**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client windsurf --key 7558c683-****-****\n```\n\n**Cline**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cline --key 7558c683-****-****\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/hellokaton/unsplash-mcp-server.git\n\n# Navigate to project directory\ncd unsplash-mcp-server\n\n# Create virtual environment\nuv venv\n\n# Install dependencies\nuv pip install .\n```\n\n**Cursor Editor Integration**\n\nAdd the following configuration to your Cursor editor's `settings.json`:\n\n⚠️ **Note:** Please adjust the following configuration according to your actual installation:\n\n- If `uv` is not in your system PATH, use an absolute path (e.g., `/path/to/uv`)\n- `./server.py` should be modified to the actual location of your server script (can use absolute path or path relative to workspace)\n\n\n\n```json\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"./server.py\"],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"${YOUR_ACCESS_KEY}\"\n      }\n    }\n  }\n}\n```\n\n### Using in Cursor\n\n\n\n## 🛠️ Available Tools\n\n### Search Photos\n\n```json\n{\n  \"tool\": \"search_photos\",\n  \"query\": \"mountain\",\n  \"per_page\": 5,\n  \"orientation\": \"landscape\"\n}\n```\n\n## 🔄 Other Implementations\n\n- Golang: [unsplash-mcp-server](https://github.com/douglarek/unsplash-mcp-server)\n- Java: [unsplash-mcp-server](https://github.com/JavaProgrammerLB/unsplash-mcp-server)\n\n## 📄 License\n\n[MIT License](LICENSE)\n\n## 📬 Contact\n\n- [Twitter/X](https://x.com/hellokaton)\n- [GitHub Issues](https://github.com/hellokaton/unsplash-mcp-server/issues)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "unsplash",
        "search",
        "unsplash mcp",
        "image search",
        "mcp server"
      ],
      "category": "search--data-extraction"
    },
    "imprvhub--mcp-domain-availability": {
      "owner": "imprvhub",
      "name": "mcp-domain-availability",
      "url": "https://github.com/imprvhub/mcp-domain-availability",
      "imageUrl": "",
      "description": "A Model Context Protocol (MCP) server that enables Claude Desktop to check domain availability across 50+ TLDs. Features DNS/WHOIS verification, bulk checking, and smart suggestions. Zero-clone installation via uvx.",
      "stars": 18,
      "forks": 5,
      "license": "Mozilla Public License 2.0",
      "language": "Python",
      "updated_at": "2025-09-29T14:44:45Z",
      "readme_content": "## MCP Domain Availability Checker\n\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-domain-availability)](https://smithery.ai/server/@imprvhub/mcp-domain-availability)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">A Model Context Protocol (MCP) integration that provides Claude Desktop with domain availability checking across popular TLDs.</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-domain-availability\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-domain-availability/badge\" alt=\"Domain Availability MCP server\" />\n</a></td>\n</tr>\n</table>\n\n### Features\n\n- **Domain Availability Checking**\n  - Check availability across 50+ popular TLD extensions\n  - Support for popular (.com, .io, .ai), country (.us, .uk, .de), and new TLDs (.app, .dev, .tech)\n  - Dual verification using DNS and WHOIS for accuracy\n  - Smart TLD suggestions organized by popularity\n\n- **Search Capabilities**\n  - Check specific domains with exact TLD matching\n  - Bulk checking across supported extensions for a given name\n  - Parallel processing for faster domain queries\n  - Organized results by TLD categories\n\n- **MCP Integration**\n  - Easy setup with uvx package management\n  - Seamless integration with Claude Desktop\n  - Real-time availability status updates\n  - Performance metrics and timing information\n\n- **AI Assistant Features**\n  - Natural language domain queries through Claude\n  - Automated domain suggestion workflows\n  - Smart recommendations based on availability\n\n### Demo\n<p>\n <a href=\"https://www.youtube.com/watch?v=pJjrkEihlWE\">\n   \n </a>\n</p>\n\n<details>\n<summary>Timestamps:</summary>\nClick on any timestamp to jump to that section of the video\n\n[**00:00**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=0s) - **Checking google.com availability**  \nTesting a well-known premium domain to demonstrate the domain checking functionality and alternative TLD suggestions.\n\n[**00:20**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=20s) - **Testing myawesomesite.com**  \nVerifying availability for a custom domain name and exploring alternative extension options.\n\n[**00:40**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=40s) - **Verifying techstartup2026.io**  \nExploring tech startup domain options and checking availability across multiple TLD extensions.\n\n[**01:00**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=60s) - **Analyzing aitools domain**  \nChecking competitive AI industry domains and analyzing market availability for startup naming.\n</details>\n\n### Requirements\n\n- Python 3.10 or higher\n- Claude Desktop\n- [uv](https://docs.astral.sh/uv/) package manager\n\n#### Dependencies Installation\n\nInstall uv package manager using one of these methods:\n\n**Official installer (recommended):**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Homebrew (macOS/Linux):**\n```bash\nbrew install uv\n```\n\n**Install Homebrew (if needed):**\n- Visit [https://brew.sh](https://brew.sh) for installation instructions on all operating systems\n- Or run: `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n\nThe MCP server automatically manages Python dependencies through uvx.\n\n### Installation\n\n#### Zero-Clone Installation (Recommended)\n\nThe MCP Domain Availability Checker supports direct installation without cloning repositories, using uvx for package management.\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the Domain Availability MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-domain-availability\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--python=3.10\",\n        \"--from\",\n        \"git+https://github.com/imprvhub/mcp-domain-availability\",\n        \"mcp-domain-availability\"\n      ]\n    }\n  }\n}\n```\n\nIf you already have other MCPs configured, simply add the \"mcp-domain-availability\" section inside the \"mcpServers\" object:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"mcp-domain-availability\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--python=3.10\",\n        \"--from\",\n        \"git+https://github.com/imprvhub/mcp-domain-availability\",\n        \"mcp-domain-availability\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install mcp-domain-availability for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@imprvhub/mcp-domain-availability):\n\n```bash\nnpx -y @smithery/cli install @imprvhub/mcp-domain-availability --client claude\n```\n\n#### Manual Installation\n\nFor development or local testing:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-domain-availability\ncd mcp-domain-availability\n```\n\n2. Install dependencies:\n```bash\nuv sync\n```\n\n3. Run locally:\n```bash\nuv run src/mcp_domain_availability/main.py\n```\n\n### How It Works\n\nThe MCP Domain Availability Checker uses multiple verification methods to determine domain availability:\n\n1. **DNS Resolution**: Checks if the domain resolves to an IP address\n2. **WHOIS Lookup**: Queries WHOIS databases for registration information\n3. **Socket Connection**: Falls back to socket-based checking when other methods aren't available\n\nThe tool combines results from these methods to provide accurate availability status, with parallel processing for checking multiple domains simultaneously.\n\n### Available Tools\n\n#### Domain Checking\n\n| Tool Name | Description | Usage |\n|-----------|-------------|-------|\n| `check_domain` | Check domain availability with --domain flag | `mysite.com --domain` or `mysite --domain` |\n\n### Supported TLD Categories\n\n#### Popular TLDs (12)\ncom, net, org, io, ai, app, dev, co, xyz, me, info, biz\n\n#### Country TLDs (35)\nus, uk, ca, au, de, fr, it, es, nl, jp, kr, cn, in, br, mx, ar, cl, co, pe, ru, pl, cz, ch, at, se, no, dk, fi, be, pt, gr, tr, za, eg, ma, ng, ke\n\n#### New TLDs\ntech, online, site, website, store, shop, cloud, digital, blog, news & more.\n\n### Example Usage\n\nHere are examples of how to use the MCP Domain Availability Checker with Claude:\n\n#### Single Domain Check\n\n```\nCheck if mysite.com is available using --domain\n```\n\n#### Domain Name Research\n\n```\nCheck availability for \"startup\" across all TLDs using --domain\n```\n\n#### Specific Domain Verification\n\n```\nIs awesome.io available? Use --domain to check\n```\n\n### Output Format\n\nThe tool provides comprehensive results including:\n\n- **Requested Domain**: Status of the exact domain queried (if specific TLD provided)\n- **Available Domains**: List of available domains sorted alphabetically\n- **Unavailable Domains**: List of registered domains\n- **Summary Statistics**: Breakdown by TLD categories (Popular, Country, New TLDs)\n- **Performance Metrics**: Check duration for each domain\n\n### Troubleshooting\n\n#### \"Server disconnected\" error\nIf you see connection errors in Claude Desktop:\n\n1. **Verify uvx installation**:\n   - Run `uvx --version` to ensure uvx is properly installed\n   - Reinstall uv if necessary: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n\n2. **Check Python version**:\n   - Ensure Python 3.10+ is available: `python3 --version`\n\n### DNS resolution issues\nIf domain checks are failing:\n\n1. **Network connectivity**:\n   - Verify internet connection is stable\n   - Check if DNS servers are accessible\n\n2. **Rate limiting**:\n   - Large bulk checks may hit rate limits from DNS/WHOIS services\n   - The tool uses a semaphore to limit concurrent requests to 20\n\n#### Configuration issues\nIf the MCP server isn't starting:\n\n1. **Verify configuration syntax**:\n   - Ensure JSON syntax is valid in `claude_desktop_config.json`\n   - Check that all brackets and quotes are properly matched\n\n2. **Restart Claude Desktop**:\n   - Close and restart Claude Desktop after configuration changes\n\n## Development\n\n#### Project Structure\n\n- `main.py`: Main entry point with MCP server and domain checking logic\n- Domain checking functions with DNS, WHOIS, and socket fallback methods\n- TLD management with categorized lists\n- Async processing for parallel domain checks\n\n#### Building\n\n```bash\nuv build\n```\n\n### Testing\n\n```bash\nuv run pytest\n```\n\n#### Local Development\n\n```bash\nuv run main.py\n```\n\n### Security Considerations\n\nThe MCP Domain Availability Checker makes external network requests to DNS servers and WHOIS services. Users should be aware that:\n\n- Domain queries may be logged by DNS providers\n- WHOIS queries are typically logged and may be rate-limited\n- No personal information is transmitted beyond the domain names being checked\n- All queries are read-only and do not modify any external systems\n\n### Contributing\n\nContributions are welcome! Areas for improvement include:\n\n- Adding support for additional TLD categories\n- Implementing caching mechanisms for faster repeated queries\n- Enhancing WHOIS parsing for more detailed domain information\n- Improving error handling and retry mechanisms\n\n### License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-domain-availability/blob/main/LICENSE) file for details.\n\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [uv Package Manager](https://docs.astral.sh/uv/)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "protocol",
        "availability",
        "mcp domain",
        "imprvhub mcp",
        "mcp server"
      ],
      "category": "search--data-extraction"
    },
    "imprvhub--mcp-rss-aggregator": {
      "owner": "imprvhub",
      "name": "mcp-rss-aggregator",
      "url": "https://github.com/imprvhub/mcp-rss-aggregator",
      "imageUrl": "",
      "description": "Model Context Protocol Server for aggregating RSS feeds in Claude Desktop.",
      "stars": 10,
      "forks": 6,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-27T11:11:34Z",
      "readme_content": "# MCP RSS Aggregator\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-rss-aggregator)](https://archestra.ai/mcp-catalog/imprvhub__mcp-rss-aggregator)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/51854dcf-6cb2-4bd0-a37f-5b87ba25d7c7)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-rss-aggregator)](https://smithery.ai/server/@imprvhub/mcp-rss-aggregator)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-rss-aggregator\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-rss-aggregator-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to fetch and read content from your favorite RSS feeds using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator/badge\" alt=\"RSS Aggregator MCP server\" />\n</a></td>\n</tr>\n</table>\n\n## Features\n\n- Read articles from your favorite RSS feeds directly in Claude Desktop\n- Support for OPML files to import your existing feed subscriptions\n- Organize feeds by categories\n- Get the latest articles across all your feeds\n- Filter articles by feed source or category\n- Well-formatted article presentation with titles, snippets, and links\n\n## Demo\n\n<p>\n  <a href=\"https://youtu.be/9pvm078fHkQ\">\n    \n  </a>\n</p>\n\n<details>\n<summary> Timestamps </summary>\n\nClick on any timestamp to jump to that section of the video\n\n[00:00](https://youtu.be/9pvm078fHkQ&t=0s) - **Sample RSS Feed Demonstration**:\nUsing the default 'sample-feeds.opml' file included in the repository. This segment displays how Claude processes and presents news content from sources like TechCrunch, The Verge, and other technology publications through the MCP (Model Context Protocol).\n\n[01:05](https://youtu.be/9pvm078fHkQ&t=65s) - **Configuration File Editing Process**:\nStep-by-step walkthrough of accessing and modifying the claude_desktop_config.json file to change the OPML file path reference from the default sample to a customized 'my-feeds.opml' file.\n\n[01:15](https://youtu.be/9pvm078fHkQ&t=75s) - **Application Restart Procedure**:\nIllustrating the necessary step of closing and reopening the Claude Desktop application to properly load and apply the modified OPML file configuration changes.\n\n[01:25](https://youtu.be/9pvm078fHkQ&t=85s) - **Custom RSS Feed Results**:\nDemonstration of the results after implementing the custom OPML file. This section highlights the expanded and more diverse news sources now available through Claude Desktop, including Spanish-language content.\n</details>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access RSS feeds\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-rss-aggregator\ncd mcp-rss-aggregator\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Feed Configuration\n\nThe RSS Aggregator supports both OPML and JSON formats for feed configuration.\n\n### Using OPML (Recommended)\n\nOPML (Outline Processor Markup Language) is a standard format used by most RSS readers to export and import feed subscriptions. \n\nA sample OPML file with popular feeds is included in the `public/sample-feeds.opml` file. You can:\n\n1. Use this file as-is\n2. Edit it to add your own feeds\n3. Replace it with an export from your existing RSS reader\n\nMost RSS readers allow you to export your subscriptions as an OPML file.\n\n### Using JSON\n\nAlternatively, you can define your feeds in a JSON file with the following format:\n\n```json\n[\n  {\n    \"title\": \"Hacker News\",\n    \"url\": \"https://news.ycombinator.com/rss\",\n    \"htmlUrl\": \"https://news.ycombinator.com/\",\n    \"category\": \"Tech News\"\n  },\n  {\n    \"title\": \"TechCrunch\",\n    \"url\": \"https://techcrunch.com/feed/\",\n    \"htmlUrl\": \"https://techcrunch.com/\",\n    \"category\": \"Tech News\"\n  }\n]\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the RSS Aggregator MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\n**Important Notes**: \n- Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-rss-aggregator`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-rss-aggregator`\n- Replace `ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml` with the path to your OPML or JSON file\n  - If omitted, the sample feeds file will be used\n\nIf you already have other MCPs configured, simply add the \"rssAggregator\" section inside the \"mcpServers\" object:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"\n      ],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `rss` command to interact with the RSS Aggregator MCP Server\n3. The MCP server runs as a subprocess managed by Claude Desktop\n\n## Available Commands\n\nThe RSS Aggregator MCP provides a tool named `rss` with several commands:\n\n| Command | Description | Parameters | Example |\n|---------|-------------|------------|---------|\n| `latest` | Show latest articles from all feeds | Optional limit (--N) | `rss latest --20` |\n| `top` or `best` | Show top articles from all feeds | Optional limit (--N) | `rss top --15` |\n| `list` | List all available feeds | None | `rss list` |\n| `--[feed-id]` | Show articles from a specific feed | Optional limit (--N) | `rss --hackernews --10` |\n| `[category]` | Show articles from a specific category | Optional limit (--N) | `rss \"Tech News\" --20` |\n| `set-feeds-path --[path]` | Set path to OPML/JSON file | Path to file | `rss set-feeds-path --/path/to/feeds.opml` |\n\n## Example Usage\n\nHere are various examples of how to use the RSS Aggregator with Claude:\n\n### Direct Commands:\n\n```\nrss latest\nrss top --20\nrss list\nrss \"Tech News\"\nrss --hackernews\nrss --techcrunch --15\n```\n\n### Natural Language Queries:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate commands:\n\n- \"What are the latest news on Hacker News?\"\n- \"Show me the top tech articles today\"\n- \"Fetch the latest articles from my programming feeds\"\n- \"List all my RSS feeds\"\n\n## Extended Usage Examples\n\n### Daily News Briefing\n\nGet your news briefing from all your sources:\n\n```\nrss latest --25\n```\n\nThis will fetch the 25 most recent articles across all your feeds, giving you a quick overview of the latest news.\n\n### Exploring Top Content\n\nFind the most important or popular articles:\n\n```\nrss top --20\n```\n\n### Category-Based Reading\n\nFocus on specific content categories:\n\n```\nrss \"Tech News\" --30\nrss \"Politics\" --15\nrss \"Science\" --10\n```\n\n### Source-Specific Updates\n\nRead updates from specific sources you follow:\n\n```\nrss --hackernews --20\nrss --nytimes\nrss --techcrunch --15\n```\n\n### Discover Your Available Feeds\n\nFind out what feeds you have configured:\n\n```\nrss list\n```\n\n### Combining Multiple Requests\n\nYou can make multiple sequential requests to build a comprehensive view:\n\n```\nrss \"Tech News\" --10\nrss \"Finance\" --10\nrss top --5\n```\n\n### Practical Workflows\n\n1. **Morning Routine**:\n   ```\n   rss top --10\n   rss \"News\" --5\n   ```\n\n2. **Industry Research**:\n   ```\n   rss \"Industry News\" --15\n   rss --bloomberg --5\n   ```\n\n3. **Tech Updates**:\n   ```\n   rss --hackernews --10\n   rss --techcrunch --5\n   ```\n\n### Working with Claude\n\nYou can ask Claude to analyze or summarize the articles:\n\n1. After running: `rss latest --10`\n   Ask: \"Can you summarize these articles?\"\n\n2. After running: `rss \"Tech News\" --15`\n   Ask: \"What are the key trends in these tech articles?\"\n\n3. After running: `rss --nytimes --washingtonpost --10`\n   Ask: \"Compare how these sources cover current events\"\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP RSS Aggregator: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n### Tools not appearing in Claude\nIf the RSS Aggregator tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n\n### Feeds not loading\nIf your feeds aren't loading properly:\n- Make sure your OPML/JSON file is correctly formatted\n- Check if the `feedsPath` in your configuration is correct\n- Try running the server manually with a known good feeds file\n\n## Contributing\n\nContributions to improve the RSS Aggregator are welcome! Here are some ways you can contribute:\n\n1. Add support for more feed formats\n2. Improve feed parsing and error handling\n3. Add more visualization options for articles\n4. Improve categorization and filtering capabilities\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-rss-aggregator/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "feeds",
        "rss",
        "aggregator",
        "rss aggregator",
        "rss feeds",
        "mcp rss"
      ],
      "category": "search--data-extraction"
    },
    "joelio--stocky": {
      "owner": "joelio",
      "name": "stocky",
      "url": "https://github.com/joelio/stocky",
      "imageUrl": "",
      "description": "An MCP server for searching and downloading royalty-free stock photography from Pexels and Unsplash. Features multi-provider search, rich metadata, pagination support, and async performance for AI assistants to find and access high-quality images.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-23T11:42:37Z",
      "readme_content": "# <div align=\"center\"><br/>Stocky<br/>*Find beautiful royalty-free stock images* 📸</div>\n\n<div align=\"center\">\n\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://github.com/modelcontextprotocol)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n</div>\n\n## ✨ Features\n\n- 🔍 **Multi-Provider Search** - Search across Pexels and Unsplash simultaneously\n- 📊 **Rich Metadata** - Get comprehensive image details including dimensions, photographer info, and licensing\n- 📄 **Pagination Support** - Browse through large result sets with ease\n- 🛡️ **Graceful Error Handling** - Robust error handling for API failures\n- ⚡ **Async Performance** - Lightning-fast concurrent API calls\n- 🎯 **Provider Flexibility** - Search specific providers or all at once\n\n\n\n**Beautiful stock photography at your fingertips**  \nExample image used for demonstration purposes\n\n\n*Stunning landscapes available through multiple providers*\n\nPhoto by [Simon Berger](https://unsplash.com/@simon_berger) on [Unsplash](https://unsplash.com/photos/twukN12EN7c)\n\n## 🚀 Quick Start\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/stocky-mcp.git\ncd stocky-mcp\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### API Key Setup\n\nYou'll need free API keys from each provider:\n\n1. **Pexels** 📷 - Get your key at [pexels.com/api](https://www.pexels.com/api/)\n2. **Unsplash** 🌅 - Sign up at [unsplash.com/developers](https://unsplash.com/developers)\n\n\n### API Key Configuration\n\nYou'll need to configure your API keys when setting up the MCP server. These keys are used to authenticate with the stock image providers.\n\n### Running as an MCP Server\n\nStocky is designed to be run as an MCP (Model Context Protocol) server, not as a standalone application. It should be configured in your MCP client configuration.\n\n## 🔧 MCP Client Configuration\n\nAdd Stocky to your MCP client configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"stocky\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/stocky_mcp.py\"],\n      \"env\": {\n        \"PEXELS_API_KEY\": \"your_pexels_key\",\n        \"UNSPLASH_ACCESS_KEY\": \"your_unsplash_key\",\n\n      }\n    }\n  }\n}\n```\n\n## 📖 Usage Examples\n\n<div align=\"center\">\n\n<p><em>Find the perfect image for your project</em></p>\n</div>\n\n### Searching for Images\n\nSearch across all providers:\n```python\nresults = await search_stock_images(\"sunset beach\")\n```\n\nSearch specific providers:\n```python\nresults = await search_stock_images(\n    query=\"mountain landscape\",\n    providers=[\"pexels\", \"unsplash\"],\n    per_page=30,\n    page=1\n)\n```\n\n### Getting Image Details\n\n```python\ndetails = await get_image_details(\"unsplash_abc123xyz\")\n```\n\n### Downloading Images\n\n```python\n# Download and save to disk\nresult = await download_image(\n    image_id=\"pexels_123456\", \n    size=\"medium\", \n    output_path=\"/path/to/save.jpg\"\n)\n\n# Get base64-encoded image data\nresult = await download_image(\n    image_id=\"unsplash_abc123\", \n    size=\"original\"\n)\n```\n\n## 🛠️ Tools Documentation\n\n### `search_stock_images`\n\nSearch for royalty-free stock images across multiple providers.\n\n**Parameters:**\n- `query` (str, required) - Search terms for finding images\n- `providers` (list, optional) - List of providers to search: `[\"pexels\", \"unsplash\"]`\n- `per_page` (int, optional) - Results per page, max 50 (default: 20)\n- `page` (int, optional) - Page number for pagination (default: 1)\n- `sort_by` (str, optional) - Sort results by \"relevance\" or \"newest\"\n\n**Returns:** List of image results with metadata\n\n### `get_image_details`\n\nGet detailed information about a specific image.\n\n**Parameters:**\n- `image_id` (str, required) - Image ID in format `provider_id` (e.g., `pexels_123456`)\n\n**Returns:** Detailed image information including full metadata\n\n### `download_image`\n\nDownload an image to local storage or get base64 encoded data.\n\n**Parameters:**\n- `image_id` (str, required) - Image ID in format `provider_id` (e.g., `pexels_123456`)\n- `size` (str, optional) - Image size variant to download (default: \"original\")\n  - Options: thumbnail, small, medium, large, original\n- `output_path` (str, optional) - Path to save the image locally\n  - If not provided, returns base64 encoded image data\n\n**Returns:** Dictionary with download information or error\n\n## 📄 License Information\n\n<div align=\"center\">\n\n<p><em>Royalty-free images for your creative projects</em></p>\n</div>\n\nAll images returned by Stocky are free to use:\n\n- **Pexels** ✅ - Free for commercial and personal use, no attribution required\n- **Unsplash** ✅ - Free under the Unsplash License\n\n\nAlways check the specific license for each image before use in production.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 🙏 Acknowledgments\n\n- Thanks to [Pexels](https://www.pexels.com) and [Unsplash](https://unsplash.com) for providing free APIs\n- Built with the [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Created with ❤️ for the developer community\n\n## 🐛 Troubleshooting\n\n### Common Issues\n\n**\"API key not found\" error**\n- Ensure your `.env` file exists and contains valid API keys\n- Check that environment variables are properly loaded\n- Verify API key names match exactly (case-sensitive)\n\n**No results returned**\n- Try different search terms\n- Check your internet connection\n- Verify API keys are active and have not exceeded rate limits\n\n**Installation issues**\n- Ensure Python 3.8+ is installed\n- Try creating a virtual environment: `python -m venv venv`\n- Update pip: `pip install --upgrade pip`\n\n### Rate Limiting\n\nEach provider has different rate limits:\n- **Pexels**: 200 requests per hour\n- **Unsplash**: 50 requests per hour (demo), 5000 per hour (production)\n\n\n---\n\n<div align=\"center\">\nMade with 💜 by the Stocky Team\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pexels",
        "searching",
        "search",
        "stocky mcp",
        "stock photography",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "just-every--mcp-read-website-fast": {
      "owner": "just-every",
      "name": "mcp-read-website-fast",
      "url": "https://github.com/just-every/mcp-read-website-fast",
      "imageUrl": "",
      "description": "Fast, token-efficient web content extraction for AI agents - converts websites to clean Markdown while preserving links. Features Mozilla Readability, smart caching, polite crawling with robots.txt support, and concurrent fetching.",
      "stars": 93,
      "forks": 14,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T06:41:09Z",
      "readme_content": "# @just-every/mcp-read-website-fast\n\nFast, token-efficient web content extraction for AI agents - converts websites to clean Markdown.\n\n[![npm version](https://badge.fury.io/js/@just-every%2Fmcp-read-website-fast.svg)](https://www.npmjs.com/package/@just-every/mcp-read-website-fast)\n[![GitHub Actions](https://github.com/just-every/mcp-read-website-fast/workflows/Release/badge.svg)](https://github.com/just-every/mcp-read-website-fast/actions)\n\n<a href=\"https://glama.ai/mcp/servers/@just-every/mcp-read-website-fast\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@just-every/mcp-read-website-fast/badge\" alt=\"read-website-fast MCP server\" />\n</a>\n\n## Overview\n\nExisting MCP web crawlers are slow and consume large quantities of tokens. This pauses the development process and provides incomplete results as LLMs need to parse whole web pages.\n\nThis MCP package fetches web pages locally, strips noise, and converts content to clean Markdown while preserving links. Designed for Claude Code, IDEs and LLM pipelines with minimal token footprint. Crawl sites locally with minimal dependencies.\n\n**Note:** This package now uses [@just-every/crawl](https://www.npmjs.com/package/@just-every/crawl) for its core crawling and markdown conversion functionality.\n\n## Features\n\n- **Fast startup** using official MCP SDK with lazy loading for optimal performance\n- **Content extraction** using Mozilla Readability (same as Firefox Reader View)\n- **HTML to Markdown** conversion with Turndown + GFM support\n- **Smart caching** with SHA-256 hashed URLs\n- **Polite crawling** with robots.txt support and rate limiting\n- **Concurrent fetching** with configurable depth crawling\n- **Stream-first design** for low memory usage\n- **Link preservation** for knowledge graphs\n- **Optional chunking** for downstream processing\n\n## Installation\n\n### Claude Code\n\n```bash\nclaude mcp add read-website-fast -s user -- npx -y @just-every/mcp-read-website-fast\n```\n\n### VS Code\n\n```bash\ncode --add-mcp '{\"name\":\"read-website-fast\",\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-read-website-fast\"]}'\n```\n\n### Cursor\n\n```bash\ncursor://anysphere.cursor-deeplink/mcp/install?name=read-website-fast&config=eyJyZWFkLXdlYnNpdGUtZmFzdCI6eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBqdXN0LWV2ZXJ5L21jcC1yZWFkLXdlYnNpdGUtZmFzdCJdfX0=\n```\n\n### JetBrains IDEs\n\nSettings → Tools → AI Assistant → Model Context Protocol (MCP) → Add\n\nChoose “As JSON” and paste:\n\n```json\n{\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-read-website-fast\"]}\n```\n\nOr, in the chat window, type /add and fill in the same JSON—both paths land the server in a single step. ￼\n\n### Raw JSON (works in any MCP client)\n\n```json\n{\n  \"mcpServers\": {\n    \"read-website-fast\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@just-every/mcp-read-website-fast\"]\n    }\n  }\n}\n```\n\nDrop this into your client’s mcp.json (e.g. .vscode/mcp.json, ~/.cursor/mcp.json, or .mcp.json for Claude).\n\n\n\n## Features\n\n- **Fast startup** using official MCP SDK with lazy loading for optimal performance\n- **Content extraction** using Mozilla Readability (same as Firefox Reader View)\n- **HTML to Markdown** conversion with Turndown + GFM support\n- **Smart caching** with SHA-256 hashed URLs\n- **Polite crawling** with robots.txt support and rate limiting\n- **Concurrent fetching** with configurable depth crawling\n- **Stream-first design** for low memory usage\n- **Link preservation** for knowledge graphs\n- **Optional chunking** for downstream processing\n\n### Available Tools\n\n- `read_website` - Fetches a webpage and converts it to clean markdown\n  - Parameters:\n    - `url` (required): The HTTP/HTTPS URL to fetch\n    - `pages` (optional): Maximum number of pages to crawl (default: 1, max: 100)\n\n### Available Resources\n\n- `read-website-fast://status` - Get cache statistics\n- `read-website-fast://clear-cache` - Clear the cache directory\n\n## Development Usage\n\n### Install\n\n```bash\nnpm install\nnpm run build\n```\n\n### Single page fetch\n```bash\nnpm run dev fetch https://example.com/article\n```\n\n### Crawl with depth\n```bash\nnpm run dev fetch https://example.com --depth 2 --concurrency 5\n```\n\n### Output formats\n```bash\n# Markdown only (default)\nnpm run dev fetch https://example.com\n\n# JSON output with metadata\nnpm run dev fetch https://example.com --output json\n\n# Both URL and markdown\nnpm run dev fetch https://example.com --output both\n```\n\n### CLI Options\n\n- `-p, --pages <number>` - Maximum number of pages to crawl (default: 1)\n- `-c, --concurrency <number>` - Max concurrent requests (default: 3)\n- `--no-robots` - Ignore robots.txt\n- `--all-origins` - Allow cross-origin crawling\n- `-u, --user-agent <string>` - Custom user agent\n- `--cache-dir <path>` - Cache directory (default: .cache)\n- `-t, --timeout <ms>` - Request timeout in milliseconds (default: 30000)\n- `-o, --output <format>` - Output format: json, markdown, or both (default: markdown)\n\n### Clear cache\n```bash\nnpm run dev clear-cache\n```\n\n## Auto-Restart Feature\n\nThe MCP server includes automatic restart capability by default for improved reliability:\n\n- Automatically restarts the server if it crashes\n- Handles unhandled exceptions and promise rejections\n- Implements exponential backoff (max 10 attempts in 1 minute)\n- Logs all restart attempts for monitoring\n- Gracefully handles shutdown signals (SIGINT, SIGTERM)\n\nFor development/debugging without auto-restart:\n```bash\n# Run directly without restart wrapper\nnpm run serve:dev\n```\n\n## Architecture\n\n```\nmcp/\n├── src/\n│   ├── crawler/        # URL fetching, queue management, robots.txt\n│   ├── parser/         # DOM parsing, Readability, Turndown conversion\n│   ├── cache/          # Disk-based caching with SHA-256 keys\n│   ├── utils/          # Logger, chunker utilities\n│   ├── index.ts        # CLI entry point\n│   ├── serve.ts        # MCP server entry point\n│   └── serve-restart.ts # Auto-restart wrapper\n```\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev fetch https://example.com\n\n# Build for production\nnpm run build\n\n# Run tests\nnpm test\n\n# Type checking\nnpm run typecheck\n\n# Linting\nnpm run lint\n```\n\n## Contributing\n\nContributions are welcome! Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Add tests for new functionality\n4. Submit a pull request\n\n## Troubleshooting\n\n### Cache Issues\n```bash\nnpm run dev clear-cache\n```\n\n### Timeout Errors\n- Increase timeout with `-t` flag\n- Check network connectivity\n- Verify URL is accessible\n\n### Content Not Extracted\n- Some sites block automated access\n- Try custom user agent with `-u` flag\n- Check if site requires JavaScript (not supported)\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mozilla",
        "web",
        "search",
        "efficient web",
        "content extraction",
        "web content"
      ],
      "category": "search--data-extraction"
    },
    "kimdonghwi94--Web-Analyzer-MCP": {
      "owner": "kimdonghwi94",
      "name": "Web-Analyzer-MCP",
      "url": "https://github.com/kimdonghwi94/web-analyzer-mcp",
      "imageUrl": "",
      "description": "Extracts clean web content for RAG and provides Q&A about web pages.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T08:50:54Z",
      "readme_content": "# 🔍 Web Analyzer MCP\n\n<a href=\"https://glama.ai/mcp/servers/@kimdonghwi94/web-analyzer-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kimdonghwi94/web-analyzer-mcp/badge\" alt=\"WebAnalyzer MCP server\" />\n</a>\n\nA powerful MCP (Model Context Protocol) server for intelligent web content analysis and summarization. Built with FastMCP, this server provides smart web scraping, content extraction, and AI-powered question-answering capabilities.\n\n## ✨ Features\n\n### 🎯 Core Tools\n\n1. **`url_to_markdown`** - Extract and summarize key web page content\n   - Analyzes content importance using custom algorithms\n   - Removes ads, navigation, and irrelevant content\n   - Keeps only essential information (tables, images, key text)\n   - Outputs structured markdown optimized for analysis\n\n2. **`web_content_qna`** - AI-powered Q&A about web content\n   - Extracts relevant content sections from web pages\n   - Uses intelligent chunking and relevance matching\n   - Answers questions using OpenAI GPT models\n\n### 🚀 Key Features\n\n- **Smart Content Ranking**: Algorithm-based content importance scoring\n- **Essential Content Only**: Removes clutter, keeps what matters\n- **Multi-IDE Support**: Works with Claude Desktop, Cursor, VS Code, PyCharm\n- **Flexible Models**: Choose from GPT-3.5, GPT-4, GPT-4 Turbo, or GPT-5\n\n## 📦 Installation\n\n### Prerequisites\n- [uv](https://docs.astral.sh/uv/getting-started/installation/) (Python package manager)\n- Chrome/Chromium browser (for Selenium)\n- OpenAI API key (for Q&A functionality)\n\n### 🚀 Quick Start with uv (Recommended)\n\n```bash\n# Clone the repository\ngit clone https://github.com/kimdonghwi94/web-analyzer-mcp.git\ncd web-analyzer-mcp\n\n# Run directly with uv (auto-installs dependencies)\nuv run mcp-webanalyzer\n```\n\n### Installing via Smithery\n\nTo install web-analyzer-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kimdonghwi94/web-analyzer-mcp):\n\n```bash\nnpx -y @smithery/cli install @kimdonghwi94/web-analyzer-mcp --client claude\n```\n\n# IDE/Editor Integration\n\n<details>\n<summary><b>Install Claude Desktop</b></summary>\n\nAdd to your Claude Desktop_config.json file. See [Claude Desktop MCP documentation](https://modelcontextprotocol.io/quickstart/user) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install Claude Code (VS Code Extension)</b></summary>\n\nAdd the server using Claude Code CLI:\n\n```bash\nclaude mcp add web-analyzer -e OPENAI_API_KEY=your_api_key_here -e OPENAI_MODEL=gpt-4 -- uv --directory /path/to/web-analyzer-mcp run mcp-webanalyzer\n```\n</details>\n\n<details>\n<summary><b>Install Cursor IDE</b></summary>\n\nAdd to your Cursor settings (`File > Preferences > Settings > Extensions > MCP`):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Install JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/idea/ai-assistant.html) for more details.\n\n1. In JetBrains IDEs go to **Settings** → **Tools** → **AI Assistant** → **Model Context Protocol (MCP)**\n2. Click **+ Add**\n3. Click on **Command** in the top-left corner of the dialog and select the **As JSON** option from the list\n4. Add this configuration and click **OK**:\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n</details>\n\n## 🎛️ Tool Descriptions\n\n### `url_to_markdown`\nConverts web pages to clean markdown format with essential content extraction.\n\n**Parameters:**\n- `url` (string): The web page URL to analyze\n\n**Returns:** Clean markdown content with structured data preservation\n\n### `web_content_qna` \nAnswers questions about web page content using intelligent content analysis.\n\n**Parameters:**\n- `url` (string): The web page URL to analyze\n- `question` (string): Question about the page content\n\n**Returns:** AI-generated answer based on page content\n\n## 🏗️ Architecture\n\n### Content Extraction Pipeline\n\n1. **URL Validation** - Ensures proper URL format\n2. **HTML Fetching** - Uses Selenium for dynamic content\n3. **Content Parsing** - BeautifulSoup for HTML processing\n4. **Element Scoring** - Custom algorithm ranks content importance\n5. **Content Filtering** - Removes duplicates and low-value content\n6. **Markdown Conversion** - Structured output generation\n\n### Q&A Processing Pipeline\n\n1. **Content Chunking** - Intelligent text segmentation\n2. **Relevance Scoring** - Matches content to questions\n3. **Context Selection** - Picks most relevant chunks\n4. **Answer Generation** - OpenAI GPT integration\n\n## 🏗️ Project Structure\n\n```\nweb-analyzer-mcp/\n├── web_analyzer_mcp/          # Main Python package\n│   ├── __init__.py           # Package initialization\n│   ├── server.py             # FastMCP server with tools\n│   ├── web_extractor.py      # Web content extraction engine\n│   └── rag_processor.py      # RAG-based Q&A processor\n├── scripts/                   # Build and utility scripts\n│   └── build.js              # Node.js build script\n├── README.md                 # English documentation\n├── README.ko.md              # Korean documentation\n├── package.json              # npm configuration and scripts\n├── pyproject.toml            # Python package configuration\n├── .env.example              # Environment variables template\n└── dist-info.json            # Build information (generated)\n```\n\n## 🛠️ Development\n\n### Modern Development with uv\n\n```bash\n# Clone repository\ngit clone https://github.com/kimdonghwi94/web-analyzer-mcp.git\ncd web-analyzer-mcp\n\n# Development commands\nuv run mcp-webanalyzer     # Start development server\nuv run python -m pytest   # Run tests\nuv run ruff check .        # Lint code\nuv run ruff format .       # Format code\nuv sync                    # Sync dependencies\n\n# Install development dependencies\nuv add --dev pytest ruff mypy\n\n# Create production build\nnpm run build\n```\n\n### Alternative: Traditional Python Development\n\n```bash\n# Setup Python environment (if not using uv)\npip install -e .[dev]\n\n# Development commands\npython -m web_analyzer_mcp.server  # Start server\npython -m pytest tests/            # Run tests\npython -m ruff check .             # Lint code\npython -m ruff format .            # Format code\npython -m mypy web_analyzer_mcp/   # Type checking\n```\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## 📋 Roadmap\n\n- [ ] Support for more content types (PDFs, videos)\n- [ ] Multi-language content extraction\n- [ ] Custom extraction rules\n- [ ] Caching for frequently accessed content\n- [ ] Webhook support for real-time updates\n\n## ⚠️ Limitations\n\n- Requires Chrome/Chromium for JavaScript-heavy sites\n- OpenAI API key needed for Q&A functionality\n- Rate limited to prevent abuse\n- Some sites may block automated access\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙋‍♂️ Support\n\n- Create an issue for bug reports or feature requests\n- Contribute to discussions in the GitHub repository\n- Check the [documentation](https://github.com/kimdonghwi94/web-analyzer-mcp) for detailed guides\n\n## 🌟 Acknowledgments\n\n- Built with [FastMCP](https://github.com/jlowin/fastmcp) framework\n- Inspired by [HTMLRAG](https://github.com/plageon/HtmlRAG) techniques for web content processing\n- Thanks to the MCP community for feedback and contributions\n\n---\n\n**Made with ❤️ for the MCP community**",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "extraction",
        "web",
        "analyzer",
        "web analyzer",
        "clean web",
        "kimdonghwi94 web"
      ],
      "category": "search--data-extraction"
    },
    "kshern--mcp-tavily": {
      "owner": "kshern",
      "name": "mcp-tavily",
      "url": "https://github.com/kshern/mcp-tavily.git",
      "imageUrl": "",
      "description": "[leehanchung/bing-search-mcp](https://github.com/leehanchung/bing-search-mcp) 📇 ☁️ - Web search capabilities using Microsoft Bing Search API",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-20T19:46:19Z",
      "readme_content": "# MCP Tavily\n\n[![smithery badge](https://smithery.ai/badge/@kshern/mcp-tavily)](https://smithery.ai/server/@kshern/mcp-tavily)\n\n[中文文档](./readme.zh-CN.md)\n\nA Model Context Protocol (MCP) server implementation for Tavily API, providing advanced search and content extraction capabilities.\n\n## Features\n\n- **Multiple Search Tools**:\n  - `search`: Basic search functionality with customizable options\n  - `searchContext`: Context-aware search for better relevance\n  - `searchQNA`: Question and answer focused search\n- **Content Extraction**: Extract content from URLs with configurable options\n- **Rich Configuration Options**: Extensive options for search depth, filtering, and content inclusion\n\n\n### Usage with MCP\n\nAdd the Tavily MCP server to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mcptools/mcp-tavily\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n> Note: Make sure to replace `your-api-key` with your actual Tavily API key. You can also set it as an environment variable `TAVILY_API_KEY` before running the server.\n\n## API Reference\n\n### Search Tools\n\nThe server provides three search tools that can be called through MCP:\n\n#### 1. Basic Search\n```typescript\n// Tool name: search\n{\n  query: \"artificial intelligence\",\n  options: {\n    searchDepth: \"advanced\",\n    topic: \"news\",\n    maxResults: 10\n  }\n}\n```\n\n#### 2. Context Search\n```typescript\n// Tool name: searchContext\n{\n  query: \"latest developments in AI\",\n  options: {\n    topic: \"news\",\n    timeRange: \"week\"\n  }\n}\n```\n\n#### 3. Q&A Search\n```typescript\n// Tool name: searchQNA\n{\n  query: \"What is quantum computing?\",\n  options: {\n    includeAnswer: true,\n    maxResults: 5\n  }\n}\n```\n\n### Extract Tool\n\n```typescript\n// Tool name: extract\n{\n  urls: [\"https://example.com/article1\", \"https://example.com/article2\"],\n  options: {\n    extractDepth: \"advanced\",\n    includeImages: true\n  }\n}\n```\n\n### Search Options\n\nAll search tools share these options:\n\n```typescript\ninterface SearchOptions {\n  searchDepth?: \"basic\" | \"advanced\";    // Search depth level\n  topic?: \"general\" | \"news\" | \"finance\"; // Search topic category\n  days?: number;                         // Number of days to search\n  maxResults?: number;                   // Maximum number of results\n  includeImages?: boolean;               // Include images in results\n  includeImageDescriptions?: boolean;    // Include image descriptions\n  includeAnswer?: boolean;               // Include answer in results\n  includeRawContent?: boolean;           // Include raw content\n  includeDomains?: string[];            // List of domains to include\n  excludeDomains?: string[];            // List of domains to exclude\n  maxTokens?: number;                    // Maximum number of tokens\n  timeRange?: \"year\" | \"month\" | \"week\" | \"day\" | \"y\" | \"m\" | \"w\" | \"d\"; // Time range for search\n}\n```\n\n### Extract Options\n\n```typescript\ninterface ExtractOptions {\n  extractDepth?: \"basic\" | \"advanced\";   // Extraction depth level\n  includeImages?: boolean;               // Include images in results\n}\n```\n\n## Response Format\n\nAll tools return responses in the following format:\n\n```typescript\n{\n  content: Array<{\n    type: \"text\",\n    text: string\n  }>\n}\n```\n\nFor search results, each item includes:\n- Title\n- Content\n- URL\n\nFor extracted content, each item includes:\n- URL\n- Raw content\n- Failed URLs list (if any)\n\n## Error Handling\n\nAll tools include proper error handling and will throw descriptive error messages if something goes wrong.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Tavily API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kshern/mcp-tavily):\n\n```bash\nnpx -y @smithery/cli install @kshern/mcp-tavily --client claude\n```\n\n### Manual Installation\n```bash\nnpm install @mcptools/mcp-tavily\n```\n\nOr use it directly with npx:\n\n```bash\nnpx @mcptools/mcp-tavily\n```\n\n\n\n### Prerequisites\n\n- Node.js 16 or higher\n- npm or yarn\n- Tavily API key (get one from [Tavily](https://tavily.com))\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Set your Tavily API key:\n```bash\nexport TAVILY_API_KEY=your_api_key\n```\n\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Debugging with MCP Inspector\n\nFor development and debugging, we recommend using [MCP Inspector](https://github.com/modelcontextprotocol/inspector), a powerful development tool for MCP servers.\n\n\nThe Inspector provides a user interface for:\n- Testing tool calls\n- Viewing server responses\n- Debugging tool execution\n- Monitoring server state\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Support\n\nFor any questions or issues:\n- Tavily API: refer to the [Tavily documentation](https://docs.tavily.com/)\n- MCP integration: refer to the [MCP documentation](https://modelcontextprotocol.io//)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "search",
        "api",
        "bing search",
        "search api",
        "microsoft bing"
      ],
      "category": "search--data-extraction"
    },
    "lfnovo--content-core": {
      "owner": "lfnovo",
      "name": "content-core",
      "url": "https://github.com/lfnovo/content-core",
      "imageUrl": "",
      "description": "Extract content from URLs, documents, videos, and audio files using intelligent auto-engine selection. Supports web pages, PDFs, Word docs, YouTube transcripts, and more with structured JSON responses.",
      "stars": 66,
      "forks": 13,
      "license": "MIT License",
      "language": "Jupyter Notebook",
      "updated_at": "2025-10-03T23:55:22Z",
      "readme_content": "# Content Core\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI version](https://badge.fury.io/py/content-core.svg)](https://badge.fury.io/py/content-core)\n[![Downloads](https://pepy.tech/badge/content-core)](https://pepy.tech/project/content-core)\n[![Downloads](https://pepy.tech/badge/content-core/month)](https://pepy.tech/project/content-core)\n[![GitHub stars](https://img.shields.io/github/stars/lfnovo/content-core?style=social)](https://github.com/lfnovo/content-core)\n[![GitHub forks](https://img.shields.io/github/forks/lfnovo/content-core?style=social)](https://github.com/lfnovo/content-core)\n[![GitHub issues](https://img.shields.io/github/issues/lfnovo/content-core)](https://github.com/lfnovo/content-core/issues)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n**Content Core** is a powerful, AI-powered content extraction and processing platform that transforms any source into clean, structured content. Extract text from websites, transcribe videos, process documents, and generate AI summaries—all through a unified interface with multiple integration options.\n\n## 🚀 What You Can Do\n\n**Extract content from anywhere:**\n- 📄 **Documents** - PDF, Word, PowerPoint, Excel, Markdown, HTML, EPUB\n- 🎥 **Media** - Videos (MP4, AVI, MOV) with automatic transcription  \n- 🎵 **Audio** - MP3, WAV, M4A with speech-to-text conversion\n- 🌐 **Web** - Any URL with intelligent content extraction\n- 🖼️ **Images** - JPG, PNG, TIFF with OCR text recognition\n- 📦 **Archives** - ZIP, TAR, GZ with content analysis\n\n**Process with AI:**\n- ✨ **Clean & format** extracted content automatically\n- 📝 **Generate summaries** with customizable styles (bullet points, executive summary, etc.)\n- 🎯 **Context-aware processing** - explain to a child, technical summary, action items\n- 🔄 **Smart engine selection** - automatically chooses the best extraction method\n\n## 🛠️ Multiple Ways to Use\n\n### 🖥️ Command Line (Zero Install)\n```bash\n# Extract content from any source\nuvx --from \"content-core\" ccore https://example.com\nuvx --from \"content-core\" ccore document.pdf\n\n# Generate AI summaries  \nuvx --from \"content-core\" csum video.mp4 --context \"bullet points\"\n```\n\n### 🤖 Claude Desktop Integration\nOne-click setup with Model Context Protocol (MCP) - extract content directly in Claude conversations.\n\n### 🔍 Raycast Extension  \nSmart auto-detection commands:\n- **Extract Content** - Full interface with format options\n- **Summarize Content** - 9 summary styles available\n- **Quick Extract** - Instant clipboard extraction\n\n### 🖱️ macOS Right-Click Integration\nRight-click any file in Finder → Services → Extract or Summarize content instantly.\n\n### 🐍 Python Library\n```python\nimport content_core as cc\n\n# Extract from any source\nresult = await cc.extract(\"https://example.com/article\")\nsummary = await cc.summarize_content(result, context=\"explain to a child\")\n```\n\n## ⚡ Key Features\n\n*   **🎯 Intelligent Auto-Detection:** Automatically selects the best extraction method based on content type and available services\n*   **🔧 Smart Engine Selection:** \n    * **URLs:** Firecrawl → Jina → BeautifulSoup fallback chain\n    * **Documents:** Docling → Enhanced PyMuPDF → Simple extraction fallback  \n    * **Media:** OpenAI Whisper transcription\n    * **Images:** OCR with multiple engine support\n*   **📊 Enhanced PDF Processing:** Advanced PyMuPDF engine with quality flags, table detection, and optional OCR for mathematical formulas\n*   **🌍 Multiple Integrations:** CLI, Python library, MCP server, Raycast extension, macOS Services\n*   **⚡ Zero-Install Options:** Use `uvx` for instant access without installation\n*   **🧠 AI-Powered Processing:** LLM integration for content cleaning and summarization\n*   **🔄 Asynchronous:** Built with `asyncio` for efficient processing\n*   **🐍 Pure Python Implementation:** No system dependencies required - simplified installation across all platforms\n\n## Getting Started\n\n### Installation\n\nInstall Content Core using `pip` - **no system dependencies required!**\n\n```bash\n# Basic installation (PyMuPDF + BeautifulSoup/Jina extraction)\npip install content-core\n\n# With enhanced document processing (adds Docling)\npip install content-core[docling]\n\n# With MCP server support (now included by default)\npip install content-core\n\n# Full installation (with enhanced document processing)\npip install content-core[docling]\n```\n\n> **Note:** Unlike many content extraction tools, Content Core uses pure Python implementations and doesn't require system libraries like libmagic. This ensures consistent, hassle-free installation across Windows, macOS, and Linux.\n\nAlternatively, if you’re developing locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/lfnovo/content-core\ncd content-core\n\n# Install with uv\nuv sync\n```\n\n### Command-Line Interface\n\nContent Core provides three CLI commands for extracting, cleaning, and summarizing content: \nccore, cclean, and csum. These commands support input from text, URLs, files, or piped data (e.g., via cat file | command).\n\n**Zero-install usage with uvx:**\n```bash\n# Extract content\nuvx --from \"content-core\" ccore https://example.com\n\n# Clean content  \nuvx --from \"content-core\" cclean \"messy content\"\n\n# Summarize content\nuvx --from \"content-core\" csum \"long text\" --context \"bullet points\"\n```\n\n#### ccore - Extract Content\n\nExtracts content from text, URLs, or files, with optional formatting.\nUsage:\n```bash\nccore [-f|--format xml|json|text] [-d|--debug] [content]\n```\nOptions:\n- `-f`, `--format`: Output format (xml, json, or text). Default: text.\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content (text, URL, or file path). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Extract from a URL as text\nccore https://example.com\n\n# Extract from a file as JSON\nccore -f json document.pdf\n\n# Extract from piped text as XML\necho \"Sample text\" | ccore --format xml\n```\n\n#### cclean - Clean Content\nCleans content by removing unnecessary formatting, spaces, or artifacts. Accepts text, JSON, XML input, URLs, or file paths.\nUsage:\n\n```bash\ncclean [-d|--debug] [content]\n```\n\nOptions:\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content to clean (text, URL, file path, JSON, or XML). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Clean a text string\ncclean \"  messy   text   \"\n\n# Clean piped JSON\necho '{\"content\": \"  messy   text   \"}' | cclean\n\n# Clean content from a URL\ncclean https://example.com\n\n# Clean a file’s content\ncclean document.txt\n```\n\n### csum - Summarize Content\n\nSummarizes content with an optional context to guide the summary style. Accepts text, JSON, XML input, URLs, or file paths.\n\nUsage:\n\n```bash\ncsum [--context \"context text\"] [-d|--debug] [content]\n```\n\nOptions:\n- `--context`: Context for summarization (e.g., \"explain to a child\"). Default: none.\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content to summarize (text, URL, file path, JSON, or XML). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Summarize text\ncsum \"AI is transforming industries.\"\n\n# Summarize with context\ncsum --context \"in bullet points\" \"AI is transforming industries.\"\n\n# Summarize piped content\ncat article.txt | csum --context \"one sentence\"\n\n# Summarize content from URL\ncsum https://example.com\n\n# Summarize a file's content\ncsum document.txt\n```\n\n## Quick Start\n\nYou can quickly integrate `content-core` into your Python projects to extract, clean, and summarize content from various sources.\n\n```python\nimport content_core as cc\n\n# Extract content from a URL, file, or text\nresult = await cc.extract(\"https://example.com/article\")\n\n# Clean messy content\ncleaned_text = await cc.clean(\"...messy text with [brackets] and extra spaces...\")\n\n# Summarize content with optional context\nsummary = await cc.summarize_content(\"long article text\", context=\"explain to a child\")\n```\n\n## Documentation\n\nFor more information on how to use the Content Core library, including details on AI model configuration and customization, refer to our [Usage Documentation](docs/usage.md).\n\n## MCP Server Integration\n\nContent Core includes a Model Context Protocol (MCP) server that enables seamless integration with Claude Desktop and other MCP-compatible applications. The MCP server exposes Content Core's powerful extraction capabilities through a standardized protocol.\n\n<a href=\"https://glama.ai/mcp/servers/@lfnovo/content-core\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@lfnovo/content-core/badge\" />\n</a>\n\n### Quick Setup with Claude Desktop\n\n```bash\n# Install Content Core (MCP server included)\npip install content-core\n\n# Or use directly with uvx (no installation required)\nuvx --from \"content-core\" content-core-mcp\n```\n\nAdd to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"content-core\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"content-core\",\n        \"content-core-mcp\"\n      ]\n    }\n  }\n}\n```\n\nFor detailed setup instructions, configuration options, and usage examples, see our [MCP Documentation](docs/mcp.md).\n\n## Enhanced PDF Processing\n\nContent Core features an optimized PyMuPDF extraction engine with significant improvements for scientific documents and complex PDFs.\n\n### Key Improvements\n\n- **🔬 Mathematical Formula Extraction**: Enhanced quality flags eliminate `<!-- formula-not-decoded -->` placeholders\n- **📊 Automatic Table Detection**: Tables converted to markdown format for LLM consumption\n- **🔧 Quality Text Rendering**: Better ligature, whitespace, and image-text integration\n- **⚡ Optional OCR Enhancement**: Selective OCR for formula-heavy pages (requires Tesseract)\n\n### Configuration for Scientific Documents\n\nFor documents with heavy mathematical content, enable OCR enhancement:\n\n```yaml\n# In cc_config.yaml\nextraction:\n  pymupdf:\n    enable_formula_ocr: true      # Enable OCR for formula-heavy pages\n    formula_threshold: 3          # Min formulas per page to trigger OCR\n    ocr_fallback: true           # Graceful fallback if OCR fails\n```\n\n```python\n# Runtime configuration\nfrom content_core.config import set_pymupdf_ocr_enabled\nset_pymupdf_ocr_enabled(True)\n```\n\n### Requirements for OCR Enhancement\n\n```bash\n# Install Tesseract OCR (optional, for formula enhancement)\n# macOS\nbrew install tesseract\n\n# Ubuntu/Debian\nsudo apt-get install tesseract-ocr\n```\n\n**Note**: OCR is optional - you get improved PDF extraction automatically without any additional setup.\n\n## macOS Services Integration\n\nContent Core provides powerful right-click integration with macOS Finder, allowing you to extract and summarize content from any file without installation. Choose between clipboard or TextEdit output for maximum flexibility.\n\n### Available Services\n\nCreate **4 convenient services** for different workflows:\n\n- **Extract Content → Clipboard** - Quick copy for immediate pasting\n- **Extract Content → TextEdit** - Review before using  \n- **Summarize Content → Clipboard** - Quick summary copying\n- **Summarize Content → TextEdit** - Formatted summary with headers\n\n### Quick Setup\n\n1. **Install uv** (if not already installed):\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Create services manually** using Automator (5 minutes setup)\n\n### Usage\n\n**Right-click any supported file** in Finder → **Services** → Choose your option:\n\n- **PDFs, Word docs** - Instant text extraction\n- **Videos, audio files** - Automatic transcription  \n- **Images** - OCR text recognition\n- **Web content** - Clean text extraction\n- **Multiple files** - Batch processing support\n\n### Features\n\n- **Zero-install processing**: Uses `uvx` for isolated execution\n- **Multiple output options**: Clipboard or TextEdit display\n- **System notifications**: Visual feedback on completion\n- **Wide format support**: 20+ file types supported\n- **Batch processing**: Handle multiple files at once\n- **Keyboard shortcuts**: Assignable hotkeys for power users\n\nFor complete setup instructions with copy-paste scripts, see [macOS Services Documentation](docs/macos.md).\n\n## Raycast Extension\n\nContent Core provides a powerful Raycast extension with smart auto-detection that handles both URLs and file paths seamlessly. Extract and summarize content directly from your Raycast interface without switching applications.\n\n### Quick Setup\n\n**From Raycast Store** (coming soon):\n1. Open Raycast and search for \"Content Core\"\n2. Install the extension by `luis_novo`\n3. Configure API keys in preferences\n\n**Manual Installation**:\n1. Download the extension from the repository\n2. Open Raycast → \"Import Extension\"\n3. Select the `raycast-content-core` folder\n\n### Commands\n\n**🔍 Extract Content** - Smart URL/file detection with full interface\n- Auto-detects URLs vs file paths in real-time\n- Multiple output formats (Text, JSON, XML)\n- Drag & drop support for files\n- Rich results view with metadata\n\n**📝 Summarize Content** - AI-powered summaries with customizable styles  \n- 9 different summary styles (bullet points, executive summary, etc.)\n- Auto-detects source type with visual feedback\n- One-click snippet creation and quicklinks\n\n**⚡ Quick Extract** - Instant extraction to clipboard\n- Type → Tab → Paste source → Enter\n- No UI, works directly from command bar\n- Perfect for quick workflows\n\n### Features\n\n- **Smart Auto-Detection**: Instantly recognizes URLs vs file paths\n- **Zero Installation**: Uses `uvx` for Content Core execution\n- **Rich Integration**: Keyboard shortcuts, clipboard actions, Raycast snippets\n- **All File Types**: Documents, videos, audio, images, archives\n- **Visual Feedback**: Real-time type detection with icons\n\nFor detailed setup, configuration, and usage examples, see [Raycast Extension Documentation](docs/raycast.md).\n\n## Using with Langchain\n\nFor users integrating with the [Langchain](https://python.langchain.com/) framework, `content-core` exposes a set of compatible tools. These tools, located in the `src/content_core/tools` directory, allow you to leverage `content-core` extraction, cleaning, and summarization capabilities directly within your Langchain agents and chains.\n\nYou can import and use these tools like any other Langchain tool. For example:\n\n```python\nfrom content_core.tools import extract_content_tool, cleanup_content_tool, summarize_content_tool\nfrom langchain.agents import initialize_agent, AgentType\n\ntools = [extract_content_tool, cleanup_content_tool, summarize_content_tool]\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(\"Extract the content from https://example.com and then summarize it.\") \n```\n\nRefer to the source code in `src/content_core/tools` for specific tool implementations and usage details.\n\n## Basic Usage\n\nThe core functionality revolves around the extract_content function.\n\n```python\nimport asyncio\nfrom content_core.extraction import extract_content\n\nasync def main():\n    # Extract from raw text\n    text_data = await extract_content({\"content\": \"This is my sample text content.\"})\n    print(text_data)\n\n    # Extract from a URL (uses 'auto' engine by default)\n    url_data = await extract_content({\"url\": \"https://www.example.com\"})\n    print(url_data)\n\n    # Extract from a local video file (gets transcript, engine='auto' by default)\n    video_data = await extract_content({\"file_path\": \"path/to/your/video.mp4\"})\n    print(video_data)\n\n    # Extract from a local markdown file (engine='auto' by default)\n    md_data = await extract_content({\"file_path\": \"path/to/your/document.md\"})\n    print(md_data)\n\n    # Per-execution override with Docling for documents\n    doc_data = await extract_content({\n        \"file_path\": \"path/to/your/document.pdf\",\n        \"document_engine\": \"docling\",\n        \"output_format\": \"html\"\n    })\n    \n    # Per-execution override with Firecrawl for URLs\n    url_data = await extract_content({\n        \"url\": \"https://www.example.com\",\n        \"url_engine\": \"firecrawl\"\n    })\n    print(doc_data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n(See `src/content_core/notebooks/run.ipynb` for more detailed examples.)\n\n## Docling Integration\n\nContent Core supports an optional Docling-based extraction engine for rich document formats (PDF, DOCX, PPTX, XLSX, Markdown, AsciiDoc, HTML, CSV, Images).\n\n\n### Enabling Docling\n\nDocling is not the default engine when parsing documents. If you don't want to use it, you need to set engine to \"simple\". \n\n#### Via configuration file\n\nIn your `cc_config.yaml` or custom config, set:\n```yaml\nextraction:\n  document_engine: docling  # 'auto' (default), 'simple', or 'docling'\n  url_engine: auto          # 'auto' (default), 'simple', 'firecrawl', or 'jina'\n  docling:\n    output_format: markdown  # markdown | html | json\n```\n\n#### Programmatically in Python\n\n```python\nfrom content_core.config import set_document_engine, set_url_engine, set_docling_output_format\n\n# switch document engine to Docling\nset_document_engine(\"docling\")\n\n# switch URL engine to Firecrawl\nset_url_engine(\"firecrawl\")\n\n# choose output format: 'markdown', 'html', or 'json'\nset_docling_output_format(\"html\")\n\n# now use ccore.extract or ccore.ccore\nresult = await cc.extract(\"document.pdf\")\n```\n\n## Configuration\n\nConfiguration settings (like API keys for external services, logging levels) can be managed through environment variables or `.env` files, loaded automatically via `python-dotenv`.\n\nExample `.env`:\n\n```plaintext\nOPENAI_API_KEY=your-key-here\nGOOGLE_API_KEY=your-key-here\n\n# Engine Selection (optional)\nCCORE_DOCUMENT_ENGINE=auto  # auto, simple, docling\nCCORE_URL_ENGINE=auto       # auto, simple, firecrawl, jina\n```\n\n### Engine Selection via Environment Variables\n\nFor deployment scenarios like MCP servers or Raycast extensions, you can override the extraction engines using environment variables:\n\n- **`CCORE_DOCUMENT_ENGINE`**: Force document engine (`auto`, `simple`, `docling`)\n- **`CCORE_URL_ENGINE`**: Force URL engine (`auto`, `simple`, `firecrawl`, `jina`)\n\nThese variables take precedence over config file settings and provide explicit control for different deployment scenarios.\n\n### Custom Prompt Templates\n\nContent Core allows you to define custom prompt templates for content processing. By default, the library uses built-in prompts located in the `prompts` directory. However, you can create your own prompt templates and store them in a dedicated directory. To specify the location of your custom prompts, set the `PROMPT_PATH` environment variable in your `.env` file or system environment.\n\nExample `.env` with custom prompt path:\n\n```plaintext\nOPENAI_API_KEY=your-key-here\nGOOGLE_API_KEY=your-key-here\nPROMPT_PATH=/path/to/your/custom/prompts\n```\n\nWhen a prompt template is requested, Content Core will first look in the custom directory specified by `PROMPT_PATH` (if set and exists). If the template is not found there, it will fall back to the default built-in prompts. This allows you to override specific prompts while still using the default ones for others.\n\n## Development\n\nTo set up a development environment:\n\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd content-core\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate\nuv sync --group dev\n\n# Run tests\nmake test\n\n# Lint code\nmake lint\n\n# See all commands\nmake help\n```\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE). See the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for more details on how to get started.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lfnovo",
        "content",
        "search",
        "lfnovo content",
        "extraction lfnovo",
        "core extract"
      ],
      "category": "search--data-extraction"
    },
    "luminati-io--brightdata-mcp": {
      "owner": "luminati-io",
      "name": "brightdata-mcp",
      "url": "https://github.com/luminati-io/brightdata-mcp",
      "imageUrl": "",
      "description": "Discover, extract, and interact with the web - one interface powering automated access across the public internet.",
      "stars": 1389,
      "forks": 187,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T00:42:56Z",
      "readme_content": "<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/mcp-server\">\n    <img src=\"https://github.com/user-attachments/assets/c21b3f7b-7ff1-40c3-b3d8-66706913d62f\" alt=\"Bright Data Logo\">\n  </a>\n\n  <h1>The Web MCP</h1>\n  \n  <p>\n    <strong>🌐 Give your AI real-time web superpowers</strong><br/>\n    <i>Seamlessly connect LLMs to the live web without getting blocked</i>\n  </p>\n\n  <p>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/v/@brightdata/mcp?style=for-the-badge&color=blue\" alt=\"npm version\"/>\n    </a>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/dw/@brightdata/mcp?style=for-the-badge&color=green\" alt=\"npm downloads\"/>\n    </a>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/LICENSE\">\n      <img src=\"https://img.shields.io/badge/license-MIT-purple?style=for-the-badge\" alt=\"License\"/>\n    </a>\n  </p>\n\n  <p>\n    <a href=\"#-quick-start\">Quick Start</a> •\n    <a href=\"#-features\">Features</a> •\n    <a href=\"#-pricing--modes\">Pricing</a> •\n    <a href=\"#-demos\">Demos</a> •\n    <a href=\"#-documentation\">Docs</a> •\n    <a href=\"#-support\">Support</a>\n  </p>\n\n  <div>\n    <h3>🎉 <strong>Free Tier Available!</strong> 🎉</h3>\n    <p><strong>5,000 requests/month FREE</strong> <br/>\n    <sub>Perfect for prototyping and everyday AI workflows</sub></p>\n  </div>\n</div>\n\n---\n\n## 🌟 Overview\n\n**The Web MCP** is your gateway to giving AI assistants true web capabilities. No more outdated responses, no more \"I can't access real-time information\" - just seamless, reliable web access that actually works.\n\nBuilt by [Bright Data](https://brightdata.com), the world's #1 web data platform, this MCP server ensures your AI never gets blocked, rate-limited, or served CAPTCHAs.\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">✅ <strong>Works with Any LLM</strong><br/><sub>Claude, GPT, Gemini, Llama</sub></td>\n      <td align=\"center\">🛡️ <strong>Never Gets Blocked</strong><br/><sub>Enterprise-grade unblocking</sub></td>\n      <td align=\"center\">🚀 <strong>5,000 Free Requests</strong><br/><sub>Monthly</sub></td>\n      <td align=\"center\">⚡ <strong>Zero Config</strong><br/><sub>Works out of the box</sub></td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🎯 Perfect For\n\n- 🔍 **Real-time Research** - Get current prices, news, and live data\n- 🛍️ **E-commerce Intelligence** - Monitor products, prices, and availability  \n- 📊 **Market Analysis** - Track competitors and industry trends\n- 🤖 **AI Agents** - Build agents that can actually browse the web\n- 📝 **Content Creation** - Access up-to-date information for writing\n- 🎓 **Academic Research** - Gather data from multiple sources efficiently\n\n---\n\n## ⚡ Quick Start\n\n\n<summary><b>📡 Use our hosted server - No installation needed!</b></summary>\n\nPerfect for users who want zero setup. Just add this URL to your MCP client:\n\n```\nhttps://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN_HERE\n```\n\n**Setup in Claude Desktop:**\n1. Go to: Settings → Connectors → Add custom connector\n2. Name: `Bright Data Web`\n3. URL: `https://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN`\n4. Click \"Add\" and you're done! ✨\n\n\n<summary><b>Run locally on your machine</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"<your-api-token-here>\"\n      }\n    }\n  }\n}\n```\n\n\n---\n\n## 🚀 Pricing & Modes\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th width=\"33%\">⚡ Rapid Mode (Free tier)</th>\n      <th width=\"33%\">💎 Pro Mode</th>\n    </tr>\n    <tr>\n      <td align=\"center\">\n        <h3>$0/month</h3>\n        <p><strong>5,000 requests</strong></p>\n        <hr/>\n        <p>✅ Web Search<br/>\n        ✅ Scraping with Web unlocker<br/>\n        ❌ Browser Automation<br/>\n        ❌ Web data tools</p>\n        <br/>\n        <code>Default Mode</code>\n      </td>\n      <td align=\"center\">\n        <h3>Pay-as-you-go</h3>\n        <p><strong>Every thing in rapid and 60+ Advanced Tools</strong></p>\n        <hr/>\n        <p>✅ Browser Control<br/>\n        ✅ Web Data APIs<br/>\n        <br/>\n        <br/>\n        <br/>\n        <code>PRO_MODE=true</code>\n      </td>\n    </tr>\n  </table>\n</div>\n\n> **💡 Note:** Pro mode is **not included** in the free tier and incurs additional charges based on usage.\n\n---\n\n## ✨ Features\n\n### 🔥 Core Capabilities\n\n<table>\n  <tr>\n    <td>🔍 <b>Smart Web Search</b><br/>Google-quality results optimized for AI</td>\n    <td>📄 <b>Clean Markdown</b><br/>AI-ready content extraction</td>\n  </tr>\n  <tr>\n    <td>🌍 <b>Global Access</b><br/>Bypass geo-restrictions automatically</td>\n    <td>🛡️ <b>Anti-Bot Protection</b><br/>Never get blocked or rate-limited</td>\n  </tr>\n  <tr>\n    <td>🤖 <b>Browser Automation</b><br/>Control real browsers remotely (Pro)</td>\n    <td>⚡ <b>Lightning Fast</b><br/>Optimized for minimal latency</td>\n  </tr>\n</table>\n\n### 🎯 Example Queries That Just Work\n\n```yaml\n✅ \"What's Tesla's current stock price?\"\n✅ \"Find the best-rated restaurants in Tokyo right now\"\n✅ \"Get today's weather forecast for New York\"\n✅ \"What movies are releasing this week?\"\n✅ \"What are the trending topics on Twitter today?\"\n```\n\n---\n\n## 🎬 Demos\n\n> **Note:** These videos show earlier versions. New demos coming soon! 🎥\n\n<details>\n<summary><b>View Demo Videos</b></summary>\n\n### Basic Web Search Demo\nhttps://github.com/user-attachments/assets/59f6ebba-801a-49ab-8278-1b2120912e33\n\n### Advanced Scraping Demo\nhttps://github.com/user-attachments/assets/61ab0bee-fdfa-4d50-b0de-5fab96b4b91d\n\n[📺 More tutorials on YouTube →](https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md)\n\n</details>\n\n---\n\n## 🔧 Available Tools\n\n### ⚡ Rapid Mode Tools (Default - Free)\n\n| Tool | Description | Use Case |\n|------|-------------|----------|\n| 🔍 `search_engine` | Web search with AI-optimized results | Research, fact-checking, current events |\n| 📄 `scrape_as_markdown` | Convert any webpage to clean markdown | Content extraction, documentation |\n\n### 💎 Pro Mode Tools (60+ Tools)\n\n<details>\n<summary><b>Click to see all Pro tools</b></summary>\n\n| Category | Tools | Description |\n|----------|-------|-------------|\n| **Browser Control** | `scraping_browser.*` | Full browser automation |\n| **Web Data APIs** | `web_data_*` | Structured data extraction |\n| **E-commerce** | Product scrapers | Amazon, eBay, Walmart data |\n| **Social Media** | Social scrapers | Twitter, LinkedIn, Instagram |\n| **Maps & Local** | Location tools | Google Maps, business data |\n\n[📚 View complete tool documentation →](https://github.com/brightdata-com/brightdata-mcp/blob/main/assets/Tools.md)\n\n</details>\n\n---\n\n## 🎮 Try It Now!\n\n### 🧪 Online Playground\nTry the Web MCP without any setup:\n\n<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/playground-chat\">\n    <img src=\"https://img.shields.io/badge/Try_on-Playground-00C7B7?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMyA3VjE3TDEyIDIyTDIxIDE3VjdMMTIgMloiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8L3N2Zz4=\" alt=\"Playground\"/>\n  </a>\n</div>\n\n---\n\n## 🔧 Configuration\n\n### Basic Setup\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Configuration\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\",\n        \"PRO_MODE\": \"true\",              // Enable all 60+ tools\n        \"RATE_LIMIT\": \"100/1h\",          // Custom rate limiting\n        \"WEB_UNLOCKER_ZONE\": \"custom\",   // Custom unlocker zone\n        \"BROWSER_ZONE\": \"custom_browser\" // Custom browser zone\n      }\n    }\n  }\n}\n```\n\n---\n\n## 📚 Documentation\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <img src=\"https://img.shields.io/badge/📖-API_Docs-blue?style=for-the-badge\" alt=\"API Docs\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/examples\">\n          <img src=\"https://img.shields.io/badge/💡-Examples-green?style=for-the-badge\" alt=\"Examples\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/CHANGELOG.md\">\n          <img src=\"https://img.shields.io/badge/📝-Changelog-orange?style=for-the-badge\" alt=\"Changelog\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://brightdata.com/blog/ai/web-scraping-with-mcp\">\n          <img src=\"https://img.shields.io/badge/📚-Tutorial-purple?style=for-the-badge\" alt=\"Tutorial\"/>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🚨 Common Issues & Solutions\n\n<details>\n<summary><b>🔧 Troubleshooting Guide</b></summary>\n\n### ❌ \"spawn npx ENOENT\" Error\n**Solution:** Install Node.js or use the full path to node:\n```json\n\"command\": \"/usr/local/bin/node\"  // macOS/Linux\n\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"  // Windows\n```\n\n### ⏱️ Timeouts on Complex Sites\n**Solution:** Increase timeout in your client settings to 180s\n\n### 🔑 Authentication Issues\n**Solution:** Ensure your API token is valid and has proper permissions\n\n### 📡 Remote Server Connection\n**Solution:** Check your internet connection and firewall settings\n\n[More troubleshooting →](https://github.com/brightdata-com/brightdata-mcp#troubleshooting)\n\n</details>\n\n---\n\n## 🤝 Contributing\n\nWe love contributions! Here's how you can help:\n\n- 🐛 [Report bugs](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 💡 [Suggest features](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 🔧 [Submit PRs](https://github.com/brightdata-com/brightdata-mcp/pulls)\n- ⭐ Star this repo!\n\nPlease follow [Bright Data's coding standards](https://brightdata.com/dna/js_code).\n\n---\n\n## 📞 Support\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/issues\">\n          <strong>🐛 GitHub Issues</strong><br/>\n          <sub>Report bugs & features</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <strong>📚 Documentation</strong><br/>\n          <sub>Complete guides</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"mailto:support@brightdata.com\">\n          <strong>✉️ Email</strong><br/>\n          <sub>support@brightdata.com</sub>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 📜 License\n\nMIT © [Bright Data Ltd.](https://brightdata.com)\n\n---\n\n<div align=\"center\">\n  <p>\n    <strong>Built with ❤️ by</strong><br/>\n    <a href=\"https://brightdata.com\">\n      <img src=\"https://idsai.net.technion.ac.il/files/2022/01/Logo-600.png\" alt=\"Bright Data\" height=\"30\"/>\n    </a>\n  </p>\n  <p>\n    <sub>The world's #1 web data platform</sub>\n  </p>\n  \n  <br/>\n  \n  <p>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp\">⭐ Star us on GitHub</a> • \n    <a href=\"https://brightdata.com/blog\">Read our Blog</a>\n  </p>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "brightdata",
        "io",
        "web",
        "io brightdata",
        "brightdata mcp",
        "data extraction"
      ],
      "category": "search--data-extraction"
    },
    "modelcontextprotocol--server-brave-search": {
      "owner": "modelcontextprotocol",
      "name": "server-brave-search",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search",
      "imageUrl": "",
      "description": "Web search capabilities using Brave's Search API",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "modelcontextprotocol",
        "brave",
        "brave search",
        "search api",
        "search capabilities"
      ],
      "category": "search--data-extraction"
    },
    "modelcontextprotocol--server-fetch": {
      "owner": "modelcontextprotocol",
      "name": "server-fetch",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/fetch",
      "imageUrl": "",
      "description": "Efficient web content fetching and processing for AI consumption",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fetching",
        "search",
        "ai",
        "content fetching",
        "search data",
        "fetching processing"
      ],
      "category": "search--data-extraction"
    },
    "nkapila6--mcp-local-rag": {
      "owner": "nkapila6",
      "name": "mcp-local-rag",
      "url": "https://github.com/nkapila6/mcp-local-rag",
      "imageUrl": "",
      "description": "\"primitive\" RAG-like web search model context protocol (MCP) server that runs locally. No APIs needed.",
      "stars": 83,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T12:41:08Z",
      "readme_content": "<a href='https://github.com/nkapila6/mcp-local-rag/'></a>\n\n<!-- omit from toc -->\n# mcp-local-rag\n\"primitive\" RAG-like web search model context protocol (MCP) server that runs locally. ✨ no APIs ✨\n\n```mermaid\n%%{init: {'theme': 'base'}}%%\nflowchart TD\n    A[User] -->|1.Submits LLM Query| B[Language Model]\n    B -->|2.Sends Query| C[mcp-local-rag Tool]\n    \n    subgraph mcp-local-rag Processing\n    C -->|Search DuckDuckGo| D[Fetch 10 search results]\n    D -->|Fetch Embeddings| E[Embeddings from Google's MediaPipe Text Embedder]\n    E -->|Compute Similarity| F[Rank Entries Against Query]\n    F -->|Select top k results| G[Context Extraction from URL]\n    end\n    \n    G -->|Returns Markdown from HTML content| B\n    B -->|3.Generated response with context| H[Final LLM Output]\n    H -->|5.Present result to user| A\n\n    classDef default stroke:#333,stroke-width:2px;\n    classDef process stroke:#333,stroke-width:2px;\n    classDef input stroke:#333,stroke-width:2px;\n    classDef output stroke:#333,stroke-width:2px;\n\n    class A input;\n    class B,C process;\n    class G output;\n```\n\n# Installation\nLocate your MCP config path [here](https://modelcontextprotocol.io/quickstart/user) or check your MCP client settings. \n\n### Run Directly via `uvx`\nThis is the easiest and quickest method. You need to install [uv](https://docs.astral.sh/uv/) for this to work. <br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\":{\n      \"command\": \"uvx\",\n        \"args\": [\n          \"--python=3.10\",\n          \"--from\",\n          \"git+https://github.com/nkapila6/mcp-local-rag\",\n          \"mcp-local-rag\"\n        ]\n      }\n  }\n}\n```\n\n### Using Docker (recommended)\nEnsure you have [Docker](https://www.docker.com) installed.<br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--init\",\n        \"-e\",\n        \"DOCKER_CONTAINER=true\",\n        \"ghcr.io/nkapila6/mcp-local-rag:latest\"\n      ]\n    }\n  }\n}\n```\n\n# Security audits\nMseeP does security audits on every MCP server, you can see the security audit of this MCP server by clicking [here](https://mseep.ai/app/nkapila6-mcp-local-rag).\n\n<a href='https://mseep.ai/app/nkapila6-mcp-local-rag'><img alt=\"nkapila6_mcp_local_rag_badge\" src='https://mseep.net/pr/nkapila6-mcp-local-rag-badge.png' width='auto' height='200'></a>\n\n# MCP Clients\nThe MCP server should work with any MCP client that supports tool calling. Has been tested on the below clients.\n\n- Claude Desktop\n- Cursor\n- Goose\n- Others? You try!\n\n# Examples on Claude Desktop\nWhen an LLM (like Claude) is asked a question requiring recent web information, it will trigger `mcp-local-rag`.\n\nWhen asked to fetch/lookup/search the web, the model prompts you to use MCP server for the chat.\n\nIn the example, have asked it about Google's latest Gemma models released yesterday. This is new info that Claude is not aware about.\n\n\n## Result\n`mcp-local-rag` performs a live web search, extracts context, and sends it back to the model—giving it fresh knowledge:\n\n\n\n# Contributing\nHave ideas or want to improve this project? Issues and pull requests are welcome!\n\n# License\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "apis",
        "nkapila6",
        "web search",
        "apis needed",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "nyxn-ai--NyxDocs": {
      "owner": "nyxn-ai",
      "name": "NyxDocs",
      "url": "https://github.com/nyxn-ai/NyxDocs",
      "imageUrl": "",
      "description": "Specialized MCP server for cryptocurrency project documentation management with multi-blockchain support (Ethereum, BSC, Polygon, Solana).",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-10T03:45:06Z",
      "readme_content": "# NyxDocs - Cryptocurrency Documentation MCP Server\n\n[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![MCP](https://img.shields.io/badge/MCP-compatible-green.svg)](https://modelcontextprotocol.io)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nNyxDocs is a specialized Model Context Protocol (MCP) server that provides comprehensive documentation management for cryptocurrency projects. Built with Python and inspired by Context7's architecture, it offers real-time access to crypto project documentation, blockchain information, and development resources.\n\n## 🚀 Features\n\n### Core Capabilities\n- **Multi-Blockchain Support**: Ethereum, BSC, Polygon, Solana, and more\n- **Real-time Documentation**: Automatically discovers and updates project docs\n- **Smart Search**: Find projects by name, category, or blockchain\n- **Content Extraction**: Supports GitHub, GitBook, Notion, and official websites\n- **Update Monitoring**: Tracks documentation changes automatically\n\n### MCP Tools\n- `search_crypto_projects`: Search cryptocurrency projects by various criteria\n- `get_project_info`: Detailed project information with blockchain context\n- `get_documentation`: Retrieve actual documentation content\n- `list_blockchains`: Available blockchain networks\n- `check_updates`: Recent documentation updates\n\n## 🏗️ Architecture\n\n```\n┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n│   Data Sources  │    │   NyxDocs Core   │    │   MCP Client    │\n│                 │    │                  │    │                 │\n│ • CoinGecko API │────│ • Project DB     │────│ • Claude        │\n│ • GitHub API    │    │ • Doc Scraper    │    │ • Cursor        │\n│ • GitBook       │    │ • Update Monitor │    │ • VS Code       │\n│ • Notion        │    │ • MCP Server     │    │ • Other Clients │\n└─────────────────┘    └──────────────────┘    └─────────────────┘\n```\n\n### Key Components\n\n1. **MCP Server Core**: FastMCP-based server handling protocol communication\n2. **Data Collectors**: Modules for gathering project information from various APIs\n3. **Documentation Scrapers**: Intelligent content extraction from different sources\n4. **Database Layer**: SQLite/PostgreSQL for storing projects and documentation\n5. **Update Monitors**: Background tasks for tracking documentation changes\n\n## 📦 Installation\n\n### Prerequisites\n- Python 3.11+\n- uv (recommended) or pip\n\n### Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/nyxn-ai/NyxDocs.git\ncd NyxDocs\n\n# Install with uv (recommended)\nuv sync\n\n# Or install with pip\npip install -e .\n\n# Set up environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# Initialize database\nuv run python -m nyxdocs.database.init\n\n# Start the server\nuv run python -m nyxdocs.server\n```\n\n### MCP Client Configuration\n\n#### Cursor\n```json\n{\n  \"mcpServers\": {\n    \"nyxdocs\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"-m\", \"nyxdocs.server\"]\n    }\n  }\n}\n```\n\n#### Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"nyxdocs\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"-m\", \"nyxdocs.server\"]\n    }\n  }\n}\n```\n\n## 🔧 Configuration\n\n### Environment Variables\n\n```env\n# API Keys\nCOINGECKO_API_KEY=your_coingecko_api_key\nGITHUB_TOKEN=your_github_token\n\n# Database\nDATABASE_URL=sqlite:///nyxdocs.db\n# Or for PostgreSQL: postgresql://user:pass@localhost/nyxdocs\n\n# Server Settings\nLOG_LEVEL=INFO\nUPDATE_INTERVAL=3600  # seconds\nMAX_CONCURRENT_SCRAPES=5\n```\n\n### Supported Data Sources\n\n- **CoinGecko**: Market data and project information\n- **GitHub**: Repository documentation and README files\n- **GitBook**: Hosted documentation platforms\n- **Notion**: Project documentation pages\n- **Official Websites**: Direct documentation scraping\n\n## 🛠️ Usage Examples\n\n### Search for DeFi Projects\n```python\n# In your MCP client\nsearch_crypto_projects(query=\"uniswap\", category=\"DeFi\", blockchain=\"ethereum\")\n```\n\n### Get Project Documentation\n```python\nget_documentation(project=\"uniswap\", format=\"markdown\")\n```\n\n### Monitor Updates\n```python\ncheck_updates(since=\"2024-01-01\", limit=10)\n```\n\n## 🧪 Development\n\n### Project Structure\n```\nNyxDocs/\n├── nyxdocs/\n│   ├── __init__.py\n│   ├── server.py              # Main MCP server\n│   ├── collectors/            # Data collection modules\n│   ├── scrapers/              # Documentation scrapers\n│   ├── database/              # Database models and operations\n│   ├── tools/                 # MCP tool implementations\n│   └── utils/                 # Utility functions\n├── tests/                     # Test suite\n├── docs/                      # Documentation\n├── pyproject.toml            # Project configuration\n└── README.md\n```\n\n### Running Tests\n```bash\nuv run pytest\n```\n\n### Code Quality\n```bash\nuv run ruff check\nuv run mypy nyxdocs\n```\n\n## 📚 Documentation\n\n- [API Reference](docs/api.md)\n- [Configuration Guide](docs/configuration.md)\n- [Development Setup](docs/development.md)\n- [Contributing Guidelines](CONTRIBUTING.md)\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- Inspired by [Context7](https://github.com/upstash/context7) by Upstash\n- Built with [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n- Cryptocurrency data provided by CoinGecko API\n\n---\n\n**NyxDocs** - Making cryptocurrency project documentation accessible and up-to-date for AI assistants.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nyxdocs",
        "nyxn",
        "ethereum",
        "extraction nyxn",
        "ai nyxdocs",
        "nyxn ai"
      ],
      "category": "search--data-extraction"
    },
    "r-huijts--opentk-mcp": {
      "owner": "r-huijts",
      "name": "opentk-mcp",
      "url": "https://github.com/r-huijts/opentk-mcp",
      "imageUrl": "",
      "description": "Access Dutch Parliament (Tweede Kamer) information including documents, debates, activities, and legislative cases through structured search capabilities (based on opentk project by Bert Hubert)",
      "stars": 15,
      "forks": 2,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-09-24T07:23:36Z",
      "readme_content": "# OpenTK Model Context Protocol Server\n\n> **Important Attribution**: This MCP server is built as a wrapper around the excellent [OpenTK project](https://berthub.eu/tkconv/) created by [Bert Hubert](https://berthub.eu/). The OpenTK project provides unprecedented access to Dutch parliamentary data through a user-friendly interface. Learn more about the project in Bert's article: [Welkom bij OpenTK](https://berthub.eu/articles/posts/welkom-bij-opentk/). All credit for the underlying data access and processing goes to Bert Hubert and his contributions to open government data.\n\nA bridge between large language models (LLMs) and Dutch parliamentary data through a standardized interface. This MCP server provides access to Dutch parliamentary documents, debates, and member information from the Tweede Kamer.\n\n<a href=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp/badge\" alt=\"OpenTK Model Context Protocol Server MCP server\" />\n</a>\n\n## Real-World Natural Language Interaction Examples\n\n## Example 1: Comparing Party Positions on AI Policies\nUser Query: \"When comparing the activities of opposition parties PvdA, GroenLinks, and Volt with government party BBB in the Dutch House of Representatives in the field of AI, what are actions they can undertake together in the short term that align with the positions and views they have demonstrated over the past year? Please use sources from OpenTK.\"\n\n## Example 2: Researching Parliamentary Discussions on Climate Policy\nUser Query: \"I'd like to analyze recent parliamentary debates on climate policy and emission reduction targets in the Netherlands. Can you help me identify key discussions and the main positions taken by different parties over the past six months?\"\n\n## Example 3: Information About a Specific MP's Voting Record\nUser Query: \"What is MP Pieter Omtzigt's voting record on healthcare reform legislation, and how does his position differ from other independent members? Has he introduced any motions on this topic?\"\n\n## Example 4: Finding Recent Housing Legislation Developments\nUser Query: \"What are the most significant parliamentary documents and debates about affordable housing legislation from the past year? I'm particularly interested in proposals addressing the rental market crisis.\"\n\n## Example 5: Finding MPs with Specific Committee Memberships\nUser Query: \"Which MPs currently serve on both the Finance Committee and the Economic Affairs Committee? What parties do they represent, and have they recently submitted any joint initiatives?\"\n\n## Example 6: Identifying Upcoming Parliamentary Activities on Digital Security\nUser Query: \"Are there any scheduled committee meetings or debates about cybersecurity and digital infrastructure planned for the next month? Which ministers will be participating and what specific topics will be addressed?\"\n\n## Project Concept\n\nThe OpenTK project is a Model Context Protocol (MCP) server that provides access to Dutch parliamentary data through a standardized interface. It serves as a bridge between large language models (LLMs) and the Dutch Parliament's information systems, allowing AI assistants to search, retrieve, and analyze parliamentary documents, debates, and member information.\n\nThe server uses the `@modelcontextprotocol/sdk` to implement the MCP specification, which enables structured communication between AI models and external data sources. By exposing parliamentary data through well-defined tools and endpoints, OpenTK makes it possible for AI assistants to:\n\n1. Search for parliamentary documents using complex queries\n2. Access information about Members of Parliament\n3. Retrieve official documents in various formats and read the full content of the documents\n4. Analyze parliamentary activities and proceedings\n5. Track legislative cases and government pledges\n\nThe project leverages Bert Hubert's tkconv service as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs.\n\n## Installation\n\n### 1. Quick Start with NPM Package (Recommended)\n\nThe fastest way to get started is using the published npm package:\n\n```bash\nnpx @r-huijts/opentk-mcp\n```\n\n### 2. Using Claude Desktop with NPM Package\n\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@r-huijts/opentk-mcp\"\n      ]\n    }\n  }\n}\n```\n\n**Alternative configurations:**\n\nFor MultiServerMCPClient (Python):\n```python\nmcp_client = MultiServerMCPClient({\n    \"opentk\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@r-huijts/opentk-mcp\"],\n        \"transport\": \"stdio\",\n    }\n})\n```\n\n### 3. From Source (Development)\n\nIf you want to modify the code or contribute to development:\n\n**Clone Repository:**\n```bash\ngit clone https://github.com/r-huijts/opentk-mcp.git\ncd opentk-mcp\n```\n\n**Install Dependencies:**\n```bash\nnpm install\n```\n\n**Build the Project:**\n```bash\nnpm run build\n```\n\n**Start the Server:**\n```bash\nnpm start\n```\n\n**Configure Claude Desktop for local development:**\n\nUpdate your Claude configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk-local\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/your/opentk-mcp/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nMake sure to replace `/absolute/path/to/your/opentk-mcp/` with the actual path to your installation.\n\n### 4. Publishing (for maintainers)\n\nTo publish a new version of the scoped package:\n\n```bash\nnpm run build\nnpm publish --access=public\n```\n\nNote: Scoped packages require the `--access=public` flag to be publicly available.\n\n## Search Functionality\n\nThe search functionality is particularly sophisticated, supporting:\n\n- Simple keyword searches: `kunstmatige intelligentie`\n- Exact phrase searches: `\"kunstmatige intelligentie\"`\n- Exclusion searches: `Hubert NOT Bruls`\n- Boolean operators: `OR`, `NEAR()`\n\nThe implementation handles various edge cases:\n- Preserves quotes in search queries\n- Uses proper content type headers\n- Implements fallback mechanisms for API errors\n- Provides meaningful error messages\n\n## Error Handling\n\nThe API service includes robust error handling:\n- Graceful handling of API errors (4xx, 5xx)\n- Fallback to simplified queries when complex ones fail\n- Detailed error messages for debugging\n- Proper logging to stderr (not stdout, which would break the stdio transport)\n\n## Configuration\n\nThe server connects to Bert Hubert's [tkconv service](https://berthub.eu/tkconv/) as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs. This service, created by Bert Hubert, does the heavy lifting of collecting, organizing, and making available Dutch parliamentary data in a developer-friendly format. Our MCP server builds upon this foundation to create a standardized interface for AI assistants to interact with this valuable data.\n\n## License\n\nMIT\n\n## Conclusion\n\nThe OpenTK MCP server provides a robust and well-structured interface to Dutch parliamentary data, making it accessible to AI assistants through the Model Context Protocol. Its modular design, comprehensive API, and thorough testing ensure reliable access to parliamentary information for AI-assisted research, analysis, and information retrieval.\n\nOnce configured, Claude will be able to access Dutch parliamentary data through the OpenTK MCP server. The server exposes all the tools described in the [Usage](#usage) section above.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "parliament",
        "search",
        "documents",
        "dutch parliament",
        "structured search",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "ricocf--mcp-wolframalpha": {
      "owner": "ricocf",
      "name": "mcp-wolframalpha",
      "url": "https://github.com/ricocf/mcp-wolframalpha",
      "imageUrl": "",
      "description": "An MCP server lets AI assistants use the Wolfram Alpha API for real-time access to computational knowledge and data.",
      "stars": 44,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T21:33:50Z",
      "readme_content": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha’s knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n  Install the required dependencies with uv:\n  Ensure [`uv`](https://github.com/astral-sh/uv) is installed.\n\n   ```bash\n   uv sync\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n## Contact\n\nFeel free to give feedback. The e-mail address is shown if you execute this in a shell:\n\n```sh\nprintf \"\\x61\\x6b\\x61\\x6c\\x61\\x72\\x69\\x63\\x31\\x40\\x6f\\x75\\x74\\x6c\\x6f\\x6f\\x6b\\x2e\\x63\\x6f\\x6d\\x0a\"\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolfram",
        "ai",
        "ricocf",
        "mcp wolframalpha",
        "ai assistants",
        "wolframalpha mcp"
      ],
      "category": "search--data-extraction"
    },
    "sascharo--gxtract": {
      "owner": "sascharo",
      "name": "gxtract",
      "url": "https://github.com/sascharo/gxtract",
      "imageUrl": "",
      "description": "GXtract is a MCP server designed to integrate with VS Code and other compatible editors (documentation: [sascharo.github.io/gxtract](https://sascharo.github.io/gxtract)). It provides a suite of tools for interacting with the GroundX platform, enabling you to leverage its powerful document understanding capabilities directly within your development environment.",
      "stars": 0,
      "forks": 2,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-05-20T04:18:08Z",
      "readme_content": "# GXtract MCP Server\n\n<div style=\"text-align: left;\">\n  \n</div>\n\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://sascharo.github.io/gxtract/)\n[![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/)\n[![UV Version](https://img.shields.io/badge/uv-0.7.6+-green.svg)](https://github.com/astral-sh/uv)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\nGXtract is a Model Context Protocol (MCP) server designed to integrate with VS Code and other compatible editors. It provides a suite of tools for interacting with the GroundX platform, enabling you to leverage its powerful document understanding capabilities directly within your development environment.\n\n## Table of Contents\n\n- [Features](#features)\n- [Architecture](#architecture)\n- [Prerequisites](#prerequisites)\n- [Installing UV](#installing-uv)\n- [Quick Start: VS Code Integration](#quick-start-vs-code-integration)\n- [Available Tools](#available-tools)\n- [Configuration](#configuration)\n  - [API Key Security](#api-key-security)\n- [Development](#development)\n- [Documentation](#documentation)\n  - [Building Documentation Locally](#building-documentation-locally)\n  - [Building Documentation (Sphinx)](#building-documentation-sphinx)\n- [Cache Management](#cache-management)\n  - [When to Manually Refresh the Cache](#when-to-manually-refresh-the-cache)\n  - [How to Refresh the Cache](#how-to-refresh-the-cache)\n  - [Troubleshooting Common Cache Issues](#troubleshooting-common-cache-issues)\n  - [Checking Cache Status](#checking-cache-status)\n- [Dependency Management](#dependency-management)\n  - [Working with Dependencies](#working-with-dependencies)\n  - [The uv.lock File](#the-uvlock-file)\n- [Versioning](#versioning)\n- [License](#license)\n\n## Features\n\n*   **GroundX Integration:** Access GroundX functionalities like document search, querying, and semantic object explanation.\n*   **MCP Compliant:** Built for use with VS Code's MCP client and other MCP-compatible systems.\n*   **Efficient and Modern:** Developed with Python 3.12+ and FastMCP v2 for performance.\n*   **Easy to Configure:** Simple setup for VS Code.\n*   **Caching:** In-memory cache for GroundX metadata to improve performance and reduce API calls.\n\n## Architecture\n\nThe high-level system architecture of GXtract illustrates how the components interact:\n\n```mermaid\ngraph TB\n    subgraph \"Client\"\n        VSC[VS Code / Editor]\n    end\n\n    subgraph \"GXtract MCP Server\"\n        MCP[MCP Interface<br>stdio/http]\n        Server[GXtract Server]\n        Cache[Metadata Cache]\n        Tools[Tool Implementations]\n    end\n\n    subgraph \"External Services\"\n        GXAPI[GroundX API]\n    end\n\n    VSC -->|MCP Protocol| MCP\n    MCP --> Server\n    Server --> Tools\n    Tools -->|Query| GXAPI\n    Tools -->|Read/Write| Cache\n    Cache -.->|Refresh| GXAPI\n```\n\nThis diagram shows:\n\n1. **Client Integration**: VS Code communicates with GXtract using the MCP protocol\n2. **Transport Layer**: Supports both stdio (for direct VS Code integration) and HTTP transport\n3. **Core Components**: Server manages tool registration and requests\n4. **Caching Layer**: Maintains metadata to reduce API calls\n5. **Tool Implementation**: Provides specialized functions for interacting with GroundX\n6. **API Communication**: Secure connection to GroundX platform\n\nFor more detailed architecture information, see the [full documentation](https://sascharo.github.io/gxtract/architecture.html).\n\n## Prerequisites\n\n*   **Python 3.12 or higher.**\n*   **UV (Python package manager):** Version 0.7.6 or higher. You can install it from [astral.sh/uv](https://astral.sh/uv).\n*   **GroundX API Key:** You need a valid API key from the [GroundX Dashboard](https://dashboard.groundx.ai/).\n\n## Installing UV\n\nBefore you can use GXtract, you need to install UV (version 0.7.6 or higher), a modern Python package manager written in Rust that offers significant performance improvements over traditional tools.\n\n### Quick Installation Methods\n\n**Windows (PowerShell 7):**\n```powershell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n**macOS and Linux:**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### Alternative Installation Methods\n\n**Using pip:**\n```bash\npip install --upgrade uv\n```\n\n**Using Homebrew (macOS):**\n```bash\nbrew install uv\n```\n\n**Using pipx (isolated environment):**\n```bash\npipx install uv\n```\n\nAfter installation, verify that UV is working correctly:\n```bash\nuv --version\n```\n\nThis should display version 0.7.6 or higher. For more information about UV, visit the [official documentation](https://docs.astral.sh/uv/).\n\n## Quick Start: VS Code Integration\n\n1.  **Clone the GXtract Repository:**\n    ```bash\n    git clone <repository_url>  # Replace <repository_url> with the actual URL\n    cd gxtract\n    ```\n\n2.  **Install Dependencies using UV:**\n    Open a terminal in the `gxtract` project directory and run:\n    ```powershell\n    uv sync\n    ```\n    This command creates a virtual environment (if one doesn't exist or isn't active) and installs all necessary dependencies specified in `pyproject.toml` and `uv.lock`.\n\n3.  **Set GroundX API Key:**\n    The GXtract server requires your GroundX API key. You need to make this key available as an environment variable named `GROUNDX_API_KEY`.\n    VS Code will pass this environment variable to the server based on the configuration below. Ensure `GROUNDX_API_KEY` is set in the environment where VS Code is launched, or configure your shell profile (e.g., `.bashrc`, `.zshrc`, PowerShell Profile) to set it.\n\n    **Option 1: Using Environment Variables (as shown above)**\n    \n    This approach reads the API key from your system environment variables:\n    \n    ```json\n    \"env\": {\n        \"GROUNDX_API_KEY\": \"${env:GROUNDX_API_KEY}\"\n    }\n    ```\n    \n    **Option 2: Using VS Code's Secure Inputs**\n    \n    VS Code can prompt for your API key and store it securely. Add this to your `settings.json`:\n    \n    ```json\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"groundx-api-key\",\n        \"description\": \"GroundX API Key\",\n        \"password\": true\n      }\n    ]\n    ```\n    \n    Then reference it in your server configuration:\n    \n    ```json\n    \"env\": {\n        \"GROUNDX_API_KEY\": \"${input:groundx-api-key}\"\n    }\n    ```\n    \n    With this approach, VS Code will prompt you for the API key the first time it launches the server, then store it securely in your system's credential manager (Windows Credential Manager, macOS Keychain, or similar).\n\n4.  **Configure VS Code `settings.json`:**\n    Open your VS Code `settings.json` file (Ctrl+Shift+P, then search for \"Preferences: Open User Settings (JSON)\"). Add or update the `mcp.servers` configuration:\n    ```jsonc\n    \"mcp\": {\n        \"servers\": {\n           \"gxtract\": { // You can name this server entry as you like, i.e. GXtract\n                \"command\": \"uv\",\n                \"type\": \"stdio\", // 💡 http is also supported but VS Code only supports stdio currently\n                \"args\": [\n                    // Adjust the path to your gxtract project directory if it's different\n                    \"--directory\", \n                    \"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\", // Example: C:\\\\Users\\\\yourname\\\\projects\\\\gxtract\n                    \"--project\",\n                    \"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\", // Example: C:\\\\Users\\\\yourname\\\\projects\\\\gxtract\n                    \"run\",\n                    \"gxtract\", // This matches the script name in pyproject.toml\n                    \"--transport\",\n                    \"stdio\" // 💡 Ensure this matches the \"type\" above\n                ],\n                \"env\": {\n                    // Option 1: Using environment variables (system-wide)\n                    \"GROUNDX_API_KEY\": \"${env:GROUNDX_API_KEY}\"\n\n                    // Option 2: Using secure VS Code input (uncomment to use)\n                    // \"GROUNDX_API_KEY\": \"${input:groundx-api-key}\"\n                }\n            }\n        }\n    }\n    ```\n    If using Option 2 (secure inputs), add this section (`settings.json`):\n    ```jsonc\n    // 💡 Only needed for Option 2 (secure inputs)\n    \"inputs\": [\n        {\n            \"type\": \"promptString\",\n            \"id\": \"groundx-api-key\",\n            \"description\": \"GroundX API Key\",\n            \"password\": true\n        }\n    ]\n    ```\n    **Important:**\n    *   Replace `\"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\"` with the **absolute path** to the `gxtract` directory on your system.\n    *   The `\"command\": \"uv\"` assumes `uv` is in your system's PATH. If not, you might need to provide the full path to the `uv` executable.\n    *   The server name `\"GXtract\"` in `settings.json` is how it will appear in VS Code's MCP interface.\n\n5.  **Reload VS Code:**\n    After saving `settings.json`, you might need to reload VS Code (Ctrl+Shift+P, \"Developer: Reload Window\") for the changes to take effect.\n\n6.  **Using GXtract Tools:**\n    Once configured, you can access GXtract's tools through VS Code's MCP features (e.g., via chat `@` mentions if your VS Code version supports it, or other MCP integrations).\n\n## Available Tools\n\nGXtract provides the following tools for interacting with GroundX:\n\n*   `groundx/searchDocuments`: Search for documents within your GroundX projects.\n*   `groundx/queryDocument`: Ask specific questions about a document in GroundX.\n*   `groundx/explainSemanticObject`: Get explanations for diagrams, tables, or other semantic objects within documents.\n*   `cache/refreshMetadataCache`: Manually refresh the GroundX metadata cache.\n*   `cache/refreshCachedResources`: Manually refresh the GroundX projects and buckets cache.\n*   `cache/getCacheStatistics`: Get statistics about the cached metadata.\n*   `cache/listCachedResources`: List all currently cached GroundX resources (projects, buckets).\n\n## Configuration\n\nThe server can be configured via command-line arguments when run directly. When used via VS Code, these are typically set in the `args` array in `settings.json`.\n\n*   `--transport {stdio|http}`: Communication transport type (default: `http`, but `stdio` is used for VS Code).\n*   `--host TEXT`: Host address for HTTP transport (default: `127.0.0.1`).\n*   `--port INTEGER`: Port for HTTP transport (default: `8080`).\n*   `--log-level {DEBUG|INFO|WARNING|ERROR|CRITICAL}`: Logging level (default: `INFO`).\n*   `--log-format {text|json}`: Log output format (default: `text`).\n*   `--disable-cache`: Disable the GroundX metadata cache.\n*   `--cache-ttl INTEGER`: Cache Time-To-Live in seconds (default: `3600`).\n\n### API Key Security\n\nThe GroundX API key is sensitive information that should be handled securely. GXtract supports several approaches to provide this key:\n\n1. **Environment Variables** (recommended for development):\n   - Set `GROUNDX_API_KEY` in your system or shell environment\n   - VS Code will pass it to the server using `${env:GROUNDX_API_KEY}` in settings.json\n\n2. **VS Code Secure Storage** (recommended for shared workstations):\n   - Configure VS Code to prompt for the key and store it securely\n   - Uses your system's credential manager (Windows Credential Manager, macOS Keychain)\n   - Setup using the `inputs` section in settings.json as shown in the Quick Start\n\n3. **Direct Environment Variable in VS Code settings** (not recommended):\n   - It's possible to set the key directly in settings.json: `\"GROUNDX_API_KEY\": \"your-api-key-here\"`\n   - This is not recommended as it stores the key in plaintext in your settings.json file\n\nAlways ensure your API key is not committed to source control or shared with unauthorized users.\n\n## Development\n\nTo set up for development:\n\n1.  Clone the repository.\n2.  Navigate to the `gxtract` directory.\n3.  Create and activate a virtual environment using `uv`:\n    ```powershell\n    uv venv # Create virtual environment in .venv\n    ```\n    * Activate with Windows PowerShell:\n      ```powershell\n      .\\.venv\\Scripts\\Activate.ps1\n      ```\n    * Activate with Linux/macOS bash/zsh:\n      ```bash\n      source .venv/bin/activate \n      ```\n4.  Install main project dependencies into the virtual environment:\n    ```powershell\n    uv sync # Install main dependencies from pyproject.toml\n    ```\n    Development tools (like Ruff, Pytest, Sphinx, etc.) are managed by Hatch and will be installed automatically\n    into a separate environment when you run Hatch scripts (see below).\n    Alternatively, to explicitly create or ensure the Hatch 'default' development environment is set up:\n    ```powershell\n    hatch env create default # Ensure your main .venv is active first\n    ```\n    If you need to force a complete refresh of this environment, you can remove it first \n    with 'hatch env remove default' before running 'hatch env create default'.\n\nRun linters/formatters (this will also install them via Hatch if not already present):\n```powershell\nuv run lint\nuv run format\n```\n\n## Documentation\n\nThe full documentation for GXtract is available at [https://sascharo.github.io/gxtract/](https://sascharo.github.io/gxtract/).\n\n### Building Documentation Locally\n\nIf you want to build and view the documentation locally:\n\n1. Ensure you have installed all development dependencies:\n   ```bash\n   uv sync\n   ```\n\n2. Build the documentation:\n   ```bash\n   uv run hatch -e default run docs-build\n   ```\n\n3. Serve the documentation locally:\n   ```bash\n   uv run hatch -e default run docs-serve\n   ```\n\n4. Open your browser and navigate to [http://127.0.0.1:8000](http://127.0.0.1:8000)\n\n### Building Documentation (Sphinx)\n\nThe project documentation is built using [Sphinx](https://www.sphinx-doc.org/). The following Hatch scripts are available to manage the documentation:\n\n*   **Build Documentation:**\n    ```bash\n    uv run docs-build\n    ```\n    This command generates the HTML documentation in the `docs/sphinx/build/html` directory.\n\n*   **Serve Documentation Locally:**\n    ```bash\n    uv run docs-serve\n    ```\n    This starts a local HTTP server (usually at `http://127.0.0.1:8000`) to preview the documentation. You can specify a different port if needed, e.g., `uv run docs-serve 8081`.\n\n*   **Clean Documentation Build:**\n    ```bash\n    uv run docs-clean\n    ```\n    This command removes the `docs/sphinx/build` directory, cleaning out old build artifacts.\n\nEnsure your virtual environment is active before running these commands.\n\n## Cache Management\n\nGXtract maintains an in-memory cache of GroundX metadata (projects and buckets) to improve performance and reduce API calls. While this cache is automatically populated during server startup and periodically refreshed, there are situations when you may need to manually refresh the cache.\n\n### When to Manually Refresh the Cache\n\nYou should manually refresh the cache when:\n\n1. You've recently created new projects or buckets in your GroundX account and want them to be immediately available in GXtract.\n2. You see warnings in the server logs about cache population failures.\n3. You're experiencing issues with project or bucket lookup when using GXtract tools.\n\n### How to Refresh the Cache\n\n#### Using VS Code's MCP Interface\n\nIf your VS Code version supports MCP chat interfaces:\n\n1. Open VS Code's chat interface.\n2. Use the `@GXtract` mention (or whatever name you assigned to the server in your settings).\n3. Type a command to refresh the cache:\n   ```\n   @GXtract Please refresh the GroundX metadata cache\n   ```\n4. The VS Code interface will use the appropriate cache refresh tool.\n\n#### Using Direct JSON-RPC Requests\n\nIf you have access to the server through HTTP (when not using stdio transport), you can make direct requests:\n\n```bash\ncurl -X POST http://127.0.0.1:8080/jsonrpc -H \"Content-Type: application/json\" -d '{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/refreshMetadataCache\",\n  \"params\": {},\n  \"id\": \"refresh-req-001\"\n}'\n```\n\n### Troubleshooting Common Cache Issues\n\n#### Warning: \"No projects (groups) found or 'groups' attribute missing in API response\"\n\nThis warning indicates that:\n- Your API key might not have access to any projects, or\n- No projects have been created in your GroundX account yet, or\n- There might be an issue with the GroundX API or connectivity.\n\n**Solution**: \n1. Verify you have correctly set up your GroundX account with at least one project.\n2. Check that your API key has proper permissions.\n3. Try refreshing the cache manually after confirming your account setup.\n\n#### Warning: \"GroundX metadata cache population failed. Check logs for details\"\n\nThis warning appears during server startup if the initial cache population failed.\n\n**Solution**:\n1. Check the full server logs for more details about the error.\n2. Verify your API key is correctly set in the environment.\n3. Check your internet connection and GroundX API availability.\n4. Try using the `cache/refreshMetadataCache` tool to manually populate the cache.\n\n### Checking Cache Status\n\nYou can check the current status of the cache with:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/getCacheStatistics\",\n  \"params\": {},\n  \"id\": \"stats-req-001\"\n}\n```\n\nOr list the currently cached resources:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/listCachedResources\",\n  \"params\": {},\n  \"id\": \"list-req-001\"\n}\n```\n\n## Dependency Management\n\nGXtract uses [uv](https://github.com/astral-sh/uv) for dependency management. Dependencies are specified in `pyproject.toml` and locked in `uv.lock` to ensure reproducible installations.\n\n### Working with Dependencies\n\n- **Installing dependencies**: Run `uv sync` to install all dependencies according to the lockfile.\n- **Adding a new dependency**: Add the dependency to `pyproject.toml` and run `uv pip compile pyproject.toml -o uv.lock` to update the lockfile.\n- **Updating dependencies**: After manually changing versions in `pyproject.toml`, run `uv pip compile pyproject.toml -o uv.lock --upgrade` to update the lockfile with newest compatible versions.\n\n### The uv.lock File\n\nThe `uv.lock` file is committed to the repository to ensure that everyone working on the project uses exactly the same dependency versions. This prevents \"works on my machine\" problems and ensures consistent behavior across development environments and CI/CD pipelines.\n\nWhen making changes to dependencies, always commit both the updated `pyproject.toml` and the `uv.lock` file.\n\n## Versioning\n\nThis project adheres to [Semantic Versioning (SemVer 2.0.0)](https://semver.org/spec/v2.0.0.html).\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE.md](LICENSE.md) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gxtract",
        "sascharo",
        "suite",
        "gxtract provides",
        "sascharo gxtract",
        "io gxtract"
      ],
      "category": "search--data-extraction"
    },
    "serkan-ozal--driflyte-mcp-server": {
      "owner": "serkan-ozal",
      "name": "driflyte-mcp-server",
      "url": "https://github.com/serkan-ozal/driflyte-mcp-server",
      "imageUrl": "",
      "description": "The Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T05:26:38Z",
      "readme_content": "# Driflyte MCP Server\n\n![Build Status](https://github.com/serkan-ozal/driflyte-mcp-server/actions/workflows/build.yml/badge.svg)\n![NPM Version](https://badge.fury.io/js/%40driflyte%2Fmcp-server.svg)\n![License](https://img.shields.io/badge/license-MIT-blue)\n[![MCP Badge](https://lobehub.com/badge/mcp/serkan-ozal-driflyte-mcp-server)](https://lobehub.com/mcp/serkan-ozal-driflyte-mcp-server)\n\nMCP Server for [Driflyte](http://console.driflyte.com).\n\nThe Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.\nWith this MCP server, Driflyte acts as a bridge between diverse, topic-aware content sources (web, GitHub, and more) and AI-powered reasoning, enabling richer, more accurate answers.\n\n\n## What It Does\n\n- **Deep Web Crawling**: Recursively follows links to crawl and index web pages.\n- **GitHub Integration**: Crawls repositories, issues, and discussions.\n- **Extensible Resource Support**: Future support planned for Slack, Microsoft Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, and more.\n- **Topic-Aware Indexing**: Each document is tagged with one or more topics, enabling targeted, topic-specific retrieval.\n- **Designed for RAG with RAG**: The server itself is built with Retrieval-Augmented Generation (RAG) in mind, and it powers RAG workflows by providing assistants with high-quality, topic-specific documents as grounding context.\n- **Designed for AI with AI**: The system is not just for AI assistants — it is also designed and evolved using AI itself, making it an AI-native component for intelligent knowledge retrieval.\n\n\n## Usage & Limits\n\n- **Free Access**: Driflyte is currently free to use.\n- **No Signup Required**: You can start using it immediately — no registration or subscription needed.\n- **Rate Limits**: To ensure fair usage, requests are limited by IP:\n  - **`100` API requests** per **`5` minutes** per **IP address**.\n- Future changes to usage policies and limits may be introduced as new features and resource integrations become available.\n\n\n## Prerequisites\n- Node.js 18+\n- An AI assistant (with MCP client) like Cursor, Claude (Desktop or Code), VS Code, Windsurf, etc ...\n\n## Configurations\n\n### CLI Arguments\n\nDriflyte MCP server supports the following CLI arguments for configuration:\n- `--transport <stdio|streamable-http>` - Configures the transport protocol (defaults to `stdio`).\n- `--port <number>` – Configures the port number to listen on when using `streamable-http` transport (defaults to `3000`).\n\n\n## Quick Start\n\nThis MCP server (using `STDIO` or `Streamable HTTP` transport) can be added to any MCP Client \nlike VS Code, Claude, Cursor, Windsurf Github Copilot via the `@driflyte/mcp-server` NPM package.\n\n### ChatGPT\n\n- Navigate to `Settings` under your profile and enable `Developer Mode` under the `Connectors` option.\n- In the chat panel, click the `+` icon, and from the dropdown, select `Developer Mode`. \n  You’ll see an option to add sources/connectors.\n- Enter the following MCP Server details and then click `Create`:\n  - `Name`: `Driflyte`\n  - `MCP Server URL`: `https://mcp.driflyte.com/openai`\n  - `Authentication`: `No authentication`\n  - `Trust Setting`: Check `I trust this application`\n\nSee [How to set up a remote MCP server and connect it to ChatGPT deep research](https://community.openai.com/t/how-to-set-up-a-remote-mcp-server-and-connect-it-to-chatgpt-deep-research/1278375) \nand [MCP server tools now in ChatGPT – developer mode](https://community.openai.com/t/mcp-server-tools-now-in-chatgpt-developer-mode/1357233) for more info.\n\n### Claude Code\n\nRun the following command.\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Local Server\n```bash\nclaude mcp add driflyte -- npx -y @driflye/mcp-server\n```\n\n#### Remote Server\n```bash\nclaude mcp add --transport http driflyte https://mcp.driflyte.com/mcp\n```\n\n### Claude Desktop\n\n#### Local Server\nAdd the following configuration into the `claude_desktop_config.json` file.\nSee the [Claude Desktop MCP docs](https://modelcontextprotocol.io/docs/develop/connect-local-servers) for more info.\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\nGo to the `Settings` > `Connectors` > `Add Custom Connector` in the Claude Desktop and add the new MCP server with the following fields: \n- Name: `Driflyte` \n- Remote MCP server URL: `https://mcp.driflyte.com/mcp`\n\n### Copilot Coding Agent\n\nAdd the following configuration to the `mcpServers` section of your Copilot Coding Agent configuration through \n`Repository` > `Settings` > `Copilot` > `Coding agent` > `MCP configuration`.\nSee the [Copilot Coding Agent MCP docs](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"local\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd the following configuration into the `~/.cursor/mcp.json` file (or `.cursor/mcp.json` in your project folder).\nOr setup by 🖱️[One Click Installation](https://cursor.com/en/install-mcp?name=driflyte&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBkcmlmbHl0ZS9tY3Atc2VydmVyIl19).\nSee the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Gemini CLI\n\nAdd the following configuration into the `~/.gemini/settings.json` file.\nSee the [Gemini CLI MCP docs](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"httpUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Smithery\n\nRun the following command.\nYou can find your Smithery API key [here](https://smithery.ai/account/api-keys).\nSee the [Smithery CLI docs](https://smithery.ai/docs/concepts/cli) for more info.\n```bash\nnpx -y @smithery/cli install @serkan-ozal/driflyte-mcp-server --client <SMITHERY-CLIENT-NAME> --key <SMITHERY-API-KEY>\n```\n\n### VS Code\n\nAdd the following configuration into the `.vscode/mcp.json` file.\nOr setup by 🖱️[One Click Installation](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22driflyte%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40driflyte%2Fmcp-server%22%5D%7D).\nSee the [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### Local Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n      }\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"http\",\n        \"url\": \"https://mcp.driflyte.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following configuration into the `~/.codeium/windsurf/mcp_config.json` file. \nSee the [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"serverUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n\n## Components\n\n### Tools\n\n- `list-topics`: Returns a list of topics for which resources (web pages, etc ...) have been crawled and content is available. \n                 This allows AI assistants to discover the most relevant and up-to-date subject areas currently indexed by the crawler.\n  - **Input Schema**: No input parameter supported.\n  - **Output Schema**:\n    - `topics`:\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: List of the supported topics.\n- `search`: Given a list of topics and a user question, this tool retrieves the top-K most relevant documents from the crawled content. \n            It is designed to help AI assistants surface the most contextually appropriate and up-to-date information for a specific topic and query.\n            This enables more informed and accurate responses based on real-world, topic-tagged web content.\n  - **Input Schema**:\n    - `topics`\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: A list of one or more topic identifiers to constrain the search space.\n                       Only documents tagged with at least one of these topics will be considered.\n    - `query`\n      - `Optinal`: `false`\n      - `Type`: `string`\n      - `Description`: The natural language query or question for which relevant information is being sought.\n                       This will be used to rank documents by semantic relevance. \n    - `topK`\n      - `Optinal`: `true`\n      - `Type`: `number`\n      - `Default Value`: `10`\n      - `Min Value`: `1`\n      - `Max Value`: `30`\n      - `Description`: The maximum number of relevant documents to return.\n                       Results are sorted by descending relevance score.\n  - **Output Schema**:\n    - `documents`:\n      - `Optional`: `false`\n      - `Type`: `Array<Document>`\n      - `Description`: Matched documents to the search query.\n      - **Type**: `Document`:\n        - `content`\n          - `Optinal`: `false`\n          - `Type`: `string`\n          - `Description`: Related content (full or partial) of the matched document.\n        - `metadata`\n          - `Optinal`: `false`\n          - `Type`: `Map<string, any>`\n          - `Description`: Metadata of the document and related content in key-value format.\n        - `score`\n          - `Optinal`: `false`\n          - `Type`: `number`\n          - `Min Value`: `0`\n          - `Max Value`: `1`\n          - `Description`: Similarity score (between `0` and `1`) for the content of the document.\n\n### Resources\n\nN/A\n\n\n## Roadmap\n\n- Support more content types (`.pdf`, `.ppt`/`.pptx`, `.doc`/`.docx`, and many others applicable including audio and video file formats ...)\n- Integrate with more data sources (Slack, Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, etc ...))\n- And more topics with their resources\n\n\n## Issues and Feedback\n\n[![Issues](https://img.shields.io/github/issues/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aopen+is%3Aissue)\n[![Closed issues](https://img.shields.io/github/issues-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aissue+is%3Aclosed)\n\nPlease use [GitHub Issues](https://github.com/serkan-ozal/driflyte-mcp-server/issues) for any bug report, feature request and support.\n\n\n## Contribution\n\n[![Pull requests](https://img.shields.io/github/issues-pr/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Aopen+is%3Apr)\n[![Closed pull requests](https://img.shields.io/github/issues-pr-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Apr+is%3Aclosed)\n[![Contributors](https://img.shields.io/github/contributors/serkan-ozal/driflyte-mcp-server.svg)]()\n\nIf you would like to contribute, please\n- Fork the repository on GitHub and clone your fork.\n- Create a branch for your changes and make your changes on it.\n- Send a pull request by explaining clearly what is your contribution.\n\n> Tip:\n> Please check the existing pull requests for similar contributions and\n> consider submit an issue to discuss the proposed feature before writing code.\n\n## License\n\nLicensed under [MIT](LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawled",
        "search",
        "indexed",
        "search data",
        "data extraction",
        "driflyte mcp"
      ],
      "category": "search--data-extraction"
    },
    "shopsavvy--shopsavvy-mcp-server": {
      "owner": "shopsavvy",
      "name": "shopsavvy-mcp-server",
      "url": "https://github.com/shopsavvy/shopsavvy-mcp-server",
      "imageUrl": "",
      "description": "Complete product and pricing data solution for AI assistants. Search for products by barcode/ASIN/URL, access detailed product metadata, access comprehensive pricing data from thousands of retailers, view and track price history, and more.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-15T17:16:59Z",
      "readme_content": "# ShopSavvy Data API MCP Server\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) (MCP) server that provides AI assistants with access to ShopSavvy's comprehensive product data, pricing information, and historical price tracking.\n\n## Overview\n\nThis MCP server enables AI assistants to:\n- **Look up products** by barcode, ASIN, URL, model number, or ShopSavvy ID\n- **Get current pricing** from multiple retailers\n- **Access historical pricing data** with date ranges\n- **Schedule products** for automatic price monitoring\n- **Track API usage** and credit consumption\n\n## Features\n\n### 🔍 Product Lookup Tools\n- `product_lookup` - Find products by various identifiers (barcode, ASIN, URL, etc.)\n- `product_lookup_batch` - Look up multiple products at once\n\n### 💰 Pricing Tools\n- `product_offers` - Get current offers from all retailers\n- `product_offers_retailer` - Get offers from a specific retailer\n- `product_price_history` - Get historical pricing data with date ranges\n\n### 📅 Scheduling Tools\n- `product_schedule` - Schedule products for automatic refresh (hourly/daily/weekly)\n- `product_unschedule` - Remove products from refresh schedule\n- `scheduled_products_list` - View all scheduled products\n\n### 📊 Analytics Tools\n- `api_usage` - View current API usage and credit consumption\n\n## Installation\n\n```bash\nnpm install @shopsavvy/mcp-server\n```\n\n## Configuration\n\n### 1. Get API Key\n\nFirst, get your ShopSavvy Data API key:\n\n1. Visit [https://shopsavvy.com/data](https://shopsavvy.com/data)\n2. Sign up and choose a subscription plan\n3. Create an API key in your dashboard\n4. Copy your API key (starts with `ss_live_` or `ss_test_`)\n\n### 2. Claude Desktop Setup\n\nAdd this to your Claude Desktop configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%\\\\Claude\\\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"shopsavvy\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shopsavvy/mcp-server\"\n      ],\n      \"env\": {\n        \"SHOPSAVVY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### 3. Environment Variables\n\nSet your API key as an environment variable:\n\n```bash\nexport SHOPSAVVY_API_KEY=\"ss_live_your_key_here\"\n```\n\nOr create a `.env` file:\n```\nSHOPSAVVY_API_KEY=ss_live_your_key_here\n```\n\n## Usage Examples\n\n### Product Lookup\n```\nLook up the product with barcode 012345678901\n```\n\n### Current Pricing\n```\nGet current prices for ASIN B08N5WRWNW from all retailers\n```\n\n### Price History\n```\nGet price history for product 012345678901 from January 1-15, 2024\n```\n\n### Schedule Monitoring\n```\nSchedule daily price monitoring for products: 012345678901, B08N5WRWNW\n```\n\n## API Limits & Pricing\n\n- **Starter Plan**: 1,000 credits/month - $49/month\n- **Professional Plan**: 10,000 credits/month - $199/month\n- **Enterprise Plan**: 100,000 credits/month - $499/month\n\n### Credit Usage:\n- Product lookup: 1 credit per product found\n- Current offers (all retailers): 3 credits per product\n- Current offers (single retailer): 2 credits per product\n- Historical data: 3 credits + 1 credit per day of history\n- Scheduling: 1 credit per product scheduled\n\n## Development\n\n### Running Locally\n\n```bash\n# Clone the repository\ngit clone https://github.com/shopsavvy/shopsavvy-mcp-server\ncd shopsavvy-mcp-server\n\n# Install dependencies\nnpm install\n\n# Set your API key\nexport SHOPSAVVY_API_KEY=\"your_key_here\"\n\n# Test with MCP CLI\nnpm run dev\n\n# Or inspect with MCP Inspector\nnpm run inspect\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Invalid API keys\n- Insufficient credits\n- Rate limiting\n- Invalid product identifiers\n- API service issues\n\n## Support\n\n- **Documentation**: [https://shopsavvy.com/data/documentation](https://shopsavvy.com/data/documentation)\n- **Dashboard**: [https://shopsavvy.com/data/dashboard](https://shopsavvy.com/data/dashboard)\n- **Issues**: [https://github.com/shopsavvy/shopsavvy-mcp-server/issues](https://github.com/shopsavvy/shopsavvy-mcp-server/issues)\n\n## Changelog\n\n### v1.0.0 (2025-07-28)\n\n🎉 **Initial Release**\n\n- **Features**: Complete ShopSavvy Data API integration with MCP support\n- **Product Tools**: Lookup by barcode, ASIN, URL, model number, or ShopSavvy ID\n- **Pricing Tools**: Current offers from all/specific retailers, historical pricing data\n- **Scheduling Tools**: Automatic product monitoring (hourly/daily/weekly)\n- **Analytics Tools**: API usage tracking and credit consumption monitoring\n- **npm Package**: Published as `@shopsavvy/mcp-server` under ShopSavvy organization\n- **TypeScript**: Full TypeScript support with proper error handling\n- **Documentation**: Comprehensive README with examples and configuration guides\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\nMade with ❤️ by [ShopSavvy](https://shopsavvy.com) - Empowering everyone to always get the best deal, every time.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "shopsavvy",
        "retailers",
        "search",
        "pricing data",
        "shopsavvy mcp",
        "search products"
      ],
      "category": "search--data-extraction"
    },
    "tianqitang1--enrichr-mcp-server": {
      "owner": "tianqitang1",
      "name": "enrichr-mcp-server",
      "url": "https://github.com/tianqitang1/enrichr-mcp-server",
      "imageUrl": "",
      "description": "A MCP server that provides gene set enrichment analysis using the Enrichr API",
      "stars": 8,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T19:49:19Z",
      "readme_content": "<!--\n * @Author: tianqitang1 Tianqi.Tang@ucsf.edu\n * @Date: 2025-06-03 14:18:58\n * @LastEditors: tianqitang1 Tianqi.Tang@ucsf.edu\n * @LastEditTime: 2025-06-29 06:46:59\n * @FilePath: /enrichr-mcp-server/README.md\n-->\n# Enrichr MCP Server\n\n<div align=\"center\">\n  \n</div>\n\nA Model Context Protocol (MCP) server that provides gene set enrichment analysis using the [Enrichr](https://maayanlab.cloud/Enrichr/) API. This server supports all available gene set libraries from Enrichr and returns only statistically significant results (corrected-$p$ < 0.05) for LLM tools to interpret.\n\n## Installation\n\nFor Claude Desktop, please download the [Desktop Extension](https://github.com/tianqitang1/enrichr-mcp-server/releases/latest) and install it by clicking `☰ (top left) -> File -> Settings` and drag and drop the downloaded file into the `Settings` window.\n\nUse the button below to install the MCP server to Cursor, VS Code, or VS Code Insiders with default settings.\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=enrichr-mcp-server&config=eyJjb21tYW5kIjoibnB4IC15IGVucmljaHItbWNwLXNlcnZlciAtLWNvbXBhY3QgLS1tYXgtdGVybXMgMTAwIn0%3D)\n[![Add to VS Code](https://img.shields.io/badge/Add_to_VS_Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect?url=vscode%3Amcp/install%3F%257B%2522name%2522%253A%2522enrichr-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522enrichr-mcp-server%2522%252C%2522--compact%2522%252C%2522--max-terms%2522%252C%2522100%2522%255D%257D)\n[![Add to VS Code Insiders](https://img.shields.io/badge/Add_to_VS_Code_Insiders-24bfa5?style=for-the-badge&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect?url=vscode-insiders%3Amcp/install%3F%257B%2522name%2522%253A%2522enrichr-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522enrichr-mcp-server%2522%252C%2522--compact%2522%252C%2522--max-terms%2522%252C%2522100%2522%255D%257D)\n\n\nFor Claude Code, use the following command:\n```bash\nclaude mcp add enrichr-mcp-server -- npx -y enrichr-mcp-server\n```\n\n## Features\n\n- **Multi-Library Enrichment Analysis**: Query multiple Enrichr libraries simultaneously (GO, pathways, diseases, tissues, drugs, etc.)\n- **Comprehensive Library Support**: Access to hundreds of gene set libraries from Enrichr including:\n  - Gene Ontology (Biological Process, Molecular Function, Cellular Component)\n  - Pathway databases (KEGG, Reactome, WikiPathways, BioCarta, MSigDB)\n  - Disease/Phenotype databases (Human Phenotype Ontology, GWAS Catalog)\n  - Tissue/Cell type libraries (GTEx, Human Cell Atlas, ARCHS4)\n  - Drug/Chemical libraries (DrugMatrix, L1000, TG-GATEs)\n  - Transcription Factor targets (ChEA, ENCODE)\n  - MicroRNA targets (TargetScan, miRTarBase)\n- **GO Enrichment Analysis**: Specialized tool for GO Biological Process enrichment analysis (I use this a lot, so I made it a tool)\n\n\n## Configuration\n\n### MCP Client Configuration\n\nAdd this server to your MCP client configuration (e.g., `.cursor/mcp.json`):\n\n#### Basic Configuration (Popular Libraries by Default)\nWith the default configuration the server will query a curated list of popular libraries.\n```json\n{\n  \"mcpServers\": {\n    \"enrichr-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\"]\n    }\n  }\n}\n```\n\n#### Custom Available Libraries Configuration\n\nYou can configure libraries that are available for the LLM to use using CLI arguments in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"enrichr-popular\": {\n      \"command\": \"npx\", \n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"--libraries\", \"pop\"]\n    }, // This will make the most popular libraries available to the LLM, namely GO_Biological_Process_2025, KEGG_2021_Human, Reactome_2022, MSigDB_Hallmark_2020, ChEA_2022, GWAS_Catalog_2023, Human_Phenotype_Ontology, STRING_Interactions_2023, DrugBank_2022, CellMarker_2024\n    \"enrichr-pathways\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"-l\", \"GO_Biological_Process_2025,KEGG_2021_Human,Reactome_2022\"]\n    },\n    \"enrichr-disease\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"--libraries\", \"Human_Phenotype_Ontology,OMIM_Disease,ClinVar_2019\"]\n    }\n  }\n}\n```\n\n### Command Line Options\n\nAdjust the CLI options to your needs, unreasonable settings might exceed the context window of the LLM and confuse it, so choose wisely:\n\n| Option | Short | Description | Default |\n|--------|-------|-------------|---------|\n| `--libraries <libs>` | `-l` | Comma-separated list of Enrichr libraries to query | `pop` |\n| `--max-terms <num>` | `-m` | Maximum terms to show per library | `50` |\n| `--format <format>` | `-f` | Output format: `detailed`, `compact`, `minimal` | `detailed` |\n| `--output <file>` | `-o` | Save complete results to TSV file | _(none)_ |\n| `--compact` | `-c` | Use compact format (same as `--format compact`) | _(flag)_ |\n| `--minimal` | | Use minimal format (same as `--format minimal`) | _(flag)_ |\n| `--help` | `-h` | Show help message | _(flag)_ |\n\n#### Format Options\n- **`detailed`**: Full details including p-values, odds ratios, and gene lists (default)\n- **`compact`**: Term name + p-value + gene count (saves ~50% tokens)\n- **`minimal`**: Just term name + p-value (saves ~80% tokens)\n\n#### Examples\n\nFor a full list of commands, options, and usage examples, run the server with the `--help` flag. This is the most up-to-date source of information.\n\n```bash\n# Show the help message\nnpx enrichr-mcp-server --help\n```\n\n### Environment Variables\n\nYou can also configure the server via environment variables:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `ENRICHR_LIBRARIES` | Comma-separated list of libraries to query | `GO_Biological_Process_2025,KEGG_2021_Human` |\n| `ENRICHR_MAX_TERMS` | Maximum terms per library | `20` |\n| `ENRICHR_FORMAT` | Output format (`detailed`/`compact`/`minimal`) | `compact` |\n| `ENRICHR_OUTPUT_FILE` | TSV output file path | `/tmp/enrichr_results.tsv` |\n\n**Note**: CLI arguments take precedence over environment variables when both are specified.\n\n\n### Popular Libraries\n\nThis table lists the libraries included when using the `-l pop` flag.\n\n| Library | Description |\n|---------|-------------|\n| `GO_Biological_Process_2025` | Gene Ontology terms describing biological objectives accomplished by gene products. |\n| `KEGG_2021_Human` | Metabolic and signaling pathways from Kyoto Encyclopedia of Genes and Genomes for human. |\n| `Reactome_2022` | Curated and peer-reviewed pathways from Reactome covering signaling, metabolism, gene expression, and disease. |\n| `MSigDB_Hallmark_2020` | Hallmark gene sets representing well-defined biological states and processes from MSigDB. |\n| `ChEA_2022` | ChIP-seq experiments from GEO, ENCODE, and publications identifying transcription factor-gene interactions from human and mouse. |\n| `GWAS_Catalog_2023` | Genome-wide association study results from NHGRI-EBI GWAS Catalog linking genes to traits. |\n| `Human_Phenotype_Ontology` | Standardized vocabulary of phenotypic abnormalities associated with human diseases. |\n| `STRING_Interactions_2023` | Protein interactions from STRING database including experimental and predicted. |\n| `DrugBank_2022` | Drug targets from DrugBank including approved drugs and experimental compounds. |\n| `CellMarker_2024` | Manually curated cell type markers from CellMarker database for human and mouse. |\n\nFor a complete list of available libraries, visit the [Enrichr Libraries page](https://maayanlab.cloud/Enrichr/#libraries).\n\n### Benefits of Library Configuration\n\n1. **Simplified Tool Calls**: When libraries aren't specified in tool calls, your configured libraries are used\n2. **Consistent Results**: Ensures consistent library usage across different queries  \n3. **Multiple Configurations**: Set up different MCP server instances for different research contexts\n4. **Override Capability**: Individual tool calls can still specify different libraries when needed\n\n## Usage\n\nThe server provides two tools:\n\n### `enrichr_analysis` (Recommended for multi-library analysis)\n\nPerforms enrichment analysis across multiple specified Enrichr libraries.\n\n**Parameters:**\n- `genes` (required): Array of gene symbols (e.g., [\"TP53\", \"BRCA1\", \"EGFR\"])\n- `libraries` (optional): Array of Enrichr library names to query (defaults to configured libraries)\n- `description` (optional): Description for the gene list (default: \"Gene list for enrichment analysis\")\n- `maxTerms` (optional): Maximum number of terms to show per library (default: 50)\n- `format` (optional): Output format: `detailed`, `compact`, `minimal` (default: `detailed`)\n- `outputFile` (optional): Path to save complete results as TSV file\n\n### `go_bp_enrichment` \n\nPerforms Gene Ontology (GO) Biological Process enrichment analysis to understand biological functions and processes overrepresented in your gene list. Perfect for interpreting gene expression data, identifying significant biological processes, and uncovering functional implications of genes from RNA-seq, microarray, or other high-throughput experiments.\n\n**Parameters:**\n- `genes` (required): Array of gene symbols (e.g., [\"TP53\", \"BRCA1\", \"EGFR\"])\n- `description` (optional): Description for the gene list (default: \"Gene list for GO BP enrichment\")\n- `outputFile` (optional): Path to save complete results as TSV file\n\n**Returns:**\nAll tools return formatted text with significant terms including:\n- Library name and summary statistics\n- Term name and identifier\n- Adjusted P-value and raw P-value (scientific notation)\n- Odds ratio and combined score\n- Overlapping genes with counts\n\n## Available Library Categories\n\nEnrichr contains hundreds of gene set libraries organized into categories:\n\n- **Gene Ontology**: Biological processes, molecular functions, cellular components\n- **Pathways**: KEGG, Reactome, WikiPathways, BioCarta, NCI, HumanCyc, Panther\n- **Disease/Phenotype**: HPO, OMIM, ClinVar, GWAS Catalog, DisGeNET\n- **Tissues/Cell Types**: GTEx, Human Cell Atlas, ARCHS4, Mouse Gene Atlas\n- **Transcription Factors**: ChEA, ENCODE, TRANSFAC, JASPAR\n- **MicroRNA Targets**: TargetScan, miRTarBase, microRNA.org\n- **Drug/Chemical**: DrugMatrix, L1000, TG-GATEs, CTD\n- **Protein Interactions**: BioGRID, STRING, hu.MAP\n- **Literature Mining**: PubMed, Geneshot, Co-expression\n- **Evolutionary**: Cross-species homologs, phylogenetic profiles\n\nFor a complete list of available libraries, visit the [Enrichr Libraries page](https://maayanlab.cloud/Enrichr/#libraries).\n\n## API Details\n\nThis server uses the Enrichr API:\n- **Add List Endpoint**: `https://maayanlab.cloud/Enrichr/addList`\n- **Enrichment Endpoint**: `https://maayanlab.cloud/Enrichr/enrich`\n- **Supported Libraries**: All libraries available through Enrichr web interface\n\n## Development\n\n- **Build**: `npm run build`\n- **Watch**: `npm run watch` (rebuilds on file changes)\n- **Inspector**: `npm run inspector` (debug with MCP inspector)\n\n## Requirements\n\n- Node.js 18+\n- TypeScript 5.3+\n- Internet connection for Enrichr API access\n\n## License\n\nThis project follows the same license as the MCP TypeScript SDK.\n\n## References\n\n- Chen EY, Tan CM, Kou Y, Duan Q, Wang Z, Meirelles GV, Clark NR, Ma'ayan A. Enrichr: interactive and collaborative HTML5 gene list enrichment analysis tool. BMC Bioinformatics. 2013; 128(14).\n\n- Kuleshov MV, Jones MR, Rouillard AD, Fernandez NF, Duan Q, Wang Z, Koplev S, Jenkins SL, Jagodnik KM, Lachmann A, McDermott MG, Monteiro CD, Gundersen GW, Ma'ayan A. Enrichr: a comprehensive gene set enrichment analysis web server 2016 update. Nucleic Acids Research. 2016; gkw377.\n\n- Xie Z, Bailey A, Kuleshov MV, Clarke DJB., Evangelista JE, Jenkins SL, Lachmann A, Wojciechowicz ML, Kropiwnicki E, Jagodnik KM, Jeon M, & Ma'ayan A. Gene set knowledge discovery with Enrichr. Current Protocols, 1, e90. 2021. doi: 10.1002/cpz1.90\n\n- [Enrichr](https://maayanlab.cloud/Enrichr/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gene",
        "enrichment",
        "enrichr",
        "enrichment analysis",
        "extraction tianqitang1",
        "using enrichr"
      ],
      "category": "search--data-extraction"
    },
    "tinyfish-io--agentql-mcp": {
      "owner": "tinyfish-io",
      "name": "agentql-mcp",
      "url": "https://github.com/tinyfish-io/agentql-mcp",
      "imageUrl": "",
      "description": "MCP server that provides [AgentQL](https://agentql.com)'s data extraction capabilities.",
      "stars": 115,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T15:53:06Z",
      "readme_content": "# AgentQL MCP Server\n\nThis is a Model Context Protocol (MCP) server that integrates [AgentQL](https://agentql.com)'s data extraction capabilities.\n\n## Features\n\n### Tools\n\n- `extract-web-data` - extract structured data from a given 'url', using 'prompt' as a description of actual data and its fields to extract.\n\n## Installation\n\nTo use AgentQL MCP Server to extract data from web pages, you need to install it via npm, get an API key from our [Dev Portal](https://dev.agentql.com), and configure it in your favorite app that supports MCP.\n\n### Install the package\n\n```bash\nnpm install -g agentql-mcp\n```\n\n### Configure Claude\n\n- Open Claude Desktop **Settings** via `⌘`+`,` (don't confuse with Claude Account Settings)\n- Go to **Developer** sidebar section\n- Click **Edit Config** and open `claude_desktop_config.json` file\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n- Restart the app\n\n```json title=\"claude_desktop_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Claude [here](https://modelcontextprotocol.io/quickstart/user).\n\n### Configure VS Code\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n#### Manual Installation\n\nClick the install buttons at the top of this section for the quickest installation method. For manual installation, follow these steps:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"AgentQL API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"agentql\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"agentql-mcp\"],\n        \"env\": {\n          \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"AgentQL API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n### Configure Cursor\n\n- Open **Cursor Settings**\n- Go to **MCP > MCP Servers**\n- Click **+ Add new MCP Server**\n- Enter the following:\n  - Name: \"agentql\" (or your preferred name)\n  - Type: \"command\"\n  - Command: `env AGENTQL_API_KEY=YOUR_API_KEY npx -y agentql-mcp`\n\nRead more about MCP configuration in Cursor [here](https://docs.cursor.com/context/model-context-protocol).\n\n### Configure Windsurf\n\n- Open **Windsurf: MCP Configuration Panel**\n- Click **Add custom server+**\n- Alternatively you can open `~/.codeium/windsurf/mcp_config.json` directly\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n\n```json title=\"mcp_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Windsurf [here](https://docs.codeium.com/windsurf/mcp).\n\n### Validate MCP integration\n\nGive your agent a task that will require extracting data from the web. For example:\n\n```text\nExtract the list of videos from the page https://www.youtube.com/results?search_query=agentql, every video should have a title, an author name, a number of views and a url to the video. Make sure to exclude ads items. Format this as a markdown table.\n```\n\n> [!TIP]\n> In case your agent complains that it can't open urls or load content from the web instead of using AgentQL, try adding \"use tools\" or \"use agentql tool\" hint.\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\nIf you want to try out development version, you can use the following config instead of the default one:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"/path/to/agentql-mcp/dist/index.js\",\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n> [!NOTE]\n> Don't forget to remove the default AgentQL MCP server config to not confuse Claude with two similar servers.\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agentql",
        "tinyfish",
        "mcp",
        "io agentql",
        "agentql mcp",
        "provides agentql"
      ],
      "category": "search--data-extraction"
    },
    "vitorpavinato--ncbi-mcp-server": {
      "owner": "vitorpavinato",
      "name": "ncbi-mcp-server",
      "url": "https://github.com/vitorpavinato/ncbi-mcp-server",
      "imageUrl": "",
      "description": "Comprehensive NCBI/PubMed literature search server with advanced analytics, caching, MeSH integration, related articles discovery, and batch processing for all life sciences and biomedical research.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-27T13:58:49Z",
      "readme_content": "# NCBI Literature Search MCP Server\n\nA Model Context Protocol (MCP) server for searching NCBI databases, designed for researchers across all life sciences and biomedical fields. This server provides seamless access to PubMed's vast collection of 35+ million scientific articles through natural language queries, enabling AI assistants to help with literature reviews, research discovery, and scientific analysis.\n\n## Features\n\n🔬 **Comprehensive Search**: Search PubMed's 35+ million articles across all biological disciplines\n📊 **Advanced Queries**: Support for complex searches with boolean operators, field tags, and filters  \n🧬 **Life Sciences Research**: Covers all biological and biomedical fields including genetics, ecology, medicine, and biotechnology\n💻 **Computational Biology**: Perfect for finding bioinformatics methods, algorithms, and computational tools\n🔬 **Research Applications**: Literature reviews, hypothesis generation, method discovery, and staying current with scientific advances\n📚 **Full Article Details**: Get abstracts, author lists, MeSH terms, DOIs, and publication information\n🔗 **Related Articles**: Discover relevant research through NCBI's relationship algorithms\n📖 **MeSH Integration**: Search and utilize Medical Subject Headings for precise terminology\n\n## Quick Start\n\n### Prerequisites\n- Python 3.8 or higher\n- Poetry (recommended) - [Install Poetry](https://python-poetry.org/docs/#installation)\n\n### Setup (5 minutes)\n\n1. **Create and initialize project**\n   ```bash\n   mkdir ncbi-mcp-server && cd ncbi-mcp-server\n   poetry init\n   ```\n   During init, add dependencies: `mcp`, `httpx`, `typing-extensions`\n\n2. **Create project structure**\n   ```bash\n   mkdir -p src/ncbi_mcp_server\n   # Save server.py code as src/ncbi_mcp_server/server.py\n   ```\n\n3. **Install dependencies**\n   ```bash\n   poetry install\n   ```\n\n4. **Test the server**\n   ```bash\n   poetry run python src/ncbi_mcp_server/server.py\n   ```\n\n5. **Configure Claude Desktop**\n   \n   Edit your Claude Desktop config file:\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n   - **Linux**: `~/.config/claude/claude_desktop_config.json`\n\n   Add this configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"ncbi-literature\": {\n         \"command\": \"poetry\",\n         \"args\": [\"run\", \"python\", \"src/ncbi_mcp_server/server.py\"],\n         \"cwd\": \"/FULL/PATH/TO/YOUR/ncbi-mcp-server\"\n       }\n     }\n   }\n   ```\n\n6. **Restart Claude Desktop** and start searching!\n\n### Alternative Setup Methods\n\n<details>\n<summary>Click to expand alternative installation methods</summary>\n\n#### **Conda Environment**\n```bash\nconda env create -f environment.yml\nconda activate ncbi-mcp\npython server.py\n```\n\n#### **Standard pip + venv**\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\npip install -r requirements.txt\npython server.py\n```\n</details>\n\n## Usage Examples\n\n### For Evolutionary Biology Research\n\n**Search for phylogenetic studies:**\n```\n\"Search for recent phylogenetic analysis papers on mammalian evolution\"\n→ Uses: search_pubmed with query \"phylogenetic analysis[ti] AND mammalian[ti] AND evolution\"\n```\n\n**Find computational phylogenetics methods:**\n```\n\"Find papers about maximum likelihood methods for phylogenetic reconstruction\"\n→ Uses: search_pubmed with query \"maximum likelihood[ti] AND phylogenetic reconstruction\"\n```\n\n**Search by specific organism:**\n```\n\"Find recent papers on Drosophila comparative genomics\"\n→ Uses: search_pubmed with query \"Drosophila[ti] AND comparative genomics[ti]\"\n```\n\n### For Computational Biology Research\n\n**Algorithm and method papers:**\n```\n\"Search for machine learning applications in genomics from the last 2 years\"\n→ Uses: search_pubmed with date_range=\"730\" and query \"machine learning AND genomics\"\n```\n\n**Software and database papers:**\n```\n\"Find papers about new bioinformatics tools for sequence analysis\"\n→ Uses: search_pubmed with query \"bioinformatics[ti] AND software[ti] AND sequence analysis\"\n```\n\n### Advanced Search Examples\n\n**Multi-criteria search:**\n```\n\"Find review articles about CRISPR applications in evolutionary studies published in Nature or Science\"\n→ Uses: advanced_search with terms=[\"CRISPR\", \"evolution\"], publication_types=[\"Review\"], journals=[\"Nature\", \"Science\"]\n```\n\n**Author-specific searches:**\n```\n\"Find recent papers by researchers working on ancient DNA and phylogenomics\"\n→ Uses: search_pubmed with query \"ancient DNA[ti] AND phylogenomics[ti]\"\n```\n\n## Tool Reference\n\n### `search_pubmed`\nPrimary search tool for PubMed database\n- **query**: Search terms (supports field tags like `[ti]` for title, `[au]` for author, `[mh]` for MeSH terms)\n- **max_results**: Number of results (1-100, default: 20)\n- **sort**: Sort by \"relevance\", \"pub_date\", \"author\", or \"journal\"\n- **date_range**: Limit to recent articles (\"30\", \"90\", \"365\", \"1095\" days)\n\n**Examples:**\n- `\"CRISPR[ti] AND evolution\"` - CRISPR in title AND evolution anywhere\n- `\"phylogenetic analysis[mh]\"` - Using MeSH term for phylogenetic analysis\n- `\"computational biology AND machine learning\"` - Boolean search\n\n### `get_article_details`\nFetch complete information for specific articles\n- **pmids**: List of PubMed IDs (up to 50)\n\nReturns full abstracts, author lists, MeSH terms, DOI, publication details\n\n### `search_mesh_terms`\nFind standardized Medical Subject Headings\n- **term**: Term to search in MeSH database\n\nHelps discover related concepts and improve search precision\n\n### `get_related_articles`\nDiscover articles related to a specific paper\n- **pmid**: PubMed ID of reference article\n- **max_results**: Number of related articles (1-50, default: 10)\n\nPerfect for literature reviews and finding relevant research\n\n### `advanced_search`\nComplex searches with multiple criteria\n- **terms**: List of search terms to combine\n- **operator**: \"AND\", \"OR\", or \"NOT\" to combine terms\n- **authors**: List of author names\n- **journals**: List of journal names\n- **publication_types**: \"Research Article\", \"Review\", \"Meta-Analysis\", etc.\n- **date_from/date_to**: Date range in YYYY/MM/DD format\n- **max_results**: Number of results (1-100, default: 20)\n\n## Analytics & Performance Monitoring\n\nThe NCBI MCP Server includes comprehensive analytics to help you understand your research patterns and optimize performance.\n\n### Analytics Tools\n\n#### `get_analytics_summary`\nGet comprehensive analytics overview\n```\n\"Show me my research analytics summary\"\n```\nReturns:\n- Total requests and uptime\n- Operation breakdown (searches, fetches, etc.)\n- Cache performance metrics\n- Recent activity and error rates\n- System health indicators\n\n#### `get_detailed_metrics`\nDetailed performance metrics for specific time periods\n```\n\"Get detailed metrics for the last 24 hours\"\n```\n- **hours**: Time period to analyze (default: 24)\n- Operation-specific performance data\n- Timeline analysis with hourly breakdowns\n- Error rates and response times per operation\n\n#### `reset_analytics`\nReset analytics data (use with caution)\n```\n\"Reset all analytics data\"\n```\n**Note**: This permanently clears all collected metrics.\n\n### What's Tracked\n\n**Usage Patterns:**\n- Search queries and frequency\n- Most used operations\n- Unique vs. repeated queries\n- Peak usage periods\n\n**Performance Metrics:**\n- Response times for each operation\n- Cache hit/miss rates\n- Error rates and types\n- Rate limiting efficiency\n\n**Research Insights:**\n- Popular search terms and patterns\n- Research workflow analysis\n- Literature access patterns\n- Most accessed journals and topics\n\n## Deployment\n\n### Quick Start\n\n1. **Configure credentials:**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your NCBI email and API key\n   ```\n\n2. **Choose deployment method:**\n   ```bash\n   # Local development\n   ./deploy.sh local\n   \n   # Docker deployment\n   ./deploy.sh docker\n   \n   # Production deployment\n   ./deploy.sh production\n   ```\n\n### Deployment Options\n\n#### 1. Local Development\nPerfect for development and testing:\n```bash\npoetry install\npoetry run python -m src.ncbi_mcp_server.server\n```\n\n#### 2. Docker Deployment\nRecommended for most users with two options:\n\n**Full setup with Redis (recommended):**\n```bash\n# Copy and configure environment\ncp .env.example .env\n# Edit .env with your NCBI email and API key\n\n# Start all services\ndocker-compose up -d\n```\n\n**Simple setup without Redis:**\n```bash\n# For basic usage without Redis dependencies\ncp .env.example .env\n# Edit .env with your NCBI email\n\ndocker-compose -f docker-compose.simple.yml up -d\n```\n\n**Full setup includes:**\n- NCBI MCP Server container\n- Redis cache for performance\n- Redis Commander UI (http://localhost:8081)\n\n**Simple setup includes:**\n- NCBI MCP Server container only\n- In-memory caching (no persistence)\n\n#### 3. Production Deployment\nFor production environments:\n```bash\n# Configure production settings\ncp .env.production .env\n# Edit with production values\n\n# Deploy\n./deploy.sh production\n```\n\n### Monitoring\n\n**Docker logs:**\n```bash\ndocker-compose logs -f ncbi-mcp-server\n```\n\n**Cache monitoring:**\n- Redis Commander: http://localhost:8081\n- Cache stats via MCP tool: `cache_stats()`\n\n**Health checks:**\n```bash\n# Test server health\ncurl http://localhost:8000/health\n\n# Test via MCP\npython -c \"from src.ncbi_mcp_server.server import cache_stats; import asyncio; print(asyncio.run(cache_stats()))\"\n```\n\n## Configuration\n\n### NCBI API Key (Optional but Recommended)\nFor higher rate limits and better performance:\n\n1. **Register at NCBI**: https://www.ncbi.nlm.nih.gov/account/\n2. **Get API key**: https://www.ncbi.nlm.nih.gov/account/settings/\n3. **Add to server code** in `src/ncbi_mcp_server/server.py`:\n\n```python\n# Replace the line: ncbi_client = NCBIClient()\n# With:\nncbi_client = NCBIClient(\n    email=\"your.email@university.edu\",\n    api_key=\"your_api_key_here\"\n)\n```\n\n### Rate Limits\n- **Without API key**: 3 requests/second\n- **With API key**: 10 requests/second  \n- **With API key + email**: Higher limits for bulk requests\n\n## Development Workflow\n\n### Poetry Commands\n```bash\npoetry shell              # Activate virtual environment\npoetry add package        # Add new dependency\npoetry remove package     # Remove dependency\npoetry update            # Update all dependencies\npoetry run python ...    # Run commands in environment\npoetry build             # Create distribution packages\n```\n\n### Code Quality (if you added dev dependencies)\n```bash\npoetry add --group dev black mypy pytest isort flake8\npoetry run black .       # Format code\npoetry run mypy .        # Type checking  \npoetry run pytest       # Run tests\npoetry run isort .       # Sort imports\n```\n\n### Sharing with Colleagues\n```bash\n# They just need:\ngit clone your-repo\ncd ncbi-mcp-server  \npoetry install\n# Everything works identically!\n```\n\n## Field Tags for Advanced Searches\n\nPubMed supports many field tags for precise searching:\n\n- `[ti]` - Title\n- `[tiab]` - Title and Abstract  \n- `[au]` - Author\n- `[mh]` - MeSH Terms\n- `[journal]` - Journal Name\n- `[pdat]` - Publication Date\n- `[pt]` - Publication Type\n- `[lang]` - Language\n- `[sb]` - Subset (e.g., medline, pubmed)\n\n**Example Advanced Queries:**\n```\n\"machine learning\"[ti] AND \"phylogen*\"[tiab] AND \"2020\"[pdat]:\"2024\"[pdat]\nevolutionary[mh] AND computational[ti] AND (genomics[tiab] OR proteomics[tiab])\n\"ancient DNA\"[ti] AND (paleogenomics[mh] OR phylogenomics[tiab])\n```\n\n## Research Workflow Examples\n\n### Literature Review Workflow\n1. **Start broad**: `search_pubmed(\"computational phylogenetics\")`\n2. **Refine with MeSH**: `search_mesh_terms(\"phylogenetics\")` \n3. **Find key papers**: Use publication dates and journal filters\n4. **Explore connections**: `get_related_articles(pmid=\"key_paper_id\")`\n5. **Deep dive**: `get_article_details(pmids=[\"12345\", \"67890\"])`\n\n### Staying Current\n1. **Recent methods**: `search_pubmed(\"new methods\", date_range=\"90\")`\n2. **Follow key authors**: `search_pubmed(\"author_name[au]\", sort=\"pub_date\")`\n3. **Track specific topics**: `advanced_search` with your research keywords\n\n### Method Discovery\n1. **Algorithm papers**: `search_pubmed(\"algorithm[ti] AND your_field\")`\n2. **Software tools**: `search_pubmed(\"software[ti] OR tool[ti] AND bioinformatics\")`\n3. **Benchmarking**: `search_pubmed(\"comparison[ti] OR benchmark[ti]\")`\n\n## Troubleshooting\n\n### Common Issues\n\n**Server won't start:**\n- Check Python version (3.8+ required)\n- Install dependencies: `pip install -r requirements.txt`\n- Verify file permissions\n\n**No search results:**\n- Check query syntax (use proper field tags)\n- Try broader search terms\n- Verify internet connection\n\n**Rate limit errors:**\n- Add delays between requests\n- Get NCBI API key for higher limits\n- Consider searching fewer results per query\n\n**XML parsing errors:**\n- Usually temporary NCBI server issues\n- Retry after a few seconds\n- Check NCBI status: https://www.ncbi.nlm.nih.gov/\n\n### Getting Help\n\n- **NCBI E-utilities documentation**: https://www.ncbi.nlm.nih.gov/books/NBK25499/\n- **PubMed search tips**: https://pubmed.ncbi.nlm.nih.gov/help/\n- **MeSH database**: https://www.ncbi.nlm.nih.gov/mesh/\n\n## Contributing\n\nThis MCP server is designed to grow with the research community. Ideas for enhancement:\n\n- **Additional databases**: PMC, BioRxiv, databases beyond NCBI\n- **Citation analysis**: Track paper impact and citation networks  \n- **Export formats**: BibTeX, EndNote, RIS for reference managers\n- **Saved searches**: Persistent search profiles and alerts\n- **Full-text integration**: When available through PMC\n\n## License\n\nThis project is open source. Feel free to modify and distribute according to your institution's policies.\n\n---\n\n**Perfect for researchers in:**\n- Evolutionary Biology & Phylogenetics\n- Computational Biology & Bioinformatics  \n- Molecular Evolution & Population Genetics\n- Comparative Genomics & Proteomics\n- Systems Biology & Network Analysis\n- Biostatistics & Mathematical Biology\n- Ancient DNA & Paleogenomics\n- Conservation Genetics & Ecology\n\nStart exploring the vast world of biological literature with powerful, precise searches!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ncbi",
        "biomedical",
        "pubmed",
        "ncbi pubmed",
        "comprehensive ncbi",
        "biomedical research"
      ],
      "category": "search--data-extraction"
    },
    "webscraping-ai--webscraping-ai-mcp-server": {
      "owner": "webscraping-ai",
      "name": "webscraping-ai-mcp-server",
      "url": "https://github.com/webscraping-ai/webscraping-ai-mcp-server",
      "imageUrl": "",
      "description": "Interact with [WebScraping.ai](https://webscraping.ai) for web data extraction and scraping.",
      "stars": 31,
      "forks": 11,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:32:52Z",
      "readme_content": "# WebScraping.AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [WebScraping.AI](https://webscraping.ai) for web data extraction capabilities.\n\n## Features\n\n- Question answering about web page content\n- Structured data extraction from web pages\n- HTML content retrieval with JavaScript rendering\n- Plain text extraction from web pages\n- CSS selector-based content extraction\n- Multiple proxy types (datacenter, residential) with country selection\n- JavaScript rendering using headless Chrome/Chromium\n- Concurrent request management with rate limiting\n- Custom JavaScript execution on target pages\n- Device emulation (desktop, mobile, tablet)\n- Account usage monitoring\n\n## Installation\n\n### Running with npx\n\n```bash\nenv WEBSCRAPING_AI_API_KEY=your_api_key npx -y webscraping-ai-mcp\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run\nnpm start\n```\n\n### Configuring in Cursor\nNote: Requires Cursor version 0.45.6+\n\nThe WebScraping.AI MCP server can be configured in two ways in Cursor:\n\n1. **Project-specific Configuration** (recommended for team projects):\n   Create a `.cursor/mcp.json` file in your project directory:\n   ```json\n   {\n     \"servers\": {\n       \"webscraping-ai\": {\n         \"type\": \"command\",\n         \"command\": \"npx -y webscraping-ai-mcp\",\n         \"env\": {\n           \"WEBSCRAPING_AI_API_KEY\": \"your-api-key\",\n           \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n         }\n       }\n     }\n   }\n   ```\n\n2. **Global Configuration** (for personal use across all projects):\n   Create a `~/.cursor/mcp.json` file in your home directory with the same configuration format as above.\n\n> If you are using Windows and are running into issues, try using `cmd /c \"set WEBSCRAPING_AI_API_KEY=your-api-key && npx -y webscraping-ai-mcp\"` as the command.\n\nThis configuration will make the WebScraping.AI tools available to Cursor's AI agent automatically when relevant for web scraping tasks.\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-webscraping-ai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webscraping-ai-mcp\"],\n      \"env\": {\n        \"WEBSCRAPING_AI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required\n\n- `WEBSCRAPING_AI_API_KEY`: Your WebScraping.AI API key\n  - Required for all operations\n  - Get your API key from [WebScraping.AI](https://webscraping.ai)\n\n#### Optional Configuration\n- `WEBSCRAPING_AI_CONCURRENCY_LIMIT`: Maximum number of concurrent requests (default: `5`)\n- `WEBSCRAPING_AI_DEFAULT_PROXY_TYPE`: Type of proxy to use (default: `residential`)\n- `WEBSCRAPING_AI_DEFAULT_JS_RENDERING`: Enable/disable JavaScript rendering (default: `true`)\n- `WEBSCRAPING_AI_DEFAULT_TIMEOUT`: Maximum web page retrieval time in ms (default: `15000`, max: `30000`)\n- `WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT`: Maximum JavaScript rendering time in ms (default: `2000`)\n\n### Configuration Examples\n\nFor standard usage:\n```bash\n# Required\nexport WEBSCRAPING_AI_API_KEY=your-api-key\n\n# Optional - customize behavior (default values)\nexport WEBSCRAPING_AI_CONCURRENCY_LIMIT=5\nexport WEBSCRAPING_AI_DEFAULT_PROXY_TYPE=residential # datacenter or residential\nexport WEBSCRAPING_AI_DEFAULT_JS_RENDERING=true\nexport WEBSCRAPING_AI_DEFAULT_TIMEOUT=15000\nexport WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT=2000\n```\n\n## Available Tools\n\n### 1. Question Tool (`webscraping_ai_question`)\n\nAsk questions about web page content.\n\n```json\n{\n  \"name\": \"webscraping_ai_question\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"question\": \"What is the main topic of this page?\",\n    \"timeout\": 30000,\n    \"js\": true,\n    \"js_timeout\": 2000,\n    \"wait_for\": \".content-loaded\",\n    \"proxy\": \"datacenter\",\n    \"country\": \"us\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"The main topic of this page is examples and documentation for HTML and web standards.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 2. Fields Tool (`webscraping_ai_fields`)\n\nExtract structured data from web pages based on instructions.\n\n```json\n{\n  \"name\": \"webscraping_ai_fields\",\n  \"arguments\": {\n    \"url\": \"https://example.com/product\",\n    \"fields\": {\n      \"title\": \"Extract the product title\",\n      \"price\": \"Extract the product price\",\n      \"description\": \"Extract the product description\"\n    },\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"title\": \"Example Product\",\n        \"price\": \"$99.99\",\n        \"description\": \"This is an example product description.\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. HTML Tool (`webscraping_ai_html`)\n\nGet the full HTML of a web page with JavaScript rendering.\n\n```json\n{\n  \"name\": \"webscraping_ai_html\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000,\n    \"wait_for\": \"#content-loaded\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<html>...[full HTML content]...</html>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 4. Text Tool (`webscraping_ai_text`)\n\nExtract the visible text content from a web page.\n\n```json\n{\n  \"name\": \"webscraping_ai_text\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Example Domain\\nThis domain is for use in illustrative examples in documents...\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 5. Selected Tool (`webscraping_ai_selected`)\n\nExtract content from a specific element using a CSS selector.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selector\": \"div.main-content\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<div class=\\\"main-content\\\">This is the main content of the page.</div>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 6. Selected Multiple Tool (`webscraping_ai_selected_multiple`)\n\nExtract content from multiple elements using CSS selectors.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected_multiple\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selectors\": [\"div.header\", \"div.product-list\", \"div.footer\"],\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": [\n        \"<div class=\\\"header\\\">Header content</div>\",\n        \"<div class=\\\"product-list\\\">Product list content</div>\",\n        \"<div class=\\\"footer\\\">Footer content</div>\"\n      ]\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Account Tool (`webscraping_ai_account`)\n\nGet information about your WebScraping.AI account.\n\n```json\n{\n  \"name\": \"webscraping_ai_account\",\n  \"arguments\": {}\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"requests\": 5000,\n        \"remaining\": 4500,\n        \"limit\": 10000,\n        \"resets_at\": \"2023-12-31T23:59:59Z\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Common Options for All Tools\n\nThe following options can be used with all scraping tools:\n\n- `timeout`: Maximum web page retrieval time in ms (15000 by default, maximum is 30000)\n- `js`: Execute on-page JavaScript using a headless browser (true by default)\n- `js_timeout`: Maximum JavaScript rendering time in ms (2000 by default)\n- `wait_for`: CSS selector to wait for before returning the page content\n- `proxy`: Type of proxy, datacenter or residential (residential by default)\n- `country`: Country of the proxy to use (US by default). Supported countries: us, gb, de, it, fr, ca, es, ru, jp, kr, in\n- `custom_proxy`: Your own proxy URL in \"http://user:password@host:port\" format\n- `device`: Type of device emulation. Supported values: desktop, mobile, tablet\n- `error_on_404`: Return error on 404 HTTP status on the target page (false by default)\n- `error_on_redirect`: Return error on redirect on the target page (false by default)\n- `js_script`: Custom JavaScript code to execute on the target page\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"API Error: 429 Too Many Requests\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Integration with LLMs\n\nThis server implements the [Model Context Protocol](https://github.com/facebookresearch/modelcontextprotocol), making it compatible with any MCP-enabled LLM platforms. You can configure your LLM to use these tools for web scraping tasks.\n\n### Example: Configuring Claude with MCP\n\n```javascript\nconst { Claude } = require('@anthropic-ai/sdk');\nconst { Client } = require('@modelcontextprotocol/sdk/client/index.js');\nconst { StdioClientTransport } = require('@modelcontextprotocol/sdk/client/stdio.js');\n\nconst claude = new Claude({\n  apiKey: process.env.ANTHROPIC_API_KEY\n});\n\nconst transport = new StdioClientTransport({\n  command: 'npx',\n  args: ['-y', 'webscraping-ai-mcp'],\n  env: {\n    WEBSCRAPING_AI_API_KEY: 'your-api-key'\n  }\n});\n\nconst client = new Client({\n  name: 'claude-client',\n  version: '1.0.0'\n});\n\nawait client.connect(transport);\n\n// Now you can use Claude with WebScraping.AI tools\nconst tools = await client.listTools();\nconst response = await claude.complete({\n  prompt: 'What is the main topic of example.com?',\n  tools: tools\n});\n```\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run tests\nnpm test\n\n# Add your .env file\ncp .env.example .env\n\n# Start the inspector\nnpx @modelcontextprotocol/inspector node src/index.js\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webscraping",
        "scraping",
        "web",
        "ai webscraping",
        "extraction webscraping",
        "webscraping ai"
      ],
      "category": "search--data-extraction"
    },
    "yamanoku--baseline-mcp-server": {
      "owner": "yamanoku",
      "name": "baseline-mcp-server",
      "url": "https://github.com/yamanoku/baseline-mcp-server",
      "imageUrl": "",
      "description": "MCP server that searches Baseline status using Web Platform API",
      "stars": 35,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-12T01:47:35Z",
      "readme_content": "<p align=\"center\">\n\t\n</p>\n\n<h1 align=\"center\">Baseline MCP Server</h1>\n\n[日本語版](./README.md) | [English Version](./README_EN.md)\n\nWeb Platform APIのサポート状況を提供するModel Context Protocolサーバーです。\n\n[![JSR Version](https://jsr.io/badges/@yamanoku/baseline-mcp-server)](https://jsr.io/@yamanoku/baseline-mcp-server)\n\n## 概要\n\nこのサーバーは、[Web Platform Dashboard](https://webstatus.dev/)のAPIを使用して、WebのAPI機能のBaselineステータス（サポート状況）を取得できるMCPサーバーを実装しています。クエリに基づいてWeb機能の情報を取得し、その結果をMCPクライアントに返します。\n\n\n\n## 機能\n\n- Web Platform DashboardのAPIを使用した機能検索\n- 機能のBaselineステータス（`widely`、`newly`、`limited`、`no_data`）の提供\n- ブラウザ実装状況（バージョンと実装日）の提供\n- 機能の使用状況データの提供\n- 特定のブラウザを除外した機能検索（`chrome`, `edge`, `firefox`, `safari`）\n- MCPを介した各種AIモデルとの連携\n\n## Baselineステータスについて\n\nBaselineステータスは、Web機能のブラウザサポート状況を示します：\n\n- **widely**:\n  広くサポートされているWeb標準機能。ほとんどのブラウザで安全に使用できます。\n- **newly**:\n  新しく標準化されたWeb機能。主要なブラウザでサポートされ始めていますが、まだ普及途上です。\n- **limited**:\n  限定的にサポートされているWeb機能。一部のブラウザでは使用できないか、フラグが必要な場合があります。\n- **no_data**:\n  現時点ではBaselineに含まれていないWeb機能。ブラウザのサポート状況を個別に確認する必要があります。\n\nBaselineについての詳細については「[Baseline (互換性) - MDN Web Docs 用語集](https://developer.mozilla.org/ja/docs/Glossary/Baseline/Compatibility)」を参照してください。\n\n## MCPクライアントでの設定\n\n- サーバーを起動するにあたり、Denoの使用を推奨します\n  - パーミッションとして`api.webstatus.dev`のみのアクセスを許可してください\n- [`@yamanoku/baseline-mcp-server`](https://jsr.io/@yamanoku/baseline-mcp-server)を指定するか、お手元のローカル環境にbaseline-mcp-server.tsを設置して読み取るように設定してください\n\n### Claude Desktop\n\nClaude\nDesktopのMCPクライアントで使用するには、以下のように`cline_mcp_settings.json`に設定を追加します。\n\n```json\n{\n  \"mcpServers\": {\n    \"baseline-mcp-server\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-net=api.webstatus.dev\",\n        \"jsr:@yamanoku/baseline-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n### Visual Studio Code\n\nVisual Studio\nCodeのMCPクライアントで使用するには、以下のように`settings.json`に設定を追加します。\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"baseline-mcp-server\": {\n        \"command\": \"deno\",\n        \"args\": [\n          \"run\",\n          \"--allow-net=api.webstatus.dev\",\n          \"jsr:@yamanoku/baseline-mcp-server\"\n        ]\n      }\n    }\n  }\n}\n```\n\n## Dockerによる起動\n\n最初にDockerイメージをビルドします。\n\n```shell\ndocker build -t baseline-mcp-server .\n```\n\nMCPクライアントの設定でDockerコンテナを実行するようにします。\n\n```json\n{\n  \"mcpServers\": {\n    \"baseline-mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"baseline-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n## 謝辞\n\nこのOSSはGPT-4o Image Generationによってロゴを製作、Claude 3.7\nSonnetによって実装、ドキュメントのサンプルを提案いただきました。感謝申し上げます。\n\n## ライセンス\n\n[MIT License](./LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "mcp",
        "search",
        "searches baseline",
        "server searches",
        "server mcp"
      ],
      "category": "search--data-extraction"
    }
  }
}