{
  "category": "open-source-data",
  "categoryDisplay": "Open Source Data",
  "description": "",
  "totalRepositories": 7,
  "repositories": {
    "anshumax--world_bank_mcp_server": {
      "owner": "anshumax",
      "name": "world_bank_mcp_server",
      "url": "https://github.com/anshumax/world_bank_mcp_server",
      "imageUrl": "https://github.com/anshumax.png",
      "description": "Enables interaction with the open World Bank data API to provide access to various indicators and analysis for different countries.",
      "stars": 40,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-04T11:58:56Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/anshumax-world-bank-mcp-server-badge.png)](https://mseep.ai/app/anshumax-world-bank-mcp-server)\n\n# World Bank MCP Server\n[![smithery badge](https://smithery.ai/badge/@anshumax/world_bank_mcp_server)](https://smithery.ai/server/@anshumax/world_bank_mcp_server)\n\nA Model Context Protocol (MCP) server that enables interaction with the open World Bank data API. This server allows AI assistants to list indicators and analyse those indicators for the countries that are available with the World Bank.\n\n## Features\n\n- List available countries in the World Bank open data API\n- List available indicators in the World Bank open data API\n- Analyse indicators, such as population segments, poverty numbers etc, for countries\n- Comprehensive logging\n\n\n## Usage\n\n### With Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"world_bank\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"path/to/world_bank_mcp_server\",\n        \"run\",\n        \"world_bank_mcp_server\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install World Bank Data Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@anshumax/world_bank_mcp_server):\n\n```bash\nnpx -y @smithery/cli install @anshumax/world_bank_mcp_server --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "doanbactam--code": {
      "owner": "doanbactam",
      "name": "code",
      "url": "https://github.com/doanbactam/code",
      "imageUrl": "https://github.com/doanbactam.png",
      "description": "Connect to a community-driven directory of open source alternatives to proprietary software. Discover, explore, and contribute to a collection of open source services that facilitate business growth.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-04-28T07:21:52Z",
      "readme_content": "![m4v](https://m4v.co/opengraph.png)\n\n<p align=\"center\"></p>\n\n<p align=\"center\">\n  Discover open source alternatives to popular software.\n  <br />\n  <a href=\"https://m4v.co\"><strong>Learn more Â»</strong></a>\n  <br />\n  <br />\n  <a href=\"https://m4v.co\">Website</a>\n  Â·\n  <a href=\"https://github.com/piotrkulpinski/m4v/issues\">Issues</a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://github.com/piotrkulpinski/m4v/stargazers\"><img src=\"https://img.shields.io/github/stars/piotrkulpinski/m4v\" alt=\"Github Stars\" /></a>\n   <a href=\"https://uptime.betterstack.com/?utm_source=status_badge\"><img src=\"https://uptime.betterstack.com/status-badges/v1/monitor/1lyos.svg\" alt=\"Better Stack\" /></a>\n   <a href=\"https://github.com/piotrkulpinski/m4v/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/piotrkulpinski/m4v\" alt=\"License\" /></a>\n   <a href=\"https://github.com/piotrkulpinski/m4v/pulse\"><img src=\"https://img.shields.io/github/commit-activity/m/piotrkulpinski/m4v\" alt=\"Commits-per-month\" /></a>\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/piotrkulpinski/m4v\">\n   <img alt=\"open in devcontainer\" src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Enabled&color=blue&logo=visualstudiocode\" />\n   </a>\n   <a href=\"https://news.ycombinator.com/item?id=39639386\"><img src=\"https://img.shields.io/badge/Hacker%20News-156-%23FF6600\" alt=\"Hacker News\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.producthunt.com/posts/m4v?utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-m4v\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=443404&theme=light&period=daily\" alt=\"m4v - Discover open source alternatives to popular software | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n  <a href=\"https://www.producthunt.com/posts/m4v?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-m4v\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=443404&theme=light\" alt=\"m4v - Discover open source alternatives to popular software | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n</p>\n\n## About this project\n\nm4v is a community driven list of **open source alternatives to proprietary software** and applications.\n\nOur goal is to be your first stop when researching for a new open source service to help you grow your business. We will help you **find alternatives** of the products you already use.\n\nJoin us in creating the biggest **directory of open source software**.\n\n## Sponsoring\n\nm4v is an GPL-3.0-licensed open source project with its ongoing development made possible entirely by the support of these awesome backers. If you'd like to join them, please consider [sponsoring m4v's development](https://github.com/sponsors/piotrkulpinski).\n\nIf you'd like to support the project, you could also consider [buying our Next.js boilerplate](https://dirstarter.com) which is the foundation of creating directory websites, just like this one.\n\n## Services\n\nm4v uses the following third-party services:\n\n- Database: [Neon](https://neon.tech)\n- Analytics: [Plausible](https://plausible.io), [PostHog](https://posthog.com)\n- Newsletter: [Beehiiv](https://go.m4v.co/beehiiv)\n- Background Jobs: [Inngest](https://inngest.com)\n- File Storage: [AWS S3](https://aws.amazon.com/s3)\n- Payments: [Stripe](https://stripe.com)\n- Screenshots: [ScreenshotOne](https://go.m4v.co/screenshotone)\n\nMake sure to set up accounts with these services and add the necessary environment variables to your `.env` file.\n\n## Project Structure\n\nm4v is built as a Turborepo monorepo with multiple packages. The project structure is organized as follows:\n\n- `/apps` - Turborepo apps\n  - `/app` - Main Next.js application using the App Router architecture\n    - `/app` - Application routes and layouts (Next.js App Router)\n    - `/components` - Reusable React components\n    - `/lib` - Core utilities and business logic\n    - `/actions` - Server actions\n    - `/utils` - Helper functions and utilities\n    - `/hooks` - React hooks\n    - `/contexts` - React context providers\n    - `/services` - Service integrations\n    - `/emails` - Email templates\n    - `/server` - Server-side code\n    - `/functions` - Utility functions\n    - `/config` - Configuration files\n    - `/content` - Content management\n    - `/types` - TypeScript type definitions\n    - `/public` - Static assets\n\n  - `/analyzer` - Data analysis tools\n\n- `/packages` - Shared packages\n  - `/db` - Database schema and utilities\n  - `/github` - GitHub integration utilities\n\nThe project uses Turborepo for task orchestration and dependency management across the monorepo.\n\n## Development\n\nThis project uses [Bun](https://bun.sh/) as the package manager and runtime. Make sure you have Bun installed before proceeding.\n\nTo set up the project for development:\n\n1. Clone the repository\n2. Run `bun install` to install all dependencies\n3. Set up the required environment variables (see below)\n4. Run `bun run db:push` to push the Prisma schema to the database\n5. Run `bun run dev` to start the application in development mode\n\n### Environment Variables\n\nRefer to the `.env.example` file for a complete list of required variables.\n\nCopy the `.env.example` file to `.env` and update the variables as needed:\n\n```bash\ncp .env.example .env\n```\n\n## ðŸ§ž Commands\n\nAll commands are run from the root of the project, from a terminal:\n\n| Command           | Action                                           |\n| :---------------- | :----------------------------------------------- |\n| `bun install`     | Installs dependencies                            |\n| `bun run dev`     | Starts local dev server at `localhost:5173`      |\n| `bun run build`   | Build production application                     |\n| `bun run start`   | Preview production build locally                 |\n| `bun run lint`    | Run linter                                       |\n| `bun run format`  | Format code                                      |\n| `bun run typecheck` | Run TypeScript type checking                   |\n| `bun run db:generate` | Generate Prisma client                       |\n| `bun run db:studio` | Start Prisma Studio                           |\n| `bun run db:push` | Push Prisma schema to database                  |\n| `bun run db:pull` | Pull Prisma schema from database                |\n| `bun run db:reset` | Reset Prisma schema                            |\n\n## Deployment\n\nThe project is set up for deployment on Vercel. To deploy manually:\n\n1. Build the project: `bun run build`\n2. Start the production server: `bun run start`\n\nEnsure all environment variables are properly set in your production environment.\n\n## License\n\nm4v is licensed under the [GPL-3.0 License](LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "Geeksfino--kb-mcp-server": {
      "owner": "Geeksfino",
      "name": "kb-mcp-server",
      "url": "https://github.com/Geeksfino/kb-mcp-server",
      "imageUrl": "https://github.com/Geeksfino.png",
      "description": "Provides semantic search capabilities, builds and queries knowledge graphs, and facilitates AI-driven text processing through a standardized interface. Integrates vector databases with relational databases for enhanced data utilization.",
      "stars": 49,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-31T17:38:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/geeksfino-kb-mcp-server-badge.png)](https://mseep.ai/app/geeksfino-kb-mcp-server)\n\n# Embedding MCP Server\n\nA Model Context Protocol (MCP) server implementation powered by txtai, providing semantic search, knowledge graph capabilities, and AI-driven text processing through a standardized interface.\n\n## The Power of txtai: All-in-one Embeddings Database\n\nThis project leverages [txtai](https://github.com/neuml/txtai), an all-in-one embeddings database for RAG leveraging semantic search, knowledge graph construction, and language model workflows. txtai offers several key advantages:\n\n- **Unified Vector Database**: Combines vector indexes, graph networks, and relational databases in a single platform\n- **Semantic Search**: Find information based on meaning, not just keywords\n- **Knowledge Graph Integration**: Automatically build and query knowledge graphs from your data\n- **Portable Knowledge Bases**: Save entire knowledge bases as compressed archives (.tar.gz) that can be easily shared and loaded\n- **Extensible Pipeline System**: Process text, documents, audio, images, and video through a unified API\n- **Local-first Architecture**: Run everything locally without sending data to external services\n\n## How It Works\n\nThe project contains a knowledge base builder tool and a MCP server. The knowledge base builder tool is a command-line interface for creating and managing knowledge bases. The MCP server provides a standardized interface to access the knowledge base. \n\nIt is not required to use the knowledge base builder tool to build a knowledge base. You can always build a knowledge base using txtai's programming interface by writing a Python script or even using a jupyter notebook. As long as the knowledge base is built using txtai, it can be loaded by the MCP server. Better yet, the knowledge base can be a folder on the file system or an exported .tar.gz file. Just give it to the MCP server and it will load it.\n\n### 1. Build a Knowledge Base with kb_builder\n\nThe `kb_builder` module provides a command-line interface for creating and managing knowledge bases:\n\n- Process documents from various sources (files, directories, JSON)\n- Extract text and create embeddings\n- Build knowledge graphs automatically\n- Export portable knowledge bases\n\nNote it is possibly limited in functionality and currently only provided for convenience.\n\n### 2. Start the MCP Server\n\nThe MCP server provides a standardized interface to access the knowledge base:\n\n- Semantic search capabilities\n- Knowledge graph querying and visualization\n- Text processing pipelines (summarization, extraction, etc.)\n- Full compliance with the Model Context Protocol\n\n## Installation\n\n### Recommended: Using uv with Python 3.10+\n\nWe recommend using [uv](https://github.com/astral-sh/uv) with Python 3.10 or newer for the best experience. This provides better dependency management and ensures consistent behavior.\n\n```bash\n# Install uv if you don't have it already\npip install -U uv\n\n# Create a virtual environment with Python 3.10 or newer\nuv venv --python=3.10  # or 3.11, 3.12, etc.\n\n# Activate the virtual environment (bash/zsh)\nsource .venv/bin/activate\n# For fish shell\n# source .venv/bin/activate.fish\n\n# Install from PyPI\nuv pip install kb-mcp-server\n```\n\n> **Note**: We pin transformers to version 4.49.0 to avoid deprecation warnings about `transformers.agents.tools` that appear in version 4.50.0 and newer. If you use a newer version of transformers, you may see these warnings, but they don't affect functionality.\n\n### Using conda\n\n```bash\n# Create a new conda environment (optional)\nconda create -n embedding-mcp python=3.10\nconda activate embedding-mcp\n\n# Install from PyPI\npip install kb-mcp-server\n```\n\n### From Source\n\n```bash\n# Create a new conda environment\nconda create -n embedding-mcp python=3.10\nconda activate embedding-mcp\n\n# Clone the repository\ngit clone https://github.com/Geeksfino/kb-mcp-server.git.git\ncd kb-mcp-server\n\n# Install dependencies\npip install -e .\n```\n\n### Using uv (Faster Alternative)\n\n```bash\n# Install uv if not already installed\npip install uv\n\n# Create a new virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Option 1: Install from PyPI\nuv pip install kb-mcp-server\n\n# Option 2: Install from source (for development)\nuv pip install -e .\n```\n\n### Using uvx (No Installation Required)\n\n[uvx](https://github.com/astral-sh/uv) allows you to run packages directly from PyPI without installing them:\n\n```bash\n# Run the MCP server\nuvx --from kb-mcp-server@0.3.0 kb-mcp-server --embeddings /path/to/knowledge_base\n\n# Build a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --config config.yml\n\n# Search a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"Your search query\"\n```\n\n## Command Line Usage\n\n### Building a Knowledge Base\n\nYou can use the command-line tools installed from PyPI, the Python module directly, or the convenient shell scripts:\n\n#### Using the PyPI Installed Commands\n\n```bash\n# Build a knowledge base from documents\nkb-build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\nkb-build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\nkb-build --input /path/to/documents --export my_knowledge_base.tar.gz\n\n# Search a knowledge base\nkb-search /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\nkb-search /path/to/knowledge_base \"What is machine learning?\" --graph --limit 10\n```\n\n#### Using uvx (No Installation Required)\n\n```bash\n# Build a knowledge base from documents\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\nuvx --from kb-mcp-server@0.3.0 kb-build --input /path/to/documents --export my_knowledge_base.tar.gz\n\n# Search a knowledge base\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\nuvx --from kb-mcp-server@0.3.0 kb-search /path/to/knowledge_base \"What is machine learning?\" --graph --limit 10\n```\n\n#### Using the Python Module\n\n```bash\n# Build a knowledge base from documents\npython -m kb_builder build --input /path/to/documents --config config.yml\n\n# Update an existing knowledge base with new documents\npython -m kb_builder build --input /path/to/new_documents --update\n\n# Export a knowledge base for portability\npython -m kb_builder build --input /path/to/documents --export my_knowledge_base.tar.gz\n```\n\n#### Using the Convenience Scripts\n\nThe repository includes convenient wrapper scripts that make it easier to build and search knowledge bases:\n\n```bash\n# Build a knowledge base using a template configuration\n./scripts/kb_build.sh /path/to/documents technical_docs\n\n# Build using a custom configuration file\n./scripts/kb_build.sh /path/to/documents /path/to/my_config.yml\n\n# Update an existing knowledge base\n./scripts/kb_build.sh /path/to/documents technical_docs --update\n\n# Search a knowledge base\n./scripts/kb_search.sh /path/to/knowledge_base \"What is machine learning?\"\n\n# Search with graph enhancement\n./scripts/kb_search.sh /path/to/knowledge_base \"What is machine learning?\" --graph\n```\n\nRun `./scripts/kb_build.sh --help` or `./scripts/kb_search.sh --help` for more options.\n\n### Starting the MCP Server\n\n#### Using the PyPI Installed Command\n\n```bash\n# Start with a specific knowledge base folder\nkb-mcp-server --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\nkb-mcp-server --embeddings /path/to/knowledge_base.tar.gz\n```\n\n#### Using uvx (No Installation Required)\n\n```bash\n# Start with a specific knowledge base folder\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base.tar.gz\n```\n\n#### Using the Python Module\n\n```bash\n# Start with a specific knowledge base folder\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base_folder\n\n# Start with a given knowledge base archive\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base.tar.gz\n```\n## MCP Server Configuration\n\nThe MCP server is configured using environment variables or command-line arguments, not YAML files. YAML files are only used for configuring txtai components during knowledge base building.\n\nHere's how to configure the MCP server:\n\n```bash\n# Start the server with command-line arguments\nkb-mcp-server --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or using uvx (no installation required)\nuvx kb-mcp-server@0.2.6 --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or using the Python module\npython -m txtai_mcp_server --embeddings /path/to/knowledge_base --host 0.0.0.0 --port 8000\n\n# Or use environment variables\nexport TXTAI_EMBEDDINGS=/path/to/knowledge_base\nexport MCP_SSE_HOST=0.0.0.0\nexport MCP_SSE_PORT=8000\npython -m txtai_mcp_server\n```\n\nCommon configuration options:\n- `--embeddings`: Path to the knowledge base (required)\n- `--host`: Host address to bind to (default: localhost)\n- `--port`: Port to listen on (default: 8000)\n- `--transport`: Transport to use, either 'sse' or 'stdio' (default: stdio)\n- `--enable-causal-boost`: Enable causal boost feature for enhanced relevance scoring\n- `--causal-config`: Path to custom causal boost configuration YAML file\n\n## Configuring LLM Clients to Use the MCP Server\n\nTo configure an LLM client to use the MCP server, you need to create an MCP configuration file. Here's an example `mcp_config.json`:\n\n### Using the server directly\n\nIf you use a virtual Python environment to install the server, you can use the following configuration - note that MCP host like Claude will not be able to connect to the server if you use a virtual environment, you need to use the absolute path to the Python executable of the virtual environment where you did \"pip install\" or \"uv pip install\", for example\n\n```json\n{\n  \"mcpServers\": {\n    \"kb-server\": {\n      \"command\": \"/your/home/project/.venv/bin/kb-mcp-server\",\n      \"args\": [\n        \"--embeddings\", \n        \"/path/to/knowledge_base.tar.gz\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n  }\n}\n```\n\n### Using system default Python\n\nIf you use your system default Python, you can use the following configuration:\n\n```json\n{\n    \"rag-server\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\",\n        \"txtai_mcp_server\",\n        \"--embeddings\",\n        \"/path/to/knowledge_base.tar.gz\",\n        \"--enable-causal-boost\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n}\n```\n\nAlternatively, if you're using uvx, assuming you have uvx installed in your system via \"brew install uvx\" etc, or you 've installed uvx and made it globally accessible via:\n```\n# Create a symlink to /usr/local/bin (which is typically in the system PATH)\nsudo ln -s /Users/cliang/.local/bin/uvx /usr/local/bin/uvx\n```\nThis creates a symbolic link from your user-specific installation to a system-wide location. For macOS applications like Claude Desktop, you can modify the system-wide PATH by creating or editing a launchd configuration file:\n```\n# Create a plist file to set environment variables for all GUI applications\nsudo nano /Library/LaunchAgents/environment.plist\n```\nAdd this content:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  <key>Label</key>\n  <string>my.startup</string>\n  <key>ProgramArguments</key>\n  <array>\n    <string>sh</string>\n    <string>-c</string>\n    <string>launchctl setenv PATH $PATH:/Users/cliang/.local/bin</string>\n  </array>\n  <key>RunAtLoad</key>\n  <true/>\n</dict>\n</plist>\n```\n\nThen load it:\n```\nsudo launchctl load -w /Library/LaunchAgents/environment.plist\n```\nYou'll need to restart your computer for this to take effect, though.\n\n\n```json\n{\n  \"mcpServers\": {\n    \"kb-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"kb-mcp-server@0.2.6\",\n        \"--embeddings\", \"/path/to/knowledge_base\",\n        \"--host\", \"localhost\",\n        \"--port\", \"8000\"\n      ],\n      \"cwd\": \"/path/to/working/directory\"\n    }\n  }\n}\n```\n\nPlace this configuration file in a location accessible to your LLM client and configure the client to use it. The exact configuration steps will depend on your specific LLM client.\n\n## Advanced Knowledge Base Configuration\n\nBuilding a knowledge base with txtai requires a YAML configuration file that controls various aspects of the embedding process. This configuration is used by the `kb_builder` tool, not the MCP server itself.\n\nOne may need to tune segmentation/chunking strategies, embedding models, and scoring methods, as well as configure graph construction, causal boosting, weights of hybrid search, and more.\n\nFortunately, txtai provides a powerful YAML configuration system that requires no coding. Here's an example of a comprehensive configuration for knowledge base building:\n\n```yaml\n# Path to save/load embeddings index\npath: ~/.txtai/embeddings\nwritable: true\n\n# Content storage in SQLite\ncontent:\n  path: sqlite:///~/.txtai/content.db\n\n# Embeddings configuration\nembeddings:\n  # Model settings\n  path: sentence-transformers/nli-mpnet-base-v2\n  backend: faiss\n  gpu: true\n  batch: 32\n  normalize: true\n  \n  # Scoring settings\n  scoring: hybrid\n  hybridalpha: 0.75\n\n# Pipeline configuration\npipeline:\n  workers: 2\n  queue: 100\n  timeout: 300\n\n# Question-answering pipeline\nextractor:\n  path: distilbert-base-cased-distilled-squad\n  maxlength: 512\n  minscore: 0.3\n\n# Graph configuration\ngraph:\n  backend: sqlite\n  path: ~/.txtai/graph.db\n  similarity: 0.75  # Threshold for creating graph connections\n  limit: 10  # Maximum connections per node\n```\n\n### Configuration Examples\n\nThe `src/kb_builder/configs` directory contains configuration templates for different use cases and storage backends:\n\n#### Storage and Backend Configurations\n- `memory.yml`: In-memory vectors (fastest for development, no persistence)\n- `sqlite-faiss.yml`: SQLite for content + FAISS for vectors (local file-based persistence)\n- `postgres-pgvector.yml`: PostgreSQL + pgvector (production-ready with full persistence)\n\n#### Domain-Specific Configurations\n- `base.yml`: Base configuration template\n- `code_repositories.yml`: Optimized for code repositories\n- `data_science.yml`: Configured for data science documents\n- `general_knowledge.yml`: General purpose knowledge base\n- `research_papers.yml`: Optimized for academic papers\n- `technical_docs.yml`: Configured for technical documentation\n\nYou can use these as starting points for your own configurations:\n\n```bash\npython -m kb_builder build --input /path/to/documents --config src/kb_builder/configs/technical_docs.yml\n\n# Or use a storage-specific configuration\npython -m kb_builder build --input /path/to/documents --config src/kb_builder/configs/postgres-pgvector.yml\n```\n\n## Advanced Features\n\n### Knowledge Graph Capabilities\n\nThe MCP server leverages txtai's built-in graph functionality to provide powerful knowledge graph capabilities:\n\n- **Automatic Graph Construction**: Build knowledge graphs from your documents automatically\n- **Graph Traversal**: Navigate through related concepts and documents\n- **Path Finding**: Discover connections between different pieces of information\n- **Community Detection**: Identify clusters of related information\n\n### Causal Boosting Mechanism\n\nThe MCP server includes a sophisticated causal boosting mechanism that enhances search relevance by identifying and prioritizing causal relationships:\n\n- **Pattern Recognition**: Detects causal language patterns in both queries and documents\n- **Multilingual Support**: Automatically applies appropriate patterns based on detected query language\n- **Configurable Boost Multipliers**: Different types of causal matches receive customizable boost factors\n- **Enhanced Relevance**: Results that explain causal relationships are prioritized in search results\n\nThis mechanism significantly improves responses to \"why\" and \"how\" questions by surfacing content that explains relationships between concepts. The causal boosting configuration is highly customizable through YAML files, allowing adaptation to different domains and languages.\n\n\n## License\n\nMIT License - see LICENSE file for details\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "joaowinderfeldbussolotto--MCP-Websearch-Server": {
      "owner": "joaowinderfeldbussolotto",
      "name": "MCP-Websearch-Server",
      "url": "https://github.com/joaowinderfeldbussolotto/MCP-Websearch-Server",
      "imageUrl": "https://github.com/joaowinderfeldbussolotto.png",
      "description": "Fetches relevant documentation snippets from Langchain, Llama Index, and OpenAI to enhance search capabilities. Provides a simple tool to retrieve information based on user queries.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-28T01:38:25Z",
      "readme_content": "## MPC Docs Server\n\nThis is a simple MCP (Model Context Protocol) server for retrieving information from the official documentation of Langchain, Llama Index, and OpenAI. It provides a tool that can be used by MCP-compatible applications to search and retrieve relevant documentation snippets.\n\n## Features\n\n-   **Documentation Retrieval:** Fetches content from the official documentation of Langchain, Llama Index, and OpenAI.\n-   **MCP Compatibility:** Implements an MCP server, allowing it to be easily integrated with other MCP-compatible applications.\n-   **Simple Tool:** Exposes a `get_docs` tool that accepts a query and library name, returning relevant documentation snippets.\n\n## How It Works\n\n```mermaid\ngraph LR\n    Client[MCP Client] -->|Calls tools| Server[MCP Server]\n    Server -->|Searches web for docs| Serper[Serper API]\n    Serper -->|Returns search results| Server\n    Server -->|Returns documentation| Client\n```\n\n## Getting Started\n\n### Installing uv Package Manager\n\n**On MacOS/Linux:**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nMake sure to restart your terminal afterwards to ensure that the `uv` command gets picked up.\n\n### Project Setup\n\nCreate and initialize the project:\n```bash\n# Create a new directory for our project\nuv init mcp-server\ncd mcp-server\n\n# Create virtual environment and activate it\nuv venv\nsource .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate\n\n# Install dependencies\nuv add \"mcp[cli]\" httpx python-dotenv bs4\n```\n\n\n### Environment Variables\n\nCreate a `.env` file in the root directory and add the following:\n\n```\nSERPER_API_KEY=YOUR_SERPER_API_KEY\n```\n\nYou'll need a SERPER API key to use the web search functionality. You can obtain one from [Serper.dev](https://serper.dev/). We are using the Serper API to search the web for relevant documentation.\n\n### Running the Server\n\nStart the MCP server:\n```bash\nuv run main.py\n```\n\nThe server will start and be ready to accept connections.\n\n\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "melaodoidao--datagov-mcp-server": {
      "owner": "melaodoidao",
      "name": "datagov-mcp-server",
      "url": "https://github.com/melaodoidao/datagov-mcp-server",
      "imageUrl": "https://github.com/melaodoidao.png",
      "description": "Access and retrieve government datasets from Data.gov, enabling interactions with a wide variety of public data resources in real-time.",
      "stars": 17,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-28T16:13:06Z",
      "readme_content": "# Data.gov MCP Server\n\nAn MCP server for accessing data from Data.gov, providing tools and resources for interacting with government datasets.\n\n<a href=\"https://glama.ai/mcp/servers/qi5i2d6sen\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/qi5i2d6sen/badge\" alt=\"Data.gov Server MCP server\" />\n</a>\n\n## Installation\n\n1.  **Install the package globally:**\n\n    ```bash\n    npm install -g @melaodoidao/datagov-mcp-server\n    ```\n\n2. **Configure the MCP Server:**\n\n   - Add the following entry to your `cline_mcp_settings.json` file (usually located in `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/` on macOS):\n\n     ```json\n      {\n        \"mcpServers\": {\n          \"datagov\": {\n            \"command\": \"datagov-mcp-server\",\n            \"args\": [],\n            \"env\": {}\n          }\n        }\n      }\n     ```\n    - If you are using the Claude Desktop app, add the entry to `~/Library/Application Support/Claude/claude_desktop_config.json` instead.\n\n## Usage\n\nThis server provides the following tools:\n\n*   `package_search`: Search for packages (datasets) on Data.gov.\n*   `package_show`: Get details for a specific package (dataset).\n*   `group_list`: List groups on Data.gov.\n*   `tag_list`: List tags on Data.gov.\n\nIt also provides the following resource template:\n\n*   `datagov://resource/{url}`: Access a Data.gov resource by its URL.\n\nYou can use these tools and resources with Cline by specifying the server name (`datagov-mcp-server`) and the tool/resource name.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit issues or pull requests.\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0
    },
    "OpenDataMCP--OpenDataMCP": {
      "owner": "OpenDataMCP",
      "name": "OpenDataMCP",
      "url": "https://github.com/OpenDataMCP/OpenDataMCP",
      "imageUrl": "https://github.com/OpenDataMCP.png",
      "description": "Python MCP servers that provide access to Open Data for large language model (LLM) clients, facilitating the integration of various data sources.",
      "stars": 134,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T20:00:48Z",
      "readme_content": "# Open Data Model Context Protocol\n\n![vc3598_Hyper-realistic_Swiss_landscape_pristine_SBB_red_train_p_40803c2e-43f5-410e-89aa-f6bdcb4cd089](https://github.com/user-attachments/assets/80c823dd-0b26-4d06-98f9-5c6d7c9103de)\n<p align=\"center\">\n    <em>Connect Open Data to LLMs in minutes!</em>\n</p>\n<p align=\"center\">\n   <a href=\"https://github.com/OpenDataMCP/OpenDataMCP/actions/workflows/ci.yml\" target=\"_blank\">\n    <img src=\"https://github.com/OpenDataMCP/OpenDataMCP/actions/workflows/ci.yml/badge.svg\" alt=\"CI\">\n   </a>\n   <a href=\"https://pypi.org/project/odmcp\" target=\"_blank\">\n       <img src=\"https://img.shields.io/pypi/v/odmcp?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n   </a>\n   <a href=\"https://github.com/OpenDataMCP/OpenDataMCP/blob/main/LICENSE\" target=\"_blank\">\n      <img src=\"https://img.shields.io/github/license/OpenDataMCP/OpenDataMCP.svg\" alt=\"License\">\n   </a>\n   <a href=\"https://pepy.tech/badge/odmcp\" target=\"_blank\">\n      <img src=\"https://pepy.tech/badge/odmcp?cache-control=no-cache\" alt=\"License\">\n   </a>\n   <a href=\"https://github.com/OpenDataMCP/OpenDataMCP/stargazers\" target=\"_blank\">\n      <img src=\"https://img.shields.io/github/stars/OpenDataMCP/OpenDataMCP.svg?cache-control=no-cache\" alt=\"Stars\">\n   </a>\n</p>\n\n## See it in action\n\nhttps://github.com/user-attachments/assets/760e1a16-add6-49a1-bf71-dfbb335e893e\n\nWe enable 2 things: \n\n* **Open Data Access**: Access to many public datasets right from your LLM application (starting with Claude, more to come).\n* **Publishing**: Get community help and a distribution network to distribute your Open Data. Get everyone to use it!\n\nHow do we do that?\n\n* **Access**: Setup our MCP servers in your LLM application in 2 clicks via our CLI tool (starting with Claude, see Roadmap for next steps).\n* **Publish**: Use provided templates and guidelines to quickly contribute and publish on Open Data MCP. Make your data easily discoverable!\n\n## Usage\n\n### <u>Access</u>: Access Open Data using Open Data MCP CLI Tool\n\n#### Prerequisites\n\nIf you want to use Open Data MCP with Claude Desktop app client you need to install the [Claude Desktop app](https://claude.ai/download).\n\nYou will also need `uv` to easily run our CLI and MCP servers.\n\n##### macOS\n\n```bash\n# you need to install uv through homebrew as using the install shell script \n# will install it locally to your user which make it unavailable in the Claude Desktop app context.\nbrew install uv\n```\n\n##### Windows\n\n```bash\n# (UNTESTED)\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n#### Open Data MCP - CLI Tool\n\n##### Overview\n\n```bash\n# show available commands\nuvx odmcp \n\n# show available providers\nuvx odmcp list\n\n# show info about a provider\nuvx odmcp info $PROVIDER_NAME\n\n# setup a provider's MCP server on your Claude Desktop app\nuvx odmcp setup $PROVIDER_NAME\n\n# remove a provider's MCP server from your Claude Desktop app\nuvx odmcp remove $PROVIDER_NAME\n```\n\n##### Example\n\nQuickstart for the Switzerland SBB (train company) provider:\n\n```bash\n# make sure claude is installed\nuvx odmcp setup ch_sbb\n```\n\nRestart Claude and you should see a new hammer icon at the bottom right of the chat.\n\nYou can now ask questions to Claude about SBB train network disruption and it will answer based on data collected on `data.sbb.ch`.\n\n### <u>Publish</u>: Contribute by building and publishing public datasets\n\n#### Prerequisites\n\n1. **Install UV Package Manager**\n   ```bash\n   # macOS\n   brew install uv\n\n   # Windows (PowerShell)\n   powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n   # Linux/WSL\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Clone & Setup Repository**\n   ```bash\n   # Clone the repository\n   git clone https://github.com/OpenDataMCP/OpenDataMCP.git\n   cd OpenDataMCP\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # Unix/macOS\n   # or\n   .venv\\Scripts\\activate     # Windows\n\n   # Install dependencies\n   uv sync\n   ```\n\n3. **Install Pre-commit Hooks**\n   ```bash\n   # Install pre-commit hooks for code quality\n   pre-commit install\n   ```\n\n#### Publishing Instructions\n\n1. **Create a New Provider Module**\n   * Each data source needs its own python module.\n   * Create a new Python module in `src/odmcp/providers/`.\n   * Use a descriptive name following the pattern: `{country_code}_{organization}.py` (e.g., `ch_sbb.py`).\n   * Start with our [template file](https://github.com/OpenDataMCP/OpenDataMCP/blob/main/src/odmcp/providers/__template__.py) as your base.\n\n2. **Implement Required Components**\n   * Define your Tools & Resources following the template structure\n   * Each Tool or Resource should have:\n     - Clear description of its purpose\n     - Well-defined input/output schemas using Pydantic models\n     - Proper error handling\n     - Documentation strings\n\n3. **Tool vs Resource**\n   * Choose **Tool** implementation if your data needs:\n     - Active querying or computation\n     - Parameter-based filtering\n     - Complex transformations\n   * Choose **Resource** implementation if your data is:\n     - Static or rarely changing\n     - Small enough to be loaded into memory\n     - Simple file-based content\n     - Reference documentation or lookup tables\n   * Reference the [MCP documentation](https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#primitives) for guidance\n\n4. **Testing**\n   * Add tests in the `tests/` directory\n   * Follow existing test patterns (see other provider tests)\n   * Required test coverage:\n     - Basic functionality\n     - Edge cases\n     - Error handling\n\n5. **Validation**\n   * Test your MCP server using our experimental client: `uv run src/odmcp/providers/client.py`\n   * Verify all endpoints respond correctly\n   * Ensure error messages are helpful\n   * Check performance with typical query loads\n\nFor other examples, check our existing providers in the `src/odmcp/providers/` directory.\n\n## Contributing\n\nWe have an ambitious roadmap and we want this project to scale with the community. The ultimate goal is to make the millions of datasets publicly available to all LLM applications. \n\nFor that we need your help!\n\n### Discord\n\nWe want to build a helping community around the challenge of bringing open data to LLM's. Join us on discord to start chatting: [https://discord.gg/QPFFZWKW](https://discord.gg/hDg4ZExjGs)\n\n### Our Core Guidelines\n\nBecause of our target scale we want to keep things simple and pragmatic at first. Tackle issues with the community as they come along.\n\n1. **Simplicity and Maintainability**\n   * Minimize abstractions to keep codebase simple and scalable\n   * Focus on clear, straightforward implementations\n   * Avoid unnecessary complexity\n\n2. **Standardization / Templates**\n   * Follow provided templates and guidelines consistently\n   * Maintain uniform structure across providers\n   * Use common patterns for similar functionality\n\n3. **Dependencies**\n   * Keep external dependencies to a minimum\n   * Prioritize single repository/package setup\n   * Carefully evaluate necessity of new dependencies\n\n4. **Code Quality**\n   * Format code using ruff\n   * Maintain comprehensive test coverage with pytest\n   * Follow consistent code style\n\n5. **Type Safety**\n   * Use Python type hints throughout\n   * Leverage Pydantic models for API request/response validation\n   * Ensure type safety in data handling\n\n### Tactical Topics (our current priorities)\n* [x] Initialize repository with guidelines, testing framework, and contribution workflow\n* [x] Implement CI/CD pipeline with automated PyPI releases\n* [x] Develop provider template and first reference implementation\n* [ ] **Integrate additional open datasets (actively seeking contributors)**\n* [ ] Establish clear guidelines for choosing between Resources and Tools\n* [ ] Develop scalable repository architecture for long-term growth\n* [ ] Expand MCP SDK parameter support (authentication, rate limiting, etc.)\n* [ ] Implement additional MCP protocol features (prompts, resource templates)\n* [ ] Add support for alternative transport protocols beyond stdio (SSE)\n* [ ] Deploy hosted MCP servers for improved accessibility\n\n## Roadmap\nLetâ€™s build the open source infrastructure that will allow all LLMs to access all Open Data together!\n\n### Access:\n* Make Open Data available to all LLM applications (beyond Claude)\n* Make Open Data data sources searchable in a scalable way\n* Make Open Data available through MCP remotely (SSE) with publicly sponsored infrastructure\n\n### Publish:\n* Build the many Open Data MCP servers to make all the Open Data truly accessible (we need you!).\n* On our side we are starting to build MCP servers for Switzerland ~12k open dataset! \n* Make it even easier to build Open Data MCP servers\n\nWe are very early, and lack of dataset available is currently the bottleneck. Help yourself! Create your Open Data MCP server and get users to use it as well from their LLMs applications. Letâ€™s connect LLMs to the millions of open datasets from governments, public entities, companies and NGOs!\n\nAs Anthropic's MCP evolves we will adapt and upgrade Open Data MCP.\n\n## Limitations\n\n* All data served by Open Data MCP servers should be Open. \n* Please oblige to the data licenses of the data providers.\n* Our License must be quoted in commercial applications.\n\n## References\n\n* Kudos to [Anthropic's open source MCP](https://spec.modelcontextprotocol.io/) release enabling initiative like this one.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n",
      "npm_url": "",
      "npm_downloads": 0
    },
    "srobbin--opengov-mcp-server": {
      "owner": "srobbin",
      "name": "opengov-mcp-server",
      "url": "https://github.com/srobbin/opengov-mcp-server",
      "imageUrl": "https://github.com/srobbin.png",
      "description": "Enable access to public government datasets from Socrata-powered portals, allowing search, retrieval, and analysis of open data from various government sources without the need for API keys.",
      "stars": 9,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-29T07:33:53Z",
      "readme_content": "# OpenGov MCP Server\n\nAn MCP (Model Context Protocol) server that enables MCP clients like Claude Desktop to access Socrata Open Data APIs. This integration allows Claude Desktop to search for, retrieve, and analyze public datasets from government data portals.\n\n## Overview\n\nThis MCP server provides access to open data from any Socrata-powered data portal, including those from cities, states, and federal agencies such as:\n- [Chicago](https://data.cityofchicago.org)\n- [NYC](https://data.cityofnewyork.us)\n- [San Francisco](https://data.sfgov.org)\n- [Los Angeles](https://data.lacity.org)\n- [And other government entities](https://dev.socrata.com/data/)\n\nNo API key is required for basic usage, as the server accesses public data.\n\n## Features\n\nWith this MCP server, clients can:\n- Search and discover datasets by keyword, category, or tags\n- View dataset metadata and column information\n- Run SQL-like queries to retrieve and analyze data\n- Get portal usage statistics\n\n## Installation for Claude Desktop\n\n### Quick Setup with npx (Recommended)\n\nThe easiest way to use this MCP server is with npx, which doesn't require any installation:\n\n1. **Create or edit your Claude Desktop configuration**:\n   \n   Create or edit `claude_desktop_config.json` in your home directory:\n\n   ```json\n   {\n     \"mcpServers\": { \n       \"opengov\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"opengov-mcp-server@latest\"],\n         \"env\": {\n           \"DATA_PORTAL_URL\": \"https://data.cityofchicago.org\"\n         }\n       }\n     }\n   }\n   ```\n\n   You can replace the DATA_PORTAL_URL with any Socrata-powered data portal.\n\n2. **Restart Claude Desktop** (if it was already running)\n\n3. **Start using the MCP server**:\n   \n   In Claude Desktop, you can now ask questions like:\n   \n   ```\n   How many cars were towed in Chicago this month?\n   ```\n\n   and you can follow up with questions that drill further into detail:\n\n   ```\n   Which make and color were towed the most?\n   Also, were there any interesting vanity plates?\n   ```\n\n   The first time you run a query, npx will automatically download and run the latest version of the server.\n\n### Manual Installation from Source\n\nIf you prefer to run from source (for development or customization):\n\n1. **Clone this repository**:\n   ```bash\n   git clone https://github.com/srobbin/opengov-mcp-server.git\n   cd opengov-mcp-server\n   ```\n\n2. **Install dependencies and build**:\n   ```bash\n   npm install\n   npm run build\n   ```\n\n3. **Create Claude Desktop configuration**:\n   \n   Create or edit `claude_desktop_config.json` in your home directory:\n\n   ```json\n   {\n     \"mcpServers\": { \n       \"opengov\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/opengov-mcp-server/dist/index.js\"\n         ],\n         \"env\": {\n           \"DATA_PORTAL_URL\": \"https://data.cityofchicago.org\"\n         }\n       }\n     }\n   }\n   ```\n\n   Replace `/path/to/your/opengov-mcp-server` with the actual path where you cloned the repository.\n\n4. **Restart Claude Desktop** (if it was already running)\n\n## Available Tool: get_data\n\nThis MCP server provides a unified `get_data` tool that Claude Desktop uses to access Socrata data.\n\n### Parameters\n\n- `type` (string, required): Operation type\n  - `catalog`: Search and list datasets\n  - `categories`: List dataset categories\n  - `tags`: List dataset tags\n  - `dataset-metadata`: Get dataset details\n  - `column-info`: Get dataset column information\n  - `data-access`: Query and retrieve records\n  - `site-metrics`: Get portal statistics\n\n- `domain` (string, optional): Data portal hostname (without protocol)\n\n- `query` (string, optional): Search query for datasets\n\n- `datasetId` (string): Dataset identifier for specific operations\n\n- `soqlQuery` (string, optional): SoQL query for filtering data\n\n- `limit` (number, optional): Maximum results to return (default: 10)\n\n- `offset` (number, optional): Results to skip for pagination (default: 0)\n\n### Example Queries\n\nThese are examples of how Claude Desktop will format queries to the MCP server:\n\n```javascript\n// Find datasets about budgets\n{\n  \"type\": \"catalog\",\n  \"query\": \"budget\",\n  \"limit\": 5\n}\n\n// Get information about a dataset\n{\n  \"type\": \"dataset-metadata\",\n  \"datasetId\": \"6zsd-86xi\"\n}\n\n// Query dataset records with SQL-like syntax\n{\n  \"type\": \"data-access\",\n  \"datasetId\": \"6zsd-86xi\",\n  \"soqlQuery\": \"SELECT * WHERE amount > 1000 ORDER BY date DESC\",\n  \"limit\": 10\n}\n```\n\n## Configuration Options\n\nThe server requires one environment variable:\n\n- `DATA_PORTAL_URL`: The Socrata data portal URL (e.g., `https://data.cityofchicago.org`)\n\nThis can be set:\n- In the Claude Desktop configuration (recommended)\n- In your environment variables\n- Via command line: `DATA_PORTAL_URL=https://data.cityofchicago.org opengov-mcp-server`\n",
      "npm_url": "",
      "npm_downloads": 0
    }
  }
}