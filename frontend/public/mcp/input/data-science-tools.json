{
  "category": "data-science-tools",
  "categoryDisplay": "Data Science Tools",
  "description": "Integrations and tools designed to simplify data exploration, analysis and enhance data science workflows.",
  "totalRepositories": 14,
  "repositories": {
    "Bright-L01--networkx-mcp-server": {
      "owner": "Bright-L01",
      "name": "networkx-mcp-server",
      "url": "https://github.com/Bright-L01/networkx-mcp-server",
      "imageUrl": "",
      "description": "The first NetworkX integration for Model Context Protocol, enabling graph analysis and visualization directly in AI conversations. Supports 13 operations including centrality algorithms, community detection, PageRank, and graph visualization.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T06:10:15Z",
      "readme_content": "# NetworkX MCP Server\n\nA comprehensive Model Context Protocol (MCP) server providing advanced graph analysis capabilities using NetworkX.\n\n## üöÄ Features\n\n- **Complete MCP Implementation**: Full Model Context Protocol support with Tools, Resources, and Prompts\n- **Modular Architecture**: Clean, maintainable codebase with 35+ focused modules\n- **Advanced Graph Analysis**: Comprehensive suite of graph algorithms and analytics\n- **Production Ready**: Enterprise-grade security, monitoring, and scalability features\n- **Developer Friendly**: Extensive documentation, testing, and development tools\n\n## üèóÔ∏è Architecture\n\nThe server follows a clean modular architecture:\n\n```\n‚îú‚îÄ‚îÄ Core Layer          # Basic graph operations and MCP server\n‚îú‚îÄ‚îÄ Handler Layer       # Function organization and re-exports\n‚îú‚îÄ‚îÄ Advanced Layer      # Specialized algorithms and features\n‚îî‚îÄ‚îÄ Supporting Layer    # Monitoring, security, and infrastructure\n```\n\nSee [ARCHITECTURE.md](ARCHITECTURE.md) for detailed architectural documentation.\n\n## üì¶ Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/username/networkx-mcp-server.git\ncd networkx-mcp-server\npip install -e .\n```\n\n### Basic Usage\n\n```python\nfrom networkx_mcp.server import create_graph, add_nodes, add_edges\n\n# Create a graph\nresult = create_graph(\"my_graph\", \"undirected\")\n\n# Add nodes and edges\nadd_nodes(\"my_graph\", [\"A\", \"B\", \"C\"])\nadd_edges(\"my_graph\", [(\"A\", \"B\"), (\"B\", \"C\")])\n```\n\n### Running the Server\n\n```bash\n# Start the MCP server\npython -m networkx_mcp\n\n# Or use the development script\n./run_tests.sh\n```\n\n## üß™ Testing\n\nThe project maintains 80%+ test coverage with comprehensive test suites:\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=src/networkx_mcp --cov-report=html\n\n# Run specific test categories\npytest tests/unit/          # Unit tests\npytest tests/integration/   # Integration tests\npytest tests/performance/   # Performance tests\n```\n\n## üìñ Documentation\n\n- [Architecture Overview](ARCHITECTURE.md) - Complete system architecture\n- [Module Structure](docs/MODULE_STRUCTURE.md) - Detailed module organization\n- [Development Guide](docs/DEVELOPMENT_GUIDE.md) - Developer handbook\n- [API Documentation](docs/api/) - Detailed API reference\n\n## ü§ù Contributing\n\nWe welcome contributions! Please see our [Development Guide](docs/DEVELOPMENT_GUIDE.md) for:\n\n- Setting up the development environment\n- Code standards and conventions\n- Testing requirements\n- Submission guidelines\n\n### Quick Development Setup\n\n```bash\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run the test suite\npytest\n```\n\n## üèÜ Quality Standards\n\nThis project maintains high quality standards:\n\n- **Code Quality**: Automated formatting with ruff, black, and isort\n- **Type Safety**: Comprehensive type hints with mypy validation\n- **Security**: Bandit security scanning and vulnerability checks\n- **Testing**: 80%+ test coverage with multiple test categories\n- **Documentation**: Comprehensive documentation and examples\n\n## üìã Requirements\n\n- Python 3.11+\n- NetworkX 3.0+\n- FastMCP (or compatible MCP implementation)\n\nSee [pyproject.toml](pyproject.toml) for complete dependency list.\n\n## üöÄ Deployment\n\n### Docker\n\n```bash\n# Build and run with Docker\ndocker build -t networkx-mcp-server .\ndocker run -p 8000:8000 networkx-mcp-server\n```\n\n### Kubernetes\n\n```bash\n# Deploy to Kubernetes\nkubectl apply -f k8s/\n```\n\nSee [deployment documentation](docs/deployment/) for production deployment guides.\n\n## üìä Performance\n\nThe server is optimized for performance:\n\n- **Modular Design**: Efficient memory usage and fast load times\n- **Algorithm Optimization**: Optimized implementations for large graphs\n- **Monitoring**: Built-in performance metrics and health checks\n- **Scalability**: Stateless design supporting horizontal scaling\n\n## üîí Security\n\nSecurity is a top priority:\n\n- **Input Validation**: Comprehensive input sanitization and validation\n- **Access Control**: Authentication and authorization layers\n- **Audit Logging**: Complete audit trail for security events\n- **Vulnerability Scanning**: Automated dependency vulnerability checks\n\n## üìà Monitoring\n\nBuilt-in observability features:\n\n- **Health Checks**: Comprehensive health monitoring endpoints\n- **Metrics**: Performance and usage metrics collection\n- **Tracing**: Distributed tracing support\n- **Logging**: Structured logging with configurable levels\n\n## üóÇÔ∏è Project Structure\n\n```\nnetworkx-mcp-server/\n‚îú‚îÄ‚îÄ src/networkx_mcp/       # Main source code\n‚îÇ   ‚îú‚îÄ‚îÄ core/               # Core graph operations\n‚îÇ   ‚îú‚îÄ‚îÄ handlers/           # Function handlers\n‚îÇ   ‚îú‚îÄ‚îÄ advanced/           # Advanced algorithms\n‚îÇ   ‚îú‚îÄ‚îÄ monitoring/         # Monitoring and observability\n‚îÇ   ‚îî‚îÄ‚îÄ security/           # Security features\n‚îú‚îÄ‚îÄ tests/                  # Comprehensive test suite\n‚îú‚îÄ‚îÄ docs/                   # Documentation\n‚îú‚îÄ‚îÄ scripts/                # Development and deployment scripts\n‚îî‚îÄ‚îÄ examples/               # Usage examples\n```\n\n## üìú License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- NetworkX team for the excellent graph analysis library\n- FastMCP team for the Model Context Protocol implementation\n- Contributors and users of this project\n\n## üìû Support\n\n- **Issues**: [GitHub Issues](https://github.com/username/networkx-mcp-server/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/username/networkx-mcp-server/discussions)\n- **Documentation**: [Project Documentation](docs/)\n\n---\n\n**Built with ‚ù§Ô∏è for the graph analysis community**\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "networkx",
        "graph",
        "visualization",
        "networkx integration",
        "server networkx",
        "l01 networkx"
      ],
      "category": "data-science-tools"
    },
    "ChronulusAI--chronulus-mcp": {
      "owner": "ChronulusAI",
      "name": "chronulus-mcp",
      "url": "https://github.com/ChronulusAI/chronulus-mcp",
      "imageUrl": "",
      "description": "Predict anything with Chronulus AI forecasting and prediction agents.",
      "stars": 98,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T16:46:42Z",
      "readme_content": "<div align=\"center\">\n<img width=\"150px\" src=\"https://www.chronulus.com/brand-assets/chronulus-logo-blue-on-alpha-square.png\" alt=\"Chronulus AI\">\n    <h1 align=\"center\">MCP Server for Chronulus</h1>\n    <h3 align=\"center\">Chat with Chronulus AI Forecasting & Prediction Agents in Claude</h3>\n</div>\n\n\n\n\n### Quickstart: Claude for Desktop\n\n#### Install \n\nClaude for Desktop is currently available on macOS and Windows.\n\nInstall Claude for Desktop [here](https://claude.ai/download)\n\n#### Configuration\n\nFollow the general instructions [here](https://modelcontextprotocol.io/quickstart/user) to configure the Claude desktop client.\n\nYou can find your Claude config at one of the following locations:\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nThen choose one of the following methods that best suits your needs and add it to your `claude_desktop_config.json`\n\n\n\n<details>\n<summary>Using pip</summary>\n\n(Option 1) Install release from PyPI\n\n```bash \npip install chronulus-mcp\n```\n\n\n(Option 2) Install from Github\n\n```bash \ngit clone https://github.com/ChronulusAI/chronulus-mcp.git\ncd chronulus-mcp\npip install .\n```\n\n\n\n```json \n{\n  \"mcpServers\": {\n    \"chronulus-agents\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"chronulus_mcp\"],\n      \"env\": {\n        \"CHRONULUS_API_KEY\": \"<YOUR_CHRONULUS_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\nNote, if you get an error like \"MCP chronulus-agents: spawn python ENOENT\", \nthen you most likely need to provide the absolute path to `python`. \nFor example `/Library/Frameworks/Python.framework/Versions/3.11/bin/python3` instead of just `python`\n\n</details>\n\n\n<details>\n<summary>Using docker</summary>\n\nHere we will build a docker image called 'chronulus-mcp' that we can reuse in our Claude config.\n\n```bash \ngit clone https://github.com/ChronulusAI/chronulus-mcp.git\ncd chronulus-mcp\n docker build . -t 'chronulus-mcp'\n```\n\nIn your Claude config, be sure that the final argument matches the name you give to the docker image in the build command.\n\n```json \n{\n  \"mcpServers\": {\n    \"chronulus-agents\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"CHRONULUS_API_KEY\", \"chronulus-mcp\"],\n      \"env\": {\n        \"CHRONULUS_API_KEY\": \"<YOUR_CHRONULUS_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Using uvx</summary>\n\n`uvx` will pull the latest version of `chronulus-mcp` from the PyPI registry, install it, and then run it.\n\n\n```json \n{\n  \"mcpServers\": {\n    \"chronulus-agents\": {\n      \"command\": \"uvx\",\n      \"args\": [\"chronulus-mcp\"],\n      \"env\": {\n        \"CHRONULUS_API_KEY\": \"<YOUR_CHRONULUS_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\nNote, if you get an error like \"MCP chronulus-agents: spawn uvx ENOENT\", then you most likely need to either:\n1. [install uv](https://docs.astral.sh/uv/getting-started/installation/) or\n2. Provide the absolute path to `uvx`. For example `/Users/username/.local/bin/uvx` instead of just `uvx`\n\n</details>\n\n#### Additional Servers (Filesystem, Fetch, etc)\n\nIn our demo, we use third-party servers like [fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) and [filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem).\n\nFor details on installing and configure third-party server, please reference the documentation provided by the server maintainer.\n\nBelow is an example of how to configure filesystem and fetch alongside Chronulus in your `claude_desktop_config.json`: \n\n```json \n{\n  \"mcpServers\": {\n    \"chronulus-agents\": {\n      \"command\": \"uvx\",\n      \"args\": [\"chronulus-mcp\"],\n      \"env\": {\n        \"CHRONULUS_API_KEY\": \"<YOUR_CHRONULUS_API_KEY>\"\n      }\n    },\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-filesystem\",\n        \"/path/to/AIWorkspace\"\n      ]\n    },\n    \"fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-fetch\"]\n    }\n  }\n} \n```\n\n\n#### Claude Preferences\n\nTo streamline your experience using Claude across multiple sets of tools, it is best to add your preferences to under Claude Settings. \n\nYou can upgrade your Claude preferences in a couple ways:\n\n* From Claude Desktop: `Settings -> General -> Claude Settings -> Profile (tab)`\n* From [claude.ai/settings](https://claude.ai/settings): `Profile (tab)`\n\nPreferences are shared across both Claude for Desktop and Claude.ai (the web interface). So your instruction need to work across both experiences.\n\nBelow are the preferences we used to achieve the results shown in our demos:\n\n```\n## Tools-Dependent Protocols\nThe following instructions apply only when tools/MCP Servers are accessible.\n\n### Filesystem - Tool Instructions\n- Do not use 'read_file' or 'read_multiple_files' on binary files (e.g., images, pdfs, docx) .\n- When working with binary files (e.g., images, pdfs, docx) use 'get_info' instead of 'read_*' tools to inspect a file.\n\n### Chronulus Agents - Tool Instructions\n- When using Chronulus, prefer to use input field types like TextFromFile, PdfFromFile, and ImageFromFile over scanning the files directly.\n- When plotting forecasts from Chronulus, always include the Chronulus-provided forecast explanation below the plot and label it as Chronulus Explanation.\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "chronulus",
        "chronulusai",
        "predict chronulus",
        "data science",
        "science tools"
      ],
      "category": "data-science-tools"
    },
    "DataEval--dingo": {
      "owner": "DataEval",
      "name": "dingo",
      "url": "https://github.com/DataEval/dingo",
      "imageUrl": "",
      "description": "MCP server for the Dingo: a comprehensive data quality evaluation tool. Server Enables interaction with Dingo's rule-based and LLM-based evaluation capabilities and rules&prompts listing.",
      "stars": 480,
      "forks": 49,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-10-04T08:17:42Z",
      "readme_content": "<div align=\"center\" xmlns=\"http://www.w3.org/1999/html\">\n<!-- logo -->\n<p align=\"center\">\n  \n</p>\n\n<!-- badges -->\n<p align=\"center\">\n  <a href=\"https://github.com/pre-commit/pre-commit\"><img src=\"https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white\" alt=\"pre-commit\"></a>\n  <a href=\"https://pypi.org/project/dingo-python/\"><img src=\"https://img.shields.io/pypi/v/dingo-python.svg\" alt=\"PyPI version\"></a>\n  <a href=\"https://pypi.org/project/dingo-python/\"><img src=\"https://img.shields.io/pypi/pyversions/dingo-python.svg\" alt=\"Python versions\"></a>\n  <a href=\"https://github.com/DataEval/dingo/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/DataEval/dingo\" alt=\"License\"></a>\n  <a href=\"https://github.com/DataEval/dingo/stargazers\"><img src=\"https://img.shields.io/github/stars/DataEval/dingo\" alt=\"GitHub stars\"></a>\n  <a href=\"https://github.com/DataEval/dingo/network/members\"><img src=\"https://img.shields.io/github/forks/DataEval/dingo\" alt=\"GitHub forks\"></a>\n  <a href=\"https://github.com/DataEval/dingo/issues\"><img src=\"https://img.shields.io/github/issues/DataEval/dingo\" alt=\"GitHub issues\"></a>\n  <a href=\"https://mseep.ai/app/dataeval-dingo\"><img src=\"https://mseep.net/pr/dataeval-dingo-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" height=\"20\"></a>\n  <a href=\"https://deepwiki.com/MigoXLab/dingo\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\"></a>\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/DataEval/dingo)](https://archestra.ai/mcp-catalog/dataeval__dingo)\n</p>\n\n</div>\n\n\n<div align=\"center\">\n\n[English](README.md) ¬∑ [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) ¬∑ [Êó•Êú¨Ë™û](README_ja.md)\n\n</div>\n\n\n<!-- join us -->\n\n<p align=\"center\">\n    üëã join us on <a href=\"https://discord.gg/Jhgb2eKWh8\" target=\"_blank\">Discord</a> and <a href=\"./docs/assets/wechat.jpg\" target=\"_blank\">WeChat</a>\n</p>\n\n\n<p align=\"center\">\n  If you like Dingo, please give us a ‚≠ê on GitHub!\n  <br/>\n  <a href=\"https://github.com/DataEval/dingo/stargazers\" target=\"_blank\">\n    \n  </a>\n</p>\n\n\n# Introduction\n\nDingo is a data quality evaluation tool that helps you automatically detect data quality issues in your datasets. Dingo provides a variety of built-in rules and model evaluation methods, and also supports custom evaluation methods. Dingo supports commonly used text datasets and multimodal datasets, including pre-training datasets, fine-tuning datasets, and evaluation datasets. In addition, Dingo supports multiple usage methods, including local CLI and SDK, making it easy to integrate into various evaluation platforms, such as [OpenCompass](https://github.com/open-compass/opencompass).\n\n## Architecture Diagram\n\n\n\n# Quick Start\n\n## Installation\n\n```shell\npip install dingo-python\n```\n\n## Example Use Cases\n\n### 1. Evaluate LLM chat data\n\n```python\nfrom dingo.config.input_args import EvaluatorLLMArgs\nfrom dingo.io.input import Data\nfrom dingo.model.llm.llm_text_quality_model_base import LLMTextQualityModelBase\nfrom dingo.model.rule.rule_common import RuleEnterAndSpace\n\ndata = Data(\n    data_id='123',\n    prompt=\"hello, introduce the world\",\n    content=\"Hello! The world is a vast and diverse place, full of wonders, cultures, and incredible natural beauty.\"\n)\n\n\ndef llm():\n    LLMTextQualityModelBase.dynamic_config = EvaluatorLLMArgs(\n        key='YOUR_API_KEY',\n        api_url='https://api.openai.com/v1/chat/completions',\n        model='gpt-4o',\n    )\n    res = LLMTextQualityModelBase.eval(data)\n    print(res)\n\n\ndef rule():\n    res = RuleEnterAndSpace().eval(data)\n    print(res)\n```\n\n### 2. Evaluate Dataset\n\n```python\nfrom dingo.config import InputArgs\nfrom dingo.exec import Executor\n\n# Evaluate a dataset from Hugging Face\ninput_data = {\n    \"input_path\": \"tatsu-lab/alpaca\",  # Dataset from Hugging Face\n    \"dataset\": {\n        \"source\": \"hugging_face\",\n        \"format\": \"plaintext\"  # Format: plaintext\n    },\n    \"executor\": {\n        \"eval_group\": \"sft\",  # Rule set for SFT data\n        \"result_save\": {\n            \"bad\": True  # Save evaluation results\n        }\n    }\n}\n\ninput_args = InputArgs(**input_data)\nexecutor = Executor.exec_map[\"local\"](input_args)\nresult = executor.execute()\nprint(result)\n```\n\n## Command Line Interface\n\n### Evaluate with Rule Sets\n\n```shell\npython -m dingo.run.cli --input test/env/local_plaintext.json\n```\n\n### Evaluate with LLM (e.g., GPT-4o)\n\n```shell\npython -m dingo.run.cli --input test/env/local_json.json\n```\n\n## GUI Visualization\n\nAfter evaluation (with `result_save.bad=True`), a frontend page will be automatically generated. To manually start the frontend:\n\n```shell\npython -m dingo.run.vsl --input output_directory\n```\n\nWhere `output_directory` contains the evaluation results with a `summary.json` file.\n\n\n\n## Online Demo\nTry Dingo on our online demo: [(Hugging Face)ü§ó](https://huggingface.co/spaces/DataEval/dingo)\n\n## Local Demo\nTry Dingo in local:\n\n```shell\ncd app_gradio\npython app.py\n```\n\n\n\n\n## Google Colab Demo\nExperience Dingo interactively with Google Colab notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DataEval/dingo/blob/dev/examples/colab/dingo_colab_demo.ipynb)\n\n\n\n# MCP Server\n\nDingo includes an experimental Model Context Protocol (MCP) server. For details on running the server and integrating it with clients like Cursor, please see the dedicated documentation:\n\n[English](README_mcp.md) ¬∑ [ÁÆÄ‰Ωì‰∏≠Êñá](README_mcp_zh-CN.md) ¬∑ [Êó•Êú¨Ë™û](README_mcp_ja.md)\n\n## Video Demonstration\n\nTo help you get started quickly with Dingo MCP, we've created a video walkthrough:\n\nhttps://github.com/user-attachments/assets/aca26f4c-3f2e-445e-9ef9-9331c4d7a37b\n\nThis video demonstrates step-by-step how to use Dingo MCP server with Cursor.\n\n\n# Data Quality Metrics\n\nDingo provides comprehensive data quality assessment through both rule-based and prompt-based evaluation metrics. These metrics cover multiple quality dimensions including effectiveness, completeness, similarity, security, and more.\n\nüìä **[View Complete Metrics Documentation ‚Üí](docs/metrics.md)**\n\nOur evaluation system includes:\n- **Text Quality Assessment Metrics**: Pre-training data quality evaluation using DataMan methodology and enhanced multi-dimensional assessment\n- **SFT Data Assessment Metrics**: Honest, Helpful, Harmless evaluation for supervised fine-tuning data\n- **Classification Metrics**: Topic categorization and content classification\n- **Multimodality Assessment Metrics**: Image classification and relevance evaluation\n- **Rule-Based Quality Metrics**: Automated quality checks using heuristic rules for effectiveness and similarity detection\n- **Factuality Assessment Metrics**: Two-stage factuality evaluation based on GPT-5 System Card\n- etc\n\nMost metrics are backed by academic sources to ensure objectivity and scientific rigor.\n\n### Using LLM Assessment in Evaluation\n\nTo use these assessment prompts in your evaluations, specify them in your configuration:\n\n```python\ninput_data = {\n    # Other parameters...\n    \"executor\": {\n        \"prompt_list\": [\"QUALITY_BAD_SIMILARITY\"],  # Specific prompt to use\n    },\n    \"evaluator\": {\n        \"llm_config\": {\n            \"LLMTextQualityPromptBase\": {  # LLM model to use\n                \"model\": \"gpt-4o\",\n                \"key\": \"YOUR_API_KEY\",\n                \"api_url\": \"https://api.openai.com/v1/chat/completions\"\n            }\n        }\n    }\n}\n```\n\nYou can customize these prompts to focus on specific quality dimensions or to adapt to particular domain requirements. When combined with appropriate LLM models, these prompts enable comprehensive evaluation of data quality across multiple dimensions.\n\n### Hallucination Detection & RAG System Evaluation\n\nFor detailed guidance on using Dingo's hallucination detection capabilities, including HHEM-2.1-Open local inference and LLM-based evaluation:\n\nüìñ **[View Hallucination Detection Guide ‚Üí](docs/hallucination_guide.md)**\n\n### Factuality Assessment\n\nFor comprehensive guidance on using Dingo's two-stage factuality evaluation system:\n\nüìñ **[View Factuality Assessment Guide ‚Üí](docs/factcheck_guide.md)**\n\n# Rule Groups\n\nDingo provides pre-configured rule groups for different types of datasets:\n\n| Group | Use Case | Example Rules |\n|-------|----------|---------------|\n| `default` | General text quality | `RuleColonEnd`, `RuleContentNull`, `RuleDocRepeat`, etc. |\n| `sft` | Fine-tuning datasets | Rules from `default` plus `RuleHallucinationHHEM` for hallucination detection |\n| `rag` | RAG system evaluation | `RuleHallucinationHHEM`, `PromptHallucination` for response consistency |\n| `hallucination` | Hallucination detection | `PromptHallucination` with LLM-based evaluation |\n| `pretrain` | Pre-training datasets | Comprehensive set of 20+ rules including `RuleAlphaWords`, `RuleCapitalWords`, etc. |\n\nTo use a specific rule group:\n\n```python\ninput_data = {\n    \"executor\": {\n        \"eval_group\": \"sft\",  # Use \"default\", \"sft\", \"rag\", \"hallucination\", or \"pretrain\"\n    }\n    # other parameters...\n}\n```\n\n# Feature Highlights\n\n## Multi-source & Multi-modal Support\n\n- **Data Sources**: Local files, Hugging Face datasets, S3 storage\n- **Data Types**: Pre-training, fine-tuning, and evaluation datasets\n- **Data Modalities**: Text and image\n\n## Rule-based & Model-based Evaluation\n\n- **Built-in Rules**: 20+ general heuristic evaluation rules\n- **LLM Integration**: OpenAI, Kimi, and local models (e.g., Llama3)\n- **Hallucination Detection**: HHEM-2.1-Open local model and GPT-based evaluation\n- **RAG System Evaluation**: Response consistency and context alignment assessment\n- **Custom Rules**: Easily extend with your own rules and models\n- **Security Evaluation**: Perspective API integration\n\n## Flexible Usage\n\n- **Interfaces**: CLI and SDK options\n- **Integration**: Easy integration with other platforms\n- **Execution Engines**: Local and Spark\n\n## Comprehensive Reporting\n\n- **Quality Metrics**: 7-dimensional quality assessment\n- **Traceability**: Detailed reports for anomaly tracking\n\n# User Guide\n\n## Custom Rules, Prompts, and Models\n\nIf the built-in rules don't meet your requirements, you can create custom ones:\n\n### Custom Rule Example\n\n```python\nfrom dingo.model import Model\nfrom dingo.model.rule.base import BaseRule\nfrom dingo.config.input_args import EvaluatorRuleArgs\nfrom dingo.io import Data\nfrom dingo.model.modelres import ModelRes\n\n@Model.rule_register('QUALITY_BAD_RELEVANCE', ['default'])\nclass MyCustomRule(BaseRule):\n    \"\"\"Check for custom pattern in text\"\"\"\n\n    dynamic_config = EvaluatorRuleArgs(pattern=r'your_pattern_here')\n\n    @classmethod\n    def eval(cls, input_data: Data) -> ModelRes:\n        res = ModelRes()\n        # Your rule implementation here\n        return res\n```\n\n### Custom LLM Integration\n\n```python\nfrom dingo.model import Model\nfrom dingo.model.llm.base_openai import BaseOpenAI\n\n@Model.llm_register('my_custom_model')\nclass MyCustomModel(BaseOpenAI):\n    # Custom implementation here\n    pass\n```\n\nSee more examples in:\n- [Register Rules](examples/register/sdk_register_rule.py)\n- [Register Prompts](examples/register/sdk_register_prompt.py)\n- [Register Models](examples/register/sdk_register_llm.py)\n\n## Execution Engines\n\n### Local Execution\n\n```python\nfrom dingo.config import InputArgs\nfrom dingo.exec import Executor\n\ninput_args = InputArgs(**input_data)\nexecutor = Executor.exec_map[\"local\"](input_args)\nresult = executor.execute()\n\n# Get results\nsummary = executor.get_summary()        # Overall evaluation summary\nbad_data = executor.get_bad_info_list() # List of problematic data\ngood_data = executor.get_good_info_list() # List of high-quality data\n```\n\n### Spark Execution\n\n```python\nfrom dingo.config import InputArgs\nfrom dingo.exec import Executor\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"Dingo\").getOrCreate()\nspark_rdd = spark.sparkContext.parallelize([...])  # Your data as Data objects\n\ninput_data = {\n    \"executor\": {\n        \"eval_group\": \"default\",\n        \"result_save\": {\"bad\": True}\n    }\n}\ninput_args = InputArgs(**input_data)\nexecutor = Executor.exec_map[\"spark\"](input_args, spark_session=spark, spark_rdd=spark_rdd)\nresult = executor.execute()\n```\n\n## Evaluation Reports\n\nAfter evaluation, Dingo generates:\n\n1. **Summary Report** (`summary.json`): Overall metrics and scores\n2. **Detailed Reports**: Specific issues for each rule violation\n\nReport Description:\n1. **score**: `num_good` / `total`\n2. **type_ratio**: The count of type / total, such as: `QUALITY_BAD_COMPLETENESS` / `total`\n3. **name_ratio**: The count of name / total, such as: `QUALITY_BAD_COMPLETENESS-RuleColonEnd` / `total`\n\nExample summary:\n```json\n{\n    \"task_id\": \"d6c922ec-981c-11ef-b723-7c10c9512fac\",\n    \"task_name\": \"dingo\",\n    \"eval_group\": \"default\",\n    \"input_path\": \"test/data/test_local_jsonl.jsonl\",\n    \"output_path\": \"outputs/d6c921ac-981c-11ef-b723-7c10c9512fac\",\n    \"create_time\": \"20241101_144510\",\n    \"score\": 50.0,\n    \"num_good\": 1,\n    \"num_bad\": 1,\n    \"total\": 2,\n    \"type_ratio\": {\n        \"QUALITY_BAD_COMPLETENESS\": 0.5,\n        \"QUALITY_BAD_RELEVANCE\": 0.5\n    },\n    \"name_ratio\": {\n        \"QUALITY_BAD_COMPLETENESS-RuleColonEnd\": 0.5,\n        \"QUALITY_BAD_RELEVANCE-RuleSpecialCharacter\": 0.5\n    }\n}\n```\n\n# Future Plans\n\n- [ ] Richer graphic and text evaluation indicators\n- [ ] Audio and video data modality evaluation\n- [ ] Small model evaluation (fasttext, Qurating)\n- [ ] Data diversity evaluation\n\n# Limitations\n\nThe current built-in detection rules and model methods focus on common data quality problems. For specialized evaluation needs, we recommend customizing detection rules.\n\n# Acknowledgments\n\n- [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)\n- [mlflow](https://github.com/mlflow/mlflow)\n- [deepeval](https://github.com/confident-ai/deepeval)\n\n# Contribution\n\nWe appreciate all the contributors for their efforts to improve and enhance `Dingo`. Please refer to the [Contribution Guide](docs/en/CONTRIBUTING.md) for guidance on contributing to the project.\n\n# License\n\nThis project uses the [Apache 2.0 Open Source License](LICENSE).\n\nThis project uses fasttext for some functionality including language detection. fasttext is licensed under the MIT License, which is compatible with our Apache 2.0 license and provides flexibility for various usage scenarios.\n\n# Citation\n\nIf you find this project useful, please consider citing our tool:\n\n```\n@misc{dingo,\n  title={Dingo: A Comprehensive AI Data Quality Evaluation Tool for Large Models},\n  author={Dingo Contributors},\n  howpublished={\\url{https://github.com/MigoXLab/dingo}},\n  year={2024}\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dataeval",
        "tools",
        "data",
        "dataeval dingo",
        "data science",
        "dingo comprehensive"
      ],
      "category": "data-science-tools"
    },
    "arrismo--kaggle-mcp": {
      "owner": "arrismo",
      "name": "kaggle-mcp",
      "url": "https://github.com/arrismo/kaggle-mcp",
      "imageUrl": "",
      "description": "Connects to Kaggle, ability to download and analyze datasets.",
      "stars": 24,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T01:19:08Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@arrismo/kaggle-mcp)](https://smithery.ai/server/@arrismo/kaggle-mcp)\n<a href=\"https://glama.ai/mcp/servers/arwswog1el\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/arwswog1el/badge\" alt=\"Kaggle MCP Server\" /></a>\n\n# Kaggle MCP (Model Context Protocol) Server\nThis repository contains an MCP (Model Context Protocol) server (`server.py`) built using the `fastmcp` library. It interacts with the Kaggle API to provide tools for searching and downloading datasets, and a prompt for generating EDA notebooks.\n\n## Project Structure\n\n-   `server.py`: The FastMCP server application. It defines resources, tools, and prompts for interacting with Kaggle.\n-   `.env.example`: An example file for environment variables (Kaggle API credentials). Rename to `.env` and fill in your details.\n-   `requirements.txt`: Lists the necessary Python packages.\n-   `pyproject.toml` & `uv.lock`: Project metadata and locked dependencies for `uv` package manager.\n-   `datasets/`: Default directory where downloaded Kaggle datasets will be stored.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    # Or use uv: uv venv\n    ```\n\n3.  **Install dependencies:**\n    Using pip:\n    ```bash\n    pip install -r requirements.txt\n    ```\n    Or using uv:\n    ```bash\n    uv sync\n    ```\n\n4.  **Set up Kaggle API credentials:**\n    -   **Method 1 (Recommended): Environment Variables**\n        -   Create `.env` file\n        -   Open the `.env` file and add your Kaggle username and API key:\n            ```dotenv\n            KAGGLE_USERNAME=your_kaggle_username\n            KAGGLE_KEY=your_kaggle_api_key\n            ```\n        -   You can obtain your API key from your Kaggle account page (`Account` > `API` > `Create New API Token`). This will download a `kaggle.json` file containing your username and key.\n    -   **Method 2: `kaggle.json` file**\n        -   Download your `kaggle.json` file from your Kaggle account.\n        -   Place the `kaggle.json` file in the expected location (usually `~/.kaggle/kaggle.json` on Linux/macOS or `C:\\Users\\<Your User Name>\\.kaggle\\kaggle.json` on Windows). The `kaggle` library will automatically detect this file if the environment variables are not set.\n\n## Running the Server\n\n1.  **Ensure your virtual environment is active.**\n2.  **Run the MCP server:**\n    ```bash\n    uv run kaggle-mcp\n    ```\n    The server will start and register its resources, tools, and prompts. You can interact with it using an MCP client or compatible tools.\n\n## Running the Docker Container\n\n### 1. Set up Kaggle API credentials\n\nThis project requires Kaggle API credentials to access Kaggle datasets.\n\n- Go to https://www.kaggle.com/settings and click \"Create New API Token\" to download your `kaggle.json` file.\n- Open the `kaggle.json` file and copy your username and key into a new `.env` file in the project root:\n\n```\nKAGGLE_USERNAME=your_username\nKAGGLE_KEY=your_key\n```\n\n### 2. Build the Docker image\n\n```sh\ndocker build -t kaggle-mcp-test .\n```\n\n### 3. Run the Docker container using your .env file\n\n```sh\ndocker run --rm -it --env-file .env kaggle-mcp-test\n```\n\nThis will automatically load your Kaggle credentials as environment variables inside the container.\n\n---\n\n\n## Server Features\n\nThe server exposes the following capabilities through the Model Context Protocol:\n### Tools\n\n*   **`search_kaggle_datasets(query: str)`**:\n    *   Searches for datasets on Kaggle matching the provided query string.\n    *   Returns a JSON list of the top 10 matching datasets with details like reference, title, download count, and last updated date.\n*   **`download_kaggle_dataset(dataset_ref: str, download_path: str | None = None)`**:\n    *   Downloads and unzips files for a specific Kaggle dataset.\n    *   `dataset_ref`: The dataset identifier in the format `username/dataset-slug` (e.g., `kaggle/titanic`).\n    *   `download_path` (Optional): Specifies where to download the dataset. If omitted, it defaults to `./datasets/<dataset_slug>/` relative to the server script's location.\n\n### Prompts\n\n*   **`generate_eda_notebook(dataset_ref: str)`**:\n    *   Generates a prompt message suitable for an AI model (like Gemini) to create a basic Exploratory Data Analysis (EDA) notebook for the specified Kaggle dataset reference.\n    *   The prompt asks for Python code covering data loading, missing value checks, visualizations, and basic statistics.\n\n## Connecting to Claude Desktop \nGo to Claude > Settings > Developer > Edit Config > claude_desktop_config.json to include the following:\n\n```\n{\n  \"mcpServers\": {\n    \"kaggle-mcp\": {\n      \"command\": \"kaggle-mcp\",\n      \"cwd\": \"<path-to-their-cloned-repo>/kaggle-mcp\"\n    }\n  }\n}\n```\n\n## Usage Example\n\nAn AI agent or MCP client could interact with this server like this:\n\n1.  **Agent:** \"Search Kaggle for datasets about 'heart disease'\"\n    *   *Server executes `search_kaggle_datasets(query='heart disease')`*\n2.  **Agent:** \"Download the dataset 'user/heart-disease-dataset'\"\n    *   *Server executes `download_kaggle_dataset(dataset_ref='user/heart-disease-dataset')`*\n3.  **Agent:** \"Generate an EDA notebook prompt for 'user/heart-disease-dataset'\"\n    *   *Server executes `generate_eda_notebook(dataset_ref='user/heart-disease-dataset')`*\n    *   *Server returns a structured prompt message.*\n4.  **Agent:** (Sends the prompt to a code-generating model) -> Receives EDA Python code.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "datasets",
        "kaggle",
        "integrations tools",
        "tools integrations",
        "data science"
      ],
      "category": "data-science-tools"
    },
    "avisangle--calculator-server": {
      "owner": "avisangle",
      "name": "calculator-server",
      "url": "https://github.com/avisangle/calculator-server",
      "imageUrl": "",
      "description": "A comprehensive Go-based MCP server for mathematical computations, implementing 13 mathematical tools across basic arithmetic, advanced functions, statistical analysis, unit conversions, and financial calculations.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-09-19T17:31:39Z",
      "readme_content": "# Calculator Server - Go MCP Server\n\nA comprehensive **Go-based MCP (Model Context Protocol) server** for mathematical computations, implementing **13 mathematical tools** with advanced features and high precision calculations.\n\n**Owner & Maintainer:** Avinash Sangle (avinash.sangle123@gmail.com)\n\n[![Go Version](https://img.shields.io/badge/go-%3E%3D1.21-blue)](https://golang.org/)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)]()\n[![Coverage](https://img.shields.io/badge/coverage-%3E95%25-brightgreen)]()\n\n## üßÆ Features\n\n### Core Mathematical Tools (13 Tools)\n\n#### Basic Mathematical Tools (6 Tools)\n\n1. **Basic Math Operations** - Precision arithmetic with configurable decimal places\n   - Addition, subtraction, multiplication, division\n   - Multiple operand support\n   - Decimal precision control (0-15 places)\n\n2. **Advanced Mathematical Functions** - Scientific calculations\n   - Trigonometric: `sin`, `cos`, `tan`, `asin`, `acos`, `atan`\n   - Logarithmic: `log`, `log10`, `ln`\n   - Other: `sqrt`, `abs`, `factorial`, `exp`, `pow`\n   - Unit support: degrees/radians for trig functions\n   - Power function with base and exponent parameters\n\n3. **Expression Evaluation** - Complex mathematical expressions\n   - Variable substitution support\n   - Mathematical constants (`œÄ`, `e`)\n   - Nested expressions with parentheses\n   - Function calls within expressions\n\n4. **Statistical Analysis** - Comprehensive data analysis\n   - Descriptive statistics: mean, median, mode\n   - Variability: standard deviation, variance\n   - Percentile calculations\n   - Data validation and error handling\n\n5. **Unit Conversion** - Multi-category unit conversion\n   - **Length**: mm, cm, m, km, in, ft, yd, mi, mil, Œºm, nm\n   - **Weight**: mg, g, kg, t, oz, lb, st (stone), ton (US ton)\n   - **Temperature**: ¬∞C, ¬∞F, K, R (Rankine)\n   - **Volume**: ml, cl, dl, l, kl, fl_oz, cup, pt, qt, gal, tsp, tbsp, bbl\n   - **Area**: mm¬≤, cm¬≤, m¬≤, km¬≤, in¬≤, ft¬≤, yd¬≤, mi¬≤, acre, ha\n\n6. **Financial Calculations** - Comprehensive financial modeling\n   - Interest calculations: simple & compound\n   - Loan payment calculations\n   - Return on Investment (ROI)\n   - Present/Future value calculations\n   - Net Present Value (NPV) & Internal Rate of Return (IRR)\n\n#### Advanced Specialized Tools (7 Tools)\n\n7. **Statistics Summary** - Comprehensive statistical summary of datasets\n   - Complete statistical overview including all measures\n   - Data preview with first/last elements\n   - Common percentiles (25th, 50th, 75th)\n\n8. **Percentile Calculation** - Calculate specific percentiles (0-100)\n   - Any percentile value between 0 and 100\n   - Data count and preview information\n   - Accurate percentile calculations using empirical method\n\n9. **Batch Unit Conversion** - Convert multiple values between units at once\n   - Bulk conversion operations\n   - Same unit categories as single conversion\n   - Efficient batch processing\n\n10. **Net Present Value (NPV)** - Advanced NPV calculations with cash flows\n    - Multiple cash flow periods\n    - Discount rate calculations\n    - Investment decision support\n\n11. **Internal Rate of Return (IRR)** - IRR calculations for investment analysis\n    - Cash flow analysis\n    - Newton-Raphson method for accurate IRR calculation\n    - Investment performance evaluation\n\n12. **Loan Comparison** - Compare multiple loan scenarios\n    - Multiple loan option analysis\n    - Payment calculations for each scenario\n    - Comparison metrics and recommendations\n\n13. **Investment Scenarios** - Compare multiple investment scenarios\n    - Multiple investment option analysis\n    - Future value calculations for each scenario\n    - Investment comparison and recommendations\n\n### Technical Features\n\n- **High Precision**: Uses `shopspring/decimal` for financial calculations\n- **Scientific Computing**: Powered by `gonum.org/v1/gonum`\n- **Expression Engine**: Advanced parsing with `govaluate`\n- **Comprehensive Testing**: >95% test coverage\n- **Error Handling**: Detailed error messages and validation\n- **MCP Protocol**: Full compliance with MCP specification\n- **Build Automation**: Complete Makefile with CI/CD support\n- **Streamable HTTP Transport**: MCP-compliant HTTP transport with SSE support\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- **Go 1.21+** (required)\n- **Git** (for version control)\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd calculator-server\n\n# Install dependencies\nmake deps\n\n# Build the server\nmake build\n\n# Run the server\nmake run\n```\n\n### Alternative Setup\n\n```bash\n# Initialize Go module\ngo mod init calculator-server\ngo mod tidy\n\n# Build and run\ngo build -o calculator-server ./cmd/server\n./calculator-server -transport=stdio\n```\n\n## üìä Usage Examples\n\n### Basic Mathematics\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"basic_math\",\n    \"arguments\": {\n      \"operation\": \"add\",\n      \"operands\": [15.5, 20.3, 10.2],\n      \"precision\": 2\n    }\n  }\n}\n```\n\n**Response:**\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"content\": [\n      {\n        \"type\": \"text\", \n        \"text\": \"{\\\"result\\\": 46.0}\"\n      }\n    ]\n  }\n}\n```\n\n### Advanced Mathematical Functions\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"advanced_math\",\n    \"arguments\": {\n      \"function\": \"pow\",\n      \"value\": 2,\n      \"exponent\": 8\n    }\n  }\n}\n```\n\n### Statistics Summary\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"stats_summary\",\n    \"arguments\": {\n      \"data\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    }\n  }\n}\n```\n\n### Percentile Calculation\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 4,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"percentile\",\n    \"arguments\": {\n      \"data\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n      \"percentile\": 90\n    }\n  }\n}\n```\n\n### Net Present Value\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 5,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"npv\",\n    \"arguments\": {\n      \"cashFlows\": [-50000, 15000, 20000, 25000, 30000],\n      \"discountRate\": 8\n    }\n  }\n}\n```\n\n### Batch Unit Conversion\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 6,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"batch_conversion\",\n    \"arguments\": {\n      \"values\": [100, 200, 300],\n      \"fromUnit\": \"cm\",\n      \"toUnit\": \"m\",\n      \"category\": \"length\"\n    }\n  }\n}\n```\n\n## üåê MCP Streamable HTTP Transport\n\nThe server implements **MCP-compliant streamable HTTP transport** according to the official MCP specification, providing real-time communication with Server-Sent Events (SSE) streaming support.\n\n### MCP Protocol Compliance\n\n‚úÖ **Single Endpoint**: `/mcp` only (per MCP specification)  \n‚úÖ **Required Headers**: `MCP-Protocol-Version`, `Accept`  \n‚úÖ **Session Management**: Cryptographically secure session IDs  \n‚úÖ **SSE Streaming**: Server-Sent Events for real-time responses  \n‚úÖ **CORS Support**: Origin validation and security headers  \n\n### HTTP Endpoints\n\n#### Single MCP Endpoint (Specification Compliant)\n- **POST /mcp** - MCP JSON-RPC with optional SSE streaming\n- **GET /mcp** - SSE stream establishment\n- **OPTIONS /mcp** - CORS preflight handling\n\n### Example Usage\n\n```bash\n# Start MCP-compliant HTTP server\n./calculator-server -transport=http -port=8080\n\n# Basic JSON-RPC request\ncurl -X POST http://localhost:8080/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json\" \\\n  -H \"MCP-Protocol-Version: 2024-11-05\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"basic_math\",\n      \"arguments\": {\n        \"operation\": \"add\",\n        \"operands\": [15, 25],\n        \"precision\": 2\n      }\n    }\n  }'\n\n# SSE streaming request (for real-time responses)\ncurl -X POST http://localhost:8080/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: text/event-stream\" \\\n  -H \"MCP-Protocol-Version: 2024-11-05\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"stats_summary\",\n      \"arguments\": {\"data\": [1,2,3,4,5]}\n    }\n  }'\n```\n\n## üèóÔ∏è Project Structure\n\n```\ncalculator-server/\n‚îú‚îÄ‚îÄ cmd/\n‚îÇ   ‚îî‚îÄ‚îÄ server/\n‚îÇ       ‚îî‚îÄ‚îÄ main.go              # Main server entry point\n‚îú‚îÄ‚îÄ internal/\n‚îÇ   ‚îú‚îÄ‚îÄ calculator/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic.go            # Basic math operations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced.go         # Advanced mathematical functions\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ expression.go       # Expression evaluation\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ statistics.go       # Statistical analysis\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ units.go           # Unit conversion\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ financial.go       # Financial calculations\n‚îÇ   ‚îú‚îÄ‚îÄ handlers/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ math_handler.go    # Math operation handlers\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stats_handler.go   # Statistics & specialized handlers\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finance_handler.go # Financial handlers\n‚îÇ   ‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.go          # Configuration structures\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.go          # Configuration loader\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ errors.go          # Configuration errors\n‚îÇ   ‚îî‚îÄ‚îÄ types/\n‚îÇ       ‚îî‚îÄ‚îÄ requests.go        # Request/response types\n‚îú‚îÄ‚îÄ pkg/\n‚îÇ   ‚îî‚îÄ‚îÄ mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ protocol.go        # MCP protocol handling\n‚îÇ       ‚îî‚îÄ‚îÄ streamable_http_transport.go # HTTP transport\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ basic_test.go         # Basic math tests\n‚îÇ   ‚îú‚îÄ‚îÄ advanced_test.go      # Advanced math tests\n‚îÇ   ‚îú‚îÄ‚îÄ expression_test.go    # Expression evaluation tests\n‚îÇ   ‚îú‚îÄ‚îÄ integration_test.go   # Integration tests\n‚îÇ   ‚îú‚îÄ‚îÄ config_test.go        # Configuration tests\n‚îÇ   ‚îî‚îÄ‚îÄ streamable_http_transport_test.go # HTTP transport tests\n‚îú‚îÄ‚îÄ config.sample.yaml        # Sample YAML configuration\n‚îú‚îÄ‚îÄ config.sample.json        # Sample JSON configuration\n‚îú‚îÄ‚îÄ go.mod                    # Go module definition\n‚îú‚îÄ‚îÄ go.sum                    # Go module checksums\n‚îú‚îÄ‚îÄ Makefile                  # Build automation\n‚îî‚îÄ‚îÄ README.md                 # Project documentation\n```\n\n## üõ†Ô∏è Development\n\n### Building\n\n```bash\n# Build for current platform\nmake build\n\n# Build for all platforms\nmake build-all\n\n# Install to $GOPATH/bin\nmake install\n```\n\n### Testing\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with coverage\nmake coverage\n\n# Run tests with race detection\nmake test-race\n\n# Run benchmarks\nmake benchmark\n```\n\n### Quality Assurance\n\n```bash\n# Format code\nmake fmt\n\n# Run linter\nmake lint\n\n# Run vet\nmake vet\n\n# Run all quality checks\nmake quality\n\n# Pre-commit checks\nmake pre-commit\n\n# CI pipeline\nmake ci\n```\n\n### Development Mode\n\n```bash\n# Run without building (development)\nmake run-dev\n\n# Run with rebuild\nmake run\n```\n\n## üìã Available Tools\n\n### Core Tools (6)\n\n#### 1. `basic_math`\n**Purpose:** Basic arithmetic operations with precision control\n\n**Parameters:**\n- `operation` (string): \"add\", \"subtract\", \"multiply\", \"divide\"\n- `operands` (array of numbers): Numbers to operate on (minimum 2)\n- `precision` (integer, optional): Decimal places (0-15, default: 2)\n\n#### 2. `advanced_math`\n**Purpose:** Advanced mathematical functions\n\n**Parameters:**\n- `function` (string): Function name (sin, cos, tan, asin, acos, atan, log, log10, ln, sqrt, abs, factorial, pow, exp)\n- `value` (number): Input value (base for pow function)\n- `exponent` (number, optional): Exponent for pow function (required for pow)\n- `unit` (string, optional): \"radians\" or \"degrees\" for trig functions\n\n#### 3. `expression_eval`\n**Purpose:** Evaluate mathematical expressions with variables\n\n**Parameters:**\n- `expression` (string): Mathematical expression to evaluate\n- `variables` (object, optional): Variable name-value pairs\n\n#### 4. `statistics`\n**Purpose:** Statistical analysis of datasets\n\n**Parameters:**\n- `data` (array of numbers): Dataset to analyze\n- `operation` (string): Statistical operation (mean, median, mode, std_dev, variance, percentile)\n\n#### 5. `unit_conversion`\n**Purpose:** Convert between measurement units\n\n**Parameters:**\n- `value` (number): Value to convert\n- `fromUnit` (string): Source unit\n- `toUnit` (string): Target unit\n- `category` (string): Unit category (length, weight, temperature, volume, area)\n\n#### 6. `financial`\n**Purpose:** Financial calculations and modeling\n\n**Parameters:**\n- `operation` (string): Financial operation type (compound_interest, simple_interest, loan_payment, roi, present_value, future_value)\n- `principal` (number): Principal amount\n- `rate` (number): Interest rate (percentage)\n- `time` (number): Time period in years\n- `periods` (integer, optional): Compounding periods per year\n- `futureValue` (number, optional): Future value for some calculations\n\n### Specialized Tools (7)\n\n#### 7. `stats_summary`\n**Purpose:** Comprehensive statistical summary of datasets\n\n**Parameters:**\n- `data` (array of numbers): Dataset for summary statistics\n\n#### 8. `percentile`\n**Purpose:** Calculate specific percentiles\n\n**Parameters:**\n- `data` (array of numbers): Dataset to analyze\n- `percentile` (number): Percentile to calculate (0-100)\n\n#### 9. `batch_conversion`\n**Purpose:** Convert multiple values between units\n\n**Parameters:**\n- `values` (array of numbers): Values to convert\n- `fromUnit` (string): Source unit\n- `toUnit` (string): Target unit\n- `category` (string): Unit category\n\n#### 10. `npv`\n**Purpose:** Calculate Net Present Value\n\n**Parameters:**\n- `cashFlows` (array of numbers): Cash flows (negative for outflows, positive for inflows)\n- `discountRate` (number): Discount rate as percentage\n\n#### 11. `irr`\n**Purpose:** Calculate Internal Rate of Return\n\n**Parameters:**\n- `cashFlows` (array of numbers): Cash flows (minimum 2 values)\n\n#### 12. `loan_comparison`\n**Purpose:** Compare multiple loan scenarios\n\n**Parameters:**\n- `loans` (array of objects): Loan scenarios with principal, rate, and time\n\n#### 13. `investment_scenarios`\n**Purpose:** Compare multiple investment scenarios\n\n**Parameters:**\n- `scenarios` (array of objects): Investment scenarios with principal, rate, and time\n\n## üîß Configuration\n\n### Command Line Options\n\n```bash\n./calculator-server [OPTIONS]\n\nOptions:\n  -transport string\n        Transport method (stdio, http) (default \"stdio\")\n  -port int\n        Port for HTTP transport (default 8080)\n  -host string\n        Host for HTTP transport (default \"127.0.0.1\")\n  -config string\n        Path to configuration file (YAML or JSON)\n\nExamples:\n  ./calculator-server                           # Run with stdio transport (default)\n  ./calculator-server -transport=http          # Run with HTTP transport on port 8080\n  ./calculator-server -transport=http -port=9000 -host=localhost  # Custom host/port\n  ./calculator-server -config=config.yaml     # Load configuration from file\n```\n\n### Configuration Files\n\nThe server supports configuration files in YAML and JSON formats. Configuration files are searched in the following locations:\n\n1. Current directory (`./config.yaml`, `./config.json`)\n2. `./config/` directory\n3. `/etc/calculator-server/`\n4. `$HOME/.calculator-server/`\n\n#### Sample YAML Configuration\n\n```yaml\nserver:\n  transport: \"http\"\n  http:\n    host: \"127.0.0.1\"  # Localhost for security\n    port: 8080\n    session_timeout: \"5m\"\n    max_connections: 100\n    cors:\n      enabled: true\n      origins: [\"http://localhost:3000\", \"http://127.0.0.1:3000\"]  # Never use \"*\" in production\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n  output: \"stdout\"\n\ntools:\n  precision:\n    max_decimal_places: 15\n    default_decimal_places: 2\n  expression_eval:\n    timeout: \"10s\"\n    max_variables: 100\n  statistics:\n    max_data_points: 10000\n  financial:\n    currency_default: \"USD\"\n\nsecurity:\n  rate_limiting:\n    enabled: true\n    requests_per_minute: 100\n  request_size_limit: \"1MB\"\n```\n\n### Environment Variables\n\nEnvironment variables override configuration file settings:\n\n- `CALCULATOR_TRANSPORT`: Transport method (stdio, http)\n- `CALCULATOR_HTTP_HOST`: HTTP server host\n- `CALCULATOR_HTTP_PORT`: HTTP server port\n- `CALCULATOR_LOG_LEVEL`: Set logging level (debug, info, warn, error)\n- `CALCULATOR_LOG_FORMAT`: Log format (json, text)\n- `CALCULATOR_LOG_OUTPUT`: Log output (stdout, stderr)\n\n## üìà Performance\n\n### Benchmarks\n\n- **Basic Operations**: ~1-5 Œºs per operation\n- **Advanced Functions**: ~10-50 Œºs per operation  \n- **Expression Evaluation**: ~100-500 Œºs per expression\n- **Statistical Operations**: ~10-100 Œºs per dataset (depends on size)\n- **Unit Conversions**: ~1-10 Œºs per conversion\n- **Financial Calculations**: ~50-200 Œºs per calculation\n\n### Memory Usage\n\n- **Base Memory**: ~10-20 MB\n- **Per Operation**: ~1-10 KB additional\n- **Large Datasets**: Linear scaling with data size\n\n## üß™ Testing\n\nThe project includes comprehensive tests with >95% coverage:\n\n- **Unit Tests**: Test individual calculators and functions\n- **Integration Tests**: Test MCP protocol integration\n- **Error Handling Tests**: Validate error conditions\n- **Performance Tests**: Benchmark critical operations\n\n```bash\n# Run specific test suites\ngo test ./tests/basic_test.go -v\ngo test ./tests/advanced_test.go -v\ngo test ./tests/integration_test.go -v\n\n# Generate coverage report\nmake coverage\n```\n\n## üö¢ Deployment\n\n### Docker Deployment\n\n```bash\n# Build Docker image\nmake docker-build\n\n# Run in Docker\nmake docker-run\n\n# Push to registry\nmake docker-push\n```\n\n### Binary Distribution\n\n```bash\n# Create release build\nmake release\n\n# Binaries will be in ./dist/release/\nls -la ./dist/release/\n```\n\n## üìù API Reference\n\n### MCP Protocol Support\n\nThe server implements the full MCP (Model Context Protocol) specification:\n\n- **Initialize**: Server initialization and capability negotiation\n- **Tools List**: Dynamic tool discovery\n- **Tools Call**: Tool execution with parameter validation\n- **Error Handling**: Comprehensive error responses\n\n### Tool Schemas\n\nAll tools include comprehensive JSON Schema definitions for parameter validation and documentation. Schemas are automatically generated and include:\n\n- Parameter types and validation rules\n- Required vs optional parameters\n- Default values and constraints\n- Documentation strings\n\n## üìè Unit Conversion Reference\n\n### Length Units\n| Unit | Abbreviation | Conversion to Meters |\n|------|--------------|---------------------|\n| Millimeter | `mm` | 0.001 |\n| Centimeter | `cm` | 0.01 |\n| Meter | `m` | 1.0 |\n| Kilometer | `km` | 1000.0 |\n| Inch | `in` | 0.0254 |\n| Foot | `ft` | 0.3048 |\n| Yard | `yd` | 0.9144 |\n| Mile | `mi` | 1609.344 |\n| Mil | `mil` | 0.0000254 |\n| Micrometer | `Œºm` | 0.000001 |\n| Nanometer | `nm` | 0.000000001 |\n\n### Weight/Mass Units\n| Unit | Abbreviation | Conversion to Grams |\n|------|--------------|-------------------|\n| Milligram | `mg` | 0.001 |\n| Gram | `g` | 1.0 |\n| Kilogram | `kg` | 1000.0 |\n| Metric Ton | `t` | 1000000.0 |\n| Ounce | `oz` | 28.3495 |\n| Pound | `lb` | 453.592 |\n| Stone | `st` | 6350.29 |\n| US Ton | `ton` | 907185 |\n\n### Temperature Units\n| Unit | Abbreviation | Description |\n|------|--------------|-------------|\n| Celsius | `C` | Degrees Celsius |\n| Fahrenheit | `F` | Degrees Fahrenheit |\n| Kelvin | `K` | Kelvin (absolute) |\n| Rankine | `R` | Degrees Rankine |\n\n### Volume Units\n| Unit | Abbreviation | Conversion to Liters |\n|------|--------------|---------------------|\n| Milliliter | `ml` | 0.001 |\n| Centiliter | `cl` | 0.01 |\n| Deciliter | `dl` | 0.1 |\n| Liter | `l` | 1.0 |\n| Kiloliter | `kl` | 1000.0 |\n| US Fluid Ounce | `fl_oz` | 0.0295735 |\n| US Cup | `cup` | 0.236588 |\n| US Pint | `pt` | 0.473176 |\n| US Quart | `qt` | 0.946353 |\n| US Gallon | `gal` | 3.78541 |\n| Teaspoon | `tsp` | 0.00492892 |\n| Tablespoon | `tbsp` | 0.0147868 |\n| Barrel | `bbl` | 158.987 |\n\n### Area Units\n| Unit | Abbreviation | Conversion to m¬≤ |\n|------|--------------|------------------|\n| Square Millimeter | `mm2` | 0.000001 |\n| Square Centimeter | `cm2` | 0.0001 |\n| Square Meter | `m2` | 1.0 |\n| Square Kilometer | `km2` | 1000000.0 |\n| Square Inch | `in2` | 0.00064516 |\n| Square Foot | `ft2` | 0.092903 |\n| Square Yard | `yd2` | 0.836127 |\n| Square Mile | `mi2` | 2589988.11 |\n| Acre | `acre` | 4046.86 |\n| Hectare | `ha` | 10000.0 |\n\n## üî¢ Mathematical Functions Reference\n\n### Trigonometric Functions\n| Function | Syntax | Description | Example |\n|----------|--------|-------------|---------|\n| Sine | `sin(x)` | Sine of x (radians) | `sin(pi/2)` ‚Üí 1.0 |\n| Cosine | `cos(x)` | Cosine of x (radians) | `cos(0)` ‚Üí 1.0 |\n| Tangent | `tan(x)` | Tangent of x (radians) | `tan(pi/4)` ‚Üí 1.0 |\n| Arcsine | `asin(x)` | Inverse sine | `asin(1)` ‚Üí 1.5708 |\n| Arccosine | `acos(x)` | Inverse cosine | `acos(1)` ‚Üí 0.0 |\n| Arctangent | `atan(x)` | Inverse tangent | `atan(1)` ‚Üí 0.7854 |\n\n### Logarithmic Functions\n| Function | Syntax | Description | Example |\n|----------|--------|-------------|---------|\n| Common Log | `log(x)` | Base-10 logarithm | `log(100)` ‚Üí 2.0 |\n| Natural Log | `ln(x)` | Natural logarithm (base e) | `ln(e)` ‚Üí 1.0 |\n\n### Power & Root Functions\n| Function | Syntax | Description | Example |\n|----------|--------|-------------|---------|\n| Square Root | `sqrt(x)` | Square root of x | `sqrt(16)` ‚Üí 4.0 |\n| Power | `pow(x, y)` | x raised to power y | `pow(2, 3)` ‚Üí 8.0 |\n| Exponential | `exp(x)` | e raised to power x | `exp(1)` ‚Üí 2.7183 |\n\n### Other Functions\n| Function | Syntax | Description | Example |\n|----------|--------|-------------|---------|\n| Absolute Value | `abs(x)` | Absolute value of x | `abs(-5)` ‚Üí 5.0 |\n| Factorial | `factorial(x)` | Factorial of x | `factorial(5)` ‚Üí 120.0 |\n\n### Mathematical Constants\n| Constant | Value | Description |\n|----------|-------|-------------|\n| `pi` | 3.14159... | Pi (œÄ) |\n| `e` | 2.71828... | Euler's number |\n\n## ü§ù Contributing\n\n1. **Fork** the repository\n2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)\n3. **Commit** changes (`git commit -m 'Add amazing feature'`)\n4. **Push** to branch (`git push origin feature/amazing-feature`)\n5. **Create** a Pull Request\n\n### Development Guidelines\n\n- Follow Go best practices and conventions\n- Maintain >95% test coverage\n- Add comprehensive documentation\n- Use meaningful commit messages\n- Run `make quality` before submitting\n\n## üìÑ License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- **Go Team**: For the excellent programming language\n- **MCP Protocol**: Model Context Protocol specification\n- **External Libraries**:\n  - [`shopspring/decimal`](https://github.com/shopspring/decimal): Precise decimal arithmetic\n  - [`Knetic/govaluate`](https://github.com/Knetic/govaluate): Expression evaluation\n  - [`gonum`](https://gonum.org/): Scientific computing\n  - [`gopkg.in/yaml.v3`](https://gopkg.in/yaml.v3): YAML configuration support\n\n## üìû Support & Contact\n\n**Primary Contact:**\n- **Maintainer**: Avinash Sangle\n- **Email**: avinash.sangle123@gmail.com\n- **GitHub**: [https://github.com/avisangle](https://github.com/avisangle)\n- **Website**: [https://avisangle.github.io/](https://avisangle.github.io/)\n\n**Project Resources:**\n- **Issues**: [GitHub Issues](https://github.com/IBM/mcp-context-forge/issues)\n- **Documentation**: This README and inline code documentation\n- **Examples**: See `make example-*` commands\n\n**Getting Help:**\n1. Check this README for comprehensive documentation\n2. Review the test files for usage examples\n3. Submit issues with detailed error information\n4. Contact the maintainer for direct support\n\n---\n\n**Built with ‚ù§Ô∏è by Avinash Sangle for the IBM MCP Context Forge project**\n\n**Connect with the Author:**\n- üåê Website: [https://avisangle.github.io/](https://avisangle.github.io/)\n- üíª GitHub: [https://github.com/avisangle](https://github.com/avisangle)\n- üìß Email: avinash.sangle123@gmail.com\n\nFor more information about MCP servers and the Context Forge project, visit the [IBM MCP Context Forge repository](https://github.com/IBM/mcp-context-forge).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "calculator",
        "computations",
        "calculator server",
        "integrations tools",
        "mathematical tools"
      ],
      "category": "data-science-tools"
    },
    "datalayer--jupyter-mcp-server": {
      "owner": "datalayer",
      "name": "jupyter-mcp-server",
      "url": "https://github.com/datalayer/jupyter-mcp-server",
      "imageUrl": "",
      "description": "Model Context Protocol (MCP) Server for Jupyter.",
      "stars": 691,
      "forks": 117,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Jupyter Notebook",
      "updated_at": "2025-10-03T20:57:42Z",
      "readme_content": "<!--\n  ~ Copyright (c) 2023-2024 Datalayer, Inc.\n  ~\n  ~ BSD 3-Clause License\n-->\n\n[![Datalayer](https://assets.datalayer.tech/datalayer-25.svg)](https://datalayer.io)\n\n[![Become a Sponsor](https://img.shields.io/static/v1?label=Become%20a%20Sponsor&message=%E2%9D%A4&logo=GitHub&style=flat&color=1ABC9C)](https://github.com/sponsors/datalayer)\n\n# ü™ê‚ú® Jupyter MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/jupyter-mcp-server)](https://pypi.org/project/jupyter-mcp-server)\n<a href=\"https://mseep.ai/app/datalayer-jupyter-mcp-server\">\n<img src=\"https://mseep.net/pr/datalayer-jupyter-mcp-server-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" width=\"100\" />\n</a>\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/datalayer/jupyter-mcp-server)](https://archestra.ai/mcp-catalog/datalayer__jupyter-mcp-server)\n\n> üö® **NEW IN 0.14.0:** Multi-notebook support! You can now seamlessly switch between multiple notebooks in a single session. [Read more in the release notes.](https://jupyter-mcp-server.datalayer.tech/releases)\n\n**Jupyter MCP Server** is a [Model Context Protocol](https://modelcontextprotocol.io) (MCP) server implementation that enables **real-time** interaction with üìì Jupyter Notebooks, allowing AI to edit, document and execute code for data analysis, visualization etc.\n\nCompatible with any Jupyter deployment (local, JupyterHub, ...) and with [Datalayer](https://datalayer.ai/) hosted Notebooks.\n\n## üöÄ Key Features\n\n- ‚ö° **Real-time control:** Instantly view notebook changes as they happen.\n- üîÅ **Smart execution:** Automatically adjusts when a cell run fails thanks to cell output feedback.\n- üß† **Context-aware:** Understands the entire notebook context for more relevant interactions.\n- üìä **Multimodal support:** Support different output types, including images, plots, and text.\n- üìÅ **Multi-notebook support:** Seamlessly switch between multiple notebooks.\n- ü§ù **MCP-compatible:** Works with any MCP client, such as Claude Desktop, Cursor, Windsurf, and more.\n\n![Jupyter MCP Server Demo](https://assets.datalayer.tech/jupyter-mcp/mcp-demo-multimodal.gif)\n\nüõ†Ô∏è This MCP offers multiple tools such as `insert_cell`, `execute_cell`, `list_all_files`, `read_cell`, and more, enabling advanced interactions with Jupyter notebooks. Explore our [tools documentation](https://jupyter-mcp-server.datalayer.tech/tools) to learn about all the tools powering Jupyter MCP Server.\n\n## üèÅ Getting Started\n\nFor comprehensive setup instructions‚Äîincluding `Streamable HTTP` transport and advanced configuration‚Äîcheck out [our documentation](https://jupyter-mcp-server.datalayer.tech/). Or, get started quickly with `JupyterLab` and `stdio` transport here below.\n\n### 1. Set Up Your Environment\n\n```bash\npip install jupyterlab==4.4.1 jupyter-collaboration==4.0.2 ipykernel\npip uninstall -y pycrdt datalayer_pycrdt\npip install datalayer_pycrdt==0.12.17\n```\n\n### 2. Start JupyterLab\n\n```bash\n# make jupyterlab\njupyter lab --port 8888 --IdentityProvider.token MY_TOKEN --ip 0.0.0.0\n```\n\n> [!NOTE]\n> If you are running notebooks through JupyterHub instead of JupyterLab as above, you should:\n>\n> - Set the environment variable `JUPYTERHUB_ALLOW_TOKEN_IN_URL=1` in the single-user environment.\n> - Ensure your API token (`MY_TOKEN`) is created with `access:servers` scope in the Hub.\n\n\n### 3. Configure Your Preferred MCP Client\n\n> [!TIP]\n>\n> 1. Ensure the `port` of the `DOCUMENT_URL` and `RUNTIME_URL` match those used in the `jupyter lab` command.\n>\n> 2. In a basic setup, `DOCUMENT_URL` and `RUNTIME_URL` are the same. `DOCUMENT_TOKEN`, and `RUNTIME_TOKEN` are also the same and is actually the Jupyter Token.\n>\n> 3. The `DOCUMENT_ID` parameter specifies the path to the notebook you want to connect to. It should be relative to the directory where JupyterLab was started.  \n> \n> - **Optional:** If you omit `DOCUMENT_ID`, the MCP client can automatically list all available notebooks on the Jupyter server, allowing you to select one interactively via your prompts.\n> - **Flexible:** Even if you set `DOCUMENT_ID`, the MCP client can still browse, list, switch to, or even create new notebooks at any time.\n> \n\n#### MacOS and Windows\n\n```json\n{\n  \"mcpServers\": {\n    \"jupyter\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"DOCUMENT_URL\",\n        \"-e\", \"DOCUMENT_TOKEN\",\n        \"-e\", \"DOCUMENT_ID\",\n        \"-e\", \"RUNTIME_URL\",\n        \"-e\", \"RUNTIME_TOKEN\",\n        \"-e\", \"ALLOW_IMG_OUTPUT\",\n        \"datalayer/jupyter-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://host.docker.internal:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://host.docker.internal:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"ALLOW_IMG_OUTPUT\": \"true\"\n      }\n    }\n  }\n}\n```\n\n#### Linux\n\n```json\n{\n  \"mcpServers\": {\n    \"jupyter\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"DOCUMENT_URL\",\n        \"-e\", \"DOCUMENT_TOKEN\",\n        \"-e\", \"DOCUMENT_ID\",\n        \"-e\", \"RUNTIME_URL\",\n        \"-e\", \"RUNTIME_TOKEN\",\n        \"-e\", \"ALLOW_IMG_OUTPUT\",\n        \"--network=host\",\n        \"datalayer/jupyter-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://localhost:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://localhost:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"ALLOW_IMG_OUTPUT\": \"true\"\n      }\n    }\n  }\n}\n```\n\nFor detailed instructions on configuring various MCP clients‚Äîincluding [Claude Desktop](https://jupyter-mcp-server.datalayer.tech/clients/claude_desktop), [VS Code](https://jupyter-mcp-server.datalayer.tech/clients/vscode), [Cursor](https://jupyter-mcp-server.datalayer.tech/clients/cursor), [Cline](https://jupyter-mcp-server.datalayer.tech/clients/cline), and [Windsurf](https://jupyter-mcp-server.datalayer.tech/clients/windsurf) ‚Äî see the [Clients documentation](https://jupyter-mcp-server.datalayer.tech/clients).\n\n## üìö Resources\n\nLooking for blog posts, videos, or other materials about Jupyter MCP Server?\n\nüëâ Visit the [Resources section](https://jupyter-mcp-server.datalayer.tech/resources).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datalayer",
        "jupyter",
        "tools",
        "datalayer jupyter",
        "server jupyter",
        "jupyter mcp"
      ],
      "category": "data-science-tools"
    },
    "growthbook--growthbook-mcp": {
      "owner": "growthbook",
      "name": "growthbook-mcp",
      "url": "https://github.com/growthbook/growthbook-mcp",
      "imageUrl": "",
      "description": "[HumanSignal/label-studio-mcp-server](https://github.com/HumanSignal/label-studio-mcp-server) üéñÔ∏è üêç ‚òÅÔ∏è ü™ü üêß üçé - Create, manage, and automate Label Studio projects, tasks, and predictions for data labeling workflows.",
      "stars": 15,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-19T19:30:43Z",
      "readme_content": "# GrowthBook MCP Server\n\nWith the GrowthBook MCP server, you can interact with GrowthBook right from your LLM client. See experiment details, add a feature flag, and more.\n\n<a href=\"https://glama.ai/mcp/servers/@growthbook/growthbook-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@growthbook/growthbook-mcp/badge\" alt=\"GrowthBook Server MCP server\" />\n</a>\n\n## Setup\n\n**Environment Variables**\nUse the following env variables to configure the MCP server.\n\n| Variable Name | Status   | Description                                                       |\n| ------------- | -------- | ----------------------------------------------------------------- |\n| GB_API_KEY    | Required | A GrowthBook API key or PAT. When using a PAT, MCP server capabilities are limited by its permissions. E.g., if the user can't create an experiment in the app, they also won't be able to create one with the MCP server.                                             |\n| GB_EMAIL      | Required | Your email address used with GrowthBook. Used when creating feature flags and experiments.|\n| GB_API_URL    | Optional | Your GrowthBook API URL. Defaults to `https://api.growthbook.io`. |\n| GB_APP_ORIGIN | Optional | Your GrowthBook app URL Defaults to `https://app.growthbook.io`.  |\n\n\nAdd the MCP server to your AI tool of choice. See the [official docs](https://docs.growthbook.io/integrations/mcp) for complete a complete guide.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "growthbook",
        "tools",
        "workflows",
        "workflows growthbook",
        "growthbook mcp",
        "tools integrations"
      ],
      "category": "data-science-tools"
    },
    "jjsantos01--jupyter-notebook-mcp": {
      "owner": "jjsantos01",
      "name": "jupyter-notebook-mcp",
      "url": "https://github.com/jjsantos01/jupyter-notebook-mcp",
      "imageUrl": "",
      "description": "connects Jupyter Notebook to Claude AI, allowing Claude to directly interact with and control Jupyter Notebooks.",
      "stars": 110,
      "forks": 21,
      "license": "MIT License",
      "language": "Jupyter Notebook",
      "updated_at": "2025-09-27T11:36:15Z",
      "readme_content": "# JupyterMCP - Jupyter Notebook Model Context Protocol Integration\n\nJupyterMCP connects [Jupyter Notebook](https://jupyter.org/) to [Claude AI](https://claude.ai/chat) through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Jupyter Notebooks. This integration enables AI-assisted code execution, data analysis, visualization, and more.\n\n## ‚ö†Ô∏è Compatibility Warning\n\n**This tool is compatible ONLY with Jupyter Notebook version 6.x.**\n\nIt does NOT work with:\n\n- Jupyter Lab\n- Jupyter Notebook v7.x\n- VS Code Notebooks\n- Google Colab\n- Any other notebook interfaces\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to Jupyter Notebook through a WebSocket-based server\n- **Cell manipulation**: Insert, execute, and manage notebook cells\n- **Notebook management**: Save notebooks and retrieve notebook information\n- **Cell execution**: Run specific cells or execute all cells in a notebook\n- **Output retrieval**: Get output content from executed cells with text limitation options\n\n## Components\n\nThe system consists of three main components:\n\n1. **WebSocket Server (`jupyter_ws_server.py`)**: Sets up a WebSocket server inside Jupyter that bridges communication between notebook and external clients\n2. **Client JavaScript (`client.js`)**: Runs in the notebook to handle operations (inserting cells, executing code, etc.)\n3. **MCP Server (`jupyter_mcp_server.py`)**: Implements the Model Context Protocol and connects to the WebSocket server\n\n## Installation\n\n### Prerequisites\n\n- [Python 3.12 or newer](https://www.python.org/downloads/) (probably also work with older versions, but not tested)\n- [`uv` package manager](/README.md#installing-uv)\n- [Claude AI desktop application](https://claude.ai/download)\n\n#### Installing uv\n\nIf you're on Mac:\n\n```bash\nbrew install uv\n```\n\nOn Windows (PowerShell):\n\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nFor other platforms, see the [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Setup\n\n1. Clone or download this repository to your computer:\n\n   ```bash\n   git clone https://github.com/jjsantos01/jupyter-notebook-mcp.git\n   ```\n\n2. Create virtual environment with required packages an install `jupyter-mcp` kernel, so it can be recognized by your jupyter installation, if you had one before.\n\n   ```bash\n   uv run python -m ipykernel install --name jupyter-mcp\n   ```\n\n3. (optional) Install additional Python packages for your analysis:\n\n   ```bash\n   uv pip install seaborn\n   ```\n\n4. Configure Claude desktop integration:\n   Go to `Claude` > `Settings` > `Developer` > `Edit Config` > `claude_desktop_config.json` to include the following:\n\n   ```json\n      {\n       \"mcpServers\": {\n           \"jupyter\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"/ABSOLUTE/PATH/TO/PARENT/REPO/FOLDER/src\",\n                   \"run\",\n                   \"jupyter_mcp_server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n   Replace `/ABSOLUTE/PATH/TO/` with the actual path to the `src` folder on your system. For example:\n   - Windows: `\"C:\\\\Users\\\\MyUser\\\\GitHub\\\\jupyter-notebook-mcp\\\\src\\\\\"`\n   - Mac: `/Users/MyUser/GitHub/jupyter-notebook-mcp/src/`\n\n   If you had previously opened Claude, then `File` > `Exit` and open it again.\n\n## Usage\n\n### Starting the Connection\n\n1. Start your Jupyter Notebook (version 6.x) server:\n\n   ```bash\n   uv run jupyter nbclassic\n   ```\n\n2. Create a new Jupyter Notebook and make sure that you choose the `jupyter-mcp` kernel: `kernel` -> `change kernel` -> `jupyter-mcp`\n\n3. In a notebook cell, run the following code to initialize the WebSocket server:\n\n   ```python\n   import sys\n   sys.path.append('/path/to/jupyter-notebook-mcp/src')  # Add the path to where the scripts are located\n   \n   from jupyter_ws_server import setup_jupyter_mcp_integration\n   \n   # Start the WebSocket server inside Jupyter\n   server, port = setup_jupyter_mcp_integration()\n   ```\n\n   Don't forget to replace here `'/path/to/jupyter-notebook-mcp/src'` with `src` folder on your system. For example:\n   - Windows: `\"C:\\\\Users\\\\MyUser\\\\GitHub\\\\jupyter-notebook-mcp\\\\src\\\\\"`\n   - Mac: `/Users/MyUser/GitHub/jupyter-notebook-mcp/src/`\n\n   \n\n4. Launch Claude desktop with MCP enabled.\n\n### Using with Claude\n\nOnce connected, Claude will have access to the following tools:\n\n- `ping` - Check server connectivity\n- `insert_and_execute_cell` - Insert a cell at the specified position and execute it\n- `save_notebook` - Save the current Jupyter notebook\n- `get_cells_info` - Get information about all cells in the notebook\n- `get_notebook_info` - Get information about the current notebook\n- `run_cell` - Run a specific cell by its index\n- `run_all_cells` - Run all cells in the notebook\n- `get_cell_text_output` - Get the output content of a specific cell\n- `get_image_output` - Get the images output of a specific cell\n- `edit_cell_content` - Edit the content of an existing cell\n- `set_slideshow_type`- Set the slide show type for cell\n\n## ‚ö†Ô∏è DISCLAIMER\n\nThis is an experimental project and should be used with caution. This tool runs arbitrary Python code in your computer, which could potentially modify or delete data if not used carefully. Always back up your important projects and data.\n\n## Example Prompts\n\nAsk Claude to perform notebook operations:\n\n### Python example\n\nYou can check the [example notebook](/notebooks/example_notebook.ipynb) and the [video demo](https://x.com/jjsantoso/status/1906780778807390562)\n\n```plain\nYou have access to a Jupyter Notebook server.\n\nI need to create a presentation about Python's Seaborn library.  \nThe content is as follows:\n\n- What is Seaborn?\n- Long vs. Wide data format\n- Advantages of Seaborn over Matplotlib\n- Commonly used Seaborn functions\n- Live demonstration (comparison of Seaborn vs. Matplotlib)\n  - Bar plot\n  - Line plot\n  - Scatter plot\n\nFor each concept, I want the main explanations provided in markdown cells, followed by one or more Python code cells demonstrating its usage. Keep the text concise‚Äîthe cells shouldn't exceed 10 lines each.\n\nUse appropriate slideshow types for each cell to make the presentation visually appealing.\n```\n\n[Check Here the full conversation](https://claude.ai/share/420b6aa6-b84b-437f-a6a6-89d310c36d52)\n\n### Stata example\n\nFor this example, you need the [Stata Software](https://www.stata.com/) (v17 or later), which is not open source. If you already have Stata, you need to install the [`stata-setup`](https://pypi.org/project/stata-setup/) package:\n\n```bash\nuv pip install stata-setup\n```\n\nThen, at the begining of your notebook, you need to additionally include:\n\n```python\nimport stata_setup\nstata_setup.config('your_stata_installation_directory', 'your_stata_edition')\n```\n\nYou can check the [example notebook](/notebooks/stata_example.ipynb) and the [video demo](https://x.com/jjsantoso/status/1906780784800731251)\n\nThis exercise comes from [Professor John Robert Warren webpage](https://www.rob-warren.com/soc3811_stata_exercises.html)\n\n```plain\nYou have access to a Jupyter Notebook server. By default it runs Python, but you can run Stata (v18) code in this server using the %%stata magic, for example:\n\n%%stata\ndisplay \"hello world\"\n\nRun the available tools to solve the exercise, execute the code, and interpret the results.\n\n**EXERCISE:**\n\nIn this exercise, you will use data from the American Community Survey (ACS). The ACS is a product of the U.S. Census Bureau and involves interviewing millions of Americans each year. For an introduction to the ACS, visit the ACS website (here).\n\nFor this exercise, I have created a data file containing two variables collected from respondents of the 2010 ACS who lived in one of two metropolitan areas: Minneapolis/St Paul and Duluth/Superior. The two variables are: (1) People's poverty status and (2) the time it takes people to commute to work.\n\nUse STATA syntax files you already have (from the first assignment or class examples) and modify them to accomplish the following goals.\n\n1. Read the data file (`\"./stata_assignment_2.dat\"`) for this assignment into STATA.\n2. Be sure to declare \"zero\" as a missing value for `TRANTIME`, the commuting time variable.\n3. Create a new dichotomous poverty variable that equals \"1\" if a person's income-to-poverty-line ratio (`POVRATIO`) is less than 100, and \"0\" otherwise; see the bottom of the assignment for an example of how to do this in STATA.\n4. Separately for Minneapolis/St Paul and Duluth/Superior, produce:\n   - a histogram of the commuting time (`TRANTIME`) variable.\n   - measures of central tendency and spread for commuting time.\n   - a frequency distribution for the poverty status (0 vs 1) variable.\n5. Separately for Minneapolis/St Paul and Duluth/Superior, use STATA code to produce:\n   - a 95% confidence interval for the mean commuting time.\n   - a 95% confidence interval for the proportion of people who are poor. See below for an example of how to do this in STATA.\n\nUse the results from step #4 above to:\n\n6. Separately for Minneapolis/St Paul and Duluth/Superior, manually calculate:\n   - a 95% confidence interval for the mean commuting time.\n   - a 95% confidence interval for the proportion of people who are poor.\n7. Confirm that your answers from steps #5 and #6 match.\n\nBased on the results above, answer this question:\n\n8. How do you interpret the confidence intervals calculated in steps #5 and #6 above?\n\n9. Finally, create a do file (.do) with the all the Stata code and the answers as comments.\n\n---\n\n**DESCRIPTION OF VARIABLES IN \"STATA ASSIGNMENT 2.DAT\"**\n\n**METAREAD** (Column 4-7)  \nMetropolitan Area  \n- `2240`: Duluth-Superior, MN/WI  \n- `5120`: Minneapolis-St. Paul, MN  \n\n**POVRATIO** (Column 18-20)  \nRatio of person's income to the poverty threshold:  \n- `<100`: Below Poverty Line  \n- `100`: At Poverty Line  \n- `>100`: Above Poverty Line  \n\n**TRANTIME** (Column 21-23)  \nTravel time to work  \n- `0`: Zero minutes  \n- `1`: 1 Minute  \n- etc.\n\n```\n\n[Check Here the full conversation](https://claude.ai/share/97b5a546-9375-434d-8224-561706782880)\n\n## Testing with External Client\n\nYou can test the functionality without using Claude Desktop with the included external client:\n\n```bash\nuv run python src/jupyter_ws_external_client.py\n```\n\nThis will provide an interactive menu to test some available functions.\n\nFor automated testing of all commands:\n\n```bash\nuv run python src/jupyter_ws_external_client.py --batch\n```\n\n## Troubleshooting\n\n- **Connection Issues**: If you experience connection timeouts, the client includes a reconnection mechanism. You can also try restarting the WebSocket server.\n- **Cell Execution Problems**: If cell execution doesn't work, check that the cell content is valid Python/Markdown and that the notebook kernel is running.\n- **WebSocket Port Conflicts**: If the default port (8765) is already in use, the server will automatically try to find an available port.\n\n## Limitations\n\n- Only supports Jupyter Notebook 6.x\n- Text output from cells is limited to 1500 characters by default\n- Does not support advanced Jupyter widget interactions\n- Connection may timeout after periods of inactivity\n\n## License\n\n[MIT](/LICENSE)\n\n## Other Jupyter MCPs\n\nThis project is inspired by similar MCP integrations for Jupyter as:\n\n- [ihrpr](https://github.com/ihrpr/mcp-server-jupyter)\n- [Datalayer](https://github.com/datalayer/jupyter-mcp-server/tree/main)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "jupyter",
        "data",
        "jupyter notebooks",
        "jupyter notebook",
        "tools integrations"
      ],
      "category": "data-science-tools"
    },
    "kdqed--zaturn": {
      "owner": "kdqed",
      "name": "zaturn",
      "url": "https://github.com/kdqed/zaturn",
      "imageUrl": "",
      "description": "Link multiple data sources (SQL, CSV, Parquet, etc.) and ask AI to analyze the data for insights and visualizations.",
      "stars": 60,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T23:36:09Z",
      "readme_content": "<h1>\n  <img alt=\"logo\" src=\"https://github.com/kdqed/zaturn/raw/main/zaturn/studio/static/logo.png\" width=\"24\" height=\"24\">\n  <span>Zaturn: Your Co-Pilot For Data Analytics & Business Insights</span>\n</h1>\n\n<a href=\"https://discord.gg/K8mECeVzpQ\">\n  Join the Discord\n</a>\n\n## Just Chat With Your Data! No SQL, No Python.\n\nZaturn provides tools that enable AI models to run SQL, so you don't have to. It can be used as an MCP or as a web interface similar to Jupyter Notebook.\n\n## Zaturn in Action\n\nhttps://github.com/user-attachments/assets/d42dc433-e5ec-4b3e-bef0-5cfc097396ab\n\n## Features:\n\n### Multiple Data Sources \n\nZaturn can currently connect to the following data sources: \n- SQL Databases: PostgreSQL, SQLite, DuckDB, MySQL, ClickHouse, SQL Server\n- Files: CSV, Parquet\n\nConnectors for more data sources are being added.\n\n### Visualizations\n\nIn addition to providing tabular and textual summaries, Zaturn can also generate the following image visualizations\n\n- Scatter and Line Plots\n- Histograms\n- Strip and Box Plots\n- Bar Plots\n\nMore visualization capabilities are being added.\n\n\n## Installation & Setup\n\nSee [https://zaturn.pro/docs/install](https://zaturn.pro/docs/install)\n\n\n## Roadmap\n\n- Support for more data source types\n- More data visualizations\n- Predictive analysis and forecasting, e.g.:\n```\nBased on the revenue of the last 3 months, forecast next month's revenue.\n```\n- Generate Presentations & PDFs\n```\nManager: \n  I need a presentation to show the boss. Can you do it by EOD?\nAnalyst: \n  EOD?! Are you still in the 2010s? \n  I can get it done right now. Actually, you can do it right now.\n  You know what? The boss can do it right now.\n```\n\n## Help And Feedback\n\n[Raise an issue](https://github.com/kdqed/zaturn/issues) or [join the Discord](https://discord.gg/K8mECeVzpQ).\n\n\n## Support The Project\n\nIf you find Zaturn useful, please support this project by:\n- Starring the Project\n- Spreading the word\n\nYour support will enable me to dedicate more of my time to Zaturn.\n\n## Example Dataset Credits\n\nThe [pokemon dataset compiled by Sarah Taha and Pok√©API](https://www.kaggle.com/datasets/sarahtaha/1025-pokemon) has been included under the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license for demonstration purposes.\n\n## Featured on glama.ai\n\n<a href=\"https://glama.ai/mcp/servers/@kdqed/zaturn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kdqed/zaturn/badge\" alt=\"Zaturn MCP server\" />\n</a>\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=kdqed/zaturn&type=Date)](https://www.star-history.com/#kdqed/zaturn&Date)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "data",
        "visualizations",
        "data science",
        "integrations tools",
        "data exploration"
      ],
      "category": "data-science-tools"
    },
    "mckinsey--vizro-mcp": {
      "owner": "mckinsey",
      "name": "vizro-mcp",
      "url": "https://github.com/mckinsey/vizro/tree/main/vizro-mcp",
      "imageUrl": "",
      "description": "Tools and templates to create validated and maintainable data charts and dashboards.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "charts",
        "integrations",
        "tools integrations",
        "integrations tools",
        "mcp tools"
      ],
      "category": "data-science-tools"
    },
    "optuna--optuna-mcp": {
      "owner": "optuna",
      "name": "optuna-mcp",
      "url": "https://github.com/optuna/optuna-mcp",
      "imageUrl": "",
      "description": "Official MCP server enabling seamless orchestration of hyperparameter search and other optimization tasks with [Optuna](https://optuna.org/).",
      "stars": 60,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T06:47:03Z",
      "readme_content": "# Optuna MCP Server\n\n[![Python](https://img.shields.io/badge/python-3.12%20%7C%203.13-blue)](https://www.python.org)\n[![pypi](https://img.shields.io/pypi/v/optuna-mcp.svg)](https://pypi.python.org/pypi/optuna-mcp)\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna-mcp)\n[![Tests](https://github.com/optuna/optuna-mcp/actions/workflows/tests.yml/badge.svg)](https://github.com/optuna/optuna-mcp/actions/workflows/tests.yml)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that automates optimization and analysis using [Optuna](http://optuna.org).\n\n<img width=\"840\" alt=\"image\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-6.png\" />\n\n## Use Cases\n\nThe Optuna MCP Server can be used in the following use cases, for example.\n\n- Automated hyperparameter optimization by LLMs\n- Interactive analysis of Optuna's optimization results via chat interface\n- Optimize input and output of other MCP tools\n\nFor details, see the [Examples section](#examples).\n\n## Installation\n\nThe Optuna MCP server can be installed using `uv` or Docker.\nThis section explains how to install the Optuna MCP server, using Claude Desktop as an example MCP client.\n\n### Usage with uv\n\nBefore starting the installation process, install `uv` from [Astral](https://docs.astral.sh/uv/getting-started/installation/).\n\nThen, add the Optuna MCP server configuration to the MCP client.\nTo include it in Claude Desktop, go to Claude > Settings > Developer > Edit Config > `claude_desktop_config.json`\nand add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"Optuna\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"optuna-mcp\"\n      ]\n    }\n  }\n}\n```\n\nAdditionally, you can specify the Optuna storage with the `--storage` argument to persist the results.\n\n```json\n{\n  \"mcpServers\": {\n    \"Optuna\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"optuna-mcp\"\n        \"--storage\",\n        \"sqlite:///optuna.db\"\n      ]\n    }\n  }\n}\n```\n\nAfter adding this, please restart Claude Desktop application.\nFor more information about Claude Desktop, check out [the quickstart page](https://modelcontextprotocol.io/quickstart/user).\n\n### Usage with Docker\n\nYou can also run the Optuna MCP server using Docker. Make sure you have Docker installed and running on your machine.\n\n```json\n{\n  \"mcpServers\": {\n    \"Optuna\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--net=host\",\n        \"-v\",\n        \"/PATH/TO/LOCAL/DIRECTORY/WHICH/INCLUDES/DB/FILE:/app/workspace\",\n        \"optuna/optuna-mcp:latest\",\n        \"--storage\",\n        \"sqlite:////app/workspace/optuna.db\"\n      ]\n    }\n  }\n}\n```\n\n## Tools provided by Optuna MCP\n\nThe Optuna MCP provides the following tools.\nSpecifically, it offers primitive functions of Optuna such as Study, Trial, Visualization, and Dashboard.\nSince MCP clients know the list of tools and the details of each tool, users do not need to remember those details.\n\n### Study\n\n- **create_study** - Create a new Optuna study with the given study_name and directions.\n  If the study already exists, it will be simply loaded.\n  - `study_name` : name of the study (string, required).\n  - `directions`: The directions of optimization (list of literal strings minimize/maximize, optional).\n- **set_sampler** - Set the sampler for the study.\n  - `name` : the name of the sampler (string, required).\n- **get_all_study_names** - Get all study names from the storage.\n- **set_metric_names** - Set metric_names. Metric_names are labels used to distinguish what each objective value is.\n  - `metric_names` : The list of metric names for each objective (list of strings, required).\n- **get_metric_names** - Get metric_names.\n  - No parameters required.\n- **get_directions** - Get the directions of the study.\n  - No parameters required.\n- **get_trials** - Get all trials in a CSV format.\n  - No parameters required.\n- **best_trial** - Get the best trial.\n  - No parameters required.\n- **best_trials** - Return trials located at the Pareto front in the study.\n  - No parameters required.\n\n### Trial\n\n- **ask** - Suggest new parameters using Optuna.\n  - `search_space` : the search space for Optuna (dictionary, required).\n- **tell** - Report the result of a trial.\n  - `trial_number` : the trial number (integer, required).\n  - `values` : the result of the trial (float or list of floats, required).\n- **set_trial_user_attr** - Set user attributes for a trial.\n  - `trial_number`: the trial number (integer, required).\n  - `key`: the key of the user attribute (string, required).\n  - `value`: the value of the user attribute (any type, required).\n- **get_trial_user_attrs** - Get user attributes in a trial.\n  - `trial_number`: the trial number (integer, required).\n\n### Visualization\n\n- **plot_optimization_history** - Return the optimization history plot as an image.\n  - `target`: index to specify which value to display (integer, optional).\n  - `target_name`: target‚Äôs name to display on the axis label (string, optional).\n- **plot_hypervolume_history** - Return the hypervolume history plot as an image.\n  - `reference_point` : a list of reference points to calculate the hypervolume (list of floats, required).\n- **plot_pareto_front** - Return the Pareto front plot as an image for multi-objective optimization.\n  - `target_names`: objective name list used as the axis titles (list of strings, optional).\n  - `include_dominated_trials`: a flag to include all dominated trial's objective values (boolean, optional).\n  - `targets`: a list of indices to specify the objective values to display. (list of integers, optional).\n- **plot_contour** - Return the contour plot as an image.\n  - `params` : parameter list to visualize (list of strings, optional).\n  - `target` : an index to specify the value to display (integer, required).\n  - `target_name` : target‚Äôs name to display on the color bar (string, required).\n- **plot_parallel_coordinate** - Return the parallel coordinate plot as an image.\n  - `params` : parameter list to visualize (list of strings, optional).\n  - `target` : an index to specify the value to display (integer, required).\n  - `target_name` : target‚Äôs name to display on the axis label and the legend (string, required).\n- **plot_slice** - Return the slice plot as an image.\n  - `params` : parameter list to visualize (list of strings, optional).\n  - `target` : an index to specify the value to display (integer, required).\n  - `target_name` : target‚Äôs name to display on the axis label (string, required).\n- **plot_param_importances** - Return the parameter importances plot as an image.\n  - `params` : parameter list to visualize (list of strings, optional).\n  - `target` : an index to specify the value to display (integer/null, optional).\n  - `target_name` : target‚Äôs name to display on the legend (string, required).\n- **plot_edf** - Return the EDF plot as an image.\n  - `target` : an index to specify the value to display (integer, required).\n  - `target_name` : target‚Äôs name to display on the axis label (string, required).\n- **plot_timeline** - Return the timeline plot as an image.\n  - No parameters required.\n- **plot_rank** - Return the rank plot as an image.\n  - `params` : parameter list to visualize (list of strings, optional).\n  - `target` : an index to specify the value to display (integer, required).\n  - `target_name` : target‚Äôs name to display on the color bar (string, required).\n\n### Web Dashboard\n\n- **launch_optuna_dashboard** - Launch the Optuna dashboard.\n  - `port`: server port (integer, optional, default: 58080).\n\n## Examples\n\n- [Optimizing the 2D-Sphere function](#optimizing-the-2d-sphere-function)\n- [Starting the Optuna dashboard and analyzing optimization results](#starting-the-optuna-dashboard-and-analyzing-optimization-results)\n- [Optimizing the FFmpeg encoding parameters](#optimizing-the-ffmpeg-encoding-parameters)\n- [Optimizing the Cookie Recipe](#optimizing-the-cookie-recipe)\n- [Optimizing the Matplotlib Configuration](#optimizing-the-matplotlib-configuration)\n\n### Optimizing the 2D-Sphere Function\n\nHere we present a simple example of optimizing the 2D-Sphere function, along with example prompts and the summary of the LLM responses.\n\n| User prompt | Output in Claude |\n| - | - |\n| (Launch Claude Desktop) | <img alt=\"1\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-1.png\" /> |\n| Please create an Optuna study named \"Optimize-2D-Sphere\" for minimization. | <img alt=\"2\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-2.png\" /> |\n| Please suggest two float parameters x, y in [-1, 1]. | <img alt=\"3\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-3.png\" /> |\n| Please report the objective value x\\*\\*2 + y\\*\\*2. To calculate the value, please use the JavaScript interpreter and do not round the values. | <img alt=\"4\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-4.png\" /> |\n| Please suggest another parameter set and evaluate it. | <img alt=\"5\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-5.png\" /> |\n| Please plot the optimization history so far. | <img alt=\"6\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/sphere2d/images/sphere2d-6.png\" /> |\n\n### Starting the Optuna Dashboard and Analyzing Optimization Results\n\nYou can also start the [Optuna dashboard](https://github.com/optuna/optuna-dashboard) via the MCP server to analyze the optimization results interactively.\n\n| User prompt | Output in Claude |\n| - | - |\n| Please launch the Optuna dashboard. | <img alt=\"7\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/optuna-dashboard/images/optuna-dashboard-1.png\" /> |\n\nBy default, the Optuna dashboard will be launched on port 58080.\nYou can access it by navigating to `http://localhost:58080` in your web browser as shown below:\n<img alt=\"8\" src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/optuna-dashboard/images/optuna-dashboard-2.png\" />\n\nOptuna dashboard provides various visualizations to analyze the optimization results, such as optimization history, parameter importances, and more.\n\n### Optimizing the FFmpeg Encoding Parameters\n\n![ffmpeg-2](https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/ffmpeg/images/demo-ffmpeg-2.png)\n\nThis demo showcases how to use the Optuna MCP server to automatically find optimal FFmpeg encoding parameters. It optimizes x264 encoding options to maximize video quality (measured by the SSIM score) while keeping encoding time reasonable.\n\nCheck out [examples/ffmpeg](https://github.com/optuna/optuna-mcp/tree/main/examples/ffmpeg/README.md) for details.\n\n### Optimizing the Cookie Recipe\n\n![cookie-recipe](https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/cookie-recipe/images/result-table.png)\n\nIn this example, we will optimize a cookie recipe, referencing the paper titled \"[Bayesian Optimization for a Better Dessert](https://research.google/pubs/bayesian-optimization-for-a-better-dessert/)\".\n\nCheck out [examples/cookie-recipe](https://github.com/optuna/optuna-mcp/tree/main/examples/cookie-recipe/README.md) for details.\n\n### Optimizing the Matplotlib Configuration\n\n<table>\n    <caption>Default and optimized figures by Optuna MCP.</caption>\n    <tr>\n        <td><img src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/auto-matplotlib/images/first-plot.png\" alt=\"\"></td>\n        <td><img src=\"https://raw.githubusercontent.com/optuna/optuna-mcp/main/examples/auto-matplotlib/images/best-plot.png\" alt=\"\"></td>\n    </tr>\n</table>\n\nThis example optimizes a Matplotlib configuration.\n\nCheck out [examples/auto-matplotlib](https://github.com/optuna/optuna-mcp/tree/main/examples/auto-matplotlib/README.md) for details.\n\n## License\n\nMIT License (see [LICENSE](./LICENSE)).\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "optuna",
        "tools",
        "mcp",
        "optuna mcp",
        "workflows optuna",
        "integrations tools"
      ],
      "category": "data-science-tools"
    },
    "phisanti--MCPR": {
      "owner": "phisanti",
      "name": "MCPR",
      "url": "https://github.com/phisanti/MCPR",
      "imageUrl": "",
      "description": "Model Context Protocol for R: enables AI agents to participate in interactive live R sessions.",
      "stars": 12,
      "forks": 4,
      "license": "Creative Commons Attribution Share Alike 4.0 International",
      "language": "R",
      "updated_at": "2025-10-02T19:26:12Z",
      "readme_content": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# MCPR: A Practical Framework for Stateful Human-AI Collaboration in R <a href=\"https://phisanti.github.io/MCPR/\" alt=\"MCPR\"></a>\n\n<!-- badges: start -->\n\n[![Lifecycle:\nexperimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)\n[![R-CMD-check](https://github.com/phisanti/MCPR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/phisanti/MCPR/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/phisanti/MCPR/branch/main/graph/badge.svg)](https://app.codecov.io/gh/phisanti/MCPR?branch=main)\n[![GitHub release (latest by\ndate)](https://img.shields.io/github/v/release/phisanti/MCPR)](https://github.com/phisanti/MCPR/releases)\n<!-- badges: end -->\n\nThe MCPR (Model Context Protocol Tools for R) package addresses a\nfundamental limitation in the current paradigm of AI-assisted R\nprogramming. Existing AI agents operate in a stateless execution model,\ninvoking `Rscript` for each command, which is antithetical to the\niterative, state-dependent nature of serious data analysis. An\nanalytical workflow is a cumulative process of exploration, modelling,\nand validation that can span hours or days. Moreover, intermediate steps\ncan involve heavy computation, and small changes in downstream code such\nas plot aesthetics require running the entire script again. MCPR aims to\ntackle this issue by enabling AI agents to establish persistent,\ninteractive sessions within a live R environment, thereby preserving\nworkspace state and enabling complex, multi-step analytical workflows.\n\n<figure>\n\n<figcaption aria-hidden=\"true\">MCPR Demo</figcaption>\n</figure>\n\n## Quick Start\n\nGet up and running with MCPR in under 2 minutes:\n\n``` r\n# 1. Install MCPR\nremotes::install_github(\"phisanti/MCPR\")\n\n# 2. Start an R session and make it discoverable\nlibrary(MCPR)\nmcpr_session_start()\n\n# 3. In your AI agent (Claude, etc.), connect to the session\n# The agent will use: manage_r_sessions(\"list\") then manage_r_sessions(\"join\", session_id)\n\n# 4. Now your AI agent can run R code in your live session!\n# Example: execute_r_code(\"summary(mtcars)\")\n```\n\n**That‚Äôs it!** Your AI agent can now execute R code, create plots, and\ninspect your workspace while preserving all session state.\n\n## Core capabilities\n\nMCPR‚Äôs design is guided by principles of modularity, robustness, and\npracticality.\n\n- **Communication Protocol:** MCPR uses JSON-RPC 2.0 over `nanonext`\n  sockets, providing a lightweight, asynchronous, and reliable messaging\n  layer. This choice ensures cross-platform compatibility and\n  non-blocking communication suitable for an interactive environment.\n- **Tool-Based Design:** Functionality is exposed to the AI agent as a\n  discrete set of tools (create_plot, execute_r_code, etc.). This\n  modular approach simplifies the agent‚Äôs interaction logic and provides\n  clear, well-defined endpoints for R operations.\n- **Session Management:** A central `mcpr_session_start()` function acts\n  as a listener, making an R session discoverable on the local machine.\n  The `manage_r_sessions` tool provides the service discovery mechanism\n  for agents to find and connect to these listeners.\n- **Graphics Subsystem:** Plot generation leverages `httpgd` when\n  available for high-performance, off-screen rendering. A fallback to\n  standard R graphics devices (grDevices) ensures broad compatibility.\n  The system includes intelligent token management to prevent oversized\n  image payloads.\n\n## Installation\n\nThe first requirement is to have R installed and then install the MCPR\npackage from GitHub:\n\n``` r\nif (!require(\"remotes\")) install.packages(\"remotes\")\n\nremotes::install_github(\"phisanti/MCPR\")\n```\n\nNext, you should install the MCP server to give the agent access to the\ntools included in the package. System integration is designed to be\nstraightforward, with both automated and manual pathways.\n\n### Automated Setup\n\nA convenience function, `install_mcpr()`, is provided to handle package\ninstallation and agent-specific MCP configuration.\n\n``` r\nlibrary(MCPR)\ninstall_mcpr(agent = \"claude\") # Supported agents: 'claude', 'gemini', 'copilot'\n```\n\n### Manual MCP Configuration\n\nFor Claude Desktop, configure `claude_desktop_config.json`. You can\nlikely find it in one of these locations depending on your OS:\n\n**macOS**:\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json` **Linux**:\n`~/.config/claude/claude_desktop_config.json`\n\nThen, add the following MCP server configuration:\n\n``` json\n{\n  \"mcpServers\": {\n    \"mcpr\": {\n      \"command\": \"R\",\n      \"args\": [\"--quiet\", \"--slave\", \"-e\", \"MCPR::mcpr_server()\"]\n    }\n  }\n}\n```\n\n## Usage Pattern\n\nThe intended workflow is simple and user-centric.\n\n1.  The user starts an R session and invokes `mcpr_session_start()` to\n    enable connections.\n2.  The user instructs their AI agent to connect.\n3.  The agent uses `manage_r_sessions('list')` to find the session ID\n    and `manage_r_sessions('join', session=ID)` to connect.\n4.  The user can now interact with the agent, making requests regarding\n    their R session. The agent can now use `execute_r_code`,\n    `create_plot`, and `view` to collaboratively assist the user with\n    their analysis, maintaining full context throughout the interaction.\n\n## Agent tools\n\nThe philosophy in the development of the MCPR package is to provide the\nagent with few, well-defined tools that can be composed to perform\ncomplex tasks. The goal was to give the agent the ability to manage\nmultiple R sessions (`manage_r_sessions`), to run R code in the session\n(`execute_r_code`), see the graphical data (`create_plot`), and inspect\nthe session (`view`). We believe these are flexible enough to accomplish\nany task in R. See the details below.\n\n### `execute_r_code(code)`\n\n**Purpose**: Execute arbitrary R code within session context  \n**Input**: Character string containing R expressions  \n**Output**: Structured response with results, output, warnings, and\nerrors\n\n``` r\nexecute_r_code(\"\n  library(dplyr)\n  data <- mtcars %>% \n    filter(mpg > 20) %>%\n    select(mpg, cyl, wt)\n  nrow(data)\n\")\n```\n\n### `create_plot(expr, width, height, format)`\n\n**Purpose**: Generate visualizations with AI-optimized output  \n**Input**: R plotting expression, dimensions, format specification  \n**Output**: Base64-encoded image with metadata and token usage\ninformation\n\n``` r\ncreate_plot(\"\n  library(ggplot2)\n  ggplot(mtcars, aes(wt, mpg)) + \n    geom_point() + \n    geom_smooth(method = 'lm')\n\", width = 600, height = 450)\n```\n\n### `manage_r_sessions(action, session)`\n\n**Purpose**: Session discovery and management  \n**Actions**:\n\n- `\"list\"`: Enumerate active sessions with metadata  \n- `\"join\"`: Connect to specific session by ID  \n- `\"start\"`: Launch new R session process\n\n``` r\nmanage_r_sessions(\"list\")        # Show available sessions\nmanage_r_sessions(\"join\", 2)     # Connect to session 2\nmanage_r_sessions(\"start\")       # Create new session\n```\n\n### `view(what, max_lines)`\n\n**Purpose**: Environment introspection and debugging  \n**what**:\n\n- `'session'`: Object summaries with statistical metadata  \n- `'terminal'`: Command history for workflow reproducibility  \n- `'workspace'`: File system context  \n- `'installed_packages'`: Available libraries\n\n## Common errors\n\n- **Connection Failed:** Ensure `mcpr_session_start()` is running in R.\n  Set the `MCPTOOLS_LOG_FILE` environment variable to a valid path and\n  inspect logs for detailed error messages.\n- **Tools Not Found:** Confirm the path in `user_mcp.json` is correct\n  and that the agent has been restarted. Manually install the MCP server\n  to verify the setup.\n- **Plotting Errors:** Ensure the plotting expression is valid and that\n  all necessary libraries are loaded, and install `httpgd`.\n\nIf these issues persist, please open an issue on the GitHub repository\nwith relevant logs and context.\n\n## Acknowledgments\n\nWe thank [Simon P. Couch](https://github.com/simonpcouch)\n([mcptools](https://github.com/posit-dev/mcptools)) for the inspiration\nto use nanonext and [Aleksander\nDietrichson](https://github.com/dietrichson)\n([mcpr](https://github.com/chi2labs/mcpr)) for the idea of using\nroxygen2 for parsing tools.\n\n------------------------------------------------------------------------\n\nThis project is licensed under the Creative Commons Attribution 4.0\nInternational License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "interactive",
        "workflows",
        "tools integrations",
        "data exploration",
        "science workflows"
      ],
      "category": "data-science-tools"
    },
    "reading-plus-ai--mcp-server-data-exploration": {
      "owner": "reading-plus-ai",
      "name": "mcp-server-data-exploration",
      "url": "https://github.com/reading-plus-ai/mcp-server-data-exploration",
      "imageUrl": "",
      "description": "Enables autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort.",
      "stars": 487,
      "forks": 60,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T10:21:39Z",
      "readme_content": "# MCP Server for Data Exploration\n\nMCP Server is a versatile tool designed for interactive data exploration.\n\nYour personal Data Scientist assistant, turning complex datasets into clear, actionable insights.\n\n<a href=\"https://glama.ai/mcp/servers/hwm8j9c422\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/hwm8j9c422/badge\" alt=\"mcp-server-data-exploration MCP server\" /></a>\n\n## üöÄ Try it Out\n\n1. **Download Claude Desktop**\n   - Get it [here](https://claude.ai/download)\n\n2. **Install and Set Up**\n   - On macOS, run the following command in your terminal:\n   ```bash\n   python setup.py\n   ```\n\n3. **Load Templates and Tools**\n   - Once the server is running, wait for the prompt template and tools to load in Claude Desktop.\n\n4. **Start Exploring**\n   - Select the explore-data prompt template from MCP\n   - Begin your conversation by providing the required inputs:\n     - `csv_path`: Local path to the CSV file\n     - `topic`: The topic of exploration (e.g., \"Weather patterns in New York\" or \"Housing prices in California\")\n\n## Examples\n\nThese are examples of how you can use MCP Server to explore data without any human intervention.\n\n### Case 1: California Real Estate Listing Prices\n- Kaggle Dataset: [USA Real Estate Dataset](https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset)\n- Size: 2,226,382 entries (178.9 MB)\n- Topic: Housing price trends in California\n\n[![Watch the video](https://img.youtube.com/vi/RQZbeuaH9Ys/hqdefault.jpg)](https://www.youtube.com/watch?v=RQZbeuaH9Ys)\n- [Data Exploration Summary](https://claude.site/artifacts/058a1593-7a14-40df-bf09-28b8c4531137)\n\n### Case 2: Weather in London\n- Kaggle Dataset: [2M+ Daily Weather History UK](https://www.kaggle.com/datasets/jakewright/2m-daily-weather-history-uk/data)\n- Size: 2,836,186 entries (169.3 MB)\n- Topic: Weather in London\n- Report: [View Report](https://claude.site/artifacts/601ea9c1-a00e-472e-9271-3efafb8edede)\n- Graphs:\n  - [London Temperature Trends](https://claude.site/artifacts/9a25bc1e-d0cf-498a-833c-5179547ee268)\n<img width=\"1622\" alt=\"Screenshot 2024-12-09 at 12 48 56‚ÄØAM\" src=\"https://github.com/user-attachments/assets/9e70fe97-8af7-4221-b1e7-00197c88bb47\">\n\n  - [Temperature-Humidity Relationship by Season](https://claude.site/artifacts/32a3371c-698d-48e3-b94e-f7e88ce8093d)\n<img width=\"1623\" alt=\"Screenshot 2024-12-09 at 12 47 54‚ÄØAM\" src=\"https://github.com/user-attachments/assets/f4ac60a8-30e3-4b10-b296-ba412c2922fa\">\n\n  - [Wind Direction Pattern by Season](https://claude.site/artifacts/32a3371c-698d-48e3-b94e-f7e88ce8093d)\n<img width=\"1622\" alt=\"Screenshot 2024-12-09 at 12 47 00‚ÄØAM\" src=\"https://github.com/user-attachments/assets/2db01054-f948-4d2e-ba39-8de8fa59f83d\">\n\n## üì¶ Components\n\n### Prompts\n- **explore-data**: Tailored for data exploration tasks\n\n### Tools\n1. **load-csv**\n   - Function: Loads a CSV file into a DataFrame\n   - Arguments:\n     - `csv_path` (string, required): Path to the CSV file\n     - `df_name` (string, optional): Name for the DataFrame. Defaults to df_1, df_2, etc., if not provided\n\n2. **run-script**\n   - Function: Executes a Python script\n   - Arguments:\n     - `script` (string, required): The script to execute\n\n## ‚öôÔ∏è Modifying the Server\n\n### Claude Desktop Configurations\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Development (Unpublished Servers)\n```json\n\"mcpServers\": {\n  \"mcp-server-ds\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/Users/username/src/mcp-server-ds\",\n      \"run\",\n      \"mcp-server-ds\"\n    ]\n  }\n}\n```\n\n### Published Servers\n```json\n\"mcpServers\": {\n  \"mcp-server-ds\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-ds\"\n    ]\n  }\n}\n```\n\n## üõ†Ô∏è Development\n\n### Building and Publishing\n1. **Sync Dependencies**\n   ```bash\n   uv sync\n   ```\n\n2. **Build Distributions**\n   ```bash\n   uv build\n   ```\n   Generates source and wheel distributions in the dist/ directory.\n\n3. **Publish to PyPI**\n   ```bash\n   uv publish\n   ```\n\n## ü§ù Contributing\n\nContributions are welcome! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n### Reporting Issues\nIf you encounter bugs or have suggestions, open an issue in the issues section. Include:\n- Steps to reproduce (if applicable)\n- Expected vs. actual behavior\n- Screenshots or error logs (if relevant)\n\n## üìú License\n\nThis project is licensed under the MIT License.\nSee the LICENSE file for details.\n\n## üí¨ Get in Touch\n\nQuestions? Feedback? Open an issue or reach out to the maintainers. Let's make this project awesome together!\n\n## About\n\nThis is an open source project run by [ReadingPlus.AI LLC](https://readingplus.ai). and open to contributions from the entire community.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datasets",
        "csv",
        "data",
        "data exploration",
        "exploration csv",
        "data science"
      ],
      "category": "data-science-tools"
    },
    "subelsky--bundler_mcp": {
      "owner": "subelsky",
      "name": "bundler_mcp",
      "url": "https://github.com/subelsky/bundler_mcp",
      "imageUrl": "",
      "description": "Enables agents to query local information about dependencies in a Ruby project's `Gemfile`.",
      "stars": 18,
      "forks": 2,
      "license": "MIT License",
      "language": "Ruby",
      "updated_at": "2025-09-01T07:16:13Z",
      "readme_content": "# BundlerMCP\n\nA Model Context Protocol (MCP) server enabling AI agents to query information about dependencies in a Ruby project's `Gemfile`. Built with [fast-mcp](https://github.com/yjacquin/fast-mcp).\n\n[![CI](https://github.com/subelsky/bundler_mcp/actions/workflows/main.yml/badge.svg)](https://github.com/subelsky/bundler_mcp/actions/workflows/main.yml)\n[![Gem Version](https://badge.fury.io/rb/bundler_mcp.svg)](https://badge.fury.io/rb/bundler_mcp)\n\n## Installation\n\nInstall the gem and add to the application's Gemfile by executing:\n\n```bash\nbundle add bundler_mcp --group=development\n```\n\n## Usage\n\n1. Generate the binstub:\n\n```bash\nbundle binstubs bundler_mcp\n```\n\n2. Configure your client to execute the binstub. Here are examples that work for Claude and Cursor:\n\n### Basic Example (mcp.json)\n\n```json\n{\n  \"mcpServers\": {\n    \"bundler-mcp\": {\n      \"command\": \"/Users/mike/my_project/bin/bundler_mcp\"\n    }\n  }\n}\n```\n\n### Example with logging and explicit Gemfile\n\n```json\n{\n  \"mcpServers\": {\n    \"bundler-mcp\": {\n      \"command\": \"/Users/mike/my_project/bin/bundler_mcp\",\n\n      \"env\": {\n        \"BUNDLER_MCP_LOG_FILE\": \"/Users/mike/my_project/log/mcp.log\",\n        \"BUNDLE_GEMFILE\": \"/Users/mike/my_project/subdir/Gemfile\"\n      }\n    }\n  }\n}\n```\n\n### Documentation\n\n[Available on RubyDoc](https://www.rubydoc.info/gems/bundler_mcp/)\n\n### Available Tools\n\nThe server provides two tools for AI agents:\n\n#### list_project_gems\n\nLists all bundled Ruby gems with their:\n\n- Versions\n- Descriptions\n- Installation paths\n- Top-level documentation locations (e.g. `README` and `CHANGELOG`)\n\n\n\n#### get_gem_details\n\nRetrieves detailed information about a specific gem, including:\n\n- Version\n- Description\n- Installation path\n- Top-level documentation locations\n- Source code file locations\n\n\n\n## Environment Variables\n\n- `BUNDLE_GEMFILE`: Used by Bundler to locate your Gemfile. If you use the binstub method described in the [Usage](#usage) section, this is usually not needed.\n- `BUNDLER_MCP_LOG_FILE`: Path to log file. Useful for troubleshooting (defaults to no logging)\n\n## Development\n\nAfter checking out the repo, run `bin/setup` to install dependencies and `bundle exec rspec` to run the tests. You can also run `bin/console` for an interactive prompt that will allow you to experiment.\n\n### Testing with the MCP Inspector\n\nYou can test the server directly using the [MCP inspector](https://modelcontextprotocol.io/docs/tools/inspector):\n\n```bash\n# Basic usage\nnpx @modelcontextprotocol/inspector ./bin/bundler_mcp\n\n# With logging enabled\nBUNDLER_MCP_LOG_FILE=/tmp/log/mcp.log npx @modelcontextprotocol/inspector ./bin/bundler_mcp\n\n# With custom Gemfile\nBUNDLE_GEMFILE=./other/Gemfile npx @modelcontextprotocol/inspector ./bin/bundler_mcp\n```\n\n### Release Process\n\nTo install this gem onto your local machine, run `bundle exec rake install`. To release a new version:\n\n1. Update the version number in `version.rb`\n2. Run `bundle exec rake release`\n\nThis will:\n\n- Create a git tag for the version\n- Push git commits and the created tag\n- Push the `.gem` file to [rubygems.org](https://rubygems.org)\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at https://github.com/subelsky/bundler_mcp.\n\n## License\n\nOpen source under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n\n## Author\n\n[Mike Subelsky](https://subelsky.com)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gemfile",
        "bundler_mcp",
        "ruby",
        "subelsky bundler_mcp",
        "ruby project",
        "tools integrations"
      ],
      "category": "data-science-tools"
    }
  }
}