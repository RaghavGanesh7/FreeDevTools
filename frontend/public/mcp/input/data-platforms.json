{
  "category": "data-platforms",
  "categoryDisplay": "Data Platforms",
  "description": "Data Platforms for data integration, transformation and pipeline orchestration.",
  "totalRepositories": 7,
  "repositories": {
    "JordiNei--mcp-databricks-server": {
      "owner": "JordiNei",
      "name": "mcp-databricks-server",
      "url": "https://github.com/JordiNeil/mcp-databricks-server",
      "imageUrl": "",
      "description": "Connect to Databricks API, allowing LLMs to run SQL queries, list jobs, and get job status.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databricks",
        "platforms",
        "llms",
        "data platforms",
        "databricks server",
        "databricks api"
      ],
      "category": "data-platforms"
    },
    "aywengo--kafka-schema-reg-mcp": {
      "owner": "aywengo",
      "name": "kafka-schema-reg-mcp",
      "url": "https://github.com/aywengo/kafka-schema-reg-mcp",
      "imageUrl": "",
      "description": "Comprehensive Kafka Schema Registry MCP server with 48 tools for multi-registry management, schema migration, and enterprise features.",
      "stars": 23,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T08:28:36Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.ai/badge.svg)](https://mseep.ai/app/aywengo-kafka-schema-reg-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)\n[![Docker Pulls](https://img.shields.io/docker/pulls/aywengo/kafka-schema-reg-mcp)](https://hub.docker.com/r/aywengo/kafka-schema-reg-mcp)\n[![GitHub Release](https://img.shields.io/github/v/release/aywengo/kafka-schema-reg-mcp)](https://github.com/aywengo/kafka-schema-reg-mcp/releases)\n[![GitHub Issues](https://img.shields.io/github/issues/aywengo/kafka-schema-reg-mcp)](https://github.com/aywengo/kafka-schema-reg-mcp/issues)\n[![Docker Image Size](https://img.shields.io/docker/image-size/aywengo/kafka-schema-reg-mcp/stable)](https://hub.docker.com/r/aywengo/kafka-schema-reg-mcp)\n[![Maintained](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/aywengo/kafka-schema-reg-mcp/graphs/commit-activity)\n[![MCP Specification](https://img.shields.io/badge/MCP-2025--06--18-brightgreen.svg)](https://modelcontextprotocol.io)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/aywengo/kafka-schema-reg-mcp)](https://archestra.ai/mcp-catalog/aywengo__kafka-schema-reg-mcp)\n\n# Kafka Schema Registry MCP Server\n\nA comprehensive **Model Context Protocol (MCP) server** that provides Claude Desktop and other MCP clients with tools for Kafka Schema Registry operations. Features advanced schema context support, multi-registry management, and comprehensive schema export capabilities.\n\n<table width=\"100%\">\n<tr>\n<td width=\"33%\" style=\"vertical-align: top;\">\n<div style=\"background-color: white; padding: 20px; border-radius: 10px;\">\n  \n</div>\n</td>\n<td width=\"67%\" style=\"vertical-align: top; padding-left: 20px;\">\n\n> **🎯 True MCP Implementation**: Uses modern **FastMCP 2.8.0+ framework** with full **MCP 2025-06-18 specification compliance**. Fully compatible with Claude Desktop and other MCP clients using JSON-RPC over `stdio`.\n\n**Latest Version:** [v2.1.3](CHANGELOG.md) | **Docker:** `aywengo/kafka-schema-reg-mcp:stable`\n</td>\n</tr>\n</table>\n\n## 📋 Table of Contents\n\n- [🚀 Quick Start](#-quick-start)\n- [✨ Key Features](#-key-features)\n- [📦 Installation](#-installation)\n- [⚙️ Configuration](#️-configuration)\n- [💬 Usage Examples](#-usage-examples)\n- [🔒 Authentication & Security](#-authentication--security)\n- [📚 Documentation](#-documentation)\n- [🧪 Testing](#-testing)\n- [🚀 Deployment](#-deployment)\n- [🤝 Contributing](#-contributing)\n- [🆕 What's New](#-whats-new)\n\n## 🚀 Quick Start\n\n### 1. Run with Docker (Recommended)\n```bash\n# Latest stable release\ndocker pull aywengo/kafka-schema-reg-mcp:stable\n\n# Recommended: Run with SLIM_MODE for optimal performance (reduced essential tool set)\ndocker run -e SCHEMA_REGISTRY_URL=http://localhost:8081 -e SLIM_MODE=true aywengo/kafka-schema-reg-mcp:stable\n\n# OR run with full feature set for administrators/SRE\ndocker run -e SCHEMA_REGISTRY_URL=http://localhost:8081 aywengo/kafka-schema-reg-mcp:stable\n```\n\n### 2. Configure Claude Desktop\nCopy a ready-to-use configuration from [`config-examples/`](config-examples/):\n\n```bash\n# macOS\ncp config-examples/claude_desktop_stable_config.json ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Linux  \ncp config-examples/claude_desktop_stable_config.json ~/.config/claude-desktop/config.json\n```\n\n### 3. Start Using with Claude\nRestart Claude Desktop and try these prompts:\n- *\"List all schema contexts\"*\n- *\"Show me the subjects in the production context\"* \n- *\"Register a new user schema with fields for id, name, and email\"*\n\n## ✨ Key Features\n\n- **🤖 Claude Desktop Integration** - Direct MCP integration with natural language interface\n- **🏢 Multi-Registry Support** - Manage up to 8 Schema Registry instances simultaneously\n- **📋 Schema Contexts** - Logical grouping for production/staging environment isolation\n- **🔄 Schema Migration** - Cross-registry migration with backup and verification\n- **📊 Comprehensive Export** - JSON, Avro IDL formats for backup and documentation\n- **🔒 Production Safety** - VIEWONLY mode and per-registry access control\n- **🔐 OAuth 2.1 Authentication** - Enterprise-grade security with scope-based permissions\n- **📈 Real-time Progress** - Async operations with progress tracking and cancellation\n- **🔗 Resource Linking** - HATEOAS navigation with enhanced tool responses\n- **🧪 Full MCP Compliance** - 57+ tools following MCP 2025-06-18 specification\n- **🚀 SLIM_MODE** - Reduce tool overhead from 57+ to ~9 essential tools for better LLM performance\n\n> **📖 See detailed feature descriptions**: [docs/api-reference.md](docs/api-reference.md)\n\n## 📦 Installation\n\n### Option A: Docker (Recommended)\n```bash\n# Production stable\ndocker pull aywengo/kafka-schema-reg-mcp:stable\n\n# Latest development  \ndocker pull aywengo/kafka-schema-reg-mcp:latest\n\n# Specific version\ndocker pull aywengo/kafka-schema-reg-mcp:2.1.3\n```\n\n#### Running with SLIM_MODE\nTo reduce LLM overhead, run with SLIM_MODE enabled:\n```bash\n# Run with a reduced essential tool set\ndocker run -e SCHEMA_REGISTRY_URL=http://localhost:8081 -e SLIM_MODE=true aywengo/kafka-schema-reg-mcp:stable\n```\n\n> **💡 SLIM_MODE Benefits:**\n> - Reduces tool count to an essential subset\n> - Significantly faster LLM response times\n> - Lower token usage and reduced costs\n> - Ideal for production read-only operations\n> - Maintains full remote deployment support\n\n### Option B: Local Python\n```bash\ngit clone https://github.com/aywengo/kafka-schema-reg-mcp\ncd kafka-schema-reg-mcp\npip install -r requirements.txt\npython kafka_schema_registry_unified_mcp.py\n```\n\n### Option C: Docker Compose\n```bash\ndocker-compose up -d  # Includes Schema Registry for testing\n```\n\n> **📖 Detailed installation guide**: [docs/deployment.md](docs/deployment.md)\n\n## ⚙️ Configuration\n\n### Single Registry Mode\n```bash\nexport SCHEMA_REGISTRY_URL=\"http://localhost:8081\"\nexport SCHEMA_REGISTRY_USER=\"\"           # Optional\nexport SCHEMA_REGISTRY_PASSWORD=\"\"       # Optional\nexport VIEWONLY=\"false\"                  # Production safety\nexport SLIM_MODE=\"false\"                 # Optional: Enable to reduce tool overhead (default: false)\n```\n\n### Multi-Registry Mode (Up to 8 Registries)\n```bash\n# Development Registry\nexport SCHEMA_REGISTRY_NAME_1=\"development\"\nexport SCHEMA_REGISTRY_URL_1=\"http://dev-registry:8081\"\nexport VIEWONLY_1=\"false\"\n\n# Production Registry (with safety)\nexport SCHEMA_REGISTRY_NAME_2=\"production\"  \nexport SCHEMA_REGISTRY_URL_2=\"http://prod-registry:8081\"\nexport VIEWONLY_2=\"true\"                     # Read-only protection\n```\n\n### Claude Desktop Configuration\nPre-configured examples available in [`config-examples/`](config-examples/):\n\n| Configuration | Use Case | File |\n|---------------|----------|------|\n| **Production** | Stable Docker deployment | [`claude_desktop_stable_config.json`](config-examples/claude_desktop_stable_config.json) |\n| **Multi-Environment** | DEV/STAGING/PROD registries | [`claude_desktop_multi_registry_docker.json`](config-examples/claude_desktop_multi_registry_docker.json) |\n| **Local Development** | Python local execution | [`claude_desktop_config.json`](config-examples/claude_desktop_config.json) |\n| **View-Only Safety** | Production with safety | [`claude_desktop_viewonly_config.json`](config-examples/claude_desktop_viewonly_config.json) |\n\n> **📖 Complete configuration guide**: [config-examples/README.md](config-examples/README.md)\n\n### SLIM_MODE Configuration (Performance Optimization)\n\n**SLIM_MODE** reduces the number of exposed MCP tools to an essential subset, significantly reducing LLM overhead and improving response times.\n\n> **💡 Recommendation:** SLIM_MODE is **recommended for most use cases** as it provides all essential schema management capabilities with optimal performance.\n\n#### When to Use SLIM_MODE (Recommended)\n- **Default choice** for most users and day-to-day operations\n- When experiencing slow LLM responses due to too many tools\n- For production environments focused on read-only operations\n- When you only need basic schema management capabilities\n- To reduce token usage and improve performance\n\n#### When to Use Non-SLIM Mode\n- **For administrators or SRE teams** performing long-running operations\n- When you need advanced operations like:\n  - Schema migrations across registries\n  - Bulk schema removals and cleanup operations\n  - Complex batch operations and workflows\n  - Interactive guided wizards for complex tasks\n  - Comprehensive export/import operations\n\n#### Enable SLIM_MODE\n```bash\nexport SLIM_MODE=\"true\"  # Reduces tools from 57+ to ~9\n# Enables reduced essential tool set\n```\n\n#### Tools Available in SLIM_MODE\n**Essential Read-Only Tools:**\n- `ping` - Server health check\n- `set_default_registry`, `get_default_registry` - Registry management\n- `count_contexts`, `count_schemas`, `count_schema_versions` - Statistics\n\n**Basic Write Operations:**\n- `register_schema` - Register new schemas\n- `check_compatibility` - Schema compatibility checking\n- `create_context` - Create new contexts\n\n**Essential Export Operations:**\n- `export_schema` - Export single schema\n- `export_subject` - Export all subject versions\n\n**Resources Available (All Modes):**\n- All 19 resources remain available in SLIM_MODE\n- `registry://`, `schema://`, `subject://` resource URIs\n- Full read access through resource-first approach\n\n**Tools Hidden in SLIM_MODE:**\n- All migration tools (`migrate_schema`, `migrate_context`)\n- All batch operations (`clear_context_batch`)\n- Advanced export/import tools (`export_context`, `export_global`)\n- All interactive/elicitation tools (`*_interactive` variants)\n- Heavy statistics tools with async operations\n- Task management and workflow tools\n- Configuration update tools\n- Delete operations\n\n> **Note:** You can switch between modes by restarting with `SLIM_MODE=false` to access the full tool set.\n\n## 📊 MCP Tools and Resources\n\nThis section provides a comprehensive analysis of all MCP tools and resources exposed by the Kafka Schema Registry MCP Server.\n\n### Backward Compatibility Wrapper Tools\nThese tools are maintained for backward compatibility with existing clients. They internally use efficient implementations but are exposed as tools to prevent \"Tool not listed\" errors. Consider migrating to the corresponding resources for better performance.\n\n| **Tool Name** | **SLIM_MODE** | **Scope** | **Recommended Resource** | **Description** |\n|---------------|---------------|-----------|--------------------------|-----------------|\n| `list_registries` | ✅ | read | `registry://names` | List all configured registries |\n| `get_registry_info` | ✅ | read | `registry://info/{name}` | Get registry information |\n| `test_registry_connection` | ✅ | read | `registry://status/{name}` | Test registry connection |\n| `test_all_registries` | ✅ | read | `registry://status` | Test all registry connections |\n| `list_subjects` | ✅ | read | `registry://{name}/subjects` | List all subjects |\n| `get_schema` | ✅ | read | `schema://{name}/{context}/{subject}` | Get schema content |\n| `get_schema_versions` | ✅ | read | `schema://{name}/{context}/{subject}/versions` | Get schema versions |\n| `get_global_config` | ✅ | read | `registry://{name}/config` | Get global configuration |\n| `get_mode` | ✅ | read | `registry://mode` | Get registry mode |\n| `list_contexts` | ✅ | read | `registry://{name}/contexts` | List all contexts |\n| `get_subject_config` | ✅ | read | `subject://{name}/{context}/{subject}/config` | Get subject configuration |\n| `get_subject_mode` | ✅ | read | `subject://{name}/{context}/{subject}/mode` | Get subject mode |\n\n### Core MCP Tools\n\n| **Category** | **Name** | **Type** | **SLIM_MODE** | **Scope** | **Description** |\n|--------------|----------|----------|---------------|-----------|-----------------|\n| **Core** | `ping` | Tool | ✅ | read | MCP ping/pong health check |\n| **Registry Management** | `set_default_registry` | Tool | ✅ | admin | Set default registry |\n| **Registry Management** | `get_default_registry` | Tool | ✅ | read | Get current default registry |\n| **Schema Operations** | `register_schema` | Tool | ✅ | write | Register new schema version |\n| **Schema Operations** | `check_compatibility` | Tool | ✅ | read | Check schema compatibility |\n| **Context Management** | `create_context` | Tool | ✅ | write | Create new context |\n| **Context Management** | `delete_context` | Tool | ❌ | admin | Delete context |\n| **Subject Management** | `delete_subject` | Tool | ❌ | admin | Delete subject and versions |\n| **Configuration** | `update_global_config` | Tool | ❌ | admin | Update global configuration |\n| **Configuration** | `update_subject_config` | Tool | ❌ | admin | Update subject configuration |\n| **Configuration** | `add_subject_alias` | Tool | ❌ | write | Create alias subject pointing to an existing subject |\n| **Configuration** | `delete_subject_alias` | Tool | ❌ | write | Remove an alias subject |\n| **Mode Management** | `update_mode` | Tool | ❌ | admin | Update registry mode |\n| **Mode Management** | `update_subject_mode` | Tool | ❌ | admin | Update subject mode |\n| **Statistics** | `count_contexts` | Tool | ✅ | read | Count contexts |\n| **Statistics** | `count_schemas` | Tool | ✅ | read | Count schemas |\n| **Statistics** | `count_schema_versions` | Tool | ✅ | read | Count schema versions |\n| **Statistics** | `get_registry_statistics` | Tool | ❌ | read | Get comprehensive registry stats |\n| **Export** | `export_schema` | Tool | ✅ | read | Export single schema |\n| **Export** | `export_subject` | Tool | ✅ | read | Export all subject versions |\n| **Export** | `export_context` | Tool | ❌ | read | Export all context subjects |\n| **Export** | `export_global` | Tool | ❌ | read | Export all contexts/schemas |\n| **Export** | `export_global_interactive` | Tool | ❌ | read | Interactive global export |\n| **Migration** | `migrate_schema` | Tool | ❌ | admin | Migrate schema between registries |\n| **Migration** | `migrate_context` | Tool | ❌ | admin | Migrate context between registries |\n| **Migration** | `migrate_context_interactive` | Tool | ❌ | admin | Interactive context migration |\n| **Migration** | `list_migrations` | Tool | ❌ | read | List migration tasks |\n| **Migration** | `get_migration_status` | Tool | ❌ | read | Get migration status |\n| **Comparison** | `compare_registries` | Tool | ❌ | read | Compare two registries |\n| **Comparison** | `compare_contexts_across_registries` | Tool | ❌ | read | Compare contexts across registries |\n| **Comparison** | `find_missing_schemas` | Tool | ❌ | read | Find missing schemas |\n| **Batch Operations** | `clear_context_batch` | Tool | ❌ | admin | Clear context with batch operations |\n| **Batch Operations** | `clear_multiple_contexts_batch` | Tool | ❌ | admin | Clear multiple contexts |\n| **Interactive** | `register_schema_interactive` | Tool | ❌ | write | Interactive schema registration |\n| **Interactive** | `check_compatibility_interactive` | Tool | ❌ | read | Interactive compatibility check |\n| **Interactive** | `create_context_interactive` | Tool | ❌ | write | Interactive context creation |\n| **Resource Discovery** | `list_available_resources` | Tool | ✅ | read | List all available resources |\n| **Resource Discovery** | `suggest_resource_for_tool` | Tool | ✅ | read | Get resource migration suggestions |\n| **Resource Discovery** | `generate_resource_templates` | Tool | ✅ | read | Generate resource URI templates |\n| **Task Management** | `get_task_status` | Tool | ❌ | read | Get task status |\n| **Task Management** | `get_task_progress` | Tool | ❌ | read | Get task progress |\n| **Task Management** | `list_active_tasks` | Tool | ❌ | read | List active tasks |\n| **Task Management** | `cancel_task` | Tool | ❌ | admin | Cancel running task |\n| **Task Management** | `list_statistics_tasks` | Tool | ❌ | read | List statistics tasks |\n| **Task Management** | `get_statistics_task_progress` | Tool | ❌ | read | Get statistics task progress |\n| **Elicitation** | `submit_elicitation_response` | Tool | ❌ | write | Submit elicitation response |\n| **Elicitation** | `list_elicitation_requests` | Tool | ❌ | read | List elicitation requests |\n| **Elicitation** | `get_elicitation_request` | Tool | ❌ | read | Get elicitation request details |\n| **Elicitation** | `cancel_elicitation_request` | Tool | ❌ | admin | Cancel elicitation request |\n| **Elicitation** | `get_elicitation_status` | Tool | ❌ | read | Get elicitation system status |\n| **Workflows** | `list_available_workflows` | Tool | ❌ | read | List available workflows |\n| **Workflows** | `get_workflow_status` | Tool | ❌ | read | Get workflow status |\n| **Workflows** | `guided_schema_migration` | Tool | ❌ | admin | Start schema migration wizard |\n| **Workflows** | `guided_context_reorganization` | Tool | ❌ | admin | Start context reorganization wizard |\n| **Workflows** | `guided_disaster_recovery` | Tool | ❌ | admin | Start disaster recovery wizard |\n| **Utility** | `get_mcp_compliance_status_tool` | Tool | ❌ | read | Get MCP compliance status |\n| **Utility** | `get_oauth_scopes_info_tool` | Tool | ❌ | read | Get OAuth scopes information |\n| **Utility** | `test_oauth_discovery_endpoints` | Tool | ❌ | read | Test OAuth discovery endpoints |\n| **Utility** | `get_operation_info_tool` | Tool | ❌ | read | Get operation metadata |\n| **Utility** | `check_viewonly_mode` | Tool | ❌ | read | Check if registry is in viewonly mode |\n| **RESOURCES** | `registry://status` | Resource | ✅ | read | Overall registry connection status |\n| **RESOURCES** | `registry://info` | Resource | ✅ | read | Detailed server configuration |\n| **RESOURCES** | `registry://mode` | Resource | ✅ | read | Registry mode detection |\n| **RESOURCES** | `registry://names` | Resource | ✅ | read | List of configured registry names |\n| **RESOURCES** | `registry://status/{name}` | Resource | ✅ | read | Specific registry connection status |\n| **RESOURCES** | `registry://info/{name}` | Resource | ✅ | read | Specific registry configuration |\n| **RESOURCES** | `registry://mode/{name}` | Resource | ✅ | read | Specific registry mode |\n| **RESOURCES** | `registry://{name}/subjects` | Resource | ✅ | read | List subjects for registry |\n| **RESOURCES** | `registry://{name}/contexts` | Resource | ✅ | read | List contexts for registry |\n| **RESOURCES** | `registry://{name}/config` | Resource | ✅ | read | Global config for registry |\n| **RESOURCES** | `schema://{name}/{context}/{subject}` | Resource | ✅ | read | Schema content with context |\n| **RESOURCES** | `schema://{name}/{subject}` | Resource | ✅ | read | Schema content default context |\n| **RESOURCES** | `schema://{name}/{context}/{subject}/versions` | Resource | ✅ | read | Schema versions with context |\n| **RESOURCES** | `schema://{name}/{subject}/versions` | Resource | ✅ | read | Schema versions default context |\n| **RESOURCES** | `subject://{name}/{context}/{subject}/config` | Resource | ✅ | read | Subject config with context |\n| **RESOURCES** | `subject://{name}/{subject}/config` | Resource | ✅ | read | Subject config default context |\n| **RESOURCES** | `subject://{name}/{context}/{subject}/mode` | Resource | ✅ | read | Subject mode with context |\n| **RESOURCES** | `subject://{name}/{subject}/mode` | Resource | ✅ | read | Subject mode default context |\n| **RESOURCES** | `elicitation://response/{request_id}` | Resource | ❌ | write | Elicitation response handling |\n\n## 💬 Usage Examples\n\n### Schema Management\n```bash\n# In Claude Desktop, use natural language:\n\"Register a user schema with id, name, email fields\"\n\"Check if my updated schema is compatible\"\n\"Export all schemas from staging context\"\n\"List subjects in production context\"\n```\n\n### Multi-Registry Operations  \n```bash\n\"Compare development and production registries\"\n\"Migrate user-events schema from staging to production\"\n\"Test connections to all registries\"\n\"Show me registry statistics\"\n```\n\n### Batch Operations\n```bash\n\"Clear all schemas from test context\"\n\"Export global schemas for backup\"\n\"Count schemas across all contexts\"\n```\n\n> **📖 More examples**: [examples/](examples/) | **📖 Use cases**: [docs/use-cases.md](docs/use-cases.md)\n\n## 🔒 Authentication & Security\n\n### OAuth 2.1 Support (Optional)\n```bash\n# Enable authentication\nexport ENABLE_AUTH=true\nexport AUTH_ISSUER_URL=\"https://your-oauth-provider.com\"\nexport AUTH_AUDIENCE=\"your-client-id\"\n```\n\n**Supported Providers:** Azure AD, Google OAuth, Keycloak, Okta, GitHub\n\n**Permission Scopes:**\n- `read` - View schemas, configurations\n- `write` - Register schemas, update configs (includes read)\n- `admin` - Delete subjects, full control (includes write + read)\n\n### Production Safety Features\n- **VIEWONLY Mode** - Prevent accidental changes in production\n- **URL Validation** - SSRF protection with configurable localhost access\n- **Scope-based Authorization** - Fine-grained tool-level permissions\n- **Per-Registry Controls** - Independent safety settings\n\n> **📖 Security guide**: [docs/deployment.md#security](docs/deployment.md#security)\n\n## 📚 Documentation\n\n| Guide | Description |\n|-------|-------------|\n| **[API Reference](docs/api-reference.md)** | Complete tool documentation with examples |\n| **[Subject Aliasing](docs/subject-alias.md)** | How to add and remove subject aliases |\n| **[Use Cases](docs/use-cases.md)** | Real-world scenarios and implementation patterns |\n| **[Deployment Guide](docs/deployment.md)** | Docker, Kubernetes, cloud platforms, CI/CD |\n| **[IDE Integration](docs/ide-integration.md)** | VS Code, Claude Code, Cursor setup |\n| **[Configuration Examples](config-examples/)** | Ready-to-use Claude Desktop configs |\n| **[Testing Guide](TESTING_SETUP_GUIDE.md)** | Comprehensive testing setup |\n| **[Changelog](CHANGELOG.md)** | Version history and migration notes |\n| **[v2.0.0 Highlights](README-v2.0.0-HIGHLIGHTS.md)** | Major version features |\n\n### Additional Resources\n- **[Examples](examples/)** - Usage examples and code samples\n- **[Scripts](scripts/)** - Utility scripts and automation\n- **[Helm Charts](helm/)** - Kubernetes deployment\n- **[Tests](tests/)** - Test suites and validation\n\n## 🧪 Testing\n\n### Quick Test\n```bash\ncd tests/\n./run_all_tests.sh --quick    # Essential tests\n./run_all_tests.sh           # Complete test suite\n```\n\n### Docker Testing\n```bash\npython tests/test_docker_mcp.py\n```\n\n### MCP Inspector Tests (UI-driven)\n```bash\n# From repository root\ncd inspector-tests\n\n# Single registry (DEV)\n./run-inspector-tests.sh stable\n\n# Multi-registry (DEV + PROD)\n./run-inspector-tests.sh multi\n\n# Test a specific Docker tag\nDOCKER_VERSION=latest ./run-inspector-tests.sh stable\n```\n\n> **📖 Testing guide**: [TESTING_SETUP_GUIDE.md](TESTING_SETUP_GUIDE.md)\n\n## 🚀 Deployment\n\n### Production Docker\n```bash\n# With docker-compose\ndocker-compose up -d\n\n# Direct Docker  \ndocker run -d -p 38000:8000 \\\n  -e SCHEMA_REGISTRY_URL=http://registry:8081 \\\n  aywengo/kafka-schema-reg-mcp:stable\n```\n\n### Kubernetes\n```bash\n# Using Helm charts\nhelm install kafka-schema-mcp ./helm/kafka-schema-reg-mcp\n```\n\n> **📖 Deployment guide**: [docs/deployment.md](docs/deployment.md)\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see:\n- **[Contributing Guidelines](.github/CONTRIBUTING.md)** \n- **[Code of Conduct](.github/CODE_OF_CONDUCT.md)**\n- **[Development Setup](docs/deployment.md#local-development)**\n\n### Quick Development Setup\n```bash\ngit clone https://github.com/aywengo/kafka-schema-reg-mcp\ncd kafka-schema-reg-mcp\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\npython kafka_schema_registry_unified_mcp.py\n```\n\n## 🆕 What's New\n\n### v2.1.x (Latest)\n- **🧭 Subject Aliasing** - New tools `add_subject_alias` and `delete_subject_alias`\n- **🛠️ Fixes** - Evolution assistant and import interactive fixes\n- **📦 Enhancements** - Continued MCP tool refinements and testing improvements\n\n### v2.0.x\n- **🔒 Security Fixes** - Resolved credential exposure in logging\n- **🤖 Interactive Schema Migration** - Smart migration with user preference elicitation\n- **💾 Automatic Backups** - Pre-migration backup creation\n- **✅ Post-Migration Verification** - Comprehensive schema validation  \n- **🚀 FastMCP 2.8.0+ Framework** - Complete architecture upgrade\n- **📊 MCP 2025-06-18 Compliance** - Latest protocol specification\n- **🔐 OAuth 2.1 Generic Discovery** - Universal provider compatibility\n- **🔗 Resource Linking** - HATEOAS navigation in tool responses\n\n> **📖 Full changelog**: [CHANGELOG.md](CHANGELOG.md) | **📖 v2.0.0 features**: [README-v2.0.0-HIGHLIGHTS.md](README-v2.0.0-HIGHLIGHTS.md)\n\n---\n**🐳 Glama.ai:** \n\n<a href=\"https://glama.ai/mcp/servers/@aywengo/kafka-schema-reg-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@aywengo/kafka-schema-reg-mcp/badge\" />\n</a>\n\n---\n\n**🐳 Docker Hub:** [`aywengo/kafka-schema-reg-mcp`](https://hub.docker.com/r/aywengo/kafka-schema-reg-mcp) | **📊 Stats:** 70+ MCP Tools (12 backward compatibility), 19 Resources, 8 Registries, OAuth 2.1, Multi-platform\n\n**License:** MIT | **Maintainer:** [@aywengo](https://github.com/aywengo) | **Issues:** [GitHub Issues](https://github.com/aywengo/kafka-schema-reg-mcp/issues)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kafka",
        "platforms",
        "pipeline",
        "data platforms",
        "kafka schema",
        "comprehensive kafka"
      ],
      "category": "data-platforms"
    },
    "dbt-labs--dbt-mcp": {
      "owner": "dbt-labs",
      "name": "dbt-mcp",
      "url": "https://github.com/dbt-labs/dbt-mcp",
      "imageUrl": "",
      "description": "Official MCP server for [dbt (data build tool)](https://www.getdbt.com/product/what-is-dbt) providing integration with dbt Core/Cloud CLI, project metadata discovery, model information, and semantic layer querying capabilities.",
      "stars": 389,
      "forks": 66,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T16:56:07Z",
      "readme_content": "# dbt MCP Server\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/11137/badge)](https://www.bestpractices.dev/projects/11137)\n\nThis MCP (Model Context Protocol) server provides various tools to interact with dbt. You can use this MCP server to provide AI agents with context of your project in dbt Core, dbt Fusion, and dbt Platform.\n\nRead our documentation [here](https://docs.getdbt.com/docs/dbt-ai/about-mcp) to learn more. [This](https://docs.getdbt.com/blog/introducing-dbt-mcp-server) blog post provides more details for what is possible with the dbt MCP server.\n\n## Feedback\n\nIf you have comments or questions, create a GitHub Issue or join us in [the community Slack](https://www.getdbt.com/community/join-the-community) in the `#tools-dbt-mcp` channel.\n\n\n## Architecture\n\nThe dbt MCP server architecture allows for your agent to connect to a variety of tools.\n\n![architecture diagram of the dbt MCP server](https://raw.githubusercontent.com/dbt-labs/dbt-mcp/refs/heads/main/docs/d2.png)\n\n## Examples\n\nCommonly, you will connect the dbt MCP server to an agent product like Claude or Cursor. However, if you are interested in creating your own agent, check out [the examples directory](https://github.com/dbt-labs/dbt-mcp/tree/main/examples) for how to get started.\n\n## Contributing\n\nRead `CONTRIBUTING.md` for instructions on how to get involved!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dbt",
        "platforms",
        "getdbt",
        "data platforms",
        "dbt core",
        "integration dbt"
      ],
      "category": "data-platforms"
    },
    "flowcore--mcp-flowcore-platform": {
      "owner": "flowcore",
      "name": "mcp-flowcore-platform",
      "url": "https://github.com/flowcore-io/mcp-flowcore-platform",
      "imageUrl": "",
      "description": "Interact with Flowcore to perform actions, ingest data, and analyse, cross reference and utilise any data in your data cores, or in public data cores; all with human language.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flowcore",
        "platforms",
        "platform",
        "flowcore platform",
        "data platforms",
        "mcp flowcore"
      ],
      "category": "data-platforms"
    },
    "keboola--keboola-mcp-server": {
      "owner": "keboola",
      "name": "keboola-mcp-server",
      "url": "https://github.com/keboola/keboola-mcp-server",
      "imageUrl": "",
      "description": "interact with Keboola Connection Data Platform. This server provides tools for listing and accessing data from Keboola Storage API.",
      "stars": 79,
      "forks": 18,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T12:49:29Z",
      "readme_content": "[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/keboola/mcp-server)\n\n\n# Keboola MCP Server\n\n> Connect your AI agents, MCP clients (**Cursor**, **Claude**, **Windsurf**, **VS Code** ...) and other AI assistants to Keboola. Expose data, transformations, SQL queries, and job triggers—no glue code required. Deliver the right data to agents when and where they need it.\n\n## Overview\n\nKeboola MCP Server is an open-source bridge between your Keboola project and modern AI tools. It turns Keboola features—like storage access, SQL transformations, and job triggers—into callable tools for Claude, Cursor, CrewAI, LangChain, Amazon Q, and more.\n\n- [Quick Start](#-quick-start-remote-mcp-server-easiest-way)\n- [Local Setup](#local-mcp-server-setup-custom-or-dev-way)\n\n## Features\n\nWith the AI Agent and MCP Server, you can:\n\n- **Storage**: Query tables directly and manage table or bucket descriptions\n- **Components**: Create, List and inspect extractors, writers, data apps, and transformation configurations\n- **SQL**: Create SQL transformations with natural language\n- **Jobs**: Run components and transformations, and retrieve job execution details\n- **Flows**: Build and manage workflow pipelines using Conditional Flows and Orchestrator Flows.\n- **Data Apps**: Create, deploy and manage Keboola Streamlit Data Apps displaying your queries over storage data.\n- **Metadata**: Search, read, and update project documentation and object metadata using natural language\n- **Dev Branches**: Work safely in development branches outside of production, where all operations are scoped to the selected branch.\n\n---\n\n## 🚀 Quick Start: Remote MCP Server (Easiest Way)\n\nThe easiest way to use Keboola MCP Server is through our **Remote MCP Server**. This hosted solution eliminates the need for local setup, configuration, or installation.\n\n### What is the Remote MCP Server?\n\nOur remote server is hosted on every multi-tenant Keboola stack and supports OAuth authentication. You can connect to it from any AI assistant that supports remote SSE connection and OAuth authentication.\n\n### How to Connect\n\n1. **Get your remote server URL**: Navigate to your Keboola Project Settings → `MCP Server` tab\n2. **Copy the server URL**: It will look like `https://mcp.<YOUR_REGION>.keboola.com/sse`\n3. **Configure your AI assistant**: Paste the URL into your AI assistant's MCP settings\n4. **Authenticate**: You'll be prompted to authenticate with your Keboola account and select your project\n\n### Supported Clients\n\n- **[Cursor](https://cursor.com)**: Use the \"Install In Cursor\" button in your project's MCP Server settings or click\n  this button\n  [![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=keboola&config=eyJ1cmwiOiJodHRwczovL21jcC51cy1lYXN0NC5nY3Aua2Vib29sYS5jb20vc3NlIn0%3D)\n- **[Claude Desktop](https://claude.ai)**: Add the integration via Settings → Integrations\n- **[Windsurf](https://windsurf.ai)**: Configure with the remote server URL\n- **[Make](https://make.com)**: Configure with the remote server URL\n- **Other MCP clients**: Configure with the remote server URL\n\nFor detailed setup instructions and region-specific URLs, see our [Remote Server Setup documentation](https://help.keboola.com/ai/mcp-server/#remote-server-setup).\n\n### Using Development Branches\nYou can work safely in [Keboola development branches](https://help.keboola.com/components/branches/) without affecting your production data. The remotely hosted MCP Servers respect the `KBC_BRANCH_ID` parameter and will scope all operations to the specified branch. You can find the development branch ID in the URL when navigating to the development branch in the UI, for example: `https://connection.us-east4.gcp.keboola.com/admin/projects/PROJECT_ID/branch/BRANCH_ID/dashboard`. The branch ID must be included in each request using the header `X-Branch-Id: <branchId>`, otherwise the MCP Server uses production branch as default. This should be managed by the AI client or the environment handling the server connection.\n\n---\n\n## Local MCP Server Setup (Custom or Dev Way)\n\nRun the MCP server on your own machine for full control and easy development. Choose this when you want to customize tools, debug locally, or iterate quickly. You’ll clone the repo, set Keboola credentials via environment variables or headers depending on the server transport, install dependencies, and start the server. This approach offers maximum flexibility (custom tools, local logging, offline iteration) but requires manual setup and you manage updates and secrets yourself.\n\nThe server supports multiple **transport** options, which can be selected by providing the `--transport <transport>` argument when starting the server:\n- `stdio` - Default when `--transport` is not specified. Standard input/output, typically used for local deployment with a single client.\n- `streamable-http` - Runs the server remotely over HTTP with a bidirectional streaming channel, allowing the client and server to continuously exchange messages. Connect via <url>/mcp (e.g., http://localhost:8000/mcp).\n- `sse` - Deprecated, use `streamable-http` instead. Runs the server remotely using Server-Sent Events (SSE) for one-way event streaming from server to client. Connect via <url>/sse (e.g., http://localhost:8000/sse).\n- `http-compat` - A custom transport supporting both `SSE` and `streamable-http`. It is currently used on Keboola remote servers but will soon be replaced by `streamable-http` only.\n\nFor client–server communication, Keboola credentials must be provided to enable working with your project in your Keboola Region. The following are required: `KBC_STORAGE_TOKEN`, `KBC_STORAGE_API_URL`, `KBC_WORKSPACE_SCHEMA` and optionally `KBC_BRANCH_ID`. You can provide these in two ways:\n- For personal use (mainly with stdio transport): set the environment variables before starting the server. All requests will reuse these predefined credentials.\n- For multi-user use: include the variables in the request headers so that each request uses the credentials provided with it.\n\n\n### KBC_STORAGE_TOKEN\n\nThis is your authentication token for Keboola:\n\nFor instructions on how to create and manage Storage API tokens, refer to the [official Keboola documentation](https://help.keboola.com/management/project/tokens/).\n\n**Note**: If you want the MCP server to have limited access, use custom storage token, if you want the MCP to access everything in your project, use the master token.\n\n### KBC_WORKSPACE_SCHEMA\n\nThis identifies your workspace in Keboola and is used for SQL queries. However, this is **only required if you're using a custom storage token** instead of the Master Token:\n\n- If using [Master Token](https://help.keboola.com/management/project/tokens/#master-tokens): The workspace is created automatically behind the scenes\n- If using [custom storage token](https://help.keboola.com/management/project/tokens/#limited-tokens): Follow this [Keboola guide](https://help.keboola.com/tutorial/manipulate/workspace/) to get your KBC_WORKSPACE_SCHEMA\n\n**Note**: When creating a workspace manually, check Grant read-only access to all Project data option\n\n**Note**: KBC_WORKSPACE_SCHEMA is called Dataset Name in BigQuery workspaces, you simply click connect and copy the Dataset Name\n\n### KBC_STORAGE_API_URL (Keboola Region)\n\nYour Keboola Region API URL depends on your deployment region. You can determine your region by looking at the URL in your browser when logged into your Keboola project:\n\n| Region | API URL |\n|--------|---------|\n| AWS North America | `https://connection.keboola.com` |\n| AWS Europe | `https://connection.eu-central-1.keboola.com` |\n| Google Cloud EU | `https://connection.europe-west3.gcp.keboola.com` |\n| Google Cloud US | `https://connection.us-east4.gcp.keboola.com` |\n| Azure EU | `https://connection.north-europe.azure.keboola.com` |\n\n### KBC_BRANCH_ID (Optional)\n\nTo operate on a specific [Keboola development branch](https://help.keboola.com/components/branches/), set the branch ID using the `KBC_BRANCH_ID` parameter. The MCP server scopes its functionality to the specified branch, ensuring all changes remain isolated and do not impact the production branch.\n\n- If not provided, the server uses the production branch by default.\n- For development work, set `KBC_BRANCH_ID` to the numeric ID of your branch (e.g., `123456`). You can find the development branch ID in the URL when navigating to the development branch in the UI, for example: `https://connection.us-east4.gcp.keboola.com/admin/projects/PROJECT_ID/branch/BRANCH_ID/dashboard`.\n- On remote transports, you can override per-request with the HTTP header `X-Branch-Id: <branchId>` or `KBC_BRANCH_ID: <branchId>`.\n\n\n### Installation\n\nMake sure you have:\n\n- [ ] Python 3.10+ installed\n- [ ] Access to a Keboola project with admin rights\n- [ ] Your preferred MCP client (Claude, Cursor, etc.)\n\n**Note**: Make sure you have `uv` installed. The MCP client will use it to automatically download and run the Keboola MCP Server.\n**Installing uv**:\n\n*macOS/Linux*:\n\n```bash\n#if homebrew is not installed on your machine use:\n# /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install using Homebrew\nbrew install uv\n```\n\n*Windows*:\n\n```powershell\n# Using the installer script\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Or using pip\npip install uv\n\n# Or using winget\nwinget install --id=astral-sh.uv -e\n```\n\nFor more installation options, see the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n\n\n### Running Keboola MCP Server\n\nThere are four ways to use the Keboola MCP Server, depending on your needs:\n\n### Option A: Integrated Mode (Recommended)\n\nIn this mode, Claude or Cursor automatically starts the MCP server for you. **You do not need to run any commands in your terminal**.\n\n1. Configure your MCP client (Claude/Cursor) with the appropriate settings\n2. The client will automatically launch the MCP server when needed\n\n#### Claude Desktop Configuration\n\n1. Go to Claude (top left corner of your screen) -> Settings → Developer → Edit Config (if you don't see the claude_desktop_config.json, create it)\n2. Add the following configuration:\n3. Restart Claude desktop for changes to take effect\n\n```json\n{\n  \"mcpServers\": {\n    \"keboola\": {\n      \"command\": \"uvx\",\n      \"args\": [\"keboola_mcp_server --transport <transport>\"],\n      \"env\": {\n        \"KBC_STORAGE_API_URL\": \"https://connection.YOUR_REGION.keboola.com\",\n        \"KBC_STORAGE_TOKEN\": \"your_keboola_storage_token\",\n        \"KBC_WORKSPACE_SCHEMA\": \"your_workspace_schema\",\n        \"KBC_BRANCH_ID\": \"your_branch_id_optional\"\n      }\n    }\n  }\n}\n```\n\nConfig file locations:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### Cursor Configuration\n\n1. Go to Settings → MCP\n2. Click \"+ Add new global MCP Server\"\n3. Configure with these settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"keboola\": {\n      \"command\": \"uvx\",\n      \"args\": [\"keboola_mcp_server --transport <transport>\"],\n      \"env\": {\n        \"KBC_STORAGE_API_URL\": \"https://connection.YOUR_REGION.keboola.com\",\n        \"KBC_STORAGE_TOKEN\": \"your_keboola_storage_token\",\n        \"KBC_WORKSPACE_SCHEMA\": \"your_workspace_schema\",\n        \"KBC_BRANCH_ID\": \"your_branch_id_optional\"\n      }\n    }\n  }\n}\n```\n\n**Note**: Use short, descriptive names for MCP servers. Since the full tool name includes the server name and must stay under ~60 characters, longer names may be filtered out in Cursor and will not be displayed to the Agent.\n\n\n#### Cursor Configuration for Windows WSL\n\nWhen running the MCP server from Windows Subsystem for Linux with Cursor AI, use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"keboola\":{\n      \"command\": \"wsl.exe\",\n      \"args\": [\n          \"bash\",\n          \"-c '\",\n          \"export KBC_STORAGE_API_URL=https://connection.YOUR_REGION.keboola.com &&\",\n          \"export KBC_STORAGE_TOKEN=your_keboola_storage_token &&\",\n          \"export KBC_WORKSPACE_SCHEMA=your_workspace_schema &&\",\n          \"export KBC_BRANCH_ID=your_branch_id_optional &&\",\n          \"/snap/bin/uvx keboola_mcp_server --transport <transport>\",\n          \"'\"\n      ]\n    }\n  }\n}\n```\n\n### Option B: Local Development Mode\n\nFor developers working on the MCP server code itself:\n\n1. Clone the repository and set up a local environment\n2. Configure Claude/Cursor to use your local Python path:\n\n```json\n{\n  \"mcpServers\": {\n    \"keboola\": {\n      \"command\": \"/absolute/path/to/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"keboola_mcp_server --transport <transport>\"\n      ],\n      \"env\": {\n        \"KBC_STORAGE_API_URL\": \"https://connection.YOUR_REGION.keboola.com\",\n        \"KBC_STORAGE_TOKEN\": \"your_keboola_storage_token\",\n        \"KBC_WORKSPACE_SCHEMA\": \"your_workspace_schema\",\n        \"KBC_BRANCH_ID\": \"your_branch_id_optional\"\n      }\n    }\n  }\n}\n```\n\n### Option C: Manual CLI Mode (For Testing Only)\n\nYou can run the server manually in a terminal for testing or debugging:\n\n```bash\n# Set environment variables\nexport KBC_STORAGE_API_URL=https://connection.YOUR_REGION.keboola.com\nexport KBC_STORAGE_TOKEN=your_keboola_storage_token\nexport KBC_WORKSPACE_SCHEMA=your_workspace_schema\nexport KBC_BRANCH_ID=your_branch_id_optional\n\nuvx keboola_mcp_server --transport sse\n```\n\n> **Note**: This mode is primarily for debugging or testing. For normal use with Claude or Cursor,\n> you do not need to manually run the server.\n\n> **Note**: The server will use the SSE transport and listen on `localhost:8000` for the incoming SSE connections.\n> You can use `--port` and `--host` parameters to make it listen elsewhere.\n\n### Option D: Using Docker\n\n```shell\ndocker pull keboola/mcp-server:latest\n\ndocker run \\\n  --name keboola_mcp_server \\\n  --rm \\\n  -it \\\n  -p 127.0.0.1:8000:8000 \\\n  -e KBC_STORAGE_API_URL=\"https://connection.YOUR_REGION.keboola.com\" \\\n  -e KBC_STORAGE_TOKEN=\"YOUR_KEBOOLA_STORAGE_TOKEN\" \\\n  -e KBC_WORKSPACE_SCHEMA=\"YOUR_WORKSPACE_SCHEMA\" \\\n  -e KBC_BRANCH_ID=\"YOUR_BRANCH_ID_OPTIONAL\" \\\n  keboola/mcp-server:latest \\\n  --transport sse \\\n  --host 0.0.0.0\n```\n\n> **Note**: The server will use the SSE transport and listen on `localhost:8000` for the incoming SSE connections.\n> You can change `-p` to map the container's port somewhere else.\n\n### Do I Need to Start the Server Myself?\n\n| Scenario | Need to Run Manually? | Use This Setup |\n|----------|----------------------|----------------|\n| Using Claude/Cursor | No | Configure MCP in app settings |\n| Developing MCP locally | No (Claude starts it) | Point config to python path |\n| Testing CLI manually | Yes | Use terminal to run |\n| Using Docker | Yes | Run docker container |\n\n## Using MCP Server\n\nOnce your MCP client (Claude/Cursor) is configured and running, you can start querying your Keboola data:\n\n### Verify Your Setup\n\nYou can start with a simple query to confirm everything is working:\n\n```text\nWhat buckets and tables are in my Keboola project?\n```\n\n### Examples of What You Can Do\n\n**Data Exploration:**\n\n- \"What tables contain customer information?\"\n- \"Run a query to find the top 10 customers by revenue\"\n\n**Data Analysis:**\n\n- \"Analyze my sales data by region for the last quarter\"\n- \"Find correlations between customer age and purchase frequency\"\n\n**Data Pipelines:**\n\n- \"Create a SQL transformation that joins customer and order tables\"\n- \"Start the data extraction job for my Salesforce component\"\n\n## Compatibility\n\n### MCP Client Support\n\n| **MCP Client** | **Support Status** | **Connection Method** |\n|----------------|-------------------|----------------------|\n| Claude (Desktop & Web) | ✅ supported | stdio |\n| Cursor | ✅ supported | stdio |\n| Windsurf, Zed, Replit | ✅ Supported | stdio |\n| Codeium, Sourcegraph | ✅ Supported | HTTP+SSE |\n| Custom MCP Clients | ✅ Supported | HTTP+SSE or stdio |\n\n## Supported Tools\n\n**Note:** Your AI agents will automatically adjust to new tools.\n\n| Category | Tool | Description |\n|----------|------|-------------|\n| **Project** | `get_project_info` | Returns structured information about your Keboola project |\n| **Storage** | `get_bucket` | Gets detailed information about a specific bucket |\n| | `get_table` | Gets detailed information about a specific table, including DB identifier and columns |\n| | `list_buckets` | Retrieves all buckets in the project |\n| | `list_tables` | Retrieves all tables in a specific bucket |\n| | `update_description` | Updates description for a bucket, table, or column |\n| **SQL** | `query_data` | Executes a SELECT query against the underlying database |\n| **Component** | `add_config_row` | Creates a configuration row for a component configuration |\n| | `create_config` | Creates a root component configuration |\n| | `create_sql_transformation` | Creates an SQL transformation from one or more SQL code blocks |\n| | `find_component_id` | Finds component IDs matching a natural-language query |\n| | `get_component` | Retrieves details of a component by ID |\n| | `get_config` | Retrieves a specific component/transformation configuration |\n| | `get_config_examples` | Retrieves example configurations for a component |\n| | `list_configs` | Lists configurations in the project, optionally filtered |\n| | `list_transformations` | Lists transformation configurations in the project |\n| | `update_config` | Updates a root component configuration |\n| | `update_config_row` | Updates a component configuration row |\n| | `update_sql_transformation` | Updates an existing SQL transformation configuration |\n| **Flow** | `create_conditional_flow` | Creates a conditional flow (`keboola.flow`) |\n| | `create_flow` | Creates a legacy flow (`keboola.orchestrator`) |\n| | `get_flow` | Retrieves details of a specific flow configuration |\n| | `get_flow_examples` | Retrieves examples of valid flow configurations |\n| | `get_flow_schema` | Returns the JSON schema for the specified flow type |\n| | `list_flows` | Lists flow configurations in the project |\n| | `update_flow` | Updates an existing flow configuration |\n| **Jobs** | `get_job` | Retrieves detailed information about a specific job |\n| | `list_jobs` | Lists jobs with optional filtering, sorting, and pagination |\n| | `run_job` | Starts a job for a component or transformation |\n| **Data Apps** | `get_data_apps` | Retrieves detailed information about a specific Data Apps or List Data Apps in the project. |\n| | `modify_data_app` | Creates or updates Data Apps |\n| | `deploy_data_app` | Deploys or supsends Streamlit Data Apps in the Keboola environment. |\n| **Documentation** | `docs_query` | Answers questions using Keboola documentation as the source |\n| **Other** | `create_oauth_url` | Generates an OAuth authorization URL for a component configuration |\n| | `search` | Searches for items in the project by name prefixes |\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| **Authentication Errors** | Verify `KBC_STORAGE_TOKEN` is valid |\n| **Workspace Issues** | Confirm `KBC_WORKSPACE_SCHEMA` is correct |\n| **Connection Timeout** | Check network connectivity |\n\n## Development\n\n### Installation\n\nBasic setup:\n\n```bash\nuv sync --extra dev\n```\n\nWith the basic setup, you can use `uv run tox` to run tests and check code style.\n\nRecommended setup:\n\n```bash\nuv sync --extra dev --extra tests --extra integtests --extra codestyle\n```\n\nWith the recommended setup, packages for testing and code style checking will be installed which allows IDEs like\nVsCode or Cursor to check the code or run tests during development.\n\n### Integration tests\n\nTo run integration tests locally, use `uv run tox -e integtests`.\nNOTE: You will need to set the following environment variables:\n\n- `INTEGTEST_STORAGE_API_URL`\n- `INTEGTEST_STORAGE_TOKEN`\n- `INTEGTEST_WORKSPACE_SCHEMA`\n\nIn order to get these values, you need a dedicated Keboola project for integration tests.\n\n### Updating `uv.lock`\n\nUpdate the `uv.lock` file if you have added or removed dependencies. Also consider updating the lock with newer dependency\nversions when creating a release (`uv lock --upgrade`).\n\n### Updating Tool Documentation\n\nWhen you make changes to any tool descriptions (docstrings in tool functions), you must regenerate the `TOOLS.md` documentation file to reflect these changes:\n\n```bash\nuv run python -m src.keboola_mcp_server.generate_tool_docs\n```\n\n## Support and Feedback\n\n**⭐ The primary way to get help, report bugs, or request features is by [opening an issue on GitHub](https://github.com/keboola/mcp-server/issues/new). ⭐**\n\nThe development team actively monitors issues and will respond as quickly as possible. For general information about Keboola, please use the resources below.\n\n## Resources\n\n- [User Documentation](https://help.keboola.com/)\n- [Developer Documentation](https://developers.keboola.com/)\n- [Keboola Platform](https://www.keboola.com)\n- [Issue Tracker](https://github.com/keboola/mcp-server/issues/new) ← **Primary contact method for MCP Server**\n\n## Connect\n\n- [LinkedIn](https://www.linkedin.com/company/keboola)\n- [Twitter](https://x.com/keboola)\n- [Changelog](https://changelog.keboola.com/)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "keboola",
        "platform",
        "platforms",
        "data keboola",
        "data platform",
        "data platforms"
      ],
      "category": "data-platforms"
    },
    "mattijsdp--dbt-docs-mcp": {
      "owner": "mattijsdp",
      "name": "dbt-docs-mcp",
      "url": "https://github.com/mattijsdp/dbt-docs-mcp",
      "imageUrl": "",
      "description": "MCP server for dbt-core (OSS) users as the official dbt MCP only supports dbt Cloud. Supports project metadata, model and column-level lineage and dbt documentation.",
      "stars": 18,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-29T08:06:47Z",
      "readme_content": "[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/ad4aaf73-63ce-42e0-b27c-8541ae1fbab8)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/mattijsdp/dbt-docs-mcp)](https://archestra.ai/mcp-catalog/mattijsdp__dbt-docs-mcp)\n\n# dbt-docs-mcp\n\nModel Context Protocol (MCP) server for interacting with dbt project metadata, including dbt Docs artifacts (`manifest.json`, `catalog.json`). This server exposes dbt graph information and allows querying node details, model/column lineage, and related metadata.\n\n## Key Functionality\n\nThis server provides tools to:\n\n*   **Search dbt Nodes:**\n    *   Find nodes (models, sources, tests, etc.) by name (`search_dbt_node_names`).\n    *   Locate nodes based on column names (`search_dbt_column_names`).\n    *   Search within the compiled SQL code of nodes (`search_dbt_sql_code`).\n*   **Inspect Nodes:**\n    *   Retrieve detailed attributes for any given node unique ID (`get_dbt_node_attributes`).\n*   **Explore Lineage:**\n    *   Find direct upstream dependencies (predecessors) of a node (`get_dbt_predecessors`).\n    *   Find direct downstream dependents (successors) of a node (`get_dbt_successors`).\n*   **Column-Level Lineage:**\n    *   Trace all upstream sources for a specific column in a model (`get_column_ancestors`).\n    *   Trace all downstream dependents of a specific column in a model (`get_column_descendants`).\n*   **Suggested extensions:**\n    *   Tool that allows executing SQL queries.\n    *   Tool that retrieves table/view/column metadata directly from the database.\n    *   Tool to search knowledge-base.\n\n## Getting Started\n\n1.  **Prerequisites:** Ensure you have Python installed and [uv](https://docs.astral.sh/uv/)\n2.  **Clone the repo:**\n    ```bash\n    git clone <repository-url>\n    cd dbt-docs-mcp\n    ```\n3.  **Optional: parse dbt manifest for column-level lineage:**\n    - Setup the required Python environment, e.g.:\n    ```bash\n    uv sync\n    ```\n    - Use the provided script `scripts/create_manifest_cl.py` and simply provide the path to your dbt manifest, dbt catalog and the desired output paths for your schema and column lineage file:\n    ```bash\n    python scripts/create_manifest_cl.py --manifest-path PATH_TO_YOUR_MANIFEST_FILE --catalog-path PATH_TO_YOUR_CATALOG_FILE --schema-mapping-path DESIRED_OUTPUT_PATH_FOR_SCHEMA_MAPPING --manifest-cl-path DESIRED_OUTPUT_PATH_FOR_MANIFEST_CL\n    ```\n    - Depending on your dbt project size, creating column-lineage can take a while (hours)\n4.  **Run the Server:**\n    - If your desired MCP client (Claude desktop, Cursor, etc.) supports mcp.json it would look as below:\n    ```json\n    {\n        \"mcpServers\": {\n            \"DBT Docs MCP\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"run\",\n                \"--with\",\n                \"networkx,mcp[cli],rapidfuzz,dbt-core,python-decouple,sqlglot,tqdm\",\n                \"mcp\",\n                \"run\",\n                \"/Users/mattijs/repos/dbt-docs-mcp/src/mcp_server.py\"\n            ],\n            \"env\": {\n                \"MANIFEST_PATH\": \"/Users/mattijs/repos/dbt-docs-mcp/inputs/manifest.json\",\n                \"SCHEMA_MAPPING_PATH\": \"/Users/mattijs/repos/dbt-docs-mcp/outputs/schema_mapping.json\",\n                \"MANIFEST_CL_PATH\": \"/Users/mattijs/repos/dbt-docs-mcp/outputs/manifest_column_lineage.json\"\n            }\n            }\n        }\n    }\n    ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dbt",
        "platforms",
        "mcp",
        "dbt cloud",
        "supports dbt",
        "data platforms"
      ],
      "category": "data-platforms"
    },
    "yashshingvi--databricks-genie-MCP": {
      "owner": "yashshingvi",
      "name": "databricks-genie-MCP",
      "url": "https://github.com/yashshingvi/databricks-genie-MCP",
      "imageUrl": "",
      "description": "A server that connects to the Databricks Genie API, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-08T02:23:08Z",
      "readme_content": "\n# Databricks Genie MCP Server\n\nA Model Context Protocol (MCP) server that connects to the Databricks Genie API, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.\n\n\n## ✨ Features\n\n- List Genie spaces available in your Databricks workspace (Currently Manual/Using Resource)\n- Fetch metadata (title, description) of a specific Genie space\n- Start new Genie conversations with natural language questions\n- Ask follow-up questions in ongoing Genie conversations\n- Retrieve SQL and result tables in structured format\n\n## 🧱 Prerequisites\n\n- Python 3.7+\n- Databricks workspace with:\n  - Personal access token\n  - Genie API enabled\n  - Permissions to access Genie spaces and run queries\n\n\n## ⚙️ Setup\n\n1. **Clone this repository**\n\n2. **Create and activate a virtual environment** (recommended):\n  \n\n```\n python -m venv .venv\n source .venv/bin/activate\n ```\n\n   \n**Install dependencies:**\n\n```\npip install -r requirements.txt\n```\n\nCreate a **.env** file in the root directory with the following variables:\n\n```\nDATABRICKS_HOST=your-databricks-instance.cloud.databricks.com # Don't add https\nDATABRICKS_TOKEN=your-personal-access-token\n```\n\n\n📌 **Manually Adding Genie Space IDs**\n\n**Note:**  \n At this time, the Databricks Genie API **does not provide a public endpoint to list all available space IDs and titles**.  (afaik)\nAs a workaround, you need to **manually add the Genie space IDs and their titles** in the `get_genie_space_id()` function in `main.py`.\n\n\n\n\n\n## 🧪 Test the Server\nYou can test the MCP server using the inspector (optional but recommended):\n\n```\nnpx @modelcontextprotocol/inspector python main.py\n```\nOR\n\n**You can directly build and run docker to test the server**\n\n## 💬 Use with Claude Desktop\n\nDownload Claude Desktop\n\n**Install Your MCP Server:**\nFrom your project directory, run:\n\n```\nmcp install main.py\n```\n**Once Server Installed**\n  1. Connect in Claude\n   \n   2. Open Claude Desktop\n   \n   3. Click Resources → Add Resource\n   \n   4. Select your Genie MCP Server\n   \n   5. Start chatting with your data using natural language! 🎯\n\n\n\n\n## 🧾 Obtaining Databricks Credentials\n**Host**\nYour Databricks instance URL (e.g., your-instance.cloud.databricks.com) — do not include https://\n\n**Token**\n\n 1. Go to your Databricks workspace\n    \n 2. Click your username (top right) → User Settings\n 3. Under the Developer tab, click Manage under \"Access tokens\"\n 4. Generate a new token and copy it\n\n\n\n\n## 🚀 Running the Server\n\n```\npython main.py\n```\nThis will start the Genie MCP server over the stdio transport for LLM interaction.\n\n## 🧰 Available MCP Tools\nThe following MCP tools are available:\n\n\n**Tool\tDescription**\n1. get_genie_space_id()\tList available Genie space IDs and titles\n2. get_space_info(space_id: str)\tRetrieve title and description of a Genie space\n3. ask_genie(space_id: str, question: str)\tStart a new Genie conversation and get results\n4. follow_up(space_id: str, conversation_id: str, question: str)\tContinue an existing Genie conversation\n\n## 🛠️ Troubleshooting\nCommon Issues\n- Invalid host: Ensure the host does not include https://\n\n- Token error: Make sure your personal access token is valid and has access to Genie\n\n- Timeout: Check if the Genie space is accessible and not idle/expired\n\n- No data returned: Ensure your query is valid for the selected space\n\n## 🔐 Security Considerations\n\n - Keep your .env file secure and never commit it to version control\n\n - Use minimal scope tokens with expiration whenever possible\n\n- Avoid exposing this server in public-facing environments unless authenticated\n\n## Claude Desktop Screenshots\n\n![image](https://github.com/user-attachments/assets/42b391d3-0ae8-48bd-8665-a1560437b8ef)\n\n![image](https://github.com/user-attachments/assets/eb80c99f-e854-4d55-bb0e-8a447c29ee51)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databricks",
        "platforms",
        "pipeline",
        "databricks genie",
        "data platforms",
        "yashshingvi databricks"
      ],
      "category": "data-platforms"
    }
  }
}