{
  "category": "scientific-research-tools",
  "categoryDisplay": "Scientific Research Tools",
  "description": "",
  "totalRepositories": 69,
  "repositories": {
    "54yyyu--zotero-mcp": {
      "owner": "54yyyu",
      "name": "zotero-mcp",
      "url": "https://github.com/54yyyu/zotero-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/54yyyu.webp",
      "description": "Zotero MCP allows you to link your Zotero research library with AI assistants, enabling you to discuss academic papers, receive summaries, and analyze citations for better research productivity.",
      "stars": 565,
      "forks": 42,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:21:36Z",
      "readme_content": "# Zotero MCP: Chat with your Research Library‚ÄîLocal or Web‚Äîin Claude, ChatGPT, and more.\n\n<p align=\"center\">\n  <a href=\"https://www.zotero.org/\">\n    <img src=\"https://img.shields.io/badge/Zotero-CC2936?style=for-the-badge&logo=zotero&logoColor=white\" alt=\"Zotero\">\n  </a>\n  <a href=\"https://www.anthropic.com/claude\">\n    <img src=\"https://img.shields.io/badge/Claude-6849C3?style=for-the-badge&logo=anthropic&logoColor=white\" alt=\"Claude\">\n  </a>\n  <a href=\"https://chatgpt.com/\">\n    <img src=\"https://img.shields.io/badge/ChatGPT-74AA9C?style=for-the-badge&logo=openai&logoColor=white\" alt=\"ChatGPT\">\n  </a>\n  <a href=\"https://modelcontextprotocol.io/introduction\">\n    <img src=\"https://img.shields.io/badge/MCP-0175C2?style=for-the-badge&logoColor=white\" alt=\"MCP\">\n  </a>\n</p>\n\n**Zotero MCP** seamlessly connects your [Zotero](https://www.zotero.org/) research library with [ChatGPT](https://openai.com), [Claude](https://www.anthropic.com/claude), and other AI assistants (e.g., [Cherry Studio](https://cherry-ai.com/), [Chorus](https://chorus.sh), [Cursor](https://www.cursor.com/)) via the [Model Context Protocol](https://modelcontextprotocol.io/introduction). Review papers, get summaries, analyze citations, extract PDF annotations, and more!\n\n## ‚ú® Features\n\n### üß† AI-Powered Semantic Search\n- **Vector-based similarity search** over your entire research library\n- **Multiple embedding models**: Default (free), OpenAI, and Gemini options\n- **Intelligent results** with similarity scores and contextual matching\n- **Auto-updating database** with configurable sync schedules\n\n### üîç Search Your Library\n- Find papers, articles, and books by title, author, or content\n- Perform complex searches with multiple criteria\n- Browse collections, tags, and recent additions\n- **NEW**: Semantic search for conceptual and topic-based discovery\n\n### üìö Access Your Content\n- Retrieve detailed metadata for any item\n- Get full text content (when available)\n- Access attachments, notes, and child items\n\n### üìù Work with Annotations\n- Extract and search PDF annotations directly\n- Access Zotero's native annotations\n- Create and update notes and annotations\n\n### üîÑ Easy Updates\n- **Smart update system** that detects your installation method (uv, pip, conda, pipx)\n- **Configuration preservation** - all settings maintained during updates\n- **Version checking** and automatic update notifications\n\n### üåê Flexible Access Methods\n- Local method for offline access (no API key needed)\n- Web API for cloud library access\n- Perfect for both local research and remote collaboration\n\n## üöÄ Quick Install\n\n### Default Installation\n\n#### Installing via uv\n\n```bash\nuv tool install \"git+https://github.com/54yyyu/zotero-mcp.git\"\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n#### Installing via pip\n\n```bash\npip install git+https://github.com/54yyyu/zotero-mcp.git\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n### Installing via Smithery\n\nTo install Zotero MCP via [Smithery](https://smithery.ai/server/@54yyyu/zotero-mcp) for Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @54yyyu/zotero-mcp --client claude\n```\n\n#### Updating Your Installation\n\nKeep zotero-mcp up to date with the smart update command:\n\n```bash\n# Check for updates\nzotero-mcp update --check-only\n\n# Update to latest version (preserves all configurations)\nzotero-mcp update\n```\n\n## üß† Semantic Search\n\nZotero MCP now includes powerful AI-powered semantic search capabilities that let you find research based on concepts and meaning, not just keywords.\n\n### Setup Semantic Search\n\nDuring setup or separately, configure semantic search:\n\n```bash\n# Configure during initial setup (recommended)\nzotero-mcp setup\n\n# Or configure semantic search separately\nzotero-mcp setup --semantic-config-only\n```\n\n**Available Embedding Models:**\n- **Default (all-MiniLM-L6-v2)**: Free, runs locally, good for most use cases\n- **OpenAI**: Better quality, requires API key (`text-embedding-3-small` or `text-embedding-3-large`)\n- **Gemini**: Better quality, requires API key (`models/text-embedding-004` or experimental models)\n\n**Update Frequency Options:**\n- **Manual**: Update only when you run `zotero-mcp update-db`\n- **Auto on startup**: Update database every time the server starts\n- **Daily**: Update once per day automatically\n- **Every N days**: Set custom interval\n\n### Using Semantic Search\n\nAfter setup, initialize your search database:\n\n```bash\n# Build the semantic search database (fast, metadata-only)\nzotero-mcp update-db\n\n# Build with full-text extraction (slower, more comprehensive)\nzotero-mcp update-db --fulltext\n\n# Check database status\nzotero-mcp db-status\n```\n\n**Example Semantic Queries in your AI assistant:**\n- *\"Find research similar to machine learning concepts in neuroscience\"*\n- *\"Papers that discuss climate change impacts on agriculture\"*\n- *\"Research related to quantum computing applications\"*\n- *\"Studies about social media influence on mental health\"*\n- *\"Find papers conceptually similar to this abstract: [paste abstract]\"*\n\nThe semantic search provides similarity scores and finds papers based on conceptual understanding, not just keyword matching.\n\n## üñ•Ô∏è Setup & Usage\n\nFull documentation is available at [Zotero MCP docs](https://stevenyuyy.us/zotero-mcp/).\n\n**Requirements**\n- Python 3.10+\n- Zotero 7+ (for local API with full-text access)\n- An MCP-compatible client (e.g., Claude Desktop, ChatGPT Developer Mode, Cherry Studio, Chorus)\n\n**For ChatGPT setup: see the [Getting Started guide](./docs/getting-started.md).**\n\n### For Claude Desktop (example MCP client)\n\n#### Configuration\nAfter installation, either:\n\n1. **Auto-configure** (recommended):\n   ```bash\n   zotero-mcp setup\n   ```\n\n2. **Manual configuration**:\n   Add to your `claude_desktop_config.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"zotero\": {\n         \"command\": \"zotero-mcp\",\n         \"env\": {\n           \"ZOTERO_LOCAL\": \"true\"\n         }\n       }\n     }\n   }\n   ```\n\n#### Usage\n\n1. Start Zotero desktop (make sure local API is enabled in preferences)\n2. Launch Claude Desktop\n3. Access the Zotero-MCP tool through Claude Desktop's tools interface\n\nExample prompts:\n- \"Search my library for papers on machine learning\"\n- \"Find recent articles I've added about climate change\"\n- \"Summarize the key findings from my paper on quantum computing\"\n- \"Extract all PDF annotations from my paper on neural networks\"\n- \"Search my notes and annotations for mentions of 'reinforcement learning'\"\n- \"Show me papers tagged '#Arm' excluding those with '#Crypt' in my library\"\n- \"Search for papers on operating system with tag '#Arm'\"\n- \"Export the BibTeX citation for papers on machine learning\"\n- **\"Find papers conceptually similar to deep learning in computer vision\"** *(semantic search)*\n- **\"Research that relates to the intersection of AI and healthcare\"** *(semantic search)*\n- **\"Papers that discuss topics similar to this abstract: [paste text]\"** *(semantic search)*\n\n### For Cherry Studio\n\n#### Configuration\nGo to Settings -> MCP Servers -> Edit MCP Configuration, and add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"name\": \"zotero\",\n      \"type\": \"stdio\",\n      \"isActive\": true,\n      \"command\": \"zotero-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"ZOTERO_LOCAL\": \"true\"\n      }\n    }\n  }\n}\n```\nThen click \"Save\".\n\nCherry Studio also provides a visual configuration method for general settings and tools selection.\n\n## üîß Advanced Configuration\n\n### Using Web API Instead of Local API\n\nFor accessing your Zotero library via the web API (useful for remote setups):\n\n```bash\nzotero-mcp setup --no-local --api-key YOUR_API_KEY --library-id YOUR_LIBRARY_ID\n```\n\n### Environment Variables\n\n**Zotero Connection:**\n- `ZOTERO_LOCAL=true`: Use the local Zotero API (default: false)\n- `ZOTERO_API_KEY`: Your Zotero API key (for web API)\n- `ZOTERO_LIBRARY_ID`: Your Zotero library ID (for web API)\n- `ZOTERO_LIBRARY_TYPE`: The type of library (user or group, default: user)\n\n**Semantic Search:**\n- `ZOTERO_EMBEDDING_MODEL`: Embedding model to use (default, openai, gemini)\n- `OPENAI_API_KEY`: Your OpenAI API key (for OpenAI embeddings)\n- `OPENAI_EMBEDDING_MODEL`: OpenAI model name (text-embedding-3-small, text-embedding-3-large)\n- `GEMINI_API_KEY`: Your Gemini API key (for Gemini embeddings)\n- `GEMINI_EMBEDDING_MODEL`: Gemini model name (models/text-embedding-004, etc.)\n\n### Command-Line Options\n\n```bash\n# Run the server directly\nzotero-mcp serve\n\n# Specify transport method\nzotero-mcp serve --transport stdio|streamable-http|sse\n\n# Setup and configuration\nzotero-mcp setup --help                    # Get help on setup options\nzotero-mcp setup --semantic-config-only    # Configure only semantic search\nzotero-mcp setup-info                      # Show installation path and config info for MCP clients\n\n# Updates and maintenance\nzotero-mcp update                          # Update to latest version\nzotero-mcp update --check-only             # Check for updates without installing\nzotero-mcp update --force                  # Force update even if up to date\n\n# Semantic search database management\nzotero-mcp update-db                       # Update semantic search database (fast, metadata-only)\nzotero-mcp update-db --fulltext             # Update with full-text extraction (comprehensive but slower)\nzotero-mcp update-db --force-rebuild       # Force complete database rebuild\nzotero-mcp update-db --fulltext --force-rebuild  # Rebuild with full-text extraction\nzotero-mcp db-status                       # Show database status and info\n\n# General\nzotero-mcp version                         # Show current version\n```\n\n## üìë PDF Annotation Extraction\n\nZotero MCP includes advanced PDF annotation extraction capabilities:\n\n- **Direct PDF Processing**: Extract annotations directly from PDF files, even if they're not yet indexed by Zotero\n- **Enhanced Search**: Search through PDF annotations and comments \n- **Image Annotation Support**: Extract image annotations from PDFs\n- **Seamless Integration**: Works alongside Zotero's native annotation system\n\nFor optimal annotation extraction, it is **highly recommended** to install the [Better BibTeX plugin](https://retorque.re/zotero-better-bibtex/installation/) for Zotero. The annotation-related functions have been primarily tested with this plugin and provide enhanced functionality when it's available.\n\n\nThe first time you use PDF annotation features, the necessary tools will be automatically downloaded.\n\n## üìö Available Tools\n\n### üß† Semantic Search Tools\n- `zotero_semantic_search`: AI-powered similarity search with embedding models\n- `zotero_update_search_database`: Manually update the semantic search database\n- `zotero_get_search_database_status`: Check database status and configuration\n\n### üîç Search Tools\n- `zotero_search_items`: Search your library by keywords\n- `zotero_advanced_search`: Perform complex searches with multiple criteria\n- `zotero_get_collections`: List collections\n- `zotero_get_collection_items`: Get items in a collection\n- `zotero_get_tags`: List all tags\n- `zotero_get_recent`: Get recently added items\n- `zotero_search_by_tag`: Search your library using custom tag filters\n\n### üìö Content Tools\n- `zotero_get_item_metadata`: Get detailed metadata (supports BibTeX export via `format=\"bibtex\"`)\n- `zotero_get_item_fulltext`: Get full text content\n- `zotero_get_item_children`: Get attachments and notes\n\n### üìù Annotation & Notes Tools\n- `zotero_get_annotations`: Get annotations (including direct PDF extraction)\n- `zotero_get_notes`: Retrieve notes from your Zotero library\n- `zotero_search_notes`: Search in notes and annotations (including PDF-extracted)\n- `zotero_create_note`: Create a new note for an item (beta feature)\n\n## üîç Troubleshooting\n\n### General Issues\n- **No results found**: Ensure Zotero is running and the local API is enabled. You need to toggle on `Allow other applications on this computer to communicate with Zotero` in Zotero preferences.\n- **Can't connect to library**: Check your API key and library ID if using web API\n- **Full text not available**: Make sure you're using Zotero 7+ for local full-text access\n- **Local library limitations**: Some functionality (tagging, library modifications) may not work with local JS API. Consider using web library setup for full functionality. (See the [docs](docs/getting-started.md#local-library-limitations) for more info.)\n- **Installation/search option switching issues**: Database problems from changing install methods or search options can often be resolved with `zotero-mcp update-db --force-rebuild`\n\n### Semantic Search Issues\n- **\"Missing required environment variables\" when running update-db**: Run `zotero-mcp setup` to configure your environment, or the CLI will automatically load settings from your MCP client config (e.g., Claude Desktop)\n- **ChromaDB warnings**: Update to the latest version - deprecation warnings have been fixed\n- **Database update takes long**: By default, `update-db` is fast (metadata-only). For comprehensive indexing with full-text, use `--fulltext` flag. Use `--limit` parameter for testing: `zotero-mcp update-db --limit 100`\n- **Semantic search returns no results**: Ensure the database is initialized with `zotero-mcp update-db` and check status with `zotero-mcp db-status`\n- **Limited search quality**: For better semantic search results, use `zotero-mcp update-db --fulltext` to index full-text content (requires local Zotero setup)\n- **OpenAI/Gemini API errors**: Verify your API keys are correctly set and have sufficient credits/quota\n\n### Update Issues  \n- **Update command fails**: Check your internet connection and try `zotero-mcp update --force`\n- **Configuration lost after update**: The update process preserves configs automatically, but check `~/.config/zotero-mcp/` for backup files\n\n## üìÑ License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zotero",
        "research",
        "mcp",
        "mcp zotero",
        "zotero research",
        "zotero mcp"
      ],
      "category": "scientific-research-tools"
    },
    "BioContext--PubChem-MCP": {
      "owner": "BioContext",
      "name": "PubChem-MCP",
      "url": "https://github.com/BioContext/PubChem-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/BioContext.webp",
      "description": "Provides access to PubChem chemical and bioassay data through a standardized MCP interface, allowing users to search for compounds, retrieve detailed chemical and bioassay information, and query molecular properties.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-04-17T20:18:58Z",
      "readme_content": "# PubChem-MCP\n\nAn MCP (Model Context Protocol) server for accessing PubChem data. This server provides tools for querying the PubChem database for compounds, substances, bioassays, and related information.\n\n## Features\n\n* Search for compounds by name, structure, or identifier\n* Retrieve detailed information about compounds and substances\n* Access bioassay data and molecular properties\n* Query chemical classifications and cross-references\n* Compatible with any MCP client, including Claude for Desktop\n\n## Installation\n\n### Option 1: From GitHub\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### Option 2: Using pip (Once published)\n\n```bash\npip install pubchem-mcp\n```\n\n## Usage\n\n### Running as a standalone server\n\n```bash\n# From source\npython -m mcp_server\n\n# If installed via pip\npubchem-mcp\n```\n\n### Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Configure Claude for Desktop to use this server by editing `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS/Linux) or `%AppData%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n    \"mcpServers\": {\n        \"pubchem\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"mcp_server\"\n            ]\n        }\n    }\n}\n```\n\n3. Restart Claude for Desktop\n4. You can now use the PubChem tools in Claude\n\n### Example queries for Claude\n\n* \"Find information about aspirin in PubChem\"\n* \"What are the properties of compound CID 2244?\"\n* \"Show me the structure of paracetamol\"\n\n## Available Tools\n\n* `search_compound`: Search for compounds by name or identifier\n* `get_compound_details`: Get detailed information about a compound\n* `get_compound_properties`: Get physical and chemical properties of a compound\n* `search_bioassay`: Search for bioassays\n* `get_substance_details`: Get detailed information about a substance\n* And more...\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biocontext",
        "pubchem",
        "bioassay",
        "biocontext pubchem",
        "tools biocontext",
        "pubchem mcp"
      ],
      "category": "scientific-research-tools"
    },
    "Garoth--wolframalpha-llm-mcp": {
      "owner": "Garoth",
      "name": "wolframalpha-llm-mcp",
      "url": "https://github.com/Garoth/wolframalpha-llm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Garoth.webp",
      "description": "Provides access to WolframAlpha's LLM API for querying complex information in natural language, allowing for structured responses on topics like science, mathematics, and history.",
      "stars": 39,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T02:59:15Z",
      "readme_content": "# WolframAlpha LLM MCP Server\n\n\n\nA Model Context Protocol (MCP) server that provides access to WolframAlpha's LLM API. https://products.wolframalpha.com/llm-api/documentation\n\n<div>\n  <br/><br/>\n  \n</div>\n\n## Features\n\n- Query WolframAlpha's LLM API with natural language questions\n- Answer complicated mathematical questions\n- Query facts about science, physics, history, geography, and more\n- Get structured responses optimized for LLM consumption\n- Support for simplified answers and detailed responses with sections\n\n## Available Tools\n\n- `ask_llm`: Ask WolframAlpha a question and get a structured llm-friendly response\n- `get_simple_answer`: Get a simplified answer\n- `validate_key`: Validate the WolframAlpha API key\n\n## Installation\n\n```bash\ngit clone https://github.com/Garoth/wolframalpha-llm-mcp.git\nnpm install\n```\n\n## Configuration\n\n1. Get your WolframAlpha API key from [developer.wolframalpha.com](https://developer.wolframalpha.com/)\n\n2. Add it to your Cline MCP settings file inside VSCode's settings (ex. ~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"wolframalpha\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/wolframalpha-mcp-server/build/index.js\"],\n      \"env\": {\n        \"WOLFRAM_LLM_APP_ID\": \"your-api-key-here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"ask_llm\",\n        \"get_simple_answer\",\n        \"validate_key\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n### Setting Up Tests\n\nThe tests use real API calls to ensure accurate responses. To run the tests:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit `.env` and add your WolframAlpha API key:\n   ```\n   WOLFRAM_LLM_APP_ID=your-api-key-here\n   ```\n   Note: The `.env` file is gitignored to prevent committing sensitive information.\n\n3. Run the tests:\n   ```bash\n   npm test\n   ```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolframalpha",
        "scientific",
        "tools",
        "research tools",
        "wolframalpha llm",
        "llm api"
      ],
      "category": "scientific-research-tools"
    },
    "HarshJ23--deepseek-claude-MCP-server": {
      "owner": "HarshJ23",
      "name": "deepseek-claude-MCP-server",
      "url": "https://github.com/HarshJ23/deepseek-claude-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/HarshJ23.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude to enhance reasoning capabilities for complex tasks. Supports intricate multi-step reasoning for improved precision and efficiency.",
      "stars": 50,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-13T00:27:41Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## üöÄ Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   \n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   \n\n\n   - If the server isn‚Äôt visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## üõ† Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## üìÑ License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "advanced",
        "reasoning engine",
        "deepseek r1",
        "integrates deepseek"
      ],
      "category": "scientific-research-tools"
    },
    "Jacck--mcp-reasoner": {
      "owner": "Jacck",
      "name": "mcp-reasoner",
      "url": "https://github.com/Jacck/mcp-reasoner",
      "imageUrl": "/freedevtools/mcp/pfp/Jacck.webp",
      "description": "Systematic reasoning server implementation for Claude Desktop, supporting Beam Search and Monte Carlo Tree Search (MCTS) for enhanced complex problem-solving. Features experimental algorithms for policy simulation and adaptive exploration.",
      "stars": 254,
      "forks": 28,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T12:29:08Z",
      "readme_content": "# MCP Reasoner\nA reasoning implementation for Claude Desktop that lets you use both Beam Search and Monte Carlo Tree Search (MCTS). tbh this started as a way to see if we could make Claude even better at complex problem-solving... turns out we definitely can.\n\n### Current Version:\n**v2.0.0**\n\n#### What's New:\n\n> Added 2 Experimental Reasoning Algorithms:\n>\n>     - `mcts-002-alpha`\n>\n>         - Uses the A* Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n>\n>     - `mcts-002alt-alpha`\n>\n>         - Uses the Bidirectional Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n\n\nWhat happened to `mcts-001-alpha` and `mcts-001alt-alpha`?\n> Quite simply: It was useless and near similar to the base `mcts` method. After initial testing the results yielded in basic thought processes was near similar showing that simply adding policy simulation may not have an effect.\n\nSo why add Polciy Simulation Layer now?\n> Well i think its important to incorporate Policy AND Search in tandem as that is how most of the algorithms implement them.\n\n#### Previous Versions:\n**v1.1.0**\n\n> Added model control over search parameters:\n>\n> beamWidth - lets Claude adjust how many paths to track (1-10)\n>\n> numSimulations - fine-tune MCTS simulation count (1-150)\n\n## Features\n- Two search strategies that you can switch between:\n   - Beam search (good for straightforward stuff)\n   - MCTS (when stuff gets complex) with alpha variations (see above)\n- Tracks how good different reasoning paths are\n- Maps out all the different ways Claude thinks through problems\n- Analyzes how the reasoning process went\n- Follows the MCP protocol (obviously)\n\n## Installation\n```\ngit clone https://github.com/frgmt0/mcp-reasoner.git \n\nOR clone the original:\n\ngit clone https://github.com/Jacck/mcp-reasoner.git\n\ncd mcp-reasoner\nnpm install\nnpm run build\n```\n\n## Configuration\nAdd to Claude Desktop config:\n```\n{\n  \"mcpServers\": {\n    \"mcp-reasoner\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-reasoner/dist/index.js\"],\n    }\n  }\n}\n```\n\n## Testing\n\n[More Testing Coming Soon]\n\n## Benchmarks\n\n[Benchmarking will be added soon]\n\nKey Benchmarks to test against:\n\n- MATH500\n\n- GPQA-Diamond\n\n- GMSK8\n\n- Maybe Polyglot &/or SWE-Bench\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "reasoner",
        "mcts",
        "systematic reasoning",
        "mcp reasoner",
        "reasoning server"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Crossref-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Crossref-MCP-Server",
      "url": "https://github.com/JackKuo666/Crossref-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connect to Crossref's database to search and access academic paper metadata, including metadata retrieval for specific papers, journals, and funding organizations using various queries.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:46Z",
      "readme_content": "# Crossref MCP Server\n\nüîç Enable AI assistants to search and access academic paper metadata through Crossref using a simple MCP interface.\n\nThe Crossref MCP Server provides a bridge between AI assistants and Crossref's database of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and retrieve journal and funder information in a programmatic way.\n\n## ‚ú® Core Features\n\n- üîé Work Search by Query: Find papers using keywords, titles, or authors ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers by DOI ‚úÖ\n- üìö Journal Search: Find journals in the Crossref database ‚úÖ\n- üí∞ Funder Search: Discover funding organizations and their supported research ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Crossref-MCP-Server.git\n   cd Crossref-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython crossref_server.py\n```\n\n## Usage with Claude Desktop or Cline\n\nAdd this configuration to your `cline_mcp_settings.json` or `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"crossref_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Crossref-MCP-Server\\\\crossref_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe Crossref MCP Server provides the following tools:\n\n1. `search_works_by_query`: Search for scholarly works using keywords, titles, or authors.\n2. `get_work_metadata`: Get detailed metadata for a specific work using its DOI.\n3. `search_journals`: Search for journals in the Crossref database.\n4. `search_funders`: Search for funding organizations in the Crossref database.\n\n### Searching Works by Query\n\nYou can ask the AI assistant to search for papers using keywords:\n```\nCan you search Crossref for papers about \"machine learning in healthcare\"?\n```\n\n### Getting Work Metadata by DOI\n\nYou can get detailed metadata for a specific paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature14539?\n```\n\n### Searching Journals\n\nYou can search for journals in the Crossref database:\n```\nCan you find journals related to \"artificial intelligence\" in Crossref?\n```\n\n### Searching Funders\n\nYou can search for funding organizations:\n```\nCan you find information about the \"National Science Foundation\" in Crossref?\n```\n\n\n## üìÅ Project Structure\n\n- `crossref_server.py`: The main MCP server implementation using FastMCP\n- `crossref_search.py`: Contains the logic for searching Crossref and retrieving metadata\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP (mcp)\n- requests\n- bs4\n- habanero\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Note\n\nThis tool uses the Crossref API to access publicly available metadata about academic works. For better API access priority, it's recommended to provide your email address when initializing the CrossrefSearch class.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crossref",
        "journals",
        "metadata",
        "research tools",
        "paper metadata",
        "metadata retrieval"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Google-Scholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Google-Scholar-MCP-Server",
      "url": "https://github.com/JackKuo666/Google-Scholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Enables AI assistants to perform searches on Google Scholar to access academic papers, retrieve paper metadata, and gather detailed author information. Facilitates efficient academic research through a seamless programmatic interface.",
      "stars": 138,
      "forks": 25,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T08:00:22Z",
      "readme_content": "# Google Scholar MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/google-scholar-mcp-server)](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server)\n\nüîç Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n\nThe Google Scholar MCP Server provides a bridge between AI assistants and Google Scholar through the Model Context Protocol (MCP). It allows AI models to search for academic papers and access their content in a programmatic way.\n\n## ‚ú® Core Features\n- üîé Paper Search: Query Google Scholar papers with custom search strings or advanced search parameters ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üë§ Author Information: Retrieve detailed information about authors ‚úÖ\n- üìä Research Support: Facilitate academic research and analysis ‚úÖ\n\n## üöÄ Quick Start\n\n### Installing Manually\n### Installing via Smithery\n\nTo install google-scholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/google-scholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/google-scholar-MCP-Server.git\n   cd google-scholar-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/Google-Scholar-MCP-Server.git\ncd Google-Scholar-MCP-Server\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython google_scholar_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_key_words\", {\n    \"query\": \"artificial intelligence ethics\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_advanced\", {\n    \"query\": \"machine learning\",\n    \"author\": \"Hinton\",\n    \"year_range\": [2020, 2023],\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get author information\n\n```python\nresult = await mcp.use_tool(\"get_author_info\", {\n    \"author_name\": \"Geoffrey Hinton\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the Google Scholar MCP Server. Adjust the parameters as needed for your specific use case.\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"google_scholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Google-Scholar-MCP-Server\\\\google_scholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/google_scholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## üõ† MCP Tools\n\nThe Google Scholar MCP Server provides the following tools:\n\n### search_google_scholar_key_words\n\nSearch for articles on Google Scholar using key words.\n\n**Parameters:**\n- `query` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### search_google_scholar_advanced\n\nPerform an advanced search for articles on Google Scholar.\n\n**Parameters:**\n- `query` (str): General search query\n- `author` (str, optional): Author name\n- `year_range` (tuple, optional): Tuple containing (start_year, end_year)\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### get_author_info\n\nGet detailed information about an author from Google Scholar.\n\n**Parameters:**\n- `author_name` (str): Name of the author to search for\n\n**Returns:** Dictionary containing author information\n\n## üìÅ Project Structure\n\n- `google_scholar_server.py`: The main MCP server implementation using FastMCP\n- `google_scholar_web_search.py`: Contains the web scraping logic for searching Google Scholar\n\n## üîß Dependencies\n\n- Python 3.10+\n- mcp[cli]>=1.4.1\n- scholarly>=1.7.0\n- asyncio>=3.4.3\n\nYou can install the required dependencies using:\n\n```bash\npip install -r requirements.txt\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect Google Scholar's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "google",
        "academic",
        "google scholar",
        "scholar access",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubChem-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubChem-MCP-Server",
      "url": "https://github.com/JackKuo666/PubChem-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Access and retrieve information about chemical compounds from PubChem, including detailed property data and molecular structures through a simple interface. Supports queries by name, SMILES, or CID to streamline research and analysis in chemistry.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-13T09:03:06Z",
      "readme_content": "# PubChem MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubchem-mcp-server)](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server)\n\nüß™ Enable AI assistants to search and access chemical compound information through a simple MCP interface.\n\nThe PubChem MCP Server provides a bridge between AI assistants and PubChem's chemical database through the Model Context Protocol (MCP). It allows AI models to search for chemical compounds and access their detailed information in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Compound Search: Query PubChem compounds by name, SMILES, or CID ‚úÖ\n- üß™ Chemical Structure: Access molecular structures and identifiers ‚úÖ\n- üìä Property Data: Retrieve detailed chemical and physical properties ‚úÖ\n- üî¨ Advanced Search: Combine multiple parameters for precise queries ‚úÖ\n- üß¨ Molecular Visualization: Generate and display molecular structures üìù\n- üìà Property Analysis: Compare properties across multiple compounds üìù\n- üóÉÔ∏è Local Storage: Save frequently used compounds for faster access üìù\n- üìù Chemistry Prompts: Specialized prompts for chemical analysis üìù\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install PubChem Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server):\n\n#### Claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubchem-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client cline --config \"{}\"\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install pubchem-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/PubChem-MCP-Server.git\ncd PubChem-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython pubchem_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for compounds by name\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_name\", {\n    \"name\": \"aspirin\",\n    \"max_results\": 3\n})\nprint(result)\n```\n\n### Example 2: Search for compounds by SMILES notation\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_smiles\", {\n    \"smiles\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin's SMILES\n    \"max_results\": 2\n})\nprint(result)\n```\n\n### Example 3: Get detailed information for a specific compound\n\n```python\nresult = await mcp.use_tool(\"get_pubchem_compound_by_cid\", {\n    \"cid\": 2244  # Aspirin's CID\n})\nprint(result)\n```\n\n### Example 4: Perform an advanced search with multiple parameters\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_advanced\", {\n    \"name\": \"caffeine\",\n    \"formula\": \"C8H10N4O2\",\n    \"max_results\": 2\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the four main tools provided by the PubChem MCP Server. Adjust the parameters as needed for your specific use case.\n\n## üõ† MCP Tools\n\nThe PubChem MCP Server provides the following tools:\n\n### search_pubchem_by_name\n\nSearch for chemical compounds on PubChem using a compound name.\n\n**Parameters:**\n- `name` (str): Name of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### search_pubchem_by_smiles\n\nSearch for chemical compounds on PubChem using a SMILES string.\n\n**Parameters:**\n- `smiles` (str): SMILES notation of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### get_pubchem_compound_by_cid\n\nFetch detailed information about a chemical compound using its PubChem CID.\n\n**Parameters:**\n- `cid` (int): PubChem Compound ID (CID)\n\n**Returns:** Dictionary containing compound information\n\n### search_pubchem_advanced\n\nPerform an advanced search for compounds on PubChem.\n\n**Parameters:**\n- `name` (str, optional): Name of the chemical compound\n- `smiles` (str, optional): SMILES notation of the chemical compound\n- `formula` (str, optional): Molecular formula\n- `cid` (int, optional): PubChem Compound ID\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubchem-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"pubchem-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/.venv/bin/activate && python /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/pubchem_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Compounds\n\nYou can ask Claude to search for chemical compounds using queries like:\n```\nCan you search PubChem for information about aspirin?\n```\n\nThe search will return basic information about matching compounds including:\n\n‚Ä¢ Compound name\n\n‚Ä¢ CID (PubChem Compound ID)\n\n‚Ä¢ Molecular formula\n\n‚Ä¢ Molecular weight\n\n### Getting Compound Details\n\nOnce you have a CID, you can ask for more details:\n```\nCan you show me the details for compound with CID 2244?\n```\n\nThis will return:\n\n‚Ä¢ IUPAC name\n\n‚Ä¢ Molecular formula\n\n‚Ä¢ Molecular weight\n\n‚Ä¢ SMILES notation\n\n‚Ä¢ InChI and InChIKey\n\n‚Ä¢ Physical and chemical properties\n\n‚Ä¢ Synonyms\n\n## üìù TODO\n\n### visualize_compound\n\nGenerate and display a 2D or 3D visualization of a chemical compound.\n\n### compare_compounds\n\nCompare properties and structures of multiple compounds.\n\n### save_compound\n\nSave a compound locally for faster access.\n\n### list_saved_compounds\n\nList all saved compounds.\n\n### üìù Chemistry Prompts\n\nThe server will offer specialized prompts to help analyze chemical compounds:\n\n#### Compound Analysis Prompt\n\nA comprehensive workflow for analyzing chemical compounds that only requires a compound ID:\n\n```python\nresult = await call_prompt(\"deep-compound-analysis\", {\n    \"compound_id\": \"2244\"\n})\n```\n\nThis prompt will include:\n\n- Detailed instructions for using available tools\n- A systematic workflow for compound analysis\n- Comprehensive analysis structure covering:\n  - Chemical structure and properties\n  - Pharmacological properties\n  - Biological activities\n  - Applications and uses\n  - Safety and toxicity information\n  - Related compounds\n\n## üìÅ Project Structure\n\n- `pubchem_server.py`: The main MCP server implementation using FastMCP\n- `pubchem_search.py`: Contains example code for searching PubChem\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- pubchempy (for PubChem API access)\n- pandas (for data handling)\n\nYou can install the required dependencies using:\n\n```bash\npip install mcp pubchempy pandas\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect PubChem's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubchem",
        "molecular",
        "compounds",
        "compounds pubchem",
        "pubchem mcp",
        "pubchem including"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubMed-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubMed-MCP-Server",
      "url": "https://github.com/JackKuo666/PubMed-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects AI models to PubMed for searching, accessing, and analyzing biomedical articles, allowing retrieval of detailed metadata and deep analysis of research papers. Facilitates access to full-text PDFs and supports specialized prompts for comprehensive paper examination.",
      "stars": 64,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T17:15:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jackkuo666-pubmed-mcp-server-badge.png)](https://mseep.ai/app/jackkuo666-pubmed-mcp-server)\n\n# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubmed-mcp-server)](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server)\n\nüîç Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n\nThe PubMed MCP Server provides a bridge between AI assistants and PubMed's vast repository of biomedical literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles, access their metadata, and perform deep analysis in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Paper Search: Query PubMed articles with keywords or advanced search ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìä Research Support: Facilitate biomedical sciences research and analysis ‚úÖ\n- üìÑ Paper Access: Attempt to download full-text PDF content ‚úÖ\n- üß† Deep Analysis: Perform comprehensive analysis of papers ‚úÖ\n- üìù Research Prompts: A set of specialized prompts for paper analysis ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n### Installing via Smithery\n\nTo install pubmed-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli install @JackKuo666/pubmed-mcp-server --client claude\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubmed-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubMed-MCP-Server.git\n   cd PubMed-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython pubmed_server.py\n```\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubmed-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubMed-MCP-Server\\\\pubmed_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubmed/.venv/bin/activate && python /home/YOUR/PATH/pubmed-mcp-server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe PubMed MCP Server provides the following tools:\n\n1. `search_pubmed_key_words`: Search for articles on PubMed using keywords.\n2. `search_pubmed_advanced`: Perform an advanced search for articles on PubMed with multiple parameters.\n3. `get_pubmed_article_metadata`: Fetch metadata for a PubMed article using its PMID.\n4. `download_pubmed_pdf`: Attempt to download the full-text PDF for a PubMed article.\n5. `deep_paper_analysis`: Perform a comprehensive analysis of a PubMed article.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search PubMed for recent papers about CRISPR?\n```\n\n### Getting Paper Details\n\nOnce you have a PMID, you can ask for more details:\n```\nCan you show me the metadata for the paper with PMID 12345678?\n```\n\n### Analyzing Papers\n\nYou can request a deep analysis of a paper:\n```\nCan you perform a deep analysis of the paper with PMID 12345678?\n```\n\n## üìÅ Project Structure\n\n- `pubmed_server.py`: The main MCP server implementation using FastMCP\n- `pubmed_web_search.py`: Contains the logic for searching PubMed and retrieving article information\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests\n- beautifulsoup4\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect PubMed's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "papers",
        "pubmed searching",
        "models pubmed",
        "pubmed mcp"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubTator-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubTator-MCP-Server",
      "url": "https://github.com/JackKuo666/PubTator-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Access and analyze biomedical literature with advanced annotation and relationship mining capabilities. Programmatically search and export literature data through a user-friendly interface.",
      "stars": 8,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T13:18:53Z",
      "readme_content": "# PubTator MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubtator-mcp-server)](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server)\n\nüîç A biomedical literature annotation and relationship mining server based on PubTator3, providing convenient access through the MCP interface.\n\nPubTator MCP Server provides AI assistants with access to the PubTator3 biomedical literature annotation system through the Model Context Protocol (MCP). It allows AI models to programmatically search scientific literature, obtain annotation information, and analyze entity relationships.\n\nü§ù Contribute ‚Ä¢ üìù Report Issues\n\n## ‚ú® Core Features\n- üîé Literature Annotation Export: Support exporting PubTator annotation results in multiple formats ‚úÖ\n- üöÄ Entity ID Lookup: Query standard identifiers for biological concepts through free text ‚úÖ\n- üìä Relationship Mining: Discover biomedical relationships between entities ‚úÖ\n- üìÑ Literature Search: Support literature retrieval by keywords and entity IDs ‚úÖ\n- üß† Batch Processing: Support batch export of annotation information from search results ‚úÖ\n\n## üöÄ Quick Start\n\n### Requirements\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n#### Via Smithery\n\nUse [Smithery](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server) to automatically install PubTator Server:\n\n##### Claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client claude --config \"{}\"\n```\n\n##### Cursor\n\nPaste in Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server:\n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubtator-mcp-server --client cursor --config \"{}\" \n```\n\n##### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client windsurf --config \"{}\"\n```\n\n##### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client cline --config \"{}\"\n```\n\n#### Manual Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubTator-MCP-Server.git\n   cd PubTator-MCP-Server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\n### Running the Server Directly\n\nStart the MCP server:\n\n```bash\npython pubtator_server.py\n```\n\nThe server now supports both stdio and TCP transports. By default, it uses TCP transport. You can configure the following environment variables:\n\n- `MCP_TRANSPORT`: Set to \"tcp\" for TCP transport or \"stdio\" for stdio transport (default is \"tcp\")\n- `MCP_HOST`: The host to bind to (default is \"0.0.0.0\")\n- `MCP_PORT`: The port to listen on (default is 8080)\n\nExample of starting the server with custom settings:\n\n```bash\nMCP_TRANSPORT=tcp MCP_HOST=127.0.0.1 MCP_PORT=8888 python pubtator_server.py\n```\n\nThe server implements lazy initialization and proper error handling. It will gracefully handle shutdown signals (SIGINT and SIGTERM) and log any errors that occur during startup or operation.\n\n### Using Docker\n\nWe provide a Dockerfile for easy deployment. To use the Docker container:\n\n1. Build the Docker image:\n   ```bash\n   docker build -t pubtator-mcp-server .\n   ```\n\n2. Run the Docker container:\n   ```bash\n   docker run -p 8080:8080 pubtator-mcp-server\n   ```\n\nThis will start the PubTator MCP server inside a Docker container, exposing it on port 8080.\n\n### Troubleshooting\n\nIf you encounter any issues starting the server:\n\n1. Check the console output for error messages.\n2. Ensure all required dependencies are installed (see Requirements section).\n3. Verify that the environment variables are set correctly.\n4. If the server fails to start, try running it with increased verbosity:\n\n```bash\npython -v pubtator_server.py\n```\n\nThis will provide more detailed logging information to help identify the source of any issues.\n\nWhen using Docker, you can check the logs with:\n\n```bash\ndocker logs <container_id>\n```\n\n### Configuration\n\n#### Claude Desktop Configuration\n\nAdd to `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubtator-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubTator-MCP-Server\\\\pubtator_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n#### CLine Configuration\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"stdio\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nTo use TCP transport, modify the configuration as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"tcp\",\n        \"MCP_HOST\": \"127.0.0.1\",\n        \"MCP_PORT\": \"8888\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† API Features\n\nPubTator MCP Server provides the following core features:\n\n### 1. Export Publications (export_publications)\n\nExport PubTator annotation results for specified PMID literature:\n```python\nresult = await export_publications(\n    ids=[\"32133824\", \"34170578\"],\n    id_type=\"pmid\",\n    format=\"biocjson\",  # Supported: pubtator, biocxml, biocjson\n    full_text=False     # Whether to include full text\n)\n```\n\n### 2. Entity ID Lookup (find_entity_id)\n\nQuery standard identifiers for biological concepts through free text:\n```python\nresult = await find_entity_id(\n    query=\"COVID-19\",\n    concept=\"disease\",  # Optional: gene, disease, chemical, species, mutation\n    limit=5             # Optional: limit number of results\n)\n```\n\n### 3. Relationship Query (find_related_entities)\n\nFind other entities related to a specified entity:\n```python\nresult = await find_related_entities(\n    entity_id=\"@DISEASE_COVID_19\",\n    relation_type=\"treat\",    # Optional: treat, cause, interact, etc.\n    target_entity_type=\"chemical\",  # Optional: gene, disease, chemical\n    max_results=5       # Optional: limit number of results\n)\n```\n\n### 4. Literature Search (search_pubtator)\n\nSearch the PubTator database:\n```python\nresults = await search_pubtator(\n    query=\"COVID-19\",\n    max_pages=1     # Optional: maximum number of pages to retrieve\n)\n```\n\n### 5. Batch Export (batch_export_from_search)\n\nSearch and batch export literature annotations:\n```python\nresults = await batch_export_from_search(\n    query=\"COVID-19\",\n    format=\"biocjson\",\n    max_pages=1,\n    full_text=False,\n    batch_size=5\n)\n```\n\nNote: The actual function calls may vary depending on your implementation. These examples are based on our recent tests and may need to be adjusted to match your exact API.\n\n## ‚ö†Ô∏è Usage Limitations\n\n- API request rate limit: maximum 3 requests per second\n- When batch exporting, use a reasonable batch_size to avoid request timeout\n- For relationship queries, entity IDs must start with \"@\", e.g., \"@DISEASE_COVID-19\"\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please comply with PubTator's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomedical",
        "annotation",
        "literature",
        "biomedical literature",
        "analyze biomedical",
        "literature data"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Sci-Hub-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Sci-Hub-MCP-Server",
      "url": "https://github.com/JackKuo666/Sci-Hub-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects to Sci-Hub for searching, accessing, and analyzing academic papers. Supports retrieving metadata and downloading full-text PDFs programmatically.",
      "stars": 35,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T00:11:18Z",
      "readme_content": "# Sci-Hub MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/sci-hub-mcp-server)](https://smithery.ai/server/@JackKuo666/sci-hub-mcp-server)\n\nüîç Enable AI assistants to search, access, and analyze academic papers through Sci-Hub using a simple MCP interface.\n\nThe Sci-Hub MCP Server provides a bridge between AI assistants and Sci-Hub's repository of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and download PDFs in a programmatic way.\n\n## ‚ú® Core Features\n\n- üîé Paper Search by DOI: Find papers using their Digital Object Identifier ‚úÖ\n- üîç Paper Search by Title: Locate papers using their full or partial title ‚úÖ\n- üîë Paper Search by Keyword: Discover papers related to specific research areas ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìÑ PDF Download: Download full-text PDF content when available ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Sci-Hub-MCP-Server.git\n   cd Sci-Hub-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython sci_hub_server.py\n```\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"sci_hub_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Sci-Hub-MCP-Server\\\\sci_hub_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe Sci-Hub MCP Server provides the following tools:\n\n1. `search_scihub_by_doi`: Search for a paper on Sci-Hub using its DOI (Digital Object Identifier).\n2. `search_scihub_by_title`: Search for a paper on Sci-Hub using its title.\n3. `search_scihub_by_keyword`: Search for papers on Sci-Hub using a keyword.\n4. `download_scihub_pdf`: Download a paper PDF from Sci-Hub.\n5. `get_paper_metadata`: Get metadata information for a paper using its DOI.\n\n### Searching Papers by DOI\n\nYou can ask the AI assistant to search for papers using DOI:\n```\nCan you search Sci-Hub for the paper with DOI 10.1038/nature09492?\n```\n\n### Searching Papers by Title\n\nYou can search for papers using their title:\n```\nCan you find the paper titled \"Choosing Assessment Instruments for Posttraumatic Stress Disorder Screening and Outcome Research\" on Sci-Hub?\n```\n\n### Searching Papers by Keyword\n\nYou can search for papers related to specific keywords:\n```\nCan you search Sci-Hub for recent papers about artificial intelligence in medicine?\n```\n\n### Downloading Papers\n\nOnce you have found a paper, you can download it:\n```\nCan you download the PDF for this paper to my_paper.pdf?\n```\n\n### Getting Paper Metadata\n\nYou can request metadata for a paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature09492?\n```\n\n## üìÅ Project Structure\n\n- `sci_hub_server.py`: The main MCP server implementation using FastMCP\n- `sci_hub_search.py`: Contains the logic for searching Sci-Hub and retrieving paper information\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- requests\n- bs4\n- scihub\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect copyright laws and use this tool responsibly. The authors do not endorse or encourage any copyright infringement.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "sci",
        "metadata",
        "research tools",
        "sci hub",
        "metadata downloading"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--bioRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "bioRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/bioRxiv-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Search for bioRxiv papers, retrieve detailed metadata, and download papers to support biological sciences research and analysis.",
      "stars": 20,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-04T18:06:12Z",
      "readme_content": "# bioRxiv MCP Server\n\nüîç Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n\nThe bioRxiv MCP Server provides a bridge between AI assistants and bioRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for biology preprints and access their metadata in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Paper Search: Query bioRxiv papers with keywords or advanced search ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìä Research Support: Facilitate biological sciences research and analysis ‚úÖ\n- üìÑ Paper Access: Download and read paper content üìù\n- üìã Paper Listing: View all downloaded papers üìù\n- üóÉÔ∏è Local Storage: Papers are saved locally for faster access üìù\n- üìù Research Prompts: A set of specialized prompts for paper analysis üìù\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/bioRxiv-MCP-Server.git\n   cd bioRxiv-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n### Installing via Smithery\n\nTo install bioRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/biorxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/biorxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client windsurf --config \"{}\"\n```\n#### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client cline --config \"{}\"\n```\n\n#### Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"biorxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"biorxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-bioRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-bioRxiv/biorxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython biorxiv_server.py\n```\n\n## üõ† MCP Tools\n\nThe bioRxiv MCP Server provides the following tools:\n\n1. `search_biorxiv_key_words`: Search for articles on bioRxiv using keywords.\n2. `search_biorxiv_advanced`: Perform an advanced search for articles on bioRxiv with multiple parameters.\n3. `get_biorxiv_metadata`: Fetch metadata for a bioRxiv article using its DOI.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search bioRxiv for recent papers about genomics?\n```\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the metadata for the paper with DOI 10.1101/123456?\n```\n\n## üìÅ Project Structure\n\n- `biorxiv_server.py`: The main MCP server implementation using FastMCP\n- `biorxiv_web_search.py`: Contains the web scraping logic for searching bioRxiv\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect bioRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biorxiv",
        "biological",
        "scientific",
        "biorxiv papers",
        "search biorxiv",
        "jackkuo666 biorxiv"
      ],
      "category": "scientific-research-tools"
    },
    "KangDeokWon-droid--Deep-Learning-": {
      "owner": "KangDeokWon-droid",
      "name": "Deep-Learning-",
      "url": "https://github.com/KangDeokWon-droid/Deep-Learning-",
      "imageUrl": "/freedevtools/mcp/pfp/KangDeokWon-droid.webp",
      "description": "Provides tools and resources for training and experimenting with artificial intelligence models. Facilitates model development and evaluation with integrated datasets and utilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2024-09-21T08:46:24Z",
      "readme_content": "# Deep-Learning\nÏù∏Í≥µÏßÄÎä• ÌïôÏäµ\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "droid",
        "datasets",
        "deep",
        "droid deep",
        "kangdeokwon droid",
        "deep learning"
      ],
      "category": "scientific-research-tools"
    },
    "MushroomFleet--DeepLucid3D-MCP": {
      "owner": "MushroomFleet",
      "name": "DeepLucid3D-MCP",
      "url": "https://github.com/MushroomFleet/DeepLucid3D-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MushroomFleet.webp",
      "description": "Provides advanced cognitive analysis and problem-solving capabilities using structured cognitive tools, enabling users to assess cognitive states, generate novel perspectives, and decompose complex problems for effective solutions.",
      "stars": 12,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-28T12:04:44Z",
      "readme_content": "# DeepLucid3D UCPF Server\n\nA Model Context Protocol (MCP) server implementing the Unified Cognitive Processing Framework (UCPF) for advanced cognitive analysis, creative problem-solving, and structured thinking.\n\n## What is it?\n\nThe DeepLucid3D UCPF Server is an implementation of the Unified Cognitive Processing Framework as an MCP server. It combines recursive self-awareness with dimensional knowledge categorization to provide a powerful approach to problem-solving and creative thinking.\n\nThis server extends AI capabilities by providing structured cognitive tools that help:\n- Assess cognitive states\n- Map knowledge dimensions\n- Apply recursive self-questioning\n- Generate creative perspectives\n- Decompose and reintegrate complex problems\n\n## What it does\n\nThe UCPF Server enables advanced cognitive processing through several key features:\n\n### Core Capabilities\n\n1. **Cognitive State Assessment**: Identifies current cognitive states (Dark Inertia, Passion, or Approaching Lucidity) to improve self-awareness during problem-solving.\n\n2. **Knowledge Dimension Mapping**: Maps knowledge across three dimensions:\n   - Awareness (Known vs. Unknown)\n   - Content (Knowns vs. Unknowns)\n   - Accessibility (Knowable vs. Unknowable)\n\n3. **Recursive Self-Questioning**: Challenges initial assumptions and identifies potential cognitive biases.\n\n4. **Creative Perspective Generation**: Produces novel viewpoints and metaphorical thinking to inspire new solutions.\n\n5. **Problem Decomposition**: Breaks complex problems into manageable components and reintegrates them with awareness of the whole system.\n\n6. **Optional State Management**: Maintains context between sessions for ongoing analysis.\n\n## Setup and Installation\n\n### Prerequisites\n\n- Node.js (v14 or higher)\n- npm (v6 or higher)\n- An environment compatible with the Model Context Protocol\n\n### Installation Steps\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/yourusername/DeepLucid3D-UCPF-Server.git\n   cd DeepLucid3D-UCPF-Server\n   ```\n\n2. **Install dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Build the project**\n   ```bash\n   npm run build\n   ```\n\n4. **Configure MCP settings**\n\n   Add the server to your MCP settings file. For Claude/Cline, this is typically located at:\n   - For Claude Desktop: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n   - For VSCode Cline: `~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json` (Linux)\n\n   Add the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n      \"ucpf\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/DeepLucid3D-MCP/build/index.js\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n5. **Restart your MCP-compatible application** (Claude, VSCode with Cline, etc.)\n\n## How to Use\n\nThe server exposes three main tools and several resources that can be accessed through MCP:\n\n### Tools\n\n1. **analyze_problem**: Process a problem statement through the full UCPF framework\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>analyze_problem</tool_name>\n   <arguments>\n   {\n     \"problem\": \"Your problem statement here\",\n     \"session_id\": \"optional-session-id\",\n     \"enable_state\": false,\n     \"detailed\": false\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n2. **creative_exploration**: Generate novel perspectives and connections for a topic\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>creative_exploration</tool_name>\n   <arguments>\n   {\n     \"topic\": \"Your topic here\",\n     \"constraints\": [\"Optional constraint 1\", \"Optional constraint 2\"],\n     \"perspective_count\": 3,\n     \"include_metaphors\": true\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n3. **manage_state**: Control state management for UCPF processing\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>manage_state</tool_name>\n   <arguments>\n   {\n     \"action\": \"enable\", // or \"disable\", \"reset\", \"status\"\n     \"session_id\": \"optional-specific-session-id\"\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n### Resources\n\nAccess framework documentation:\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://framework/overview</uri>\n</access_mcp_resource>\n```\n\nAvailable resources:\n- `ucpf://framework/overview`: Overview of the UCPF framework\n- `ucpf://framework/cognitive-states`: Guide to cognitive states\n- `ucpf://framework/knowledge-dimensions`: Reference for knowledge dimensions\n- `ucpf://session/{sessionId}/analysis`: Analysis results for a specific session\n\n## Example Use Case: Using UCPF to Solve a Riddle\n\nLet's solve the classic riddle: \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\"\n\n### Step 1: Enable state management for session persistence\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>manage_state</tool_name>\n<arguments>\n{\n  \"action\": \"enable\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Step 2: Analyze the riddle using the UCPF framework\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>analyze_problem</tool_name>\n<arguments>\n{\n  \"problem\": \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\",\n  \"session_id\": \"sphinx-riddle\",\n  \"enable_state\": true\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe analysis will provide:\n- Cognitive state assessment (likely identifying potential metaphorical thinking)\n- Knowledge mapping of what we know and don't know\n- Recursive questions to challenge initial assumptions (e.g., \"Are we assuming literal legs?\")\n- Structured perspectives on different interpretations\n\n### Step 3: Explore creative perspectives to find the solution\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>creative_exploration</tool_name>\n<arguments>\n{\n  \"topic\": \"Walking with different numbers of legs at different times of day\",\n  \"constraints\": [\"morning\", \"afternoon\", \"evening\", \"four\", \"two\", \"three\"],\n  \"include_metaphors\": true,\n  \"session_id\": \"sphinx-riddle\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThis exploration might reveal:\n- The metaphorical interpretation of \"legs\" as support structures\n- The metaphorical interpretation of times of day as stages of life\n- Leading to the classic answer: a human, who crawls on four limbs as a baby, walks on two legs as an adult, and uses a cane (third \"leg\") in old age\n\n### Step 4: Review the session analysis\n\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://session/sphinx-riddle/analysis</uri>\n</access_mcp_resource>\n```\n\nThis provides the complete analysis journey, showing how the framework led to the solution through structured cognitive processing.\n\n## Acknowledgments\n\nThis project stands on the shoulders of giants:\n\n- The **Model Context Protocol (MCP)** team for creating the foundational protocol that enables AI systems to access external tools and resources\n- The **Anthropic Claude** team for their work on advanced AI systems capable of utilizing MCP\n- Contributors to the **Unified Cognitive Processing Framework** concepts that power the cognitive analysis methodology\n- The open-source community whose libraries and tools make projects like this possible\n\n## License\n\nMIT License\n\n## Project Structure\n\n```\nDeepLucid3D-UCPF-Server/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ engine/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ucpf-core.ts       # Core UCPF processing logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ creative-patterns.ts  # Creative thinking utilities\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state-manager.ts   # Session state management\n‚îÇ   ‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze-problem.ts # Problem analysis tool\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ creative-exploration.ts  # Creative exploration tool\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts               # Main server implementation\n‚îú‚îÄ‚îÄ build/                     # Compiled JavaScript files\n‚îú‚îÄ‚îÄ package.json               # Project dependencies and scripts\n‚îî‚îÄ‚îÄ README.md                  # This documentation\n```\n\n---\n\n¬© 2025 DeepLucid3D UCPF Server\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cognitive",
        "deeplucid3d",
        "tools",
        "cognitive tools",
        "structured cognitive",
        "mushroomfleet deeplucid3d"
      ],
      "category": "scientific-research-tools"
    },
    "PRIDE-Archive--mcp_pride_archive_search": {
      "owner": "PRIDE-Archive",
      "name": "mcp_pride_archive_search",
      "url": "https://github.com/PRIDE-Archive/mcp_pride_archive_search",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Search and explore proteomics datasets from the PRIDE Archive using structured queries. It facilitates the integration of AI models with biomedical research data for efficient data retrieval and analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteomics",
        "mcp_pride_archive_search",
        "datasets",
        "proteomics datasets",
        "explore proteomics",
        "mcp_pride_archive_search search"
      ],
      "category": "scientific-research-tools"
    },
    "ProgramComputer--NASA-MCP-server": {
      "owner": "ProgramComputer",
      "name": "NASA-MCP-server",
      "url": "https://github.com/ProgramComputer/NASA-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/ProgramComputer.webp",
      "description": "Connects AI models to over 20 NASA APIs, providing a standardized interface for data retrieval and interaction. Supports automatic parameter validation and error handling for optimized data consumption.",
      "stars": 72,
      "forks": 15,
      "license": "ISC License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T04:02:10Z",
      "readme_content": "[![NPM Version](https://img.shields.io/npm/v/%40programcomputer%2Fnasa-mcp-server?link=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40programcomputer%2Fnasa-mcp-server)](https://www.npmjs.com/package/@programcomputer/nasa-mcp-server)\n\n# NASA MCP Server\n\nA Model Context Protocol (MCP) server for NASA APIs, providing a standardized interface for AI models to interact with NASA's vast array of data sources. This server implements the official Model Context Protocol specification.\n\nBig thanks to the MCP community for their support and guidance!\n\n## Features\n\n* Access to 20+ NASA data sources through a single, consistent interface\n* Standardized data formats optimized for AI consumption\n* Automatic parameter validation and error handling\n* Rate limit management for NASA API keys\n* Comprehensive documentation and examples\n* Support for various NASA imagery formats\n* Data conversion and formatting for LLM compatibility\n* Cross-platform support (Windows, macOS, Linux)\n\n## Disclaimer\n\n**This project is not affiliated with, endorsed by, or related to NASA (National Aeronautics and Space Administration) or any of its subsidiaries or its affiliates.** It is an independent implementation that accesses NASA's publicly available APIs. All NASA data used is publicly available and subject to NASA's data usage policies.\n\n## Installation\n\n### Running with npx\n\n```bash\nenv NASA_API_KEY=YOUR_API_KEY npx -y @programcomputer/nasa-mcp-server@latest\n```\n\nYou can also pass the API key as a command line argument:\n\n```bash\nnpx -y @programcomputer/nasa-mcp-server@latest --nasa-api-key=YOUR_API_KEY\n```\n\n### Using SuperGateway for Server-Sent Events (SSE)\n\nYou can use [SuperGateway](https://github.com/supercorp-ai/supergateway) for Server-Sent Events (SSE).\n\n**The developers of NASA-MCP-server DO NOT ENDORSE the SuperGateway repository. This information is provided for those who wish to implement SSE functionality at their own discretion.**\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\ncd NASA-MCP-server\nnpm install\n\n# Run with your API key\nNASA_API_KEY=YOUR_API_KEY npm start\n```\n\n### Running on Cursor\n\nConfiguring Cursor üñ•Ô∏è Note: Requires Cursor version 0.45.6+\n\nTo configure NASA MCP Server in Cursor:\n\nCreate or edit an `mcp.json` file in your Cursor configuration directory with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"nasa-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@programcomputer/nasa-mcp-server@latest\"],\n      \"env\": {\n        \"NASA_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key` with your NASA API key from https://api.nasa.gov/.\n\nAfter adding the configuration, restart Cursor to see the new NASA tools. The Composer Agent will automatically use NASA MCP when appropriate for space-related queries.\n\n## Environment Variables\n\nThe server can be configured with the following environment variables:\n\n| Variable | Description |\n|----------|-------------|\n| `NASA_API_KEY` | Your NASA API key (get at api.nasa.gov) |\n\n## Included NASA APIs\n\nThis MCP server integrates the following NASA APIs:\n\n1. **NASA Open API** (api.nasa.gov):\n   - APOD (Astronomy Picture of the Day)\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - DONKI (Space Weather Database Of Notifications, Knowledge, Information)\n   - Insight (Mars Weather Service)\n   - Mars Rover Photos\n   - NEO (Near Earth Object Web Service)\n   - EONET (Earth Observatory Natural Event Tracker)\n   - TLE (Two-Line Element)\n   - NASA Image and Video Library\n   - Exoplanet Archive\n   - NASA Sounds API (Beta)\n   - POWER (Prediction Of Worldwide Energy Resources)\n\n2. **JPL Solar System Dynamics API** (ssd-api.jpl.nasa.gov):\n   - SBDB (Small-Body DataBase)\n   - SBDB Close-Approach Data\n   - Fireball Data\n   - Scout API\n\n3. **Earth Data APIs**:\n   - GIBS (Global Imagery Browse Services)\n   - CMR (Common Metadata Repository) - Enhanced with advanced search capabilities\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - FIRMS (Fire Information for Resource Management System)\n\n## API Methods\n\nEach NASA API is exposed through standardized MCP methods:\n\n### APOD (Astronomy Picture of the Day)\n\n```json\n{\n  \"method\": \"nasa/apod\",\n  \"params\": {\n    \"date\": \"2023-01-01\", // Optional: YYYY-MM-DD format\n    \"count\": 5, // Optional: Return a specified number of random images\n    \"thumbs\": true // Optional: Return URL of video thumbnail\n  }\n}\n```\n\n### Mars Rover Photos\n\n```json\n{\n  \"method\": \"nasa/mars-rover\",\n  \"params\": {\n    \"rover\": \"curiosity\", // Required: \"curiosity\", \"opportunity\", or \"spirit\"\n    \"sol\": 1000, // Either sol or earth_date is required\n    \"earth_date\": \"2023-01-01\", // YYYY-MM-DD format\n    \"camera\": \"FHAZ\" // Optional: Filter by camera type\n  }\n}\n```\n\n### Near Earth Objects\n\n```json\n{\n  \"method\": \"nasa/neo\",\n  \"params\": {\n    \"start_date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"end_date\": \"2023-01-07\" // Required: YYYY-MM-DD format (max 7 days from start)\n  }\n}\n```\n\n### GIBS (Global Imagery Browse Services)\n\n```json\n{\n  \"method\": \"nasa/gibs\",\n  \"params\": {\n    \"layer\": \"MODIS_Terra_CorrectedReflectance_TrueColor\", // Required: Layer ID\n    \"date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"format\": \"png\" // Optional: \"png\" or \"jpg\"\n  }\n}\n```\n\n### POWER (Prediction Of Worldwide Energy Resources)\n\n```json\n{\n  \"method\": \"nasa/power\",\n  \"params\": {\n    \"parameters\": \"T2M,PRECTOTCORR,WS10M\", // Required: Comma-separated list\n    \"community\": \"re\", // Required: Community identifier\n    \"latitude\": 40.7128, // Required: Latitude\n    \"longitude\": -74.0060, // Required: Longitude\n    \"start\": \"20220101\", // Required: Start date (YYYYMMDD)\n    \"end\": \"20220107\" // Required: End date (YYYYMMDD)\n  }\n}\n```\n\nFor complete documentation of all available methods and parameters, see the API reference in the `/docs` directory.\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n* Operation status and progress\n* Performance metrics\n* Rate limit tracking\n* Error conditions\n* Request validation\n\nExample log messages:\n\n```\n[INFO] NASA MCP Server initialized successfully\n[INFO] Processing APOD request for date: 2023-01-01\n[INFO] Fetching Mars Rover data for Curiosity, sol 1000\n[WARNING] Rate limit threshold reached (80%)\n[ERROR] Invalid parameter: 'date' must be in YYYY-MM-DD format\n```\n\n## Security Considerations\n\nThis MCP server implements security best practices following the Model Context Protocol specifications:\n\n* Input validation and sanitization using Zod schemas\n* No execution of arbitrary code\n* Protection against command injection\n* Proper error handling to prevent information leakage\n* Rate limiting and timeout controls for API requests\n* No persistent state that could be exploited across sessions\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\nnpm install\n\n# Copy the example environment file and update with your API keys\ncp .env.example .env\n\n# Build the TypeScript code\nnpm run build\n\n# Start the development server\nnpm run dev\n\n# Run tests\nnpm test\n```\n\n## Testing with MCP Inspector\n\nThe NASA MCP Server includes a script to help you test the APIs using the MCP Inspector:\n\n```bash\n# Run the provided test script\n./scripts/test-with-inspector.sh\n```\n\nThis will:\n1. Build the project to ensure the latest changes are included\n2. Start the MCP Inspector with the NASA MCP server running\n3. Allow you to interactively test all the NASA APIs\n\n### Example Test Requests\n\nThe repository includes example test requests for each API that you can copy and paste into the MCP Inspector:\n\n```bash\n# View the example test requests\ncat docs/inspector-test-examples.md\n```\n\nFor detailed examples, see the [Inspector Test Examples](docs/inspector-test-examples.md) document.\n\n## MCP Client Usage\n\nThis server follows the official Model Context Protocol. Here's an example of how to use it with the MCP SDK:\n\n```typescript\nimport { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\nimport { HttpClientTransport } from \"@modelcontextprotocol/sdk/client/http.js\";\n\nconst transport = new HttpClientTransport({\n  url: \"http://localhost:3000\",\n});\n\nconst client = new Client({\n  name: \"mcp-client\",\n  version: \"1.0.0\",\n});\n\nawait client.connect(transport);\n\n// Example: Get today's Astronomy Picture of the Day\nconst apodResult = await client.request({\n  method: \"nasa/apod\", \n  params: {}\n});\n\n// Example: Get Mars Rover photos\nconst marsRoverResult = await client.request({\n  method: \"nasa/mars-rover\",\n  params: { rover: \"curiosity\", sol: 1000 }\n});\n\n// Example: Search for Near Earth Objects\nconst neoResults = await client.request({\n  method: \"nasa/neo\",\n  params: {\n    start_date: '2023-01-01',\n    end_date: '2023-01-07'\n  }\n});\n\n// Example: Get satellite imagery from GIBS\nconst satelliteImage = await client.request({\n  method: \"nasa/gibs\",\n  params: {\n    layer: 'MODIS_Terra_CorrectedReflectance_TrueColor',\n    date: '2023-01-01'\n  }\n});\n\n// Example: Use the new POWER API\nconst powerData = await client.request({\n  method: \"nasa/power\",\n  params: {\n    parameters: \"T2M,PRECTOTCORR,WS10M\",\n    community: \"re\",\n    latitude: 40.7128,\n    longitude: -74.0060,\n    start: \"20220101\",\n    end: \"20220107\"\n  }\n});\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nISC License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nasa",
        "apis",
        "ai",
        "nasa apis",
        "programcomputer nasa",
        "nasa mcp"
      ],
      "category": "scientific-research-tools"
    },
    "SeanMcLoughlin--mcp-vcd": {
      "owner": "SeanMcLoughlin",
      "name": "mcp-vcd",
      "url": "https://github.com/SeanMcLoughlin/mcp-vcd",
      "imageUrl": "/freedevtools/mcp/pfp/SeanMcLoughlin.webp",
      "description": "Handles value change dump (VCD) waveforms for AI models, providing specific signal changes within defined timestamp ranges to optimize context management for large waveform files.",
      "stars": 5,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-13T03:09:17Z",
      "readme_content": "# mcp-vcd\r\n\r\nA model context protocol (MCP) server for value change dump (VCD) waveforms.\r\n\r\nhttps://github.com/user-attachments/assets/9d1a6a64-de22-4b5a-a597-685c663c9c79\r\n\r\n<a href=\"https://glama.ai/mcp/servers/kdvs90ijbl\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kdvs90ijbl/badge\" alt=\"mcp-vcd MCP server\" /></a>\r\n\r\n# Tools\r\n\r\n- `get-signal`: Provide all changes of the specified signal name to the model's context. This is useful for large waveform files with many signals where you cannot fit the entire VCD file into the model's context window.\r\n  - Can optionally specify a start and end timestamp range for the model to look at.\r\n\r\n# Installation\r\n\r\n`uv pip install --system mcp-vcd`\r\n\r\nAnd add the following to your `claude_desktop_config.json`:\r\n\r\n```json\r\n\"mcpServers\": {\r\n  \"mcp-vcd\": {\r\n    \"command\": \"uv\",\r\n    \"args\": [\r\n      \"run\",\r\n      \"mcp-vcd\"\r\n    ]\r\n  }\r\n}\r\n```\r\nSee [Anthropic's MCP documentation](https://modelcontextprotocol.io/quickstart/user) for more info.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vcd",
        "mcp",
        "waveforms",
        "mcp vcd",
        "vcd waveforms",
        "dump vcd"
      ],
      "category": "scientific-research-tools"
    },
    "StarGazer1995--mcp-stargazing": {
      "owner": "StarGazer1995",
      "name": "mcp-stargazing",
      "url": "https://github.com/StarGazer1995/mcp-stargazing",
      "imageUrl": "/freedevtools/mcp/pfp/StarGazer1995.webp",
      "description": "Calculate altitude, rise, and set times of celestial objects based on geographic location, while also analyzing light pollution to improve stargazing conditions. Supports a variety of celestial objects including the Sun, Moon, planets, and deep-space entities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-18T09:45:10Z",
      "readme_content": "# mcp-stargazing\n\nCalculate the altitude, rise, and set times of celestial objects (Sun, Moon, planets, stars, and deep-space objects) for any location on Earth, with optional light pollution analysis.\n\n## Features\n- **Altitude/Azimuth Calculation**: Get elevation and compass direction for any celestial object.\n- **Rise/Set Times**: Determine when objects appear/disappear above the horizon.\n- **Light Pollution Analysis**: Load and analyze light pollution maps (GeoTIFF format).\n- **Supports**:\n  - Solar system objects (Sun, Moon, planets)\n  - Stars (e.g., \"sirius\")\n  - Deep-space objects (e.g., \"andromeda\", \"orion_nebula\")\n- **Time Zone Aware**: Works with local or UTC times.\n\n## Installation\n```bash\npip install astropy pytz numpy astroquery rasterio geopy\n```\n\n## Usage\n\n### Calculate Altitude/Azimuth\n```python src/main.py\nfrom src.celestial import celestial_pos\nfrom astropy.coordinates import EarthLocation\nimport pytz\nfrom datetime import datetime\n\n# Observer location (New York)\nlocation = EarthLocation(lat=40.7128, lon=-74.0060)\n\n# Time (local timezone-aware)\nlocal_time = pytz.timezone(\"America/New_York\").localize(datetime(2023, 10, 1, 12, 0))\naltitude, azimuth = celestial_pos(\"sun\", location, local_time)\nprint(f\"Sun Position: Altitude={altitude:.1f}¬∞, Azimuth={azimuth:.1f}¬∞\")\n```\n\n### Calculate Rise/Set Times\n```python src/main.py\nfrom src.celestial import celestial_rise_set\n\nrise, set_ = celestial_rise_set(\"andromeda\", location, local_time.date())\nprint(f\"Andromeda: Rise={rise.iso}, Set={set_.iso}\")\n```\n\n### Load Light Pollution Map\n```python src/light_pollution.py\nfrom src.light_pollution import load_map\n\n# Load a GeoTIFF light pollution map\nvriis_data, bounds, crs, transform = load_map(\"path/to/map.tif\")\nprint(f\"Map Bounds: {bounds}\")\n```\n\n## API Reference\n\n### `celestial_pos(celestial_object, observer_location, time)` (`src/celestial.py`)\n- **Inputs**:\n  - `celestial_object`: Name (e.g., `\"sun\"`, `\"andromeda\"`).\n  - `observer_location`: `EarthLocation` object.\n  - `time`: `datetime` (timezone-aware) or Astropy `Time`.\n- **Returns**: `(altitude_degrees, azimuth_degrees)`.\n\n### `celestial_rise_set(celestial_object, observer_location, date, horizon=0.0)` (`src/celestial.py`)\n- **Inputs**: \n  - `date`: Timezone-aware `datetime`.\n  - `horizon`: Horizon elevation (default: 0¬∞).\n- **Returns**: `(rise_time, set_time)` as UTC `Time` objects.\n\n### `load_map(map_path)` (`src/light_pollution.py`)\n- **Inputs**:\n  - `map_path`: Path to GeoTIFF file.\n- **Returns**: Tuple `(vriis_data, bounds, crs, transform)` for light pollution analysis.\n\n## Testing\nRun tests with:\n```bash\npytest tests/\n```\n\n### Key Test Cases (`tests/test_celestial.py`)\n```python tests/test_celestial.py\ndef test_calculate_altitude_deepspace():\n    \"\"\"Test deep-space object resolution.\"\"\"\n    altitude, _ = celestial_pos(\"andromeda\", NYC, Time.now())\n    assert -90 <= altitude <= 90\n\ndef test_calculate_rise_set_sun():\n    \"\"\"Validate Sun rise/set times.\"\"\"\n    rise, set_ = celestial_rise_set(\"sun\", NYC, datetime(2023, 10, 1))\n    assert rise < set_\n```\n\n## Project Structure\n```\n.\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ celestial.py          # Core celestial calculations\n‚îÇ   ‚îú‚îÄ‚îÄ light_pollution.py    # Light pollution map utilities\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py              # Time/location helpers\n‚îÇ   ‚îî‚îÄ‚îÄ main.py               # CLI entry point\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_celestial.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îî‚îÄ‚îÄ README.md\n```\n\n## Future Work\n- Add support for comets/asteroids.\n- Optimize SIMBAD queries for offline use.\n- Integrate light pollution data into visibility predictions.\n\n### Key Updates:\n1. **Light Pollution**: Added `light_pollution.py` to features and API reference.\n2. **Dependencies**: Added `rasterio` and `geopy` to installation instructions.\n3. **Project Structure**: Clarified file roles and test coverage.\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stargazing",
        "stargazer1995",
        "celestial",
        "mcp stargazing",
        "stargazer1995 mcp",
        "stargazing calculate"
      ],
      "category": "scientific-research-tools"
    },
    "TakumiY235--uniprot-mcp-server": {
      "owner": "TakumiY235",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/TakumiY235/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/TakumiY235.webp",
      "description": "Fetch protein information from UniProt, enabling access to detailed data about protein functions and sequences. Supports both individual and batch retrieval of protein information, including name, function description, full sequence, and organism details.",
      "stars": 9,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-14T07:44:52Z",
      "readme_content": "# UniProt MCP Server\n\nA Model Context Protocol (MCP) server that provides access to UniProt protein information. This server allows AI assistants to fetch protein function and sequence information directly from UniProt.\n\n<a href=\"https://glama.ai/mcp/servers/ttjbai3lpx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ttjbai3lpx/badge\" alt=\"UniProt Server MCP server\" />\n</a>\n\n## Features\n\n- Get protein information by UniProt accession number\n- Batch retrieval of multiple proteins\n- Caching for improved performance (24-hour TTL)\n- Error handling and logging\n- Information includes:\n  - Protein name\n  - Function description\n  - Full sequence\n  - Sequence length\n  - Organism\n\n## Quick Start\n\n1. Ensure you have Python 3.10 or higher installed\n2. Clone this repository:\n   ```bash\n   git clone https://github.com/TakumiY235/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n3. Install dependencies:\n   ```bash\n   # Using uv (recommended)\n   uv pip install -r requirements.txt\n   \n   # Or using pip\n   pip install -r requirements.txt\n   ```\n\n## Configuration\n\nAdd to your Claude Desktop config file:\n\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"uniprot\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path/to/uniprot-mcp-server\", \"run\", \"uniprot-mcp-server\"]\n    }\n  }\n}\n```\n\n## Usage Examples\n\nAfter configuring the server in Claude Desktop, you can ask questions like:\n\n```\nCan you get the protein information for UniProt accession number P98160?\n```\n\nFor batch queries:\n\n```\nCan you get and compare the protein information for both P04637 and P02747?\n```\n\n## API Reference\n\n### Tools\n\n1. `get_protein_info`\n   - Get information for a single protein\n   - Required parameter: `accession` (UniProt accession number)\n   - Example response:\n     ```json\n     {\n       \"accession\": \"P12345\",\n       \"protein_name\": \"Example protein\",\n       \"function\": [\"Description of protein function\"],\n       \"sequence\": \"MLTVX...\",\n       \"length\": 123,\n       \"organism\": \"Homo sapiens\"\n     }\n     ```\n\n2. `get_batch_protein_info`\n   - Get information for multiple proteins\n   - Required parameter: `accessions` (array of UniProt accession numbers)\n   - Returns an array of protein information objects\n\n## Development\n\n### Setting up development environment\n\n1. Clone the repository\n2. Create a virtual environment:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n3. Install development dependencies:\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n### Running tests\n\n```bash\npytest\n```\n\n### Code style\n\nThis project uses:\n- Black for code formatting\n- isort for import sorting\n- flake8 for linting\n- mypy for type checking\n- bandit for security checks\n- safety for dependency vulnerability checks\n\nRun all checks:\n```bash\nblack .\nisort .\nflake8 .\nmypy .\nbandit -r src/\nsafety check\n```\n\n## Technical Details\n\n- Built using the MCP Python SDK\n- Uses httpx for async HTTP requests\n- Implements caching with 24-hour TTL using an OrderedDict-based cache\n- Handles rate limiting and retries\n- Provides detailed error messages\n\n### Error Handling\n\nThe server handles various error scenarios:\n- Invalid accession numbers (404 responses)\n- API connection issues (network errors)\n- Rate limiting (429 responses)\n- Malformed responses (JSON parsing errors)\n- Cache management (TTL and size limits)\n\n## Contributing\n\nWe welcome contributions! Please feel free to submit a Pull Request. Here's how you can contribute:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and adhere to the existing coding style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- UniProt for providing the protein data API\n- Anthropic for the Model Context Protocol specification\n- Contributors who help improve this project",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "uniprot",
        "protein",
        "retrieval",
        "protein information",
        "retrieval protein",
        "uniprot mcp"
      ],
      "category": "scientific-research-tools"
    },
    "Tsuchijo--matlab-mcp": {
      "owner": "Tsuchijo",
      "name": "matlab-mcp",
      "url": "https://github.com/Tsuchijo/matlab-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Tsuchijo.webp",
      "description": "Integrate with MATLAB to create and execute MATLAB scripts and functions via MCP clients. Supports automation of MATLAB programming tasks within various applications.",
      "stars": 32,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T18:58:11Z",
      "readme_content": "# MATLAB MCP Server\n\nThis Model Context Protocol (MCP) server provides integration with MATLAB, allowing you to create and execute MATLAB scripts and functions through Claude or other MCP clients.\n\n## Setup Requirements\n\n- Python 3.11 (Python 3.13 and 3.12 are not currently supported by MATLAB Engine)\n- MATLAB R2024a (or compatible version)\n- uv package manager\n\n## Installation\n\n1. Create and set up the Python environment:\n```bash\n# Pin Python version\nuv python pin 3.11\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Install MCP\nuv add \"mcp[cli]\"\n```\n\n2. Install MATLAB Engine\nThe MATLAB Engine will be installed automatically when the server first runs, using the MATLAB installation specified in the `MATLAB_PATH` environment variable.\n\n## Directory Structure\n\n- `matlab_server.py`: The main MCP server implementation\n- `matlab_scripts/`: Directory where all MATLAB scripts and functions are saved (created automatically)\n- `pyproject.toml`: Python project configuration\n- `.python-version`: Specifies Python version for uv\n\n## Claude Desktop Integration\n\n1. Open your Claude Desktop configuration:\n```bash\n# On macOS\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n2. Add the MATLAB server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"matlab\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/absolute/path/to/matlab-mcp\",\n                \"run\",\n                \"matlab_server.py\"\n            ],\n            \"env\": {\n                \"MATLAB_PATH\": \"/Applications/MATLAB_R2024a.app\"\n            }\n        }\n    }\n}\n```\n\nMake sure to:\n- Replace `/absolute/path/to/matlab-mcp` with the actual path to your project directory\n- Verify the `MATLAB_PATH` points to your MATLAB installation\n- Use absolute paths (not relative)\n\n## Features\n\nThe server provides several tools:\n\n1. `create_matlab_script`: Create a new MATLAB script file\n   - Scripts are saved in the `matlab_scripts` directory\n   - File names must be valid MATLAB identifiers\n\n2. `create_matlab_function`: Create a new MATLAB function file\n   - Functions are saved in the `matlab_scripts` directory\n   - Must include valid function definition\n\n3. `execute_matlab_script`: Run a MATLAB script and get results\n   - Returns output text, generated figures, and workspace variables\n   - Can pass arguments to scripts\n\n4. `call_matlab_function`: Call a MATLAB function with arguments\n   - Returns function output and any generated figures\n\n## Testing\n\nYou can test the server using the MCP Inspector:\n```bash\n# Make sure you're in your virtual environment\nsource .venv/bin/activate\n\n# Run the inspector\nMATLAB_PATH=/Applications/MATLAB_R2024a.app mcp dev matlab_server.py\n```\n\nExample test script:\n```matlab\nt = 0:0.01:2*pi;\ny = sin(t);\nplot(t, y);\ntitle('Test Plot');\nxlabel('Time');\nylabel('Amplitude');\n```\n\n## Script Storage\n\n- All MATLAB scripts and functions are saved in the `matlab_scripts` directory\n- This directory is created automatically when the server starts\n- Files are named `<script_name>.m` or `<function_name>.m`\n- The directory is in the same location as `matlab_server.py`\n\n## Environment Variables\n\n- `MATLAB_PATH`: Path to your MATLAB installation\n  - Default: `/Applications/MATLAB_R2024a.app`\n  - Set in Claude Desktop config or when running directly\n\n## Troubleshooting\n\n1. **MATLAB Engine Installation Fails**\n   - Verify MATLAB_PATH is correct\n   - Try installing engine manually:\n     ```bash\n     cd $MATLAB_PATH/extern/engines/python\n     python setup.py install\n     ```\n\n2. **Python Version Issues**\n   - Make sure you're using Python 3.11\n   - Check with: `python --version`\n   - Use `uv python pin 3.11` if needed\n\n3. **Script Execution Errors**\n   - Check the `matlab_scripts` directory exists\n   - Verify script syntax is valid\n   - Look for error messages in MATLAB output\n\n## Updates and Maintenance\n\n- Keep your MATLAB installation updated\n- Update Python packages as needed: `uv pip install --upgrade mcp[cli]`\n- Check MATLAB engine compatibility when updating Python",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "matlab",
        "mcp",
        "scripts",
        "matlab scripts",
        "matlab mcp",
        "automation matlab"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--quantum-simulator-mcp": {
      "owner": "YuChenSSR",
      "name": "quantum-simulator-mcp",
      "url": "https://github.com/YuChenSSR/quantum-simulator-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Simulates quantum circuits incorporating noise models and visualizes results. Supports OpenQASM 2.0 and various quantum algorithms with integration into MCP clients for streamlined use.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-06-19T23:08:17Z",
      "readme_content": "# Quantum Simulator MCP Server\n\nA Docker image providing a quantum circuit simulator that implements the Model Context Protocol (MCP), allowing integration with MCP clients such as Claude for Desktop.\n\n\n\n## Features\n\n- Quantum computing simulator with noise models\n- Support for OpenQASM 2.0 quantum circuits\n- Quantum circuit simulation using Qiskit\n- Support for various noise models (depolarizing, thermal relaxation, readout error)\n- Multiple result types including counts, statevector, and visualized histograms\n- Pre-configured example circuits\n- Seamless integration with MCP clients\n\n## Quick Start\n\nget the docker image\n\n```bash\ndocker pull ychen94/quantum-simulator-mcp:latest\n```\n\n\nSimply run the container with the following command:\n\n```bash\ndocker run -i --rm -v /tmp:/data/quantum_simulator_results -e HOST_OUTPUT_DIR=\"/tmp\" ychen94/quantum-simulator-mcp:latest\n```\n\nThis command:\n- Mounts the `/tmp` directory on your host to store histogram output files\n- Sets the `HOST_OUTPUT_DIR` environment variable to `/tmp`\n- Keeps the container running with `-i` (interactive mode)\n- Automatically removes the container when it exits with `--rm`\n\n## Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Edit the Claude configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"quantum-simulator\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\", \"/tmp:/data/quantum_simulator_results\",\n        \"-e\", \"HOST_OUTPUT_DIR=/tmp\",\n        \"ychen94/quantum-simulator-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n5. Look for the hammer icon in the Claude UI, indicating available MCP tools\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n- **run_circuit**: Run a quantum circuit with specified noise model\n- **list_noise_models**: List all available noise models and their descriptions\n- **list_result_types**: List all available result types and their descriptions\n- **get_circuit_stats**: Analyze a quantum circuit and return statistics\n- **create_test_histogram**: Create a test histogram file to verify output directory configuration\n\n## MCP Resources\n\nThe server provides example quantum circuits:\n\n- `qasm://examples/bell-state.qasm`: Bell state preparation circuit\n- `qasm://examples/grover-2qubit.qasm`: 2-qubit Grover's algorithm implementation\n- `qasm://examples/qft-4qubit.qasm`: 4-qubit Quantum Fourier Transform\n- `quantum://noise-models/examples.json`: Example noise model configurations\n\n## Example Usage in Claude\n\nHere are some prompts you can use in Claude:\n\n1. \"Run a Bell state circuit and show me the results\"\n\n2. \"What noise models are available in the quantum simulator?\"\n\n3. \"Simulate a 2-qubit Grover's algorithm with 0.01 depolarizing noise\"\n\n4. \"Create a test histogram and show me the file path\"\n\n5. \"Please provide a simple QAOA algorithm, only get the result_types: histogram, and view the histogram using iterm\"\n\n![chat](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/FHE8cIDqLRN36pOm.png)\n\n![result_pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/OKD2nqE0aHYuWBan.png)\n\n\n## Volume Mapping\n\nThe container generates histogram PNG files in `/data/quantum_simulator_results`. These files need to be accessible from your host system. The volume mapping (`-v /tmp:/data/quantum_simulator_results`) makes these files available in your host's `/tmp` directory.\n\n## Environment Variables\n\n- `QUANTUM_OUTPUT_DIR`: Output directory for histogram files inside the container (default: `/data/quantum_simulator_results`)\n- `HOST_OUTPUT_DIR`: Corresponding path on the host system (default: `/tmp`)\n\n## Multi-Architecture Support\n\nThis image supports the following architectures:\n- linux/amd64\n- linux/arm64 (confirmed working on Mac M-series chips)\n\nNote: The image has not been tested on Windows systems yet, but should work as long as Docker Desktop is properly configured.\n\n## Troubleshooting\n\n**Issue**: Claude cannot access the histogram files.  \n**Solution**: Ensure the volume mapping is correct and the `HOST_OUTPUT_DIR` environment variable matches the host path in your volume mapping.\n\n**Issue**: Docker container exits immediately.  \n**Solution**: Make sure to use the `-i` flag to keep stdin open, which is required for the MCP STDIO transport.\n\n## License\n\nThis project is licensed under the MIT License. For more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/quantum-simulator-mcp).\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "openqasm",
        "simulates",
        "quantum simulator",
        "simulates quantum",
        "quantum algorithms"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--symbolica-mcp": {
      "owner": "YuChenSSR",
      "name": "symbolica-mcp",
      "url": "https://github.com/YuChenSSR/symbolica-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Facilitates symbolic computing, mathematical analysis, and data visualization for scientific and engineering applications using libraries like NumPy, SciPy, and Matplotlib in a containerized setup. Supports complex matrix calculations, statistical analysis, and machine learning tasks.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-07-28T02:42:49Z",
      "readme_content": "# symbolica-mcp\n\nA scientific computing Model Context Protocol (MCP) server allows AI, such as Claude, to perform symbolic computing, conduct calculations, analyze data, and generate visualizations. This is particularly useful for scientific and engineering applications, including quantum computing, all within a containerized environment.\n\n## Features\n\n- Run scientific computing operations with NumPy, SciPy, SymPy, Pandas\n- Perform symbolic mathematics and solve differential equations\n- Support for linear algebra operations and matrix manipulations\n- Quantum computing analysis\n- Create data visualizations with Matplotlib and Seaborn\n- Perform machine learning operations with scikit-learn\n- Execute tensor operations and complex matrix calculations\n- Analyze data sets with statistical tools\n- Cross-platform support (automatically detects Windows, macOS, and Linux), especially for users with Mac M series chips\n- Works on both Intel/AMD (x86_64) and ARM processors\n\n## Quick Start\n\n### Using the Docker image\n\n```bash\n# Pull the image from Docker Hub\ndocker pull ychen94/computing-mcp:latest\n\n# Run the container (automatically detects host OS)\ndocker run -i --rm -v /tmp:/app/shared ychen94/computing-mcp:latest\n```\n\nFor Windows users:\n```powershell\ndocker run -i --rm -v $env:TEMP:/app/shared ychen94/computing-mcp:latest\n```\n\n### Integrating with Claude for Desktop\n\n1. Open Claude for Desktop\n2. Open Settings ‚ûù Developer ‚ûù Edit Config\n3. Add the following configuration:\n\nFor MacOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/tmp:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\nFor Windows:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"%TEMP%:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n\n\n## Examples\n\n### Tensor Products\n\n```\nCan you calculate and visualize the tensor product of two matrices? Please run:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define two matrices\nA = np.array([[1, 2], \n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\n# Calculate tensor product using np.kron (Kronecker product)\ntensor_product = np.kron(A, B)\n\n# Display the result\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nTensor Product A ‚äó B:\")\nprint(tensor_product)\n\n# Create a visualization of the tensor product\nplt.figure(figsize=(8, 6))\nplt.imshow(tensor_product, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Visualization of Tensor Product A ‚äó B')\n```\n\n### Symbolic Mathematics\n\n```\nCan you solve this differential equation? Please run:\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define symbolic variable\nx = sp.Symbol('x')\ny = sp.Function('y')(x)\n\n# Define the differential equation: y''(x) + 2*y'(x) + y(x) = 0\ndiff_eq = sp.Eq(sp.diff(y, x, 2) + 2*sp.diff(y, x) + y, 0)\n\n# Solve the equation\nsolution = sp.dsolve(diff_eq)\nprint(\"Solution:\")\nprint(solution)\n\n# Plot a particular solution (C1=1, C2=0)\nsolution_func = solution.rhs.subs({sp.symbols('C1'): 1, sp.symbols('C2'): 0})\nprint(\"Particular solution:\")\nprint(solution_func)\n\n# Create a numerical function we can evaluate\nsolution_lambda = sp.lambdify(x, solution_func)\n\n# Plot the solution\nx_vals = np.linspace(0, 5, 100)\ny_vals = [float(solution_lambda(x_val)) for x_val in x_vals]\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_vals, y_vals)\nplt.grid(True)\nplt.title(\"Solution to y''(x) + 2*y'(x) + y(x) = 0\")\nplt.xlabel('x')\nplt.ylabel('y(x)')\nplt.show()\n```\n\n### Data Analysis\n\n```\nCan you perform a clustering analysis on this dataset? Please run:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a sample dataset\nnp.random.seed(42)\nn_samples = 300\n\n# Create three clusters\ncluster1 = np.random.normal(loc=[2, 2], scale=0.5, size=(n_samples//3, 2))\ncluster2 = np.random.normal(loc=[7, 7], scale=0.5, size=(n_samples//3, 2))\ncluster3 = np.random.normal(loc=[2, 7], scale=0.5, size=(n_samples//3, 2))\n\n# Combine clusters\nX = np.vstack([cluster1, cluster2, cluster3])\n\n# Create DataFrame\ndf = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\nprint(df.head())\n\n# Standardize data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(X_scaled)\n\n# Plot the clusters\nplt.figure(figsize=(10, 6))\nfor cluster_id in range(3):\n    cluster_data = df[df['Cluster'] == cluster_id]\n    plt.scatter(cluster_data['Feature1'], cluster_data['Feature2'], \n                label=f'Cluster {cluster_id}', alpha=0.7)\n\n# Plot cluster centers\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\nplt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centers')\n\nplt.title('K-Means Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True)\n```\n\n### Quantum Computing\n\n![quantum example](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/UaybujIK2o3tLUtR.png)\n\n### Gallery\n\n**laser physics**:\n![laser](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/4t06m3iM17NXpW6O.png)\n\n**elliptic integral**:\n![elliptic integral](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/rvshlS2blGv7jnoi.png)\n![elliptic integral pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/oRGJIHgKQV8kMMHd.png)\n\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Permission errors with volume mounts**\n   - Ensure the mount directory exists and has appropriate permissions\n\n2. **Plot pciture files not appearing**\n   - Check the path in your host system: `/tmp` for macOS/Linux or your temp folder for Windows\n   - Verify Docker has permissions to write to the mount location\n   - check the mcp tool's output content\n     ![find the pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/Ik3JZeLhdptLqgq3.png)\n     then open it in the terminal or your picture viewer.\n\n      ‚≠êÔ∏è ‚≠êÔ∏è\n     I use the [iterm-mcp-server](https://github.com/ferrislucas/iterm-mcp) or other terminals' mcp servers to open the file  without interrupting your workflow. \n      ‚≠êÔ∏è ‚≠êÔ∏è\n\n\n\n### Support\n\nIf you encounter issues, please open a GitHub issue with:\n1. Error messages\n2. Your operating system and Docker version\n3. Steps to reproduce the problem\n\n## License\nThis project is licensed under the MIT License.   \nFor more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/symbolica-mcp).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scipy",
        "matplotlib",
        "symbolica",
        "symbolic computing",
        "symbolica mcp",
        "visualization scientific"
      ],
      "category": "scientific-research-tools"
    },
    "acashmoney--bio-mcp": {
      "owner": "acashmoney",
      "name": "bio-mcp",
      "url": "https://github.com/acashmoney/bio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/acashmoney.webp",
      "description": "Enhances biomedical research through advanced protein structure analysis, enabling the examination of protein active sites and the discovery of disease-related proteins. It integrates with established protein databases for reliable insights and data access.",
      "stars": 17,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-27T01:43:39Z",
      "readme_content": "# BioMCP: Enabling agent-based biomedical R&D\n\n[![smithery badge](https://smithery.ai/badge/@acashmoney/bio-mcp)](https://smithery.ai/server/@acashmoney/bio-mcp)\n\n\n## Overview\n\nBioMCP is a Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server designed to enhance large language models with protein structure analysis capabilities. It provides tools for analyzing protein active sites and searching for disease-related proteins by interfacing with established protein databases. \n\nFuture work will be centered around enabling agents to utilize the BioMCP.\n\n## Features\n\n- **Active Site Analysis**: Examine the binding sites and functional residues of proteins using PDB IDs\n- **Disease-Protein Search**: Find protein structures associated with specific diseases or medical conditions\n- **Integrated Data Access**: Connect seamlessly with RCSB Protein Data Bank [(PDB)](https://www.rcsb.org/)\n\n## Technical Details\n\nBioMCP implements the Model Context Protocol, allowing language models to access specialized protein structure knowledge without requiring this information to be part of their training data. The server handles API connections, data formatting, and error handling to provide reliable protein structure insights.\n\n## API Endpoints\n\nBioMCP exposes two primary tools:\n\n1. `analyze-active-site`: Provides detailed information about protein binding sites using a PDB ID\n2. `search-disease-proteins`: Returns proteins related to specified diseases or medical conditions\n\n## Getting Started\n\n### Installing via Smithery\n\nTo install BioMCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@acashmoney/bio-mcp):\n\n```bash\nnpx -y @smithery/cli install @acashmoney/bio-mcp --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/acashmoney/bio-mcp.git\n\n# Install dependencies\nnpm install\n\n# Start the server\nnpm start\n```\n\n## Setup Instructions\n\n### Running the MCP Inspector\n\n1. Start the BioMCP server:\n   ```bash\n   npm start\n   ```\n\n2. In a separate terminal, install the MCP Inspector globally (if not already installed):\n   ```bash\n   npm install -g @anthropic-ai/mcp-inspector\n   ```\n\n3. Launch the MCP Inspector and connect to your local BioMCP server:\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n4. Use the inspector interface to test tools and view responses.\n\n### Using with Claude Desktop\n\n1. Build the BioMCP server:\n   ```bash\n   npm run build\n   ```\n\n2. Configure Claude Desktop to launch the MCP server:\n\n   a. Locate your Claude Desktop config.json file (typically in your user directory)\n   \n   b. Edit the config.json to include the BioMCP server build path. Example configuration:\n   ```json\n   {\n     \"globalShortcut\": \"\",\n     \"mcpServers\": {\n       \"bio-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n   \n   c. Replace `/path/to/your/build` with your actual path to the project directory.\n\n3. Restart Claude Desktop for the changes to take effect.\n\n4. You can now ask Claude questions that utilize the BioMCP tools:\n   - \"What are the key residues in the active site of PDB structure 6LU7?\"\n   - \"Find proteins related to Alzheimer's disease\"\n\n## Example Usage\n\nWhen integrated with a compatible language model, Bio-MCP enables queries like:\n\n- \"What are the key residues in the active site of PDB structure 6LU7?\"\n- \"Find proteins related to Alzheimer's disease\"\n\n## Requirements\n\n- Node.js 20.0.0 or higher\n- TypeScript 5.0+\n- Compatible MCP client implementation\n\n## Testing\n\nBioMCP includes a comprehensive testing suite with unit, integration, and end-to-end tests.\n\n### Running Tests\n\nRun all tests:\n```bash\nnpm test\n```\n\nRun specific test suites:\n```bash\n# Unit tests only\nnpm run test:unit\n\n# Integration tests only (API interactions)\nnpm run test:integration\n\n# End-to-end tests only\nnpm run test:e2e\n```\n\n### Linting\n\nCheck code quality:\n```bash\nnpm run lint\n```\n\nFix linting issues automatically:\n```bash\nnpm run lint:fix\n```\n\n## Roadmap\n\n- Expand level of detail for active site descriptions\n- Leverage 3-D coordinates\n- Tools for interfacing with literature\n- Tools for interfacing with computational biology models:\n  - RFdiffusion\n  - ProteinMPNN\n  - ColabFold\n  - Additional protein design and structure prediction tools\n- Agent-based research pipelines\n- Introduce client with protein visualization tools",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteins",
        "protein",
        "mcp",
        "protein databases",
        "advanced protein",
        "bio mcp"
      ],
      "category": "scientific-research-tools"
    },
    "akalaric--mcp-wolframalpha": {
      "owner": "akalaric",
      "name": "mcp-wolframalpha",
      "url": "https://github.com/akalaric/mcp-wolframalpha",
      "imageUrl": "/freedevtools/mcp/pfp/akalaric.webp",
      "description": "Integrates Wolfram Alpha into chat applications to perform computational queries and retrieve structured knowledge, enhancing conversational capabilities with large language models. Supports a modular architecture for easy extension and multiple client interactions.",
      "stars": 44,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T21:33:50Z",
      "readme_content": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha‚Äôs knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n  Install the required dependencies with uv:\n  Ensure [`uv`](https://github.com/astral-sh/uv) is installed.\n\n   ```bash\n   uv sync\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n## Contact\n\nFeel free to give feedback. The e-mail address is shown if you execute this in a shell:\n\n```sh\nprintf \"\\x61\\x6b\\x61\\x6c\\x61\\x72\\x69\\x63\\x31\\x40\\x6f\\x75\\x74\\x6c\\x6f\\x6f\\x6b\\x2e\\x63\\x6f\\x6d\\x0a\"\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolfram",
        "wolframalpha",
        "chat",
        "alpha chat",
        "chat applications",
        "wolfram alpha"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-logic": {
      "owner": "angrysky56",
      "name": "mcp-logic",
      "url": "https://github.com/angrysky56/mcp-logic",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides automated reasoning capabilities through Prover9 and Mace4, enabling logical theorem proving and model verification. Facilitates formal validation of knowledge representations and implications via a clean MCP interface.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Shell",
      "updated_at": "2025-10-03T23:37:00Z",
      "readme_content": "# MCP-Logic\n\nAn MCP server providing automated reasoning capabilities using Prover9/Mace4 for AI systems. This server enables logical theorem proving and logical model verification through a clean MCP interface.\n\n## Design Philosophy\n\nMCP-Logic bridges the gap between AI systems and formal logic by providing a robust interface to Prover9/Mace4. What makes it special:\n\n- **AI-First Design**: Built specifically for AI systems to perform automated reasoning\n- **Knowledge Validation**: Enables formal verification of knowledge representations and logical implications\n- **Clean Integration**: Seamless integration with the Model Context Protocol (MCP) ecosystem\n- **Deep Reasoning**: Support for complex logical proofs with nested quantifiers and multiple premises\n- **Real-World Applications**: Particularly useful for validating AI knowledge models and reasoning chains\n\n## Features\n\n- Seamless integration with Prover9 for automated theorem proving\n- Support for complex logical formulas and proofs\n- Built-in syntax validation\n- Clean MCP server interface\n- Extensive error handling and logging\n- Support for knowledge representation and reasoning about AI systems\n\n## Quick Example\n\n![image](https://github.com/user-attachments/assets/42756e3d-c2fa-475f-8e8a-25f7e444b2a4)\n\n```python\n# Prove that understanding + context leads to application\nresult = await prove(\n    premises=[\n        \"all x all y (understands(x,y) -> can_explain(x,y))\",\n        \"all x all y (can_explain(x,y) -> knows(x,y))\",\n        \"all x all y (knows(x,y) -> believes(x,y))\",\n        \"all x all y (believes(x,y) -> can_reason_about(x,y))\",\n        \"all x all y (can_reason_about(x,y) & knows_context(x,y) -> can_apply(x,y))\",\n        \"understands(system,domain)\",\n        \"knows_context(system,domain)\"\n    ],\n    conclusion=\"can_apply(system,domain)\"\n)\n# Returns successful proof!\n```\n\n![image](https://github.com/user-attachments/assets/61cecc1f-9ba1-4586-a6a2-83823088f763)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10+\n- UV package manager\n- Git for cloning the repository\n- CMake and build tools (for building LADR/Prover9)\n\n### Setup\n\nClone this repository\n\n```bash\ngit clone https://github.com/angrysky56/mcp-logic\ncd mcp-logic\n```\n\nRun the setup script:\nWindows run:\n\n```bash\nwindows-setup-mcp-logic.bat\n```\n\nLinux/macOS:\n\n```bash\nchmod +x linux-setup-script.sh\n./linux-setup-script.sh\n```\n\nThe setup script:\n\n- Checks for dependencies (git, cmake, build tools)\n- Downloads LADR (Prover9/Mace4) from the external repository: [laitep/LADR](https://github.com/laitep/ladr.git)\n- Builds the LADR library to create Prover9 binaries in the ladr/bin directory\n- Creates a Python virtual environment\n- Sets up configuration files for running with or without Docker\n\nIMPORTANT: The LADR directory is not included in the repository itself and will be installed through the setup script or manually.\n\n### Using Docker- no idea if this is working right, mainly designed for direct use with Claude Desktop\n\nIf you prefer to run with Docker this script:\n\n- Finds an available port\n- Activates the virtual environment\n- Runs the server with the correct paths to the installed Prover9\n\n```bash\n# Linux/macOS\n./run-mcp-logic.sh\n```\n\n```bash\n# Windows\nrun-mcp-logic.bat\n```\n\nThese scripts will build and run a Docker container with the necessary environment.\n\n### Claude Desktop Integration\n\nTo use MCP-Logic with Claude Desktop, use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-logic\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"/path/to/mcp-logic/src/mcp_logic\",\n        \"run\", \n        \"mcp_logic\", \n        \"--prover-path\", \n        \"/path/to/mcp-logic/ladr/bin\"\n      ]\n    }\n  }\n}\n```\n\nReplace \"/path/to/mcp-logic\" with your actual repository path.\n\n## Available Tools\n\n![image](https://github.com/user-attachments/assets/a78ba32a-c0a8-4e44-9429-b2c9af57129d)\n\n### prove\n\nRun logical proofs using Prover9:\n\n```json\n{\n  \"tool\": \"prove\",\n  \"arguments\": {\n    \"premises\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ],\n    \"conclusion\": \"mortal(socrates)\"\n  }\n}\n```\n\n### check-well-formed\n\nValidate logical statement syntax:\n\n```json\n{\n  \"tool\": \"check-well-formed\",\n  \"arguments\": {\n    \"statements\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ]\n  }\n}\n```\n\n## Documentation\n\nSee the [Documents](./Documents) folder for detailed analysis and examples:\n\n- [Knowledge to Application](./Documents/KnowledgeToApplication.md): A formal logical analysis of understanding and practical application in AI systems\n\n## Project Structure\n\n```markdown\nmcp-logic/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ mcp_logic/\n‚îÇ       ‚îî‚îÄ‚îÄ server.py   # Main MCP server implementation\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_proofs.py  # Core functionality tests\n‚îÇ   ‚îî‚îÄ‚îÄ test_debug.py   # Debug utilities\n‚îú‚îÄ‚îÄ Documents/          # Analysis and documentation\n‚îú‚îÄ‚îÄ pyproject.toml      # Python package config\n‚îú‚îÄ‚îÄ setup-script.sh     # Setup script (installs LADR & dependencies)\n‚îú‚îÄ‚îÄ run-mcp-logic.sh    # Docker-based run script (Linux/macOS)\n‚îú‚îÄ‚îÄ run-mcp-logic.bat   # Docker-based run script (Windows)\n‚îú‚îÄ‚îÄ run-mcp-logic-local.sh # Local run script (no Docker)\n‚îî‚îÄ‚îÄ README.md           # This file\n```\n\nNote: After running setup-script.sh, a \"ladr\" directory will be created containing the Prover9 binaries, but this directory is not included in the repository itself.\n\n## Development\n\nRun tests:\n\n```bash\nuv pip install pytest\nuv run pytest\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prover9",
        "mcp",
        "logic",
        "mcp logic",
        "automated reasoning",
        "logic provides"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-rocq": {
      "owner": "angrysky56",
      "name": "mcp-rocq",
      "url": "https://github.com/angrysky56/mcp-rocq",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides advanced logical reasoning capabilities through integration with the Coq proof assistant, enabling automated dependent type checking, inductive type definitions, and property proving with custom tactics and automation.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-23T14:43:03Z",
      "readme_content": "# MCP-RoCQ (Coq Reasoning Server)\n\n# Currently shows tools but Claude can't use it properly for some reason- invalid syntax generally seems the issue but there could be something else.\n\nThere may be a better way to set this up with the coq cli or something.\nAnyone want to try and fix it who knows what they are doing would be great.\n\nMCP-RoCQ is a Model Context Protocol server that provides advanced logical reasoning capabilities through integration with the Coq proof assistant. It enables automated dependent type checking, inductive type definitions, and property proving with both custom tactics and automation.\n\n## Features\n\n- **Automated Dependent Type Checking**: Verify terms against complex dependent types\n- **Inductive Type Definition**: Define and automatically verify custom inductive data types\n- **Property Proving**: Prove logical properties using custom tactics and automation\n- **XML Protocol Integration**: Reliable structured communication with Coq\n- **Rich Error Handling**: Detailed feedback for type errors and failed proofs\n\n## Installation\n\n1. Install the Coq Platform 8.19 (2024.10)\n\nCoq is a formal proof management system. It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs.\n\n[https://github.com/coq/platform](https://github.com/coq/platform)\n\n2. Clone this repository:\n\n```bash\ngit clone https://github.com/angrysky56/mcp-rocq.git\n```\n\ncd to the repo\n\n```bash\nuv venv\n./venv/Scripts/activate\nuv pip install -e .\n```\n\n# JSON for the Claude App or mcphost config- set your paths according to how you installed coq and the repository.\n\n```json\n    \"mcp-rocq\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"F:/GithubRepos/mcp-rocq\",\n        \"run\",\n        \"mcp_rocq\",\n        \"--coq-path\",\n        \"F:/Coq-Platform~8.19~2024.10/bin/coqtop.exe\",\n        \"--lib-path\",\n        \"F:/Coq-Platform~8.19~2024.10/lib/coq\"\n      ]\n    },\n```\n\n\n# This might work- I got it going with uv and most of this could be hallucinatory though:\n\n3. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nThe server provides three main capabilities:\n\n### 1. Type Checking\n\n```python\n{\n    \"tool\": \"type_check\",\n    \"args\": {\n        \"term\": \"<term to check>\",\n        \"expected_type\": \"<type>\",\n        \"context\": [\"relevant\", \"modules\"] \n    }\n}\n```\n\n### 2. Inductive Types\n\n```python\n{\n    \"tool\": \"define_inductive\",\n    \"args\": {\n        \"name\": \"Tree\",\n        \"constructors\": [\n            \"Leaf : Tree\",\n            \"Node : Tree -> Tree -> Tree\"\n        ],\n        \"verify\": true\n    }\n}\n```\n\n### 3. Property Proving\n\n```python\n{\n    \"tool\": \"prove_property\",\n    \"args\": {\n        \"property\": \"<statement>\",\n        \"tactics\": [\"<tactic sequence>\"],\n        \"use_automation\": true\n    }\n}\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rocq",
        "automated",
        "mcp",
        "rocq provides",
        "mcp rocq",
        "advanced logical"
      ],
      "category": "scientific-research-tools"
    },
    "blake365--macrostrat-mcp": {
      "owner": "blake365",
      "name": "macrostrat-mcp",
      "url": "https://github.com/blake365/macrostrat-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/blake365.webp",
      "description": "Query and analyze comprehensive geologic data using the Macrostrat API, providing access to geologic units, minerals, timescales, and more. Create visualizations based on geological data to enhance understanding.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-26T01:43:52Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/blake365-macrostrat-mcp-badge.png)](https://mseep.ai/app/blake365-macrostrat-mcp)\n\n# macrostrat-mcp\nMCP server for Macrostrat API\n\n[![smithery badge](https://smithery.ai/badge/@blake365/macrostrat-mcp)](https://smithery.ai/server/@blake365/macrostrat-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/v67anfiq0s\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v67anfiq0s/badge\" alt=\"Macrostrat Server MCP server\" />\n</a>\n\nAn MCP server implementation for providing access to the [Macrostrat API](https://macrostrat.org/api) within [Claude Desktop](https://claude.ai/download).\n\n## Overview\n\nThis Model Context Protocol (MCP) server enables AI assistants and applications to:\n\n- Query comprehensive geologic data and information provided by the Macrostrat API\n- Access geologic units, columns, minerals, timescales, and more\n- Get geologic map tiles that the LLM can interpret (questionable utility, but might be useful for some applications)\n- Analyze geologic data to improve geological understanding\n- Create graphics and perform data analysis from the data\n\nUsers can run queries using natural language.\n\n## Sample Output\n\n**Q. What is the geologic history of Nashville, TN?**\n\nA. \nLet me check the geological data for Nashville's location.\n\nView Result from find-units from macrostrat (local)\n```json\n{\n  \"lat\": 36.1627,\n  \"lng\": -86.7816,\n  \"responseType\": \"long\"\n}\n```\n\n### Geological History of Nashville, Tennessee\n1. Mesoproterozoic Basement (1.6-1.1 billion years ago)\n- Igneous basement rocks (granite and rhyolite)\n- Part of the ancient North American craton\n\n2. Early-Middle Cambrian (518-504 million years ago)\n- Rome Formation: Mixed carbonate-siliciclastic sediments\n- Shallow marine environment\n- Part of the developing continental shelf\n\n3. Middle-Late Cambrian (504-498 million years ago)\n- Conasauga Formation: Limestone and mixed sediments\n- Continued marine deposition\n- Alternating limestone and shale indicating fluctuating sea levels\n\n4. Late Cambrian-Early Ordovician (498-470 million years ago)\n- Knox Dolomite: Massive dolomite sequence\n- Extensive shallow marine platform\n- Up to 1,200 meters thick\n- Represents long period of stable marine conditions\n\n5. Middle-Late Ordovician (470-443 million years ago)\nSeveral important formations:\n- Stones River Group (including Murfreesboro, Pierce, Lebanon Limestones)\n- Nashville Group (including Hermitage, Bigby-Cannon, Catheys Formations)\n- Primarily limestone deposits\n- Rich in fossils\n- Shallow marine environment\n- Multiple cycles of sea level change\n\n6. Early-Middle Silurian (443-423 million years ago)\n- Brassfield through Decatur Formations\n- Continued marine limestone deposition\n- Various marine environments from shallow to deeper water\n\n7. Middle Devonian-Early Mississippian (390-357 million years ago)\n- Pegram Formation through Chattanooga Shale\n- Shift to more clastic (shale) deposition\n- Chattanooga Shale represents deeper, oxygen-poor conditions\n\n8. Early-Late Mississippian (357-331 million years ago)\nSeveral limestone formations:\n- Fort Payne Formation\n- Warsaw Limestone\n- St. Louis Limestone\n- Monteagle Limestone\n- Hartselle Formation\n- Return to carbonate platform conditions\n- Diverse marine environments\n\n9. Cenozoic to Present\n- Long period of erosion and exposure\n- Development of the current landscape\n- Formation of the Nashville Dome structure\n- River and stream deposits\n\n### Key Features\n- The area was predominantly a marine environment for most of its geological history\n- Dominated by limestone formations indicating warm, shallow seas\n- Multiple cycles of sea level change\n- Rich fossil record, especially in Ordovician rocks\n- Part of the Nashville Dome structural feature\n- Long periods of erosion have removed younger rocks\n- Current topography shaped by differential erosion of various limestone units\n\n### Present Day Bedrock\nThe bedrock of Nashville today consists primarily of Ordovician limestone formations exposed at the surface, particularly the Stones River and Nashville Group limestones. These rocks are well exposed in many areas and have influenced the development of local karst topography, including caves and sinkholes that are common in the region.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Macrostrat API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@blake365/macrostrat-mcp):\n\n```bash\nnpx -y @smithery/cli install @blake365/macrostrat-mcp --client claude\n```\n\n### Extra Steps\n\nIf you want to make changes to the server you can do so by editing the `src/index.ts` file.\n- Run `npm install` to install dependencies\n- Run `npm run build` to build the server\n- Quit and restart Claude Desktop after making changes\n\n\n## Connecting with Claude Desktop\n\n1. Open your Claude Desktop configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"macrostrat\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Full/Route/to/Folder/macrostrat/build/index.js\"\n            ]\n        }\n    }\n}\n```\n\n3. Close/Quit then restart Claude Desktop\n\nOnce you restart you should see an icon for search and tools. Click this button to see the MCP tools available.\n\n## Troubleshooting\n\nIf you get errors when running the server you may need to provide the full path to the `node` command. For example, on macOS: `/usr/local/bin/node`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geologic",
        "geological",
        "macrostrat",
        "geologic data",
        "geological data",
        "access geologic"
      ],
      "category": "scientific-research-tools"
    },
    "blazickjp--arxiv-mcp-server": {
      "owner": "blazickjp",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/blazickjp/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/blazickjp.webp",
      "description": "Provides access to arXiv research papers via a straightforward MCP interface, enabling AI models to search and retrieve academic articles.",
      "stars": 1743,
      "forks": 121,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T01:41:17Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## üìù Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "articles",
        "search",
        "research",
        "arxiv mcp",
        "academic articles",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "bmorphism--penrose-mcp": {
      "owner": "bmorphism",
      "name": "penrose-mcp",
      "url": "https://github.com/bmorphism/penrose-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Create mathematical diagrams using domain-specific languages that define mathematical types, objects, relationships, and visual representation rules.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T21:49:31Z",
      "readme_content": "# Penrose MCP Server\n\nA Model Context Protocol (MCP) server for [Penrose](https://penrose.cs.cmu.edu/) - Create beautiful mathematical diagrams through natural language.\n\n## Overview\n\nThis MCP server provides tools and resources for creating mathematical diagrams using Penrose's domain-specific languages:\n\n- **Domain (DSL)**: Define mathematical types and relationships\n- **Substance**: Describe mathematical objects and their relationships\n- **Style**: Specify visual representation rules\n\n## Project Structure\n\n- `.topos/`: Research materials and documentation (gitignored)\n  - `penrose-research/`: Design documents and specifications\n  - `mcp-examples/`: Reference MCP server implementations\n  - `mcp-spec/`: Official MCP protocol documentation\n\n## Development\n\nUse the justfile to access documentation and reference materials:\n\n```bash\n# List all available commands\njust --list\n\n# View the server architecture\njust architecture\n\n# Access MCP specifications\njust mcp-spec\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrams",
        "mathematical",
        "types",
        "mathematical diagrams",
        "mathematical types",
        "diagrams using"
      ],
      "category": "scientific-research-tools"
    },
    "cnitlrt--headless-ida-mcp-server": {
      "owner": "cnitlrt",
      "name": "headless-ida-mcp-server",
      "url": "https://github.com/cnitlrt/headless-ida-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cnitlrt.webp",
      "description": "Analyze binary files and manage functions and variables remotely using IDA Pro's headless mode. Provides tools for efficient remote operations through the Multi-Client Protocol.",
      "stars": 18,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-11T13:34:27Z",
      "readme_content": "# Acknowledgments\n\nThis project builds upon the work of:\n- Tools code adapted from [ida-pro-mcp](https://github.com/mrexodia/ida-pro-mcp) by mrexodia\n- Utilizes the [headless-ida](https://github.com/DennyDai/headless-ida) library by DennyDai\n\n# Headless IDA MCP Server\n\nIf you want to run the server directly as a cli app, rather than an IDA plugin interactively,you can chose it.\n\n## Project Description\n\nThis project uses IDA Pro's headless mode to analyze binary files and provides a suite of tools via MCP to manage and manipulate functions, variables, and more.\n\n## Prerequisites\n\n- Python 3.12 or higher\n- IDA Pro with headless support (idat) https://github.com/DennyDai/headless-ida\n\n## Installation\n\n1. Clone the project locally:\n\n   ```bash\n   git clone https://github.com/cnitlrt/headless-ida-mcp-server.git \n   cd headless-ida-mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   uv python install 3.12\n   uv venv --python 3.12\n   uv pip install -e .\n   ```\n\n## Configuration\n\n1. Copy the example environment file:\n   ```bash\n   cp .env_example .env\n   ```\n\n2. Configure the following environment variables in `.env`:\n\n   - `IDA_PATH`: Path to IDA Pro's headless executable (idat), e.g., `/home/ubuntu/idapro/idat`\n   - `PORT`: Port number for the MCP server, e.g., `8888`\n   - `HOST`: Host address for the MCP server, e.g., `127.0.0.1`\n   - `TRANSPORT`: MCP transport mode (`sse` or `stdio`)\n\n## Usage\n\n1. Start the server:\n   ```bash\n   uv run headless_ida_mcp_server\n   ```\n\n2. Connect to the server using an MCP client:\n\n   Debug it: \n   ```bash\n   npx -y @modelcontextprotocol/inspector\n   ```\n   or\n   ```json\n   {\n   \"mcpServers\": {\n      \"ida\": {\n         \"command\": \"/path/to/uv\",\n         \"args\": [\"--directory\",\"path/to/headless-ida-mcp-server\",\"run\",\"headless_ida_mcp_server\"]\n      }\n   }\n   }\n   ```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "cnitlrt",
        "headless",
        "headless ida",
        "using ida",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "daheepk--arxiv-paper-mcp": {
      "owner": "daheepk",
      "name": "arxiv-paper-mcp",
      "url": "https://github.com/daheepk/arxiv-paper-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/daheepk.webp",
      "description": "Facilitates efficient search and exploration of research papers from arXiv.org, enabling users to access detailed metadata, conduct keyword and author-based searches, and generate prompts for summarization and comparison. Provides updates on trending topics across various research categories.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T18:39:37Z",
      "readme_content": "# üß† arXiv Research Assistant MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@daheepk/arxiv-paper-mcp)](https://smithery.ai/server/@daheepk/arxiv-paper-mcp)\n\n\nThis project is an MCP (Model Context Protocol) server built to interact with the vast arXiv.org paper database.\n\nIt allows clients like **Claude AI** to search, explore, and compare arXiv papers efficiently ‚Äî all through a custom-built, local server. It‚Äôs built with **Python** and the **FastMCP** framework, and uses **uv** for lightweight package management.\n\n<table>\n  <tr>\n    <td>\n      <a href=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server\">\n        <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server/badge\" alt=\"arXiv Research Assistant Server MCP server\" />\n      </a>\n    </td>\n    <td style=\"vertical-align: top; padding-left: 10px;\">\n      <a href=\"https://mseep.ai/app/daheepk-arxiv-paper-mcp\">\n        <img src=\"https://mseep.net/pr/daheepk-arxiv-paper-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" width=\"140\" />\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n## ‚ú® Features\n\n- **üîç Keyword-based Paper Search**  \n  Search arXiv papers by keywords, with options to sort by relevance or most recent.\n\n- **üìö Latest Papers by Category**  \n  Specify an arXiv category code (e.g., `cs.AI`, `math.AP`) to fetch the most recent papers in that field.\n\n- **üìÑ Paper Details Lookup**  \n  Fetch detailed metadata using a paper's arXiv ID: title, authors, abstract, categories, DOI, PDF link, and more.\n\n- **üßë‚Äçüî¨ Author-based Paper Search**  \n  Retrieve a list of papers published by a specific author.\n\n- **üìä Trend Analysis (Experimental)**  \n  Get an overview of trending keywords or topics based on recent papers in a category (currently uses mock data).\n\n- **üìù Summarization Prompt Generator**  \n  Dynamically generate prompts that help LLMs summarize a selected paper more effectively.\n\n- **üÜö Comparison Prompt Generator**  \n  Provide two paper IDs to generate a structured prompt for comparing their content.\n\n---\n\n## üõ†Ô∏è Tech Stack\n\n- Python 3.11+\n- [FastMCP](https://github.com/modelcontextprotocol/fastmcp)\n- uv (for dependency & environment management)\n- requests (for API communication)\n- xml.etree.ElementTree (for parsing XML responses)\n\n---\n\n## üöÄ Getting Started\n\n### Installing via Smithery\n\nTo install arXiv Research Assistant MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-paper-mcp):\n\n```bash\nnpx -y @smithery/cli install arxiv-paper-mcp --client claude\n```\n\n### Installation from PyPI\n\n```bash\nuv pip install arxiv-paper-mcp\n```\n\n### üîß Clone the repository (for development)\n```bash\ngit clone https://github.com/daheepk/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n### üîß Install Dependencies (for development)\n\nUse `uv` to install all dependencies in editable mode:\n\n```bash\nuv pip install -e .\n```\n\n\n## ‚öôÔ∏è How to Run\n\n### ‚ñ∂Ô∏è Run the server (locally)\n\n```bash\narxiv-paper-mcp\n```\n\n## üîå Use with Claude\n\nTo use this MCP server with Claude, add the following JSON configuration to Claude's MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"arXivPaper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"arxiv-paper-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Project Structure\n```\narxiv-mcp-server/\n‚îú‚îÄ‚îÄ arxiv_mcp/              # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ app.py              # FastMCP app setup\n‚îÇ   ‚îú‚îÄ‚îÄ server.py           # Server entry point\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # arXiv API communication logic\n‚îÇ   ‚îú‚îÄ‚îÄ resources/          # MCP resources (categories, authors, etc.)\n‚îÇ   ‚îú‚îÄ‚îÄ tools/              # MCP tools (search, detail lookup, trends)\n‚îÇ   ‚îî‚îÄ‚îÄ prompts/            # Prompt templates (summarize, compare)\n‚îú‚îÄ‚îÄ pyproject.toml          # Project config & dependencies\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "searches",
        "arxiv",
        "research tools",
        "research papers",
        "papers arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "doomdagadiggiedahdah--iacr-mcp-server": {
      "owner": "doomdagadiggiedahdah",
      "name": "iacr-mcp-server",
      "url": "https://github.com/doomdagadiggiedahdah/iacr-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/doomdagadiggiedahdah.webp",
      "description": "Provides a programmatic interface to retrieve and search for cryptographic research papers from the IACR Cryptology ePrint Archive, along with access to paper metadata.",
      "stars": 2,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-16T06:16:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/doomdagadiggiedahdah-iacr-mcp-server-badge.png)](https://mseep.ai/app/doomdagadiggiedahdah-iacr-mcp-server)\n\n# IACR Cryptology ePrint Archive MCP Server\n\n[![smithery badge](https://smithery.ai/badge/iacr-mcp-server)](https://smithery.ai/server/iacr-mcp-server)\n\n## Overview\n\nThis Model Context Protocol (MCP) server provides a programmatic interface to the IACR Cryptology ePrint Archive, enabling efficient retrieval of cryptographic research papers.\n\n<a href=\"https://glama.ai/mcp/servers/e2oh3a96de\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e2oh3a96de/badge\" alt=\"IACR Server MCP server\" /></a>\n\n## Features\n\n- üîç Search cryptographic papers\n- üìã Retrieve paper metadata\n- üîí Secure access to research publications\n\n## Prerequisites\n\n- Node.js (v16+)\n- npm or yarn\n\n## Installation\n\n### Installing via Smithery\n\nTo install IACR Cryptology ePrint Archive for Claude Desktop automatically via [Smithery](https://smithery.ai/server/iacr-mcp-server):\n\n```bash\nnpx -y @smithery/cli install iacr-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/yourusername/iacr-mcp-server.git\ncd iacr-mcp-server\nnpm install\n```\n\n## Configuration\n\nNo additional configuration is required. The server uses the IACR ePrint Archive's RSS feed for data retrieval.\n\n## Usage\n\n### Available Tools\n\n1. `search_papers`: Search for papers\n   - Parameters:\n     - `query`: Search term (required)\n     - `year`: Publication year (optional)\n     - `max_results`: Maximum number of results (default: 20)\n\n2. `get_paper_details`: Retrieve details for a specific paper\n   - Parameters:\n     - `paper_id`: Unique paper identifier (required)\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Disclaimer\n\nThis is an unofficial tool. Always refer to the original IACR Cryptology ePrint Archive for the most accurate and up-to-date research publications.\n\n## Contact\n\nFor issues, questions, or suggestions, please open a GitHub issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cryptology",
        "archive",
        "iacr",
        "iacr cryptology",
        "cryptology eprint",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "dougwithseismic--withseismic-mcp": {
      "owner": "dougwithseismic",
      "name": "withseismic-mcp",
      "url": "https://github.com/dougwithseismic/withseismic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/dougwithseismic.webp",
      "description": "Facilitates integration of AI models with various tools and prompts, allowing for rapid development and automated type safety in TypeScript without infrastructure management.",
      "stars": 14,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T12:23:11Z",
      "readme_content": "# D0UGWITHSEISMIC/WITHSEISMIC-MCP\n\n\n\n```\nSYSTEMS ONLINE ‚Ä¢ NEURAL INTERFACE ACTIVE ‚Ä¢ COMBAT DATA ANALYSIS ‚Ä¢ TACTICAL OVERLAY ENABLED ‚Ä¢ PERFORMANCE METRICS NOMINAL\n```\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)\n[![Node.js](https://img.shields.io/badge/Node.js-18.x-green.svg)](https://nodejs.org/)\n[![pnpm](https://img.shields.io/badge/pnpm-8.x-orange.svg)](https://pnpm.io/)\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da.svg?logo=discord&logoColor=white)](https://discord.gg/BX25FcS53N)\n[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/RTm6ec?referralCode=dougie)\n\nA production-ready MCP server template for hosting your own AI tools and prompts. Deploy remotely or run locally - built for developers who want to expose their tools to AI models without the infrastructure headaches.\n\n## ://WHO_IS_THIS_FOR?\n\nThis template is perfect for:\n\n- **AI Tool Developers**: Build and host your own tools that AI models can interact with\n- **Teams & Organizations**: Set up a shared MCP server for your AI applications\n- **Individual Developers**: Quick-start your MCP development with a production-ready setup\n- **API Providers**: Expose your services to AI models through a standardized interface\n\nWhether you want to:\n\n- üåê Host a remote MCP server that multiple clients can connect to\n- üîß Run locally via stdio for development and testing\n- üöÄ Deploy a production-ready MCP implementation\n- üõ†Ô∏è Build tools without worrying about infrastructure\n\nThis template provides everything you need to get started.\n\n## ://OVERVIEW\n\nThe MCP server provides two ways to expose your tools to AI models:\n\n1. **Remote Server Mode (SSE)**: Deploy as a remote server that multiple clients can connect to\n\n   - Perfect for production deployments\n     (`pnpm start:sse`)\n   - Supports multiple simultaneous connections\n   - Easy deployment to platforms like Railway\n   - Secure and scalable\n\n2. **Local Mode (stdio)**: Run locally for development and testing\n   (`pnpm start`)\n   - Great for local development\n   - Direct stdio communication\n   - Fast iteration and debugging\n\nKey Features:\n\n- **Tools**: Executable functions that models can call\n- **Prompts**: Template-based message generators\n- **Auto-Discovery**: Automatic registration of your components\n- **Type Safety**: Full TypeScript support throughout\n\n## ://WHY_USE_THIS_MCP_SERVER_TEMPLATE?\n\nWhile the Model Context Protocol (MCP) is in its early stages, one thing is clear: **rapid adoption requires frictionless development**. This implementation is built with a singular focus: **letting developers focus on building great tools, not infrastructure**.\n\n### INSTANT_DEPLOYMENT\n\n```bash\n# Option 1: One-Click Deploy\nClick the \"Deploy on Railway\" button above ‚òùÔ∏è\n\n# Option 2: Local Setup\npnpm install && pnpm dev\n```\n\n### ZERO_FRICTION_DEVELOPMENT\n\n- **No Boilerplate**: Define your tool, import it, and you're done. We handle the rest.\n- **Auto-Registration**: Tools and prompts are automatically discovered and registered.\n- **Type Safety**: Full TypeScript support with zero configuration.\n\n### BATTLE_TESTED_ARCHITECTURE\n\n- **Production Ready**: Built for reliability and performance.\n- **Scalable Design**: From simple tools to complex AI interactions.\n- **Error Resilient**: Robust error handling and debugging built-in.\n\n### DEVELOPER_EXPERIENCE_FIRST\n\n```typescript\n// That's it. This is all you need to create a tool.\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: z.object({ query: z.string() }),\n    outputSchema: z.object({ result: z.string() }),\n  },\n  async (args) => {\n    // Your logic here\n    return { result: \"Done!\" };\n  },\n);\n```\n\nWe handle:\n\n- ‚ö° Connection management\n- üîÑ Tool registration\n- üõ°Ô∏è Input validation\n- üì¶ State management\n- üö® Error handling\n- üìä Logging and metrics\n\nYou focus on:\n\n- üéØ Building amazing tools\n- üß† Creating powerful prompts\n- üöÄ Shipping features\n\n## ://GETTING_STARTED\n\n### INSTALLATION\n\n```bash\npnpm install\n```\n\n### RUNNING_THE_SERVER\n\nTwo modes are available:\n\n1. Standard mode (stdio):\n\n```bash\npnpm dev      # Development with hot reload\npnpm start    # Production\n```\n\n2. SSE (Server-Sent Events) mode:\n\n```bash\npnpm dev:sse  # Development with hot reload\npnpm start:sse # Production\n```\n\nWhen running in SSE mode, connect to: `http://localhost:3001/sse`\n\n## ://CORE_CONCEPTS\n\n### TOOLS\n\nTools are executable functions that models can invoke. Each tool:\n\n- Has defined input/output schemas using Zod\n- Is automatically registered with the registry\n- Can perform any operation (calculation, file I/O, API calls, etc.)\n\nExample tool:\n\n```typescript\nimport { z } from \"zod\";\nimport { Tool } from \"../core\";\n\nconst MyToolInputSchema = z.object({\n  param1: z.string().describe(\"Parameter description\"),\n});\n\nconst MyToolOutputSchema = z.object({\n  result: z.string().describe(\"Result description\"),\n});\n\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: MyToolInputSchema,\n    outputSchema: MyToolOutputSchema,\n  },\n  async (args) => {\n    const input = MyToolInputSchema.parse(args);\n    // Tool logic here\n    return { result: \"processed result\" };\n  },\n);\n```\n\n### PROMPTS\n\nPrompts are message generators that help structure model interactions. Each prompt:\n\n- Defines its argument schema\n- Generates messages in a consistent format\n- Is automatically registered with the registry\n\nExample prompt:\n\n```typescript\nimport { Prompt } from \"../core\";\n\nexport const myPrompt = new Prompt(\n  {\n    name: \"myPrompt\",\n    description: \"What my prompt does\",\n    arguments: [\n      {\n        name: \"arg1\",\n        description: \"Argument description\",\n        required: true,\n      },\n    ],\n  },\n  async (args) => {\n    return [\n      {\n        role: \"system\",\n        content: {\n          type: \"text\",\n          text: `Generated message using ${args.arg1}`,\n        },\n      },\n    ];\n  },\n);\n```\n\n## ://ADDING_NEW_COMPONENTS\n\n### Creating a New Tool\n\n1. Create a new file in `src/modules/tools/`\n2. Define your input/output schemas using Zod\n3. Create and export your tool instance\n4. Add the export to `src/modules/tools/index.ts`\n\nThe registry will automatically:\n\n- Register your tool\n- Make it available to models\n- Handle validation and error handling\n\n### Creating a New Prompt\n\n1. Create a new file in `src/modules/prompts/`\n2. Define your argument schema\n3. Create and export your prompt instance\n4. Add the export to `src/modules/prompts/index.ts`\n\nThe registry will automatically:\n\n- Register your prompt\n- Make it available to models\n- Handle message generation and errors\n\n## Architecture\n\n### Core Components\n\n- **Registry**: Central manager for all tools and prompts\n- **Tool**: Base class for executable functions\n- **Prompt**: Base class for message generators\n- **Server**: MCP protocol implementation\n\n### Auto-Registration\n\nThe system uses a singleton Registry pattern that:\n\n1. Automatically registers tools and prompts on import\n2. Provides type-safe access to components\n3. Handles all MCP protocol interactions\n\n### Error Handling\n\nThe system includes robust error handling:\n\n- Type validation via Zod schemas\n- Execution error wrapping\n- Detailed error messages for debugging\n\n## Development\n\n### Type Safety\n\nAll components use TypeScript for full type safety:\n\n- Input/output schemas are defined using Zod\n- Type inference for tool arguments and results\n- Comprehensive error types\n\n### Testing\n\nRun tests using:\n\n```bash\npnpm test\n```\n\n### Best Practices\n\n1. Always define clear input/output schemas\n2. Use descriptive names and documentation\n3. Handle errors gracefully\n4. Follow the TypeScript guidelines in the codebase\n\n## ://CONTRIBUTING\n\n```\nNEURAL INTERFACE DETECTED ‚Ä¢ INITIATING COLLABORATION PROTOCOLS ‚Ä¢ READY FOR UPLINK\n```\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on:\n\n- Development workflow\n- Code style guidelines\n- Pull request process\n- Issue reporting\n\nJoin our [Discord community](https://discord.gg/withseismic) to connect with other contributors!\n\n## ://SUPPORT\n\n```\nSUPPORT PROTOCOLS ACTIVE ‚Ä¢ COMMUNICATION CHANNELS OPEN ‚Ä¢ READY TO ASSIST\n```\n\n- üì´ Email: <hello@withseismic.com>\n- üí¨ Discord: [Join our server](https://discord.gg/BX25FcS53N)\n\n## ://LICENSE\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n¬© 2025 Doug, at WithSeismic dot com.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "withseismic",
        "typescript",
        "ai",
        "tools dougwithseismic",
        "withseismic mcp",
        "dougwithseismic withseismic"
      ],
      "category": "scientific-research-tools"
    },
    "finite-sample--rmcp": {
      "owner": "finite-sample",
      "name": "rmcp",
      "url": "https://github.com/finite-sample/rmcp",
      "imageUrl": "/freedevtools/mcp/pfp/finite-sample.webp",
      "description": "Perform advanced econometric analyses using R, including linear regression, panel data modeling, and various diagnostic tests. Facilitate data-driven decision-making through sophisticated statistical analyses.",
      "stars": 183,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:17:45Z",
      "readme_content": "# RMCP: Statistical Analysis through Natural Conversation\n\n[![Python application](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml/badge.svg)](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml)\n[![PyPI version](https://img.shields.io/pypi/v/rmcp.svg)](https://pypi.org/project/rmcp/)\n[![Downloads](https://pepy.tech/badge/rmcp)](https://pepy.tech/project/rmcp)\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://finite-sample.github.io/rmcp/)\n[![License](https://img.shields.io/github/license/finite-sample/rmcp)](https://github.com/finite-sample/rmcp/blob/main/LICENSE)\n\n**Turn conversations into comprehensive statistical analysis** - A Model Context Protocol (MCP) server with 44 statistical analysis tools across 11 categories. RMCP enables AI assistants to perform sophisticated statistical modeling, econometric analysis, machine learning, time series analysis, and data science tasks through natural conversation.\n\n## üöÄ Quick Start (30 seconds)\n\n```bash\npip install rmcp\nrmcp start\n```\n\nThat's it! RMCP is now ready to handle statistical analysis requests via Claude Desktop or any MCP client.\n\n**üéØ [Working examples ‚Üí](examples/quick_start_guide.md)** | **üîß [Troubleshooting ‚Üí](#-quick-troubleshooting)**\n\n## ‚ú® What Can RMCP Do?\n\n### üìä **Regression & Economics** \nLinear regression, logistic models, panel data, instrumental variables ‚Üí *\"Analyze ROI of marketing spend\"*\n\n### ‚è∞ **Time Series & Forecasting**\nARIMA models, decomposition, stationarity testing ‚Üí *\"Forecast next quarter's sales\"*\n\n### üß† **Machine Learning** \nClustering, decision trees, random forests ‚Üí *\"Segment customers by behavior\"*\n\n### üìà **Statistical Testing**\nT-tests, ANOVA, chi-square, normality tests ‚Üí *\"Is my A/B test significant?\"*\n\n### üìã **Data Analysis**\nDescriptive stats, outlier detection, correlation analysis ‚Üí *\"Summarize this dataset\"*\n\n### üîÑ **Data Transformation**\nStandardization, winsorization, lag/lead variables ‚Üí *\"Prepare data for modeling\"*\n\n### üìä **Professional Visualizations**\nInline plots in Claude: scatter plots, histograms, heatmaps ‚Üí *\"Show me a correlation matrix\"*\n\n### üìÅ **Smart File Operations**\nCSV, Excel, JSON import with validation ‚Üí *\"Load and analyze my sales data\"*\n\n### ü§ñ **Natural Language Features**\nFormula building, error recovery, example datasets ‚Üí *\"Help me build a regression formula\"*\n\n**üëâ [See working examples ‚Üí](examples/quick_start_guide.md)**\n\n## üìä Real Usage with Claude\n\n### Business Analysis\n**You:** *\"I have sales data and marketing spend. Can you analyze the ROI?\"*\n\n**Claude:** *\"I'll run a regression analysis to measure marketing effectiveness...\"*\n\n**Result:** *\"Every $1 spent on marketing generates $4.70 in sales. The relationship is highly significant (p < 0.001) with R¬≤ = 0.979\"*\n\n### Economic Research\n**You:** *\"Test if GDP growth and unemployment follow Okun's Law using my country data\"*\n\n**Claude:** *\"I'll analyze the correlation between GDP growth and unemployment...\"*\n\n**Result:** *\"Strong support for Okun's Law: correlation r = -0.944. Higher GDP growth significantly reduces unemployment.\"*\n\n### Customer Analytics  \n**You:** *\"Predict customer churn using tenure and monthly charges\"*\n\n**Claude:** *\"I'll build a logistic regression model for churn prediction...\"*\n\n**Result:** *\"Model achieves 100% accuracy. Each additional month of tenure reduces churn risk by 11.3%. Higher charges increase churn risk by 3% per dollar.\"*\n\n## üì¶ Installation\n\n### Prerequisites\n- **Python 3.10+** \n- **R 4.0+** with packages: Install all at once:\n\n```r\ninstall.packages(c(\n  \"jsonlite\", \"plm\", \"lmtest\", \"sandwich\", \"AER\", \"dplyr\",\n  \"forecast\", \"vars\", \"urca\", \"tseries\", \"nortest\", \"car\",\n  \"rpart\", \"randomForest\", \"ggplot2\", \"gridExtra\", \"tidyr\", \n  \"rlang\", \"knitr\", \"broom\"\n))\n```\n\n### Install RMCP\n\n```bash\n# Standard installation\npip install rmcp\n\n# With HTTP transport support\npip install rmcp[http]\n\n# Development installation\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n```\n\n### Claude Desktop Integration\n\nAdd to your Claude Desktop MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"rmcp\": {\n      \"command\": \"rmcp\",\n      \"args\": [\"start\"]\n    }\n  }\n}\n```\n\n### Command Line Usage\n\n```bash\n# Start MCP server (for Claude Desktop)\nrmcp start\n\n# Start HTTP server (for web apps)\nrmcp serve-http --port 8080\n\n# Check installation\nrmcp --version\n```\n\n## üî• Key Features\n\n- **üéØ Natural Conversation**: Ask questions in plain English, get statistical analysis\n- **üìä Professional Output**: Formatted results with markdown tables and inline visualizations  \n- **üîí Production Ready**: Full MCP protocol compliance with HTTP transport and SSE\n- **‚ö° Fast & Reliable**: 100% test success rate across all scenarios\n- **üåê Multiple Transports**: stdio (Claude Desktop) and HTTP (web applications)\n- **üõ°Ô∏è Secure**: Controlled R execution with configurable permissions\n\n## üìö Documentation\n\n| Resource | Description |\n|----------|-------------|\n| **[Quick Start Guide](examples/quick_start_guide.md)** | Copy-paste ready examples with real data |\n| **[Economic Research Examples](examples/economic_research_example.md)** | Panel data, time series, advanced econometrics |\n| **[Time Series Examples](examples/advanced_time_series_example.md)** | ARIMA, forecasting, decomposition |\n| **[Image Display Examples](examples/image_display_example.md)** | Inline visualizations in Claude |\n| **[API Documentation](docs/)** | Auto-generated API reference |\n\n## üß™ Validation\n\nRMCP has been tested with real-world scenarios achieving **100% success rate**:\n\n- ‚úÖ **Business Analysts**: Sales forecasting with 97.9% R¬≤, $4.70 ROI per marketing dollar\n- ‚úÖ **Economists**: Macroeconomic analysis confirming Okun's Law (r=-0.944)  \n- ‚úÖ **Data Scientists**: Customer churn prediction with 100% accuracy\n- ‚úÖ **Researchers**: Treatment effect analysis with significant results (p<0.001)\n\n## ü§ù Contributing\n\nWe welcome contributions! \n\n```bash\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n\n# Run tests\npython tests/unit/test_new_tools.py\npython tests/e2e/test_claude_desktop_scenarios.py\n\n# Format code\nblack rmcp/\n```\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## üõ†Ô∏è Quick Troubleshooting\n\n**R not found?**\n```bash\n# macOS: brew install r\n# Ubuntu: sudo apt install r-base\nR --version\n```\n\n**Missing R packages?**\n```bash\nrmcp check-r-packages  # Check what's missing\n```\n\n**MCP connection issues?**\n```bash\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | rmcp start\n```\n\n**üìñ Need more help?** Check the [examples](examples/) directory for working code.\n\n## üôã Support\n\n- üêõ **Issues**: [GitHub Issues](https://github.com/finite-sample/rmcp/issues)\n- üí¨ **Discussions**: [GitHub Discussions](https://github.com/finite-sample/rmcp/discussions)\n- üìñ **Examples**: [Working examples](examples/quick_start_guide.md)\n\n---\n\n**Ready to turn conversations into statistical insights?** Install RMCP and start analyzing data through AI assistants today! üöÄ\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "econometric",
        "analyses",
        "rmcp",
        "econometric analyses",
        "advanced econometric",
        "statistical analyses"
      ],
      "category": "scientific-research-tools"
    },
    "gp-zhong--Physics": {
      "owner": "gp-zhong",
      "name": "Physics",
      "url": "https://github.com/gp-zhong/Physics",
      "imageUrl": "/freedevtools/mcp/pfp/gp-zhong.webp",
      "description": "A high-fidelity prototype for learning physics concepts through interactive 3D simulations and AI-assisted learning. It offers a data visualization feature to help students understand physical experiments more intuitively.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "HTML",
      "updated_at": "2025-04-05T09:09:24Z",
      "readme_content": "# Áâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫è - È´ò‰øùÁúüÂéüÂûã\n\nËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é HTML„ÄÅCSS Âíå JavaScript ÂºÄÂèëÁöÑÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÁöÑÈ´ò‰øùÁúüÂéüÂûã„ÄÇÈÄöËøáÊ®°ÊãüÁúüÂÆûÁöÑÁâ©ÁêÜÂÆûÈ™åÂú∫ÊôØÔºåÂ∏ÆÂä©Áî®Êà∑ÈÄöËøá‰∫íÂä®ÂíåËßÜËßâÂåñÁöÑÊñπÂºèÂ≠¶‰π†Áâ©ÁêÜÁü•ËØÜ„ÄÇ\n\n## È°πÁõÆÊ¶ÇËø∞\n\nÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÊó®Âú®ÈÄöËøá‰∫§‰∫íÂºè 3D Ê®°ÊãüÂÆûÈ™åÔºåÂ∏ÆÂä©Â≠¶ÁîüÊõ¥Áõ¥ËßÇÂú∞ÁêÜËß£ÂíåÂ≠¶‰π†Áâ©ÁêÜÊ¶ÇÂøµ„ÄÇÂéüÂûãÈááÁî®Áé∞‰ª£ÁÆÄÁ∫¶ÁöÑËÆæËÆ°È£éÊ†ºÔºåÂº∫Ë∞ÉÁ∫øÊù°ÂíåÂá†‰ΩïÂΩ¢Áä∂ÔºåËê•ÈÄ†È´òÁ´ØÁßëÊäÄÊÑüÔºåÈÄÇÂêàÂêÑÂπ¥ÈæÑÊÆµÁöÑÂ≠¶‰π†ËÄÖ„ÄÇ\n\n## ËÆæËÆ°ËßÑËåÉ\n\n- **Êï¥‰ΩìÈ£éÊ†º**ÔºöÁé∞‰ª£ÁÆÄÁ∫¶ÁöÑÈªëÁôΩÁÅ∞ÈÖçËâ≤ÔºåÂº∫Ë∞ÉÁ∫øÊù°ÂíåÂá†‰ΩïÂΩ¢Áä∂\n- **ÈÖçËâ≤ÊñπÊ°à**Ôºö\n  - ‰∏ªËâ≤Ë∞ÉÔºöÂ§©Á©∫Ëìù (#4A90E2) ÂíåÁôΩËâ≤ (#FFFFFF)\n  - ËæÖÂä©Ëâ≤ÔºöÊµÖÁªøËâ≤ (#7ED321)\n  - ÁÇπÁºÄËâ≤ÔºöÈªÑËâ≤ (#F8E71C)\n  - ËÉåÊôØËâ≤ÔºöÊµÖÁÅ∞ (#F5F5F5)\n  - ÊñáÊú¨Ëâ≤ÔºöÊ∑±ÁÅ∞ (#333333)\n- **Â≠ó‰ΩìÈÄâÊã©**ÔºöÊÄùÊ∫êÈªë‰Ωì/Noto Sans SC\n- **ËßÜËßâÂÖÉÁ¥†**ÔºöÁÆÄÊ¥ÅÁ∫øÊù°ÂõæÊ†áÔºåÈ´òË¥®ÈáèÁâπÂÜô‰∫ßÂìÅÂõæÁâáÔºåÁÆÄÂçïÂá†‰ΩïÂõæÂΩ¢ËÉåÊôØ\n\n## Â¶Ç‰ΩïËøêË°åÂéüÂûã\n\n1. ÂÖãÈöÜÊàñ‰∏ãËΩΩÊú¨È°πÁõÆ‰ª£Á†Å\n2. ‰ΩøÁî®Áé∞‰ª£ÊµèËßàÂô®ÔºàÂ¶Ç Chrome„ÄÅFirefox„ÄÅSafariÔºâÊâìÂºÄ `index.html` Êñá‰ª∂\n3. ‰Ω†Â∞ÜÁúãÂà∞ÂåÖÂê´ÊâÄÊúâÂéüÂûãÈ°µÈù¢ÁöÑÈ¢ÑËßàÈ°µÈù¢ÔºåÊØè‰∏™È°µÈù¢ÊòæÁ§∫Âú® iPhone 15 Pro ÁöÑÊ®°ÊãüÊ°ÜÊû∂‰∏≠\n4. ÁÇπÂáªÂêÑ‰∏™È°µÈù¢ÂèØ‰ª•Êü•ÁúãËØ¶ÁªÜÁöÑ‰∫§‰∫íÂéüÂûã\n\n## ÂåÖÂê´ÁöÑÈ°µÈù¢\n\n1. **ÂêØÂä®È°µ**ÔºöÂ±ïÁ§∫Â∫îÁî®ÂêçÁß∞„ÄÅlogoÂíåÂºÄÂßãÊåâÈíÆ\n2. **È¶ñÈ°µ/ÂÆûÈ™åÂ∫ì**ÔºöÂ±ïÁ§∫ÂàÜÁ±ªÂÆûÈ™å„ÄÅÊé®ËçêÂÆûÈ™åÂíåÊúÄËøë‰ΩøÁî®\n3. **ÂÆûÈ™åËØ¶ÊÉÖÈ°µ**ÔºöÂ±ïÁ§∫ÂÆûÈ™å‰ø°ÊÅØ„ÄÅ‰ªãÁªçÂíåÁõ∏ÂÖ≥Áü•ËØÜÁÇπ\n4. **3DÂÆûÈ™åÊìç‰ΩúÁïåÈù¢**ÔºöËá™Áî±ËêΩ‰ΩìÂÆûÈ™åÊ®°ÊãüÔºåÂèØË∞ÉÊï¥ÂêÑÁßçÁâ©ÁêÜÂèÇÊï∞\n5. **ÂÆûÈ™åÊï∞ÊçÆÂàÜÊûêÈ°µÈù¢**ÔºöÂ±ïÁ§∫ÂÆûÈ™åÊï∞ÊçÆÁöÑÂõæË°®ÂíåË°®Ê†ºÂàÜÊûê\n6. **AIËæÖÂä©Â≠¶‰π†ÂØπËØùÁïåÈù¢**ÔºöÊèê‰æõÊô∫ËÉΩÈóÆÁ≠îÂäüËÉΩËß£ÈáäÁâ©ÁêÜÊ¶ÇÂøµ\n7. **Ê∏∏ÊàèÂåñÊàêÂ∞±Á≥ªÁªü**ÔºöÂ±ïÁ§∫Áî®Êà∑Â≠¶‰π†ËøõÂ∫¶„ÄÅÂããÁ´†ÂíåÊåëÊàò‰ªªÂä°\n8. **‰∏™‰∫∫‰∏≠ÂøÉÔºàÊàëÁöÑÔºâ**ÔºöÂ±ïÁ§∫Áî®Êà∑‰ø°ÊÅØ„ÄÅÂ≠¶‰π†ÁªüËÆ°„ÄÅÂÆûÈ™åÂéÜÂè≤ÂíåÂäüËÉΩÂÖ•Âè£\n\n## Ê†∏ÂøÉÂäüËÉΩ\n\n- **ÂÆûÈ™åÂ∫ì‰∏éÂàÜÁ±ªÁÆ°ÁêÜ**ÔºöÁõ¥ËßÇÁöÑÂç°ÁâáÁïåÈù¢ÔºåÊñπ‰æøÁî®Êà∑Êü•ÊâæÂÆûÈ™å\n- **3D‰∫§‰∫íÂºèÂÆûÈ™åÊ®°Êãü**ÔºöÂèÇÊï∞ÂèØË∞ÉÊï¥ÁöÑËá™Áî±ËêΩ‰ΩìÂÆûÈ™åÊ®°Êãü\n- **ÂÆûÈ™åÊïôÂ≠¶‰∏éÂºïÂØº**ÔºöËØ¶ÁªÜÁöÑÂÆûÈ™å‰ªãÁªç„ÄÅÁõ∏ÂÖ≥Áü•ËØÜÁÇπÂíåÊ≠•È™§ÂºïÂØº\n- **Ê∏∏ÊàèÂåñÂ≠¶‰π†Á≥ªÁªü**ÔºöÊàêÂ∞±Á≥ªÁªüÂíåÂããÁ´†Â±ïÁ§∫ÊøÄÂèëÂ≠¶‰π†Âä®Âäõ\n- **Êï∞ÊçÆÂèØËßÜÂåñ‰∏éÊä•ÂëäÁîüÊàê**ÔºöÂõæË°®ÂíåÊï∞ÊçÆË°®Ê†ºÂ±ïÁ§∫ÂÆûÈ™åÁªìÊûú\n- **AIËæÖÂä©Â≠¶‰π†**ÔºöÊô∫ËÉΩÈóÆÁ≠îËß£ÂÜ≥Áâ©ÁêÜÈóÆÈ¢ò\n- **Á§æ‰∫§‰∏éÂçè‰ΩúÂäüËÉΩ**ÔºöÊéíË°åÊ¶úÂíåÊàêÂ∞±ÂàÜ‰∫´ÂäüËÉΩ\n- **‰∏™‰∫∫‰∏≠ÂøÉÁÆ°ÁêÜ**ÔºöÊü•ÁúãÂ≠¶‰π†Êï∞ÊçÆ„ÄÅÂÆûÈ™åÂéÜÂè≤ÂíåÁ≥ªÁªüËÆæÁΩÆ\n\n## ÊäÄÊúØÊ†à\n\n- HTML5\n- CSS3Ôºà‰ΩøÁî® TailwindCSSÔºâ\n- JavaScript\n- Font Awesome ÂõæÊ†áÂ∫ì\n- Chart.js Êï∞ÊçÆÂèØËßÜÂåñÂ∫ì\n\n## ÂêéÁª≠ÂºÄÂèëÂª∫ËÆÆ\n\n1. ‰ΩøÁî® React Êàñ Vue Á≠âÂâçÁ´ØÊ°ÜÊû∂ÈáçÊûÑÁïåÈù¢ÔºåÊèêÂçá‰ª£Á†ÅÂ§çÁî®ÊÄßÂíåÁª¥Êä§ÊÄß\n2. ÈõÜÊàê Three.js Âíå Cannon.js Á≠âÁâ©ÁêÜÂºïÊìéÂÆûÁé∞ÁúüÂÆûÁöÑ 3D Ê®°Êãü\n3. ‰ΩøÁî® ECharts Êàñ D3.js Â¢ûÂº∫Êï∞ÊçÆÂèØËßÜÂåñËÉΩÂäõ\n4. Êé•ÂÖ•ÂÆûÈôÖÁöÑ AI Êé•Âè£ÂÆûÁé∞Êô∫ËÉΩÈóÆÁ≠îÂäüËÉΩ\n5. ‰ΩøÁî® WebSocket Êàñ WebRTC ÊäÄÊúØÂÆûÁé∞Â§ö‰∫∫Âçè‰ΩúÂäüËÉΩ\n\n## È¢ÑËßà\n\nÊâìÂºÄ `index.html` Êü•ÁúãÂÆåÊï¥ÁöÑÂéüÂûãÈ¢ÑËßà„ÄÇ\n\n## ‰ΩúËÄÖ\n\nÊú¨ÂéüÂûãÊòØÂü∫‰∫éÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÁöÑ‰Ωé‰øùÁúüÂéüÂûãËÆæËÆ°ÔºåÂçáÁ∫ß‰∏∫È´ò‰øùÁúüÂéüÂûãÁöÑËÆæËÆ°ÂÆûÁé∞„ÄÇ ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "physics",
        "simulations",
        "zhong",
        "zhong physics",
        "learning physics",
        "gp zhong"
      ],
      "category": "scientific-research-tools"
    },
    "gradusnikov--pubmed-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "pubmed-search-mcp-server",
      "url": "https://github.com/gradusnikov/pubmed-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Search and retrieve academic papers from the PubMed database, providing access to titles, authors, journals, abstracts, and DOIs.",
      "stars": 6,
      "forks": 5,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-07-08T04:10:26Z",
      "readme_content": "# PubMedSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/pubmed-search-mcp-server)](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server)\n\nA Model Content Protocol server that provides tools to search and retrieve academic papers from PubMed database.\n\n## Features\n\n- Search PubMed by keywords in title/abstract or author names\n- Retrieve detailed information including title, authors, journal, abstract, DOI and more\n\n### Installing via Smithery\n\nTo install pubmed-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/pubmed-search-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```\n   git clone <repository-url>\n   cd pubmed-search-mcp-server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests python-dotenv\n   ```\n\n3. Create a `.env` file in the project root (if needed for configuration)\n\n## Usage\n\n1. Start the server in development mode:\n   ```\n   mcp dev pubmed_search_mcp_server.py\n   ```\n\n2. or add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n   ```\n   \"pubmed-search\": {\n               \"command\": \"wsl.exe\",\n               \"args\": [\n                   \"bash\",\n                   \"-c\",\n                   \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && mcp run /home/[user]/pubmed-search-mpc-server/pubmed_search_mcp_server.py\"\n               ]\n           },\n   ```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "journals",
        "gradusnikov",
        "pubmed search",
        "gradusnikov pubmed",
        "pubmed database"
      ],
      "category": "scientific-research-tools"
    },
    "hbg--mcp-paperswithcode": {
      "owner": "hbg",
      "name": "mcp-paperswithcode",
      "url": "https://github.com/hbg/mcp-paperswithcode",
      "imageUrl": "/freedevtools/mcp/pfp/hbg.webp",
      "description": "Interface with the PapersWithCode API to retrieve information about research papers, authors, conferences, and methods. Utilize helper tools for enhancing the information processing and searching academic papers.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-09T01:30:55Z",
      "readme_content": "# mcp-paperswithcode\n\n[![smithery badge](https://smithery.ai/badge/@hbg/mcp-paperswithcode)](https://smithery.ai/server/@hbg/mcp-paperswithcode)\n\n# ü¶æ Features\n\n> Allows AI assistants to find and read papers, as well as view related code repositories for further context.\n\nThis MCP server provides a Model Context Protocol (MCP) client that interfaces with the PapersWithCode API.\n\nIt includes tools for searching, retrieving, and parsing information on research papers, authors, datasets, conferences, and more.\n\n# üöÄ Getting Started\n\n### Installing via Smithery\n\nTo install mcp-paperswithcode for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hbg/mcp-paperswithcode):\n\n```bash\nnpx -y @smithery/cli install @hbg/mcp-paperswithcode --client claude\n```\n\n# üõ†Ô∏è Tools\n\n## üìö Paper Tools\n\n### `search_papers`\nSearch for papers using optional filters.\n\n- `abstract` (str, optional): Filter by abstract text.\n- `title` (str, optional): Filter by title text.\n- `arxiv_id` (str, optional): Filter by ArXiv ID.\n\n### `get_paper`\nGet a paper's metadata by its ID.\n\n- `paper_id` (str): The paper ID.\n\n### `read_paper_from_url`\nExtract readable text from a paper given its URL.\n\n- `paper_url` (str): The direct PDF or HTML URL to a paper.\n\n### `list_paper_results`\nList benchmark results associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_tasks`\nList tasks associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_methods`\nList methods discussed in a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_repositories`\nList code repositories linked to a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_datasets`\nList datasets mentioned or used in a paper.\n\n- `paper_id` (str): The paper ID.\n\n## üß† Research Area Tools\n\n### `search_research_areas`\nSearch research areas by name.\n\n- `name` (str): Partial or full name of the research area.\n\n### `get_research_area`\nGet metadata for a specific research area.\n\n- `area_id` (str): The area ID.\n\n### `list_research_area_tasks`\nList tasks associated with a research area.\n\n- `area_id` (str): The area ID.\n\n## üë®‚Äçüî¨ Author Tools\n\n### `search_authors`\nSearch authors by full name.\n\n- `full_name` (str): Full name of the author.\n\n### `get_paper_author`\nGet metadata for an author by ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_id`\nList all papers written by an author via ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_name`\nSearch by name and return papers for the first matching author.\n\n- `author_name` (str): Full name of the author.\n\n## üéì Conference Tools\n\n### `list_conferences`\nList conferences, optionally filter by name.\n\n- `conference_name` (str, optional): Full or partial name.\n\n### `get_conference`\nGet metadata for a specific conference.\n\n- `conference_id` (str): The conference ID.\n\n### `list_conference_proceedings`\nList all proceedings under a conference.\n\n- `conference_id` (str): The conference ID.\n\n### `get_conference_proceeding`\nGet details for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n\n### `list_conference_papers`\nList all papers for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperswithcode",
        "papers",
        "searching",
        "mcp paperswithcode",
        "paperswithcode interface",
        "paperswithcode api"
      ],
      "category": "scientific-research-tools"
    },
    "huanongfish--arxiv-mcp": {
      "owner": "huanongfish",
      "name": "arxiv-mcp",
      "url": "https://github.com/huanongfish/arxiv-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/huanongfish.webp",
      "description": "Search and access arXiv research papers through a simple interface, facilitating the retrieval and download of academic content for analysis. Streamline research with local storage and efficient access to extensive academic resources.",
      "stars": 6,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-02T06:37:47Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Message Control Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pear Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "arxiv",
        "retrieval",
        "arxiv research",
        "research tools",
        "huanongfish arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "hugohow--mcp-music-analysis": {
      "owner": "hugohow",
      "name": "mcp-music-analysis",
      "url": "https://github.com/hugohow/mcp-music-analysis",
      "imageUrl": "/freedevtools/mcp/pfp/hugohow.webp",
      "description": "Analyze audio files to extract features such as beat, duration, and MFCCs, including support for audio from local files, YouTube links, or direct audio links.",
      "stars": 16,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T23:39:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hugohow-mcp-music-analysis-badge.png)](https://mseep.ai/app/hugohow-mcp-music-analysis)\n\n# MCP Music Analysis\n[![smithery badge](https://smithery.ai/badge/@hugohow/mcp-music-analysis)](https://smithery.ai/server/@hugohow/mcp-music-analysis)\n\nThis repository contains a **Model Context Provider (MCP)** that uses MCP and [librosa](https://librosa.org/) for audio analysis on audio in local, youtube link, or audio link.\n\n## Usage with Claude Desktop\n\n<div style=\"display: flex; gap: 1rem;\">\n  \n  \n</div>\n\n## Installation\n\n### Installing via Smithery\n\nTo install Music Analysis for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hugohow/mcp-music-analysis):\n\n```bash\nnpx -y @smithery/cli install @hugohow/mcp-music-analysis --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone repository\ngit clone git@github.com:hugohow/mcp-music-analysis.git\ncd mcp-music-analysis\n\n# Create virtual environment and install\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n```\n\n### Usage with Claude Desktop\n\n#### Locate Configuration File\n\nThe configuration file location depends on your operating system:\n\n- **macOS**:\n  ```\n  ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n- **Windows**:\n  ```\n  %APPDATA%\\Claude\\claude_desktop_config.json\n  ```\n\n- **Linux**:\n  ```\n  ~/.config/Claude/claude_desktop_config.json\n  ```\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"music-analysis\": {\n      \"command\": \"uvx\",\n      \"args\": [\"-n\", \"mcp-music-analysis\"]\n    }\n  }\n}\n```\n\n## Example Prompts\n\nHere are some sample prompts you might use in a conversational or chat-based context once the server is running. The MCP will understand these requests and execute the relevant tools:\n\n```\nCan you analyze the beat of /Users/hugohow-choong/Desktop/sample-6s.mp3?\nCould you give me the duration of https://download.samplelib.com/mp3/sample-15s.mp3 ?\nPlease compute the MFCC for this file: /path/to/another_audio.mp3\nWhat are the spectral centroid values for /path/to/music.wav?\nI'd like to know the onset times for https://www.youtube.com/watch?v=8HFiFd9vx1c\n```\n\n## To-Do List\n\n- [x] Add URL to audio file download\n- [x] Add YouTube to audio file transformation\n- [ ] Experiment with multiple Python environments (testing)\n- [ ] Improve installation guide\n- [ ] Integrate Whisper for lyrics\n- [ ] Implement a Docker solution\n\n## Author\n\nHugo How-Choong",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "mfccs",
        "audio",
        "mcp music",
        "analyze audio",
        "hugohow mcp"
      ],
      "category": "scientific-research-tools"
    },
    "iamleoluo--Quantum-Computing-Approach": {
      "owner": "iamleoluo",
      "name": "Quantum-Computing-Approach",
      "url": "https://github.com/iamleoluo/Quantum-Computing-Approach",
      "imageUrl": "/freedevtools/mcp/pfp/iamleoluo.webp",
      "description": "Simulate classical physics phenomena using quantum computing techniques and tools like Qiskit. Provides interactive visualizations and insights into complex systems such as superconductivity and magnetism.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2024-11-17T09:32:14Z",
      "readme_content": "# Quantum Computing Approach\n\n**Author:** Sheng Yun Wu  \n\n**Converted by:** Wei Che Lo \n\n**Course Website** [https://sites.google.com/gms.ndhu.edu.tw/qca](https://sites.google.com/gms.ndhu.edu.tw/qca)\n\n## Course Overview\n\nThe **Quantum Computing Approach** course introduces the application of quantum computing to simulate classical physics topics using qubits and quantum gates. It provides precise simulations of phenomena such as mechanics, thermodynamics, magnetism, and phase transitions.\n\n### Course Structure:\n- **Duration:** One to two semesters (12‚Äì15 weeks)\n- **Prerequisites:** Basic linear algebra, probability theory, and classical computing concepts\n- **Tools:** IBM Qiskit, Python\n\n## Introduction\n\nQuantum computing offers a revolutionary method for simulating and understanding classical physics topics by utilizing quantum mechanical principles like superposition, entanglement, and quantum interference. Classical physics often describes phenomena such as magnetism, thermodynamics, and phase transitions using deterministic models. Quantum computing, however, allows us to model these systems with unprecedented precision, capturing their quantum nature through qubits and quantum gates.\n\nThis approach allows quantum computers to simulate:\n- Spin systems\n- Wave-particle duality\n- Quantum tunneling\n- Quantum phase transitions\n- Superconductivity\n- Magnetism\n\nUsing frameworks like Qiskit, quantum computing provides tools for interactive simulations, enabling deeper insights into both classical and quantum phenomena.\n\n## Features\n\n- **Quantum Simulations**: Use quantum circuits to simulate classical physics phenomena.\n- **Interactive Visualizations**: Leverage Qiskit to visualize the evolution of quantum systems.\n- **Real-World Physics Applications**: Explore complex systems like superconductivity and magnetism with quantum models.\n\n## Installation\n\nTo set up the project, follow these steps:\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/iamleoluo/Quantum-Computing-Approach.git\n    ```\n2. Install the required dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n## Usage\n\nWorking on it, please wait.\n\n## Addition Note for Qiskit 1.0 feature changes\nQiskit 1.0 brings new stability guarantees and performance improvements, but it does break compatibility with all previous versions of Qiskit.\nfor more information please check out https://docs.quantum.ibm.com/migration-guides/qiskit-1.0-features\n\n## Contact\n\nFor more information, contact the **Modern NanoMag Lab**, founded by Professor Sheng Yun Wu:\n\n**Address:**  \nDepartment of Physics, National Dong Hwa University  \n1-12, Sec 2, Da-Hsuen Rd., Shou-Feng, Hualien, 974 Taiwan\n\n**Phone:** +886-3-8903732  \n**Fax:** +886-3-8900166  \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "qiskit",
        "computing",
        "quantum computing",
        "iamleoluo quantum",
        "qiskit provides"
      ],
      "category": "scientific-research-tools"
    },
    "jayanthbagare--mcp-scholarly": {
      "owner": "jayanthbagare",
      "name": "mcp-scholarly",
      "url": "https://github.com/jayanthbagare/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/jayanthbagare.webp",
      "description": "Search for accurate academic articles by keyword to facilitate research workflows. Integrates with applications to provide seamless retrieval of scholarly papers.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-05-06T10:07:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/adityak74-mcp-scholarly-badge.png)](https://mseep.ai/app/adityak74-mcp-scholarly)\n\n# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-202551](https://github.com/user-attachments/assets/f22d5796-017c-4c53-b230-101a09a28118)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "search",
        "retrieval scholarly",
        "scholarly search",
        "scholarly papers"
      ],
      "category": "scientific-research-tools"
    },
    "jinsoo--DDA_julia.jl": {
      "owner": "jinsoo",
      "name": "DDA_julia.jl",
      "url": "https://github.com/jinsoo/DDA_julia.jl",
      "imageUrl": "/freedevtools/mcp/pfp/jinsoo.webp",
      "description": "Perform scattering calculations using the discrete-dipole approximation for various geometrical shapes, including spheres. Utilize wave parameters to analyze scattering effects based on defined objects and their properties.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Julia",
      "updated_at": "2023-09-18T12:48:13Z",
      "readme_content": "# JuliaDDA\n\n[![Build Status](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml?query=branch%3Amain)\n\n## Usage\n\nThis package is based on the reference \n\n*Draine, B. T., & Flatau, P. J. (1994). Discrete-dipole approximation for scattering calculations. Josa a, 11(4), 1491-1499.*\n\n# Testing\n\n    using JuliaDDA\n    using StaticArrays\n\n    # Wave parameters\n    k = 1\n    ka = 7\n    a = ka / k\n\n    # Define Container for load objects \n    C = Container(k)\n\n    # Generate Sphere object \n    Object = Sphere(a, 20, 2+1.5im, 1)\n    # You can also Rotate or Pan the Object\n    # Ojbect = Rotate(Object, pi/4, nÃÇ=SA[0, 0, 1])\n\n    # plane wave\n    # propagation axis :z\n    # polarization axis : x\n    Src = PlaneWave(SA[0, 0, 1], SA[1, 0, 0]) \n\n    # Define Recorder to save Electric field at certain position\n    Rec = SphericalRecorder(50, 50, R=100, mode=\"full\")\n\n    # push object to the container\n    push!(C, Object)\n\n    # Calculate the incident field\n    CalEinc(C, Src)\n\n    # Calculate the Polarization of the dipoles\n    CalPolarization(C)\n\n    # Calculate the Farfield at recorder positions\n    CalFarField(C, Rec, \"sca\")\n\n    Volume = 4/3 * pi * a^3\n    Po_in = œµ0 * Volume \n\n    # Plot scatteres in the Container (backend : PlotlyJS)\n    PlotScatterers(C, :Px, plot_mode=:real)\n\n    # Plot Far field power\n    Plot3DPower(Rec, log=true, Po_in = Po_in)\n\n    # Plot DifferentialCrossSection (not accurate yet)\n    PlotDifferentialCrossSection(C, 50, 50, scale=\"log\", r=100)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scattering",
        "spheres",
        "dipole",
        "scattering calculations",
        "scattering effects",
        "analyze scattering"
      ],
      "category": "scientific-research-tools"
    },
    "jkingsman--qanon-mcp-server": {
      "owner": "jkingsman",
      "name": "qanon-mcp-server",
      "url": "https://github.com/jkingsman/qanon-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jkingsman.webp",
      "description": "Provides access to a dataset of Q-Anon posts for research purposes, enabling users to search, filter, and analyze the content for sociological insights.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-20T16:40:02Z",
      "readme_content": "## QAnon is a dangerous cult. This archive is for research purposes only, and I do _not_ endorse any material in this repo.\r\n\r\n# Q-Anon Posts/Drops MCP Server\r\n\r\n![](https://badge.mcpx.dev?type=server 'qanon-mcp')\r\n[![smithery badge](https://smithery.ai/badge/@jkingsman/qanon-mcp-server)](https://smithery.ai/server/@jkingsman/qanon-mcp-server)\r\n\r\nAn MCP (Model Context Protocol) server that provides access to a dataset of Q-Anon posts for anthropological/sociological research. This server allows AI assistants like Claude to search, filter, and analyze the Q-Anon drops.\r\n\r\nPosts are drawn from https://github.com/jkingsman/JSON-QAnon. You can learn more about how the source data was composed there, as well as find alternate formats, schemas, etc.\r\n\r\n### Warning: This tool was entirely vibe coded. Use at your own risk.\r\n\r\n## Prerequisites\r\n\r\n- Python 3.10 or higher\r\n- `uv` package manager\r\n- Claude Desktop (for Claude integration)\r\n\r\n## Installation\r\n\r\nThis tool is compatible with `uvx` and doesn't need to be cloned/installed.\r\n\r\n### Installing via Smithery\r\n\r\nTo install qanon-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jkingsman/qanon-mcp-server):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @jkingsman/qanon-mcp-server --client claude\r\n```\r\n\r\n### Manual\r\n\r\n1. Clone or download this repository to your local machine\r\n2. Install the required packages using `uv`:\r\n\r\n```bash\r\nuv pip install -e .\r\n```\r\n\r\n## Usage\r\n\r\nYou can run the server directly with `uvx`:\r\n\r\n```bash\r\nuvx qanon_mcp\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nTo use this MCP server with Claude Desktop:\r\n\r\n1. Make sure you have [Claude Desktop](https://claude.ai/download) installed\r\n2. Open the Claude menu and select \"Settings...\"\r\n3. Click on \"Developer\" in the left-hand bar and then \"Edit Config\"\r\n4. Add the following configuration to the `claude_desktop_config.json` file:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nor, if you don't have `uvx` installed:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"tool\",\r\n        \"run\",\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n5. Save the file and restart Claude Desktop\r\n6. Start a new conversation in Claude Desktop\r\n7. You should see a hammer icon in the input box, indicating that tools are available\r\n\r\n## Features\r\n\r\n### Resources\r\n\r\n- `qanon://posts/count` - Get the total number of posts\r\n- `qanon://posts/{post_id}` - Access a specific post by ID\r\n- `qanon://posts/raw/{post_id}` - Get the raw JSON data for a specific post\r\n- `qanon://authors` - List all unique authors\r\n- `qanon://stats` - Get dataset statistics\r\n\r\n### Tools\r\n\r\n- **get_post_by_id_tool** - Retrieve a specific post by its ID\r\n- **search_posts** - Find posts containing specific keywords or phrases\r\n- **get_posts_by_date** - Retrieve posts from a specific date range\r\n- **get_posts_by_author_id** - Find posts by a specific author ID\r\n- **analyze_post** - Get detailed analysis of a specific post including references and context\r\n- **get_timeline_summary** - Generate a chronological timeline, optionally within a date range\r\n- **word_cloud_by_post_ids** - Generate a word frequency analysis for posts within a specified ID range\r\n- **word_cloud_by_date_range** - Generate a word frequency analysis for posts within a specified date range\r\n\r\n## Example Queries for Claude\r\n\r\nOnce the MCP server is connected to Claude Desktop, you can ask questions like:\r\n\r\n- \"How many Q-Anon posts are in the dataset?\"\r\n- \"Search for posts that mention 'storm'\"\r\n- \"Show me posts from October 2020\"\r\n- \"Analyze post #3725\"\r\n- \"Create a timeline of Q-Anon posts from 2018\"\r\n- \"Generate a word cloud for Q-Anon posts between January and March 2019\"\r\n- \"Get the raw data for post #4500\"\r\n- \"What are the most common words used in posts #1000-2000?\"\r\n\r\n## Troubleshooting\r\n\r\n- If Claude Desktop doesn't show the hammer icon, check your configuration and restart Claude Desktop\r\n- Ensure the `posts.json` file is in the same directory as the script\r\n- Check the output in the terminal for any error messages\r\n- Make sure you're using the absolute path to the script in your Claude Desktop configuration\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sociological",
        "research",
        "qanon",
        "posts research",
        "research tools",
        "content sociological"
      ],
      "category": "scientific-research-tools"
    },
    "leescot--pubmed-mcp-smithery": {
      "owner": "leescot",
      "name": "pubmed-mcp-smithery",
      "url": "https://github.com/leescot/pubmed-mcp-smithery",
      "imageUrl": "/freedevtools/mcp/pfp/leescot.webp",
      "description": "Search and retrieve academic papers from the PubMed database using enhanced tools such as MeSH term lookup, publication statistics, and structured PICO-based evidence searches.",
      "stars": 5,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-12T02:43:23Z",
      "readme_content": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "searches",
        "search",
        "pubmed database",
        "papers pubmed",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "mrexodia--ida-pro-mcp": {
      "owner": "mrexodia",
      "name": "ida-pro-mcp",
      "url": "https://github.com/mrexodia/ida-pro-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mrexodia.webp",
      "description": "Enables reverse engineering in IDA Pro through a simple integration that allows interaction with various MCP tools. Provides functionality for checking connection status, retrieving metadata, and accessing functions by name or address.",
      "stars": 3801,
      "forks": 382,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T11:29:25Z",
      "readme_content": "# IDA Pro MCP\r\n\r\nSimple [MCP Server](https://modelcontextprotocol.io/introduction) to allow vibe reversing in IDA Pro.\r\n\r\nhttps://github.com/user-attachments/assets/6ebeaa92-a9db-43fa-b756-eececce2aca0\r\n\r\nThe binaries and prompt for the video are available in the [mcp-reversing-dataset](https://github.com/mrexodia/mcp-reversing-dataset) repository.\r\n\r\nAvailable functionality:\r\n\r\n- `check_connection()`: Check if the IDA plugin is running.\r\n- `get_metadata()`: Get metadata about the current IDB.\r\n- `get_function_by_name(name)`: Get a function by its name.\r\n- `get_function_by_address(address)`: Get a function by its address.\r\n- `get_current_address()`: Get the address currently selected by the user.\r\n- `get_current_function()`: Get the function currently selected by the user.\r\n- `convert_number(text, size)`: Convert a number (decimal, hexadecimal) to different representations.\r\n- `list_functions(offset, count)`: List all functions in the database (paginated).\r\n- `list_globals_filter(offset, count, filter)`: List matching globals in the database (paginated, filtered).\r\n- `list_globals(offset, count)`: List all globals in the database (paginated).\r\n- `list_strings_filter(offset, count, filter)`: List matching strings in the database (paginated, filtered).\r\n- `list_strings(offset, count)`: List all strings in the database (paginated).\r\n- `list_local_types()`: List all Local types in the database.\r\n- `decompile_function(address)`: Decompile a function at the given address.\r\n- `disassemble_function(start_address)`: Get assembly code (address: instruction; comment) for a function.\r\n- `get_xrefs_to(address)`: Get all cross references to the given address.\r\n- `get_xrefs_to_field(struct_name, field_name)`: Get all cross references to a named struct field (member).\r\n- `get_entry_points()`: Get all entry points in the database.\r\n- `set_comment(address, comment)`: Set a comment for a given address in the function disassembly and pseudocode.\r\n- `rename_local_variable(function_address, old_name, new_name)`: Rename a local variable in a function.\r\n- `rename_global_variable(old_name, new_name)`: Rename a global variable.\r\n- `set_global_variable_type(variable_name, new_type)`: Set a global variable's type.\r\n- `rename_function(function_address, new_name)`: Rename a function.\r\n- `set_function_prototype(function_address, prototype)`: Set a function's prototype.\r\n- `declare_c_type(c_declaration)`: Create or update a local type from a C declaration.\r\n- `set_local_variable_type(function_address, variable_name, new_type)`: Set a local variable's type.\r\n\r\nUnsafe functions (`--unsafe` flag required):\r\n\r\n- `dbg_get_registers()`: Get all registers and their values. This function is only available when debugging.\r\n- `dbg_get_call_stack()`: Get the current call stack.\r\n- `dbg_list_breakpoints()`: List all breakpoints in the program.\r\n- `dbg_start_process()`: Start the debugger.\r\n- `dbg_exit_process()`: Exit the debugger.\r\n- `dbg_continue_process()`: Continue the debugger.\r\n- `dbg_run_to(address)`: Run the debugger to the specified address.\r\n- `dbg_set_breakpoint(address)`: Set a breakpoint at the specified address.\r\n- `dbg_delete_breakpoint(address)`: del a breakpoint at the specified address.\r\n- `dbg_enable_breakpoint(address, enable)`: Enable or disable a breakpoint at the specified address.\r\n\r\n## Prerequisites\r\n\r\n- [Python](https://www.python.org/downloads/) (**3.11 or higher**) \r\n  - Use `idapyswitch` to switch to the newest Python version\r\n- [IDA Pro](https://hex-rays.com/ida-pro) (8.3 or higher, 9 recommended), **IDA Free is not supported**\r\n- Supported MCP Client (pick one you like)\r\n  - [Cline](https://cline.bot)\r\n  - [Roo Code](https://roocode.com)\r\n  - [Claude](https://claude.ai/download)\r\n  - [Cursor](https://cursor.com)\r\n  - [VSCode Agent Mode](https://github.blog/news-insights/product-news/github-copilot-agent-mode-activated/)\r\n  - [Windsurf](https://windsurf.com)\r\n  - [Other MCP Clients](https://modelcontextprotocol.io/clients#example-clients): Run `ida-pro-mcp --config` to get the JSON config for your client.\r\n\r\n## Installation\r\n\r\nInstall the latest version of the IDA Pro MCP package:\r\n\r\n```sh\r\npip uninstall ida-pro-mcp\r\npip install https://github.com/mrexodia/ida-pro-mcp/archive/refs/heads/main.zip\r\n```\r\n\r\nConfigure the MCP servers and install the IDA Plugin:\r\n\r\n```\r\nida-pro-mcp --install\r\n```\r\n\r\n**Important**: Make sure you completely restart IDA/Visual Studio Code/Claude for the installation to take effect. Claude runs in the background and you need to quit it from the tray icon.\r\n\r\nhttps://github.com/user-attachments/assets/65ed3373-a187-4dd5-a807-425dca1d8ee9\r\n\r\n_Note_: You need to load a binary in IDA before the plugin menu will show up.\r\n\r\n## Prompt Engineering\r\n\r\nLLMs are prone to hallucinations and you need to be specific with your prompting. For reverse engineering the conversion between integers and bytes are especially problematic. Below is a minimal example prompt, feel free to start a discussion or open an issue if you have good results with a different prompt:\r\n\r\n> Your task is to analyze a crackme in IDA Pro. You can use the MCP tools to retrieve information. In general use the following strategy:\r\n> - Inspect the decompilation and add comments with your findings\r\n> - Rename variables to more sensible names\r\n> - Change the variable and argument types if necessary (especially pointer and array types)\r\n> - Change function names to be more descriptive\r\n> - If more details are necessary, disassemble the function and add comments with your findings\r\n> - NEVER convert number bases yourself. Use the convert_number MCP tool if needed!\r\n> - Do not attempt brute forcing, derive any solutions purely from the disassembly and simple python scripts\r\n> - Create a report.md with your findings and steps taken at the end\r\n> - When you find a solution, prompt to user for feedback with the password you found\r\n\r\nThis prompt was just the first experiment, please share if you found ways to improve the output!\r\n\r\nLive stream discussing prompting and showing some real-world malware analysis:\r\n\r\n[![0](https://img.youtube.com/vi/iFxNuk3kxhk/0.jpg)](https://www.youtube.com/watch?v=iFxNuk3kxhk)\r\n\r\n## Tips for Enhancing LLM Accuracy\r\n\r\nLarge Language Models (LLMs) are powerful tools, but they can sometimes struggle with complex mathematical calculations or exhibit \"hallucinations\" (making up facts). Make sure to tell the LLM to use the `conver_number` MCP and you might also need [math-mcp](https://github.com/EthanHenrickson/math-mcp) for certain operations.\r\n\r\nAnother thing to keep in mind is that LLMs will not perform well on obfuscated code. Before trying to use an LLM to solve the problem, take a look around the binary and spend some time (automatically) removing the following things:\r\n\r\n- String encryption\r\n- Import hashing\r\n- Control flow flattening\r\n- Code encryption\r\n- Anti-decompilation tricks\r\n\r\nYou should also use a tool like Lumina or FLIRT to try and resolve all the open source library code and the C++ STL, this will further improve the accuracy.\r\n\r\n## SSE Transport & Headless MCP\r\n\r\nYou can run an SSE server to connect to the user interface like this:\r\n\r\n```sh\r\nuv run ida-pro-mcp --transport http://127.0.0.1:8744/sse\r\n```\r\n\r\nAfter installing [`idalib`](https://docs.hex-rays.com/user-guide/idalib) you can also run a headless SSE server:\r\n\r\n```sh\r\nuv run idalib-mcp --host 127.0.0.1 --port 8745 path/to/executable\r\n```\r\n\r\n_Note_: The `idalib` feature was contributed by [Willi Ballenthin](https://github.com/williballenthin).\r\n\r\n## Manual Installation\r\n\r\n_Note_: This section is for LLMs and power users who need detailed installation instructions.\r\n\r\n<details>\r\n\r\n## Manual MCP Server Installation (Cline/Roo Code)\r\n\r\nTo install the MCP server yourself, follow these steps:\r\n\r\n1. Install [uv](https://github.com/astral-sh/uv) globally:\r\n   - Windows: `pip install uv`\r\n   - Linux/Mac: `curl -LsSf https://astral.sh/uv/install.sh | sh`\r\n2. Clone this repository, for this example `C:\\MCP\\ida-pro-mcp`.\r\n3. Navigate to the Cline/Roo Code _MCP Servers_ configuration (see screenshot).\r\n4. Click on the _Installed_ tab.\r\n5. Click on _Configure MCP Servers_, which will open `cline_mcp_settings.json`.\r\n6. Add the `ida-pro-mcp` server:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"github.com/mrexodia/ida-pro-mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"c:\\\\MCP\\\\ida-pro-mcp\",\r\n        \"run\",\r\n        \"server.py\",\r\n        \"--install-plugin\"\r\n      ],\r\n      \"timeout\": 1800,\r\n      \"disabled\": false\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo check if the connection works you can perform the following tool call:\r\n\r\n```\r\n<use_mcp_tool>\r\n<server_name>github.com/mrexodia/ida-pro-mcp</server_name>\r\n<tool_name>check_connection</tool_name>\r\n<arguments></arguments>\r\n</use_mcp_tool>\r\n```\r\n\r\n## IDA Plugin installation\r\n\r\nThe IDA Pro plugin will be installed automatically when the MCP server starts. If you disabled the `--install-plugin` option, use the following steps:\r\n\r\n1. Copy (**not move**) `src/ida_pro_mcp/mcp-plugin.py` in your plugins folder (`%appdata%\\Hex-Rays\\IDA Pro\\plugins` on Windows).\r\n2. Open an IDB and click `Edit -> Plugins -> MCP` to start the server.\r\n\r\n</details>\r\n\r\n## Comparison with other MCP servers\r\n\r\nThere are a few IDA Pro MCP servers floating around, but I created my own for a few reasons:\r\n\r\n1. Installation should be fully automated.\r\n2. The architecture of other plugins make it difficult to add new functionality quickly (too much boilerplate of unnecessary dependencies).\r\n3. Learning new technologies is fun!\r\n\r\nIf you want to check them out, here is a list (in the order I discovered them):\r\n\r\n- https://github.com/taida957789/ida-mcp-server-plugin (SSE protocol only, requires installing dependencies in IDAPython).\r\n- https://github.com/fdrechsler/mcp-server-idapro (MCP Server in TypeScript, excessive boilerplate required to add new functionality).\r\n- https://github.com/MxIris-Reverse-Engineering/ida-mcp-server (custom socket protocol, boilerplate).\r\n\r\nFeel free to open a PR to add your IDA Pro MCP server here.\r\n\r\n## Development\r\n\r\nAdding new features is a super easy and streamlined process. All you have to do is add a new `@jsonrpc` function to [`mcp-plugin.py`](https://github.com/mrexodia/ida-pro-mcp/blob/164df8cf4ae251cc9cc0f464591fa6df8e0d9df4/src/ida_pro_mcp/mcp-plugin.py#L406-L419) and your function will be available in the MCP server without any additional boilerplate! Below is a video where I add the `get_metadata` function in less than 2 minutes (including testing):\r\n\r\nhttps://github.com/user-attachments/assets/951de823-88ea-4235-adcb-9257e316ae64\r\n\r\nTo test the MCP server itself:\r\n\r\n```sh\r\nuv run mcp dev src/ida_pro_mcp/server.py\r\n```\r\n\r\nThis will open a web interface at http://localhost:5173 and allow you to interact with the MCP tools for testing.\r\n\r\nFor testing I create a symbolic link to the IDA plugin and then POST a JSON-RPC request directly to `http://localhost:13337/mcp`. After [enabling symbolic links](https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) you can run the following command:\r\n\r\n```sh\r\nuv run ida-pro-mcp --install\r\n```\r\n\r\nGenerate the changelog of direct commits to `main`:\r\n\r\n```sh\r\ngit log --first-parent --no-merges 1.2.0..main \"--pretty=- %s\"\r\n```\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "mrexodia",
        "tools",
        "mrexodia ida",
        "mcp tools",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "noahzeidenberg--ncbi-mcp": {
      "owner": "noahzeidenberg",
      "name": "ncbi-mcp",
      "url": "https://github.com/noahzeidenberg/ncbi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/noahzeidenberg.webp",
      "description": "Query and analyze NCBI Entrez databases, including PubMed, Gene, and Protein, to retrieve detailed gene information and summaries. Facilitate exploration of gene relationships and integrate with bioinformatics workflows.",
      "stars": 3,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-24T16:47:25Z",
      "readme_content": "# NCBI Model Context Protocol (MCP)\n\nA Python implementation of the Model Context Protocol for interacting with NCBI databases.\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n3. Create a `.env` file with your NCBI API key:\n   ```\n   NCBI_API_KEY=your_api_key_here\n   NCBI_EMAIL=your_email@example.com\n   ```\n\n## Running the MCP Server\n\n```\npython ncbi_mcp.py\n```\n\n## Using with Cursor/Claude\n\nOnce the MCP server is running, you can interact with it using natural language in Cursor/Claude.\n\n### Using Natural Language Queries\n\nYou can use natural language to perform searches and retrieve information:\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Find research articles about BRCA1\"\n  }\n}\n```\n\nOr more simply, just use the query directly:\n\n```\n@ncbi-mcp Find research articles about BRCA1\n```\n\n### Example Natural Language Queries\n\nHere are some example natural language queries you can try:\n\n1. Gene function information:\n   ```\n   @ncbi-mcp Please summarize the function of TNF-alpha\n   ```\n\n2. Genome size and statistics:\n   ```\n   @ncbi-mcp How big is the genome for Saccharomyces cerevisiae?\n   ```\n\n3. Assembly statistics:\n   ```\n   @ncbi-mcp What is the reported L50 and N50 statistics for the most recent E.coli genome?\n   ```\n\n4. Dataset counts:\n   ```\n   @ncbi-mcp How many datasets are available in the biosample database for b16f10 mouse melanoma cells?\n   ```\n\n5. Search for scientific articles:\n   ```\n   @ncbi-mcp Find the latest research on COVID-19 vaccines\n   ```\n\n6. Get gene information:\n   ```\n   @ncbi-mcp Tell me about the BRCA1 gene\n   ```\n\n7. Fetch genome information:\n   ```\n   @ncbi-mcp Get genome information for Homo sapiens\n   ```\n\n## Testing\n\nTo test the MCP server with various queries, you can use the included test files:\n\n```\n# Test natural language query functionality (default)\n.\\run_test.bat\n\n# Test all tools\n.\\run_test.bat all\n\n# Test specific test file\n.\\run_test.bat test_all_tools.jsonl\n\n# Test high-level tools\n.\\run_test.bat test_high_level_tools.jsonl\n```\n\nThe test script will:\n1. Start the MCP server in background\n2. Send test requests from the specified file\n3. Wait for a few seconds to allow processing\n4. Terminate the server and display the output\n\nThis approach is used because the MCP server is designed to run continuously as a service. For manual testing without automatic termination, you can use:\n\n```\n# Run manually with any test file\ntype test_nlp_query.jsonl | python ncbi_mcp.py\n```\n\nThe test files contain example JSON-RPC requests that simulate how Cursor/Claude would interact with the MCP server.\n\n## Available Tools\n\nThe NCBI MCP provides both high-level tools that understand natural language and low-level tools for direct database interaction.\n\n## Tool Usage Guidelines for LLMs\n\n### Recommended Workflow Patterns\n\n**For most biological queries, start with `nlp-query`** - it's the most intelligent tool that can handle complex questions and automatically route to appropriate specialized tools.\n\n**Common Research Workflows:**\n\n1. **Gene Analysis Workflow:**\n   - Start with `nlp-query` for general gene questions\n   - Use `summarize-gene` for comprehensive gene information\n   - Use `get_gene_info` for detailed structured data\n   - Use `ncbi-search` + `ncbi-fetch` for specific database queries\n\n2. **Genome Analysis Workflow:**\n   - Use `genome-stats` for organism genome statistics\n   - Use `get_genome_info` for detailed genome metadata\n   - Use `count-datasets` to explore available genome assemblies\n\n3. **Literature Research Workflow:**\n   - Use `nlp-query` for natural language literature searches\n   - Use `ncbi-search` with database=\"pubmed\" for precise searches\n   - Use `ncbi-fetch` to get full publication details\n\n4. **Dataset Discovery Workflow:**\n   - Use `count-datasets` to assess data availability\n   - Use `nlp-query` to explore datasets with natural language\n   - Use `ncbi-search` for systematic database exploration\n\n5. **E-utilities Workflow (Advanced):**\n   - Use `ncbi-info` to discover available databases\n   - Use `ncbi-global-query` to see which databases contain your search term\n   - Use `ncbi-search` to find specific UIDs in target databases\n   - Use `ncbi-summary` to get overview information about records\n   - Use `ncbi-fetch` to retrieve complete records\n   - Use `ncbi-link` to find related records across databases\n\n6. **Cross-Database Analysis Workflow:**\n   - Use `ncbi-search` to find genes of interest\n   - Use `ncbi-link` to find related proteins, structures, or literature\n   - Use `ncbi-summary` to get metadata about related records\n   - Use `ncbi-fetch` to retrieve detailed information\n\n### Tool Selection Guide\n\n**High-Level Tools (Recommended for most users):**\n- **`nlp-query`**: Use for general biological questions, complex queries, and when you're unsure which tool to use\n- **`summarize-gene`**: Use for comprehensive gene analysis and understanding gene function\n- **`genome-stats`**: Use for genome size, assembly quality, and organism comparison\n- **`count-datasets`**: Use for research planning and data availability assessment\n- **`get_gene_info`**: Use for detailed, structured gene information\n- **`get_genome_info`**: Use for detailed, structured genome information\n\n**Low-Level E-utilities Tools (For advanced users):**\n- **`ncbi-search` (ESearch)**: Use for precise database searches with specific filters, Boolean operators, and field qualifiers\n- **`ncbi-fetch` (EFetch)**: Use to retrieve complete records after searching, supports multiple formats (GenBank, FASTA, XML)\n- **`ncbi-summary` (ESummary)**: Use to get document summaries without fetching complete records\n- **`ncbi-link` (ELink)**: Use to find related records across databases (e.g., gene to protein, protein to structure)\n- **`ncbi-info` (EInfo)**: Use to discover available databases and their capabilities\n- **`ncbi-global-query` (EGQuery)**: Use to search across all databases simultaneously\n- **`ncbi-spell` (ESpell)**: Use to get spelling suggestions for search terms\n- **`ncbi-citation-match` (ECitMatch)**: Use to find PMIDs from citation information\n\n### Biological Context and Terminology\n\n**Understanding NCBI Databases:**\n- **Gene**: Contains gene records with symbols, names, functions, and genomic locations\n- **Protein**: Contains protein sequences and annotations\n- **Nucleotide**: Contains DNA/RNA sequences (genes, transcripts, genomic regions)\n- **PubMed**: Contains scientific literature and publications\n- **BioSample**: Contains biological sample metadata (tissues, cell lines, etc.)\n- **BioProject**: Contains research project information\n- **SRA**: Contains raw sequencing data\n- **Assembly**: Contains genome assembly information\n\n**Common Biological Terms:**\n- **Gene Symbol**: Short abbreviation (e.g., BRCA1, TP53, TNF)\n- **Gene ID**: Unique NCBI identifier (e.g., 672 for BRCA1)\n- **Accession**: Unique sequence identifier (e.g., NM_001126114.3)\n- **N50/L50**: Assembly quality metrics (larger N50 = better assembly)\n- **Reference Genome**: High-quality representative genome for a species\n- **Organism**: Use scientific names (Homo sapiens) or common names (human)\n\n**Search Strategies:**\n- Use specific gene symbols for precise results\n- Include organism names to avoid ambiguity\n- Use Boolean operators (AND, OR, NOT) for complex searches\n- Use field qualifiers like [Gene], [Organism], [Protein Name] for targeted searches\n\n### High-Level Tools\n\n#### Natural Language Query Processor\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Please summarize the function of TNF-alpha\"\n  }\n}\n```\n\n#### Gene Summarizer\n\n```\ntools/call\n{\n  \"name\": \"summarize-gene\",\n  \"arguments\": {\n    \"gene_name\": \"BRCA1\"\n  }\n}\n```\n\n#### Genome Statistics\n\n```\ntools/call\n{\n  \"name\": \"genome-stats\",\n  \"arguments\": {\n    \"organism\": \"Escherichia coli\"\n  }\n}\n```\n\n#### Dataset Counter\n\n```\ntools/call\n{\n  \"name\": \"count-datasets\",\n  \"arguments\": {\n    \"database\": \"biosample\",\n    \"query\": \"mouse melanoma b16f10\"\n  }\n}\n```\n\n### Low-Level Tools\n\n#### Search NCBI Databases\n\n```\ntools/call\n{\n  \"name\": \"ncbi-search\",\n  \"arguments\": {\n    \"database\": \"pubmed\",\n    \"term\": \"BRCA1\",\n    \"filters\": {\n      \"organism\": \"Homo sapiens\",\n      \"date_range\": {\n        \"start\": \"2020\"\n      }\n    }\n  }\n}\n```\n\n#### Fetch NCBI Records\n\n```\ntools/call\n{\n  \"name\": \"ncbi-fetch\",\n  \"arguments\": {\n    \"database\": \"gene\",\n    \"ids\": [\"70\"],\n    \"rettype\": \"gb\"\n  }\n}\n```\n\n#### Get Gene Information\n\n```\ntools/call\n{\n  \"name\": \"get_gene_info\",\n  \"arguments\": {\n    \"gene_id\": \"672\"\n  }\n}\n```\n\n#### Get Genome Information\n\n```\ntools/call\n{\n  \"name\": \"get_genome_info\",\n  \"arguments\": {\n    \"organism\": \"Homo sapiens\",\n    \"reference\": true\n  }\n}\n```\n\n## License\n\nApache-2.0 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "ncbi",
        "gene",
        "noahzeidenberg ncbi",
        "analyze ncbi",
        "integrate bioinformatics"
      ],
      "category": "scientific-research-tools"
    },
    "olaservo--shannon-thinking": {
      "owner": "olaservo",
      "name": "shannon-thinking",
      "url": "https://github.com/olaservo/shannon-thinking",
      "imageUrl": "/freedevtools/mcp/pfp/olaservo.webp",
      "description": "Guides complex problem-solving by breaking down challenges into structured thoughts using a systematic methodology. Supports iterative revisions and empirical validation for engineering design and optimization tasks.",
      "stars": 43,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T21:33:51Z",
      "readme_content": "# shannon-thinking\n\nAn MCP server demonstrating Claude Shannon's systematic problem-solving methodology. This server provides a tool that helps break down complex problems into structured thoughts following Shannon's approach of problem definition, mathematical modeling, and practical implementation.\n\n<a href=\"https://glama.ai/mcp/servers/iffffhwqqw\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/iffffhwqqw/badge\" alt=\"Shannon Thinking Server MCP server\" />\n</a>\n\n## Overview\n\nClaude Shannon, known as the father of information theory, approached complex problems through a systematic methodology:\n\n1. **Problem Definition**: Strip the problem to its fundamental elements\n2. **Constraints**: Identify system limitations and boundaries\n3. **Model**: Develop mathematical/theoretical frameworks\n4. **Proof/Validation**: Validate through formal proofs or experimental testing\n5. **Implementation/Experiment**: Design and test practical solutions\n\nThis MCP server demonstrates this methodology as a tool that helps guide systematic problem-solving through these stages.\n\n## Installation\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"shannon-thinking\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"server-shannon-thinking@latest\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `shannonthinking` that structures problem-solving thoughts according to Shannon's methodology.\n\nEach thought must include:\n- The actual thought content\n- Type (problem_definition/constraints/model/proof/implementation)\n- Thought number and total thoughts estimate\n- Confidence level (uncertainty: 0-1)\n- Dependencies on previous thoughts\n- Explicit assumptions\n- Whether another thought step is needed\n\nAdditional capabilities:\n- **Revision**: Thoughts can revise earlier steps as understanding evolves\n- **Recheck**: Mark steps that need re-examination with new information\n- **Experimental Validation**: Support for empirical testing alongside formal proofs\n- **Implementation Notes**: Practical constraints and proposed solutions\n\n### Example Usage\n\n```typescript\nconst thought = {\n  thought: \"The core problem can be defined as an information flow optimization\",\n  thoughtType: \"problem_definition\",\n  thoughtNumber: 1,\n  totalThoughts: 5,\n  uncertainty: 0.2,\n  dependencies: [],\n  assumptions: [\"System has finite capacity\", \"Information flow is continuous\"],\n  nextThoughtNeeded: true,\n  // Optional: Mark as revision of earlier definition\n  isRevision: false,\n  // Optional: Indicate step needs recheck\n  recheckStep: {\n    stepToRecheck: \"constraints\",\n    reason: \"New capacity limitations discovered\",\n    newInformation: \"System shows non-linear scaling\"\n  }\n};\n\n// Use with MCP client\nconst result = await client.callTool(\"shannonthinking\", thought);\n```\n\n## Features\n\n- **Iterative Problem-Solving**: Supports revisions and rechecks as understanding evolves\n- **Flexible Validation**: Combines formal proofs with experimental validation\n- **Dependency Tracking**: Explicitly tracks how thoughts build upon previous ones\n- **Assumption Management**: Requires clear documentation of assumptions\n- **Confidence Levels**: Quantifies uncertainty in each step\n- **Rich Feedback**: Formatted console output with color-coding, symbols, and validation results\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Watch mode during development\nnpm run watch\n```\n\n## Tool Schema\n\nThe tool accepts thoughts with the following structure:\n\n```typescript\ninterface ShannonThought {\n  thought: string;\n  thoughtType: \"problem_definition\" | \"constraints\" | \"model\" | \"proof\" | \"implementation\";\n  thoughtNumber: number;\n  totalThoughts: number;\n  uncertainty: number; // 0-1\n  dependencies: number[];\n  assumptions: string[];\n  nextThoughtNeeded: boolean;\n  \n  // Optional revision fields\n  isRevision?: boolean;\n  revisesThought?: number;\n  \n  // Optional recheck field\n  recheckStep?: {\n    stepToRecheck: ThoughtType;\n    reason: string;\n    newInformation?: string;\n  };\n  \n  // Optional validation fields\n  proofElements?: {\n    hypothesis: string;\n    validation: string;\n  };\n  experimentalElements?: {\n    testDescription: string;\n    results: string;\n    confidence: number; // 0-1\n    limitations: string[];\n  };\n  \n  // Optional implementation fields\n  implementationNotes?: {\n    practicalConstraints: string[];\n    proposedSolution: string;\n  };\n}\n```\n\n## When to Use\n\nThis thinking pattern is particularly valuable for:\n- Complex system analysis\n- Information processing problems\n- Engineering design challenges\n- Problems requiring theoretical frameworks\n- Optimization problems\n- Systems requiring practical implementation\n- Problems that need iterative refinement\n- Cases where experimental validation complements theory\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "thinking",
        "methodology",
        "challenges",
        "thinking guides",
        "challenges structured",
        "structured thoughts"
      ],
      "category": "scientific-research-tools"
    },
    "openags--paper-search-mcp": {
      "owner": "openags",
      "name": "paper-search-mcp",
      "url": "https://github.com/openags/paper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/openags.webp",
      "description": "Search and download academic papers from various sources such as arXiv, PubMed, bioRxiv, and Sci-Hub. Integrates with large language models to streamline access to scholarly articles and enhance research workflows.",
      "stars": 368,
      "forks": 57,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T06:37:54Z",
      "readme_content": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "pubmed",
        "research",
        "research tools",
        "scholarly articles",
        "access scholarly"
      ],
      "category": "scientific-research-tools"
    },
    "opensensor--bn_cline_mcp": {
      "owner": "opensensor",
      "name": "bn_cline_mcp",
      "url": "https://github.com/opensensor/bn_cline_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/opensensor.webp",
      "description": "Analyze binaries efficiently using Binary Ninja's powerful analysis capabilities through the MCP framework. Connect to Cline for enhanced functionality and seamless integration.",
      "stars": 4,
      "forks": 1,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-09-29T15:37:18Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/opensensor-bn-cline-mcp-badge.png)](https://mseep.ai/app/opensensor-bn-cline-mcp)\n\n# binary_ninja_cline_mcp\nAn MCP server for Cline that works with Binary Ninja (Personal License)\n\nThis repository contains an MCP server that allows Cline to analyze binaries using Binary Ninja.\nNote:  Not all files will be used, there is also prototype of using headless Binary Ninja but my \nlicense is Personal so I can't test it.\n\n## Setup\n\n1. Install the latest of Binary Ninja MCP Plugin https://github.com/fosdickio/binary_ninja_mcp\n2. Open your binary and start the MCP server from within Binary Ninja.\n3. Open a terminal and run python binary_ninja_mcp_http_server.py --port 8088\n4. Open another terminal and run `npm start`\n5. Open Cline and add the following tool:{\nExample:\n```\n{\n  \"mcpServers\": {\n    \"BN MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/home/matteius/binary_ninja_cline/bn_cline_mcp/binaryninja-mcp-bridge.js\"],\n      \"env\": {\n        \"BN_HTTP_SERVER\": \"http://localhost:8088\"\n      },\n      \"autoApprove\": [],\n      \"disabled\": false,\n      \"timeout\": 30\n    }\n  }\n}\n\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bn_cline_mcp",
        "binaries",
        "opensensor",
        "bn_cline_mcp analyze",
        "opensensor bn_cline_mcp",
        "analyze binaries"
      ],
      "category": "scientific-research-tools"
    },
    "prashalruchiranga--arxiv-mcp-server": {
      "owner": "prashalruchiranga",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/prashalruchiranga/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/prashalruchiranga.webp",
      "description": "Interact with the arXiv API to retrieve scholarly article metadata, download PDFs, and search for articles using natural language queries. Enhance research workflows by directly accessing arXiv content within a large language model environment.",
      "stars": 31,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T07:45:53Z",
      "readme_content": "# arXiv MCP Server\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io)\n[![Python](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@prashalruchiranga/arxiv-mcp-server)](https://smithery.ai/server/@prashalruchiranga/arxiv-mcp-server)\n\nA Model Context Protocol (MCP) server that enables interacting with the arXiv API using natural language.\n\n## Features\n- Retrieve metadata about scholarly articles hosted on arXiv.org\n- Download articles in PDF format to the local machine\n- Search arXiv database for a particular query\n- Retrieve articles and load them into a large language model (LLM) context\n\n## Tools\n- **get_article_url**\n    - Retrieve the URL of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **download_article**\n    - Download the article hosted on arXiv.org as a PDF file \n        - `title` (String): Article title\n- **load_article_to_context**\n    - Load the article hosted on arXiv.org into context of a LLM \n        - `title` (String): Article title\n- **get_details**\n    - Retrieve metadata of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **search_arxiv**\n    - Performs a search query on the arXiv API based on specified parameters and returns matching article metadata\n        - `all_fields` (String): General keyword search across all metadata fields\n        - `title` (String): Keyword(s) to search for within the titles of articles\n        - `author` (String): Author name(s) to filter results by\n        - `abstract` (String): Keyword(s) to search for within article abstracts\n        - `start` (int): Index of the first result to return\n\n## Setup\n\n### MacOS\n\nClone the repository\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Using Homebrew\nbrew install uv\n\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv/bin/activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n### Windows\n\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Use irm to download the script and execute it with iex\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\nClose and reopen the shell, then clone the repository.\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv\\Scripts\\activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n## Usage with Claude Desktop\nTo enable this integration, add the server configuration to your `claude_desktop_config.json` file. Make sure to create the file if it doesn‚Äôt exist.\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json` On Windows: `%APPDATA%/Roaming/Claude/claude_desktop_config.json`\n\n```\n{\n  \"mcpServers\": {\n    \"arxiv-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/arxiv-mcp-server/src/arxiv_server\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"DOWNLOAD_PATH\": \"/ABSOLUTE/PATH/TO/DOWNLOADS/FOLDER\"\n      }\n    }\n  }\n}\n```\n\nYou may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS or `where uv` on Windows.\n\n## Example Prompts\n```\nCan you get the details of 'Reasoning to Learn from Latent Thoughts' paper?\n```\n```\nGet the papers authored or co-authored by Yann Lecun on convolutional neural networks\n```\n```\nDownload the attention is all you need paper\n```\n```\nCan you get the papers by Andrew NG which have 'convolutional neural networks' in title?\n```\n```\nCan you display the paper?\n```\n```\nList the titles of papers by Yann LeCun. Paginate through the API until there are 30 titles\n```\n\n## License\n\nLicensed under MIT. See the [LICENSE](https://github.com/prashalruchiranga/arxiv-mcp-server/blob/main/LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "metadata",
        "arxiv",
        "retrieve scholarly",
        "research tools",
        "scholarly article"
      ],
      "category": "scientific-research-tools"
    },
    "reading-plus-ai--mcp-server-deep-research": {
      "owner": "reading-plus-ai",
      "name": "mcp-server-deep-research",
      "url": "https://github.com/reading-plus-ai/mcp-server-deep-research",
      "imageUrl": "/freedevtools/mcp/pfp/reading-plus-ai.webp",
      "description": "Conduct comprehensive research on complex topics by generating structured research reports and synthesizing information from diverse sources. This tool enables in-depth exploration of questions and identification of relevant sources.",
      "stars": 181,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T10:09:42Z",
      "readme_content": "# MCP Server for Deep Research\n\nMCP Server for Deep Research is a tool designed for conducting comprehensive research on complex topics. It helps you explore questions in depth, find relevant sources, and generate structured research reports.\n\nYour personal Research Assistant, turning research questions into comprehensive, well-cited reports.\n\n## üöÄ Try it Out\n\n[![Watch the demo](https://img.youtube.com/vi/_a7sfo5yxoI/maxresdefault.jpg)]([VIDEO_URL](https://youtu.be/_a7sfo5yxoI))\nYoutube: https://youtu.be/_a7sfo5yxoI\n\n1. **Download Claude Desktop**\n   - Get it [here](https://claude.ai/download)\n\n2. **Install and Set Up**\n   - On macOS, run the following command in your terminal:\n   ```bash\n   python setup.py\n   ```\n\n3. **Start Researching**\n   - Select the deep-research prompt template from MCP\n   - Begin your research by providing a research question\n\n## Features\n\nThe Deep Research MCP Server offers a complete research workflow:\n\n1. **Question Elaboration**\n   - Expands and clarifies your research question\n   - Identifies key terms and concepts\n   - Defines scope and parameters\n\n2. **Subquestion Generation**\n   - Creates focused subquestions that address different aspects\n   - Ensures comprehensive coverage of the main topic\n   - Provides structure for systematic research\n\n3. **Web Search Integration**\n   - Uses Claude's built-in web search capabilities\n   - Performs targeted searches for each subquestion\n   - Identifies relevant and authoritative sources\n   - Collects diverse perspectives on the topic\n\n4. **Content Analysis**\n   - Evaluates information quality and relevance\n   - Synthesizes findings from multiple sources\n   - Provides proper citations for all sources\n\n5. **Report Generation**\n   - Creates well-structured, comprehensive reports as artifacts\n   - Properly cites all sources used\n   - Presents a balanced view with evidence-based conclusions\n   - Uses appropriate formatting for clarity and readability\n\n## üì¶ Components\n\n### Prompts\n- **deep-research**: Tailored for comprehensive research tasks with a structured approach\n\n## ‚öôÔ∏è Modifying the Server\n\n### Claude Desktop Configurations\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Development (Unpublished Servers)\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/Users/username/repos/mcp-server-application/mcp-server-deep-research\",\n      \"run\",\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n### Published Servers\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n## üõ†Ô∏è Development\n\n### Building and Publishing\n1. **Sync Dependencies**\n   ```bash\n   uv sync\n   ```\n\n2. **Build Distributions**\n   ```bash\n   uv build\n   ```\n   Generates source and wheel distributions in the dist/ directory.\n\n3. **Publish to PyPI**\n   ```bash\n   uv publish\n   ```\n\n## ü§ù Contributing\n\nContributions are welcome! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n## üìú License\n\nThis project is licensed under the MIT License.\nSee the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "comprehensive",
        "topics",
        "deep research",
        "structured research",
        "research complex"
      ],
      "category": "scientific-research-tools"
    },
    "sirusb--samtools_mcp": {
      "owner": "sirusb",
      "name": "samtools_mcp",
      "url": "https://github.com/sirusb/samtools_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sirusb.webp",
      "description": "Provides tools for viewing, converting, sorting, indexing, and analyzing SAM/BAM/CRAM genomic files. Facilitates efficient bioinformatics workflows with region-specific analysis and statistics generation.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-12T12:36:19Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/sirusb-samtools-mcp-badge.png)](https://mseep.ai/app/sirusb-samtools-mcp)\n\n# SAMtools MCP (Model Control Protocol)\n\nA Model Control Protocol implementation for SAMtools, providing a standardized interface for working with SAM/BAM/CRAM files.\n\n## Features\n\n- View and convert SAM/BAM/CRAM files\n- Sort alignment files\n- Index BAM/CRAM files\n- Generate statistics\n- Merge multiple BAM files\n- Calculate read depth\n- Index FASTA files\n- And more...\n\n### Core Capabilities\n- **File Format Support**: Handle SAM (text), BAM (binary), and CRAM (compressed) alignment files\n- **Format Conversion**: Convert between SAM, BAM, and CRAM formats seamlessly\n- **Region-Specific Analysis**: Extract and analyze specific genomic regions\n- **Flag-Based Filtering**: Filter reads based on SAM flags\n- **Performance Optimization**: Multi-threaded operations for sorting and merging\n- **Statistical Analysis**: Generate comprehensive alignment statistics\n\n### Tools Overview\n\n| Tool | Description | Key Features |\n|------|-------------|--------------|\n| `view` | View and convert alignment files | - Format conversion (SAM/BAM/CRAM)<br>- Region filtering<br>- Flag-based filtering<br>- Header manipulation |\n| `sort` | Sort alignment files | - Coordinate-based sorting<br>- Name-based sorting<br>- Memory per thread control<br>- Multi-threading support |\n| `index` | Index BAM/CRAM files | - BAI index generation<br>- CSI index support<br>- CRAM index creation |\n| `merge` | Merge multiple BAM/CRAM files | - Multi-file merging<br>- Thread-enabled processing<br>- Header reconciliation |\n| `depth` | Calculate read depth | - Per-base depth calculation<br>- Region-specific analysis<br>- Multi-file support |\n| `flagstat` | Generate alignment statistics | - Comprehensive flag statistics<br>- Quality checks<br>- Paired-end metrics |\n| `idxstats` | BAM/CRAM index statistics | - Reference sequence stats<br>- Mapped/unmapped counts<br>- Length information |\n| `faidx` | Index FASTA files | - FASTA indexing<br>- Region extraction<br>- Sequence retrieval |\n\n\n## Installation\n\n### Using Docker (Recommended)\n\nThe easiest way to use SAMtools MCP is through Docker:\n\n```bash\n# Pull the Docker image\ndocker pull nadhir/samtools-mcp:latest\n\n# Run the container\ndocker run -it --rm nadhir/samtools-mcp:latest\n\n# To process BAM files, mount a volume:\ndocker run -it --rm -v /path/to/your/bam/files:/data nadhir/samtools-mcp:latest\n```\n\n### Local Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/your-username/samtools_mcp.git\ncd samtools_mcp\n```\n\n2. Install dependencies:\n```bash\npip install uv\nuv pip install -r requirements.txt\n```\n\n## Configuration\n\n### MCP Server Configuration\n\nTo configure the MCP server to use the Docker image, add the following to your MCP configuration file:\n\n```json\n{\n  \"servers\": {\n    \"samtools\": {\n      \"type\": \"docker\",\n      \"image\": \"nadhir/samtools-mcp:latest\",\n      \"volumes\": [\n        {\n          \"source\": \"/path/to/your/data\",\n          \"target\": \"/data\"\n        }\n      ]\n    }\n  }\n}\n```\n\n### Local MCP Configuration\n\nTo configure the MCP to run using `uv`, add the following to your `~/.cursor/mcp.json`:\n\n```json\n{\n  \"samtools_mcp\": {\n    \"command\": \"uv\",\n    \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"/path/to/samtools_mcp.py\"]\n  }\n}\n```\n\nReplace `/path/to/samtools_mcp.py` with the actual path to your `samtools_mcp.py` file.\n\n## Usage\n\n### Basic Commands\n\n1. View BAM file:\n```python\nfrom samtools_mcp import SamtoolsMCP\n\nmcp = SamtoolsMCP()\nresult = mcp.view(input_file=\"/data/example.bam\")\n```\n\n2. Sort BAM file:\n```python\nresult = mcp.sort(input_file=\"/data/example.bam\", output_file=\"/data/sorted.bam\")\n```\n\n3. Index BAM file:\n```python\nresult = mcp.index(input_file=\"/data/sorted.bam\")\n```\n\n### Advanced Usage\n\n1. View specific region with flags:\n```python\nresult = mcp.view(\n    input_file=\"/data/example.bam\",\n    region=\"chr1:1000-2000\",\n    flags_required=\"0x2\",\n    output_format=\"SAM\"\n)\n```\n\n2. Sort by read name:\n```python\nresult = mcp.sort(\n    input_file=\"/data/example.bam\",\n    output_file=\"/data/namesorted.bam\",\n    sort_by_name=True\n)\n```\n\n3. Calculate depth with multiple input files:\n```python\nresult = mcp.depth(\n    input_files=[\"/data/sample1.bam\", \"/data/sample2.bam\"],\n    region=\"chr1:1-1000000\"\n)\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "samtools_mcp",
        "genomic",
        "samtools_mcp provides",
        "cram genomic",
        "bioinformatics workflows"
      ],
      "category": "scientific-research-tools"
    },
    "szeider--mcp-solver": {
      "owner": "szeider",
      "name": "mcp-solver",
      "url": "https://github.com/szeider/mcp-solver",
      "imageUrl": "/freedevtools/mcp/pfp/szeider.webp",
      "description": "Integrates SAT, SMT, and constraint solving capabilities with Large Language Models, enabling interactive creation, editing, and solving of constraint models in MiniZinc, SAT models in PySAT, and SMT formulas using Z3 Python.",
      "stars": 133,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T09:11:19Z",
      "readme_content": "------\n\n# MCP Solver\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![Python Version](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)\n\nA Model Context Protocol (MCP) server that exposes constraint solving, SAT, SMT, and ASP capabilities to Large Language Models.\n\n------\n\n## Overview\n\nThe *MCP Solver* integrates constraint solving, SAT, SMT, and ASP with LLMs through the Model Context Protocol, enabling AI models to interactively create, edit, and solve:\n\n- Constraint models in [MiniZinc](https://www.minizinc.org/)\n- SAT models in [PySAT](https://pysathq.github.io/)\n- MaxSAT optimization problems in [PySAT](https://pysathq.github.io/)\n- SMT formulas in [Z3 Python](https://ericpony.github.io/z3py-tutorial/guide-examples.htm)\n- Answer Set Programs in [Clingo](https://potassco.org/clingo/)\n\nFor a detailed description of the *MCP Solver's* system architecture and theoretical foundations, see the accompanying research paper: Stefan Szeider, [\"Bridging Language Models and Symbolic Solvers via the Model Context Protocol\"](https://doi.org/10.4230/LIPIcs.SAT.2025.30), SAT 2025.\n\n## Available Tools\n\nIn the following, *item* refers to some part of the (MiniZinc/PySAT/Z3/ASP) code, and *model* to the encoding. \n\n| Tool Name      | Description                                   |\n| -------------- | --------------------------------------------- |\n| `clear_model`  | Remove all items from the model               |\n| `add_item`     | Add new item at a specific index              |\n| `delete_item`  | Delete item at index                          |\n| `replace_item` | Replace item at index                         |\n| `get_model`    | Get current model content with numbered items |\n| `solve_model`  | Solve the model (with timeout parameter)      |\n\n------\n\n## System Requirements\n\n- Python and project manager [uv](https://docs.astral.sh/uv/) \n- Python 3.11+\n- Mode-specific requirements: MiniZinc, PySAT, Python Z3 (required packages are installed via pip)\n- Operating systems: macOS, Windows, Linux (with appropriate adaptations)\n\n------\n\n## Installation\n\nMCP Solver requires Python 3.11+, the `uv` package manager, and solver-specific dependencies (MiniZinc, Z3, or PySAT).\n\nFor detailed installation instructions for Windows, macOS, and Linux, see [INSTALL.md](INSTALL.md).\n\nQuick start:\n```bash\ngit clone https://github.com/szeider/mcp-solver.git\ncd mcp-solver\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[all]\"  # Install all solvers\n```\n\n------\n\n## Available Modes / Solving Backends\n\nThe MCP Solver provides five distinct operational modes, each integrating with a different constraint solving backend. Each mode requires specific dependencies and offers unique capabilities for addressing different classes of problems.\n\n### MiniZinc Mode\n\nMiniZinc mode provides integration with the MiniZinc constraint modeling language with the following features:\n\n- Rich constraint expression with global constraints\n- Integration with the Chuffed constraint solver\n- Optimization capabilities\n- Access to solution values via `get_solution`\n\n**Dependencies**: Requires the `minizinc` package (`uv pip install -e \".[mzn]\"`)\n\n**Configuration**: To run in MiniZinc mode, use:\n\n```\nmcp-solver-mzn\n```\n\n### PySAT Mode\n\nPySAT mode allows interaction with the Python SAT solving toolkit with the following features:\n\n- Propositional constraint modeling using CNF (Conjunctive Normal Form)\n- Access to various SAT solvers (Glucose3, Glucose4, Lingeling, etc.)\n- Cardinality constraints (at_most_k, at_least_k, exactly_k)\n- Support for boolean constraint solving\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in PySAT mode, use:\n\n```\nmcp-solver-pysat\n```\n\n### MaxSAT Mode\n\nMaxSAT mode provides specialized support for optimization problems with PySAT, featuring:\n\n- Weighted Conjunctive Normal Form (WCNF) support\n- Integration with the RC2 MaxSAT solver\n- Optimization capabilities with objective tracking\n- Support for both hard and soft constraints\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in MaxSAT mode, use:\n\n```\nmcp-solver-maxsat\n```\n\n### Z3 Mode\n\nZ3 mode provides access to Z3 SMT (Satisfiability Modulo Theories) solving capabilities with the following features:\n\n- Rich type system: booleans, integers, reals, bitvectors, arrays\n- Constraint solving with quantifiers\n- Optimization capabilities\n- Template library for common modeling patterns\n\n**Dependencies**: Requires the `z3-solver` package (`uv pip install -e \".[z3]\"`)\n\n**Configuration**: To run in Z3 mode, use:\n\n```\nmcp-solver-z3\n```\n\n### ASP Mode\n\nASP (Answer Set Programming) mode provides integration with ASP solvers (e.g., Clingo) for declarative problem solving with logic programs. Features include:\n\n- Expressive logic programming for combinatorial and knowledge representation problems\n- Support for constraints, choice rules, aggregates, and optimization statements\n- Integration with the Clingo solver for efficient answer set computation\n- Access to answer sets and model inspection\n\n**Dependencies**: Requires the `clingo` package (`uv pip install -e \".[asp]\"` or included in `[all]`)\n\n**Configuration**: To run in ASP mode, use:\n\n```\nmcp-solver-asp\n```\n\n## MCP Test Client\n\nThe MCP Solver includes an MCP client for development, experimentation, and diagnostic purposes, based on the *ReAct* agent framework. This client serves as an intermediary between an LLM and the MCP server, facilitating the translation of natural language problem statements into formal constraint programming solutions. \n\n### Installation\n\n```bash\n# Install client dependencies\nuv pip install -e \".[client]\"\n\n# Verify client installation and configuration\nuv run test-setup-client\n```\n\nThe client requires an **API key** from an LLM provider. For Anthropic (the default LLM is Claude Sonnet 3.7), set the `ANTHROPIC_API_KEY` environment variable. This can be set in your environment or included in a `.env` file in the project root.\n\nThe client also supports other LLM providers including OpenAI, Google (Gemini), OpenRouter and even local models via LM Studio. You can specify which model to use with the `--mc` command line flag. See [INSTALL.md](INSTALL.md) for details on model code formats.\n\n### Usage\n\nThe client provides specialized commands for each solver backend:\n\n```bash\n# MiniZinc mode\nuv run run-test mzn --problem <path/to/problem.md>\n\n# PySAT mode\nuv run run-test pysat --problem <path/to/problem.md>\n\n# MaxSAT mode\nuv run run-test maxsat --problem <path/to/problem.md>\n\n# Z3 mode\nuv run run-test z3 --problem <path/to/problem.md>\n\n# ASP mode\nuv run run-test asp --problem <path/to/problem.md>\n```\n\n------\n\n## Examples\n\n### Example 1: Casting Problem (MiniZinc)\n\nThis is based on an example by George Johnson (NYT, July 13, 1999).\n\n**User**\n\n> Suppose that a theatrical director feels obligated to cast either his ingenue, Actress Alvarez, or his nephew, Actor Cohen, in a production. But Miss Alvarez won't be in a play with Mr. Cohen (her former lover), and she demands that the cast include her new flame, Actor Davenport. The producer, with her own favors to repay, insists that Actor Branislavsky have a part. But Mr. Branislavsky won't be in any play with Miss Alvarez or Mr. Davenport. Can the director cast the play?\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> var bool: alvarez;\n> var bool: cohen;\n> var bool: branislavsky;\n> var bool: davenport;\n> constraint alvarez \\/ cohen;                          % Must cast either Alvarez or Cohen\n> constraint not (alvarez /\\ cohen);                    % Alvarez won't work with Cohen\n> constraint alvarez -> davenport;                      % If Alvarez is cast, Davenport must be cast\n> constraint branislavsky;                              % Branislavsky must be cast\n> constraint not (branislavsky /\\ alvarez);             % Branislavsky won't work with Alvarez\n> constraint not (branislavsky /\\ davenport);           % Branislavsky won't work with Davenport\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(alvarez=False, cohen=True, branislavsky=True, davenport=False, _checker=''), 'solve_time': 0.0}\n> ```\n\nThe solution (cast Cohen and Branislavsky, but not Alvarez or Davenport) satisfies all constraints.\n\n### Example 2: N-Queens Problem (MiniZinc)\n\n**User**\n\n> Check whether you can place n Queens on an nxn chessboard. Try n=10,20,30,40 and compare the solving times\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> int: n = 10;\n> array[1..n] of var 1..n: queens;\n> constraint alldifferent(queens);\n> constraint alldifferent([queens[i] + i | i in 1..n]);\n> constraint alldifferent([queens[i] - i | i in 1..n]);\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`** (with n=10, 20, 30, and 40 by changing the value of n)\n\nSolve times comparison: n=10: 0.001s n=20: 0.005s n=30: 0.040s n=40: 0.043s\n\nSolutions exist for all tested board sizes. Time complexity increases non-linearly but remains manageable up to n=40.\n\n### Example 3: Traveling Salesperson Problem (MiniZinc)\n\n**User**\n\n> A saleswoman based in Vienna needs to plan her upcoming tour through Austria, visiting each province capital once. Help find the shortest route. Distances in km: 1=Vienna, 2=St. P√∂lten, 3=Eisenstadt, 4=Linz, 5=Graz, 6=Klagenfurt, 7=Salzburg, 8=Innsbruck, 9=Bregenz\n>\n> [Distance matrix table provided in user message]\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> include \"globals.mzn\";\n> int: n = 9;\n> array[1..n,1..n] of int: dist = [|0,65,60,184,195,319,299,478,631|65,0,125,119,130,254,234,413,566|60,125,0,184,157,281,261,440,593|184,119,184,0,208,252,136,315,468|195,130,157,208,0,136,280,459,629|319,254,281,252,136,0,217,391,566|299,234,261,136,280,217,0,188,343|478,413,440,315,459,391,188,0,157|631,566,593,468,629,566,343,157,0|];\n> array[1..n] of var 1..n: succ;\n> var int: total_dist = sum(i in 1..n)(dist[i,succ[i]]);\n> constraint circuit(succ);\n> solve minimize total_dist;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(objective=1564, succ=[2, 4, 1, 7, 3, 5, 9, 6, 8], _checker=''), 'solve_time': 0.002}\n> ```\n\nOptimal route: Vienna ‚Üí St. P√∂lten ‚Üí Linz ‚Üí Salzburg ‚Üí Bregenz ‚Üí Innsbruck ‚Üí Klagenfurt ‚Üí Graz ‚Üí Eisenstadt ‚Üí Vienna with a total distance of 1,564 km.\n\n## Feedback\n\nYou can provide feedback to the author via this [form](https://form.jotform.com/szeider/mcp-solver-feedback-form).\n\n## Disclaimer\n\nThis MCP Solver is in its prototype stage and should be used with caution. Users are encouraged to experiment, but any use in critical environments is at their own risk.\n\n------\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](https://claude.ai/chat/LICENSE) file for details.\n\n------\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "solver",
        "python",
        "pysat",
        "mcp solver",
        "smt constraint",
        "sat models"
      ],
      "category": "scientific-research-tools"
    },
    "t0mst0ne--pubmed-mcp-easy": {
      "owner": "t0mst0ne",
      "name": "pubmed-mcp-easy",
      "url": "https://github.com/t0mst0ne/pubmed-mcp-easy",
      "imageUrl": "/freedevtools/mcp/pfp/t0mst0ne.webp",
      "description": "Search and retrieve biomedical literature and research papers from PubMed. Supports advanced searches, batch querying, and retrieval of abstracts and full texts of open access articles.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-18T01:29:22Z",
      "readme_content": "# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@t0mst0ne/pubmed-mcp-easy)](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy)\n\nEnhanced Python-based PubMed MCP Server with API key support for faster and unlimited downloads.\n\n## Certified by MCP Review\n[https://mcpreview.com/mcp-servers/t0mst0ne/pubmed-mcp-easy]\n\n## Features\n\n- Search PubMed for biomedical literature and research papers\n- Find similar articles, cited articles, and citing articles\n- Retrieve abstracts and full text of open access articles\n- Batch search and advanced search capabilities\n- API key support for faster and unlimited downloads\n\n## API Key and Email Setup\n\nThe NCBI E-utilities API recommends using an API key and email address with your requests. This brings several benefits:\n\n- **Higher rate limits**: 10 requests/second instead of 3\n- **More results per query**: Up to 200 results per request instead of 100\n- **Priority service**: Better queue position for your requests\n\n### How to Get an API Key\n\n1. Create an NCBI account at [https://www.ncbi.nlm.nih.gov/account/](https://www.ncbi.nlm.nih.gov/account/)\n2. Go to the API Keys Management page\n3. Generate a new API key\n\n### Setting Up API Key and Email\n\nThe easiest way to set up your API key and email is using the setup script:\n\n```bash\npython setup_api.py\n```\n\nThis interactive script will guide you through the process and offer multiple setup options.\n\nAlternatively, you can set up your API key and email manually using one of these three methods:\n\n#### 1. Environment Variables\n\n```bash\nexport NCBI_API_KEY=your_api_key_here\nexport NCBI_EMAIL=your_email@example.com\n```\n\n#### 2. Command Line Arguments\n\n```bash\npython python-pubmed-mcp-enhanced.py --api-key your_api_key_here --email your_email@example.com\n```\n\n#### 3. Configuration File\n\nCreate a `config.json` file based on the example:\n\n```bash\ncp config.json.example config.json\n```\n\nEdit `config.json` to include your API key and email:\n\n```json\n{\n  \"api_key\": \"your_api_key_here\",\n  \"email\": \"your_email@example.com\"\n}\n```\n\nThen run the server with the config file:\n\n```bash\npython python-pubmed-mcp-enhanced.py --config config.json\n```\n\n## Usage\n\n### Installing via Smithery\n\nTo install pubmed-mcp-easy for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy):\n\n```bash\nnpx -y @smithery/cli install @t0mst0ne/pubmed-mcp-easy --client claude\n```\n\n### Standard Usage\n\nRun the server:\n\n```bash\npython python-pubmed-mcp-enhanced.py\n```\n\n### Claude Desktop Integration\n\nTo integrate with Claude Desktop, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"pubmed-easy\": {\n    \"command\": \"/opt/anaconda3/bin/python\",\n    \"args\": [\n        \"/GITHUB_cloned_dir/pubmed-mcp-easy/python-pubmed-mcp-enhanced.py\", \"--config\", \"config.json\"\n    ]\n}\n```\n\nMake sure to:\n1. Replace `/opt/anaconda3/bin/python` with the path to your Python executable\n2. Replace `/GITHUB_cloned_dir` with the actual path to your GitHub directory\n3. Create a `config.json` file with your API key and email as described above\n\nAfter adding this configuration, you can access PubMed tools directly from Claude Desktop.\n\n### Available Tools\n\nThe server provides the following MCP tools:\n\n- `pubmed_search`: Search for articles by keyword or query\n- `pubmed_similar`: Find similar articles\n- `pubmed_cites`: Find articles cited by a specific paper\n- `pubmed_cited_by`: Find articles that cite a specific paper\n- `pubmed_abstract`: Retrieve the abstract of an article\n- `pubmed_open_access`: Check if an article is open access\n- `pubmed_full_text`: Retrieve the full text of an open access article\n- `pubmed_batch_search`: Perform multiple searches in parallel\n- `pubmed_author_search`: Search for papers by a specific author\n- `pubmed_advanced_search`: Perform advanced field-based searches\n- `pubmed_journal_search`: Search for papers in a specific journal\n\n## Important Notes\n\n1. Including an email address is recommended by NCBI as it allows them to contact you if there are issues with your requests.\n2. If you make heavy use of the E-utilities, NCBI recommends that you limit large jobs to either weekends or between 9 pm and 5 am Eastern Time weekdays.\n3. Always be considerate in your usage and follow NCBI's usage guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "searches",
        "pubmed mcp",
        "papers pubmed",
        "t0mst0ne pubmed"
      ],
      "category": "scientific-research-tools"
    },
    "tutumomo--MCP-civil-tools": {
      "owner": "tutumomo",
      "name": "MCP-civil-tools",
      "url": "https://github.com/tutumomo/MCP-civil-tools",
      "imageUrl": "/freedevtools/mcp/pfp/tutumomo.webp",
      "description": "Provides civil engineering calculation tools including coordinate conversions between latitude/longitude and UTM/TWD97, Manning's coefficient lookup, soil pressure coefficient calculations, and drainage flow computations. Integrates seamlessly with LLM applications and AI tools for streamlined civil engineering workflows.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-19T06:16:30Z",
      "readme_content": "# MCP Civil Tools ‰º∫ÊúçÂô®\n\nÊú¨Â∞àÊ°àÊòØ‰∏ÄÂÄãÂü∫Êñº MCP ÂçîË≠∞ÁöÑ Python ‰º∫ÊúçÂô®Ôºå‰∏ªË¶ÅÊòØÂ∞áÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑ(2023.05.04)ÈÄèÈÅé MCP Server ÁöÑÊñπÂºèÊèê‰æõÁµ¶ÊîØÊè¥ MCP ÁöÑ client Ë™øÁî®ÔºåÂØ¶Áèæ‰ª•Ëá™ÁÑ∂Ë™ûË®ÄËàáÂ§ßÊ®°ÂûãÂ∞çË©±ÁöÑÊñπÂºè‰æÜÈÄ≤Ë°åÊü•Ë©¢ÂèäË®àÁÆóÔºåÊèê‰æõÂ¶ÇÊõºÂØß‰øÇÊï∏Êü•Ë©¢„ÄÅÂúüÂ£ìÂäõ‰øÇÊï∏„ÄÅÊéíÊ∞¥Ê∫ùÊµÅÈÄü„ÄÅÈÇäÂù°Á©©ÂÆö„ÄÅÂúüÂ£§‰æµËùï„ÄÅÈÄïÊµÅÈáè„ÄÅÊìãÂúüÁâÜÊ™¢Ê†∏„ÄÅÊ§çÁîüÂª∫Ë≠∞„ÄÅÊùêÊñôÂèÉÊï∏„ÄÅÂù°Èù¢‰øùË≠∑„ÄÅÊª≤Ê∞¥Ë®≠ÊñΩ„ÄÅIDFÊõ≤Á∑öÁ≠âÊü•Ë©¢ÂèäË®àÁÆóÂäüËÉΩ„ÄÇÂè¶Â§ñÈÇÑÊèê‰æõÂè∞ÁÅ£Âú∞ÂçÄÁ∂ìÁ∑ØÂ∫¶Ëàá UTM/TWD97 Â∫ßÊ®ôÁöÑËΩâÊèõ„ÄÇÈÅ©Áî®Êñº LLM Â∑•ÂÖ∑„ÄÅClaude Desktop Á≠â AI ÊáâÁî®Êï¥Âêà„ÄÇ\n\nÊ≥®ÊÑè! Á®ãÂºèÂ∞öÊú™ÂÆåÊï¥Ôºå‰∏îÊØèÂÄãÂ§ßÊ®°ÂûãÊô∫Âäõ‰∏çÁ≠âÔºåËº∏Âá∫ÁµêÊûú‰ΩøÁî®ËÄÖÂãôÂøÖÂÜçÊ¨°Ê†∏ÂØ¶ÔºåËã•ÊúâÈåØË™§ÔºåÊ≠°ËøéÂèçÈ•ã~\n---\n\n## ÁõÆÈåÑÁµêÊßã\n\n```\nMCP-civil-tools/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ mcp_server.py         # MCP ‰º∫ÊúçÂô®‰∏ªÁ®ãÂºè\n‚îÇ   ‚îú‚îÄ‚îÄ util.py              # Â∫ßÊ®ôËΩâÊèõËàáÂúüÊú®Â∑•Á®ãÂ∑•ÂÖ∑ÈÇèËºØ\n‚îÇ   ‚îî‚îÄ‚îÄ utm_types/\n‚îÇ       ‚îî‚îÄ‚îÄ __init__.py       # ÂûãÂà•ÂÆöÁæ©\n‚îú‚îÄ‚îÄ requirements.txt          # ‰æùË≥¥Â•ó‰ª∂\n‚îú‚îÄ‚îÄ README.md                 # Â∞àÊ°àË™™Êòé\n‚îî‚îÄ‚îÄ .gitignore                # Git ÂøΩÁï•Ë¶èÂâá\n```\n\n---\n\n## ÂÆâË£ù\n\n1. Âª∫Ë≠∞ÂÖàÂª∫Á´ãËôõÊì¨Áí∞Â¢ÉÔºö\n   ```bash\n   python -m venv .venv\n   .venv\\Scripts\\activate  # Windows\n   # Êàñ source .venv/bin/activate  # Linux/macOS\n   ```\n2. ÂÆâË£ù‰æùË≥¥Ôºö\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n---\n\n## ÂïüÂãïÊñπÂºè\n\n### 1. CLI (stdio) Ê®°Âºè\n\n```bash\npython src/mcp_server.py\n```\n\n### 2. HTTP ÊúçÂãôÊ®°ÂºèÔºàÂª∫Ë≠∞Áî®Êñº API Ê∏¨Ë©¶Êàñ HTTP Êï¥ÂêàÔºâ\n\n```bash\nuvicorn src.mcp_server:app --port 8000\n```\n\n---\n\n## mcp.json Ë®≠ÂÆöÁØÑ‰æã\n\nËã•Ë¶ÅËÆì Claude Desktop ÊàñÂÖ∂‰ªñ MCP ÂÆ¢Êà∂Á´ØËá™ÂãïÂÅµÊ∏¨‰∏¶ÂïüÂãïÊú¨‰º∫ÊúçÂô®ÔºåMCP setting JSON ÂÖßÂÆπÂ¶Ç‰∏ãÔºàË´ã‰æùÂØ¶ÈöõË∑ØÂæëË™øÊï¥ÔºâÔºö\n\n```json\n{\n  \"mcpServers\": {\n    \"MCP-civil-tools\": {\n      \"command\": \"path/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"path/src/mcp_server.py\"\n      ]\n    }\n  }\n}\nÊàñÊòØ\n    \"MCP-civil-tools\": {\n      \"command\": \"C:/TOMO/MCP-civil-tools/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"C:/TOMO/MCP-civil-tools/src/mcp_server.py\"\n      ],\n      \"disabled\": true,\n      \"autoApprove\": []\n    },\n\n```\n- `command` Ë´ãÂ°´ÂÖ•‰Ω†ËôõÊì¨Áí∞Â¢ÉÁöÑ python.exe ÁµïÂ∞çË∑ØÂæë„ÄÇ\n- `args` Ë´ãÂ°´ÂÖ• mcp_server.py ÁöÑÁµïÂ∞çË∑ØÂæë„ÄÇ\n\n‰ª• Windowss Á≥ªÁµ±ÔºåCommand argsÔºåËº∏ÂÖ•Ê†ºÂºèÂ¶Ç‰∏ãÔºö\nC:\\TOMO\\MCP-civil-tools\\.venv\\Scripts\\python.exe C:\\TOMO\\MCP-civil-tools\\src\\mcp_server.py\n\n‰ª• MAC Á≥ªÁµ±ÔºåCommand argsÔºåËº∏ÂÖ•Ê†ºÂºèÂ¶Ç‰∏ãÔºö\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/.venv/bin/python3\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/src/mcp_server.py\n\n---\n\n## ÂèÉÊï∏È†êË®≠Ë°åÁÇ∫Ë™™Êòé\n\n- ‰ΩøÁî®ËÄÖÂè™Ëº∏ÂÖ•Á∑ØÂ∫¶„ÄÅÁ∂ìÂ∫¶ÊôÇÔºåÊ≤íÊúâËº∏ÂÖ•ÂÖ∂‰ªñË≥áË®äÊôÇÔºåÈ†êË®≠ÁöÑ UTM/TM2 Zone Â∞±ÊòØ TM2-121ÔºåÈ†êË®≠ÁöÑÂçäÁêÉÂ∞±ÊòØÂåóÂçäÁêÉ„ÄÇ\n- Áï∂‰ΩøÁî®ËÄÖÂè™Ëº∏ÂÖ•Âπ≥Èù¢Â∫ßÊ®ô X, Y ÊôÇÔºåÈ†êË®≠ÁöÑ UTM Zone ÊòØ TWD97ÔºåÂçäÁêÉÊòØÂåóÂçäÁêÉ„ÄÇ\n\n---\n\n## ‰∏ªË¶ÅÂäüËÉΩÊõ¥Êñ∞\n\n- **ÊâÄÊúâÊü•Ë°®ÂûãÂ∑•ÂÖ∑ÁöÜÊîØÊè¥„ÄåÊîØÊè¥Ê∏ÖÂñÆÊü•Ë©¢„ÄçAPI**Ôºö\n  - ‰æãÂ¶ÇÔºö`list_supported_materials`ÔºàÂ∏∏Áî®ÊùêÊñôÔºâ„ÄÅ`list_supported_manning_materials`ÔºàÊõºÂØß‰øÇÊï∏ÊùêÊñôÔºâ„ÄÅ`list_supported_max_velocity_materials`ÔºàÊúÄÂ§ßÊµÅÈÄüÊùêÊñôÔºâ„ÄÅ`list_supported_regions`ÔºàÂú∞ÂçÄ/IDF/Âπ¥Èõ®ÈáèÔºâ„ÄÅ`list_supported_soil_types`ÔºàÂúüÂ£§È°ûÂûãÔºâ„ÄÅ`list_supported_land_uses`ÔºàÂúüÂú∞Âà©Áî®Ôºâ„ÄÅ`list_supported_practices`ÔºàÊ∞¥‰øùÊé™ÊñΩÔºâ„ÄÅ`list_supported_runoff_land_uses`ÔºàÈÄïÊµÅ‰øÇÊï∏ÂúüÂú∞Âà©Áî®Ôºâ„ÄÅ`list_supported_slope_protection_methods`ÔºàÂù°Èù¢‰øùË≠∑Â∑•Ê≥ïÔºâ„ÄÅ`list_supported_soil_k_types`ÔºàÊª≤ÈÄè‰øÇÊï∏ÂúüÂ£§Ôºâ„ÄÅ`list_supported_idf_locations`ÔºàIDFÊõ≤Á∑öÂú∞ÈªûÔºâÁ≠â„ÄÇ\n  - Êü•Ë©¢ÊôÇËã•Ëº∏ÂÖ•ÈåØË™§ÊàñÊü•ÁÑ°Ë≥áÊñôÔºåÊúÉËá™ÂãïÊèêÁ§∫ÊâÄÊúâÂèØÊü•Ë©¢ÁöÑÊîØÊè¥È†ÖÁõÆÔºåÊèêÂçá‰ΩøÁî®È´îÈ©ó„ÄÇ\n\n- **2023/04/23 Êõ¥Êñ∞**Ôºö\n  - ‰øÆÊ≠£ `list_supported_materials` ÂäüËÉΩÔºåÁèæÂú®ÂèØÊ≠£Á¢∫ËøîÂõûÊâÄÊúâÊîØÊè¥ÁöÑÂ∏∏Áî®ÊùêÊñôË®≠Ë®àÂèÉÊï∏ÊùêÊñôÂêçÁ®±Ê∏ÖÂñÆÔºåÂåÖÊã¨Ôºö‰∏ÄËà¨ÈªèÂúü„ÄÅÁ†ÇÂúü„ÄÅÁ§´Áü≥„ÄÅÊ∑∑ÂáùÂúü„ÄÅËä±Â¥óÂ≤©„ÄÅÁü≥ÁÅ∞Â≤©„ÄÅÈ†ÅÂ≤©„ÄÅÁ†ÇÂ≤©Á≠â„ÄÇ\n  - Êñ∞Â¢ûÂõõÂÄã USLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑Ôºö\n    - `query_r_factor_tool`ÔºöÊü•Ë©¢ÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_k_factor_tool`ÔºöÊü•Ë©¢ÂúüÂ£§Ê≤ñËùïÊåáÊï∏ K ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_c_factor_tool`ÔºöÊü•Ë©¢Ë¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_p_factor_tool`ÔºöÊü•Ë©¢Ê∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠ê P ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n  - ‰øÆÊ≠£ `calc_catchment_runoff` ÂäüËÉΩÔºåÂ¢ûÂº∑ÂèÉÊï∏ËôïÁêÜËÉΩÂäõÔºåÁèæÂú®ÂèØ‰ª•Ê≠£Á¢∫ËôïÁêÜËá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•Ôºå‰æãÂ¶Ç„ÄåË®àÁÆó‰∏ÄÂÄãÈù¢Á©ç2ÂÖ¨È†É„ÄÅÈôçÈõ®Âº∑Â∫¶100mm/hr„ÄÅÈÄïÊµÅ‰øÇÊï∏0.6ÁöÑÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè„Äç„ÄÇ\n  - Êì¥ÂÖÖ `check_retaining_wall` ÂäüËÉΩÔºåÊñ∞Â¢ûÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÂÆâÂÖ®‰øÇÊï∏Ê™¢Ê†∏Ôºö\n    - ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨120Ê¢ùÔºåÂ¢ûÂä†Âú∞ÈúáÊôÇÊªëÂãïÂÆâÂÖ®‰øÇÊï∏Ôºà‚âß1.2ÔºâÂíåÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏Ôºà‚âß1.5ÔºâÁöÑÊ™¢Ê†∏\n    - Â¢ûÂä†ÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÊ™¢Ê†∏ÔºåÊîØÊè¥Â≤©Áõ§Âü∫Á§éÂíåÂúüÂ±§Âü∫Á§éÂÖ©Á®ÆÊÉÖÊ≥Å\n    - ‰øÆÊ≠£ÊâøËºâÂäõÂÆâÂÖ®‰øÇÊï∏Ë¶ÅÊ±ÇÁÇ∫3.0ÔºåÁ¨¶ÂêàË¶èÁØÑË¶ÅÊ±Ç\n    - Ëº∏Âá∫Â†±Ë°®‰∏≠ÊòéÁ¢∫ÂçÄÂàÜÂ∏∏ÊôÇÊÉÖÊ≥ÅÂíåÂú∞ÈúáÊÉÖÊ≥ÅÁöÑÂÆâÂÖ®‰øÇÊï∏\n\n---\n\n## ÂÄãÂà•Â∑•ÂÖ∑‰ΩøÁî®ÁØÑ‰æã\n\n### Á∂ìÁ∑ØÂ∫¶ËΩâ UTM\n- Ëº∏ÂÖ•ÔºöËá™ÁÑ∂Ë™ûË®ÄÊèèËø∞Á∂ìÁ∑ØÂ∫¶ÔºåÂ¶Ç„ÄåÂ∞áÁ∑ØÂ∫¶24.125„ÄÅÁ∂ìÂ∫¶120.641ËΩâÊèõÁÇ∫UTMÂ∫ßÊ®ô„Äç\n- ÂõûÂÇ≥Ôºö`\"X,Y\"` Â≠ó‰∏≤ÔºåÊï∏ÂÄºÂõõÊç®‰∫îÂÖ•Âà∞Â∞èÊï∏Èªû‰∏ã4‰Ωç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÂ∞áÁ∑ØÂ∫¶24.125193616011536„ÄÅÁ∂ìÂ∫¶120.64098341751337ËΩâÊèõÁÇ∫UTMÂ∫ßÊ®ô\nÂõûÂÇ≥Ôºö203650.6040,2670482.4250\n```\n\n### UTM ËΩâÁ∂ìÁ∑ØÂ∫¶\n- Ëº∏ÂÖ•ÔºöX, Y Â∫ßÊ®ôÔºàÂèØÈÅ∏ zone, datum, southÔºåÈ†êË®≠ TWD97 ÂåóÂçäÁêÉÔºâ\n- ÂõûÂÇ≥Ôºö`\"Á∑ØÂ∫¶,Á∂ìÂ∫¶\"` Â≠ó‰∏≤ÔºåÊï∏ÂÄºÂõõÊç®‰∫îÂÖ•Âà∞Â∞èÊï∏Èªû‰∏ã15‰Ωç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÂ∞á UTM Â∫ßÊ®ô X=203650.604„ÄÅY=2670482.425 ËΩâÊèõÁÇ∫Á∂ìÁ∑ØÂ∫¶\nÂõûÂÇ≥Ôºö24.125193616011536,120.64098341751337\n```\n\n### ÊõºÂØß‰øÇÊï∏Êü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÊùêÊñôÂêçÁ®±ÔºàÂ¶Ç„ÄåÊ∑∑ÂáùÂúü„Äç„ÄÅ„ÄåÁ¥îÁ¥∞Á†Ç„Äç„ÄÅ„ÄåÂÖ®Èù¢ÂØÜËçâÁîü„ÄçÁ≠âÔºâ\n- ÂõûÂÇ≥ÔºöË©≤ÊùêÊñôÁöÑÊõºÂØß‰øÇÊï∏ n ÂèäÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖ®Èù¢ÂØÜËçâÁîüÁöÑÊõºÂØß‰øÇÊï∏ÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂÖ®Èù¢ÂØÜËçâÁîü ÁöÑÊõºÂØß‰øÇÊï∏ n = 0.040ÔºåÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúçÔºö1.5~2.5 m/s\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÊõºÂØß‰øÇÊï∏Ë°®\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ê∑∑ÂáùÂúüÁöÑÊõºÂØß‰øÇÊï∏\nÂõûÂÇ≥ÔºöÊ∑∑ÂáùÂúü ÁöÑÊõºÂØß‰øÇÊï∏ n = 0.013ÔºåÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúçÔºö3.0~6.0 m/s\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÊõºÂØß‰øÇÊï∏Ë°®\n```\n\n### ‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂÖßÊë©Êì¶ËßíÁ≠âË≥áË®ä\n- ÂõûÂÇ≥Ôºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Ka„ÄÅË®àÁÆóÂÖ¨Âºè„ÄÅ‰æùÊìöÁ≠â\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖßÊë©Êì¶Ëßí30Â∫¶ÁöÑ‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏KaÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥Ôºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117Ê¢ùÂèäÈôÑ‰ª∂ÂúüÂ£ìÂäõË®àÁÆóÂÖ¨Âºè\nË™™ÊòéÔºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Áî®ÊñºË®àÁÆóÊìãÂúüÁµêÊßãÊâÄÊâøÂèóÁöÑÊ∞¥Âπ≥Êé®ÂäõÔºåÊòØÊìãÂúüÁâÜË®≠Ë®àÁöÑÈáçË¶ÅÂèÉÊï∏„ÄÇ\n```\n\n### Ë¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂÖßÊë©Êì¶ËßíÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Kp„ÄÅË®àÁÆóÂÖ¨Âºè„ÄÅ‰æùÊìöÁ≠â\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖßÊë©Êì¶Ëßí30Â∫¶ÁöÑË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏KpÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Kp = 3.0\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117Ê¢ùÂèäÈôÑ‰ª∂ÂúüÂ£ìÂäõË®àÁÆóÂÖ¨Âºè\nË™™ÊòéÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Áî®ÊñºË®àÁÆóÊìãÂúüÁµêÊßãÁöÑÊäµÊäóÂäõÔºåÊòØÊìãÂúüÁâÜÁ©©ÂÆöÂàÜÊûêÁöÑÈáçË¶ÅÂèÉÊï∏„ÄÇ\n```\n\n### ÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆóÔºàÂ§öÊñ∑Èù¢ÊîØÊè¥Ôºâ\n- Ëº∏ÂÖ•ÔºöÊñ∑Èù¢ÂûãÂºè„ÄÅÊµÅÈáè„ÄÅÂù°Â∫¶„ÄÅÊõºÂØß‰øÇÊï∏„ÄÅÊ∏†ÈÅìÊùêË≥™„ÄÅÂ∞∫ÂØ∏Á≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÊµÅÈÄü„ÄÅÊµÅÊ∑±„ÄÅÊñ∑Èù¢Á©ç„ÄÅÊ∞¥ÂäõÂçäÂæëÁ≠âË®àÁÆóÁµêÊûúÔºå‰ª•ÂèäÂÆåÊï¥Â†±ÂëäÊõ∏\n\n#### Ë®àÁÆóÈÇèËºØËàáË≠¶ÂëäË™™ÊòéÔºà2024/06‰øÆÊ≠£ÁâàÔºâ\n- Ëã•Ëº∏ÂÖ•ÊµÅÈáèË∂ÖÈÅéË©≤Êñ∑Èù¢ÊªøÊµÅÊôÇÁöÑÊúÄÂ§ßÊµÅÈáèÔºåÁ≥ªÁµ±ÊúÉÁõ¥Êé•ÂõûÂÇ≥ÈåØË™§Ë®äÊÅØÔºåÊèêÁ§∫„ÄåÊâÄÈúÄÊµÅÈáèÂ∑≤Ë∂ÖÈÅéÊªøÊµÅÊúÄÂ§ßÊµÅÈáèÔºåË´ãÂä†Â§ßÂ∞∫ÂØ∏ÊàñÂù°Â∫¶„Äç„ÄÇ\n- Âè™Ë¶ÅÊµÅÈáèÊú™Ë∂ÖÈÅéÊªøÊµÅÔºåË®àÁÆóÊúÉËá™ÂãïÊ±ÇÂæó‰∏ÄÂÄãÂêàÁêÜÁöÑÊµÅÊ∑±ÔºàÂ∞èÊñºÊ∫ùÈ´òÔºâÔºå‰∏çÊúÉË™§Â†±ÊªøÊµÅ„ÄÇ\n- ÊªøÊµÅË≠¶ÂëäÂÉÖÂú®ÊµÅÊ∑±Ê•µÊé•ËøëÈ´òÂ∫¶‰∏îÊµÅÈáèÊ•µÊé•ËøëÊªøÊµÅÊôÇÊâçÊúÉÂá∫Áèæ„ÄÇ\n- Ë®àÁÆóÁ≤æÂ∫¶ËàáÊî∂ÊñÇÊ¢ù‰ª∂Â∑≤ÂÑ™ÂåñÔºåÈÅøÂÖçÂ∑•Á®ãÂ∏∏Áî®ÊÉÖÂ¢É‰∏ãÂá∫ÁèæË™§Âà§„ÄÇ\n\n#### ÊúÄÂ§ß/ÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüË¶èÂâá\n- ÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÔºö\n  - ÂÉÖÊ™¢Ê†∏ÊúÄÂ§ßÂÄºÔºàÂ¶ÇÊ∑∑ÂáùÂúü6.1m/s„ÄÅÈãºÁ≠ãÊ∑∑ÂáùÂúü12.0m/s„ÄÅÁ≤óÁ†ÇÂèäÁ¥∞Á†Ç0.61m/sÁ≠âÔºâÔºåË∂ÖÈÅéÊôÇÁ≥ªÁµ±Â∞áË≠¶Âëä‰∏¶Âª∫Ë≠∞Ë®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n  - Ë°®Ê†º‰∏≠ÁÑ°\"ÈãºÁ≠ãÊ∑∑ÂáùÂúü\"‰πãÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÔºåÁ≥ªÁµ±Â∑≤Êñ∞Â¢ûÊúÄÂ§ßÂÄº12.0m/s„ÄÇ\n- ÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüÔºö\n  - ÂÉÖÊ∑∑ÂáùÂúüËàáÈãºÁ≠ãÊ∑∑ÂáùÂúüÊéíÊ∞¥Ë®≠ÊñΩÊòéÁ¢∫Ë¶èÂÆöÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüÁÇ∫0.8 m/sÔºå‰ΩéÊñºÊôÇÁ≥ªÁµ±Â∞áË≠¶ÂëäÂèØËÉΩÂ∞éËá¥Ê≥•Á†ÇÊ∑§Á©ç„ÄÇ\n  - ÂÖ∂È§òÊùêË≥™Êú™Ë¶ãÊòéÁ¢∫Ë¶èÂÆöÔºåÁ≥ªÁµ±‰∏çÊ™¢Ê†∏ÊúÄÂ∞èÂÄº„ÄÇ\n- Ê™¢Ê†∏Âª∫Ë≠∞ÔºöË®≠Ë®àÊéíÊ∞¥‰πãÊµÅÈÄüË∂ÖÈÅéÂÖ∂ÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüËÄÖÔºåÊáâÊñºÈÅ©Áï∂‰ΩçÁΩÆË®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n\n#### Âá∫Ê∞¥È´òÔºàÊµÅÊ∑±ÔºâË¶èÁØÑÊ™¢Ê†∏ÔºàÁ¨¨86Ê¢ùÔºâ\n- Áü©ÂΩ¢„ÄÅÊ¢ØÂΩ¢ÊéíÊ∞¥Ê∫ù‰πãÂá∫Ê∞¥È´òÔºàÊµÅÊ∑±ÔºâÊáâÁ¨¶ÂêàÔºö\n  - ‰∏çÂæóÂ∞èÊñºË®≠Ë®àÊ∞¥Ê∑±ÁöÑ25%\n  - ÊúÄÂ∞èÂÄºÁÇ∫20ÂÖ¨ÂàÜ\n  - LÂûã„ÄÅÊããÁâ©Á∑öÂûãÊéíÊ∞¥Ê∫ù‰∏çÂú®Ê≠§Èôê\n- Ëã•Ë®àÁÆóÊµÅÊ∑±‰ΩéÊñº‰∏äËø∞Ê®ôÊ∫ñÔºåÁ≥ªÁµ±Â∞áÊñºÂõûÂÇ≥ÁµêÊûúËàáÂ†±ÂëäÊõ∏‰∏≠Áµ¶‰∫àÊòéÁ¢∫Ë≠¶ÂëäÔºåË´ãË™øÊï¥Ë®≠Ë®àÂèÉÊï∏„ÄÇ\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè1cmsÔºåÂù°Â∫¶5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\nÂõûÂÇ≥Ôºö\nÊµÅÈÄü: 7.20 m/sÔºåÊµÅÊ∑±: 0.250 m„ÄÇË≠¶ÂëäÔºöË®àÁÆóÊµÅÈÄü v = 7.2000 m/s Â∑≤Ë∂ÖÈÅé„ÄéÈãºÁ≠ãÊ∑∑ÂáùÂúü„ÄèÊúÄÂ§ßÂÆπË®±ÊµÅÈÄü 12.0 m/sÔºåÊáâÊñºÈÅ©Áï∂‰ΩçÁΩÆË®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n\nËº∏ÂÖ•ÔºöË®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè0.2cmsÔºåÂù°Â∫¶0.5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±?\nÂõûÂÇ≥Ôºö\nÊµÅÈÄü: 0.65 m/sÔºåÊµÅÊ∑±: 0.120 m„ÄÇË≠¶ÂëäÔºöË®àÁÆóÊµÅÈÄü v = 0.6500 m/s ‰ΩéÊñº„ÄéÊ∑∑ÂáùÂúü„ÄèÊúÄÂ∞èÂÆπË®±ÊµÅÈÄü 0.8 m/sÔºåÂèØËÉΩÂ∞éËá¥Ê≥•Á†ÇÊ∑§Á©ç„ÄÇ\n\n„ÄêÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆóÂ†±Âëä„Äë\nÊñ∑Èù¢ÂûãÂºèÔºöÁü©ÂΩ¢\nÁü©ÂΩ¢Â∫ïÂØ¨ b=50.0cm, È´òÂ∫¶ h=70.0cm\nÊµÅÈáè Q = 1.000 cms\nÂù°Â∫¶ S = 5.000%\nÊõºÂØß‰øÇÊï∏ n = 0.013\nÊ∏†ÈÅìÊùêË≥™ÔºöÈãºÁ≠ãÊ∑∑ÂáùÂúü\n\n„ÄêË®àÁÆóÂÖ¨Âºè„Äë\nQ = A √ó V, V = (1/n) √ó R^(2/3) √ó S^(1/2)\n„ÄêË®àÁÆóÊ≠•È©ü„Äë\nA = b√óy, P = b+2y, R = A/P, V = (1/n)R^(2/3)S^(1/2), Q = A√óV\n\n„ÄêË®àÁÆóÁµêÊûú„Äë\nÊµÅÈÄü V = 2.340 m/s\nÊµÅÊ∑± y = 0.210 m\nÊñ∑Èù¢Á©ç A = 0.1050 m¬≤\nÊ∞¥ÂäõÂçäÂæë R = 0.0700 m\nÂë®Èï∑ P = 1.0000 m\n\nË®àÁÆóÁµêÊûúÁ¨¶ÂêàÂÆâÂÖ®ÊµÅÈÄüË¶èÁØÑ„ÄÇ\n„ÄêÊ™¢Ê†∏Ë≠¶Âëä„ÄëË®àÁÆóÊµÅÊ∑± y = 0.210 m ‰∏çÁ¨¶Á¨¨86Ê¢ùË¶èÁØÑÔºåÊáâ‚âßmax(0.2m, Ë®≠Ë®àÊ∞¥Ê∑±25%)=0.250 m„ÄÇË´ãË™øÊï¥Ë®≠Ë®à„ÄÇ\n```\n\n### ÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂñÆ‰ΩçÈáç„ÄÅÊë©Êì¶Ëßí„ÄÅÂáùËÅöÂäõ„ÄÅÂú∞‰∏ãÊ∞¥‰Ωç„ÄÅÊñπÊ≥ïÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂÆâÂÖ®‰øÇÊï∏„ÄÅÊñπÊ≥ï„ÄÅÊòØÂê¶ÂêàÊ†º„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãË®àÁÆóÂù°Â∫¶30Â∫¶„ÄÅÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPaÁöÑÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏\nÂõûÂÇ≥ÔºöÂÆâÂÖ®‰øÇÊï∏ = 1.50ÔºåÊñπÊ≥ïÔºöÁ∞°ÂåñÊ≥ïÔºåÂêàÊ†ºÔºöTrue\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨31Ê¢ùÂèäÈôÑ‰ª∂ÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏ÊáâÂ§ßÊñº1.5ÔºåÊú¨Ê°à‰æãÁ¨¶ÂêàË¶èÁØÑË¶ÅÊ±Ç„ÄÇ\n```\n\n### ÂúüÂ£§‰æµËùïÊ®°Êï∏/ÊµÅÂ§±ÈáèË®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂù°Èï∑„ÄÅÂù°Â∫¶„ÄÅÈôçÈõ®„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÂúüÂú∞Âà©Áî®„ÄÅÊ∞¥‰øùÊé™ÊñΩÁ≠âË≥áË®ä\n- ÂõûÂÇ≥Ôºö‰æµËùïÊ®°Êï∏„ÄÅÊµÅÂ§±Èáè„ÄÅÊñπÊ≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆóÂù°Èï∑100ÂÖ¨Â∞∫„ÄÅÂù°Â∫¶10%„ÄÅÂπ¥ÈôçÈõ®Èáè1200mm„ÄÅÁ†ÇÂúü„ÄÅËçâÂú∞„ÄÅÁ≠âÈ´òËÄï‰ΩúÁöÑÂúüÂ£§ÊµÅÂ§±Èáè\nÂõûÂÇ≥ÔºöÂúüÂ£§‰æµËùïÊ®°Êï∏/ÊµÅÂ§±Èáè = 42.75 ÂÖ¨Âô∏/ÂÖ¨È†É/Âπ¥\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÂèäÈôÑ‰ª∂USLEÂÖ¨Âºè„ÄÅÂèÉÊï∏Ë°®\nË™™ÊòéÔºöUSLEÂÖ¨ÂºèÁÇ∫ÂúãÈöõÈÄöÁî®ÂúüÂ£§ÊµÅÂ§±ÈáèÊé®‰º∞ÊñπÊ≥ïÔºåÂè∞ÁÅ£Ê∞¥‰øùË¶èÁØÑÊòéÂÆöÂèØÁî®ÊñºÂù°Âú∞ÂúüÂ£§‰æµËùïË©ï‰º∞„ÄÇ\n```\n\n### ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆó\n- Ëº∏ÂÖ•ÔºöÈõÜÊ∞¥ÂçÄÈù¢Á©ç„ÄÅÈôçÈõ®Âº∑Â∫¶„ÄÅÈÄïÊµÅ‰øÇÊï∏Á≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÊúÄÂ§ßÈÄïÊµÅÈáè„ÄÅÊñπÊ≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆó‰∏ÄÂÄãÈù¢Á©ç2ÂÖ¨È†É„ÄÅÈôçÈõ®Âº∑Â∫¶100mm/hr„ÄÅÈÄïÊµÅ‰øÇÊï∏0.6ÁöÑÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè\nÂõûÂÇ≥ÔºöÊúÄÂ§ßÈÄïÊµÅÈáè Q = 0.33 cmsÔºåÊñπÊ≥ïÔºöRational\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨16„ÄÅ17„ÄÅ18Ê¢ùÂèäÈôÑ‰ª∂ÈôçÈõ®Âº∑Â∫¶„ÄÅÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈõÜÊ∞¥ÂçÄÈù¢Á©çÂ∞èÊñº1000ÂÖ¨È†ÉÊôÇÔºåÁÑ°ÂØ¶Ê∏¨Ë≥áÊñôÂèØÊé°RationalÂÖ¨ÂºèË®àÁÆóÊ¥™Â≥∞ÊµÅÈáè„ÄÇ\n```\n\n### Ë≠∑Â≤∏/ÊìãÂúüÁâÜÁ©©ÂÆöÊ™¢Ê†∏\n- Ëº∏ÂÖ•ÔºöÁâÜÈ´ò„ÄÅÂéöÂ∫¶ÔºàÁü©ÂΩ¢Êñ∑Èù¢ÔºâÊàñÈ†ÇÂØ¨/Â∫ïÂØ¨ÔºàÊ¢ØÂΩ¢Êñ∑Èù¢Ôºâ„ÄÅÂñÆ‰ΩçÈáç„ÄÅÊë©Êì¶Ëßí„ÄÅÂáùËÅöÂäõ„ÄÅËÉåÂ°´Âù°Â∫¶„ÄÅÂú∞‰∏ãÊ∞¥‰Ωç„ÄÅÂú∞Èúá‰øÇÊï∏„ÄÅÂü∫Á§éÈ°ûÂûãÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂ∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏„ÄÅÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆ„ÄÅÊòØÂê¶ÂêàÊ†º„ÄÅË™™Êòé\n\n#### ÁØÑ‰æãÔºàÁü©ÂΩ¢Êñ∑Èù¢Ôºâ\n```\nËº∏ÂÖ•ÔºöÊ™¢Ê†∏‰∏ÄÂ∫ßÈ´ò2ÂÖ¨Â∞∫„ÄÅÂéö1ÂÖ¨Â∞∫ÁöÑÊìãÂúüÁâÜÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPa„ÄÅËÉåÂ°´Âù°Â∫¶10Â∫¶ÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§é\nÂõûÂÇ≥Ôºö\n„ÄêÂ∏∏ÊôÇÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.65Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=2.30Ôºà‚âß2.0 ÂêàÊ†ºÔºâÔºåÊâøËºâSF=3.10Ôºà‚âß3.0 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n„ÄêÂú∞ÈúáÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.25Ôºà‚âß1.2 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=1.60Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117„ÄÅ118„ÄÅ120„ÄÅ164Ê¢ùÂèäÈôÑ‰ª∂ÊúÄÂ∞èÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÊìãÂúüÁâÜË®≠Ë®àÈ†àÂêåÊôÇÊ™¢Ê†∏Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏ÂèäÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÔºå‰∏¶Á¨¶ÂêàË¶èÁØÑÂª∫Ë≠∞Ê®ôÊ∫ñ„ÄÇ\n```\n\n#### ÁØÑ‰æãÔºàÊ¢ØÂΩ¢Êñ∑Èù¢Ôºâ\n```\nËº∏ÂÖ•ÔºöÊ™¢Ê†∏‰∏ÄÂ∫ßÈ´ò2ÂÖ¨Â∞∫„ÄÅÈ†ÇÂØ¨0.5ÂÖ¨Â∞∫„ÄÅÂ∫ïÂØ¨1ÂÖ¨Â∞∫ÁöÑÊ¢ØÂΩ¢ÊìãÂúüÁâÜÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPaÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§é\nÂõûÂÇ≥Ôºö\n„ÄêÁâÜÈ´îÂπæ‰ΩïËàáÈáçÈáè„Äë\nÊ¢ØÂΩ¢Êñ∑Èù¢ÔºöÈ´òÂ∫¶ = 2.00 mÔºåÈ†ÇÂØ¨ = 0.50 mÔºåÂ∫ïÂØ¨ = 1.00 mÔºåÁâÜÈ´îÂñÆ‰ΩçÈáç = 24.00 kN/m¬≥\nÁâÜÈ´îÈáçÈáè W = 24.00 √ó (1.00 + 0.50) √ó 2.00 / 2 = 36.00 kN/m\nÈáçÂøÉ‰ΩçÁΩÆ x_g = 0.56 m\n\n„ÄêÂ∏∏ÊôÇÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.78Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=2.45Ôºà‚âß2.0 ÂêàÊ†ºÔºâÔºåÊâøËºâSF=3.25Ôºà‚âß3.0 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n„ÄêÂú∞ÈúáÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.32Ôºà‚âß1.2 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=1.75Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117„ÄÅ118„ÄÅ120„ÄÅ164Ê¢ùÂèäÈôÑ‰ª∂ÊúÄÂ∞èÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÊìãÂúüÁâÜË®≠Ë®àÈ†àÂêåÊôÇÊ™¢Ê†∏Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏ÂèäÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÔºå‰∏¶Á¨¶ÂêàË¶èÁØÑÂª∫Ë≠∞Ê®ôÊ∫ñ„ÄÇ\n```\n\n### Ê§çÁîüË≠∑Âù°Ë®≠Ë®àÂª∫Ë≠∞\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÊ∞£ÂÄôÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ï„ÄÅËçâÁ®Æ„ÄÅË¶ÜËìãÁéá„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÈáùÂ∞çÂù°Â∫¶30Â∫¶„ÄÅÂ£§Âúü„ÄÅ‰∫ûÁÜ±Â∏∂Ê∞£ÂÄôÁöÑÈÇäÂù°Êèê‰æõÊ§çÁîüË≠∑Âù°Ë®≠Ë®àÂª∫Ë≠∞\nÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ïÔºöÂô¥Êí≠ËçâÁöÆ+Ê†ºÊ°ÜÊàñÂúüÂ∑•Á∂≤ÔºåÂàÜÂçÄÂàÜÊúüÊñΩÂ∑•\nËçâÁ®ÆÔºöÁôæÊÖïÈÅîËçâ„ÄÅÁãóÁâôÊ†π„ÄÅÂú∞ÊØØËçâÁ≠âËÄêÊó±ËÄêÊ≤ñËùïËçâÁ®Æ\nË¶ÜËìãÁéáÔºö90.0%\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨8„ÄÅ167„ÄÅ172Ê¢ùÂèäÈôÑ‰ª∂Âù°Â∫¶ÂàÜÁ¥ö„ÄÅÊ®£ÂçÄÈù¢Á©ç„ÄÅË¶ÜËìãÁéáÁ≠âË¶èÂÆö\n```\n\n### Â∏∏Áî®ÊùêÊñôË®≠Ë®àÂèÉÊï∏Êü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÊùêÊñôÂêçÁ®±\n- ÂõûÂÇ≥ÔºöÂñÆ‰ΩçÈáç„ÄÅÂáùËÅöÂäõ„ÄÅÊë©Êì¶Ëßí„ÄÅÂº∑Â∫¶„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊèê‰æõ‰∏ÄËà¨ÈªèÂúüÁöÑË®≠Ë®àÂèÉÊï∏\nÂõûÂÇ≥ÔºöÊùêÊñôÔºö‰∏ÄËà¨ÈªèÂúüÔºåÂñÆ‰ΩçÈáçÔºö18.0kN/m¬≥ÔºåÂáùËÅöÂäõÔºö20.0kPaÔºåÊë©Êì¶ËßíÔºö25.0¬∞ÔºåÂº∑Â∫¶Ôºö200.0kPa\n‰æùÊìöÔºöÂ∏∏Áî®ÂúüÂ£§Â∑•Á®ãÊâãÂÜä\n```\n\n### Âù°Èù¢‰øùË≠∑Â∑•Ê≥ïÂª∫Ë≠∞\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÈôçÈõ®ÈáèÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂù°Â∫¶30%„ÄÅÁ†ÇÂúü„ÄÅÂπ¥ÈôçÈõ®Èáè1200mmÁöÑÂù°Èù¢ÔºåÂª∫Ë≠∞Êé°Áî®‰ªÄÈ∫º‰øùË≠∑Â∑•Ê≥ïÔºü\nÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ïÔºöÂô¥Êí≠ËçâÁöÆ+Ê†ºÊ°ÜÊàñÂúüÂ∑•Á∂≤\nË™™ÊòéÔºöÂù°Â∫¶ËºÉÂ§ßÊôÇÊáâÂä†Âº∑ÊéíÊ∞¥Ë®≠ÊñΩËàáÂù°Èù¢Á©©ÂÆöÊé™ÊñΩ\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨8„ÄÅ167„ÄÅ172Ê¢ùÂèäÈôÑ‰ª∂Âù°Â∫¶ÂàÜÁ¥öË°®\n```\n\n### Êª≤Ê∞¥Ë®≠ÊñΩË®≠Ë®à\n- Ëº∏ÂÖ•ÔºöË®≠ÊñΩÂûãÂºè„ÄÅÂúüÂ£§Êª≤ÈÄè‰øÇÊï∏„ÄÅÈõÜÊ∞¥Èù¢Á©ç„ÄÅÈôçÈõ®ÈáèÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöË®≠Ë®àÊµÅÈáè„ÄÅÂª∫Ë≠∞Â∞∫ÂØ∏„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãË®≠Ë®à‰∏ÄÂÄãÊª≤Ê∞¥‰∫ïÔºåÂúüÂ£§Êª≤ÈÄè‰øÇÊï∏k=0.001cm/sÔºåÈõÜÊ∞¥Èù¢Á©ç100Âπ≥ÊñπÂÖ¨Â∞∫ÔºåÂπ¥ÈôçÈõ®Èáè1200mm\nÂõûÂÇ≥ÔºöË®≠ÊñΩÂûãÂºèÔºöÊª≤Ê∞¥‰∫ïÔºåË®≠Ë®àÊµÅÈáèÔºö0.033cmsÔºåÂª∫Ë≠∞Â∞∫ÂØ∏ÔºöÁõ¥Âæë1.2mÔºåÊ∑±Â∫¶1.8m\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨94Ê¢ùÂèäÈôÑ‰ª∂Êª≤ÈÄèË®≠ÊñΩË®≠Ë®àÊ∫ñÂâá\nË™™ÊòéÔºöÊª≤Ê∞¥‰∫ïË®≠Ë®àÈúÄËÄÉÊÖÆÂúüÂ£§Êª≤ÈÄèÊÄß„ÄÅÈõÜÊ∞¥Èù¢Á©çÂèäÁï∂Âú∞ÈôçÈõ®ÁâπÊÄßÔºå‰∏¶ÂÆöÊúüÁ∂≠Ë≠∑‰ª•Á¢∫‰øùÂäüËÉΩ„ÄÇ\n```\n\n### IDFÊõ≤Á∑öÊü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÂú∞Èªû„ÄÅÈáçÁèæÊúü„ÄÅÊ≠∑ÊôÇÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÈôçÈõ®Âº∑Â∫¶„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Âè∞‰∏≠Â∏Ç10Âπ¥ÈáçÁèæÊúü„ÄÅ60ÂàÜÈêòÊ≠∑ÊôÇÁöÑÈôçÈõ®Âº∑Â∫¶\nÂõûÂÇ≥ÔºöÂú∞ÈªûÔºöÂè∞‰∏≠Â∏ÇÔºåÈáçÁèæÊúüÔºö10Âπ¥ÔºåÊ≠∑ÊôÇÔºö60ÂàÜÈêòÔºåÂº∑Â∫¶Ôºö78.5 mm/hr\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨15Ê¢ùÂèäÈôÑ‰ª∂IDFÊõ≤Á∑öÂÖ¨Âºè\nË™™ÊòéÔºöIDFÊõ≤Á∑öÂèçÊò†‰∏çÂêåÈáçÁèæÊúüËàáÊ≠∑ÊôÇ‰∏ãÁöÑÈôçÈõ®Âº∑Â∫¶ÔºåÁî®ÊñºÊ∞¥ÊñáÂàÜÊûêËàáÊ∞¥Âà©Ë®≠ÊñΩË®≠Ë®à„ÄÇ\n```\n\n### Êü•Ë©¢ÊîØÊè¥Ê∏ÖÂñÆ\n- ‰Ω†ÂèØ‰ª•Áõ¥Êé•Êü•Ë©¢ÊúâÂì™‰∫õÂèØÁî®ÁöÑÊùêÊñô„ÄÅÂú∞ÂçÄ„ÄÅÂ∑•Ê≥ïÁ≠âÔºö\n\n```\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õÂ∏∏Áî®ÊùêÊñôÔºü\nÂõûÂÇ≥Ôºö['‰∏ÄËà¨ÈªèÂúü', 'Á†ÇÂúü', 'Á§´Áü≥', 'Ê∑∑ÂáùÂúü', ...]\n\nÊü•Ë©¢ÔºöÂèØ‰ª•Êü•Ë©¢Âì™‰∫õÊ∞¥Ê∫ùÈã™Èù¢ÁöÑÊõºÂØß‰øÇÊï∏Ôºü\nÂõûÂÇ≥Ôºö['Á¥îÁ¥∞Á†Ç', 'Ê∑∑ÂáùÂúü', 'ÂÖ®Èù¢ÂØÜËçâÁîü', ...]\n\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õÂù°Èù¢‰øùË≠∑Â∑•Ê≥ïÔºü\nÂõûÂÇ≥Ôºö['ËçâÁöÆÊàñÁõ¥Êé•Êí≠Á®Æ', 'Âô¥Êí≠ËçâÁöÆ+Ê†ºÊ°Ü/ÂúüÂ∑•Á∂≤', ...]\n\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õIDFÂú∞ÈªûÔºü\nÂõûÂÇ≥Ôºö['Âè∞ÂåóÂ∏Ç', 'Êñ∞ÂåóÂ∏Ç', 'Âè∞‰∏≠Â∏Ç']\n```\n\n### Êü•Ë©¢Â§±ÊïóÊôÇËá™ÂãïÊèêÁ§∫\n- Ëã•Êü•Ë©¢ÊôÇËº∏ÂÖ•ÈåØË™§ÔºåÊúÉËá™ÂãïÂõûÂÇ≥ÊâÄÊúâÊîØÊè¥Êü•Ë©¢ÁöÑÈ†ÖÁõÆÔºö\n\n```\nËº∏ÂÖ•ÔºöÊü•Ë©¢‰∏çÂ≠òÂú®ÁöÑÊùêÊñôÂêçÁ®±\nÂõûÂÇ≥ÔºöÊü•ÁÑ°Ê≠§ÊùêÊñôÔºåÊîØÊè¥Êü•Ë©¢ÁöÑÊùêÊñôÊúâÔºö‰∏ÄËà¨ÈªèÂúü, Á†ÇÂúü, Á§´Áü≥, Ê∑∑ÂáùÂúü, ...\n```\n## ÂúüÁü≥Á±†ÊìãÂúüÁâÜÁ©©ÂÆöÂàÜÊûê\n\nÊ≠§ÂäüËÉΩÁî®ÊñºÂàÜÊûêÂúüÁü≥Á±†ÊìãÂúüÁâÜÁöÑÁ©©ÂÆöÊÄßÔºåÂåÖÊã¨‰∏ªÂãïÂíåË¢´ÂãïÂúüÂ£ìÂäõË®àÁÆó„ÄÇ\n\n### ‰ΩøÁî®ÊñπÊ≥ï\n\n```python\nresult = check_gabion_stability(\n    height=3.0,          # ÂúüÁü≥Á±†È´òÂ∫¶ (m)\n    width=2.0,           # ÂúüÁü≥Á±†ÂØ¨Â∫¶ (m)\n    wall_weight=100,     # ÊìãÂúüÁâÜÁ∏ΩÈáç (kN/m)\n    phi=30,              # ÂúüÂ£§ÂÖßÊë©Êì¶Ëßí (¬∞)\n    delta=20,            # ÁâÜËÉåÊë©Êì¶Ëßí (¬∞)ÔºåÈ†êË®≠ 0\n    theta=0,             # ÁâÜËÉåÂÇæÊñúËßí (¬∞)ÔºåÈ†êË®≠ 0\n    i=0,                 # Âú∞Ë°®ÂÇæÊñúËßí (¬∞)ÔºåÈ†êË®≠ 0\n    gamma=18,            # ÂúüÂ£§È£ΩÂíåÂñÆ‰ΩçÈáç (kN/m¬≥)ÔºåÈ†êË®≠ 18\n    friction_coef=0.5,   # Êë©Êì¶‰øÇÊï∏ÔºåÈ†êË®≠ 0.5\n    pressure_mode=\"active\"  # ÂúüÂ£ìÂäõÊ®°Âºè (\"active\" Êàñ \"passive\")ÔºåÈ†êË®≠ \"active\"\n)\n```\n\n### ÂõûÂÇ≥ÁµêÊûú\n\nÂáΩÊï∏ÂõûÂÇ≥‰∏ÄÂÄãÂ≠óÂÖ∏ÔºåÂåÖÂê´‰ª•‰∏ãÂÖßÂÆπÔºö\n\n- `success`: Â∏ÉÊûóÂÄºÔºåË°®Á§∫Ë®àÁÆóÊòØÂê¶ÊàêÂäü\n- `data`: Ë®àÁÆóÁµêÊûúÊï∏ÊìöÔºåÂåÖÂê´Ôºö\n  - `earth_pressure_coef`: ÂúüÂ£ìÂäõ‰øÇÊï∏\n  - `total_pressure`: Á∏ΩÂúüÂ£ìÂäõ (kN/m)\n  - `vertical_force`: ÂûÇÁõ¥ÂäõÂàÜÈáè (kN/m)\n  - `horizontal_force`: Ê∞¥Âπ≥ÂäõÂàÜÈáè (kN/m)\n  - `restoring_moment`: ÊäóÂÇæË¶ÜÂäõÁü© (kN¬∑m/m)\n  - `overturning_moment`: ÂÇæË¶ÜÂäõÁü© (kN¬∑m/m)\n  - `overturning_safety_factor`: ÊäóÂÇæË¶ÜÂÆâÂÖ®‰øÇÊï∏\n  - `sliding_safety_factor`: ÊäóÊªëÂãïÂÆâÂÖ®‰øÇÊï∏\n- `message`: Ë®àÁÆóÁµêÊûúÊëòË¶Å\n- `report`: ÂÆåÊï¥ÁöÑË®àÁÆóÂ†±ÂëäÊõ∏ÔºàMarkdown Ê†ºÂºèÔºâ\n\n### Ë®àÁÆóÂ†±ÂëäÊõ∏ÂÖßÂÆπ\n\nÂ†±ÂëäÊõ∏ÂåÖÂê´‰ª•‰∏ãÁ´†ÁØÄÔºö\n\n1. Ëº∏ÂÖ•ÂèÉÊï∏\n2. Ë®àÁÆóÂÖ¨Âºè\n3. Ë®àÁÆóÁµêÊûú\n4. Á©©ÂÆöË©ï‰º∞\n\n### UÂûãÊ∫ùÈãºÁ≠ãÈáèË®àÁÆó\n- Ëº∏ÂÖ•Ôºö\n  - height: Ê∫ùÈ´ò (m)\n  - wall_slope: Ê∫ùÂ£ÅÂÇæËßí (m)\n  - soil_slope: ÂúüÊñπÂÇæËßí (¬∞)\n  - soil_angle: ÂÆâÊÅØËßí (¬∞)\n  - effective_depth: ÊúâÊïàÂéöÂ∫¶ (m)\n  - soil_weight: ÂúüÈáç (kN/m¬≥)ÔºåÈ†êË®≠ 18.0\n- ÂõûÂÇ≥ÔºödictÔºåÂê´Ë®àÁÆóÁµêÊûúËàáÂ†±ÂëäÊõ∏\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÊ∑∑ÂáùÂúüÊ∫ùÔºåheight=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2ÔºåË®àÁÆóÊ∞¥Ê∫ùÊâÄÈúÄÁöÑÈãºÁ≠ãÈáèÔºåÂª∫Ë≠∞ÁöÑÈÖçÁ≠ã?\nÂõûÂÇ≥Ôºö\n{\n  \"success\": true,\n  \"data\": {\n    \"earth_pressure_coef\": 0.3333,\n    \"earth_pressure\": 6.750,\n    \"moment\": 3.375,\n    \"rebar_area\": 13.780\n  },\n  \"message\": \"ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333, ÂúüÂ£ìÂäõ P = 6.750 kN/m, ÂΩéÁü© M = 3.375 kN¬∑m/m, ÈãºÁ≠ãÈáè As = 13.780 cm¬≤/m\",\n  \"report\": \"„ÄêUÂûãÊ∫ùÈãºÁ≠ãÈáèË®àÁÆóÂ†±Âëä„Äë\\n\\nËº∏ÂÖ•ÂèÉÊï∏Ôºö\\n- Ê∫ùÈ´ò H = 1.500 m\\n- Ê∫ùÂ£ÅÂÇæËßí m = 0.500\\n- ÂúüÊñπÂÇæËßí i = 15.00¬∞\\n- ÂÆâÊÅØËßí œà = 30.00¬∞\\n- ÊúâÊïàÂéöÂ∫¶ d = 0.200 m\\n- ÂúüÈáç Œ≥ = 18.0 kN/m¬≥\\n\\nË®àÁÆóÂÖ¨ÂºèÔºö\\n1. ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = cos¬≤(œà+m) / [cos¬≤m¬∑(1+‚àöQ)¬≤]\\n   ÂÖ∂‰∏≠ Q = [sinœà¬∑sin(œà-i)] / [cos(m+i)¬∑cosm]\\n2. ÂúüÂ£ìÂäõ P = Œ≥¬∑H¬≤¬∑Ka / (2¬∑cosm)\\n3. ÂΩéÁü© M = Œ≥¬∑H¬≥¬∑Ka / (6¬∑cosm)\\n4. ÈãºÁ≠ãÈáè As = M / (fs¬∑d) √ó 10‚Å∂ / 1000\\n\\nË®àÁÆóÁµêÊûúÔºö\\n- ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333\\n- ÂúüÂ£ìÂäõ P = 6.750 kN/m\\n- ÂΩéÁü© M = 3.375 kN¬∑m/m\\n- ÈãºÁ≠ãÈáè As = 13.780 cm¬≤/m\"\n}\n```\n## ÈãºÁ≠ãÊü•Ë©¢ÂäüËÉΩ\nÊú¨Â∑•ÂÖ∑Êèê‰æõ‰ª•‰∏ãÈãºÁ≠ãË≥áÊñôÊü•Ë©¢ÂäüËÉΩÔºö\n\n1. ÂàóÂá∫ÊâÄÊúâÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®Ëôü\n   ```python\n   list_rebar_numbers()\n   ```\n\n2. Êü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁ∑®ËôüÁöÑË¶èÊ†ºË≥áÊñô\n   ```python\n   get_rebar_specs(rebar_number=\"#3\")\n   ```\n\n3. Ë®àÁÆóÈãºÁ≠ãÈáçÈáè\n   ```python\n   calculate_rebar_weight(rebar_number=\"#3\", length=10.0)\n   ```\n\nÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®ËôüÂåÖÊã¨Ôºö#3„ÄÅ#4„ÄÅ#5„ÄÅ#6„ÄÅ#7„ÄÅ#8„ÄÅ#9„ÄÅ#10„ÄÅ#11ÔºåÊØèÂÄãÁ∑®ËôüÂ∞çÊáâÁöÑË¶èÊ†ºË≥áÊñôÂåÖÊã¨Ôºö\n- Áõ¥ÂæëÔºàmmÔºâ\n- Êà™Èù¢Á©çÔºàcm¬≤Ôºâ\n- ÂñÆ‰ΩçÈáçÈáèÔºàkg/mÔºâ\n- Âë®Èï∑ÔºàmmÔºâ\n\n### ÈãºÁ≠ãË¶èÊ†ºÊü•Ë©¢\nËº∏ÂÖ•„ÄåÈãºÁ≠ãË¶èÊ†º #3„ÄçÊàñ„ÄåÈãºÁ≠ãË≥áÊñô #3„ÄçÂèØÊü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁöÑË©≥Á¥∞Ë≥áÊñôÔºåÂåÖÊã¨Ôºö\n- ÈãºÁ≠ãÁ∑®Ëôü\n- Áõ¥Âæë (mm)\n- Êà™Èù¢Á©ç (cm¬≤)\n- ÂñÆ‰ΩçÈáçÈáè (kg/m)\n- Âë®Èï∑ (mm)\n\n### ÈãºÁ≠ãÈáçÈáèË®àÁÆó\nËº∏ÂÖ•„ÄåÈãºÁ≠ãÈáçÈáè Èï∑Â∫¶ 6 #3„ÄçÂèØË®àÁÆóÊåáÂÆöÈï∑Â∫¶ÁöÑÈãºÁ≠ãÈáçÈáèÔºå‰æãÂ¶ÇÔºö\n- Èï∑Â∫¶Ôºö6m\n- ÈãºÁ≠ãÔºö#3\n- ÈáçÈáèÔºö2.04 kg\n\n### ÈãºÁ≠ãÊà™Èù¢Á©çÊü•Ë©¢\nËº∏ÂÖ•„ÄåÈãºÁ≠ãÊà™Èù¢Á©ç #3„ÄçÂèØÊü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁöÑÊà™Èù¢Á©çÔºå‰æãÂ¶ÇÔºö\n- #3 ÈãºÁ≠ãÊà™Èù¢Á©çÔºö0.71 cm¬≤\n\n### ÊâÄÊúâÈãºÁ≠ãÂàóË°®\nËº∏ÂÖ•„ÄåÊâÄÊúâÈãºÁ≠ã„ÄçÂèØÂàóÂá∫ÊâÄÊúâÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®ËôüÔºåÂåÖÊã¨Ôºö\n- #3 Ëá≥ #11 ÈãºÁ≠ã\n- ÂêÑÈãºÁ≠ãÁöÑÂü∫Êú¨Ë¶èÊ†º\n\n### U ÂûãÊ∫ùÈÖçÁ≠ãÂª∫Ë≠∞\nËº∏ÂÖ•„ÄåUÂûãÊ∫ùÈÖçÁ≠ã Èù¢Á©ç 10cm2„ÄçÂèØÊü•Ë©¢Âª∫Ë≠∞ÁöÑÈÖçÁ≠ãÊñπÂºèÔºåÁ≥ªÁµ±ÊúÉÔºö\n1. Ê†πÊìöËº∏ÂÖ•ÁöÑÈãºÁ≠ãÊñ∑Èù¢Á©ç (cm¬≤/m)\n2. Ëá™ÂãïË®àÁÆó‰∏¶Âª∫Ë≠∞‰∏ªÁ≠ãËàáÂâØÁ≠ãÁöÑÈÖçÁ≠ãÊñπÂºè\n3. ÈñìË∑ùÊúÉÂèñÊï¥Âà∞ÊúÄÊé•ËøëÁöÑ 5cm\n4. Êèê‰æõÂÆåÊï¥ÁöÑÈÖçÁ≠ãÂª∫Ë≠∞Â†±Âëä\n\n### ÈÄïÊµÅ‰øÇÊï∏Êü•Ë©¢\nÈÄïÊµÅ‰øÇÊï∏Êü•Ë©¢Â∑•ÂÖ∑ÂèØ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÔºåÊü•Ë©¢‰∏çÂêåÂúüÂú∞Âà©Áî®È°ûÂûãÊàñÈõÜÊ∞¥ÂçÄÁãÄÊ≥ÅÁöÑÈÄïÊµÅ‰øÇÊï∏CÂÄº„ÄÇ\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ëæ≤Ê•≠ÂçÄÁöÑÈÄïÊµÅ‰øÇÊï∏\nÂõûÂÇ≥ÔºöÈÄïÊµÅ‰øÇÊï∏ C = 0.53\n‰æÜÊ∫êÔºöËæ≤Ê•≠ÂçÄÈÄïÊµÅ‰øÇÊï∏ÁØÑÂúçÔºö0.45~0.60ÔºåÊé°Áî®Âπ≥ÂùáÂÄº0.53\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÄïÊµÅ‰øÇÊï∏CÂÄºÂèçÊò†ÈõÜÊ∞¥ÂçÄÂú∞Ë°®ÁâπÊÄßÂ∞çÈôçÈõ®ÈÄïÊµÅ‰πãÂΩ±ÈüøÔºåÈñãÁôº‰∏≠ÁãÄÊÖãCÂÄº‰ª•1.0Ë®àÁÆó„ÄÇ\n\nËº∏ÂÖ•ÔºöÈñãÁôº‰∏≠ÁöÑÂπ≥Âù¶ËÄïÂú∞ÈÄïÊµÅ‰øÇÊï∏ÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÈÄïÊµÅ‰øÇÊï∏ C = 0.85\n‰æÜÊ∫êÔºöÈñãÁôº‰∏≠ÁãÄÊÖãÔºåÂπ≥Âù¶ËÄïÂú∞\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÄïÊµÅ‰øÇÊï∏CÂÄºÂèçÊò†ÈõÜÊ∞¥ÂçÄÂú∞Ë°®ÁâπÊÄßÂ∞çÈôçÈõ®ÈÄïÊµÅ‰πãÂΩ±ÈüøÔºåÈñãÁôº‰∏≠ÁãÄÊÖãCÂÄºËºÉÈ´ò„ÄÇ\n```\n\nÊü•Ë©¢ÁµêÊûúÂåÖÂê´Ôºö\n- ÈÄïÊµÅ‰øÇÊï∏ÂÄº\n- ‰æÜÊ∫êË™™Êòé\n- Ë¶èÁØÑ‰æùÊìö\n- ÂúüÂú∞Âà©Áî®È°ûÂûã\n- ÈñãÁôºÁãÄÊÖã\n- Êï∏ÂÄºÁØÑÂúçÔºàÂ¶ÇÈÅ©Áî®Ôºâ\n\n### USLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑\nUSLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑ÂèØ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºåÊü•Ë©¢ÂúüÂ£§ÊµÅÂ§±ÈáèË®àÁÆóÊâÄÈúÄÁöÑÂêÑÈ†ÖÂõ†Â≠êÂÄº„ÄÇ\n\n#### ÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Âè∞ÂåóÂ∏ÇÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏RÂÄº\nÂõûÂÇ≥ÔºöÂú∞ÂçÄ Âè∞ÂåóÂ∏Ç ÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R = 350ÔºåÂñÆ‰ΩçÔºöÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê/ÂÖ¨È†É¬∑Â∞èÊôÇ¬∑Âπ¥\n\nËº∏ÂÖ•ÔºöÂπ¥Âπ≥ÂùáÈôçÈõ®Èáè2000mmÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏RÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂπ¥Âπ≥ÂùáÈôçÈõ®Èáè 2000 mm Êé®‰º∞ÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R = 600ÔºåÂñÆ‰ΩçÔºöÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê/ÂÖ¨È†É¬∑Â∞èÊôÇ¬∑Âπ¥\n```\n\n#### ÂúüÂ£§Ê≤ñËùïÊåáÊï∏ K ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöÁ†ÇÂúüÁöÑÊ≤ñËùïÊåáÊï∏KÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂúüÂ£§È°ûÂûã Á†ÇÂúü ÁöÑÊ≤ñËùïÊåáÊï∏ K = 0.30ÔºåÂñÆ‰ΩçÔºöÂÖ¨Âô∏¬∑ÂÖ¨È†É¬∑Âπ¥/ÂÖ¨È†É¬∑ÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢ÈªèÂúüÁöÑKÂÄº\nÂõûÂÇ≥ÔºöÂúüÂ£§È°ûÂûã ÈªèÂúü ÁöÑÊ≤ñËùïÊåáÊï∏ K = 0.20ÔºåÂñÆ‰ΩçÔºöÂÖ¨Âô∏¬∑ÂÖ¨È†É¬∑Âπ¥/ÂÖ¨È†É¬∑ÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê\n```\n\n#### Ë¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöËçâÂú∞ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠êCÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂúüÂú∞Âà©Áî®È°ûÂûã ËçâÂú∞ ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C = 0.05ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ëæ≤Âú∞ÁöÑCÂÄº\nÂõûÂÇ≥ÔºöÂúüÂú∞Âà©Áî®È°ûÂûã Ëæ≤Âú∞ ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C = 0.30ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n```\n\n#### Ê∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠ê P ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöÁ≠âÈ´òËÄï‰ΩúÁöÑÊ∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠êPÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÊ∞¥Âúü‰øùÊåÅÊé™ÊñΩ Á≠âÈ´òËÄï‰Ωú ÁöÑËôïÁêÜÂõ†Â≠ê P = 0.50ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ê¢ØÁî∞ÁöÑPÂÄº\nÂõûÂÇ≥ÔºöÊ∞¥Âúü‰øùÊåÅÊé™ÊñΩ Ê¢ØÁî∞ ÁöÑËôïÁêÜÂõ†Â≠ê P = 0.20ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n```\n\nÊü•Ë©¢ÁµêÊûúÂåÖÂê´Ôºö\n- Âõ†Â≠êÂÄº\n- ÂñÆ‰Ωç\n- ‰æÜÊ∫êË™™Êòé\n- Ë¶èÁØÑ‰æùÊìö\n\n\n## ‰ΩøÁî®ÁØÑ‰æã(ÂäüËÉΩÂ∞éÂêëÔºå‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÂ∞çË©±ÔºåËÆìÂ§ßÊ®°ÂûãËá™ÂãïË™øÁî®Áõ∏ÈóúÂ∑•ÂÖ∑‰æÜÊ±ÇËß£)\n\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈáçÁèæÊúü50Âπ¥ÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË®àÁÆóË©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè?\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË´ãÂàÜÂà•Ë®àÁÆóÈáçÁèæÊúü25Âπ¥„ÄÅ50Âπ¥Ë©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè? ‰∏¶Ëº∏Âá∫ÂÆåÊï¥Ë®àÁÆóÂºèÁöÑÂ†±ÂëäÊõ∏„ÄÇ\n- ‰æùÊìö25Âπ¥ÁöÑÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∑±70cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåË®≠Ë®àÂù°Â∫¶ÂÆπË®±ÁØÑÂúç?\n- ‰æùÊìöÈÄôÂÄãÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∫ùÊ∑±60cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåÂù°Â∫¶2.5%ÔºåÊ™¢Ê†∏Ë©≤Ë®≠Ë®àÊòØÂê¶OKÔºå‰∏¶Âá∫ÂÖ∑‰∏Ä‰ªΩÂÆåÊï¥ÁöÑÊ™¢Ê†∏Â†±ÂëäÊõ∏„ÄÇ\n- Ê∞¥Ê∫ùÁ∑®Ëôü,B(cm),H(cm),Âù°Â∫¶(%),ÊµÅÈáè(CMS),Ê∞¥Ê∫ùÈã™Èù¢ÂûãÂºè\n  1-1,40,60,1.5,0.25,ÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ù\n  1-2,40,70,1.5,0.30,ÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ù\n  Ê™¢Ê†∏‰∏äÂàóÊ∞¥Ê∫ùË®≠Ë®àÔºå‰ª•markdownË°®Ê†ºÂûãÂºèËº∏Âá∫ÁµêÊûú„ÄÇ \n- Êúâ‰∏ÄÂÄãÁ†¥Á¢éÂ≤©Áõ§ÁöÑÂù°Èù¢ÔºåËßíÂ∫¶Á¥Ñ60Â∫¶Ôºå‰ΩçÊñºÁÜ±Â∏∂Â§öÈõ®Âú∞ÂçÄÔºåË´ãÊèêÂá∫Âù°Èù¢‰øùË≠∑Âª∫Ë≠∞„ÄÇ\n- ÁÑ°Âü∫Á§éÁöÑÊ∑∑ÂáùÂúüÈáçÂäõÂºèÊìãÂúüÁâÜÔºåÂÖ®È´ò2.3MÔºåÁâÜÈ†ÇÂØ¨50cmÔºåÁâÜÂ∫ïÂØ¨100cmÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m3ÔºåÊë©Êì¶Ëßí30Â∫¶ÔºåÂáùËÅöÂäõ10kPaÔºåÂú∞‰∏ãÊ∞¥‰ΩçÂú®ÁâÜÈ†Ç‰ª•‰∏ã2MÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§éÔºåË´ãË™øÁî®MCPÂ∑•ÂÖ∑ÔºåÊ™¢Ê†∏ÂÖ∂Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÁ©©ÂÆöÊÄßÔºå‰∏¶Ëº∏Âá∫ÂÆåÊï¥Ë®àÁÆóÂ†±ÂëäÊõ∏„ÄÇ\n- Ê∑∑ÂáùÂúüÊ∫ùÔºåheight=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2ÔºåË™øÁî®Â∑•ÂÖ∑ÔºåË®àÁÆóÊ∞¥Ê∫ùÊâÄÈúÄÁöÑÈãºÁ≠ãÈáèÂèäÂª∫Ë≠∞ÈÖçÁ≠ã?\n- Âè∞‰∏≠ÁöÑ \"Âπ¥Âπ≥ÂùáÂúüÂ£§ÊµÅÂ§±Èáè\"Â¶Ç‰Ωï‰º∞ÁÆó? ÈúÄÊèê‰æõÂì™‰∫õÂèÉÊï∏? \n- Ë´ãË®àÁÆó‰∏ÄÊ¢ùÁõ¥Âæë60ÂÖ¨ÂàÜÁöÑÊ∑∑ÂáùÂúüÂúìÂΩ¢ÁÆ°Ê∏†ÔºåÊµÅÈáè0.5cmsÔºåÂù°Â∫¶1.2%ÔºåÊõºÂØß‰øÇÊï∏0.013ÁöÑÊµÅÈÄüÂíåÊµÅÊ∑±\n- Ë®àÁÆóÂØ¨30ÂÖ¨ÂàÜÔºåÈ´ò50cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè0.5cmsÔºåÂù°Â∫¶1.2%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±?\n- Ë®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè1cmsÔºåÂù°Â∫¶5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\n- Ë®àÁÆóÂ∫ïÂØ¨50ÂÖ¨ÂàÜÔºåÈ†ÇÂØ¨100cmÔºåÈ´ò70cmÁöÑ\"Âπ≥Â∏∏Á†ÇÂúüÊ¢ØÂΩ¢Ê∫ù\"ÔºåÊµÅÈáè0.2cmsÔºåÂù°Â∫¶0.7%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈáçÁèæÊúü50Âπ¥ÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË®àÁÆóË©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè? \n- ‰æùÊìöÈÄôÂÄãÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∫ùÊ∑±60cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåÂù°Â∫¶2.5%ÔºåÊ™¢Ê†∏Ë©≤Ë®≠Ë®àÊòØÂê¶OKÔºå‰∏¶Âá∫ÂÖ∑‰∏Ä‰ªΩÂÆåÊï¥ÁöÑÊ™¢Ê†∏Â†±ÂëäÊõ∏(Âê´ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè)„ÄÇ\n- ‰ΩøÁî® Gemini 2.5 Pro ÁöÑËº∏Âá∫ÁµêÊûúÂ¶Ç‰∏ã:Ë™øÁî®Â∑•ÂÖ∑Ôºöquery_idf_curve(ÈôçÈõ®Âº∑Â∫¶-Ê≠∑ÊôÇ-È†ªÁéáÔºàIDFÔºâÊõ≤Á∑öÊü•Ë©¢)„ÄÅquery_runoff_coeff(ÈÄïÊµÅ‰øÇÊï∏CÂÄºÊü•Ë©¢Â∑•ÂÖ∑)„ÄÅcalc_catchment_runoff(ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆó)„ÄÅget_manning_n(Â∏∏Ë¶ãÊùêÊñôÁöÑÊõºÂØß‰øÇÊï∏Êü•Ë©¢)„ÄÅcalc_channel_section_flow(ÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆó)\n\n# Ê∑∑ÂáùÂúüÊ∫ùË®≠Ë®àÊ™¢Ê†∏Â†±ÂëäÊõ∏\n\n## 1. Ë®≠Ë®àÁõÆÊ®ô\n\nÊú¨Â†±ÂëäÊó®Âú®Ê™¢Ê†∏ÊâÄË®≠Ë®à‰πãÊ∑∑ÂáùÂúüÊ∫ùÊñ∑Èù¢ÔºåÊòØÂê¶ËÉΩÂÆâÂÖ®ÂÆπÁ¥ç‰∏äÊ∏∏ÈõÜÊ∞¥ÂçÄÊñºÁâπÂÆöÈáçÁèæÊúü‰∏ãÊâÄÁî¢Áîü‰πãÊúÄÂ§ßÈÄïÊµÅÈáè„ÄÇ\n\n## 2. ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆóÊëòË¶Å\n\n*   **ÈõÜÊ∞¥ÂçÄÊ¢ù‰ª∂:**\n    *   Èù¢Á©ç (A): 5.0 ÂÖ¨È†É (ha)\n    *   ÂúüÂú∞Âà©Áî®: Âπ≥Âù¶ËÄïÂú∞\n    *   Âú∞Èªû: Êñ∞ÂåóÂ∏Ç\n*   **Ë®≠Ë®àÊ¢ù‰ª∂:**\n    *   ÈáçÁèæÊúü (T): 50 Âπ¥\n    *   ÈôçÈõ®Âª∂ÊôÇ (D): 60 ÂàÜÈêò\n*   **Ë®àÁÆóÂèÉÊï∏:**\n    *   ÈôçÈõ®Âº∑Â∫¶ (I): 140.0 mm/hr (‰æùÊìöÊñ∞ÂåóÂ∏Ç50Âπ¥60ÂàÜÈêòIDFÊõ≤Á∑ö)\n    *   ÈÄïÊµÅ‰øÇÊï∏ (C): 0.53 (Âπ≥Âù¶ËÄïÂú∞ÔºåÂèñË¶èÁØÑÂπ≥ÂùáÂÄº)\n*   **Ë®àÁÆóÂÖ¨Âºè (ÂêàÁêÜÂåñÂÖ¨Âºè):** Q = (C √ó I √ó A) / 360\n*   **Ë®àÁÆóÁµêÊûú (Ë®≠Ë®àÊµÅÈáè Q_design):**\n    *   Q_design = (0.53 √ó 140.0 √ó 5.0) / 360 ‚âà **1.03 cms**\n\n## 3. Ê∑∑ÂáùÂúüÊ∫ùË®≠Ë®àÂèÉÊï∏\n\n*   Êñ∑Èù¢ÂûãÂºè: Áü©ÂΩ¢\n*   Ê∏†ÈÅìÊùêË≥™: Ê∑∑ÂáùÂúü\n*   ÊõºÂØßÁ≥ôÂ∫¶‰øÇÊï∏ (n): 0.012\n*   Ê∫ùÊ∏†ÂØ¨Â∫¶ (b): 50 cm (0.5 m)\n*   Ê∫ùÊ∏†Ê∑±Â∫¶ (H): 60 cm (0.6 m)\n*   Ê∏†ÈÅìÂù°Â∫¶ (S): 2.5 % (0.025 m/m)\n\n## 4. Ê∞¥ÁêÜË®àÁÆóËàáÊ™¢Ê†∏\n\n‰ΩøÁî®ÊõºÂØßÂÖ¨ÂºèÔºåÈáùÂ∞çË®≠Ë®àÊµÅÈáè (Q = 1.03 cms) Âèä‰∏äËø∞Ê∫ùÊ∏†ÂèÉÊï∏ÈÄ≤Ë°åÊ∞¥ÁêÜË®àÁÆóÔºö\n\n*   **Ë®àÁÆóÂÖ¨Âºè:**\n    *   Q = A √ó V\n    *   V = (1/n) √ó R^(2/3) √ó S^(1/2)\n    *   A = b √ó y\n    *   P = b + 2y\n    *   R = A / P\n*   **Ë®àÁÆóÁµêÊûú:**\n    *   ÂØ¶ÈöõÊµÅÈÄü (V): **4.014 m/s**\n    *   ÂØ¶ÈöõÊ∞¥Ê∑± (y): **0.513 m (51.3 cm)**\n\n*   **Ê™¢Ê†∏È†ÖÁõÆ:**\n    *   **ÈÄöÊ∞¥ËÉΩÂäõ (Capacity Check):**\n        *   Ë®àÁÆóÊ∞¥Ê∑± (y) = 51.3 cm\n        *   Ë®≠Ë®àÊ∫ùÊ∑± (H) = 60.0 cm\n        *   **Âá∫Ê∞¥È´ò (Freeboard) = H - y = 60.0 - 51.3 = 8.7 cm**\n        *   *Ë©ï‰º∞:* Ê∫ùÊ∏†Êñ∑Èù¢ÂèØÂÆπÁ¥çË®≠Ë®àÊµÅÈáèÔºåÂ∞öÊúâ 8.7 cm ÁöÑÂá∫Ê∞¥È´ò„ÄÇ‰∏ÄËà¨Â∑•Á®ãË®≠Ë®àÂª∫Ë≠∞‰øùÁïôÁ¥Ñ10-20%Ê∫ùÊ∑±ÊàñËá≥Â∞ë10-15cmÁöÑÂá∫Ê∞¥È´ò‰ΩúÁÇ∫ÂÆâÂÖ®È§òË£ïÔºåÊ≠§ËôïÂá∫Ê∞¥È´òÁï•È°Ø‰∏çË∂≥ÔºåÂª∫Ë≠∞ÂèØËÄÉÊÖÆÂä†Â§ßÊ∫ùÊ∑±‰ª•Â¢ûÂä†ÂÆâÂÖ®‰øÇÊï∏„ÄÇ\n    *   **ÊµÅÈÄüÊ™¢Ê†∏ (Velocity Check):**\n        *   Ë®àÁÆóÊµÅÈÄü (V) = 4.014 m/s\n        *   Ê∑∑ÂáùÂúüÂÆπË®±ÊµÅÈÄüÁØÑÂúç (V_allowable): 4.57 ~ 6.1 m/s (‰æùÊìöÊ∞¥‰øùÊäÄË°ìË¶èÁØÑÂ∏∏Ë¶ãÂÄº)\n        *   *Ë©ï‰º∞:* Ë®àÁÆóÊµÅÈÄü 4.014 m/s ‰ΩéÊñºÊ∑∑ÂáùÂúüÁöÑÂÆπË®±ÊµÅÈÄü‰∏ãÈôê (4.57 m/s)ÔºåË°®Á§∫Ê≠§ÊµÅÈÄüÂ∞çÊ∑∑ÂáùÂúüÊ∏†ÈÅìÊú¨Ë∫´ÊòØÂÆâÂÖ®ÁöÑÔºå‰∏çÊúÉÈÄ†ÊàêÊ∏†ÈÅìÊ≤ñÂà∑„ÄÇ\n\n## 5. ÁµêË´ñËàáÂª∫Ë≠∞\n\n1.  **ÈÄöÊ∞¥ËÉΩÂäõ:** Ë®≠Ë®à‰πãÊ∑∑ÂáùÂúüÊ∫ùÊñ∑Èù¢ (ÂØ¨50cm, Ê∑±60cm) **ÂèØ‰ª•ÂÆπÁ¥ç** 1.03 cms ÁöÑË®≠Ë®àÊµÅÈáè„ÄÇ\n2.  **ÊµÅÈÄü:** Ë®àÁÆóÊµÅÈÄü 4.014 m/s Âú®Ê∑∑ÂáùÂúüÁöÑÂÆâÂÖ®ÂÆπË®±ÁØÑÂúçÂÖß„ÄÇ\n3.  **Âá∫Ê∞¥È´ò:** Ë®àÁÆóÊâÄÂæóÁöÑÂá∫Ê∞¥È´òÁÇ∫ 8.7 cmÔºåÁõ∏Â∞çÂÅè‰Ωé„ÄÇ\n4.  **Âª∫Ë≠∞:** ÈõñÁÑ∂Ê≠§Êñ∑Èù¢Âú®Ë®àÁÆó‰∏äÂèØÈÄöÈÅéË®≠Ë®àÊµÅÈáèÔºå‰ΩÜËÄÉÈáèÂà∞ÂØ¶ÈöõÊñΩÂ∑•Ë™§Â∑Æ„ÄÅÊ≥•Á†ÇÊ∑§Á©ç„ÄÅÊàñÊú™‰æÜÊµÅÈáèÂ¢ûÂä†ÁöÑÂèØËÉΩÊÄßÔºå**Âª∫Ë≠∞Â∞áÊ∫ùÊ∏†Ê∑±Â∫¶ÈÅ©Â∫¶Âä†Â§ß (‰æãÂ¶ÇÂ¢ûÂä†Ëá≥ 70 cm Êàñ‰ª•‰∏ä)**Ôºå‰ª•Êèê‰æõÊõ¥ÂÖÖË∂≥ÁöÑÂá∫Ê∞¥È´òÔºåÂ¢ûÂä†Êï¥È´îË®≠Ë®àÁöÑÂÆâÂÖ®ÊÄßËàáÈüåÊÄß„ÄÇ\n\n---\n\n## ÊéàÊ¨ä\n\nÊú¨Â∞àÊ°àÊé°Áî® MIT License ÊéàÊ¨äÔºåÊ≠°ËøéËá™Áî±‰ΩøÁî®ËàáË≤¢Áçª„ÄÇ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "mcp",
        "tutumomo",
        "civil tools",
        "civil engineering",
        "calculation tools"
      ],
      "category": "scientific-research-tools"
    },
    "vitamin3615--uniprot-mcp-server": {
      "owner": "vitamin3615",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/vitamin3615/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vitamin3615.webp",
      "description": "Fetch protein information seamlessly from the UniProt database, including detailed data, sequences, functions, and structures. Supports integration with MCP-compatible AI applications for enhanced protein analysis.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-01T01:49:15Z",
      "readme_content": "# UniProt MCP Server\n\nAn MCP (Model Context Protocol) server that allows language models to fetch protein information from the UniProt database. This server can be integrated with Claude Desktop and other MCP-compatible AI applications.\n\n## Features\n\n- **Search UniProt**: Search for proteins by name, gene, or other criteria\n- **Get Protein Details**: Retrieve detailed information about a protein\n- **Get Protein Sequences**: Fetch amino acid sequences\n- **Get Protein Functions**: Retrieve functional annotations including GO terms\n- **Get Protein Structures**: Find related PDB structures\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n\n2. Create a virtual environment (optional but recommended):\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Local Setup with Claude Desktop\n\n1. Make the server script executable:\n   ```bash\n   chmod +x server.py\n   ```\n\n2. Configure Claude Desktop to use this MCP server:\n\n   ### For macOS/Linux:\n   ```bash\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   ### For Windows:\n   ```powershell\n   code $env:AppData\\Claude\\claude_desktop_config.json\n   ```\n\n3. Add the following configuration (update the path to match your actual file location):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"uniprot\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/ABSOLUTE/PATH/TO/uniprot-mcp-server/server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Restart Claude Desktop\n\n## Usage Examples\n\nOnce set up with Claude Desktop, you can ask questions like:\n\n- \"Find proteins related to Alzheimer's disease\"\n- \"Get the sequence for protein P05067 (Amyloid-beta precursor protein)\"\n- \"What are the functions of the TP53 protein?\"\n- \"Are there any known 3D structures for BRCA1 protein?\"\n\n## Development\n\nTo run the server directly for testing:\n\n```bash\npython server.py\n```\n\n## Testing Utilities\n\nThis project includes two testing utilities to verify functionality:\n\n### 1. Server Health Check\n\nVerifies that the MCP server starts correctly without errors:\n\n```bash\npython server_health_check.py\n```\n\n### 2. UniProt API Test\n\nTests connectivity with the UniProt API and verifies that all API calls work as expected:\n\n```bash\npython test_server.py\n```\n\nRun these tests before using the server to ensure everything is properly configured.\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "protein",
        "vitamin3615",
        "uniprot",
        "vitamin3615 uniprot",
        "protein information",
        "uniprot database"
      ],
      "category": "scientific-research-tools"
    },
    "vrtejus--pymol-mcp": {
      "owner": "vrtejus",
      "name": "pymol-mcp",
      "url": "https://github.com/vrtejus/pymol-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vrtejus.webp",
      "description": "Connects PyMOL to Claude AI for natural language interaction with molecular visualization and analysis, enabling intelligent command parsing and structural analysis.",
      "stars": 22,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T21:34:23Z",
      "readme_content": "# PyMOL-MCP: Integrating PyMOL with Claude AI\n\nPyMOL-MCP connects PyMOL to Claude AI through the Model Context Protocol (MCP), enabling Claude to directly interact with and control PyMOL. This powerful integration allows for conversational structural biology, molecular visualization, and analysis through natural language.\n\n\n\nhttps://github.com/user-attachments/assets/687f43dc-d45e-477e-ac2b-7438e175cb36\n\n\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to PyMOL through a socket-based server\n- **Intelligent command parsing**: Natural language processing for PyMOL commands\n- **Molecular visualization control**: Manipulate representations, colors, and views\n- **Structural analysis**: Perform measurements, alignments, and other analyses\n- **Code execution**: Run arbitrary Python code in PyMOL from Claude\n\n## Installation Guide\n\n### Prerequisites\n\n- PyMOL installed on your system\n- Claude for Desktop\n- Python 3.10 or newer\n- Git\n\n### Step 1: Install the UV Package Manager\n\n**On macOS:**\n\n```bash\nbrew install uv\n```\n\n**On Windows:**\n\n```bash\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nset Path=C:\\Users\\[YourUsername]\\.local\\bin;%Path%\n```\n\nFor other platforms, visit the [UV installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Step 2: Clone the Repository\n\n```bash\ngit clone https://github.com/vrtejus/pymol-mcp\ncd pymol-mcp\n```\n\n### Step 3: Set Up the Environment\n\nCreate and activate a Python virtual environment:\n\n```bash\npython -m venv venv\n```\n\n**On macOS/Linux:**\n\n```bash\nsource venv/bin/activate\n```\n\n**On Windows:**\n\n```bash\nvenv\\Scripts\\activate\n```\n\n### Step 4: Install Dependencies\n\nWith the virtual environment activated:\n\n```bash\npip install mcp\n```\n\n### Step 5: Configure Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Claude > Settings > Developer > Edit Config\n3. This will open the `claude_desktop_config.json` file\n4. Add the MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"[Full path to your venv python]\",\n      \"args\": [\"[Full path to pymol_mcp_server.py]\"]\n    }\n  }\n}\n```\n\nFor example:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"/Users/username/pymol-mcp/venv/bin/python\",\n      \"args\": [\"/Users/username/pymol-mcp/pymol_mcp_server.py\"]\n    }\n  }\n}\n```\n\n> **Note:** Use the actual full paths on your system. On Windows, use forward slashes (/) instead of backslashes.\n\n### Step 6: Install the PyMOL Plugin\n\n1. Open PyMOL\n2. Go to Plugin ‚Üí Plugin Manager\n3. Click on \"Install New Plugin\" tab\n4. Select \"Choose file...\" and navigate to the cloned repository\n5. Select the `pymol-mcp-socket-plugin/__init__.py` file\n6. Click \"Open\" and follow the prompts to install the plugin\n\n## Usage\n\n### Starting the Connection\n\n1. In PyMOL:\n\n   - Go to Plugin ‚Üí PyMOL MCP Socket Plugin\n   - Click \"Start Listening\"\n   - The status should change to \"Listening on port 9876\"\n\n2. In Claude Desktop:\n   - You should see a hammer icon in the tools section when chatting\n   - Click it to access the PyMOL tools\n\n### Example Commands\n\nHere are some examples of what you can ask Claude to do:\n\n- \"Load PDB 1UBQ and display it as cartoon\"\n- \"Color the protein by secondary structure\"\n- \"Highlight the active site residues with sticks representation\"\n- \"Align two structures and show their differences\"\n- \"Calculate the distance between these two residues\"\n- \"Save this view as a high-resolution image\"\n\n## Troubleshooting\n\n- **Connection issues**: Make sure the PyMOL plugin is listening before attempting to connect from Claude\n- **Command errors**: Check the PyMOL output window for any error messages\n- **Plugin not appearing**: Restart PyMOL and check that the plugin was correctly installed\n- **Claude not connecting**: Verify the paths in your Claude configuration file are correct\n\n## Limitations & Notes\n\n- The socket connection requires both PyMOL and Claude to be running on the same machine\n- Some complex operations may need to be broken down into simpler steps\n- Always save your work before using experimental features\n- Join our Bio-MCP Community to troubleshoot, provide feedback & improve Bio-MCPS! https://join.slack.com/t/bio-mcpslack/shared_invite/zt-31z4pho39-K5tb6sZ1hUvrFyoPmKihAA\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pymol",
        "molecular",
        "parsing",
        "molecular visualization",
        "vrtejus pymol",
        "connects pymol"
      ],
      "category": "scientific-research-tools"
    },
    "waldzellai--model-enhancement-servers": {
      "owner": "waldzellai",
      "name": "model-enhancement-servers",
      "url": "https://github.com/waldzellai/model-enhancement-servers",
      "imageUrl": "/freedevtools/mcp/pfp/waldzellai.webp",
      "description": "Enables language models to perform visual and spatial reasoning by creating and manipulating diagrammatic representations such as graphs and flowcharts. Supports multi-modal outputs and iterative refinement through a flexible visual element manipulation system.",
      "stars": 63,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T04:52:01Z",
      "readme_content": "# Cognitive Enhancement MCP Servers\n\nA collection of Model Context Protocol servers that provide cognitive enhancement tools for large language models.\n\n## Servers\n\nThis monorepo contains the following MCP servers:\n\n1. **Structured Argumentation** - A server for formal dialectical reasoning\n2. **Visual Reasoning** - A server for diagrammatic thinking and spatial representation\n3. **Scientific Method** - A server for hypothesis testing and evidence evaluation\n4. **Analogical Reasoning** - A server for structured metaphorical thinking\n5. **Metacognitive Monitoring** - A server for knowledge assessment and confidence tracking\n6. **Decision Framework** - A server for structured decision analysis\n7. **Collaborative Reasoning** - A server for multi-perspective problem solving\n8. **Ethical Reasoning** - A server for evaluating actions with moral frameworks\n9. **Bias Detection** - A server for flagging potentially biased wording\n10. **Constraint Solver** - A server for validating logical and numeric constraints\n11. **Narrative Planner** - A server for generating simple story outlines\n12. **Goal Tracker** - A server for maintaining and completing objectives\n13. **Multimodal Synthesizer** - A server for combining text and image descriptions\n\n## Potential Future Servers\n\nThe following server ideas are under consideration to further extend model reasoning capabilities:\n\n1. **Emotion-Aware Interaction** - Provides sentiment tracking and mood-aware responses to improve empathic communication.\n2. **Long-Term Memory** - Maintains persistent context across sessions for continuity and recall of past interactions.\n\n\n## Installation\n\nEach server can be installed individually:\n\n```bash\n# Using npm\nnpm install @waldzellai/structured-argumentation\n\n# Using yarn\nyarn add @waldzellai/structured-argumentation\n```\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"structured-argumentation\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@waldzellai/structured-argumentation\"\n      ]\n    }\n  }\n}\n```\n\n## Docker\n\nAll servers are available as Docker images:\n\n```bash\ndocker run --rm -i waldzellai/structured-argumentation\n```\n\n## Development\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/waldzellai/model-enhancement-servers.git\ncd model-enhancement-servers\nnpm install\n```\n\nBuild all packages:\n\n```bash\nnpm run build\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrammatic",
        "visual",
        "models",
        "diagrammatic representations",
        "flexible visual",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wheattoast11--openrouter-deep-research-mcp": {
      "owner": "wheattoast11",
      "name": "openrouter-deep-research-mcp",
      "url": "https://github.com/wheattoast11/openrouter-deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wheattoast11.webp",
      "description": "Connects to specialized AI agents for conducting complex research tasks through an orchestration system that enables multi-stage planning and parallel execution, resulting in high-quality, tailored research outcomes.",
      "stars": 29,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T22:15:51Z",
      "readme_content": "[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n# OpenRouter Agents MCP Server\n\n[![npm version](https://img.shields.io/npm/v/%40terminals-tech%2Fopenrouter-agents?color=2ea043)](https://www.npmjs.com/package/@terminals-tech/openrouter-agents) [![GitHub Packages](https://img.shields.io/badge/GitHub%20Packages-available-24292e?logo=github)](../../packages)\n\n[UPDATE ‚Äì 2025-08-26] Two modes (set MODE env):\n- AGENT: one simple tool (`agent`) that routes research / follow_up / retrieve / query\n- MANUAL: individual tools for each action\n- ALL (default): both AGENT and MANUAL, plus always-on ops tools\n\nDiagram (simple)\n```\n[Always-On Ops]  ping ‚Ä¢ get_server_status ‚Ä¢ job_status ‚Ä¢ cancel_job\n\nAGENT MODE\nclient ‚Üí agent ‚Üí (research | follow_up | retrieve | query)\n\nMANUAL MODE\nclient ‚Üí (submit_research | conduct_research | retrieve | query | research_follow_up | get_report_content | list_research_history)\n```\n\n- Killer features\n  - Plan ‚Üí parallelize ‚Üí synthesize workflow with bounded parallelism\n  - Dynamic model catalog; supports Anthropic Sonnet‚Äë4 and OpenAI GPT‚Äë5 family\n  - Built‚Äëin semantic KB (PGlite + pgvector) with backup, export/import, health, and reindex tools\n  - Lightweight web helpers: quick search and page fetch for context\n  - Robust streaming (SSE), per‚Äëconnection auth, clean logs\n\n## Install / Run\n- Install (project dependency)\n```bash\nnpm install @terminals-tech/openrouter-agents\n```\n\n- Global install (optional)\n```bash\nnpm install -g @terminals-tech/openrouter-agents\n```\n\n- Run with npx (no install)\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=devkey npx @terminals-tech/openrouter-agents\n```\n\n## What‚Äôs new (v1.5.0)\n- Version parity across npm, GitHub Releases, and GitHub Packages\n- Dual publish workflow enabled\n\n[Changelog ‚Üí](docs/CHANGELOG.md)\n\n## Quick start\n1) Prereqs\n- Node 18+ (20 LTS recommended), npm, Git, OpenRouter API key\n\n2) Install\n```bash\nnpm install\n```\n\n3) Configure (.env)\n```dotenv\nOPENROUTER_API_KEY=your_openrouter_key\nSERVER_API_KEY=your_http_transport_key\nSERVER_PORT=3002\n\n# Modes (pick one; default ALL)\n# AGENT  = agent-only + always-on ops (ping/status/jobs)\n# MANUAL = individual tools + always-on ops\n# ALL    = agent + individual tools + always-on ops\nMODE=ALL\n\n# Orchestration\nENSEMBLE_SIZE=2\nPARALLELISM=4\n\n# Models (override as needed) - Updated with state-of-the-art cost-effective models\nPLANNING_MODEL=openai/gpt-5-chat\nPLANNING_CANDIDATES=openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4\nHIGH_COST_MODELS=x-ai/grok-4,openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4,morph/morph-v3-large\nLOW_COST_MODELS=deepseek/deepseek-chat-v3.1,z-ai/glm-4.5v,qwen/qwen3-coder,openai/gpt-5-mini,google/gemini-2.5-flash\nVERY_LOW_COST_MODELS=openai/gpt-5-nano,deepseek/deepseek-chat-v3.1\n\n# Storage\nPGLITE_DATA_DIR=./researchAgentDB\nPGLITE_RELAXED_DURABILITY=true\nREPORT_OUTPUT_PATH=./research_outputs/\n\n# Indexer\nINDEXER_ENABLED=true\nINDEXER_AUTO_INDEX_REPORTS=true\nINDEXER_AUTO_INDEX_FETCHED=true\n\n# MCP features\nMCP_ENABLE_PROMPTS=true\nMCP_ENABLE_RESOURCES=true\n\n# Prompt strategy\nPROMPTS_COMPACT=true\nPROMPTS_REQUIRE_URLS=true\nPROMPTS_CONFIDENCE=true\n\n```\n\n4) Run\n- STDIO (for Cursor/VS Code MCP):\n```bash\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE (local daemon):\n```bash\nSERVER_API_KEY=$SERVER_API_KEY node src/server/mcpServer.js\n```\n\n### Windows PowerShell examples\n- STDIO\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:INDEXER_ENABLED='true'\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:SERVER_API_KEY='devkey'\n$env:SERVER_PORT='3002'\nnode src/server/mcpServer.js\n```\n\n### One-liner demo scripts\nDev (HTTP/SSE):\n```bash\nSERVER_API_KEY=devkey INDEXER_ENABLED=true node src/server/mcpServer.js\n```\n\nSTDIO (Cursor/VS Code):\n```bash\nOPENROUTER_API_KEY=your_key INDEXER_ENABLED=true node src/server/mcpServer.js --stdio\n```\n\n### MCP client JSON configuration (no manual start required)\nYou can register this server directly in MCP clients that support JSON server manifests.\n\nMinimal examples:\n\n1) STDIO transport (recommended for IDEs)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"command\": \"npx\",\n      \"args\": [\"@terminals-tech/openrouter-agents\", \"--stdio\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"${OPENROUTER_API_KEY}\",\n        \"SERVER_API_KEY\": \"${SERVER_API_KEY}\",\n        \"PGLITE_DATA_DIR\": \"./researchAgentDB\",\n        \"INDEXER_ENABLED\": \"true\"\n      }\n    }\n  }\n}\n```\n\n2) HTTP/SSE transport (daemon mode)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"url\": \"http://127.0.0.1:3002\",\n      \"sse\": \"/sse\",\n      \"messages\": \"/messages\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${SERVER_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\nWith the package installed globally (or via npx), MCP clients can spawn the server automatically. See your client‚Äôs docs for where to place this JSON (e.g., `~/.config/client/mcp.json`).\n\n## Tools (high‚Äëvalue)\n- Always‚Äëon (all modes): `ping`, `get_server_status`, `job_status`, `get_job_status`, `cancel_job`\n- AGENT: `agent` (single entrypoint for research / follow_up / retrieve / query)\n- MANUAL/ALL toolset: `submit_research` (async), `conduct_research` (sync/stream), `research_follow_up`, `search` (hybrid), `retrieve` (index/sql), `query` (SELECT), `get_report_content`, `list_research_history`\n- Jobs: `get_job_status`, `cancel_job`\n- Retrieval: `search` (hybrid BM25+vector with optional LLM rerank), `retrieve` (index/sql wrapper)\n- SQL: `query` (SELECT‚Äëonly, optional `explain`)\n- Knowledge base: `get_past_research`, `list_research_history`, `get_report_content`\n- DB ops: `backup_db` (tar.gz), `export_reports`, `import_reports`, `db_health`, `reindex_vectors`\n- Models: `list_models`\n- Web: `search_web`, `fetch_url`\n- Indexer: `index_texts`, `index_url`, `search_index`, `index_status`\n\n### Tool usage patterns (for LLMs)\nUse `tool_patterns` resource to view JSON recipes describing effective chaining, e.g.:\n- Search ‚Üí Fetch ‚Üí Research\n- Async research: submit, stream via SSE `/jobs/:id/events`, then get report content\n\nNotes\n- Data lives locally under `PGLITE_DATA_DIR` (default `./researchAgentDB`). Backups are tarballs in `./backups`.\n- Use `list_models` to discover current provider capabilities and ids.\n\n## Architecture at a glance\nSee `docs/diagram-architecture.mmd` (Mermaid). Render to SVG with Mermaid CLI if installed:\n```bash\nnpx @mermaid-js/mermaid-cli -i docs/diagram-architecture.mmd -o docs/diagram-architecture.svg\n```\nOr use the script:\n```bash\nnpm run gen:diagram\n```\n\n\n\nIf the image doesn‚Äôt render in your viewer, open `docs/diagram-architecture-branded.svg` directly.\n\n### Answer crystallization view\n\n\nHow it differs from typical ‚Äúagent chains‚Äù:\n- Not just hardcoded handoffs; the plan is computed, then parallel agents search, then a synthesis step reasons over consensus, contradictions, and gaps.\n- The system indexes what it reads during research, so subsequent queries get faster/smarter.\n- Guardrails shape attention: explicit URL citations, [Unverified] labelling, and confidence scoring.\n\n## Minimal‚Äëtoken prompt strategy\n- Compact mode strips preambles to essential constraints; everything else is inferred.\n- Enforced rules: explicit URL citations, no guessing IDs/URLs, confidence labels.\n- Short tool specs: use concise param names and rely on server defaults.\n\n## Common user journeys\n- ‚ÄúGive me an executive briefing on MCP status as of July 2025.‚Äù\n  - Server plans sub‚Äëqueries, fetches authoritative sources, synthesizes with citations.\n  - Indexed outputs make related follow‚Äëups faster.\n\n- ‚ÄúFind vision‚Äëcapable models and route images gracefully.‚Äù\n  - `/models` discovered and filtered, router template generated, fallback to text models.\n\n- ‚ÄúCompare orchestration patterns for bounded parallelism.‚Äù\n  - Pulls OTel/Airflow/Temporal docs, produces a MECE synthesis and code pointers.\n\n## Cursor IDE usage\n- Add this server in Cursor MCP settings pointing to `node src/server/mcpServer.js --stdio`.\n- Use the new prompts (`planning_prompt`, `synthesis_prompt`) directly in Cursor to scaffold tasks.\n\n## FAQ (quick glance)\n- How does it avoid hallucinations?\n  - Strict citation rules, [Unverified] labels, retrieval of past work, on‚Äëthe‚Äëfly indexing.\n- Can I disable features?\n  - Yes, via env flags listed above.\n- Does it support streaming?\n  - Yes, SSE for HTTP; stdio for MCP.\n\n## Command Map (quick reference)\n- Start (stdio): `npm run stdio`\n- Start (HTTP/SSE): `npm start`\n- Run via npx (scoped): `npx @terminals-tech/openrouter-agents --stdio`\n- Generate examples: `npm run gen:examples`\n- List models: MCP `list_models { refresh:false }`\n- Submit research (async): `submit_research { q:\"<query>\", cost:\"low\", aud:\"intermediate\", fmt:\"report\", src:true }`\n- Track job: `get_job_status { job_id:\"...\" }`, cancel: `cancel_job { job_id:\"...\" }`\n- Unified search: `search { q:\"<query>\", k:10, scope:\"both\" }`\n- SQL (read‚Äëonly): `query { sql:\"SELECT ... WHERE id = $1\", params:[1], explain:true }`\n- Get past research: `get_past_research { query:\"<query>\", limit:5 }`\n- Index URL (if enabled): `index_url { url:\"https://...\" }`\n- Micro UI (ghost): visit `http://localhost:3002/ui` to stream job events (SSE).\n\n## Package publishing\n- Name: `@terminals-tech/openrouter-agents`\n- Version: 1.3.2\n- Bin: `openrouter-agents`\n- Author: Tej Desai <admin@terminals.tech>\n- Homepage: https://terminals.tech\n\nInstall and run without cloning:\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=your_key npx @terminals-tech/openrouter-agents\n```\n\n### Publish (scoped)\n```bash\nnpm login\nnpm version 1.3.2 -m \"chore(release): %s\"\ngit push --follow-tags\nnpm publish --access public --provenance\n```\n\n## Validation ‚Äì MSeeP (Multi‚ÄëSource Evidence & Evaluation Protocol)\n- **Citations enforced**: explicit URLs, confidence tags; unknowns marked `[Unverified]`.\n- **Cross‚Äëmodel triangulation**: plan fans out to multiple models; synthesis scores consensus vs contradictions.\n- **KB grounding**: local hybrid index (BM25+vector) retrieves past work for cross‚Äëchecking.\n- **Human feedback**: `rate_research_report { rating, comment }` stored to DB; drives follow‚Äëups.\n- **Reproducibility**: `export_reports` + `backup_db` capture artifacts for audit.\n\n## Quality feedback loop\n- Run examples: `npm run gen:examples`\n- Review: `list_research_history`, `get_report_content {reportId}`\n- Rate: `rate_research_report { reportId, rating:1..5, comment }`\n- Improve retrieval: `reindex_vectors`, `index_status`, `search_index { query }`\n\n## Architecture diagram (branded)\n- See `docs/diagram-architecture-branded.svg` (logo links to `https://terminals.tech`).\n\n## Stargazers\n[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=wheattoast11/openrouter-deep-research&type=Date)](https://star-history.com/#wheattoast11/openrouter-deep-research)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "research",
        "planning",
        "deep research",
        "research tasks",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wiSCADA--gpu.cpp": {
      "owner": "wiSCADA",
      "name": "gpu.cpp",
      "url": "https://github.com/wiSCADA/gpu.cpp",
      "imageUrl": "/freedevtools/mcp/pfp/wiSCADA.webp",
      "description": "A lightweight C++ library for portable GPU computation that uses the WebGPU specification, enabling seamless integration of GPU capabilities across a variety of hardware platforms. It supports multiple GPU backends, including Vulkan, Metal, and DirectX, facilitating efficient development and fast iterations.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-07-30T09:11:07Z",
      "readme_content": "# gpu.cpp\n\ngpu.cpp is a lightweight library that makes portable GPU compute with C++ simple.\n\nIt focuses on general purpose native GPU computation, leveraging the WebGPU\nspecification as a portable low-level GPU interface. This means we can drop in\nGPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs.\nThe same C++ code can work on a wide variety of laptops, workstations, mobile\ndevices or virtually any hardware with Vulkan, Metal, or DirectX support.\n\n## Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate\n\nWith gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:\n\n- High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.\n- Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be <5 seconds on a modern laptop.\n- Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.\n\nThe implementation aims for a small API surface area with minimum boilerplate. There are a small number of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it's needed.\n\nIn this spirit of fast experimentation, we also want near-instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.\n\nThe core library implementation in the header-only `gpu.h` source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high.\nWe also pre-build Google's Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google's powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles.\n\nFor more advanced users and release deployments, we include `cmake` examples for building both Dawn with gpu.cpp end-to-end, but this is not required nor recommended for most users to get started.\n\n## Quick Start: Building and Running\n\nTo build a gpu.cpp project, you will need to have installed on your system:\n\n- `clang++` compiler installed with support for C++17.\n- `python3` and above, to run the script which downloads the Dawn shared library.\n  make to build the project.\n- `make` to build the project.\n- Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run `sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools` to install them.\n\nThe only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).\n\nOptionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.\n\nAfter cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:\n\n```\nmake\n```\n\nThe first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see `libdawn.dylib` on MacOS or `libdawn.so` on Linux. This download only occurs once.\n\nThe build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:\n\n```\nHello gpu.cpp!\n--------------\n\n  gelu(0.00) = 0.00\n  gelu(0.10) = 0.05\n  gelu(0.20) = 0.12\n  gelu(0.30) = 0.19\n  gelu(0.40) = 0.26\n  gelu(0.50) = 0.35\n  gelu(0.60) = 0.44\n  gelu(0.70) = 0.53\n  gelu(0.80) = 0.63\n  gelu(0.90) = 0.73\n  gelu(1.00) = 0.84\n  gelu(1.10) = 0.95\n  ...\n\nComputed 10000 values of GELU(x)\n```\n\nIf you need to clean up the build artifacts, you can run:\n\n```\nmake clean\n```\n\n## Hello World Tutorial: A GELU Kernel\n\nAs a real-world example for how to use gpu.cpp, let's start with a practical-but-simple example of a GPU kernel from neural networks.\n\nGELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.\n\nIt takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.\n\nThe GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:\n\n1. The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute operation.\n\n2. The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.\n\n3. The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.\n\nHere's a GELU kernel implemented (based on the CUDA implementation in [llm.c](https://github.com/karpathy/llm.c)) as on-device WebGPU WGSL code and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):\n\n```\n#include <array>\n#include <cstdio>\n#include <future>\n\n#include \"gpu.h\"\n\nusing namespace gpu; // createContext, createTensor, createKernel,\n                     // dispatchKernel, wait, toCPU Bindings,\n                     // Tensor, Kernel, Context, Shape, kf32\n\nstatic const char *kGelu = R\"(\nconst GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)\n@group(0) @binding(0) var<storage, read_write> inp: array<{{precision}}>;\n@group(0) @binding(1) var<storage, read_write> out: array<{{precision}}>;\n@compute @workgroup_size({{workgroupSize}})\nfn main(\n    @builtin(global_invocation_id) GlobalInvocationID: vec3<u32>) {\n    let i: u32 = GlobalInvocationID.x;\n    if (i < arrayLength(&inp)) {\n        let x: f32 = inp[i];\n        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR\n                 * (x + .044715 * x * x * x))), x, x > 10.0);\n    }\n}\n)\";\n\nint main(int argc, char **argv) {\n  Context ctx = createContext();\n  static constexpr size_t N = 10000;\n  std::array<float, N> inputArr, outputArr;\n  for (int i = 0; i < N; ++i) {\n    inputArr[i] = static_cast<float>(i) / 10.0; // dummy input data\n  }\n  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());\n  Tensor output = createTensor(ctx, Shape{N}, kf32);\n  std::promise<void> promise;\n  std::future<void> future = promise.get_future();\n  Kernel op = createKernel(ctx, {kGelu, /* 1-D workgroup size */ 256, kf32},\n                           Bindings{input, output},\n                           /* number of workgroups */ {cdiv(N, 256), 1, 1});\n  dispatchKernel(ctx, op, promise);\n  wait(ctx, future);\n  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));\n  for (int i = 0; i < 16; ++i) {\n    printf(\"  gelu(%.2f) = %.2f\\n\", inputArr[i], outputArr[i]);\n  }\n  return 0;\n}\n```\n\nHere we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui) for a demonstration of live WGSL code re-loading).\n\nThe CPU code in main() sets up the host coordination for the GPU computation.\nWe can think of the use of gpu.cpp library as a collection of GPU nouns and\nverbs.\n\nThe \"nouns\" are GPU resources modeled by the type definitions of the library\nand the \"verbs\" actions on GPU resources, modeled by the functions of the\nlibrary. The ahead-of-time resource acquisition functions are prefaced with\n`create*`, such as:\n\n- `createContext()` - constructs a reference to the GPU device context (`Context`).\n- `createTensor()` - acquires a contiguous buffer on the GPU (`Tensor`).\n- `createKernel()` - constructs a handle to resources for the GPU computation (`Kernel`), taking the shader code as input and the tensor resources to bind.\n\nThese resource acquisition functions are tied to resource types for interacting with the GPU:\n\n- `Context` - a handle to the state of resources for interacting with the GPU device.\n- `Tensor` - a buffer of data on the GPU.\n- `KernelCode` - the code for a WGSL program that can be dispatched to the\n  GPU. This is a thin wrapper around a WGSL string and also includes the\n  workgroup size the code is designed to run with.\n- `Kernel` - a GPU program that can be dispatched to the GPU. This accepts a\n  `KernelCode` and a list of `Tensor` resources to bind for the dispatch\n  computation. This takes an argument `Bindings` that is a list of `Tensor` instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the `input` buffer on the GPU and the `ouptut` buffer on the GPU.\n\nIn this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the [examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.\n\nBesides the `create*` resource acquisition functions, there are a few more \"verbs\" in the gpu.cpp library for handling dispatching execution to the GPU and data movement:\n\n- `dispatchKernel()` - dispatches a `Kernel` to the GPU for computation. This is an asynchronous operation that returns immediately.\n- `wait()` - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.\n- `toCPU()` - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.\n- `toGPU()` - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, `toGPU()` is not used because there's only one data movement from CPU to GPU in the program and that happens when the `createTensor()` function is called.\n\nThis example is available in [examples/hello_world/run.cpp](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp).\n\n## Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering\n\nYou can explore the example projects in\n[examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) which\nillustrate how to use gpu.cpp as a library.\n\nAfter you have run `make` in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running `make` from the example's directory.\n\nAn example of tiled matrix multiplication is in [examples/matmul](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/). This implements a WebGPU version of the first few kernels of Simon Boehm's [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM) post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point.\n\nA parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in [examples/physics](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics).\n\n<div align=\"center\">\n\n\n</div>\n\nWe also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in [examples/render](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render]) and a shadertoy-like live-reloading example is in [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui).\n\nInterestingly, given a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.\n\n<div align=\"center\">\n  \n</div>\n\n## Who is gpu.cpp for?\n\ngpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:\n\n- Development of GPU algorithms to be run on personal computing devices\n- Direct standalone implementations of neural network models\n- Physics simulations and simulation environments\n- Multimodal applications - audio and video processing\n- Offline graphics rendering\n- ML inference engines and runtimes\n- Parallel compute intensive data processing applications\n\nAlthough gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.\n\nTo date, AI research has primarily been built with CUDA as the privileged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.\n\nGPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastructure scale efforts - game engines, production ML inference engines, large software packages.\n\nWe want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.\n\ngpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.\n\n## What gpu.cpp is not\n\ngpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.\n\nSecond, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable _native_ GPU API first and foremost, with the possibility of running in the browser being a convenient additional benefit in the future.\n\nIf you find it counterintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk [\"WebGPU is Not Just About the Web\"](https://www.youtube.com/watch?v=qHrx41aOTUQ).\n\nFinally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.\n\n## Limitations and Upcoming Features\n\n_API Improvements_ - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.\n\n_Browser Targets_ - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority.\n\n_Reusable Kernel Library_ - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in `examples/`. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.\n\n_More Use Case Examples and Tests_ - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from [llm.c](https://github.com/karpathy/llm.c) in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.\n\n## Troubleshooting\n\nIf you run into issues building the project, please open an issue.\n\n## Acknowledgements\n\ngpu.cpp makes use of:\n\n- [Dawn](https://dawn.googlesource.com/dawn) as the WebGPU implementation\n- [webgpu-dawn-binaries](https://github.com/jspanchu/webgpu-dawn-binaries) by\n  @jspanchu to build a binary artifact of Dawn.\n- [webgpu-distribution](https://github.com/eliemichel/WebGPU-distribution) by\n  @eliemichel for cmake builds.\n\nThanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback.\n\n## Discord Community and Contributing\n\nJoin our community in the `#gpu-cpp` channel on the [AnswerDotAI Discord with this invite link](https://discord.gg/zmJVhXsC7f). Feel free to get in touch via X [@austinvhuang](https://twitter.com/austinvhuang) as well.\n\nFeedback, issues and pull requests are welcome.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpu",
        "webgpu",
        "vulkan",
        "wiscada gpu",
        "gpu cpp",
        "gpu computation"
      ],
      "category": "scientific-research-tools"
    },
    "wj2333wj--arxiv-mcp-server": {
      "owner": "wj2333wj",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/wj2333wj/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/wj2333wj.webp",
      "description": "Enables searching, downloading, and analyzing arXiv research papers through an MCP interface, managing local storage for efficient retrieval and utilizing specialized prompts for comprehensive analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-27T06:38:09Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## üìù Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "arxiv",
        "retrieval",
        "mcp",
        "arxiv mcp",
        "arxiv research",
        "analyzing arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "wjlim--nf-core_mcp": {
      "owner": "wjlim",
      "name": "nf-core_mcp",
      "url": "https://github.com/wjlim/nf-core_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wjlim.webp",
      "description": "Manage and navigate nf-core pipeline repositories, providing access to configurations, workflows, and modules while enabling the search through pipeline files to enhance bioinformatics project efficiency.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-08T14:41:02Z",
      "readme_content": "# nf-core MCP Server\n\nAn MCP server for managing and navigating nf-core pipeline repositories.\n\n## Features\n\n- List local nf-core repositories (rnaseq, sarek, modules, tools)\n- Access pipeline configurations and workflows\n- Search through pipeline files\n- Explore pipeline modules\n\n## Installation\n\n### NPM Version\n\n```bash\n\ncd nf-core_mcp\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Start the server\nnpm start\n```\n\n### Docker Version\n\n```bash\n# Build the Docker image\ncd nf-core_mcp\ndocker build -t nf-core-mcp .\n# Run the container\ndocker run -i --rm \\\n  -v \"/path/to/your/workspace:/app/workspace\" \\\n  nf-core-mcp\n```\n\n## Adding nf-core Repositories\n\nTo add new nf-core pipeline repositories to the workspace:\n\n1. **Clone the repositories**:\n   ```bash\n   # Navigate to your workspace directory (example for Windows)\n   cd /path/to/your/workspace\n\n   # Clone desired nf-core repositories\n   git clone https://github.com/nf-core/rnaseq.git\n   git clone https://github.com/nf-core/sarek.git\n   git clone https://github.com/nf-core/modules.git\n   # Add any other nf-core pipeline you want to manage\n   ```\n\n2. **Directory Structure**:\n   Your workspace should look like this:\n   ```\n   workspace/\n   ‚îú‚îÄ‚îÄ rnaseq/\n   ‚îú‚îÄ‚îÄ sarek/\n   ‚îú‚îÄ‚îÄ modules/\n   ‚îî‚îÄ‚îÄ your-new-pipeline/\n   ```\n\n3. **Verify Installation**:\n   After starting the MCP server, use the `list-pipelines` command to verify that your new pipelines are detected:\n   ```\n   list-pipelines\n   ```\n\nNote: The MCP server will automatically detect and manage any nf-core pipeline repositories in your workspace directory.\n\n## Available Tools\n\n1. `list-pipelines`\n   - Lists all nf-core pipelines in the workspace\n   - Shows configuration file status\n   - No parameters required\n\n2. `get-pipeline-modules`\n   - Gets module information from a pipeline\n   - Parameters:\n     - `pipeline`: Pipeline name (rnaseq, sarek, or modules)\n\n3. `search-pipelines`\n   - Searches through pipeline files\n   - Parameters:\n     - `query`: Search query\n     - `pipeline` (optional): Specific pipeline to search\n\n## Available Resources\n\n1. `pipeline-config`\n   - Gets pipeline configuration\n   - URI format: `pipeline://{name}/config`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n2. `pipeline-workflow`\n   - Gets pipeline workflow\n   - URI format: `pipeline://{name}/workflow`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n## Usage with Cursor IDE\n\n### Using NPX (Recommended)\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"nf-core-mcp\"]\n    }\n  }\n}\n```\n\n### Using Docker\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/your/workspace:/app/workspace\",\n        \"nf-core-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Example Usage\n\nUsing the MCP server in Cursor:\n\n```\n# List available pipelines\nlist-pipelines\n\n# Get modules from rnaseq pipeline\nget-pipeline-modules pipeline=rnaseq\n\n# Search in all pipelines\nsearch-pipelines query=\"fastqc\"\n\n# Search in specific pipeline\nsearch-pipelines query=\"fastqc\" pipeline=rnaseq\n\n# Access pipeline configuration\npipeline://rnaseq/config\n\n# Access workflow\npipeline://rnaseq/workflow\n```\n\n## Running the Server\n\n### Using NPM\n\n```bash\n# If installed globally\nnf-core-mcp\n\n# If installed locally\nnpx nf-core-mcp\n\n# Using npx without installation\nnpx -y nf-core-mcp\n```\n\n### Using Docker\n\n```bash\ndocker run -it --rm \\\n  -v /path/to/your/workspace:/app/workspace \\\n  nf-core-mcp\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Run tests\nnpm test\n\n# Run linter\nnpm run lint\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "core_mcp",
        "pipeline",
        "nf core_mcp",
        "nf core",
        "core pipeline"
      ],
      "category": "scientific-research-tools"
    },
    "ygboy1986--AIClaude": {
      "owner": "ygboy1986",
      "name": "AIClaude",
      "url": "https://github.com/ygboy1986/AIClaude",
      "imageUrl": "/freedevtools/mcp/pfp/ygboy1986.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude for complex multi-step reasoning tasks. Enhances language processing capabilities to deliver precise and efficient responses.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-20T03:09:57Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## üöÄ Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   \n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   \n\n\n   - If the server isn‚Äôt visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## üõ† Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## üìÑ License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "aiclaude",
        "reasoning engine",
        "reasoning tasks",
        "advanced reasoning"
      ],
      "category": "scientific-research-tools"
    },
    "yonaka15--mcp-pyodide": {
      "owner": "yonaka15",
      "name": "mcp-pyodide",
      "url": "https://github.com/yonaka15/mcp-pyodide",
      "imageUrl": "/freedevtools/mcp/pfp/yonaka15.webp",
      "description": "Execute Python code through the Model Context Protocol interface, enabling Large Language Models to leverage Python capabilities within their responses. The server supports both stdio and SSE transport modes and is implemented in TypeScript.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-08T05:40:45Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yonaka15-mcp-pyodide-badge.png)](https://mseep.ai/app/yonaka15-mcp-pyodide)\n\n# mcp-pyodide\n\nA Pyodide server implementation for the Model Context Protocol (MCP). This server enables Large Language Models (LLMs) to execute Python code through the MCP interface.\n\n<a href=\"https://glama.ai/mcp/servers/pxls43joly\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pxls43joly/badge\" alt=\"mcp-pyodide MCP server\" />\n</a>\n\n## Features\n\n- Python code execution capability for LLMs using Pyodide\n- MCP compliant server implementation\n- Support for both stdio and SSE transport modes\n- Robust implementation written in TypeScript\n- Available as a command-line tool\n\n## Installation\n\n```bash\nnpm install mcp-pyodide\n```\n\n## Usage\n\n### As a Server\n\n```typescript\nimport { runServer } from \"mcp-pyodide\";\n\n// Start the server\nrunServer().catch((error: unknown) => {\n  console.error(\"Error starting server:\", error);\n  process.exit(1);\n});\n```\n\n### As a Command-line Tool\n\nStart in stdio mode (default):\n\n```bash\nmcp-pyodide\n```\n\nStart in SSE mode:\n\n```bash\nmcp-pyodide --sse\n```\n\n### SSE Mode\n\nWhen running in SSE mode, the server provides the following endpoints:\n\n- SSE Connection: `http://localhost:3020/sse`\n- Message Handler: `http://localhost:3020/messages`\n\nExample client connection:\n\n```typescript\nconst eventSource = new EventSource(\"http://localhost:3020/sse\");\neventSource.onmessage = (event) => {\n  console.log(\"Received:\", JSON.parse(event.data));\n};\n```\n\n## Project Structure\n\n```\nmcp-pyodide/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ formatters/    # Data formatting handlers\n‚îÇ   ‚îú‚îÄ‚îÄ handlers/      # Request handlers\n‚îÇ   ‚îú‚îÄ‚îÄ lib/          # Library code\n‚îÇ   ‚îú‚îÄ‚îÄ tools/        # Utility tools\n‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Utility functions\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts      # Main entry point\n‚îú‚îÄ‚îÄ build/            # Build artifacts\n‚îú‚îÄ‚îÄ pyodide-packages/ # Pyodide-related packages\n‚îî‚îÄ‚îÄ package.json\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: MCP SDK (^1.4.0)\n- `pyodide`: Python runtime environment (^0.27.1)\n- `arktype`: Type validation library (^2.0.1)\n- `express`: Web framework for SSE support\n- `cors`: CORS middleware for SSE support\n\n## Development\n\n### Requirements\n\n- Node.js 18 or higher\n- npm 9 or higher\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone <repository-url>\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n```\n\n### Scripts\n\n- `npm run build`: Compile TypeScript and set execution permissions\n- `npm start`: Run server in stdio mode\n- `npm run start:sse`: Run server in SSE mode\n\n## Environment Variables\n\n- `PYODIDE_CACHE_DIR`: Directory for Pyodide cache (default: \"./cache\")\n- `PYODIDE_DATA_DIR`: Directory for mounted data (default: \"./data\")\n- `PORT`: Port for SSE server (default: 3020)\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -am 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Create a Pull Request\n\n## Important Notes\n\n- This project is under development, and the API may change\n- Thoroughly test before using in production\n- Exercise caution when executing untrusted code for security reasons\n- When using SSE mode, ensure proper CORS configuration if needed\n\n## Support\n\nPlease use the Issue tracker for problems and questions.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "pyodide",
        "mcp",
        "python capabilities",
        "mcp pyodide",
        "pyodide execute"
      ],
      "category": "scientific-research-tools"
    },
    "ywwAHU--mcp-scholarly": {
      "owner": "ywwAHU",
      "name": "mcp-scholarly",
      "url": "https://github.com/ywwAHU/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/ywwAHU.webp",
      "description": "Search for academic articles using the MCP server by querying a dedicated tool for relevant papers based on keywords. Access trusted academic sources for streamlined research.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-15T07:27:21Z",
      "readme_content": "# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-2025323](https://github.com/user-attachments/assets/b73c916d-194c-429e-a7f9-0ff7d0db0b02)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "mcp",
        "mcp scholarly",
        "scholarly search",
        "search academic"
      ],
      "category": "scientific-research-tools"
    },
    "zchaffee1--mcp-server": {
      "owner": "zchaffee1",
      "name": "mcp-server",
      "url": "https://github.com/zchaffee1/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zchaffee1.webp",
      "description": "Provides access to scientific computing resources, enabling HDF5 file operations, Slurm job management, and hardware information retrieval. Facilitates data compression and decompression using gzip or zlib.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-21T04:43:22Z",
      "readme_content": "# mcp-server\n\nBy: Zack Chaffee A20478873\n\nA server implementing Model Coupling Protocol (MCP) capabilities for HDF5 file operations and Slurm job management.\n\n## Features\n\n### HDF5 file operations:\n  - Read datasets\n  - List file contents\n\n### Slurm job management:\n  - Submit jobs\n  - Check job status\n\n### Node Hardware Operations\n- Get CPU information\n- Get memory information\n- Get disk information\n- Get comprehensive system information\n\n### Compression Operations\n- Compress string data with gzip or zlib\n- Compress files with gzip or zlib\n- Decompress data\n\n# Initialization\n\nOnce you clone this reponsitory cd into it \n\nAfter this hwe will create a virtual enviornment and install all dependincies:\n```\nuv venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\nuv pip install -e \".[test]\"\n```\n\n# Running\n\nTo startup the server run:\n```\npython -m src.server\n```\n\nThis will autoclocate the server at http://localhost:8000.\n\n## Endpoints\n- `POST /mcp`: Main endpoint for MCP requests\n- `GET /health`: Health check endpoint\n\n## Examples:\n```python\nimport httpx\n\nasync with httpx.AsyncClient() as client:\n    # Read a dataset\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"read_dataset\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"dataset_path\": \"/path/to/dataset\"\n        }\n    })\n    \n    # List contents\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"list_contents\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"group_path\": \"/\"\n        }\n    })\n```\n\n```bash\ncurl -X POST http://localhost:8000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"mcp/listTools\", \n    \"params\": {}, \n    \"id\": \"1\"\n  }'\n```\n\n# Testing\nFor testing rung: \n```\npytest\n```\n\nFor tests with coverage:\n```\npytest --cov=src\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gzip",
        "zlib",
        "hdf5",
        "gzip zlib",
        "zchaffee1 mcp",
        "tools zchaffee1"
      ],
      "category": "scientific-research-tools"
    },
    "zongmin-yu--semantic-scholar-fastmcp-mcp-server": {
      "owner": "zongmin-yu",
      "name": "semantic-scholar-fastmcp-mcp-server",
      "url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zongmin-yu.webp",
      "description": "Provides access to academic paper data, author information, and citation networks through the Semantic Scholar API. Facilitates interaction with various academic resources for research purposes.",
      "stars": 63,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:36:41Z",
      "readme_content": "# Semantic Scholar MCP Server\n\n[![smithery badge](https://smithery.ai/badge/semantic-scholar-fastmcp-mcp-server)](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server)\n\nA FastMCP server implementation for the Semantic Scholar API, providing comprehensive access to academic paper data, author information, and citation networks.\n\n## Project Structure\n\nThe project has been refactored into a modular structure for better maintainability:\n\n```\nsemantic-scholar-server/\n‚îú‚îÄ‚îÄ semantic_scholar/            # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Package initialization\n‚îÇ   ‚îú‚îÄ‚îÄ server.py               # Server setup and main functionality\n‚îÇ   ‚îú‚îÄ‚îÄ mcp.py                  # Centralized FastMCP instance definition\n‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration classes\n‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Utility modules\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.py           # Error handling\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ http.py             # HTTP client and rate limiting\n‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API endpoints\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ papers.py           # Paper-related endpoints\n‚îÇ       ‚îú‚îÄ‚îÄ authors.py          # Author-related endpoints\n‚îÇ       ‚îî‚îÄ‚îÄ recommendations.py  # Recommendation endpoints\n‚îú‚îÄ‚îÄ run.py                      # Entry point script\n```\n\nThis structure:\n\n- Separates concerns into logical modules\n- Makes the codebase easier to understand and maintain\n- Allows for better testing and future extensions\n- Keeps related functionality grouped together\n- Centralizes the FastMCP instance to avoid circular imports\n\n## Features\n\n- **Paper Search & Discovery**\n\n  - Full-text search with advanced filtering\n  - Title-based paper matching\n  - Paper recommendations (single and multi-paper)\n  - Batch paper details retrieval\n  - Advanced search with ranking strategies\n\n- **Citation Analysis**\n\n  - Citation network exploration\n  - Reference tracking\n  - Citation context and influence analysis\n\n- **Author Information**\n\n  - Author search and profile details\n  - Publication history\n  - Batch author details retrieval\n\n- **Advanced Features**\n  - Complex search with multiple ranking strategies\n  - Customizable field selection\n  - Efficient batch operations\n  - Rate limiting compliance\n  - Support for both authenticated and unauthenticated access\n  - Graceful shutdown and error handling\n  - Connection pooling and resource management\n\n## System Requirements\n\n- Python 3.8+\n- FastMCP framework\n- Environment variable for API key (optional)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Semantic Scholar MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server):\n\n```bash\nnpx -y @smithery/cli install semantic-scholar-fastmcp-mcp-server --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/YUZongmin/semantic-scholar-fastmcp-mcp-server.git\ncd semantic-scholar-server\n```\n\n2. Install FastMCP and other dependencies following: https://github.com/jlowin/fastmcp\n\n3. Configure FastMCP:\n\nFor Claude Desktop users, you'll need to configure the server in your FastMCP configuration file. Add the following to your configuration (typically in `~/.config/claude-desktop/config.json`):\n\n```json\n{\n  \"mcps\": {\n    \"Semantic Scholar Server\": {\n      \"command\": \"/path/to/your/venv/bin/fastmcp\",\n      \"args\": [\n        \"run\",\n        \"/path/to/your/semantic-scholar-server/run.py\"\n      ],\n      \"env\": {\n        \"SEMANTIC_SCHOLAR_API_KEY\": \"your-api-key-here\"  # Optional\n      }\n    }\n  }\n}\n```\n\nMake sure to:\n\n- Replace `/path/to/your/venv/bin/fastmcp` with the actual path to your FastMCP installation\n- Replace `/path/to/your/semantic-scholar-server/run.py` with the actual path to run.py on your machine\n- If you have a Semantic Scholar API key, add it to the `env` section. If not, you can remove the `env` section entirely\n\n4. Start using the server:\n\nThe server will now be available to your Claude Desktop instance. No need to manually run any commands - Claude will automatically start and manage the server process when needed.\n\n### API Key (Optional)\n\nTo get higher rate limits and better performance:\n\n1. Get an API key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n2. Add it to your FastMCP configuration as shown above in the `env` section\n\nIf no API key is provided, the server will use unauthenticated access with lower rate limits.\n\n## Configuration\n\n### Environment Variables\n\n- `SEMANTIC_SCHOLAR_API_KEY`: Your Semantic Scholar API key (optional)\n  - Get your key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n  - If not provided, the server will use unauthenticated access\n\n### Rate Limits\n\nThe server automatically adjusts to the appropriate rate limits:\n\n**With API Key**:\n\n- Search, batch and recommendation endpoints: 1 request per second\n- Other endpoints: 10 requests per second\n\n**Without API Key**:\n\n- All endpoints: 100 requests per 5 minutes\n- Longer timeouts for requests\n\n## Available MCP Tools\n\n> Note: All tools are aligned with the official [Semantic Scholar API documentation](https://api.semanticscholar.org/api-docs/). Please refer to the official documentation for detailed field specifications and the latest updates.\n\n### Paper Search Tools\n\n- `paper_relevance_search`: Search for papers using relevance ranking\n\n  - Supports comprehensive query parameters including year range and citation count filters\n  - Returns paginated results with customizable fields\n\n- `paper_bulk_search`: Bulk paper search with sorting options\n\n  - Similar to relevance search but optimized for larger result sets\n  - Supports sorting by citation count, publication date, etc.\n\n- `paper_title_search`: Find papers by exact title match\n\n  - Useful for finding specific papers when you know the title\n  - Returns detailed paper information with customizable fields\n\n- `paper_details`: Get comprehensive details about a specific paper\n\n  - Accepts various paper ID formats (S2 ID, DOI, ArXiv, etc.)\n  - Returns detailed paper metadata with nested field support\n\n- `paper_batch_details`: Efficiently retrieve details for multiple papers\n  - Accepts up to 1000 paper IDs per request\n  - Supports the same ID formats and fields as single paper details\n\n### Citation Tools\n\n- `paper_citations`: Get papers that cite a specific paper\n\n  - Returns paginated list of citing papers\n  - Includes citation context when available\n  - Supports field customization and sorting\n\n- `paper_references`: Get papers referenced by a specific paper\n  - Returns paginated list of referenced papers\n  - Includes reference context when available\n  - Supports field customization and sorting\n\n### Author Tools\n\n- `author_search`: Search for authors by name\n\n  - Returns paginated results with customizable fields\n  - Includes affiliations and publication counts\n\n- `author_details`: Get detailed information about an author\n\n  - Returns comprehensive author metadata\n  - Includes metrics like h-index and citation counts\n\n- `author_papers`: Get papers written by an author\n\n  - Returns paginated list of author's publications\n  - Supports field customization and sorting\n\n- `author_batch_details`: Get details for multiple authors\n  - Efficiently retrieve information for up to 1000 authors\n  - Returns the same fields as single author details\n\n### Recommendation Tools\n\n- `paper_recommendations_single`: Get recommendations based on a single paper\n\n  - Returns similar papers based on content and citation patterns\n  - Supports field customization for recommended papers\n\n- `paper_recommendations_multi`: Get recommendations based on multiple papers\n  - Accepts positive and negative example papers\n  - Returns papers similar to positive examples and dissimilar to negative ones\n\n## Usage Examples\n\n### Basic Paper Search\n\n```python\nresults = await paper_relevance_search(\n    context,\n    query=\"machine learning\",\n    year=\"2020-2024\",\n    min_citation_count=50,\n    fields=[\"title\", \"abstract\", \"authors\"]\n)\n```\n\n### Paper Recommendations\n\n```python\n# Single paper recommendation\nrecommendations = await paper_recommendations_single(\n    context,\n    paper_id=\"649def34f8be52c8b66281af98ae884c09aef38b\",\n    fields=\"title,authors,year\"\n)\n\n# Multi-paper recommendation\nrecommendations = await paper_recommendations_multi(\n    context,\n    positive_paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    negative_paper_ids=[\"ArXiv:1805.02262\"],\n    fields=\"title,abstract,authors\"\n)\n```\n\n### Batch Operations\n\n```python\n# Get details for multiple papers\npapers = await paper_batch_details(\n    context,\n    paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    fields=\"title,authors,year,citations\"\n)\n\n# Get details for multiple authors\nauthors = await author_batch_details(\n    context,\n    author_ids=[\"1741101\", \"1780531\"],\n    fields=\"name,hIndex,citationCount,paperCount\"\n)\n```\n\n## Error Handling\n\nThe server provides standardized error responses:\n\n```python\n{\n    \"error\": {\n        \"type\": \"error_type\",  # rate_limit, api_error, validation, timeout\n        \"message\": \"Error description\",\n        \"details\": {\n            # Additional context\n            \"authenticated\": true/false  # Indicates if request was authenticated\n        }\n    }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "academic",
        "research",
        "scholar api",
        "semantic scholar",
        "scholar fastmcp"
      ],
      "category": "scientific-research-tools"
    },
    "zygi--r-playground-mcp": {
      "owner": "zygi",
      "name": "r-playground-mcp",
      "url": "https://github.com/zygi/r-playground-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zygi.webp",
      "description": "Executes R code, visualizes plots, and interacts with scientific data within stateful sessions. Supports multimodal outputs to enhance conversations around scientific topics.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-06T20:19:18Z",
      "readme_content": "# MCP R Playground\n[![PyPI version](https://img.shields.io/pypi/v/rplayground-mcp.svg)](https://pypi.org/project/rplayground-mcp/)\n[![smithery badge](https://smithery.ai/badge/@zygi/r-playground-mcp)](https://smithery.ai/server/@zygi/r-playground-mcp)\n\nAn MCP server that allows AI models to execute R code, see its results, and draw and observe plots. \nIt can be used for sophisticated agentic deployments, but also as a way to augment AI clients like Claude Desktop when talking to them about scientific papers. \n\n## Features:\n- Stateful sessions: each conversation thread gets a new session, but the session can persist across calls and user/assistant interactions. \n- Graphics output: multimodal models can draw plots using standard R libraries like ggplot, see those plots, and react to them.\n- Works in all common operating systems/architectures - Windows x64 / arm64, MacOS, Linux.\n- When running locally, ‚ö†Ô∏è__NO HOST ISOLATION__‚ö†Ô∏è: while each session runs as a separate R environment, they have access to global dependencies and all files on the computer. While unlikely, a rogue model could write R code that deletes your important files.\n    - If you need host isolation, you should run this MCP in Docker. Instructions for that are provided below.\n\n## Configuration\nCurrently there's just one configuration parameter that can be set as an environment variable:\n- `RPLAYGROUND_MCP_SUPPORT_IMAGE_OUTPUT`, default True. If set to False, image output will be disabled, and tool descriptions will be made to reflect that.\n\n## Usage\nBy default, the AIs will have access to all globally installed R packages, and can install whatever package they want. These installations will persist. You can pre-install important packages to make them available in advance.\n\n\n## Installation\nBasic instructions for technical users:\n1) Have R installed, and the R_HOME environment variable set\n2) Have a recent version of the `uv` installed\n3) run `uvx --python=3.13 rplayground-mcp`, and it should just work.\n\n## Detailed Installation\nThis section is for less technical users who want to set up this MCP to use with Claude Desktop or similar AI user interfaces that support MCP extensions.\n\n### Windows\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n\n### MacOS\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n## Installation (Docker)\nWe also provide Dockerfiles to run this MCP in an isolated context. This only supports the platform linux/amd64. \nThe image uses the [`r2u`](https://github.com/eddelbuettel/r2u) project to make precompiled CRAN packages available. The Dockerfile comes with two versions:\n- Slim: `docker build --platform=linux/amd64 .`. This sets up the `r2u` repositories but installing a new package from the R session will still take ~10s.\n- Fat: `docker build --build-arg PREINSTALL_PACKAGES=true --platform=linux/amd64 .`. This preinstalls a big selection of packages to the Docker image, and loading them in an MCP session becomes instant.\n\nYou are also welcome to edit the Dockerfile and preinstall just the packages relevant to your workflow.\n\n## Issues and Contributions\nFeel free to create an Issue if you have questions or requests. Small PRs are welcome anytime, larger PRs should be discussed by creating an Issue before a PR is started. \n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "multimodal",
        "zygi",
        "tools",
        "tools zygi",
        "research tools",
        "interacts scientific"
      ],
      "category": "scientific-research-tools"
    },
    "zzaebok--mcp-wikidata": {
      "owner": "zzaebok",
      "name": "mcp-wikidata",
      "url": "https://github.com/zzaebok/mcp-wikidata",
      "imageUrl": "/freedevtools/mcp/pfp/zzaebok.webp",
      "description": "Interact with Wikidata to search for entities, extract metadata, and execute SPARQL queries for enhanced data insights.",
      "stars": 29,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T01:29:15Z",
      "readme_content": "## Wikidata MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@zzaebok/mcp-wikidata)](https://smithery.ai/server/@zzaebok/mcp-wikidata)\n\nA server implementation for Wikidata API using the Model Context Protocol (MCP).\nThis project provides tools to interact with Wikidata, such as **searching identifiers** (entity and property), **extracting metadata** (label and description) and **executing sparql query**.\n\n---\n\n### Installation\n\n#### Installing via Smithery\n\nTo install Wikidata MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zzaebok/mcp-wikidata):\n\n```bash\nnpx -y @smithery/cli install @zzaebok/mcp-wikidata --client claude\n```\n\n#### Installing Manually\nInstall `uv` if it is not installed yet.\n\n```bash\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nThen, install dependencies.\n\n```bash\n$ git clone https://github.com/zzaebok/mcp-wikidata.git\n$ cd mcp-wikidata\n$ uv sync\n# if you want to run client example together\n$ uv sync --extra example\n```\n\n---\n\n### Run\n\nRun the server with:\n\n```bash\n$ uv run src/server.py\n```\n\nIf you want to test it with a simple client code (with `langchain-mcp-adapters`), run the client with:\n\n```bash\n# in another shell\n$ uv run src/client.py\n```\n\nThe LLM extracts valid entity and property identifiers, executes a sparql query, and finally recommend a movie directed by Bong Joon-ho.\n\n<details>\n  <summary><i>See the execution output</i></summary>\n\n```python\n{\n  \"messages\": [\n      HumanMessage(\n          content=\"Can you recommend me a movie directed by Bong Joonho?\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_entity\",\n                  \"args\": {\"query\": \"Bong Joon-ho\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"Q495980\",\n          name=\"search_entity\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"get_properties\",\n                  \"args\": {\"entity_id\": \"Q495980\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[\"P345\", \"P244\", \"P214\", \"P227\", ...]',\n          name=\"get_properties\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_property\",\n                  \"args\": {\"query\": \"director\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"P57\",\n          name=\"search_property\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"execute_sparql\",\n                  \"args\": {\n                      \"sparql_query\": 'SELECT ?film ?filmLabel WHERE {\\n  ?film wdt:P57 wd:Q495980.\\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\\n} LIMIT 1'\n                  },\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[{\"film\": {\"type\": \"uri\", \"value\": \"http://www.wikidata.org/entity/Q483761\"}, \"filmLabel\": {\"xml:lang\": \"en\", \"type\": \"literal\", \"value\": \"Mother\"}}]',\n          name=\"execute_sparql\",\n      ),\n      AIMessage(\n          content='I recommend the movie \"Mother,\" which was directed by Bong Joon-ho.',\n      ),\n  ]\n}\n```\n\n</details>\n\n---\n\n### Wikidata MCP Tools\n\nThe following tools are implemented in the server:\n\n| Tool                                                 | Description                                                                |\n| ---------------------------------------------------- | -------------------------------------------------------------------------- |\n| `search_entity(query: str)`                          | Search for a Wikidata entity ID by its query.                              |\n| `search_property(query: str)`                        | Search for a Wikidata property ID by its query.                            |\n| `get_properties(entity_id: str)`                     | Get the properties associated with a given Wikidata entity ID.             |\n| `execute_sparql(sparql_query: str)`                  | Execute a SPARQL query on Wikidata.                                        |\n| `get_metadata(entity_id: str, language: str = \"en\")` | Retrieve the English label and description for a given Wikidata entity ID. |\n\n---\n\n#### License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wikidata",
        "metadata",
        "sparql",
        "wikidata search",
        "wikidata interact",
        "mcp wikidata"
      ],
      "category": "scientific-research-tools"
    }
  }
}