{
  "category": "scientific-research-tools",
  "categoryDisplay": "Scientific Research Tools",
  "description": "",
  "totalRepositories": 94,
  "repositories": {
    "54yyyu--zotero-mcp": {
      "owner": "54yyyu",
      "name": "zotero-mcp",
      "url": "https://github.com/54yyyu/zotero-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/54yyyu.webp",
      "description": "Zotero MCP allows you to link your Zotero research library with AI assistants, enabling you to discuss academic papers, receive summaries, and analyze citations for better research productivity.",
      "stars": 565,
      "forks": 42,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:21:36Z",
      "readme_content": "# Zotero MCP: Chat with your Research Library‚ÄîLocal or Web‚Äîin Claude, ChatGPT, and more.\n\n<p align=\"center\">\n  <a href=\"https://www.zotero.org/\">\n    <img src=\"https://img.shields.io/badge/Zotero-CC2936?style=for-the-badge&logo=zotero&logoColor=white\" alt=\"Zotero\">\n  </a>\n  <a href=\"https://www.anthropic.com/claude\">\n    <img src=\"https://img.shields.io/badge/Claude-6849C3?style=for-the-badge&logo=anthropic&logoColor=white\" alt=\"Claude\">\n  </a>\n  <a href=\"https://chatgpt.com/\">\n    <img src=\"https://img.shields.io/badge/ChatGPT-74AA9C?style=for-the-badge&logo=openai&logoColor=white\" alt=\"ChatGPT\">\n  </a>\n  <a href=\"https://modelcontextprotocol.io/introduction\">\n    <img src=\"https://img.shields.io/badge/MCP-0175C2?style=for-the-badge&logoColor=white\" alt=\"MCP\">\n  </a>\n</p>\n\n**Zotero MCP** seamlessly connects your [Zotero](https://www.zotero.org/) research library with [ChatGPT](https://openai.com), [Claude](https://www.anthropic.com/claude), and other AI assistants (e.g., [Cherry Studio](https://cherry-ai.com/), [Chorus](https://chorus.sh), [Cursor](https://www.cursor.com/)) via the [Model Context Protocol](https://modelcontextprotocol.io/introduction). Review papers, get summaries, analyze citations, extract PDF annotations, and more!\n\n## ‚ú® Features\n\n### üß† AI-Powered Semantic Search\n- **Vector-based similarity search** over your entire research library\n- **Multiple embedding models**: Default (free), OpenAI, and Gemini options\n- **Intelligent results** with similarity scores and contextual matching\n- **Auto-updating database** with configurable sync schedules\n\n### üîç Search Your Library\n- Find papers, articles, and books by title, author, or content\n- Perform complex searches with multiple criteria\n- Browse collections, tags, and recent additions\n- **NEW**: Semantic search for conceptual and topic-based discovery\n\n### üìö Access Your Content\n- Retrieve detailed metadata for any item\n- Get full text content (when available)\n- Access attachments, notes, and child items\n\n### üìù Work with Annotations\n- Extract and search PDF annotations directly\n- Access Zotero's native annotations\n- Create and update notes and annotations\n\n### üîÑ Easy Updates\n- **Smart update system** that detects your installation method (uv, pip, conda, pipx)\n- **Configuration preservation** - all settings maintained during updates\n- **Version checking** and automatic update notifications\n\n### üåê Flexible Access Methods\n- Local method for offline access (no API key needed)\n- Web API for cloud library access\n- Perfect for both local research and remote collaboration\n\n## üöÄ Quick Install\n\n### Default Installation\n\n#### Installing via uv\n\n```bash\nuv tool install \"git+https://github.com/54yyyu/zotero-mcp.git\"\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n#### Installing via pip\n\n```bash\npip install git+https://github.com/54yyyu/zotero-mcp.git\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n### Installing via Smithery\n\nTo install Zotero MCP via [Smithery](https://smithery.ai/server/@54yyyu/zotero-mcp) for Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @54yyyu/zotero-mcp --client claude\n```\n\n#### Updating Your Installation\n\nKeep zotero-mcp up to date with the smart update command:\n\n```bash\n# Check for updates\nzotero-mcp update --check-only\n\n# Update to latest version (preserves all configurations)\nzotero-mcp update\n```\n\n## üß† Semantic Search\n\nZotero MCP now includes powerful AI-powered semantic search capabilities that let you find research based on concepts and meaning, not just keywords.\n\n### Setup Semantic Search\n\nDuring setup or separately, configure semantic search:\n\n```bash\n# Configure during initial setup (recommended)\nzotero-mcp setup\n\n# Or configure semantic search separately\nzotero-mcp setup --semantic-config-only\n```\n\n**Available Embedding Models:**\n- **Default (all-MiniLM-L6-v2)**: Free, runs locally, good for most use cases\n- **OpenAI**: Better quality, requires API key (`text-embedding-3-small` or `text-embedding-3-large`)\n- **Gemini**: Better quality, requires API key (`models/text-embedding-004` or experimental models)\n\n**Update Frequency Options:**\n- **Manual**: Update only when you run `zotero-mcp update-db`\n- **Auto on startup**: Update database every time the server starts\n- **Daily**: Update once per day automatically\n- **Every N days**: Set custom interval\n\n### Using Semantic Search\n\nAfter setup, initialize your search database:\n\n```bash\n# Build the semantic search database (fast, metadata-only)\nzotero-mcp update-db\n\n# Build with full-text extraction (slower, more comprehensive)\nzotero-mcp update-db --fulltext\n\n# Check database status\nzotero-mcp db-status\n```\n\n**Example Semantic Queries in your AI assistant:**\n- *\"Find research similar to machine learning concepts in neuroscience\"*\n- *\"Papers that discuss climate change impacts on agriculture\"*\n- *\"Research related to quantum computing applications\"*\n- *\"Studies about social media influence on mental health\"*\n- *\"Find papers conceptually similar to this abstract: [paste abstract]\"*\n\nThe semantic search provides similarity scores and finds papers based on conceptual understanding, not just keyword matching.\n\n## üñ•Ô∏è Setup & Usage\n\nFull documentation is available at [Zotero MCP docs](https://stevenyuyy.us/zotero-mcp/).\n\n**Requirements**\n- Python 3.10+\n- Zotero 7+ (for local API with full-text access)\n- An MCP-compatible client (e.g., Claude Desktop, ChatGPT Developer Mode, Cherry Studio, Chorus)\n\n**For ChatGPT setup: see the [Getting Started guide](./docs/getting-started.md).**\n\n### For Claude Desktop (example MCP client)\n\n#### Configuration\nAfter installation, either:\n\n1. **Auto-configure** (recommended):\n   ```bash\n   zotero-mcp setup\n   ```\n\n2. **Manual configuration**:\n   Add to your `claude_desktop_config.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"zotero\": {\n         \"command\": \"zotero-mcp\",\n         \"env\": {\n           \"ZOTERO_LOCAL\": \"true\"\n         }\n       }\n     }\n   }\n   ```\n\n#### Usage\n\n1. Start Zotero desktop (make sure local API is enabled in preferences)\n2. Launch Claude Desktop\n3. Access the Zotero-MCP tool through Claude Desktop's tools interface\n\nExample prompts:\n- \"Search my library for papers on machine learning\"\n- \"Find recent articles I've added about climate change\"\n- \"Summarize the key findings from my paper on quantum computing\"\n- \"Extract all PDF annotations from my paper on neural networks\"\n- \"Search my notes and annotations for mentions of 'reinforcement learning'\"\n- \"Show me papers tagged '#Arm' excluding those with '#Crypt' in my library\"\n- \"Search for papers on operating system with tag '#Arm'\"\n- \"Export the BibTeX citation for papers on machine learning\"\n- **\"Find papers conceptually similar to deep learning in computer vision\"** *(semantic search)*\n- **\"Research that relates to the intersection of AI and healthcare\"** *(semantic search)*\n- **\"Papers that discuss topics similar to this abstract: [paste text]\"** *(semantic search)*\n\n### For Cherry Studio\n\n#### Configuration\nGo to Settings -> MCP Servers -> Edit MCP Configuration, and add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"name\": \"zotero\",\n      \"type\": \"stdio\",\n      \"isActive\": true,\n      \"command\": \"zotero-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"ZOTERO_LOCAL\": \"true\"\n      }\n    }\n  }\n}\n```\nThen click \"Save\".\n\nCherry Studio also provides a visual configuration method for general settings and tools selection.\n\n## üîß Advanced Configuration\n\n### Using Web API Instead of Local API\n\nFor accessing your Zotero library via the web API (useful for remote setups):\n\n```bash\nzotero-mcp setup --no-local --api-key YOUR_API_KEY --library-id YOUR_LIBRARY_ID\n```\n\n### Environment Variables\n\n**Zotero Connection:**\n- `ZOTERO_LOCAL=true`: Use the local Zotero API (default: false)\n- `ZOTERO_API_KEY`: Your Zotero API key (for web API)\n- `ZOTERO_LIBRARY_ID`: Your Zotero library ID (for web API)\n- `ZOTERO_LIBRARY_TYPE`: The type of library (user or group, default: user)\n\n**Semantic Search:**\n- `ZOTERO_EMBEDDING_MODEL`: Embedding model to use (default, openai, gemini)\n- `OPENAI_API_KEY`: Your OpenAI API key (for OpenAI embeddings)\n- `OPENAI_EMBEDDING_MODEL`: OpenAI model name (text-embedding-3-small, text-embedding-3-large)\n- `GEMINI_API_KEY`: Your Gemini API key (for Gemini embeddings)\n- `GEMINI_EMBEDDING_MODEL`: Gemini model name (models/text-embedding-004, etc.)\n\n### Command-Line Options\n\n```bash\n# Run the server directly\nzotero-mcp serve\n\n# Specify transport method\nzotero-mcp serve --transport stdio|streamable-http|sse\n\n# Setup and configuration\nzotero-mcp setup --help                    # Get help on setup options\nzotero-mcp setup --semantic-config-only    # Configure only semantic search\nzotero-mcp setup-info                      # Show installation path and config info for MCP clients\n\n# Updates and maintenance\nzotero-mcp update                          # Update to latest version\nzotero-mcp update --check-only             # Check for updates without installing\nzotero-mcp update --force                  # Force update even if up to date\n\n# Semantic search database management\nzotero-mcp update-db                       # Update semantic search database (fast, metadata-only)\nzotero-mcp update-db --fulltext             # Update with full-text extraction (comprehensive but slower)\nzotero-mcp update-db --force-rebuild       # Force complete database rebuild\nzotero-mcp update-db --fulltext --force-rebuild  # Rebuild with full-text extraction\nzotero-mcp db-status                       # Show database status and info\n\n# General\nzotero-mcp version                         # Show current version\n```\n\n## üìë PDF Annotation Extraction\n\nZotero MCP includes advanced PDF annotation extraction capabilities:\n\n- **Direct PDF Processing**: Extract annotations directly from PDF files, even if they're not yet indexed by Zotero\n- **Enhanced Search**: Search through PDF annotations and comments \n- **Image Annotation Support**: Extract image annotations from PDFs\n- **Seamless Integration**: Works alongside Zotero's native annotation system\n\nFor optimal annotation extraction, it is **highly recommended** to install the [Better BibTeX plugin](https://retorque.re/zotero-better-bibtex/installation/) for Zotero. The annotation-related functions have been primarily tested with this plugin and provide enhanced functionality when it's available.\n\n\nThe first time you use PDF annotation features, the necessary tools will be automatically downloaded.\n\n## üìö Available Tools\n\n### üß† Semantic Search Tools\n- `zotero_semantic_search`: AI-powered similarity search with embedding models\n- `zotero_update_search_database`: Manually update the semantic search database\n- `zotero_get_search_database_status`: Check database status and configuration\n\n### üîç Search Tools\n- `zotero_search_items`: Search your library by keywords\n- `zotero_advanced_search`: Perform complex searches with multiple criteria\n- `zotero_get_collections`: List collections\n- `zotero_get_collection_items`: Get items in a collection\n- `zotero_get_tags`: List all tags\n- `zotero_get_recent`: Get recently added items\n- `zotero_search_by_tag`: Search your library using custom tag filters\n\n### üìö Content Tools\n- `zotero_get_item_metadata`: Get detailed metadata (supports BibTeX export via `format=\"bibtex\"`)\n- `zotero_get_item_fulltext`: Get full text content\n- `zotero_get_item_children`: Get attachments and notes\n\n### üìù Annotation & Notes Tools\n- `zotero_get_annotations`: Get annotations (including direct PDF extraction)\n- `zotero_get_notes`: Retrieve notes from your Zotero library\n- `zotero_search_notes`: Search in notes and annotations (including PDF-extracted)\n- `zotero_create_note`: Create a new note for an item (beta feature)\n\n## üîç Troubleshooting\n\n### General Issues\n- **No results found**: Ensure Zotero is running and the local API is enabled. You need to toggle on `Allow other applications on this computer to communicate with Zotero` in Zotero preferences.\n- **Can't connect to library**: Check your API key and library ID if using web API\n- **Full text not available**: Make sure you're using Zotero 7+ for local full-text access\n- **Local library limitations**: Some functionality (tagging, library modifications) may not work with local JS API. Consider using web library setup for full functionality. (See the [docs](docs/getting-started.md#local-library-limitations) for more info.)\n- **Installation/search option switching issues**: Database problems from changing install methods or search options can often be resolved with `zotero-mcp update-db --force-rebuild`\n\n### Semantic Search Issues\n- **\"Missing required environment variables\" when running update-db**: Run `zotero-mcp setup` to configure your environment, or the CLI will automatically load settings from your MCP client config (e.g., Claude Desktop)\n- **ChromaDB warnings**: Update to the latest version - deprecation warnings have been fixed\n- **Database update takes long**: By default, `update-db` is fast (metadata-only). For comprehensive indexing with full-text, use `--fulltext` flag. Use `--limit` parameter for testing: `zotero-mcp update-db --limit 100`\n- **Semantic search returns no results**: Ensure the database is initialized with `zotero-mcp update-db` and check status with `zotero-mcp db-status`\n- **Limited search quality**: For better semantic search results, use `zotero-mcp update-db --fulltext` to index full-text content (requires local Zotero setup)\n- **OpenAI/Gemini API errors**: Verify your API keys are correctly set and have sufficient credits/quota\n\n### Update Issues  \n- **Update command fails**: Check your internet connection and try `zotero-mcp update --force`\n- **Configuration lost after update**: The update process preserves configs automatically, but check `~/.config/zotero-mcp/` for backup files\n\n## üìÑ License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zotero",
        "research",
        "mcp",
        "mcp zotero",
        "zotero research",
        "zotero mcp"
      ],
      "category": "scientific-research-tools"
    },
    "BioContext--PubChem-MCP": {
      "owner": "BioContext",
      "name": "PubChem-MCP",
      "url": "https://github.com/BioContext/PubChem-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/BioContext.webp",
      "description": "Provides access to PubChem chemical and bioassay data through a standardized MCP interface, allowing users to search for compounds, retrieve detailed chemical and bioassay information, and query molecular properties.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-04-17T20:18:58Z",
      "readme_content": "# PubChem-MCP\n\nAn MCP (Model Context Protocol) server for accessing PubChem data. This server provides tools for querying the PubChem database for compounds, substances, bioassays, and related information.\n\n## Features\n\n* Search for compounds by name, structure, or identifier\n* Retrieve detailed information about compounds and substances\n* Access bioassay data and molecular properties\n* Query chemical classifications and cross-references\n* Compatible with any MCP client, including Claude for Desktop\n\n## Installation\n\n### Option 1: From GitHub\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### Option 2: Using pip (Once published)\n\n```bash\npip install pubchem-mcp\n```\n\n## Usage\n\n### Running as a standalone server\n\n```bash\n# From source\npython -m mcp_server\n\n# If installed via pip\npubchem-mcp\n```\n\n### Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Configure Claude for Desktop to use this server by editing `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS/Linux) or `%AppData%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n    \"mcpServers\": {\n        \"pubchem\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"mcp_server\"\n            ]\n        }\n    }\n}\n```\n\n3. Restart Claude for Desktop\n4. You can now use the PubChem tools in Claude\n\n### Example queries for Claude\n\n* \"Find information about aspirin in PubChem\"\n* \"What are the properties of compound CID 2244?\"\n* \"Show me the structure of paracetamol\"\n\n## Available Tools\n\n* `search_compound`: Search for compounds by name or identifier\n* `get_compound_details`: Get detailed information about a compound\n* `get_compound_properties`: Get physical and chemical properties of a compound\n* `search_bioassay`: Search for bioassays\n* `get_substance_details`: Get detailed information about a substance\n* And more...\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biocontext",
        "pubchem",
        "bioassay",
        "biocontext pubchem",
        "tools biocontext",
        "pubchem mcp"
      ],
      "category": "scientific-research-tools"
    },
    "DMontgomery40--mcp-server-birdstats": {
      "owner": "DMontgomery40",
      "name": "mcp-server-birdstats",
      "url": "https://github.com/DMontgomery40/mcp-server-birdstats",
      "imageUrl": "/freedevtools/mcp/pfp/DMontgomery40.webp",
      "description": "Analyze BirdNET-Pi data and eBird observations through a natural language interface while providing visual representations of the data.",
      "stars": 1,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2024-12-13T00:47:19Z",
      "readme_content": "# mcp-server-birdstats\n\n> Cross-reference your BirdNET-Pi data with eBird observations using natural language\n\n## Overview\n\nThis MCP plugin allows you to analyze BirdNET-Pi/BirdWeather detections by connecting them with eBird's extensive database. \n\n## Features\n\n- Natural language interface for querying birdnet-pi / birdweather, and eBird data\n- Cross-reference BirdWeather detections with eBird observations\n- Data-driven visualizations with React artifacts\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-server-birdstats.git\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n## Claude Config\n```\n\"mcp-birdstats\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-birdstats\"]\n    },\n```\n\n\n## Usage\n\nThe plugin processes natural language queries about your bird detection data. Example queries:\n\n- \"My BirdNET heard a Swainson's Thrush - is that unusual?\"\n- \"What's my station's rarest detection this week?\"\n- \"Graph my detections by time of day\"\n- \"Are others seeing these warblers on eBird?\"\n\n## Data Integrity\n\nThis plugin strictly adheres to data integrity principles:\n- Only uses actual API response data\n- Never creates example or estimated data\n- Always shows observation counts with percentages\n- Clear indication when data is unavailable\n\n## Core Logic\n\nThe core functionality is contained in a single text file under `src/index.txt` which includes:\n- BirdWeather API interactions\n- eBird API interactions\n- Data analysis functions\n- Type definitions\n- Helper utilities\n\n## API Integration\n\n### BirdWeather API\n- Station statistics\n- Species lists\n- Detection records\n\n### eBird API\n- Recent observations\n- Notable sightings\n- Hotspot data\n- Species information\n\n## License\n\nMIT\n\n## Support\n\nFor issues and feature requests, please open an issue on the GitHub repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "birdstats",
        "birdnet",
        "ebird",
        "birdstats analyze",
        "analyze birdnet",
        "server birdstats"
      ],
      "category": "scientific-research-tools"
    },
    "Garoth--wolframalpha-llm-mcp": {
      "owner": "Garoth",
      "name": "wolframalpha-llm-mcp",
      "url": "https://github.com/Garoth/wolframalpha-llm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Garoth.webp",
      "description": "Provides access to WolframAlpha's LLM API for querying complex information in natural language, allowing for structured responses on topics like science, mathematics, and history.",
      "stars": 39,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T02:59:15Z",
      "readme_content": "# WolframAlpha LLM MCP Server\n\n<img src=\"assets/wolfram-llm-logo.png\" width=\"256\" alt=\"WolframAlpha LLM MCP Logo\" />\n\nA Model Context Protocol (MCP) server that provides access to WolframAlpha's LLM API. https://products.wolframalpha.com/llm-api/documentation\n\n<div>\n  <img src=\"assets/readme-screen-1.png\" width=\"609\" alt=\"WolframAlpha MCP Server Example 1\" /><br/><br/>\n  <img src=\"assets/readme-screen-2.png\" width=\"609\" alt=\"WolframAlpha MCP Server Example 2\" />\n</div>\n\n## Features\n\n- Query WolframAlpha's LLM API with natural language questions\n- Answer complicated mathematical questions\n- Query facts about science, physics, history, geography, and more\n- Get structured responses optimized for LLM consumption\n- Support for simplified answers and detailed responses with sections\n\n## Available Tools\n\n- `ask_llm`: Ask WolframAlpha a question and get a structured llm-friendly response\n- `get_simple_answer`: Get a simplified answer\n- `validate_key`: Validate the WolframAlpha API key\n\n## Installation\n\n```bash\ngit clone https://github.com/Garoth/wolframalpha-llm-mcp.git\nnpm install\n```\n\n## Configuration\n\n1. Get your WolframAlpha API key from [developer.wolframalpha.com](https://developer.wolframalpha.com/)\n\n2. Add it to your Cline MCP settings file inside VSCode's settings (ex. ~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"wolframalpha\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/wolframalpha-mcp-server/build/index.js\"],\n      \"env\": {\n        \"WOLFRAM_LLM_APP_ID\": \"your-api-key-here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"ask_llm\",\n        \"get_simple_answer\",\n        \"validate_key\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n### Setting Up Tests\n\nThe tests use real API calls to ensure accurate responses. To run the tests:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit `.env` and add your WolframAlpha API key:\n   ```\n   WOLFRAM_LLM_APP_ID=your-api-key-here\n   ```\n   Note: The `.env` file is gitignored to prevent committing sensitive information.\n\n3. Run the tests:\n   ```bash\n   npm test\n   ```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolframalpha",
        "scientific",
        "tools",
        "research tools",
        "wolframalpha llm",
        "llm api"
      ],
      "category": "scientific-research-tools"
    },
    "HarshJ23--deepseek-claude-MCP-server": {
      "owner": "HarshJ23",
      "name": "deepseek-claude-MCP-server",
      "url": "https://github.com/HarshJ23/deepseek-claude-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/HarshJ23.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude to enhance reasoning capabilities for complex tasks. Supports intricate multi-step reasoning for improved precision and efficiency.",
      "stars": 50,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-13T00:27:41Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## üöÄ Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   ![claude_desktop_config.json file](public/images/edit_config.png)\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   ![tool visible](public/images/tool1.png)\n![tool verify](public/images/tool2.png)\n\n   - If the server isn‚Äôt visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## üõ† Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## üìÑ License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "advanced",
        "reasoning engine",
        "deepseek r1",
        "integrates deepseek"
      ],
      "category": "scientific-research-tools"
    },
    "Jacck--mcp-reasoner": {
      "owner": "Jacck",
      "name": "mcp-reasoner",
      "url": "https://github.com/Jacck/mcp-reasoner",
      "imageUrl": "/freedevtools/mcp/pfp/Jacck.webp",
      "description": "Systematic reasoning server implementation for Claude Desktop, supporting Beam Search and Monte Carlo Tree Search (MCTS) for enhanced complex problem-solving. Features experimental algorithms for policy simulation and adaptive exploration.",
      "stars": 254,
      "forks": 28,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T12:29:08Z",
      "readme_content": "# MCP Reasoner\nA reasoning implementation for Claude Desktop that lets you use both Beam Search and Monte Carlo Tree Search (MCTS). tbh this started as a way to see if we could make Claude even better at complex problem-solving... turns out we definitely can.\n\n### Current Version:\n**v2.0.0**\n\n#### What's New:\n\n> Added 2 Experimental Reasoning Algorithms:\n>\n>     - `mcts-002-alpha`\n>\n>         - Uses the A* Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n>\n>     - `mcts-002alt-alpha`\n>\n>         - Uses the Bidirectional Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n\n\nWhat happened to `mcts-001-alpha` and `mcts-001alt-alpha`?\n> Quite simply: It was useless and near similar to the base `mcts` method. After initial testing the results yielded in basic thought processes was near similar showing that simply adding policy simulation may not have an effect.\n\nSo why add Polciy Simulation Layer now?\n> Well i think its important to incorporate Policy AND Search in tandem as that is how most of the algorithms implement them.\n\n#### Previous Versions:\n**v1.1.0**\n\n> Added model control over search parameters:\n>\n> beamWidth - lets Claude adjust how many paths to track (1-10)\n>\n> numSimulations - fine-tune MCTS simulation count (1-150)\n\n## Features\n- Two search strategies that you can switch between:\n   - Beam search (good for straightforward stuff)\n   - MCTS (when stuff gets complex) with alpha variations (see above)\n- Tracks how good different reasoning paths are\n- Maps out all the different ways Claude thinks through problems\n- Analyzes how the reasoning process went\n- Follows the MCP protocol (obviously)\n\n## Installation\n```\ngit clone https://github.com/frgmt0/mcp-reasoner.git \n\nOR clone the original:\n\ngit clone https://github.com/Jacck/mcp-reasoner.git\n\ncd mcp-reasoner\nnpm install\nnpm run build\n```\n\n## Configuration\nAdd to Claude Desktop config:\n```\n{\n  \"mcpServers\": {\n    \"mcp-reasoner\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-reasoner/dist/index.js\"],\n    }\n  }\n}\n```\n\n## Testing\n\n[More Testing Coming Soon]\n\n## Benchmarks\n\n[Benchmarking will be added soon]\n\nKey Benchmarks to test against:\n\n- MATH500\n\n- GPQA-Diamond\n\n- GMSK8\n\n- Maybe Polyglot &/or SWE-Bench\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "reasoner",
        "mcts",
        "systematic reasoning",
        "mcp reasoner",
        "reasoning server"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Crossref-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Crossref-MCP-Server",
      "url": "https://github.com/JackKuo666/Crossref-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connect to Crossref's database to search and access academic paper metadata, including metadata retrieval for specific papers, journals, and funding organizations using various queries.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:46Z",
      "readme_content": "# Crossref MCP Server\n\nüîç Enable AI assistants to search and access academic paper metadata through Crossref using a simple MCP interface.\n\nThe Crossref MCP Server provides a bridge between AI assistants and Crossref's database of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and retrieve journal and funder information in a programmatic way.\n\n## ‚ú® Core Features\n\n- üîé Work Search by Query: Find papers using keywords, titles, or authors ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers by DOI ‚úÖ\n- üìö Journal Search: Find journals in the Crossref database ‚úÖ\n- üí∞ Funder Search: Discover funding organizations and their supported research ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Crossref-MCP-Server.git\n   cd Crossref-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython crossref_server.py\n```\n\n## Usage with Claude Desktop or Cline\n\nAdd this configuration to your `cline_mcp_settings.json` or `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"crossref_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Crossref-MCP-Server\\\\crossref_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe Crossref MCP Server provides the following tools:\n\n1. `search_works_by_query`: Search for scholarly works using keywords, titles, or authors.\n2. `get_work_metadata`: Get detailed metadata for a specific work using its DOI.\n3. `search_journals`: Search for journals in the Crossref database.\n4. `search_funders`: Search for funding organizations in the Crossref database.\n\n### Searching Works by Query\n\nYou can ask the AI assistant to search for papers using keywords:\n```\nCan you search Crossref for papers about \"machine learning in healthcare\"?\n```\n\n### Getting Work Metadata by DOI\n\nYou can get detailed metadata for a specific paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature14539?\n```\n\n### Searching Journals\n\nYou can search for journals in the Crossref database:\n```\nCan you find journals related to \"artificial intelligence\" in Crossref?\n```\n\n### Searching Funders\n\nYou can search for funding organizations:\n```\nCan you find information about the \"National Science Foundation\" in Crossref?\n```\n\n\n## üìÅ Project Structure\n\n- `crossref_server.py`: The main MCP server implementation using FastMCP\n- `crossref_search.py`: Contains the logic for searching Crossref and retrieving metadata\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP (mcp)\n- requests\n- bs4\n- habanero\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Note\n\nThis tool uses the Crossref API to access publicly available metadata about academic works. For better API access priority, it's recommended to provide your email address when initializing the CrossrefSearch class.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crossref",
        "journals",
        "metadata",
        "research tools",
        "paper metadata",
        "metadata retrieval"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Google-Scholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Google-Scholar-MCP-Server",
      "url": "https://github.com/JackKuo666/Google-Scholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Enables AI assistants to perform searches on Google Scholar to access academic papers, retrieve paper metadata, and gather detailed author information. Facilitates efficient academic research through a seamless programmatic interface.",
      "stars": 138,
      "forks": 25,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T08:00:22Z",
      "readme_content": "# Google Scholar MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/google-scholar-mcp-server)](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server)\n\nüîç Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n\nThe Google Scholar MCP Server provides a bridge between AI assistants and Google Scholar through the Model Context Protocol (MCP). It allows AI models to search for academic papers and access their content in a programmatic way.\n\n## ‚ú® Core Features\n- üîé Paper Search: Query Google Scholar papers with custom search strings or advanced search parameters ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üë§ Author Information: Retrieve detailed information about authors ‚úÖ\n- üìä Research Support: Facilitate academic research and analysis ‚úÖ\n\n## üöÄ Quick Start\n\n### Installing Manually\n### Installing via Smithery\n\nTo install google-scholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/google-scholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/google-scholar-MCP-Server.git\n   cd google-scholar-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/Google-Scholar-MCP-Server.git\ncd Google-Scholar-MCP-Server\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython google_scholar_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_key_words\", {\n    \"query\": \"artificial intelligence ethics\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_advanced\", {\n    \"query\": \"machine learning\",\n    \"author\": \"Hinton\",\n    \"year_range\": [2020, 2023],\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get author information\n\n```python\nresult = await mcp.use_tool(\"get_author_info\", {\n    \"author_name\": \"Geoffrey Hinton\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the Google Scholar MCP Server. Adjust the parameters as needed for your specific use case.\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"google_scholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Google-Scholar-MCP-Server\\\\google_scholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/google_scholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## üõ† MCP Tools\n\nThe Google Scholar MCP Server provides the following tools:\n\n### search_google_scholar_key_words\n\nSearch for articles on Google Scholar using key words.\n\n**Parameters:**\n- `query` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### search_google_scholar_advanced\n\nPerform an advanced search for articles on Google Scholar.\n\n**Parameters:**\n- `query` (str): General search query\n- `author` (str, optional): Author name\n- `year_range` (tuple, optional): Tuple containing (start_year, end_year)\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### get_author_info\n\nGet detailed information about an author from Google Scholar.\n\n**Parameters:**\n- `author_name` (str): Name of the author to search for\n\n**Returns:** Dictionary containing author information\n\n## üìÅ Project Structure\n\n- `google_scholar_server.py`: The main MCP server implementation using FastMCP\n- `google_scholar_web_search.py`: Contains the web scraping logic for searching Google Scholar\n\n## üîß Dependencies\n\n- Python 3.10+\n- mcp[cli]>=1.4.1\n- scholarly>=1.7.0\n- asyncio>=3.4.3\n\nYou can install the required dependencies using:\n\n```bash\npip install -r requirements.txt\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect Google Scholar's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "google",
        "academic",
        "google scholar",
        "scholar access",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubChem-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubChem-MCP-Server",
      "url": "https://github.com/JackKuo666/PubChem-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Access and retrieve information about chemical compounds from PubChem, including detailed property data and molecular structures through a simple interface. Supports queries by name, SMILES, or CID to streamline research and analysis in chemistry.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-13T09:03:06Z",
      "readme_content": "# PubChem MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubchem-mcp-server)](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server)\n\nüß™ Enable AI assistants to search and access chemical compound information through a simple MCP interface.\n\nThe PubChem MCP Server provides a bridge between AI assistants and PubChem's chemical database through the Model Context Protocol (MCP). It allows AI models to search for chemical compounds and access their detailed information in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Compound Search: Query PubChem compounds by name, SMILES, or CID ‚úÖ\n- üß™ Chemical Structure: Access molecular structures and identifiers ‚úÖ\n- üìä Property Data: Retrieve detailed chemical and physical properties ‚úÖ\n- üî¨ Advanced Search: Combine multiple parameters for precise queries ‚úÖ\n- üß¨ Molecular Visualization: Generate and display molecular structures üìù\n- üìà Property Analysis: Compare properties across multiple compounds üìù\n- üóÉÔ∏è Local Storage: Save frequently used compounds for faster access üìù\n- üìù Chemistry Prompts: Specialized prompts for chemical analysis üìù\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install PubChem Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server):\n\n#### Claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubchem-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client cline --config \"{}\"\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install pubchem-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/PubChem-MCP-Server.git\ncd PubChem-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython pubchem_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for compounds by name\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_name\", {\n    \"name\": \"aspirin\",\n    \"max_results\": 3\n})\nprint(result)\n```\n\n### Example 2: Search for compounds by SMILES notation\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_smiles\", {\n    \"smiles\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin's SMILES\n    \"max_results\": 2\n})\nprint(result)\n```\n\n### Example 3: Get detailed information for a specific compound\n\n```python\nresult = await mcp.use_tool(\"get_pubchem_compound_by_cid\", {\n    \"cid\": 2244  # Aspirin's CID\n})\nprint(result)\n```\n\n### Example 4: Perform an advanced search with multiple parameters\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_advanced\", {\n    \"name\": \"caffeine\",\n    \"formula\": \"C8H10N4O2\",\n    \"max_results\": 2\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the four main tools provided by the PubChem MCP Server. Adjust the parameters as needed for your specific use case.\n\n## üõ† MCP Tools\n\nThe PubChem MCP Server provides the following tools:\n\n### search_pubchem_by_name\n\nSearch for chemical compounds on PubChem using a compound name.\n\n**Parameters:**\n- `name` (str): Name of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### search_pubchem_by_smiles\n\nSearch for chemical compounds on PubChem using a SMILES string.\n\n**Parameters:**\n- `smiles` (str): SMILES notation of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### get_pubchem_compound_by_cid\n\nFetch detailed information about a chemical compound using its PubChem CID.\n\n**Parameters:**\n- `cid` (int): PubChem Compound ID (CID)\n\n**Returns:** Dictionary containing compound information\n\n### search_pubchem_advanced\n\nPerform an advanced search for compounds on PubChem.\n\n**Parameters:**\n- `name` (str, optional): Name of the chemical compound\n- `smiles` (str, optional): SMILES notation of the chemical compound\n- `formula` (str, optional): Molecular formula\n- `cid` (int, optional): PubChem Compound ID\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubchem-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"pubchem-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/.venv/bin/activate && python /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/pubchem_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Compounds\n\nYou can ask Claude to search for chemical compounds using queries like:\n```\nCan you search PubChem for information about aspirin?\n```\n\nThe search will return basic information about matching compounds including:\n\n‚Ä¢ Compound name\n\n‚Ä¢ CID (PubChem Compound ID)\n\n‚Ä¢ Molecular formula\n\n‚Ä¢ Molecular weight\n\n### Getting Compound Details\n\nOnce you have a CID, you can ask for more details:\n```\nCan you show me the details for compound with CID 2244?\n```\n\nThis will return:\n\n‚Ä¢ IUPAC name\n\n‚Ä¢ Molecular formula\n\n‚Ä¢ Molecular weight\n\n‚Ä¢ SMILES notation\n\n‚Ä¢ InChI and InChIKey\n\n‚Ä¢ Physical and chemical properties\n\n‚Ä¢ Synonyms\n\n## üìù TODO\n\n### visualize_compound\n\nGenerate and display a 2D or 3D visualization of a chemical compound.\n\n### compare_compounds\n\nCompare properties and structures of multiple compounds.\n\n### save_compound\n\nSave a compound locally for faster access.\n\n### list_saved_compounds\n\nList all saved compounds.\n\n### üìù Chemistry Prompts\n\nThe server will offer specialized prompts to help analyze chemical compounds:\n\n#### Compound Analysis Prompt\n\nA comprehensive workflow for analyzing chemical compounds that only requires a compound ID:\n\n```python\nresult = await call_prompt(\"deep-compound-analysis\", {\n    \"compound_id\": \"2244\"\n})\n```\n\nThis prompt will include:\n\n- Detailed instructions for using available tools\n- A systematic workflow for compound analysis\n- Comprehensive analysis structure covering:\n  - Chemical structure and properties\n  - Pharmacological properties\n  - Biological activities\n  - Applications and uses\n  - Safety and toxicity information\n  - Related compounds\n\n## üìÅ Project Structure\n\n- `pubchem_server.py`: The main MCP server implementation using FastMCP\n- `pubchem_search.py`: Contains example code for searching PubChem\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- pubchempy (for PubChem API access)\n- pandas (for data handling)\n\nYou can install the required dependencies using:\n\n```bash\npip install mcp pubchempy pandas\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect PubChem's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubchem",
        "molecular",
        "compounds",
        "compounds pubchem",
        "pubchem mcp",
        "pubchem including"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubMed-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubMed-MCP-Server",
      "url": "https://github.com/JackKuo666/PubMed-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Connects AI models to PubMed for searching, accessing, and analyzing biomedical articles, allowing retrieval of detailed metadata and deep analysis of research papers. Facilitates access to full-text PDFs and supports specialized prompts for comprehensive paper examination.",
      "stars": 64,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T17:15:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jackkuo666-pubmed-mcp-server-badge.png)](https://mseep.ai/app/jackkuo666-pubmed-mcp-server)\n\n# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubmed-mcp-server)](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server)\n\nüîç Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n\nThe PubMed MCP Server provides a bridge between AI assistants and PubMed's vast repository of biomedical literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles, access their metadata, and perform deep analysis in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Paper Search: Query PubMed articles with keywords or advanced search ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìä Research Support: Facilitate biomedical sciences research and analysis ‚úÖ\n- üìÑ Paper Access: Attempt to download full-text PDF content ‚úÖ\n- üß† Deep Analysis: Perform comprehensive analysis of papers ‚úÖ\n- üìù Research Prompts: A set of specialized prompts for paper analysis ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n### Installing via Smithery\n\nTo install pubmed-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli install @JackKuo666/pubmed-mcp-server --client claude\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubmed-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubMed-MCP-Server.git\n   cd PubMed-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython pubmed_server.py\n```\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubmed-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubMed-MCP-Server\\\\pubmed_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubmed/.venv/bin/activate && python /home/YOUR/PATH/pubmed-mcp-server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe PubMed MCP Server provides the following tools:\n\n1. `search_pubmed_key_words`: Search for articles on PubMed using keywords.\n2. `search_pubmed_advanced`: Perform an advanced search for articles on PubMed with multiple parameters.\n3. `get_pubmed_article_metadata`: Fetch metadata for a PubMed article using its PMID.\n4. `download_pubmed_pdf`: Attempt to download the full-text PDF for a PubMed article.\n5. `deep_paper_analysis`: Perform a comprehensive analysis of a PubMed article.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search PubMed for recent papers about CRISPR?\n```\n\n### Getting Paper Details\n\nOnce you have a PMID, you can ask for more details:\n```\nCan you show me the metadata for the paper with PMID 12345678?\n```\n\n### Analyzing Papers\n\nYou can request a deep analysis of a paper:\n```\nCan you perform a deep analysis of the paper with PMID 12345678?\n```\n\n## üìÅ Project Structure\n\n- `pubmed_server.py`: The main MCP server implementation using FastMCP\n- `pubmed_web_search.py`: Contains the logic for searching PubMed and retrieving article information\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests\n- beautifulsoup4\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect PubMed's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "papers",
        "pubmed searching",
        "models pubmed",
        "pubmed mcp"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubTator-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubTator-MCP-Server",
      "url": "https://github.com/JackKuo666/PubTator-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Access and analyze biomedical literature with advanced annotation and relationship mining capabilities. Programmatically search and export literature data through a user-friendly interface.",
      "stars": 8,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T13:18:53Z",
      "readme_content": "# PubTator MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubtator-mcp-server)](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server)\n\nüîç A biomedical literature annotation and relationship mining server based on PubTator3, providing convenient access through the MCP interface.\n\nPubTator MCP Server provides AI assistants with access to the PubTator3 biomedical literature annotation system through the Model Context Protocol (MCP). It allows AI models to programmatically search scientific literature, obtain annotation information, and analyze entity relationships.\n\nü§ù Contribute ‚Ä¢ üìù Report Issues\n\n## ‚ú® Core Features\n- üîé Literature Annotation Export: Support exporting PubTator annotation results in multiple formats ‚úÖ\n- üöÄ Entity ID Lookup: Query standard identifiers for biological concepts through free text ‚úÖ\n- üìä Relationship Mining: Discover biomedical relationships between entities ‚úÖ\n- üìÑ Literature Search: Support literature retrieval by keywords and entity IDs ‚úÖ\n- üß† Batch Processing: Support batch export of annotation information from search results ‚úÖ\n\n## üöÄ Quick Start\n\n### Requirements\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n#### Via Smithery\n\nUse [Smithery](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server) to automatically install PubTator Server:\n\n##### Claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client claude --config \"{}\"\n```\n\n##### Cursor\n\nPaste in Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server:\n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubtator-mcp-server --client cursor --config \"{}\" \n```\n\n##### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client windsurf --config \"{}\"\n```\n\n##### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client cline --config \"{}\"\n```\n\n#### Manual Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubTator-MCP-Server.git\n   cd PubTator-MCP-Server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\n### Running the Server Directly\n\nStart the MCP server:\n\n```bash\npython pubtator_server.py\n```\n\nThe server now supports both stdio and TCP transports. By default, it uses TCP transport. You can configure the following environment variables:\n\n- `MCP_TRANSPORT`: Set to \"tcp\" for TCP transport or \"stdio\" for stdio transport (default is \"tcp\")\n- `MCP_HOST`: The host to bind to (default is \"0.0.0.0\")\n- `MCP_PORT`: The port to listen on (default is 8080)\n\nExample of starting the server with custom settings:\n\n```bash\nMCP_TRANSPORT=tcp MCP_HOST=127.0.0.1 MCP_PORT=8888 python pubtator_server.py\n```\n\nThe server implements lazy initialization and proper error handling. It will gracefully handle shutdown signals (SIGINT and SIGTERM) and log any errors that occur during startup or operation.\n\n### Using Docker\n\nWe provide a Dockerfile for easy deployment. To use the Docker container:\n\n1. Build the Docker image:\n   ```bash\n   docker build -t pubtator-mcp-server .\n   ```\n\n2. Run the Docker container:\n   ```bash\n   docker run -p 8080:8080 pubtator-mcp-server\n   ```\n\nThis will start the PubTator MCP server inside a Docker container, exposing it on port 8080.\n\n### Troubleshooting\n\nIf you encounter any issues starting the server:\n\n1. Check the console output for error messages.\n2. Ensure all required dependencies are installed (see Requirements section).\n3. Verify that the environment variables are set correctly.\n4. If the server fails to start, try running it with increased verbosity:\n\n```bash\npython -v pubtator_server.py\n```\n\nThis will provide more detailed logging information to help identify the source of any issues.\n\nWhen using Docker, you can check the logs with:\n\n```bash\ndocker logs <container_id>\n```\n\n### Configuration\n\n#### Claude Desktop Configuration\n\nAdd to `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubtator-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubTator-MCP-Server\\\\pubtator_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n#### CLine Configuration\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"stdio\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nTo use TCP transport, modify the configuration as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"tcp\",\n        \"MCP_HOST\": \"127.0.0.1\",\n        \"MCP_PORT\": \"8888\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† API Features\n\nPubTator MCP Server provides the following core features:\n\n### 1. Export Publications (export_publications)\n\nExport PubTator annotation results for specified PMID literature:\n```python\nresult = await export_publications(\n    ids=[\"32133824\", \"34170578\"],\n    id_type=\"pmid\",\n    format=\"biocjson\",  # Supported: pubtator, biocxml, biocjson\n    full_text=False     # Whether to include full text\n)\n```\n\n### 2. Entity ID Lookup (find_entity_id)\n\nQuery standard identifiers for biological concepts through free text:\n```python\nresult = await find_entity_id(\n    query=\"COVID-19\",\n    concept=\"disease\",  # Optional: gene, disease, chemical, species, mutation\n    limit=5             # Optional: limit number of results\n)\n```\n\n### 3. Relationship Query (find_related_entities)\n\nFind other entities related to a specified entity:\n```python\nresult = await find_related_entities(\n    entity_id=\"@DISEASE_COVID_19\",\n    relation_type=\"treat\",    # Optional: treat, cause, interact, etc.\n    target_entity_type=\"chemical\",  # Optional: gene, disease, chemical\n    max_results=5       # Optional: limit number of results\n)\n```\n\n### 4. Literature Search (search_pubtator)\n\nSearch the PubTator database:\n```python\nresults = await search_pubtator(\n    query=\"COVID-19\",\n    max_pages=1     # Optional: maximum number of pages to retrieve\n)\n```\n\n### 5. Batch Export (batch_export_from_search)\n\nSearch and batch export literature annotations:\n```python\nresults = await batch_export_from_search(\n    query=\"COVID-19\",\n    format=\"biocjson\",\n    max_pages=1,\n    full_text=False,\n    batch_size=5\n)\n```\n\nNote: The actual function calls may vary depending on your implementation. These examples are based on our recent tests and may need to be adjusted to match your exact API.\n\n## ‚ö†Ô∏è Usage Limitations\n\n- API request rate limit: maximum 3 requests per second\n- When batch exporting, use a reasonable batch_size to avoid request timeout\n- For relationship queries, entity IDs must start with \"@\", e.g., \"@DISEASE_COVID-19\"\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please comply with PubTator's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomedical",
        "annotation",
        "literature",
        "biomedical literature",
        "analyze biomedical",
        "literature data"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Sci-Hub-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Sci-Hub-MCP-Server",
      "url": "https://github.com/JackKuo666/Sci-Hub-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Connects to Sci-Hub for searching, accessing, and analyzing academic papers. Supports retrieving metadata and downloading full-text PDFs programmatically.",
      "stars": 35,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T00:11:18Z",
      "readme_content": "# Sci-Hub MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/sci-hub-mcp-server)](https://smithery.ai/server/@JackKuo666/sci-hub-mcp-server)\n\nüîç Enable AI assistants to search, access, and analyze academic papers through Sci-Hub using a simple MCP interface.\n\nThe Sci-Hub MCP Server provides a bridge between AI assistants and Sci-Hub's repository of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and download PDFs in a programmatic way.\n\n## ‚ú® Core Features\n\n- üîé Paper Search by DOI: Find papers using their Digital Object Identifier ‚úÖ\n- üîç Paper Search by Title: Locate papers using their full or partial title ‚úÖ\n- üîë Paper Search by Keyword: Discover papers related to specific research areas ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìÑ PDF Download: Download full-text PDF content when available ‚úÖ\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Sci-Hub-MCP-Server.git\n   cd Sci-Hub-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython sci_hub_server.py\n```\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"sci_hub_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Sci-Hub-MCP-Server\\\\sci_hub_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## üõ† MCP Tools\n\nThe Sci-Hub MCP Server provides the following tools:\n\n1. `search_scihub_by_doi`: Search for a paper on Sci-Hub using its DOI (Digital Object Identifier).\n2. `search_scihub_by_title`: Search for a paper on Sci-Hub using its title.\n3. `search_scihub_by_keyword`: Search for papers on Sci-Hub using a keyword.\n4. `download_scihub_pdf`: Download a paper PDF from Sci-Hub.\n5. `get_paper_metadata`: Get metadata information for a paper using its DOI.\n\n### Searching Papers by DOI\n\nYou can ask the AI assistant to search for papers using DOI:\n```\nCan you search Sci-Hub for the paper with DOI 10.1038/nature09492?\n```\n\n### Searching Papers by Title\n\nYou can search for papers using their title:\n```\nCan you find the paper titled \"Choosing Assessment Instruments for Posttraumatic Stress Disorder Screening and Outcome Research\" on Sci-Hub?\n```\n\n### Searching Papers by Keyword\n\nYou can search for papers related to specific keywords:\n```\nCan you search Sci-Hub for recent papers about artificial intelligence in medicine?\n```\n\n### Downloading Papers\n\nOnce you have found a paper, you can download it:\n```\nCan you download the PDF for this paper to my_paper.pdf?\n```\n\n### Getting Paper Metadata\n\nYou can request metadata for a paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature09492?\n```\n\n## üìÅ Project Structure\n\n- `sci_hub_server.py`: The main MCP server implementation using FastMCP\n- `sci_hub_search.py`: Contains the logic for searching Sci-Hub and retrieving paper information\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- requests\n- bs4\n- scihub\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect copyright laws and use this tool responsibly. The authors do not endorse or encourage any copyright infringement.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "sci",
        "metadata",
        "research tools",
        "sci hub",
        "metadata downloading"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--bioRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "bioRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/bioRxiv-MCP-Server",
      "imageUrl": "https://github.com/JackKuo666.png",
      "description": "Search for bioRxiv papers, retrieve detailed metadata, and download papers to support biological sciences research and analysis.",
      "stars": 20,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-04T18:06:12Z",
      "readme_content": "# bioRxiv MCP Server\n\nüîç Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n\nThe bioRxiv MCP Server provides a bridge between AI assistants and bioRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for biology preprints and access their metadata in a programmatic way.\n\nü§ù Contribute ‚Ä¢ üìù Report Bug\n\n## ‚ú® Core Features\n- üîé Paper Search: Query bioRxiv papers with keywords or advanced search ‚úÖ\n- üöÄ Efficient Retrieval: Fast access to paper metadata ‚úÖ\n- üìä Metadata Access: Retrieve detailed metadata for specific papers ‚úÖ\n- üìä Research Support: Facilitate biological sciences research and analysis ‚úÖ\n- üìÑ Paper Access: Download and read paper content üìù\n- üìã Paper Listing: View all downloaded papers üìù\n- üóÉÔ∏è Local Storage: Papers are saved locally for faster access üìù\n- üìù Research Prompts: A set of specialized prompts for paper analysis üìù\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/bioRxiv-MCP-Server.git\n   cd bioRxiv-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n### Installing via Smithery\n\nTo install bioRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/biorxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/biorxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client windsurf --config \"{}\"\n```\n#### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client cline --config \"{}\"\n```\n\n#### Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"biorxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"biorxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-bioRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-bioRxiv/biorxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## üìä Usage\n\nStart the MCP server:\n\n```bash\npython biorxiv_server.py\n```\n\n## üõ† MCP Tools\n\nThe bioRxiv MCP Server provides the following tools:\n\n1. `search_biorxiv_key_words`: Search for articles on bioRxiv using keywords.\n2. `search_biorxiv_advanced`: Perform an advanced search for articles on bioRxiv with multiple parameters.\n3. `get_biorxiv_metadata`: Fetch metadata for a bioRxiv article using its DOI.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search bioRxiv for recent papers about genomics?\n```\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the metadata for the paper with DOI 10.1101/123456?\n```\n\n## üìÅ Project Structure\n\n- `biorxiv_server.py`: The main MCP server implementation using FastMCP\n- `biorxiv_web_search.py`: Contains the web scraping logic for searching bioRxiv\n\n## üîß Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis tool is for research purposes only. Please respect bioRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biorxiv",
        "biological",
        "scientific",
        "biorxiv papers",
        "search biorxiv",
        "jackkuo666 biorxiv"
      ],
      "category": "scientific-research-tools"
    },
    "JotaDeRodriguez--Bonsai_mcp": {
      "owner": "JotaDeRodriguez",
      "name": "Bonsai_mcp",
      "url": "https://github.com/JotaDeRodriguez/Bonsai_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/JotaDeRodriguez.webp",
      "description": "Connects Blender with Claude to analyze and manipulate IFC models, enabling querying of project information, exploration of spatial structures, and execution of Python code within Blender.",
      "stars": 26,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T21:36:13Z",
      "readme_content": "\n# Bonsai-mcp - Model Context Protocol Integration for IFC through IfcOpenShell and Blender\n\nBonsai-mcp is a fork of [BlenderMCP](https://github.com/ahujasid/blender-mcp) that extends the original functionality with dedicated support for IFC (Industry Foundation Classes) models through Bonsai. This integration is a platform to let LLMs read and modify IFC files.\n\n## Features\n\n- **IFC-specific functionality**: Query IFC models, analyze spatial structures, examine building elements and extract quantities\n\n- **Eleven IFC tools included**: Inspect project info, list entities, examine properties, explore spatial structure, analyze relationships and more\n\n- **Sequential Thinking**: Includes the sequential thinking tool from [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking) for structured problem solving\n\n- **Execute Code tool from the original BlenderMCP implementation**: Create and modify objects, apply materials, and execute Python code in Blender\n\n## Components\n\nThe system consists of two main components:\n\n1. **Blender Addon (`addon.py`)**: A Blender addon that creates a socket server within Blender to receive and execute commands, including IFC-specific operations\n\n2. **MCP Server (`tools.py`)**: A Python server that implements the Model Context Protocol and connects to the Blender addon\n\n## Installation - Through MCP Client Settings\n\n### Prerequisites\n\n- Blender 4.0 or newer\n\n- Python 3.12 or newer\n\n- uv package manager\n\n- Bonsai BIM addon for Blender (for IFC functionality)\n\n**Installing uv:**\n\n**Mac:**\n\n```bash\n\nbrew  install  uv\n\n```\n\n**Windows:**\n\n```bash\n\npowershell  -c  \"irm https://astral.sh/uv/install.ps1 | iex\"\n\nset  Path=C:\\Users\\[username]\\.local\\bin;%Path%\n\n```\n\nFor other platforms, see the [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Clone the repository\n\n```bash\n\ngit  clone  https://github.com/JotaDeRodriguez/Bonsai_mcp\n\n```\n\n### Claude for Desktop Integration\n\nEdit your `claude_desktop_config.json` file (Claude > Settings > Developer > Edit Config) to include:\n\n```json\n{\n    \"mcpServers\": {\n        \"Bonsai-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n              \"--directory\",\n              \"\\\\your\\\\path\\\\to\\\\Bonsai_mcp\",\n              \"run\",\n              \"tools.py\"\n          ]\n        }\n    }\n}\n\n```\n\n## Installation via Docker\n\nThe repository comes with a Dockerfile that makes deployment simple and consistent across different environments.\n\n## Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/JotaDeRodriguez/Bonsai_mcp\ncd Bonsai_mcp\n\n# Build the Docker image\ndocker build -t bonsai_mcp .\n\n# Run the container\ndocker run -p 8000:8000 --name bonsai_mcp bonsai_mcp\n```\n\nOnce running, the container will expose the MCP tools as REST/OpenAPI APIs at `http://localhost:8000`.\n\n- To verify youtr installation, open your browser and navigate to\n- `http://localhost:8000/docs`\n- You'll see the Swagger UI with all available endpoints\n- Test an endpoint by clicking on it, then click \"Try it out\" and \"Execute\"\n\n### Connecting to Open WebUI or Other API Clients\n\nTo connect this API to Open WebUI:\n\n1. In Open WebUI, go to Settings > Manage Tool Servers\n2. Add a new connection with:\n\n- URL: `http://localhost:8000`\n- Path to OpenAPI spec: `/openapi.json`\n- Authentication: None (unless configured otherwise)\n\n### Environment Variables\n\nThe Docker container accepts several environment variables to customize its behavior:\n\n```bash\n# Example with custom settings\ndocker run -p 8000:8000 \\\n  -e BLENDER_HOST=host.docker.internal \\\n  -e BLENDER_PORT=9876 \\\n  -e MCP_HOST=0.0.0.0 \\\n  -e MCP_PORT=8000 \\\n  --name bonsai_mcp bonsai_mcp\n```\n\n## Installing the Blender Addon\n\n1. Download the `addon.py` file from this repo\n\n2. Open Blender\n\n3. Go to Edit > Preferences > Add-ons\n\n4. Click \"Install...\" and select the `addon.py` file\n\n5. Enable the addon by checking the box next to \"Interface: Blender MCP - IFC\"\n\n## Usage\n\n### Starting the Connection\n\n1. In Blender, go to the 3D View sidebar (press N if not visible)\n\n2. Find the \"Blender MCP - IFC\" tab\n\n3. Click \"Connect to Claude\"\n\n4. Make sure the MCP server is running\n\n### Using with Claude\n\nOnce connected, you'll see a hammer icon in Claude's interface with tools for the Blender MCP IFC integration.\n\n## IFC Tools\n\nThis repo includes multiple IFC-specific tools that enable comprehensive querying and manipulation of IFC models:\n\n**get_ifc_project_info**: Retrieves basic information about the IFC project, including name, description, and counts of different entity types. Example: \"What is the basic information about this IFC project?\"\n\n**list_ifc_entities**: Lists IFC entities of a specific type (walls, doors, spaces, etc.) with options to limit results and filter by selection. Example: \"List all the walls in this IFC model\" or \"Show me the windows in this building\"\n\n**get_ifc_properties**: Retrieves all properties of a specific IFC entity by its GlobalId or from currently selected objects. Example: \"What are the properties of this wall with ID 1Dvrgv7Tf5IfTEapMkwDQY?\"\n\n**get_ifc_spatial_structure**: Gets the spatial hierarchy of the IFC model (site, building, storeys, spaces). Example: \"Show me the spatial structure of this building\"\n\n**get_ifc_relationships**: Retrieves all relationships for a specific IFC entity. Example: \"What are the relationships of the entrance door?\"\n\n**get_selected_ifc_entities**: Gets information about IFC entities corresponding to objects currently selected in the Blender UI. Example: \"Tell me about the elements I've selected in Blender\"\n\n**get_user_view**: Captures the current Blender viewport as an image, allowing visualization of the model from the user's perspective. Example: \"Show me what the user is currently seeing in Blender\"\n\n**export_ifc_data**: Exports IFC data to a structured JSON or CSV file, with options to filter by entity type or building level. Example: \"Export all wall data to a CSV file\"\n\n**place_ifc_object**: Creates and positions an IFC element in the model at specified coordinates with optional rotation. Example: \"Place a door at coordinates X:10, Y:5, Z:0 with 90 degrees rotation\"\n\n**get_ifc_quantities**: Calculate and get quantities (m2, m3, etc.) for IFC elements, with options to filter by entity type or selected ones. Example: \"Give me the area of all the walls in the building using the tool get_ifc_quantities\"\n\n**export_drawing_png**: Exports 2D and 3D drawings as high-resolution PNG images with customizable resolution and view parameters. Creates orthographic plan views from above at specified height offsets. Example: \"Generate a floor plan PNG for the ground floor at 1920x1080 resolution\"\n\n**get_ifc_georeferencing_info**: Retrieves comprehensive georeferencing information from IFC files including coordinate reference systems (CRS), map conversions, world coordinate systems, true north direction, and site geographic coordinates. Example: \"What georeferencing information is available in this IFC model?\"\n\n**georeference_ifc_model**: Creates or updates georeferencing information in IFC models, allowing you to set coordinate reference systems using EPSG codes or custom CRS definitions, establish map conversions with eastings/northings coordinates, and configure site geographic positioning. Example: \"Georeference this IFC model using EPSG:4326 with coordinates at latitude 40.7589, longitude -73.9851\"\n\n## MCP Resources\n\nThis integration provides access to structured documentation through MCP resources:\n\n**file://table_of_contents.md**: Contains the complete technical report structure template for generating comprehensive building reports. This resource provides a standardized table of contents that can be used as a reference when creating technical documentation from IFC models.\n\n## MCP Prompts\n\nThe server includes specialized MCP Prompts for automated report generation:\n\n**Technical_building_report**: Generates comprehensive technical building reports based on IFC models loaded in Blender. This prompt provides a structured workflow for creating professional architectural documentation in multiple languages (English, Spanish, French, German, Italian, Portuguese). The prompt guides the analysis through systematic data extraction from the IFC model, including spatial structure, quantities, materials, and building systems, culminating in a complete technical report with drawings and 3D visualizations.\n\n## Execute Blender Code\n\nLegacy feature from the original MCP implementation. Allows Claude to execute arbitrary Python code in Blender. Use with caution.\n\n## Sequential Thinking Tool\n\nThis integration includes the Sequential Thinking tool for structured problem-solving and analysis. It facilitates a step-by-step thinking process that can branch, revise, and adapt as understanding deepens - perfect for complex IFC model analysis or planning tasks.\n\nExample: \"Use sequential thinking to analyze this building's energy efficiency based on the IFC model\"\n\n## Example Commands\n\nHere are some examples of what you can ask Claude to do with IFC models:\n\n- \"Analyze this IFC model and tell me how many walls, doors and windows it has\"\n\n- \"Show me the spatial structure of this building model\"\n\n- \"List all spaces in this IFC model and their properties\"\n\n- \"Identify all structural elements in this building\"\n\n- \"What are the relationships between this wall and other elements?\"\n\n- \"Generate a report of the measurements from the IFC model opened in Blender\"\n\n- \"Use sequential thinking to create a maintenance plan for this building based on the IFC model\"\n\n## Troubleshooting\n\n- **Connection issues**: Make sure the Blender addon server is running, and the MCP server is configured in Claude\n- **IFC model not loading**: Verify that you have the Bonsai BIM addon installed and that an IFC file is loaded\n- **Timeout errors**: Try simplifying your requests or breaking them into smaller steps\n\n**Docker:**\n\n- **\"Connection refused\" errors**: Make sure Blender is running and the addon is enabled with the server started\n- **CORS issues**: The API has CORS enabled by default for all origins. If you encounter issues, check your client's CORS settings\n- **Performance concerns**: For large IFC models, the API responses might be slower. Consider adjusting timeouts in your client\n\n## Technical Details\n\nThe IFC integration uses the Bonsai BIM module to access ifcopenshell functionality within Blender. The communication follows the same JSON-based protocol over TCP sockets as the original BlenderMCP.\n\n## Limitations & Security Considerations\n\n- The `execute_blender_code` tool from the original project is still available, allowing running arbitrary Python code in Blender. Use with caution and always save your work.\n\n- Complex IFC models may require breaking down operations into smaller steps.\n\n- IFC query performance depends on model size and complexity.\n\n- Get User View tool returns a base64 encoded image. Please ensure the client supports it.\n\n## Contributions\n\nThis MIT licensed repo is open to be forked, modified and used in any way. I'm open to ideas and collaborations, so don't hesitate to get in contact with me for contributions.\n\n## Credits\n\n- Original BlenderMCP by [Siddharth Ahuja](https://github.com/ahujasid/blender-mcp)\n\n- Sequential Thinking tool from [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking)\n\n- IFC integration built upon the Bonsai BIM addon for Blender\n\n## TO DO\n\nIntegration and testing with more MCP Clients\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "blender",
        "tools",
        "ifc models",
        "research tools",
        "code blender"
      ],
      "category": "scientific-research-tools"
    },
    "KangDeokWon-droid--Deep-Learning-": {
      "owner": "KangDeokWon-droid",
      "name": "Deep-Learning-",
      "url": "https://github.com/KangDeokWon-droid/Deep-Learning-",
      "imageUrl": "/freedevtools/mcp/pfp/KangDeokWon-droid.webp",
      "description": "Provides tools and resources for training and experimenting with artificial intelligence models. Facilitates model development and evaluation with integrated datasets and utilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2024-09-21T08:46:24Z",
      "readme_content": "# Deep-Learning\nÏù∏Í≥µÏßÄÎä• ÌïôÏäµ\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "droid",
        "datasets",
        "deep",
        "droid deep",
        "kangdeokwon droid",
        "deep learning"
      ],
      "category": "scientific-research-tools"
    },
    "MJBeauty--open_model_zoo": {
      "owner": "MJBeauty",
      "name": "open_model_zoo",
      "url": "https://github.com/MJBeauty/open_model_zoo",
      "imageUrl": "/freedevtools/mcp/pfp/MJBeauty.webp",
      "description": "A collection of optimized deep learning models and demos designed to facilitate the development of high-performance deep learning inference applications using pre-trained models. Offers integration and validation tools with automation scripts for rapid deployment.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2020-07-16T05:29:44Z",
      "readme_content": "# [OpenVINO‚Ñ¢ Toolkit](https://01.org/openvinotoolkit) - Open Model Zoo repository\n[![Stable release](https://img.shields.io/badge/version-2020.2-green.svg)](https://github.com/opencv/open_model_zoo/releases/tag/2020.2)\n[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/open_model_zoo/community)\n[![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)\n\nThis repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.\n\n## Repository Components:\n* [Pre-Trained Models](models/intel/index.md)\n* [Public Models Description](models/public/index.md)\n* [Model Downloader](tools/downloader/README.md) and other automation tools\n* [Demos](demos/README.md) that demonstrate models usage with Deep Learning Deployment Toolkit\n* [Accuracy Checker](tools/accuracy_checker/README.md) tool for models accuracy validation\n\n## License\nOpen Model Zoo is licensed under [Apache License Version 2.0](LICENSE).\n\n## Documentation\n* [OpenVINO‚Ñ¢ Release Notes](https://software.intel.com/en-us/articles/OpenVINO-RelNotes)\n* [Pre-Trained Models](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models)\n* [Demos and samples](https://software.intel.com/en-us/articles/OpenVINO-IE-Samples)\n\n## Other usage examples\n* [Open Visual Cloud](https://01.org/openvisualcloud)\n  * [Tutorial: Build and Run the AD Insertion Sample on public cloud or local machine](https://01.org/openvisualcloud/documents/tutorial-build-and-run-ad-insertion-sample-public-cloud-or-local-machine)\n  * [GitHub Repo for Ad Insertion Sample](https://github.com/OpenVisualCloud/Ad-Insertion-Sample)\n* [OpenVINO for Smart City](https://github.com/incluit/OpenVino-For-SmartCity)\n* [OpenVINO Driver Behavior](https://github.com/incluit/OpenVino-Driver-Behaviour)\n\n## How to Contribute\nWe welcome community contributions to the Open Model Zoo repository. If you have an idea how to improve the product, please share it with us doing the following steps:\n* Make sure you can build the product and run all the demos with your patch.\n* In case of a larger feature, provide a relevant demo.\n* Submit a pull request at https://github.com/opencv/open_model_zoo/pulls\n\nYou can find additional information about model contribution [here](CONTRIBUTING.md).\n\nWe will review your contribution and, if any additional fixes or modifications are needed, may give you feedback to guide you. When accepted, your pull request will be merged into the GitHub* repositories.\n\nOpen Model Zoo is licensed under Apache License, Version 2.0. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.\n\n## Support\nPlease report questions, issues and suggestions using:\n* [\\#open_model_zoo](https://stackoverflow.com/search?q=%23open_model_zoo) tag on StackOverflow*\n* [GitHub* Issues](https://github.com/opencv/open_model_zoo/issues)\n* [Forum](https://software.intel.com/en-us/forums/intel-distribution-of-openvino-toolkit)\n* [Gitter](https://gitter.im/open_model_zoo/community)\n\n---\n\\* Other names and brands may be claimed as the property of others.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "open_model_zoo",
        "models",
        "deep",
        "mjbeauty open_model_zoo",
        "deep learning",
        "optimized deep"
      ],
      "category": "scientific-research-tools"
    },
    "MushroomFleet--DeepLucid3D-MCP": {
      "owner": "MushroomFleet",
      "name": "DeepLucid3D-MCP",
      "url": "https://github.com/MushroomFleet/DeepLucid3D-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MushroomFleet.webp",
      "description": "Provides advanced cognitive analysis and problem-solving capabilities using structured cognitive tools, enabling users to assess cognitive states, generate novel perspectives, and decompose complex problems for effective solutions.",
      "stars": 12,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-28T12:04:44Z",
      "readme_content": "# DeepLucid3D UCPF Server\n\nA Model Context Protocol (MCP) server implementing the Unified Cognitive Processing Framework (UCPF) for advanced cognitive analysis, creative problem-solving, and structured thinking.\n\n## What is it?\n\nThe DeepLucid3D UCPF Server is an implementation of the Unified Cognitive Processing Framework as an MCP server. It combines recursive self-awareness with dimensional knowledge categorization to provide a powerful approach to problem-solving and creative thinking.\n\nThis server extends AI capabilities by providing structured cognitive tools that help:\n- Assess cognitive states\n- Map knowledge dimensions\n- Apply recursive self-questioning\n- Generate creative perspectives\n- Decompose and reintegrate complex problems\n\n## What it does\n\nThe UCPF Server enables advanced cognitive processing through several key features:\n\n### Core Capabilities\n\n1. **Cognitive State Assessment**: Identifies current cognitive states (Dark Inertia, Passion, or Approaching Lucidity) to improve self-awareness during problem-solving.\n\n2. **Knowledge Dimension Mapping**: Maps knowledge across three dimensions:\n   - Awareness (Known vs. Unknown)\n   - Content (Knowns vs. Unknowns)\n   - Accessibility (Knowable vs. Unknowable)\n\n3. **Recursive Self-Questioning**: Challenges initial assumptions and identifies potential cognitive biases.\n\n4. **Creative Perspective Generation**: Produces novel viewpoints and metaphorical thinking to inspire new solutions.\n\n5. **Problem Decomposition**: Breaks complex problems into manageable components and reintegrates them with awareness of the whole system.\n\n6. **Optional State Management**: Maintains context between sessions for ongoing analysis.\n\n## Setup and Installation\n\n### Prerequisites\n\n- Node.js (v14 or higher)\n- npm (v6 or higher)\n- An environment compatible with the Model Context Protocol\n\n### Installation Steps\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/yourusername/DeepLucid3D-UCPF-Server.git\n   cd DeepLucid3D-UCPF-Server\n   ```\n\n2. **Install dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Build the project**\n   ```bash\n   npm run build\n   ```\n\n4. **Configure MCP settings**\n\n   Add the server to your MCP settings file. For Claude/Cline, this is typically located at:\n   - For Claude Desktop: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n   - For VSCode Cline: `~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json` (Linux)\n\n   Add the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n      \"ucpf\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/DeepLucid3D-MCP/build/index.js\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n5. **Restart your MCP-compatible application** (Claude, VSCode with Cline, etc.)\n\n## How to Use\n\nThe server exposes three main tools and several resources that can be accessed through MCP:\n\n### Tools\n\n1. **analyze_problem**: Process a problem statement through the full UCPF framework\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>analyze_problem</tool_name>\n   <arguments>\n   {\n     \"problem\": \"Your problem statement here\",\n     \"session_id\": \"optional-session-id\",\n     \"enable_state\": false,\n     \"detailed\": false\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n2. **creative_exploration**: Generate novel perspectives and connections for a topic\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>creative_exploration</tool_name>\n   <arguments>\n   {\n     \"topic\": \"Your topic here\",\n     \"constraints\": [\"Optional constraint 1\", \"Optional constraint 2\"],\n     \"perspective_count\": 3,\n     \"include_metaphors\": true\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n3. **manage_state**: Control state management for UCPF processing\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>manage_state</tool_name>\n   <arguments>\n   {\n     \"action\": \"enable\", // or \"disable\", \"reset\", \"status\"\n     \"session_id\": \"optional-specific-session-id\"\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n### Resources\n\nAccess framework documentation:\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://framework/overview</uri>\n</access_mcp_resource>\n```\n\nAvailable resources:\n- `ucpf://framework/overview`: Overview of the UCPF framework\n- `ucpf://framework/cognitive-states`: Guide to cognitive states\n- `ucpf://framework/knowledge-dimensions`: Reference for knowledge dimensions\n- `ucpf://session/{sessionId}/analysis`: Analysis results for a specific session\n\n## Example Use Case: Using UCPF to Solve a Riddle\n\nLet's solve the classic riddle: \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\"\n\n### Step 1: Enable state management for session persistence\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>manage_state</tool_name>\n<arguments>\n{\n  \"action\": \"enable\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Step 2: Analyze the riddle using the UCPF framework\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>analyze_problem</tool_name>\n<arguments>\n{\n  \"problem\": \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\",\n  \"session_id\": \"sphinx-riddle\",\n  \"enable_state\": true\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe analysis will provide:\n- Cognitive state assessment (likely identifying potential metaphorical thinking)\n- Knowledge mapping of what we know and don't know\n- Recursive questions to challenge initial assumptions (e.g., \"Are we assuming literal legs?\")\n- Structured perspectives on different interpretations\n\n### Step 3: Explore creative perspectives to find the solution\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>creative_exploration</tool_name>\n<arguments>\n{\n  \"topic\": \"Walking with different numbers of legs at different times of day\",\n  \"constraints\": [\"morning\", \"afternoon\", \"evening\", \"four\", \"two\", \"three\"],\n  \"include_metaphors\": true,\n  \"session_id\": \"sphinx-riddle\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThis exploration might reveal:\n- The metaphorical interpretation of \"legs\" as support structures\n- The metaphorical interpretation of times of day as stages of life\n- Leading to the classic answer: a human, who crawls on four limbs as a baby, walks on two legs as an adult, and uses a cane (third \"leg\") in old age\n\n### Step 4: Review the session analysis\n\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://session/sphinx-riddle/analysis</uri>\n</access_mcp_resource>\n```\n\nThis provides the complete analysis journey, showing how the framework led to the solution through structured cognitive processing.\n\n## Acknowledgments\n\nThis project stands on the shoulders of giants:\n\n- The **Model Context Protocol (MCP)** team for creating the foundational protocol that enables AI systems to access external tools and resources\n- The **Anthropic Claude** team for their work on advanced AI systems capable of utilizing MCP\n- Contributors to the **Unified Cognitive Processing Framework** concepts that power the cognitive analysis methodology\n- The open-source community whose libraries and tools make projects like this possible\n\n## License\n\nMIT License\n\n## Project Structure\n\n```\nDeepLucid3D-UCPF-Server/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ engine/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ucpf-core.ts       # Core UCPF processing logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ creative-patterns.ts  # Creative thinking utilities\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state-manager.ts   # Session state management\n‚îÇ   ‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze-problem.ts # Problem analysis tool\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ creative-exploration.ts  # Creative exploration tool\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts               # Main server implementation\n‚îú‚îÄ‚îÄ build/                     # Compiled JavaScript files\n‚îú‚îÄ‚îÄ package.json               # Project dependencies and scripts\n‚îî‚îÄ‚îÄ README.md                  # This documentation\n```\n\n---\n\n¬© 2025 DeepLucid3D UCPF Server\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cognitive",
        "deeplucid3d",
        "tools",
        "cognitive tools",
        "structured cognitive",
        "mushroomfleet deeplucid3d"
      ],
      "category": "scientific-research-tools"
    },
    "PRIDE-Archive--mcp_pride_archive_search": {
      "owner": "PRIDE-Archive",
      "name": "mcp_pride_archive_search",
      "url": "https://github.com/PRIDE-Archive/mcp_pride_archive_search",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Search and explore proteomics datasets from the PRIDE Archive using structured queries. It facilitates the integration of AI models with biomedical research data for efficient data retrieval and analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteomics",
        "mcp_pride_archive_search",
        "datasets",
        "proteomics datasets",
        "explore proteomics",
        "mcp_pride_archive_search search"
      ],
      "category": "scientific-research-tools"
    },
    "ProgramComputer--NASA-MCP-server": {
      "owner": "ProgramComputer",
      "name": "NASA-MCP-server",
      "url": "https://github.com/ProgramComputer/NASA-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/ProgramComputer.webp",
      "description": "Connects AI models to over 20 NASA APIs, providing a standardized interface for data retrieval and interaction. Supports automatic parameter validation and error handling for optimized data consumption.",
      "stars": 72,
      "forks": 15,
      "license": "ISC License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T04:02:10Z",
      "readme_content": "[![NPM Version](https://img.shields.io/npm/v/%40programcomputer%2Fnasa-mcp-server?link=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40programcomputer%2Fnasa-mcp-server)](https://www.npmjs.com/package/@programcomputer/nasa-mcp-server)\n\n# NASA MCP Server\n\nA Model Context Protocol (MCP) server for NASA APIs, providing a standardized interface for AI models to interact with NASA's vast array of data sources. This server implements the official Model Context Protocol specification.\n\nBig thanks to the MCP community for their support and guidance!\n\n## Features\n\n* Access to 20+ NASA data sources through a single, consistent interface\n* Standardized data formats optimized for AI consumption\n* Automatic parameter validation and error handling\n* Rate limit management for NASA API keys\n* Comprehensive documentation and examples\n* Support for various NASA imagery formats\n* Data conversion and formatting for LLM compatibility\n* Cross-platform support (Windows, macOS, Linux)\n\n## Disclaimer\n\n**This project is not affiliated with, endorsed by, or related to NASA (National Aeronautics and Space Administration) or any of its subsidiaries or its affiliates.** It is an independent implementation that accesses NASA's publicly available APIs. All NASA data used is publicly available and subject to NASA's data usage policies.\n\n## Installation\n\n### Running with npx\n\n```bash\nenv NASA_API_KEY=YOUR_API_KEY npx -y @programcomputer/nasa-mcp-server@latest\n```\n\nYou can also pass the API key as a command line argument:\n\n```bash\nnpx -y @programcomputer/nasa-mcp-server@latest --nasa-api-key=YOUR_API_KEY\n```\n\n### Using SuperGateway for Server-Sent Events (SSE)\n\nYou can use [SuperGateway](https://github.com/supercorp-ai/supergateway) for Server-Sent Events (SSE).\n\n**The developers of NASA-MCP-server DO NOT ENDORSE the SuperGateway repository. This information is provided for those who wish to implement SSE functionality at their own discretion.**\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\ncd NASA-MCP-server\nnpm install\n\n# Run with your API key\nNASA_API_KEY=YOUR_API_KEY npm start\n```\n\n### Running on Cursor\n\nConfiguring Cursor üñ•Ô∏è Note: Requires Cursor version 0.45.6+\n\nTo configure NASA MCP Server in Cursor:\n\nCreate or edit an `mcp.json` file in your Cursor configuration directory with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"nasa-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@programcomputer/nasa-mcp-server@latest\"],\n      \"env\": {\n        \"NASA_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key` with your NASA API key from https://api.nasa.gov/.\n\nAfter adding the configuration, restart Cursor to see the new NASA tools. The Composer Agent will automatically use NASA MCP when appropriate for space-related queries.\n\n## Environment Variables\n\nThe server can be configured with the following environment variables:\n\n| Variable | Description |\n|----------|-------------|\n| `NASA_API_KEY` | Your NASA API key (get at api.nasa.gov) |\n\n## Included NASA APIs\n\nThis MCP server integrates the following NASA APIs:\n\n1. **NASA Open API** (api.nasa.gov):\n   - APOD (Astronomy Picture of the Day)\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - DONKI (Space Weather Database Of Notifications, Knowledge, Information)\n   - Insight (Mars Weather Service)\n   - Mars Rover Photos\n   - NEO (Near Earth Object Web Service)\n   - EONET (Earth Observatory Natural Event Tracker)\n   - TLE (Two-Line Element)\n   - NASA Image and Video Library\n   - Exoplanet Archive\n   - NASA Sounds API (Beta)\n   - POWER (Prediction Of Worldwide Energy Resources)\n\n2. **JPL Solar System Dynamics API** (ssd-api.jpl.nasa.gov):\n   - SBDB (Small-Body DataBase)\n   - SBDB Close-Approach Data\n   - Fireball Data\n   - Scout API\n\n3. **Earth Data APIs**:\n   - GIBS (Global Imagery Browse Services)\n   - CMR (Common Metadata Repository) - Enhanced with advanced search capabilities\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - FIRMS (Fire Information for Resource Management System)\n\n## API Methods\n\nEach NASA API is exposed through standardized MCP methods:\n\n### APOD (Astronomy Picture of the Day)\n\n```json\n{\n  \"method\": \"nasa/apod\",\n  \"params\": {\n    \"date\": \"2023-01-01\", // Optional: YYYY-MM-DD format\n    \"count\": 5, // Optional: Return a specified number of random images\n    \"thumbs\": true // Optional: Return URL of video thumbnail\n  }\n}\n```\n\n### Mars Rover Photos\n\n```json\n{\n  \"method\": \"nasa/mars-rover\",\n  \"params\": {\n    \"rover\": \"curiosity\", // Required: \"curiosity\", \"opportunity\", or \"spirit\"\n    \"sol\": 1000, // Either sol or earth_date is required\n    \"earth_date\": \"2023-01-01\", // YYYY-MM-DD format\n    \"camera\": \"FHAZ\" // Optional: Filter by camera type\n  }\n}\n```\n\n### Near Earth Objects\n\n```json\n{\n  \"method\": \"nasa/neo\",\n  \"params\": {\n    \"start_date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"end_date\": \"2023-01-07\" // Required: YYYY-MM-DD format (max 7 days from start)\n  }\n}\n```\n\n### GIBS (Global Imagery Browse Services)\n\n```json\n{\n  \"method\": \"nasa/gibs\",\n  \"params\": {\n    \"layer\": \"MODIS_Terra_CorrectedReflectance_TrueColor\", // Required: Layer ID\n    \"date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"format\": \"png\" // Optional: \"png\" or \"jpg\"\n  }\n}\n```\n\n### POWER (Prediction Of Worldwide Energy Resources)\n\n```json\n{\n  \"method\": \"nasa/power\",\n  \"params\": {\n    \"parameters\": \"T2M,PRECTOTCORR,WS10M\", // Required: Comma-separated list\n    \"community\": \"re\", // Required: Community identifier\n    \"latitude\": 40.7128, // Required: Latitude\n    \"longitude\": -74.0060, // Required: Longitude\n    \"start\": \"20220101\", // Required: Start date (YYYYMMDD)\n    \"end\": \"20220107\" // Required: End date (YYYYMMDD)\n  }\n}\n```\n\nFor complete documentation of all available methods and parameters, see the API reference in the `/docs` directory.\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n* Operation status and progress\n* Performance metrics\n* Rate limit tracking\n* Error conditions\n* Request validation\n\nExample log messages:\n\n```\n[INFO] NASA MCP Server initialized successfully\n[INFO] Processing APOD request for date: 2023-01-01\n[INFO] Fetching Mars Rover data for Curiosity, sol 1000\n[WARNING] Rate limit threshold reached (80%)\n[ERROR] Invalid parameter: 'date' must be in YYYY-MM-DD format\n```\n\n## Security Considerations\n\nThis MCP server implements security best practices following the Model Context Protocol specifications:\n\n* Input validation and sanitization using Zod schemas\n* No execution of arbitrary code\n* Protection against command injection\n* Proper error handling to prevent information leakage\n* Rate limiting and timeout controls for API requests\n* No persistent state that could be exploited across sessions\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\nnpm install\n\n# Copy the example environment file and update with your API keys\ncp .env.example .env\n\n# Build the TypeScript code\nnpm run build\n\n# Start the development server\nnpm run dev\n\n# Run tests\nnpm test\n```\n\n## Testing with MCP Inspector\n\nThe NASA MCP Server includes a script to help you test the APIs using the MCP Inspector:\n\n```bash\n# Run the provided test script\n./scripts/test-with-inspector.sh\n```\n\nThis will:\n1. Build the project to ensure the latest changes are included\n2. Start the MCP Inspector with the NASA MCP server running\n3. Allow you to interactively test all the NASA APIs\n\n### Example Test Requests\n\nThe repository includes example test requests for each API that you can copy and paste into the MCP Inspector:\n\n```bash\n# View the example test requests\ncat docs/inspector-test-examples.md\n```\n\nFor detailed examples, see the [Inspector Test Examples](docs/inspector-test-examples.md) document.\n\n## MCP Client Usage\n\nThis server follows the official Model Context Protocol. Here's an example of how to use it with the MCP SDK:\n\n```typescript\nimport { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\nimport { HttpClientTransport } from \"@modelcontextprotocol/sdk/client/http.js\";\n\nconst transport = new HttpClientTransport({\n  url: \"http://localhost:3000\",\n});\n\nconst client = new Client({\n  name: \"mcp-client\",\n  version: \"1.0.0\",\n});\n\nawait client.connect(transport);\n\n// Example: Get today's Astronomy Picture of the Day\nconst apodResult = await client.request({\n  method: \"nasa/apod\", \n  params: {}\n});\n\n// Example: Get Mars Rover photos\nconst marsRoverResult = await client.request({\n  method: \"nasa/mars-rover\",\n  params: { rover: \"curiosity\", sol: 1000 }\n});\n\n// Example: Search for Near Earth Objects\nconst neoResults = await client.request({\n  method: \"nasa/neo\",\n  params: {\n    start_date: '2023-01-01',\n    end_date: '2023-01-07'\n  }\n});\n\n// Example: Get satellite imagery from GIBS\nconst satelliteImage = await client.request({\n  method: \"nasa/gibs\",\n  params: {\n    layer: 'MODIS_Terra_CorrectedReflectance_TrueColor',\n    date: '2023-01-01'\n  }\n});\n\n// Example: Use the new POWER API\nconst powerData = await client.request({\n  method: \"nasa/power\",\n  params: {\n    parameters: \"T2M,PRECTOTCORR,WS10M\",\n    community: \"re\",\n    latitude: 40.7128,\n    longitude: -74.0060,\n    start: \"20220101\",\n    end: \"20220107\"\n  }\n});\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nISC License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nasa",
        "apis",
        "ai",
        "nasa apis",
        "programcomputer nasa",
        "nasa mcp"
      ],
      "category": "scientific-research-tools"
    },
    "SeanMcLoughlin--mcp-vcd": {
      "owner": "SeanMcLoughlin",
      "name": "mcp-vcd",
      "url": "https://github.com/SeanMcLoughlin/mcp-vcd",
      "imageUrl": "/freedevtools/mcp/pfp/SeanMcLoughlin.webp",
      "description": "Handles value change dump (VCD) waveforms for AI models, providing specific signal changes within defined timestamp ranges to optimize context management for large waveform files.",
      "stars": 5,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-13T03:09:17Z",
      "readme_content": "# mcp-vcd\r\n\r\nA model context protocol (MCP) server for value change dump (VCD) waveforms.\r\n\r\nhttps://github.com/user-attachments/assets/9d1a6a64-de22-4b5a-a597-685c663c9c79\r\n\r\n<a href=\"https://glama.ai/mcp/servers/kdvs90ijbl\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kdvs90ijbl/badge\" alt=\"mcp-vcd MCP server\" /></a>\r\n\r\n# Tools\r\n\r\n- `get-signal`: Provide all changes of the specified signal name to the model's context. This is useful for large waveform files with many signals where you cannot fit the entire VCD file into the model's context window.\r\n  - Can optionally specify a start and end timestamp range for the model to look at.\r\n\r\n# Installation\r\n\r\n`uv pip install --system mcp-vcd`\r\n\r\nAnd add the following to your `claude_desktop_config.json`:\r\n\r\n```json\r\n\"mcpServers\": {\r\n  \"mcp-vcd\": {\r\n    \"command\": \"uv\",\r\n    \"args\": [\r\n      \"run\",\r\n      \"mcp-vcd\"\r\n    ]\r\n  }\r\n}\r\n```\r\nSee [Anthropic's MCP documentation](https://modelcontextprotocol.io/quickstart/user) for more info.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vcd",
        "mcp",
        "waveforms",
        "mcp vcd",
        "vcd waveforms",
        "dump vcd"
      ],
      "category": "scientific-research-tools"
    },
    "StarGazer1995--mcp-stargazing": {
      "owner": "StarGazer1995",
      "name": "mcp-stargazing",
      "url": "https://github.com/StarGazer1995/mcp-stargazing",
      "imageUrl": "/freedevtools/mcp/pfp/StarGazer1995.webp",
      "description": "Calculate altitude, rise, and set times of celestial objects based on geographic location, while also analyzing light pollution to improve stargazing conditions. Supports a variety of celestial objects including the Sun, Moon, planets, and deep-space entities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-18T09:45:10Z",
      "readme_content": "# mcp-stargazing\n\nCalculate the altitude, rise, and set times of celestial objects (Sun, Moon, planets, stars, and deep-space objects) for any location on Earth, with optional light pollution analysis.\n\n## Features\n- **Altitude/Azimuth Calculation**: Get elevation and compass direction for any celestial object.\n- **Rise/Set Times**: Determine when objects appear/disappear above the horizon.\n- **Light Pollution Analysis**: Load and analyze light pollution maps (GeoTIFF format).\n- **Supports**:\n  - Solar system objects (Sun, Moon, planets)\n  - Stars (e.g., \"sirius\")\n  - Deep-space objects (e.g., \"andromeda\", \"orion_nebula\")\n- **Time Zone Aware**: Works with local or UTC times.\n\n## Installation\n```bash\npip install astropy pytz numpy astroquery rasterio geopy\n```\n\n## Usage\n\n### Calculate Altitude/Azimuth\n```python src/main.py\nfrom src.celestial import celestial_pos\nfrom astropy.coordinates import EarthLocation\nimport pytz\nfrom datetime import datetime\n\n# Observer location (New York)\nlocation = EarthLocation(lat=40.7128, lon=-74.0060)\n\n# Time (local timezone-aware)\nlocal_time = pytz.timezone(\"America/New_York\").localize(datetime(2023, 10, 1, 12, 0))\naltitude, azimuth = celestial_pos(\"sun\", location, local_time)\nprint(f\"Sun Position: Altitude={altitude:.1f}¬∞, Azimuth={azimuth:.1f}¬∞\")\n```\n\n### Calculate Rise/Set Times\n```python src/main.py\nfrom src.celestial import celestial_rise_set\n\nrise, set_ = celestial_rise_set(\"andromeda\", location, local_time.date())\nprint(f\"Andromeda: Rise={rise.iso}, Set={set_.iso}\")\n```\n\n### Load Light Pollution Map\n```python src/light_pollution.py\nfrom src.light_pollution import load_map\n\n# Load a GeoTIFF light pollution map\nvriis_data, bounds, crs, transform = load_map(\"path/to/map.tif\")\nprint(f\"Map Bounds: {bounds}\")\n```\n\n## API Reference\n\n### `celestial_pos(celestial_object, observer_location, time)` (`src/celestial.py`)\n- **Inputs**:\n  - `celestial_object`: Name (e.g., `\"sun\"`, `\"andromeda\"`).\n  - `observer_location`: `EarthLocation` object.\n  - `time`: `datetime` (timezone-aware) or Astropy `Time`.\n- **Returns**: `(altitude_degrees, azimuth_degrees)`.\n\n### `celestial_rise_set(celestial_object, observer_location, date, horizon=0.0)` (`src/celestial.py`)\n- **Inputs**: \n  - `date`: Timezone-aware `datetime`.\n  - `horizon`: Horizon elevation (default: 0¬∞).\n- **Returns**: `(rise_time, set_time)` as UTC `Time` objects.\n\n### `load_map(map_path)` (`src/light_pollution.py`)\n- **Inputs**:\n  - `map_path`: Path to GeoTIFF file.\n- **Returns**: Tuple `(vriis_data, bounds, crs, transform)` for light pollution analysis.\n\n## Testing\nRun tests with:\n```bash\npytest tests/\n```\n\n### Key Test Cases (`tests/test_celestial.py`)\n```python tests/test_celestial.py\ndef test_calculate_altitude_deepspace():\n    \"\"\"Test deep-space object resolution.\"\"\"\n    altitude, _ = celestial_pos(\"andromeda\", NYC, Time.now())\n    assert -90 <= altitude <= 90\n\ndef test_calculate_rise_set_sun():\n    \"\"\"Validate Sun rise/set times.\"\"\"\n    rise, set_ = celestial_rise_set(\"sun\", NYC, datetime(2023, 10, 1))\n    assert rise < set_\n```\n\n## Project Structure\n```\n.\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ celestial.py          # Core celestial calculations\n‚îÇ   ‚îú‚îÄ‚îÄ light_pollution.py    # Light pollution map utilities\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py              # Time/location helpers\n‚îÇ   ‚îî‚îÄ‚îÄ main.py               # CLI entry point\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_celestial.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îî‚îÄ‚îÄ README.md\n```\n\n## Future Work\n- Add support for comets/asteroids.\n- Optimize SIMBAD queries for offline use.\n- Integrate light pollution data into visibility predictions.\n\n### Key Updates:\n1. **Light Pollution**: Added `light_pollution.py` to features and API reference.\n2. **Dependencies**: Added `rasterio` and `geopy` to installation instructions.\n3. **Project Structure**: Clarified file roles and test coverage.\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stargazing",
        "stargazer1995",
        "celestial",
        "mcp stargazing",
        "stargazer1995 mcp",
        "stargazing calculate"
      ],
      "category": "scientific-research-tools"
    },
    "TakumiY235--uniprot-mcp-server": {
      "owner": "TakumiY235",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/TakumiY235/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/TakumiY235.webp",
      "description": "Fetch protein information from UniProt, enabling access to detailed data about protein functions and sequences. Supports both individual and batch retrieval of protein information, including name, function description, full sequence, and organism details.",
      "stars": 9,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-14T07:44:52Z",
      "readme_content": "# UniProt MCP Server\n\nA Model Context Protocol (MCP) server that provides access to UniProt protein information. This server allows AI assistants to fetch protein function and sequence information directly from UniProt.\n\n<a href=\"https://glama.ai/mcp/servers/ttjbai3lpx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ttjbai3lpx/badge\" alt=\"UniProt Server MCP server\" />\n</a>\n\n## Features\n\n- Get protein information by UniProt accession number\n- Batch retrieval of multiple proteins\n- Caching for improved performance (24-hour TTL)\n- Error handling and logging\n- Information includes:\n  - Protein name\n  - Function description\n  - Full sequence\n  - Sequence length\n  - Organism\n\n## Quick Start\n\n1. Ensure you have Python 3.10 or higher installed\n2. Clone this repository:\n   ```bash\n   git clone https://github.com/TakumiY235/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n3. Install dependencies:\n   ```bash\n   # Using uv (recommended)\n   uv pip install -r requirements.txt\n   \n   # Or using pip\n   pip install -r requirements.txt\n   ```\n\n## Configuration\n\nAdd to your Claude Desktop config file:\n\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"uniprot\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path/to/uniprot-mcp-server\", \"run\", \"uniprot-mcp-server\"]\n    }\n  }\n}\n```\n\n## Usage Examples\n\nAfter configuring the server in Claude Desktop, you can ask questions like:\n\n```\nCan you get the protein information for UniProt accession number P98160?\n```\n\nFor batch queries:\n\n```\nCan you get and compare the protein information for both P04637 and P02747?\n```\n\n## API Reference\n\n### Tools\n\n1. `get_protein_info`\n   - Get information for a single protein\n   - Required parameter: `accession` (UniProt accession number)\n   - Example response:\n     ```json\n     {\n       \"accession\": \"P12345\",\n       \"protein_name\": \"Example protein\",\n       \"function\": [\"Description of protein function\"],\n       \"sequence\": \"MLTVX...\",\n       \"length\": 123,\n       \"organism\": \"Homo sapiens\"\n     }\n     ```\n\n2. `get_batch_protein_info`\n   - Get information for multiple proteins\n   - Required parameter: `accessions` (array of UniProt accession numbers)\n   - Returns an array of protein information objects\n\n## Development\n\n### Setting up development environment\n\n1. Clone the repository\n2. Create a virtual environment:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n3. Install development dependencies:\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n### Running tests\n\n```bash\npytest\n```\n\n### Code style\n\nThis project uses:\n- Black for code formatting\n- isort for import sorting\n- flake8 for linting\n- mypy for type checking\n- bandit for security checks\n- safety for dependency vulnerability checks\n\nRun all checks:\n```bash\nblack .\nisort .\nflake8 .\nmypy .\nbandit -r src/\nsafety check\n```\n\n## Technical Details\n\n- Built using the MCP Python SDK\n- Uses httpx for async HTTP requests\n- Implements caching with 24-hour TTL using an OrderedDict-based cache\n- Handles rate limiting and retries\n- Provides detailed error messages\n\n### Error Handling\n\nThe server handles various error scenarios:\n- Invalid accession numbers (404 responses)\n- API connection issues (network errors)\n- Rate limiting (429 responses)\n- Malformed responses (JSON parsing errors)\n- Cache management (TTL and size limits)\n\n## Contributing\n\nWe welcome contributions! Please feel free to submit a Pull Request. Here's how you can contribute:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and adhere to the existing coding style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- UniProt for providing the protein data API\n- Anthropic for the Model Context Protocol specification\n- Contributors who help improve this project",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "uniprot",
        "protein",
        "retrieval",
        "protein information",
        "retrieval protein",
        "uniprot mcp"
      ],
      "category": "scientific-research-tools"
    },
    "Tsuchijo--matlab-mcp": {
      "owner": "Tsuchijo",
      "name": "matlab-mcp",
      "url": "https://github.com/Tsuchijo/matlab-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Tsuchijo.webp",
      "description": "Integrate with MATLAB to create and execute MATLAB scripts and functions via MCP clients. Supports automation of MATLAB programming tasks within various applications.",
      "stars": 32,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T18:58:11Z",
      "readme_content": "# MATLAB MCP Server\n\nThis Model Context Protocol (MCP) server provides integration with MATLAB, allowing you to create and execute MATLAB scripts and functions through Claude or other MCP clients.\n\n## Setup Requirements\n\n- Python 3.11 (Python 3.13 and 3.12 are not currently supported by MATLAB Engine)\n- MATLAB R2024a (or compatible version)\n- uv package manager\n\n## Installation\n\n1. Create and set up the Python environment:\n```bash\n# Pin Python version\nuv python pin 3.11\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Install MCP\nuv add \"mcp[cli]\"\n```\n\n2. Install MATLAB Engine\nThe MATLAB Engine will be installed automatically when the server first runs, using the MATLAB installation specified in the `MATLAB_PATH` environment variable.\n\n## Directory Structure\n\n- `matlab_server.py`: The main MCP server implementation\n- `matlab_scripts/`: Directory where all MATLAB scripts and functions are saved (created automatically)\n- `pyproject.toml`: Python project configuration\n- `.python-version`: Specifies Python version for uv\n\n## Claude Desktop Integration\n\n1. Open your Claude Desktop configuration:\n```bash\n# On macOS\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n2. Add the MATLAB server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"matlab\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/absolute/path/to/matlab-mcp\",\n                \"run\",\n                \"matlab_server.py\"\n            ],\n            \"env\": {\n                \"MATLAB_PATH\": \"/Applications/MATLAB_R2024a.app\"\n            }\n        }\n    }\n}\n```\n\nMake sure to:\n- Replace `/absolute/path/to/matlab-mcp` with the actual path to your project directory\n- Verify the `MATLAB_PATH` points to your MATLAB installation\n- Use absolute paths (not relative)\n\n## Features\n\nThe server provides several tools:\n\n1. `create_matlab_script`: Create a new MATLAB script file\n   - Scripts are saved in the `matlab_scripts` directory\n   - File names must be valid MATLAB identifiers\n\n2. `create_matlab_function`: Create a new MATLAB function file\n   - Functions are saved in the `matlab_scripts` directory\n   - Must include valid function definition\n\n3. `execute_matlab_script`: Run a MATLAB script and get results\n   - Returns output text, generated figures, and workspace variables\n   - Can pass arguments to scripts\n\n4. `call_matlab_function`: Call a MATLAB function with arguments\n   - Returns function output and any generated figures\n\n## Testing\n\nYou can test the server using the MCP Inspector:\n```bash\n# Make sure you're in your virtual environment\nsource .venv/bin/activate\n\n# Run the inspector\nMATLAB_PATH=/Applications/MATLAB_R2024a.app mcp dev matlab_server.py\n```\n\nExample test script:\n```matlab\nt = 0:0.01:2*pi;\ny = sin(t);\nplot(t, y);\ntitle('Test Plot');\nxlabel('Time');\nylabel('Amplitude');\n```\n\n## Script Storage\n\n- All MATLAB scripts and functions are saved in the `matlab_scripts` directory\n- This directory is created automatically when the server starts\n- Files are named `<script_name>.m` or `<function_name>.m`\n- The directory is in the same location as `matlab_server.py`\n\n## Environment Variables\n\n- `MATLAB_PATH`: Path to your MATLAB installation\n  - Default: `/Applications/MATLAB_R2024a.app`\n  - Set in Claude Desktop config or when running directly\n\n## Troubleshooting\n\n1. **MATLAB Engine Installation Fails**\n   - Verify MATLAB_PATH is correct\n   - Try installing engine manually:\n     ```bash\n     cd $MATLAB_PATH/extern/engines/python\n     python setup.py install\n     ```\n\n2. **Python Version Issues**\n   - Make sure you're using Python 3.11\n   - Check with: `python --version`\n   - Use `uv python pin 3.11` if needed\n\n3. **Script Execution Errors**\n   - Check the `matlab_scripts` directory exists\n   - Verify script syntax is valid\n   - Look for error messages in MATLAB output\n\n## Updates and Maintenance\n\n- Keep your MATLAB installation updated\n- Update Python packages as needed: `uv pip install --upgrade mcp[cli]`\n- Check MATLAB engine compatibility when updating Python",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "matlab",
        "mcp",
        "scripts",
        "matlab scripts",
        "matlab mcp",
        "automation matlab"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--quantum-simulator-mcp": {
      "owner": "YuChenSSR",
      "name": "quantum-simulator-mcp",
      "url": "https://github.com/YuChenSSR/quantum-simulator-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Simulates quantum circuits incorporating noise models and visualizes results. Supports OpenQASM 2.0 and various quantum algorithms with integration into MCP clients for streamlined use.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-06-19T23:08:17Z",
      "readme_content": "# Quantum Simulator MCP Server\n\nA Docker image providing a quantum circuit simulator that implements the Model Context Protocol (MCP), allowing integration with MCP clients such as Claude for Desktop.\n\n\n\n## Features\n\n- Quantum computing simulator with noise models\n- Support for OpenQASM 2.0 quantum circuits\n- Quantum circuit simulation using Qiskit\n- Support for various noise models (depolarizing, thermal relaxation, readout error)\n- Multiple result types including counts, statevector, and visualized histograms\n- Pre-configured example circuits\n- Seamless integration with MCP clients\n\n## Quick Start\n\nget the docker image\n\n```bash\ndocker pull ychen94/quantum-simulator-mcp:latest\n```\n\n\nSimply run the container with the following command:\n\n```bash\ndocker run -i --rm -v /tmp:/data/quantum_simulator_results -e HOST_OUTPUT_DIR=\"/tmp\" ychen94/quantum-simulator-mcp:latest\n```\n\nThis command:\n- Mounts the `/tmp` directory on your host to store histogram output files\n- Sets the `HOST_OUTPUT_DIR` environment variable to `/tmp`\n- Keeps the container running with `-i` (interactive mode)\n- Automatically removes the container when it exits with `--rm`\n\n## Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Edit the Claude configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"quantum-simulator\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\", \"/tmp:/data/quantum_simulator_results\",\n        \"-e\", \"HOST_OUTPUT_DIR=/tmp\",\n        \"ychen94/quantum-simulator-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n5. Look for the hammer icon in the Claude UI, indicating available MCP tools\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n- **run_circuit**: Run a quantum circuit with specified noise model\n- **list_noise_models**: List all available noise models and their descriptions\n- **list_result_types**: List all available result types and their descriptions\n- **get_circuit_stats**: Analyze a quantum circuit and return statistics\n- **create_test_histogram**: Create a test histogram file to verify output directory configuration\n\n## MCP Resources\n\nThe server provides example quantum circuits:\n\n- `qasm://examples/bell-state.qasm`: Bell state preparation circuit\n- `qasm://examples/grover-2qubit.qasm`: 2-qubit Grover's algorithm implementation\n- `qasm://examples/qft-4qubit.qasm`: 4-qubit Quantum Fourier Transform\n- `quantum://noise-models/examples.json`: Example noise model configurations\n\n## Example Usage in Claude\n\nHere are some prompts you can use in Claude:\n\n1. \"Run a Bell state circuit and show me the results\"\n\n2. \"What noise models are available in the quantum simulator?\"\n\n3. \"Simulate a 2-qubit Grover's algorithm with 0.01 depolarizing noise\"\n\n4. \"Create a test histogram and show me the file path\"\n\n5. \"Please provide a simple QAOA algorithm, only get the result_types: histogram, and view the histogram using iterm\"\n\n![chat](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/FHE8cIDqLRN36pOm.png)\n\n![result_pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/OKD2nqE0aHYuWBan.png)\n\n\n## Volume Mapping\n\nThe container generates histogram PNG files in `/data/quantum_simulator_results`. These files need to be accessible from your host system. The volume mapping (`-v /tmp:/data/quantum_simulator_results`) makes these files available in your host's `/tmp` directory.\n\n## Environment Variables\n\n- `QUANTUM_OUTPUT_DIR`: Output directory for histogram files inside the container (default: `/data/quantum_simulator_results`)\n- `HOST_OUTPUT_DIR`: Corresponding path on the host system (default: `/tmp`)\n\n## Multi-Architecture Support\n\nThis image supports the following architectures:\n- linux/amd64\n- linux/arm64 (confirmed working on Mac M-series chips)\n\nNote: The image has not been tested on Windows systems yet, but should work as long as Docker Desktop is properly configured.\n\n## Troubleshooting\n\n**Issue**: Claude cannot access the histogram files.  \n**Solution**: Ensure the volume mapping is correct and the `HOST_OUTPUT_DIR` environment variable matches the host path in your volume mapping.\n\n**Issue**: Docker container exits immediately.  \n**Solution**: Make sure to use the `-i` flag to keep stdin open, which is required for the MCP STDIO transport.\n\n## License\n\nThis project is licensed under the MIT License. For more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/quantum-simulator-mcp).\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "openqasm",
        "simulates",
        "quantum simulator",
        "simulates quantum",
        "quantum algorithms"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--symbolica-mcp": {
      "owner": "YuChenSSR",
      "name": "symbolica-mcp",
      "url": "https://github.com/YuChenSSR/symbolica-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Facilitates symbolic computing, mathematical analysis, and data visualization for scientific and engineering applications using libraries like NumPy, SciPy, and Matplotlib in a containerized setup. Supports complex matrix calculations, statistical analysis, and machine learning tasks.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-07-28T02:42:49Z",
      "readme_content": "# symbolica-mcp\n\nA scientific computing Model Context Protocol (MCP) server allows AI, such as Claude, to perform symbolic computing, conduct calculations, analyze data, and generate visualizations. This is particularly useful for scientific and engineering applications, including quantum computing, all within a containerized environment.\n\n## Features\n\n- Run scientific computing operations with NumPy, SciPy, SymPy, Pandas\n- Perform symbolic mathematics and solve differential equations\n- Support for linear algebra operations and matrix manipulations\n- Quantum computing analysis\n- Create data visualizations with Matplotlib and Seaborn\n- Perform machine learning operations with scikit-learn\n- Execute tensor operations and complex matrix calculations\n- Analyze data sets with statistical tools\n- Cross-platform support (automatically detects Windows, macOS, and Linux), especially for users with Mac M series chips\n- Works on both Intel/AMD (x86_64) and ARM processors\n\n## Quick Start\n\n### Using the Docker image\n\n```bash\n# Pull the image from Docker Hub\ndocker pull ychen94/computing-mcp:latest\n\n# Run the container (automatically detects host OS)\ndocker run -i --rm -v /tmp:/app/shared ychen94/computing-mcp:latest\n```\n\nFor Windows users:\n```powershell\ndocker run -i --rm -v $env:TEMP:/app/shared ychen94/computing-mcp:latest\n```\n\n### Integrating with Claude for Desktop\n\n1. Open Claude for Desktop\n2. Open Settings ‚ûù Developer ‚ûù Edit Config\n3. Add the following configuration:\n\nFor MacOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/tmp:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\nFor Windows:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"%TEMP%:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n\n\n## Examples\n\n### Tensor Products\n\n```\nCan you calculate and visualize the tensor product of two matrices? Please run:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define two matrices\nA = np.array([[1, 2], \n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\n# Calculate tensor product using np.kron (Kronecker product)\ntensor_product = np.kron(A, B)\n\n# Display the result\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nTensor Product A ‚äó B:\")\nprint(tensor_product)\n\n# Create a visualization of the tensor product\nplt.figure(figsize=(8, 6))\nplt.imshow(tensor_product, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Visualization of Tensor Product A ‚äó B')\n```\n\n### Symbolic Mathematics\n\n```\nCan you solve this differential equation? Please run:\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define symbolic variable\nx = sp.Symbol('x')\ny = sp.Function('y')(x)\n\n# Define the differential equation: y''(x) + 2*y'(x) + y(x) = 0\ndiff_eq = sp.Eq(sp.diff(y, x, 2) + 2*sp.diff(y, x) + y, 0)\n\n# Solve the equation\nsolution = sp.dsolve(diff_eq)\nprint(\"Solution:\")\nprint(solution)\n\n# Plot a particular solution (C1=1, C2=0)\nsolution_func = solution.rhs.subs({sp.symbols('C1'): 1, sp.symbols('C2'): 0})\nprint(\"Particular solution:\")\nprint(solution_func)\n\n# Create a numerical function we can evaluate\nsolution_lambda = sp.lambdify(x, solution_func)\n\n# Plot the solution\nx_vals = np.linspace(0, 5, 100)\ny_vals = [float(solution_lambda(x_val)) for x_val in x_vals]\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_vals, y_vals)\nplt.grid(True)\nplt.title(\"Solution to y''(x) + 2*y'(x) + y(x) = 0\")\nplt.xlabel('x')\nplt.ylabel('y(x)')\nplt.show()\n```\n\n### Data Analysis\n\n```\nCan you perform a clustering analysis on this dataset? Please run:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a sample dataset\nnp.random.seed(42)\nn_samples = 300\n\n# Create three clusters\ncluster1 = np.random.normal(loc=[2, 2], scale=0.5, size=(n_samples//3, 2))\ncluster2 = np.random.normal(loc=[7, 7], scale=0.5, size=(n_samples//3, 2))\ncluster3 = np.random.normal(loc=[2, 7], scale=0.5, size=(n_samples//3, 2))\n\n# Combine clusters\nX = np.vstack([cluster1, cluster2, cluster3])\n\n# Create DataFrame\ndf = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\nprint(df.head())\n\n# Standardize data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(X_scaled)\n\n# Plot the clusters\nplt.figure(figsize=(10, 6))\nfor cluster_id in range(3):\n    cluster_data = df[df['Cluster'] == cluster_id]\n    plt.scatter(cluster_data['Feature1'], cluster_data['Feature2'], \n                label=f'Cluster {cluster_id}', alpha=0.7)\n\n# Plot cluster centers\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\nplt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centers')\n\nplt.title('K-Means Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True)\n```\n\n### Quantum Computing\n\n![quantum example](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/UaybujIK2o3tLUtR.png)\n\n### Gallery\n\n**laser physics**:\n![laser](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/4t06m3iM17NXpW6O.png)\n\n**elliptic integral**:\n![elliptic integral](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/rvshlS2blGv7jnoi.png)\n![elliptic integral pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/oRGJIHgKQV8kMMHd.png)\n\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Permission errors with volume mounts**\n   - Ensure the mount directory exists and has appropriate permissions\n\n2. **Plot pciture files not appearing**\n   - Check the path in your host system: `/tmp` for macOS/Linux or your temp folder for Windows\n   - Verify Docker has permissions to write to the mount location\n   - check the mcp tool's output content\n     ![find the pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/Ik3JZeLhdptLqgq3.png)\n     then open it in the terminal or your picture viewer.\n\n      ‚≠êÔ∏è ‚≠êÔ∏è\n     I use the [iterm-mcp-server](https://github.com/ferrislucas/iterm-mcp) or other terminals' mcp servers to open the file  without interrupting your workflow. \n      ‚≠êÔ∏è ‚≠êÔ∏è\n\n\n\n### Support\n\nIf you encounter issues, please open a GitHub issue with:\n1. Error messages\n2. Your operating system and Docker version\n3. Steps to reproduce the problem\n\n## License\nThis project is licensed under the MIT License.   \nFor more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/symbolica-mcp).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scipy",
        "matplotlib",
        "symbolica",
        "symbolic computing",
        "symbolica mcp",
        "visualization scientific"
      ],
      "category": "scientific-research-tools"
    },
    "acashmoney--bio-mcp": {
      "owner": "acashmoney",
      "name": "bio-mcp",
      "url": "https://github.com/acashmoney/bio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/acashmoney.webp",
      "description": "Enhances biomedical research through advanced protein structure analysis, enabling the examination of protein active sites and the discovery of disease-related proteins. It integrates with established protein databases for reliable insights and data access.",
      "stars": 17,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-27T01:43:39Z",
      "readme_content": "# BioMCP: Enabling agent-based biomedical R&D\n\n[![smithery badge](https://smithery.ai/badge/@acashmoney/bio-mcp)](https://smithery.ai/server/@acashmoney/bio-mcp)\n![BioMCP](cover.png)\n\n## Overview\n\nBioMCP is a Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server designed to enhance large language models with protein structure analysis capabilities. It provides tools for analyzing protein active sites and searching for disease-related proteins by interfacing with established protein databases. \n\nFuture work will be centered around enabling agents to utilize the BioMCP.\n\n## Features\n\n- **Active Site Analysis**: Examine the binding sites and functional residues of proteins using PDB IDs\n- **Disease-Protein Search**: Find protein structures associated with specific diseases or medical conditions\n- **Integrated Data Access**: Connect seamlessly with RCSB Protein Data Bank [(PDB)](https://www.rcsb.org/)\n\n## Technical Details\n\nBioMCP implements the Model Context Protocol, allowing language models to access specialized protein structure knowledge without requiring this information to be part of their training data. The server handles API connections, data formatting, and error handling to provide reliable protein structure insights.\n\n## API Endpoints\n\nBioMCP exposes two primary tools:\n\n1. `analyze-active-site`: Provides detailed information about protein binding sites using a PDB ID\n2. `search-disease-proteins`: Returns proteins related to specified diseases or medical conditions\n\n## Getting Started\n\n### Installing via Smithery\n\nTo install BioMCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@acashmoney/bio-mcp):\n\n```bash\nnpx -y @smithery/cli install @acashmoney/bio-mcp --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/acashmoney/bio-mcp.git\n\n# Install dependencies\nnpm install\n\n# Start the server\nnpm start\n```\n\n## Setup Instructions\n\n### Running the MCP Inspector\n\n1. Start the BioMCP server:\n   ```bash\n   npm start\n   ```\n\n2. In a separate terminal, install the MCP Inspector globally (if not already installed):\n   ```bash\n   npm install -g @anthropic-ai/mcp-inspector\n   ```\n\n3. Launch the MCP Inspector and connect to your local BioMCP server:\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n4. Use the inspector interface to test tools and view responses.\n\n### Using with Claude Desktop\n\n1. Build the BioMCP server:\n   ```bash\n   npm run build\n   ```\n\n2. Configure Claude Desktop to launch the MCP server:\n\n   a. Locate your Claude Desktop config.json file (typically in your user directory)\n   \n   b. Edit the config.json to include the BioMCP server build path. Example configuration:\n   ```json\n   {\n     \"globalShortcut\": \"\",\n     \"mcpServers\": {\n       \"bio-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n   \n   c. Replace `/path/to/your/build` with your actual path to the project directory.\n\n3. Restart Claude Desktop for the changes to take effect.\n\n4. You can now ask Claude questions that utilize the BioMCP tools:\n   - \"What are the key residues in the active site of PDB structure 6LU7?\"\n   - \"Find proteins related to Alzheimer's disease\"\n\n## Example Usage\n\nWhen integrated with a compatible language model, Bio-MCP enables queries like:\n\n- \"What are the key residues in the active site of PDB structure 6LU7?\"\n- \"Find proteins related to Alzheimer's disease\"\n\n## Requirements\n\n- Node.js 20.0.0 or higher\n- TypeScript 5.0+\n- Compatible MCP client implementation\n\n## Testing\n\nBioMCP includes a comprehensive testing suite with unit, integration, and end-to-end tests.\n\n### Running Tests\n\nRun all tests:\n```bash\nnpm test\n```\n\nRun specific test suites:\n```bash\n# Unit tests only\nnpm run test:unit\n\n# Integration tests only (API interactions)\nnpm run test:integration\n\n# End-to-end tests only\nnpm run test:e2e\n```\n\n### Linting\n\nCheck code quality:\n```bash\nnpm run lint\n```\n\nFix linting issues automatically:\n```bash\nnpm run lint:fix\n```\n\n## Roadmap\n\n- Expand level of detail for active site descriptions\n- Leverage 3-D coordinates\n- Tools for interfacing with literature\n- Tools for interfacing with computational biology models:\n  - RFdiffusion\n  - ProteinMPNN\n  - ColabFold\n  - Additional protein design and structure prediction tools\n- Agent-based research pipelines\n- Introduce client with protein visualization tools\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteins",
        "protein",
        "mcp",
        "protein databases",
        "advanced protein",
        "bio mcp"
      ],
      "category": "scientific-research-tools"
    },
    "agan2021--stata-prov-map": {
      "owner": "agan2021",
      "name": "stata-prov-map",
      "url": "https://github.com/agan2021/stata-prov-map",
      "imageUrl": "/freedevtools/mcp/pfp/agan2021.webp",
      "description": "Provides data and visualization tools for creating detailed Chinese provincial maps in Stata, including features for scale bars, north arrows, and various geographic lines. Supports plotting discrete and continuous variables, point plotting, and combined charts like pie charts on maps, along with geographic coordinate transformations and map customization.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "",
      "updated_at": "2024-08-03T06:31:54Z",
      "readme_content": "# ‰ΩøÁî® Stata ÁªòÂà∂‰∏≠ÂõΩÁúÅÁ∫ßÂú∞ÂõæÔºÅÊØî‰æãÂ∞∫„ÄÅÊåáÂåóÈíà„ÄÅÁß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø„ÄÅËÉ°ÁÑïÂ∫∏Á∫ø„ÄÅÊµ∑Â≤∏Á∫ø„ÄÅÊú™ÂÆöÂõΩÁïåÈÉΩÊúâÔºÅ\n\n> Ê¨¢ËøéÂâçÂæÄ [ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV12L411J7Vt/) Êü•ÁúãËØ•È°πÁõÆÁöÑËßÜÈ¢ëËÆ≤Ëß£Ôºöhttps://www.bilibili.com/video/BV12L411J7Vt/ \n\nÊ†πÊçÆÂ§ßÂÆ∂‰πãÂâçÁöÑ‰∏Ä‰∫õÂèçÈ¶àÔºåÊàë‰ª¨ÂèàÈáçÊñ∞ËÆæËÆ°‰∫Ü‰∏Ä‰ªΩ‰ΩøÁî® Stata ÁªòÂà∂ÁúÅÁ∫ßÂú∞ÂõæÁöÑÊï∞ÊçÆÔºåËøôÊ¨°ÁöÑÂú∞ÂõæÊï∞ÊçÆÊØîËµ∑‰πãÂâçÁöÑÁâàÊú¨ÊúâÂ¶Ç‰∏ãÂèòÂåñÔºö\n\n1. ÊåáÂåóÈíàÂèòÂæóÁ´ñÁõ¥‰∫ÜÔºÅ\n2. ÂåÖÂê´‰∫ÜÊú™ÂÆöÂõΩÁïåÔºõ\n3. Ê∑ªÂä†‰∫ÜÊµ∑Â≤∏Á∫øÔºõ\n4. Ê∑ªÂä†‰∫ÜÁß¶Â≤≠-Ê∑ÆÊ≤≥Á∫øÔºõ\n5. Ê∑ªÂä†‰∫ÜËÉ°ÁÑïÂ∫∏Á∫øÔºõ\n6. Ê∑ªÂä†‰∫ÜËã±ÊñáÁâàÊú¨ÔºÅ\n7. ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÁöÑ‰ΩçÁΩÆÂèØ‰ª•ÁßªÂä®‰∫ÜÔºÅ\n\n‰∏∫‰∫ÜËÆ©Â§ßÂÆ∂Êõ¥Â•ΩÁöÑÂ≠¶‰π†Ëøô‰ªΩÂú∞ÂõæÊï∞ÊçÆÁöÑ‰ΩøÁî®ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ãÈù¢ÁöÑËøô‰∫õÊ°à‰æãÔºö\n\n## 2019 Âπ¥‰∏≠ÂõΩÁúÅÁ∫ßË°åÊîøÂå∫Âàí\n\nÊàë‰ª¨È¶ñÂÖàÁúã‰∏Ä‰∏ãÁ¶ªÊï£ÂèòÈáèÁöÑÂ°´ÂÖÖÂú∞ÂõæÂ¶Ç‰ΩïÁªòÂà∂Ôºå‰æãÂ¶Ç `chinaprov40_db.dta` Êï∞ÊçÆÈáåÈù¢Êúâ‰∏™Á±ªÂûãÂèòÈáèÂ∞±ÊòØÁ¶ªÊï£ÂèòÈáèÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî® encode ÂëΩ‰ª§ÁîüÊàêÂõ†Â≠êÂèòÈáèÁî®‰∫éÂ°´ÂÖÖÁªòÂõæÔºö\n\n```stata\n* Á¶ªÊï£ÂèòÈáèÁöÑÁªòÂà∂\nuse chinaprov40_db.dta, clear \nencode Á±ªÂûã, gen(type)\ncodebook type\n\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"‰∏çÁªüËÆ°\" 3 \"ÁâπÂà´Ë°åÊîøÂå∫\" 4 \"Áõ¥ËæñÂ∏Ç\" 5 \"ÁúÅ\" 6 \"Ëá™Ê≤ªÂå∫\" 11 \"Áß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø\" 14 \"ËÉ°ÁÑïÂ∫∏Á∫ø\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        \"24 188 156\" /// Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        \"227 26 28\" /// ËÉ°ÁÑïÂ∫∏Á∫øÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"‰ΩøÁî® Stata ÁªòÂà∂ 2019 Âπ¥‰∏≠ÂõΩÁúÅÁ∫ßË°åÊîøÂå∫Âàí\") ///\n  subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n  caption(\"ÁâàÊú¨Ôºö‰ΩøÁî® Stata ÁªòÂà∂‰∏≠ÂõΩÁúÅÁ∫ßÂú∞ÂõæÊï∞ÊçÆÂåÖ 4.0\", size(*0.8))\ngr export pic1.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204201.png)\n\nline() ÈÄâÈ°πÈáåÈù¢ÁöÑ size() Âíå color() Â≠êÈÄâÈ°πÂàÜÂà´Áî®‰∫Ü 7 ÁßçÂ∞∫ÂØ∏ÂíåÈ¢úËâ≤Ôºå‰æùÁÖßÈ°∫Â∫èÂàÜÂà´ÂØπÂ∫îÁúÅÁïåÁ∫ø„ÄÅÂõΩÁïåÁ∫ø„ÄÅÊµ∑Â≤∏Á∫ø„ÄÅÁß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø„ÄÅÂ∞èÂú∞ÂõæÊ°ÜÊ†º„ÄÅÊØî‰æãÂ∞∫ÊåáÂåóÈíà„ÄÅËÉ°ÁÑïÂ∫∏Á∫ø„ÄÇ\n\n> ÂÖ≥‰∫éÈ¢úËâ≤ÁöÑÈÄâÊã©ÔºåÂèØ‰ª•‰ΩøÁî®Ëøô‰∏™ÁΩëÁ´ôÔºöhttps://tidyfriday.cn/colors \n\n## Ëã±ÊñáÂú∞Âõæ\n\n`chinaprov40_label.dta` Êï∞ÊçÆÂÜ≥ÂÆö‰∫ÜÊñáÊú¨Ê†áÁ≠æÁöÑ‰ΩçÁΩÆÂíåÂÜÖÂÆπÔºåÂ¶ÇÊûúÊÉ≥ÁªòÂà∂Ëã±ÊñáÁâàÁöÑÂú∞ÂõæÔºåÂèØ‰ª•‰ΩøÁî® ename ÂèòÈáèÔºö\n\n```stata\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"Not within the scope of statistics\" 3 \"Special administrative region\" 4 \"Municipality directly under\" \"the Central Government\" 5 \"Province\" 6 \"Autonomous Region\" 11 \"Qinling Huaihe River Line\" 14 \"Hu Huanyong line\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        \"24 188 156\" /// Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        \"227 26 28\" /// ËÉ°ÁÑïÂ∫∏Á∫øÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(ename) length(20) size(*0.6)) ///\n  ti(\"Using Stata to draw China's provincial map in 2019\") ///\n  subti(\"WeChat Subscription: RStata\") ///\n  caption(\"Version: 4.0\", size(*0.8))\ngr export pic2.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204222.png)\n\n## 2020 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº\n\nËøûÁª≠ÂèòÈáèÁöÑÁªòÂà∂ÊñπÊ≥ïÂíåÁ¶ªÊï£ÂèòÈáèÁöÑÁ±ª‰ººÔºåÈÄöËøáÊÅ∞ÂΩìÁöÑÂàÜÁªÑÂíåÈ¢úËâ≤Â°´ÂÖÖÂ∞±ÂèØ‰ª•ÁªòÂà∂Âá∫Á≤æÁæéÁöÑÂõæË°®‰∫ÜÔºå‰∏ãÈù¢‰ª•2020 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄºÁöÑÂ°´ÂÖÖÂú∞ÂõæÁªòÂà∂‰∏∫‰æãÔºö\n\n```stata\nimport delimited using \"2020Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº.csv\", clear encoding(utf8)\ngen prov = substr(ÁúÅ‰ªΩ, 1, 6)\nsave 2020Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº, replace \n\nuse chinaprov40_db.dta, clear \ngen prov = substr(ÁúÅ, 1, 6)\nmerge 1:1 prov using 2020Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº\nreplace Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº = -1 if missing(Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº)\ngrmap Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(-1 0 20000 40000 60000 80000 120000) ///\n  fcolor(gray \"224 242 241\" \"178 223 219\" \"128 203 196\" \"77 182 172\" \"38 166 154\") ///\n  leg(order(2 \"Êó†Êï∞ÊçÆ\" 3 \"< 2 ‰∏á‰∫øÂÖÉ\" 4 \"2ÔΩû4 ‰∏á‰∫øÂÖÉ\" 5 \"4ÔΩû6 ‰∏á‰∫øÂÖÉ\" 6 \"6ÔΩû8 ‰∏á‰∫øÂÖÉ\" 7 \"> 8 ‰∏á‰∫øÂÖÉ\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// ÂéªÈô§Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫ø(4)„ÄÅËÉ°ÁÑïÂ∫∏Á∫ø(7)\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"2020 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂ∏ÇÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº\") ///\n  subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n  caption(\"Êï∞ÊçÆÊù•Ê∫êÔºöÂêÑÂú∞ÁªüËÆ°Â±Ä\", size(*0.8))\ngr export pic3.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204252.png)\n\n## 2013 Âπ¥‰∏≠ÂõΩÂ∑•‰ºÅ‰∏öÂàÜÂ∏ÉÂèäË∑ùÁ¶ªÁß¶Â≤≠Ê∑ÆÊ≤≥ÁöÑË∑ùÁ¶ª\n\nÈô§‰∫ÜÂ°´ÂÖÖÂú∞ÂõæÂ§ñÔºåÊàë‰ª¨ËøòÁªèÂ∏∏‰ºöÈúÄË¶ÅÁªòÂà∂ÊèèÁÇπÂú∞ÂõæÔºå‰πüÂ∞±ÊòØÂú®Âú∞Âõæ‰∏äÊ∑ªÂä†ÂùêÊ†áÁÇπÔºå‰∏ãÈù¢‰ª•2013 Âπ¥‰∏≠ÂõΩÂ∑•‰ºÅ‰∏öÂàÜÂ∏ÉÂèäË∑ùÁ¶ªÁß¶Â≤≠Ê∑ÆÊ≤≥ÁöÑË∑ùÁ¶ªÂèØËßÜÂåñ‰∏∫‰æã„ÄÇ\n\nÁî±‰∫éÊèê‰æõÁöÑÂú∞ÂõæÂ∫ïÂõæÊï∞ÊçÆÊòØÁªèËøáÊäïÂΩ±ÂèòÊç¢ÁöÑÔºåÊâÄ‰ª•Â¶ÇÊûúÂ§ßÂÆ∂ÊÉ≥ÊääÁªèÁ∫¨Â∫¶ÂùêÊ†áÁÇπÊèèÂà∂Âú®Â∫ïÂõæ‰∏äÔºå‰πüÈúÄË¶ÅÂØπÂùêÊ†áËøõË°åÊäïÂΩ±ÂèòÊç¢Ôºå‰∏∫Ê≠§ÔºåÊàëÁºñÂÜô‰∫Ü‰∏Ä‰∏™ÁΩëÈ°µÂ∫îÁî®Â∏ÆÂä©Â§ßÂÆ∂ËøõË°åÊäïÂΩ±ÂèòÊç¢Ôºöhttps://czxb.shinyapps.io/crs-trans/\n\nÂè¶Â§ñÂ¶ÇÊûú‰Ω†Êúâ‰∏Ä‰∫õ R ËØ≠Ë®ÄÁöÑ‰ΩøÁî®Âü∫Á°ÄÔºå‰πüÂèØ‰ª•‰ΩøÁî® `ÂùêÊ†áËΩ¨Êç¢.R` ËøõË°åÂùêÊ†áËΩ¨Êç¢„ÄÇ\n\n```stata\n* ËΩ¨Êç¢ÂùêÊ†áÁ≥ª\n* ËΩ¨Êç¢ÊñπÂºè‰∏ÄÔºöhttps://czxb.shinyapps.io/crs-trans/\n* Ê≥®ÊÑè‰∫ãÈ°πÔºå‰∏ä‰º†ÁöÑ csv Êñá‰ª∂Â∫îËØ•ÂåÖÂê´Êï∞ÂÄºÂûãÁöÑ lon Âíå lat ÂèòÈáèÔºåËßÇÊµãÂÄº‰∏äÈôêÂ§ßÊ¶ÇÊòØ 10 ‰∏á‰∏™Ôºå‰∏çÂèØÂ§ö‰∫∫ÂêåÊó∂‰ΩøÁî®„ÄÇ\nuse gq2013sample, clear \nkeep ÁªèÂ∫¶ Á∫¨Â∫¶\nren ÁªèÂ∫¶ lon\nren Á∫¨Â∫¶ lat\nexport delimited using \"ÂæÖËΩ¨Êç¢.csv\", replace \n\n* ËΩ¨Êç¢ÊñπÂºè‰∫åÔºö‰ΩøÁî®ÈôÑ‰ª∂‰∏≠ÁöÑ R ËÑöÊú¨ËΩ¨Êç¢\n\n* Â§ÑÁêÜËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆ\nimport delimited using \"ËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆ.csv\", clear \ngen id = _n\nsave ËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆ, replace \n\nuse gq2013sample, clear \ngen id = _n\nmerge 1:1 id using ËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆ\ndrop _m id *Â∫¶\nencode ÂåóÊñπÊàñÂçóÊñπ, gen(north)\nsave pointdata, replace \n\nuse chinaprov40_db.dta, clear\nspmap using chinaprov40_coord.dta, id(ID) ///\n  ocolor(\"black\" ...) osize(vvthin ...) ///\n    line(data(chinaprov40_line_coord.dta) ///\n    /// ËÉ°ÁÑïÂ∫∏Á∫øÔºà7Ôºâ\n    select(keep if inlist(group, 1, 2, 3, 4, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *1.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        \"0 85 170\" /// Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n    point(data(pointdata) by(north) ///\n      fcolor(\"227 26 28%30\" \"24 188 156%30\") ///\n        x(x) y(y) ///\n        proportional(‰∏éÁß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÁöÑË∑ùÁ¶ª) ///\n        size(*0.1) legenda(on)) ///\n    leg(order(7 \"Áß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø\" 10 \"ÂåóÊñπÂ∑•‰ºÅ‰∏ö\" 11 \"ÂçóÊñπÂ∑•‰ºÅ‰∏ö\")) ///\n    ti(\"2013 Âπ¥‰∏≠ÂõΩÂ∑•‰∏ö‰ºÅ‰∏ö‰∏éÁß¶Â≤≠-Ê∑ÆÊ≤≥Á∫øÁöÑË∑ùÁ¶ª\", color(black)) /// \n    subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n    graphr(margin(medium)) ///\n    caption(\"Êï∞ÊçÆÊù•Ê∫êÔºö2013 Âπ¥‰∏≠ÂõΩÂ∑•‰∏ö‰ºÅ‰∏öÊï∞ÊçÆÂ∫ìÔºå‰ΩøÁî®È´òÂæ∑Âú∞ÂõæÂú∞ÁêÜÁºñÁ†ÅÊé•Âè£Ëß£ÊûêÁªèÁ∫¨Â∫¶\", size(*0.8))\ngr export pic4.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204304.png)\n\n## 2019 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº & ‰∫ß‰∏öÁªìÊûÑ\n\n‰πüÂèØ‰ª•ÁªòÂà∂Âú∞Âõæ+È•ºÂõæÔºåÂÖ∂‰∏≠È•ºÂõæÁöÑ‰ΩçÁΩÆÂèØ‰ª•Áî®Ê†áÁ≠æ‰ΩçÁΩÆÔºö\n\n```stata\nuse ÂêÑÁúÅÂéÜÂπ¥GDP, clear \ndrop if ÁúÅ‰ªΩ == \"‰∏≠ÂõΩ\"\n\nreplace Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº_‰∫øÂÖÉ = Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº_‰∫øÂÖÉ / 1000\nmerge m:m ÁúÅ‰ª£Á†Å using chinaprov40_db.dta\nreplace Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº_‰∫øÂÖÉ = -1 if missing(Âπ¥‰ªΩ)\ngrmap Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº_‰∫øÂÖÉ if Âπ¥‰ªΩ == 2019 | missing(Âπ¥‰ªΩ) ///\n  using chinaprov40_coord.dta, id(ID) ///\n  clmethod(custom) clbreaks(-1 0 40 60 80 100 120) /// \n  fcolor(\"gray\" \"237 248 233\" \"199 233 192\" \"161 217 155\" \"116 196 118\" \"49 163 84\") ///\n  ocolor(\"gray\" ...) ///\n  ti(\"2019 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº & ‰∫ß‰∏öÁªìÊûÑ\", size(*1.1)) ///\n  subtitle(\"Êï∞ÊçÆÊù•Ê∫êÔºöCSMARÁªèÊµéÈáëËûçÊï∞ÊçÆÂ∫ì\") ///\n  graphr(margin(medium)) ///\n  osize(vvthin ...) ///\n  legend(size(*1.1) ///\n    order(2 \"Êó†Êï∞ÊçÆ\" 3 \"< 40ÂçÉ‰∫ø\" ///\n      4 \"40ÔΩû60ÂçÉ‰∫ø\" 5 \"60ÔΩû80ÂçÉ‰∫ø\" ///\n      6 \"80ÔΩû100ÂçÉ‰∫ø\" 7 \"> 100ÂçÉ‰∫ø\" ///\n      14 \"Á¨¨‰∏Ä‰∫ß‰∏ö\" 15 \"Á¨¨‰∫å‰∫ß‰∏ö\" 16 \"Á¨¨‰∏â‰∫ß‰∏ö\")) ///\n  caption(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\", size(*0.8)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// ÂéªÈô§Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫ø(4) ËÉ°ÁÑïÂ∫∏Á∫øÔºà7Ôºâ\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  diagram(data(piedata) x(X) y(Y) v(Á¨¨‰∏Ä‰∫ß‰∏öÂç†GDPÊØîÈáç_ÁôæÂàÜÊØî Á¨¨‰∫å‰∫ß‰∏öÂç†GDPÊØîÈáç_ÁôæÂàÜÊØî Á¨¨‰∏â‰∫ß‰∏öÂç†GDPÊØîÈáç_ÁôæÂàÜÊØî) ///\n    type(pie) legenda(on) os(vvthin) ///\n      size(1.5) fc(\"102 194 165\" \"252 141 98\" \"229 196 148\") ///\n      oc(\"102 194 165\" \"252 141 98\" \"229 196 148\"))\n\ngr export \"pic5.png\", replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204357.png)\n\nÂè¶Â§ñÈ•ºÂõæ‰πüÂèØ‰ª•Êç¢ÊàêÊ°ÜÊû∂Áü©ÂΩ¢ÂõæÔºö\n\n```stata\ngrmap Âú∞Âå∫Áîü‰∫ßÊÄªÂÄº_‰∫øÂÖÉ if Âπ¥‰ªΩ == 2019 | missing(Âπ¥‰ªΩ) ///\n  using chinaprov40_coord.dta, id(ID) ///\n  clmethod(custom) clbreaks(-1 0 40 60 80 100 120) /// \n  fcolor(\"gray\" \"237 248 233\" \"199 233 192\" \"161 217 155\" \"116 196 118\" \"49 163 84\") ///\n  ocolor(\"gray\" ...) ///\n  ti(\"2019 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂú∞Âå∫Áîü‰∫ßÊÄªÂÄº & Á¨¨‰∏Ä‰∫ß‰∏öÊØîÈáç\", size(*1.1)) ///\n  subtitle(\"Êï∞ÊçÆÊù•Ê∫êÔºöCSMARÁªèÊµéÈáëËûçÊï∞ÊçÆÂ∫ì\") ///\n  graphr(margin(medium)) ///\n  osize(vvthin ...) ///\n  legend(size(*1.1) ///\n    order(2 \"Êó†Êï∞ÊçÆ\" 3 \"< 40ÂçÉ‰∫ø\" ///\n      4 \"40ÔΩû60ÂçÉ‰∫ø\" 5 \"60ÔΩû80ÂçÉ‰∫ø\" ///\n      6 \"80ÔΩû100ÂçÉ‰∫ø\" 7 \"> 100ÂçÉ‰∫ø\" ///\n      15 \"Á¨¨‰∏Ä‰∫ß‰∏öÊØîÈáç\")) ///\n  caption(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\", size(*0.8)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// ÂéªÈô§Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫ø(4) ËÉ°ÁÑïÂ∫∏Á∫øÔºà7Ôºâ\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  diagram(data(piedata) x(X) y(Y) v(Á¨¨‰∏Ä‰∫ß‰∏öÂç†GDPÊØîÈáç_ÁôæÂàÜÊØî) ///\n    type(frect) legenda(on) os(vvthin) ///\n      size(1.5) fc(\"252 141 98\") ///\n      oc(\"252 141 98\") refsize(none))\ngr export \"pic6.png\", replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204522.png)\n\n## ÂêÑÁúÅ‰∫∫Âè£ÂØÜÂ∫¶\n\nÊúÄÂêéÊàë‰ª¨ÂÜçÊºîÁ§∫‰∏ãËÉ°ÁÑïÂ∫∏Á∫øÁöÑÁî®Ê≥ïÔºö\n\n```stata\nuse ‰∏≠ÂõΩ‰∫∫Âè£Á©∫Èó¥ÂàÜÂ∏ÉÁúÅÁ∫ßÈù¢ÊùøÊï∞ÊçÆÈõÜ.dta, clear \nren ÁúÅ‰ªΩ ÁúÅ\nmerge m:1 ÁúÅ using chinaprov40_db.dta\nkeep if Âπ¥‰ªΩ == 2015 | missing(Âπ¥‰ªΩ)\nreplace ÂùáÂÄº = -1 if missing(ÂùáÂÄº)\ngrmap ÂùáÂÄº using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(-1 0 100 1000 2000 3000 4000) ///\n  fcolor(gray \"224 242 241\" \"178 223 219\" \"128 203 196\" \"77 182 172\" \"38 166 154\") ///\n  leg(order(2 \"Êó†Êï∞ÊçÆ\" 3 \"< 100 ‰∫∫/Âπ≥ÊñπÂÖ¨Èáå\" 4 \"100ÔΩû1000 ‰∫∫/Âπ≥ÊñπÂÖ¨Èáå\" 5 \"1000ÔΩû2000 ‰∫∫/Âπ≥ÊñπÂÖ¨Èáå\" 6 \"2000ÔΩû3000 ‰∫∫/Âπ≥ÊñπÂÖ¨Èáå\" 7 \"> 3000 ‰∫∫/Âπ≥ÊñπÂÖ¨Èáå\" 14 \"ËÉ°ÁÑïÂ∫∏Á∫ø\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// ÂéªÈô§Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫ø(4)\n    select(keep if inlist(group, 1, 2, 3, 5, 6, 7)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5 *1.2) ///\n    pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        \"227 26 28\" /// ËÉ°ÁÑïÂ∫∏Á∫øÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"2015 Âπ¥‰∏≠ÂõΩÂêÑÁúÅÂπ≥Âùá‰∫∫Âè£ÂØÜÂ∫¶\") ///\n  subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n  caption(\"Êï∞ÊçÆÊù•Ê∫êÔºö‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËµÑÊ∫êÁéØÂ¢ÉÁßëÂ≠¶‰∏éÊï∞ÊçÆ‰∏≠ÂøÉ\", size(*0.8))\ngr export pic7.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204537.png)\n\n## Ë∞ÉÊï¥ÊåáÂåóÈíàÂíåÊØî‰æãÂ∞∫ÁöÑ‰ΩçÁΩÆ\n\nÊåáÂåóÈíàÂíåÊØî‰æãÂ∞∫ÈÉΩÂåÖÊã¨‰∏â‰∏™ÈÉ®ÂàÜÔºåline„ÄÅpolygon Âíå labelÔºåÊâÄ‰ª•Êàë‰ª¨Ë∞ÉÊï¥ÊåáÂåóÈíàÂíåÊØî‰æãÂ∞∫ÁöÑ‰ΩçÁΩÆ‰πüÈúÄË¶Å‰øÆÊîπËøô‰∏â‰∏™Êï∞ÊçÆÈáåÈù¢ÁöÑÂØπÂ∫îÂùêÊ†áÔºåÂú®Ë∞ÉÊï¥ÂùêÊ†áÂâçÊàë‰ª¨ÂèØ‰ª•ÁªôÁªòÂõæËØ≠Âè•Âä†‰∏ä freestyle ÈÄâÈ°πÊù•Â∏ÆÂä©Á°ÆÂÆöË¶ÅÁßªÂä®ÁöÑÊï∞ÂÄºÔºö\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204555.png)\n\nÁÑ∂ÂêéÂ∞±ÂèØ‰ª•ÈÄöËøá‰øÆÊîπÊï∞ÊçÆÊù•Ë∞ÉÊï¥ÊåáÂåóÈíàÂíåÊØî‰æãÂ∞∫ÁöÑ‰ΩçÁΩÆ‰∫ÜÔºö\n\n```stata\n* ÁßªÂä®ÊåáÂåóÈíàÁöÑ‰ΩçÁΩÆÂà∞Âè≥‰∏äÊñπ\nuse chinaprov40_line_db.dta, clear\n* ÊåáÂåóÈíàÂØπÂ∫îÁöÑ ID ÊòØ 40 Âíå 41\nuse chinaprov40_line_coord.dta, clear\nreplace _X = _X + 3000000 if inlist(_ID, 40, 41)\nreplace _Y = _Y + 4000000 if inlist(_ID, 40, 41)\nsave chinaprov40_line_coord2.dta, replace \n\nuse polygon, clear\nreplace _X = _X + 3000000 if _ID == 38\nreplace _Y = _Y + 4000000 if _ID == 38\nsave polygon2, replace\n\nuse chinaprov40_label, clear\nreplace X = X + 3000000 if cname == \"N\"\nreplace Y = Y + 4000000 if cname == \"N\"\nsave chinaprov40_label2, replace \n\nuse chinaprov40_db.dta, clear \nencode Á±ªÂûã, gen(type)\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"‰∏çÁªüËÆ°\" 3 \"ÁâπÂà´Ë°åÊîøÂå∫\" 4 \"Áõ¥ËæñÂ∏Ç\" 5 \"ÁúÅ\" 6 \"Ëá™Ê≤ªÂå∫\" 11 \"Áß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø\" 14 \"ËÉ°ÁÑïÂ∫∏Á∫ø\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord2.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        \"24 188 156\" /// Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        \"227 26 28\" /// ËÉ°ÁÑïÂ∫∏Á∫øÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon2) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label2) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"‰ΩøÁî® Stata ÁªòÂà∂ 2019 Âπ¥‰∏≠ÂõΩÁúÅÁ∫ßË°åÊîøÂå∫Âàí\") ///\n  subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n  caption(\"ÁâàÊú¨Ôºö‰ΩøÁî® Stata ÁªòÂà∂‰∏≠ÂõΩÁúÅÁ∫ßÂú∞ÂõæÊï∞ÊçÆÂåÖ 4.0\", size(*0.8))\ngr export pic8.png, replace width(1200)\n```\n\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204623.png)\n\n\n```stata\n* Ë∞ÉËäÇÊØî‰æãÂ∞∫ÁöÑ‰ΩçÁΩÆÔºàÂæÆÂæÆ‰∏äÁßªÔºâ\nuse chinaprov40_line_db.dta, clear\n* ÊØî‰æãÂ∞∫ÂØπÂ∫îÁöÑ ID ÊòØ 42 Âíå 43\nuse chinaprov40_line_coord2.dta, clear\nreplace _Y = _Y + 200000 if inlist(_ID, 42, 43)\nsave chinaprov40_line_coord3.dta, replace \n\nuse polygon2, clear\nreplace _Y = _Y + 200000 if _ID == 39\nsave polygon3, replace\n\nuse chinaprov40_label2, clear\nreplace Y = Y + 200000 if cname == \"1000km\"\nsave chinaprov40_label3, replace \n\nuse chinaprov40_db.dta, clear \nencode Á±ªÂûã, gen(type)\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"‰∏çÁªüËÆ°\" 3 \"ÁâπÂà´Ë°åÊîøÂå∫\" 4 \"Áõ¥ËæñÂ∏Ç\" 5 \"ÁúÅ\" 6 \"Ëá™Ê≤ªÂå∫\" 11 \"Áß¶Â≤≠-Ê∑ÆÊ≤≥Á∫ø\" 14 \"ËÉ°ÁÑïÂ∫∏Á∫ø\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord3.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// ÁúÅÁïåÈ¢úËâ≤\n        black /// ÂõΩÁïåÁ∫øÈ¢úËâ≤\n        \"0 85 170\" /// Êµ∑Â≤∏Á∫øÈ¢úËâ≤\n        \"24 188 156\" /// Áß¶Â≤≠Ê∑ÆÊ≤≥Á∫øÈ¢úËâ≤\n        black /// Â∞èÂú∞ÂõæÊ°ÜÊ†ºÈ¢úËâ≤\n        black /// ÊØî‰æãÂ∞∫ÂíåÊåáÂåóÈíàÈ¢úËâ≤\n        \"227 26 28\" /// ËÉ°ÁÑïÂ∫∏Á∫øÈ¢úËâ≤\n        )) ///\n  polygon(data(polygon3) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label3) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"‰ΩøÁî® Stata ÁªòÂà∂ 2019 Âπ¥‰∏≠ÂõΩÁúÅÁ∫ßË°åÊîøÂå∫Âàí\") ///\n  subti(\"ÁªòÂà∂ÔºöÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\") ///\n  caption(\"ÁâàÊú¨Ôºö‰ΩøÁî® Stata ÁªòÂà∂‰∏≠ÂõΩÁúÅÁ∫ßÂú∞ÂõæÊï∞ÊçÆÂåÖ 4.0\", size(*0.8))\ngr export pic9.png, replace width(1200)\n```\n\n![](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204639.png)\n\n\n------------\n\n<h4 align=\"center\">\n\nCode of Conduct\n\n</h4>\n\n<h6 align=\"center\">\n\nPlease note that this project is released with a [Contributor Code of\nConduct](CODE_OF_CONDUCT.md).<br>By participating in this project you\nagree to abide by its terms.\n\n</h6>\n\n<h4 align=\"center\">\n\nLicense\n\n</h4>\n\n<h6 align=\"center\">\n\nMIT ¬© ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ RStata\n\n</h6>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "charts",
        "stata",
        "maps",
        "maps stata",
        "provincial maps",
        "charts maps"
      ],
      "category": "scientific-research-tools"
    },
    "akalaric--mcp-wolframalpha": {
      "owner": "akalaric",
      "name": "mcp-wolframalpha",
      "url": "https://github.com/akalaric/mcp-wolframalpha",
      "imageUrl": "/freedevtools/mcp/pfp/akalaric.webp",
      "description": "Integrates Wolfram Alpha into chat applications to perform computational queries and retrieve structured knowledge, enhancing conversational capabilities with large language models. Supports a modular architecture for easy extension and multiple client interactions.",
      "stars": 44,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T21:33:50Z",
      "readme_content": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha‚Äôs knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n  Install the required dependencies with uv:\n  Ensure [`uv`](https://github.com/astral-sh/uv) is installed.\n\n   ```bash\n   uv sync\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n![UI](configs/gradio_ui.png)\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n## Contact\n\nFeel free to give feedback. The e-mail address is shown if you execute this in a shell:\n\n```sh\nprintf \"\\x61\\x6b\\x61\\x6c\\x61\\x72\\x69\\x63\\x31\\x40\\x6f\\x75\\x74\\x6c\\x6f\\x6f\\x6b\\x2e\\x63\\x6f\\x6d\\x0a\"\n```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolfram",
        "wolframalpha",
        "chat",
        "alpha chat",
        "chat applications",
        "wolfram alpha"
      ],
      "category": "scientific-research-tools"
    },
    "alti3--stk-mcp": {
      "owner": "alti3",
      "name": "stk-mcp",
      "url": "https://github.com/alti3/stk-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alti3.webp",
      "description": "Facilitates interaction between AI models and Ansys/AGI STK for advanced simulation tasks, enabling control over STK Desktop using natural language. Allows users to create simulation scenarios and manage satellite configurations through an intuitive MCP interface.",
      "stars": 16,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-23T06:47:53Z",
      "readme_content": "# STK-MCP\n\n[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue.svg)](https://www.python.org/downloads/) [![MCP Version](https://img.shields.io/pypi/v/mcp.svg)](https://pypi.org/project/mcp/)\n\nSTK-MCP is an MCP (Model Context Protocol) server designed to enable Large Language Models (LLMs) or other MCP clients to interact with [Ansys/AGI STK](https://www.ansys.com/products/missions/ansys-stk) (Systems Tool Kit) - the leading Digital Mission Engineering software.\n\nThis project allows controlling STK Desktop via MCP tools, leveraging the STK Python API. It utilizes `FastMCP` from the official [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk). The server logic resides within the `src/stk_mcp` package.\n\n## Overview\n\nThe primary goal of this project is to bridge the gap between programmatic or natural language interaction (via LLMs) and the powerful simulation capabilities of STK. By exposing STK functionalities through an MCP server, users can command STK simulations using MCP clients or potentially LLM-driven applications.\n\nThe MCP server, defined in `src/stk_mcp/server.py`, manages STK interaction and exposes functionality, showcasing how to:\n*   Connect to a running STK Desktop instance or launch a new one via the server's lifecycle management (`src/stk_mcp/stk_logic/core.py`).\n*   Expose STK operations as MCP tools (defined in `src/stk_mcp/tools/`).\n*   Create and configure STK scenarios using the `setup_scenario` tool.\n*   Programmatically add and configure Satellite objects using the `create_satellite` tool.\n*   Define satellite orbits using common orbital parameters.\n*   Utilize the TwoBody propagator for orbit simulation within the tools.\n\n## Features (MCP Server)\n\n*   **MCP Server Implementation:** Uses `FastMCP` within `src/stk_mcp/server.py`.\n*   **Modular Structure:** Code organized into server setup (`server.py`), STK interaction logic (`stk_logic/`), and MCP tools (`tools/`).\n*   **STK Desktop Integration:** Automatically connects to or starts STK 12 (requires Windows) when the server starts, managed via MCP lifespan.\n*   **Scenario Management Tool (`setup_scenario`):** Creates new scenarios, sets analysis time periods, and closes any pre-existing scenario.\n*   **Satellite Creation Tool (`create_satellite`):** Adds Satellite objects to the currently active scenario.\n*   **Orbit Definition:** Configures satellite orbits using Apogee/Perigee Altitude, RAAN, and Inclination.\n*   **TwoBody Propagation:** Sets up and runs the TwoBody propagator for created satellites.\n\n## Prerequisites\n\n*   **Operating System:** Windows (due to STK Desktop and `win32com` dependency).\n*   **Python:** Version 3.12 or higher.\n*   **Ansys/AGI STK:** Version 12.x Desktop installed.\n*   **STK Python API:** The `agi.stk12` Python wheel corresponding to your STK installation must be installed. This typically involves:\n    *   Locating the wheel file (e.g., `agi.stk12-py3-none-any.whl`) within your STK installation directory (often under `CodeSamples\\Automation\\Python`).\n    *   Installing it using pip: `uv pip install path/to/agi.stk12-py3-none-any.whl`\n*   **Project Dependencies:** Requires the `mcp` library (installed via `uv sync`).\n\n## Installation\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd stk-mcp\n    ```\n2.  **Ensure Prerequisites are met:** Install Python, STK Desktop, and the STK Python API wheel as described above.\n3.  **Install project dependencies:**\n    ```bash\n    # Install dependencies defined in pyproject.toml (includes mcp[cli])\n    uv sync\n    ```\n\n## Usage (MCP Server)\n\nThe MCP server logic is now structured within the `src/stk_mcp` directory. You interact with it using an MCP client or development tools.\n\n1.  **Ensure STK Desktop is closed** (the server will launch its own instance via the lifespan manager).\n2.  **Run the MCP server in development mode:**\n    *   **Option A (Using the helper script):**\n        ```bash\n        python scripts/run_dev.py\n        ```\n    *   **Option B (Directly using `mcp` command from the project root):**\n        ```bash\n        mcp dev src/stk_mcp/server.py\n        ```\n    *   The server will start, attempt to launch/connect to STK, and print logs.\n    *   STK Desktop should become visible.\n    *   The server listens for MCP connections.\n3.  **Interact with the Server (Example using MCP Inspector):**\n    *   Open the MCP Inspector URL provided in the console (e.g., `http://127.0.0.1:8765`).\n    *   Find the \"STK Control\" server.\n    *   Use the \"Tools\" section to execute `setup_scenario` and `create_satellite` as before.\n4.  **Stop the Server:** Press `Ctrl+C` in the terminal where the server is running. The lifespan manager will attempt to close STK.\n\n## MCP Tools Available\n\n*   **`setup_scenario`**: (Defined in `src/stk_mcp/tools/scenario.py`)\n    *   Description: Creates/Configures an STK Scenario. Closes any existing scenario first.\n    *   Arguments: `scenario_name` (str, default: \"MCP_STK_Scenario\"), `start_time` (str, default: \"20 Jan 2020 17:00:00.000\"), `duration_hours` (float, default: 48.0).\n*   **`create_satellite`**: (Defined in `src/stk_mcp/tools/satellite.py`)\n    *   Description: Creates or modifies an STK satellite. Assumes a scenario is active.\n    *   Arguments: `name` (str), `apogee_alt_km` (float), `perigee_alt_km` (float), `raan_deg` (float), `inclination_deg` (float).\n\n## Future Development (MCP Server & Expanded Automation)\n\nPlanned enhancements include:\n\n*   **Adding Ground Locations:** MCP tool to add a Facility or Place object.\n*   **Resource Endpoints:** Add MCP resources (e.g., `get_scenario_details`, `list_objects`) to query STK state.\n*   **Enhanced Satellite/Object Definition:** More comprehensive configuration options.\n*   **Access Analysis Tool:** Compute and report access between objects.\n*   **Error Handling:** More robust error reporting back to the MCP client.\n\n## Dependencies\n\n*   `agi.stk12`: For interacting with STK Desktop (Requires manual installation from STK).\n*   `mcp[cli]>=1.6.0`: Model Context Protocol library.\n*   `pywin32`: Automatically installed on Windows; used for COM interactions via `win32com.client`.\n\n## Limitations\n\n*   **Windows Only:** Relies heavily on STK Desktop automation via COM.\n*   **STK Desktop Required:** Does not currently support STK Engine.\n*   **Basic Functionality:** Currently limited to scenario setup and basic satellite creation.\n*   **Single Scenario Focus:** Assumes interaction with a single active scenario managed by the `setup_scenario` tool.\n\n## Contributing\n\nContributions are welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stk",
        "ai",
        "alti3",
        "alti3 stk",
        "agi stk",
        "stk advanced"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-logic": {
      "owner": "angrysky56",
      "name": "mcp-logic",
      "url": "https://github.com/angrysky56/mcp-logic",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides automated reasoning capabilities through Prover9 and Mace4, enabling logical theorem proving and model verification. Facilitates formal validation of knowledge representations and implications via a clean MCP interface.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Shell",
      "updated_at": "2025-10-03T23:37:00Z",
      "readme_content": "# MCP-Logic\n\nAn MCP server providing automated reasoning capabilities using Prover9/Mace4 for AI systems. This server enables logical theorem proving and logical model verification through a clean MCP interface.\n\n## Design Philosophy\n\nMCP-Logic bridges the gap between AI systems and formal logic by providing a robust interface to Prover9/Mace4. What makes it special:\n\n- **AI-First Design**: Built specifically for AI systems to perform automated reasoning\n- **Knowledge Validation**: Enables formal verification of knowledge representations and logical implications\n- **Clean Integration**: Seamless integration with the Model Context Protocol (MCP) ecosystem\n- **Deep Reasoning**: Support for complex logical proofs with nested quantifiers and multiple premises\n- **Real-World Applications**: Particularly useful for validating AI knowledge models and reasoning chains\n\n## Features\n\n- Seamless integration with Prover9 for automated theorem proving\n- Support for complex logical formulas and proofs\n- Built-in syntax validation\n- Clean MCP server interface\n- Extensive error handling and logging\n- Support for knowledge representation and reasoning about AI systems\n\n## Quick Example\n\n![image](https://github.com/user-attachments/assets/42756e3d-c2fa-475f-8e8a-25f7e444b2a4)\n\n```python\n# Prove that understanding + context leads to application\nresult = await prove(\n    premises=[\n        \"all x all y (understands(x,y) -> can_explain(x,y))\",\n        \"all x all y (can_explain(x,y) -> knows(x,y))\",\n        \"all x all y (knows(x,y) -> believes(x,y))\",\n        \"all x all y (believes(x,y) -> can_reason_about(x,y))\",\n        \"all x all y (can_reason_about(x,y) & knows_context(x,y) -> can_apply(x,y))\",\n        \"understands(system,domain)\",\n        \"knows_context(system,domain)\"\n    ],\n    conclusion=\"can_apply(system,domain)\"\n)\n# Returns successful proof!\n```\n\n![image](https://github.com/user-attachments/assets/61cecc1f-9ba1-4586-a6a2-83823088f763)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10+\n- UV package manager\n- Git for cloning the repository\n- CMake and build tools (for building LADR/Prover9)\n\n### Setup\n\nClone this repository\n\n```bash\ngit clone https://github.com/angrysky56/mcp-logic\ncd mcp-logic\n```\n\nRun the setup script:\nWindows run:\n\n```bash\nwindows-setup-mcp-logic.bat\n```\n\nLinux/macOS:\n\n```bash\nchmod +x linux-setup-script.sh\n./linux-setup-script.sh\n```\n\nThe setup script:\n\n- Checks for dependencies (git, cmake, build tools)\n- Downloads LADR (Prover9/Mace4) from the external repository: [laitep/LADR](https://github.com/laitep/ladr.git)\n- Builds the LADR library to create Prover9 binaries in the ladr/bin directory\n- Creates a Python virtual environment\n- Sets up configuration files for running with or without Docker\n\nIMPORTANT: The LADR directory is not included in the repository itself and will be installed through the setup script or manually.\n\n### Using Docker- no idea if this is working right, mainly designed for direct use with Claude Desktop\n\nIf you prefer to run with Docker this script:\n\n- Finds an available port\n- Activates the virtual environment\n- Runs the server with the correct paths to the installed Prover9\n\n```bash\n# Linux/macOS\n./run-mcp-logic.sh\n```\n\n```bash\n# Windows\nrun-mcp-logic.bat\n```\n\nThese scripts will build and run a Docker container with the necessary environment.\n\n### Claude Desktop Integration\n\nTo use MCP-Logic with Claude Desktop, use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-logic\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"/path/to/mcp-logic/src/mcp_logic\",\n        \"run\", \n        \"mcp_logic\", \n        \"--prover-path\", \n        \"/path/to/mcp-logic/ladr/bin\"\n      ]\n    }\n  }\n}\n```\n\nReplace \"/path/to/mcp-logic\" with your actual repository path.\n\n## Available Tools\n\n![image](https://github.com/user-attachments/assets/a78ba32a-c0a8-4e44-9429-b2c9af57129d)\n\n### prove\n\nRun logical proofs using Prover9:\n\n```json\n{\n  \"tool\": \"prove\",\n  \"arguments\": {\n    \"premises\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ],\n    \"conclusion\": \"mortal(socrates)\"\n  }\n}\n```\n\n### check-well-formed\n\nValidate logical statement syntax:\n\n```json\n{\n  \"tool\": \"check-well-formed\",\n  \"arguments\": {\n    \"statements\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ]\n  }\n}\n```\n\n## Documentation\n\nSee the [Documents](./Documents) folder for detailed analysis and examples:\n\n- [Knowledge to Application](./Documents/KnowledgeToApplication.md): A formal logical analysis of understanding and practical application in AI systems\n\n## Project Structure\n\n```markdown\nmcp-logic/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ mcp_logic/\n‚îÇ       ‚îî‚îÄ‚îÄ server.py   # Main MCP server implementation\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_proofs.py  # Core functionality tests\n‚îÇ   ‚îî‚îÄ‚îÄ test_debug.py   # Debug utilities\n‚îú‚îÄ‚îÄ Documents/          # Analysis and documentation\n‚îú‚îÄ‚îÄ pyproject.toml      # Python package config\n‚îú‚îÄ‚îÄ setup-script.sh     # Setup script (installs LADR & dependencies)\n‚îú‚îÄ‚îÄ run-mcp-logic.sh    # Docker-based run script (Linux/macOS)\n‚îú‚îÄ‚îÄ run-mcp-logic.bat   # Docker-based run script (Windows)\n‚îú‚îÄ‚îÄ run-mcp-logic-local.sh # Local run script (no Docker)\n‚îî‚îÄ‚îÄ README.md           # This file\n```\n\nNote: After running setup-script.sh, a \"ladr\" directory will be created containing the Prover9 binaries, but this directory is not included in the repository itself.\n\n## Development\n\nRun tests:\n\n```bash\nuv pip install pytest\nuv run pytest\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prover9",
        "mcp",
        "logic",
        "mcp logic",
        "automated reasoning",
        "logic provides"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-rocq": {
      "owner": "angrysky56",
      "name": "mcp-rocq",
      "url": "https://github.com/angrysky56/mcp-rocq",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides advanced logical reasoning capabilities through integration with the Coq proof assistant, enabling automated dependent type checking, inductive type definitions, and property proving with custom tactics and automation.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-23T14:43:03Z",
      "readme_content": "# MCP-RoCQ (Coq Reasoning Server)\n\n# Currently shows tools but Claude can't use it properly for some reason- invalid syntax generally seems the issue but there could be something else.\n\nThere may be a better way to set this up with the coq cli or something.\nAnyone want to try and fix it who knows what they are doing would be great.\n\nMCP-RoCQ is a Model Context Protocol server that provides advanced logical reasoning capabilities through integration with the Coq proof assistant. It enables automated dependent type checking, inductive type definitions, and property proving with both custom tactics and automation.\n\n## Features\n\n- **Automated Dependent Type Checking**: Verify terms against complex dependent types\n- **Inductive Type Definition**: Define and automatically verify custom inductive data types\n- **Property Proving**: Prove logical properties using custom tactics and automation\n- **XML Protocol Integration**: Reliable structured communication with Coq\n- **Rich Error Handling**: Detailed feedback for type errors and failed proofs\n\n## Installation\n\n1. Install the Coq Platform 8.19 (2024.10)\n\nCoq is a formal proof management system. It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs.\n\n[https://github.com/coq/platform](https://github.com/coq/platform)\n\n2. Clone this repository:\n\n```bash\ngit clone https://github.com/angrysky56/mcp-rocq.git\n```\n\ncd to the repo\n\n```bash\nuv venv\n./venv/Scripts/activate\nuv pip install -e .\n```\n\n# JSON for the Claude App or mcphost config- set your paths according to how you installed coq and the repository.\n\n```json\n    \"mcp-rocq\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"F:/GithubRepos/mcp-rocq\",\n        \"run\",\n        \"mcp_rocq\",\n        \"--coq-path\",\n        \"F:/Coq-Platform~8.19~2024.10/bin/coqtop.exe\",\n        \"--lib-path\",\n        \"F:/Coq-Platform~8.19~2024.10/lib/coq\"\n      ]\n    },\n```\n\n\n# This might work- I got it going with uv and most of this could be hallucinatory though:\n\n3. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nThe server provides three main capabilities:\n\n### 1. Type Checking\n\n```python\n{\n    \"tool\": \"type_check\",\n    \"args\": {\n        \"term\": \"<term to check>\",\n        \"expected_type\": \"<type>\",\n        \"context\": [\"relevant\", \"modules\"] \n    }\n}\n```\n\n### 2. Inductive Types\n\n```python\n{\n    \"tool\": \"define_inductive\",\n    \"args\": {\n        \"name\": \"Tree\",\n        \"constructors\": [\n            \"Leaf : Tree\",\n            \"Node : Tree -> Tree -> Tree\"\n        ],\n        \"verify\": true\n    }\n}\n```\n\n### 3. Property Proving\n\n```python\n{\n    \"tool\": \"prove_property\",\n    \"args\": {\n        \"property\": \"<statement>\",\n        \"tactics\": [\"<tactic sequence>\"],\n        \"use_automation\": true\n    }\n}\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rocq",
        "automated",
        "mcp",
        "rocq provides",
        "mcp rocq",
        "advanced logical"
      ],
      "category": "scientific-research-tools"
    },
    "blake365--macrostrat-mcp": {
      "owner": "blake365",
      "name": "macrostrat-mcp",
      "url": "https://github.com/blake365/macrostrat-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/blake365.webp",
      "description": "Query and analyze comprehensive geologic data using the Macrostrat API, providing access to geologic units, minerals, timescales, and more. Create visualizations based on geological data to enhance understanding.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-26T01:43:52Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/blake365-macrostrat-mcp-badge.png)](https://mseep.ai/app/blake365-macrostrat-mcp)\n\n# macrostrat-mcp\nMCP server for Macrostrat API\n\n[![smithery badge](https://smithery.ai/badge/@blake365/macrostrat-mcp)](https://smithery.ai/server/@blake365/macrostrat-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/v67anfiq0s\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v67anfiq0s/badge\" alt=\"Macrostrat Server MCP server\" />\n</a>\n\nAn MCP server implementation for providing access to the [Macrostrat API](https://macrostrat.org/api) within [Claude Desktop](https://claude.ai/download).\n\n## Overview\n\nThis Model Context Protocol (MCP) server enables AI assistants and applications to:\n\n- Query comprehensive geologic data and information provided by the Macrostrat API\n- Access geologic units, columns, minerals, timescales, and more\n- Get geologic map tiles that the LLM can interpret (questionable utility, but might be useful for some applications)\n- Analyze geologic data to improve geological understanding\n- Create graphics and perform data analysis from the data\n\nUsers can run queries using natural language.\n\n## Sample Output\n\n**Q. What is the geologic history of Nashville, TN?**\n\nA. \nLet me check the geological data for Nashville's location.\n\nView Result from find-units from macrostrat (local)\n```json\n{\n  \"lat\": 36.1627,\n  \"lng\": -86.7816,\n  \"responseType\": \"long\"\n}\n```\n\n### Geological History of Nashville, Tennessee\n1. Mesoproterozoic Basement (1.6-1.1 billion years ago)\n- Igneous basement rocks (granite and rhyolite)\n- Part of the ancient North American craton\n\n2. Early-Middle Cambrian (518-504 million years ago)\n- Rome Formation: Mixed carbonate-siliciclastic sediments\n- Shallow marine environment\n- Part of the developing continental shelf\n\n3. Middle-Late Cambrian (504-498 million years ago)\n- Conasauga Formation: Limestone and mixed sediments\n- Continued marine deposition\n- Alternating limestone and shale indicating fluctuating sea levels\n\n4. Late Cambrian-Early Ordovician (498-470 million years ago)\n- Knox Dolomite: Massive dolomite sequence\n- Extensive shallow marine platform\n- Up to 1,200 meters thick\n- Represents long period of stable marine conditions\n\n5. Middle-Late Ordovician (470-443 million years ago)\nSeveral important formations:\n- Stones River Group (including Murfreesboro, Pierce, Lebanon Limestones)\n- Nashville Group (including Hermitage, Bigby-Cannon, Catheys Formations)\n- Primarily limestone deposits\n- Rich in fossils\n- Shallow marine environment\n- Multiple cycles of sea level change\n\n6. Early-Middle Silurian (443-423 million years ago)\n- Brassfield through Decatur Formations\n- Continued marine limestone deposition\n- Various marine environments from shallow to deeper water\n\n7. Middle Devonian-Early Mississippian (390-357 million years ago)\n- Pegram Formation through Chattanooga Shale\n- Shift to more clastic (shale) deposition\n- Chattanooga Shale represents deeper, oxygen-poor conditions\n\n8. Early-Late Mississippian (357-331 million years ago)\nSeveral limestone formations:\n- Fort Payne Formation\n- Warsaw Limestone\n- St. Louis Limestone\n- Monteagle Limestone\n- Hartselle Formation\n- Return to carbonate platform conditions\n- Diverse marine environments\n\n9. Cenozoic to Present\n- Long period of erosion and exposure\n- Development of the current landscape\n- Formation of the Nashville Dome structure\n- River and stream deposits\n\n### Key Features\n- The area was predominantly a marine environment for most of its geological history\n- Dominated by limestone formations indicating warm, shallow seas\n- Multiple cycles of sea level change\n- Rich fossil record, especially in Ordovician rocks\n- Part of the Nashville Dome structural feature\n- Long periods of erosion have removed younger rocks\n- Current topography shaped by differential erosion of various limestone units\n\n### Present Day Bedrock\nThe bedrock of Nashville today consists primarily of Ordovician limestone formations exposed at the surface, particularly the Stones River and Nashville Group limestones. These rocks are well exposed in many areas and have influenced the development of local karst topography, including caves and sinkholes that are common in the region.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Macrostrat API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@blake365/macrostrat-mcp):\n\n```bash\nnpx -y @smithery/cli install @blake365/macrostrat-mcp --client claude\n```\n\n### Extra Steps\n\nIf you want to make changes to the server you can do so by editing the `src/index.ts` file.\n- Run `npm install` to install dependencies\n- Run `npm run build` to build the server\n- Quit and restart Claude Desktop after making changes\n\n\n## Connecting with Claude Desktop\n\n1. Open your Claude Desktop configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"macrostrat\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Full/Route/to/Folder/macrostrat/build/index.js\"\n            ]\n        }\n    }\n}\n```\n\n3. Close/Quit then restart Claude Desktop\n\nOnce you restart you should see an icon for search and tools. Click this button to see the MCP tools available.\n\n## Troubleshooting\n\nIf you get errors when running the server you may need to provide the full path to the `node` command. For example, on macOS: `/usr/local/bin/node`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geologic",
        "geological",
        "macrostrat",
        "geologic data",
        "geological data",
        "access geologic"
      ],
      "category": "scientific-research-tools"
    },
    "blazickjp--arxiv-mcp-server": {
      "owner": "blazickjp",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/blazickjp/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/blazickjp.webp",
      "description": "Provides access to arXiv research papers via a straightforward MCP interface, enabling AI models to search and retrieve academic articles.",
      "stars": 1743,
      "forks": 121,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T01:41:17Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## üìù Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "articles",
        "search",
        "research",
        "arxiv mcp",
        "academic articles",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "bmorphism--penrose-mcp": {
      "owner": "bmorphism",
      "name": "penrose-mcp",
      "url": "https://github.com/bmorphism/penrose-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Create mathematical diagrams using domain-specific languages that define mathematical types, objects, relationships, and visual representation rules.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T21:49:31Z",
      "readme_content": "# Penrose MCP Server\n\nA Model Context Protocol (MCP) server for [Penrose](https://penrose.cs.cmu.edu/) - Create beautiful mathematical diagrams through natural language.\n\n## Overview\n\nThis MCP server provides tools and resources for creating mathematical diagrams using Penrose's domain-specific languages:\n\n- **Domain (DSL)**: Define mathematical types and relationships\n- **Substance**: Describe mathematical objects and their relationships\n- **Style**: Specify visual representation rules\n\n## Project Structure\n\n- `.topos/`: Research materials and documentation (gitignored)\n  - `penrose-research/`: Design documents and specifications\n  - `mcp-examples/`: Reference MCP server implementations\n  - `mcp-spec/`: Official MCP protocol documentation\n\n## Development\n\nUse the justfile to access documentation and reference materials:\n\n```bash\n# List all available commands\njust --list\n\n# View the server architecture\njust architecture\n\n# Access MCP specifications\njust mcp-spec\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrams",
        "mathematical",
        "types",
        "mathematical diagrams",
        "mathematical types",
        "diagrams using"
      ],
      "category": "scientific-research-tools"
    },
    "cnitlrt--headless-ida-mcp-server": {
      "owner": "cnitlrt",
      "name": "headless-ida-mcp-server",
      "url": "https://github.com/cnitlrt/headless-ida-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cnitlrt.webp",
      "description": "Analyze binary files and manage functions and variables remotely using IDA Pro's headless mode. Provides tools for efficient remote operations through the Multi-Client Protocol.",
      "stars": 18,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-11T13:34:27Z",
      "readme_content": "# Acknowledgments\n\nThis project builds upon the work of:\n- Tools code adapted from [ida-pro-mcp](https://github.com/mrexodia/ida-pro-mcp) by mrexodia\n- Utilizes the [headless-ida](https://github.com/DennyDai/headless-ida) library by DennyDai\n\n# Headless IDA MCP Server\n\nIf you want to run the server directly as a cli app, rather than an IDA plugin interactively,you can chose it.\n\n## Project Description\n\nThis project uses IDA Pro's headless mode to analyze binary files and provides a suite of tools via MCP to manage and manipulate functions, variables, and more.\n\n## Prerequisites\n\n- Python 3.12 or higher\n- IDA Pro with headless support (idat) https://github.com/DennyDai/headless-ida\n\n## Installation\n\n1. Clone the project locally:\n\n   ```bash\n   git clone https://github.com/cnitlrt/headless-ida-mcp-server.git \n   cd headless-ida-mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   uv python install 3.12\n   uv venv --python 3.12\n   uv pip install -e .\n   ```\n\n## Configuration\n\n1. Copy the example environment file:\n   ```bash\n   cp .env_example .env\n   ```\n\n2. Configure the following environment variables in `.env`:\n\n   - `IDA_PATH`: Path to IDA Pro's headless executable (idat), e.g., `/home/ubuntu/idapro/idat`\n   - `PORT`: Port number for the MCP server, e.g., `8888`\n   - `HOST`: Host address for the MCP server, e.g., `127.0.0.1`\n   - `TRANSPORT`: MCP transport mode (`sse` or `stdio`)\n\n## Usage\n\n1. Start the server:\n   ```bash\n   uv run headless_ida_mcp_server\n   ```\n\n2. Connect to the server using an MCP client:\n\n   Debug it: \n   ```bash\n   npx -y @modelcontextprotocol/inspector\n   ```\n   or\n   ```json\n   {\n   \"mcpServers\": {\n      \"ida\": {\n         \"command\": \"/path/to/uv\",\n         \"args\": [\"--directory\",\"path/to/headless-ida-mcp-server\",\"run\",\"headless_ida_mcp_server\"]\n      }\n   }\n   }\n   ```\n![](./images/pic.png)\n\n![](./images/pic2.png)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "cnitlrt",
        "headless",
        "headless ida",
        "using ida",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "couchrishi--sd-for-designers": {
      "owner": "couchrishi",
      "name": "sd-for-designers",
      "url": "https://github.com/couchrishi/sd-for-designers",
      "imageUrl": "/freedevtools/mcp/pfp/couchrishi.webp",
      "description": "Automate the fine-tuning and deployment of custom Stable Diffusion models using a fully automated MLOps pipeline that integrates various Google Cloud components. The workflow facilitates Triggering, managing, and tracking training jobs while simplifying machine learning processes for model enhancement.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2023-08-21T14:02:15Z",
      "readme_content": "# sd-for-designers\nA fully automated workflow for triggering, running &amp; managing fine tuning, training &amp; deploying custom stable diffusion models using Vertex AI\n\n## Description\n\nSd-aa-S is a full automated MLOps pipeline for triggering, managing & tracking Stable diffusion finetuning jobs on GCP using GCP components such as Google Cloud Storage, Cloud Build, Cloud PubSub, Firestore, Cloud Run, Cloud Functions and Vertex AI. It aims to simplify the ML workflows for tuning Stable diffusion using different techniques, starting with Dreambooth. Support for Lora, ControlNet etc. coming soon. The project is targeted ML/Data Engineers, Data Scientists & anybody else interested in or on the road towards building a platform for finetuning stable diffusion at scale. \n\n## Three Parts\n\n# 1. The App part\n\n    1. Set up your Cloud Environment\n    2. Create a backend service for handling uploads to a GCS bucket\n        - Receive images from clients and store them under a predefined GCS bucket path\n        - Track the status of individual uploads in a Firestore collection\n        - Track the status of the overall upload job in a separate Firestore collection \n        - Once the job is compelted, publish the jobID as the message on a predefined PubSub topic\n    3. Deploy this backend service as a Cloud Run endpoint using Cloud build\n    4. Create a frontend portal to upload images using ReactJs\n    5. Deploy the frontend service on Cloud Run\n\n# 2. The Vertex AI part\n\n    1. Set up your Cloud Environment\n    2. Create a new custom container artifact for running the pipeline components\n    3. Create a new custom container artifact for running the training job itself \n    4. Create a Jupyter notebook outlining the Pipeline flow & components\n    5. Compile a YAML file from a Vertex AI workbench and store the precompiled YAML file under a GCS bucket path\n\n \n# 3. The Plumbing part\n\n    1. Set up your Cloud Environment\n    2. Create a cloud function that gets triggered every time the jobID is published on a predefinied topic (from 1st part)\n    3. Within the cloud function, the python code subscribes to the topic and triggers a Vertex AI pipeline job using the precomiled YAML file (from 2nd part)\n    4. The pipeline jobs finetunes the stable diffusion model using Dreambooth, uploads the new custom model to Model registy & deploys an endpoint\n    5. The job also updates Firestore with the status of the pipeline job from start to end",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mlops",
        "cloud",
        "pipeline",
        "automated mlops",
        "mlops pipeline",
        "google cloud"
      ],
      "category": "scientific-research-tools"
    },
    "daheepk--arxiv-paper-mcp": {
      "owner": "daheepk",
      "name": "arxiv-paper-mcp",
      "url": "https://github.com/daheepk/arxiv-paper-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/daheepk.webp",
      "description": "Facilitates efficient search and exploration of research papers from arXiv.org, enabling users to access detailed metadata, conduct keyword and author-based searches, and generate prompts for summarization and comparison. Provides updates on trending topics across various research categories.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T18:39:37Z",
      "readme_content": "# üß† arXiv Research Assistant MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@daheepk/arxiv-paper-mcp)](https://smithery.ai/server/@daheepk/arxiv-paper-mcp)\n\n\nThis project is an MCP (Model Context Protocol) server built to interact with the vast arXiv.org paper database.\n\nIt allows clients like **Claude AI** to search, explore, and compare arXiv papers efficiently ‚Äî all through a custom-built, local server. It‚Äôs built with **Python** and the **FastMCP** framework, and uses **uv** for lightweight package management.\n\n<table>\n  <tr>\n    <td>\n      <a href=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server\">\n        <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server/badge\" alt=\"arXiv Research Assistant Server MCP server\" />\n      </a>\n    </td>\n    <td style=\"vertical-align: top; padding-left: 10px;\">\n      <a href=\"https://mseep.ai/app/daheepk-arxiv-paper-mcp\">\n        <img src=\"https://mseep.net/pr/daheepk-arxiv-paper-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" width=\"140\" />\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n## ‚ú® Features\n\n- **üîç Keyword-based Paper Search**  \n  Search arXiv papers by keywords, with options to sort by relevance or most recent.\n\n- **üìö Latest Papers by Category**  \n  Specify an arXiv category code (e.g., `cs.AI`, `math.AP`) to fetch the most recent papers in that field.\n\n- **üìÑ Paper Details Lookup**  \n  Fetch detailed metadata using a paper's arXiv ID: title, authors, abstract, categories, DOI, PDF link, and more.\n\n- **üßë‚Äçüî¨ Author-based Paper Search**  \n  Retrieve a list of papers published by a specific author.\n\n- **üìä Trend Analysis (Experimental)**  \n  Get an overview of trending keywords or topics based on recent papers in a category (currently uses mock data).\n\n- **üìù Summarization Prompt Generator**  \n  Dynamically generate prompts that help LLMs summarize a selected paper more effectively.\n\n- **üÜö Comparison Prompt Generator**  \n  Provide two paper IDs to generate a structured prompt for comparing their content.\n\n---\n\n## üõ†Ô∏è Tech Stack\n\n- Python 3.11+\n- [FastMCP](https://github.com/modelcontextprotocol/fastmcp)\n- uv (for dependency & environment management)\n- requests (for API communication)\n- xml.etree.ElementTree (for parsing XML responses)\n\n---\n\n## üöÄ Getting Started\n\n### Installing via Smithery\n\nTo install arXiv Research Assistant MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-paper-mcp):\n\n```bash\nnpx -y @smithery/cli install arxiv-paper-mcp --client claude\n```\n\n### Installation from PyPI\n\n```bash\nuv pip install arxiv-paper-mcp\n```\n\n### üîß Clone the repository (for development)\n```bash\ngit clone https://github.com/daheepk/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n### üîß Install Dependencies (for development)\n\nUse `uv` to install all dependencies in editable mode:\n\n```bash\nuv pip install -e .\n```\n\n\n## ‚öôÔ∏è How to Run\n\n### ‚ñ∂Ô∏è Run the server (locally)\n\n```bash\narxiv-paper-mcp\n```\n\n## üîå Use with Claude\n\nTo use this MCP server with Claude, add the following JSON configuration to Claude's MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"arXivPaper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"arxiv-paper-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Project Structure\n```\narxiv-mcp-server/\n‚îú‚îÄ‚îÄ arxiv_mcp/              # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ app.py              # FastMCP app setup\n‚îÇ   ‚îú‚îÄ‚îÄ server.py           # Server entry point\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # arXiv API communication logic\n‚îÇ   ‚îú‚îÄ‚îÄ resources/          # MCP resources (categories, authors, etc.)\n‚îÇ   ‚îú‚îÄ‚îÄ tools/              # MCP tools (search, detail lookup, trends)\n‚îÇ   ‚îî‚îÄ‚îÄ prompts/            # Prompt templates (summarize, compare)\n‚îú‚îÄ‚îÄ pyproject.toml          # Project config & dependencies\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "searches",
        "arxiv",
        "research tools",
        "research papers",
        "papers arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "datalayer--earthdata-mcp-server": {
      "owner": "datalayer",
      "name": "earthdata-mcp-server",
      "url": "https://github.com/datalayer/earthdata-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/datalayer.webp",
      "description": "Enables efficient discovery and retrieval of NASA Earth Data datasets and granules, facilitating geospatial analysis and data exploration. Integrates with workflows for AI-powered analysis of Earthdata datasets.",
      "stars": 21,
      "forks": 6,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-09-19T09:33:45Z",
      "readme_content": "<!--\n  ~ Copyright (c) 2023-2024 Datalayer, Inc.\n  ~\n  ~ BSD 3-Clause License\n-->\n\n[![Datalayer](https://assets.datalayer.tech/datalayer-25.svg)](https://datalayer.io)\n\n[![Become a Sponsor](https://img.shields.io/static/v1?label=Become%20a%20Sponsor&message=%E2%9D%A4&logo=GitHub&style=flat&color=1ABC9C)](https://github.com/sponsors/datalayer)\n\n# ü™ê ‚ú® Earthdata MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/earthdata-mcp-server)](https://pypi.org/project/earthdata-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@datalayer/earthdata-mcp-server)](https://smithery.ai/server/@datalayer/earthdata-mcp-server)\n\nEarthdata MCP Server is a [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server implementation that provides tools to interact with [NASA Earth Data](https://www.earthdata.nasa.gov/). It enables efficient dataset discovery, retrieval and analysis for Geospatial analysis.\n\nüöÄ **NEW**: This server now includes all [Jupyter MCP Server](https://github.com/datalayer/jupyter-mcp-server) tools through composition, providing a unified interface for both Earth data discovery and analysis in Jupyter Notebooks.\n\n## üöÄ Key Features\n\n- **Efficient Data Retrieval**: Search and download Earthdata datasets\n- **Unified Interface**: Combines Earthdata research and Jupyter notebook manipulation tools for analysis\n\nThe following demo uses this MCP server to search for datasets and data granules on NASA Earthdata, download the data in Jupyter and run further analysis.\n\n<div>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <p>Analyzing Sea Level Rise with AI-Powered Geospatial Tools and Jupyter - Watch Video</p>\n  </a>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <img style=\"max-width:100%;\" src=\"https://cdn.loom.com/sessions/thumbnails/c2b5b05f548d4f1492d5c107f0c48dbc-598a84f02de7e74e-full-play.gif\">\n  </a>\n</div>\n\n## üèÅ Getting Started\n\nFor comprehensive setup instructions‚Äîincluding `Streamable HTTP` transport and advanced configuration‚Äîcheck out [the Jupyter MCP Server documentation](https://jupyter-mcp-server.datalayer.tech/). Or, get started quickly with `JupyterLab` and `stdio` transport here below.\n\n### 1. Set Up Your Environment\n\n```bash\npip install jupyterlab==4.4.1 jupyter-collaboration==4.0.2 ipykernel\npip uninstall -y pycrdt datalayer_pycrdt\npip install datalayer_pycrdt==0.12.17\n```\n\n### 2. Start JupyterLab\n\n```bash\n# make jupyterlab\njupyter lab --port 8888 --IdentityProvider.token MY_TOKEN --ip 0.0.0.0\n```\n\n### 3. Configure Your Preferred MCP Client\n\n> [!NOTE]\n>\n> Ensure the `port` of the `DOCUMENT_URL` and `RUNTIME_URL` match those used in the `jupyter lab` command.\n>\n> The `DOCUMENT_ID` which is the path to the notebook you want to connect to, should be relative to the directory where JupyterLab was started.\n>\n> In a basic setup, `DOCUMENT_URL` and `RUNTIME_URL` are the same. `DOCUMENT_TOKEN`, and `RUNTIME_TOKEN` are also the same and is actually the Jupyter Token.\n\n> [!NOTE]\n> \n> The `EARTHDATA_USERNAME` and `EARTHDATA_PASSWORD` environment variables are used for NASA Earthdata authentication to download datasets via the `earthaccess` library. See [NASA Earthdata Authentication](./docs/authentication.md) for more details.\n\n#### MacOS and Windows\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://host.docker.internal:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://host.docker.internal:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### Linux\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"--network=host\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://localhost:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://localhost:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\nThe server offers **15 tools total**: 3 Earthdata-specific tools plus 12 Jupyter notebook manipulation tools.\n\n### Earthdata Tools\n\n#### `search_earth_datasets`\n\n- Search for datasets on NASA Earthdata.\n- Input:\n  - search_keywords (str): Keywords to search for in the dataset titles.\n  - count (int): Number of datasets to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of dataset abstracts.\n\n#### `search_earth_datagranules`\n\n- Search for data granules on NASA Earthdata.\n- Input:\n  - short_name (str): Short name of the dataset.\n  - count (int): Number of data granules to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of data granules.\n\n#### `download_earth_data_granules`\n\n- Download Earth data granules from NASA Earth Data and integrate with Jupyter notebooks.\n- This tool combines earthdata search capabilities with jupyter notebook manipulation to create a seamless download workflow.\n- **Authentication**: Requires NASA Earthdata Login credentials (see [Authentication section](#nasa-earthdata-authentication))\n- Input:\n  - folder_name (str): Local folder name to save the data.\n  - short_name (str): Short name of the Earth dataset to download.\n  - count (int): Number of data granules to download.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: Success message with download code preparation details.\n\n### Jupyter Tools (Composed)\n\nThe following Jupyter notebook manipulation tools are available:\n\n- **`append_markdown_cell`**: Add markdown cells to notebooks\n- **`insert_markdown_cell`**: Insert markdown cells at specific positions\n- **`overwrite_cell_source`**: Modify existing cell content\n- **`append_execute_code_cell`**: Add and execute code cells\n- **`insert_execute_code_cell`**: Insert and execute code cells at specific positions\n- **`execute_cell_with_progress`**: Execute cells with progress monitoring\n- **`execute_cell_simple_timeout`**: Execute cells with timeout\n- **`execute_cell_streaming`**: Execute cells with streaming output\n- **`read_all_cells`**: Read all notebook cells\n- **`read_cell`**: Read specific notebook cells\n- **`get_notebook_info`**: Get notebook metadata\n- **`delete_cell`**: Delete notebook cells\n\nFor detailed documentation of the Jupyter tools, see the [Jupyter MCP Server documentation](https://github.com/datalayer/jupyter-mcp-server).\n\n## Prompts\n\n1. `download_analyze_global_sea_level` üÜï\n   - Generate a comprehensive workflow for downloading and analyzing Global Mean Sea Level Trend dataset.\n   - Uses both earthdata download tools and jupyter analysis capabilities.\n   - Returns: Detailed prompt for complete sea level analysis workflow.\n\n2. `sealevel_rise_dataset`\n   - Search for datasets related to sea level rise worldwide.\n   - Input:\n     - `start_year` (int): Start year to consider.\n      - `end_year` (int): End year to consider.\n   - Returns: Prompt correctly formatted.\n\n3. `ask_datasets_format`\n    - To ask about the format of the datasets.\n    - Returns: Prompt correctly formatted.\n\n## Building\n\n```bash\n# or run `docker build -t datalayer/earthdata-mcp-server .`\nmake build-docker\n```\n\nIf you prefer, you can pull the prebuilt images.\n\n```bash\nmake pull-docker\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthdata",
        "nasa",
        "datasets",
        "earthdata mcp",
        "earthdata datasets",
        "analysis earthdata"
      ],
      "category": "scientific-research-tools"
    },
    "doomdagadiggiedahdah--iacr-mcp-server": {
      "owner": "doomdagadiggiedahdah",
      "name": "iacr-mcp-server",
      "url": "https://github.com/doomdagadiggiedahdah/iacr-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/doomdagadiggiedahdah.webp",
      "description": "Provides a programmatic interface to retrieve and search for cryptographic research papers from the IACR Cryptology ePrint Archive, along with access to paper metadata.",
      "stars": 2,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-16T06:16:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/doomdagadiggiedahdah-iacr-mcp-server-badge.png)](https://mseep.ai/app/doomdagadiggiedahdah-iacr-mcp-server)\n\n# IACR Cryptology ePrint Archive MCP Server\n\n[![smithery badge](https://smithery.ai/badge/iacr-mcp-server)](https://smithery.ai/server/iacr-mcp-server)\n\n## Overview\n\nThis Model Context Protocol (MCP) server provides a programmatic interface to the IACR Cryptology ePrint Archive, enabling efficient retrieval of cryptographic research papers.\n\n<a href=\"https://glama.ai/mcp/servers/e2oh3a96de\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e2oh3a96de/badge\" alt=\"IACR Server MCP server\" /></a>\n\n## Features\n\n- üîç Search cryptographic papers\n- üìã Retrieve paper metadata\n- üîí Secure access to research publications\n\n## Prerequisites\n\n- Node.js (v16+)\n- npm or yarn\n\n## Installation\n\n### Installing via Smithery\n\nTo install IACR Cryptology ePrint Archive for Claude Desktop automatically via [Smithery](https://smithery.ai/server/iacr-mcp-server):\n\n```bash\nnpx -y @smithery/cli install iacr-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/yourusername/iacr-mcp-server.git\ncd iacr-mcp-server\nnpm install\n```\n\n## Configuration\n\nNo additional configuration is required. The server uses the IACR ePrint Archive's RSS feed for data retrieval.\n\n## Usage\n\n### Available Tools\n\n1. `search_papers`: Search for papers\n   - Parameters:\n     - `query`: Search term (required)\n     - `year`: Publication year (optional)\n     - `max_results`: Maximum number of results (default: 20)\n\n2. `get_paper_details`: Retrieve details for a specific paper\n   - Parameters:\n     - `paper_id`: Unique paper identifier (required)\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Disclaimer\n\nThis is an unofficial tool. Always refer to the original IACR Cryptology ePrint Archive for the most accurate and up-to-date research publications.\n\n## Contact\n\nFor issues, questions, or suggestions, please open a GitHub issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cryptology",
        "archive",
        "iacr",
        "iacr cryptology",
        "cryptology eprint",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "dougwithseismic--withseismic-mcp": {
      "owner": "dougwithseismic",
      "name": "withseismic-mcp",
      "url": "https://github.com/dougwithseismic/withseismic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/dougwithseismic.webp",
      "description": "Facilitates integration of AI models with various tools and prompts, allowing for rapid development and automated type safety in TypeScript without infrastructure management.",
      "stars": 14,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T12:23:11Z",
      "readme_content": "# D0UGWITHSEISMIC/WITHSEISMIC-MCP\n\n![WithSeismic MCP](withseismic-mcp.png)\n\n```\nSYSTEMS ONLINE ‚Ä¢ NEURAL INTERFACE ACTIVE ‚Ä¢ COMBAT DATA ANALYSIS ‚Ä¢ TACTICAL OVERLAY ENABLED ‚Ä¢ PERFORMANCE METRICS NOMINAL\n```\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)\n[![Node.js](https://img.shields.io/badge/Node.js-18.x-green.svg)](https://nodejs.org/)\n[![pnpm](https://img.shields.io/badge/pnpm-8.x-orange.svg)](https://pnpm.io/)\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da.svg?logo=discord&logoColor=white)](https://discord.gg/BX25FcS53N)\n[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/RTm6ec?referralCode=dougie)\n\nA production-ready MCP server template for hosting your own AI tools and prompts. Deploy remotely or run locally - built for developers who want to expose their tools to AI models without the infrastructure headaches.\n\n## ://WHO_IS_THIS_FOR?\n\nThis template is perfect for:\n\n- **AI Tool Developers**: Build and host your own tools that AI models can interact with\n- **Teams & Organizations**: Set up a shared MCP server for your AI applications\n- **Individual Developers**: Quick-start your MCP development with a production-ready setup\n- **API Providers**: Expose your services to AI models through a standardized interface\n\nWhether you want to:\n\n- üåê Host a remote MCP server that multiple clients can connect to\n- üîß Run locally via stdio for development and testing\n- üöÄ Deploy a production-ready MCP implementation\n- üõ†Ô∏è Build tools without worrying about infrastructure\n\nThis template provides everything you need to get started.\n\n## ://OVERVIEW\n\nThe MCP server provides two ways to expose your tools to AI models:\n\n1. **Remote Server Mode (SSE)**: Deploy as a remote server that multiple clients can connect to\n\n   - Perfect for production deployments\n     (`pnpm start:sse`)\n   - Supports multiple simultaneous connections\n   - Easy deployment to platforms like Railway\n   - Secure and scalable\n\n2. **Local Mode (stdio)**: Run locally for development and testing\n   (`pnpm start`)\n   - Great for local development\n   - Direct stdio communication\n   - Fast iteration and debugging\n\nKey Features:\n\n- **Tools**: Executable functions that models can call\n- **Prompts**: Template-based message generators\n- **Auto-Discovery**: Automatic registration of your components\n- **Type Safety**: Full TypeScript support throughout\n\n## ://WHY_USE_THIS_MCP_SERVER_TEMPLATE?\n\nWhile the Model Context Protocol (MCP) is in its early stages, one thing is clear: **rapid adoption requires frictionless development**. This implementation is built with a singular focus: **letting developers focus on building great tools, not infrastructure**.\n\n### INSTANT_DEPLOYMENT\n\n```bash\n# Option 1: One-Click Deploy\nClick the \"Deploy on Railway\" button above ‚òùÔ∏è\n\n# Option 2: Local Setup\npnpm install && pnpm dev\n```\n\n### ZERO_FRICTION_DEVELOPMENT\n\n- **No Boilerplate**: Define your tool, import it, and you're done. We handle the rest.\n- **Auto-Registration**: Tools and prompts are automatically discovered and registered.\n- **Type Safety**: Full TypeScript support with zero configuration.\n\n### BATTLE_TESTED_ARCHITECTURE\n\n- **Production Ready**: Built for reliability and performance.\n- **Scalable Design**: From simple tools to complex AI interactions.\n- **Error Resilient**: Robust error handling and debugging built-in.\n\n### DEVELOPER_EXPERIENCE_FIRST\n\n```typescript\n// That's it. This is all you need to create a tool.\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: z.object({ query: z.string() }),\n    outputSchema: z.object({ result: z.string() }),\n  },\n  async (args) => {\n    // Your logic here\n    return { result: \"Done!\" };\n  },\n);\n```\n\nWe handle:\n\n- ‚ö° Connection management\n- üîÑ Tool registration\n- üõ°Ô∏è Input validation\n- üì¶ State management\n- üö® Error handling\n- üìä Logging and metrics\n\nYou focus on:\n\n- üéØ Building amazing tools\n- üß† Creating powerful prompts\n- üöÄ Shipping features\n\n## ://GETTING_STARTED\n\n### INSTALLATION\n\n```bash\npnpm install\n```\n\n### RUNNING_THE_SERVER\n\nTwo modes are available:\n\n1. Standard mode (stdio):\n\n```bash\npnpm dev      # Development with hot reload\npnpm start    # Production\n```\n\n2. SSE (Server-Sent Events) mode:\n\n```bash\npnpm dev:sse  # Development with hot reload\npnpm start:sse # Production\n```\n\nWhen running in SSE mode, connect to: `http://localhost:3001/sse`\n\n## ://CORE_CONCEPTS\n\n### TOOLS\n\nTools are executable functions that models can invoke. Each tool:\n\n- Has defined input/output schemas using Zod\n- Is automatically registered with the registry\n- Can perform any operation (calculation, file I/O, API calls, etc.)\n\nExample tool:\n\n```typescript\nimport { z } from \"zod\";\nimport { Tool } from \"../core\";\n\nconst MyToolInputSchema = z.object({\n  param1: z.string().describe(\"Parameter description\"),\n});\n\nconst MyToolOutputSchema = z.object({\n  result: z.string().describe(\"Result description\"),\n});\n\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: MyToolInputSchema,\n    outputSchema: MyToolOutputSchema,\n  },\n  async (args) => {\n    const input = MyToolInputSchema.parse(args);\n    // Tool logic here\n    return { result: \"processed result\" };\n  },\n);\n```\n\n### PROMPTS\n\nPrompts are message generators that help structure model interactions. Each prompt:\n\n- Defines its argument schema\n- Generates messages in a consistent format\n- Is automatically registered with the registry\n\nExample prompt:\n\n```typescript\nimport { Prompt } from \"../core\";\n\nexport const myPrompt = new Prompt(\n  {\n    name: \"myPrompt\",\n    description: \"What my prompt does\",\n    arguments: [\n      {\n        name: \"arg1\",\n        description: \"Argument description\",\n        required: true,\n      },\n    ],\n  },\n  async (args) => {\n    return [\n      {\n        role: \"system\",\n        content: {\n          type: \"text\",\n          text: `Generated message using ${args.arg1}`,\n        },\n      },\n    ];\n  },\n);\n```\n\n## ://ADDING_NEW_COMPONENTS\n\n### Creating a New Tool\n\n1. Create a new file in `src/modules/tools/`\n2. Define your input/output schemas using Zod\n3. Create and export your tool instance\n4. Add the export to `src/modules/tools/index.ts`\n\nThe registry will automatically:\n\n- Register your tool\n- Make it available to models\n- Handle validation and error handling\n\n### Creating a New Prompt\n\n1. Create a new file in `src/modules/prompts/`\n2. Define your argument schema\n3. Create and export your prompt instance\n4. Add the export to `src/modules/prompts/index.ts`\n\nThe registry will automatically:\n\n- Register your prompt\n- Make it available to models\n- Handle message generation and errors\n\n## Architecture\n\n### Core Components\n\n- **Registry**: Central manager for all tools and prompts\n- **Tool**: Base class for executable functions\n- **Prompt**: Base class for message generators\n- **Server**: MCP protocol implementation\n\n### Auto-Registration\n\nThe system uses a singleton Registry pattern that:\n\n1. Automatically registers tools and prompts on import\n2. Provides type-safe access to components\n3. Handles all MCP protocol interactions\n\n### Error Handling\n\nThe system includes robust error handling:\n\n- Type validation via Zod schemas\n- Execution error wrapping\n- Detailed error messages for debugging\n\n## Development\n\n### Type Safety\n\nAll components use TypeScript for full type safety:\n\n- Input/output schemas are defined using Zod\n- Type inference for tool arguments and results\n- Comprehensive error types\n\n### Testing\n\nRun tests using:\n\n```bash\npnpm test\n```\n\n### Best Practices\n\n1. Always define clear input/output schemas\n2. Use descriptive names and documentation\n3. Handle errors gracefully\n4. Follow the TypeScript guidelines in the codebase\n\n## ://CONTRIBUTING\n\n```\nNEURAL INTERFACE DETECTED ‚Ä¢ INITIATING COLLABORATION PROTOCOLS ‚Ä¢ READY FOR UPLINK\n```\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on:\n\n- Development workflow\n- Code style guidelines\n- Pull request process\n- Issue reporting\n\nJoin our [Discord community](https://discord.gg/withseismic) to connect with other contributors!\n\n## ://SUPPORT\n\n```\nSUPPORT PROTOCOLS ACTIVE ‚Ä¢ COMMUNICATION CHANNELS OPEN ‚Ä¢ READY TO ASSIST\n```\n\n- üì´ Email: <hello@withseismic.com>\n- üí¨ Discord: [Join our server](https://discord.gg/BX25FcS53N)\n\n## ://LICENSE\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n¬© 2025 Doug, at WithSeismic dot com.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "withseismic",
        "typescript",
        "ai",
        "tools dougwithseismic",
        "withseismic mcp",
        "dougwithseismic withseismic"
      ],
      "category": "scientific-research-tools"
    },
    "emekaokoye--mcp-rdf-explorer": {
      "owner": "emekaokoye",
      "name": "mcp-rdf-explorer",
      "url": "https://github.com/emekaokoye/mcp-rdf-explorer",
      "imageUrl": "/freedevtools/mcp/pfp/emekaokoye.webp",
      "description": "Explore and analyze RDF data with a conversational interface, enabling execution of SPARQL queries in both local and endpoint modes for insights from knowledge graphs.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T07:49:44Z",
      "readme_content": "# RDF Explorer v1.0.0 \r\n\r\n## Overview\r\nA Model Context Protocol (MCP) server that provides conversational interface for the exploration and analysis of RDF (Turtle) based Knowledge Graph in Local File mode or SPARQL Endpoint mode. This server facilitates communication between AI applications (hosts/clients) and RDF data, making graph exploration and analyzing graph data through SPARQL queries. A perfect tool for knowledge graph research and AI data preparation. \r\n\r\n\r\n## Components\r\n\r\n### Tools\r\nThe server implements SPARQL queries and search functionality:\r\n\r\n- `execute_on_endpoint`\r\n   - Execute a SPARQL query directly on an external endpoint\r\n   - Input:\r\n     - `endpoint` (str): The SPARQL endpoint URL to query.\r\n     - `query` (str): The SPARQL query to execute.\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Query results as a newline-separated string, or an error message.\r\n\r\n- `sparql_query`\r\n   - Execute a SPARQL query on the current graph or active external endpoint\r\n   - Input:\r\n     - `query` (str): The SPARQL query to execute.\r\n     - `ctx` (Context): The FastMCP context object.\r\n     - `use_service` (bool): Whether to use a SERVICE clause for federated queries in local mode (default: True).\r\n   - Returns: Query results as a newline-separated string, or an error message.\r\n\r\n- `graph_stats`\r\n   - Calculate and return statistics about the graph in JSON format\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: JSON string containing graph statistics (e.g., triple count, unique subjects).\r\n\r\n- `count_triples`\r\n   - Count triples in the graph. Disabled in SPARQL Endpoint Mode; use a custom prompt instead.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Number of triples as a string, or an error message.\r\n\r\n\r\n- `full_text_search`\r\n   - Perform a full-text search on the graph or endpoint, avoiding proprietary syntax.\r\n   - Input:\r\n     - `search_term` (str): The term to search for.\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Search results as a newline-separated string, or an error message.\r\n\r\n\r\n- `health_check`\r\n   - Check the health of the triplestore connection.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: 'Healthy' if the connection is good, 'Unhealthy: <error>' otherwise.\r\n\r\n\r\n- `get_mode`\r\n   - Get the current mode of RDF Explorer. Useful for knowledge graph and semantic tech users to verify data source.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: A message indicating the mode and dataset or endpoint.\r\n\r\n\r\n### Resources\r\n\r\nThe server exposes the following resources:\r\n- `schema://all`: Retrieve schema information (classes and properties) from the graph.\r\n  - Returns: A newline-separated string of schema elements (classes and properties).\r\n\r\n- `queries://{template_name}`: Retrieve a predefined SPARQL query template by name.\r\n  - Returns: The SPARQL query string or 'Template not found'.\r\n\r\n- `explore://{query_name}`: Execute an exploratory SPARQL query by name and return results in JSON.\r\n  - `query_name` (str): The name of the exploratory query (e.g., 'classes', 'relationships/URI').\r\n  - Returns: JSON string of query results.\r\n\r\n- `explore://report`: Generate a Markdown report of exploratory queries.\r\n  - Returns: A Markdown-formatted report string.\r\n\r\n\r\n\r\n### Prompts\r\n\r\nThe server exposes the following prompts:\r\n- `analyze_graph_structure`: Initiate an analysis of the graph structure with schema data.\r\n  - Returns: A list of messages to guide graph structure analysis.\r\n\r\n- `find_relationships`: Generate a SPARQL query to find relationships for a given subject.\r\n  - Returns: A SPARQL query string to find relationships.\r\n\r\n- `text_to_sparql`: Convert a text prompt to a SPARQL query and execute it, with token limit checks.\r\n  - `prompt` (str): The text prompt to convert to SPARQL.\r\n  - Returns: Query results with usage stats, or an error message.\r\n \r\n\r\n\r\n\r\n## Setup\r\n\r\n## Configuration\r\n\r\n### Installing on Claude Desktop\r\nBefore starting make sure [Claude Desktop](https://claude.ai/download) is installed.\r\n1. Go to: Settings > Developer > Edit Config\r\n\r\n2. Add the following to your `claude_desktop_config.json`:\r\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\r\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\r\n\r\nTo use with a local RDF Turtle file, use this version with `--triple-file` args\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"rdf_explorer\": {\r\n      \"command\": \"C:\\\\path\\\\to\\\\venv\\\\Scripts\\\\python.exe\",\r\n      \"args\": [\"C:\\\\path\\\\to\\\\server.py\", \"--triple-file\", \"your_file.ttl\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo use with a SPARQL Endpoint, use this version with `--sparql-endpoint` args\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"rdf_explorer\": {\r\n      \"command\": \"C:\\\\path\\\\to\\\\venv\\\\Scripts\\\\python.exe\",\r\n      \"args\": [\"C:\\\\path\\\\to\\\\server.py\", \"--sparql-endpoint\", \"https://example.com/sparql\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n3. Restart Claude Desktop and start querying and exploring graph data.\r\n\r\n4. Prompt: \"what mode is RDF Explorer running?\"\r\n\r\n\r\n\r\n\r\n## Usage Examples\r\n\r\nHere are examples of how you can explore RDF data using natural language:\r\n\r\n### Querying Data in Local File Mode\r\n\r\nYou can ask questions like:\r\n- \"Show me all employees in the Sales department\"\r\n- \"Find the top 5 oldest customers\"\r\n- \"Who has purchased more than 3 products in the last month?\"\r\n- \"List all entities\" \r\n- \"Using the DBpedia endpoint, list 10 songs by Michael Jackson\" \r\n- \"Using the Wikidata endpoint, list 5 cities\"\r\n- \"count the triples\"\r\n- \"analyze the graph structure\"\r\n- \"Select ...\"\r\n- \"search '{text}' \"\r\n- \"find relationships of '{URI}'\"\r\n- \"what mode is RDF Explorer running?\"\r\n\r\n### Querying Data in SPARQL Endpoint Mode\r\n\r\nYou can ask questions like:\r\n- \"Using the DBpedia endpoint, list 10 songs by Michael Jackson\" \r\n- \"Using the Wikidata endpoint, list 5 cities\"\r\n- \"Select ...\"\r\n- \"search '{text}' \"\r\n- \"find relationships of '{URI}'\"\r\n- \"what mode is RDF Explorer running?\"\r\n\r\n## Development\r\n```\r\n# clone the repository\r\ngit clone https://github.com/emekaokoye/mcp-rdf-explorer.git\r\ncd mcp-rdf-explorer\r\n\r\n# setup a virtual environment\r\npython -m venv venv\r\nsource venv/bin/activate\r\n# windows: venv\\Scripts\\activate\r\n\r\n# install development dependencies\r\npip install -r requirements.txt\r\n\r\n# run tests\r\npytest test_rdf_explorer.py -v\r\n```\r\n\r\n## License\r\n\r\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [license](LICENSE) file in the project repository.\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sparql",
        "rdf",
        "queries",
        "analyze rdf",
        "rdf data",
        "rdf explorer"
      ],
      "category": "scientific-research-tools"
    },
    "fdrechsler--mcp-server-idapro": {
      "owner": "fdrechsler",
      "name": "mcp-server-idapro",
      "url": "https://github.com/fdrechsler/mcp-server-idapro",
      "imageUrl": "/freedevtools/mcp/pfp/fdrechsler.webp",
      "description": "Automate reverse engineering and binary analysis tasks using IDA Pro through a standardized interface. Execute scripts, retrieve binary information, and perform advanced analysis to enhance productivity in software analysis.",
      "stars": 72,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T02:08:23Z",
      "readme_content": "# IDA Pro MCP Server\n\nA Model Context Protocol (MCP) server that enables AI assistants to interact with IDA Pro for reverse engineering and binary analysis tasks.\n\n<a href=\"https://glama.ai/mcp/servers/@fdrechsler/mcp-server-idapro\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@fdrechsler/mcp-server-idapro/badge\" alt=\"IDA Pro Server MCP server\" />\n</a>\n\n## Overview\n\nThis project provides a bridge between AI assistants and IDA Pro, a popular disassembler and debugger used for reverse engineering software. It consists of three main components:\n\n1. **IDA Pro Remote Control Plugin** (`ida_remote_server.py`): An IDA Pro plugin that creates an HTTP server to remotely control IDA Pro functions.\n2. **IDA Remote Client** (`idaremoteclient.ts`): A TypeScript client for interacting with the IDA Pro Remote Control Server.\n3. **MCP Server** (`index.ts`): A Model Context Protocol server that exposes IDA Pro functionality to AI assistants.\n\n## Features\n\n- Execute Python scripts in IDA Pro from AI assistants\n- Retrieve information about binaries:\n  - Strings\n  - Imports\n  - Exports\n  - Functions\n- Advanced binary analysis capabilities:\n  - Search for immediate values in instructions\n  - Search for text strings in the binary\n  - Search for specific byte sequences\n  - Get disassembly for address ranges\n- Automate IDA Pro operations through a standardized interface\n- Secure communication between components\n\n## Prerequisites\n\n- IDA Pro 8.3 or later\n- Node.js 18 or later\n- TypeScript\n\n### Example usage ida_remote_server.py\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"script\":\"print(\\\"Script initialization...\\\")\"}' http://127.0.0.1:9045/api/execute\n{\"success\": true, \"output\": \"Script initialization...\\n\"}\n```\n\n### Example usage MCP Server\n\n![Roo Output](/image.png)\n\n## Installation\n\n### 1. Install the IDA Pro Remote Control Plugin\n\n1. Copy `ida_remote_server.py` to your IDA Pro plugins directory:\n   - Windows: `%PROGRAMFILES%\\IDA Pro\\plugins`\n   - macOS: `/Applications/IDA Pro.app/Contents/MacOS/plugins`\n   - Linux: `/opt/idapro/plugins`\n\n2. Start IDA Pro and open a binary file.\n\n3. The plugin will automatically start an HTTP server on `127.0.0.1:9045`.\n\n### 2. Install the MCP Server\n\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd ida-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n4. Configure the MCP server in your AI assistant's MCP settings file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"ida-pro\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/ida-server/dist/index.js\"],\n         \"env\": {}\n       }\n     }\n   }\n   ```\n\n## Usage\n\nOnce installed and configured, the MCP server provides the following tool to AI assistants:\n\n### run_ida_command\n\nExecutes an IDA Pro Python script.\n\n**Parameters:**\n- `scriptPath` (required): Absolute path to the script file to execute\n- `outputPath` (optional): Absolute path to save the script's output to\n\n**Example:**\n\n```python\n# Example IDA Pro script (save as /path/to/script.py)\nimport idautils\n\n# Count functions\nfunction_count = len(list(idautils.Functions()))\nprint(f\"Binary has {function_count} functions\")\n\n# Get the first 5 function names\nfunctions = list(idautils.Functions())[:5]\nfor func_ea in functions:\n    print(f\"Function: {ida_name.get_ea_name(func_ea)} at {hex(func_ea)}\")\n\n# Return data\nreturn_value = function_count\n```\n\nThe AI assistant can then use this script with:\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>run_ida_command</tool_name>\n<arguments>\n{\n  \"scriptPath\": \"/path/to/script.py\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_immediate_value\n\nSearches for immediate values in the binary's instructions.\n\n**Parameters:**\n- `value` (required): Value to search for (number or string)\n- `radix` (optional): Radix for number conversion (default: 16)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_immediate_value</tool_name>\n<arguments>\n{\n  \"value\": \"42\",\n  \"radix\": 10\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_text\n\nSearches for text strings in the binary.\n\n**Parameters:**\n- `text` (required): Text to search for\n- `caseSensitive` (optional): Whether the search is case sensitive (default: false)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_text</tool_name>\n<arguments>\n{\n  \"text\": \"password\",\n  \"caseSensitive\": false\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_byte_sequence\n\nSearches for a specific byte sequence in the binary.\n\n**Parameters:**\n- `bytes` (required): Byte sequence to search for (e.g., \"90 90 90\" for three NOPs)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_byte_sequence</tool_name>\n<arguments>\n{\n  \"bytes\": \"90 90 90\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_disassembly\n\nGets disassembly for an address range.\n\n**Parameters:**\n- `startAddress` (required): Start address for disassembly\n- `endAddress` (optional): End address for disassembly\n- `count` (optional): Number of instructions to disassemble\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_disassembly</tool_name>\n<arguments>\n{\n  \"startAddress\": \"0x401000\",\n  \"count\": 10\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_functions\n\nGets the list of functions from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_functions</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_exports\n\nGets the list of exports from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_exports</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_strings\n\nGets the list of strings from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_strings</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n## IDA Pro Remote Control API\n\nThe IDA Pro Remote Control Plugin exposes the following HTTP endpoints:\n\n- `GET /api/info`: Get plugin information\n- `GET /api/strings`: Get strings from the binary\n- `GET /api/exports`: Get exports from the binary\n- `GET /api/imports`: Get imports from the binary\n- `GET /api/functions`: Get function list\n- `GET /api/search/immediate`: Search for immediate values in instructions\n- `GET /api/search/text`: Search for text in the binary\n- `GET /api/search/bytes`: Search for byte sequences in the binary\n- `GET /api/disassembly`: Get disassembly for an address range\n- `POST /api/execute`: Execute Python script (JSON/Form)\n- `POST /api/executebypath`: Execute Python script from file path\n- `POST /api/executebody`: Execute Python script from raw body\n\n## Security Considerations\n\nBy default, the IDA Pro Remote Control Plugin only listens on `127.0.0.1` (localhost) for security reasons. This prevents remote access to your IDA Pro instance.\n\nIf you need to allow remote access, you can modify the `DEFAULT_HOST` variable in `ida_remote_server.py`, but be aware of the security implications.\n\n## Development\n\n### Building from Source\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Author\n\nFlorian Drechsler (@fdrechsler) fd@fdrechsler.com",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "idapro",
        "ida",
        "binary",
        "idapro automate",
        "server idapro",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "finite-sample--rmcp": {
      "owner": "finite-sample",
      "name": "rmcp",
      "url": "https://github.com/finite-sample/rmcp",
      "imageUrl": "/freedevtools/mcp/pfp/finite-sample.webp",
      "description": "Perform advanced econometric analyses using R, including linear regression, panel data modeling, and various diagnostic tests. Facilitate data-driven decision-making through sophisticated statistical analyses.",
      "stars": 183,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:17:45Z",
      "readme_content": "# RMCP: Statistical Analysis through Natural Conversation\n\n[![Python application](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml/badge.svg)](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml)\n[![PyPI version](https://img.shields.io/pypi/v/rmcp.svg)](https://pypi.org/project/rmcp/)\n[![Downloads](https://pepy.tech/badge/rmcp)](https://pepy.tech/project/rmcp)\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://finite-sample.github.io/rmcp/)\n[![License](https://img.shields.io/github/license/finite-sample/rmcp)](https://github.com/finite-sample/rmcp/blob/main/LICENSE)\n\n**Turn conversations into comprehensive statistical analysis** - A Model Context Protocol (MCP) server with 44 statistical analysis tools across 11 categories. RMCP enables AI assistants to perform sophisticated statistical modeling, econometric analysis, machine learning, time series analysis, and data science tasks through natural conversation.\n\n## üöÄ Quick Start (30 seconds)\n\n```bash\npip install rmcp\nrmcp start\n```\n\nThat's it! RMCP is now ready to handle statistical analysis requests via Claude Desktop or any MCP client.\n\n**üéØ [Working examples ‚Üí](examples/quick_start_guide.md)** | **üîß [Troubleshooting ‚Üí](#-quick-troubleshooting)**\n\n## ‚ú® What Can RMCP Do?\n\n### üìä **Regression & Economics** \nLinear regression, logistic models, panel data, instrumental variables ‚Üí *\"Analyze ROI of marketing spend\"*\n\n### ‚è∞ **Time Series & Forecasting**\nARIMA models, decomposition, stationarity testing ‚Üí *\"Forecast next quarter's sales\"*\n\n### üß† **Machine Learning** \nClustering, decision trees, random forests ‚Üí *\"Segment customers by behavior\"*\n\n### üìà **Statistical Testing**\nT-tests, ANOVA, chi-square, normality tests ‚Üí *\"Is my A/B test significant?\"*\n\n### üìã **Data Analysis**\nDescriptive stats, outlier detection, correlation analysis ‚Üí *\"Summarize this dataset\"*\n\n### üîÑ **Data Transformation**\nStandardization, winsorization, lag/lead variables ‚Üí *\"Prepare data for modeling\"*\n\n### üìä **Professional Visualizations**\nInline plots in Claude: scatter plots, histograms, heatmaps ‚Üí *\"Show me a correlation matrix\"*\n\n### üìÅ **Smart File Operations**\nCSV, Excel, JSON import with validation ‚Üí *\"Load and analyze my sales data\"*\n\n### ü§ñ **Natural Language Features**\nFormula building, error recovery, example datasets ‚Üí *\"Help me build a regression formula\"*\n\n**üëâ [See working examples ‚Üí](examples/quick_start_guide.md)**\n\n## üìä Real Usage with Claude\n\n### Business Analysis\n**You:** *\"I have sales data and marketing spend. Can you analyze the ROI?\"*\n\n**Claude:** *\"I'll run a regression analysis to measure marketing effectiveness...\"*\n\n**Result:** *\"Every $1 spent on marketing generates $4.70 in sales. The relationship is highly significant (p < 0.001) with R¬≤ = 0.979\"*\n\n### Economic Research\n**You:** *\"Test if GDP growth and unemployment follow Okun's Law using my country data\"*\n\n**Claude:** *\"I'll analyze the correlation between GDP growth and unemployment...\"*\n\n**Result:** *\"Strong support for Okun's Law: correlation r = -0.944. Higher GDP growth significantly reduces unemployment.\"*\n\n### Customer Analytics  \n**You:** *\"Predict customer churn using tenure and monthly charges\"*\n\n**Claude:** *\"I'll build a logistic regression model for churn prediction...\"*\n\n**Result:** *\"Model achieves 100% accuracy. Each additional month of tenure reduces churn risk by 11.3%. Higher charges increase churn risk by 3% per dollar.\"*\n\n## üì¶ Installation\n\n### Prerequisites\n- **Python 3.10+** \n- **R 4.0+** with packages: Install all at once:\n\n```r\ninstall.packages(c(\n  \"jsonlite\", \"plm\", \"lmtest\", \"sandwich\", \"AER\", \"dplyr\",\n  \"forecast\", \"vars\", \"urca\", \"tseries\", \"nortest\", \"car\",\n  \"rpart\", \"randomForest\", \"ggplot2\", \"gridExtra\", \"tidyr\", \n  \"rlang\", \"knitr\", \"broom\"\n))\n```\n\n### Install RMCP\n\n```bash\n# Standard installation\npip install rmcp\n\n# With HTTP transport support\npip install rmcp[http]\n\n# Development installation\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n```\n\n### Claude Desktop Integration\n\nAdd to your Claude Desktop MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"rmcp\": {\n      \"command\": \"rmcp\",\n      \"args\": [\"start\"]\n    }\n  }\n}\n```\n\n### Command Line Usage\n\n```bash\n# Start MCP server (for Claude Desktop)\nrmcp start\n\n# Start HTTP server (for web apps)\nrmcp serve-http --port 8080\n\n# Check installation\nrmcp --version\n```\n\n## üî• Key Features\n\n- **üéØ Natural Conversation**: Ask questions in plain English, get statistical analysis\n- **üìä Professional Output**: Formatted results with markdown tables and inline visualizations  \n- **üîí Production Ready**: Full MCP protocol compliance with HTTP transport and SSE\n- **‚ö° Fast & Reliable**: 100% test success rate across all scenarios\n- **üåê Multiple Transports**: stdio (Claude Desktop) and HTTP (web applications)\n- **üõ°Ô∏è Secure**: Controlled R execution with configurable permissions\n\n## üìö Documentation\n\n| Resource | Description |\n|----------|-------------|\n| **[Quick Start Guide](examples/quick_start_guide.md)** | Copy-paste ready examples with real data |\n| **[Economic Research Examples](examples/economic_research_example.md)** | Panel data, time series, advanced econometrics |\n| **[Time Series Examples](examples/advanced_time_series_example.md)** | ARIMA, forecasting, decomposition |\n| **[Image Display Examples](examples/image_display_example.md)** | Inline visualizations in Claude |\n| **[API Documentation](docs/)** | Auto-generated API reference |\n\n## üß™ Validation\n\nRMCP has been tested with real-world scenarios achieving **100% success rate**:\n\n- ‚úÖ **Business Analysts**: Sales forecasting with 97.9% R¬≤, $4.70 ROI per marketing dollar\n- ‚úÖ **Economists**: Macroeconomic analysis confirming Okun's Law (r=-0.944)  \n- ‚úÖ **Data Scientists**: Customer churn prediction with 100% accuracy\n- ‚úÖ **Researchers**: Treatment effect analysis with significant results (p<0.001)\n\n## ü§ù Contributing\n\nWe welcome contributions! \n\n```bash\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n\n# Run tests\npython tests/unit/test_new_tools.py\npython tests/e2e/test_claude_desktop_scenarios.py\n\n# Format code\nblack rmcp/\n```\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## üõ†Ô∏è Quick Troubleshooting\n\n**R not found?**\n```bash\n# macOS: brew install r\n# Ubuntu: sudo apt install r-base\nR --version\n```\n\n**Missing R packages?**\n```bash\nrmcp check-r-packages  # Check what's missing\n```\n\n**MCP connection issues?**\n```bash\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | rmcp start\n```\n\n**üìñ Need more help?** Check the [examples](examples/) directory for working code.\n\n## üôã Support\n\n- üêõ **Issues**: [GitHub Issues](https://github.com/finite-sample/rmcp/issues)\n- üí¨ **Discussions**: [GitHub Discussions](https://github.com/finite-sample/rmcp/discussions)\n- üìñ **Examples**: [Working examples](examples/quick_start_guide.md)\n\n---\n\n**Ready to turn conversations into statistical insights?** Install RMCP and start analyzing data through AI assistants today! üöÄ\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "econometric",
        "analyses",
        "rmcp",
        "econometric analyses",
        "advanced econometric",
        "statistical analyses"
      ],
      "category": "scientific-research-tools"
    },
    "gp-zhong--Physics": {
      "owner": "gp-zhong",
      "name": "Physics",
      "url": "https://github.com/gp-zhong/Physics",
      "imageUrl": "/freedevtools/mcp/pfp/gp-zhong.webp",
      "description": "A high-fidelity prototype for learning physics concepts through interactive 3D simulations and AI-assisted learning. It offers a data visualization feature to help students understand physical experiments more intuitively.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "HTML",
      "updated_at": "2025-04-05T09:09:24Z",
      "readme_content": "# Áâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫è - È´ò‰øùÁúüÂéüÂûã\n\nËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é HTML„ÄÅCSS Âíå JavaScript ÂºÄÂèëÁöÑÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÁöÑÈ´ò‰øùÁúüÂéüÂûã„ÄÇÈÄöËøáÊ®°ÊãüÁúüÂÆûÁöÑÁâ©ÁêÜÂÆûÈ™åÂú∫ÊôØÔºåÂ∏ÆÂä©Áî®Êà∑ÈÄöËøá‰∫íÂä®ÂíåËßÜËßâÂåñÁöÑÊñπÂºèÂ≠¶‰π†Áâ©ÁêÜÁü•ËØÜ„ÄÇ\n\n## È°πÁõÆÊ¶ÇËø∞\n\nÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÊó®Âú®ÈÄöËøá‰∫§‰∫íÂºè 3D Ê®°ÊãüÂÆûÈ™åÔºåÂ∏ÆÂä©Â≠¶ÁîüÊõ¥Áõ¥ËßÇÂú∞ÁêÜËß£ÂíåÂ≠¶‰π†Áâ©ÁêÜÊ¶ÇÂøµ„ÄÇÂéüÂûãÈááÁî®Áé∞‰ª£ÁÆÄÁ∫¶ÁöÑËÆæËÆ°È£éÊ†ºÔºåÂº∫Ë∞ÉÁ∫øÊù°ÂíåÂá†‰ΩïÂΩ¢Áä∂ÔºåËê•ÈÄ†È´òÁ´ØÁßëÊäÄÊÑüÔºåÈÄÇÂêàÂêÑÂπ¥ÈæÑÊÆµÁöÑÂ≠¶‰π†ËÄÖ„ÄÇ\n\n## ËÆæËÆ°ËßÑËåÉ\n\n- **Êï¥‰ΩìÈ£éÊ†º**ÔºöÁé∞‰ª£ÁÆÄÁ∫¶ÁöÑÈªëÁôΩÁÅ∞ÈÖçËâ≤ÔºåÂº∫Ë∞ÉÁ∫øÊù°ÂíåÂá†‰ΩïÂΩ¢Áä∂\n- **ÈÖçËâ≤ÊñπÊ°à**Ôºö\n  - ‰∏ªËâ≤Ë∞ÉÔºöÂ§©Á©∫Ëìù (#4A90E2) ÂíåÁôΩËâ≤ (#FFFFFF)\n  - ËæÖÂä©Ëâ≤ÔºöÊµÖÁªøËâ≤ (#7ED321)\n  - ÁÇπÁºÄËâ≤ÔºöÈªÑËâ≤ (#F8E71C)\n  - ËÉåÊôØËâ≤ÔºöÊµÖÁÅ∞ (#F5F5F5)\n  - ÊñáÊú¨Ëâ≤ÔºöÊ∑±ÁÅ∞ (#333333)\n- **Â≠ó‰ΩìÈÄâÊã©**ÔºöÊÄùÊ∫êÈªë‰Ωì/Noto Sans SC\n- **ËßÜËßâÂÖÉÁ¥†**ÔºöÁÆÄÊ¥ÅÁ∫øÊù°ÂõæÊ†áÔºåÈ´òË¥®ÈáèÁâπÂÜô‰∫ßÂìÅÂõæÁâáÔºåÁÆÄÂçïÂá†‰ΩïÂõæÂΩ¢ËÉåÊôØ\n\n## Â¶Ç‰ΩïËøêË°åÂéüÂûã\n\n1. ÂÖãÈöÜÊàñ‰∏ãËΩΩÊú¨È°πÁõÆ‰ª£Á†Å\n2. ‰ΩøÁî®Áé∞‰ª£ÊµèËßàÂô®ÔºàÂ¶Ç Chrome„ÄÅFirefox„ÄÅSafariÔºâÊâìÂºÄ `index.html` Êñá‰ª∂\n3. ‰Ω†Â∞ÜÁúãÂà∞ÂåÖÂê´ÊâÄÊúâÂéüÂûãÈ°µÈù¢ÁöÑÈ¢ÑËßàÈ°µÈù¢ÔºåÊØè‰∏™È°µÈù¢ÊòæÁ§∫Âú® iPhone 15 Pro ÁöÑÊ®°ÊãüÊ°ÜÊû∂‰∏≠\n4. ÁÇπÂáªÂêÑ‰∏™È°µÈù¢ÂèØ‰ª•Êü•ÁúãËØ¶ÁªÜÁöÑ‰∫§‰∫íÂéüÂûã\n\n## ÂåÖÂê´ÁöÑÈ°µÈù¢\n\n1. **ÂêØÂä®È°µ**ÔºöÂ±ïÁ§∫Â∫îÁî®ÂêçÁß∞„ÄÅlogoÂíåÂºÄÂßãÊåâÈíÆ\n2. **È¶ñÈ°µ/ÂÆûÈ™åÂ∫ì**ÔºöÂ±ïÁ§∫ÂàÜÁ±ªÂÆûÈ™å„ÄÅÊé®ËçêÂÆûÈ™åÂíåÊúÄËøë‰ΩøÁî®\n3. **ÂÆûÈ™åËØ¶ÊÉÖÈ°µ**ÔºöÂ±ïÁ§∫ÂÆûÈ™å‰ø°ÊÅØ„ÄÅ‰ªãÁªçÂíåÁõ∏ÂÖ≥Áü•ËØÜÁÇπ\n4. **3DÂÆûÈ™åÊìç‰ΩúÁïåÈù¢**ÔºöËá™Áî±ËêΩ‰ΩìÂÆûÈ™åÊ®°ÊãüÔºåÂèØË∞ÉÊï¥ÂêÑÁßçÁâ©ÁêÜÂèÇÊï∞\n5. **ÂÆûÈ™åÊï∞ÊçÆÂàÜÊûêÈ°µÈù¢**ÔºöÂ±ïÁ§∫ÂÆûÈ™åÊï∞ÊçÆÁöÑÂõæË°®ÂíåË°®Ê†ºÂàÜÊûê\n6. **AIËæÖÂä©Â≠¶‰π†ÂØπËØùÁïåÈù¢**ÔºöÊèê‰æõÊô∫ËÉΩÈóÆÁ≠îÂäüËÉΩËß£ÈáäÁâ©ÁêÜÊ¶ÇÂøµ\n7. **Ê∏∏ÊàèÂåñÊàêÂ∞±Á≥ªÁªü**ÔºöÂ±ïÁ§∫Áî®Êà∑Â≠¶‰π†ËøõÂ∫¶„ÄÅÂããÁ´†ÂíåÊåëÊàò‰ªªÂä°\n8. **‰∏™‰∫∫‰∏≠ÂøÉÔºàÊàëÁöÑÔºâ**ÔºöÂ±ïÁ§∫Áî®Êà∑‰ø°ÊÅØ„ÄÅÂ≠¶‰π†ÁªüËÆ°„ÄÅÂÆûÈ™åÂéÜÂè≤ÂíåÂäüËÉΩÂÖ•Âè£\n\n## Ê†∏ÂøÉÂäüËÉΩ\n\n- **ÂÆûÈ™åÂ∫ì‰∏éÂàÜÁ±ªÁÆ°ÁêÜ**ÔºöÁõ¥ËßÇÁöÑÂç°ÁâáÁïåÈù¢ÔºåÊñπ‰æøÁî®Êà∑Êü•ÊâæÂÆûÈ™å\n- **3D‰∫§‰∫íÂºèÂÆûÈ™åÊ®°Êãü**ÔºöÂèÇÊï∞ÂèØË∞ÉÊï¥ÁöÑËá™Áî±ËêΩ‰ΩìÂÆûÈ™åÊ®°Êãü\n- **ÂÆûÈ™åÊïôÂ≠¶‰∏éÂºïÂØº**ÔºöËØ¶ÁªÜÁöÑÂÆûÈ™å‰ªãÁªç„ÄÅÁõ∏ÂÖ≥Áü•ËØÜÁÇπÂíåÊ≠•È™§ÂºïÂØº\n- **Ê∏∏ÊàèÂåñÂ≠¶‰π†Á≥ªÁªü**ÔºöÊàêÂ∞±Á≥ªÁªüÂíåÂããÁ´†Â±ïÁ§∫ÊøÄÂèëÂ≠¶‰π†Âä®Âäõ\n- **Êï∞ÊçÆÂèØËßÜÂåñ‰∏éÊä•ÂëäÁîüÊàê**ÔºöÂõæË°®ÂíåÊï∞ÊçÆË°®Ê†ºÂ±ïÁ§∫ÂÆûÈ™åÁªìÊûú\n- **AIËæÖÂä©Â≠¶‰π†**ÔºöÊô∫ËÉΩÈóÆÁ≠îËß£ÂÜ≥Áâ©ÁêÜÈóÆÈ¢ò\n- **Á§æ‰∫§‰∏éÂçè‰ΩúÂäüËÉΩ**ÔºöÊéíË°åÊ¶úÂíåÊàêÂ∞±ÂàÜ‰∫´ÂäüËÉΩ\n- **‰∏™‰∫∫‰∏≠ÂøÉÁÆ°ÁêÜ**ÔºöÊü•ÁúãÂ≠¶‰π†Êï∞ÊçÆ„ÄÅÂÆûÈ™åÂéÜÂè≤ÂíåÁ≥ªÁªüËÆæÁΩÆ\n\n## ÊäÄÊúØÊ†à\n\n- HTML5\n- CSS3Ôºà‰ΩøÁî® TailwindCSSÔºâ\n- JavaScript\n- Font Awesome ÂõæÊ†áÂ∫ì\n- Chart.js Êï∞ÊçÆÂèØËßÜÂåñÂ∫ì\n\n## ÂêéÁª≠ÂºÄÂèëÂª∫ËÆÆ\n\n1. ‰ΩøÁî® React Êàñ Vue Á≠âÂâçÁ´ØÊ°ÜÊû∂ÈáçÊûÑÁïåÈù¢ÔºåÊèêÂçá‰ª£Á†ÅÂ§çÁî®ÊÄßÂíåÁª¥Êä§ÊÄß\n2. ÈõÜÊàê Three.js Âíå Cannon.js Á≠âÁâ©ÁêÜÂºïÊìéÂÆûÁé∞ÁúüÂÆûÁöÑ 3D Ê®°Êãü\n3. ‰ΩøÁî® ECharts Êàñ D3.js Â¢ûÂº∫Êï∞ÊçÆÂèØËßÜÂåñËÉΩÂäõ\n4. Êé•ÂÖ•ÂÆûÈôÖÁöÑ AI Êé•Âè£ÂÆûÁé∞Êô∫ËÉΩÈóÆÁ≠îÂäüËÉΩ\n5. ‰ΩøÁî® WebSocket Êàñ WebRTC ÊäÄÊúØÂÆûÁé∞Â§ö‰∫∫Âçè‰ΩúÂäüËÉΩ\n\n## È¢ÑËßà\n\nÊâìÂºÄ `index.html` Êü•ÁúãÂÆåÊï¥ÁöÑÂéüÂûãÈ¢ÑËßà„ÄÇ\n\n## ‰ΩúËÄÖ\n\nÊú¨ÂéüÂûãÊòØÂü∫‰∫éÁâ©ÁêÜÊ®°ÊãüÂÆûÈ™åÂÆ§Â∞èÁ®ãÂ∫èÁöÑ‰Ωé‰øùÁúüÂéüÂûãËÆæËÆ°ÔºåÂçáÁ∫ß‰∏∫È´ò‰øùÁúüÂéüÂûãÁöÑËÆæËÆ°ÂÆûÁé∞„ÄÇ ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "physics",
        "simulations",
        "zhong",
        "zhong physics",
        "learning physics",
        "gp zhong"
      ],
      "category": "scientific-research-tools"
    },
    "gradusnikov--pubmed-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "pubmed-search-mcp-server",
      "url": "https://github.com/gradusnikov/pubmed-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Search and retrieve academic papers from the PubMed database, providing access to titles, authors, journals, abstracts, and DOIs.",
      "stars": 6,
      "forks": 5,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-07-08T04:10:26Z",
      "readme_content": "# PubMedSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/pubmed-search-mcp-server)](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server)\n\nA Model Content Protocol server that provides tools to search and retrieve academic papers from PubMed database.\n\n## Features\n\n- Search PubMed by keywords in title/abstract or author names\n- Retrieve detailed information including title, authors, journal, abstract, DOI and more\n\n### Installing via Smithery\n\nTo install pubmed-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/pubmed-search-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```\n   git clone <repository-url>\n   cd pubmed-search-mcp-server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests python-dotenv\n   ```\n\n3. Create a `.env` file in the project root (if needed for configuration)\n\n## Usage\n\n1. Start the server in development mode:\n   ```\n   mcp dev pubmed_search_mcp_server.py\n   ```\n\n2. or add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n   ```\n   \"pubmed-search\": {\n               \"command\": \"wsl.exe\",\n               \"args\": [\n                   \"bash\",\n                   \"-c\",\n                   \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && mcp run /home/[user]/pubmed-search-mpc-server/pubmed_search_mcp_server.py\"\n               ]\n           },\n   ```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "journals",
        "gradusnikov",
        "pubmed search",
        "gradusnikov pubmed",
        "pubmed database"
      ],
      "category": "scientific-research-tools"
    },
    "graphistry--graphistry-mcp": {
      "owner": "graphistry",
      "name": "graphistry-mcp",
      "url": "https://github.com/graphistry/graphistry-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/graphistry.webp",
      "description": "Provides GPU-accelerated graph visualization and advanced network analytics for complex network data, facilitating pattern discovery and community detection through a standardized API.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-24T09:08:33Z",
      "readme_content": "# Graphistry MCP Integration\n\nGPU-accelerated graph visualization and analytics for Large Language Models using Graphistry and MCP.\n\n## Overview\n\nThis project integrates Graphistry's powerful GPU-accelerated graph visualization platform with the Model Control Protocol (MCP), enabling advanced graph analytics capabilities for AI assistants and LLMs. It allows LLMs to visualize and analyze complex network data through a standardized, LLM-friendly interface.\n\n**Key features:**\n- GPU-accelerated graph visualization via Graphistry\n- Advanced pattern discovery and relationship analysis\n- Network analytics (community detection, centrality, path finding, anomaly detection)\n- Support for various data formats (Pandas, NetworkX, edge lists)\n- LLM-friendly API: single `graph_data` dict for graph tools\n\n## üö® Important: Graphistry Registration Required\n\n**This MCP server requires a free Graphistry account to use visualization features.**\n\n1. Sign up for a free account at [hub.graphistry.com](https://hub.graphistry.com)\n2. Set your credentials as environment variables or in a `.env` file before starting the server:\n   ```bash\n   export GRAPHISTRY_USERNAME=your_username\n   export GRAPHISTRY_PASSWORD=your_password\n   # or create a .env file with:\n   # GRAPHISTRY_USERNAME=your_username\n   # GRAPHISTRY_PASSWORD=your_password\n   ```\n   See `.env.example` for a template.\n\n## MCP Configuration (.mcp.json)\n\nTo use this project with Cursor or other MCP-compatible tools, you need a `.mcp.json` file in your project root. A template is provided as `.mcp.json.example`.\n\n**Setup:**\n\n```bash\ncp .mcp.json.example .mcp.json\n```\n\nEdit `.mcp.json` to:\n- Set the correct paths for your environment (e.g., project root, Python executable, server script)\n- Set your Graphistry credentials (or use environment variables/.env)\n- Choose between HTTP and stdio modes:\n  - `graphistry-http`: Connects via HTTP (set the `url` to match your server's port)\n  - `graphistry`: Connects via stdio (set the `command`, `args`, and `env` as needed)\n\n**Note:**\n- `.mcp.json.example` contains both HTTP and stdio configurations. Enable/disable as needed by setting the `disabled` field.\n- See `.env.example` for environment variable setup.\n\n## Installation\n\n### Recommended Installation (Python venv + pip)\n\n```bash\n# Clone the repository\ngit clone https://github.com/graphistry/graphistry-mcp.git\ncd graphistry-mcp\n\n# Set up virtual environment and install dependencies\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n\n# Set up your Graphistry credentials (see above)\n```\n\nOr use the setup script:\n\n```bash\n./setup-graphistry-mcp.sh\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\n# Activate your virtual environment if not already active\nsource .venv/bin/activate\n\n# Start the server (stdio mode)\npython run_graphistry_mcp.py\n\n# Or use the start script for HTTP or stdio mode (recommended, sources .env securely)\n./start-graphistry-mcp.sh --http 8080\n```\n\n### Security & Credential Handling\n\n- The server loads credentials from environment variables or `.env` using [python-dotenv](https://pypi.org/project/python-dotenv/), so you can safely use a `.env` file for local development.\n- The `start-graphistry-mcp.sh` script sources `.env` and is the most robust and secure way to launch the server.\n\n### Adding to Cursor (or other LLM tools)\n\n- Add the MCP server to your `.cursor/mcp.json` or equivalent config:\n  ```json\n  {\n    \"graphistry\": {\n      \"command\": \"/path/to/your/.venv/bin/python\",\n      \"args\": [\"/path/to/your/run_graphistry_mcp.py\"],\n      \"env\": {\n        \"GRAPHISTRY_USERNAME\": \"your_username\",\n        \"GRAPHISTRY_PASSWORD\": \"your_password\"\n      },\n      \"type\": \"stdio\"\n    }\n  }\n  ```\n- Make sure the virtual environment is used (either by using the full path to the venv's python, or by activating it before launching).\n- If you see errors about API version or missing credentials, double-check your environment variables and registration.\n\n### Example: Visualizing a Graph (LLM-friendly API)\n\nThe main tool, `visualize_graph`, now accepts a single `graph_data` dictionary. Example:\n\n```python\n{\n  \"graph_data\": {\n    \"graph_type\": \"graph\",\n    \"edges\": [\n      {\"source\": \"A\", \"target\": \"B\"},\n      {\"source\": \"A\", \"target\": \"C\"},\n      {\"source\": \"A\", \"target\": \"D\"},\n      {\"source\": \"A\", \"target\": \"E\"},\n      {\"source\": \"B\", \"target\": \"C\"},\n      {\"source\": \"B\", \"target\": \"D\"},\n      {\"source\": \"B\", \"target\": \"E\"},\n      {\"source\": \"C\", \"target\": \"D\"},\n      {\"source\": \"C\", \"target\": \"E\"},\n      {\"source\": \"D\", \"target\": \"E\"}\n    ],\n    \"nodes\": [\n      {\"id\": \"A\"}, {\"id\": \"B\"}, {\"id\": \"C\"}, {\"id\": \"D\"}, {\"id\": \"E\"}\n    ],\n    \"title\": \"5-node, 10-edge Complete Graph\",\n    \"description\": \"A complete graph of 5 nodes (K5) where every node is connected to every other node.\"\n  }\n}\n```\n\nExample (hypergraph):\n\n```python\n{\n  \"graph_data\": {\n    \"graph_type\": \"hypergraph\",\n    \"edges\": [\n      {\"source\": \"A\", \"target\": \"B\", \"group\": \"G1\", \"weight\": 0.7},\n      {\"source\": \"A\", \"target\": \"C\", \"group\": \"G1\", \"weight\": 0.6},\n      {\"source\": \"B\", \"target\": \"C\", \"group\": \"G2\", \"weight\": 0.8},\n      {\"source\": \"A\", \"target\": \"D\", \"group\": \"G2\", \"weight\": 0.5}\n    ],\n    \"columns\": [\"source\", \"target\", \"group\"],\n    \"title\": \"Test Hypergraph\",\n    \"description\": \"A simple test hypergraph.\"\n  }\n}\n```\n\n## Available MCP Tools\n\nThe following MCP tools are available for graph visualization, analysis, and manipulation:\n\n- **visualize_graph**: Visualize a graph or hypergraph using Graphistry's GPU-accelerated renderer.\n- **get_graph_ids**: List all stored graph IDs in the current session.\n- **get_graph_info**: Get metadata (node/edge counts, title, description) for a stored graph.\n- **apply_layout**: Apply a standard layout (force_directed, radial, circle, grid) to a graph.\n- **detect_patterns**: Run network analysis (centrality, community detection, path finding, anomaly detection).\n- **encode_point_color**: Set node color encoding by column (categorical or continuous).\n- **encode_point_size**: Set node size encoding by column (categorical or continuous).\n- **encode_point_icon**: Set node icon encoding by column (categorical, with icon mapping or binning).\n- **encode_point_badge**: Set node badge encoding by column (categorical, with icon mapping or binning).\n- **apply_ring_categorical_layout**: Arrange nodes in rings by a categorical column (e.g., group/type).\n- **apply_group_in_a_box_layout**: Arrange nodes in group-in-a-box layout (requires igraph).\n- **apply_modularity_weighted_layout**: Arrange nodes by modularity-weighted layout (requires igraph).\n- **apply_ring_continuous_layout**: Arrange nodes in rings by a continuous column (e.g., score).\n- **apply_time_ring_layout**: Arrange nodes in rings by a datetime column (e.g., created_at).\n- **apply_tree_layout**: Arrange nodes in a tree (layered hierarchical) layout.\n- **set_graph_settings**: Set advanced visualization settings (point size, edge influence, etc.).\n\n## Contributing\n\nPRs and issues welcome! This project is evolving rapidly as we learn more about LLM-driven graph analytics and tool integration.\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "graphistry",
        "graph",
        "visualization",
        "tools graphistry",
        "graphistry mcp",
        "graphistry graphistry"
      ],
      "category": "scientific-research-tools"
    },
    "hbg--mcp-paperswithcode": {
      "owner": "hbg",
      "name": "mcp-paperswithcode",
      "url": "https://github.com/hbg/mcp-paperswithcode",
      "imageUrl": "/freedevtools/mcp/pfp/hbg.webp",
      "description": "Interface with the PapersWithCode API to retrieve information about research papers, authors, conferences, and methods. Utilize helper tools for enhancing the information processing and searching academic papers.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-09T01:30:55Z",
      "readme_content": "# mcp-paperswithcode\n\n[![smithery badge](https://smithery.ai/badge/@hbg/mcp-paperswithcode)](https://smithery.ai/server/@hbg/mcp-paperswithcode)\n\n# ü¶æ Features\n\n> Allows AI assistants to find and read papers, as well as view related code repositories for further context.\n\nThis MCP server provides a Model Context Protocol (MCP) client that interfaces with the PapersWithCode API.\n\nIt includes tools for searching, retrieving, and parsing information on research papers, authors, datasets, conferences, and more.\n\n# üöÄ Getting Started\n\n### Installing via Smithery\n\nTo install mcp-paperswithcode for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hbg/mcp-paperswithcode):\n\n```bash\nnpx -y @smithery/cli install @hbg/mcp-paperswithcode --client claude\n```\n\n# üõ†Ô∏è Tools\n\n## üìö Paper Tools\n\n### `search_papers`\nSearch for papers using optional filters.\n\n- `abstract` (str, optional): Filter by abstract text.\n- `title` (str, optional): Filter by title text.\n- `arxiv_id` (str, optional): Filter by ArXiv ID.\n\n### `get_paper`\nGet a paper's metadata by its ID.\n\n- `paper_id` (str): The paper ID.\n\n### `read_paper_from_url`\nExtract readable text from a paper given its URL.\n\n- `paper_url` (str): The direct PDF or HTML URL to a paper.\n\n### `list_paper_results`\nList benchmark results associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_tasks`\nList tasks associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_methods`\nList methods discussed in a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_repositories`\nList code repositories linked to a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_datasets`\nList datasets mentioned or used in a paper.\n\n- `paper_id` (str): The paper ID.\n\n## üß† Research Area Tools\n\n### `search_research_areas`\nSearch research areas by name.\n\n- `name` (str): Partial or full name of the research area.\n\n### `get_research_area`\nGet metadata for a specific research area.\n\n- `area_id` (str): The area ID.\n\n### `list_research_area_tasks`\nList tasks associated with a research area.\n\n- `area_id` (str): The area ID.\n\n## üë®‚Äçüî¨ Author Tools\n\n### `search_authors`\nSearch authors by full name.\n\n- `full_name` (str): Full name of the author.\n\n### `get_paper_author`\nGet metadata for an author by ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_id`\nList all papers written by an author via ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_name`\nSearch by name and return papers for the first matching author.\n\n- `author_name` (str): Full name of the author.\n\n## üéì Conference Tools\n\n### `list_conferences`\nList conferences, optionally filter by name.\n\n- `conference_name` (str, optional): Full or partial name.\n\n### `get_conference`\nGet metadata for a specific conference.\n\n- `conference_id` (str): The conference ID.\n\n### `list_conference_proceedings`\nList all proceedings under a conference.\n\n- `conference_id` (str): The conference ID.\n\n### `get_conference_proceeding`\nGet details for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n\n### `list_conference_papers`\nList all papers for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperswithcode",
        "papers",
        "searching",
        "mcp paperswithcode",
        "paperswithcode interface",
        "paperswithcode api"
      ],
      "category": "scientific-research-tools"
    },
    "huanongfish--arxiv-mcp": {
      "owner": "huanongfish",
      "name": "arxiv-mcp",
      "url": "https://github.com/huanongfish/arxiv-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/huanongfish.webp",
      "description": "Search and access arXiv research papers through a simple interface, facilitating the retrieval and download of academic content for analysis. Streamline research with local storage and efficient access to extensive academic resources.",
      "stars": 6,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-02T06:37:47Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Message Control Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pear Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "arxiv",
        "retrieval",
        "arxiv research",
        "research tools",
        "huanongfish arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "hugohow--mcp-music-analysis": {
      "owner": "hugohow",
      "name": "mcp-music-analysis",
      "url": "https://github.com/hugohow/mcp-music-analysis",
      "imageUrl": "/freedevtools/mcp/pfp/hugohow.webp",
      "description": "Analyze audio files to extract features such as beat, duration, and MFCCs, including support for audio from local files, YouTube links, or direct audio links.",
      "stars": 16,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T23:39:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hugohow-mcp-music-analysis-badge.png)](https://mseep.ai/app/hugohow-mcp-music-analysis)\n\n# MCP Music Analysis\n[![smithery badge](https://smithery.ai/badge/@hugohow/mcp-music-analysis)](https://smithery.ai/server/@hugohow/mcp-music-analysis)\n\nThis repository contains a **Model Context Provider (MCP)** that uses MCP and [librosa](https://librosa.org/) for audio analysis on audio in local, youtube link, or audio link.\n\n## Usage with Claude Desktop\n\n<div style=\"display: flex; gap: 1rem;\">\n  <img src=\"public/screen.png\" alt=\"alt text\" width=\"40%\">\n  <img src=\"public/screen1.png\" alt=\"alt text\" width=\"40%\">\n</div>\n\n## Installation\n\n### Installing via Smithery\n\nTo install Music Analysis for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hugohow/mcp-music-analysis):\n\n```bash\nnpx -y @smithery/cli install @hugohow/mcp-music-analysis --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone repository\ngit clone git@github.com:hugohow/mcp-music-analysis.git\ncd mcp-music-analysis\n\n# Create virtual environment and install\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n```\n\n### Usage with Claude Desktop\n\n#### Locate Configuration File\n\nThe configuration file location depends on your operating system:\n\n- **macOS**:\n  ```\n  ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n- **Windows**:\n  ```\n  %APPDATA%\\Claude\\claude_desktop_config.json\n  ```\n\n- **Linux**:\n  ```\n  ~/.config/Claude/claude_desktop_config.json\n  ```\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"music-analysis\": {\n      \"command\": \"uvx\",\n      \"args\": [\"-n\", \"mcp-music-analysis\"]\n    }\n  }\n}\n```\n\n## Example Prompts\n\nHere are some sample prompts you might use in a conversational or chat-based context once the server is running. The MCP will understand these requests and execute the relevant tools:\n\n```\nCan you analyze the beat of /Users/hugohow-choong/Desktop/sample-6s.mp3?\nCould you give me the duration of https://download.samplelib.com/mp3/sample-15s.mp3 ?\nPlease compute the MFCC for this file: /path/to/another_audio.mp3\nWhat are the spectral centroid values for /path/to/music.wav?\nI'd like to know the onset times for https://www.youtube.com/watch?v=8HFiFd9vx1c\n```\n\n## To-Do List\n\n- [x] Add URL to audio file download\n- [x] Add YouTube to audio file transformation\n- [ ] Experiment with multiple Python environments (testing)\n- [ ] Improve installation guide\n- [ ] Integrate Whisper for lyrics\n- [ ] Implement a Docker solution\n\n## Author\n\nHugo How-Choong\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "mfccs",
        "audio",
        "mcp music",
        "analyze audio",
        "hugohow mcp"
      ],
      "category": "scientific-research-tools"
    },
    "iamleoluo--Quantum-Computing-Approach": {
      "owner": "iamleoluo",
      "name": "Quantum-Computing-Approach",
      "url": "https://github.com/iamleoluo/Quantum-Computing-Approach",
      "imageUrl": "/freedevtools/mcp/pfp/iamleoluo.webp",
      "description": "Simulate classical physics phenomena using quantum computing techniques and tools like Qiskit. Provides interactive visualizations and insights into complex systems such as superconductivity and magnetism.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2024-11-17T09:32:14Z",
      "readme_content": "# Quantum Computing Approach\n\n**Author:** Sheng Yun Wu  \n\n**Converted by:** Wei Che Lo \n\n**Course Website** [https://sites.google.com/gms.ndhu.edu.tw/qca](https://sites.google.com/gms.ndhu.edu.tw/qca)\n\n## Course Overview\n\nThe **Quantum Computing Approach** course introduces the application of quantum computing to simulate classical physics topics using qubits and quantum gates. It provides precise simulations of phenomena such as mechanics, thermodynamics, magnetism, and phase transitions.\n\n### Course Structure:\n- **Duration:** One to two semesters (12‚Äì15 weeks)\n- **Prerequisites:** Basic linear algebra, probability theory, and classical computing concepts\n- **Tools:** IBM Qiskit, Python\n\n## Introduction\n\nQuantum computing offers a revolutionary method for simulating and understanding classical physics topics by utilizing quantum mechanical principles like superposition, entanglement, and quantum interference. Classical physics often describes phenomena such as magnetism, thermodynamics, and phase transitions using deterministic models. Quantum computing, however, allows us to model these systems with unprecedented precision, capturing their quantum nature through qubits and quantum gates.\n\nThis approach allows quantum computers to simulate:\n- Spin systems\n- Wave-particle duality\n- Quantum tunneling\n- Quantum phase transitions\n- Superconductivity\n- Magnetism\n\nUsing frameworks like Qiskit, quantum computing provides tools for interactive simulations, enabling deeper insights into both classical and quantum phenomena.\n\n## Features\n\n- **Quantum Simulations**: Use quantum circuits to simulate classical physics phenomena.\n- **Interactive Visualizations**: Leverage Qiskit to visualize the evolution of quantum systems.\n- **Real-World Physics Applications**: Explore complex systems like superconductivity and magnetism with quantum models.\n\n## Installation\n\nTo set up the project, follow these steps:\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/iamleoluo/Quantum-Computing-Approach.git\n    ```\n2. Install the required dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n## Usage\n\nWorking on it, please wait.\n\n## Addition Note for Qiskit 1.0 feature changes\nQiskit 1.0 brings new stability guarantees and performance improvements, but it does break compatibility with all previous versions of Qiskit.\nfor more information please check out https://docs.quantum.ibm.com/migration-guides/qiskit-1.0-features\n\n## Contact\n\nFor more information, contact the **Modern NanoMag Lab**, founded by Professor Sheng Yun Wu:\n\n**Address:**  \nDepartment of Physics, National Dong Hwa University  \n1-12, Sec 2, Da-Hsuen Rd., Shou-Feng, Hualien, 974 Taiwan\n\n**Phone:** +886-3-8903732  \n**Fax:** +886-3-8900166  \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "qiskit",
        "computing",
        "quantum computing",
        "iamleoluo quantum",
        "qiskit provides"
      ],
      "category": "scientific-research-tools"
    },
    "jayanthbagare--mcp-scholarly": {
      "owner": "jayanthbagare",
      "name": "mcp-scholarly",
      "url": "https://github.com/jayanthbagare/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/jayanthbagare.webp",
      "description": "Search for accurate academic articles by keyword to facilitate research workflows. Integrates with applications to provide seamless retrieval of scholarly papers.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-05-06T10:07:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/adityak74-mcp-scholarly-badge.png)](https://mseep.ai/app/adityak74-mcp-scholarly)\n\n# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n![demo1.jpeg](examples/demo1.png)\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-202551](https://github.com/user-attachments/assets/f22d5796-017c-4c53-b230-101a09a28118)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "search",
        "retrieval scholarly",
        "scholarly search",
        "scholarly papers"
      ],
      "category": "scientific-research-tools"
    },
    "jigarbhoye04--MatlabMCP": {
      "owner": "jigarbhoye04",
      "name": "MatlabMCP",
      "url": "https://github.com/jigarbhoye04/MatlabMCP",
      "imageUrl": "/freedevtools/mcp/pfp/jigarbhoye04.webp",
      "description": "Execute MATLAB code seamlessly from Python, enabling efficient computation and data analysis with a shared MATLAB session that optimizes performance and resource utilization.",
      "stars": 20,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T09:06:12Z",
      "readme_content": "# MATLAB MCP Integration\n\nThis is an implementation of a Model Context Protocol (MCP) server for MATLAB. It allows MCP clients (like LLM agents or Claude Desktop) to interact with a shared MATLAB session using the MATLAB Engine API for Python.\n\n## Features\n\n*   **Execute MATLAB Code:** Run arbitrary MATLAB code snippets via the `runMatlabCode` tool.\n*   **Retrieve Variables:** Get the value of variables from the MATLAB workspace using the `getVariable` tool.\n*   **Structured Communication:** Tools return results and errors as structured JSON for easier programmatic use by clients.\n*   **Non-Blocking Execution:** MATLAB engine calls are run asynchronously using `asyncio.to_thread` to prevent blocking the server.\n*   **Standard Logging:** Uses Python's standard `logging` module, outputting to `stderr` for visibility in client logs.\n*   **Shared Session:** Connects to an existing shared MATLAB session.\n\n> [!TIP]\n> MatlabMCP wikipedia by DEVIN.\n Checkout [DeepWiki](https://deepwiki.com/jigarbhoye04/MatlabMCP) for more detailed and illustrative information about architecture.\n\n## Requirements\n\n*   Python 3.12 or higher\n*   MATLAB (**R2023a or higher recommended** - check MATLAB Engine API for Python compatibility) with the MATLAB Engine API for Python installed.\n*   `numpy` Python package.\n\n## Installation\n\n1.  Clone this repository:\n    ```bash\n    git clone https://github.com/jigarbhoye04/MatlabMCP.git\n    cd MatlabMCP\n    ```\n\n2.  Set up a Python virtual environment (recommended):\n    ```bash\n    # Install uv if you haven't already: https://github.com/astral-sh/uv\n    uv init\n    uv venv\n    source .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate\n    ```\n\n3.  Install dependencies:\n    ```bash\n    uv pip sync\n    ```\n\n4.  Ensure MATLAB is installed and the MATLAB Engine API for Python is configured for your Python environment. See [MATLAB Documentation](https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html).\n\n5.  **Start MATLAB and share its engine:** Run the following command in the MATLAB Command Window:\n    ```matlab\n    matlab.engine.shareEngine\n    ```\n    You can verify it's shared by running `matlab.engine.isEngineShared` in MATLAB (it should return `true` or `1`). The MCP server needs this shared engine to connect.\n\n## Configuration (for Claude Desktop)\n\nTo use this server with Claude Desktop:\n\n1.  Go to Claude Desktop -> Settings -> Developer -> Edit Config.\n2.  This will open `claude_desktop_config.json`. Add or modify the `mcpServers` section to include the `MatlabMCP` configuration:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"MatlabMCP\": {\n          \"command\": \"C:\\\\Users\\\\username\\\\.local\\\\bin\\\\uv.exe\", // Path to your uv executable\n          \"args\": [\n            \"--directory\",\n            \"C:\\\\Users\\\\username\\\\Desktop\\\\MatlabMCP\\\\\", // ABSOLUTE path to the cloned repository directory\n            \"run\",\n            \"main.py\"\n          ]\n          // Optional: Add environment variables if needed\n          // \"env\": {\n          //   \"MY_VAR\": \"value\"\n          // }\n        }\n        // Add other MCP servers here if you have them\n      }\n    }\n    ```\n3.  **IMPORTANT:** Replace `C:\\\\Users\\\\username\\\\...` paths with the correct **absolute paths** for your system.\n4.  Save the file and **restart Claude Desktop**.\n5.  **Logging:** Server logs (from Python's `logging` module) will appear in Claude Desktop's MCP log files (accessible via `tail -f ~/Library/Logs/Claude/mcp-server-MatlabMCP.log` on macOS or checking `%APPDATA%\\Claude\\logs\\` on Windows).\n\n\n## Development\n\nProject Structure:\n```\nMatlabMCP/\n‚îú‚îÄ‚îÄ .venv/                     # Virtual environment created by uv\n‚îú‚îÄ‚îÄ Docs/\n‚îÇ   ‚îî‚îÄ‚îÄ Images/\n‚îÇ   ‚îî‚îÄ‚îÄ Updates.md             # Documentation for updates and changes\n‚îú‚îÄ‚îÄ main.py                    # The MCP server script\n‚îú‚îÄ‚îÄ pyproject.toml             # Project metadata and dependencies\n‚îú‚îÄ‚îÄ README.md                  # This file\n‚îî‚îÄ‚îÄ uv.lock                    # Lock file for dependencies\n```\n\n## Documentation\nCheck out [Updates](./Docs/Updates.md) for detailed documentation on the server's features, usage, and development notes.\n\n## Contributing\nContributions are welcome! If you have any suggestions or improvements, feel free to open an issue or submit a pull request.\n\nLet's make this even better together!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "matlabmcp",
        "matlab",
        "python",
        "jigarbhoye04 matlabmcp",
        "execute matlab",
        "shared matlab"
      ],
      "category": "scientific-research-tools"
    },
    "jinsoo--DDA_julia.jl": {
      "owner": "jinsoo",
      "name": "DDA_julia.jl",
      "url": "https://github.com/jinsoo/DDA_julia.jl",
      "imageUrl": "/freedevtools/mcp/pfp/jinsoo.webp",
      "description": "Perform scattering calculations using the discrete-dipole approximation for various geometrical shapes, including spheres. Utilize wave parameters to analyze scattering effects based on defined objects and their properties.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Julia",
      "updated_at": "2023-09-18T12:48:13Z",
      "readme_content": "# JuliaDDA\n\n[![Build Status](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml?query=branch%3Amain)\n\n## Usage\n\nThis package is based on the reference \n\n*Draine, B. T., & Flatau, P. J. (1994). Discrete-dipole approximation for scattering calculations. Josa a, 11(4), 1491-1499.*\n\n# Testing\n\n    using JuliaDDA\n    using StaticArrays\n\n    # Wave parameters\n    k = 1\n    ka = 7\n    a = ka / k\n\n    # Define Container for load objects \n    C = Container(k)\n\n    # Generate Sphere object \n    Object = Sphere(a, 20, 2+1.5im, 1)\n    # You can also Rotate or Pan the Object\n    # Ojbect = Rotate(Object, pi/4, nÃÇ=SA[0, 0, 1])\n\n    # plane wave\n    # propagation axis :z\n    # polarization axis : x\n    Src = PlaneWave(SA[0, 0, 1], SA[1, 0, 0]) \n\n    # Define Recorder to save Electric field at certain position\n    Rec = SphericalRecorder(50, 50, R=100, mode=\"full\")\n\n    # push object to the container\n    push!(C, Object)\n\n    # Calculate the incident field\n    CalEinc(C, Src)\n\n    # Calculate the Polarization of the dipoles\n    CalPolarization(C)\n\n    # Calculate the Farfield at recorder positions\n    CalFarField(C, Rec, \"sca\")\n\n    Volume = 4/3 * pi * a^3\n    Po_in = œµ0 * Volume \n\n    # Plot scatteres in the Container (backend : PlotlyJS)\n    PlotScatterers(C, :Px, plot_mode=:real)\n\n    # Plot Far field power\n    Plot3DPower(Rec, log=true, Po_in = Po_in)\n\n    # Plot DifferentialCrossSection (not accurate yet)\n    PlotDifferentialCrossSection(C, 50, 50, scale=\"log\", r=100)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scattering",
        "spheres",
        "dipole",
        "scattering calculations",
        "scattering effects",
        "analyze scattering"
      ],
      "category": "scientific-research-tools"
    },
    "jjsantos01--qgis_mcp": {
      "owner": "jjsantos01",
      "name": "qgis_mcp",
      "url": "https://github.com/jjsantos01/qgis_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jjsantos01.webp",
      "description": "Connects Claude AI to QGIS for project management and layer manipulation, enabling the creation, loading, and saving of projects, along with executing processing algorithms and running Python code directly within QGIS.",
      "stars": 660,
      "forks": 88,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T08:20:10Z",
      "readme_content": "# QGISMCP - QGIS Model Context Protocol Integration\n\nQGISMCP connects [QGIS](https://qgis.org/) to [Claude AI](https://claude.ai/chat) through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro), allowing Claude to directly interact with and control QGIS. This integration enables prompt assisted project creation, layer loading, code execution and more.\n\nThis project is strongly based on the [BlenderMCP](https://github.com/ahujasid/blender-mcp/tree/main) project by [Siddharth Ahuja](https://x.com/sidahuj)\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to QGIS through a socket-based server.\n- **Project manipulation**: Create, load and save projects in QGIS.\n- **Layer manipulation**: Add and remove vector or raster layers to a project.\n- **Execute processing**: Execute processing algorithms ([Processing Toolbox](https://docs.qgis.org/3.40/en/docs/user_manual/processing/toolbox.html)).\n- **Code execution**: Run arbitrary Python code in QGIS from Claude. Very powerful, but also be very cautious using this tool.\n\n## Components\n\nThe system consists of two main components:\n\n1. **[QGIS plugin](/qgis_mcp_plugin/)**: A QGIS plugin that creates a socket server within QGIS to receive and execute commands.\n2. **[MCP Server](/src/qgis_mcp/qgis_mcp_server.py)**: A Python server that implements the Model Context Protocol and connects to the QGIS plugin.\n\n## Installation\n\n### Prerequisites\n\n- QGIS 3.X (only tested on 3.22)\n- Claude desktop\n- Python 3.10 or newer\n- uv package manager:\n\nIf you're on Mac, please install uv as\n\n```bash\nbrew install uv\n```\n\nOn Windows Powershell\n\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nOtherwise installation instructions are on their website: [Install uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n**‚ö†Ô∏è Do not proceed before installing UV**\n\n### Download code\n\nDownload this repo to your computer. You can clone it with:\n\n```bash\ngit clone git@github.com:jjsantos01/qgis_mcp.git\n```\n\n### QGIS plugin\n\nYou need to copy the folder [qgis_mcp_plugin](/qgis_mcp_plugin/) and its content on your QGIS profile plugins folder.\n\nYou can get your profile folder in QGIS going to menu `Settings` -> `User profiles` -> `Open active profile folder` Then, go to `Python/plugins` and paste the folder `qgis_mcp_plugin`.\n\n> On a Windows machine the plugins folder is usually located at: `C:\\Users\\USER\\AppData\\Roaming\\QGIS\\QGIS3\\profiles\\default\\python\\plugins` \n\nand on MacOS: `~/Library/Application\\ Support/QGIS/QGIS3/profiles/default/python/plugins`\n\n Then close QGIS and open it again. Go to the menu option `Plugins` > `Installing and Managing Plugins`, select the `All` tab and search for \"QGIS MCP\", then mark the QGIS MCP checkbox.\n\n### Claude for Desktop Integration\n\nGo to `Claude` > `Settings` > `Developer` > `Edit Config` > `claude_desktop_config.json` to include the following:\n\n> If you can't find the \"Developers tab\" or the `claude_desktop_config.json` look at this [documentation](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server).\n\n```json\n{\n    \"mcpServers\": {\n        \"qgis\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/ABSOLUTE/PATH/TO/PARENT/REPO/FOLDER/qgis_mcp/src/qgis_mcp\",\n                \"run\",\n                \"qgis_mcp_server.py\"\n            ]\n        }\n\n    }\n}\n```\n\n## Usage\n\n### Starting the Connection\n\n1. In QGIS, go to `plugins` > `QGIS MCP` > `QGIS MCP`\n    ![plugins menu](/assets/imgs/qgis-plugins-menu.png)\n2. Click \"Start Server\"\n    ![start server](/assets/imgs/qgis-mcp-start-server.png)\n\n### Using with Claude\n\nOnce the config file has been set on Claude, and the server is running on QGIS, you will see a hammer icon with tools for the QGIS MCP.\n\n![Claude tools](assets/imgs/claude-available-tools.png)\n\n#### Tools\n\n- `ping` - Simple ping command to check server connectivity\n- `get_qgis_info` - Get QGIS information about the current installation\n- `load_project` - Load a QGIS project from the specified path\n- `create_new_project` - Create a new project and save it\n- `get_project_info` - Get current project information\n- `add_vector_layer` - Add a vector layer to the project\n- `add_raster_layer` - Add a raster layer to the project\n- `get_layers` - Retrieve all layers in the current project\n- `remove_layer` - Remove a layer from the project by its ID\n- `zoom_to_layer` - Zoom to the extent of a specified layer\n- `get_layer_features` - Retrieve features from a vector layer with an optional limit\n- `execute_processing` - Execute a processing algorithm with the given parameters\n- `save_project` - Save the current project to the given path\n- `render_map` - Render the current map view to an image file\n- `execute_code` - Execute arbitrary PyQGIS code provided as a string\n\n### Example Commands\n\nThis is the example I used for the [demo](https://x.com/jjsantoso/status/1900293848271667395):\n\n```plain\nYou have access to the tools to work with QGIS. You will do the following:\n\t1. Ping to check the connection. If it works, continue with the following steps.\n\t2. Create a new project and save it at: \"C:/Users/USER/GitHub/qgis_mcp/data/cdmx.qgz\"\n\t3. Load the vector layer: \"\"C:/Users/USER/GitHub/qgis_mcp/data/cdmx/mgpc_2019.shp\" and name it \"Colonias\".\n\t4. Load the raster layer: \"C:/Users/USER/GitHub/qgis_mcp/data/09014.tif\" and name it \"BJ\"\n\t5. Zoom to the \"BJ\" layer.\n\t6. Execute the centroid algorithm on the \"Colonias\" layer. Skip the geometry check. Save the output to \"colonias_centroids.geojson\".\n\t7. Execute code to create a choropleth map using the \"POB2010\" field in the \"Colonias\" layer. Use the quantile classification method with 5 classes and the Spectral color ramp.\n\t8. Render the map to \"C:/Users/USER/GitHub/qgis_mcp/data/cdmx.png\"\n\t9. Save the project.\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "qgis_mcp",
        "qgis",
        "projects",
        "jjsantos01 qgis_mcp",
        "ai qgis",
        "qgis project"
      ],
      "category": "scientific-research-tools"
    },
    "jkingsman--qanon-mcp-server": {
      "owner": "jkingsman",
      "name": "qanon-mcp-server",
      "url": "https://github.com/jkingsman/qanon-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jkingsman.webp",
      "description": "Provides access to a dataset of Q-Anon posts for research purposes, enabling users to search, filter, and analyze the content for sociological insights.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-20T16:40:02Z",
      "readme_content": "## QAnon is a dangerous cult. This archive is for research purposes only, and I do _not_ endorse any material in this repo.\r\n\r\n# Q-Anon Posts/Drops MCP Server\r\n\r\n![](https://badge.mcpx.dev?type=server 'qanon-mcp')\r\n[![smithery badge](https://smithery.ai/badge/@jkingsman/qanon-mcp-server)](https://smithery.ai/server/@jkingsman/qanon-mcp-server)\r\n\r\nAn MCP (Model Context Protocol) server that provides access to a dataset of Q-Anon posts for anthropological/sociological research. This server allows AI assistants like Claude to search, filter, and analyze the Q-Anon drops.\r\n\r\nPosts are drawn from https://github.com/jkingsman/JSON-QAnon. You can learn more about how the source data was composed there, as well as find alternate formats, schemas, etc.\r\n\r\n### Warning: This tool was entirely vibe coded. Use at your own risk.\r\n\r\n## Prerequisites\r\n\r\n- Python 3.10 or higher\r\n- `uv` package manager\r\n- Claude Desktop (for Claude integration)\r\n\r\n## Installation\r\n\r\nThis tool is compatible with `uvx` and doesn't need to be cloned/installed.\r\n\r\n### Installing via Smithery\r\n\r\nTo install qanon-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jkingsman/qanon-mcp-server):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @jkingsman/qanon-mcp-server --client claude\r\n```\r\n\r\n### Manual\r\n\r\n1. Clone or download this repository to your local machine\r\n2. Install the required packages using `uv`:\r\n\r\n```bash\r\nuv pip install -e .\r\n```\r\n\r\n## Usage\r\n\r\nYou can run the server directly with `uvx`:\r\n\r\n```bash\r\nuvx qanon_mcp\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nTo use this MCP server with Claude Desktop:\r\n\r\n1. Make sure you have [Claude Desktop](https://claude.ai/download) installed\r\n2. Open the Claude menu and select \"Settings...\"\r\n3. Click on \"Developer\" in the left-hand bar and then \"Edit Config\"\r\n4. Add the following configuration to the `claude_desktop_config.json` file:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nor, if you don't have `uvx` installed:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"tool\",\r\n        \"run\",\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n5. Save the file and restart Claude Desktop\r\n6. Start a new conversation in Claude Desktop\r\n7. You should see a hammer icon in the input box, indicating that tools are available\r\n\r\n## Features\r\n\r\n### Resources\r\n\r\n- `qanon://posts/count` - Get the total number of posts\r\n- `qanon://posts/{post_id}` - Access a specific post by ID\r\n- `qanon://posts/raw/{post_id}` - Get the raw JSON data for a specific post\r\n- `qanon://authors` - List all unique authors\r\n- `qanon://stats` - Get dataset statistics\r\n\r\n### Tools\r\n\r\n- **get_post_by_id_tool** - Retrieve a specific post by its ID\r\n- **search_posts** - Find posts containing specific keywords or phrases\r\n- **get_posts_by_date** - Retrieve posts from a specific date range\r\n- **get_posts_by_author_id** - Find posts by a specific author ID\r\n- **analyze_post** - Get detailed analysis of a specific post including references and context\r\n- **get_timeline_summary** - Generate a chronological timeline, optionally within a date range\r\n- **word_cloud_by_post_ids** - Generate a word frequency analysis for posts within a specified ID range\r\n- **word_cloud_by_date_range** - Generate a word frequency analysis for posts within a specified date range\r\n\r\n## Example Queries for Claude\r\n\r\nOnce the MCP server is connected to Claude Desktop, you can ask questions like:\r\n\r\n- \"How many Q-Anon posts are in the dataset?\"\r\n- \"Search for posts that mention 'storm'\"\r\n- \"Show me posts from October 2020\"\r\n- \"Analyze post #3725\"\r\n- \"Create a timeline of Q-Anon posts from 2018\"\r\n- \"Generate a word cloud for Q-Anon posts between January and March 2019\"\r\n- \"Get the raw data for post #4500\"\r\n- \"What are the most common words used in posts #1000-2000?\"\r\n\r\n## Troubleshooting\r\n\r\n- If Claude Desktop doesn't show the hammer icon, check your configuration and restart Claude Desktop\r\n- Ensure the `posts.json` file is in the same directory as the script\r\n- Check the output in the terminal for any error messages\r\n- Make sure you're using the absolute path to the script in your Claude Desktop configuration\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sociological",
        "research",
        "qanon",
        "posts research",
        "research tools",
        "content sociological"
      ],
      "category": "scientific-research-tools"
    },
    "kbsooo--MCP_Atom_of_Thoughts": {
      "owner": "kbsooo",
      "name": "MCP_Atom_of_Thoughts",
      "url": "https://github.com/kbsooo/MCP_Atom_of_Thoughts",
      "imageUrl": "/freedevtools/mcp/pfp/kbsooo.webp",
      "description": "Enhance applications with advanced language models for idea generation, analysis, and manipulation. Context-aware interactions enable transformation of workflows through intelligent thought processing.",
      "stars": 45,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T00:52:14Z",
      "readme_content": "# Atom of Thoughts (AoT)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kbsooo-mcp-atom-of-thoughts-badge.png)](https://mseep.ai/app/kbsooo-mcp-atom-of-thoughts)\n\n[![smithery badge](https://smithery.ai/badge/@kbsooo/mcp_atom_of_thoughts)](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/672e0733-4c20-461f-b9cf-59ec2c9dafff)\n\nA Model Context Protocol (MCP) server implementation of Atom of Thoughts, a decomposition-based reasoning framework.\n\n> **Note**: This implementation is based on the research paper [\"Atom of Thoughts for Markov LLM Test-Time Scaling\"](https://arxiv.org/abs/2502.12018) (Teng et al., 2025).\n\n<a href=\"https://glama.ai/mcp/servers/@kbsooo/MCP_Atom_of_Thoughts\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kbsooo/MCP_Atom_of_Thoughts/badge\" />\n</a>\n\n[MCP.so](https://mcp.so/server/atom-of-thoughts/kbsooo)\n\n[ÌïúÍµ≠Ïñ¥ ÏÑ§Î™Ö](https://github.com/kbsooo/MCP_Atom_of_Thoughts?tab=readme-ov-file#%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%84%A4%EB%AA%85)\n\n## English Documentation\n\nThis repository implements Atom of Thoughts (AoT), a decomposition-based reasoning framework, as a Model Context Protocol (MCP) server. This implementation is based on the concepts presented in the paper [\"Atom of Thoughts for Markov LLM Test-Time Scaling\"](https://arxiv.org/abs/2502.12018) (Teng et al., 2025).\n\n### Available Tools\n\nTwo main tools are provided:\n\n1. **AoT (Full Version)**: A complete Atom of Thoughts tool with full capabilities for deep analysis and complex problem solving.\n2. **AoT-light (Lightweight Version)**: A streamlined version optimized for faster processing and quicker results.\n\n### AoT-light: Lightweight Version\n\nAoT-light is designed for faster processing in time-sensitive situations:\n\n- **Key Features**:\n  - Lower maximum depth (3 instead of 5) for faster processing\n  - Simplified verification process\n  - Immediate conclusion suggestion for high-confidence hypotheses\n  - Reduced computational overhead and response payload\n  - Optimized for speed rather than exhaustive analysis\n\n- **Use Cases**:\n  - Quick brainstorming sessions requiring atomic thought organization\n  - Time-sensitive problem solving where speed is prioritized over exhaustive analysis\n  - Simpler reasoning tasks that don't require deep decomposition\n  - Initial exploration before using the full AoT for deeper analysis\n  - Learning or demonstration purposes where response time is important\n\n### Use Cases\n\nAtom of Thoughts is effective in the following scenarios:\n- Solving problems requiring complex reasoning\n- Generating hypotheses that need verification from multiple perspectives\n- Deriving high-confidence conclusions in scenarios where accuracy is crucial\n- Minimizing logical errors in critical tasks\n- Decision-making requiring multiple verification steps\n\n### Atom Types\n\nAoT uses five types of atoms:\n\n1. **premise**: Basic assumptions or given information for problem solving\n2. **reasoning**: Logical reasoning process based on other atoms\n3. **hypothesis**: Proposed solutions or intermediate conclusions\n4. **verification**: Process to evaluate the validity of other atoms (especially hypotheses)\n5. **conclusion**: Verified hypotheses or final problem solutions\n\n### Core Features\n\n#### 1. Decomposition-Contraction Mechanism\n\nA mechanism to decompose atoms into smaller sub-atoms and contract them back after verification.\n\n- **Decomposition**: Breaking complex atoms into smaller sub-atoms.\n  - `startDecomposition(atomId)`: Start atom decomposition\n  - `addToDecomposition(decompositionId, atomId)`: Add sub-atom to decomposition\n  - `completeDecomposition(decompositionId)`: Complete decomposition process\n\n- **Contraction**: Contract back to the original atom once all sub-atoms are verified.\n  - Calculate confidence of the original atom based on sub-atoms' confidence levels\n  - Automatically suggest conclusions for high-confidence verified hypotheses\n\n#### 2. Automatic Termination Mechanism\n\n- Automatically terminates when reaching maximum depth or finding a high-confidence conclusion.\n- `getTerminationStatus()`: Return current termination status and reason\n- `getBestConclusion()`: Return the conclusion with highest confidence\n\n### Parameter Descriptions\n\n- **atomId**: Unique identifier for the atom (e.g., 'A1', 'H2')\n- **content**: Actual content of the atom\n- **atomType**: Type of atom (one of: premise, reasoning, hypothesis, verification, conclusion)\n- **dependencies**: List of IDs of other atoms this atom depends on\n- **confidence**: Confidence level of this atom (value between 0-1)\n- **isVerified**: Whether this atom has been verified\n- **depth**: Depth level of this atom in the decomposition-contraction process\n\n### Usage Method\n\n1. Understand the problem and define necessary premise atoms\n2. Create reasoning atoms based on premises\n3. Create hypothesis atoms based on reasoning\n4. Create verification atoms to verify hypotheses\n5. Derive conclusion atoms based on verified hypotheses\n6. Use atom decomposition to explore deeper when necessary\n7. Present the high-confidence conclusion atom as the final answer\n\n### Comparing Sequential Thinking and Atom of Thoughts (More Testing Needed)\n\nAfter applying both thinking tools to the same topic, the following differences and performance characteristics were observed:\n\n#### Structural Differences\n\n**Sequential Thinking:**\n- Linear thinking process: progresses sequentially from one thought to the next\n- Predicts the total number of thoughts in advance\n- Each thinking stage is built upon previous stages\n\n**Atom of Thoughts:**\n- Non-linear, network structure: multiple thought units (atoms) interconnect with dependencies\n- Forms systematic structure according to atom types (premise, reasoning, hypothesis, verification, conclusion)\n- Explicitly evaluates the confidence level of each atom\n\n#### Comparative Strengths\n\n**Sequential Thinking Strengths:**\n- Intuitive flow: similar to natural human thinking processes\n- Simplicity: simple structure allows quick application to straightforward problems\n- Flexibility: can modify previous stages or change direction during the thinking process\n\n**Atom of Thoughts Strengths:**\n- Confidence evaluation: explicitly measures the confidence of each thought to improve conclusion validity\n- Verification process: evaluates hypotheses through systematic verification stages\n- Dependency tracking: clearly tracks which premises or reasoning influenced specific conclusions\n- Parallel processing: can consider multiple thought atoms simultaneously\n\n#### Efficiency and Accuracy\n\n**Efficiency:**\n- Sequential Thinking: more efficient for simple problems, with faster progression of thought\n- Atom of Thoughts: more efficient for complex problems, but has initial overhead in building systematic structures\n\n**Accuracy:**\n- Sequential Thinking: possibility of error accumulation from previous stages as the thinking process deepens\n- Atom of Thoughts: reduced error possibility through verification stages and confidence assessment, leading to more reliable conclusions\n\n#### Suitability by Purpose\n\n**Cases Suitable for Sequential Thinking:**\n- Simple to moderately complex problems\n- Time-constrained situations\n- When natural storytelling or explanation is needed\n\n**Cases Suitable for Atom of Thoughts:**\n- Highly complex problems\n- Situations where accuracy and reliability are crucial\n- Hypotheses requiring verification from multiple perspectives\n- Reasoning with complex dependency relationships\n\n#### Conclusion\nBoth tools can contribute to improving artificial intelligence's reasoning abilities, but the appropriate tool varies depending on the nature of the problem and requirements. Sequential Thinking is useful when intuitive and quick thinking processes are needed, while Atom of Thoughts is more suitable for complex problems requiring systematic verification and high reliability.\n\n### Command Tool (atomcommands)\n\nA command tool to control the decomposition-contraction mechanism and automatic termination of Atom of Thoughts.\n\n**Available Commands**:\n1. **decompose**: Decompose a specified atom into smaller sub-atoms\n   - Required parameter: `atomId`\n2. **complete_decomposition**: Complete an ongoing decomposition process\n   - Required parameter: `decompositionId`\n3. **termination_status**: Check the termination status of the current AoT process\n4. **best_conclusion**: Get the verified conclusion with the highest confidence\n5. **set_max_depth**: Change the maximum depth limit\n   - Required parameter: `maxDepth`\n\n### Installing via Smithery\n\nTo install Atom of Thoughts for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts):\n\n```bash\nnpx -y @smithery/cli install @kbsooo/mcp_atom_of_thoughts --client claude\n```\n\n### MCP Server Configuration\n\nTo use the Atom of Thoughts MCP server, you need to register it in your Claude Desktop or Cline MCP settings. Here is an example configuration:\n\n```json\n{ \n  \"mcpServers\": { \n    \"atom-of-thoughts\": { \n      \"command\": \"node\", \n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/atom-of-thoughts/build/index.js\"], \n      \"disabled\": false, \n      \"autoApprove\": [] \n    } \n  } \n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/PARENT/FOLDER` with the actual absolute path to the project on your system. After saving the configuration, restart Claude Desktop or Cline to use the Atom of Thoughts MCP server.\n\nFor detailed implementation and code-level documentation, please refer to the source code in this repository.\n\n## ÌïúÍµ≠Ïñ¥ ÏÑ§Î™Ö\n\n### Atom of ThoughtsÎûÄ?\n\nAtom of Thoughts(AoT)Îäî Î≥µÏû°Ìïú Î¨∏Ï†úÎ•º ÎèÖÎ¶ΩÏ†ÅÏù¥Í≥† Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏõêÏûê Îã®ÏúÑÏùò ÏÇ¨Í≥†Î°ú Î∂ÑÌï¥ÌïòÏó¨ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî ÎèÑÍµ¨ÏûÖÎãàÎã§. Ïù¥ ÎèÑÍµ¨Îäî Ï†ÑÌÜµÏ†ÅÏù∏ ÏàúÏ∞®Ï†Å ÏÇ¨Í≥† Î∞©ÏãùÍ≥º Îã¨Î¶¨, ÏÇ¨Í≥†Ïùò Í∏∞Î≥∏ Îã®ÏúÑÏù∏ 'ÏõêÏûê'Îì§Ïù¥ ÏÑúÎ°ú ÏùòÏ°¥ÏÑ±ÏùÑ Í∞ñÍ≥† Íµ¨ÏÑ±ÎêòÏñ¥ Îçî Í∞ïÎ†•Ìïú Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ Í∞ÄÎä•ÌïòÍ≤å Ìï©ÎãàÎã§. Ïù¥ Íµ¨ÌòÑÏùÄ \"Atom of Thoughts for Markov LLM Test-Time Scaling\"(Teng et al., 2025) ÎÖºÎ¨∏Ïùò Í∞úÎÖêÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìï©ÎãàÎã§.\n\n### Ï†úÍ≥µÎêòÎäî ÎèÑÍµ¨\n\nÌòÑÏû¨ Îã§ÏùåÍ≥º Í∞ôÏùÄ Îëê Í∞ÄÏßÄ Ï£ºÏöî ÎèÑÍµ¨Í∞Ä Ï†úÍ≥µÎê©ÎãàÎã§:\n\n1. **AoT (Ï†ÑÏ≤¥ Î≤ÑÏ†Ñ)**: Ïã¨Ï∏µÏ†ÅÏù∏ Î∂ÑÏÑùÍ≥º Î≥µÏû°Ìïú Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌïú ÏôÑÏ†ÑÌïú Í∏∞Îä•ÏùÑ Í∞ñÏ∂ò Atom of Thoughts ÎèÑÍµ¨ÏûÖÎãàÎã§.\n2. **AoT-light (Í≤ΩÎüâ Î≤ÑÏ†Ñ)**: Îçî Îπ†Î•∏ Ï≤òÎ¶¨ÏôÄ Ïã†ÏÜçÌïú Í≤∞Í≥ºÎ•º ÏúÑÌï¥ ÏµúÏ†ÅÌôîÎêú Í≤ΩÎüâ Î≤ÑÏ†ÑÏûÖÎãàÎã§.\n\n### AoT-light: Í≤ΩÎüâ Î≤ÑÏ†Ñ\n\nAoT-lightÎäî ÏãúÍ∞ÑÏù¥ Ï§ëÏöîÌïú ÏÉÅÌô©ÏóêÏÑú Îçî Îπ†Î•∏ Ï≤òÎ¶¨Î•º ÏúÑÌï¥ ÏÑ§Í≥ÑÎêú Í≤ΩÎüâ Î≤ÑÏ†ÑÏûÖÎãàÎã§:\n\n- **Ï£ºÏöî ÌäπÏßï**:\n  - ÎÇÆÏùÄ ÏµúÎåÄ ÍπäÏù¥ (5 ÎåÄÏã† 3) ÏÑ§Ï†ïÏúºÎ°ú Îπ†Î•∏ Ï≤òÎ¶¨\n  - Í∞ÑÏÜåÌôîÎêú Í≤ÄÏ¶ù ÌîÑÎ°úÏÑ∏Ïä§\n  - ÎÜíÏùÄ Ïã†Î¢∞ÎèÑÏùò Í∞ÄÏÑ§Ïóê ÎåÄÌïú Ï¶âÍ∞ÅÏ†ÅÏù∏ Í≤∞Î°† Ï†úÏïà\n  - Ï∂ïÏÜåÎêú Í≥ÑÏÇ∞ Ïò§Î≤ÑÌó§Îìú Î∞è ÏùëÎãµ Îç∞Ïù¥ÌÑ∞\n  - Ï≤†Ï†ÄÌïú Î∂ÑÏÑùÎ≥¥Îã§ ÏÜçÎèÑÏóê ÏµúÏ†ÅÌôî\n\n- **ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§**:\n  - ÏõêÏûêÏ†Å ÏÇ¨Í≥† Íµ¨ÏÑ±Ïù¥ ÌïÑÏöîÌïú Îπ†Î•∏ Î∏åÎ†àÏù∏Ïä§ÌÜ†Î∞ç ÏÑ∏ÏÖò\n  - Ï≤†Ï†ÄÌïú Î∂ÑÏÑùÎ≥¥Îã§ ÏÜçÎèÑÍ∞Ä Ï§ëÏöîÌïú ÏãúÍ∞ÑÏóê ÎØºÍ∞êÌïú Î¨∏Ï†ú Ìï¥Í≤∞\n  - ÍπäÏùÄ Î∂ÑÌï¥Í∞Ä ÌïÑÏöîÌïòÏßÄ ÏïäÏùÄ Îã®ÏàúÌïú Ï∂îÎ°† ÏûëÏóÖ\n  - Ï†ÑÏ≤¥ AoTÎ•º ÏÇ¨Ïö©Ìïú Ïã¨Ï∏µ Î∂ÑÏÑù Ï†Ñ Ï¥àÍ∏∞ ÌÉêÏÉâ\n  - ÏùëÎãµ ÏãúÍ∞ÑÏù¥ Ï§ëÏöîÌïú ÌïôÏäµ ÎòêÎäî ÏãúÏó∞ Î™©Ï†Å\n\n### ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§\n\nÎã§ÏùåÍ≥º Í∞ôÏùÄ Í≤ΩÏö∞Ïóê Atom of ThoughtsÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ Ìö®Í≥ºÏ†ÅÏûÖÎãàÎã§:\n- Î≥µÏû°Ìïú Ï∂îÎ°†Ïù¥ ÌïÑÏöîÌïú Î¨∏Ï†ú Ìï¥Í≤∞\n- Ïó¨Îü¨ Í¥ÄÏ†êÏóêÏÑú Í≤ÄÏ¶ùÏù¥ ÌïÑÏöîÌïú Í∞ÄÏÑ§ ÏÉùÏÑ±\n- Ï†ïÌôïÎèÑÍ∞Ä Ï§ëÏöîÌïú Î¨∏Ï†úÏóêÏÑú Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ Í≤∞Î°† ÎèÑÏ∂ú\n- ÎÖºÎ¶¨Ï†Å Ïò§Î•òÎ•º ÏµúÏÜåÌôîÌï¥Ïïº ÌïòÎäî ÏûëÏóÖ\n- Ïó¨Îü¨ Îã®Í≥ÑÏùò Í≤ÄÏ¶ùÏù¥ ÌïÑÏöîÌïú ÏùòÏÇ¨Í≤∞Ï†ï\n\n### ÏõêÏûê Ïú†Ìòï\n\nAtom of ThoughtsÏóêÏÑúÎäî Îã§ÏÑØ Í∞ÄÏßÄ Ïú†ÌòïÏùò ÏõêÏûêÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§:\n\n1. **premise (Ï†ÑÏ†ú)**: Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Í∏∞Î≥∏ Í∞ÄÏ†ïÏù¥ÎÇò Ï£ºÏñ¥ÏßÑ Ï†ïÎ≥¥\n2. **reasoning (Ï∂îÎ°†)**: Îã§Î•∏ ÏõêÏûêÎì§ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú ÎÖºÎ¶¨Ï†Å Ï∂îÎ°† Í≥ºÏ†ï\n3. **hypothesis (Í∞ÄÏÑ§)**: Í∞ÄÎä•Ìïú Ìï¥Í≤∞Ï±ÖÏù¥ÎÇò Ï§ëÍ∞Ñ Í≤∞Î°†Ïóê ÎåÄÌïú Ï†úÏïà\n4. **verification (Í≤ÄÏ¶ù)**: Îã§Î•∏ ÏõêÏûê(ÌäπÌûà Í∞ÄÏÑ§)Ïùò Ïú†Ìö®ÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÎäî Í≥ºÏ†ï\n5. **conclusion (Í≤∞Î°†)**: Í≤ÄÏ¶ùÎêú Í∞ÄÏÑ§Ïù¥ÎÇò ÏµúÏ¢Ö Î¨∏Ï†ú Ìï¥Í≤∞Ï±Ö\n\n### ÌïµÏã¨ Í∏∞Îä•\n\n#### 1. Î∂ÑÌï¥-ÏàòÏ∂ï Î©îÏª§ÎãàÏ¶ò (Decomposition-Contraction)\n\nÏõêÏûêÎ•º Îçî ÏûëÏùÄ ÌïòÏúÑ ÏõêÏûêÎ°ú Î∂ÑÌï¥ÌïòÍ≥† Í≤ÄÏ¶ù ÌõÑ Îã§Ïãú ÏàòÏ∂ïÌïòÎäî Î©îÏª§ÎãàÏ¶òÏûÖÎãàÎã§.\n\n- **ÏõêÏûê Î∂ÑÌï¥ (Decomposition)**: Î≥µÏû°Ìïú ÏõêÏûêÎ•º Îçî ÏûëÏùÄ ÌïòÏúÑ ÏõêÏûêÎ°ú Î∂ÑÌï¥Ìï©ÎãàÎã§.\n  - `startDecomposition(atomId)`: ÏõêÏûê Î∂ÑÌï¥ ÏãúÏûë\n  - `addToDecomposition(decompositionId, atomId)`: Î∂ÑÌï¥Ïóê ÌïòÏúÑ ÏõêÏûê Ï∂îÍ∞Ä\n  - `completeDecomposition(decompositionId)`: Î∂ÑÌï¥ Í≥ºÏ†ï ÏôÑÎ£å\n\n- **ÏõêÏûê ÏàòÏ∂ï (Contraction)**: ÌïòÏúÑ ÏõêÏûêÎì§Ïù¥ Î™®Îëê Í≤ÄÏ¶ùÎêòÎ©¥ ÏõêÎûò ÏõêÏûêÎ°ú Îã§Ïãú ÏàòÏ∂ïÌï©ÎãàÎã§.\n  - ÌïòÏúÑ ÏõêÏûêÎì§Ïùò Ïã†Î¢∞ÎèÑÏóê Í∏∞Î∞òÌïòÏó¨ ÏõêÎûò ÏõêÏûêÏùò Ïã†Î¢∞ÎèÑÎ•º Í≥ÑÏÇ∞\n  - Í≤ÄÏ¶ùÎêú Í∞ÄÏÑ§Ïù¥ Í≥†Ïã†Î¢∞ÎèÑÎ•º Í∞ÄÏßÄÎ©¥ ÏûêÎèôÏúºÎ°ú Í≤∞Î°†ÏùÑ Ï†úÏïà\n\n#### 2. ÏûêÎèô Ï¢ÖÎ£å Î©îÏª§ÎãàÏ¶ò (Automatic Termination)\n\n- ÏµúÎåÄ ÍπäÏù¥(depth)Ïóê ÎèÑÎã¨ÌïòÍ±∞ÎÇò ÎÜíÏùÄ Ïã†Î¢∞ÎèÑÏùò Í≤∞Î°†ÏùÑ Ï∞æÏúºÎ©¥ ÏûêÎèô Ï¢ÖÎ£åÎê©ÎãàÎã§.\n- `getTerminationStatus()`: ÌòÑÏû¨ Ï¢ÖÎ£å ÏÉÅÌÉúÏôÄ Ïù¥Ïú†Î•º Î∞òÌôò\n- `getBestConclusion()`: Í∞ÄÏû• ÎÜíÏùÄ Ïã†Î¢∞ÎèÑÏùò Í≤∞Î°†ÏùÑ Î∞òÌôò\n\n### Îß§Í∞úÎ≥ÄÏàò ÏÑ§Î™Ö\n\n- **atomId**: ÏõêÏûêÏùò Í≥†Ïú† ÏãùÎ≥ÑÏûê (Ïòà: 'A1', 'H2' Îì±)\n- **content**: ÏõêÏûêÏùò Ïã§Ï†ú ÎÇ¥Ïö©\n- **atomType**: ÏõêÏûêÏùò Ïú†Ìòï (premise, reasoning, hypothesis, verification, conclusion Ï§ë ÌïòÎÇò)\n- **dependencies**: Ïù¥ ÏõêÏûêÍ∞Ä ÏùòÏ°¥ÌïòÎäî Îã§Î•∏ ÏõêÏûêÎì§Ïùò ID Î™©Î°ù\n- **confidence**: Ïù¥ ÏõêÏûêÏùò Ïã†Î¢∞ÎèÑ (0~1 ÏÇ¨Ïù¥Ïùò Í∞í)\n- **isVerified**: Ïù¥ ÏõêÏûêÍ∞Ä Í≤ÄÏ¶ùÎêòÏóàÎäîÏßÄ Ïó¨Î∂Ä\n- **depth**: Ïù¥ ÏõêÏûêÏùò ÍπäÏù¥ (Î∂ÑÌï¥-ÏàòÏ∂ï ÌîÑÎ°úÏÑ∏Ïä§ÏóêÏÑúÏùò ÍπäÏù¥ ÏàòÏ§Ä)\n\n### ÏÇ¨Ïö© Î∞©Î≤ï\n\n1. Î¨∏Ï†úÎ•º Ïù¥Ìï¥ÌïòÍ≥† ÌïÑÏöîÌïú Ï†ÑÏ†ú(premise) ÏõêÏûêÎì§ÏùÑ Ï†ïÏùò\n2. Ï†ÑÏ†úÎ•º Î∞îÌÉïÏúºÎ°ú Ï∂îÎ°†(reasoning) ÏõêÏûê ÏÉùÏÑ±\n3. Ï∂îÎ°†ÏùÑ Î∞îÌÉïÏúºÎ°ú Í∞ÄÏÑ§(hypothesis) ÏõêÏûê ÏÉùÏÑ±\n4. Í∞ÄÏÑ§ÏùÑ Í≤ÄÏ¶ù(verification)ÌïòÎäî ÏõêÏûê ÏÉùÏÑ±\n5. Í≤ÄÏ¶ùÎêú Í∞ÄÏÑ§ÏùÑ Î∞îÌÉïÏúºÎ°ú Í≤∞Î°†(conclusion) ÏõêÏûê ÎèÑÏ∂ú\n6. ÌïÑÏöîÏãú ÏõêÏûê Î∂ÑÌï¥(decomposition)Î•º ÏÇ¨Ïö©ÌïòÏó¨ Îçî ÍπäÏù¥ ÌÉêÏÉâ\n7. ÎÜíÏùÄ Ïã†Î¢∞ÎèÑÏùò Í≤∞Î°† ÏõêÏûêÎ•º ÏµúÏ¢Ö ÎãµÎ≥ÄÏúºÎ°ú Ï†úÏãú\n\n### Sequential ThinkingÍ≥º Atom of Thoughts ÎπÑÍµê (Ï°∞Í∏à Îçî ÌÖåÏä§Ìä∏Í∞Ä ÌïÑÏöîÌï®)\n\nÎëê Í∞ÄÏßÄ ÏÇ¨Í≥† ÎèÑÍµ¨Î•º ÎèôÏùºÌïú Ï£ºÏ†úÏóê Ï†ÅÏö©Ìïú ÌõÑ Í¥ÄÏ∞∞Îêú Ï∞®Ïù¥Ï†êÍ≥º ÏÑ±Îä• ÌäπÏÑ±ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\n\n#### Íµ¨Ï°∞Ï†Å Ï∞®Ïù¥Ï†ê\n\n**Sequential Thinking:**\n- ÏÑ†ÌòïÏ†Å ÏÇ¨Í≥† Í≥ºÏ†ï: Ìïú ÏÇ¨Í≥†ÏóêÏÑú Îã§Ïùå ÏÇ¨Í≥†Î°ú ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏßÑÌñâ\n- Ï†ÑÏ≤¥ ÏÇ¨Í≥† ÏàòÎ•º ÎØ∏Î¶¨ ÏòàÏ∏°\n- Í∞Å ÏÇ¨Í≥† Îã®Í≥ÑÎäî Ïù¥Ï†Ñ Îã®Í≥ÑÎ•º Í∏∞Î∞òÏúºÎ°ú Íµ¨Ï∂ïÎê®\n\n**Atom of Thoughts:**\n- ÎπÑÏÑ†Ìòï, ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨Ï°∞: Ïó¨Îü¨ ÏÇ¨Í≥† Îã®ÏúÑ(ÏõêÏûê)Í∞Ä ÏùòÏ°¥ÏÑ±ÏùÑ Í∞ÄÏßÄÍ≥† Ïó∞Í≤∞Îê®\n- ÏõêÏûê Ïú†Ìòï(Ï†ÑÏ†ú, Ï∂îÎ°†, Í∞ÄÏÑ§, Í≤ÄÏ¶ù, Í≤∞Î°†)Ïóê Îî∞Îùº Ï≤¥Í≥ÑÏ†ÅÏù∏ Íµ¨Ï°∞ ÌòïÏÑ±\n- Í∞Å ÏõêÏûêÏùò Ïã†Î¢∞ÎèÑ ÏàòÏ§ÄÏùÑ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌèâÍ∞Ä\n\n#### ÎπÑÍµê Í∞ïÏ†ê\n\n**Sequential Thinking Í∞ïÏ†ê:**\n- ÏßÅÍ¥ÄÏ†Å ÌùêÎ¶Ñ: ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïù∏Í∞ÑÏùò ÏÇ¨Í≥† Í≥ºÏ†ïÍ≥º Ïú†ÏÇ¨\n- Îã®ÏàúÏÑ±: Í∞ÑÎã®Ìïú Íµ¨Ï°∞Î°ú ÏßÅÍ¥ÄÏ†ÅÏù∏ Î¨∏Ï†úÏóê Îπ†Î•¥Í≤å Ï†ÅÏö© Í∞ÄÎä•\n- Ïú†Ïó∞ÏÑ±: ÏÇ¨Í≥† Í≥ºÏ†ï Ï§ëÏóê Ïù¥Ï†Ñ Îã®Í≥ÑÎ•º ÏàòÏ†ïÌïòÍ±∞ÎÇò Î∞©Ìñ•ÏùÑ Î≥ÄÍ≤ΩÌï† Ïàò ÏûàÏùå\n\n**Atom of Thoughts Í∞ïÏ†ê:**\n- Ïã†Î¢∞ÎèÑ ÌèâÍ∞Ä: Í∞Å ÏÇ¨Í≥†Ïùò Ïã†Î¢∞ÎèÑÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú Ï∏°Ï†ïÌïòÏó¨ Í≤∞Î°†Ïùò Ïú†Ìö®ÏÑ± Í∞úÏÑ†\n- Í≤ÄÏ¶ù Í≥ºÏ†ï: Ï≤¥Í≥ÑÏ†ÅÏù∏ Í≤ÄÏ¶ù Îã®Í≥ÑÎ•º ÌÜµÌï¥ Í∞ÄÏÑ§ ÌèâÍ∞Ä\n- ÏùòÏ°¥ÏÑ± Ï∂îÏ†Å: Ïñ¥Îñ§ Ï†ÑÏ†úÎÇò Ï∂îÎ°†Ïù¥ ÌäπÏ†ï Í≤∞Î°†Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏Ï≥§ÎäîÏßÄ Î™ÖÌôïÌïòÍ≤å Ï∂îÏ†Å\n- Î≥ëÎ†¨ Ï≤òÎ¶¨: Ïó¨Îü¨ ÏÇ¨Í≥† ÏõêÏûêÎ•º ÎèôÏãúÏóê Í≥†Î†§ Í∞ÄÎä•\n\n#### Ìö®Ïú®ÏÑ±Í≥º Ï†ïÌôïÏÑ±\n\n**Ìö®Ïú®ÏÑ±:**\n- Sequential Thinking: Îã®ÏàúÌïú Î¨∏Ï†úÏóê Îçî Ìö®Ïú®Ï†ÅÏù¥Î©∞, ÏÇ¨Í≥†Í∞Ä Îπ†Î•¥Í≤å ÏßÑÌñâÎê®\n- Atom of Thoughts: Î≥µÏû°Ìïú Î¨∏Ï†úÏóê Îçî Ìö®Ïú®Ï†ÅÏù¥ÏßÄÎßå, Ï≤¥Í≥ÑÏ†ÅÏù∏ Íµ¨Ï°∞Î•º ÎßåÎìúÎäî Ï¥àÍ∏∞ Ïò§Î≤ÑÌó§ÎìúÍ∞Ä ÏûàÏùå\n\n**Ï†ïÌôïÏÑ±:**\n- Sequential Thinking: ÏÇ¨Í≥† Í≥ºÏ†ïÏù¥ ÍπäÏñ¥ÏßàÏàòÎ°ù Ïù¥Ï†Ñ Îã®Í≥ÑÏóêÏÑúÏùò Ïò§Î•ò ÎàÑÏ†Å Í∞ÄÎä•ÏÑ±\n- Atom of Thoughts: Í≤ÄÏ¶ù Îã®Í≥ÑÏôÄ Ïã†Î¢∞ÎèÑ ÌèâÍ∞ÄÎ•º ÌÜµÌï¥ Ïò§Î•ò Í∞ÄÎä•ÏÑ± Í∞êÏÜå, Îçî Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî Í≤∞Î°† ÎèÑÏ∂ú\n\n#### Î™©Ï†ÅÎ≥Ñ Ï†ÅÌï©ÏÑ±\n\n**Sequential ThinkingÏóê Ï†ÅÌï©Ìïú Í≤ΩÏö∞:**\n- Îã®ÏàúÌïòÍ±∞ÎÇò Ï§ëÍ∞Ñ Ï†ïÎèÑ Î≥µÏû°Ìïú Î¨∏Ï†ú\n- ÏãúÍ∞Ñ Ï†úÏïΩÏù¥ ÏûàÎäî ÏÉÅÌô©\n- ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïä§ÌÜ†Î¶¨ÌÖîÎßÅÏù¥ÎÇò ÏÑ§Î™ÖÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞\n\n**Atom of ThoughtsÏóê Ï†ÅÌï©Ìïú Í≤ΩÏö∞:**\n- Îß§Ïö∞ Î≥µÏû°Ìïú Î¨∏Ï†ú\n- Ï†ïÌôïÏÑ±Í≥º Ïã†Î¢∞ÏÑ±Ïù¥ Ï§ëÏöîÌïú ÏÉÅÌô©\n- Ïó¨Îü¨ Í¥ÄÏ†êÏóêÏÑú Í≤ÄÏ¶ùÏù¥ ÌïÑÏöîÌïú Í∞ÄÏÑ§\n- Î≥µÏû°Ìïú ÏùòÏ°¥ Í¥ÄÍ≥ÑÍ∞Ä ÏûàÎäî Ï∂îÎ°†\n\n#### Í≤∞Î°†\nÎëê ÎèÑÍµ¨ Î™®Îëê Ïù∏Í≥µ ÏßÄÎä•Ïùò Ï∂îÎ°† Îä•Î†•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Îç∞ Í∏∞Ïó¨Ìï† Ïàò ÏûàÏßÄÎßå, Ï†ÅÏ†àÌïú ÎèÑÍµ¨Îäî Î¨∏Ï†úÏùò ÌäπÏÑ±Í≥º ÏöîÍµ¨ ÏÇ¨Ìï≠Ïóê Îî∞Îùº Îã¨ÎùºÏßëÎãàÎã§. Sequential ThinkingÏùÄ ÏßÅÍ¥ÄÏ†ÅÏù¥Í≥† Îπ†Î•∏ ÏÇ¨Í≥† Í≥ºÏ†ïÏù¥ ÌïÑÏöîÌï† Îïå Ïú†Ïö©ÌïòÎ©∞, Atom of ThoughtsÎäî Ï≤¥Í≥ÑÏ†ÅÏù∏ Í≤ÄÏ¶ùÍ≥º ÎÜíÏùÄ Ïã†Î¢∞ÏÑ±Ïù¥ ÌïÑÏöîÌïú Î≥µÏû°Ìïú Î¨∏Ï†úÏóê Îçî Ï†ÅÌï©Ìï©ÎãàÎã§.\n\n### Î™ÖÎ†πÏñ¥ ÎèÑÍµ¨ (atomcommands)\n\nAtom of ThoughtsÏùò Î∂ÑÌï¥-ÏàòÏ∂ï Î©îÏª§ÎãàÏ¶òÍ≥º ÏûêÎèô Ï¢ÖÎ£åÎ•º Ï†úÏñ¥ÌïòÎäî Î™ÖÎ†πÏñ¥ ÎèÑÍµ¨ÏûÖÎãàÎã§.\n\n**ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™ÖÎ†πÏñ¥**:\n1. **decompose**: ÏßÄÏ†ïÎêú ÏõêÏûêÎ•º Îçî ÏûëÏùÄ ÌïòÏúÑ ÏõêÏûêÎ°ú Î∂ÑÌï¥Ìï©ÎãàÎã§.\n   - ÌïÑÏöî Îß§Í∞úÎ≥ÄÏàò: `atomId`\n2. **complete_decomposition**: ÏßÑÌñâ Ï§ëÏù∏ Î∂ÑÌï¥ ÌîÑÎ°úÏÑ∏Ïä§Î•º ÏôÑÎ£åÌï©ÎãàÎã§.\n   - ÌïÑÏöî Îß§Í∞úÎ≥ÄÏàò: `decompositionId`\n3. **termination_status**: ÌòÑÏû¨ AoT ÌîÑÎ°úÏÑ∏Ïä§Ïùò Ï¢ÖÎ£å ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï©ÎãàÎã§.\n4. **best_conclusion**: Í∞ÄÏû• ÎÜíÏùÄ Ïã†Î¢∞ÎèÑÏùò Í≤ÄÏ¶ùÎêú Í≤∞Î°†ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.\n5. **set_max_depth**: ÏµúÎåÄ ÍπäÏù¥ Ï†úÌïúÏùÑ Î≥ÄÍ≤ΩÌï©ÎãàÎã§.\n   - ÌïÑÏöî Îß§Í∞úÎ≥ÄÏàò: `maxDepth`\n\n### Installing via Smithery\n\nTo install Atom of Thoughts for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts):\n\n```bash\nnpx -y @smithery/cli install @kbsooo/mcp_atom_of_thoughts --client claude\n```\n\n### MCP ÏÑúÎ≤Ñ ÏÑ§Ï†ï Î∞©Î≤ï\n\nAtom of Thoughts MCP ÏÑúÎ≤ÑÎ•º ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Claude Desktop ÎòêÎäî ClineÏùò MCP ÏÑ§Ï†ïÏóê ÏÑúÎ≤ÑÎ•º Îì±Î°ùÌï¥Ïïº Ìï©ÎãàÎã§. Îã§ÏùåÏùÄ ÏÑúÎ≤Ñ Íµ¨ÏÑ±Ïùò ÏòàÏãúÏûÖÎãàÎã§:\n\n```json\n{ \n  \"mcpServers\": { \n    \"atom-of-thoughts\": { \n      \"command\": \"node\", \n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/atom-of-thoughts/build/index.js\"], \n      \"disabled\": false, \n      \"autoApprove\": [] \n    } \n  } \n}\n```\n\nÏúÑ ÏÑ§Ï†ïÏóêÏÑú `/ABSOLUTE/PATH/TO/PARENT/FOLDER`Îäî Ïã§Ï†ú ÌîÑÎ°úÏ†ùÌä∏Í∞Ä ÏúÑÏπòÌïú Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî. ÏÑ§Ï†ïÏùÑ Ï†ÄÏû•Ìïú ÌõÑ Claude Desktop ÎòêÎäî ClineÏùÑ Ïû¨ÏãúÏûëÌïòÎ©¥ Atom of Thoughts MCP ÏÑúÎ≤ÑÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_atom_of_thoughts",
        "workflows",
        "kbsooo",
        "kbsooo mcp_atom_of_thoughts",
        "mcp_atom_of_thoughts enhance",
        "thought processing"
      ],
      "category": "scientific-research-tools"
    },
    "leescot--pubmed-mcp-smithery": {
      "owner": "leescot",
      "name": "pubmed-mcp-smithery",
      "url": "https://github.com/leescot/pubmed-mcp-smithery",
      "imageUrl": "/freedevtools/mcp/pfp/leescot.webp",
      "description": "Search and retrieve academic papers from the PubMed database using enhanced tools such as MeSH term lookup, publication statistics, and structured PICO-based evidence searches.",
      "stars": 5,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-12T02:43:23Z",
      "readme_content": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "searches",
        "search",
        "pubmed database",
        "papers pubmed",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "liuwanglei-tal--nasa-mcp": {
      "owner": "liuwanglei-tal",
      "name": "nasa-mcp",
      "url": "https://github.com/liuwanglei-tal/nasa-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/liuwanglei-tal.webp",
      "description": "Access a variety of NASA data through natural language queries, including astronomical images, Mars rover photos, Earth satellite images, near-Earth object data, and space weather information.",
      "stars": 2,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-10T14:15:58Z",
      "readme_content": "# NASA MCP Service\n\n‰∏Ä‰∏™ÂäüËÉΩ‰∏∞ÂØåÁöÑ NASA Êï∞ÊçÆÊü•ËØ¢ MCP Â∑•ÂÖ∑ÔºåÊîØÊåÅÂ§öÁßç NASA API ÊúçÂä°ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢„ÄÇ\n\n## ÂäüËÉΩÁâπÁÇπ\n\n- Â§©ÊñáÂõæÁâáÊúçÂä° (APOD)\n  - Ëé∑Âèñ NASA ÊØèÊó•Â§©ÊñáÂõæÁâá\n  - ÊîØÊåÅÂéÜÂè≤Êó•ÊúüÊü•ËØ¢\n  - ËøîÂõûÂõæÁâáÊ†áÈ¢ò„ÄÅÊèèËø∞„ÄÅURLÁ≠â‰ø°ÊÅØ\n\n- ÁÅ´ÊòüÊé¢ÊµãÂô®ÁÖßÁâá (Mars Rovers)\n  - ÊîØÊåÅÂ§ö‰∏™ÁÅ´ÊòüÊé¢ÊµãÂô®ÔºöÂ•ΩÂ•áÂè∑(Curiosity)„ÄÅÊú∫ÈÅáÂè∑(Opportunity)„ÄÅÂãáÊ∞îÂè∑(Spirit)„ÄÅÊØÖÂäõÂè∑(Perseverance)\n  - ÊåâÊó•ÊúüÊü•ËØ¢ÁÅ´ÊòüÁÖßÁâá\n  - Ëé∑Âèñ‰∏çÂêåÁõ∏Êú∫ÊãçÊëÑÁöÑÂõæÁâá\n\n- Âú∞ÁêÉÂç´ÊòüÂõæÂÉè (Earth)\n  - Ê†πÊçÆÁªèÁ∫¨Â∫¶Ëé∑ÂèñÂç´ÊòüÂõæÂÉè\n  - ÊîØÊåÅÂéÜÂè≤ÂõæÂÉèÊü•ËØ¢\n  - Êèê‰æõÈ´òÂàÜËæ®ÁéáÂç´ÊòüÁÖßÁâá\n\n- ËøëÂú∞Â§©‰ΩìÊï∞ÊçÆ (NEO - Near Earth Objects)\n  - Ëé∑ÂèñÁâπÂÆöÊó•ÊúüÁöÑËøëÂú∞Â∞èË°åÊòü‰ø°ÊÅØ\n  - ÂåÖÂê´Â∞èË°åÊòüÂ§ßÂ∞è„ÄÅË∑ùÁ¶ª„ÄÅÈÄüÂ∫¶Á≠âÊï∞ÊçÆ\n  - Âç±Èô©Á≥ªÊï∞ËØÑ‰º∞\n\n- Â§™Á©∫Â§©Ê∞îÊï∞ÊçÆ (Space Weather)\n  - Â§™Èò≥ËÄÄÊñë‰∫ã‰ª∂‰ø°ÊÅØ\n  - Êó•ÂÜïÁâ©Ë¥®ÊäõÂ∞ÑÊï∞ÊçÆ\n  - ÂÆûÊó∂Â§™Á©∫Â§©Ê∞îÁä∂ÂÜµ\n\n## Cursor ÂÆâË£Ö\n\n1. ÊâìÂºÄ Cursor IDE\n2. Âú®ÂëΩ‰ª§Èù¢Êùø‰∏≠ËæìÂÖ•Ôºö\n```bash\n/mcp install @cursor/nasa-mcp\n```\n\n## ÈÖçÁΩÆ\n\n1. ËÆøÈóÆ [NASA API Èó®Êà∑](https://api.nasa.gov/)\n2. Ëé∑Âèñ‰Ω†ÁöÑ API key\n3. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè `NASA_API_KEY` ÊàñÂú®‰ΩøÁî®Êó∂ÈÄöËøáÂèÇÊï∞‰º†ÂÖ•\n\n## ‰ΩøÁî®ÊñπÊ≥ï\n\n### Âú® Cursor ‰∏≠‰ΩøÁî®\n\n```bash\n# Ëé∑Âèñ‰ªäÂ§©ÁöÑÂ§©ÊñáÂõæÁâá\n‰ªäÂ§©ÁöÑÂ§©ÊñáÂõæ\n\n# Ëé∑ÂèñÁÅ´ÊòüÁÖßÁâá\nÁÅ´Êòü Â•ΩÂ•áÂè∑ 2024Âπ¥3Êúà18Êó•\n\n# Ëé∑ÂèñÂú∞ÁêÉÂç´ÊòüÂõæÂÉè\nÂú∞ÁêÉ 39.9042,116.4074\n\n# Ëé∑ÂèñËøëÂú∞Â§©‰ΩìÊï∞ÊçÆ\nËøëÂú∞Â∞èË°åÊòü 2024Âπ¥3Êúà18Êó•\n\n# Ëé∑ÂèñÂ§™Á©∫Â§©Ê∞î‰ø°ÊÅØ\nÂ§™Á©∫Â§©Ê∞î ‰ªäÂ§©\n```\n\n### Âú® Node.js ‰∏≠‰ΩøÁî®\n\n```javascript\nconst client = require('@smithery/client');\n\nasync function main() {\n  // Ëé∑ÂèñÁÅ´ÊòüÁÖßÁâá\n  const marsPhotos = await client.call('@cursor/nasa-mcp', 'ÁÅ´Êòü Â•ΩÂ•áÂè∑ ‰ªäÂ§©');\n  \n  // Ëé∑ÂèñËøëÂú∞Â§©‰ΩìÊï∞ÊçÆ\n  const neoData = await client.call('@cursor/nasa-mcp', 'ËøëÂú∞Â∞èË°åÊòü ‰ªäÂ§©');\n  \n  console.log(marsPhotos);\n  console.log(neoData);\n}\n\nmain();\n```\n\n### ÂëΩ‰ª§Ë°å‰ΩøÁî®\n\n```bash\n# ‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè‰∏≠ÁöÑ API key\necho \"ÁÅ´Êòü Â•ΩÂ•áÂè∑ ‰ªäÂ§©\" | NASA_API_KEY=your_api_key node nasa-mcp.js\n\n# ‰ΩøÁî®Ëá™ÂÆö‰πâ API key\necho \"Âú∞ÁêÉ 39.9042,116.4074\" | node nasa-mcp.js --api_key=your-api-key\n```\n\n## ËøîÂõûÊï∞ÊçÆÊ†ºÂºè\n\n### Â§©ÊñáÂõæÁâá (APOD)\n```json\n{\n  \"Ê†áÈ¢ò\": \"ÂõæÁâáÊ†áÈ¢ò\",\n  \"Êó•Êúü\": \"2024-03-19\",\n  \"ËØ¥Êòé\": \"ËØ¶ÁªÜËß£Èáä\",\n  \"ÂõæÁâáÈìæÊé•\": \"ÂõæÁâáURL\",\n  \"Â™í‰ΩìÁ±ªÂûã\": \"image\",\n  \"ÁâàÊùÉ\": \"ÁâàÊùÉ‰ø°ÊÅØ\"\n}\n```\n\n### ÁÅ´ÊòüÁÖßÁâá\n```json\n{\n  \"ÁÅ´ÊòüËΩ¶\": \"curiosity\",\n  \"Êó•Êúü\": \"2024-03-19\",\n  \"ÁÖßÁâáÊï∞Èáè\": 5,\n  \"ÁÖßÁâáÂàóË°®\": [\n    {\n      \"id\": 1234567,\n      \"ÊãçÊëÑÊó∂Èó¥\": \"2024-03-19\",\n      \"Áõ∏Êú∫\": \"NAVCAM\",\n      \"ÂõæÁâáÈìæÊé•\": \"https://...\"\n    }\n  ]\n}\n```\n\n### ËøëÂú∞Â§©‰ΩìÊï∞ÊçÆ\n```json\n{\n  \"Êó•Êúü\": \"2024-03-19\",\n  \"Â∞èË°åÊòüÊï∞Èáè\": 5,\n  \"Â∞èË°åÊòüÂàóË°®\": [\n    {\n      \"ÂêçÁß∞\": \"Â∞èË°åÊòüÂêçÁß∞\",\n      \"Áõ¥ÂæÑ\": {\n        \"ÊúÄÂ∞è\": 100,\n        \"ÊúÄÂ§ß\": 200\n      },\n      \"ÊòØÂê¶Âç±Èô©\": \"Âê¶\",\n      \"ÊúÄËøëË∑ùÁ¶ª\": \"1000000 ÂÖ¨Èáå\",\n      \"Áõ∏ÂØπÈÄüÂ∫¶\": \"50000 ÂÖ¨Èáå/Â∞èÊó∂\"\n    }\n  ]\n}\n```\n\n### Â§™Á©∫Â§©Ê∞îÊï∞ÊçÆ\n```json\n{\n  \"Êó•Êúü\": \"2024-03-19\",\n  \"Â§™Èò≥ËÄÄÊñë\": [\n    {\n      \"ÂºÄÂßãÊó∂Èó¥\": \"2024-03-19T10:00:00Z\",\n      \"ÁªìÊùüÊó∂Èó¥\": \"2024-03-19T11:00:00Z\",\n      \"Á≠âÁ∫ß\": \"M2.5\",\n      \"‰ΩçÁΩÆ\": \"N30W60\"\n    }\n  ],\n  \"Êó•ÂÜïÁâ©Ë¥®ÊäõÂ∞Ñ\": [\n    {\n      \"Êó∂Èó¥\": \"2024-03-19T12:00:00Z\",\n      \"ÈÄüÂ∫¶\": \"800 km/s\",\n      \"Á±ªÂûã\": \"CME\"\n    }\n  ]\n}\n```\n\n## ËÆ∏ÂèØËØÅ\n\nMIT\n\n## ‰ΩúËÄÖ\n\nsimon liu",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nasa",
        "liuwanglei",
        "mars",
        "nasa mcp",
        "nasa data",
        "tools liuwanglei"
      ],
      "category": "scientific-research-tools"
    },
    "luojunhui1--ImageOnC": {
      "owner": "luojunhui1",
      "name": "ImageOnC",
      "url": "https://github.com/luojunhui1/ImageOnC",
      "imageUrl": "/freedevtools/mcp/pfp/luojunhui1.webp",
      "description": "Implement vehicle license plate recognition using C/C++ on FPGA, utilizing OpenCV for image display and Eigen for optimized matrix operations. The project includes code for training neural networks and processing license plate images.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "C++",
      "updated_at": "2024-05-17T18:19:28Z",
      "readme_content": "# ImageOnC\n## 1.‰ªãÁªç\nÊú¨‰ªìÂ∫ì‰∏∫ÂÆûÁé∞Âú®FPGA‰∏äÁöÑËΩ¶ÁâåËØÜÂà´ËÄåÂàõÂª∫Ôºå‰ΩÜÊú¨‰ªìÂ∫ìÂè™‰øùÊúâC/C++ÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÔºåÂπ∂Êú™‰øùÂ≠ò‰ΩøÁî®HLSÂ∑•ÂÖ∑ÂêéÁöÑ‰ª£Á†Å„ÄÇË¶ÅÁâπÂà´ËØ¥ÊòéÁöÑÊòØÔºåÊú¨È°πÁõÆ‰∏≠ÁöÑ‰ª£Á†ÅÂùáÁî®CÂÆûÁé∞ÔºåÂÖ∂‰∏≠Âá∫Áé∞ÁöÑC++‰∏ªË¶Å‰∏∫‰æø‰∫éOpenCVËøõË°åÂõæÂÉèÊòæÁ§∫ÊàñËÄÖEigenÂ∫ìÂä†ÈÄüÁü©ÈòµËøêÁÆóÔºå‰ΩÜÂùáÂèØÂà†Èô§ÊàñÊîπÊàêC‰∏≠ÁöÑÊï∞ÁªÑËÄå‰∏çÂΩ±ÂìçÂÖ∂Ê≠£Â∏∏ÂäüËÉΩ„ÄÇ\n## 2.Êñá‰ª∂ÁªÑÊàê\n```\n.\n‚îú‚îÄ‚îÄ build\n‚îú‚îÄ‚îÄ cmake-build-debug\n‚îú‚îÄ‚îÄ CMakeLists.txt\n‚îú‚îÄ‚îÄ database\n‚îú‚îÄ‚îÄ Fit.cpp\n‚îú‚îÄ‚îÄ Han\n‚îú‚îÄ‚îÄ include\n‚îú‚îÄ‚îÄ Letters\n‚îú‚îÄ‚îÄ main.cpp\n‚îú‚îÄ‚îÄ paramLetters.txt\n‚îú‚îÄ‚îÄ param.txt\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ Train.cpp\n```\n\nÂÖ∂‰∏≠buildÂíåcmake-build-debugÊñá‰ª∂Âùá‰∏∫ÁºñËØëÊâßË°åËøáÁ®ã‰∫ßÁîüÁöÑÊñá‰ª∂ÔºõCMakeLists.txtÁî®‰∫éÊåáÂØºÁºñËØëÊñπÂºèÔºõdatabase‰∏∫ËΩ¶ÁâåÂõæÁâáÊñá‰ª∂Â§πÔºõFit.cppÂéü‰ΩúÊµãËØïÁΩëÁªúÂáÜÁ°ÆÊÄßÔºå‰ΩÜÂÖ∂ÂÜÖÂÆπÂú®ÊµãËØïÂêéË¢´Êï¥ÂêàÂà∞main.cpp‰∏≠ÔºåÊïÖËØ•Êñá‰ª∂Êó†ÂÆûÈôÖÊÑè‰πâÔºõHanÊñá‰ª∂Â§π‰øùÂ≠ò‰∫ÜÁî®‰∫éËÆ≠ÁªÉÊ±âÂ≠óËØÜÂà´ÁöÑÂõæÂÉèÔºõLetters‰∏≠Âàô‰øùÂ≠ò‰∫ÜÁî®‰∫éËÆ≠ÁªÉÂ≠óÊØçÂíåÊï∞Â≠óËØÜÂà´ÁöÑ‰ª£Á†ÅÔºõmain.cpp‰∏∫ÊâßË°åÁöÑËØÜÂà´ËΩ¶ÁâåÁöÑ‰∏ªÂáΩÊï∞ÔºõTrain.cppÁî®‰∫éËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÔºõparam.txtÂèäparamLetters.txtÂàô‰øùÂ≠ò‰∫ÜÁΩëÁªúÂèÇÊï∞ÔºõincludeÊñá‰ª∂‰∏≠‰øùÂ≠ò‰∫Ü‰∏ÄÂÜôËá™ÂÆö‰πâÁöÑÂäüËÉΩÂáΩÊï∞ÔºåÂÖ∂Êñá‰ª∂Ê†ëÂ¶Ç‰∏ãÔºö\n```\n.\n‚îú‚îÄ‚îÄ Config.h\n‚îú‚îÄ‚îÄ Eigen\n‚îú‚îÄ‚îÄ FileProcess.h\n‚îú‚îÄ‚îÄ ModelTrans.h\n‚îú‚îÄ‚îÄ Net.h\n‚îú‚îÄ‚îÄ Process.h\n‚îú‚îÄ‚îÄ SaveLoad.h\n‚îî‚îÄ‚îÄ unsupported\n```\n**Config.h**: Áî®‰∫éÁ∫¶ÂÆöÁΩëÁªúÂèÇÊï∞Âíå‰∏Ä‰∫õÂÖ®Â±ÄÂèòÈáèÔºå‰æø‰∫éÈ°πÁõÆ‰ª£Á†ÅÁªÑÁªá\n\n**Eigen**: EigenÂ∫ì‰ª£Á†Å\n\n**unsupported**: EigenÂ∫ì‰ª£Á†Å,Âéü‰∏∫‰ΩøÁî®TensorÁ±ªË°®Á§∫È´òÁª¥Áü©ÈòµÔºå‰ΩÜTensor‰ΩøÁî®‰∏ç‰æøÔºåÂÆûÈôÖÊú™‰ΩøÁî®\n\n**FileProcess**: Áî®‰∫éÁ≥ªÁªüÊñá‰ª∂Êìç‰ΩúÔºå‰∏ªË¶ÅÊòØÊü•ËØ¢Êñá‰ª∂Â§π‰∏ãÁöÑÊâÄÊúâÊñá‰ª∂Âπ∂ÈÅçÂéÜ\n\n**ModelTrans**: Áî®‰∫é‰ªéÂõæÂÉèÁöÑÊï∞ÊçÆÁü©Èòµ‰∏≠ËØªÂèñBGRÂõæÂÉèÂπ∂Â∞ÜÂÖ∂ÂàÜÂâ≤„ÄÅ‰øùÂ≠ò\n\n**Net.h**: Á•ûÁªèÁΩëÁªúÁöÑÂÆö‰πâ„ÄÅËÆ≠ÁªÉÂèä‰ΩøÁî®ÈÉ®ÂàÜ\n\n**SaveLoad**: Áî®‰∫é‰ªéÂõæÂÉèË∑ØÂæÑËØªÂèñbmpÂõæÂÉèÂπ∂ÂàÜÈÄöÈÅì‰øùÂ≠òÂõæÂÉèÊï∞ÊçÆÈÉ®ÂàÜ\n\n## 3. ÂÆûÈôÖÊïàÊûú\nÊï∞ÊçÆÈõÜÊØîËæÉÁÆÄÂçïÔºåËÉΩÂÅöÂà∞100%„ÄÇ\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opencv",
        "recognition",
        "imageonc",
        "plate recognition",
        "opencv image",
        "utilizing opencv"
      ],
      "category": "scientific-research-tools"
    },
    "mahdin75--gis-mcp": {
      "owner": "mahdin75",
      "name": "gis-mcp",
      "url": "https://github.com/mahdin75/gis-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mahdin75.webp",
      "description": "Integrates GIS capabilities for geospatial analysis and operations, supporting geometric operations, coordinate transformations, and spatial calculations. Utilizes GIS libraries such as Shapely and PyProj for enhanced AI spatial functionality.",
      "stars": 58,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T02:01:53Z",
      "readme_content": "# GIS MCP Server\n\n<div align=\"center\">\n  <h3>‚ú® Want to perform accurate geospatial analysis in your chatbot? ‚ú®</h3>\n  <p><strong>Install GIS-MCP and transform your AI's spatial capabilities!</strong></p>\n  <br/>\n  <img src=\"docs/Logo.png\" alt=\"GIS MCP Server Logo\" width=\"300\"/>\n  <br/>\n  <br/>\n</div>\n\nA Model Context Protocol (MCP) server implementation that connects Large Language Models (LLMs) to GIS operations using GIS libraries, enabling AI assistants to perform geospatial operations and transformations.\n\nüåê **Website:** [gis-mcp.com](https://gis-mcp.com)\n\n> Current version is 0.9.0 (Beta):\n>\n> Version 0.10.0 (Beta) is under active development. We welcome contributions and developers to join us in building this project.\n\n## üé• Demo\n\n<div align=\"center\">\n  <img src=\"docs/demo.gif\" alt=\"GIS MCP Server Demo\" width=\"800\"/>\n</div>\n\n### Rasterio Demo\n\n<div align=\"center\">\n  <a href=\"https://www.veed.io/view/95ff85f4-efbb-4154-9a04-d966c6ae1737?panel=share\">\n    <br/>\n    <em>Click to watch the Rasterio demo video or go to docs folder</em>\n  </a>\n</div>\n\n## üìã Table of Contents\n\n- [Features](#-features)\n- [Prerequisites](#-prerequisites)\n- [Installation](#-installation)\n  - [Installing via Smithery](#-installing-via-smithery)\n  - [pip Installation](#-pip-installation)\n  - [Development Installation](#-development-installation)\n- [Available Functions](#-available-functions)\n  - [Shapely Functions](#-shapely-functions-29-total)\n  - [PyProj Functions](#-pyproj-functions-13-total)\n  - [GeoPandas Functions](#-geopandas-functions-13-total)\n  - [Rasterio Functions](#-rasterio-functions-20-total)\n  - [PySAL Functions](#-pysal-functions-15-total)\n  - [Visualization Functions](#-visualization-functions-2-total)\n    - [Static Map Functions](#-static-map-functions-1-total)\n    - [Interactive Web Map Functions](#-interactive-web-map-functions-1-total)\n  - [Administrative Boundaries Functions](#-administrative-boundaries-functions-1-total)\n  - [Climate Data Functions](#-climate-data-functions-1-total)\n  - [Ecology Data Functions](#-ecology-data-functions-2-total)\n  - [Movement Data Functions](#-movement-data-functions-2-total)\n  - [Land Cover Data Functions](#-land-cover-data-functions-2-total)\n  - [Satellite Imagery Functions](#-satellite-imagery-functions-1-total)\n- [Client Development](#-client-development)\n- [Planned Features](#-planned-features)\n- [Contributing](#-contributing)\n- [License](#-license)\n- [Related Projects](#-related-projects)\n- [Support](#-support)\n- [Badges](#-badges)\n\n## üöÄ Features\n\nGIS MCP Server empowers AI assistants with advanced geospatial intelligence. Key features include:\n\n- üîπ **Comprehensive Geometry Operations** ‚Äì Perform intersection, union, buffer, difference, and other geometric transformations with ease.\n- üîπ **Advanced Coordinate Transformations** ‚Äì Effortlessly reproject and transform geometries between coordinate reference systems.\n- üîπ **Accurate Measurements** ‚Äì Compute distances, areas, lengths, and centroids precisely.\n- üîπ **Spatial Analysis & Validation** ‚Äì Validate geometries, run proximity checks, and perform spatial overlays or joins.\n- üîπ **Raster & Vector Support** ‚Äì Process raster layers, compute indices like NDVI, clip, resample, and merge with vector data.\n- üîπ **Spatial Statistics & Modeling** ‚Äì Leverage PySAL for spatial autocorrelation, clustering, and neighborhood analysis.\n- üîπ **Easy Integration** ‚Äì Connect seamlessly with MCP-compatible clients like Claude Desktop or Cursor IDE.\n- üîπ **Flexible & Extensible** ‚Äì Supports Python-based GIS libraries and is ready for custom tools or workflow extensions.\n\n> üåü **Tip:** With GIS MCP Server, your AI can now ‚Äúthink spatially,‚Äù unlocking new capabilities for environmental analysis, mapping, and location intelligence.\n\n---\n\n## üìã Prerequisites\n\n- Python 3.10 or higher\n- MCP-compatible client (like Claude Desktop or Cursor)\n- Internet connection for package installation\n\n## üõ† Installation\n\nChoose the installation method that best suits your needs:\n\n### üõ† Installing via Smithery\n\nTo install GIS MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mahdin75/gis-mcp):\n\n```bash\nnpx -y @smithery/cli install @mahdin75/gis-mcp --client claude\n```\n\n### üì¶ pip Installation\n\nThe pip installation is recommended for most users:\n\n1. Install uv package manager:\n\n```bash\npip install uv\n```\n\n2. Create the Virtual Environment (Python 3.10+):\n\n```bash\nuv venv --python=3.10\n```\n\n3. Install the package:\n\n```bash\nuv pip install gis-mcp\n```\n\n#### Install with Visualization Features\n\nTo install with visualization capabilities (Folium and PyDeck for interactive maps):\n\n```bash\nuv pip install gis-mcp[visualize]\n```\n\nThis will install additional dependencies:\n\n- `folium>=0.15.0` - For creating interactive web maps\n- `pydeck>=0.9.0` - For advanced 3D visualizations\n\n4. Start the server:\n\n```bash\ngis-mcp\n```\n\n#### pip Configuration\n\nTo use the pip installation with Claude or Cursor, add the following configuration:\n\n**Claude Desktop:**\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\.venv\\\\Scripts\\\\gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/home/YourUsername/.venv/bin/gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Cursor IDE** (create `.cursor/mcp.json`):\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\.venv\\\\Scripts\\\\gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/home/YourUsername/.venv/bin/gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\nAfter configuration:\n\n1. Make sure to replace `YourUsername` with your actual username\n2. For development installation, replace `/path/to/gis-mcp` with the actual path to your project\n3. Restart your IDE to apply the changes\n4. You can now use all GIS operations through Claude or Cursor!\n\n### üõ† Development Installation\n\nFor contributors and developers:\n\n1. Install uv package manager:\n\n```bash\npip install uv\n```\n\n2. Create the Virtual Environment:\n\n```bash\nuv venv --python=3.10\n```\n\n3. Install the package in development mode:\n\n```bash\nuv pip install -e .\n```\n\n4. Start the server:\n\n```bash\npython -m gis_mcp\n```\n\n#### Development Configuration\n\nTo use the development installation with Claude or Cursor, add the following configuration:\n\n**Claude Desktop:**\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\path\\\\to\\\\gis-mcp\\\\.venv\\\\Scripts\\\\python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/path/to/gis-mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Cursor IDE** (create `.cursor/mcp.json`):\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\path\\\\to\\\\gis-mcp\\\\.venv\\\\Scripts\\\\python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/path/to/gis-mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\nAfter configuration:\n\n1. Make sure to replace `YourUsername` with your actual username\n2. For development installation, replace `/path/to/gis-mcp` with the actual path to your project\n3. Restart your IDE to apply the changes\n4. You can now use all GIS operations through Claude or Cursor!\n\n## üìö Available Functions\n\nThis section provides a comprehensive list of all available functions organized by library.\n\n### üî∑ Shapely Functions (29 total)\n\n**Basic Geometric Operations:**\n\n- `buffer` - Create buffer around geometry\n- `intersection` - Find intersection of two geometries\n- `union` - Combine two geometries\n- `difference` - Find difference between geometries\n- `symmetric_difference` - Find symmetric difference\n\n**Geometric Properties:**\n\n- `convex_hull` - Calculate convex hull\n- `envelope` - Get bounding box\n- `minimum_rotated_rectangle` - Get minimum rotated rectangle\n- `get_centroid` - Get centroid point\n- `get_bounds` - Get geometry bounds\n- `get_coordinates` - Extract coordinate array\n- `get_geometry_type` - Get geometry type name\n\n**Transformations:**\n\n- `rotate_geometry` - Rotate geometry by angle\n- `scale_geometry` - Scale geometry by factors\n- `translate_geometry` - Move geometry by offset\n\n**Advanced Operations:**\n\n- `triangulate_geometry` - Create triangulation\n- `voronoi` - Create Voronoi diagram\n- `unary_union_geometries` - Union multiple geometries\n\n**Measurements:**\n\n- `get_length` - Calculate geometry length\n- `get_area` - Calculate geometry area\n\n**Validation & Utilities:**\n\n- `is_valid` - Check geometry validity\n- `make_valid` - Fix invalid geometry\n- `simplify` - Simplify geometry\n- `snap_geometry` - Snap to reference geometry\n- `nearest_point_on_geometry` - Find nearest point\n- `normalize_geometry` - Normalize orientation\n- `geometry_to_geojson` - Convert to GeoJSON\n- `geojson_to_geometry` - Convert from GeoJSON\n\n### üî∑ PyProj Functions (13 total)\n\n**Coordinate Transformations:**\n\n- `transform_coordinates` - Transform point coordinates\n- `project_geometry` - Project geometry between CRS\n\n**CRS Information:**\n\n- `get_crs_info` - Get detailed CRS information\n- `get_available_crs` - List available CRS systems\n- `get_utm_zone` - Get UTM zone for coordinates\n- `get_utm_crs` - Get UTM CRS for coordinates\n- `get_geocentric_crs` - Get geocentric CRS\n\n**Geodetic Calculations:**\n\n- `get_geod_info` - Get ellipsoid information\n- `calculate_geodetic_distance` - Calculate distance on ellipsoid\n- `calculate_geodetic_point` - Calculate point at distance/azimuth\n- `calculate_geodetic_area` - Calculate area on ellipsoid\n\n### üî∑ GeoPandas Functions (13 total)\n\n**I/O Operations:**\n\n- `read_file_gpd` - Read geospatial file with preview\n- `write_file_gpd` - Export GeoDataFrame to file\n\n**Join & Merge Operations:**\n\n- `append_gpd` - Concatenate GeoDataFrames vertically\n- `merge_gpd` - Database-style attribute joins\n- `overlay_gpd` - Spatial overlay operations\n- `dissolve_gpd` - Dissolve by attribute\n- `explode_gpd` - Split multi-part geometries\n\n**Spatial Operations:**\n\n- `clip_vector` - Clip geometries\n- `sjoin_gpd` - Spatial joins\n- `sjoin_nearest_gpd` - Nearest neighbor spatial joins\n- `point_in_polygon` - Point-in-polygon tests\n\n### üî∑ Rasterio Functions (20 total)\n\n**Basic Raster Operations:**\n\n- `metadata_raster` - Get raster metadata\n- `get_raster_crs` - Get raster CRS\n- `extract_band` - Extract single band\n- `raster_band_statistics` - Calculate band statistics\n- `raster_histogram` - Compute pixel histograms\n\n**Raster Processing:**\n\n- `clip_raster_with_shapefile` - Clip raster with polygons\n- `resample_raster` - Resample by scale factor\n- `reproject_raster` - Reproject to new CRS\n- `tile_raster` - Split into tiles\n\n**Raster Analysis:**\n\n- `compute_ndvi` - Calculate vegetation index\n- `raster_algebra` - Mathematical operations on bands\n- `concat_bands` - Combine single-band rasters\n- `weighted_band_sum` - Weighted band combination\n\n**Advanced Analysis:**\n\n- `zonal_statistics` - Statistics within polygons\n- `reclassify_raster` - Reclassify pixel values\n- `focal_statistics` - Moving window statistics\n- `hillshade` - Generate hillshade from DEM\n- `write_raster` - Write array to raster file\n\n### üî∑ PySAL Functions (15 total)\n\n**Spatial Autocorrelation:**\n\n- `morans_i` - Global Moran's I statistic\n- `gearys_c` - Global Geary's C statistic\n- `gamma_statistic` - Gamma index\n- `getis_ord_g` - Global Getis-Ord G statistic\n\n**Local Statistics:**\n\n- `moran_local` - Local Moran's I\n- `getis_ord_g_local` - Local Getis-Ord G\\*\n- `join_counts_local` - Local join counts\n\n**Global Statistics:**\n\n- `join_counts` - Binary join counts test\n- `adbscan` - Adaptive density-based clustering\n\n**Spatial Weights:**\n\n- `weights_from_shapefile` - Create weights from shapefile\n- `distance_band_weights` - Distance-based weights\n- `knn_weights` - K-nearest neighbors weights\n- `build_transform_and_save_weights` - Build, transform, and save weights\n- `ols_with_spatial_diagnostics_safe` - OLS regression with spatial diagnostics\n- `build_and_transform_weights` - Build and transform weights\n\n### üî∑ Visualization Functions (2 total)\n\n**Static Map Visualization (Matplotlib/GeoPandas):**\n\n- `create_map` ‚Äì Generate high-quality static maps (PNG, PDF, JPG) from multiple geospatial data sources including shapefiles, rasters, WKT geometries, and coordinate arrays. Supports multiple layers with individual styling options, legends, titles, and grid overlays.\n\n**Interactive Web Map Visualization (Folium):**\n\n- `create_web_map` ‚Äì Generate interactive HTML maps using Folium with layer controls, legends, scale bars, dynamic titles, tooltips, and minimap. Supports multiple basemap options and responsive design for web browsers.\n\n### üî∑ Administrative Boundaries Functions (1 total)\n\n**Boundary Download:**\n\n- `download_boundaries` - Download GADM administrative boundaries and save as GeoJSON\n\n### üî∑ Climate Data Functions (1 total)\n\n**Climate Data Download:**\n\n- `download_climate_data` - Download climate data (ERA5 or other CDS datasets)\n\n### üî∑ Ecology Data Functions (2 total)\n\n**Ecology Data Download and Info:**\n\n- `get_species_info` ‚Äì Retrieve taxonomic information for a given species name\n- `download_species_occurrences` ‚Äì Download occurrence records for a given species and save as JSON\n\n### üî∑ Movement Data Functions (2 total)\n\n**Movement Data Download and Routing (via [OSMnx](https://osmnx.readthedocs.io/en/stable/)):**\n\n- `download_street_network` ‚Äì Download a street network for a given place and save as GraphML\n- `calculate_shortest_path` ‚Äì Calculate the shortest path between two points using a saved street network\n\n### üî∑ Land Cover Data Functions (2 total)\n\n**Land Cover from Planetary Computer:**\n\n- `download_worldcover` ‚Äì Download ESA WorldCover for AOI/year; optional crop and reprojection\n- `compute_s2_ndvi` ‚Äì Compute NDVI from Sentinel-2 L2A; crop and reprojection supported\n\n### üî∑ Satellite Imagery Functions (1 total)\n\n**STAC-based Satellite Download:**\n\n- `download_satellite_imagery` ‚Äì Download and stack bands from STAC items (e.g., Sentinel-2, Landsat), with optional crop and reprojection\n\n**Total Functions Available: 89**\n\n## üõ† Client Development\n\nExample usage of the tools:\n\n### Buffer Operation\n\n```python\nTool: buffer\nParameters: {\n    \"geometry\": \"POINT(0 0)\",\n    \"distance\": 10,\n    \"resolution\": 16,\n    \"join_style\": 1,\n    \"mitre_limit\": 5.0,\n    \"single_sided\": false\n}\n```\n\n### Coordinate Transformation\n\n```python\nTool: transform_coordinates\nParameters: {\n    \"coordinates\": [0, 0],\n    \"source_crs\": \"EPSG:4326\",\n    \"target_crs\": \"EPSG:3857\"\n}\n```\n\n### Geodetic Distance\n\n```python\nTool: calculate_geodetic_distance\nParameters: {\n    \"point1\": [0, 0],\n    \"point2\": [10, 10],\n    \"ellps\": \"WGS84\"\n}\n```\n\n### Static Map Creation\n\n```python\nTool: create_map\nParameters: {\n    \"layers\": [\n        {\n            \"data\": \"buildings.shp\",\n            \"style\": {\"label\": \"Buildings\", \"color\": \"red\", \"alpha\": 0.7}\n        },\n        {\n            \"data\": \"roads.shp\",\n            \"style\": {\"label\": \"Roads\", \"color\": \"black\", \"linewidth\": 1}\n        }\n    ],\n    \"filename\": \"city_analysis\",\n    \"filetype\": \"png\",\n    \"title\": \"City Infrastructure Analysis\",\n    \"show_grid\": true,\n    \"add_legend\": true\n}\n```\n\n### Interactive Web Map Creation\n\n```python\nTool: create_web_map\nParameters: {\n    \"layers\": [\n        {\n            \"data\": \"buildings.shp\",\n            \"style\": {\"label\": \"Buildings\", \"color\": \"red\"}\n        },\n        {\n            \"data\": \"parks.geojson\",\n            \"style\": {\"label\": \"Parks\", \"color\": \"green\"}\n        }\n    ],\n    \"filename\": \"city_interactive.html\",\n    \"title\": \"City Infrastructure Map\",\n    \"basemap\": \"CartoDB positron\",\n    \"show_grid\": true,\n    \"add_legend\": true,\n    \"add_minimap\": true\n}\n```\n\n## üîÆ Planned Features\n\n- Implement advanced spatial indexing\n- Implement network analysis capabilities\n- Add support for 3D geometries\n- Implement performance optimizations\n- Add support for more GIS libraries\n\n## ü§ù Contributing\n\nWe welcome contributions! Here's how you can help:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\nPlease ensure your PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## üîó Related Projects\n\n| Project Name                                                                                        | Category                   | Description                                                                                                   |\n| --------------------------------------------------------------------------------------------------- | -------------------------- | ------------------------------------------------------------------------------------------------------------- |\n| [Model Context Protocol](https://github.com/anthropics/model-context-protocol)                      | MCP Related                | The core MCP implementation                                                                                   |\n| [Shapely](https://github.com/shapely/shapely)                                                       | Geospatial Analysis        | Python package for manipulation and analysis of geometric objects                                             |\n| [PyProj](https://github.com/pyproj4/pyproj)                                                         | Geospatial Analysis        | Python interface to PROJ library                                                                              |\n| [GeoPandas](https://github.com/geopandas/geopandas)                                                 | Geospatial Analysis        | Python package for working with geospatial data                                                               |\n| [Rasterio](https://github.com/rasterio/rasterio)                                                    | Geospatial Analysis        | Python package for reading and writing geospatial raster data                                                 |\n| [PySAL](https://github.com/pysal/pysal)                                                             | Geospatial Analysis        | Python spatial analysis library for geospatial data science                                                   |\n| [cdsapi](https://github.com/ecmwf/cdsapi)                                                           | Geospatial Data Collecting | Python API to access the Copernicus Climate Data Store (CDS)                                                  |\n| [pygadm](https://github.com/12rambau/pygadm)                                                        | Geospatial Data Collecting | Easy access to administrative boundary defined by GADM from Python scripts                                    |\n| [pygbif](https://github.com/gbif/pygbif)                                                            | Geospatial Data Collecting | Python client for the GBIF API (ecology and biodiversity data)                                                |\n| [OSMnx](https://osmnx.readthedocs.io/en/stable/)                                                    | Geospatial Data Collecting | Python package for downloading, modeling, and analyzing street networks and urban features from OpenStreetMap |\n| [pystac-client](https://github.com/stac-utils/pystac-client)                                        | Geospatial Data Collecting | Python client for STAC catalogs; search and access spatiotemporal assets                                      |\n| [Planetary Computer SDK for Python](https://github.com/microsoft/planetary-computer-sdk-for-python) | Geospatial Data Collecting | Python SDK for Microsoft Planetary Computer; auth and helpers for STAC/COGs                                   |\n\n## üìû Support\n\nFor support, please open an issue in the GitHub repository.\n\n## üí¨ Community\n\nJoin our Discord community for discussions, updates, and support:\n\n[![Join our Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/dzkXZsZK)\n\n## üë• Contributors\n\n<a href=\"https://github.com/mahdin75/gis-mcp/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=mahdin75/gis-mcp\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n<br/>\n\n## üèÜ Badges\n\n<div align=\"center\">\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/mahdin75/gis-mcp)](https://archestra.ai/mcp-catalog/mahdin75__gis-mcp)\n<br/></br>\n\n  <a href=\"https://glama.ai/mcp/servers/@mahdin75/gis-mcp\">\n    <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mahdin75/gis-mcp/badge\" alt=\"GIS Server MCP server\" />\n  </a>\n  <br/><br/><br/>\n  \n  <a href=\"https://mcp.so/server/gis-mcp-server/mahdin75\">\n    <img src=\"https://mcp.so/logo.png\" alt=\"MCP.so Badge\" width=\"150\"/>\n  </a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gis",
        "mahdin75",
        "mcp",
        "mahdin75 gis",
        "gis mcp",
        "utilizes gis"
      ],
      "category": "scientific-research-tools"
    },
    "mrexodia--ida-pro-mcp": {
      "owner": "mrexodia",
      "name": "ida-pro-mcp",
      "url": "https://github.com/mrexodia/ida-pro-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mrexodia.webp",
      "description": "Enables reverse engineering in IDA Pro through a simple integration that allows interaction with various MCP tools. Provides functionality for checking connection status, retrieving metadata, and accessing functions by name or address.",
      "stars": 3801,
      "forks": 382,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T11:29:25Z",
      "readme_content": "# IDA Pro MCP\r\n\r\nSimple [MCP Server](https://modelcontextprotocol.io/introduction) to allow vibe reversing in IDA Pro.\r\n\r\nhttps://github.com/user-attachments/assets/6ebeaa92-a9db-43fa-b756-eececce2aca0\r\n\r\nThe binaries and prompt for the video are available in the [mcp-reversing-dataset](https://github.com/mrexodia/mcp-reversing-dataset) repository.\r\n\r\nAvailable functionality:\r\n\r\n- `check_connection()`: Check if the IDA plugin is running.\r\n- `get_metadata()`: Get metadata about the current IDB.\r\n- `get_function_by_name(name)`: Get a function by its name.\r\n- `get_function_by_address(address)`: Get a function by its address.\r\n- `get_current_address()`: Get the address currently selected by the user.\r\n- `get_current_function()`: Get the function currently selected by the user.\r\n- `convert_number(text, size)`: Convert a number (decimal, hexadecimal) to different representations.\r\n- `list_functions(offset, count)`: List all functions in the database (paginated).\r\n- `list_globals_filter(offset, count, filter)`: List matching globals in the database (paginated, filtered).\r\n- `list_globals(offset, count)`: List all globals in the database (paginated).\r\n- `list_strings_filter(offset, count, filter)`: List matching strings in the database (paginated, filtered).\r\n- `list_strings(offset, count)`: List all strings in the database (paginated).\r\n- `list_local_types()`: List all Local types in the database.\r\n- `decompile_function(address)`: Decompile a function at the given address.\r\n- `disassemble_function(start_address)`: Get assembly code (address: instruction; comment) for a function.\r\n- `get_xrefs_to(address)`: Get all cross references to the given address.\r\n- `get_xrefs_to_field(struct_name, field_name)`: Get all cross references to a named struct field (member).\r\n- `get_entry_points()`: Get all entry points in the database.\r\n- `set_comment(address, comment)`: Set a comment for a given address in the function disassembly and pseudocode.\r\n- `rename_local_variable(function_address, old_name, new_name)`: Rename a local variable in a function.\r\n- `rename_global_variable(old_name, new_name)`: Rename a global variable.\r\n- `set_global_variable_type(variable_name, new_type)`: Set a global variable's type.\r\n- `rename_function(function_address, new_name)`: Rename a function.\r\n- `set_function_prototype(function_address, prototype)`: Set a function's prototype.\r\n- `declare_c_type(c_declaration)`: Create or update a local type from a C declaration.\r\n- `set_local_variable_type(function_address, variable_name, new_type)`: Set a local variable's type.\r\n\r\nUnsafe functions (`--unsafe` flag required):\r\n\r\n- `dbg_get_registers()`: Get all registers and their values. This function is only available when debugging.\r\n- `dbg_get_call_stack()`: Get the current call stack.\r\n- `dbg_list_breakpoints()`: List all breakpoints in the program.\r\n- `dbg_start_process()`: Start the debugger.\r\n- `dbg_exit_process()`: Exit the debugger.\r\n- `dbg_continue_process()`: Continue the debugger.\r\n- `dbg_run_to(address)`: Run the debugger to the specified address.\r\n- `dbg_set_breakpoint(address)`: Set a breakpoint at the specified address.\r\n- `dbg_delete_breakpoint(address)`: del a breakpoint at the specified address.\r\n- `dbg_enable_breakpoint(address, enable)`: Enable or disable a breakpoint at the specified address.\r\n\r\n## Prerequisites\r\n\r\n- [Python](https://www.python.org/downloads/) (**3.11 or higher**) \r\n  - Use `idapyswitch` to switch to the newest Python version\r\n- [IDA Pro](https://hex-rays.com/ida-pro) (8.3 or higher, 9 recommended), **IDA Free is not supported**\r\n- Supported MCP Client (pick one you like)\r\n  - [Cline](https://cline.bot)\r\n  - [Roo Code](https://roocode.com)\r\n  - [Claude](https://claude.ai/download)\r\n  - [Cursor](https://cursor.com)\r\n  - [VSCode Agent Mode](https://github.blog/news-insights/product-news/github-copilot-agent-mode-activated/)\r\n  - [Windsurf](https://windsurf.com)\r\n  - [Other MCP Clients](https://modelcontextprotocol.io/clients#example-clients): Run `ida-pro-mcp --config` to get the JSON config for your client.\r\n\r\n## Installation\r\n\r\nInstall the latest version of the IDA Pro MCP package:\r\n\r\n```sh\r\npip uninstall ida-pro-mcp\r\npip install https://github.com/mrexodia/ida-pro-mcp/archive/refs/heads/main.zip\r\n```\r\n\r\nConfigure the MCP servers and install the IDA Plugin:\r\n\r\n```\r\nida-pro-mcp --install\r\n```\r\n\r\n**Important**: Make sure you completely restart IDA/Visual Studio Code/Claude for the installation to take effect. Claude runs in the background and you need to quit it from the tray icon.\r\n\r\nhttps://github.com/user-attachments/assets/65ed3373-a187-4dd5-a807-425dca1d8ee9\r\n\r\n_Note_: You need to load a binary in IDA before the plugin menu will show up.\r\n\r\n## Prompt Engineering\r\n\r\nLLMs are prone to hallucinations and you need to be specific with your prompting. For reverse engineering the conversion between integers and bytes are especially problematic. Below is a minimal example prompt, feel free to start a discussion or open an issue if you have good results with a different prompt:\r\n\r\n> Your task is to analyze a crackme in IDA Pro. You can use the MCP tools to retrieve information. In general use the following strategy:\r\n> - Inspect the decompilation and add comments with your findings\r\n> - Rename variables to more sensible names\r\n> - Change the variable and argument types if necessary (especially pointer and array types)\r\n> - Change function names to be more descriptive\r\n> - If more details are necessary, disassemble the function and add comments with your findings\r\n> - NEVER convert number bases yourself. Use the convert_number MCP tool if needed!\r\n> - Do not attempt brute forcing, derive any solutions purely from the disassembly and simple python scripts\r\n> - Create a report.md with your findings and steps taken at the end\r\n> - When you find a solution, prompt to user for feedback with the password you found\r\n\r\nThis prompt was just the first experiment, please share if you found ways to improve the output!\r\n\r\nLive stream discussing prompting and showing some real-world malware analysis:\r\n\r\n[![](https://img.youtube.com/vi/iFxNuk3kxhk/0.jpg)](https://www.youtube.com/watch?v=iFxNuk3kxhk)\r\n\r\n## Tips for Enhancing LLM Accuracy\r\n\r\nLarge Language Models (LLMs) are powerful tools, but they can sometimes struggle with complex mathematical calculations or exhibit \"hallucinations\" (making up facts). Make sure to tell the LLM to use the `conver_number` MCP and you might also need [math-mcp](https://github.com/EthanHenrickson/math-mcp) for certain operations.\r\n\r\nAnother thing to keep in mind is that LLMs will not perform well on obfuscated code. Before trying to use an LLM to solve the problem, take a look around the binary and spend some time (automatically) removing the following things:\r\n\r\n- String encryption\r\n- Import hashing\r\n- Control flow flattening\r\n- Code encryption\r\n- Anti-decompilation tricks\r\n\r\nYou should also use a tool like Lumina or FLIRT to try and resolve all the open source library code and the C++ STL, this will further improve the accuracy.\r\n\r\n## SSE Transport & Headless MCP\r\n\r\nYou can run an SSE server to connect to the user interface like this:\r\n\r\n```sh\r\nuv run ida-pro-mcp --transport http://127.0.0.1:8744/sse\r\n```\r\n\r\nAfter installing [`idalib`](https://docs.hex-rays.com/user-guide/idalib) you can also run a headless SSE server:\r\n\r\n```sh\r\nuv run idalib-mcp --host 127.0.0.1 --port 8745 path/to/executable\r\n```\r\n\r\n_Note_: The `idalib` feature was contributed by [Willi Ballenthin](https://github.com/williballenthin).\r\n\r\n## Manual Installation\r\n\r\n_Note_: This section is for LLMs and power users who need detailed installation instructions.\r\n\r\n<details>\r\n\r\n## Manual MCP Server Installation (Cline/Roo Code)\r\n\r\nTo install the MCP server yourself, follow these steps:\r\n\r\n1. Install [uv](https://github.com/astral-sh/uv) globally:\r\n   - Windows: `pip install uv`\r\n   - Linux/Mac: `curl -LsSf https://astral.sh/uv/install.sh | sh`\r\n2. Clone this repository, for this example `C:\\MCP\\ida-pro-mcp`.\r\n3. Navigate to the Cline/Roo Code _MCP Servers_ configuration (see screenshot).\r\n4. Click on the _Installed_ tab.\r\n5. Click on _Configure MCP Servers_, which will open `cline_mcp_settings.json`.\r\n6. Add the `ida-pro-mcp` server:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"github.com/mrexodia/ida-pro-mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"c:\\\\MCP\\\\ida-pro-mcp\",\r\n        \"run\",\r\n        \"server.py\",\r\n        \"--install-plugin\"\r\n      ],\r\n      \"timeout\": 1800,\r\n      \"disabled\": false\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo check if the connection works you can perform the following tool call:\r\n\r\n```\r\n<use_mcp_tool>\r\n<server_name>github.com/mrexodia/ida-pro-mcp</server_name>\r\n<tool_name>check_connection</tool_name>\r\n<arguments></arguments>\r\n</use_mcp_tool>\r\n```\r\n\r\n## IDA Plugin installation\r\n\r\nThe IDA Pro plugin will be installed automatically when the MCP server starts. If you disabled the `--install-plugin` option, use the following steps:\r\n\r\n1. Copy (**not move**) `src/ida_pro_mcp/mcp-plugin.py` in your plugins folder (`%appdata%\\Hex-Rays\\IDA Pro\\plugins` on Windows).\r\n2. Open an IDB and click `Edit -> Plugins -> MCP` to start the server.\r\n\r\n</details>\r\n\r\n## Comparison with other MCP servers\r\n\r\nThere are a few IDA Pro MCP servers floating around, but I created my own for a few reasons:\r\n\r\n1. Installation should be fully automated.\r\n2. The architecture of other plugins make it difficult to add new functionality quickly (too much boilerplate of unnecessary dependencies).\r\n3. Learning new technologies is fun!\r\n\r\nIf you want to check them out, here is a list (in the order I discovered them):\r\n\r\n- https://github.com/taida957789/ida-mcp-server-plugin (SSE protocol only, requires installing dependencies in IDAPython).\r\n- https://github.com/fdrechsler/mcp-server-idapro (MCP Server in TypeScript, excessive boilerplate required to add new functionality).\r\n- https://github.com/MxIris-Reverse-Engineering/ida-mcp-server (custom socket protocol, boilerplate).\r\n\r\nFeel free to open a PR to add your IDA Pro MCP server here.\r\n\r\n## Development\r\n\r\nAdding new features is a super easy and streamlined process. All you have to do is add a new `@jsonrpc` function to [`mcp-plugin.py`](https://github.com/mrexodia/ida-pro-mcp/blob/164df8cf4ae251cc9cc0f464591fa6df8e0d9df4/src/ida_pro_mcp/mcp-plugin.py#L406-L419) and your function will be available in the MCP server without any additional boilerplate! Below is a video where I add the `get_metadata` function in less than 2 minutes (including testing):\r\n\r\nhttps://github.com/user-attachments/assets/951de823-88ea-4235-adcb-9257e316ae64\r\n\r\nTo test the MCP server itself:\r\n\r\n```sh\r\nuv run mcp dev src/ida_pro_mcp/server.py\r\n```\r\n\r\nThis will open a web interface at http://localhost:5173 and allow you to interact with the MCP tools for testing.\r\n\r\nFor testing I create a symbolic link to the IDA plugin and then POST a JSON-RPC request directly to `http://localhost:13337/mcp`. After [enabling symbolic links](https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) you can run the following command:\r\n\r\n```sh\r\nuv run ida-pro-mcp --install\r\n```\r\n\r\nGenerate the changelog of direct commits to `main`:\r\n\r\n```sh\r\ngit log --first-parent --no-merges 1.2.0..main \"--pretty=- %s\"\r\n```\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "mrexodia",
        "tools",
        "mrexodia ida",
        "mcp tools",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "/freedevtools/mcp/pfp/nansasuke.webp",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\nÂõæÁâáËØÜÂà´„ÄÅËØ≠Èü≥ËØÜÂà´„ÄÅÂûÉÂúæÂàÜÁ±ª\n\n‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂûÉÂúæÂàÜÁ±ªÁöÑapp\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recycling",
        "waste",
        "garbagesorting",
        "classify waste",
        "recycling process",
        "nansasuke garbagesorting"
      ],
      "category": "scientific-research-tools"
    },
    "nguyenvanduocit--all-in-one-model-context-protocol": {
      "owner": "nguyenvanduocit",
      "name": "all-in-one-model-context-protocol",
      "url": "https://github.com/nguyenvanduocit/all-in-one-model-context-protocol",
      "imageUrl": "/freedevtools/mcp/pfp/nguyenvanduocit.webp",
      "description": "Integrate various tools for enhanced development workflows, including managing emails, calendars, and research tasks. It provides functionalities for development tools, data fetching, and leveraging academic resources.",
      "stars": 88,
      "forks": 8,
      "license": "No License",
      "language": "",
      "updated_at": "2025-09-30T10:28:07Z",
      "readme_content": "# THE PROJECT IS MOVED\n\nTHE PROJECT HAS BEEN SPLIT AND MOVED TO INDIVIDUAL REPOSITORIES.\n\n- [Google Kit](https://github.com/nguyenvanduocit/google-kit): Tools for Gmail, Google Calendar, Google Chat\n- [RAG Kit](https://github.com/nguyenvanduocit/rag-kit): Tools for RAG, Memory\n- [Dev Kit](https://github.com/nguyenvanduocit/dev-kit): Tools for developers, jira, confluence, gitlab, github, ...\n- [Fetch Kit](https://github.com/nguyenvanduocit/fetch-kit): Tools for fetch, scrape, ...\n- [Research Kit](https://github.com/nguyenvanduocit/research-kit): Tools for research, academic, reasoning, ...\n\n# MyMCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nguyenvanduocit/all-in-one-model-context-protocol)](https://smithery.ai/server/@nguyenvanduocit/all-in-one-model-context-protocol)\nA powerful Model Context Protocol (MCP) server implementation with integrations for GitLab, Jira, Confluence, YouTube, and more. This server provides AI-powered search capabilities and various utility tools for development workflows.\n\n[Tutorial](https://www.youtube.com/watch?v=XnDFtYKU6xU)\n\n## Community\n\nFor community support, discussions, and updates, please visit our forum at [community.aiocean.io](https://community.aiocean.io/).\n\n\n## Prerequisites\n\n- Go 1.23.2 or higher\n- Various API keys and tokens for the services you want to use\n\n## Installation\n\n### Installing via Smithery\n\nTo install MyMCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nguyenvanduocit/all-in-one-model-context-protocol) (will guide you through interactive CLI setup):\n\n```bash\nnpx -y @smithery/cli install @nguyenvanduocit/all-in-one-model-context-protocol --client claude\n```\n\n*Note: Smithery will interactively prompt you for required configuration values and handle environment setup automatically*\n\n### Installing via Go\n\n1. Install the server:\n```bash\ngo install github.com/nguyenvanduocit/all-in-one-model-context-protocol@latest\n```\n\n2. **Manual setup required** - Create a `.env` file with your configuration:\n```env\nENABLE_TOOLS=\nQDRANT_HOST=\nATLASSIAN_HOST=\nATLASSIAN_EMAIL=\nGITLAB_HOST=\nGITLAB_TOKEN=\nBRAVE_API_KEY=\nATLASSIAN_TOKEN=\nGOOGLE_AI_API_KEY=\nPROXY_URL=\nOPENAI_API_KEY=\nDEEPSEEK_API_KEY=\nQDRANT_PORT=\nGOOGLE_TOKEN_FILE=\nGOOGLE_CREDENTIALS_FILE=\nQDRANT_API_KEY=\n```\n\n3. Config your claude's config:\n\n```json{claude_desktop_config.json}\n{\n  \"mcpServers\": {\n    \"my_mcp_server\": {\n      \"command\": \"all-in-one-model-context-protocol\",\n      \"args\": [\"-env\", \"/path/to/.env\"],\n    }\n  }\n}\n```\n\n## Secret\n\n*GOOGLE_TOKEN_FILE* is using for Google AI like Gemini, Google Search, Google Calendar, etc.\n\n*GOOGLE_CREDENTIALS_FILE* is service account for advanced feature like Google Chat. You need to create project in Google Cloud Console and create service account. Then also need to create Google cloud bot chat for it. Ask any LLM model to create these for you.\n\n\n\n## Enable Tools\n\nThere are a hidden variable `ENABLE_TOOLS` in the environment variable. It is a comma separated list of tools group to enable. If not set, all tools will be enabled. Leave it empty to enable all tools.\n\n\nHere is the list of tools group:\n\n- `gemini`: Gemini-powered search\n- `fetch`: Fetch tools\n- `confluence`: Confluence tools\n- `youtube`: YouTube tools\n- `jira`: Jira tools\n- `gitlab`: GitLab tools\n- `script`: Script tools\n- `rag`: RAG tools\n- `deepseek`: Deepseek AI tools\n\n## Available Tools\n\n### calendar_create_event\n\nCreate a new event in Google Calendar\n\nArguments:\n\n- `summary` (String) (Required): Title of the event\n- `description` (String): Description of the event\n- `start_time` (String) (Required): Start time of the event in RFC3339 format (e.g., 2023-12-25T09:00:00Z)\n- `end_time` (String) (Required): End time of the event in RFC3339 format\n- `attendees` (String): Comma-separated list of attendee email addresses\n\n### calendar_list_events\n\nList upcoming events in Google Calendar\n\nArguments:\n\n- `time_min` (String): Start time for the search in RFC3339 format (default: now)\n- `time_max` (String): End time for the search in RFC3339 format (default: 1 week from now)\n- `max_results` (Number): Maximum number of events to return (default: 10)\n\n### calendar_update_event\n\nUpdate an existing event in Google Calendar\n\nArguments:\n\n- `event_id` (String) (Required): ID of the event to update\n- `summary` (String): New title of the event\n- `description` (String): New description of the event\n- `start_time` (String): New start time of the event in RFC3339 format\n- `end_time` (String): New end time of the event in RFC3339 format\n- `attendees` (String): Comma-separated list of new attendee email addresses\n\n### calendar_respond_to_event\n\nRespond to an event invitation in Google Calendar\n\nArguments:\n\n- `event_id` (String) (Required): ID of the event to respond to\n- `response` (String) (Required): Your response (accepted, declined, or tentative)\n\n### confluence_search\n\nSearch Confluence\n\nArguments:\n\n- `query` (String) (Required): Atlassian Confluence Query Language (CQL)\n\n### confluence_get_page\n\nGet Confluence page content\n\nArguments:\n\n- `page_id` (String) (Required): Confluence page ID\n\n### confluence_create_page\n\nCreate a new Confluence page\n\nArguments:\n\n- `space_key` (String) (Required): The key of the space where the page will be created\n- `title` (String) (Required): Title of the page\n- `content` (String) (Required): Content of the page in storage format (XHTML)\n- `parent_id` (String): ID of the parent page (optional)\n\n### confluence_update_page\n\nUpdate an existing Confluence page\n\nArguments:\n\n- `page_id` (String) (Required): ID of the page to update\n- `title` (String): New title of the page (optional)\n- `content` (String): New content of the page in storage format (XHTML)\n- `version_number` (String): Version number for optimistic locking (optional)\n\n### deepseek_reasoning\n\nadvanced reasoning engine using Deepseek's AI capabilities for multi-step problem solving, critical analysis, and strategic decision support\n\nArguments:\n\n- `question` (String) (Required): The structured query or problem statement requiring deep analysis and reasoning\n- `context` (String) (Required): Defines the operational context and purpose of the query within the MCP ecosystem\n- `knowledge` (String): Provides relevant chat history, knowledge base entries, and structured data context for MCP-aware reasoning\n\n### get_web_content\n\nFetches content from a given HTTP/HTTPS URL. This tool allows you to retrieve text content from web pages, APIs, or any accessible HTTP endpoints. Returns the raw content as text.\n\nArguments:\n\n- `url` (String) (Required): The complete HTTP/HTTPS URL to fetch content from (e.g., https://example.com)\n\n### gchat_list_spaces\n\nList all available Google Chat spaces/rooms\n\n### gchat_send_message\n\nSend a message to a Google Chat space or direct message\n\nArguments:\n\n- `space_name` (String) (Required): Name of the space to send the message to\n- `message` (String) (Required): Text message to send\n\n### ai_web_search\n\nsearch the web by using Google AI Search. Best tool to update realtime information\n\nArguments:\n\n- `question` (String) (Required): The question to ask. Should be a question\n- `context` (String) (Required): Context/purpose of the question, helps Gemini to understand the question better\n\n### gitlab_list_projects\n\nList GitLab projects\n\nArguments:\n\n- `group_id` (String) (Required): gitlab group ID\n- `search` (String): Multiple terms can be provided, separated by an escaped space, either + or %20, and will be ANDed together. Example: one+two will match substrings one and two (in any order).\n\n### gitlab_get_project\n\nGet GitLab project details\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n\n### gitlab_list_mrs\n\nList merge requests\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `state` (String) (Default: all): MR state (opened/closed/merged)\n\n### gitlab_get_mr_details\n\nGet merge request details\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `mr_iid` (String) (Required): Merge request IID\n\n### gitlab_create_MR_note\n\nCreate a note on a merge request\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `mr_iid` (String) (Required): Merge request IID\n- `comment` (String) (Required): Comment text\n\n### gitlab_get_file_content\n\nGet file content from a GitLab repository\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `file_path` (String) (Required): Path to the file in the repository\n- `ref` (String) (Required): Branch name, tag, or commit SHA\n\n### gitlab_list_pipelines\n\nList pipelines for a GitLab project\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `status` (String) (Default: all): Pipeline status (running/pending/success/failed/canceled/skipped/all)\n\n### gitlab_list_commits\n\nList commits in a GitLab project within a date range\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `since` (String) (Required): Start date (YYYY-MM-DD)\n- `until` (String): End date (YYYY-MM-DD). If not provided, defaults to current date\n- `ref` (String) (Required): Branch name, tag, or commit SHA\n\n### gitlab_get_commit_details\n\nGet details of a commit\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `commit_sha` (String) (Required): Commit SHA\n\n### gitlab_list_user_events\n\nList GitLab user events within a date range\n\nArguments:\n\n- `username` (String) (Required): GitLab username\n- `since` (String) (Required): Start date (YYYY-MM-DD)\n- `until` (String): End date (YYYY-MM-DD). If not provided, defaults to current date\n\n### gitlab_list_group_users\n\nList all users in a GitLab group\n\nArguments:\n\n- `group_id` (String) (Required): GitLab group ID\n\n### gitlab_create_mr\n\nCreate a new merge request\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `source_branch` (String) (Required): Source branch name\n- `target_branch` (String) (Required): Target branch name\n- `title` (String) (Required): Merge request title\n- `description` (String): Merge request description\n\n### gmail_search\n\nSearch emails in Gmail using Gmail's search syntax\n\nArguments:\n\n- `query` (String) (Required): Gmail search query. Follow Gmail's search syntax\n\n### gmail_move_to_spam\n\nMove specific emails to spam folder in Gmail by message IDs\n\nArguments:\n\n- `message_ids` (String) (Required): Comma-separated list of message IDs to move to spam\n\n### gmail_create_filter\n\nCreate a Gmail filter with specified criteria and actions\n\nArguments:\n\n- `from` (String): Filter emails from this sender\n- `to` (String): Filter emails to this recipient\n- `subject` (String): Filter emails with this subject\n- `query` (String): Additional search query criteria\n- `add_label` (Boolean): Add label to matching messages\n- `label_name` (String): Name of the label to add (required if add_label is true)\n- `mark_important` (Boolean): Mark matching messages as important\n- `mark_read` (Boolean): Mark matching messages as read\n- `archive` (Boolean): Archive matching messages\n\n### gmail_list_filters\n\nList all Gmail filters in the account\n\n### gmail_list_labels\n\nList all Gmail labels in the account\n\n### gmail_delete_filter\n\nDelete a Gmail filter by its ID\n\nArguments:\n\n- `filter_id` (String) (Required): The ID of the filter to delete\n\n### gmail_delete_label\n\nDelete a Gmail label by its ID\n\nArguments:\n\n- `label_id` (String) (Required): The ID of the label to delete\n\n### jira_get_issue\n\nRetrieve detailed information about a specific Jira issue including its status, assignee, description, subtasks, and available transitions\n\nArguments:\n\n- `issue_key` (String) (Required): The unique identifier of the Jira issue (e.g., KP-2, PROJ-123)\n\n### jira_search_issue\n\nSearch for Jira issues using JQL (Jira Query Language). Returns key details like summary, status, assignee, and priority for matching issues\n\nArguments:\n\n- `jql` (String) (Required): JQL query string (e.g., 'project = KP AND status = \\\"In Progress\\\"')\n\n### jira_list_sprints\n\nList all active and future sprints for a specific Jira board, including sprint IDs, names, states, and dates\n\nArguments:\n\n- `board_id` (String) (Required): Numeric ID of the Jira board (can be found in board URL)\n\n### jira_create_issue\n\nCreate a new Jira issue with specified details. Returns the created issue's key, ID, and URL\n\nArguments:\n\n- `project_key` (String) (Required): Project identifier where the issue will be created (e.g., KP, PROJ)\n- `summary` (String) (Required): Brief title or headline of the issue\n- `description` (String) (Required): Detailed explanation of the issue\n- `issue_type` (String) (Required): Type of issue to create (common types: Bug, Task, Story, Epic)\n\n### jira_update_issue\n\nModify an existing Jira issue's details. Supports partial updates - only specified fields will be changed\n\nArguments:\n\n- `issue_key` (String) (Required): The unique identifier of the issue to update (e.g., KP-2)\n- `summary` (String): New title for the issue (optional)\n- `description` (String): New description for the issue (optional)\n\n### jira_list_statuses\n\nRetrieve all available issue status IDs and their names for a specific Jira project\n\nArguments:\n\n- `project_key` (String) (Required): Project identifier (e.g., KP, PROJ)\n\n### jira_transition_issue\n\nTransition an issue through its workflow using a valid transition ID. Get available transitions from jira_get_issue\n\nArguments:\n\n- `issue_key` (String) (Required): The issue to transition (e.g., KP-123)\n- `transition_id` (String) (Required): Transition ID from available transitions list\n- `comment` (String): Optional comment to add with transition\n\n### RAG_memory_index_content\n\nIndex a content into memory, can be inserted or updated\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): content file path\n- `payload` (String) (Required): Plain text payload\n\n### RAG_memory_index_file\n\nIndex a local file into memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): Path to the local file to be indexed\n\n### RAG_memory_create_collection\n\nCreate a new vector collection in memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n\n### RAG_memory_delete_collection\n\nDelete a vector collection in memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n\n### RAG_memory_list_collections\n\nList all vector collections in memory\n\n### RAG_memory_search\n\nSearch for memory in a collection based on a query\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `query` (String) (Required): search query, should be a keyword\n\n### RAG_memory_delete_index_by_filepath\n\nDelete a vector index by filePath\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): Path to the local file to be deleted\n\n### execute_comand_line_script\n\nSafely execute command line scripts on the user's system with security restrictions. Features sandboxed execution, timeout protection, and output capture. Supports cross-platform scripting with automatic environment detection.\n\nArguments:\n\n- `content` (String) (Required): \n- `interpreter` (String) (Default: /bin/sh): Path to interpreter binary (e.g. /bin/sh, /bin/bash, /usr/bin/python, cmd.exe). Validated against allowed list for security\n- `working_dir` (String): Execution directory path (default: user home). Validated to prevent unauthorized access to system locations\n\n### web_search\n\nSearch the web using Brave Search API\n\nArguments:\n\n- `query` (String) (Required): Query to search for (max 400 chars, 50 words)\n- `count` (Number) (Default: 5): Number of results (1-20, default 5)\n- `country` (String) (Default: ALL): Country code\n\n### sequentialthinking\n\n`A detailed tool for dynamic and reflective problem-solving through thoughts.\nThis tool helps analyze problems through a flexible thinking process that can adapt and evolve.\nEach thought can build on, question, or revise previous insights as understanding deepens.\n\nWhen to use this tool:\n- Breaking down complex problems into steps\n- Planning and design with room for revision\n- Analysis that might need course correction\n- Problems where the full scope might not be clear initially\n- Problems that require a multi-step solution\n- Tasks that need to maintain context over multiple steps\n- Situations where irrelevant information needs to be filtered out\n\nKey features:\n- You can adjust total_thoughts up or down as you progress\n- You can question or revise previous thoughts\n- You can add more thoughts even after reaching what seemed like the end\n- You can express uncertainty and explore alternative approaches\n- Not every thought needs to build linearly - you can branch or backtrack\n- Generates a solution hypothesis\n- Verifies the hypothesis based on the Chain of Thought steps\n- Repeats the process until satisfied\n- Provides a correct answer\n\nParameters explained:\n- thought: Your current thinking step, which can include:\n* Regular analytical steps\n* Revisions of previous thoughts\n* Questions about previous decisions\n* Realizations about needing more analysis\n* Changes in approach\n* Hypothesis generation\n* Hypothesis verification\n- next_thought_needed: True if you need more thinking, even if at what seemed like the end\n- thought_number: Current number in sequence (can go beyond initial total if needed)\n- total_thoughts: Current estimate of thoughts needed (can be adjusted up/down)\n- is_revision: A boolean indicating if this thought revises previous thinking\n- revises_thought: If is_revision is true, which thought number is being reconsidered\n- branch_from_thought: If branching, which thought number is the branching point\n- branch_id: Identifier for the current branch (if any)\n- needs_more_thoughts: If reaching end but realizing more thoughts needed\n\nYou should:\n1. Start with an initial estimate of needed thoughts, but be ready to adjust\n2. Feel free to question or revise previous thoughts\n3. Don't hesitate to add more thoughts if needed, even at the \"end\"\n4. Express uncertainty when present\n5. Mark thoughts that revise previous thinking or branch into new paths\n6. Ignore information that is irrelevant to the current step\n7. Generate a solution hypothesis when appropriate\n8. Verify the hypothesis based on the Chain of Thought steps\n9. Repeat the process until satisfied with the solution\n10. Provide a single, ideally correct answer as the final output\n11. Only set next_thought_needed to false when truly done and a satisfactory answer is reached`\n\nArguments:\n\n- `thought` (String) (Required): Your current thinking step\n- `nextThoughtNeeded` (Boolean) (Required): Whether another thought step is needed\n- `thoughtNumber` (Number) (Required): Current thought number\n- `totalThoughts` (Number) (Required): Estimated total thoughts needed\n- `isRevision` (Boolean): Whether this revises previous thinking\n- `revisesThought` (Number): Which thought is being reconsidered\n- `branchFromThought` (Number): Branching point thought number\n- `branchId` (String): Branch identifier\n- `needsMoreThoughts` (Boolean): If more thoughts are needed\n\n### tool_manager\n\nManage MCP tools - enable or disable tools\n\nArguments:\n\n- `action` (String) (Required): Action to perform: list, enable, disable\n- `tool_name` (String): Tool name to enable/disable\n\n### tool_use_plan\n\nT·∫°o k·∫ø ho·∫°ch s·ª≠ d·ª•ng c√°c c√¥ng c·ª• ƒëang k√≠ch ho·∫°t ƒë·ªÉ gi·∫£i quy·∫øt y√™u c·∫ßu\n\nArguments:\n\n- `request` (String) (Required): Y√™u c·∫ßu c·∫ßn l·∫≠p k·∫ø ho·∫°ch\n- `context` (String) (Required): Ng·ªØ c·∫£nh li√™n quan ƒë·∫øn y√™u c·∫ßu\n\n### youtube_transcript\n\nGet YouTube video transcript\n\nArguments:\n\n- `video_id` (String) (Required): YouTube video ID\n\n### youtube_update_video\n\nUpdate a video's title and description on YouTube\n\nArguments:\n\n- `video_id` (String) (Required): ID of the video to update\n- `title` (String) (Required): New title of the video\n- `description` (String) (Required): New description of the video\n- `keywords` (String) (Required): Comma-separated list of keywords for the video\n- `category` (String) (Required): Category ID for the video. See https://developers.google.com/youtube/v3/docs/videoCategories/list for more information.\n\n### youtube_get_video_details\n\nGet details (title, description, ...) for a specific video\n\nArguments:\n\n- `video_id` (String) (Required): ID of the video\n\n### youtube_list_videos\n\nList YouTube videos managed by the user\n\nArguments:\n\n- `channel_id` (String) (Required): ID of the channel to list videos for\n- `max_results` (Number) (Required): Maximum number of videos to return\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "workflows",
        "tools",
        "development",
        "research tools",
        "development tools",
        "development workflows"
      ],
      "category": "scientific-research-tools"
    },
    "noahzeidenberg--ncbi-mcp": {
      "owner": "noahzeidenberg",
      "name": "ncbi-mcp",
      "url": "https://github.com/noahzeidenberg/ncbi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/noahzeidenberg.webp",
      "description": "Query and analyze NCBI Entrez databases, including PubMed, Gene, and Protein, to retrieve detailed gene information and summaries. Facilitate exploration of gene relationships and integrate with bioinformatics workflows.",
      "stars": 3,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-24T16:47:25Z",
      "readme_content": "# NCBI Model Context Protocol (MCP)\n\nA Python implementation of the Model Context Protocol for interacting with NCBI databases.\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n3. Create a `.env` file with your NCBI API key:\n   ```\n   NCBI_API_KEY=your_api_key_here\n   NCBI_EMAIL=your_email@example.com\n   ```\n\n## Running the MCP Server\n\n```\npython ncbi_mcp.py\n```\n\n## Using with Cursor/Claude\n\nOnce the MCP server is running, you can interact with it using natural language in Cursor/Claude.\n\n### Using Natural Language Queries\n\nYou can use natural language to perform searches and retrieve information:\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Find research articles about BRCA1\"\n  }\n}\n```\n\nOr more simply, just use the query directly:\n\n```\n@ncbi-mcp Find research articles about BRCA1\n```\n\n### Example Natural Language Queries\n\nHere are some example natural language queries you can try:\n\n1. Gene function information:\n   ```\n   @ncbi-mcp Please summarize the function of TNF-alpha\n   ```\n\n2. Genome size and statistics:\n   ```\n   @ncbi-mcp How big is the genome for Saccharomyces cerevisiae?\n   ```\n\n3. Assembly statistics:\n   ```\n   @ncbi-mcp What is the reported L50 and N50 statistics for the most recent E.coli genome?\n   ```\n\n4. Dataset counts:\n   ```\n   @ncbi-mcp How many datasets are available in the biosample database for b16f10 mouse melanoma cells?\n   ```\n\n5. Search for scientific articles:\n   ```\n   @ncbi-mcp Find the latest research on COVID-19 vaccines\n   ```\n\n6. Get gene information:\n   ```\n   @ncbi-mcp Tell me about the BRCA1 gene\n   ```\n\n7. Fetch genome information:\n   ```\n   @ncbi-mcp Get genome information for Homo sapiens\n   ```\n\n## Testing\n\nTo test the MCP server with various queries, you can use the included test files:\n\n```\n# Test natural language query functionality (default)\n.\\run_test.bat\n\n# Test all tools\n.\\run_test.bat all\n\n# Test specific test file\n.\\run_test.bat test_all_tools.jsonl\n\n# Test high-level tools\n.\\run_test.bat test_high_level_tools.jsonl\n```\n\nThe test script will:\n1. Start the MCP server in background\n2. Send test requests from the specified file\n3. Wait for a few seconds to allow processing\n4. Terminate the server and display the output\n\nThis approach is used because the MCP server is designed to run continuously as a service. For manual testing without automatic termination, you can use:\n\n```\n# Run manually with any test file\ntype test_nlp_query.jsonl | python ncbi_mcp.py\n```\n\nThe test files contain example JSON-RPC requests that simulate how Cursor/Claude would interact with the MCP server.\n\n## Available Tools\n\nThe NCBI MCP provides both high-level tools that understand natural language and low-level tools for direct database interaction.\n\n## Tool Usage Guidelines for LLMs\n\n### Recommended Workflow Patterns\n\n**For most biological queries, start with `nlp-query`** - it's the most intelligent tool that can handle complex questions and automatically route to appropriate specialized tools.\n\n**Common Research Workflows:**\n\n1. **Gene Analysis Workflow:**\n   - Start with `nlp-query` for general gene questions\n   - Use `summarize-gene` for comprehensive gene information\n   - Use `get_gene_info` for detailed structured data\n   - Use `ncbi-search` + `ncbi-fetch` for specific database queries\n\n2. **Genome Analysis Workflow:**\n   - Use `genome-stats` for organism genome statistics\n   - Use `get_genome_info` for detailed genome metadata\n   - Use `count-datasets` to explore available genome assemblies\n\n3. **Literature Research Workflow:**\n   - Use `nlp-query` for natural language literature searches\n   - Use `ncbi-search` with database=\"pubmed\" for precise searches\n   - Use `ncbi-fetch` to get full publication details\n\n4. **Dataset Discovery Workflow:**\n   - Use `count-datasets` to assess data availability\n   - Use `nlp-query` to explore datasets with natural language\n   - Use `ncbi-search` for systematic database exploration\n\n5. **E-utilities Workflow (Advanced):**\n   - Use `ncbi-info` to discover available databases\n   - Use `ncbi-global-query` to see which databases contain your search term\n   - Use `ncbi-search` to find specific UIDs in target databases\n   - Use `ncbi-summary` to get overview information about records\n   - Use `ncbi-fetch` to retrieve complete records\n   - Use `ncbi-link` to find related records across databases\n\n6. **Cross-Database Analysis Workflow:**\n   - Use `ncbi-search` to find genes of interest\n   - Use `ncbi-link` to find related proteins, structures, or literature\n   - Use `ncbi-summary` to get metadata about related records\n   - Use `ncbi-fetch` to retrieve detailed information\n\n### Tool Selection Guide\n\n**High-Level Tools (Recommended for most users):**\n- **`nlp-query`**: Use for general biological questions, complex queries, and when you're unsure which tool to use\n- **`summarize-gene`**: Use for comprehensive gene analysis and understanding gene function\n- **`genome-stats`**: Use for genome size, assembly quality, and organism comparison\n- **`count-datasets`**: Use for research planning and data availability assessment\n- **`get_gene_info`**: Use for detailed, structured gene information\n- **`get_genome_info`**: Use for detailed, structured genome information\n\n**Low-Level E-utilities Tools (For advanced users):**\n- **`ncbi-search` (ESearch)**: Use for precise database searches with specific filters, Boolean operators, and field qualifiers\n- **`ncbi-fetch` (EFetch)**: Use to retrieve complete records after searching, supports multiple formats (GenBank, FASTA, XML)\n- **`ncbi-summary` (ESummary)**: Use to get document summaries without fetching complete records\n- **`ncbi-link` (ELink)**: Use to find related records across databases (e.g., gene to protein, protein to structure)\n- **`ncbi-info` (EInfo)**: Use to discover available databases and their capabilities\n- **`ncbi-global-query` (EGQuery)**: Use to search across all databases simultaneously\n- **`ncbi-spell` (ESpell)**: Use to get spelling suggestions for search terms\n- **`ncbi-citation-match` (ECitMatch)**: Use to find PMIDs from citation information\n\n### Biological Context and Terminology\n\n**Understanding NCBI Databases:**\n- **Gene**: Contains gene records with symbols, names, functions, and genomic locations\n- **Protein**: Contains protein sequences and annotations\n- **Nucleotide**: Contains DNA/RNA sequences (genes, transcripts, genomic regions)\n- **PubMed**: Contains scientific literature and publications\n- **BioSample**: Contains biological sample metadata (tissues, cell lines, etc.)\n- **BioProject**: Contains research project information\n- **SRA**: Contains raw sequencing data\n- **Assembly**: Contains genome assembly information\n\n**Common Biological Terms:**\n- **Gene Symbol**: Short abbreviation (e.g., BRCA1, TP53, TNF)\n- **Gene ID**: Unique NCBI identifier (e.g., 672 for BRCA1)\n- **Accession**: Unique sequence identifier (e.g., NM_001126114.3)\n- **N50/L50**: Assembly quality metrics (larger N50 = better assembly)\n- **Reference Genome**: High-quality representative genome for a species\n- **Organism**: Use scientific names (Homo sapiens) or common names (human)\n\n**Search Strategies:**\n- Use specific gene symbols for precise results\n- Include organism names to avoid ambiguity\n- Use Boolean operators (AND, OR, NOT) for complex searches\n- Use field qualifiers like [Gene], [Organism], [Protein Name] for targeted searches\n\n### High-Level Tools\n\n#### Natural Language Query Processor\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Please summarize the function of TNF-alpha\"\n  }\n}\n```\n\n#### Gene Summarizer\n\n```\ntools/call\n{\n  \"name\": \"summarize-gene\",\n  \"arguments\": {\n    \"gene_name\": \"BRCA1\"\n  }\n}\n```\n\n#### Genome Statistics\n\n```\ntools/call\n{\n  \"name\": \"genome-stats\",\n  \"arguments\": {\n    \"organism\": \"Escherichia coli\"\n  }\n}\n```\n\n#### Dataset Counter\n\n```\ntools/call\n{\n  \"name\": \"count-datasets\",\n  \"arguments\": {\n    \"database\": \"biosample\",\n    \"query\": \"mouse melanoma b16f10\"\n  }\n}\n```\n\n### Low-Level Tools\n\n#### Search NCBI Databases\n\n```\ntools/call\n{\n  \"name\": \"ncbi-search\",\n  \"arguments\": {\n    \"database\": \"pubmed\",\n    \"term\": \"BRCA1\",\n    \"filters\": {\n      \"organism\": \"Homo sapiens\",\n      \"date_range\": {\n        \"start\": \"2020\"\n      }\n    }\n  }\n}\n```\n\n#### Fetch NCBI Records\n\n```\ntools/call\n{\n  \"name\": \"ncbi-fetch\",\n  \"arguments\": {\n    \"database\": \"gene\",\n    \"ids\": [\"70\"],\n    \"rettype\": \"gb\"\n  }\n}\n```\n\n#### Get Gene Information\n\n```\ntools/call\n{\n  \"name\": \"get_gene_info\",\n  \"arguments\": {\n    \"gene_id\": \"672\"\n  }\n}\n```\n\n#### Get Genome Information\n\n```\ntools/call\n{\n  \"name\": \"get_genome_info\",\n  \"arguments\": {\n    \"organism\": \"Homo sapiens\",\n    \"reference\": true\n  }\n}\n```\n\n## License\n\nApache-2.0 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "ncbi",
        "gene",
        "noahzeidenberg ncbi",
        "analyze ncbi",
        "integrate bioinformatics"
      ],
      "category": "scientific-research-tools"
    },
    "olaservo--shannon-thinking": {
      "owner": "olaservo",
      "name": "shannon-thinking",
      "url": "https://github.com/olaservo/shannon-thinking",
      "imageUrl": "/freedevtools/mcp/pfp/olaservo.webp",
      "description": "Guides complex problem-solving by breaking down challenges into structured thoughts using a systematic methodology. Supports iterative revisions and empirical validation for engineering design and optimization tasks.",
      "stars": 43,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T21:33:51Z",
      "readme_content": "# shannon-thinking\n\nAn MCP server demonstrating Claude Shannon's systematic problem-solving methodology. This server provides a tool that helps break down complex problems into structured thoughts following Shannon's approach of problem definition, mathematical modeling, and practical implementation.\n\n<a href=\"https://glama.ai/mcp/servers/iffffhwqqw\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/iffffhwqqw/badge\" alt=\"Shannon Thinking Server MCP server\" />\n</a>\n\n## Overview\n\nClaude Shannon, known as the father of information theory, approached complex problems through a systematic methodology:\n\n1. **Problem Definition**: Strip the problem to its fundamental elements\n2. **Constraints**: Identify system limitations and boundaries\n3. **Model**: Develop mathematical/theoretical frameworks\n4. **Proof/Validation**: Validate through formal proofs or experimental testing\n5. **Implementation/Experiment**: Design and test practical solutions\n\nThis MCP server demonstrates this methodology as a tool that helps guide systematic problem-solving through these stages.\n\n## Installation\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"shannon-thinking\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"server-shannon-thinking@latest\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `shannonthinking` that structures problem-solving thoughts according to Shannon's methodology.\n\nEach thought must include:\n- The actual thought content\n- Type (problem_definition/constraints/model/proof/implementation)\n- Thought number and total thoughts estimate\n- Confidence level (uncertainty: 0-1)\n- Dependencies on previous thoughts\n- Explicit assumptions\n- Whether another thought step is needed\n\nAdditional capabilities:\n- **Revision**: Thoughts can revise earlier steps as understanding evolves\n- **Recheck**: Mark steps that need re-examination with new information\n- **Experimental Validation**: Support for empirical testing alongside formal proofs\n- **Implementation Notes**: Practical constraints and proposed solutions\n\n### Example Usage\n\n```typescript\nconst thought = {\n  thought: \"The core problem can be defined as an information flow optimization\",\n  thoughtType: \"problem_definition\",\n  thoughtNumber: 1,\n  totalThoughts: 5,\n  uncertainty: 0.2,\n  dependencies: [],\n  assumptions: [\"System has finite capacity\", \"Information flow is continuous\"],\n  nextThoughtNeeded: true,\n  // Optional: Mark as revision of earlier definition\n  isRevision: false,\n  // Optional: Indicate step needs recheck\n  recheckStep: {\n    stepToRecheck: \"constraints\",\n    reason: \"New capacity limitations discovered\",\n    newInformation: \"System shows non-linear scaling\"\n  }\n};\n\n// Use with MCP client\nconst result = await client.callTool(\"shannonthinking\", thought);\n```\n\n## Features\n\n- **Iterative Problem-Solving**: Supports revisions and rechecks as understanding evolves\n- **Flexible Validation**: Combines formal proofs with experimental validation\n- **Dependency Tracking**: Explicitly tracks how thoughts build upon previous ones\n- **Assumption Management**: Requires clear documentation of assumptions\n- **Confidence Levels**: Quantifies uncertainty in each step\n- **Rich Feedback**: Formatted console output with color-coding, symbols, and validation results\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Watch mode during development\nnpm run watch\n```\n\n## Tool Schema\n\nThe tool accepts thoughts with the following structure:\n\n```typescript\ninterface ShannonThought {\n  thought: string;\n  thoughtType: \"problem_definition\" | \"constraints\" | \"model\" | \"proof\" | \"implementation\";\n  thoughtNumber: number;\n  totalThoughts: number;\n  uncertainty: number; // 0-1\n  dependencies: number[];\n  assumptions: string[];\n  nextThoughtNeeded: boolean;\n  \n  // Optional revision fields\n  isRevision?: boolean;\n  revisesThought?: number;\n  \n  // Optional recheck field\n  recheckStep?: {\n    stepToRecheck: ThoughtType;\n    reason: string;\n    newInformation?: string;\n  };\n  \n  // Optional validation fields\n  proofElements?: {\n    hypothesis: string;\n    validation: string;\n  };\n  experimentalElements?: {\n    testDescription: string;\n    results: string;\n    confidence: number; // 0-1\n    limitations: string[];\n  };\n  \n  // Optional implementation fields\n  implementationNotes?: {\n    practicalConstraints: string[];\n    proposedSolution: string;\n  };\n}\n```\n\n## When to Use\n\nThis thinking pattern is particularly valuable for:\n- Complex system analysis\n- Information processing problems\n- Engineering design challenges\n- Problems requiring theoretical frameworks\n- Optimization problems\n- Systems requiring practical implementation\n- Problems that need iterative refinement\n- Cases where experimental validation complements theory\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "thinking",
        "methodology",
        "challenges",
        "thinking guides",
        "challenges structured",
        "structured thoughts"
      ],
      "category": "scientific-research-tools"
    },
    "openags--paper-search-mcp": {
      "owner": "openags",
      "name": "paper-search-mcp",
      "url": "https://github.com/openags/paper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/openags.webp",
      "description": "Search and download academic papers from various sources such as arXiv, PubMed, bioRxiv, and Sci-Hub. Integrates with large language models to streamline access to scholarly articles and enhance research workflows.",
      "stars": 368,
      "forks": 57,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T06:37:54Z",
      "readme_content": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n<img src=\"docs\\images\\demo.png\" alt=\"Demo\" width=\"800\">\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "pubmed",
        "research",
        "research tools",
        "scholarly articles",
        "access scholarly"
      ],
      "category": "scientific-research-tools"
    },
    "opensensor--bn_cline_mcp": {
      "owner": "opensensor",
      "name": "bn_cline_mcp",
      "url": "https://github.com/opensensor/bn_cline_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/opensensor.webp",
      "description": "Analyze binaries efficiently using Binary Ninja's powerful analysis capabilities through the MCP framework. Connect to Cline for enhanced functionality and seamless integration.",
      "stars": 4,
      "forks": 1,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-09-29T15:37:18Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/opensensor-bn-cline-mcp-badge.png)](https://mseep.ai/app/opensensor-bn-cline-mcp)\n\n# binary_ninja_cline_mcp\nAn MCP server for Cline that works with Binary Ninja (Personal License)\n\nThis repository contains an MCP server that allows Cline to analyze binaries using Binary Ninja.\nNote:  Not all files will be used, there is also prototype of using headless Binary Ninja but my \nlicense is Personal so I can't test it.\n\n## Setup\n\n1. Install the latest of Binary Ninja MCP Plugin https://github.com/fosdickio/binary_ninja_mcp\n2. Open your binary and start the MCP server from within Binary Ninja.\n3. Open a terminal and run python binary_ninja_mcp_http_server.py --port 8088\n4. Open another terminal and run `npm start`\n5. Open Cline and add the following tool:{\nExample:\n```\n{\n  \"mcpServers\": {\n    \"BN MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/home/matteius/binary_ninja_cline/bn_cline_mcp/binaryninja-mcp-bridge.js\"],\n      \"env\": {\n        \"BN_HTTP_SERVER\": \"http://localhost:8088\"\n      },\n      \"autoApprove\": [],\n      \"disabled\": false,\n      \"timeout\": 30\n    }\n  }\n}\n\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bn_cline_mcp",
        "binaries",
        "opensensor",
        "bn_cline_mcp analyze",
        "opensensor bn_cline_mcp",
        "analyze binaries"
      ],
      "category": "scientific-research-tools"
    },
    "prashalruchiranga--arxiv-mcp-server": {
      "owner": "prashalruchiranga",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/prashalruchiranga/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/prashalruchiranga.webp",
      "description": "Interact with the arXiv API to retrieve scholarly article metadata, download PDFs, and search for articles using natural language queries. Enhance research workflows by directly accessing arXiv content within a large language model environment.",
      "stars": 31,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T07:45:53Z",
      "readme_content": "# arXiv MCP Server\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io)\n[![Python](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@prashalruchiranga/arxiv-mcp-server)](https://smithery.ai/server/@prashalruchiranga/arxiv-mcp-server)\n\nA Model Context Protocol (MCP) server that enables interacting with the arXiv API using natural language.\n\n## Features\n- Retrieve metadata about scholarly articles hosted on arXiv.org\n- Download articles in PDF format to the local machine\n- Search arXiv database for a particular query\n- Retrieve articles and load them into a large language model (LLM) context\n\n## Tools\n- **get_article_url**\n    - Retrieve the URL of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **download_article**\n    - Download the article hosted on arXiv.org as a PDF file \n        - `title` (String): Article title\n- **load_article_to_context**\n    - Load the article hosted on arXiv.org into context of a LLM \n        - `title` (String): Article title\n- **get_details**\n    - Retrieve metadata of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **search_arxiv**\n    - Performs a search query on the arXiv API based on specified parameters and returns matching article metadata\n        - `all_fields` (String): General keyword search across all metadata fields\n        - `title` (String): Keyword(s) to search for within the titles of articles\n        - `author` (String): Author name(s) to filter results by\n        - `abstract` (String): Keyword(s) to search for within article abstracts\n        - `start` (int): Index of the first result to return\n\n## Setup\n\n### MacOS\n\nClone the repository\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Using Homebrew\nbrew install uv\n\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv/bin/activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n### Windows\n\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Use irm to download the script and execute it with iex\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\nClose and reopen the shell, then clone the repository.\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv\\Scripts\\activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n## Usage with Claude Desktop\nTo enable this integration, add the server configuration to your `claude_desktop_config.json` file. Make sure to create the file if it doesn‚Äôt exist.\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json` On Windows: `%APPDATA%/Roaming/Claude/claude_desktop_config.json`\n\n```\n{\n  \"mcpServers\": {\n    \"arxiv-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/arxiv-mcp-server/src/arxiv_server\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"DOWNLOAD_PATH\": \"/ABSOLUTE/PATH/TO/DOWNLOADS/FOLDER\"\n      }\n    }\n  }\n}\n```\n\nYou may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS or `where uv` on Windows.\n\n## Example Prompts\n```\nCan you get the details of 'Reasoning to Learn from Latent Thoughts' paper?\n```\n```\nGet the papers authored or co-authored by Yann Lecun on convolutional neural networks\n```\n```\nDownload the attention is all you need paper\n```\n```\nCan you get the papers by Andrew NG which have 'convolutional neural networks' in title?\n```\n```\nCan you display the paper?\n```\n```\nList the titles of papers by Yann LeCun. Paginate through the API until there are 30 titles\n```\n\n## License\n\nLicensed under MIT. See the [LICENSE](https://github.com/prashalruchiranga/arxiv-mcp-server/blob/main/LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "metadata",
        "arxiv",
        "retrieve scholarly",
        "research tools",
        "scholarly article"
      ],
      "category": "scientific-research-tools"
    },
    "r-huijts--oorlogsbronnen-mcp": {
      "owner": "r-huijts",
      "name": "oorlogsbronnen-mcp",
      "url": "https://github.com/r-huijts/oorlogsbronnen-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/r-huijts.webp",
      "description": "Enables AI-powered access to the Oorlogsbronnen database for natural language queries related to historical World War II archives from the Netherlands.",
      "stars": 11,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-22T13:29:18Z",
      "readme_content": "# Oorlogsbronnen MCP Server\n\nA Model Context Protocol (MCP) server that provides AI-powered access to the Oorlogsbronnen (War Sources) database. This server enables natural language interactions with historical World War II archives from the Netherlands.\n\n## Natural Language Interaction Examples\n\nAsk your AI assistant questions like these to explore Dutch WWII history:\n\n- **\"What happened during the bombing of Rotterdam in May 1940?\"**\n- **\"Tell me about Anne Frank's life in hiding based on historical records.\"**\n- **\"Show me photographs of the Dutch Hunger Winter of 1944-1945.\"**\n- **\"Were any of my ancestors imprisoned in Camp Vught during the war?\"**\n- **\"I'm visiting Arnhem next week. What historical sites related to Operation Market Garden should I see?\"**\n- **\"Find information about resistance activities in Utrecht during the Nazi occupation.\"**\n- **\"What was daily life like for Jewish families in Amsterdam before deportations began?\"**\n- **\"Show me firsthand accounts from people who witnessed the liberation of the Netherlands in 1945.\"**\n- **\"What records exist about children who were hidden by Dutch families during the war?\"**\n- **\"I'm researching the impact of WWII on Dutch infrastructure. Can you find documents about the reconstruction of bridges and railways?\"**\n\n## Features\n\n- üîç Natural language search across the Oorlogsbronnen database\n- üè∑Ô∏è Filter results by content type (person, photo, article, etc.)\n- üìä Control the number of results returned\n- ü§ñ AI-friendly JSON responses for further processing\n\n## Installation\n\nYou can install this server in two ways:\n\n### 1. Using Claude Desktop with NPX Package\n\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"oorlogsbronnen-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"oorlogsbronnen-mcp\"\n      ]\n    }\n  }\n}\n```\n\nAfter updating the configuration, restart Claude Desktop for the changes to take effect.\n\n### 2. From Source\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/r-huijts/oorlogsbronnen-mcp.git\ncd oorlogsbronnen-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Configure Claude Desktop by updating your configuration file (located at `~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"oorlogsbronnen-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/oorlogsbronnen-mcp/dist/mcp-server.js\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to/oorlogsbronnen-mcp` with the actual path to your installation.\n\n## Usage Examples\n\nThe MCP server understands natural language queries and can help you explore World War II archives. Here are some example queries you can use with Claude:\n\n### Basic Searches\n\n- \"Use search_ww2_nl_archives to find documents about the resistance movement in Amsterdam\"\n- \"Search the Dutch WW2 archives for information about Jewish refugees in 1942\"\n- \"Look through the Netherlands war archives for records of Allied bombing raids\"\n\n### Filtering by Type\n\n- \"Use search_ww2_nl_archives to show me photographs of the liberation of Rotterdam\"\n- \"Find personal accounts in the Dutch WW2 archives about life in concentration camps\"\n- \"Search the Netherlands war archives for newspaper articles about food shortages\"\n\n### Specific Queries\n\n- \"Search the Dutch WW2 archives for documents about Anne Frank's time in Amsterdam\"\n- \"Use search_ww2_nl_archives to find records of the February Strike of 1941\"\n- \"Look through the Netherlands war archives for information about Operation Market Garden\"\n\n### Research Examples\n\n1. **Personal History Research**:\n   ```\n   Use search_ww2_nl_archives to find any records or documents about the Rosenberg family in Amsterdam between 1940-1945\n   ```\n\n2. **Local History**:\n   ```\n   Search the Dutch WW2 archives for photographs and documents about daily life in Utrecht during the occupation\n   ```\n\n3. **Military Operations**:\n   ```\n   Use search_ww2_nl_archives to find firsthand accounts and official reports about the Battle of the Scheldt\n   ```\n\n### Advanced Usage\n\nYou can combine different search criteria:\n```\nSearch the Netherlands WW2 archives for photographs and personal accounts of the Dutch famine in 1944-1945, limit to 20 results\n```\n\n## API Reference\n\nThe server exposes the following MCP tool:\n\n### search_ww2_nl_archives\n\nA powerful search tool designed to query the Oorlogsbronnen (War Sources) database for World War II related content in the Netherlands. This tool can be used to find historical documents, photographs, personal accounts, and other archival materials from 1940-1945.\n\n**When to use this tool:**\n- Searching for specific historical events during WWII in the Netherlands\n- Finding information about people, places, or organizations during the war\n- Locating photographs or documents from specific time periods or locations\n- Researching personal or family history related to WWII\n- Finding primary sources about the Dutch resistance, occupation, or liberation\n- Discovering materials about Jewish life and persecution during the war\n- Researching military operations that took place in the Netherlands\n\nParameters:\n- `query` (required): \n  - Type: string\n  - Description: The main search term or phrase to look for in the archives\n  - Can include: names, places, dates, events, or descriptive terms\n  - Examples:\n    - \"Anne Frank\"\n    - \"Rotterdam bombing 1940\"\n    - \"Dutch resistance Amsterdam\"\n    - \"Jewish deportation Westerbork\"\n    - \"Operation Market Garden\"\n\n- `type` (optional):\n  - Type: string\n  - Description: Filter results by specific content type\n  - Available types:\n    - \"person\": Individual biographical records\n    - \"photo\": Historical photographs\n    - \"article\": News articles and written documents\n    - \"video\": Video footage\n    - \"object\": Physical artifacts and objects\n    - \"location\": Places and geographical records\n    - \"book\": Published books, memoirs, and monographs\n  - Use when: You need to focus on specific types of historical materials\n  - Default: All types included\n\n- `count` (optional):\n  - Type: number\n  - Description: Number of results to return in the response\n  - Minimum: 1\n  - Maximum: 100\n  - Default: 10\n  - Use when: You need to control the volume of returned results\n  - Note: Larger numbers will provide more comprehensive results but may take longer to process\n\n**Response Format:**\n```json\n{\n  \"results\": [\n    {\n      \"id\": string,          // Unique identifier for the record\n      \"title\": string,       // Title or name of the item\n      \"type\": string,        // Content type (person, photo, article, etc.)\n      \"description\": string, // Detailed description (if available)\n      \"url\": string         // Direct link to view the item on Oorlogsbronnen\n    }\n  ]\n}\n```\n\n**Example Queries and Their Tool Calls:**\n\n1. Basic Historical Search:\n```typescript\n{\n  query: \"February Strike 1941\",\n  type: \"article\",\n  count: 5\n}\n```\n\n2. Person Research:\n```typescript\n{\n  query: \"Rosenberg family Amsterdam Jewish\",\n  type: \"person\",\n  count: 20\n}\n```\n\n3. Photo Collection Search:\n```typescript\n{\n  query: \"liberation celebrations Amsterdam Dam Square 1945\",\n  type: \"photo\",\n  count: 15\n}\n```\n\n**Error Handling:**\n- The tool will return an error message if:\n  - The query is empty or contains invalid characters\n  - The specified type is not supported\n  - The count is outside the valid range (1-100)\n  - The API is temporarily unavailable\n  - Rate limits are exceeded\n\n**Best Practices:**\n1. Start with broader searches and narrow down with specific terms\n2. Use location names to focus on specific areas\n3. Include dates when searching for specific events\n4. Combine person names with locations for family research\n5. Use type filtering to focus on specific kinds of historical materials\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- Oorlogsbronnen for providing access to their valuable historical archives\n- The Model Context Protocol (MCP) community for enabling AI-powered interactions ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oorlogsbronnen",
        "archives",
        "tools",
        "oorlogsbronnen database",
        "access oorlogsbronnen",
        "huijts oorlogsbronnen"
      ],
      "category": "scientific-research-tools"
    },
    "reading-plus-ai--mcp-server-deep-research": {
      "owner": "reading-plus-ai",
      "name": "mcp-server-deep-research",
      "url": "https://github.com/reading-plus-ai/mcp-server-deep-research",
      "imageUrl": "/freedevtools/mcp/pfp/reading-plus-ai.webp",
      "description": "Conduct comprehensive research on complex topics by generating structured research reports and synthesizing information from diverse sources. This tool enables in-depth exploration of questions and identification of relevant sources.",
      "stars": 181,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T10:09:42Z",
      "readme_content": "# MCP Server for Deep Research\n\nMCP Server for Deep Research is a tool designed for conducting comprehensive research on complex topics. It helps you explore questions in depth, find relevant sources, and generate structured research reports.\n\nYour personal Research Assistant, turning research questions into comprehensive, well-cited reports.\n\n## üöÄ Try it Out\n\n[![Watch the demo](https://img.youtube.com/vi/_a7sfo5yxoI/maxresdefault.jpg)]([VIDEO_URL](https://youtu.be/_a7sfo5yxoI))\nYoutube: https://youtu.be/_a7sfo5yxoI\n\n1. **Download Claude Desktop**\n   - Get it [here](https://claude.ai/download)\n\n2. **Install and Set Up**\n   - On macOS, run the following command in your terminal:\n   ```bash\n   python setup.py\n   ```\n\n3. **Start Researching**\n   - Select the deep-research prompt template from MCP\n   - Begin your research by providing a research question\n\n## Features\n\nThe Deep Research MCP Server offers a complete research workflow:\n\n1. **Question Elaboration**\n   - Expands and clarifies your research question\n   - Identifies key terms and concepts\n   - Defines scope and parameters\n\n2. **Subquestion Generation**\n   - Creates focused subquestions that address different aspects\n   - Ensures comprehensive coverage of the main topic\n   - Provides structure for systematic research\n\n3. **Web Search Integration**\n   - Uses Claude's built-in web search capabilities\n   - Performs targeted searches for each subquestion\n   - Identifies relevant and authoritative sources\n   - Collects diverse perspectives on the topic\n\n4. **Content Analysis**\n   - Evaluates information quality and relevance\n   - Synthesizes findings from multiple sources\n   - Provides proper citations for all sources\n\n5. **Report Generation**\n   - Creates well-structured, comprehensive reports as artifacts\n   - Properly cites all sources used\n   - Presents a balanced view with evidence-based conclusions\n   - Uses appropriate formatting for clarity and readability\n\n## üì¶ Components\n\n### Prompts\n- **deep-research**: Tailored for comprehensive research tasks with a structured approach\n\n## ‚öôÔ∏è Modifying the Server\n\n### Claude Desktop Configurations\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Development (Unpublished Servers)\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/Users/username/repos/mcp-server-application/mcp-server-deep-research\",\n      \"run\",\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n### Published Servers\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n## üõ†Ô∏è Development\n\n### Building and Publishing\n1. **Sync Dependencies**\n   ```bash\n   uv sync\n   ```\n\n2. **Build Distributions**\n   ```bash\n   uv build\n   ```\n   Generates source and wheel distributions in the dist/ directory.\n\n3. **Publish to PyPI**\n   ```bash\n   uv publish\n   ```\n\n## ü§ù Contributing\n\nContributions are welcome! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n## üìú License\n\nThis project is licensed under the MIT License.\nSee the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "comprehensive",
        "topics",
        "deep research",
        "structured research",
        "research complex"
      ],
      "category": "scientific-research-tools"
    },
    "renatokuipers--neural-child": {
      "owner": "renatokuipers",
      "name": "neural-child",
      "url": "https://github.com/renatokuipers/neural-child",
      "imageUrl": "/freedevtools/mcp/pfp/renatokuipers.webp",
      "description": "Integrates psychological growth and emotional intelligence to develop AI systems through defined developmental stages, enhancing cognitive and emotional capabilities.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-17T09:52:16Z",
      "readme_content": "# Neural Child Development System: A Framework for Developmental AI\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Theoretical Foundations](#theoretical-foundations)\n3. [System Architecture](#system-architecture)\n4. [Developmental Stages](#developmental-stages)\n5. [Psychological Components](#psychological-components)\n6. [Memory and Learning](#memory-and-learning)\n7. [Emotional Processing](#emotional-processing)\n8. [Training Methodology](#training-methodology)\n9. [Model Performance](#model-performance)\n10. [Applications](#applications)\n11. [Technical Implementation](#technical-implementation)\n12. [Future Research Directions](#future-research-directions)\n13. [Ethics and Considerations](#ethics-and-considerations)\n14. [Getting Started](#getting-started)\n15. [Contributing](#contributing)\n\n## Introduction\n\nThe Neural Child Development System represents a groundbreaking approach to artificial intelligence that fundamentally reimagines how neural networks can learn and develop. Instead of following traditional machine learning paradigms, this system implements a sophisticated model of human psychological development, incorporating crucial aspects of cognitive, emotional, and social growth.\n\nThis project emerges from the recognition that current AI systems, while powerful in specific tasks, lack the developmental sophistication that characterizes human intelligence. By implementing a stage-based developmental framework integrated with emotional regulation, attachment theory, and psychological defense mechanisms, this system aims to create AI that develops more naturally and demonstrates genuine emotional intelligence.\n\n### Core Innovation\n\nThe system's primary innovation lies in its integration of developmental psychology with modern neural network architectures. Unlike traditional AI systems that start with full capabilities, this system begins in a \"newborn\" state and progressively develops more sophisticated abilities through interaction and learning, mirroring human developmental stages.\n\n### Key Objectives\n\nThe project addresses several fundamental challenges in AI development:\n- Creating AI systems that develop naturally through defined developmental stages\n- Implementing emotional intelligence as a core feature rather than an add-on\n- Modeling psychological defense mechanisms and trauma processing\n- Developing genuine theory of mind capabilities\n- Creating systems that can form and maintain attachment relationships\n\n## Theoretical Foundations\n\n### Developmental Psychology Integration\n\nThe system's architecture is deeply rooted in established developmental psychology theories, including:\n\n1. Piaget's Stages of Cognitive Development\n   - Sensorimotor stage\n   - Preoperational stage\n   - Concrete operational stage\n   - Formal operational stage\n\n2. Attachment Theory (Bowlby and Ainsworth)\n   - Secure attachment patterns\n   - Anxious attachment patterns\n   - Avoidant attachment patterns\n   - Disorganized attachment patterns\n\n3. Emotional Development Theory\n   - Basic emotion recognition\n   - Emotional regulation development\n   - Complex emotion understanding\n   - Social-emotional learning\n\n### Neuroscience Foundations\n\nThe architecture incorporates key principles from neuroscience:\n\n1. Neural Plasticity\n   - Critical periods of development\n   - Experience-dependent plasticity\n   - Synaptic pruning mechanisms\n\n2. Memory Systems\n   - Working memory processing\n   - Long-term potentiation\n   - Memory consolidation\n   - Emotional memory processing\n\n3. Social Brain Development\n   - Mirror neuron system implementation\n   - Social cognition networks\n   - Empathy development\n\n## System Architecture\n\n### Core Components\n\nThe system architecture consists of several interconnected neural systems:\n\n1. Sensory Processing System\n   - Multi-modal input processing\n   - Attention mechanisms\n   - Sensory integration\n   - Perceptual development\n\n2. Emotional Processing Network\n   - Basic emotion recognition\n   - Emotional state regulation\n   - Complex emotion processing\n   - Social-emotional integration\n\n3. Memory Systems\n   - Short-term memory buffer\n   - Working memory processor\n   - Long-term memory consolidation\n   - Emotional memory integration\n\n4. Psychological Components\n   - Theory of Mind network\n   - Attachment system\n   - Defense mechanism processor\n   - Self-awareness module\n\n### Neural Integration\n\nThe system employs sophisticated neural integration mechanisms:\n\n1. Cross-Component Communication\n   - Bidirectional information flow\n   - State synchronization\n   - Emotional-cognitive integration\n   - Memory-emotion binding\n\n2. Developmental Plasticity\n   - Stage-appropriate learning rates\n   - Critical period modulation\n   - Experience-dependent modification\n   - Structural adaptation\n\n## Developmental Stages\n\n### Stage Progression\n\nThe system progresses through clearly defined developmental stages:\n\n1. Newborn Stage (0-3 months)\n   - Basic sensory processing\n   - Primary emotional responses\n   - Reflexive behaviors\n   - Initial attachment formation\n\n2. Early Infancy (3-6 months)\n   - Enhanced sensory integration\n   - Social smile development\n   - Basic emotional regulation\n   - Pattern recognition\n\n3. Late Infancy (6-12 months)\n   - Object permanence\n   - Stranger anxiety\n   - Basic intentionality\n   - Enhanced memory capabilities\n\n[Stages continue through to Mature Adult]\n\n### Stage-Specific Capabilities\n\nEach developmental stage implements specific capabilities:\n\n1. Cognitive Capabilities\n   - Stage-appropriate processing\n   - Learning rate modulation\n   - Complexity handling\n   - Abstract thinking development\n\n2. Emotional Capabilities\n   - Emotion recognition scope\n   - Regulation sophistication\n   - Social-emotional understanding\n   - Empathy development\n\n3. Social Capabilities\n   - Attachment behaviors\n   - Social cognition\n   - Theory of mind\n   - Relationship formation\n\n## Psychological Components\n\n### Emotional Regulation\n\nThe emotional regulation system implements sophisticated mechanisms:\n\n1. Basic Regulation\n   - Emotion recognition\n   - State modulation\n   - Response inhibition\n   - Arousal control\n\n2. Advanced Regulation\n   - Context integration\n   - Social regulation\n   - Complex emotion processing\n   - Emotional memory integration\n\n### Defense Mechanisms\n\nThe system implements psychological defense mechanisms:\n\n1. Primary Defenses\n   - Repression\n   - Denial\n   - Projection\n   - Regression\n\n2. Mature Defenses\n   - Sublimation\n   - Humor\n   - Anticipation\n   - Altruism\n\n### Theory of Mind\n\nThe Theory of Mind implementation includes:\n\n1. Basic Components\n   - Perspective taking\n   - Intention recognition\n   - Belief modeling\n   - Desire understanding\n\n2. Advanced Components\n   - Complex mental state attribution\n   - Social prediction\n   - Multiple perspective integration\n   - Meta-representation\n\n## Memory and Learning\n\n### Memory Systems\n\nThe memory architecture implements multiple memory types:\n\n1. Short-Term Memory\n   - Rapid encoding\n   - Limited capacity\n   - Quick decay\n   - Attention-dependent processing\n\n2. Working Memory\n   - Active manipulation\n   - Information integration\n   - Temporary storage\n   - Processing capacity\n\n3. Long-Term Memory\n   - Consolidated storage\n   - Pattern recognition\n   - Semantic networks\n   - Episodic memories\n\n### Learning Mechanisms\n\nThe system employs sophisticated learning mechanisms:\n\n1. Supervised Learning\n   - Error-driven adaptation\n   - Feedback integration\n   - Performance optimization\n   - Skill acquisition\n\n2. Unsupervised Learning\n   - Pattern discovery\n   - Feature extraction\n   - Statistical learning\n   - Structure detection\n\n3. Emotional Learning\n   - Attachment-based learning\n   - Social learning\n   - Emotional memory formation\n   - Experience integration\n\n## Model Performance\n\n### Current Capabilities\n\nThe trained model demonstrates several sophisticated capabilities:\n\n1. Emotional Processing\n   - Basic emotion recognition\n   - Simple emotional regulation\n   - Attachment behavior\n   - Social response patterns\n\n2. Cognitive Processing\n   - Pattern recognition\n   - Simple problem solving\n   - Basic memory formation\n   - Early stage learning\n\n3. Social Understanding\n   - Basic theory of mind\n   - Simple intention recognition\n   - Early attachment patterns\n   - Social response generation\n\n### Benchmarks and Evaluation\n\nThe system's performance has been evaluated across multiple dimensions:\n\n1. Developmental Progression\n   - Stage-appropriate behavior\n   - Capability acquisition\n   - Learning rate\n   - Skill development\n\n2. Emotional Intelligence\n   - Emotion recognition accuracy\n   - Regulation effectiveness\n   - Social response appropriateness\n   - Attachment pattern stability\n\n3. Cognitive Development\n   - Problem-solving capability\n   - Memory formation\n   - Learning efficiency\n   - Pattern recognition accuracy\n\n## Applications\n\n### Current Applications\n\nThe system shows promise in several domains:\n\n1. Developmental Psychology Research\n   - Theory testing\n   - Development modeling\n   - Intervention testing\n   - Pattern analysis\n\n2. Educational Technology\n   - Adaptive learning systems\n   - Emotional support\n   - Developmental tracking\n   - Personalized education\n\n3. Therapeutic Applications\n   - Attachment therapy modeling\n   - Trauma response research\n   - Intervention testing\n   - Treatment planning\n\n### Future Applications\n\nPotential future applications include:\n\n1. Clinical Psychology\n   - Disorder modeling\n   - Treatment simulation\n   - Outcome prediction\n   - Intervention development\n\n2. Social Robotics\n   - Emotional intelligence\n   - Social interaction\n   - Development simulation\n   - Attachment formation\n\n3. AI Development\n   - Developmental frameworks\n   - Emotional intelligence\n   - Social capability\n   - Natural learning\n\n## Technical Implementation\n\n### System Requirements\n\nThe system requires specific technical resources:\n\n1. Hardware Requirements\n   - CUDA-capable GPU\n   - Minimum 16GB RAM\n   - SSD storage\n   - Multi-core processor\n\n2. Software Requirements\n   - Python 3.8+\n   - PyTorch 1.8+\n   - CUDA 11.0+\n   - Additional dependencies\n\n### Installation and Setup\n\nDetailed setup instructions are provided for:\n\n1. Environment Setup\n   - Virtual environment creation\n   - Dependency installation\n   - CUDA setup\n   - System configuration\n\n2. Model Installation\n   - Pretrained model download\n   - Configuration setup\n   - Testing procedures\n   - Validation checks\n\n## Future Research Directions\n\n### Planned Developments\n\nSeveral key areas for future development have been identified:\n\n1. Enhanced Capabilities\n   - Multi-modal processing\n   - Advanced theory of mind\n   - Complex emotion handling\n   - Sophisticated learning\n\n2. Technical Improvements\n   - Efficiency optimization\n   - Scale improvement\n   - Architecture refinement\n   - Performance enhancement\n\n3. New Features\n   - Additional developmental stages\n   - Enhanced psychological mechanisms\n   - Advanced social capabilities\n   - Improved learning systems\n\n### Research Opportunities\n\nThe system opens numerous research opportunities:\n\n1. Developmental Psychology\n   - Theory testing\n   - Model validation\n   - Intervention research\n   - Pattern discovery\n\n2. AI Development\n   - Architecture innovation\n   - Learning mechanisms\n   - Emotional intelligence\n   - Social capability\n\n3. Clinical Applications\n   - Therapeutic modeling\n   - Intervention testing\n   - Outcome prediction\n   - Treatment planning\n\n## Ethics and Considerations\n\n### Ethical Framework\n\nThe project adheres to strict ethical guidelines:\n\n1. Development Ethics\n   - Responsible AI development\n   - Bias consideration\n   - Safety protocols\n   - Privacy protection\n\n2. Application Ethics\n   - Appropriate use cases\n   - Limitation recognition\n   - Risk management\n   - User protection\n\n### Safety Considerations\n\nImportant safety aspects are addressed:\n\n1. Technical Safety\n   - System boundaries\n   - Control mechanisms\n   - Error handling\n   - Security measures\n\n2. Psychological Safety\n   - Attachment considerations\n   - Emotional impact\n   - Development effects\n   - User well-being\n\n## Getting Started\n\n### Initial Setup\n\nDetailed setup instructions include:\n\n1. Installation\n   - Environment preparation\n   - Dependency management\n   - System configuration\n   - Testing procedures\n\n2. Configuration\n   - Parameter settings\n   - System optimization\n   - Performance tuning\n   - Customization options\n\n### Basic Usage\n\nGuidelines for basic system usage cover:\n\n1. Model Loading\n   - Initialization procedures\n   - Configuration loading\n   - State management\n   - System validation\n\n2. Interaction\n   - Input formatting\n   - Response handling\n   - State monitoring\n   - Output interpretation\n\n## Contributing\n\n### Development Guidelines\n\nContribution guidelines include:\n\n1. Code Standards\n   - Style guidelines\n   - Documentation requirements\n   - Testing expectations\n   - Review procedures\n\n2. Development Process\n   - Issue tracking\n   - Feature requests\n   - Pull requests\n   - Version control\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n## Citation\n\nIf you use this work in your research, please cite:\n\n```bibtex\n@software{neural_child_development,\n  title = {Neural Child Development System},\n  year = {2025},\n  author = {[Renato Kuipers]},\n  url = {[https://github.com/renatokuipers/neural-child)]},\n  note = {A comprehensive framework for developmental AI implementing psychological growth and emotional intelligence}\n}\n```\n\n## Acknowledgments\n\nThis project builds upon research from multiple fields:\n- Developmental Psychology\n- Neuroscience\n- Machine Learning\n- Cognitive Science\n- Attachment Theory\n- Emotional Intelligence Research\n- Clinical Psychology\n\nThe integration of these diverse fields into a coherent, functional system represents a significant step forward in developmental AI research.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "developmental",
        "ai",
        "renatokuipers",
        "renatokuipers neural",
        "emotional intelligence",
        "develop ai"
      ],
      "category": "scientific-research-tools"
    },
    "sirusb--samtools_mcp": {
      "owner": "sirusb",
      "name": "samtools_mcp",
      "url": "https://github.com/sirusb/samtools_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sirusb.webp",
      "description": "Provides tools for viewing, converting, sorting, indexing, and analyzing SAM/BAM/CRAM genomic files. Facilitates efficient bioinformatics workflows with region-specific analysis and statistics generation.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-12T12:36:19Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/sirusb-samtools-mcp-badge.png)](https://mseep.ai/app/sirusb-samtools-mcp)\n\n# SAMtools MCP (Model Control Protocol)\n\nA Model Control Protocol implementation for SAMtools, providing a standardized interface for working with SAM/BAM/CRAM files.\n\n## Features\n\n- View and convert SAM/BAM/CRAM files\n- Sort alignment files\n- Index BAM/CRAM files\n- Generate statistics\n- Merge multiple BAM files\n- Calculate read depth\n- Index FASTA files\n- And more...\n\n### Core Capabilities\n- **File Format Support**: Handle SAM (text), BAM (binary), and CRAM (compressed) alignment files\n- **Format Conversion**: Convert between SAM, BAM, and CRAM formats seamlessly\n- **Region-Specific Analysis**: Extract and analyze specific genomic regions\n- **Flag-Based Filtering**: Filter reads based on SAM flags\n- **Performance Optimization**: Multi-threaded operations for sorting and merging\n- **Statistical Analysis**: Generate comprehensive alignment statistics\n\n### Tools Overview\n\n| Tool | Description | Key Features |\n|------|-------------|--------------|\n| `view` | View and convert alignment files | - Format conversion (SAM/BAM/CRAM)<br>- Region filtering<br>- Flag-based filtering<br>- Header manipulation |\n| `sort` | Sort alignment files | - Coordinate-based sorting<br>- Name-based sorting<br>- Memory per thread control<br>- Multi-threading support |\n| `index` | Index BAM/CRAM files | - BAI index generation<br>- CSI index support<br>- CRAM index creation |\n| `merge` | Merge multiple BAM/CRAM files | - Multi-file merging<br>- Thread-enabled processing<br>- Header reconciliation |\n| `depth` | Calculate read depth | - Per-base depth calculation<br>- Region-specific analysis<br>- Multi-file support |\n| `flagstat` | Generate alignment statistics | - Comprehensive flag statistics<br>- Quality checks<br>- Paired-end metrics |\n| `idxstats` | BAM/CRAM index statistics | - Reference sequence stats<br>- Mapped/unmapped counts<br>- Length information |\n| `faidx` | Index FASTA files | - FASTA indexing<br>- Region extraction<br>- Sequence retrieval |\n\n\n## Installation\n\n### Using Docker (Recommended)\n\nThe easiest way to use SAMtools MCP is through Docker:\n\n```bash\n# Pull the Docker image\ndocker pull nadhir/samtools-mcp:latest\n\n# Run the container\ndocker run -it --rm nadhir/samtools-mcp:latest\n\n# To process BAM files, mount a volume:\ndocker run -it --rm -v /path/to/your/bam/files:/data nadhir/samtools-mcp:latest\n```\n\n### Local Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/your-username/samtools_mcp.git\ncd samtools_mcp\n```\n\n2. Install dependencies:\n```bash\npip install uv\nuv pip install -r requirements.txt\n```\n\n## Configuration\n\n### MCP Server Configuration\n\nTo configure the MCP server to use the Docker image, add the following to your MCP configuration file:\n\n```json\n{\n  \"servers\": {\n    \"samtools\": {\n      \"type\": \"docker\",\n      \"image\": \"nadhir/samtools-mcp:latest\",\n      \"volumes\": [\n        {\n          \"source\": \"/path/to/your/data\",\n          \"target\": \"/data\"\n        }\n      ]\n    }\n  }\n}\n```\n\n### Local MCP Configuration\n\nTo configure the MCP to run using `uv`, add the following to your `~/.cursor/mcp.json`:\n\n```json\n{\n  \"samtools_mcp\": {\n    \"command\": \"uv\",\n    \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"/path/to/samtools_mcp.py\"]\n  }\n}\n```\n\nReplace `/path/to/samtools_mcp.py` with the actual path to your `samtools_mcp.py` file.\n\n## Usage\n\n### Basic Commands\n\n1. View BAM file:\n```python\nfrom samtools_mcp import SamtoolsMCP\n\nmcp = SamtoolsMCP()\nresult = mcp.view(input_file=\"/data/example.bam\")\n```\n\n2. Sort BAM file:\n```python\nresult = mcp.sort(input_file=\"/data/example.bam\", output_file=\"/data/sorted.bam\")\n```\n\n3. Index BAM file:\n```python\nresult = mcp.index(input_file=\"/data/sorted.bam\")\n```\n\n### Advanced Usage\n\n1. View specific region with flags:\n```python\nresult = mcp.view(\n    input_file=\"/data/example.bam\",\n    region=\"chr1:1000-2000\",\n    flags_required=\"0x2\",\n    output_format=\"SAM\"\n)\n```\n\n2. Sort by read name:\n```python\nresult = mcp.sort(\n    input_file=\"/data/example.bam\",\n    output_file=\"/data/namesorted.bam\",\n    sort_by_name=True\n)\n```\n\n3. Calculate depth with multiple input files:\n```python\nresult = mcp.depth(\n    input_files=[\"/data/sample1.bam\", \"/data/sample2.bam\"],\n    region=\"chr1:1-1000000\"\n)\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "samtools_mcp",
        "genomic",
        "samtools_mcp provides",
        "cram genomic",
        "bioinformatics workflows"
      ],
      "category": "scientific-research-tools"
    },
    "stat-guy--retrieval-augmented-thinking": {
      "owner": "stat-guy",
      "name": "retrieval-augmented-thinking",
      "url": "https://github.com/stat-guy/retrieval-augmented-thinking",
      "imageUrl": "/freedevtools/mcp/pfp/stat-guy.webp",
      "description": "Enhances AI model reasoning and problem-solving capabilities by employing structured retrieval-augmented thinking processes. Provides features like adaptive thought chains, iterative hypothesis generation, and real-time context coherence for complex reasoning tasks.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T07:53:28Z",
      "readme_content": "# RAT MCP Server (Node.js)\n\nRetrieval Augmented Thinking MCP Server - A reasoning tool that processes structured thoughts with metrics, branching, and revision capabilities.\n\n## Installation\n\n### Simple 3-Step Process\n```bash\ngit clone https://github.com/stat-guy/retrieval-augmented-thinking.git\ncd retrieval-augmented-thinking\nnpm install -g .\n```\n\n### Verify Installation\nTest that the installation worked:\n```bash\nnpx mcp-server-rat-node --help\n```\n\n**Success indicator:** If you see `RAT MCP Server (Node.js) running on stdio`, your installation is ready!\n\n## Claude Desktop Configuration\n\nAdd to your Claude Desktop configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"retrieval-augmented-thinking\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-rat-node\"]\n    }\n  }\n}\n```\n\nAfter adding the configuration, restart Claude Desktop to load the RAT server.\n\n## Usage\n\nThe server provides a single `rat` tool for processing structured thoughts:\n\n```javascript\n// Basic usage\n{\n  \"thought\": \"I need to analyze this problem step by step...\",\n  \"nextThoughtNeeded\": true,\n  \"thoughtNumber\": 1,\n  \"totalThoughts\": 3\n}\n\n// With revision\n{\n  \"thought\": \"Let me reconsider my previous analysis...\",\n  \"nextThoughtNeeded\": false,\n  \"thoughtNumber\": 2,\n  \"totalThoughts\": 3,\n  \"isRevision\": true,\n  \"revisesThought\": 1\n}\n\n// With branching\n{\n  \"thought\": \"Alternative approach: what if we consider...\",\n  \"nextThoughtNeeded\": true,\n  \"thoughtNumber\": 2,\n  \"totalThoughts\": 4,\n  \"branchFromThought\": 1,\n  \"branchId\": \"alt-path-1\"\n}\n```\n\n## Tool Parameters\n\n### Required\n- `thought` (string): The thought content to process\n- `nextThoughtNeeded` (boolean): Whether another thought is needed to continue\n- `thoughtNumber` (integer): Current thought number in the sequence\n- `totalThoughts` (integer): Total expected thoughts (adjustable)\n\n### Optional\n- `isRevision` (boolean): Whether this revises a previous thought\n- `revisesThought` (integer): The thought number being revised\n- `branchFromThought` (integer): Thought number to branch from\n- `branchId` (string): Unique identifier for this branch\n- `needsMoreThoughts` (boolean): Extend beyond totalThoughts if needed\n\n## Response Format\n\n```json\n{\n  \"thought_number\": 1,\n  \"total_thoughts\": 3,\n  \"metrics\": {\n    \"complexity\": 0.342,\n    \"depth\": 0.521,\n    \"quality\": 0.643,\n    \"impact\": 0.289,\n    \"confidence\": 0.758\n  },\n  \"analytics\": {\n    \"total_thoughts\": 5,\n    \"average_quality\": 0.612,\n    \"chain_effectiveness\": 0.145\n  },\n  \"next_thought_needed\": true,\n  \"visual_output\": \"‚îå‚îÄ üí≠ Thought 1/3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\\\n‚îÇ Analysis shows clear patterns... ‚îÇ\\\\n‚îú‚îÄ Metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\\\\n‚îÇ Quality: 0.64 | Impact: 0.29... ‚îÇ\\\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\"\n}\n```\n\n## Troubleshooting\n\n### If Installation Fails\nThe installation process includes automatic permission fixes. If you encounter issues:\n\n1. **Ensure you have Node.js and npm installed**\n2. **Try the alternative installation method:**\n   ```bash\n   npm install -g git+https://github.com/stat-guy/retrieval-augmented-thinking.git\n   ```\n3. **For rare permission issues:**\n   ```bash\n   chmod +x $(npm bin -g)/mcp-server-rat-node\n   ```\n\n### Verification Steps\nBefore configuring Claude Desktop, always verify:\n```bash\nnpx mcp-server-rat-node --help\n```\n\nIf this shows \"RAT MCP Server (Node.js) running on stdio\", you're ready to configure Claude Desktop.\n\n## Testing\n\nRun the test suite:\n```bash\nnpm test\n```\n\nTest tool execution:\n```bash\nnode test-tool.js\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "retrieval",
        "augmented",
        "augmented thinking",
        "reasoning tasks",
        "retrieval augmented"
      ],
      "category": "scientific-research-tools"
    },
    "svmlearn---": {
      "owner": "svmlearn",
      "name": "-",
      "url": "https://github.com/svmlearn/-",
      "imageUrl": "/freedevtools/mcp/pfp/svmlearn.webp",
      "description": "Supports training, testing, and deploying support vector machine models for classification tasks. Facilitates advanced algorithm utilization to improve predictive performance within machine learning projects.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2022-05-09T07:49:49Z",
      "readme_content": "# -",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "svmlearn",
        "classification",
        "vector",
        "tools svmlearn",
        "svmlearn supports",
        "vector machine"
      ],
      "category": "scientific-research-tools"
    },
    "szeider--mcp-solver": {
      "owner": "szeider",
      "name": "mcp-solver",
      "url": "https://github.com/szeider/mcp-solver",
      "imageUrl": "/freedevtools/mcp/pfp/szeider.webp",
      "description": "Integrates SAT, SMT, and constraint solving capabilities with Large Language Models, enabling interactive creation, editing, and solving of constraint models in MiniZinc, SAT models in PySAT, and SMT formulas using Z3 Python.",
      "stars": 133,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T09:11:19Z",
      "readme_content": "------\n\n# MCP Solver\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![Python Version](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)\n\nA Model Context Protocol (MCP) server that exposes constraint solving, SAT, SMT, and ASP capabilities to Large Language Models.\n\n------\n\n## Overview\n\nThe *MCP Solver* integrates constraint solving, SAT, SMT, and ASP with LLMs through the Model Context Protocol, enabling AI models to interactively create, edit, and solve:\n\n- Constraint models in [MiniZinc](https://www.minizinc.org/)\n- SAT models in [PySAT](https://pysathq.github.io/)\n- MaxSAT optimization problems in [PySAT](https://pysathq.github.io/)\n- SMT formulas in [Z3 Python](https://ericpony.github.io/z3py-tutorial/guide-examples.htm)\n- Answer Set Programs in [Clingo](https://potassco.org/clingo/)\n\nFor a detailed description of the *MCP Solver's* system architecture and theoretical foundations, see the accompanying research paper: Stefan Szeider, [\"Bridging Language Models and Symbolic Solvers via the Model Context Protocol\"](https://doi.org/10.4230/LIPIcs.SAT.2025.30), SAT 2025.\n\n## Available Tools\n\nIn the following, *item* refers to some part of the (MiniZinc/PySAT/Z3/ASP) code, and *model* to the encoding. \n\n| Tool Name      | Description                                   |\n| -------------- | --------------------------------------------- |\n| `clear_model`  | Remove all items from the model               |\n| `add_item`     | Add new item at a specific index              |\n| `delete_item`  | Delete item at index                          |\n| `replace_item` | Replace item at index                         |\n| `get_model`    | Get current model content with numbered items |\n| `solve_model`  | Solve the model (with timeout parameter)      |\n\n------\n\n## System Requirements\n\n- Python and project manager [uv](https://docs.astral.sh/uv/) \n- Python 3.11+\n- Mode-specific requirements: MiniZinc, PySAT, Python Z3 (required packages are installed via pip)\n- Operating systems: macOS, Windows, Linux (with appropriate adaptations)\n\n------\n\n## Installation\n\nMCP Solver requires Python 3.11+, the `uv` package manager, and solver-specific dependencies (MiniZinc, Z3, or PySAT).\n\nFor detailed installation instructions for Windows, macOS, and Linux, see [INSTALL.md](INSTALL.md).\n\nQuick start:\n```bash\ngit clone https://github.com/szeider/mcp-solver.git\ncd mcp-solver\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[all]\"  # Install all solvers\n```\n\n------\n\n## Available Modes / Solving Backends\n\nThe MCP Solver provides five distinct operational modes, each integrating with a different constraint solving backend. Each mode requires specific dependencies and offers unique capabilities for addressing different classes of problems.\n\n### MiniZinc Mode\n\nMiniZinc mode provides integration with the MiniZinc constraint modeling language with the following features:\n\n- Rich constraint expression with global constraints\n- Integration with the Chuffed constraint solver\n- Optimization capabilities\n- Access to solution values via `get_solution`\n\n**Dependencies**: Requires the `minizinc` package (`uv pip install -e \".[mzn]\"`)\n\n**Configuration**: To run in MiniZinc mode, use:\n\n```\nmcp-solver-mzn\n```\n\n### PySAT Mode\n\nPySAT mode allows interaction with the Python SAT solving toolkit with the following features:\n\n- Propositional constraint modeling using CNF (Conjunctive Normal Form)\n- Access to various SAT solvers (Glucose3, Glucose4, Lingeling, etc.)\n- Cardinality constraints (at_most_k, at_least_k, exactly_k)\n- Support for boolean constraint solving\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in PySAT mode, use:\n\n```\nmcp-solver-pysat\n```\n\n### MaxSAT Mode\n\nMaxSAT mode provides specialized support for optimization problems with PySAT, featuring:\n\n- Weighted Conjunctive Normal Form (WCNF) support\n- Integration with the RC2 MaxSAT solver\n- Optimization capabilities with objective tracking\n- Support for both hard and soft constraints\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in MaxSAT mode, use:\n\n```\nmcp-solver-maxsat\n```\n\n### Z3 Mode\n\nZ3 mode provides access to Z3 SMT (Satisfiability Modulo Theories) solving capabilities with the following features:\n\n- Rich type system: booleans, integers, reals, bitvectors, arrays\n- Constraint solving with quantifiers\n- Optimization capabilities\n- Template library for common modeling patterns\n\n**Dependencies**: Requires the `z3-solver` package (`uv pip install -e \".[z3]\"`)\n\n**Configuration**: To run in Z3 mode, use:\n\n```\nmcp-solver-z3\n```\n\n### ASP Mode\n\nASP (Answer Set Programming) mode provides integration with ASP solvers (e.g., Clingo) for declarative problem solving with logic programs. Features include:\n\n- Expressive logic programming for combinatorial and knowledge representation problems\n- Support for constraints, choice rules, aggregates, and optimization statements\n- Integration with the Clingo solver for efficient answer set computation\n- Access to answer sets and model inspection\n\n**Dependencies**: Requires the `clingo` package (`uv pip install -e \".[asp]\"` or included in `[all]`)\n\n**Configuration**: To run in ASP mode, use:\n\n```\nmcp-solver-asp\n```\n\n## MCP Test Client\n\nThe MCP Solver includes an MCP client for development, experimentation, and diagnostic purposes, based on the *ReAct* agent framework. This client serves as an intermediary between an LLM and the MCP server, facilitating the translation of natural language problem statements into formal constraint programming solutions. \n\n### Installation\n\n```bash\n# Install client dependencies\nuv pip install -e \".[client]\"\n\n# Verify client installation and configuration\nuv run test-setup-client\n```\n\nThe client requires an **API key** from an LLM provider. For Anthropic (the default LLM is Claude Sonnet 3.7), set the `ANTHROPIC_API_KEY` environment variable. This can be set in your environment or included in a `.env` file in the project root.\n\nThe client also supports other LLM providers including OpenAI, Google (Gemini), OpenRouter and even local models via LM Studio. You can specify which model to use with the `--mc` command line flag. See [INSTALL.md](INSTALL.md) for details on model code formats.\n\n### Usage\n\nThe client provides specialized commands for each solver backend:\n\n```bash\n# MiniZinc mode\nuv run run-test mzn --problem <path/to/problem.md>\n\n# PySAT mode\nuv run run-test pysat --problem <path/to/problem.md>\n\n# MaxSAT mode\nuv run run-test maxsat --problem <path/to/problem.md>\n\n# Z3 mode\nuv run run-test z3 --problem <path/to/problem.md>\n\n# ASP mode\nuv run run-test asp --problem <path/to/problem.md>\n```\n\n------\n\n## Examples\n\n### Example 1: Casting Problem (MiniZinc)\n\nThis is based on an example by George Johnson (NYT, July 13, 1999).\n\n**User**\n\n> Suppose that a theatrical director feels obligated to cast either his ingenue, Actress Alvarez, or his nephew, Actor Cohen, in a production. But Miss Alvarez won't be in a play with Mr. Cohen (her former lover), and she demands that the cast include her new flame, Actor Davenport. The producer, with her own favors to repay, insists that Actor Branislavsky have a part. But Mr. Branislavsky won't be in any play with Miss Alvarez or Mr. Davenport. Can the director cast the play?\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> var bool: alvarez;\n> var bool: cohen;\n> var bool: branislavsky;\n> var bool: davenport;\n> constraint alvarez \\/ cohen;                          % Must cast either Alvarez or Cohen\n> constraint not (alvarez /\\ cohen);                    % Alvarez won't work with Cohen\n> constraint alvarez -> davenport;                      % If Alvarez is cast, Davenport must be cast\n> constraint branislavsky;                              % Branislavsky must be cast\n> constraint not (branislavsky /\\ alvarez);             % Branislavsky won't work with Alvarez\n> constraint not (branislavsky /\\ davenport);           % Branislavsky won't work with Davenport\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(alvarez=False, cohen=True, branislavsky=True, davenport=False, _checker=''), 'solve_time': 0.0}\n> ```\n\nThe solution (cast Cohen and Branislavsky, but not Alvarez or Davenport) satisfies all constraints.\n\n### Example 2: N-Queens Problem (MiniZinc)\n\n**User**\n\n> Check whether you can place n Queens on an nxn chessboard. Try n=10,20,30,40 and compare the solving times\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> int: n = 10;\n> array[1..n] of var 1..n: queens;\n> constraint alldifferent(queens);\n> constraint alldifferent([queens[i] + i | i in 1..n]);\n> constraint alldifferent([queens[i] - i | i in 1..n]);\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`** (with n=10, 20, 30, and 40 by changing the value of n)\n\nSolve times comparison: n=10: 0.001s n=20: 0.005s n=30: 0.040s n=40: 0.043s\n\nSolutions exist for all tested board sizes. Time complexity increases non-linearly but remains manageable up to n=40.\n\n### Example 3: Traveling Salesperson Problem (MiniZinc)\n\n**User**\n\n> A saleswoman based in Vienna needs to plan her upcoming tour through Austria, visiting each province capital once. Help find the shortest route. Distances in km: 1=Vienna, 2=St. P√∂lten, 3=Eisenstadt, 4=Linz, 5=Graz, 6=Klagenfurt, 7=Salzburg, 8=Innsbruck, 9=Bregenz\n>\n> [Distance matrix table provided in user message]\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> include \"globals.mzn\";\n> int: n = 9;\n> array[1..n,1..n] of int: dist = [|0,65,60,184,195,319,299,478,631|65,0,125,119,130,254,234,413,566|60,125,0,184,157,281,261,440,593|184,119,184,0,208,252,136,315,468|195,130,157,208,0,136,280,459,629|319,254,281,252,136,0,217,391,566|299,234,261,136,280,217,0,188,343|478,413,440,315,459,391,188,0,157|631,566,593,468,629,566,343,157,0|];\n> array[1..n] of var 1..n: succ;\n> var int: total_dist = sum(i in 1..n)(dist[i,succ[i]]);\n> constraint circuit(succ);\n> solve minimize total_dist;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(objective=1564, succ=[2, 4, 1, 7, 3, 5, 9, 6, 8], _checker=''), 'solve_time': 0.002}\n> ```\n\nOptimal route: Vienna ‚Üí St. P√∂lten ‚Üí Linz ‚Üí Salzburg ‚Üí Bregenz ‚Üí Innsbruck ‚Üí Klagenfurt ‚Üí Graz ‚Üí Eisenstadt ‚Üí Vienna with a total distance of 1,564 km.\n\n## Feedback\n\nYou can provide feedback to the author via this [form](https://form.jotform.com/szeider/mcp-solver-feedback-form).\n\n## Disclaimer\n\nThis MCP Solver is in its prototype stage and should be used with caution. Users are encouraged to experiment, but any use in critical environments is at their own risk.\n\n------\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](https://claude.ai/chat/LICENSE) file for details.\n\n------\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "solver",
        "python",
        "pysat",
        "mcp solver",
        "smt constraint",
        "sat models"
      ],
      "category": "scientific-research-tools"
    },
    "t0mst0ne--pubmed-mcp-easy": {
      "owner": "t0mst0ne",
      "name": "pubmed-mcp-easy",
      "url": "https://github.com/t0mst0ne/pubmed-mcp-easy",
      "imageUrl": "/freedevtools/mcp/pfp/t0mst0ne.webp",
      "description": "Search and retrieve biomedical literature and research papers from PubMed. Supports advanced searches, batch querying, and retrieval of abstracts and full texts of open access articles.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-18T01:29:22Z",
      "readme_content": "# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@t0mst0ne/pubmed-mcp-easy)](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy)\n\nEnhanced Python-based PubMed MCP Server with API key support for faster and unlimited downloads.\n\n## Certified by MCP Review\n[https://mcpreview.com/mcp-servers/t0mst0ne/pubmed-mcp-easy]\n\n## Features\n\n- Search PubMed for biomedical literature and research papers\n- Find similar articles, cited articles, and citing articles\n- Retrieve abstracts and full text of open access articles\n- Batch search and advanced search capabilities\n- API key support for faster and unlimited downloads\n\n## API Key and Email Setup\n\nThe NCBI E-utilities API recommends using an API key and email address with your requests. This brings several benefits:\n\n- **Higher rate limits**: 10 requests/second instead of 3\n- **More results per query**: Up to 200 results per request instead of 100\n- **Priority service**: Better queue position for your requests\n\n### How to Get an API Key\n\n1. Create an NCBI account at [https://www.ncbi.nlm.nih.gov/account/](https://www.ncbi.nlm.nih.gov/account/)\n2. Go to the API Keys Management page\n3. Generate a new API key\n\n### Setting Up API Key and Email\n\nThe easiest way to set up your API key and email is using the setup script:\n\n```bash\npython setup_api.py\n```\n\nThis interactive script will guide you through the process and offer multiple setup options.\n\nAlternatively, you can set up your API key and email manually using one of these three methods:\n\n#### 1. Environment Variables\n\n```bash\nexport NCBI_API_KEY=your_api_key_here\nexport NCBI_EMAIL=your_email@example.com\n```\n\n#### 2. Command Line Arguments\n\n```bash\npython python-pubmed-mcp-enhanced.py --api-key your_api_key_here --email your_email@example.com\n```\n\n#### 3. Configuration File\n\nCreate a `config.json` file based on the example:\n\n```bash\ncp config.json.example config.json\n```\n\nEdit `config.json` to include your API key and email:\n\n```json\n{\n  \"api_key\": \"your_api_key_here\",\n  \"email\": \"your_email@example.com\"\n}\n```\n\nThen run the server with the config file:\n\n```bash\npython python-pubmed-mcp-enhanced.py --config config.json\n```\n\n## Usage\n\n### Installing via Smithery\n\nTo install pubmed-mcp-easy for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy):\n\n```bash\nnpx -y @smithery/cli install @t0mst0ne/pubmed-mcp-easy --client claude\n```\n\n### Standard Usage\n\nRun the server:\n\n```bash\npython python-pubmed-mcp-enhanced.py\n```\n\n### Claude Desktop Integration\n\nTo integrate with Claude Desktop, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"pubmed-easy\": {\n    \"command\": \"/opt/anaconda3/bin/python\",\n    \"args\": [\n        \"/GITHUB_cloned_dir/pubmed-mcp-easy/python-pubmed-mcp-enhanced.py\", \"--config\", \"config.json\"\n    ]\n}\n```\n\nMake sure to:\n1. Replace `/opt/anaconda3/bin/python` with the path to your Python executable\n2. Replace `/GITHUB_cloned_dir` with the actual path to your GitHub directory\n3. Create a `config.json` file with your API key and email as described above\n\nAfter adding this configuration, you can access PubMed tools directly from Claude Desktop.\n\n### Available Tools\n\nThe server provides the following MCP tools:\n\n- `pubmed_search`: Search for articles by keyword or query\n- `pubmed_similar`: Find similar articles\n- `pubmed_cites`: Find articles cited by a specific paper\n- `pubmed_cited_by`: Find articles that cite a specific paper\n- `pubmed_abstract`: Retrieve the abstract of an article\n- `pubmed_open_access`: Check if an article is open access\n- `pubmed_full_text`: Retrieve the full text of an open access article\n- `pubmed_batch_search`: Perform multiple searches in parallel\n- `pubmed_author_search`: Search for papers by a specific author\n- `pubmed_advanced_search`: Perform advanced field-based searches\n- `pubmed_journal_search`: Search for papers in a specific journal\n\n## Important Notes\n\n1. Including an email address is recommended by NCBI as it allows them to contact you if there are issues with your requests.\n2. If you make heavy use of the E-utilities, NCBI recommends that you limit large jobs to either weekends or between 9 pm and 5 am Eastern Time weekdays.\n3. Always be considerate in your usage and follow NCBI's usage guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "searches",
        "pubmed mcp",
        "papers pubmed",
        "t0mst0ne pubmed"
      ],
      "category": "scientific-research-tools"
    },
    "tutumomo--MCP-civil-tools": {
      "owner": "tutumomo",
      "name": "MCP-civil-tools",
      "url": "https://github.com/tutumomo/MCP-civil-tools",
      "imageUrl": "/freedevtools/mcp/pfp/tutumomo.webp",
      "description": "Provides civil engineering calculation tools including coordinate conversions between latitude/longitude and UTM/TWD97, Manning's coefficient lookup, soil pressure coefficient calculations, and drainage flow computations. Integrates seamlessly with LLM applications and AI tools for streamlined civil engineering workflows.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-19T06:16:30Z",
      "readme_content": "# MCP Civil Tools ‰º∫ÊúçÂô®\n\nÊú¨Â∞àÊ°àÊòØ‰∏ÄÂÄãÂü∫Êñº MCP ÂçîË≠∞ÁöÑ Python ‰º∫ÊúçÂô®Ôºå‰∏ªË¶ÅÊòØÂ∞áÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑ(2023.05.04)ÈÄèÈÅé MCP Server ÁöÑÊñπÂºèÊèê‰æõÁµ¶ÊîØÊè¥ MCP ÁöÑ client Ë™øÁî®ÔºåÂØ¶Áèæ‰ª•Ëá™ÁÑ∂Ë™ûË®ÄËàáÂ§ßÊ®°ÂûãÂ∞çË©±ÁöÑÊñπÂºè‰æÜÈÄ≤Ë°åÊü•Ë©¢ÂèäË®àÁÆóÔºåÊèê‰æõÂ¶ÇÊõºÂØß‰øÇÊï∏Êü•Ë©¢„ÄÅÂúüÂ£ìÂäõ‰øÇÊï∏„ÄÅÊéíÊ∞¥Ê∫ùÊµÅÈÄü„ÄÅÈÇäÂù°Á©©ÂÆö„ÄÅÂúüÂ£§‰æµËùï„ÄÅÈÄïÊµÅÈáè„ÄÅÊìãÂúüÁâÜÊ™¢Ê†∏„ÄÅÊ§çÁîüÂª∫Ë≠∞„ÄÅÊùêÊñôÂèÉÊï∏„ÄÅÂù°Èù¢‰øùË≠∑„ÄÅÊª≤Ê∞¥Ë®≠ÊñΩ„ÄÅIDFÊõ≤Á∑öÁ≠âÊü•Ë©¢ÂèäË®àÁÆóÂäüËÉΩ„ÄÇÂè¶Â§ñÈÇÑÊèê‰æõÂè∞ÁÅ£Âú∞ÂçÄÁ∂ìÁ∑ØÂ∫¶Ëàá UTM/TWD97 Â∫ßÊ®ôÁöÑËΩâÊèõ„ÄÇÈÅ©Áî®Êñº LLM Â∑•ÂÖ∑„ÄÅClaude Desktop Á≠â AI ÊáâÁî®Êï¥Âêà„ÄÇ\n\nÊ≥®ÊÑè! Á®ãÂºèÂ∞öÊú™ÂÆåÊï¥Ôºå‰∏îÊØèÂÄãÂ§ßÊ®°ÂûãÊô∫Âäõ‰∏çÁ≠âÔºåËº∏Âá∫ÁµêÊûú‰ΩøÁî®ËÄÖÂãôÂøÖÂÜçÊ¨°Ê†∏ÂØ¶ÔºåËã•ÊúâÈåØË™§ÔºåÊ≠°ËøéÂèçÈ•ã~\n---\n\n## ÁõÆÈåÑÁµêÊßã\n\n```\nMCP-civil-tools/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ mcp_server.py         # MCP ‰º∫ÊúçÂô®‰∏ªÁ®ãÂºè\n‚îÇ   ‚îú‚îÄ‚îÄ util.py              # Â∫ßÊ®ôËΩâÊèõËàáÂúüÊú®Â∑•Á®ãÂ∑•ÂÖ∑ÈÇèËºØ\n‚îÇ   ‚îî‚îÄ‚îÄ utm_types/\n‚îÇ       ‚îî‚îÄ‚îÄ __init__.py       # ÂûãÂà•ÂÆöÁæ©\n‚îú‚îÄ‚îÄ requirements.txt          # ‰æùË≥¥Â•ó‰ª∂\n‚îú‚îÄ‚îÄ README.md                 # Â∞àÊ°àË™™Êòé\n‚îî‚îÄ‚îÄ .gitignore                # Git ÂøΩÁï•Ë¶èÂâá\n```\n\n---\n\n## ÂÆâË£ù\n\n1. Âª∫Ë≠∞ÂÖàÂª∫Á´ãËôõÊì¨Áí∞Â¢ÉÔºö\n   ```bash\n   python -m venv .venv\n   .venv\\Scripts\\activate  # Windows\n   # Êàñ source .venv/bin/activate  # Linux/macOS\n   ```\n2. ÂÆâË£ù‰æùË≥¥Ôºö\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n---\n\n## ÂïüÂãïÊñπÂºè\n\n### 1. CLI (stdio) Ê®°Âºè\n\n```bash\npython src/mcp_server.py\n```\n\n### 2. HTTP ÊúçÂãôÊ®°ÂºèÔºàÂª∫Ë≠∞Áî®Êñº API Ê∏¨Ë©¶Êàñ HTTP Êï¥ÂêàÔºâ\n\n```bash\nuvicorn src.mcp_server:app --port 8000\n```\n\n---\n\n## mcp.json Ë®≠ÂÆöÁØÑ‰æã\n\nËã•Ë¶ÅËÆì Claude Desktop ÊàñÂÖ∂‰ªñ MCP ÂÆ¢Êà∂Á´ØËá™ÂãïÂÅµÊ∏¨‰∏¶ÂïüÂãïÊú¨‰º∫ÊúçÂô®ÔºåMCP setting JSON ÂÖßÂÆπÂ¶Ç‰∏ãÔºàË´ã‰æùÂØ¶ÈöõË∑ØÂæëË™øÊï¥ÔºâÔºö\n\n```json\n{\n  \"mcpServers\": {\n    \"MCP-civil-tools\": {\n      \"command\": \"path/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"path/src/mcp_server.py\"\n      ]\n    }\n  }\n}\nÊàñÊòØ\n    \"MCP-civil-tools\": {\n      \"command\": \"C:/TOMO/MCP-civil-tools/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"C:/TOMO/MCP-civil-tools/src/mcp_server.py\"\n      ],\n      \"disabled\": true,\n      \"autoApprove\": []\n    },\n\n```\n- `command` Ë´ãÂ°´ÂÖ•‰Ω†ËôõÊì¨Áí∞Â¢ÉÁöÑ python.exe ÁµïÂ∞çË∑ØÂæë„ÄÇ\n- `args` Ë´ãÂ°´ÂÖ• mcp_server.py ÁöÑÁµïÂ∞çË∑ØÂæë„ÄÇ\n\n‰ª• Windowss Á≥ªÁµ±ÔºåCommand argsÔºåËº∏ÂÖ•Ê†ºÂºèÂ¶Ç‰∏ãÔºö\nC:\\TOMO\\MCP-civil-tools\\.venv\\Scripts\\python.exe C:\\TOMO\\MCP-civil-tools\\src\\mcp_server.py\n\n‰ª• MAC Á≥ªÁµ±ÔºåCommand argsÔºåËº∏ÂÖ•Ê†ºÂºèÂ¶Ç‰∏ãÔºö\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/.venv/bin/python3\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/src/mcp_server.py\n\n---\n\n## ÂèÉÊï∏È†êË®≠Ë°åÁÇ∫Ë™™Êòé\n\n- ‰ΩøÁî®ËÄÖÂè™Ëº∏ÂÖ•Á∑ØÂ∫¶„ÄÅÁ∂ìÂ∫¶ÊôÇÔºåÊ≤íÊúâËº∏ÂÖ•ÂÖ∂‰ªñË≥áË®äÊôÇÔºåÈ†êË®≠ÁöÑ UTM/TM2 Zone Â∞±ÊòØ TM2-121ÔºåÈ†êË®≠ÁöÑÂçäÁêÉÂ∞±ÊòØÂåóÂçäÁêÉ„ÄÇ\n- Áï∂‰ΩøÁî®ËÄÖÂè™Ëº∏ÂÖ•Âπ≥Èù¢Â∫ßÊ®ô X, Y ÊôÇÔºåÈ†êË®≠ÁöÑ UTM Zone ÊòØ TWD97ÔºåÂçäÁêÉÊòØÂåóÂçäÁêÉ„ÄÇ\n\n---\n\n## ‰∏ªË¶ÅÂäüËÉΩÊõ¥Êñ∞\n\n- **ÊâÄÊúâÊü•Ë°®ÂûãÂ∑•ÂÖ∑ÁöÜÊîØÊè¥„ÄåÊîØÊè¥Ê∏ÖÂñÆÊü•Ë©¢„ÄçAPI**Ôºö\n  - ‰æãÂ¶ÇÔºö`list_supported_materials`ÔºàÂ∏∏Áî®ÊùêÊñôÔºâ„ÄÅ`list_supported_manning_materials`ÔºàÊõºÂØß‰øÇÊï∏ÊùêÊñôÔºâ„ÄÅ`list_supported_max_velocity_materials`ÔºàÊúÄÂ§ßÊµÅÈÄüÊùêÊñôÔºâ„ÄÅ`list_supported_regions`ÔºàÂú∞ÂçÄ/IDF/Âπ¥Èõ®ÈáèÔºâ„ÄÅ`list_supported_soil_types`ÔºàÂúüÂ£§È°ûÂûãÔºâ„ÄÅ`list_supported_land_uses`ÔºàÂúüÂú∞Âà©Áî®Ôºâ„ÄÅ`list_supported_practices`ÔºàÊ∞¥‰øùÊé™ÊñΩÔºâ„ÄÅ`list_supported_runoff_land_uses`ÔºàÈÄïÊµÅ‰øÇÊï∏ÂúüÂú∞Âà©Áî®Ôºâ„ÄÅ`list_supported_slope_protection_methods`ÔºàÂù°Èù¢‰øùË≠∑Â∑•Ê≥ïÔºâ„ÄÅ`list_supported_soil_k_types`ÔºàÊª≤ÈÄè‰øÇÊï∏ÂúüÂ£§Ôºâ„ÄÅ`list_supported_idf_locations`ÔºàIDFÊõ≤Á∑öÂú∞ÈªûÔºâÁ≠â„ÄÇ\n  - Êü•Ë©¢ÊôÇËã•Ëº∏ÂÖ•ÈåØË™§ÊàñÊü•ÁÑ°Ë≥áÊñôÔºåÊúÉËá™ÂãïÊèêÁ§∫ÊâÄÊúâÂèØÊü•Ë©¢ÁöÑÊîØÊè¥È†ÖÁõÆÔºåÊèêÂçá‰ΩøÁî®È´îÈ©ó„ÄÇ\n\n- **2023/04/23 Êõ¥Êñ∞**Ôºö\n  - ‰øÆÊ≠£ `list_supported_materials` ÂäüËÉΩÔºåÁèæÂú®ÂèØÊ≠£Á¢∫ËøîÂõûÊâÄÊúâÊîØÊè¥ÁöÑÂ∏∏Áî®ÊùêÊñôË®≠Ë®àÂèÉÊï∏ÊùêÊñôÂêçÁ®±Ê∏ÖÂñÆÔºåÂåÖÊã¨Ôºö‰∏ÄËà¨ÈªèÂúü„ÄÅÁ†ÇÂúü„ÄÅÁ§´Áü≥„ÄÅÊ∑∑ÂáùÂúü„ÄÅËä±Â¥óÂ≤©„ÄÅÁü≥ÁÅ∞Â≤©„ÄÅÈ†ÅÂ≤©„ÄÅÁ†ÇÂ≤©Á≠â„ÄÇ\n  - Êñ∞Â¢ûÂõõÂÄã USLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑Ôºö\n    - `query_r_factor_tool`ÔºöÊü•Ë©¢ÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_k_factor_tool`ÔºöÊü•Ë©¢ÂúüÂ£§Ê≤ñËùïÊåáÊï∏ K ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_c_factor_tool`ÔºöÊü•Ë©¢Ë¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n    - `query_p_factor_tool`ÔºöÊü•Ë©¢Ê∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠ê P ÂÄºÔºà‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºâ\n  - ‰øÆÊ≠£ `calc_catchment_runoff` ÂäüËÉΩÔºåÂ¢ûÂº∑ÂèÉÊï∏ËôïÁêÜËÉΩÂäõÔºåÁèæÂú®ÂèØ‰ª•Ê≠£Á¢∫ËôïÁêÜËá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•Ôºå‰æãÂ¶Ç„ÄåË®àÁÆó‰∏ÄÂÄãÈù¢Á©ç2ÂÖ¨È†É„ÄÅÈôçÈõ®Âº∑Â∫¶100mm/hr„ÄÅÈÄïÊµÅ‰øÇÊï∏0.6ÁöÑÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè„Äç„ÄÇ\n  - Êì¥ÂÖÖ `check_retaining_wall` ÂäüËÉΩÔºåÊñ∞Â¢ûÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÂÆâÂÖ®‰øÇÊï∏Ê™¢Ê†∏Ôºö\n    - ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨120Ê¢ùÔºåÂ¢ûÂä†Âú∞ÈúáÊôÇÊªëÂãïÂÆâÂÖ®‰øÇÊï∏Ôºà‚âß1.2ÔºâÂíåÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏Ôºà‚âß1.5ÔºâÁöÑÊ™¢Ê†∏\n    - Â¢ûÂä†ÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÊ™¢Ê†∏ÔºåÊîØÊè¥Â≤©Áõ§Âü∫Á§éÂíåÂúüÂ±§Âü∫Á§éÂÖ©Á®ÆÊÉÖÊ≥Å\n    - ‰øÆÊ≠£ÊâøËºâÂäõÂÆâÂÖ®‰øÇÊï∏Ë¶ÅÊ±ÇÁÇ∫3.0ÔºåÁ¨¶ÂêàË¶èÁØÑË¶ÅÊ±Ç\n    - Ëº∏Âá∫Â†±Ë°®‰∏≠ÊòéÁ¢∫ÂçÄÂàÜÂ∏∏ÊôÇÊÉÖÊ≥ÅÂíåÂú∞ÈúáÊÉÖÊ≥ÅÁöÑÂÆâÂÖ®‰øÇÊï∏\n\n---\n\n## ÂÄãÂà•Â∑•ÂÖ∑‰ΩøÁî®ÁØÑ‰æã\n\n### Á∂ìÁ∑ØÂ∫¶ËΩâ UTM\n- Ëº∏ÂÖ•ÔºöËá™ÁÑ∂Ë™ûË®ÄÊèèËø∞Á∂ìÁ∑ØÂ∫¶ÔºåÂ¶Ç„ÄåÂ∞áÁ∑ØÂ∫¶24.125„ÄÅÁ∂ìÂ∫¶120.641ËΩâÊèõÁÇ∫UTMÂ∫ßÊ®ô„Äç\n- ÂõûÂÇ≥Ôºö`\"X,Y\"` Â≠ó‰∏≤ÔºåÊï∏ÂÄºÂõõÊç®‰∫îÂÖ•Âà∞Â∞èÊï∏Èªû‰∏ã4‰Ωç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÂ∞áÁ∑ØÂ∫¶24.125193616011536„ÄÅÁ∂ìÂ∫¶120.64098341751337ËΩâÊèõÁÇ∫UTMÂ∫ßÊ®ô\nÂõûÂÇ≥Ôºö203650.6040,2670482.4250\n```\n\n### UTM ËΩâÁ∂ìÁ∑ØÂ∫¶\n- Ëº∏ÂÖ•ÔºöX, Y Â∫ßÊ®ôÔºàÂèØÈÅ∏ zone, datum, southÔºåÈ†êË®≠ TWD97 ÂåóÂçäÁêÉÔºâ\n- ÂõûÂÇ≥Ôºö`\"Á∑ØÂ∫¶,Á∂ìÂ∫¶\"` Â≠ó‰∏≤ÔºåÊï∏ÂÄºÂõõÊç®‰∫îÂÖ•Âà∞Â∞èÊï∏Èªû‰∏ã15‰Ωç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÂ∞á UTM Â∫ßÊ®ô X=203650.604„ÄÅY=2670482.425 ËΩâÊèõÁÇ∫Á∂ìÁ∑ØÂ∫¶\nÂõûÂÇ≥Ôºö24.125193616011536,120.64098341751337\n```\n\n### ÊõºÂØß‰øÇÊï∏Êü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÊùêÊñôÂêçÁ®±ÔºàÂ¶Ç„ÄåÊ∑∑ÂáùÂúü„Äç„ÄÅ„ÄåÁ¥îÁ¥∞Á†Ç„Äç„ÄÅ„ÄåÂÖ®Èù¢ÂØÜËçâÁîü„ÄçÁ≠âÔºâ\n- ÂõûÂÇ≥ÔºöË©≤ÊùêÊñôÁöÑÊõºÂØß‰øÇÊï∏ n ÂèäÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúç\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖ®Èù¢ÂØÜËçâÁîüÁöÑÊõºÂØß‰øÇÊï∏ÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂÖ®Èù¢ÂØÜËçâÁîü ÁöÑÊõºÂØß‰øÇÊï∏ n = 0.040ÔºåÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúçÔºö1.5~2.5 m/s\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÊõºÂØß‰øÇÊï∏Ë°®\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ê∑∑ÂáùÂúüÁöÑÊõºÂØß‰øÇÊï∏\nÂõûÂÇ≥ÔºöÊ∑∑ÂáùÂúü ÁöÑÊõºÂØß‰øÇÊï∏ n = 0.013ÔºåÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÁØÑÂúçÔºö3.0~6.0 m/s\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÊõºÂØß‰øÇÊï∏Ë°®\n```\n\n### ‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂÖßÊë©Êì¶ËßíÁ≠âË≥áË®ä\n- ÂõûÂÇ≥Ôºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Ka„ÄÅË®àÁÆóÂÖ¨Âºè„ÄÅ‰æùÊìöÁ≠â\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖßÊë©Êì¶Ëßí30Â∫¶ÁöÑ‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏KaÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥Ôºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117Ê¢ùÂèäÈôÑ‰ª∂ÂúüÂ£ìÂäõË®àÁÆóÂÖ¨Âºè\nË™™ÊòéÔºö‰∏ªÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Áî®ÊñºË®àÁÆóÊìãÂúüÁµêÊßãÊâÄÊâøÂèóÁöÑÊ∞¥Âπ≥Êé®ÂäõÔºåÊòØÊìãÂúüÁâÜË®≠Ë®àÁöÑÈáçË¶ÅÂèÉÊï∏„ÄÇ\n```\n\n### Ë¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂÖßÊë©Êì¶ËßíÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Kp„ÄÅË®àÁÆóÂÖ¨Âºè„ÄÅ‰æùÊìöÁ≠â\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂÖßÊë©Êì¶Ëßí30Â∫¶ÁöÑË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏KpÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏ Kp = 3.0\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117Ê¢ùÂèäÈôÑ‰ª∂ÂúüÂ£ìÂäõË®àÁÆóÂÖ¨Âºè\nË™™ÊòéÔºöË¢´ÂãïÂúüÂ£ìÂäõ‰øÇÊï∏Áî®ÊñºË®àÁÆóÊìãÂúüÁµêÊßãÁöÑÊäµÊäóÂäõÔºåÊòØÊìãÂúüÁâÜÁ©©ÂÆöÂàÜÊûêÁöÑÈáçË¶ÅÂèÉÊï∏„ÄÇ\n```\n\n### ÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆóÔºàÂ§öÊñ∑Èù¢ÊîØÊè¥Ôºâ\n- Ëº∏ÂÖ•ÔºöÊñ∑Èù¢ÂûãÂºè„ÄÅÊµÅÈáè„ÄÅÂù°Â∫¶„ÄÅÊõºÂØß‰øÇÊï∏„ÄÅÊ∏†ÈÅìÊùêË≥™„ÄÅÂ∞∫ÂØ∏Á≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÊµÅÈÄü„ÄÅÊµÅÊ∑±„ÄÅÊñ∑Èù¢Á©ç„ÄÅÊ∞¥ÂäõÂçäÂæëÁ≠âË®àÁÆóÁµêÊûúÔºå‰ª•ÂèäÂÆåÊï¥Â†±ÂëäÊõ∏\n\n#### Ë®àÁÆóÈÇèËºØËàáË≠¶ÂëäË™™ÊòéÔºà2024/06‰øÆÊ≠£ÁâàÔºâ\n- Ëã•Ëº∏ÂÖ•ÊµÅÈáèË∂ÖÈÅéË©≤Êñ∑Èù¢ÊªøÊµÅÊôÇÁöÑÊúÄÂ§ßÊµÅÈáèÔºåÁ≥ªÁµ±ÊúÉÁõ¥Êé•ÂõûÂÇ≥ÈåØË™§Ë®äÊÅØÔºåÊèêÁ§∫„ÄåÊâÄÈúÄÊµÅÈáèÂ∑≤Ë∂ÖÈÅéÊªøÊµÅÊúÄÂ§ßÊµÅÈáèÔºåË´ãÂä†Â§ßÂ∞∫ÂØ∏ÊàñÂù°Â∫¶„Äç„ÄÇ\n- Âè™Ë¶ÅÊµÅÈáèÊú™Ë∂ÖÈÅéÊªøÊµÅÔºåË®àÁÆóÊúÉËá™ÂãïÊ±ÇÂæó‰∏ÄÂÄãÂêàÁêÜÁöÑÊµÅÊ∑±ÔºàÂ∞èÊñºÊ∫ùÈ´òÔºâÔºå‰∏çÊúÉË™§Â†±ÊªøÊµÅ„ÄÇ\n- ÊªøÊµÅË≠¶ÂëäÂÉÖÂú®ÊµÅÊ∑±Ê•µÊé•ËøëÈ´òÂ∫¶‰∏îÊµÅÈáèÊ•µÊé•ËøëÊªøÊµÅÊôÇÊâçÊúÉÂá∫Áèæ„ÄÇ\n- Ë®àÁÆóÁ≤æÂ∫¶ËàáÊî∂ÊñÇÊ¢ù‰ª∂Â∑≤ÂÑ™ÂåñÔºåÈÅøÂÖçÂ∑•Á®ãÂ∏∏Áî®ÊÉÖÂ¢É‰∏ãÂá∫ÁèæË™§Âà§„ÄÇ\n\n#### ÊúÄÂ§ß/ÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüË¶èÂâá\n- ÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÔºö\n  - ÂÉÖÊ™¢Ê†∏ÊúÄÂ§ßÂÄºÔºàÂ¶ÇÊ∑∑ÂáùÂúü6.1m/s„ÄÅÈãºÁ≠ãÊ∑∑ÂáùÂúü12.0m/s„ÄÅÁ≤óÁ†ÇÂèäÁ¥∞Á†Ç0.61m/sÁ≠âÔºâÔºåË∂ÖÈÅéÊôÇÁ≥ªÁµ±Â∞áË≠¶Âëä‰∏¶Âª∫Ë≠∞Ë®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n  - Ë°®Ê†º‰∏≠ÁÑ°\"ÈãºÁ≠ãÊ∑∑ÂáùÂúü\"‰πãÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüÔºåÁ≥ªÁµ±Â∑≤Êñ∞Â¢ûÊúÄÂ§ßÂÄº12.0m/s„ÄÇ\n- ÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüÔºö\n  - ÂÉÖÊ∑∑ÂáùÂúüËàáÈãºÁ≠ãÊ∑∑ÂáùÂúüÊéíÊ∞¥Ë®≠ÊñΩÊòéÁ¢∫Ë¶èÂÆöÊúÄÂ∞èÂÆπË®±ÊµÅÈÄüÁÇ∫0.8 m/sÔºå‰ΩéÊñºÊôÇÁ≥ªÁµ±Â∞áË≠¶ÂëäÂèØËÉΩÂ∞éËá¥Ê≥•Á†ÇÊ∑§Á©ç„ÄÇ\n  - ÂÖ∂È§òÊùêË≥™Êú™Ë¶ãÊòéÁ¢∫Ë¶èÂÆöÔºåÁ≥ªÁµ±‰∏çÊ™¢Ê†∏ÊúÄÂ∞èÂÄº„ÄÇ\n- Ê™¢Ê†∏Âª∫Ë≠∞ÔºöË®≠Ë®àÊéíÊ∞¥‰πãÊµÅÈÄüË∂ÖÈÅéÂÖ∂ÊúÄÂ§ßÂÆπË®±ÊµÅÈÄüËÄÖÔºåÊáâÊñºÈÅ©Áï∂‰ΩçÁΩÆË®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n\n#### Âá∫Ê∞¥È´òÔºàÊµÅÊ∑±ÔºâË¶èÁØÑÊ™¢Ê†∏ÔºàÁ¨¨86Ê¢ùÔºâ\n- Áü©ÂΩ¢„ÄÅÊ¢ØÂΩ¢ÊéíÊ∞¥Ê∫ù‰πãÂá∫Ê∞¥È´òÔºàÊµÅÊ∑±ÔºâÊáâÁ¨¶ÂêàÔºö\n  - ‰∏çÂæóÂ∞èÊñºË®≠Ë®àÊ∞¥Ê∑±ÁöÑ25%\n  - ÊúÄÂ∞èÂÄºÁÇ∫20ÂÖ¨ÂàÜ\n  - LÂûã„ÄÅÊããÁâ©Á∑öÂûãÊéíÊ∞¥Ê∫ù‰∏çÂú®Ê≠§Èôê\n- Ëã•Ë®àÁÆóÊµÅÊ∑±‰ΩéÊñº‰∏äËø∞Ê®ôÊ∫ñÔºåÁ≥ªÁµ±Â∞áÊñºÂõûÂÇ≥ÁµêÊûúËàáÂ†±ÂëäÊõ∏‰∏≠Áµ¶‰∫àÊòéÁ¢∫Ë≠¶ÂëäÔºåË´ãË™øÊï¥Ë®≠Ë®àÂèÉÊï∏„ÄÇ\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè1cmsÔºåÂù°Â∫¶5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\nÂõûÂÇ≥Ôºö\nÊµÅÈÄü: 7.20 m/sÔºåÊµÅÊ∑±: 0.250 m„ÄÇË≠¶ÂëäÔºöË®àÁÆóÊµÅÈÄü v = 7.2000 m/s Â∑≤Ë∂ÖÈÅé„ÄéÈãºÁ≠ãÊ∑∑ÂáùÂúü„ÄèÊúÄÂ§ßÂÆπË®±ÊµÅÈÄü 12.0 m/sÔºåÊáâÊñºÈÅ©Áï∂‰ΩçÁΩÆË®≠ÁΩÆÊ∂àËÉΩË®≠ÊñΩ„ÄÇ\n\nËº∏ÂÖ•ÔºöË®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè0.2cmsÔºåÂù°Â∫¶0.5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±?\nÂõûÂÇ≥Ôºö\nÊµÅÈÄü: 0.65 m/sÔºåÊµÅÊ∑±: 0.120 m„ÄÇË≠¶ÂëäÔºöË®àÁÆóÊµÅÈÄü v = 0.6500 m/s ‰ΩéÊñº„ÄéÊ∑∑ÂáùÂúü„ÄèÊúÄÂ∞èÂÆπË®±ÊµÅÈÄü 0.8 m/sÔºåÂèØËÉΩÂ∞éËá¥Ê≥•Á†ÇÊ∑§Á©ç„ÄÇ\n\n„ÄêÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆóÂ†±Âëä„Äë\nÊñ∑Èù¢ÂûãÂºèÔºöÁü©ÂΩ¢\nÁü©ÂΩ¢Â∫ïÂØ¨ b=50.0cm, È´òÂ∫¶ h=70.0cm\nÊµÅÈáè Q = 1.000 cms\nÂù°Â∫¶ S = 5.000%\nÊõºÂØß‰øÇÊï∏ n = 0.013\nÊ∏†ÈÅìÊùêË≥™ÔºöÈãºÁ≠ãÊ∑∑ÂáùÂúü\n\n„ÄêË®àÁÆóÂÖ¨Âºè„Äë\nQ = A √ó V, V = (1/n) √ó R^(2/3) √ó S^(1/2)\n„ÄêË®àÁÆóÊ≠•È©ü„Äë\nA = b√óy, P = b+2y, R = A/P, V = (1/n)R^(2/3)S^(1/2), Q = A√óV\n\n„ÄêË®àÁÆóÁµêÊûú„Äë\nÊµÅÈÄü V = 2.340 m/s\nÊµÅÊ∑± y = 0.210 m\nÊñ∑Èù¢Á©ç A = 0.1050 m¬≤\nÊ∞¥ÂäõÂçäÂæë R = 0.0700 m\nÂë®Èï∑ P = 1.0000 m\n\nË®àÁÆóÁµêÊûúÁ¨¶ÂêàÂÆâÂÖ®ÊµÅÈÄüË¶èÁØÑ„ÄÇ\n„ÄêÊ™¢Ê†∏Ë≠¶Âëä„ÄëË®àÁÆóÊµÅÊ∑± y = 0.210 m ‰∏çÁ¨¶Á¨¨86Ê¢ùË¶èÁØÑÔºåÊáâ‚âßmax(0.2m, Ë®≠Ë®àÊ∞¥Ê∑±25%)=0.250 m„ÄÇË´ãË™øÊï¥Ë®≠Ë®à„ÄÇ\n```\n\n### ÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏Ë®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂñÆ‰ΩçÈáç„ÄÅÊë©Êì¶Ëßí„ÄÅÂáùËÅöÂäõ„ÄÅÂú∞‰∏ãÊ∞¥‰Ωç„ÄÅÊñπÊ≥ïÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂÆâÂÖ®‰øÇÊï∏„ÄÅÊñπÊ≥ï„ÄÅÊòØÂê¶ÂêàÊ†º„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãË®àÁÆóÂù°Â∫¶30Â∫¶„ÄÅÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPaÁöÑÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏\nÂõûÂÇ≥ÔºöÂÆâÂÖ®‰øÇÊï∏ = 1.50ÔºåÊñπÊ≥ïÔºöÁ∞°ÂåñÊ≥ïÔºåÂêàÊ†ºÔºöTrue\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨31Ê¢ùÂèäÈôÑ‰ª∂ÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÇäÂù°Á©©ÂÆöÂÆâÂÖ®‰øÇÊï∏ÊáâÂ§ßÊñº1.5ÔºåÊú¨Ê°à‰æãÁ¨¶ÂêàË¶èÁØÑË¶ÅÊ±Ç„ÄÇ\n```\n\n### ÂúüÂ£§‰æµËùïÊ®°Êï∏/ÊµÅÂ§±ÈáèË®àÁÆó\n- Ëº∏ÂÖ•ÔºöÂù°Èï∑„ÄÅÂù°Â∫¶„ÄÅÈôçÈõ®„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÂúüÂú∞Âà©Áî®„ÄÅÊ∞¥‰øùÊé™ÊñΩÁ≠âË≥áË®ä\n- ÂõûÂÇ≥Ôºö‰æµËùïÊ®°Êï∏„ÄÅÊµÅÂ§±Èáè„ÄÅÊñπÊ≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆóÂù°Èï∑100ÂÖ¨Â∞∫„ÄÅÂù°Â∫¶10%„ÄÅÂπ¥ÈôçÈõ®Èáè1200mm„ÄÅÁ†ÇÂúü„ÄÅËçâÂú∞„ÄÅÁ≠âÈ´òËÄï‰ΩúÁöÑÂúüÂ£§ÊµÅÂ§±Èáè\nÂõûÂÇ≥ÔºöÂúüÂ£§‰æµËùïÊ®°Êï∏/ÊµÅÂ§±Èáè = 42.75 ÂÖ¨Âô∏/ÂÖ¨È†É/Âπ¥\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÂèäÈôÑ‰ª∂USLEÂÖ¨Âºè„ÄÅÂèÉÊï∏Ë°®\nË™™ÊòéÔºöUSLEÂÖ¨ÂºèÁÇ∫ÂúãÈöõÈÄöÁî®ÂúüÂ£§ÊµÅÂ§±ÈáèÊé®‰º∞ÊñπÊ≥ïÔºåÂè∞ÁÅ£Ê∞¥‰øùË¶èÁØÑÊòéÂÆöÂèØÁî®ÊñºÂù°Âú∞ÂúüÂ£§‰æµËùïË©ï‰º∞„ÄÇ\n```\n\n### ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆó\n- Ëº∏ÂÖ•ÔºöÈõÜÊ∞¥ÂçÄÈù¢Á©ç„ÄÅÈôçÈõ®Âº∑Â∫¶„ÄÅÈÄïÊµÅ‰øÇÊï∏Á≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÊúÄÂ§ßÈÄïÊµÅÈáè„ÄÅÊñπÊ≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË®àÁÆó‰∏ÄÂÄãÈù¢Á©ç2ÂÖ¨È†É„ÄÅÈôçÈõ®Âº∑Â∫¶100mm/hr„ÄÅÈÄïÊµÅ‰øÇÊï∏0.6ÁöÑÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè\nÂõûÂÇ≥ÔºöÊúÄÂ§ßÈÄïÊµÅÈáè Q = 0.33 cmsÔºåÊñπÊ≥ïÔºöRational\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨16„ÄÅ17„ÄÅ18Ê¢ùÂèäÈôÑ‰ª∂ÈôçÈõ®Âº∑Â∫¶„ÄÅÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈõÜÊ∞¥ÂçÄÈù¢Á©çÂ∞èÊñº1000ÂÖ¨È†ÉÊôÇÔºåÁÑ°ÂØ¶Ê∏¨Ë≥áÊñôÂèØÊé°RationalÂÖ¨ÂºèË®àÁÆóÊ¥™Â≥∞ÊµÅÈáè„ÄÇ\n```\n\n### Ë≠∑Â≤∏/ÊìãÂúüÁâÜÁ©©ÂÆöÊ™¢Ê†∏\n- Ëº∏ÂÖ•ÔºöÁâÜÈ´ò„ÄÅÂéöÂ∫¶ÔºàÁü©ÂΩ¢Êñ∑Èù¢ÔºâÊàñÈ†ÇÂØ¨/Â∫ïÂØ¨ÔºàÊ¢ØÂΩ¢Êñ∑Èù¢Ôºâ„ÄÅÂñÆ‰ΩçÈáç„ÄÅÊë©Êì¶Ëßí„ÄÅÂáùËÅöÂäõ„ÄÅËÉåÂ°´Âù°Â∫¶„ÄÅÂú∞‰∏ãÊ∞¥‰Ωç„ÄÅÂú∞Èúá‰øÇÊï∏„ÄÅÂü∫Á§éÈ°ûÂûãÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂ∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏„ÄÅÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆ„ÄÅÊòØÂê¶ÂêàÊ†º„ÄÅË™™Êòé\n\n#### ÁØÑ‰æãÔºàÁü©ÂΩ¢Êñ∑Èù¢Ôºâ\n```\nËº∏ÂÖ•ÔºöÊ™¢Ê†∏‰∏ÄÂ∫ßÈ´ò2ÂÖ¨Â∞∫„ÄÅÂéö1ÂÖ¨Â∞∫ÁöÑÊìãÂúüÁâÜÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPa„ÄÅËÉåÂ°´Âù°Â∫¶10Â∫¶ÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§é\nÂõûÂÇ≥Ôºö\n„ÄêÂ∏∏ÊôÇÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.65Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=2.30Ôºà‚âß2.0 ÂêàÊ†ºÔºâÔºåÊâøËºâSF=3.10Ôºà‚âß3.0 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n„ÄêÂú∞ÈúáÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.25Ôºà‚âß1.2 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=1.60Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117„ÄÅ118„ÄÅ120„ÄÅ164Ê¢ùÂèäÈôÑ‰ª∂ÊúÄÂ∞èÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÊìãÂúüÁâÜË®≠Ë®àÈ†àÂêåÊôÇÊ™¢Ê†∏Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏ÂèäÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÔºå‰∏¶Á¨¶ÂêàË¶èÁØÑÂª∫Ë≠∞Ê®ôÊ∫ñ„ÄÇ\n```\n\n#### ÁØÑ‰æãÔºàÊ¢ØÂΩ¢Êñ∑Èù¢Ôºâ\n```\nËº∏ÂÖ•ÔºöÊ™¢Ê†∏‰∏ÄÂ∫ßÈ´ò2ÂÖ¨Â∞∫„ÄÅÈ†ÇÂØ¨0.5ÂÖ¨Â∞∫„ÄÅÂ∫ïÂØ¨1ÂÖ¨Â∞∫ÁöÑÊ¢ØÂΩ¢ÊìãÂúüÁâÜÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m¬≥„ÄÅÊë©Êì¶Ëßí30Â∫¶„ÄÅÂáùËÅöÂäõ10kPaÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§é\nÂõûÂÇ≥Ôºö\n„ÄêÁâÜÈ´îÂπæ‰ΩïËàáÈáçÈáè„Äë\nÊ¢ØÂΩ¢Êñ∑Èù¢ÔºöÈ´òÂ∫¶ = 2.00 mÔºåÈ†ÇÂØ¨ = 0.50 mÔºåÂ∫ïÂØ¨ = 1.00 mÔºåÁâÜÈ´îÂñÆ‰ΩçÈáç = 24.00 kN/m¬≥\nÁâÜÈ´îÈáçÈáè W = 24.00 √ó (1.00 + 0.50) √ó 2.00 / 2 = 36.00 kN/m\nÈáçÂøÉ‰ΩçÁΩÆ x_g = 0.56 m\n\n„ÄêÂ∏∏ÊôÇÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.78Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=2.45Ôºà‚âß2.0 ÂêàÊ†ºÔºâÔºåÊâøËºâSF=3.25Ôºà‚âß3.0 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n„ÄêÂú∞ÈúáÊÉÖÊ≥Å„Äë\nÊªëÂãïSF=1.32Ôºà‚âß1.2 ÂêàÊ†ºÔºâÔºåÂÇæÂÄíSF=1.75Ôºà‚âß1.5 ÂêàÊ†ºÔºâÔºåÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÂêàÊ†º\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨117„ÄÅ118„ÄÅ120„ÄÅ164Ê¢ùÂèäÈôÑ‰ª∂ÊúÄÂ∞èÂÆâÂÖ®‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÊìãÂúüÁâÜË®≠Ë®àÈ†àÂêåÊôÇÊ™¢Ê†∏Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÊªëÂãï„ÄÅÂÇæÂÄíÂÆâÂÖ®‰øÇÊï∏ÂèäÂêàÂäõ‰ΩúÁî®Èªû‰ΩçÁΩÆÔºå‰∏¶Á¨¶ÂêàË¶èÁØÑÂª∫Ë≠∞Ê®ôÊ∫ñ„ÄÇ\n```\n\n### Ê§çÁîüË≠∑Âù°Ë®≠Ë®àÂª∫Ë≠∞\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÊ∞£ÂÄôÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ï„ÄÅËçâÁ®Æ„ÄÅË¶ÜËìãÁéá„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÈáùÂ∞çÂù°Â∫¶30Â∫¶„ÄÅÂ£§Âúü„ÄÅ‰∫ûÁÜ±Â∏∂Ê∞£ÂÄôÁöÑÈÇäÂù°Êèê‰æõÊ§çÁîüË≠∑Âù°Ë®≠Ë®àÂª∫Ë≠∞\nÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ïÔºöÂô¥Êí≠ËçâÁöÆ+Ê†ºÊ°ÜÊàñÂúüÂ∑•Á∂≤ÔºåÂàÜÂçÄÂàÜÊúüÊñΩÂ∑•\nËçâÁ®ÆÔºöÁôæÊÖïÈÅîËçâ„ÄÅÁãóÁâôÊ†π„ÄÅÂú∞ÊØØËçâÁ≠âËÄêÊó±ËÄêÊ≤ñËùïËçâÁ®Æ\nË¶ÜËìãÁéáÔºö90.0%\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨8„ÄÅ167„ÄÅ172Ê¢ùÂèäÈôÑ‰ª∂Âù°Â∫¶ÂàÜÁ¥ö„ÄÅÊ®£ÂçÄÈù¢Á©ç„ÄÅË¶ÜËìãÁéáÁ≠âË¶èÂÆö\n```\n\n### Â∏∏Áî®ÊùêÊñôË®≠Ë®àÂèÉÊï∏Êü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÊùêÊñôÂêçÁ®±\n- ÂõûÂÇ≥ÔºöÂñÆ‰ΩçÈáç„ÄÅÂáùËÅöÂäõ„ÄÅÊë©Êì¶Ëßí„ÄÅÂº∑Â∫¶„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊèê‰æõ‰∏ÄËà¨ÈªèÂúüÁöÑË®≠Ë®àÂèÉÊï∏\nÂõûÂÇ≥ÔºöÊùêÊñôÔºö‰∏ÄËà¨ÈªèÂúüÔºåÂñÆ‰ΩçÈáçÔºö18.0kN/m¬≥ÔºåÂáùËÅöÂäõÔºö20.0kPaÔºåÊë©Êì¶ËßíÔºö25.0¬∞ÔºåÂº∑Â∫¶Ôºö200.0kPa\n‰æùÊìöÔºöÂ∏∏Áî®ÂúüÂ£§Â∑•Á®ãÊâãÂÜä\n```\n\n### Âù°Èù¢‰øùË≠∑Â∑•Ê≥ïÂª∫Ë≠∞\n- Ëº∏ÂÖ•ÔºöÂù°Â∫¶„ÄÅÂúüÂ£§È°ûÂûã„ÄÅÈôçÈõ®ÈáèÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ï„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÂù°Â∫¶30%„ÄÅÁ†ÇÂúü„ÄÅÂπ¥ÈôçÈõ®Èáè1200mmÁöÑÂù°Èù¢ÔºåÂª∫Ë≠∞Êé°Áî®‰ªÄÈ∫º‰øùË≠∑Â∑•Ê≥ïÔºü\nÂõûÂÇ≥ÔºöÂª∫Ë≠∞Â∑•Ê≥ïÔºöÂô¥Êí≠ËçâÁöÆ+Ê†ºÊ°ÜÊàñÂúüÂ∑•Á∂≤\nË™™ÊòéÔºöÂù°Â∫¶ËºÉÂ§ßÊôÇÊáâÂä†Âº∑ÊéíÊ∞¥Ë®≠ÊñΩËàáÂù°Èù¢Á©©ÂÆöÊé™ÊñΩ\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨8„ÄÅ167„ÄÅ172Ê¢ùÂèäÈôÑ‰ª∂Âù°Â∫¶ÂàÜÁ¥öË°®\n```\n\n### Êª≤Ê∞¥Ë®≠ÊñΩË®≠Ë®à\n- Ëº∏ÂÖ•ÔºöË®≠ÊñΩÂûãÂºè„ÄÅÂúüÂ£§Êª≤ÈÄè‰øÇÊï∏„ÄÅÈõÜÊ∞¥Èù¢Á©ç„ÄÅÈôçÈõ®ÈáèÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöË®≠Ë®àÊµÅÈáè„ÄÅÂª∫Ë≠∞Â∞∫ÂØ∏„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãË®≠Ë®à‰∏ÄÂÄãÊª≤Ê∞¥‰∫ïÔºåÂúüÂ£§Êª≤ÈÄè‰øÇÊï∏k=0.001cm/sÔºåÈõÜÊ∞¥Èù¢Á©ç100Âπ≥ÊñπÂÖ¨Â∞∫ÔºåÂπ¥ÈôçÈõ®Èáè1200mm\nÂõûÂÇ≥ÔºöË®≠ÊñΩÂûãÂºèÔºöÊª≤Ê∞¥‰∫ïÔºåË®≠Ë®àÊµÅÈáèÔºö0.033cmsÔºåÂª∫Ë≠∞Â∞∫ÂØ∏ÔºöÁõ¥Âæë1.2mÔºåÊ∑±Â∫¶1.8m\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨94Ê¢ùÂèäÈôÑ‰ª∂Êª≤ÈÄèË®≠ÊñΩË®≠Ë®àÊ∫ñÂâá\nË™™ÊòéÔºöÊª≤Ê∞¥‰∫ïË®≠Ë®àÈúÄËÄÉÊÖÆÂúüÂ£§Êª≤ÈÄèÊÄß„ÄÅÈõÜÊ∞¥Èù¢Á©çÂèäÁï∂Âú∞ÈôçÈõ®ÁâπÊÄßÔºå‰∏¶ÂÆöÊúüÁ∂≠Ë≠∑‰ª•Á¢∫‰øùÂäüËÉΩ„ÄÇ\n```\n\n### IDFÊõ≤Á∑öÊü•Ë©¢\n- Ëº∏ÂÖ•ÔºöÂú∞Èªû„ÄÅÈáçÁèæÊúü„ÄÅÊ≠∑ÊôÇÁ≠âË≥áË®ä\n- ÂõûÂÇ≥ÔºöÈôçÈõ®Âº∑Â∫¶„ÄÅË™™Êòé\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Âè∞‰∏≠Â∏Ç10Âπ¥ÈáçÁèæÊúü„ÄÅ60ÂàÜÈêòÊ≠∑ÊôÇÁöÑÈôçÈõ®Âº∑Â∫¶\nÂõûÂÇ≥ÔºöÂú∞ÈªûÔºöÂè∞‰∏≠Â∏ÇÔºåÈáçÁèæÊúüÔºö10Âπ¥ÔºåÊ≠∑ÊôÇÔºö60ÂàÜÈêòÔºåÂº∑Â∫¶Ôºö78.5 mm/hr\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨15Ê¢ùÂèäÈôÑ‰ª∂IDFÊõ≤Á∑öÂÖ¨Âºè\nË™™ÊòéÔºöIDFÊõ≤Á∑öÂèçÊò†‰∏çÂêåÈáçÁèæÊúüËàáÊ≠∑ÊôÇ‰∏ãÁöÑÈôçÈõ®Âº∑Â∫¶ÔºåÁî®ÊñºÊ∞¥ÊñáÂàÜÊûêËàáÊ∞¥Âà©Ë®≠ÊñΩË®≠Ë®à„ÄÇ\n```\n\n### Êü•Ë©¢ÊîØÊè¥Ê∏ÖÂñÆ\n- ‰Ω†ÂèØ‰ª•Áõ¥Êé•Êü•Ë©¢ÊúâÂì™‰∫õÂèØÁî®ÁöÑÊùêÊñô„ÄÅÂú∞ÂçÄ„ÄÅÂ∑•Ê≥ïÁ≠âÔºö\n\n```\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õÂ∏∏Áî®ÊùêÊñôÔºü\nÂõûÂÇ≥Ôºö['‰∏ÄËà¨ÈªèÂúü', 'Á†ÇÂúü', 'Á§´Áü≥', 'Ê∑∑ÂáùÂúü', ...]\n\nÊü•Ë©¢ÔºöÂèØ‰ª•Êü•Ë©¢Âì™‰∫õÊ∞¥Ê∫ùÈã™Èù¢ÁöÑÊõºÂØß‰øÇÊï∏Ôºü\nÂõûÂÇ≥Ôºö['Á¥îÁ¥∞Á†Ç', 'Ê∑∑ÂáùÂúü', 'ÂÖ®Èù¢ÂØÜËçâÁîü', ...]\n\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õÂù°Èù¢‰øùË≠∑Â∑•Ê≥ïÔºü\nÂõûÂÇ≥Ôºö['ËçâÁöÆÊàñÁõ¥Êé•Êí≠Á®Æ', 'Âô¥Êí≠ËçâÁöÆ+Ê†ºÊ°Ü/ÂúüÂ∑•Á∂≤', ...]\n\nÊü•Ë©¢ÔºöÊúâÂì™‰∫õIDFÂú∞ÈªûÔºü\nÂõûÂÇ≥Ôºö['Âè∞ÂåóÂ∏Ç', 'Êñ∞ÂåóÂ∏Ç', 'Âè∞‰∏≠Â∏Ç']\n```\n\n### Êü•Ë©¢Â§±ÊïóÊôÇËá™ÂãïÊèêÁ§∫\n- Ëã•Êü•Ë©¢ÊôÇËº∏ÂÖ•ÈåØË™§ÔºåÊúÉËá™ÂãïÂõûÂÇ≥ÊâÄÊúâÊîØÊè¥Êü•Ë©¢ÁöÑÈ†ÖÁõÆÔºö\n\n```\nËº∏ÂÖ•ÔºöÊü•Ë©¢‰∏çÂ≠òÂú®ÁöÑÊùêÊñôÂêçÁ®±\nÂõûÂÇ≥ÔºöÊü•ÁÑ°Ê≠§ÊùêÊñôÔºåÊîØÊè¥Êü•Ë©¢ÁöÑÊùêÊñôÊúâÔºö‰∏ÄËà¨ÈªèÂúü, Á†ÇÂúü, Á§´Áü≥, Ê∑∑ÂáùÂúü, ...\n```\n## ÂúüÁü≥Á±†ÊìãÂúüÁâÜÁ©©ÂÆöÂàÜÊûê\n\nÊ≠§ÂäüËÉΩÁî®ÊñºÂàÜÊûêÂúüÁü≥Á±†ÊìãÂúüÁâÜÁöÑÁ©©ÂÆöÊÄßÔºåÂåÖÊã¨‰∏ªÂãïÂíåË¢´ÂãïÂúüÂ£ìÂäõË®àÁÆó„ÄÇ\n\n### ‰ΩøÁî®ÊñπÊ≥ï\n\n```python\nresult = check_gabion_stability(\n    height=3.0,          # ÂúüÁü≥Á±†È´òÂ∫¶ (m)\n    width=2.0,           # ÂúüÁü≥Á±†ÂØ¨Â∫¶ (m)\n    wall_weight=100,     # ÊìãÂúüÁâÜÁ∏ΩÈáç (kN/m)\n    phi=30,              # ÂúüÂ£§ÂÖßÊë©Êì¶Ëßí (¬∞)\n    delta=20,            # ÁâÜËÉåÊë©Êì¶Ëßí (¬∞)ÔºåÈ†êË®≠ 0\n    theta=0,             # ÁâÜËÉåÂÇæÊñúËßí (¬∞)ÔºåÈ†êË®≠ 0\n    i=0,                 # Âú∞Ë°®ÂÇæÊñúËßí (¬∞)ÔºåÈ†êË®≠ 0\n    gamma=18,            # ÂúüÂ£§È£ΩÂíåÂñÆ‰ΩçÈáç (kN/m¬≥)ÔºåÈ†êË®≠ 18\n    friction_coef=0.5,   # Êë©Êì¶‰øÇÊï∏ÔºåÈ†êË®≠ 0.5\n    pressure_mode=\"active\"  # ÂúüÂ£ìÂäõÊ®°Âºè (\"active\" Êàñ \"passive\")ÔºåÈ†êË®≠ \"active\"\n)\n```\n\n### ÂõûÂÇ≥ÁµêÊûú\n\nÂáΩÊï∏ÂõûÂÇ≥‰∏ÄÂÄãÂ≠óÂÖ∏ÔºåÂåÖÂê´‰ª•‰∏ãÂÖßÂÆπÔºö\n\n- `success`: Â∏ÉÊûóÂÄºÔºåË°®Á§∫Ë®àÁÆóÊòØÂê¶ÊàêÂäü\n- `data`: Ë®àÁÆóÁµêÊûúÊï∏ÊìöÔºåÂåÖÂê´Ôºö\n  - `earth_pressure_coef`: ÂúüÂ£ìÂäõ‰øÇÊï∏\n  - `total_pressure`: Á∏ΩÂúüÂ£ìÂäõ (kN/m)\n  - `vertical_force`: ÂûÇÁõ¥ÂäõÂàÜÈáè (kN/m)\n  - `horizontal_force`: Ê∞¥Âπ≥ÂäõÂàÜÈáè (kN/m)\n  - `restoring_moment`: ÊäóÂÇæË¶ÜÂäõÁü© (kN¬∑m/m)\n  - `overturning_moment`: ÂÇæË¶ÜÂäõÁü© (kN¬∑m/m)\n  - `overturning_safety_factor`: ÊäóÂÇæË¶ÜÂÆâÂÖ®‰øÇÊï∏\n  - `sliding_safety_factor`: ÊäóÊªëÂãïÂÆâÂÖ®‰øÇÊï∏\n- `message`: Ë®àÁÆóÁµêÊûúÊëòË¶Å\n- `report`: ÂÆåÊï¥ÁöÑË®àÁÆóÂ†±ÂëäÊõ∏ÔºàMarkdown Ê†ºÂºèÔºâ\n\n### Ë®àÁÆóÂ†±ÂëäÊõ∏ÂÖßÂÆπ\n\nÂ†±ÂëäÊõ∏ÂåÖÂê´‰ª•‰∏ãÁ´†ÁØÄÔºö\n\n1. Ëº∏ÂÖ•ÂèÉÊï∏\n2. Ë®àÁÆóÂÖ¨Âºè\n3. Ë®àÁÆóÁµêÊûú\n4. Á©©ÂÆöË©ï‰º∞\n\n### UÂûãÊ∫ùÈãºÁ≠ãÈáèË®àÁÆó\n- Ëº∏ÂÖ•Ôºö\n  - height: Ê∫ùÈ´ò (m)\n  - wall_slope: Ê∫ùÂ£ÅÂÇæËßí (m)\n  - soil_slope: ÂúüÊñπÂÇæËßí (¬∞)\n  - soil_angle: ÂÆâÊÅØËßí (¬∞)\n  - effective_depth: ÊúâÊïàÂéöÂ∫¶ (m)\n  - soil_weight: ÂúüÈáç (kN/m¬≥)ÔºåÈ†êË®≠ 18.0\n- ÂõûÂÇ≥ÔºödictÔºåÂê´Ë®àÁÆóÁµêÊûúËàáÂ†±ÂëäÊõ∏\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöÊ∑∑ÂáùÂúüÊ∫ùÔºåheight=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2ÔºåË®àÁÆóÊ∞¥Ê∫ùÊâÄÈúÄÁöÑÈãºÁ≠ãÈáèÔºåÂª∫Ë≠∞ÁöÑÈÖçÁ≠ã?\nÂõûÂÇ≥Ôºö\n{\n  \"success\": true,\n  \"data\": {\n    \"earth_pressure_coef\": 0.3333,\n    \"earth_pressure\": 6.750,\n    \"moment\": 3.375,\n    \"rebar_area\": 13.780\n  },\n  \"message\": \"ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333, ÂúüÂ£ìÂäõ P = 6.750 kN/m, ÂΩéÁü© M = 3.375 kN¬∑m/m, ÈãºÁ≠ãÈáè As = 13.780 cm¬≤/m\",\n  \"report\": \"„ÄêUÂûãÊ∫ùÈãºÁ≠ãÈáèË®àÁÆóÂ†±Âëä„Äë\\n\\nËº∏ÂÖ•ÂèÉÊï∏Ôºö\\n- Ê∫ùÈ´ò H = 1.500 m\\n- Ê∫ùÂ£ÅÂÇæËßí m = 0.500\\n- ÂúüÊñπÂÇæËßí i = 15.00¬∞\\n- ÂÆâÊÅØËßí œà = 30.00¬∞\\n- ÊúâÊïàÂéöÂ∫¶ d = 0.200 m\\n- ÂúüÈáç Œ≥ = 18.0 kN/m¬≥\\n\\nË®àÁÆóÂÖ¨ÂºèÔºö\\n1. ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = cos¬≤(œà+m) / [cos¬≤m¬∑(1+‚àöQ)¬≤]\\n   ÂÖ∂‰∏≠ Q = [sinœà¬∑sin(œà-i)] / [cos(m+i)¬∑cosm]\\n2. ÂúüÂ£ìÂäõ P = Œ≥¬∑H¬≤¬∑Ka / (2¬∑cosm)\\n3. ÂΩéÁü© M = Œ≥¬∑H¬≥¬∑Ka / (6¬∑cosm)\\n4. ÈãºÁ≠ãÈáè As = M / (fs¬∑d) √ó 10‚Å∂ / 1000\\n\\nË®àÁÆóÁµêÊûúÔºö\\n- ÂúüÂ£ìÂäõ‰øÇÊï∏ Ka = 0.3333\\n- ÂúüÂ£ìÂäõ P = 6.750 kN/m\\n- ÂΩéÁü© M = 3.375 kN¬∑m/m\\n- ÈãºÁ≠ãÈáè As = 13.780 cm¬≤/m\"\n}\n```\n## ÈãºÁ≠ãÊü•Ë©¢ÂäüËÉΩ\nÊú¨Â∑•ÂÖ∑Êèê‰æõ‰ª•‰∏ãÈãºÁ≠ãË≥áÊñôÊü•Ë©¢ÂäüËÉΩÔºö\n\n1. ÂàóÂá∫ÊâÄÊúâÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®Ëôü\n   ```python\n   list_rebar_numbers()\n   ```\n\n2. Êü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁ∑®ËôüÁöÑË¶èÊ†ºË≥áÊñô\n   ```python\n   get_rebar_specs(rebar_number=\"#3\")\n   ```\n\n3. Ë®àÁÆóÈãºÁ≠ãÈáçÈáè\n   ```python\n   calculate_rebar_weight(rebar_number=\"#3\", length=10.0)\n   ```\n\nÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®ËôüÂåÖÊã¨Ôºö#3„ÄÅ#4„ÄÅ#5„ÄÅ#6„ÄÅ#7„ÄÅ#8„ÄÅ#9„ÄÅ#10„ÄÅ#11ÔºåÊØèÂÄãÁ∑®ËôüÂ∞çÊáâÁöÑË¶èÊ†ºË≥áÊñôÂåÖÊã¨Ôºö\n- Áõ¥ÂæëÔºàmmÔºâ\n- Êà™Èù¢Á©çÔºàcm¬≤Ôºâ\n- ÂñÆ‰ΩçÈáçÈáèÔºàkg/mÔºâ\n- Âë®Èï∑ÔºàmmÔºâ\n\n### ÈãºÁ≠ãË¶èÊ†ºÊü•Ë©¢\nËº∏ÂÖ•„ÄåÈãºÁ≠ãË¶èÊ†º #3„ÄçÊàñ„ÄåÈãºÁ≠ãË≥áÊñô #3„ÄçÂèØÊü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁöÑË©≥Á¥∞Ë≥áÊñôÔºåÂåÖÊã¨Ôºö\n- ÈãºÁ≠ãÁ∑®Ëôü\n- Áõ¥Âæë (mm)\n- Êà™Èù¢Á©ç (cm¬≤)\n- ÂñÆ‰ΩçÈáçÈáè (kg/m)\n- Âë®Èï∑ (mm)\n\n### ÈãºÁ≠ãÈáçÈáèË®àÁÆó\nËº∏ÂÖ•„ÄåÈãºÁ≠ãÈáçÈáè Èï∑Â∫¶ 6 #3„ÄçÂèØË®àÁÆóÊåáÂÆöÈï∑Â∫¶ÁöÑÈãºÁ≠ãÈáçÈáèÔºå‰æãÂ¶ÇÔºö\n- Èï∑Â∫¶Ôºö6m\n- ÈãºÁ≠ãÔºö#3\n- ÈáçÈáèÔºö2.04 kg\n\n### ÈãºÁ≠ãÊà™Èù¢Á©çÊü•Ë©¢\nËº∏ÂÖ•„ÄåÈãºÁ≠ãÊà™Èù¢Á©ç #3„ÄçÂèØÊü•Ë©¢ÁâπÂÆöÈãºÁ≠ãÁöÑÊà™Èù¢Á©çÔºå‰æãÂ¶ÇÔºö\n- #3 ÈãºÁ≠ãÊà™Èù¢Á©çÔºö0.71 cm¬≤\n\n### ÊâÄÊúâÈãºÁ≠ãÂàóË°®\nËº∏ÂÖ•„ÄåÊâÄÊúâÈãºÁ≠ã„ÄçÂèØÂàóÂá∫ÊâÄÊúâÂèØÁî®ÁöÑÈãºÁ≠ãÁ∑®ËôüÔºåÂåÖÊã¨Ôºö\n- #3 Ëá≥ #11 ÈãºÁ≠ã\n- ÂêÑÈãºÁ≠ãÁöÑÂü∫Êú¨Ë¶èÊ†º\n\n### U ÂûãÊ∫ùÈÖçÁ≠ãÂª∫Ë≠∞\nËº∏ÂÖ•„ÄåUÂûãÊ∫ùÈÖçÁ≠ã Èù¢Á©ç 10cm2„ÄçÂèØÊü•Ë©¢Âª∫Ë≠∞ÁöÑÈÖçÁ≠ãÊñπÂºèÔºåÁ≥ªÁµ±ÊúÉÔºö\n1. Ê†πÊìöËº∏ÂÖ•ÁöÑÈãºÁ≠ãÊñ∑Èù¢Á©ç (cm¬≤/m)\n2. Ëá™ÂãïË®àÁÆó‰∏¶Âª∫Ë≠∞‰∏ªÁ≠ãËàáÂâØÁ≠ãÁöÑÈÖçÁ≠ãÊñπÂºè\n3. ÈñìË∑ùÊúÉÂèñÊï¥Âà∞ÊúÄÊé•ËøëÁöÑ 5cm\n4. Êèê‰æõÂÆåÊï¥ÁöÑÈÖçÁ≠ãÂª∫Ë≠∞Â†±Âëä\n\n### ÈÄïÊµÅ‰øÇÊï∏Êü•Ë©¢\nÈÄïÊµÅ‰øÇÊï∏Êü•Ë©¢Â∑•ÂÖ∑ÂèØ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÔºåÊü•Ë©¢‰∏çÂêåÂúüÂú∞Âà©Áî®È°ûÂûãÊàñÈõÜÊ∞¥ÂçÄÁãÄÊ≥ÅÁöÑÈÄïÊµÅ‰øÇÊï∏CÂÄº„ÄÇ\n\n#### ÁØÑ‰æã\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ëæ≤Ê•≠ÂçÄÁöÑÈÄïÊµÅ‰øÇÊï∏\nÂõûÂÇ≥ÔºöÈÄïÊµÅ‰øÇÊï∏ C = 0.53\n‰æÜÊ∫êÔºöËæ≤Ê•≠ÂçÄÈÄïÊµÅ‰øÇÊï∏ÁØÑÂúçÔºö0.45~0.60ÔºåÊé°Áî®Âπ≥ÂùáÂÄº0.53\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÄïÊµÅ‰øÇÊï∏CÂÄºÂèçÊò†ÈõÜÊ∞¥ÂçÄÂú∞Ë°®ÁâπÊÄßÂ∞çÈôçÈõ®ÈÄïÊµÅ‰πãÂΩ±ÈüøÔºåÈñãÁôº‰∏≠ÁãÄÊÖãCÂÄº‰ª•1.0Ë®àÁÆó„ÄÇ\n\nËº∏ÂÖ•ÔºöÈñãÁôº‰∏≠ÁöÑÂπ≥Âù¶ËÄïÂú∞ÈÄïÊµÅ‰øÇÊï∏ÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÈÄïÊµÅ‰øÇÊï∏ C = 0.85\n‰æÜÊ∫êÔºöÈñãÁôº‰∏≠ÁãÄÊÖãÔºåÂπ≥Âù¶ËÄïÂú∞\n‰æùÊìöÔºöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨18Ê¢ùÂèäÈôÑ‰ª∂ÈÄïÊµÅ‰øÇÊï∏Ë°®\nË™™ÊòéÔºöÈÄïÊµÅ‰øÇÊï∏CÂÄºÂèçÊò†ÈõÜÊ∞¥ÂçÄÂú∞Ë°®ÁâπÊÄßÂ∞çÈôçÈõ®ÈÄïÊµÅ‰πãÂΩ±ÈüøÔºåÈñãÁôº‰∏≠ÁãÄÊÖãCÂÄºËºÉÈ´ò„ÄÇ\n```\n\nÊü•Ë©¢ÁµêÊûúÂåÖÂê´Ôºö\n- ÈÄïÊµÅ‰øÇÊï∏ÂÄº\n- ‰æÜÊ∫êË™™Êòé\n- Ë¶èÁØÑ‰æùÊìö\n- ÂúüÂú∞Âà©Áî®È°ûÂûã\n- ÈñãÁôºÁãÄÊÖã\n- Êï∏ÂÄºÁØÑÂúçÔºàÂ¶ÇÈÅ©Áî®Ôºâ\n\n### USLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑\nUSLE Âõ†Â≠êÊü•Ë©¢Â∑•ÂÖ∑ÂèØ‰æùÊìöÊ∞¥Âúü‰øùÊåÅÊäÄË°ìË¶èÁØÑÁ¨¨35Ê¢ùÔºåÊü•Ë©¢ÂúüÂ£§ÊµÅÂ§±ÈáèË®àÁÆóÊâÄÈúÄÁöÑÂêÑÈ†ÖÂõ†Â≠êÂÄº„ÄÇ\n\n#### ÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Âè∞ÂåóÂ∏ÇÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏RÂÄº\nÂõûÂÇ≥ÔºöÂú∞ÂçÄ Âè∞ÂåóÂ∏Ç ÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R = 350ÔºåÂñÆ‰ΩçÔºöÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê/ÂÖ¨È†É¬∑Â∞èÊôÇ¬∑Âπ¥\n\nËº∏ÂÖ•ÔºöÂπ¥Âπ≥ÂùáÈôçÈõ®Èáè2000mmÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏RÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂπ¥Âπ≥ÂùáÈôçÈõ®Èáè 2000 mm Êé®‰º∞ÁöÑÈôçÈõ®Ê≤ñËùïÊåáÊï∏ R = 600ÔºåÂñÆ‰ΩçÔºöÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê/ÂÖ¨È†É¬∑Â∞èÊôÇ¬∑Âπ¥\n```\n\n#### ÂúüÂ£§Ê≤ñËùïÊåáÊï∏ K ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöÁ†ÇÂúüÁöÑÊ≤ñËùïÊåáÊï∏KÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂúüÂ£§È°ûÂûã Á†ÇÂúü ÁöÑÊ≤ñËùïÊåáÊï∏ K = 0.30ÔºåÂñÆ‰ΩçÔºöÂÖ¨Âô∏¬∑ÂÖ¨È†É¬∑Âπ¥/ÂÖ¨È†É¬∑ÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢ÈªèÂúüÁöÑKÂÄº\nÂõûÂÇ≥ÔºöÂúüÂ£§È°ûÂûã ÈªèÂúü ÁöÑÊ≤ñËùïÊåáÊï∏ K = 0.20ÔºåÂñÆ‰ΩçÔºöÂÖ¨Âô∏¬∑ÂÖ¨È†É¬∑Âπ¥/ÂÖ¨È†É¬∑ÁôæËê¨ÁÑ¶ËÄ≥¬∑ÂÖ¨Èáê\n```\n\n#### Ë¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöËçâÂú∞ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠êCÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÂúüÂú∞Âà©Áî®È°ûÂûã ËçâÂú∞ ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C = 0.05ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ëæ≤Âú∞ÁöÑCÂÄº\nÂõûÂÇ≥ÔºöÂúüÂú∞Âà©Áî®È°ûÂûã Ëæ≤Âú∞ ÁöÑË¶ÜËìãËàáÁÆ°ÁêÜÂõ†Â≠ê C = 0.30ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n```\n\n#### Ê∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠ê P ÂÄºÊü•Ë©¢\n```\nËº∏ÂÖ•ÔºöÁ≠âÈ´òËÄï‰ΩúÁöÑÊ∞¥Âúü‰øùÊåÅËôïÁêÜÂõ†Â≠êPÂÄºÊòØÂ§öÂ∞ëÔºü\nÂõûÂÇ≥ÔºöÊ∞¥Âúü‰øùÊåÅÊé™ÊñΩ Á≠âÈ´òËÄï‰Ωú ÁöÑËôïÁêÜÂõ†Â≠ê P = 0.50ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n\nËº∏ÂÖ•ÔºöË´ãÊü•Ë©¢Ê¢ØÁî∞ÁöÑPÂÄº\nÂõûÂÇ≥ÔºöÊ∞¥Âúü‰øùÊåÅÊé™ÊñΩ Ê¢ØÁî∞ ÁöÑËôïÁêÜÂõ†Â≠ê P = 0.20ÔºåÂñÆ‰ΩçÔºöÁÑ°Âõ†Ê¨°\n```\n\nÊü•Ë©¢ÁµêÊûúÂåÖÂê´Ôºö\n- Âõ†Â≠êÂÄº\n- ÂñÆ‰Ωç\n- ‰æÜÊ∫êË™™Êòé\n- Ë¶èÁØÑ‰æùÊìö\n\n\n## ‰ΩøÁî®ÁØÑ‰æã(ÂäüËÉΩÂ∞éÂêëÔºå‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÂ∞çË©±ÔºåËÆìÂ§ßÊ®°ÂûãËá™ÂãïË™øÁî®Áõ∏ÈóúÂ∑•ÂÖ∑‰æÜÊ±ÇËß£)\n\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈáçÁèæÊúü50Âπ¥ÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË®àÁÆóË©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè?\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË´ãÂàÜÂà•Ë®àÁÆóÈáçÁèæÊúü25Âπ¥„ÄÅ50Âπ¥Ë©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè? ‰∏¶Ëº∏Âá∫ÂÆåÊï¥Ë®àÁÆóÂºèÁöÑÂ†±ÂëäÊõ∏„ÄÇ\n- ‰æùÊìö25Âπ¥ÁöÑÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∑±70cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåË®≠Ë®àÂù°Â∫¶ÂÆπË®±ÁØÑÂúç?\n- ‰æùÊìöÈÄôÂÄãÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∫ùÊ∑±60cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåÂù°Â∫¶2.5%ÔºåÊ™¢Ê†∏Ë©≤Ë®≠Ë®àÊòØÂê¶OKÔºå‰∏¶Âá∫ÂÖ∑‰∏Ä‰ªΩÂÆåÊï¥ÁöÑÊ™¢Ê†∏Â†±ÂëäÊõ∏„ÄÇ\n- Ê∞¥Ê∫ùÁ∑®Ëôü,B(cm),H(cm),Âù°Â∫¶(%),ÊµÅÈáè(CMS),Ê∞¥Ê∫ùÈã™Èù¢ÂûãÂºè\n  1-1,40,60,1.5,0.25,ÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ù\n  1-2,40,70,1.5,0.30,ÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ù\n  Ê™¢Ê†∏‰∏äÂàóÊ∞¥Ê∫ùË®≠Ë®àÔºå‰ª•markdownË°®Ê†ºÂûãÂºèËº∏Âá∫ÁµêÊûú„ÄÇ \n- Êúâ‰∏ÄÂÄãÁ†¥Á¢éÂ≤©Áõ§ÁöÑÂù°Èù¢ÔºåËßíÂ∫¶Á¥Ñ60Â∫¶Ôºå‰ΩçÊñºÁÜ±Â∏∂Â§öÈõ®Âú∞ÂçÄÔºåË´ãÊèêÂá∫Âù°Èù¢‰øùË≠∑Âª∫Ë≠∞„ÄÇ\n- ÁÑ°Âü∫Á§éÁöÑÊ∑∑ÂáùÂúüÈáçÂäõÂºèÊìãÂúüÁâÜÔºåÂÖ®È´ò2.3MÔºåÁâÜÈ†ÇÂØ¨50cmÔºåÁâÜÂ∫ïÂØ¨100cmÔºåÂúüÂ£§ÂñÆ‰ΩçÈáç18kN/m3ÔºåÊë©Êì¶Ëßí30Â∫¶ÔºåÂáùËÅöÂäõ10kPaÔºåÂú∞‰∏ãÊ∞¥‰ΩçÂú®ÁâÜÈ†Ç‰ª•‰∏ã2MÔºåÂú∞Èúá‰øÇÊï∏0.15ÔºåÂúüÂ±§Âü∫Á§éÔºåË´ãË™øÁî®MCPÂ∑•ÂÖ∑ÔºåÊ™¢Ê†∏ÂÖ∂Â∏∏ÊôÇËàáÂú∞ÈúáÊÉÖÊ≥Å‰∏ãÁöÑÁ©©ÂÆöÊÄßÔºå‰∏¶Ëº∏Âá∫ÂÆåÊï¥Ë®àÁÆóÂ†±ÂëäÊõ∏„ÄÇ\n- Ê∑∑ÂáùÂúüÊ∫ùÔºåheight=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2ÔºåË™øÁî®Â∑•ÂÖ∑ÔºåË®àÁÆóÊ∞¥Ê∫ùÊâÄÈúÄÁöÑÈãºÁ≠ãÈáèÂèäÂª∫Ë≠∞ÈÖçÁ≠ã?\n- Âè∞‰∏≠ÁöÑ \"Âπ¥Âπ≥ÂùáÂúüÂ£§ÊµÅÂ§±Èáè\"Â¶Ç‰Ωï‰º∞ÁÆó? ÈúÄÊèê‰æõÂì™‰∫õÂèÉÊï∏? \n- Ë´ãË®àÁÆó‰∏ÄÊ¢ùÁõ¥Âæë60ÂÖ¨ÂàÜÁöÑÊ∑∑ÂáùÂúüÂúìÂΩ¢ÁÆ°Ê∏†ÔºåÊµÅÈáè0.5cmsÔºåÂù°Â∫¶1.2%ÔºåÊõºÂØß‰øÇÊï∏0.013ÁöÑÊµÅÈÄüÂíåÊµÅÊ∑±\n- Ë®àÁÆóÂØ¨30ÂÖ¨ÂàÜÔºåÈ´ò50cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè0.5cmsÔºåÂù°Â∫¶1.2%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±?\n- Ë®àÁÆóÂØ¨50ÂÖ¨ÂàÜÔºåÈ´ò70cmÁöÑÈãºÁ≠ãÊ∑∑ÂáùÂúüÁü©ÂΩ¢Ê∫ùÔºåÊµÅÈáè1cmsÔºåÂù°Â∫¶5%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\n- Ë®àÁÆóÂ∫ïÂØ¨50ÂÖ¨ÂàÜÔºåÈ†ÇÂØ¨100cmÔºåÈ´ò70cmÁöÑ\"Âπ≥Â∏∏Á†ÇÂúüÊ¢ØÂΩ¢Ê∫ù\"ÔºåÊµÅÈáè0.2cmsÔºåÂù°Â∫¶0.7%ÔºåÊµÅÈÄüÂíåÊµÅÊ∑±? Ëº∏Âá∫ÂÆåÊï¥Â†±ÂëäÔºåÂê´Ë®àÁÆóÈÅéÁ®ãÂèäË®àÁÆóÂºè„ÄÇ\n- Êúâ‰∏ÄÈõÜÊ∞¥ÂçÄÈù¢Á©çÁ¥Ñ5haÔºåÂπ≥Âù¶ËÄïÂú∞ÔºåÊñ∞ÂåóÂ∏ÇÔºåÈáçÁèæÊúü50Âπ¥ÔºåÈôçÈõ®Âª∂ÊôÇ60minÔºåË™øÁî®Â∑•ÂÖ∑ÂèñÂæóË©≤Âú∞ÂçÄÁöÑÈôçÈõ®Âº∑Â∫¶ÂÄºÔºåË®àÁÆóË©≤ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè? \n- ‰æùÊìöÈÄôÂÄãÈÄïÊµÅÈáèÔºåË®≠Ë®à‰∏ÄÊ¢ùÂØ¨50cmÔºåÊ∫ùÊ∑±60cmÁöÑÊ∑∑ÂáùÂúüÊ∫ùÔºåÂù°Â∫¶2.5%ÔºåÊ™¢Ê†∏Ë©≤Ë®≠Ë®àÊòØÂê¶OKÔºå‰∏¶Âá∫ÂÖ∑‰∏Ä‰ªΩÂÆåÊï¥ÁöÑÊ™¢Ê†∏Â†±ÂëäÊõ∏(Âê´ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáè)„ÄÇ\n- ‰ΩøÁî® Gemini 2.5 Pro ÁöÑËº∏Âá∫ÁµêÊûúÂ¶Ç‰∏ã:Ë™øÁî®Â∑•ÂÖ∑Ôºöquery_idf_curve(ÈôçÈõ®Âº∑Â∫¶-Ê≠∑ÊôÇ-È†ªÁéáÔºàIDFÔºâÊõ≤Á∑öÊü•Ë©¢)„ÄÅquery_runoff_coeff(ÈÄïÊµÅ‰øÇÊï∏CÂÄºÊü•Ë©¢Â∑•ÂÖ∑)„ÄÅcalc_catchment_runoff(ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆó)„ÄÅget_manning_n(Â∏∏Ë¶ãÊùêÊñôÁöÑÊõºÂØß‰øÇÊï∏Êü•Ë©¢)„ÄÅcalc_channel_section_flow(ÊéíÊ∞¥Êñ∑Èù¢ÊµÅÈÄü/ÊµÅÊ∑±/ÊµÅÈáèË®àÁÆó)\n\n# Ê∑∑ÂáùÂúüÊ∫ùË®≠Ë®àÊ™¢Ê†∏Â†±ÂëäÊõ∏\n\n## 1. Ë®≠Ë®àÁõÆÊ®ô\n\nÊú¨Â†±ÂëäÊó®Âú®Ê™¢Ê†∏ÊâÄË®≠Ë®à‰πãÊ∑∑ÂáùÂúüÊ∫ùÊñ∑Èù¢ÔºåÊòØÂê¶ËÉΩÂÆâÂÖ®ÂÆπÁ¥ç‰∏äÊ∏∏ÈõÜÊ∞¥ÂçÄÊñºÁâπÂÆöÈáçÁèæÊúü‰∏ãÊâÄÁî¢Áîü‰πãÊúÄÂ§ßÈÄïÊµÅÈáè„ÄÇ\n\n## 2. ÈõÜÊ∞¥ÂçÄÊúÄÂ§ßÈÄïÊµÅÈáèË®àÁÆóÊëòË¶Å\n\n*   **ÈõÜÊ∞¥ÂçÄÊ¢ù‰ª∂:**\n    *   Èù¢Á©ç (A): 5.0 ÂÖ¨È†É (ha)\n    *   ÂúüÂú∞Âà©Áî®: Âπ≥Âù¶ËÄïÂú∞\n    *   Âú∞Èªû: Êñ∞ÂåóÂ∏Ç\n*   **Ë®≠Ë®àÊ¢ù‰ª∂:**\n    *   ÈáçÁèæÊúü (T): 50 Âπ¥\n    *   ÈôçÈõ®Âª∂ÊôÇ (D): 60 ÂàÜÈêò\n*   **Ë®àÁÆóÂèÉÊï∏:**\n    *   ÈôçÈõ®Âº∑Â∫¶ (I): 140.0 mm/hr (‰æùÊìöÊñ∞ÂåóÂ∏Ç50Âπ¥60ÂàÜÈêòIDFÊõ≤Á∑ö)\n    *   ÈÄïÊµÅ‰øÇÊï∏ (C): 0.53 (Âπ≥Âù¶ËÄïÂú∞ÔºåÂèñË¶èÁØÑÂπ≥ÂùáÂÄº)\n*   **Ë®àÁÆóÂÖ¨Âºè (ÂêàÁêÜÂåñÂÖ¨Âºè):** Q = (C √ó I √ó A) / 360\n*   **Ë®àÁÆóÁµêÊûú (Ë®≠Ë®àÊµÅÈáè Q_design):**\n    *   Q_design = (0.53 √ó 140.0 √ó 5.0) / 360 ‚âà **1.03 cms**\n\n## 3. Ê∑∑ÂáùÂúüÊ∫ùË®≠Ë®àÂèÉÊï∏\n\n*   Êñ∑Èù¢ÂûãÂºè: Áü©ÂΩ¢\n*   Ê∏†ÈÅìÊùêË≥™: Ê∑∑ÂáùÂúü\n*   ÊõºÂØßÁ≥ôÂ∫¶‰øÇÊï∏ (n): 0.012\n*   Ê∫ùÊ∏†ÂØ¨Â∫¶ (b): 50 cm (0.5 m)\n*   Ê∫ùÊ∏†Ê∑±Â∫¶ (H): 60 cm (0.6 m)\n*   Ê∏†ÈÅìÂù°Â∫¶ (S): 2.5 % (0.025 m/m)\n\n## 4. Ê∞¥ÁêÜË®àÁÆóËàáÊ™¢Ê†∏\n\n‰ΩøÁî®ÊõºÂØßÂÖ¨ÂºèÔºåÈáùÂ∞çË®≠Ë®àÊµÅÈáè (Q = 1.03 cms) Âèä‰∏äËø∞Ê∫ùÊ∏†ÂèÉÊï∏ÈÄ≤Ë°åÊ∞¥ÁêÜË®àÁÆóÔºö\n\n*   **Ë®àÁÆóÂÖ¨Âºè:**\n    *   Q = A √ó V\n    *   V = (1/n) √ó R^(2/3) √ó S^(1/2)\n    *   A = b √ó y\n    *   P = b + 2y\n    *   R = A / P\n*   **Ë®àÁÆóÁµêÊûú:**\n    *   ÂØ¶ÈöõÊµÅÈÄü (V): **4.014 m/s**\n    *   ÂØ¶ÈöõÊ∞¥Ê∑± (y): **0.513 m (51.3 cm)**\n\n*   **Ê™¢Ê†∏È†ÖÁõÆ:**\n    *   **ÈÄöÊ∞¥ËÉΩÂäõ (Capacity Check):**\n        *   Ë®àÁÆóÊ∞¥Ê∑± (y) = 51.3 cm\n        *   Ë®≠Ë®àÊ∫ùÊ∑± (H) = 60.0 cm\n        *   **Âá∫Ê∞¥È´ò (Freeboard) = H - y = 60.0 - 51.3 = 8.7 cm**\n        *   *Ë©ï‰º∞:* Ê∫ùÊ∏†Êñ∑Èù¢ÂèØÂÆπÁ¥çË®≠Ë®àÊµÅÈáèÔºåÂ∞öÊúâ 8.7 cm ÁöÑÂá∫Ê∞¥È´ò„ÄÇ‰∏ÄËà¨Â∑•Á®ãË®≠Ë®àÂª∫Ë≠∞‰øùÁïôÁ¥Ñ10-20%Ê∫ùÊ∑±ÊàñËá≥Â∞ë10-15cmÁöÑÂá∫Ê∞¥È´ò‰ΩúÁÇ∫ÂÆâÂÖ®È§òË£ïÔºåÊ≠§ËôïÂá∫Ê∞¥È´òÁï•È°Ø‰∏çË∂≥ÔºåÂª∫Ë≠∞ÂèØËÄÉÊÖÆÂä†Â§ßÊ∫ùÊ∑±‰ª•Â¢ûÂä†ÂÆâÂÖ®‰øÇÊï∏„ÄÇ\n    *   **ÊµÅÈÄüÊ™¢Ê†∏ (Velocity Check):**\n        *   Ë®àÁÆóÊµÅÈÄü (V) = 4.014 m/s\n        *   Ê∑∑ÂáùÂúüÂÆπË®±ÊµÅÈÄüÁØÑÂúç (V_allowable): 4.57 ~ 6.1 m/s (‰æùÊìöÊ∞¥‰øùÊäÄË°ìË¶èÁØÑÂ∏∏Ë¶ãÂÄº)\n        *   *Ë©ï‰º∞:* Ë®àÁÆóÊµÅÈÄü 4.014 m/s ‰ΩéÊñºÊ∑∑ÂáùÂúüÁöÑÂÆπË®±ÊµÅÈÄü‰∏ãÈôê (4.57 m/s)ÔºåË°®Á§∫Ê≠§ÊµÅÈÄüÂ∞çÊ∑∑ÂáùÂúüÊ∏†ÈÅìÊú¨Ë∫´ÊòØÂÆâÂÖ®ÁöÑÔºå‰∏çÊúÉÈÄ†ÊàêÊ∏†ÈÅìÊ≤ñÂà∑„ÄÇ\n\n## 5. ÁµêË´ñËàáÂª∫Ë≠∞\n\n1.  **ÈÄöÊ∞¥ËÉΩÂäõ:** Ë®≠Ë®à‰πãÊ∑∑ÂáùÂúüÊ∫ùÊñ∑Èù¢ (ÂØ¨50cm, Ê∑±60cm) **ÂèØ‰ª•ÂÆπÁ¥ç** 1.03 cms ÁöÑË®≠Ë®àÊµÅÈáè„ÄÇ\n2.  **ÊµÅÈÄü:** Ë®àÁÆóÊµÅÈÄü 4.014 m/s Âú®Ê∑∑ÂáùÂúüÁöÑÂÆâÂÖ®ÂÆπË®±ÁØÑÂúçÂÖß„ÄÇ\n3.  **Âá∫Ê∞¥È´ò:** Ë®àÁÆóÊâÄÂæóÁöÑÂá∫Ê∞¥È´òÁÇ∫ 8.7 cmÔºåÁõ∏Â∞çÂÅè‰Ωé„ÄÇ\n4.  **Âª∫Ë≠∞:** ÈõñÁÑ∂Ê≠§Êñ∑Èù¢Âú®Ë®àÁÆó‰∏äÂèØÈÄöÈÅéË®≠Ë®àÊµÅÈáèÔºå‰ΩÜËÄÉÈáèÂà∞ÂØ¶ÈöõÊñΩÂ∑•Ë™§Â∑Æ„ÄÅÊ≥•Á†ÇÊ∑§Á©ç„ÄÅÊàñÊú™‰æÜÊµÅÈáèÂ¢ûÂä†ÁöÑÂèØËÉΩÊÄßÔºå**Âª∫Ë≠∞Â∞áÊ∫ùÊ∏†Ê∑±Â∫¶ÈÅ©Â∫¶Âä†Â§ß (‰æãÂ¶ÇÂ¢ûÂä†Ëá≥ 70 cm Êàñ‰ª•‰∏ä)**Ôºå‰ª•Êèê‰æõÊõ¥ÂÖÖË∂≥ÁöÑÂá∫Ê∞¥È´òÔºåÂ¢ûÂä†Êï¥È´îË®≠Ë®àÁöÑÂÆâÂÖ®ÊÄßËàáÈüåÊÄß„ÄÇ\n\n---\n\n## ÊéàÊ¨ä\n\nÊú¨Â∞àÊ°àÊé°Áî® MIT License ÊéàÊ¨äÔºåÊ≠°ËøéËá™Áî±‰ΩøÁî®ËàáË≤¢Áçª„ÄÇ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "mcp",
        "tutumomo",
        "civil tools",
        "civil engineering",
        "calculation tools"
      ],
      "category": "scientific-research-tools"
    },
    "vishwa684--unet": {
      "owner": "vishwa684",
      "name": "unet",
      "url": "https://github.com/vishwa684/unet",
      "imageUrl": "/freedevtools/mcp/pfp/vishwa684.webp",
      "description": "Train and deploy U-Net models for biomedical image segmentation using the Medical Decathlon dataset, with support for both 2D and 3D U-Net scripts. Visualize predictions and assess model performance through comprehensive demos and visual outputs.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-05-11T00:13:12Z",
      "readme_content": "# Deep Learning Medical Decathlon Demos for Python*\n### U-Net Biomedical Image Segmentation with Medical Decathlon Dataset.\n\nThis repository contains [2D](https://github.com/IntelAI/unet/tree/master/2D) and [3D](https://github.com/IntelAI/unet/tree/master/3D) U-Net scripts for training models using the [Medical Decathlon](http://medicaldecathlon.com/) dataset (http://medicaldecathlon.com/).\n\n![pred152_3D](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_152_img3D.gif\n\"BRATS image #152:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\").  ![pred195](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif \"BRATS image #195:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\")\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomedical",
        "segmentation",
        "dataset",
        "biomedical image",
        "models biomedical",
        "decathlon dataset"
      ],
      "category": "scientific-research-tools"
    },
    "vitamin3615--uniprot-mcp-server": {
      "owner": "vitamin3615",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/vitamin3615/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vitamin3615.webp",
      "description": "Fetch protein information seamlessly from the UniProt database, including detailed data, sequences, functions, and structures. Supports integration with MCP-compatible AI applications for enhanced protein analysis.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-01T01:49:15Z",
      "readme_content": "# UniProt MCP Server\n\nAn MCP (Model Context Protocol) server that allows language models to fetch protein information from the UniProt database. This server can be integrated with Claude Desktop and other MCP-compatible AI applications.\n\n## Features\n\n- **Search UniProt**: Search for proteins by name, gene, or other criteria\n- **Get Protein Details**: Retrieve detailed information about a protein\n- **Get Protein Sequences**: Fetch amino acid sequences\n- **Get Protein Functions**: Retrieve functional annotations including GO terms\n- **Get Protein Structures**: Find related PDB structures\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n\n2. Create a virtual environment (optional but recommended):\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Local Setup with Claude Desktop\n\n1. Make the server script executable:\n   ```bash\n   chmod +x server.py\n   ```\n\n2. Configure Claude Desktop to use this MCP server:\n\n   ### For macOS/Linux:\n   ```bash\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   ### For Windows:\n   ```powershell\n   code $env:AppData\\Claude\\claude_desktop_config.json\n   ```\n\n3. Add the following configuration (update the path to match your actual file location):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"uniprot\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/ABSOLUTE/PATH/TO/uniprot-mcp-server/server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Restart Claude Desktop\n\n## Usage Examples\n\nOnce set up with Claude Desktop, you can ask questions like:\n\n- \"Find proteins related to Alzheimer's disease\"\n- \"Get the sequence for protein P05067 (Amyloid-beta precursor protein)\"\n- \"What are the functions of the TP53 protein?\"\n- \"Are there any known 3D structures for BRCA1 protein?\"\n\n## Development\n\nTo run the server directly for testing:\n\n```bash\npython server.py\n```\n\n## Testing Utilities\n\nThis project includes two testing utilities to verify functionality:\n\n### 1. Server Health Check\n\nVerifies that the MCP server starts correctly without errors:\n\n```bash\npython server_health_check.py\n```\n\n### 2. UniProt API Test\n\nTests connectivity with the UniProt API and verifies that all API calls work as expected:\n\n```bash\npython test_server.py\n```\n\nRun these tests before using the server to ensure everything is properly configured.\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "protein",
        "vitamin3615",
        "uniprot",
        "vitamin3615 uniprot",
        "protein information",
        "uniprot database"
      ],
      "category": "scientific-research-tools"
    },
    "vrtejus--pymol-mcp": {
      "owner": "vrtejus",
      "name": "pymol-mcp",
      "url": "https://github.com/vrtejus/pymol-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vrtejus.webp",
      "description": "Connects PyMOL to Claude AI for natural language interaction with molecular visualization and analysis, enabling intelligent command parsing and structural analysis.",
      "stars": 22,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T21:34:23Z",
      "readme_content": "# PyMOL-MCP: Integrating PyMOL with Claude AI\n\nPyMOL-MCP connects PyMOL to Claude AI through the Model Context Protocol (MCP), enabling Claude to directly interact with and control PyMOL. This powerful integration allows for conversational structural biology, molecular visualization, and analysis through natural language.\n\n\n\nhttps://github.com/user-attachments/assets/687f43dc-d45e-477e-ac2b-7438e175cb36\n\n\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to PyMOL through a socket-based server\n- **Intelligent command parsing**: Natural language processing for PyMOL commands\n- **Molecular visualization control**: Manipulate representations, colors, and views\n- **Structural analysis**: Perform measurements, alignments, and other analyses\n- **Code execution**: Run arbitrary Python code in PyMOL from Claude\n\n## Installation Guide\n\n### Prerequisites\n\n- PyMOL installed on your system\n- Claude for Desktop\n- Python 3.10 or newer\n- Git\n\n### Step 1: Install the UV Package Manager\n\n**On macOS:**\n\n```bash\nbrew install uv\n```\n\n**On Windows:**\n\n```bash\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nset Path=C:\\Users\\[YourUsername]\\.local\\bin;%Path%\n```\n\nFor other platforms, visit the [UV installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Step 2: Clone the Repository\n\n```bash\ngit clone https://github.com/vrtejus/pymol-mcp\ncd pymol-mcp\n```\n\n### Step 3: Set Up the Environment\n\nCreate and activate a Python virtual environment:\n\n```bash\npython -m venv venv\n```\n\n**On macOS/Linux:**\n\n```bash\nsource venv/bin/activate\n```\n\n**On Windows:**\n\n```bash\nvenv\\Scripts\\activate\n```\n\n### Step 4: Install Dependencies\n\nWith the virtual environment activated:\n\n```bash\npip install mcp\n```\n\n### Step 5: Configure Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Claude > Settings > Developer > Edit Config\n3. This will open the `claude_desktop_config.json` file\n4. Add the MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"[Full path to your venv python]\",\n      \"args\": [\"[Full path to pymol_mcp_server.py]\"]\n    }\n  }\n}\n```\n\nFor example:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"/Users/username/pymol-mcp/venv/bin/python\",\n      \"args\": [\"/Users/username/pymol-mcp/pymol_mcp_server.py\"]\n    }\n  }\n}\n```\n\n> **Note:** Use the actual full paths on your system. On Windows, use forward slashes (/) instead of backslashes.\n\n### Step 6: Install the PyMOL Plugin\n\n1. Open PyMOL\n2. Go to Plugin ‚Üí Plugin Manager\n3. Click on \"Install New Plugin\" tab\n4. Select \"Choose file...\" and navigate to the cloned repository\n5. Select the `pymol-mcp-socket-plugin/__init__.py` file\n6. Click \"Open\" and follow the prompts to install the plugin\n\n## Usage\n\n### Starting the Connection\n\n1. In PyMOL:\n\n   - Go to Plugin ‚Üí PyMOL MCP Socket Plugin\n   - Click \"Start Listening\"\n   - The status should change to \"Listening on port 9876\"\n\n2. In Claude Desktop:\n   - You should see a hammer icon in the tools section when chatting\n   - Click it to access the PyMOL tools\n\n### Example Commands\n\nHere are some examples of what you can ask Claude to do:\n\n- \"Load PDB 1UBQ and display it as cartoon\"\n- \"Color the protein by secondary structure\"\n- \"Highlight the active site residues with sticks representation\"\n- \"Align two structures and show their differences\"\n- \"Calculate the distance between these two residues\"\n- \"Save this view as a high-resolution image\"\n\n## Troubleshooting\n\n- **Connection issues**: Make sure the PyMOL plugin is listening before attempting to connect from Claude\n- **Command errors**: Check the PyMOL output window for any error messages\n- **Plugin not appearing**: Restart PyMOL and check that the plugin was correctly installed\n- **Claude not connecting**: Verify the paths in your Claude configuration file are correct\n\n## Limitations & Notes\n\n- The socket connection requires both PyMOL and Claude to be running on the same machine\n- Some complex operations may need to be broken down into simpler steps\n- Always save your work before using experimental features\n- Join our Bio-MCP Community to troubleshoot, provide feedback & improve Bio-MCPS! https://join.slack.com/t/bio-mcpslack/shared_invite/zt-31z4pho39-K5tb6sZ1hUvrFyoPmKihAA\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pymol",
        "molecular",
        "parsing",
        "molecular visualization",
        "vrtejus pymol",
        "connects pymol"
      ],
      "category": "scientific-research-tools"
    },
    "waldzellai--model-enhancement-servers": {
      "owner": "waldzellai",
      "name": "model-enhancement-servers",
      "url": "https://github.com/waldzellai/model-enhancement-servers",
      "imageUrl": "/freedevtools/mcp/pfp/waldzellai.webp",
      "description": "Enables language models to perform visual and spatial reasoning by creating and manipulating diagrammatic representations such as graphs and flowcharts. Supports multi-modal outputs and iterative refinement through a flexible visual element manipulation system.",
      "stars": 63,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T04:52:01Z",
      "readme_content": "# Cognitive Enhancement MCP Servers\n\nA collection of Model Context Protocol servers that provide cognitive enhancement tools for large language models.\n\n## Servers\n\nThis monorepo contains the following MCP servers:\n\n1. **Structured Argumentation** - A server for formal dialectical reasoning\n2. **Visual Reasoning** - A server for diagrammatic thinking and spatial representation\n3. **Scientific Method** - A server for hypothesis testing and evidence evaluation\n4. **Analogical Reasoning** - A server for structured metaphorical thinking\n5. **Metacognitive Monitoring** - A server for knowledge assessment and confidence tracking\n6. **Decision Framework** - A server for structured decision analysis\n7. **Collaborative Reasoning** - A server for multi-perspective problem solving\n8. **Ethical Reasoning** - A server for evaluating actions with moral frameworks\n9. **Bias Detection** - A server for flagging potentially biased wording\n10. **Constraint Solver** - A server for validating logical and numeric constraints\n11. **Narrative Planner** - A server for generating simple story outlines\n12. **Goal Tracker** - A server for maintaining and completing objectives\n13. **Multimodal Synthesizer** - A server for combining text and image descriptions\n\n## Potential Future Servers\n\nThe following server ideas are under consideration to further extend model reasoning capabilities:\n\n1. **Emotion-Aware Interaction** - Provides sentiment tracking and mood-aware responses to improve empathic communication.\n2. **Long-Term Memory** - Maintains persistent context across sessions for continuity and recall of past interactions.\n\n\n## Installation\n\nEach server can be installed individually:\n\n```bash\n# Using npm\nnpm install @waldzellai/structured-argumentation\n\n# Using yarn\nyarn add @waldzellai/structured-argumentation\n```\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"structured-argumentation\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@waldzellai/structured-argumentation\"\n      ]\n    }\n  }\n}\n```\n\n## Docker\n\nAll servers are available as Docker images:\n\n```bash\ndocker run --rm -i waldzellai/structured-argumentation\n```\n\n## Development\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/waldzellai/model-enhancement-servers.git\ncd model-enhancement-servers\nnpm install\n```\n\nBuild all packages:\n\n```bash\nnpm run build\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrammatic",
        "visual",
        "models",
        "diagrammatic representations",
        "flexible visual",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wheattoast11--openrouter-deep-research-mcp": {
      "owner": "wheattoast11",
      "name": "openrouter-deep-research-mcp",
      "url": "https://github.com/wheattoast11/openrouter-deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wheattoast11.webp",
      "description": "Connects to specialized AI agents for conducting complex research tasks through an orchestration system that enables multi-stage planning and parallel execution, resulting in high-quality, tailored research outcomes.",
      "stars": 29,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T22:15:51Z",
      "readme_content": "[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n# OpenRouter Agents MCP Server\n\n[![npm version](https://img.shields.io/npm/v/%40terminals-tech%2Fopenrouter-agents?color=2ea043)](https://www.npmjs.com/package/@terminals-tech/openrouter-agents) [![GitHub Packages](https://img.shields.io/badge/GitHub%20Packages-available-24292e?logo=github)](../../packages)\n\n[UPDATE ‚Äì 2025-08-26] Two modes (set MODE env):\n- AGENT: one simple tool (`agent`) that routes research / follow_up / retrieve / query\n- MANUAL: individual tools for each action\n- ALL (default): both AGENT and MANUAL, plus always-on ops tools\n\nDiagram (simple)\n```\n[Always-On Ops]  ping ‚Ä¢ get_server_status ‚Ä¢ job_status ‚Ä¢ cancel_job\n\nAGENT MODE\nclient ‚Üí agent ‚Üí (research | follow_up | retrieve | query)\n\nMANUAL MODE\nclient ‚Üí (submit_research | conduct_research | retrieve | query | research_follow_up | get_report_content | list_research_history)\n```\n\n- Killer features\n  - Plan ‚Üí parallelize ‚Üí synthesize workflow with bounded parallelism\n  - Dynamic model catalog; supports Anthropic Sonnet‚Äë4 and OpenAI GPT‚Äë5 family\n  - Built‚Äëin semantic KB (PGlite + pgvector) with backup, export/import, health, and reindex tools\n  - Lightweight web helpers: quick search and page fetch for context\n  - Robust streaming (SSE), per‚Äëconnection auth, clean logs\n\n## Install / Run\n- Install (project dependency)\n```bash\nnpm install @terminals-tech/openrouter-agents\n```\n\n- Global install (optional)\n```bash\nnpm install -g @terminals-tech/openrouter-agents\n```\n\n- Run with npx (no install)\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=devkey npx @terminals-tech/openrouter-agents\n```\n\n## What‚Äôs new (v1.5.0)\n- Version parity across npm, GitHub Releases, and GitHub Packages\n- Dual publish workflow enabled\n\n[Changelog ‚Üí](docs/CHANGELOG.md)\n\n## Quick start\n1) Prereqs\n- Node 18+ (20 LTS recommended), npm, Git, OpenRouter API key\n\n2) Install\n```bash\nnpm install\n```\n\n3) Configure (.env)\n```dotenv\nOPENROUTER_API_KEY=your_openrouter_key\nSERVER_API_KEY=your_http_transport_key\nSERVER_PORT=3002\n\n# Modes (pick one; default ALL)\n# AGENT  = agent-only + always-on ops (ping/status/jobs)\n# MANUAL = individual tools + always-on ops\n# ALL    = agent + individual tools + always-on ops\nMODE=ALL\n\n# Orchestration\nENSEMBLE_SIZE=2\nPARALLELISM=4\n\n# Models (override as needed) - Updated with state-of-the-art cost-effective models\nPLANNING_MODEL=openai/gpt-5-chat\nPLANNING_CANDIDATES=openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4\nHIGH_COST_MODELS=x-ai/grok-4,openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4,morph/morph-v3-large\nLOW_COST_MODELS=deepseek/deepseek-chat-v3.1,z-ai/glm-4.5v,qwen/qwen3-coder,openai/gpt-5-mini,google/gemini-2.5-flash\nVERY_LOW_COST_MODELS=openai/gpt-5-nano,deepseek/deepseek-chat-v3.1\n\n# Storage\nPGLITE_DATA_DIR=./researchAgentDB\nPGLITE_RELAXED_DURABILITY=true\nREPORT_OUTPUT_PATH=./research_outputs/\n\n# Indexer\nINDEXER_ENABLED=true\nINDEXER_AUTO_INDEX_REPORTS=true\nINDEXER_AUTO_INDEX_FETCHED=true\n\n# MCP features\nMCP_ENABLE_PROMPTS=true\nMCP_ENABLE_RESOURCES=true\n\n# Prompt strategy\nPROMPTS_COMPACT=true\nPROMPTS_REQUIRE_URLS=true\nPROMPTS_CONFIDENCE=true\n\n```\n\n4) Run\n- STDIO (for Cursor/VS Code MCP):\n```bash\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE (local daemon):\n```bash\nSERVER_API_KEY=$SERVER_API_KEY node src/server/mcpServer.js\n```\n\n### Windows PowerShell examples\n- STDIO\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:INDEXER_ENABLED='true'\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:SERVER_API_KEY='devkey'\n$env:SERVER_PORT='3002'\nnode src/server/mcpServer.js\n```\n\n### One-liner demo scripts\nDev (HTTP/SSE):\n```bash\nSERVER_API_KEY=devkey INDEXER_ENABLED=true node src/server/mcpServer.js\n```\n\nSTDIO (Cursor/VS Code):\n```bash\nOPENROUTER_API_KEY=your_key INDEXER_ENABLED=true node src/server/mcpServer.js --stdio\n```\n\n### MCP client JSON configuration (no manual start required)\nYou can register this server directly in MCP clients that support JSON server manifests.\n\nMinimal examples:\n\n1) STDIO transport (recommended for IDEs)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"command\": \"npx\",\n      \"args\": [\"@terminals-tech/openrouter-agents\", \"--stdio\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"${OPENROUTER_API_KEY}\",\n        \"SERVER_API_KEY\": \"${SERVER_API_KEY}\",\n        \"PGLITE_DATA_DIR\": \"./researchAgentDB\",\n        \"INDEXER_ENABLED\": \"true\"\n      }\n    }\n  }\n}\n```\n\n2) HTTP/SSE transport (daemon mode)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"url\": \"http://127.0.0.1:3002\",\n      \"sse\": \"/sse\",\n      \"messages\": \"/messages\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${SERVER_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\nWith the package installed globally (or via npx), MCP clients can spawn the server automatically. See your client‚Äôs docs for where to place this JSON (e.g., `~/.config/client/mcp.json`).\n\n## Tools (high‚Äëvalue)\n- Always‚Äëon (all modes): `ping`, `get_server_status`, `job_status`, `get_job_status`, `cancel_job`\n- AGENT: `agent` (single entrypoint for research / follow_up / retrieve / query)\n- MANUAL/ALL toolset: `submit_research` (async), `conduct_research` (sync/stream), `research_follow_up`, `search` (hybrid), `retrieve` (index/sql), `query` (SELECT), `get_report_content`, `list_research_history`\n- Jobs: `get_job_status`, `cancel_job`\n- Retrieval: `search` (hybrid BM25+vector with optional LLM rerank), `retrieve` (index/sql wrapper)\n- SQL: `query` (SELECT‚Äëonly, optional `explain`)\n- Knowledge base: `get_past_research`, `list_research_history`, `get_report_content`\n- DB ops: `backup_db` (tar.gz), `export_reports`, `import_reports`, `db_health`, `reindex_vectors`\n- Models: `list_models`\n- Web: `search_web`, `fetch_url`\n- Indexer: `index_texts`, `index_url`, `search_index`, `index_status`\n\n### Tool usage patterns (for LLMs)\nUse `tool_patterns` resource to view JSON recipes describing effective chaining, e.g.:\n- Search ‚Üí Fetch ‚Üí Research\n- Async research: submit, stream via SSE `/jobs/:id/events`, then get report content\n\nNotes\n- Data lives locally under `PGLITE_DATA_DIR` (default `./researchAgentDB`). Backups are tarballs in `./backups`.\n- Use `list_models` to discover current provider capabilities and ids.\n\n## Architecture at a glance\nSee `docs/diagram-architecture.mmd` (Mermaid). Render to SVG with Mermaid CLI if installed:\n```bash\nnpx @mermaid-js/mermaid-cli -i docs/diagram-architecture.mmd -o docs/diagram-architecture.svg\n```\nOr use the script:\n```bash\nnpm run gen:diagram\n```\n\n![Architecture Diagram (branded)](docs/diagram-architecture-branded.svg)\n\nIf the image doesn‚Äôt render in your viewer, open `docs/diagram-architecture-branded.svg` directly.\n\n### Answer crystallization view\n![Answer Crystallization Diagram](docs/answer-crystallization-architecture.svg)\n\nHow it differs from typical ‚Äúagent chains‚Äù:\n- Not just hardcoded handoffs; the plan is computed, then parallel agents search, then a synthesis step reasons over consensus, contradictions, and gaps.\n- The system indexes what it reads during research, so subsequent queries get faster/smarter.\n- Guardrails shape attention: explicit URL citations, [Unverified] labelling, and confidence scoring.\n\n## Minimal‚Äëtoken prompt strategy\n- Compact mode strips preambles to essential constraints; everything else is inferred.\n- Enforced rules: explicit URL citations, no guessing IDs/URLs, confidence labels.\n- Short tool specs: use concise param names and rely on server defaults.\n\n## Common user journeys\n- ‚ÄúGive me an executive briefing on MCP status as of July 2025.‚Äù\n  - Server plans sub‚Äëqueries, fetches authoritative sources, synthesizes with citations.\n  - Indexed outputs make related follow‚Äëups faster.\n\n- ‚ÄúFind vision‚Äëcapable models and route images gracefully.‚Äù\n  - `/models` discovered and filtered, router template generated, fallback to text models.\n\n- ‚ÄúCompare orchestration patterns for bounded parallelism.‚Äù\n  - Pulls OTel/Airflow/Temporal docs, produces a MECE synthesis and code pointers.\n\n## Cursor IDE usage\n- Add this server in Cursor MCP settings pointing to `node src/server/mcpServer.js --stdio`.\n- Use the new prompts (`planning_prompt`, `synthesis_prompt`) directly in Cursor to scaffold tasks.\n\n## FAQ (quick glance)\n- How does it avoid hallucinations?\n  - Strict citation rules, [Unverified] labels, retrieval of past work, on‚Äëthe‚Äëfly indexing.\n- Can I disable features?\n  - Yes, via env flags listed above.\n- Does it support streaming?\n  - Yes, SSE for HTTP; stdio for MCP.\n\n## Command Map (quick reference)\n- Start (stdio): `npm run stdio`\n- Start (HTTP/SSE): `npm start`\n- Run via npx (scoped): `npx @terminals-tech/openrouter-agents --stdio`\n- Generate examples: `npm run gen:examples`\n- List models: MCP `list_models { refresh:false }`\n- Submit research (async): `submit_research { q:\"<query>\", cost:\"low\", aud:\"intermediate\", fmt:\"report\", src:true }`\n- Track job: `get_job_status { job_id:\"...\" }`, cancel: `cancel_job { job_id:\"...\" }`\n- Unified search: `search { q:\"<query>\", k:10, scope:\"both\" }`\n- SQL (read‚Äëonly): `query { sql:\"SELECT ... WHERE id = $1\", params:[1], explain:true }`\n- Get past research: `get_past_research { query:\"<query>\", limit:5 }`\n- Index URL (if enabled): `index_url { url:\"https://...\" }`\n- Micro UI (ghost): visit `http://localhost:3002/ui` to stream job events (SSE).\n\n## Package publishing\n- Name: `@terminals-tech/openrouter-agents`\n- Version: 1.3.2\n- Bin: `openrouter-agents`\n- Author: Tej Desai <admin@terminals.tech>\n- Homepage: https://terminals.tech\n\nInstall and run without cloning:\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=your_key npx @terminals-tech/openrouter-agents\n```\n\n### Publish (scoped)\n```bash\nnpm login\nnpm version 1.3.2 -m \"chore(release): %s\"\ngit push --follow-tags\nnpm publish --access public --provenance\n```\n\n## Validation ‚Äì MSeeP (Multi‚ÄëSource Evidence & Evaluation Protocol)\n- **Citations enforced**: explicit URLs, confidence tags; unknowns marked `[Unverified]`.\n- **Cross‚Äëmodel triangulation**: plan fans out to multiple models; synthesis scores consensus vs contradictions.\n- **KB grounding**: local hybrid index (BM25+vector) retrieves past work for cross‚Äëchecking.\n- **Human feedback**: `rate_research_report { rating, comment }` stored to DB; drives follow‚Äëups.\n- **Reproducibility**: `export_reports` + `backup_db` capture artifacts for audit.\n\n## Quality feedback loop\n- Run examples: `npm run gen:examples`\n- Review: `list_research_history`, `get_report_content {reportId}`\n- Rate: `rate_research_report { reportId, rating:1..5, comment }`\n- Improve retrieval: `reindex_vectors`, `index_status`, `search_index { query }`\n\n## Architecture diagram (branded)\n- See `docs/diagram-architecture-branded.svg` (logo links to `https://terminals.tech`).\n\n## Stargazers\n[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=wheattoast11/openrouter-deep-research&type=Date)](https://star-history.com/#wheattoast11/openrouter-deep-research)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "research",
        "planning",
        "deep research",
        "research tasks",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wiSCADA--gpu.cpp": {
      "owner": "wiSCADA",
      "name": "gpu.cpp",
      "url": "https://github.com/wiSCADA/gpu.cpp",
      "imageUrl": "/freedevtools/mcp/pfp/wiSCADA.webp",
      "description": "A lightweight C++ library for portable GPU computation that uses the WebGPU specification, enabling seamless integration of GPU capabilities across a variety of hardware platforms. It supports multiple GPU backends, including Vulkan, Metal, and DirectX, facilitating efficient development and fast iterations.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-07-30T09:11:07Z",
      "readme_content": "# gpu.cpp\n\ngpu.cpp is a lightweight library that makes portable GPU compute with C++ simple.\n\nIt focuses on general purpose native GPU computation, leveraging the WebGPU\nspecification as a portable low-level GPU interface. This means we can drop in\nGPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs.\nThe same C++ code can work on a wide variety of laptops, workstations, mobile\ndevices or virtually any hardware with Vulkan, Metal, or DirectX support.\n\n## Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate\n\nWith gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:\n\n- High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.\n- Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be <5 seconds on a modern laptop.\n- Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.\n\nThe implementation aims for a small API surface area with minimum boilerplate. There are a small number of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it's needed.\n\nIn this spirit of fast experimentation, we also want near-instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.\n\nThe core library implementation in the header-only `gpu.h` source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high.\nWe also pre-build Google's Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google's powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles.\n\nFor more advanced users and release deployments, we include `cmake` examples for building both Dawn with gpu.cpp end-to-end, but this is not required nor recommended for most users to get started.\n\n## Quick Start: Building and Running\n\nTo build a gpu.cpp project, you will need to have installed on your system:\n\n- `clang++` compiler installed with support for C++17.\n- `python3` and above, to run the script which downloads the Dawn shared library.\n  make to build the project.\n- `make` to build the project.\n- Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run `sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools` to install them.\n\nThe only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).\n\nOptionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.\n\nAfter cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:\n\n```\nmake\n```\n\nThe first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see `libdawn.dylib` on MacOS or `libdawn.so` on Linux. This download only occurs once.\n\nThe build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:\n\n```\nHello gpu.cpp!\n--------------\n\n  gelu(0.00) = 0.00\n  gelu(0.10) = 0.05\n  gelu(0.20) = 0.12\n  gelu(0.30) = 0.19\n  gelu(0.40) = 0.26\n  gelu(0.50) = 0.35\n  gelu(0.60) = 0.44\n  gelu(0.70) = 0.53\n  gelu(0.80) = 0.63\n  gelu(0.90) = 0.73\n  gelu(1.00) = 0.84\n  gelu(1.10) = 0.95\n  ...\n\nComputed 10000 values of GELU(x)\n```\n\nIf you need to clean up the build artifacts, you can run:\n\n```\nmake clean\n```\n\n## Hello World Tutorial: A GELU Kernel\n\nAs a real-world example for how to use gpu.cpp, let's start with a practical-but-simple example of a GPU kernel from neural networks.\n\nGELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.\n\nIt takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.\n\nThe GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:\n\n1. The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute operation.\n\n2. The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.\n\n3. The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.\n\nHere's a GELU kernel implemented (based on the CUDA implementation in [llm.c](https://github.com/karpathy/llm.c)) as on-device WebGPU WGSL code and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):\n\n```\n#include <array>\n#include <cstdio>\n#include <future>\n\n#include \"gpu.h\"\n\nusing namespace gpu; // createContext, createTensor, createKernel,\n                     // dispatchKernel, wait, toCPU Bindings,\n                     // Tensor, Kernel, Context, Shape, kf32\n\nstatic const char *kGelu = R\"(\nconst GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)\n@group(0) @binding(0) var<storage, read_write> inp: array<{{precision}}>;\n@group(0) @binding(1) var<storage, read_write> out: array<{{precision}}>;\n@compute @workgroup_size({{workgroupSize}})\nfn main(\n    @builtin(global_invocation_id) GlobalInvocationID: vec3<u32>) {\n    let i: u32 = GlobalInvocationID.x;\n    if (i < arrayLength(&inp)) {\n        let x: f32 = inp[i];\n        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR\n                 * (x + .044715 * x * x * x))), x, x > 10.0);\n    }\n}\n)\";\n\nint main(int argc, char **argv) {\n  Context ctx = createContext();\n  static constexpr size_t N = 10000;\n  std::array<float, N> inputArr, outputArr;\n  for (int i = 0; i < N; ++i) {\n    inputArr[i] = static_cast<float>(i) / 10.0; // dummy input data\n  }\n  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());\n  Tensor output = createTensor(ctx, Shape{N}, kf32);\n  std::promise<void> promise;\n  std::future<void> future = promise.get_future();\n  Kernel op = createKernel(ctx, {kGelu, /* 1-D workgroup size */ 256, kf32},\n                           Bindings{input, output},\n                           /* number of workgroups */ {cdiv(N, 256), 1, 1});\n  dispatchKernel(ctx, op, promise);\n  wait(ctx, future);\n  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));\n  for (int i = 0; i < 16; ++i) {\n    printf(\"  gelu(%.2f) = %.2f\\n\", inputArr[i], outputArr[i]);\n  }\n  return 0;\n}\n```\n\nHere we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui) for a demonstration of live WGSL code re-loading).\n\nThe CPU code in main() sets up the host coordination for the GPU computation.\nWe can think of the use of gpu.cpp library as a collection of GPU nouns and\nverbs.\n\nThe \"nouns\" are GPU resources modeled by the type definitions of the library\nand the \"verbs\" actions on GPU resources, modeled by the functions of the\nlibrary. The ahead-of-time resource acquisition functions are prefaced with\n`create*`, such as:\n\n- `createContext()` - constructs a reference to the GPU device context (`Context`).\n- `createTensor()` - acquires a contiguous buffer on the GPU (`Tensor`).\n- `createKernel()` - constructs a handle to resources for the GPU computation (`Kernel`), taking the shader code as input and the tensor resources to bind.\n\nThese resource acquisition functions are tied to resource types for interacting with the GPU:\n\n- `Context` - a handle to the state of resources for interacting with the GPU device.\n- `Tensor` - a buffer of data on the GPU.\n- `KernelCode` - the code for a WGSL program that can be dispatched to the\n  GPU. This is a thin wrapper around a WGSL string and also includes the\n  workgroup size the code is designed to run with.\n- `Kernel` - a GPU program that can be dispatched to the GPU. This accepts a\n  `KernelCode` and a list of `Tensor` resources to bind for the dispatch\n  computation. This takes an argument `Bindings` that is a list of `Tensor` instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the `input` buffer on the GPU and the `ouptut` buffer on the GPU.\n\nIn this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the [examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.\n\nBesides the `create*` resource acquisition functions, there are a few more \"verbs\" in the gpu.cpp library for handling dispatching execution to the GPU and data movement:\n\n- `dispatchKernel()` - dispatches a `Kernel` to the GPU for computation. This is an asynchronous operation that returns immediately.\n- `wait()` - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.\n- `toCPU()` - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.\n- `toGPU()` - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, `toGPU()` is not used because there's only one data movement from CPU to GPU in the program and that happens when the `createTensor()` function is called.\n\nThis example is available in [examples/hello_world/run.cpp](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp).\n\n## Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering\n\nYou can explore the example projects in\n[examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) which\nillustrate how to use gpu.cpp as a library.\n\nAfter you have run `make` in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running `make` from the example's directory.\n\nAn example of tiled matrix multiplication is in [examples/matmul](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/). This implements a WebGPU version of the first few kernels of Simon Boehm's [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM) post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point.\n\nA parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in [examples/physics](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics).\n\n<div align=\"center\">\n<img src=\"docs/images/matmul.png\" alt=\"matmul example output\" width=40%>\n<img src=\"docs/images/pendulum.gif\" alt=\"physics example animated gif\" width=42%>\n</div>\n\nWe also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in [examples/render](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render]) and a shadertoy-like live-reloading example is in [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui).\n\nInterestingly, given a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.\n\n<div align=\"center\">\n  <img src=\"docs/images/shadertui.gif\" alt=\"shadertui example animated gif\" width=88%>\n</div>\n\n## Who is gpu.cpp for?\n\ngpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:\n\n- Development of GPU algorithms to be run on personal computing devices\n- Direct standalone implementations of neural network models\n- Physics simulations and simulation environments\n- Multimodal applications - audio and video processing\n- Offline graphics rendering\n- ML inference engines and runtimes\n- Parallel compute intensive data processing applications\n\nAlthough gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.\n\nTo date, AI research has primarily been built with CUDA as the privileged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.\n\nGPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastructure scale efforts - game engines, production ML inference engines, large software packages.\n\nWe want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.\n\ngpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.\n\n## What gpu.cpp is not\n\ngpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.\n\nSecond, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable _native_ GPU API first and foremost, with the possibility of running in the browser being a convenient additional benefit in the future.\n\nIf you find it counterintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk [\"WebGPU is Not Just About the Web\"](https://www.youtube.com/watch?v=qHrx41aOTUQ).\n\nFinally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.\n\n## Limitations and Upcoming Features\n\n_API Improvements_ - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.\n\n_Browser Targets_ - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority.\n\n_Reusable Kernel Library_ - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in `examples/`. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.\n\n_More Use Case Examples and Tests_ - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from [llm.c](https://github.com/karpathy/llm.c) in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.\n\n## Troubleshooting\n\nIf you run into issues building the project, please open an issue.\n\n## Acknowledgements\n\ngpu.cpp makes use of:\n\n- [Dawn](https://dawn.googlesource.com/dawn) as the WebGPU implementation\n- [webgpu-dawn-binaries](https://github.com/jspanchu/webgpu-dawn-binaries) by\n  @jspanchu to build a binary artifact of Dawn.\n- [webgpu-distribution](https://github.com/eliemichel/WebGPU-distribution) by\n  @eliemichel for cmake builds.\n\nThanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback.\n\n## Discord Community and Contributing\n\nJoin our community in the `#gpu-cpp` channel on the [AnswerDotAI Discord with this invite link](https://discord.gg/zmJVhXsC7f). Feel free to get in touch via X [@austinvhuang](https://twitter.com/austinvhuang) as well.\n\nFeedback, issues and pull requests are welcome.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpu",
        "webgpu",
        "vulkan",
        "wiscada gpu",
        "gpu cpp",
        "gpu computation"
      ],
      "category": "scientific-research-tools"
    },
    "wj2333wj--arxiv-mcp-server": {
      "owner": "wj2333wj",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/wj2333wj/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/wj2333wj.webp",
      "description": "Enables searching, downloading, and analyzing arXiv research papers through an MCP interface, managing local storage for efficient retrieval and utilizing specialized prompts for comprehensive analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-27T06:38:09Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \nü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ \nüìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ‚ú® Core Features\n\n- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- üìÑ **Paper Access**: Download and read paper content\n- üìã **Paper Listing**: View all downloaded papers\n- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access\n- üìù **Prompts**: A Set of Research Prompts\n\n## üöÄ Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### üîå MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## üí° Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## üìù Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ‚öôÔ∏è Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## üß™ Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## üìÑ License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ‚ù§Ô∏è by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "arxiv",
        "retrieval",
        "mcp",
        "arxiv mcp",
        "arxiv research",
        "analyzing arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "wjlim--nf-core_mcp": {
      "owner": "wjlim",
      "name": "nf-core_mcp",
      "url": "https://github.com/wjlim/nf-core_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wjlim.webp",
      "description": "Manage and navigate nf-core pipeline repositories, providing access to configurations, workflows, and modules while enabling the search through pipeline files to enhance bioinformatics project efficiency.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-08T14:41:02Z",
      "readme_content": "# nf-core MCP Server\n\nAn MCP server for managing and navigating nf-core pipeline repositories.\n\n## Features\n\n- List local nf-core repositories (rnaseq, sarek, modules, tools)\n- Access pipeline configurations and workflows\n- Search through pipeline files\n- Explore pipeline modules\n\n## Installation\n\n### NPM Version\n\n```bash\n\ncd nf-core_mcp\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Start the server\nnpm start\n```\n\n### Docker Version\n\n```bash\n# Build the Docker image\ncd nf-core_mcp\ndocker build -t nf-core-mcp .\n# Run the container\ndocker run -i --rm \\\n  -v \"/path/to/your/workspace:/app/workspace\" \\\n  nf-core-mcp\n```\n\n## Adding nf-core Repositories\n\nTo add new nf-core pipeline repositories to the workspace:\n\n1. **Clone the repositories**:\n   ```bash\n   # Navigate to your workspace directory (example for Windows)\n   cd /path/to/your/workspace\n\n   # Clone desired nf-core repositories\n   git clone https://github.com/nf-core/rnaseq.git\n   git clone https://github.com/nf-core/sarek.git\n   git clone https://github.com/nf-core/modules.git\n   # Add any other nf-core pipeline you want to manage\n   ```\n\n2. **Directory Structure**:\n   Your workspace should look like this:\n   ```\n   workspace/\n   ‚îú‚îÄ‚îÄ rnaseq/\n   ‚îú‚îÄ‚îÄ sarek/\n   ‚îú‚îÄ‚îÄ modules/\n   ‚îî‚îÄ‚îÄ your-new-pipeline/\n   ```\n\n3. **Verify Installation**:\n   After starting the MCP server, use the `list-pipelines` command to verify that your new pipelines are detected:\n   ```\n   list-pipelines\n   ```\n\nNote: The MCP server will automatically detect and manage any nf-core pipeline repositories in your workspace directory.\n\n## Available Tools\n\n1. `list-pipelines`\n   - Lists all nf-core pipelines in the workspace\n   - Shows configuration file status\n   - No parameters required\n\n2. `get-pipeline-modules`\n   - Gets module information from a pipeline\n   - Parameters:\n     - `pipeline`: Pipeline name (rnaseq, sarek, or modules)\n\n3. `search-pipelines`\n   - Searches through pipeline files\n   - Parameters:\n     - `query`: Search query\n     - `pipeline` (optional): Specific pipeline to search\n\n## Available Resources\n\n1. `pipeline-config`\n   - Gets pipeline configuration\n   - URI format: `pipeline://{name}/config`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n2. `pipeline-workflow`\n   - Gets pipeline workflow\n   - URI format: `pipeline://{name}/workflow`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n## Usage with Cursor IDE\n\n### Using NPX (Recommended)\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"nf-core-mcp\"]\n    }\n  }\n}\n```\n\n### Using Docker\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/your/workspace:/app/workspace\",\n        \"nf-core-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Example Usage\n\nUsing the MCP server in Cursor:\n\n```\n# List available pipelines\nlist-pipelines\n\n# Get modules from rnaseq pipeline\nget-pipeline-modules pipeline=rnaseq\n\n# Search in all pipelines\nsearch-pipelines query=\"fastqc\"\n\n# Search in specific pipeline\nsearch-pipelines query=\"fastqc\" pipeline=rnaseq\n\n# Access pipeline configuration\npipeline://rnaseq/config\n\n# Access workflow\npipeline://rnaseq/workflow\n```\n\n## Running the Server\n\n### Using NPM\n\n```bash\n# If installed globally\nnf-core-mcp\n\n# If installed locally\nnpx nf-core-mcp\n\n# Using npx without installation\nnpx -y nf-core-mcp\n```\n\n### Using Docker\n\n```bash\ndocker run -it --rm \\\n  -v /path/to/your/workspace:/app/workspace \\\n  nf-core-mcp\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Run tests\nnpm test\n\n# Run linter\nnpm run lint\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "core_mcp",
        "pipeline",
        "nf core_mcp",
        "nf core",
        "core pipeline"
      ],
      "category": "scientific-research-tools"
    },
    "woodamsim--models": {
      "owner": "woodamsim",
      "name": "models",
      "url": "https://github.com/woodamsim/models",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Provides a collection of TensorFlow models, including examples and research implementations, facilitating access to various machine learning solutions and learning resources. Users can explore TensorFlow capabilities through well-maintained models and associated tutorials.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tensorflow",
        "models",
        "woodamsim",
        "tensorflow models",
        "explore tensorflow",
        "woodamsim models"
      ],
      "category": "scientific-research-tools"
    },
    "ygboy1986--AIClaude": {
      "owner": "ygboy1986",
      "name": "AIClaude",
      "url": "https://github.com/ygboy1986/AIClaude",
      "imageUrl": "/freedevtools/mcp/pfp/ygboy1986.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude for complex multi-step reasoning tasks. Enhances language processing capabilities to deliver precise and efficient responses.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-20T03:09:57Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## üöÄ Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   ![claude_desktop_config.json file](public/images/edit_config.png)\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   ![tool visible](public/images/tool1.png)\n![tool verify](public/images/tool2.png)\n\n   - If the server isn‚Äôt visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## üõ† Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## üìÑ License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "aiclaude",
        "reasoning engine",
        "reasoning tasks",
        "advanced reasoning"
      ],
      "category": "scientific-research-tools"
    },
    "yonaka15--mcp-pyodide": {
      "owner": "yonaka15",
      "name": "mcp-pyodide",
      "url": "https://github.com/yonaka15/mcp-pyodide",
      "imageUrl": "/freedevtools/mcp/pfp/yonaka15.webp",
      "description": "Execute Python code through the Model Context Protocol interface, enabling Large Language Models to leverage Python capabilities within their responses. The server supports both stdio and SSE transport modes and is implemented in TypeScript.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-08T05:40:45Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yonaka15-mcp-pyodide-badge.png)](https://mseep.ai/app/yonaka15-mcp-pyodide)\n\n# mcp-pyodide\n\nA Pyodide server implementation for the Model Context Protocol (MCP). This server enables Large Language Models (LLMs) to execute Python code through the MCP interface.\n\n<a href=\"https://glama.ai/mcp/servers/pxls43joly\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pxls43joly/badge\" alt=\"mcp-pyodide MCP server\" />\n</a>\n\n## Features\n\n- Python code execution capability for LLMs using Pyodide\n- MCP compliant server implementation\n- Support for both stdio and SSE transport modes\n- Robust implementation written in TypeScript\n- Available as a command-line tool\n\n## Installation\n\n```bash\nnpm install mcp-pyodide\n```\n\n## Usage\n\n### As a Server\n\n```typescript\nimport { runServer } from \"mcp-pyodide\";\n\n// Start the server\nrunServer().catch((error: unknown) => {\n  console.error(\"Error starting server:\", error);\n  process.exit(1);\n});\n```\n\n### As a Command-line Tool\n\nStart in stdio mode (default):\n\n```bash\nmcp-pyodide\n```\n\nStart in SSE mode:\n\n```bash\nmcp-pyodide --sse\n```\n\n### SSE Mode\n\nWhen running in SSE mode, the server provides the following endpoints:\n\n- SSE Connection: `http://localhost:3020/sse`\n- Message Handler: `http://localhost:3020/messages`\n\nExample client connection:\n\n```typescript\nconst eventSource = new EventSource(\"http://localhost:3020/sse\");\neventSource.onmessage = (event) => {\n  console.log(\"Received:\", JSON.parse(event.data));\n};\n```\n\n## Project Structure\n\n```\nmcp-pyodide/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ formatters/    # Data formatting handlers\n‚îÇ   ‚îú‚îÄ‚îÄ handlers/      # Request handlers\n‚îÇ   ‚îú‚îÄ‚îÄ lib/          # Library code\n‚îÇ   ‚îú‚îÄ‚îÄ tools/        # Utility tools\n‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Utility functions\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts      # Main entry point\n‚îú‚îÄ‚îÄ build/            # Build artifacts\n‚îú‚îÄ‚îÄ pyodide-packages/ # Pyodide-related packages\n‚îî‚îÄ‚îÄ package.json\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: MCP SDK (^1.4.0)\n- `pyodide`: Python runtime environment (^0.27.1)\n- `arktype`: Type validation library (^2.0.1)\n- `express`: Web framework for SSE support\n- `cors`: CORS middleware for SSE support\n\n## Development\n\n### Requirements\n\n- Node.js 18 or higher\n- npm 9 or higher\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone <repository-url>\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n```\n\n### Scripts\n\n- `npm run build`: Compile TypeScript and set execution permissions\n- `npm start`: Run server in stdio mode\n- `npm run start:sse`: Run server in SSE mode\n\n## Environment Variables\n\n- `PYODIDE_CACHE_DIR`: Directory for Pyodide cache (default: \"./cache\")\n- `PYODIDE_DATA_DIR`: Directory for mounted data (default: \"./data\")\n- `PORT`: Port for SSE server (default: 3020)\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -am 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Create a Pull Request\n\n## Important Notes\n\n- This project is under development, and the API may change\n- Thoroughly test before using in production\n- Exercise caution when executing untrusted code for security reasons\n- When using SSE mode, ensure proper CORS configuration if needed\n\n## Support\n\nPlease use the Issue tracker for problems and questions.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "pyodide",
        "mcp",
        "python capabilities",
        "mcp pyodide",
        "pyodide execute"
      ],
      "category": "scientific-research-tools"
    },
    "ywwAHU--mcp-scholarly": {
      "owner": "ywwAHU",
      "name": "mcp-scholarly",
      "url": "https://github.com/ywwAHU/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/ywwAHU.webp",
      "description": "Search for academic articles using the MCP server by querying a dedicated tool for relevant papers based on keywords. Access trusted academic sources for streamlined research.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-15T07:27:21Z",
      "readme_content": "# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n![demo1.jpeg](examples/demo1.png)\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-2025323](https://github.com/user-attachments/assets/b73c916d-194c-429e-a7f9-0ff7d0db0b02)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "mcp",
        "mcp scholarly",
        "scholarly search",
        "search academic"
      ],
      "category": "scientific-research-tools"
    },
    "zchaffee1--mcp-server": {
      "owner": "zchaffee1",
      "name": "mcp-server",
      "url": "https://github.com/zchaffee1/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zchaffee1.webp",
      "description": "Provides access to scientific computing resources, enabling HDF5 file operations, Slurm job management, and hardware information retrieval. Facilitates data compression and decompression using gzip or zlib.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-21T04:43:22Z",
      "readme_content": "# mcp-server\n\nBy: Zack Chaffee A20478873\n\nA server implementing Model Coupling Protocol (MCP) capabilities for HDF5 file operations and Slurm job management.\n\n## Features\n\n### HDF5 file operations:\n  - Read datasets\n  - List file contents\n\n### Slurm job management:\n  - Submit jobs\n  - Check job status\n\n### Node Hardware Operations\n- Get CPU information\n- Get memory information\n- Get disk information\n- Get comprehensive system information\n\n### Compression Operations\n- Compress string data with gzip or zlib\n- Compress files with gzip or zlib\n- Decompress data\n\n# Initialization\n\nOnce you clone this reponsitory cd into it \n\nAfter this hwe will create a virtual enviornment and install all dependincies:\n```\nuv venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\nuv pip install -e \".[test]\"\n```\n\n# Running\n\nTo startup the server run:\n```\npython -m src.server\n```\n\nThis will autoclocate the server at http://localhost:8000.\n\n## Endpoints\n- `POST /mcp`: Main endpoint for MCP requests\n- `GET /health`: Health check endpoint\n\n## Examples:\n```python\nimport httpx\n\nasync with httpx.AsyncClient() as client:\n    # Read a dataset\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"read_dataset\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"dataset_path\": \"/path/to/dataset\"\n        }\n    })\n    \n    # List contents\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"list_contents\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"group_path\": \"/\"\n        }\n    })\n```\n\n```bash\ncurl -X POST http://localhost:8000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"mcp/listTools\", \n    \"params\": {}, \n    \"id\": \"1\"\n  }'\n```\n\n# Testing\nFor testing rung: \n```\npytest\n```\n\nFor tests with coverage:\n```\npytest --cov=src\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gzip",
        "zlib",
        "hdf5",
        "gzip zlib",
        "zchaffee1 mcp",
        "tools zchaffee1"
      ],
      "category": "scientific-research-tools"
    },
    "zinja-coder--apktool-mcp-server": {
      "owner": "zinja-coder",
      "name": "apktool-mcp-server",
      "url": "https://github.com/zinja-coder/apktool-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zinja-coder.webp",
      "description": "Integrate Apktool with LLMs to enable real-time reverse engineering of Android APKs. Perform decompilation, code review, and AI-driven analysis while managing resources and vulnerabilities.",
      "stars": 224,
      "forks": 30,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T12:48:07Z",
      "readme_content": "<div align=\"center\">\n\n# apktool-mcp-server (Part of Zin's Reverse Engineering MCP Suite)\n\n‚ö° Fully automated MCP server built on top of apktool to analyze Android APKs using LLMs like Claude ‚Äî uncover vulnerabilities, parse manifests, and reverse engineer effortlessly.\n\n![GitHub contributors apktool-mcp-server](https://img.shields.io/github/contributors/zinja-coder/apktool-mcp-server)\n![GitHub all releases](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/total)\n![GitHub release (latest by SemVer)](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/latest/total)\n![Latest release](https://img.shields.io/github/release/zinja-coder/apktool-mcp-server.svg)\n![Python 3.10+](https://img.shields.io/badge/python-3%2E10%2B-blue)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n</div>\n\n<!-- It is a still in early stage of development, so expects bugs, crashes and logical erros.-->\n\n<div align=\"center\">\n    <img alt=\"banner\" height=\"480px\" widht=\"620px\" src=\"https://github.com/user-attachments/assets/eb9037f2-d1c7-45e0-8871-ca8aaade0dd0\">\n</div>\n\n<!--![apktool-mcp-server-banner.png](docs/assets/apktool-mcp-server-banner.png)-->\n\nImage generated using AI tools.\n\n---\n\n## ü§ñ What is apktool-mcp-server?\n\n**apktool-mcp-server** is a MCP server for the [Apk Tool](https://github.com/iBotPeaches/apktool) that integrates directly with [Model Context Protocol (MCP)](https://github.com/anthropic/mcp) to provide **live reverse engineering support with LLMs like Claude**.\n\nThink: \"Decompile ‚Üí Context-Aware Code Review ‚Üí AI Recommendations\" ‚Äî all in real time.\n\nWatch the demo!\n\nhttps://github.com/user-attachments/assets/d50251b8-6b1c-4341-b18e-ae54eb24a847\n\n- **Solving the CTFs**\n\n\n\nhttps://github.com/user-attachments/assets/c783a604-a636-4e70-9fa8-37e3d219b20b\n\n\n## Other projects in Zin MCP Suite\n - **[JADX-AI-MCP](https://github.com/zinja-coder/jadx-ai-mcp)**\n - **[JADX-MCP-Server](https://github.com/zinja-coder/jadx-mcp-server)**\n - **[ZIN-MCP-Client](https://github.com/zinja-coder/zin-mcp-client)**\n\n## Current MCP Tools\n\nThe following MCP tools are available:\n\n- `build_apk()` ‚Äî Build an APK from a decoded APKTool Project.\n- `get_manifest()` ‚Äî Get the AndroidManifest.xml content from a decoded APK project. \n- `get_apktool_yml()` ‚Äî Get apktool.yml information from a decoded APK project. \n- `list_smali_directories()` ‚Äî List all smali directories in a project. \n- `list_smali_files()` ‚Äî List smali files in a specific smali directory, optinally filtered by package prefix.\n- `get_smali_file()` ‚Äî Get content of a specific smali file by class name. \n- `modify_smali_file()` ‚Äî Modify the content of a specific smali file. \n- `list_resources()` ‚Äî List resources in a project, optionally filtered by resource type. \n- `get_resource_file()` ‚Äî Get Content of a specific resource file. \n- `modify_resource_file()` ‚Äî Modify the content of a specific resource file. \n- `search_in_file()` ‚Äî Search for a pattern in files with specified extensions. \n- `clean_project()` ‚Äî Clean a project directory to prepare for rebuilding.\n- `decode_apk()` ‚Äî Decode an APK file using APKTool, extracting resources and smali code. \n\n---\n\n## üóíÔ∏è Sample Prompts\n\n\n### üîç Basic Code Understanding\n\n- ‚ÄúList all smali directories for the dvac project.‚Äù\n\n- ‚ÄúShow me all the smali files under the package prefix com.vulnerable.component in the dvac project.‚Äù\n\n- ‚ÄúGet the smali code for the class com.vulnerable.component.MainActivity.‚Äù\n\n- ‚ÄúCompare MainActivity.smali with its previous version and show differences.‚Äù\n\n- ‚ÄúSearch for usage of startActivity in smali files of dvac project.‚Äù\n\n### üõ°Ô∏è Vulnerability Detection\n\n- ‚ÄúAnalyze declared permissions in the dvac AndroidManifest.xml and flag dangerous ones.‚Äù\n\n- ‚ÄúSearch for hardcoded URLs or IPs in all .xml and .smali files in the project.‚Äù\n\n- ‚ÄúFind all uses of PendingIntent.getActivity in smali files.‚Äù\n\n- ‚ÄúCheck for exported activities or receivers in dvac‚Äôs AndroidManifest.xml.‚Äù\n\n- ‚ÄúList all smali files that access android.permission.SEND_SMS or READ_CONTACTS.‚Äù\n\n### üõ†Ô∏è Reverse Engineering Helpers\n\n- ‚ÄúDecode this APK: dvac.apk and create a project called dvac.‚Äù\n\n- ‚ÄúCreate a new APKTool project called test-harness.‚Äù\n\n- ‚ÄúClean the dvac project before rebuild.‚Äù\n\n- ‚ÄúExtract DEX files from dvac project for external analysis.‚Äù\n\n- ‚ÄúModify MainActivity.smali to insert a log line at the beginning of onCreate().‚Äù\n\n### üì¶ Static Analysis\n\n- ‚ÄúGet the complete AndroidManifest.xml from dvac project.‚Äù\n\n- ‚ÄúShow the contents of apktool.yml for the dvac project.‚Äù\n\n- ‚ÄúList all resource files of type layout.‚Äù\n\n- ‚ÄúSearch for the word password in all resource and smali files.‚Äù\n\n- ‚ÄúCheck which permissions are used and compare them against typical over-permissioning risks.‚Äù\n\n### ü§ñ AI Code Modification\n\n- ‚ÄúModify the onCreate() method in MainActivity.smali to add a toast message.‚Äù\n\n- ‚ÄúReplace all http:// links with https:// in strings.xml.‚Äù\n\n- ‚ÄúAdd the android:exported=false attribute to all activities in the AndroidManifest.xml.‚Äù\n\n- ‚ÄúPatch the method validateLogin in LoginManager.smali to always return true.‚Äù\n\n- ‚ÄúAdd logging statements to every method in MainActivity.smali.‚Äù\n\n### üìÑ Documentation & Metadata\n\n- ‚ÄúList all decoded APKTool projects in the workspace.‚Äù\n\n- ‚ÄúShow me the apktool.yml config to review the version, original APK metadata, and compression settings.‚Äù\n\n- ‚ÄúGet all available Android devices connected via ADB. (To be migrated to ADB MCP Server.)‚Äù\n\n- ‚ÄúGet metadata about the project dvac from its apktool.yml.‚Äù\n\n- ‚ÄúCheck which APKTool version is currently installed on the server.‚Äù\n---\n\n## üõ†Ô∏è Getting Started \n### 1. Downlaod from Releases: https://github.com/zinja-coder/apktool-mcp-server/releases\n\n```bash\n# 0. Download and install apktool\nhttps://apktool.org/docs/install\n\n# 1. Test whether apktool has been correctly configured in the environment variables\n$ apktool -version\n\n# 2. Download the apktool-mcp-server-<version>.zip\nhttps://github.com/zinja-coder/apktool-mcp-server/releases\n\n# 3. \nunzip apktool-mcp-server-<version>.zip\n\n‚îúapktool-mcp-server/\n  ‚îú‚îÄ‚îÄ apktool_mcp_server.py\n  ‚îú‚îÄ‚îÄ requirements.txt\n  ‚îú‚îÄ‚îÄ README.md\n  ‚îú‚îÄ‚îÄ LICENSE\n\n# 4. Navigate to apktool-mcp-server directory\ncd apktool-mcp-server\n\n# 5. This project uses uv - https://github.com/astral-sh/uv instead of pip for dependency management.\n    ## a. Install uv (if you dont have it yet)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n    ## b. OPTIONAL, if for any reasons, you get dependecy errors in apktool-mcp-server, Set up the environment\nuv venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n    ## c. OPTIONAL Install dependencies\nuv pip install httpx fastmcp\n\n# The setup for apktool-mcp-server is done.\n```\n\n## 2. Running on Local LLM Using Ollama and Zin MCP Client - Recommended\n\n<div align=\"center\">\n    <a href=\"https://github.com/zinja-coder/zin-mcp-client\">\n    <img alt=\"zin-mcp-client\" height=\"360px\" widht=\"480px\" src=\"https://github.com/user-attachments/assets/0e8e0ecd-0520-422e-a007-03dc62c4118e\">\n    </a>\n</div>\n\n‚ö° Lightweight, Fast, Simple, CLI-Based MCP Client for STDIO MCP Servers, to fill the gap and provide bridge between your local LLMs running Ollama and MCP Servers.\n\nCheck Now: https://github.com/zinja-coder/zin-mcp-client\n\nDemo: Coming soon...\n\n## ü§ñ 3. Claude Desktop Setup\n\nMake sure Claude Desktop is running with MCP enabled.\n\nFor instance, I have used following for Kali Linux: https://github.com/aaddrick/claude-desktop-debian\n\nConfigure and add MCP server to LLM file:\n```bash\nnano ~/.config/Claude/claude_desktop_config.json\n```\n\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAnd following content in it:\n```json\n{\n    \"mcpServers\": {\n        \"apktool-mcp-server\": {\n            \"command\": \"/<path>/<to>/uv\", \n            \"args\": [\n                \"--directory\",\n                \"</PATH/TO/>apktool-mcp-server/\",\n                \"run\",\n                \"apktool_mcp_server.py\"\n            ]\n        }\n    }\n}\n```\n\nReplace:\n\n- `path/to/uv` with the actual path to your `uv` executable\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\nThen, navigate code and interact via real-time code review prompts using the built-in integration.\n\n## 4. Cherry Studio Setup\n\nIf you want to configure the MCP tool in Cherry Studio, you can refer to the following configuration.\n- Type: stdio\n- command: uv\n- argument:\n```bash\n--directory\npath/to/apktool-mcp-server\nrun\napktool_mcp_server.py\n```\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\n## To report bugs, issues, feature suggestion, Performance issue, general question, Documentation issue.\n - Kindly open an issue with respective template.\n\n - Tested on Claude Desktop Client, support for other AI will be tested soon!\n\n## üôè Credits\n\nThis project is a MCP Server for [Apktool](https://github.com/iBotPeaches/apktool), an amazing open-source Android reverse engineering tool created and maintained by [@iBotPeaches](https://github.com/iBotPeaches). All core APK decoding and resource processing logic belongs to them. I have only extended it to support my MCP server with AI capabilities.\n\n[üìé Original README (Apktool)](https://github.com/iBotPeaches/apktool)\n\nThe original README.md from Apktool is included here in this repository for reference and credit.\n\nAlso huge thanks to [@aaddrick](https://github.com/aaddrick) for developing Claude desktop for Debian based Linux.\n\nAnd in last, thanks to [@anthropics](https://github.com/anthropics) for developing the Model Context Protocol and [@FastMCP](https://github.com/jlowin/fastmcp) team.\n\nAnd all open source project maintainers and contributos which provies libraries and dependencies to make project like this possible.\n\n## üìÑ License\n\napktool-mcp-server and all related projects inherits the Apache 2.0 \n\n## ‚öñÔ∏è Legal Warning\n\n**Disclaimer**\n\nThe tools `apktool-mcp-server` and all related tools under this project are intended strictly for educational, research, and ethical security assessment purposes. They are provided \"as-is\" without any warranties, expressed or implied. Users are solely responsible for ensuring that their use of these tools complies with all applicable laws, regulations, and ethical guidelines.\n\nBy using `apktool-mcp-server`, you agree to use them only in environments you are authorized to test, such as applications you own or have explicit permission to analyze. Any misuse of these tools for unauthorized reverse engineering, infringement of intellectual property rights, or malicious activity is strictly prohibited.\n\nThe developers of `apktool-mcp-server` shall not be held liable for any damage, data loss, legal consequences, or other consequences resulting from the use or misuse of these tools. Users assume full responsibility for their actions and any impact caused by their usage.\n\nUse responsibly. Respect intellectual property. Follow ethical hacking practices.\n\n---\n\n## üôå Contribute or Support\n\n## Contributing\n\n[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](CONTRIBUTE.md)\n\n- Found it useful? Give it a ‚≠êÔ∏è\n- Got ideas? Open an [issue](https://github.com/zinja-coder/apktool-mcp-server/issues) or submit a PR\n- Built something on top? DM me or mention me ‚Äî I‚Äôll add it to the README!\n\n---\n## Audited and Received Assessment Badge\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zinja-coder-apktool-mcp-server-badge.png)](https://mseep.ai/app/zinja-coder-apktool-mcp-server)\n\nThank you Mseep.net for auditing and providing Assessment Badge.\n---\n\nBuilt with ‚ù§Ô∏è for the reverse engineering and AI communities.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "android",
        "apktool",
        "apks",
        "engineering android",
        "android apks",
        "coder apktool"
      ],
      "category": "scientific-research-tools"
    },
    "zongmin-yu--semantic-scholar-fastmcp-mcp-server": {
      "owner": "zongmin-yu",
      "name": "semantic-scholar-fastmcp-mcp-server",
      "url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zongmin-yu.webp",
      "description": "Provides access to academic paper data, author information, and citation networks through the Semantic Scholar API. Facilitates interaction with various academic resources for research purposes.",
      "stars": 63,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:36:41Z",
      "readme_content": "# Semantic Scholar MCP Server\n\n[![smithery badge](https://smithery.ai/badge/semantic-scholar-fastmcp-mcp-server)](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server)\n\nA FastMCP server implementation for the Semantic Scholar API, providing comprehensive access to academic paper data, author information, and citation networks.\n\n## Project Structure\n\nThe project has been refactored into a modular structure for better maintainability:\n\n```\nsemantic-scholar-server/\n‚îú‚îÄ‚îÄ semantic_scholar/            # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Package initialization\n‚îÇ   ‚îú‚îÄ‚îÄ server.py               # Server setup and main functionality\n‚îÇ   ‚îú‚îÄ‚îÄ mcp.py                  # Centralized FastMCP instance definition\n‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration classes\n‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Utility modules\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.py           # Error handling\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ http.py             # HTTP client and rate limiting\n‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API endpoints\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ papers.py           # Paper-related endpoints\n‚îÇ       ‚îú‚îÄ‚îÄ authors.py          # Author-related endpoints\n‚îÇ       ‚îî‚îÄ‚îÄ recommendations.py  # Recommendation endpoints\n‚îú‚îÄ‚îÄ run.py                      # Entry point script\n```\n\nThis structure:\n\n- Separates concerns into logical modules\n- Makes the codebase easier to understand and maintain\n- Allows for better testing and future extensions\n- Keeps related functionality grouped together\n- Centralizes the FastMCP instance to avoid circular imports\n\n## Features\n\n- **Paper Search & Discovery**\n\n  - Full-text search with advanced filtering\n  - Title-based paper matching\n  - Paper recommendations (single and multi-paper)\n  - Batch paper details retrieval\n  - Advanced search with ranking strategies\n\n- **Citation Analysis**\n\n  - Citation network exploration\n  - Reference tracking\n  - Citation context and influence analysis\n\n- **Author Information**\n\n  - Author search and profile details\n  - Publication history\n  - Batch author details retrieval\n\n- **Advanced Features**\n  - Complex search with multiple ranking strategies\n  - Customizable field selection\n  - Efficient batch operations\n  - Rate limiting compliance\n  - Support for both authenticated and unauthenticated access\n  - Graceful shutdown and error handling\n  - Connection pooling and resource management\n\n## System Requirements\n\n- Python 3.8+\n- FastMCP framework\n- Environment variable for API key (optional)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Semantic Scholar MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server):\n\n```bash\nnpx -y @smithery/cli install semantic-scholar-fastmcp-mcp-server --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/YUZongmin/semantic-scholar-fastmcp-mcp-server.git\ncd semantic-scholar-server\n```\n\n2. Install FastMCP and other dependencies following: https://github.com/jlowin/fastmcp\n\n3. Configure FastMCP:\n\nFor Claude Desktop users, you'll need to configure the server in your FastMCP configuration file. Add the following to your configuration (typically in `~/.config/claude-desktop/config.json`):\n\n```json\n{\n  \"mcps\": {\n    \"Semantic Scholar Server\": {\n      \"command\": \"/path/to/your/venv/bin/fastmcp\",\n      \"args\": [\n        \"run\",\n        \"/path/to/your/semantic-scholar-server/run.py\"\n      ],\n      \"env\": {\n        \"SEMANTIC_SCHOLAR_API_KEY\": \"your-api-key-here\"  # Optional\n      }\n    }\n  }\n}\n```\n\nMake sure to:\n\n- Replace `/path/to/your/venv/bin/fastmcp` with the actual path to your FastMCP installation\n- Replace `/path/to/your/semantic-scholar-server/run.py` with the actual path to run.py on your machine\n- If you have a Semantic Scholar API key, add it to the `env` section. If not, you can remove the `env` section entirely\n\n4. Start using the server:\n\nThe server will now be available to your Claude Desktop instance. No need to manually run any commands - Claude will automatically start and manage the server process when needed.\n\n### API Key (Optional)\n\nTo get higher rate limits and better performance:\n\n1. Get an API key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n2. Add it to your FastMCP configuration as shown above in the `env` section\n\nIf no API key is provided, the server will use unauthenticated access with lower rate limits.\n\n## Configuration\n\n### Environment Variables\n\n- `SEMANTIC_SCHOLAR_API_KEY`: Your Semantic Scholar API key (optional)\n  - Get your key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n  - If not provided, the server will use unauthenticated access\n\n### Rate Limits\n\nThe server automatically adjusts to the appropriate rate limits:\n\n**With API Key**:\n\n- Search, batch and recommendation endpoints: 1 request per second\n- Other endpoints: 10 requests per second\n\n**Without API Key**:\n\n- All endpoints: 100 requests per 5 minutes\n- Longer timeouts for requests\n\n## Available MCP Tools\n\n> Note: All tools are aligned with the official [Semantic Scholar API documentation](https://api.semanticscholar.org/api-docs/). Please refer to the official documentation for detailed field specifications and the latest updates.\n\n### Paper Search Tools\n\n- `paper_relevance_search`: Search for papers using relevance ranking\n\n  - Supports comprehensive query parameters including year range and citation count filters\n  - Returns paginated results with customizable fields\n\n- `paper_bulk_search`: Bulk paper search with sorting options\n\n  - Similar to relevance search but optimized for larger result sets\n  - Supports sorting by citation count, publication date, etc.\n\n- `paper_title_search`: Find papers by exact title match\n\n  - Useful for finding specific papers when you know the title\n  - Returns detailed paper information with customizable fields\n\n- `paper_details`: Get comprehensive details about a specific paper\n\n  - Accepts various paper ID formats (S2 ID, DOI, ArXiv, etc.)\n  - Returns detailed paper metadata with nested field support\n\n- `paper_batch_details`: Efficiently retrieve details for multiple papers\n  - Accepts up to 1000 paper IDs per request\n  - Supports the same ID formats and fields as single paper details\n\n### Citation Tools\n\n- `paper_citations`: Get papers that cite a specific paper\n\n  - Returns paginated list of citing papers\n  - Includes citation context when available\n  - Supports field customization and sorting\n\n- `paper_references`: Get papers referenced by a specific paper\n  - Returns paginated list of referenced papers\n  - Includes reference context when available\n  - Supports field customization and sorting\n\n### Author Tools\n\n- `author_search`: Search for authors by name\n\n  - Returns paginated results with customizable fields\n  - Includes affiliations and publication counts\n\n- `author_details`: Get detailed information about an author\n\n  - Returns comprehensive author metadata\n  - Includes metrics like h-index and citation counts\n\n- `author_papers`: Get papers written by an author\n\n  - Returns paginated list of author's publications\n  - Supports field customization and sorting\n\n- `author_batch_details`: Get details for multiple authors\n  - Efficiently retrieve information for up to 1000 authors\n  - Returns the same fields as single author details\n\n### Recommendation Tools\n\n- `paper_recommendations_single`: Get recommendations based on a single paper\n\n  - Returns similar papers based on content and citation patterns\n  - Supports field customization for recommended papers\n\n- `paper_recommendations_multi`: Get recommendations based on multiple papers\n  - Accepts positive and negative example papers\n  - Returns papers similar to positive examples and dissimilar to negative ones\n\n## Usage Examples\n\n### Basic Paper Search\n\n```python\nresults = await paper_relevance_search(\n    context,\n    query=\"machine learning\",\n    year=\"2020-2024\",\n    min_citation_count=50,\n    fields=[\"title\", \"abstract\", \"authors\"]\n)\n```\n\n### Paper Recommendations\n\n```python\n# Single paper recommendation\nrecommendations = await paper_recommendations_single(\n    context,\n    paper_id=\"649def34f8be52c8b66281af98ae884c09aef38b\",\n    fields=\"title,authors,year\"\n)\n\n# Multi-paper recommendation\nrecommendations = await paper_recommendations_multi(\n    context,\n    positive_paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    negative_paper_ids=[\"ArXiv:1805.02262\"],\n    fields=\"title,abstract,authors\"\n)\n```\n\n### Batch Operations\n\n```python\n# Get details for multiple papers\npapers = await paper_batch_details(\n    context,\n    paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    fields=\"title,authors,year,citations\"\n)\n\n# Get details for multiple authors\nauthors = await author_batch_details(\n    context,\n    author_ids=[\"1741101\", \"1780531\"],\n    fields=\"name,hIndex,citationCount,paperCount\"\n)\n```\n\n## Error Handling\n\nThe server provides standardized error responses:\n\n```python\n{\n    \"error\": {\n        \"type\": \"error_type\",  # rate_limit, api_error, validation, timeout\n        \"message\": \"Error description\",\n        \"details\": {\n            # Additional context\n            \"authenticated\": true/false  # Indicates if request was authenticated\n        }\n    }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "academic",
        "research",
        "scholar api",
        "semantic scholar",
        "scholar fastmcp"
      ],
      "category": "scientific-research-tools"
    },
    "zygi--r-playground-mcp": {
      "owner": "zygi",
      "name": "r-playground-mcp",
      "url": "https://github.com/zygi/r-playground-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zygi.webp",
      "description": "Executes R code, visualizes plots, and interacts with scientific data within stateful sessions. Supports multimodal outputs to enhance conversations around scientific topics.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-06T20:19:18Z",
      "readme_content": "# MCP R Playground\n[![PyPI version](https://img.shields.io/pypi/v/rplayground-mcp.svg)](https://pypi.org/project/rplayground-mcp/)\n[![smithery badge](https://smithery.ai/badge/@zygi/r-playground-mcp)](https://smithery.ai/server/@zygi/r-playground-mcp)\n\nAn MCP server that allows AI models to execute R code, see its results, and draw and observe plots. \nIt can be used for sophisticated agentic deployments, but also as a way to augment AI clients like Claude Desktop when talking to them about scientific papers. \n\n## Features:\n- Stateful sessions: each conversation thread gets a new session, but the session can persist across calls and user/assistant interactions. \n- Graphics output: multimodal models can draw plots using standard R libraries like ggplot, see those plots, and react to them.\n- Works in all common operating systems/architectures - Windows x64 / arm64, MacOS, Linux.\n- When running locally, ‚ö†Ô∏è__NO HOST ISOLATION__‚ö†Ô∏è: while each session runs as a separate R environment, they have access to global dependencies and all files on the computer. While unlikely, a rogue model could write R code that deletes your important files.\n    - If you need host isolation, you should run this MCP in Docker. Instructions for that are provided below.\n\n## Configuration\nCurrently there's just one configuration parameter that can be set as an environment variable:\n- `RPLAYGROUND_MCP_SUPPORT_IMAGE_OUTPUT`, default True. If set to False, image output will be disabled, and tool descriptions will be made to reflect that.\n\n## Usage\nBy default, the AIs will have access to all globally installed R packages, and can install whatever package they want. These installations will persist. You can pre-install important packages to make them available in advance.\n\n\n## Installation\nBasic instructions for technical users:\n1) Have R installed, and the R_HOME environment variable set\n2) Have a recent version of the `uv` installed\n3) run `uvx --python=3.13 rplayground-mcp`, and it should just work.\n\n## Detailed Installation\nThis section is for less technical users who want to set up this MCP to use with Claude Desktop or similar AI user interfaces that support MCP extensions.\n\n### Windows\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n\n### MacOS\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n## Installation (Docker)\nWe also provide Dockerfiles to run this MCP in an isolated context. This only supports the platform linux/amd64. \nThe image uses the [`r2u`](https://github.com/eddelbuettel/r2u) project to make precompiled CRAN packages available. The Dockerfile comes with two versions:\n- Slim: `docker build --platform=linux/amd64 .`. This sets up the `r2u` repositories but installing a new package from the R session will still take ~10s.\n- Fat: `docker build --build-arg PREINSTALL_PACKAGES=true --platform=linux/amd64 .`. This preinstalls a big selection of packages to the Docker image, and loading them in an MCP session becomes instant.\n\nYou are also welcome to edit the Dockerfile and preinstall just the packages relevant to your workflow.\n\n## Issues and Contributions\nFeel free to create an Issue if you have questions or requests. Small PRs are welcome anytime, larger PRs should be discussed by creating an Issue before a PR is started. \n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "multimodal",
        "zygi",
        "tools",
        "tools zygi",
        "research tools",
        "interacts scientific"
      ],
      "category": "scientific-research-tools"
    },
    "zzaebok--mcp-wikidata": {
      "owner": "zzaebok",
      "name": "mcp-wikidata",
      "url": "https://github.com/zzaebok/mcp-wikidata",
      "imageUrl": "/freedevtools/mcp/pfp/zzaebok.webp",
      "description": "Interact with Wikidata to search for entities, extract metadata, and execute SPARQL queries for enhanced data insights.",
      "stars": 29,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T01:29:15Z",
      "readme_content": "## Wikidata MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@zzaebok/mcp-wikidata)](https://smithery.ai/server/@zzaebok/mcp-wikidata)\n\nA server implementation for Wikidata API using the Model Context Protocol (MCP).\nThis project provides tools to interact with Wikidata, such as **searching identifiers** (entity and property), **extracting metadata** (label and description) and **executing sparql query**.\n\n---\n\n### Installation\n\n#### Installing via Smithery\n\nTo install Wikidata MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zzaebok/mcp-wikidata):\n\n```bash\nnpx -y @smithery/cli install @zzaebok/mcp-wikidata --client claude\n```\n\n#### Installing Manually\nInstall `uv` if it is not installed yet.\n\n```bash\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nThen, install dependencies.\n\n```bash\n$ git clone https://github.com/zzaebok/mcp-wikidata.git\n$ cd mcp-wikidata\n$ uv sync\n# if you want to run client example together\n$ uv sync --extra example\n```\n\n---\n\n### Run\n\nRun the server with:\n\n```bash\n$ uv run src/server.py\n```\n\nIf you want to test it with a simple client code (with `langchain-mcp-adapters`), run the client with:\n\n```bash\n# in another shell\n$ uv run src/client.py\n```\n\nThe LLM extracts valid entity and property identifiers, executes a sparql query, and finally recommend a movie directed by Bong Joon-ho.\n\n<details>\n  <summary><i>See the execution output</i></summary>\n\n```python\n{\n  \"messages\": [\n      HumanMessage(\n          content=\"Can you recommend me a movie directed by Bong Joonho?\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_entity\",\n                  \"args\": {\"query\": \"Bong Joon-ho\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"Q495980\",\n          name=\"search_entity\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"get_properties\",\n                  \"args\": {\"entity_id\": \"Q495980\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[\"P345\", \"P244\", \"P214\", \"P227\", ...]',\n          name=\"get_properties\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_property\",\n                  \"args\": {\"query\": \"director\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"P57\",\n          name=\"search_property\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"execute_sparql\",\n                  \"args\": {\n                      \"sparql_query\": 'SELECT ?film ?filmLabel WHERE {\\n  ?film wdt:P57 wd:Q495980.\\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\\n} LIMIT 1'\n                  },\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[{\"film\": {\"type\": \"uri\", \"value\": \"http://www.wikidata.org/entity/Q483761\"}, \"filmLabel\": {\"xml:lang\": \"en\", \"type\": \"literal\", \"value\": \"Mother\"}}]',\n          name=\"execute_sparql\",\n      ),\n      AIMessage(\n          content='I recommend the movie \"Mother,\" which was directed by Bong Joon-ho.',\n      ),\n  ]\n}\n```\n\n</details>\n\n---\n\n### Wikidata MCP Tools\n\nThe following tools are implemented in the server:\n\n| Tool                                                 | Description                                                                |\n| ---------------------------------------------------- | -------------------------------------------------------------------------- |\n| `search_entity(query: str)`                          | Search for a Wikidata entity ID by its query.                              |\n| `search_property(query: str)`                        | Search for a Wikidata property ID by its query.                            |\n| `get_properties(entity_id: str)`                     | Get the properties associated with a given Wikidata entity ID.             |\n| `execute_sparql(sparql_query: str)`                  | Execute a SPARQL query on Wikidata.                                        |\n| `get_metadata(entity_id: str, language: str = \"en\")` | Retrieve the English label and description for a given Wikidata entity ID. |\n\n---\n\n#### License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wikidata",
        "metadata",
        "sparql",
        "wikidata search",
        "wikidata interact",
        "mcp wikidata"
      ],
      "category": "scientific-research-tools"
    }
  }
}