{
  "category": "realtime-collaboration",
  "categoryDisplay": "Real-Time Collaboration",
  "description": "",
  "totalRepositories": 27,
  "repositories": {
    "1313057--TEN-Agent": {
      "owner": "1313057",
      "name": "TEN-Agent",
      "url": "https://github.com/1313057/TEN-Agent",
      "imageUrl": "/freedevtools/mcp/pfp/1313057.webp",
      "description": "TEN Agent is an open-source server designed to enhance AI models by integrating various features that allow them to process multiple types of data and respond in real-time.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-12-18T16:43:07Z",
      "readme_content": "<div align=\"center\">\n\n[![Follow on X](https://img.shields.io/twitter/follow/TenFramework?logo=X&color=%20%23f5f5f5)](https://twitter.com/intent/follow?screen_name=TenFramework)\n[![Discussion posts](https://img.shields.io/github/discussions/TEN-framework/ten-agent?labelColor=%20%23FDB062&color=%20%23f79009)](https://github.com/TEN-framework/ten-agent/discussions/)\n[![Commits](https://img.shields.io/github/commit-activity/m/TEN-framework/ten-agent?labelColor=%20%237d89b0&color=%20%235d6b98)](https://github.com/TEN-framework/ten-agent/graphs/commit-activity)\n[![Issues closed](https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-agent%20is%3Aclosed&label=issues%20closed&labelColor=green&color=green)](https://github.com/TEN-framework/ten-agent/issues)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/TEN-framework/ten-agent/pulls)\n[![GitHub license](https://img.shields.io/badge/License-Apache_2.0-blue.svg?labelColor=%20%23155EEF&color=%20%23528bff)](https://github.com/TEN-framework/ten-agent/blob/main/LICENSE)\n\n[](https://discord.gg/VnPftUzAMJ)\n\n<a href=\"https://trendshift.io/repositories/11978\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11978\" alt=\"TEN-framework%2FTEN-Agent | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![GitHub watchers](https://img.shields.io/github/watchers/TEN-framework/ten-agent?style=social&label=Watch)](https://GitHub.com/TEN-framework/ten-agent/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/TEN-framework/ten-agent?style=social&label=Fork)](https://GitHub.com/TEN-framework/ten-agent/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/TEN-framework/ten-agent?style=social&label=Star)](https://GitHub.com/TEN-framework/ten-agent/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n<a href=\"https://github.com/TEN-framework/ten-agent/blob/main/README.md\"><img alt=\"README in English\" src=\"https://img.shields.io/badge/English-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-CN.md\"><img alt=\"简体中文操作指南\" src=\"https://img.shields.io/badge/简体中文-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-JP.md\"><img alt=\"日本語のREADME\" src=\"https://img.shields.io/badge/日本語-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-KR.md\"><img alt=\"README in 한국어\" src=\"https://img.shields.io/badge/한국어-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-ES.md\"><img alt=\"README en Español\" src=\"https://img.shields.io/badge/Español-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-FR.md\"><img alt=\"README en Français\" src=\"https://img.shields.io/badge/Français-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-IT.md\"><img alt=\"README in Italiano\" src=\"https://img.shields.io/badge/Italiano-lightgrey\"></a>\n\n[Getting Started](https://doc.theten.ai/ten-agent/getting_started)\n<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n[Create Extensions](https://doc.theten.ai/ten-agent/create_a_hello_world_extension)\n<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n[TEN Framework Repository](https://github.com/TEN-framework/ten_framework)\n\n</div>\n\n<br>\n<h2>🌟 Gemini Multimodal Live API Extension with RTC</h2>\n<!-- \n![Usecases](https://github.com/TEN-framework/docs/blob/main/assets/jpg/gemini-with-ten.jpg?raw=true) -->\n\n\n\n[agent.theten.ai](https://agent.theten.ai)\n\nTry **Google Gemini Multimodal Live API** with **realtime vision** and **realtime screenshare detection** capabilities, it is a ready-to-use extension, along with powerful tools like **Weather Check** and **Web Search** integrated perfectly into TEN Agent.\n\n\n<br>\n<h2>TEN Agent Usecases</h2>\n\n\n\n<br>\n<h2>Ready-to-use Extensions</h2>\n\n\n\n\n<br>\n<h2>TEN Agent Playground in Local Environment</h2>\n\n### Prerequisites\n\n| Category | Requirements |\n|----------|-------------|\n| **Keys** | • Agora [ App ID ](https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project) and [ App Certificate ](https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project)(free minutes every month) <br>• [OpenAI](https://openai.com/index/openai-api/) API key<br>• [ Deepgram ](https://deepgram.com/) ASR (free credits available with signup)<br>• [ FishAudio ](https://fish.audio/) TTS (free credits available with signup)|\n| **Installation** | • [Docker](https://www.docker.com/) / [Docker Compose](https://docs.docker.com/compose/)<br>• [Node.js(LTS) v18](https://nodejs.org/en) |\n| **Minimum System Requirements** | • CPU >= 2 Core<br>• RAM >= 4 GB |\n\n<br>\n\n### macOS: Docker setting on Apple Silicon\n\nFor Apple Silicon Macs, uncheck \"Use Rosetta for x86/amd64 emulation\" in Docker settings. Note: This may result in slower build times on ARM, but performance will be normal when deployed to x64 servers.\n\n\n\n<br>\n\n### Next step\n\n#### 1. Create `.env` file\n\n```bash\ncp ./.env.example ./.env\n```\n\n#### 2. Setup Agora App ID and App Certificate in `.env`\n\n```bash\nAGORA_APP_ID=\nAGORA_APP_CERTIFICATE=\n```\n\n#### 3. Start agent development containers\n```bash\ndocker compose up -d\n```\n\n#### 4. Enter container\n```bash\ndocker exec -it ten_agent_dev bash\n```\n\n#### 5. Build agent \n```bash\ntask use\n```\n\n#### 6. Start the web server\n```bash\ntask run\n```\n\n#### 7. Edit playground settings\nOpen the playground at [localhost:3000](http://localhost:3000) to configure your agent.\n 1. Select a graph type (e.g. Voice Agent, Realtime Agent)\n 2. Choose a corresponding module\n 3. Select an extension and configure its API key settings\n\n\n\n#### Running Gemini Realtime Extension\nOpen the playground at [localhost:3000](http://localhost:3000).\n\n 1. Select voice_assistant_realtime graph\n 2. Choose Gemini Realtime module\n 3. Select v2v extension and enter Gemini API key\n\n\n\n<br>\n<h2>TEN Agent Components</h2>\n\n\n\n<br>\n<h2>Stay Tuned</h2>\n\nBefore we get started, be sure to star our repository and get instant notifications for all new releases!\n\n\n\n<br>\n<h2>Join Community</h2>\n\n- [Discord](https://discord.gg/VnPftUzAMJ): Ideal for sharing your applications and engaging with the community.\n- [GitHub Discussion](https://github.com/TEN-framework/ten-agent/discussions): Perfect for providing feedback and asking questions.\n- [GitHub Issues](https://github.com/TEN-framework/ten-agent/issues): Best for reporting bugs and proposing new features. Refer to our [contribution guidelines](./docs/code-of-conduct/contributing.md) for more details.\n- [X](https://img.shields.io/twitter/follow/TenFramework?logo=X&color=%20%23f5f5f5): Great for sharing your agents and interacting with the community.\n\n<br>\n<h2>Star History</h2>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=ten-framework/ten-agent&type=Date)](https://star-history.com/#ten-framework/ten-agent&Date)\n\n <br>\n <h2>Code Contributors</h2>\n\n[![TEN](https://contrib.rocks/image?repo=TEN-framework/ten-agent)](https://github.com/TEN-framework/ten-agent/graphs/contributors)\n\n<br>\n<h2>Contribution Guidelines</h2>\n\nContributions are welcome! Please read the [contribution guidelines](./docs/code-of-conduct/contributing.md) first.\n\n<br>\n<h2>License</h2>\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agent",
        "ai",
        "realtime",
        "realtime collaboration",
        "agent agent",
        "agent open"
      ],
      "category": "realtime-collaboration"
    },
    "Chandrakant0110--slack-mcp": {
      "owner": "Chandrakant0110",
      "name": "slack-mcp",
      "url": "https://github.com/Chandrakant0110/slack-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Chandrakant0110.webp",
      "description": "Interact with Slack by managing channels, posting messages, and retrieving user profiles through a standardized API interface. Enhance team collaboration and workflow automation within a Slack workspace.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-10T12:46:55Z",
      "readme_content": "# Slack MCP (Model Context Protocol) Server\n\nThis is a Slack MCP server implementation that provides various Slack API functionalities through the Model Context Protocol. It allows AI models to interact with Slack through a standardized interface.\n\n## Features\n\n- List public channels\n- Post messages\n- Reply to threads\n- Add reactions\n- Get channel history\n- Get thread replies\n- List users\n- Get user profiles\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm or yarn\n- A Slack workspace with admin access\n- A Slack Bot Token\n- Your Slack Team ID\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n\n```bash\nnpm install\n# or\nyarn install\n```\n\n3. Build the TypeScript code:\n\n```bash\nnpm run build\n# or\nyarn build\n```\n\n## Configuration\n\n1. Open `index.ts` and replace the placeholder values:\n\n```typescript\npublic static readonly BOT_TOKEN = \"enter-your-bot-token-here\";\npublic static readonly TEAM_ID = \"enter-your-team-id-here\";\n```\n\nReplace these with your actual Slack Bot Token and Team ID.\n\n## Usage\n\n### Running the Server\n\nAfter building the project, you can run the server:\n\n```bash\nnode dist/index.js\n```\n\n### Setting up in Cursor\n\nTo use this MCP server in Cursor:\n\n1. Open Cursor settings\n2. Navigate to the \"Model Context Protocol\" section\n3. Add a new tool with the following configuration:\n   - Name: `slack`\n   - Command: `node /path/to/your/dist/index.js`\n   - Working Directory: `/path/to/your/project`\n\nReplace `/path/to/your` with the actual path to your project directory.\n\n## Available Tools\n\n1. `slack_list_channels`\n   - Lists public channels in the workspace\n   - Optional parameters: limit, cursor\n\n2. `slack_post_message`\n   - Posts a message to a channel\n   - Required parameters: channel_id, text\n\n3. `slack_reply_to_thread`\n   - Replies to a message thread\n   - Required parameters: channel_id, thread_ts, text\n\n4. `slack_add_reaction`\n   - Adds an emoji reaction to a message\n   - Required parameters: channel_id, timestamp, reaction\n\n5. `slack_get_channel_history`\n   - Gets recent messages from a channel\n   - Required parameters: channel_id\n   - Optional parameters: limit\n\n6. `slack_get_thread_replies`\n   - Gets all replies in a thread\n   - Required parameters: channel_id, thread_ts\n\n7. `slack_get_users`\n   - Lists all users in the workspace\n   - Optional parameters: limit, cursor\n\n8. `slack_get_user_profile`\n   - Gets detailed profile information for a user\n   - Required parameters: user_id\n\n## Development\n\nTo modify the server:\n\n1. Make changes to `index.ts`\n2. Rebuild the project:\n```bash\nnpm run build\n# or\nyarn build\n```\n\n## Security Notes\n\n- Never commit your actual Slack Bot Token or Team ID to version control\n- Consider using environment variables for production deployments\n- Ensure your Slack Bot has the necessary OAuth scopes for the actions you want to perform\n\n## Contributing\n\nFeel free to submit issues and pull requests for improvements.\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "collaboration",
        "realtime",
        "interact slack",
        "slack managing",
        "automation slack"
      ],
      "category": "realtime-collaboration"
    },
    "ChatterBoxIO--chatterboxio-mcp-server": {
      "owner": "ChatterBoxIO",
      "name": "chatterboxio-mcp-server",
      "url": "https://github.com/ChatterBoxIO/chatterboxio-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ChatterBoxIO.webp",
      "description": "Integrates with online meeting platforms like Zoom and Google Meet to facilitate AI agents joining meetings, capturing transcripts, and generating concise summaries of discussions.",
      "stars": 7,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T03:17:05Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chatterboxio-chatterboxio-mcp-server-badge.png)](https://mseep.ai/app/chatterboxio-chatterboxio-mcp-server)\n\n# ChatterBox MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@OverQuotaAI/chatterboxio-mcp-server)](https://smithery.ai/server/@OverQuotaAI/chatterboxio-mcp-server)\n\nA Model Context Protocol server implementation for ChatterBox, enabling AI agents to interact with online meetings and generate meeting summaries.\n\n<a href=\"https://glama.ai/mcp/servers/@ChatterBoxIO/chatterboxio-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ChatterBoxIO/chatterboxio-mcp-server/badge\" alt=\"ChatterBox MCP Server\" />\n</a>\n\n## Overview\n\nThe ChatterBox MCP Server provides tools for AI agents to:\n\n- Join online meetings (Zoom, Google Meet, or Microsoft Teams)\n- Capture transcripts and recordings\n- Generate meeting summaries\n\n## Installation\n\n### Installing via Smithery\n\nTo install chatterboxio-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@OverQuotaAI/chatterboxio-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @OverQuotaAI/chatterboxio-mcp-server --client claude\n```\n\n### Manual Installation\n\nYou can install the dependencies using either npm or pnpm:\n\n```bash\n# Using npm\nnpm install\n\n# Using pnpm\npnpm install\n```\n\n## Configuration\n\n### Getting API Keys\n\nYou can get your API keys for free by registering on our website at [ChatterBox](https://chatter-box.io). After registration, you'll receive your API endpoint and key.\n\n### Environment Setup\n\nCreate a `.env` file in the root directory with the following variables:\n\n```env\nCHATTERBOX_API_ENDPOINT=https://api.chatter-box.io\nCHATTERBOX_API_KEY=your_api_key_here\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\npnpm start\n```\n\n### Available Tools\n\n#### joinMeeting\n\nJoin a Zoom or Google Meet meeting and capture transcript and audio recording.\n\n**Parameters:**\n\n- `platform` (string): The online conference platform (\"zoom\", \"googlemeet\", or \"teams\")\n- `meetingId` (string): The ID of the meeting\n- `meetingPassword` (string, optional): The password or the passcode for the meeting\n- `botName` (string): The name of the bot\n- `webhookUrl` (string, optional): URL to receive webhook events for meeting status\n\n#### getMeetingInfo\n\nGet information about a meeting, including transcript and recording.\n\n**Parameters:**\n\n- `sessionId` (string): The session ID to get information for\n\n#### summarizeMeeting\n\nGenerate a concise summary of a meeting's contents from its transcript.\n\n**Parameters:**\n\n- `transcript` (string): The meeting transcript to summarize\n\n## Development\n\n### Prerequisites\n\n- Node.js 16+\n- npm or yarn\n\n### Building\n\n```bash\npnpm run build\n```\n\n### Debugging\n\nTo debug the MCP server using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Support\n\nFor support, please visit [ChatterBox Documentation](https://chatter-box.io/documentation) or contact support@chatter-box.io.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatterboxio",
        "meetings",
        "meeting",
        "collaboration chatterboxio",
        "chatterboxio mcp",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "DynamicEndpoints--Autogen_MCP": {
      "owner": "DynamicEndpoints",
      "name": "Autogen_MCP",
      "url": "https://github.com/DynamicEndpoints/Autogen_MCP",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Facilitates the creation and management of AI agents that engage in natural language interactions and collaborate to solve problems. Supports orchestration of both individual and group conversations with customizable configurations and built-in error handling.",
      "stars": 15,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T01:15:10Z",
      "readme_content": "# Enhanced AutoGen MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/autogen_mcp)](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp)\n\nA comprehensive MCP server that provides deep integration with Microsoft's AutoGen framework v0.9+, featuring the latest capabilities including prompts, resources, advanced workflows, and enhanced agent types. This server enables sophisticated multi-agent conversations through a standardized Model Context Protocol interface.\n\n## 🚀 Latest Features (v0.2.0)\n\n### ✨ **Enhanced MCP Support**\n- **Prompts**: Pre-built templates for common workflows (code review, research, creative writing)\n- **Resources**: Real-time access to agent status, chat history, and configurations\n- **Dynamic Content**: Template-based prompts with arguments and embedded resources\n- **Latest MCP SDK**: Version 1.12.3 with full feature support\n\n### 🤖 **Advanced Agent Types**\n- **Assistant Agents**: Enhanced with latest LLM capabilities\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence\n- **Retrievable Agents**: Knowledge base integration\n- **Multimodal Agents**: Image and document processing (when available)\n\n### 🔄 **Sophisticated Workflows**\n- **Code Generation**: Architect → Developer → Reviewer → Executor pipeline\n- **Research Analysis**: Researcher → Analyst → Critic → Synthesizer workflow\n- **Creative Writing**: Multi-stage creative collaboration\n- **Problem Solving**: Structured approach to complex problems\n- **Code Review**: Security → Performance → Style review teams\n- **Custom Workflows**: Build your own agent collaboration patterns\n\n### 🎯 **Enhanced Chat Capabilities**\n- **Smart Speaker Selection**: Auto, manual, random, round-robin modes\n- **Nested Conversations**: Hierarchical agent interactions\n- **Swarm Intelligence**: Coordinated multi-agent problem solving\n- **Memory Management**: Persistent agent knowledge and preferences\n- **Quality Checks**: Built-in validation and improvement loops\n\n## 🛠️ Available Tools\n\n### Core Agent Management\n- `create_agent` - Create agents with advanced configurations\n- `create_workflow` - Build complete multi-agent workflows\n- `get_agent_status` - Detailed agent metrics and health monitoring\n\n### Conversation Execution\n- `execute_chat` - Enhanced two-agent conversations\n- `execute_group_chat` - Multi-agent group discussions\n- `execute_nested_chat` - Hierarchical conversation structures\n- `execute_swarm` - Swarm-based collaborative problem solving\n\n### Workflow Orchestration\n- `execute_workflow` - Run predefined workflow templates\n- `manage_agent_memory` - Handle agent learning and persistence\n- `configure_teachability` - Enable/configure agent learning capabilities\n\n## 📝 Available Prompts\n\n### `autogen-workflow`\nCreate sophisticated multi-agent workflows with customizable parameters:\n- **Arguments**: `task_description`, `agent_count`, `workflow_type`\n- **Use case**: Rapid workflow prototyping and deployment\n\n### `code-review`\nSet up collaborative code review with specialized agents:\n- **Arguments**: `code`, `language`, `focus_areas`\n- **Use case**: Comprehensive code quality assessment\n\n### `research-analysis`\nDeploy research teams for in-depth topic analysis:\n- **Arguments**: `topic`, `depth`\n- **Use case**: Academic research, market analysis, technical investigation\n\n## 📊 Available Resources\n\n### `autogen://agents/list`\nLive list of active agents with status and capabilities\n\n### `autogen://workflows/templates`\nAvailable workflow templates and configurations\n\n### `autogen://chat/history`\nRecent conversation history and interaction logs\n\n### `autogen://config/current`\nCurrent server configuration and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/autogen_mcp --client claude\n```\n\n### Manual Installation\n\n1. **Clone the repository:**\n```bash\ngit clone https://github.com/yourusername/autogen-mcp.git\ncd autogen-mcp\n```\n\n2. **Install Node.js dependencies:**\n```bash\nnpm install\n```\n\n3. **Install Python dependencies:**\n```bash\npip install -r requirements.txt --user\n```\n\n4. **Build the TypeScript project:**\n```bash\nnpm run build\n```\n\n5. **Set up configuration:**\n```bash\ncp .env.example .env\ncp config.json.example config.json\n# Edit .env and config.json with your settings\n```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file from the template:\n\n```bash\n# Required\nOPENAI_API_KEY=your-openai-api-key-here\n\n# Optional - Path to configuration file\nAUTOGEN_MCP_CONFIG=config.json\n\n# Enhanced Features\nENABLE_PROMPTS=true\nENABLE_RESOURCES=true\nENABLE_WORKFLOWS=true\nENABLE_TEACHABILITY=true\n\n# Performance Settings\nMAX_CHAT_TURNS=10\nDEFAULT_OUTPUT_FORMAT=json\n```\n\n### Configuration File\n\nUpdate `config.json` with your preferences:\n\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0.7\n  },\n  \"enhanced_features\": {\n    \"prompts\": { \"enabled\": true },\n    \"resources\": { \"enabled\": true },\n    \"workflows\": { \"enabled\": true }\n  }\n}\n```\n\n## Usage Examples\n\n### Using with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"autogen\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/autogen-mcp/build/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Command Line Testing\n\nTest the server functionality:\n\n```bash\n# Run comprehensive tests\npython test_server.py\n\n# Test CLI interface\npython cli_example.py create_agent \"researcher\" \"assistant\" \"You are a research specialist\"\npython cli_example.py execute_workflow \"code_generation\" '{\"task\":\"Hello world\",\"language\":\"python\"}'\n```\n\n### Using Prompts\n\nThe server provides several built-in prompts:\n\n1. **autogen-workflow** - Create multi-agent workflows\n2. **code-review** - Set up collaborative code review\n3. **research-analysis** - Deploy research teams\n\n### Accessing Resources\n\nAvailable resources provide real-time data:\n\n- `autogen://agents/list` - Current active agents\n- `autogen://workflows/templates` - Available workflow templates  \n- `autogen://chat/history` - Recent conversation history\n- `autogen://config/current` - Server configuration\n\n## Workflow Examples\n\n### Code Generation Workflow\n\n```json\n{\n  \"workflow_name\": \"code_generation\",\n  \"input_data\": {\n    \"task\": \"Create a REST API endpoint\",\n    \"language\": \"python\",\n    \"requirements\": [\"FastAPI\", \"Pydantic\", \"Error handling\"]\n  },\n  \"quality_checks\": true\n}\n```\n\n### Research Workflow\n\n```json\n{\n  \"workflow_name\": \"research\", \n  \"input_data\": {\n    \"topic\": \"AI Ethics in 2025\",\n    \"depth\": \"comprehensive\"\n  },\n  \"output_format\": \"markdown\"\n}\n```\n\n## Advanced Features\n\n### Agent Types\n\n- **Assistant Agents**: LLM-powered conversational agents\n- **User Proxy Agents**: Code execution and human interaction\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence (when available)\n- **Retrievable Agents**: Knowledge base integration (when available)\n\n### Chat Modes\n\n- **Two-Agent Chat**: Direct conversation between agents\n- **Group Chat**: Multi-agent discussions with smart speaker selection\n- **Nested Chat**: Hierarchical conversation structures  \n- **Swarm Intelligence**: Coordinated problem solving (experimental)\n\n### Memory Management\n\n- Persistent agent memory across sessions\n- Conversation history tracking\n- Learning from interactions (teachable agents)\n- Memory cleanup and optimization\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Errors**: Ensure your OpenAI API key is valid and has sufficient credits\n2. **Import Errors**: Install all dependencies with `pip install -r requirements.txt --user`\n3. **Build Failures**: Check Node.js version (>= 18) and run `npm install`\n4. **Chat Failures**: Verify agent creation succeeded before attempting conversations\n\n### Debug Mode\n\nEnable detailed logging:\n\n```bash\nexport LOG_LEVEL=DEBUG\npython test_server.py\n```\n\n### Performance Tips\n\n- Use `gpt-4o-mini` for faster, cost-effective operations\n- Enable caching for repeated operations\n- Set appropriate timeout values for long-running workflows\n- Use quality checks only when needed (increases execution time)\n\n## Development\n\n### Running Tests\n\n```bash\n# Full test suite\npython test_server.py\n\n# Individual workflow tests  \npython -c \"\nimport asyncio\nfrom src.autogen_mcp.workflows import WorkflowManager\nwm = WorkflowManager()\nprint(asyncio.run(wm.execute_workflow('code_generation', {'task': 'test'})))\n\"\n```\n\n### Building\n\n```bash\nnpm run build\nnpm run lint\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Submit a pull request\n\n## Version History\n\n### v0.2.0 (Latest)\n- ✨ Enhanced MCP support with prompts and resources\n- 🤖 Advanced agent types (teachable, retrievable)\n- 🔄 Sophisticated workflows with quality checks\n- 🎯 Smart speaker selection and nested conversations\n- 📊 Real-time resource monitoring\n- 🧠 Memory management and persistence\n\n### v0.1.0\n- Basic AutoGen integration\n- Simple agent creation and chat execution\n- MCP tool interface\n\n## Support\n\nFor issues and questions:\n- Check the troubleshooting section above\n- Review the test examples in `test_server.py`\n- Open an issue on GitHub with detailed reproduction steps\n\n## License\n\nMIT License - see LICENSE file for details.\n\n# OpenAI API Key (optional, can also be set in config.json)\nOPENAI_API_KEY=your-openai-api-key\n```\n\n### Server Configuration\n\n1. Copy `config.json.example` to `config.json`:\n```bash\ncp config.json.example config.json\n```\n\n2. Configure the server settings:\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0\n  },\n  \"code_execution_config\": {\n    \"work_dir\": \"workspace\",\n    \"use_docker\": false\n  }\n}\n```\n\n## Available Operations\n\nThe server supports three main operations:\n\n### 1. Creating Agents\n\n```json\n{\n  \"name\": \"create_agent\",\n  \"arguments\": {\n    \"name\": \"tech_lead\",\n    \"type\": \"assistant\",\n    \"system_message\": \"You are a technical lead with expertise in software architecture and design patterns.\"\n  }\n}\n```\n\n### 2. One-on-One Chat\n\n```json\n{\n  \"name\": \"execute_chat\",\n  \"arguments\": {\n    \"initiator\": \"agent1\",\n    \"responder\": \"agent2\",\n    \"message\": \"Let's discuss the system architecture.\"\n  }\n}\n```\n\n### 3. Group Chat\n\n```json\n{\n  \"name\": \"execute_group_chat\",\n  \"arguments\": {\n    \"agents\": [\"agent1\", \"agent2\", \"agent3\"],\n    \"message\": \"Let's review the proposed solution.\"\n  }\n}\n```\n\n## Error Handling\n\nCommon error scenarios include:\n\n1. Agent Creation Errors\n```json\n{\n  \"error\": \"Agent already exists\"\n}\n```\n\n2. Execution Errors\n```json\n{\n  \"error\": \"Agent not found\"\n}\n```\n\n3. Configuration Errors\n```json\n{\n  \"error\": \"AUTOGEN_MCP_CONFIG environment variable not set\"\n}\n```\n\n## Architecture\n\nThe server follows a modular architecture:\n\n```\nsrc/\n├── autogen_mcp/\n│   ├── __init__.py\n│   ├── agents.py      # Agent management and configuration\n│   ├── config.py      # Configuration handling and validation\n│   ├── server.py      # MCP server implementation\n│   └── workflows.py   # Conversation workflow management\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "autogen_mcp",
        "ai",
        "collaboration",
        "realtime collaboration",
        "collaboration dynamicendpoints",
        "dynamicendpoints autogen_mcp"
      ],
      "category": "realtime-collaboration"
    },
    "Mimo-Inverse--realtime-chat-supabase-react": {
      "owner": "Mimo-Inverse",
      "name": "realtime-chat-supabase-react",
      "url": "https://github.com/Mimo-Inverse/realtime-chat-supabase-react",
      "imageUrl": "/freedevtools/mcp/pfp/Mimo-Inverse.webp",
      "description": "Facilitates real-time chat experiences using React and Supabase, allowing users to send and receive messages instantly. The application supports seamless communication through a user-friendly interface, underpinned by a PostgreSQL database.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-03-20T04:19:41Z",
      "readme_content": "# Full-stack real-time chat\n\n[![Netlify Status](https://api.netlify.com/api/v1/badges/38b6f457-50d2-42ac-b9a8-9ca962febebd/deploy-status)](https://app.netlify.com/sites/random-chat/deploys)\n\n- **Data:** PostgeSQL managed by [Supabase](https://supabase.io/) [@supabase_io](https://twitter.com/supabase_io) (awsome real-time API).\n- **Front-end**: React + Vite\n- **UI library**: [chakra-ui](https://chakra-ui.com/) [@chakra_ui](https://twitter.com/chakra_ui)\n- **Hosting**: [Netlify](https://www.netlify.com/)\n- Country flags from [Flagpedia](https://flagpedia.net)\n\n## Install\n\n`npm install` to setup dependencies\n\n## Supabase variables\n\nCreate a `.env` file with `VITE_SUPABASE_URL` and `VITE_SUPABASE_KEY` (see env.example)\n\n## Setup your Supabase project\n\nThe following database table is required:\n\n| Field            | Type      |\n| ---------------- | --------- |\n| id               | BIGINT    |\n| username         | VARCHAR   |\n| text             | TEXT      |\n| country          | VARCHAR   |\n| is_authenticated | BOOLEAN   |\n| timestamp        | timestamp |\n\nSQL query if not using the Supabase interface:\n\n```sql\nCREATE TABLE messages (\n  id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  username VARCHAR NOT NULL,\n  text TEXT NOT NULL,\n  country VARCHAR,\n  is_authenticated BOOLEAN DEFAULT FALSE,\n  timestamp timestamp default now() NOT NULL\n);\n```\n\nNote: If you're using Supabase interface, don't forget to tick `Enable Realtime` setting after you created the table.\n\n## Setup GitHub authentication (optional)\n\nFollow instrunction [here](https://supabase.io/docs/guides/auth/auth-github)\n\n## Dev\n\n`npm run dev` to run server on port 3000\n\n## Build\n\n`npm run build` to build the react client\n\n# Demo\n\n[https://random-chat.netlify.app](https://random-chat.netlify.app/)\n\n!['demo'](https://random-chat.netlify.app/demo.png \"demo\")\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chat",
        "supabase",
        "realtime",
        "chat supabase",
        "realtime chat",
        "supabase react"
      ],
      "category": "realtime-collaboration"
    },
    "Monadical-SAS--zulip-mcp": {
      "owner": "Monadical-SAS",
      "name": "zulip-mcp",
      "url": "https://github.com/Monadical-SAS/zulip-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Monadical-SAS.webp",
      "description": "Enable interaction with Zulip workspaces by managing channels, posting messages, sending direct messages, adding reactions, and retrieving conversation history for effective communication and automation.",
      "stars": 4,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-27T14:27:30Z",
      "readme_content": "# Zulip MCP Server\n\nMCP Server for the Zulip API, enabling AI assistants like Claude to interact with Zulip workspaces.\n\n## Tools\n\n1. `zulip_list_channels`\n   - List available channels (streams) in the Zulip organization\n   - Optional inputs:\n     - `include_private` (boolean, default: false): Whether to include private streams \n     - `include_web_public` (boolean, default: true): Whether to include web-public streams\n     - `include_subscribed` (boolean, default: true): Whether to include streams the bot is subscribed to\n   - Returns: List of streams with their IDs and information\n\n2. `zulip_post_message`\n   - Post a new message to a Zulip channel (stream)\n   - Required inputs:\n     - `channel_name` (string): The name of the stream to post to\n     - `topic` (string): The topic within the stream\n     - `content` (string): The message content to post\n   - Returns: Message posting confirmation and ID\n\n3. `zulip_send_direct_message`\n   - Send a direct message to one or more users\n   - Required inputs:\n     - `recipients` (string[]): Email addresses or user IDs of recipients\n     - `content` (string): The message content to send\n   - Returns: Message sending confirmation and ID\n\n4. `zulip_add_reaction`\n   - Add an emoji reaction to a message\n   - Required inputs:\n     - `message_id` (number): The ID of the message to react to\n     - `emoji_name` (string): Emoji name without colons\n   - Returns: Reaction confirmation\n\n5. `zulip_get_channel_history`\n   - Get recent messages from a channel (stream) and topic\n   - Required inputs:\n     - `channel_name` (string): The name of the stream\n     - `topic` (string): The topic name\n   - Optional inputs:\n     - `limit` (number, default: 20): Number of messages to retrieve\n     - `anchor` (string, default: \"newest\"): Message ID to start from\n   - Returns: List of messages with their content and metadata\n\n6. `zulip_get_topics`\n   - Get topics in a channel (stream)\n   - Required inputs:\n     - `channel_id` (number): The ID of the stream\n   - Returns: List of topics in the stream\n\n7. `zulip_subscribe_to_channel`\n   - Subscribe the bot to a channel (stream)\n   - Required inputs:\n     - `channel_name` (string): The name of the stream to subscribe to\n   - Returns: Subscription confirmation\n\n8. `zulip_get_users`\n   - Get list of users in the Zulip organization\n   - Returns: List of users with their basic information\n\n## Setup\n\n1. Create a Zulip Bot:\n   - Log in to your Zulip instance\n   - Navigate to Settings > Personal > Bots\n   - Click \"Add a new bot\"\n   - Select \"Generic bot\" type\n   - Fill in the required information\n   - Click \"Create bot\"\n\n2. Permissions:\n   - By default, Zulip bots have limited permissions\n   - Make sure to subscribe the bot to any streams it needs to access\n   - If you need the bot to have more permissions, consider using a full user account instead\n\n3. Get the API credentials:\n   - Bot's email address\n   - Bot's API key (displayed when you create the bot)\n   - Zulip instance URL (e.g., https://example.zulipchat.com)\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n#### npx\n\n```json\n{\n  \"mcpServers\": {\n    \"zulip\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-zulip\"\n      ],\n      \"env\": {\n        \"ZULIP_EMAIL\": \"your-bot@example.zulipchat.com\",\n        \"ZULIP_API_KEY\": \"your-bot-api-key\",\n        \"ZULIP_URL\": \"https://example.zulipchat.com\"\n      }\n    }\n  }\n}\n```\n\n#### docker\n\n```json\n{\n  \"mcpServers\": {\n    \"zulip\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"ZULIP_EMAIL\",\n        \"-e\",\n        \"ZULIP_API_KEY\",\n        \"-e\",\n        \"ZULIP_URL\",\n        \"mcp/zulip\"\n      ],\n      \"env\": {\n        \"ZULIP_EMAIL\": \"your-bot@example.zulipchat.com\",\n        \"ZULIP_API_KEY\": \"your-bot-api-key\",\n        \"ZULIP_URL\": \"https://example.zulipchat.com\"\n      }\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf you encounter permission errors, verify that:\n1. The bot API key is correct\n2. The bot has been subscribed to the channels it needs to access\n3. The Zulip URL is correct and accessible\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/zulip .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zulip",
        "collaboration",
        "realtime",
        "realtime collaboration",
        "interaction zulip",
        "zulip workspaces"
      ],
      "category": "realtime-collaboration"
    },
    "a1351995160--Auto-GPT": {
      "owner": "a1351995160",
      "name": "Auto-GPT",
      "url": "https://github.com/a1351995160/Auto-GPT",
      "imageUrl": "/freedevtools/mcp/pfp/a1351995160.webp",
      "description": "Autonomously achieve goals by chaining thoughts of the GPT-4 model while accessing the internet, managing memory, and storing files. The server supports plugin extensibility to enhance functionality.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-05-22T08:25:34Z",
      "readme_content": "# Auto-GPT: An Autonomous GPT-4 Experiment\n[![Official Website](https://img.shields.io/badge/Official%20Website-agpt.co-blue?style=flat&logo=world&logoColor=white)](https://agpt.co)\n[![Unit Tests](https://img.shields.io/github/actions/workflow/status/Significant-Gravitas/Auto-GPT/ci.yml?label=unit%20tests)](https://github.com/Significant-Gravitas/Auto-GPT/actions/workflows/ci.yml)\n[](https://discord.gg/autogpt)\n[![GitHub Repo stars](https://img.shields.io/github/stars/Significant-Gravitas/auto-gpt?style=social)](https://github.com/Significant-Gravitas/Auto-GPT/stargazers)\n[![Twitter Follow](https://img.shields.io/twitter/follow/siggravitas?style=social)](https://twitter.com/SigGravitas)\n\n## 💡 Get help - [Q&A](https://github.com/Significant-Gravitas/Auto-GPT/discussions/categories/q-a) or [Discord 💬](https://discord.gg/autogpt)\n\n<hr/>\n\n### 🔴 USE `stable` not `master` 🔴\n\n**Download the latest `stable` release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.**\nThe `master` branch is under heavy development and may often be in a **broken** state.\n\n<hr/>\n\n\nAuto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \"thoughts\", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.\n\n<h2 align=\"center\"> Demo April 16th 2023 </h2>\n\nhttps://user-images.githubusercontent.com/70048414/232352935-55c6bf7c-3958-406e-8610-0913475a0b05.mp4\n\nDemo made by <a href=https://twitter.com/BlakeWerlinger>Blake Werlinger</a>\n\n<h2 align=\"center\"> 💖 Help Fund Auto-GPT's Development 💖</h2>\n<p align=\"center\">\nIf you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!\nYour support is greatly appreciated. Development of this free, open-source project is made possible by all the <a href=\"https://github.com/Significant-Gravitas/Auto-GPT/graphs/contributors\">contributors</a> and <a href=\"https://github.com/sponsors/Torantulino\">sponsors</a>. If you'd like to sponsor this project and have your avatar or company logo appear below <a href=\"https://github.com/sponsors/Torantulino\">click here</a>.\n</p>\n\n\n<p align=\"center\">\n<div align=\"center\" class=\"logo-container\">\n<a href=\"https://www.zilliz.com/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234158272-7917382e-ff80-469e-8d8c-94f4477b8b5a.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234158222-30e2d7a7-f0a9-433d-a305-e3aa0b194444.png\" height=\"40px\" alt=\"Zilliz\" />\n</picture>\n</a>\n\n<a href=\"https://roost.ai\">\n<img src=\"https://user-images.githubusercontent.com/22963551/234180283-b58cb03c-c95a-4196-93c1-28b52a388e9d.png\" height=\"40px\" alt=\"Roost.AI\" />\n</a>\n  \n<a href=\"https://nuclei.ai/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234153428-24a6f31d-c0c6-4c9b-b3f4-9110148f67b4.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181283-691c5d71-ca94-4646-a1cf-6e818bd86faa.png\" height=\"40px\" alt=\"NucleiAI\" />\n</picture>\n</a>\n\n<a href=\"https://www.algohash.org/\">\n<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234180375-1365891c-0ba6-4d49-94c3-847c85fe03b0.png\" >\n  <img src=\"https://user-images.githubusercontent.com/22963551/234180359-143e4a7a-4a71-4830-99c8-9b165cde995f.png\" height=\"40px\" alt=\"Algohash\" />\n</picture>\n</a>\n\n<a href=\"https://www.typingmind.com/?utm_source=autogpt\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/233202971-61e77209-58a0-47d9-9f7e-dd081111437b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234157731-f908b5db-8fe7-4036-89b6-7b2a21f87e3a.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://github.com/weaviate/weaviate\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234181699-3d7f6ea8-5a7f-4e98-b812-37be1081be4b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181695-fc895159-b921-4895-9a13-65e6eff5b0e7.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://chatgpv.com/?ref=spni76459e4fa3f30a\">\n<img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/22963551/239132565-623a2dd6-eaeb-4941-b40f-c5a29ca6bebc.png\" height=\"40px\" alt=\"ChatGPV\" />\n</a>\n  \n</div>\n</br>\n\n\n\n<p align=\"center\"><a href=\"https://github.com/robinicus\"><img src=\"https://avatars.githubusercontent.com/robinicus?v=4\" width=\"50px\" alt=\"robinicus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/0xmatchmaker\"><img src=\"https://avatars.githubusercontent.com/0xmatchmaker?v=4\" width=\"50px\" alt=\"0xmatchmaker\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jazgarewal\"><img src=\"https://avatars.githubusercontent.com/jazgarewal?v=4\" width=\"50px\" alt=\"jazgarewal\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MayurVirkar\"><img src=\"https://avatars.githubusercontent.com/MayurVirkar?v=4\" width=\"50px\" alt=\"MayurVirkar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/avy-ai\"><img src=\"https://avatars.githubusercontent.com/avy-ai?v=4\" width=\"50px\" alt=\"avy-ai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/TheStoneMX\"><img src=\"https://avatars.githubusercontent.com/TheStoneMX?v=4\" width=\"50px\" alt=\"TheStoneMX\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/goldenrecursion\"><img src=\"https://avatars.githubusercontent.com/goldenrecursion?v=4\" width=\"50px\" alt=\"goldenrecursion\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MatthewAgs\"><img src=\"https://avatars.githubusercontent.com/MatthewAgs?v=4\" width=\"50px\" alt=\"MatthewAgs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/eelbaz\"><img src=\"https://avatars.githubusercontent.com/eelbaz?v=4\" width=\"50px\" alt=\"eelbaz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rapidstartup\"><img src=\"https://avatars.githubusercontent.com/rapidstartup?v=4\" width=\"50px\" alt=\"rapidstartup\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/gklab\"><img src=\"https://avatars.githubusercontent.com/gklab?v=4\" width=\"50px\" alt=\"gklab\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/VoiceBeer\"><img src=\"https://avatars.githubusercontent.com/VoiceBeer?v=4\" width=\"50px\" alt=\"VoiceBeer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DailyBotHQ\"><img src=\"https://avatars.githubusercontent.com/DailyBotHQ?v=4\" width=\"50px\" alt=\"DailyBotHQ\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lucas-chu\"><img src=\"https://avatars.githubusercontent.com/lucas-chu?v=4\" width=\"50px\" alt=\"lucas-chu\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/knifour\"><img src=\"https://avatars.githubusercontent.com/knifour?v=4\" width=\"50px\" alt=\"knifour\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/refinery1\"><img src=\"https://avatars.githubusercontent.com/refinery1?v=4\" width=\"50px\" alt=\"refinery1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/st617\"><img src=\"https://avatars.githubusercontent.com/st617?v=4\" width=\"50px\" alt=\"st617\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neodenit\"><img src=\"https://avatars.githubusercontent.com/neodenit?v=4\" width=\"50px\" alt=\"neodenit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrazySwami\"><img src=\"https://avatars.githubusercontent.com/CrazySwami?v=4\" width=\"50px\" alt=\"CrazySwami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Heitechsoft\"><img src=\"https://avatars.githubusercontent.com/Heitechsoft?v=4\" width=\"50px\" alt=\"Heitechsoft\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RealChrisSean\"><img src=\"https://avatars.githubusercontent.com/RealChrisSean?v=4\" width=\"50px\" alt=\"RealChrisSean\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/abhinav-pandey29\"><img src=\"https://avatars.githubusercontent.com/abhinav-pandey29?v=4\" width=\"50px\" alt=\"abhinav-pandey29\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Explorergt92\"><img src=\"https://avatars.githubusercontent.com/Explorergt92?v=4\" width=\"50px\" alt=\"Explorergt92\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SparkplanAI\"><img src=\"https://avatars.githubusercontent.com/SparkplanAI?v=4\" width=\"50px\" alt=\"SparkplanAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/crizzler\"><img src=\"https://avatars.githubusercontent.com/crizzler?v=4\" width=\"50px\" alt=\"crizzler\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kreativai\"><img src=\"https://avatars.githubusercontent.com/kreativai?v=4\" width=\"50px\" alt=\"kreativai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/omphos\"><img src=\"https://avatars.githubusercontent.com/omphos?v=4\" width=\"50px\" alt=\"omphos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Jahmazon\"><img src=\"https://avatars.githubusercontent.com/Jahmazon?v=4\" width=\"50px\" alt=\"Jahmazon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tjarmain\"><img src=\"https://avatars.githubusercontent.com/tjarmain?v=4\" width=\"50px\" alt=\"tjarmain\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ddtarazona\"><img src=\"https://avatars.githubusercontent.com/ddtarazona?v=4\" width=\"50px\" alt=\"ddtarazona\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/saten-private\"><img src=\"https://avatars.githubusercontent.com/saten-private?v=4\" width=\"50px\" alt=\"saten-private\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/anvarazizov\"><img src=\"https://avatars.githubusercontent.com/anvarazizov?v=4\" width=\"50px\" alt=\"anvarazizov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lazzacapital\"><img src=\"https://avatars.githubusercontent.com/lazzacapital?v=4\" width=\"50px\" alt=\"lazzacapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/m\"><img src=\"https://avatars.githubusercontent.com/m?v=4\" width=\"50px\" alt=\"m\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Pythagora-io\"><img src=\"https://avatars.githubusercontent.com/Pythagora-io?v=4\" width=\"50px\" alt=\"Pythagora-io\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Web3Capital\"><img src=\"https://avatars.githubusercontent.com/Web3Capital?v=4\" width=\"50px\" alt=\"Web3Capital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/toverly1\"><img src=\"https://avatars.githubusercontent.com/toverly1?v=4\" width=\"50px\" alt=\"toverly1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/digisomni\"><img src=\"https://avatars.githubusercontent.com/digisomni?v=4\" width=\"50px\" alt=\"digisomni\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/concreit\"><img src=\"https://avatars.githubusercontent.com/concreit?v=4\" width=\"50px\" alt=\"concreit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/LeeRobidas\"><img src=\"https://avatars.githubusercontent.com/LeeRobidas?v=4\" width=\"50px\" alt=\"LeeRobidas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Josecodesalot\"><img src=\"https://avatars.githubusercontent.com/Josecodesalot?v=4\" width=\"50px\" alt=\"Josecodesalot\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/dexterityx\"><img src=\"https://avatars.githubusercontent.com/dexterityx?v=4\" width=\"50px\" alt=\"dexterityx\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rickscode\"><img src=\"https://avatars.githubusercontent.com/rickscode?v=4\" width=\"50px\" alt=\"rickscode\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Brodie0\"><img src=\"https://avatars.githubusercontent.com/Brodie0?v=4\" width=\"50px\" alt=\"Brodie0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/FSTatSBS\"><img src=\"https://avatars.githubusercontent.com/FSTatSBS?v=4\" width=\"50px\" alt=\"FSTatSBS\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nocodeclarity\"><img src=\"https://avatars.githubusercontent.com/nocodeclarity?v=4\" width=\"50px\" alt=\"nocodeclarity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jsolejr\"><img src=\"https://avatars.githubusercontent.com/jsolejr?v=4\" width=\"50px\" alt=\"jsolejr\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/amr-elsehemy\"><img src=\"https://avatars.githubusercontent.com/amr-elsehemy?v=4\" width=\"50px\" alt=\"amr-elsehemy\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RawBanana\"><img src=\"https://avatars.githubusercontent.com/RawBanana?v=4\" width=\"50px\" alt=\"RawBanana\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/horazius\"><img src=\"https://avatars.githubusercontent.com/horazius?v=4\" width=\"50px\" alt=\"horazius\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SwftCoins\"><img src=\"https://avatars.githubusercontent.com/SwftCoins?v=4\" width=\"50px\" alt=\"SwftCoins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tob-le-rone\"><img src=\"https://avatars.githubusercontent.com/tob-le-rone?v=4\" width=\"50px\" alt=\"tob-le-rone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RThaweewat\"><img src=\"https://avatars.githubusercontent.com/RThaweewat?v=4\" width=\"50px\" alt=\"RThaweewat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jun784\"><img src=\"https://avatars.githubusercontent.com/jun784?v=4\" width=\"50px\" alt=\"jun784\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/joaomdmoura\"><img src=\"https://avatars.githubusercontent.com/joaomdmoura?v=4\" width=\"50px\" alt=\"joaomdmoura\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rejunity\"><img src=\"https://avatars.githubusercontent.com/rejunity?v=4\" width=\"50px\" alt=\"rejunity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/mathewhawkins\"><img src=\"https://avatars.githubusercontent.com/mathewhawkins?v=4\" width=\"50px\" alt=\"mathewhawkins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/caitlynmeeks\"><img src=\"https://avatars.githubusercontent.com/caitlynmeeks?v=4\" width=\"50px\" alt=\"caitlynmeeks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jd3655\"><img src=\"https://avatars.githubusercontent.com/jd3655?v=4\" width=\"50px\" alt=\"jd3655\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Odin519Tomas\"><img src=\"https://avatars.githubusercontent.com/Odin519Tomas?v=4\" width=\"50px\" alt=\"Odin519Tomas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DataMetis\"><img src=\"https://avatars.githubusercontent.com/DataMetis?v=4\" width=\"50px\" alt=\"DataMetis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/webbcolton\"><img src=\"https://avatars.githubusercontent.com/webbcolton?v=4\" width=\"50px\" alt=\"webbcolton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rocks6\"><img src=\"https://avatars.githubusercontent.com/rocks6?v=4\" width=\"50px\" alt=\"rocks6\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cxs\"><img src=\"https://avatars.githubusercontent.com/cxs?v=4\" width=\"50px\" alt=\"cxs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fruition\"><img src=\"https://avatars.githubusercontent.com/fruition?v=4\" width=\"50px\" alt=\"fruition\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nnkostov\"><img src=\"https://avatars.githubusercontent.com/nnkostov?v=4\" width=\"50px\" alt=\"nnkostov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/morcos\"><img src=\"https://avatars.githubusercontent.com/morcos?v=4\" width=\"50px\" alt=\"morcos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingbotan\"><img src=\"https://avatars.githubusercontent.com/pingbotan?v=4\" width=\"50px\" alt=\"pingbotan\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/maxxflyer\"><img src=\"https://avatars.githubusercontent.com/maxxflyer?v=4\" width=\"50px\" alt=\"maxxflyer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommi-joentakanen\"><img src=\"https://avatars.githubusercontent.com/tommi-joentakanen?v=4\" width=\"50px\" alt=\"tommi-joentakanen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/hunteraraujo\"><img src=\"https://avatars.githubusercontent.com/hunteraraujo?v=4\" width=\"50px\" alt=\"hunteraraujo\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/projectonegames\"><img src=\"https://avatars.githubusercontent.com/projectonegames?v=4\" width=\"50px\" alt=\"projectonegames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tullytim\"><img src=\"https://avatars.githubusercontent.com/tullytim?v=4\" width=\"50px\" alt=\"tullytim\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/comet-ml\"><img src=\"https://avatars.githubusercontent.com/comet-ml?v=4\" width=\"50px\" alt=\"comet-ml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thepok\"><img src=\"https://avatars.githubusercontent.com/thepok?v=4\" width=\"50px\" alt=\"thepok\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/prompthero\"><img src=\"https://avatars.githubusercontent.com/prompthero?v=4\" width=\"50px\" alt=\"prompthero\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sunchongren\"><img src=\"https://avatars.githubusercontent.com/sunchongren?v=4\" width=\"50px\" alt=\"sunchongren\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neverinstall\"><img src=\"https://avatars.githubusercontent.com/neverinstall?v=4\" width=\"50px\" alt=\"neverinstall\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephcmiller2\"><img src=\"https://avatars.githubusercontent.com/josephcmiller2?v=4\" width=\"50px\" alt=\"josephcmiller2\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/yx3110\"><img src=\"https://avatars.githubusercontent.com/yx3110?v=4\" width=\"50px\" alt=\"yx3110\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MBassi91\"><img src=\"https://avatars.githubusercontent.com/MBassi91?v=4\" width=\"50px\" alt=\"MBassi91\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SpacingLily\"><img src=\"https://avatars.githubusercontent.com/SpacingLily?v=4\" width=\"50px\" alt=\"SpacingLily\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arthur-x88\"><img src=\"https://avatars.githubusercontent.com/arthur-x88?v=4\" width=\"50px\" alt=\"arthur-x88\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ciscodebs\"><img src=\"https://avatars.githubusercontent.com/ciscodebs?v=4\" width=\"50px\" alt=\"ciscodebs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/christian-gheorghe\"><img src=\"https://avatars.githubusercontent.com/christian-gheorghe?v=4\" width=\"50px\" alt=\"christian-gheorghe\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/EngageStrategies\"><img src=\"https://avatars.githubusercontent.com/EngageStrategies?v=4\" width=\"50px\" alt=\"EngageStrategies\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jondwillis\"><img src=\"https://avatars.githubusercontent.com/jondwillis?v=4\" width=\"50px\" alt=\"jondwillis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Cameron-Fulton\"><img src=\"https://avatars.githubusercontent.com/Cameron-Fulton?v=4\" width=\"50px\" alt=\"Cameron-Fulton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AryaXAI\"><img src=\"https://avatars.githubusercontent.com/AryaXAI?v=4\" width=\"50px\" alt=\"AryaXAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AuroraHolding\"><img src=\"https://avatars.githubusercontent.com/AuroraHolding?v=4\" width=\"50px\" alt=\"AuroraHolding\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mr-Bishop42\"><img src=\"https://avatars.githubusercontent.com/Mr-Bishop42?v=4\" width=\"50px\" alt=\"Mr-Bishop42\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/doverhq\"><img src=\"https://avatars.githubusercontent.com/doverhq?v=4\" width=\"50px\" alt=\"doverhq\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/johnculkin\"><img src=\"https://avatars.githubusercontent.com/johnculkin?v=4\" width=\"50px\" alt=\"johnculkin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/marv-technology\"><img src=\"https://avatars.githubusercontent.com/marv-technology?v=4\" width=\"50px\" alt=\"marv-technology\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ikarosai\"><img src=\"https://avatars.githubusercontent.com/ikarosai?v=4\" width=\"50px\" alt=\"ikarosai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ColinConwell\"><img src=\"https://avatars.githubusercontent.com/ColinConwell?v=4\" width=\"50px\" alt=\"ColinConwell\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/humungasaurus\"><img src=\"https://avatars.githubusercontent.com/humungasaurus?v=4\" width=\"50px\" alt=\"humungasaurus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/terpsfreak\"><img src=\"https://avatars.githubusercontent.com/terpsfreak?v=4\" width=\"50px\" alt=\"terpsfreak\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/iddelacruz\"><img src=\"https://avatars.githubusercontent.com/iddelacruz?v=4\" width=\"50px\" alt=\"iddelacruz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thisisjeffchen\"><img src=\"https://avatars.githubusercontent.com/thisisjeffchen?v=4\" width=\"50px\" alt=\"thisisjeffchen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nicoguyon\"><img src=\"https://avatars.githubusercontent.com/nicoguyon?v=4\" width=\"50px\" alt=\"nicoguyon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arjunb023\"><img src=\"https://avatars.githubusercontent.com/arjunb023?v=4\" width=\"50px\" alt=\"arjunb023\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Nalhos\"><img src=\"https://avatars.githubusercontent.com/Nalhos?v=4\" width=\"50px\" alt=\"Nalhos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/belharethsami\"><img src=\"https://avatars.githubusercontent.com/belharethsami?v=4\" width=\"50px\" alt=\"belharethsami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mobivs\"><img src=\"https://avatars.githubusercontent.com/Mobivs?v=4\" width=\"50px\" alt=\"Mobivs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/txtr99\"><img src=\"https://avatars.githubusercontent.com/txtr99?v=4\" width=\"50px\" alt=\"txtr99\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ntwrite\"><img src=\"https://avatars.githubusercontent.com/ntwrite?v=4\" width=\"50px\" alt=\"ntwrite\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/founderblocks-sils\"><img src=\"https://avatars.githubusercontent.com/founderblocks-sils?v=4\" width=\"50px\" alt=\"founderblocks-sils\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kMag410\"><img src=\"https://avatars.githubusercontent.com/kMag410?v=4\" width=\"50px\" alt=\"kMag410\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/angiaou\"><img src=\"https://avatars.githubusercontent.com/angiaou?v=4\" width=\"50px\" alt=\"angiaou\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/garythebat\"><img src=\"https://avatars.githubusercontent.com/garythebat?v=4\" width=\"50px\" alt=\"garythebat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lmaugustin\"><img src=\"https://avatars.githubusercontent.com/lmaugustin?v=4\" width=\"50px\" alt=\"lmaugustin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/shawnharmsen\"><img src=\"https://avatars.githubusercontent.com/shawnharmsen?v=4\" width=\"50px\" alt=\"shawnharmsen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/clortegah\"><img src=\"https://avatars.githubusercontent.com/clortegah?v=4\" width=\"50px\" alt=\"clortegah\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MetaPath01\"><img src=\"https://avatars.githubusercontent.com/MetaPath01?v=4\" width=\"50px\" alt=\"MetaPath01\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sekomike910\"><img src=\"https://avatars.githubusercontent.com/sekomike910?v=4\" width=\"50px\" alt=\"sekomike910\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MediConCenHK\"><img src=\"https://avatars.githubusercontent.com/MediConCenHK?v=4\" width=\"50px\" alt=\"MediConCenHK\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/svpermari0\"><img src=\"https://avatars.githubusercontent.com/svpermari0?v=4\" width=\"50px\" alt=\"svpermari0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jacobyoby\"><img src=\"https://avatars.githubusercontent.com/jacobyoby?v=4\" width=\"50px\" alt=\"jacobyoby\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/turintech\"><img src=\"https://avatars.githubusercontent.com/turintech?v=4\" width=\"50px\" alt=\"turintech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/allenstecat\"><img src=\"https://avatars.githubusercontent.com/allenstecat?v=4\" width=\"50px\" alt=\"allenstecat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CatsMeow492\"><img src=\"https://avatars.githubusercontent.com/CatsMeow492?v=4\" width=\"50px\" alt=\"CatsMeow492\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommygeee\"><img src=\"https://avatars.githubusercontent.com/tommygeee?v=4\" width=\"50px\" alt=\"tommygeee\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/judegomila\"><img src=\"https://avatars.githubusercontent.com/judegomila?v=4\" width=\"50px\" alt=\"judegomila\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cfarquhar\"><img src=\"https://avatars.githubusercontent.com/cfarquhar?v=4\" width=\"50px\" alt=\"cfarquhar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZoneSixGames\"><img src=\"https://avatars.githubusercontent.com/ZoneSixGames?v=4\" width=\"50px\" alt=\"ZoneSixGames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kenndanielso\"><img src=\"https://avatars.githubusercontent.com/kenndanielso?v=4\" width=\"50px\" alt=\"kenndanielso\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrypteorCapital\"><img src=\"https://avatars.githubusercontent.com/CrypteorCapital?v=4\" width=\"50px\" alt=\"CrypteorCapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sultanmeghji\"><img src=\"https://avatars.githubusercontent.com/sultanmeghji?v=4\" width=\"50px\" alt=\"sultanmeghji\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jenius-eagle\"><img src=\"https://avatars.githubusercontent.com/jenius-eagle?v=4\" width=\"50px\" alt=\"jenius-eagle\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephjacks\"><img src=\"https://avatars.githubusercontent.com/josephjacks?v=4\" width=\"50px\" alt=\"josephjacks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingshian0131\"><img src=\"https://avatars.githubusercontent.com/pingshian0131?v=4\" width=\"50px\" alt=\"pingshian0131\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AIdevelopersAI\"><img src=\"https://avatars.githubusercontent.com/AIdevelopersAI?v=4\" width=\"50px\" alt=\"AIdevelopersAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ternary5\"><img src=\"https://avatars.githubusercontent.com/ternary5?v=4\" width=\"50px\" alt=\"ternary5\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ChrisDMT\"><img src=\"https://avatars.githubusercontent.com/ChrisDMT?v=4\" width=\"50px\" alt=\"ChrisDMT\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AcountoOU\"><img src=\"https://avatars.githubusercontent.com/AcountoOU?v=4\" width=\"50px\" alt=\"AcountoOU\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/chatgpt-prompts\"><img src=\"https://avatars.githubusercontent.com/chatgpt-prompts?v=4\" width=\"50px\" alt=\"chatgpt-prompts\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Partender\"><img src=\"https://avatars.githubusercontent.com/Partender?v=4\" width=\"50px\" alt=\"Partender\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Daniel1357\"><img src=\"https://avatars.githubusercontent.com/Daniel1357?v=4\" width=\"50px\" alt=\"Daniel1357\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/KiaArmani\"><img src=\"https://avatars.githubusercontent.com/KiaArmani?v=4\" width=\"50px\" alt=\"KiaArmani\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/zkonduit\"><img src=\"https://avatars.githubusercontent.com/zkonduit?v=4\" width=\"50px\" alt=\"zkonduit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fabrietech\"><img src=\"https://avatars.githubusercontent.com/fabrietech?v=4\" width=\"50px\" alt=\"fabrietech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/scryptedinc\"><img src=\"https://avatars.githubusercontent.com/scryptedinc?v=4\" width=\"50px\" alt=\"scryptedinc\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/coreyspagnoli\"><img src=\"https://avatars.githubusercontent.com/coreyspagnoli?v=4\" width=\"50px\" alt=\"coreyspagnoli\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AntonioCiolino\"><img src=\"https://avatars.githubusercontent.com/AntonioCiolino?v=4\" width=\"50px\" alt=\"AntonioCiolino\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Dradstone\"><img src=\"https://avatars.githubusercontent.com/Dradstone?v=4\" width=\"50px\" alt=\"Dradstone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CarmenCocoa\"><img src=\"https://avatars.githubusercontent.com/CarmenCocoa?v=4\" width=\"50px\" alt=\"CarmenCocoa\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/bentoml\"><img src=\"https://avatars.githubusercontent.com/bentoml?v=4\" width=\"50px\" alt=\"bentoml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/merwanehamadi\"><img src=\"https://avatars.githubusercontent.com/merwanehamadi?v=4\" width=\"50px\" alt=\"merwanehamadi\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/vkozacek\"><img src=\"https://avatars.githubusercontent.com/vkozacek?v=4\" width=\"50px\" alt=\"vkozacek\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ASmithOWL\"><img src=\"https://avatars.githubusercontent.com/ASmithOWL?v=4\" width=\"50px\" alt=\"ASmithOWL\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tekelsey\"><img src=\"https://avatars.githubusercontent.com/tekelsey?v=4\" width=\"50px\" alt=\"tekelsey\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/GalaxyVideoAgency\"><img src=\"https://avatars.githubusercontent.com/GalaxyVideoAgency?v=4\" width=\"50px\" alt=\"GalaxyVideoAgency\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/wenfengwang\"><img src=\"https://avatars.githubusercontent.com/wenfengwang?v=4\" width=\"50px\" alt=\"wenfengwang\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rviramontes\"><img src=\"https://avatars.githubusercontent.com/rviramontes?v=4\" width=\"50px\" alt=\"rviramontes\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/indoor47\"><img src=\"https://avatars.githubusercontent.com/indoor47?v=4\" width=\"50px\" alt=\"indoor47\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZERO-A-ONE\"><img src=\"https://avatars.githubusercontent.com/ZERO-A-ONE?v=4\" width=\"50px\" alt=\"ZERO-A-ONE\" /></a>&nbsp;&nbsp;</p>\n\n\n\n## 🚀 Features\n\n- 🌐 Internet access for searches and information gathering\n- 💾 Long-term and short-term memory management\n- 🧠 GPT-4 instances for text generation\n- 🔗 Access to popular websites and platforms\n- 🗃️ File storage and summarization with GPT-3.5\n- 🔌 Extensibility with Plugins\n\n## Quickstart\n\n0. Check out the [wiki](https://github.com/Significant-Gravitas/Nexus/wiki)\n1. Get an OpenAI [API Key](https://platform.openai.com/account/api-keys)\n2. Download the [latest release](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest)\n3. Follow the [installation instructions][docs/setup]\n4. Configure any additional features you want, or install some [plugins][docs/plugins]\n5. [Run][docs/usage] the app\n\nPlease see the [documentation][docs] for full setup instructions and configuration options.\n\n[docs]: https://docs.agpt.co/\n\n## 📖 Documentation\n* [⚙️ Setup][docs/setup]\n* [💻 Usage][docs/usage]\n* [🔌 Plugins][docs/plugins]\n* Configuration\n  * [🔍 Web Search](https://docs.agpt.co/configuration/search/)\n  * [🧠 Memory](https://docs.agpt.co/configuration/memory/)\n  * [🗣️ Voice (TTS)](https://docs.agpt.co/configuration/voice/)\n  * [🖼️ Image Generation](https://docs.agpt.co/configuration/imagegen/)\n\n[docs/setup]: https://docs.agpt.co/setup/\n[docs/usage]: https://docs.agpt.co/usage/\n[docs/plugins]: https://docs.agpt.co/plugins/\n\n## ⚠️ Limitations\n\nThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:\n\n1. Not a polished application or product, just an experiment\n2. May not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!\n3. Quite expensive to run, so set and monitor your API key limits with OpenAI!\n\n## 🛡 Disclaimer\n\nThis project, Auto-GPT, is an experimental application and is provided \"as-is\" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.\n\nThe developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.\n\n**Please note that the use of the GPT-4 language model can be expensive due to its token usage.** By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.\n\nAs an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.\n\nBy using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.\n\n## 🐦 Connect with Us on Twitter\n\nStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.\n\n- **Developer**: Follow [@siggravitas](https://twitter.com/siggravitas) for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.\n- **Entrepreneur-GPT**: Join the conversation with the AI itself by following [@En_GPT](https://twitter.com/En_GPT). Share your experiences, discuss the AI's outputs, and engage with the growing community of users.\n\nWe look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!\n\n<p align=\"center\">\n  <a href=\"https://star-history.com/#Torantulino/auto-gpt&Date\">\n    <img src=\"https://api.star-history.com/svg?repos=Torantulino/auto-gpt&type=Date\" alt=\"Star History Chart\">\n  </a>\n</p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpt",
        "realtime",
        "auto",
        "auto gpt",
        "gpt autonomously",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "a1j9o94--swiss": {
      "owner": "a1j9o94",
      "name": "swiss",
      "url": "https://github.com/a1j9o94/swiss",
      "imageUrl": "/freedevtools/mcp/pfp/a1j9o94.webp",
      "description": "Swiss MCP is designed to orchestrate complex AI tasks by coordinating multiple AI tools, enabling streamlined interactions across various resources.",
      "stars": 13,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-07T15:06:13Z",
      "readme_content": "# 🇨🇭 Swiss MCP: A Swiss Army Knife for Multi-Step AI Task Orchestration 🚀\n\nSwiss MCP is your AI-powered command center for orchestrating complex tasks with ease. Think of it as your personal AI assistant that can coordinate multiple AI tools to accomplish amazing things! 🎯\n\n---\n\n## ⚡ Quick Start: Install and Get Running\n\n### Installation\n\n1. **Install `uv` for package management** 📦:\n   ```bash\n   pip install uv\n   ```\n\n2. **Clone this repository** 📥:\n   ```bash\n   git clone https://github.com/your-repo/swiss-mcp.git\n   cd swiss-mcp\n   ```\n\n3. **Install Swiss MCP** 🔧:\n   ```bash\n   pip install fastmcp\n   fastmcp install swiss\n   ```\n\n4. **Start your AI journey** 🎉:\n   ```bash\n   fastmcp swiss\n   ```\n\nYou're all set! Time to unleash the power of Swiss MCP! 💪\n\n---\n\n## 🎮 Mind-Blowing Examples: What Swiss MCP Can Do\n\n### Example 1: AI-Powered Content Creation Studio 🎨\n\n**Assistant**: \"How can I help you create today?\"\n\n**User**: \"Create a viral social media campaign for my new product.\"\n\n**Swiss MCP**:\n1. 🧠 Strategic Planning:\n   - Analyzes market trends using `market-analyzer`\n   - Identifies target audience demographics\n   - Generates content ideas using multiple AI models\n\n2. 🎨 Content Creation:\n   - Creates eye-catching visuals with `stable-diffusion`\n   - Writes engaging copy with `content-writer`\n   - Generates video shorts with `video-generator`\n\n3. 📊 Campaign Optimization:\n   - A/B tests different versions\n   - Schedules posts for optimal timing\n   - Tracks engagement metrics\n\n**Result**: A complete, ready-to-launch campaign with images, videos, and copy optimized for multiple platforms! 🚀\n\n---\n\n### Example 2: Full-Stack App Development Assistant 👨‍💻\n\n**Assistant**: \"Let's build something amazing!\"\n\n**User**: \"Create a modern web app with AI features.\"\n\n**Swiss MCP**:\n1. 🏗️ Architecture Design:\n   - Generates system architecture diagram\n   - Sets up project structure\n   - Creates CI/CD pipeline\n\n2. 🔧 Development:\n   - Scaffolds frontend with `react-builder`\n   - Implements backend with `api-generator`\n   - Integrates AI features using `ai-integrator`\n\n3. 🚀 Deployment:\n   - Containerizes application\n   - Sets up cloud infrastructure\n   - Deploys with monitoring\n\n**Result**: A production-ready web app with AI capabilities, complete with documentation and monitoring! 🎉\n\n---\n\n## 🛠️ Tool Library\n\nThe MCP ecosystem is constantly growing! Just ask Swiss MCP to install any tool you need, and it'll handle the rest. \n\n🔍 Browse the [MCP Tool Library](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file) for more amazing tools!\n\nReady to start building amazing things? Let Swiss MCP be your AI-powered companion! 🚀✨",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "realtime",
        "collaboration",
        "ai tools",
        "multiple ai",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "abhinavkale-dev--webRTC-basics": {
      "owner": "abhinavkale-dev",
      "name": "webRTC-basics",
      "url": "https://github.com/abhinavkale-dev/webRTC-basics",
      "imageUrl": "/freedevtools/mcp/pfp/abhinavkale-dev.webp",
      "description": "A basic project for learning and experimenting with WebRTC concepts, enabling peer-to-peer communication. Provides foundational tools and implementations for real-time communication technologies.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-03T17:07:03Z",
      "readme_content": "Basic WebRTC project \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webrtc",
        "realtime",
        "collaboration",
        "webrtc concepts",
        "experimenting webrtc",
        "dev webrtc"
      ],
      "category": "realtime-collaboration"
    },
    "aech-ai--mcp-teams-test": {
      "owner": "aech-ai",
      "name": "mcp-teams-test",
      "url": "https://github.com/aech-ai/mcp-teams-test",
      "imageUrl": "/freedevtools/mcp/pfp/aech-ai.webp",
      "description": "Integrates Microsoft Teams chat and messaging with MCP-compatible clients, offering advanced search, persistent storage, and live event streaming. Utilizes PostgreSQL and DuckDB for efficient message retrieval and management, while providing a CLI client for local testing and token management.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-25T18:13:52Z",
      "readme_content": "# Teams Messenger MCP App\n\nThis project implements a pure Model Context Protocol (MCP) server that bridges Microsoft Teams and MCP-compatible clients (LLMs, agentic frameworks, and a rich CLI MCP client). All features are exposed via MCP tools, resources, and events—no REST API endpoints.\n\n## Features\n- Microsoft Teams chat/message integration via MCP\n- PostgreSQL-based Information Retrieval (IR) server for advanced search capabilities\n- Persistent storage in DuckDB for chat/message history\n- Hybrid semantic and lexical search (BM25 + vector, FlockMTL-style)\n- CLI for login/token management and a rich MCP client for local testing\n- Polling-based event emission for new messages\n- Live event streaming and search for LLMs and CLI\n- Single-agent (bot) account, not multi-user\n\n## Architecture\n```\n+-------------------+      +-------------------+      +-------------------+\n|   CLI MCP Client  |<---->|    MCP Server     |<---->|  Microsoft Teams  |\n| (rich terminal UI)|      | (Python, FastMCP) |      |  (Graph API)      |\n+-------------------+      +-------------------+      +-------------------+\n         |                        |                          \n         |                        v                          \n         |                +-------------------+      +-------------------+\n         |                |     DuckDB DB     |      |    IR Server      |\n         |                +-------------------+      | (PostgreSQL, API) |\n                                                     +-------------------+\n                                                              |\n                                                              v\n                                                     +-------------------+\n                                                     |  PostgreSQL DB    |\n                                                     |  (with pgvector)  |\n                                                     +-------------------+\n```\n- All chat/message/search logic is via MCP tools/resources/events\n- Teams MCP server uses DuckDB for message storage\n- IR server provides advanced search capabilities with PostgreSQL and pgvector\n- IR server exposes an HTTP API for MCP server communication\n\n## Installation\n\n### Requirements\n- Python 3.9+\n- [pip](https://pip.pypa.io/en/stable/)\n- [Docker](https://www.docker.com/) and [Docker Compose](https://docs.docker.com/compose/) (for containerized deployment)\n\n### Option 1: Local Installation\n\n#### 1. Clone the repository\n```bash\ngit clone <your-repo-url>\ncd mcp-teams\n```\n\n#### 2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n#### 3. Configure environment variables\nCopy the template and fill in your Azure AD/Teams credentials:\n```bash\ncp .env.template .env\n# Edit .env and fill in your Azure AD and other settings\n```\nSee the table below for variable descriptions.\n\n### Option 2: Docker Deployment (Recommended)\n\n#### 1. Clone the repository\n```bash\ngit clone <your-repo-url>\ncd mcp-teams\n```\n\n#### 2. Configure environment variables\nCopy the template and fill in your credentials:\n```bash\ncp .env.template .env\n# Edit .env and fill in your settings\n```\n\n#### 3. Build and start services\n```bash\ndocker-compose up -d\n```\n\n## Environment Variables (.env)\n\n| Variable            | Description                                                      | Example / Default           |\n|---------------------|------------------------------------------------------------------|-----------------------------|\n| AZURE_CLIENT_ID      | Azure AD Application (client) ID                                 | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| AZURE_CLIENT_SECRET  | Azure AD Application secret                                      | `your-secret`               |\n| AZURE_TENANT_ID      | Azure AD Tenant ID                                               | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| AZURE_APP_OBJECT_ID  | Azure AD Application object ID                                  | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| DUCKDB_PATH         | Path to DuckDB database file                                     | `db/teams_mcp.duckdb`       |\n| TOKEN_PATH          | Path to store persistent token cache                             | `db/token_cache.json`        |\n| POLL_INTERVAL       | Polling interval (seconds) for new messages                      | `10`                        |\n| DEMO_MODE           | Set to `true` for mock/demo mode (no real Teams API calls)       | `false`                     |\n| OPENAI_API_KEY      | OpenAI API key for embedding generation                         | `sk-...`                    |\n| POSTGRES_USER       | PostgreSQL username                                              | `postgres`                  |\n| POSTGRES_PASSWORD   | PostgreSQL password                                              | `postgres`                  |\n| POSTGRES_DB         | PostgreSQL database name                                         | `mcp_ir`                    |\n| IR_SERVER_HOST      | IR server hostname                                               | `ir_server`                 |\n| IR_SERVER_PORT      | IR server port                                                   | `8090`                      |\n\n## Running the MCP Server\n\n### Local Mode (without Docker)\n```bash\npython mcp_server/server.py\n```\n\n### Docker Mode (All Services)\n```bash\ndocker-compose up -d\n```\n\nTo check logs:\n```bash\ndocker-compose logs -f teams_mcp  # Teams MCP server logs\ndocker-compose logs -f ir_server  # IR server logs\n```\n\n### Demo Mode (no real Teams API calls)\nSet `DEMO_MODE=true` in your `.env` and run as above.\n\n## CLI Usage\n\n### 1. Login and Token Management\n```bash\npython cli/login.py login\npython cli/login.py status\npython cli/login.py logout\n```\n\n### 2. Rich CLI MCP Client\nAll commands below use the MCP stdio protocol to talk to the server.\n\n#### List chats\n```bash\npython cli/mcp_client.py list_chats\n```\n\n#### Get messages from a chat\n```bash\npython cli/mcp_client.py get_messages <chat_id>\n```\n\n#### Send a message\n```bash\npython cli/mcp_client.py send_message <chat_id> \"Hello from CLI!\"\n```\n\n#### Create a new 1:1 chat\n```bash\npython cli/mcp_client.py create_chat <user_id_or_email>\n```\n\n#### Search messages (hybrid, BM25, or vector)\n```bash\npython cli/mcp_client.py search_messages \"project update\" --mode hybrid --top_k 5\n```\n\n#### Stream new incoming messages (live event subscription)\n```bash\npython cli/mcp_client.py stream\n```\n\n## IR Server Usage\n\nThe IR server provides advanced search capabilities with PostgreSQL and pgvector. It exposes an HTTP API for MCP server communication.\n\n### IR Server API Endpoints\n\n#### 1. Health Check\n```\nGET http://localhost:8090/\n```\n\n#### 2. List Available Tools\n```\nGET http://localhost:8090/api/tools\n```\n\n#### 3. Search Content\n```\nPOST http://localhost:8090/api/tools/search\n```\nBody:\n```json\n{\n  \"query\": \"your search query\",\n  \"search_type\": \"hybrid\",\n  \"limit\": 10\n}\n```\n\n#### 4. Index Content\n```\nPOST http://localhost:8090/api/tools/index_content\n```\nBody:\n```json\n{\n  \"content\": \"Text content to index\",\n  \"source_type\": \"teams\",\n  \"metadata\": {\n    \"author\": \"User Name\",\n    \"created\": \"2025-04-01T12:00:00Z\"\n  }\n}\n```\n\nFor more detailed IR server documentation, see [ir/README.md](ir/README.md).\n\n## Search and Event Streaming\n- **Hybrid search**: Combines BM25 and vector search with LLM reranking\n- **Live streaming**: Subscribe to `messages/incoming` for real-time updates\n\n## Development & Extension\n- Add new MCP tools/resources in `mcp_server/server.py`\n- Extend Teams integration in `teams/graph.py`\n- Modify IR capabilities in the IR server\n- Add analytics, summarization, or RAG features using DuckDB, PostgreSQL, and LLMs\n- Use the CLI as a test harness for all MCP features\n\n## Troubleshooting & FAQ\n- **Login fails**: Check your Azure AD credentials and `.env` values\n- **No messages appear**: Ensure polling is running and your bot account is in the Teams chat\n- **DuckDB errors**: Check file permissions and paths in `.env`\n- **IR server not responding**: Check Docker logs and ensure the container is running\n- **Demo mode**: Set `DEMO_MODE=true` for local testing without real Teams\n\n## References\n- [Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB (FlockMTL)](https://arxiv.org/html/2504.01157v1)\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Microsoft Graph API docs](https://learn.microsoft.com/en-us/graph/overview)\n- [PostgreSQL with pgvector extension](https://github.com/pgvector/pgvector)\n\n---\nFor full product details, see [`specs/app-spec.md`](specs/app-spec.md).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "chat",
        "realtime",
        "microsoft teams",
        "teams chat",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "agree-able--room-mcp": {
      "owner": "agree-able",
      "name": "room-mcp",
      "url": "https://github.com/agree-able/room-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/agree-able.webp",
      "description": "Connects and interacts with virtual rooms using the Room protocol, enabling agents to collaborate in a peer-to-peer environment for various tasks.",
      "stars": 16,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-03T16:37:58Z",
      "readme_content": "# Room MCP\n\n[![smithery badge](https://smithery.ai/badge/@agree-able/room-mcp)](https://smithery.ai/server/@agree-able/room-mcp)\n\nA command-line tool for using MCP (Model Context Protocol) with the Room protocol.\n\nThis allows claude to create virutal rooms in a p2p space with other agents to accomplish a goal.\n\n<a href=\"https://glama.ai/mcp/servers/p6xyqb1e9e\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/p6xyqb1e9e/badge\" alt=\"Room MCP server\" />\n</a>\n\nHere is claude hosting a room, and giving out the invite code for the other party to join.\n\n<p align=\"center\">\n  \n</p>\n\nHere is an example of connecting to a room for [20 Questions](https://github.com/agree-able/20-questions-bot)\n\n<p align=\"center\">\n  \n</p>\n\nWe've also adding in directives to help the agent balance goals and risk in performing its task.\n\n<p align=\"center\">\n  \n</p>\n\nYou should check out the other [exciting examples](docs/examples.md)\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install Room MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@agree-able/room-mcp):\n\n```bash\nnpx -y @smithery/cli install @agree-able/room-mcp --client claude\n```\n\n### Manual Installation\nYou can use this tool directly with npm:\n\n```bash\nnpm -y @agree-able/room-mcp\n```\n## Adding to Claude Desktop\n\nSee https://modelcontextprotocol.io/quickstart/user for more details.\n\nAdd the following to your claude_desktop_config.json:\n\n```\n{\n  \"mcpServers\": {\n    \"room\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@agree-able/room-mcp\"\n      ],\n      \"env\": {\n        \"ROOM_TRANSCRIPTS_FOLDER\": \"/path/to/transcripts\" // Optional: Set to save room transcripts\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `ROOM_TRANSCRIPTS_FOLDER`: When set, conversation transcripts will be saved as JSON files in this folder when a room is exited. If the folder doesn't exist, it will be created automatically.\n\n## Available Tools\n\nThe Room MCP package provides the following capabilities:\n\n- **Room Protocol Integration**: Connect to and interact with rooms using the Room protocol\n- **MCP Support**: Utilize Model Context Protocol for enhanced model interactions\n- **Invitation Management**: Create and manage invitations using the @agree-able/invite package\n- **Transcript Storage**: Save conversation transcripts to disk when `ROOM_TRANSCRIPTS_FOLDER` environment variable is set\n\n## Related Packages\n\nThis tool depends on:\n\n- [@agree-able/invite](https://github.com/agree-able/invite): For invitation management\n- [@agree-able/room](https://github.com/agree-able/room): For Room protocol implementation\n- [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk): For MCP functionality\n\n## License\n\nApache License\nVersion 2.0",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rooms",
        "collaboration",
        "realtime",
        "room protocol",
        "virtual rooms",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "bsmi021--mcp-conversation-server": {
      "owner": "bsmi021",
      "name": "mcp-conversation-server",
      "url": "https://github.com/bsmi021/mcp-conversation-server",
      "imageUrl": "/freedevtools/mcp/pfp/bsmi021.webp",
      "description": "Manage and interact with multiple conversations using OpenRouter's language models through a standardized interface, facilitating message creation, real-time streaming, and persistent conversation states. Features include error handling and recovery, along with automatic token counting and model context management.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T22:18:09Z",
      "readme_content": "# MCP Conversation Server\n\nA Model Context Protocol (MCP) server implementation for managing conversations with OpenRouter's language models. This server provides a standardized interface for applications to interact with various language models through a unified conversation management system.\n\n## Features\n\n- **MCP Protocol Support**\n  - Full MCP protocol compliance\n  - Resource management and discovery\n  - Tool-based interaction model\n  - Streaming response support\n  - Error handling and recovery\n\n- **OpenRouter Integration**\n  - Support for all OpenRouter models\n  - Real-time streaming responses\n  - Automatic token counting\n  - Model context window management\n  - Available models include:\n    - Claude 3 Opus\n    - Claude 3 Sonnet\n    - Llama 2 70B\n    - And many more from OpenRouter's catalog\n\n- **Conversation Management**\n  - Create and manage multiple conversations\n  - Support for system messages\n  - Message history tracking\n  - Token usage monitoring\n  - Conversation filtering and search\n\n- **Streaming Support**\n  - Real-time message streaming\n  - Chunked response handling\n  - Token counting\n\n- **File System Persistence**\n  - Conversation state persistence\n  - Configurable storage location\n  - Automatic state management\n\n## Installation\n\n```bash\nnpm install mcp-conversation-server\n```\n\n## Configuration\n\n### Configuration\n\nAll configuration for the MCP Conversation Server is now provided via YAML. Please update the `config/models.yaml` file with your settings. For example:\n\n```yaml\n# MCP Server Configuration\nopenRouter:\n  apiKey: \"YOUR_OPENROUTER_API_KEY\"  # Replace with your actual OpenRouter API key.\n\npersistence:\n  path: \"./conversations\"  # Directory for storing conversation data.\n\nmodels:\n  # Define your models here\n  'provider/model-name':\n    id: 'provider/model-name'\n    contextWindow: 123456\n    streaming: true\n    temperature: 0.7\n    description: 'Model description'\n\n# Default model to use if none specified\ndefaultModel: 'provider/model-name'\n```\n\n### Server Configuration\n\nThe MCP Conversation Server now loads all its configuration from the YAML file. In your application, you can load the configuration as follows:\n\n```typescript\nconst config = await loadModelsConfig(); // Loads openRouter, persistence, models, and defaultModel settings from 'config/models.yaml'\n```\n\n*Note: Environment variables are no longer required as all configuration is provided via the YAML file.*\n\n## Usage\n\n### Basic Server Setup\n\n```typescript\nimport { ConversationServer } from 'mcp-conversation-server';\n\nconst server = new ConversationServer(config);\nserver.run().catch(console.error);\n```\n\n### Available Tools\n\nThe server exposes several MCP tools:\n\n1. **create-conversation**\n\n   ```typescript\n   {\n       provider: 'openrouter',    // Provider is always 'openrouter'\n       model: string,             // OpenRouter model ID (e.g., 'anthropic/claude-3-opus-20240229')\n       title?: string;            // Optional conversation title\n   }\n   ```\n\n2. **send-message**\n\n   ```typescript\n   {\n       conversationId: string;  // Conversation ID\n       content: string;         // Message content\n       stream?: boolean;        // Enable streaming responses\n   }\n   ```\n\n3. **list-conversations**\n\n   ```typescript\n   {\n       filter?: {\n           model?: string;      // Filter by model\n           startDate?: string;  // Filter by start date\n           endDate?: string;    // Filter by end date\n       }\n   }\n   ```\n\n### Resources\n\nThe server provides access to several resources:\n\n1. **conversation://{id}**\n   - Access specific conversation details\n   - View message history\n   - Check conversation metadata\n\n2. **conversation://list**\n   - List all active conversations\n   - Filter conversations by criteria\n   - Sort by recent activity\n\n## Development\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n### Debugging\n\nThe server provides several debugging features:\n\n1. **Error Logging**\n   - All errors are logged with stack traces\n   - Token usage tracking\n   - Rate limit monitoring\n\n2. **MCP Inspector**\n\n   ```bash\n   npm run inspector\n   ```\n\n   Use the MCP Inspector to:\n   - Test tool execution\n   - View resource contents\n   - Monitor message flow\n   - Validate protocol compliance\n\n3. **Provider Validation**\n\n   ```typescript\n   await server.providerManager.validateProviders();\n   ```\n\n   Validates:\n   - API key validity\n   - Model availability\n   - Rate limit status\n\n### Troubleshooting\n\nCommon issues and solutions:\n\n1. **OpenRouter Connection Issues**\n   - Verify your API key is valid\n   - Check rate limits on [OpenRouter's dashboard](https://openrouter.ai/dashboard)\n   - Ensure the model ID is correct\n   - Monitor credit usage\n\n2. **Message Streaming Errors**\n   - Verify model streaming support\n   - Check connection stability\n   - Monitor token limits\n   - Handle timeout settings\n\n3. **File System Errors**\n   - Check directory permissions\n   - Verify path configuration\n   - Monitor disk space\n   - Handle concurrent access\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nISC License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bsmi021",
        "conversations",
        "conversation",
        "conversation server",
        "collaboration bsmi021",
        "conversations using"
      ],
      "category": "realtime-collaboration"
    },
    "czh2774--cocosMCP": {
      "owner": "czh2774",
      "name": "cocosMCP",
      "url": "https://github.com/czh2774/cocosMCP",
      "imageUrl": "/freedevtools/mcp/pfp/czh2774.webp",
      "description": "Synchronizes logs between Cocos Creator and Cursor AI, enabling efficient problem analysis and resolution. Features include real-time log syncing, intelligent filtering, and scene management capabilities.",
      "stars": 23,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-20T05:09:21Z",
      "readme_content": "# Cocos MCP Log Bridge\n\n一个强大的日志桥接工具，用于在 Cocos Creator 编辑器和 Cursor AI 之间同步日志信息，帮助开发者更有效地分析和解决问题。\n\n[![GitHub stars](https://img.shields.io/github/stars/czh2774/cocosMCP.svg)](https://github.com/czh2774/cocosMCP/stargazers)\n[![License](https://img.shields.io/github/license/czh2774/cocosMCP.svg)](https://github.com/czh2774/cocosMCP/blob/main/LICENSE)\n\n![Cocos Creator](https://img.shields.io/badge/Cocos%20Creator-3.8.0%2B-blue)\n![Cursor AI](https://img.shields.io/badge/Cursor%20AI-Compatible-green)\n\n## 🌟 功能特点\n\n- **实时日志同步**: 直接从 Cocos Creator 编辑器获取最新日志\n- **智能过滤**: 支持按类型过滤（普通日志、警告、错误）\n- **关键词搜索**: 精确定位特定问题\n- **一键清除**: 随时清空日志以减少干扰\n- **场景信息**: 获取当前场景的基本信息和节点列表\n- **场景操作**: 支持打开场景等基础操作\n- **TCP 通信桥接**: 稳定可靠的通信机制\n- **Cursor AI 集成**: 完全兼容 Cursor MCP 协议\n\n## 🚀 快速入门\n\n### 前置条件\n\n- Cocos Creator 3.8.0 或更高版本\n- Python 3.7 或更高版本\n- uv 包管理器 (推荐) 或 pip\n\n### 安装步骤\n\n1. **克隆仓库**\n   ```bash\n   git clone https://github.com/czh2774/cocosMCP.git\n   ```\n\n2. **复制到 Cocos Creator 项目**\n   \n   将克隆的 `cocosMCP` 目录复制到你的 Cocos Creator 项目的 `extensions` 目录下。\n\n3. **安装 Python 依赖**\n   ```bash\n   cd your-project/extensions/cocosMCP/Python\n   uv pip install -r requirements.txt\n   ```\n\n4. **在 Cocos Creator 中启用扩展**\n   \n   启动 Cocos Creator，进入 `扩展 -> 扩展管理器`，确保 `cocosMCP` 扩展已启用。\n\n5. **配置 Cursor AI**\n   \n   在 Cursor AI 设置中配置 MCP 服务器，指向 Python 服务器脚本。\n\n### 基本用法\n\n```python\n# 查询日志\nlogs = await mcp.query_logs({\n    \"show_logs\": True,\n    \"show_warnings\": True,\n    \"show_errors\": True\n})\n\n# 清除日志\nawait mcp.clear_logs()\n\n# 检查连接状态\nstatus = await mcp.connection_status()\n\n# 获取场景信息\nscene_info = await mcp.get_scene_info()\n\n# 列出场景中的所有节点\nnodes = await mcp.list_scene_nodes()\n\n# 打开指定UUID的场景\nawait mcp.open_scene(\"scene-uuid-here\")\n```\n\n## 📚 详细文档\n\n本项目包含三个详细的文档:\n\n- [用户使用指南](USAGE.md): 安装、配置和使用方法\n- [开发者指南](DEVELOPMENT.md): 代码结构、扩展功能和维护说明\n- [问题排查](TROUBLESHOOTING.md): 常见问题和解决方案\n\n## 🔧 技术架构\n\nCocos MCP 由三个主要部分组成:\n\n1. **Cocos Creator 扩展**: TypeScript 编写的编辑器扩展\n2. **TCP 通信桥**: 连接编辑器和 Python 服务器\n3. **Python MCP 服务器**: 处理 Cursor AI 的请求\n\n![架构图](https://via.placeholder.com/800x400?text=Cocos+MCP+Architecture)\n\n## 🤝 贡献指南\n\n欢迎贡献代码、报告问题或提出新功能建议！请查看 [开发者指南](DEVELOPMENT.md) 了解详情。\n\n## 📄 许可证\n\n本项目采用 MIT 许可证 - 详情请参阅 [LICENSE](LICENSE) 文件。\n\n## 🙏 致谢\n\n- Cocos Creator 团队提供的优秀游戏引擎\n- Cursor AI 团队开发的智能编程助手\n- 所有贡献者和用户的支持和反馈\n\n---\n\n如有问题或建议，请提交 [Issues](https://github.com/czh2774/cocosMCP/issues)。 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cocosmcp",
        "cocos",
        "realtime",
        "cocosmcp synchronizes",
        "logs cocos",
        "cocos creator"
      ],
      "category": "realtime-collaboration"
    },
    "eperez28--tldraw": {
      "owner": "eperez28",
      "name": "tldraw",
      "url": "https://github.com/eperez28/tldraw",
      "imageUrl": "/freedevtools/mcp/pfp/eperez28.webp",
      "description": "A digital whiteboard library designed for creating infinite canvas experiences in React applications, enabling collaborative drawing and design functionalities.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2025-04-29T08:01:08Z",
      "readme_content": "# tldraw\n\nWelcome to the public monorepo for [tldraw](https://github.com/tldraw/tldraw). tldraw is a library for creating infinite canvas experiences in React. It's the software behind the digital whiteboard [tldraw.com](https://tldraw.com).\n\n- Read the docs and learn more at [tldraw.dev](https://tldraw.dev).\n- Learn about [our license](https://github.com/tldraw/tldraw#License).\n\n> [Click here](https://tldraw.dev/#pricing) to learn about our license and pricing.\n\n## Installation\n\n```bash\nnpm i tldraw\n```\n\n## Usage\n\n```tsx\nimport { Tldraw } from 'tldraw'\nimport 'tldraw/tldraw.css'\n\nexport default function App() {\n\treturn (\n\t\t<div style={{ position: 'fixed', inset: 0 }}>\n\t\t\t<Tldraw />\n\t\t</div>\n\t)\n}\n```\n\nLearn more at [tldraw.dev](https://tldraw.dev).\n\n## Local development\n\nThe local development server will run our examples app. The basic example will show any changes you've made to the codebase.\n\nTo run the local development server, first clone this repo.\n\nEnable [corepack](https://nodejs.org/api/corepack.html) to make sure you have the right version of `yarn`:\n\n```bash\nnpm i -g corepack\n```\n\nInstall dependencies:\n\n```bash\nyarn\n```\n\nStart the local development server:\n\n```bash\nyarn dev\n```\n\nOpen the example project at `localhost:5420`.\n\n## License\n\nThe tldraw SDK is provided under the [tldraw license](https://github.com/tldraw/tldraw/blob/main/LICENSE.md).\n\nYou can use the tldraw SDK in commercial or non-commercial projects so long as you preserve the \"Made with tldraw\" watermark on the canvas. To remove the watermark, you can purchase a [business license](https://tldraw.dev#pricing). Visit [tldraw.dev](https://tldraw.dev) to learn more.\n\n## Trademarks\n\nCopyright (c) 2024-present tldraw Inc. The tldraw name and logo are trademarks of tldraw. Please see our [trademark guidelines](https://github.com/tldraw/tldraw/blob/main/TRADEMARKS.md) for info on acceptable usage.\n\n## Distributions\n\nYou can find tldraw on npm [here](https://www.npmjs.com/package/@tldraw/tldraw?activeTab=versions).\n\n## Contribution\n\nPlease see our [contributing guide](https://github.com/tldraw/tldraw/blob/main/CONTRIBUTING.md). Found a bug? Please [submit an issue](https://github.com/tldraw/tldraw/issues/new).\n\n## Community\n\nHave questions, comments or feedback? [Join our discord](https://discord.tldraw.com/?utm_source=github&utm_medium=readme&utm_campaign=sociallink). For the latest news and release notes, visit [tldraw.dev](https://tldraw.dev).\n\n## Contributors\n\n<a href=\"https://github.com/tldraw/tldraw/graphs/contributors\">\n  <img alt=\"tldraw_max_400_columns_20\" src=\"https://contrib.rocks/image?repo=tldraw/tldraw&max=400&columns=20\" width=\"100%\"/>\n</a>\n\n## Star History\n\n<a href=\"https://star-history.com/#tldraw/tldraw\">\n\t<picture>\n\t  <source\n\t    media=\"(prefers-color-scheme: dark)\"\n\t    srcset=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date&theme=dark\"\n\t  />\n\t  <source\n\t    media=\"(prefers-color-scheme: light)\"\n\t    srcset=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date\"\n\t  />\n\t  <img src=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date\" alt=\"Star History Chart\" width=\"100%\" />\n\t</picture>\n</a>\n\n## Contact\n\nFind us on Twitter/X at [@tldraw](https://twitter.com/tldraw). You can contact us by email at [hello@tldraw.com](mailto:hello@tldraw.com).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "canvas",
        "react",
        "collaborative",
        "canvas experiences",
        "collaborative drawing",
        "collaboration eperez28"
      ],
      "category": "realtime-collaboration"
    },
    "getzep--graphiti": {
      "owner": "getzep",
      "name": "graphiti",
      "url": "https://github.com/getzep/graphiti",
      "imageUrl": "/freedevtools/mcp/pfp/getzep.webp",
      "description": "Enables the construction and querying of real-time, temporally-aware knowledge graphs, managing entities, relationships, and episodes. Facilitates semantic and hybrid searches to enhance memory and reasoning in AI agents.",
      "stars": 18616,
      "forks": 1712,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T11:15:53Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://www.getzep.com/\">\n    <img src=\"https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73\" width=\"150\" alt=\"Zep Logo\">\n  </a>\n</p>\n\n<h1 align=\"center\">\nGraphiti\n</h1>\n<h2 align=\"center\"> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=\"center\">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/12986\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/12986\" alt=\"getzep%2Fgraphiti | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful\n> Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=\"center\">\n    \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _\"Kendra loves Adidas shoes.\"_ Each fact is a \"triplet\"\nrepresented by two entities, or\nnodes (\"Kendra\", \"Adidas shoes\"), and their relationship, or edge (\"loves\"). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.\n\n## Graphiti and Zep's Context Engineering Platform.\n\nGraphiti powers the core of [Zep](https://www.getzep.com), a turn-key context engineering platform for AI Agents. Zep\noffers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.\n\nUsing Graphiti, we've demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2501.13956\"></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for\nfrequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time\n  queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve\n  low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through\n  straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=\"center\">\n    \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n|----------------------------|---------------------------------------|--------------------------------------------------|\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it\nparticularly suitable for applications requiring real-time interaction and precise historical queries.\n\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon\n  OpenSearch Serverless collection (serves as the full text search backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### Installing with Kuzu Support\n\nIf you plan to use Kuzu as your graph database backend, install with the Kuzu extra:\n\n```bash\npip install graphiti-core[kuzu]\n\n# or with uv\nuv add graphiti-core[kuzu]\n```\n\n### Installing with Amazon Neptune Support\n\nIf you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:\n\n```bash\npip install graphiti-core[neptune]\n\n# or with uv\nuv add graphiti-core[neptune]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n\n# Install with Amazon Neptune\npip install graphiti-core[neptune]\n```\n\n## Default to Low Concurrency; LLM Provider 429 Rate Limit Errors\n\nGraphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM\nProvider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.\n\nConcurrency controlled by the `SEMAPHORE_LIMIT` environment variable. By default, `SEMAPHORE_LIMIT` is set to `10`\nconcurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try\nlowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion\nperformance.\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your\n> environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory.\nThe quickstart demonstrates:\n\n1. Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with\nsetup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server\nallows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant\nworkflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).\n\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it\nto the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=\"bolt://localhost:7687\",\n    user=\"neo4j\",\n    password=\"password\",\n    database=\"my_custom_database\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=\"localhost\",\n    port=6379,\n    username=\"falkor_user\",  # Optional\n    password=\"falkor_password\",  # Optional\n    database=\"my_custom_graph\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### Kuzu\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.kuzu_driver import KuzuDriver\n\n# Create a Kuzu driver\ndriver = KuzuDriver(db=\"/tmp/graphiti.kuzu\")\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### Amazon Neptune\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neptune_driver import NeptuneDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = NeptuneDriver(\n    host= < NEPTUNE\nENDPOINT >,\naoss_host = < Amazon\nOpenSearch\nServerless\nHost >,\nport = < PORT >  # Optional, defaults to 8182,\n         aoss_port = < PORT >  # Optional, defaults to 443\n)\n\ndriver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different\nendpoints for LLM and embedding services, and separate deployments for default and small models.\n\n> [!IMPORTANT]\n> **Azure OpenAI v1 API Opt-in Required for Structured Outputs**\n>\n> Graphiti uses structured outputs via the `client.beta.chat.completions.parse()` method, which requires Azure OpenAI\n> deployments to opt into the v1 API. Without this opt-in, you'll encounter 404 Resource not found errors during episode\n> ingestion.\n>\n> To enable v1 API support in your Azure OpenAI deployment, follow Microsoft's\n> guide: [Azure OpenAI API version lifecycle](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/api-version-lifecycle?tabs=key#api-evolution).\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = \"<your-api-key>\"\napi_version = \"<your-api-version>\"\nllm_endpoint = \"<your-llm-endpoint>\"  # e.g., \"https://your-llm-resource.openai.azure.com/\"\nembedding_endpoint = \"<your-embedding-endpoint>\"  # e.g., \"https://your-embedding-resource.openai.azure.com/\"\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=\"gpt-4.1-nano\",\n    model=\"gpt-4.1-mini\",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIClient(\n        config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=\"text-embedding-3-small-deployment\"  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```\n\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match\nyour Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini,\nyou'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add \"graphiti-core[google-genai]\"\n\n# or\n\npip install \"graphiti-core[google-genai]\"\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = \"<your-google-api-key>\"\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash\"\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=\"embedding-001\"\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.5-flash-lite-preview-06-17\"\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for\ncost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI\nreranker, leveraging Gemini's log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal\nfor privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\n\n```bash\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=\"ollama\",  # Ollama doesn't require a real API key, but some placeholder is needed\n    model=\"deepseek-r1:7b\",\n    small_model=\"deepseek-r1:7b\",\n    base_url=\"http://localhost:11434/v1\",  # Ollama's OpenAI-compatible endpoint\n)\n\nllm_client = OpenAIGenericClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"ollama\",  # Placeholder API key\n            embedding_model=\"nomic-embed-text\",\n            embedding_dim=768,\n            base_url=\"http://localhost:11434/v1\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.\n\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/integrations/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for\neveryone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n    - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n    - Database backend (Neo4j, FalkorDB, Kuzu, Amazon Neptune Database or Neptune Analytics)\n    - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\n\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\n\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti\n  functionality\n- The anonymous ID is stored locally and is not tied to any personal information\n\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n    - Allow developers to provide their own defined node and edge classes when ingesting episodes\n    - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "graphiti",
        "knowledge",
        "ai agents",
        "reasoning ai",
        "knowledge graphs"
      ],
      "category": "realtime-collaboration"
    },
    "hanweg--mcp-discord": {
      "owner": "hanweg",
      "name": "mcp-discord",
      "url": "https://github.com/hanweg/mcp-discord",
      "imageUrl": "/freedevtools/mcp/pfp/hanweg.webp",
      "description": "Integrate and manage Discord servers, facilitate message sending, and handle user roles and interactions within channels.",
      "stars": 126,
      "forks": 37,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:05Z",
      "readme_content": "# Discord MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@hanweg/mcp-discord)](https://smithery.ai/server/@hanweg/mcp-discord)\nA Model Context Protocol (MCP) server that provides Discord integration capabilities to MCP clients like Claude Desktop.\n\n<a href=\"https://glama.ai/mcp/servers/wvwjgcnppa\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/wvwjgcnppa/badge\" alt=\"mcp-discord MCP server\" /></a>\n\n## Available Tools\n\n### Server Information\n- `list_servers`: List available servers\n- `get_server_info`: Get detailed server information\n- `get_channels`: List channels in a server\n- `list_members`: List server members and their roles\n- `get_user_info`: Get detailed information about a user\n\n### Message Management\n- `send_message`: Send a message to a channel\n- `read_messages`: Read recent message history\n- `add_reaction`: Add a reaction to a message\n- `add_multiple_reactions`: Add multiple reactions to a message\n- `remove_reaction`: Remove a reaction from a message\n- `moderate_message`: Delete messages and timeout users\n\n### Channel Management\n- `create_text_channel`: Create a new text channel\n- `delete_channel`: Delete an existing channel\n\n### Role Management\n- `add_role`: Add a role to a user\n- `remove_role`: Remove a role from a user\n\n## Installation\n\n1. Set up your Discord bot:\n   - Create a new application at [Discord Developer Portal](https://discord.com/developers/applications)\n   - Create a bot and copy the token\n   - Enable required privileged intents:\n     - MESSAGE CONTENT INTENT\n     - PRESENCE INTENT\n     - SERVER MEMBERS INTENT\n   - Invite the bot to your server using OAuth2 URL Generator\n\n2. Clone and install the package:\n```bash\n# Clone the repository\ngit clone https://github.com/hanweg/mcp-discord.git\ncd mcp-discord\n\n# Create and activate virtual environment\nuv venv\n.venv\\Scripts\\activate # On macOS/Linux, use: source .venv/bin/activate\n\n### If using Python 3.13+ - install audioop library: `uv pip install audioop-lts`\n\n# Install the package\nuv pip install -e .\n```\n\n3. Configure Claude Desktop (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):\n```json\n    \"discord\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\PATH\\\\TO\\\\mcp-discord\",\n        \"run\",\n        \"mcp-discord\"\n      ],\n      \"env\": {\n        \"DISCORD_TOKEN\": \"your_bot_token\"\n      }\n    }\n```\n\n### Installing via Smithery\n\nTo install Discord Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hanweg/mcp-discord):\n\n```bash\nnpx -y @smithery/cli install @hanweg/mcp-discord --client claude\n```\n\n## License\n\nMIT License - see LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "mcp",
        "hanweg",
        "mcp discord",
        "manage discord",
        "discord servers"
      ],
      "category": "realtime-collaboration"
    },
    "hirosuke0520--line-mcp-demo": {
      "owner": "hirosuke0520",
      "name": "line-mcp-demo",
      "url": "https://github.com/hirosuke0520/line-mcp-demo",
      "imageUrl": "/freedevtools/mcp/pfp/hirosuke0520.webp",
      "description": "Integrates with the LINE Messaging API to facilitate real-time communication and user engagement through text and rich messages. Allows AI agents to connect seamlessly to LINE Official Accounts for enhanced conversational interactions.",
      "stars": 0,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-04-15T03:21:59Z",
      "readme_content": "[日本語版 READMEはこちら](README.ja.md)\n\n# LINE Bot MCP Server\n\n[Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) server implementation that integrates the LINE Messaging API to connect an AI Agent to the LINE Official Account.\n\n\n\n> [!NOTE]\n> This repository is provided as a preview version. While we offer it for experimental purposes, please be aware that it may not include complete functionality or comprehensive support.\n\n## Tools\n\n1. **push_text_message**\n   - Push a simple text message to user via LINE.\n   - **Inputs:**\n     - `user_id` (string): The user ID to receive a message. Defaults to DESTINATION_USER_ID.\n     - `message.text` (string): The plain text content to send to the user.\n2. **push_flex_message**\n   - Push a highly customizable flex message to user via LINE. Supports both bubble (single container) and carousel (multiple swipeable bubbles) layouts.\n   - **Inputs:**\n     - `user_id` (string): The user ID to receive a message. Defaults to DESTINATION_USER_ID.\n     - `message.altText` (string): Alternative text shown when flex message cannot be displayed.\n     - `message.content` (any): The content of the flex message. This is a JSON object that defines the layout and components of the message.\n     - `message.contents.type` (enum): Type of the container. 'bubble' for single container, 'carousel' for multiple swipeable bubbles.\n3. **get_profile**\n   - Get detailed profile information of a LINE user including display name, profile picture URL, status message and language.\n   - **Inputs:**\n     - `user_id` (string): The ID of the user whose profile you want to retrieve. Defaults to DESTINATION_USER_ID.\n\n\n## Installation\n\n### Step 1: Install line-bot-mcp-server\n\nrequirements:\n- Node.js v20 or later\n\nClone this repository:\n\n```\ngit clone git@github.com/line/line-bot-mcp-server.git\n```\n\nInstall the necessary dependencies and build line-bot-mcp-server when using Node.js. This step is not required when using Docker:\n\n```\ncd line-bot-mcp-server && npm install && npm run build\n```\n\n### Step 2: Get a channel access token\n\nThis MCP server utilizes a LINE Official Account. If you do not have one, please create it by following [this instructions](https://www.linebiz.com/jp-en/manual/OfficialAccountManager/new_account/). \n\nTo connect to the Messaging API, you need to have a channel access token. You can confirm this by following [this instructions](https://developers.line.biz/en/docs/basics/channel-access-token/#long-lived-channel-access-token).\n\nAdditionally, you will need the user ID of the recipient user for messages. You can confirm this by following [this instructions](https://developers.line.biz/en/docs/messaging-api/getting-user-ids/#get-own-user-id).\n\n### Step 3: Configure AI Agent\n\nPlease add the following configuration for an AI Agent like Claude Desktop or Cline. \nInsert the channel access token and user ID you obtained earlier into `CHANNEL_ACCESS_TOKEN` and `DESTINATION_USER_ID`, respectively. \nAdditionally, update the path to `line-bot-mcp-server` in  `mcpServers.args`.\n\n#### Option 1: Use Node\n\n```json\n{\n  \"mcpServers\": {\n    \"line-bot\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"PATH/TO/line-bot-mcp-server/dist/index.js\"\n      ],\n      \"env\": {\n        \"CHANNEL_ACCESS_TOKEN\" : \"FILL_HERE\",\n        \"DESTINATION_USER_ID\" : \"FILL_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Use Docker\n\nBuild the Docker image first:\n```\ndocker build -t line/line-bot-mcp-server .\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"line-bot\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"CHANNEL_ACCESS_TOKEN\",\n        \"-e\",\n        \"DESTINATION_USER_ID\",\n        \"line/line-bot-mcp-server\"\n      ],\n      \"env\": {\n        \"CHANNEL_ACCESS_TOKEN\" : \"FILL_HERE\",\n        \"DESTINATION_USER_ID\" : \"FILL_HERE\"\n      }\n    }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "realtime",
        "line",
        "messaging",
        "line messaging",
        "realtime collaboration",
        "line official"
      ],
      "category": "realtime-collaboration"
    },
    "jmagar--yarr": {
      "owner": "jmagar",
      "name": "yarr",
      "url": "https://github.com/jmagar/yarr",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Automate and control your media services using natural language commands.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "automate",
        "realtime",
        "jmagar",
        "collaboration jmagar",
        "realtime collaboration",
        "yarr automate"
      ],
      "category": "realtime-collaboration"
    },
    "kakehashi-inc--mcp-server-mattermost": {
      "owner": "kakehashi-inc",
      "name": "mcp-server-mattermost",
      "url": "https://github.com/kakehashi-inc/mcp-server-mattermost",
      "imageUrl": "/freedevtools/mcp/pfp/kakehashi-inc.webp",
      "description": "Connects to Mattermost API endpoints to retrieve and process information in real-time, allowing for monitoring of specific teams and channels with secure token-based authentication.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T19:09:15Z",
      "readme_content": "# mcp-server-mattermost\n\nThis project implements a Model Context Protocol (MCP) server for Mattermost integration. It connects to Mattermost API endpoints to retrieve and process various information, making it available through standard MCP transports.\n\n## Features\n\n- Secure, token-based connection to Mattermost API endpoints\n- Supports multiple transport modes:\n  - `stdio`\n  - `http-stream`\n  - `sse`\n- Search for messages across multiple Mattermost channels\n- Customizable default channels and message fetch limits\n\n## Requirements\n\n- Node.js >= 22\n- npm >= 10\n\n## Setup\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/kakehashi-inc/mcp-server-mattermost.git\ncd mcp-server-mattermost\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Set up your environment variables:\n\n### Required Environment Variables\n\n- `MATTERMOST_ENDPOINT`: Your Mattermost server URL\n- `MATTERMOST_TOKEN`: Your Mattermost authentication token\n- `MATTERMOST_TEAM`: The name of the team to monitor\n- `MATTERMOST_CHANNELS`: Comma-separated list of channel names to monitor\n\n### Environment Variable Setup Options\n\n#### Option 1: Direct Environment Variables\n```bash\nexport MATTERMOST_ENDPOINT=\"https://your-mattermost-server.com\"\nexport MATTERMOST_TOKEN=\"your-token-here\"\nexport MATTERMOST_TEAM=\"your-team-name\"\nexport MATTERMOST_CHANNELS=\"general,random,dev\"\n```\n\n#### Option 2: Using .env file (with dotenvx)\n```bash\n# Install dotenvx (optional)\nnpm install -g @dotenvx/dotenvx\n\n# Create .env file\ncp .env.example .env\n# Edit .env file with your values\n\n# Encrypt your .env file (recommended for production)\ndotenvx encrypt\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n## Usage\n\nThe server supports three transport modes: stdio (default), sse, and http-stream.\n\n### Standard I/O Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:stdio\n\n# Direct execution\nnode dist/main.js --transport stdio\n\n# Using npx\nnpx mcp-server-mattermost --transport stdio\n```\n\n### SSE Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:sse\n\n# Direct execution\nnode dist/main.js --transport sse\n```\n\n### HTTP Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:http\n\n# Direct execution\nnode dist/main.js --transport http-stream\n```\n\n## Claude Desktop Integration\n\nTo use this MCP server with Claude Desktop, add the following configuration to your Claude Desktop settings:\n\n### Sample Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"mattermost\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-mattermost@latest\",\n        \"--transport\", \"stdio\",\n        \"--endpoint\", \"https://your-mattermost-server/api/v4\",\n        \"--token\", \"your_personal_access_token\",\n        \"--team\", \"your_team_name\",\n        \"--channels\", \"town-square,general,your_channel_name\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n- `npm run dev`: Start the server in development mode with hot reload\n- `npm run lint`: Run ESLint\n- `npm run format`: Format code using Prettier\n- `npm test`: Run tests\n- `npm run inspect`: Run MCP inspector\n\n## References\n\n- [Model Context Protocol TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [MCP inspector](https://github.com/modelcontextprotocol/inspector)\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kakehashi",
        "mcp",
        "monitoring",
        "collaboration kakehashi",
        "mattermost api",
        "kakehashi mcp"
      ],
      "category": "realtime-collaboration"
    },
    "kazuph--mcp-devin": {
      "owner": "kazuph",
      "name": "mcp-devin",
      "url": "https://github.com/kazuph/mcp-devin",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Integrate Devin AI sessions with Slack for automated task posting, maintaining context in conversations, and managing session information efficiently. Facilitates communication between Devin AI and Slack threads while providing session details and task updates.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-29T19:33:59Z",
      "readme_content": "# @kazuph/mcp-devin MCP Server with Slack Integration\n\nMCP server for Devin AI with Slack integration\n\nThis is a TypeScript-based MCP server that provides integration between Devin AI and Slack. The server enables:\n\n- Creating Devin sessions and automatically posting tasks to Slack\n- Sending messages to Devin sessions and the corresponding Slack threads\n- Managing sessions with enhanced Slack integration\n\n## Features\n\n### Slack Integration\n- Automatically posts Devin tasks to Slack with `@Devin` mentions\n- Maintains thread context between Devin sessions and Slack threads\n- Uses Slack Bot token for authentication\n\n### Tools\n- `create_devin_session` - Create a new Devin session and post to Slack\n  - Posts task to a designated Slack channel with `@Devin` mention\n  - Returns session details and Slack message information\n- `send_message_to_session` - Send a message to a Devin session with optional Slack thread\n  - Can simultaneously post to the Slack thread when provided\n- `get_devin_session` - Get session details with optional Slack message history\n- `list_devin_sessions` - List all Devin sessions\n- `get_organization_info` - Get information about your Devin organization\n\n## Development\n\nInstall dependencies:\n```bash\npnpm install\n```\n\nBuild the server:\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\npnpm run watch\n```\n\n## Configuration\n\n### MCP Server Configuration\n\nThe server is configured through the MCP server configuration file. Add the following to your configuration:\n\n```json\n\"devin-mono\": {\n  \"command\": \"node\",\n  \"args\": [\"/path/to/mcp-devin/build/index.js\"],\n  \"env\": {\n    \"DEVIN_API_KEY\": \"your-devin-api-key\",\n    \"DEVIN_ORG_NAME\": \"Your Organization\",\n    \"SLACK_BOT_TOKEN\": \"xoxb-your-slack-bot-token\",\n    \"SLACK_DEFAULT_CHANNEL\": \"general\"\n  }\n}\n```\n\n### Required Environment Variables\n\nThe following environment variables must be set in the `env` section:\n\n- `DEVIN_API_KEY`: Your Devin API key\n- `DEVIN_ORG_NAME`: (Optional) Your organization name, defaults to \"Default Organization\"\n- `DEVIN_BASE_URL`: (Optional) Base URL for the Devin API, defaults to \"https://api.devin.ai/v1\"\n- `SLACK_BOT_TOKEN`: Your Slack Bot User OAuth Token (starts with xoxb-)\n- `SLACK_DEFAULT_CHANNEL`: The default Slack channel where messages will be posted. You can use either:\n  - Channel ID (e.g. `C123ABC456`)\n  - Channel name (e.g. `general` or `#general`)\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"@kazuph/mcp-devin\": {\n      \"command\": \"/path/to/@kazuph/mcp-devin/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "realtime",
        "session",
        "realtime collaboration",
        "slack automated",
        "ai sessions"
      ],
      "category": "realtime-collaboration"
    },
    "kstrikis--ephor-mcp": {
      "owner": "kstrikis",
      "name": "ephor-mcp",
      "url": "https://github.com/kstrikis/ephor-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kstrikis.webp",
      "description": "Enables multiple AI agents to collaboratively share and access each other's responses to the same prompt, facilitating deeper dialogue and understanding across models.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-24T12:24:48Z",
      "readme_content": "# LLM Responses MCP Server\n\nA Model Context Protocol (MCP) server that allows multiple AI agents to share and read each other's responses to the same prompt.\n\n## Overview\n\nThis project implements an MCP server with two main tool calls:\n\n1. `submit-response`: Allows an LLM to submit its response to a prompt\n2. `get-responses`: Allows an LLM to retrieve all responses from other LLMs for a specific prompt\n\nThis enables a scenario where multiple AI agents can be asked the same question by a user, and then using these tools, the agents can read and reflect on what other LLMs said to the same question.\n\n## Installation\n\n```bash\n# Install dependencies\nbun install\n```\n\n## Development\n\n```bash\n# Build the TypeScript code\nbun run build\n\n# Start the server in development mode\nbun run dev\n```\n\n## Testing with MCP Inspector\n\nThe project includes support for the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is a tool for testing and debugging MCP servers.\n\n```bash\n# Run the server with MCP Inspector\nbun run inspect\n```\n\nThe `inspect` script uses `npx` to run the MCP Inspector, which will launch a web interface in your browser for interacting with your MCP server.\n\nThis will allow you to:\n- Explore available tools and resources\n- Test tool calls with different parameters\n- View the server's responses\n- Debug your MCP server implementation\n\n## Usage\n\nThe server exposes two endpoints:\n\n- `/sse` - Server-Sent Events endpoint for MCP clients to connect\n- `/messages` - HTTP endpoint for MCP clients to send messages\n\n### MCP Tools\n\n#### submit-response\n\nSubmit an LLM's response to a prompt:\n\n```typescript\n// Example tool call\nconst result = await client.callTool({\n  name: 'submit-response',\n  arguments: {\n    llmId: 'claude-3-opus',\n    prompt: 'What is the meaning of life?',\n    response: 'The meaning of life is...'\n  }\n});\n```\n\n#### get-responses\n\nRetrieve all LLM responses, optionally filtered by prompt:\n\n```typescript\n// Example tool call\nconst result = await client.callTool({\n  name: 'get-responses',\n  arguments: {\n    prompt: 'What is the meaning of life?' // Optional\n  }\n});\n```\n\n## License\n\nMIT \n\n## Deployment to EC2\n\nThis project includes Docker configuration for easy deployment to EC2 or any other server environment.\n\n### Prerequisites\n\n- An EC2 instance running Amazon Linux 2 or Ubuntu\n- Security group configured to allow inbound traffic on port 62886\n- SSH access to the instance\n\n### Deployment Steps\n\n1. Clone the repository to your EC2 instance:\n   ```bash\n   git clone <your-repository-url>\n   cd <repository-directory>\n   ```\n\n2. Make the deployment script executable:\n   ```bash\n   chmod +x deploy.sh\n   ```\n\n3. Run the deployment script:\n   ```bash\n   ./deploy.sh\n   ```\n\nThe script will:\n- Install Docker and Docker Compose if they're not already installed\n- Build the Docker image\n- Start the container in detached mode\n- Display the public URL where your MCP server is accessible\n\n### Manual Deployment\n\nIf you prefer to deploy manually:\n\n1. Build the Docker image:\n   ```bash\n   docker-compose build\n   ```\n\n2. Start the container:\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Verify the container is running:\n   ```bash\n   docker-compose ps\n   ```\n\n### Accessing the Server\n\nOnce deployed, your MCP server will be accessible at:\n- `http://<ec2-public-ip>:62886/sse` - SSE endpoint\n- `http://<ec2-public-ip>:62886/messages` - Messages endpoint\n\nMake sure port 62886 is open in your EC2 security group! ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "collaboratively",
        "collaboration",
        "realtime collaboration",
        "agents collaboratively",
        "collaboration kstrikis"
      ],
      "category": "realtime-collaboration"
    },
    "kurror--mcp": {
      "owner": "kurror",
      "name": "mcp",
      "url": "https://github.com/kurror/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kurror.webp",
      "description": "Facilitates simultaneous communication with multiple unichat-based servers to combine responses from diverse language models, enhancing the richness of insights. Manages client connections to various unichat servers through a unified interface.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-20T18:10:14Z",
      "readme_content": "# Multichat MCP Server\n\n## Project Overview\n\nThis project is part of a larger effort to refactor a FiveM resource, aiming for a more robust and maintainable codebase. We are leveraging the Model Context Protocol (MCP) to extend the resource's capabilities by integrating with external services and APIs.\n\nThe `multichat-mcp` server specifically focuses on enabling communication with multiple unichat-based MCP servers simultaneously. This allows us to query different language models and combine their responses, potentially leading to more comprehensive and nuanced results.  It acts as a standard MCP server, exposing a `multichat` tool that the host (Roo/Cline) can use.  The `multichat-mcp` server then manages the client connections to the other unichat servers.\n\n## Current Issue\n\nWe were facing an issue with making cross-server MCP calls. The initial goal was to use the `multichat-mcp` server to directly call the `unichat` tool on other MCP servers, specifically `Lacayo 1` and `openrouter-chat`.  However, these calls were consistently returning a \"Method not found\" error (-32601).\n\nDirect calls to the `unichat` tools on `Lacayo 1` and `openrouter-chat` using the `use_mcp_tool` command *do* work correctly. This initially suggested the problem was within `multichat-mcp`'s cross-server communication. However, after extensive troubleshooting and research, we discovered that MCP does *not* support direct server-to-server communication.  The host (Roo/Cline) is responsible for coordinating all communication between servers.\n\n## Troubleshooting Steps and Approaches Tried\n\nWe have taken the following steps to diagnose and resolve the \"Method not found\" error, exploring various approaches:\n\n1.  **Verified MCP Server Configurations:** We carefully reviewed the `cline_mcp_settings.json` file to ensure that all servers (`multichat`, `Lacayo 1`, and `openrouter-chat`) are correctly configured, with the correct commands, arguments, and environment variables.\n\n2.  **Checked JSON-RPC Request Formats:** We consulted the MCP documentation and examples to ensure that the JSON-RPC requests sent by `multichat-mcp` were correctly formatted, including the `method`, `params`, and `id` fields.\n\n3.  **Tested Direct Calls:** We confirmed that direct calls to the `unichat` tools on `Lacayo 1` and `openrouter-chat` using `use_mcp_tool` work as expected. This isolated the issue to the cross-server communication attempts within `multichat-mcp`.\n\n4.  **Consulted Documentation via Perplexity:** We used the Perplexity MCP server extensively to search for relevant MCP documentation, examples, and troubleshooting tips.  We specifically searched for:\n\n    *   \"Model Context Protocol (MCP) cross-server communication best practices. How to make requests between MCP servers, server discovery, and client connection management. Focus on official documentation and examples.\"\n    *   \"Model Context Protocol (MCP) client connection handling and authentication. How to properly establish and verify connections between clients and servers in MCP. Focus on official documentation and TypeScript SDK examples.\"\n    *   \"MCP (Model Context Protocol) exact method names for tool execution. Looking for real-world examples of tool/call usage, JSON-RPC method names, and successful client-server interactions in the TypeScript SDK. Focus on GitHub issues and discussions about method naming.\"\n    *   \"Model Context Protocol (MCP) implementations on GitHub, focusing on server-to-server communication and cross-server request handling. Look for TypeScript/JavaScript examples from the last year. Include request routing and message passing patterns.\"\n\n    These searches helped us understand the core principles of MCP, the correct method names (`tools/list` and `tools/call`), and the client-host-server architecture.  We learned that direct server-to-server communication is *not* supported.\n\n5.  **Modified Server Code (Multiple Iterations):** We iteratively modified the `multichat-mcp` server code (`src/index.ts` and `src/server.ts`) to try different approaches, including:\n\n    *   **Incorrect Approaches (Discarded):**\n        *   Attempting direct server-to-server calls using raw JSON-RPC requests and stdio manipulation. This was based on a misunderstanding of the MCP architecture.\n        *   Creating multiple `Client` instances within a loop, each attempting to connect to a different server. This is incorrect as each client should connect to a single server.\n        *   Trying to use a custom `mcp_instructions` response type to instruct the host to make calls. MCP does not support custom message types for cross-server communication.\n        *   Using incorrect method names like `tool/call` (singular) instead of `tools/call` (plural).\n        *   Attempting to use `rpc.discover` and `mcp.tools.list` which are not standard MCP methods.\n        *   Trying to use a `tool/route` notification, which is not a standard MCP method.\n\n    *   **Correct Approach (Current Implementation):**\n        *   Creating a single `Client` instance *per target server* (Lacayo 1, openrouter-chat) and storing them in a `Map`.\n        *   Using the `StdioClientTransport` to spawn the `unichat-ts-mcp-server` as a subprocess. This is necessary because `multichat-mcp` needs to act as a *client* to the unichat servers.\n        *   Using the `client.listTools()` method to verify the connection and discover available tools.\n        *   Using the `client.request()` method with the correct `tools/call` method name and parameters, following the standard JSON-RPC 2.0 format.\n        *   Using the `client.connect()` and handling the transport correctly.\n        *   Properly initializing the client with capabilities.\n        *   Using Zod schema validation to ensure the request and response formats are correct.\n        *   Ensuring the necessary dependencies are installed (`package.json`) and the code is correctly built (`tsconfig.json` and `npm run build`).\n\n## Current Status\n\nThe `multichat-mcp` server is currently still returning \"Method not found\" errors. We are still debugging the issue, but we have made significant progress in understanding the correct MCP architecture and implementation patterns. We are now using the correct client-server communication model, but there may still be subtle issues with our request formatting or server configuration.\n\n**Files Involved:**\n\n*   `src/server.ts`: The main server implementation.\n*   `src/index.ts`: The server entry point.\n*   `package.json`: Dependencies and build scripts.\n*   `tsconfig.json`: TypeScript configuration.\n*   `../../../../../../Users/kurror/AppData/Roaming/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`:  MCP server configuration file.\n\n## Installation\n\n**Prerequisites:**\n\n*   Node.js and npm installed on your system.\n\n**Steps:**\n\n1.  Navigate to the MCP servers directory:\n    ```bash\n    cd C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Roo-Code\\\\MCP\n    ```\n\n2.  Clone or create the `multichat-mcp` directory.\n\n3.  Place the server files (`package.json`, `tsconfig.json`, `src/index.ts`, `src/server.ts`) inside the `multichat-mcp` directory.\n\n4.  Install the dependencies:\n    ```bash\n    npm install\n    ```\n\n5.  Build the TypeScript code:\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\nTo enable the `multichat-mcp` server, you need to add its configuration to the `cline_mcp_settings.json` file, located at `C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Code\\\\User\\\\globalStorage\\\\rooveterinaryinc.roo-cline\\\\settings\\\\cline_mcp_settings.json`.\n\nAdd the following entry to the `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"multichat\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Roo-Code\\\\MCP\\\\multichat-mcp\\\\build\\\\index.js\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## Usage (Testing)\n\nYou can test the `multichat-mcp` server using the `use_mcp_tool` command in your Cline environment.\n\n**`multichat` tool:**\n\nTo send messages to multiple unichat servers and save their responses, use the following format:\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>multichat</tool_name>\n<arguments>\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What is your opinion about async programming?\"\n    }\n  ],\n  \"servers\": [\"Lacayo 1\", \"openrouter-chat\"],\n  \"outputDir\": \"test_output\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n*   `messages`: An array of messages to send to each server. The format follows the standard unichat message format (array of objects with `role` and `content`).\n*   `servers`: An array of the names of the unichat servers to call (e.g., `\"Lacayo 1\"`, `\"openrouter-chat\"`).\n*   `outputDir`: The directory name (within the MCP server's working directory) where the responses will be saved.  **Important:** The responses are *not* currently being saved correctly due to the \"Method not found\" error.\n\n**`read_response` tool:**\nTo read a saved response file (once the server is functioning correctly), use:\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>read_response</tool_name>\n<arguments>\n{\n  \"outputDir\": \"test_output\",\n  \"server\": \"Lacayo 1\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n*   `outputDir`: The directory where responses are saved.\n*   `server`: The name of the server whose response you want to read.\n\n**Important Notes for Testing:**\n\n*   Ensure that the `Lacayo 1` and `openrouter-chat` servers are running and correctly configured in `cline_mcp_settings.json`.\n*   After making code changes to `multichat-mcp`, you *must* run `npm run build` in the `multichat-mcp` directory to compile the TypeScript code.\n*   The MCP host (Roo/Cline) automatically restarts servers when the configuration file changes, but it may be necessary to manually restart if you encounter issues.\n\n## Using Perplexity and Unichat\n\n*   **Perplexity:** You can use the Perplexity MCP server to research and gather information related to MCP, FiveM development, and any other technical topics. This can be helpful for finding documentation, examples, and solutions to problems.\n\n*   **Unichat Servers:** The `Lacayo 1` and `openrouter-chat` servers provide access to language models through the `unichat` tool. You can use these servers for general coding assistance, debugging, and generating code snippets.  You can test them directly using `use_mcp_tool` to ensure they are functioning correctly.\n\n## Updated Usage and Troubleshooting (Corrected)\n\nThe original documentation and troubleshooting steps described an incorrect approach for cross-server communication. This section provides the corrected usage instructions and addresses the timeout issues we encountered.\n\n### Dependencies\n-   `@modelcontextprotocol/sdk`: Provides the core functionality for building MCP servers and clients.\n-   `zod`: Used for schema validation and type safety.\n-   `fs/promises`, `path`, `url`, `crypto`: Node.js built-in modules for file system operations, path manipulation, URL parsing, and cryptographic functions.\n\n### `multichat` tool (Corrected Usage)\n\nThe `multichat` tool is designed to send the *same* message to multiple unichat servers and collect their responses. It does *not* facilitate direct server-to-server communication. The correct way to use it is as follows:\n\n**Important:** The `multichat` server and the `unichat` servers it communicates with must be running in *separate* terminal windows.\n\n1.  **Start the unichat servers:** Open separate terminal windows for each unichat server you want to use (e.g., \"Lacayo 1\", \"openrouter-chat\"). In each terminal, navigate to the unichat server directory and run:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\unichat-ts-mcp-server\n    $env:UNICHAT_MODEL=\"gpt-4o\"  # Or your desired model\n    $env:UNICHAT_API_KEY=\"your_api_key\"  # Replace with your actual API key\n    node ./build/index.js\n    ```\n\n2.  **Start the multichat server:** In a *separate* terminal window, navigate to the `multichat-mcp` directory and run:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\multichat-mcp\n    node ./build/index.js\n    ```\n\n3.  **Send the request:** In a *third* terminal window, navigate to the `multichat-mcp` directory and create a `request.json` file with the request content.  Then, send the request using PowerShell:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\multichat-mcp\n    $request = @{\n        jsonrpc = \"2.0\"\n        id = 1\n        method = \"tools/call\"\n        params = @{\n            name = \"multichat\"\n            arguments = @{\n                messages = @(\n                    @{role = \"system\"; content = \"You are a helpful assistant.\"},\n                    @{role = \"user\"; content = \"Hello, world!\"}\n                )\n                servers = @(\"Lacayo 1\", \"openrouter-chat\")\n                outputDir = \"my-test-output\"\n            }\n        }\n    } | ConvertTo-Json -Depth 10\n\n    $request | Out-File -FilePath \"request.json\" -Encoding utf8\n    Get-Content \"request.json\" | node ./build/index.js\n    ```\n\n    This will create a directory `responses/my-test-output` within the `multichat-mcp` directory, containing the responses from each server (e.g., `Lacayo 1.json`, `openrouter-chat.json`) and a `_session.json` file to track the session. If a server fails to respond, an error file (e.g., `Lacayo 1_error.json`) will be created instead.\n\n### `read_response` tool (Corrected Usage)\n\nThe `read_response` tool reads a saved response file generated by a previous `multichat` call.\n\n**Example Request (within Roo):**\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>read_response</tool_name>\n<arguments>\n{\n  \"outputDir\": \"my-test-output\",\n  \"server\": \"Lacayo 1\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n**Important:** The `outputDir` is relative to the `multichat-mcp/responses` directory. The tool expects to find a subdirectory within `responses` that matches the `outputDir` value. Inside that subdirectory, it looks for either a `<server>.json` file (for successful responses) or a `<server>_error.json` file (for errors).\n\n### Troubleshooting (Updated)\n\n*   **Timeouts:** The primary cause of timeouts was the incorrect assumption that `multichat` could directly call other servers.  The corrected usage, with all servers running in separate terminals, addresses this. Ensure all unichat servers are running *before* sending the `multichat` request.\n*   **Invalid session directory:** This error occurs when `read_response` cannot find the specified `outputDir` within the `multichat-mcp/responses` directory. Double-check the `outputDir` value and ensure that the `multichat` command was run successfully and created the output directory.\n*   **No response files created:** If no files are created in the `responses` directory, even after following the corrected usage, there might be an issue with file system permissions or a silent error within the `multichat` server's response handling. Check the server logs for any error messages. It's also crucial to verify that the unichat servers you are targeting are running and responding correctly. You can test them individually using `use_mcp_tool` with the `unichat` tool.\n\nThe errors were:\n\n1.  Incorrect usage of `setRequestHandler`: I was passing the method name and schema separately, instead of including the method name within the schema.\n2.  Incorrect parameter access: `request.params` was not correctly typed due to the schema issue.\n3.  Incorrect response schema in `client.request`: I was using `z.any()`, which is not compatible with `RequestOptions`.\n\nThe fixes are:\n\n1.  **Define `ForwardRequestSchema` correctly:** Include the `method` field with `z.literal(\"mcp.forward\")`.\n2.  **Use `setRequestHandler` correctly:** Pass only the schema and the handler function.\n3.  **Access parameters correctly:** Use `request.params` after parsing with the schema.\n4. Use `z.unknown()` in the client.request call.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "servers",
        "kurror",
        "collaboration",
        "unichat servers",
        "collaboration kurror",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "liveblocks--liveblocks-mcp-server": {
      "owner": "liveblocks",
      "name": "liveblocks-mcp-server",
      "url": "https://github.com/liveblocks/liveblocks-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/liveblocks.webp",
      "description": "Interact with Liveblocks to manage rooms, threads, comments, and notifications while providing read access to Liveblocks Storage and Yjs for enhanced collaborative features.",
      "stars": 11,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-08-24T01:49:07Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://liveblocks.io#gh-light-mode-only\">\n    <img src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-light.svg\" alt=\"Liveblocks\" />\n  </a>\n  <a href=\"https://liveblocks.io#gh-dark-mode-only\">\n    <img src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-dark.svg\" alt=\"Liveblocks\" />\n  </a>\n</p>\n\n# `liveblocks-mcp-server`\n\n[![smithery badge](https://smithery.ai/badge/@liveblocks/liveblocks-mcp-server)](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server)\n\nThis MCP server allows AI to use a number of functions from our [REST API](https://liveblocks.io/docs/api-reference/rest-api-endpoints). For example, it can create, modify, and delete different aspects of Liveblocks such as rooms, threads, comments, notifications, and more. It also has read access to Storage and Yjs. [Learn more in our docs](https://liveblocks.io/docs/tools/mcp-server).\n\n## Automatic setup\n\nTo install automatically, copy your Liveblocks secret key from a project in [your dashboard](https://liveblocks.io/dashboard) and run one of the following commands, replacing `[key]` with your secret key.\n\n### Cursor\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client cursor --key [key]\n```\n\n### Claude Desktop\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client claude --key [key]\n```\n\n### VS Code\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client vscode --key [key]\n```\n\n### Other clients\n\nFind installation information for other clients on [Smithery](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server).\n\n## Manual setup\n\n<details><summary>Read more</summary>\n\n<p></p>\n\n1. Clone this repo.\n\n```bash\ngit clone https://github.com/liveblocks/liveblocks-mcp-server.git\n```\n\n2. Build the project.\n\n```bash\nnpm install\nnpm run build\n```\n\n3. Get your Liveblocks secret key from the [dashboard](https://liveblocks.io/dashboard).\n\n```\nsk_dev_Ns35f5G...\n```\n\n### Cursor\n\n4. Go to File → Cursor Settings → MCP → Add new server.\n\n5. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n6. Check it's enabled in the MCP menu.\n\n### Claude Desktop\n\n4. Go to File → Settings → Developer → Edit Config.\n\n5. Open the JSON file, `claude_desktop_config.json`.\n\n6. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n</details>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "liveblocks",
        "realtime",
        "collaborative",
        "collaboration liveblocks",
        "liveblocks mcp",
        "liveblocks manage"
      ],
      "category": "realtime-collaboration"
    },
    "quazaai--UnityMCPIntegration": {
      "owner": "quazaai",
      "name": "UnityMCPIntegration",
      "url": "https://github.com/quazaai/UnityMCPIntegration",
      "imageUrl": "/freedevtools/mcp/pfp/quazaai.webp",
      "description": "Facilitates real-time interaction between AI assistants and Unity projects by allowing access to scene information, execution of C# code, and log monitoring within the Unity Editor. Enhances development workflows with additional file access functionalities for large language models (LLMs).",
      "stars": 94,
      "forks": 20,
      "license": "MIT License",
      "language": "C#",
      "updated_at": "2025-10-02T17:19:35Z",
      "readme_content": "# 🚀 Advacned Unity MCP Integration \n\n[![MCP](https://badge.mcpx.dev)](https://modelcontextprotocol.io/introduction)\n[![smithery badge](https://smithery.ai/badge/@quazaai/unitymcpintegration)](https://smithery.ai/server/@quazaai/unitymcpintegration)\n[![Unity](https://img.shields.io/badge/Unity-2021.3%2B-green?logo=https://w7.pngwing.com/pngs/426/535/png-transparent-unity-new-logo-tech-companies-thumbnail.png)](https://unity.com)\n[![Node.js](https://img.shields.io/badge/Node.js-18%2B-green)](https://nodejs.org)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)](https://www.typescriptlang.org)\n[![WebSockets](https://img.shields.io/badge/WebSockets-API-orange)](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)\n\n[![Stars](https://img.shields.io/github/stars/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/stargazers)\n[![Forks](https://img.shields.io/github/forks/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/network/members)\n[![License](https://img.shields.io/github/license/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/blob/main/LICENSE)\n\n<div align=\"center\">\n  \n</div>\n\nThis package provides a seamless integration between [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) and Unity Editor, allowing AI assistants to understand and interact with your Unity projects in real-time. With this integration, AI assistants can access information about your scene hierarchy, project settings, and execute code directly in the Unity Editor context.\n\n## 📚 Features\n- Browse and manipulate project files directly\n- Access real-time information about your Unity project\n- Understand your scene hierarchy and game objects\n- Execute C# code directly in the Unity Editor\n- Monitor logs and errors\n- Control the Editor's play mode\n- Wait For Code Execution\n\n\n\n\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Unity 2021.3 or later\n- Node.js 18+ (for running the MCP server)\n\n### Installation\n\n#### 1. Install Unity Package\n\nYou have several options to install the Unity package:\n\n**Option A: Package Manager (Git URL)**\n1. Open the Unity Package Manager (`Window > Package Manager`)\n2. Click the `+` button and select `Add package from git URL...`\n3. Enter the repository URL: `https://github.com/quazaai/UnityMCPIntegration.git`\n4. Click `Add`\n\n**Option B: Import Custom Package**\n1. Clone this repository or [download it as a unityPackage](https://github.com/quazaai/UnityMCPIntegration/releases)\n2. In Unity, go to `Assets > Import Package > Custom Package`\n3. Select the `UnityMCPIntegration.unitypackage` file\n\n\n\n#### 2. Set up the MCP Server\n\nYou have two options to run the MCP server:\n\n**Option A: Run the server directly**\n\n1. Navigate to the `mcpServer (likely <path-to-project>\\Library\\PackageCache\\com.quaza.unitymcp@d2b8f1260bca\\mcpServer\\)` directory\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Run the server:\n   ```\n   node build/index.js\n   ```\n\n**Option B: Add to MCP Host configuration**\n\nAdd the server to your MCP Host configuration for Claude Desktop, Custom Implementation etc\n\n```json\n{\n  \"mcpServers\": {\n    \"unity-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"path-to-project>\\\\Library\\\\PackageCache\\\\com.quaza.unitymcp@d2b8f1260bca\\\\mcpServer\\\\mcpServer\\\\build\\\\index.js\"\n      ],\n      \"env\": {\n        \"MCP_WEBSOCKET_PORT\": \"5010\"\n      }\n    }\n  }\n}\n```\n### Demo Video\n[![YouTube](http://i.ytimg.com/vi/GxTlahBXs74/hqdefault.jpg)](https://www.youtube.com/watch?v=GxTlahBXs74)\n\n### Installing via Smithery\n\nTo install Unity MCP Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@quazaai/unitymcpintegration):\n\n```bash\nnpx -y @smithery/cli install @quazaai/unitymcpintegration --client claude\n```\n\n### 🔧 Usage\n\n#### Debugging and Monitoring\n\nYou can open the MCP Debug window in Unity to monitor the connection and test features:\n\n1. Go to `Window > MCP Debug`\n2. Use the debug window to:\n   - Check connection status\n   - Test code execution\n   - View logs\n   - Monitor events\n\n#### Available Tools\n\nThe Unity MCP integration provides several tools to AI assistants:\n\n##### Unity Editor Tools\n- **get_editor_state**: Get comprehensive information about the Unity project and editor state\n- **get_current_scene_info**: Get detailed information about the current scene\n- **get_game_objects_info**: Get information about specific GameObjects in the scene\n- **execute_editor_command**: Execute C# code directly in the Unity Editor\n- **get_logs**: Retrieve and filter Unity console logs\n- **verify_connection**: Check if there's an active connection to Unity Editor\n\n##### Filesystem Tools\n- **read_file**: Read contents of a file in your Unity project\n- **read_multiple_files**: Read multiple files at once\n- **write_file**: Create or overwrite a file with new content\n- **edit_file**: Make targeted edits to existing files with diff preview\n- **list_directory**: Get a listing of files and folders in a directory\n- **directory_tree**: Get a hierarchical view of directories and files\n- **search_files**: Find files matching a search pattern\n- **get_file_info**: Get metadata about a specific file or directory\n- **find_assets_by_type**: Find all assets of a specific type (e.g. Material, Prefab)\n- **list_scripts**: Get a listing of all C# scripts in the project\n\nFile paths can be absolute or relative to the Unity project's Assets folder. For example, `\"Scenes/MyScene.unity\"` refers to `<project>/Assets/Scenes/MyScene.unity`.\n\n## 🛠️ Architecture\n\nThe integration consists of two main components:\n\n1. **Unity Plugin (C#)**: Resides in the Unity Editor and provides access to Editor APIs\n2. **MCP Server (TypeScript/Node.js)**: Implements the MCP protocol and communicates with the Unity plugin\n\nCommunication between them happens via WebSocket, transferring JSON messages for commands and data.\n\n## File System Access\n\nThe Unity MCP integration now includes powerful filesystem tools that allow AI assistants to:\n\n- Browse, read, and edit files in your Unity project\n- Create new files and directories\n- Search for specific files or asset types\n- Analyze your project structure\n- Make targeted code changes with diff previews\n\nAll file operations are restricted to the Unity project directory for security. The system intelligently handles both absolute and relative paths, always resolving them relative to your project's Assets folder for convenience.\n\nExample usages:\n- Get a directory listing: `list_directory(path: \"Scenes\")`\n- Read a script file: `read_file(path: \"Scripts/Player.cs\")`\n- Edit a configuration file: `edit_file(path: \"Resources/config.json\", edits: [{oldText: \"value: 10\", newText: \"value: 20\"}], dryRun: true)`\n- Find all materials: `find_assets_by_type(assetType: \"Material\")`\n\n## 👥 Contributing\n\nContributions are welcome! Here's how you can contribute:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Commit your changes (`git commit -m 'Add some amazing feature'`)\n5. Push to the branch (`git push origin feature/amazing-feature`)\n6. Open a Pull Request\n\n### Development Setup\n\n**Unity Side**:\n- Open the project in Unity\n- Modify the C# scripts in the `UnityMCPConnection/Editor` directory\n\n**Server Side**:\n- Navigate to the `mcpServer` directory\n- Install dependencies: `npm install`\n- Make changes to the TypeScript files in the `src` directory\n- Build the server: `npm run build`\n- Run the server: `node build/index.js`\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 📞 Support\n\nIf you encounter any issues or have questions, please file an issue on the GitHub repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "unity",
        "unitymcpintegration",
        "ai",
        "assistants unity",
        "ai assistants",
        "collaboration quazaai"
      ],
      "category": "realtime-collaboration"
    },
    "tian1ll1--mcp-server-demo": {
      "owner": "tian1ll1",
      "name": "mcp-server-demo",
      "url": "https://github.com/tian1ll1/mcp-server-demo",
      "imageUrl": "/freedevtools/mcp/pfp/tian1ll1.webp",
      "description": "Enables real-time communication between AI models and external tools while managing conversation history and tool registries. Implements a WebSocket interface for interactive client-server communication and includes context management capabilities.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-24T01:04:34Z",
      "readme_content": "# MCP Server Demo\n\nThis project demonstrates the implementation of a Model Context Protocol (MCP) server. MCP is a protocol designed to facilitate communication between AI models and external tools/services while maintaining context awareness.\n\n## Features\n\n- Basic MCP server implementation\n- Example tool integrations\n- Context management demonstration\n- WebSocket-based real-time communication\n- Simple client example\n\n## Project Structure\n\n```\nmcp-server-demo/\n├── src/\n│   ├── server.py           # Main MCP server implementation\n│   ├── tools/              # Tool implementations\n│   │   ├── __init__.py\n│   │   └── basic_tools.py\n│   ├── context/            # Context management\n│   │   ├── __init__.py\n│   │   └── manager.py\n│   └── utils/             # Utility functions\n│       ├── __init__.py\n│       └── helpers.py\n├── examples/              # Example usage\n│   ├── client.py\n│   └── tools_demo.py\n├── tests/                # Test cases\n│   └── test_server.py\n├── requirements.txt      # Project dependencies\n└── README.md            # This file\n```\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/tian1ll1/mcp-server-demo.git\ncd mcp-server-demo\n```\n\n2. Create a virtual environment (recommended):\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n1. Start the MCP server:\n```bash\npython src/server.py\n```\n\n2. Run the example client:\n```bash\npython examples/client.py\n```\n\n## How It Works\n\nThe MCP server implements the following key components:\n\n1. **Context Management**: Maintains conversation history and relevant context for each session.\n2. **Tool Registry**: Manages available tools and their specifications.\n3. **Message Processing**: Handles incoming messages and routes them to appropriate tools.\n4. **WebSocket Server**: Provides real-time communication with clients.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websocket",
        "realtime",
        "server",
        "realtime collaboration",
        "websocket interface",
        "interactive client"
      ],
      "category": "realtime-collaboration"
    },
    "yeonupark--mcp-soccer-data": {
      "owner": "yeonupark",
      "name": "mcp-soccer-data",
      "url": "https://github.com/yeonupark/mcp-soccer-data",
      "imageUrl": "/freedevtools/mcp/pfp/yeonupark.webp",
      "description": "Provides real-time football match information through natural language queries, including live scores, match events, team lineups, betting odds, and league metadata for ongoing, upcoming, and recently finished matches worldwide.",
      "stars": 19,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-20T03:01:59Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yeonupark-mcp-soccer-data-badge.png)](https://mseep.ai/app/yeonupark-mcp-soccer-data)\n\n# ⚽️ Soccerdata MCP Server\n[![smithery badge](https://smithery.ai/badge/@yeonupark/mcp-soccer-data)](https://smithery.ai/server/@yeonupark/mcp-soccer-data)\n- **MCP-Soccerdata** is an open-source [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that connects to the **SoccerDataAPI to deliver up-to-date football match information via natural language interactions**. \n\n- Designed for use with MCP-enabled clients such as Claude Desktop, it allows users to retrieve football data by leveraging large language models (LLMs).\n\n---\n\n## ✨ Features\n\n### 🏟️ Live Football Match Insights\nMCP-Soccerdata focuses on delivering **real-time information about ongoing football matches around the world.**\n\n> \"What football matches are being played right now?\"      \n> \"What are the predicted lineups for PSG vs Aston Villa today?\"       \n> \"Please tell me the scores and number of goals from recent football matches.\"\n\n→ Provides relevant football data in a structured format, including the detailed categories described below.\n\n### - Match Listings & Basic Info\n- Global list of all currently active matches\n- Home and away team names\n- Kickoff time and match date\n- Stadium details\n- Current score\n\n\n### - Match Details\n- Match status: scheduled, in progress, or finished\n- Goal breakdown: first half, second half, extra time, penalty shootout\n- Final result: win, draw, or loss\n\n\n### - Key Match Events\n- Goal events (who scored, when, how)\n- Substitutions\n- Yellow and red cards\n- Penalties\n\n\n### - Team Lineups\n- Starting XI\n- Bench players\n- Injury status\n- Team formation\n\n\n### - Odds & Betting Information\n- Win / Draw / Lose odds\n- Over / Under odds\n- Handicap betting odds\n\n\n### - League Metadata\n- League name\n- Country\n- Competition format (e.g., regular season, knockout stage)\n\n\n> ⚠️ Focused exclusively on **live**, **upcoming**, and **recently finished** matches\n\n---\n## 🎥 Demo\n\n\n\n---\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install Amadeus MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yeonupark/mcp-soccer-data):\n\n```bash\nnpx -y @smithery/cli install @yeonupark/mcp-soccer-data --client claude\n```\n\n### Prerequisites\n- Python 3.12+\n- `uv` package manager\n- Soccerdata API account\n- MCP-compatible client (e.g., Claude for Desktop)\n\n\n### 1. Clone and Setup\n\n- Clone the repository\n```bash\ngit clone https://github.com/yeonupark/mcp-soccer-data.git\ncd mcp-soccer-data\n```\n- Install dependencies\n```\nuv sync\n```\n\n### 2. Get Your API Key and Set Environment\n\n- Create a .env file with your credentials:\n```\nAUTH_KEY=your_auth_key\n```\n> Sign up on https://soccerdataapi.com/ and get your own Auth keys.\n\n### 3. Configure MCP Client\n- Register this server in your MCP client (e.g., Claude for Desktop).\n\nEdit `~/Library/Application Support/Claude/claude_desktop_config.json:`\n```\n{\n  \"mcpServers\": {\n      \"mcp-soccer-data\": {\n          \"command\": \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/uv\",\n          \"args\": [\n              \"--directory\",\n              \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/src/\",\n              \"run\",\n              \"--env-file\",\n              \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/.env\",\n              \"server.py\"\n          ]\n      }\n  }\n}\n```\n\n---\n## 🛠️ Tools\nThe follwing tool is exposed to MCP clients:  \n### `get_livescores()`\n-> Returns real-time information about ongoing football matches around the world.\n\n\n---\n## 📝 License\n- This project is licensed under the [MIT License](LICENSE). See the LICENSE file for details.\n- Built with [Model Context Protocol](https://modelcontextprotocol.io/introduction)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "realtime",
        "matches",
        "soccer",
        "soccer data",
        "mcp soccer",
        "football match"
      ],
      "category": "realtime-collaboration"
    },
    "z9905080--mcp-slack": {
      "owner": "z9905080",
      "name": "mcp-slack",
      "url": "https://github.com/z9905080/mcp-slack",
      "imageUrl": "/freedevtools/mcp/pfp/z9905080.webp",
      "description": "Integrate AI assistants with Slack workspaces to manage channels, send messages, reply to threads, add reactions, and access user and message data. Streamline communication and collaboration within Slack environments using AI-driven tools.",
      "stars": 2,
      "forks": 2,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-12T19:49:04Z",
      "readme_content": "# MCP Server for Slack\n\n[![npm version](https://img.shields.io/npm/v/shouting-mcp-slack.svg)](https://www.npmjs.com/package/shouting-mcp-slack)\n\nA Model Context Protocol (MCP) server implementation for integrating AI assistants with Slack workspaces.\n\n## Overview\n\nThis package provides an MCP server that enables AI assistants to interact with Slack workspaces. It allows AI models to:\n\n- List and browse channels\n- Send messages to channels\n- Reply to threads\n- Add reactions to messages\n- Retrieve channel history\n- Get thread replies\n- List users and retrieve user profiles\n\n## Installation\n\n```bash\n# Install from npm\nnpm install shouting-mcp-slack\n\n# Or install globally\nnpm install -g shouting-mcp-slack\n```\n\nYou can find the package on npm: [shouting-mcp-slack](https://www.npmjs.com/package/shouting-mcp-slack/access)\n\n## Prerequisites\n\nYou need to set up a Slack Bot and obtain the necessary credentials:\n\n1. Create a Slack App in the [Slack API Console](https://api.slack.com/apps)\n2. Add the following Bot Token Scopes:\n   - `channels:history`\n   - `channels:read`\n   - `chat:write`\n   - `reactions:write`\n   - `users:read`\n   - `users:read.email`\n3. Install the app to your workspace\n4. Copy the Bot User OAuth Token\n\n## Configuration\n\nThe server requires the following environment variables:\n\n- `SLACK_BOT_TOKEN`: Your Slack Bot User OAuth Token\n- `SLACK_TEAM_ID`: Your Slack Team ID\n\n## Usage\n\n### Running as a CLI Tool\n\n```bash\n# Set environment variables\nexport SLACK_BOT_TOKEN=xoxb-your-token\nexport SLACK_TEAM_ID=your-team-id\n\n# Run the server\nmcp-server-slack\n```\n\n### Using in Your Code\n\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SlackClient } from \"shouting-mcp-slack\";\n\n// Initialize the server and client\nconst server = new Server({...});\nconst slackClient = new SlackClient(process.env.SLACK_BOT_TOKEN);\n\n// Register your custom handlers\n// ...\n```\n\n## Available Tools\n\nThe server provides the following Slack integration tools:\n\n- `slack_list_channels`: List available channels\n- `slack_post_message`: Send a message to a channel\n- `slack_reply_to_thread`: Reply to a thread\n- `slack_add_reaction`: Add a reaction to a message\n- `slack_get_channel_history`: Get message history from a channel\n- `slack_get_thread_replies`: Get replies in a thread\n- `slack_get_users`: List users in the workspace\n- `slack_get_user_profile`: Get a user's profile\n\n## License\n\nISC\n\n## Author\n\nshouting.hsiao@gmail.com\n\n## Repository\n\nhttps://github.com/z9905080/mcp-slack",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "ai",
        "realtime",
        "mcp slack",
        "collaboration slack",
        "assistants slack"
      ],
      "category": "realtime-collaboration"
    }
  }
}