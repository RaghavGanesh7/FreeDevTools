{
  "category": "databases",
  "categoryDisplay": "Databases",
  "description": "Secure database access with schema inspection capabilities. Enables querying and analyzing data with configurable security controls including read-only access.",
  "totalRepositories": 332,
  "repositories": {
    "0xTrxz--supabase-mcp": {
      "owner": "0xTrxz",
      "name": "supabase-mcp",
      "url": "https://github.com/0xTrxz/supabase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/0xTrxz.webp",
      "description": "The Supabase MCP Server connects your Supabase projects with AI assistants, allowing them to manage databases, query data, and handle project configurations easily.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-05-08T20:36:07Z",
      "readme_content": "# Supabase MCP Server\n\n> Connect your Supabase projects to Cursor, Claude, Windsurf, and other AI assistants.\n\n![supabase-mcp-demo](https://github.com/user-attachments/assets/3fce101a-b7d4-482f-9182-0be70ed1ad56)\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) standardizes how Large Language Models (LLMs) talk to external services like Supabase. It connects AI assistants directly with your Supabase project and allows them to perform tasks like managing tables, fetching config, and querying data. See the [full list of tools](#tools).\n\n## Prerequisites\n\nYou will need Node.js installed on your machine. You can check this by running:\n\n```shell\nnode -v\n```\n\nIf you don't have Node.js installed, you can download it from [nodejs.org](https://nodejs.org/).\n\n## Setup\n\n### 1. Personal access token (PAT)\n\nFirst, go to your [Supabase settings](https://supabase.com/dashboard/account/tokens) and create a personal access token. Give it a name that describes its purpose, like \"Cursor MCP Server\".\n\nThis will be used to authenticate the MCP server with your Supabase account. Make sure to copy the token, as you won't be able to see it again.\n\n### 2. Configure MCP client\n\nNext, configure your MCP client (such as Cursor) to use this server. Most MCP clients store the configuration as JSON in the following format:\n\n```json\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@supabase/mcp-server-supabase@latest\",\n        \"--access-token\",\n        \"<personal-access-token>\"\n      ]\n    }\n  }\n}\n```\n\nReplace `<personal-access-token>` with the token you created in step 1. Alternatively you can omit `--access-token` and instead set the `SUPABASE_ACCESS_TOKEN` environment variable to your personal access token (you will need to restart your MCP client after setting this). This allows you to keep your token out of version control if you plan on committing this configuration to a repository.\n\nThe following additional options are available:\n\n- `--project-ref`: Used to scope the server to a specific project. See [project scoped mode](#project-scoped-mode).\n- `--read-only`: Used to restrict the server to read-only queries. See [read-only mode](#read-only-mode).\n\nIf you are on Windows, you will need to [prefix the command](#windows). If your MCP client doesn't accept JSON, the direct CLI command is:\n\n```shell\nnpx -y @supabase/mcp-server-supabase@latest --access-token=<personal-access-token>\n```\n\n> Note: Do not run this command directly - this is meant to be executed by your MCP client in order to start the server. `npx` automatically downloads the latest version of the MCP server from `npm` and runs it in a single command.\n\n#### Windows\n\nOn Windows, you will need to prefix the command with `cmd /c`:\n\n```json\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@supabase/mcp-server-supabase@latest\",\n        \"--access-token\",\n        \"<personal-access-token>\"\n      ]\n    }\n  }\n}\n```\n\nor with `wsl` if you are running Node.js inside WSL:\n\n```json\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"wsl\",\n      \"args\": [\n        \"npx\",\n        \"-y\",\n        \"@supabase/mcp-server-supabase@latest\",\n        \"--access-token\",\n        \"<personal-access-token>\"\n      ]\n    }\n  }\n}\n```\n\nMake sure Node.js is available in your system `PATH` environment variable. If you are running Node.js natively on Windows, you can set this by running the following commands in your terminal.\n\n1. Get the path to `npm`:\n\n   ```shell\n   npm config get prefix\n   ```\n\n2. Add the directory to your PATH:\n\n   ```shell\n   setx PATH \"%PATH%;<path-to-dir>\"\n   ```\n\n3. Restart your MCP client.\n\n### Project scoped mode\n\nBy default, the MCP server will have access to all organizations and projects in your Supabase account. If you want to restrict the server to a specific project, you can set the `--project-ref` flag on the CLI command:\n\n```shell\nnpx -y @supabase/mcp-server-supabase@latest --access-token=<personal-access-token> --project-ref=<project-ref>\n```\n\nReplace `<project-ref>` with the ID of your project. You can find this under **Project ID** in your Supabase [project settings](https://supabase.com/dashboard/project/_/settings/general).\n\nAfter scoping the server to a project, [account-level](#project-management) tools like `list_projects` and `list_organizations` will no longer be available. The server will only have access to the specified project and its resources.\n\n### Read-only mode\n\nIf you wish to restrict the Supabase MCP server to read-only queries, set the `--read-only` flag on the CLI command:\n\n```shell\nnpx -y @supabase/mcp-server-supabase@latest --access-token=<personal-access-token> --read-only\n```\n\nThis prevents write operations on any of your databases by executing SQL as a read-only Postgres user. Note that this flag only applies to database tools (`execute_sql` and `apply_migration`) and not to other tools like `create_project` or `create_branch`.\n\n## Tools\n\n_**Note:** This server is pre-1.0, so expect some breaking changes between versions. Since LLMs will automatically adapt to the tools available, this shouldn't affect most users._\n\nThe following Supabase tools are available to the LLM:\n\n#### Project Management\n\n_**Note:** these tools will be unavailable if the server is [scoped to a project](#project-scoped-mode)._\n\n- `list_projects`: Lists all Supabase projects for the user.\n- `get_project`: Gets details for a project.\n- `create_project`: Creates a new Supabase project.\n- `pause_project`: Pauses a project.\n- `restore_project`: Restores a project.\n- `list_organizations`: Lists all organizations that the user is a member of.\n- `get_organization`: Gets details for an organization.\n\n#### Database Operations\n\n- `list_tables`: Lists all tables within the specified schemas.\n- `list_extensions`: Lists all extensions in the database.\n- `list_migrations`: Lists all migrations in the database.\n- `apply_migration`: Applies a SQL migration to the database. SQL passed to this tool will be tracked within the database, so LLMs should use this for DDL operations (schema changes).\n- `execute_sql`: Executes raw SQL in the database. LLMs should use this for regular queries that don't change the schema.\n- `get_logs`: Gets logs for a Supabase project by service type (api, postgres, edge functions, auth, storage, realtime). LLMs can use this to help with debugging and monitoring service performance.\n\n#### Edge Function Management\n\n- `list_edge_functions`: Lists all Edge Functions in a Supabase project.\n- `deploy_edge_function`: Deploys a new Edge Function to a Supabase project. LLMs can use this to deploy new functions or update existing ones.\n\n#### Project Configuration\n\n- `get_project_url`: Gets the API URL for a project.\n- `get_anon_key`: Gets the anonymous API key for a project.\n\n#### Branching (Experimental, requires a paid plan)\n\n- `create_branch`: Creates a development branch with migrations from production branch.\n- `list_branches`: Lists all development branches.\n- `delete_branch`: Deletes a development branch.\n- `merge_branch`: Merges migrations and edge functions from a development branch to production.\n- `reset_branch`: Resets migrations of a development branch to a prior version.\n- `rebase_branch`: Rebases development branch on production to handle migration drift.\n\n#### Development Tools\n\n- `generate_typescript_types`: Generates TypeScript types based on the database schema. LLMs can save this to a file and use it in their code.\n\n#### Cost Confirmation\n\n- `get_cost`: Gets the cost of a new project or branch for an organization.\n- `confirm_cost`: Confirms the user's understanding of new project or branch costs. This is required to create a new project or branch.\n\n## Other MCP servers\n\n### `@supabase/mcp-server-postgrest`\n\nThe PostgREST MCP server allows you to connect your own users to your app via REST API. See more details on its [project README](./packages/mcp-server-postgrest).\n\n## Resources\n\n- [**Model Context Protocol**](https://modelcontextprotocol.io/introduction): Learn more about MCP and its capabilities.\n- [**From development to production**](/docs/production.md): Learn how to safely promote changes to production environments.\n\n## For developers\n\nThis repo uses npm for package management, and the latest LTS version of Node.js.\n\nClone the repo and run:\n\n```\nnpm install --ignore-scripts\n```\n\n> [!NOTE]\n> On recent versions of MacOS, you may have trouble installing the `libpg-query` transient dependency without the `--ignore-scripts` flag.\n\n## License\n\nThis project is licensed under Apache 2.0. See the [LICENSE](./LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase mcp",
        "mcp supabase",
        "supabase projects"
      ],
      "category": "databases"
    },
    "1092705638--dameng-mcp-server": {
      "owner": "1092705638",
      "name": "dameng-mcp-server",
      "url": "https://github.com/1092705638/dameng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/1092705638.webp",
      "description": "This server provides a connection to the Dameng 8 database using the Model Context Protocol (MCP), allowing users to easily access and integrate database content for more efficient data utilization.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-29T01:42:23Z",
      "readme_content": "# dameng-mcp-server\n达梦8数据库的MCP服务\n\n>测试git提交",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "dameng",
        "dameng database",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "11bluetree--mysql-mcp": {
      "owner": "11bluetree",
      "name": "mysql-mcp",
      "url": "https://github.com/11bluetree/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/11bluetree.webp",
      "description": "The MySQL MCP Server allows users to connect to MySQL databases safely and execute read-only queries. It retrieves database structure and data in JSON format, making it easier for applications to access live data without modifying it.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-01T03:47:30Z",
      "readme_content": "# MySQL MCP Server\n\nMySQL用のModel Context Protocol (MCP) サーバー。SELECTクエリの実行とデータベーススキーマの取得に対応しています。\n\n## 特徴\n\n- TypeScriptで実装\n- MySQLへの接続とSELECTクエリの実行\n- データベースのスキーマ情報（テーブル構造、列情報、リレーションシップ）の取得\n- クエリ結果をJSON形式で返却\n- セキュリティのためSELECT文のみに制限\n- 環境変数による接続設定\n\n## 必要条件\n\n- Node.js\n- MySQL/MariaDBデータベース\n\n## インストールと使い方\n\n```bash\n# パッケージをインストール\nnpm install\n\n# ビルド\nnpm run build\n\n# 実行\nnpx -y mysql-client\n\n# または環境変数を設定して実行\nMYSQL_HOST=localhost MYSQL_PORT=3306 MYSQL_USER=root MYSQL_PASSWORD=password MYSQL_DATABASE=test npx -y mysql-client\n```\n\n## 提供ツール\n\n- `select`: SELECT SQLクエリを実行し、結果をJSON形式で返します\n- `schema`: データベースのスキーマ情報を取得し、テーブル構造、カラム情報、テーブル間の関係を返します\n\n## セキュリティ注意事項\n\nこのMCPサーバーは、セキュリティ上の理由からSELECTクエリのみを許可しています。データ変更操作（INSERT、UPDATE、DELETE等）は実行できません。\n\n## VS CodeでのMCP設定\n\n### ワークスペースでの設定\n\n1. VS Codeのワークスペースで、`.vscode/mcp.json`ファイルを作成します\n2. 以下のような設定を追加します：\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"mysql-password\",\n      \"description\": \"MySQLパスワード\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"mysql-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mysql-mcp@1.1.2\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_DATABASE\": \"データベース名\",\n        \"MYSQL_USER\": \"ユーザー名\",\n        \"MYSQL_PASSWORD\": \"${input:mysql-password}\"\n      }\n    }\n  }\n}\n```\n\n### ユーザー設定での設定\n\nすべてのワークスペースでMCPサーバーを利用するには、VS Codeのユーザー設定に追加します：\n\n1. コマンドパレット（`Ctrl+Shift+P` または `Cmd+Shift+P`）を開き、`MCP: Add Server`を選択します\n2. サーバー情報を入力し、`User Settings`を選択して追加します\n3. または、`settings.json`に直接追加することもできます：\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"mysql-mcp-server\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"mysql-mcp@1.1.2\"\n        ],\n        \"env\": {\n          \"MYSQL_HOST\": \"localhost\",\n          \"MYSQL_PORT\": \"3306\",\n          \"MYSQL_DATABASE\": \"データベース名\",\n          \"MYSQL_USER\": \"ユーザー名\",\n          \"MYSQL_PASSWORD\": \"パスワード\"\n        }\n      }\n    }\n  }\n}\n```\n\n### MCPサーバーの利用方法\n\n1. VS Codeでチャットビュー（`Ctrl+Alt+I`）を開きます\n2. ドロップダウンから`Agent`モードを選択します\n3. `Tools`ボタンをクリックして利用可能なツールを確認します\n4. チャットでSQLクエリやデータベーススキーマについて質問すると、`select`や`schema`ツールが自動的に呼び出されます\n\n### MCPサーバーの管理\n\n- コマンドパレットから`MCP: List Servers`を実行するとMCPサーバーの一覧が表示されます\n- サーバーの起動、停止、再起動、設定の確認、ログの表示ができます\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "mcp mysql",
        "secure database",
        "mysql mcp"
      ],
      "category": "databases"
    },
    "1RB--mongo-mcp": {
      "owner": "1RB",
      "name": "mongo-mcp",
      "url": "https://github.com/1RB/mongo-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/1RB.webp",
      "description": "The MongoDB MCP Server connects AI models to MongoDB databases, allowing users to query data, view database structures, and manage information using natural language commands.",
      "stars": 12,
      "forks": 7,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-07T12:48:37Z",
      "readme_content": "# 🗄️ MongoDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![npm version](https://badge.fury.io/js/%40coderay%2Fmongo-mcp-server.svg)](https://www.npmjs.com/package/@coderay/mongo-mcp-server)\n[![smithery badge](https://smithery.ai/badge/mongo-mcp)](https://smithery.ai/server/mongo-mcp)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with MongoDB databases. Query collections, inspect schemas, and manage data seamlessly through natural language.\n\n## 📚 What is Model Context Protocol (MCP)?\n\nThe Model Context Protocol (MCP) is an open standard developed by Anthropic that creates a universal way for AI systems to connect with external data sources and tools. MCP establishes a standardized communication channel between:\n\n- **MCP Clients**: AI assistants like Claude that consume data (e.g., Claude Desktop, Cursor.ai)\n- **MCP Servers**: Services that expose data and functionality (like this MongoDB server)\n\nKey benefits of MCP:\n- **Universal Access**: Provides a single protocol for AI assistants to query data from various sources\n- **Standardized Connections**: Handles authentication, usage policies, and data formats consistently\n- **Sustainable Ecosystem**: Promotes reusable connectors that work across multiple LLM clients\n\n## ✨ Features\n\n- 🔍 Collection schema inspection\n- 📊 Document querying and filtering\n- 📈 Index management\n- 📝 Document operations (insert, update, delete)\n- 🔒 Secure database access through connection strings\n- 📋 Comprehensive error handling and validation\n\n## 📋 Prerequisites\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [MongoDB](https://www.mongodb.com/) instance (local or remote)\n- An MCP client like [Claude Desktop](https://claude.ai/download) or [Cursor.ai](https://cursor.sh/)\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n```\n\n## 🚀 Quick Start\n\nTo get started, find your MongoDB connection URL and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mongo-mcp\",\n        \"mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\n[Smithery.ai](https://smithery.ai) is a registry platform for MCP servers that simplifies discovery and installation. To install MongoDB MCP Server for Claude Desktop automatically via Smithery:\n\n```bash\nnpx -y @smithery/cli install mongo-mcp --client claude\n```\n\n### Cursor.ai Integration\n\nTo use MongoDB MCP with Cursor.ai:\n\n1. Open Cursor.ai and navigate to Settings > Features\n2. Look for \"MCP Servers\" in the features panel\n3. Add a new MCP server with the following configuration:\n   - **Name**: `mongodb`\n   - **Command**: `npx`\n   - **Args**: `mongo-mcp mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin`\n\n*Note: Cursor currently supports MCP tools only in the Agent in Composer feature.*\n\n### Test Sandbox Setup\n\nIf you don't have a MongoDB server to connect to and want to create a sample sandbox, follow these steps:\n\n1. Start MongoDB using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n2. Seed the database with test data:\n\n```bash\nnpm run seed\n```\n\n### Configure Claude Desktop\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"dist/index.js\",\n        \"mongodb://root:example@localhost:27017/test?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Test Sandbox Data Structure\n\nThe seed script creates three collections with sample data:\n\n#### Users\n\n- Personal info (name, email, age)\n- Nested address with coordinates\n- Arrays of interests\n- Membership dates\n\n#### Products\n\n- Product details (name, SKU, category)\n- Nested specifications\n- Price and inventory info\n- Tags and ratings\n\n#### Orders\n\n- Order details with items\n- User references\n- Shipping and payment info\n- Status tracking\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n### Basic Operations\n\n```\n\"What collections are available in the database?\"\n\"Show me the schema for the users collection\"\n\"Find all users in San Francisco\"\n```\n\n### Advanced Queries\n\n```\n\"Find all electronics products that are in stock and cost less than $1000\"\n\"Show me all orders from the user john@example.com\"\n\"List the products with ratings above 4.5\"\n```\n\n### Index Management\n\n```\n\"What indexes exist on the users collection?\"\n\"Create an index on the products collection for the 'category' field\"\n\"List all indexes across all collections\"\n```\n\n### Document Operations\n\n```\n\"Insert a new product with name 'Gaming Laptop' in the products collection\"\n\"Update the status of order with ID X to 'shipped'\"\n\"Find and delete all products that are out of stock\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for database interaction:\n\n### Query Tools\n\n- `listCollections`: Lists available collections in the database\n- `find`: Queries documents with filtering and projection\n- `insertOne`: Inserts a single document into a collection\n- `updateOne`: Updates a single document in a collection\n- `deleteOne`: Deletes a single document from a collection\n\n### Index Tools\n\n- `createIndex`: Creates a new index on a collection\n- `dropIndex`: Removes an index from a collection\n- `indexes`: Lists indexes for a collection\n\n## 🛠️ Development\n\nThis project is built with:\n\n- TypeScript for type-safe development\n- MongoDB Node.js driver for database operations\n- Zod for schema validation\n- Model Context Protocol SDK for server implementation\n\nTo set up the development environment:\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Run tests\nnpm test\n```\n\n## 🔒 Security Considerations\n\nWhen using this MCP server with your MongoDB database:\n\n1. **Create a dedicated MongoDB user** with minimal permissions needed for your use case\n2. **Never use admin credentials** in production environments\n3. **Enable access logging** for audit purposes\n4. **Set appropriate read/write permissions** on collections\n5. **Use connection string parameters** to restrict access (e.g., `readPreference=secondary`)\n6. **Consider IP allow-listing** to restrict database access\n\n⚠️ **IMPORTANT**: Always follow the principle of least privilege when configuring database access.\n\n## 🌐 How It Works\n\nThe MongoDB MCP server:\n\n1. Connects to your MongoDB database using the connection string provided\n2. Exposes MongoDB operations as tools that follow the MCP specification\n3. Validates inputs using Zod for type safety and security\n4. Executes queries and returns structured data to the LLM client\n5. Manages connection pooling and proper error handling\n\nAll operations are executed with proper validation to prevent security issues such as injection attacks.\n\n## 📦 Deployment\n\nYou can deploy this MCP server in several ways:\n\n- Locally via npx (as shown in Quick Start)\n- As a global npm package: `npm install -g @coderay/mongo-mcp-server`\n- In a Docker container (see Dockerfile in the repository)\n- As a service on platforms like Heroku, Vercel, or AWS\n\n## ❓ Troubleshooting\n\n### Common Issues\n\n1. **Connection Errors**\n   - Verify your MongoDB connection string is correct\n   - Check that your MongoDB server is running and accessible\n   - Ensure network permissions allow the connection\n\n2. **Authentication Issues**\n   - Confirm username and password are correct\n   - Verify the authentication database is specified (usually `authSource=admin`)\n   - Check if MongoDB requires TLS/SSL connections\n\n3. **Tool Execution Problems**\n   - Restart Claude Desktop or Cursor.ai completely\n   - Check the logs for detailed error messages:\n     ```bash\n     # macOS\n     tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n     ```\n\n4. **Performance Issues**\n   - Consider adding appropriate indexes to frequently queried fields\n   - Use projection to limit the data returned in queries\n   - Use limit and skip parameters for pagination\n\n### Getting Help\n\nIf you encounter issues:\n- Review the [MCP Documentation](https://modelcontextprotocol.io)\n- Submit an issue on our [GitHub repository](https://github.com/1rb/mongo-mcp/issues)\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "mongo",
        "databases",
        "mcp mongodb",
        "mongodb databases",
        "mongodb mcp"
      ],
      "category": "databases"
    },
    "Abeautifulsnow--tdengine-mcp": {
      "owner": "Abeautifulsnow",
      "name": "tdengine-mcp",
      "url": "https://github.com/Abeautifulsnow/tdengine-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Abeautifulsnow.webp",
      "description": "Enables read-only queries on TDengine databases, allowing AI assistants to execute SELECT, SHOW, and DESCRIBE queries to retrieve metadata without altering data. Facilitates integration of TDengine data access within AI tools for data exploration and analysis.",
      "stars": 9,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-03T03:07:50Z",
      "readme_content": "# TDengine Query MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@Abeautifulsnow/tdengine-mcp)](https://smithery.ai/server/@Abeautifulsnow/tdengine-mcp)\n\nA Model Context Protocol (MCP) server that provides **read-only** TDengine database queries for AI assistants. Execute queries, explore database structures, and investigate your data directly from your AI-powered tools.\n\n## Supported AI Tools\n\nThis MCP server works with any tool that supports the Model Context Protocol, including:\n\n- **Cursor IDE**: Set up in `.cursor/mcp.json`\n- **Anthropic Claude**: Use with a compatible MCP client\n- **Other MCP-compatible AI assistants**: Follow the tool's MCP configuration instructions\n\n## Features & Limitations\n\n### What It Does\n\n- ✅ Execute **read-only** TDengine queries (SELECT, SHOW, DESCRIBE only)\n- ✅ Provide database/stable information and metadata\n- ✅ List available database and stables\n\n### What It Doesn't Do\n\n- ❌ Execute write operations (INSERT, UPDATE, DELETE, CREATE, ALTER, etc.)\n- ❌ Provide database design or schema generation capabilities\n- ❌ Function as a full database management tool\n\nThis tool is designed specifically for **data investigation and exploration** through read-only queries. **It is not intended for database administration, schema management, or data modification.**\n\n## How to use\n\n### Run from source code\n\n**The recommended way** to use this MCP server is to run it directly with `uv` without installation. This is how both Claude Desktop and Cursor are configured to use it in the examples below.\n\nIf you want to clone the repository:\n\n```bash\ngit clone https://github.com/Abeautifulsnow/tdengine-mcp.git\ncd tdengine-mcp\n```\n\nThen you can run the server directly:\n\n```bash\nuv run src/tdengine_mcp_server -th 192.100.8.22 -db log -ll debug\n```\n\nAlternatively you can change the `.env` file in the `src/tdengine_mcp_server/` directory to set the environment variables and run the server with the following command:\n\n```bash\nuv run src/tdengine_mcp_server\n```\n\n> Important: the .env file will have **higher priority** than the command line arguments.\n\n### Install From Pypi by `pip` command\n\n```bash\n# Install globally with pip\npip install tdengine_mcp_server\n```\n\nand then run:\n\n```bash\npython -m tdengine_mcp_server -h\n```\n\n### Install by `uvx` command\n\n```bash\nuvx tdengine-mcp-server -h\n```\n\n### Install From smithery by `npx` command\n\n```bash\nnpx -y @smithery/cli@latest install @Abeautifulsnow/tdengine-mcp --client cursor --config '\"{}\"'\n```\n\nYou can change the client after the `--client` option with alternatives `claude`, 'windsurf' and so on. Also you can refer to this: [smithery/tdengine-mcp-server](https://smithery.ai/server/@Abeautifulsnow/tdengine-mcp)\n\n## Configuration Options\n\n### .env file\n\n| Environment Variable | Description | Default |\n|---------------------|-------------|---------|\n| LOG_LEVEL | Set the log level (DEBUG, INFO, WARN, ERROR) | INFO |\n| TDENGINE_HOST | Database host for environment | localhost |\n| TDENGINE_PORT | Database port | 6041 |\n| TDENGINE_USERNAME | Database username | root |\n| TDENGINE_PASSWORD | Database password | taosdata |\n| TDENGINE_DATABASE | Database name | log |\n| TDENGINE_TIMEOUT | Set the connection timeout in seconds | 30 |\n| TRANSPORT | Control the transport to use | stdio |\n\n### cli usage\n\n```text\n$ python3 -m tdengine_mcp_server -h\n\nusage: __main__.py [-h] [-th TAOS_HOST] [-tp TAOS_PORT] [-tu TAOS_USERNAME] [-pwd TAOS_PASSWORD] [-db TAOS_DATABASE] [-to TAOS_TIMEOUT] [-ll LOG_LEVEL]\n\nTDengine MCP Server\n\noptions:\n  -h, --help            show this help message and exit\n  -th, --taos-host TAOS_HOST\n                        TDengine host address. Default: `localhost`\n  -tp, --taos-port TAOS_PORT\n                        TDengine port number. Default: `6041`\n  -tu, --taos-username TAOS_USERNAME\n                        TDengine username. Default: `root`\n  -pwd, --taos-password TAOS_PASSWORD\n                        TDengine password. Default: `taosdata`\n  -db, --taos-database TAOS_DATABASE\n                        TDengine database name. Default: `default`\n  -to, --taos-timeout TAOS_TIMEOUT\n                        TDengine connection timeout. Default: `30`\n  -ll, --log-level LOG_LEVEL\n                        Log level. Default: `INFO`\n  -trans, --transport {sse,stdio}\n                        The transport to use. Default: `sse`\n```\n\n## Integration with AI Assistants\n\nYour AI assistant can interact with TDengine databases through the MCP server. Here are some examples:\n\nExample queries:\n\n```\nCan you use the query tool to show me the first 10 records from the database?\n```\n\n```\nI need to analyze our sales data. Can you run a SQL query to get the total sales per region for last month from the development database?\n```\n\n```\nCan you list all the available databases we have?\n```\n\n### Using TDengine MCP Tools\n\nThe TDengine Query MCP server provides three main tools that your AI assistant can use:\n\n#### 1. query\n\nExecute read-only SQL queries against a specific stable:\n\n```\nUse the query tool to run:\n\nSELECT * FROM customers WHERE itemid > '2025-01-01' LIMIT 10;\n```\n\n#### 2. info\n\nGet detailed information about your stable:\n\n```\nUse the info tool to check the meta info about the specified stable.\n\nDESCRIBE disks_info;\n```\n\n## Security Considerations\n\n- ✅ Only read-only queries are allowed (SELECT, SHOW, DESCRIBE)\n\n## Troubleshooting\n\n### Connection Issues\n\nIf you're having trouble connecting:\n\n1. Verify your database credentials in your MCP configuration\n2. Ensure the TDengine server is running and accessible\n3. Check for firewall rules blocking connections\n4. Enable debug mode by setting `LOG_LEVEL` in your configuration\n\n### Common Errors\n\n**Error: Query execution failed**\n\n- Verify your SQL syntax\n- Check that you're only using supported query types (SELECT, SHOW, DESCRIBE)\n- Ensure your query is truly read-only\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n---\n\nFor more information or support, please [open an issue](https://github.com/Abeautifulsnow/tdengine-mcp/issues) on the GitHub repository. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "tdengine",
        "database",
        "tdengine databases",
        "queries tdengine",
        "tdengine data"
      ],
      "category": "databases"
    },
    "Aniket310101--MCP-Server-Couchbase": {
      "owner": "Aniket310101",
      "name": "MCP-Server-Couchbase",
      "url": "https://github.com/Aniket310101/MCP-Server-Couchbase",
      "imageUrl": "/freedevtools/mcp/pfp/Aniket310101.webp",
      "description": "Facilitates interaction with Couchbase databases on Capella clusters through natural language, enabling CRUD operations, N1QL query execution, and data management.",
      "stars": 7,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-23T08:05:57Z",
      "readme_content": "[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/69df67ac-e748-4b8d-954e-c98e632fd53f)\n\n# 🗄️ Couchbase MCP Server for LLMs\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with Couchbase databases on Capella clusters. Query buckets, perform CRUD operations, execute N1QL queries, and manage data seamlessly through natural language.\n\n## 🚀 Quick Start\n\n1. **Prerequisites**\n\n   - Node.js 16 or higher\n   - A running Couchbase instance on Capella\n   - Claude Desktop application\n\n2. **Installation**\n\n   Couchbase MCP Server can be installed in two ways:\n\n   ### Option 1: Using NPX (Recommended)\n\n   The quickest way to get started is using NPX:\n\n   ```bash\n   npx -y @couchbasedatabase/couchbase-mcp\n   ```\n\n   ### Option 2: Manual Installation\n\n   If you prefer to clone and run the project manually:\n\n   ```bash\n   # Clone the repository\n   git clone https://github.com/Aniket310101/MCP-Server-Couchbase.git\n   cd MCP-Server-Couchbase\n\n   # Install dependencies\n   npm install\n\n   # Build the project\n   npm run build\n   ```\n\n3. **Claude Desktop Integration**\n\n   Add this configuration to your Claude Desktop config file:\n\n   **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`  \n   **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n   ### Option 1: With Package Installation\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"couchbase\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@couchbasedatabase/couchbase-mcp\"],\n         \"env\": {\n           \"COUCHBASE_URL\": \"<COUCHBASE CONNECTION STRING>\",\n           \"COUCHBASE_BUCKET\": \"<BUCKET NAME>\",\n           \"COUCHBASE_USERNAME\": \"<COUCHBASE USERNAME>\",\n           \"COUCHBASE_PASSWORD\": \"<COUCHBASE PASSWORD>\"\n         }\n       }\n     }\n   }\n   ```\n\n   ### Option 2: With Manual Installation\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"couchbase\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/MCP-Server-Couchbase/dist/index.js\"],\n         \"env\": {\n           \"COUCHBASE_URL\": \"<COUCHBASE CONNECTION STRING>\",\n           \"COUCHBASE_BUCKET\": \"<BUCKET NAME>\",\n           \"COUCHBASE_USERNAME\": \"<COUCHBASE USERNAME>\",\n           \"COUCHBASE_PASSWORD\": \"<COUCHBASE PASSWORD>\"\n         }\n       }\n     }\n   }\n   ```\n\n4. **Verify Connection**\n\n   - Restart Claude Desktop\n   - The Couchbase MCP server tools should now be available in your conversations\n\n## 📝 Available Tools\n\n### Basic Operations\n\n- `query`: Execute N1QL queries\n- `listBuckets`: List available buckets\n\n### Scope Management\n\n- `createScope`: Create a new scope in a bucket\n- `deleteScope`: Delete an existing scope\n- `listScopes`: List all scopes in a bucket\n\n### Collection Management\n\n- `createCollection`: Create a new collection in a scope\n- `dropCollection`: Delete a collection from a scope\n\n### Document Operations\n\n- `createDocument`: Create a new document\n- `getDocument`: Retrieve a document by ID\n- `updateDocument`: Update an existing document\n- `deleteDocument`: Delete a document by ID\n- `bulkCreateDocuments`: Create multiple documents at once\n\n### Index Management\n\n- `createIndex`: Create a new index on specified fields\n- `createPrimaryIndex`: Create a primary index on a collection\n- `listIndexes`: List all indexes in a bucket\n- `dropIndex`: Drop an existing index\n\nEach tool supports optional `collection` and `scope` parameters for targeting specific data containers.\n\n## 🔒 Security Considerations\n\n- Always use environment variables for sensitive credentials\n- Consider running the server behind a reverse proxy for production use\n- Implement appropriate access controls and authentication as needed\n\n## 📚 Examples\n\nHere are some example interactions with Claude using the MCP server:\n\n1. List all buckets:\n\n   ```\n   Could you show me all available buckets in the database?\n   ```\n\n2. Create a scope and collection:\n\n   ```\n   Create a new scope called \"users\" and a collection called \"profiles\" in it\n   ```\n\n3. Query documents:\n\n   ```\n   Find all users who signed up in the last 30 days\n   ```\n\n4. Create a document:\n   ```\n   Create a new user document with name \"John Doe\" and email \"john@example.com\"\n   ```\n\n## 🤝 Contribution\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 🛡️ Security Assessment Badge (MseeP.ai)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/aniket310101-mcp-server-couchbase-badge.png)](https://mseep.ai/app/aniket310101-mcp-server-couchbase)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "couchbase",
        "databases",
        "database",
        "couchbase databases",
        "server couchbase",
        "couchbase facilitates"
      ],
      "category": "databases"
    },
    "Anthony9906--supabase-mcp-server": {
      "owner": "Anthony9906",
      "name": "supabase-mcp-server",
      "url": "https://github.com/Anthony9906/supabase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Anthony9906.webp",
      "description": "Handles SQL query execution, database management, API access, and user authentication with integrated safety controls.",
      "stars": 1,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-06-11T01:38:36Z",
      "readme_content": "# Query MCP (Supabase MCP Server)\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/4a363bcd-7c15-47fa-a72a-d159916517f7\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" />\n    <img alt=\"Supabase\" src=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" height=\"40\" />\n  </picture>\n  &nbsp;&nbsp;\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/38db1bcd-50df-4a49-a106-1b5afd924cb2\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" />\n    <img alt=\"MCP\" src=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" height=\"40\" />\n  </picture>\n</p>\n\n<p align=\"center\">\n  <strong>Enable your favorite IDE to safely execute SQL queries, manage your database end-to-end, access Management API, and handle user authentication with built-in safety controls.</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://thequery.dev\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/supabase-mcp-server/\"><img src=\"https://img.shields.io/pypi/v/supabase-mcp-server.svg\" alt=\"PyPI version\" /></a>\n  <a href=\"https://github.com/alexander-zuev/supabase-mcp-server/actions\"><img src=\"https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg\" alt=\"CI Status\" /></a>\n  <a href=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server\"><img src=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server/branch/main/graph/badge.svg\" alt=\"Code Coverage\" /></a>\n  <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.12%2B-blue.svg\" alt=\"Python 3.12+\" /></a>\n  <a href=\"https://github.com/astral-sh/uv\"><img src=\"https://img.shields.io/badge/uv-package%20manager-blueviolet\" alt=\"uv package manager\" /></a>\n  <a href=\"https://pepy.tech/project/supabase-mcp-server\"><img src=\"https://static.pepy.tech/badge/supabase-mcp-server\" alt=\"PyPI Downloads\" /></a>\n  <a href=\"https://smithery.ai/server/@alexander-zuev/supabase-mcp-server\"><img src=\"https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server\" alt=\"Smithery.ai Downloads\" /></a>\n  <a href=\"https://modelcontextprotocol.io/introduction\"><img src=\"https://img.shields.io/badge/MCP-Server-orange\" alt=\"MCP Server\" /></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n</p>\n\n\n## 🎉 The Future of Supabase MCP Server -> Query MCP\n\n**I'm thrilled to announce that Supabase MCP Server is evolving into [thequery.dev](https://thequery.dev)!**\n\nWhile I have big plans for the future, I want to make these commitments super clear:\n- **The core tool will stay free forever** - free & open-source software is how I got into coding\n- **Premium features will be added on top** - enhancing capabilities without limiting existing functionality\n- **First 2,000 early adopters will get special perks** - join early for an exclusive treat!\n\n**🚀 BIG v4 Launch Coming Soon!**\n\n[**👉 Join Early Access at thequery.dev**](https://thequery.dev)\n\n## Table of contents\n<p align=\"center\">\n  <a href=\"#getting-started\">Getting started</a> •\n  <a href=\"#feature-overview\">Feature overview</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#changelog\">Changelog</a>\n</p>\n\n## ✨ Key features\n- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol\n- 🔐 Control read-only and read-write modes of SQL query execution\n- 🔍 Runtime SQL query validation with risk level assessment\n- 🛡️ Three-tier safety system for SQL operations: safe, write, and destructive\n- 🔄 Robust transaction handling for both direct and pooled database connections\n- 📝 Automatic versioning of database schema changes\n- 💻 Manage your Supabase projects with Supabase Management API\n- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK\n- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively\n- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)\n\n\n## Getting Started\n\n### Prerequisites\nInstalling the server requires the following on your system:\n- Python 3.12+\n\nIf you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).\n\n### PostgreSQL Installation\nPostgreSQL installation is no longer required for the MCP server itself, as it now uses asyncpg which doesn't depend on PostgreSQL development libraries.\n\nHowever, you'll still need PostgreSQL if you're running a local Supabase instance:\n\n**MacOS**\n```bash\nbrew install postgresql@16\n```\n\n**Windows**\n  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/\n  - Ensure \"PostgreSQL Server\" and \"Command Line Tools\" are selected during installation\n\n### Step 1. Installation\n\nSince v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:\n\n```bash\n# if pipx is installed (recommended)\npipx install supabase-mcp-server\n\n# if uv is installed\nuv pip install supabase-mcp-server\n```\n\n`pipx` is recommended because it creates isolated environments for each package.\n\nYou can also install the server manually by cloning the repository and running `pipx install -e .` from the root directory.\n\n#### Installing from source\nIf you would like to install from source, for example for local development:\n```bash\nuv venv\n# On Mac\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n# Install package in editable mode\nuv pip install -e .\n```\n\n#### Installing via Smithery.ai\n\nYou can find the full instructions on how to use Smithery.ai to connect to this MCP server [here](https://smithery.ai/server/@alexander-zuev/supabase-mcp-server).\n\n\n### Step 2. Configuration\n\nThe Supabase MCP server requires configuration to connect to your Supabase database, access the Management API, and use the Auth Admin SDK. This section explains all available configuration options and how to set them up.\n\n#### Environment Variables\n\nThe server uses the following environment variables:\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `SUPABASE_PROJECT_REF` | Yes | `127.0.0.1:54322` | Your Supabase project reference ID (or local host:port) |\n| `SUPABASE_DB_PASSWORD` | Yes | `postgres` | Your database password |\n| `SUPABASE_REGION` | Yes* | `us-east-1` | AWS region where your Supabase project is hosted |\n| `SUPABASE_ACCESS_TOKEN` | No | None | Personal access token for Supabase Management API |\n| `SUPABASE_SERVICE_ROLE_KEY` | No | None | Service role key for Auth Admin SDK |\n\n> **Note**: The default values are configured for local Supabase development. For remote Supabase projects, you must provide your own values for `SUPABASE_PROJECT_REF` and `SUPABASE_DB_PASSWORD`.\n\n> 🚨 **CRITICAL CONFIGURATION NOTE**: For remote Supabase projects, you MUST specify the correct region where your project is hosted using `SUPABASE_REGION`. If you encounter a \"Tenant or user not found\" error, this is almost certainly because your region setting doesn't match your project's actual region. You can find your project's region in the Supabase dashboard under Project Settings.\n\n#### Connection Types\n\n##### Database Connection\n- The server connects to your Supabase PostgreSQL database using the transaction pooler endpoint\n- Local development uses a direct connection to `127.0.0.1:54322`\n- Remote projects use the format: `postgresql://postgres.[project_ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres`\n\n> ⚠️ **Important**: Session pooling connections are not supported. The server exclusively uses transaction pooling for better compatibility with the MCP server architecture.\n\n##### Management API Connection\n- Requires `SUPABASE_ACCESS_TOKEN` to be set\n- Connects to the Supabase Management API at `https://api.supabase.com`\n- Only works with remote Supabase projects (not local development)\n\n##### Auth Admin SDK Connection\n- Requires `SUPABASE_SERVICE_ROLE_KEY` to be set\n- For local development, connects to `http://127.0.0.1:54321`\n- For remote projects, connects to `https://[project_ref].supabase.co`\n\n#### Configuration Methods\n\nThe server looks for configuration in this order (highest to lowest priority):\n\n1. **Environment Variables**: Values set directly in your environment\n2. **Local `.env` File**: A `.env` file in your current working directory (only works when running from source)\n3. **Global Config File**:\n   - Windows: `%APPDATA%\\supabase-mcp\\.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n4. **Default Settings**: Local development defaults (if no other config is found)\n\n> ⚠️ **Important**: When using the package installed via pipx or uv, local `.env` files in your project directory are **not** detected. You must use either environment variables or the global config file.\n\n#### Setting Up Configuration\n\n##### Option 1: Client-Specific Configuration (Recommended)\n\nSet environment variables directly in your MCP client configuration (see client-specific setup instructions in Step 3). Most MCP clients support this approach, which keeps your configuration with your client settings.\n\n##### Option 2: Global Configuration\n\nCreate a global `.env` configuration file that will be used for all MCP server instances:\n\n```bash\n# Create config directory\n# On macOS/Linux\nmkdir -p ~/.config/supabase-mcp\n# On Windows (PowerShell)\nmkdir -Force \"$env:APPDATA\\supabase-mcp\"\n\n# Create and edit .env file\n# On macOS/Linux\nnano ~/.config/supabase-mcp/.env\n# On Windows (PowerShell)\nnotepad \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nAdd your configuration values to the file:\n\n```\nSUPABASE_PROJECT_REF=your-project-ref\nSUPABASE_DB_PASSWORD=your-db-password\nSUPABASE_REGION=us-east-1\nSUPABASE_ACCESS_TOKEN=your-access-token\nSUPABASE_SERVICE_ROLE_KEY=your-service-role-key\n```\n\n##### Option 3: Project-Specific Configuration (Source Installation Only)\n\nIf you're running the server from source (not via package), you can create a `.env` file in your project directory with the same format as above.\n\n#### Finding Your Supabase Project Information\n\n- **Project Reference**: Found in your Supabase project URL: `https://supabase.com/dashboard/project/<project-ref>`\n- **Database Password**: Set during project creation or found in Project Settings → Database\n- **Access Token**: Generate at https://supabase.com/dashboard/account/tokens\n- **Service Role Key**: Found in Project Settings → API → Project API keys\n\n#### Supported Regions\n\nThe server supports all Supabase regions:\n\n- `us-west-1` - West US (North California)\n- `us-east-1` - East US (North Virginia) - default\n- `us-east-2` - East US (Ohio)\n- `ca-central-1` - Canada (Central)\n- `eu-west-1` - West EU (Ireland)\n- `eu-west-2` - West Europe (London)\n- `eu-west-3` - West EU (Paris)\n- `eu-central-1` - Central EU (Frankfurt)\n- `eu-central-2` - Central Europe (Zurich)\n- `eu-north-1` - North EU (Stockholm)\n- `ap-south-1` - South Asia (Mumbai)\n- `ap-southeast-1` - Southeast Asia (Singapore)\n- `ap-northeast-1` - Northeast Asia (Tokyo)\n- `ap-northeast-2` - Northeast Asia (Seoul)\n- `ap-southeast-2` - Oceania (Sydney)\n- `sa-east-1` - South America (São Paulo)\n\n#### Limitations\n\n- **No Self-Hosted Support**: The server only supports official Supabase.com hosted projects and local development\n- **No Connection String Support**: Custom connection strings are not supported\n- **No Session Pooling**: Only transaction pooling is supported for database connections\n- **API and SDK Features**: Management API and Auth Admin SDK features only work with remote Supabase projects, not local development\n\n### Step 3. Usage\n\nIn general, any MCP client that supports `stdio` protocol should work with this MCP server. This server was explicitly tested to work with:\n- Cursor\n- Windsurf\n- Cline\n- Claude Desktop\n\nAdditionally, you can also use smithery.ai to install this server a number of clients, including the ones above.\n\nFollow the guides below to install this MCP server in your client.\n\n#### Cursor\nGo to Settings -> Features -> MCP Servers and add a new server with this configuration:\n```bash\n# can be set to any name\nname: supabase\ntype: command\n# if you installed with pipx\ncommand: supabase-mcp-server\n# if you installed with uv\ncommand: uv run supabase-mcp-server\n# if the above doesn't work, use the full path (recommended)\ncommand: /full/path/to/supabase-mcp-server  # Find with 'which supabase-mcp-server' (macOS/Linux) or 'where supabase-mcp-server' (Windows)\n```\n\nIf configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.\n![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)\n\n#### Windsurf\nGo to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n          \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n        }\n      }\n    }\n}\n```\nIf configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)\n\n#### Claude Desktop\nClaude Desktop also supports MCP servers through a JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Claude Desktop:\n   - Open Claude Desktop\n   - Go to Settings → Developer -> Edit Config MCP Servers\n   - Add a new configuration with the following JSON:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\n> ⚠️ **Important**: Unlike Windsurf and Cursor, Claude Desktop requires the **full absolute path** to the executable. Using just the command name (`supabase-mcp-server`) will result in a \"spawn ENOENT\" error.\n\nIf configuration is correct, you should see the Supabase MCP server listed as available in Claude Desktop.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/500bcd40-6245-40a7-b23b-189827ed2923)\n\n#### Cline\nCline also supports MCP servers through a similar JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Cline:\n   - Open Cline in VS Code\n   - Click on the \"MCP Servers\" tab in the Cline sidebar\n   - Click \"Configure MCP Servers\"\n   - This will open the `cline_mcp_settings.json` file\n   - Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\nIf configuration is correct, you should see a green indicator next to the Supabase MCP server in the Cline MCP Servers list, and a message confirming \"supabase MCP server connected\" at the bottom of the panel.\n\n![How successful configuration in Cline looks like](https://github.com/user-attachments/assets/6c4446ad-7a58-44c6-bf12-6c82222bbe59)\n\n### Troubleshooting\n\nHere are some tips & tricks that might help you:\n- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.\n- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.\n- **\"No tools found\" error** - If you see \"Client closed - no tools available\" in Cursor despite the package being installed:\n  - Find the full path to the executable by running `which supabase-mcp-server` (macOS/Linux) or `where supabase-mcp-server` (Windows)\n  - Use the full path in your MCP server configuration instead of just `supabase-mcp-server`\n  - For example: `/Users/username/.local/bin/supabase-mcp-server` or `C:\\Users\\username\\.local\\bin\\supabase-mcp-server.exe`\n- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\\supabase-mcp\\.env` on Windows).\n- **Accessing logs** - The MCP server writes detailed logs to a file:\n  - Log file location:\n    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`\n    - Windows: `%USERPROFILE%\\.local\\share\\supabase-mcp\\mcp_server.log`\n  - Logs include connection status, configuration details, and operation results\n  - View logs using any text editor or terminal commands:\n    ```bash\n    # On macOS/Linux\n    cat ~/.local/share/supabase-mcp/mcp_server.log\n\n    # On Windows (PowerShell)\n    Get-Content \"$env:USERPROFILE\\.local\\share\\supabase-mcp\\mcp_server.log\"\n    ```\n\nIf you are stuck or any of the instructions above are incorrect, please raise an issue.\n\n### MCP Inspector\nA super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.\n> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the coming release.\n\n## Feature Overview\n\n### Database query tools\n\nSince v0.3+ server provides comprehensive database management capabilities with built-in safety controls:\n\n- **SQL Query Execution**: Execute PostgreSQL queries with risk assessment\n  - **Three-tier safety system**:\n    - `safe`: Read-only operations (SELECT) - always allowed\n    - `write`: Data modifications (INSERT, UPDATE, DELETE) - require unsafe mode\n    - `destructive`: Schema changes (DROP, CREATE) - require unsafe mode + confirmation\n\n- **SQL Parsing and Validation**:\n  - Uses PostgreSQL's parser (pglast) for accurate analysis and provides clear feedback on safety requirements\n\n- **Automatic Migration Versioning**:\n  - Database-altering operations operations are automatically versioned\n  - Generates descriptive names based on operation type and target\n\n\n- **Safety Controls**:\n  - Default SAFE mode allows only read-only operations\n  - All statements run in transaction mode via `asyncpg`\n  - 2-step confirmation for high-risk operations\n\n- **Available Tools**:\n  - `get_schemas`: Lists schemas with sizes and table counts\n  - `get_tables`: Lists tables, foreign tables, and views with metadata\n  - `get_table_schema`: Gets detailed table structure (columns, keys, relationships)\n  - `execute_postgresql`: Executes SQL statements against your database\n  - `confirm_destructive_operation`: Executes high-risk operations after confirmation\n  - `retrieve_migrations`: Gets migrations with filtering and pagination options\n  - `live_dangerously`: Toggles between safe and unsafe modes\n\n### Management API tools\n\nSince v0.3.0 server provides secure access to the Supabase Management API with built-in safety controls:\n\n- **Available Tools**:\n  - `send_management_api_request`: Sends arbitrary requests to Supabase Management API with auto-injection of project ref\n  - `get_management_api_spec`: Gets the enriched API specification with safety information\n    - Supports multiple query modes: by domain, by specific path/method, or all paths\n    - Includes risk assessment information for each endpoint\n    - Provides detailed parameter requirements and response formats\n    - Helps LLMs understand the full capabilities of the Supabase Management API\n  - `get_management_api_safety_rules`: Gets all safety rules with human-readable explanations\n  - `live_dangerously`: Toggles between safe and unsafe operation modes\n\n- **Safety Controls**:\n  - Uses the same safety manager as database operations for consistent risk management\n  - Operations categorized by risk level:\n    - `safe`: Read-only operations (GET) - always allowed\n    - `unsafe`: State-changing operations (POST, PUT, PATCH, DELETE) - require unsafe mode\n    - `blocked`: Destructive operations (delete project, etc.) - never allowed\n  - Default safe mode prevents accidental state changes\n  - Path-based pattern matching for precise safety rules\n\n**Note**: Management API tools only work with remote Supabase instances and are not compatible with local Supabase development setups.\n\n### Auth Admin tools\n\nI was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.\n\nSince v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:\n  - Includes the following tools:\n    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods\n    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling\n  - Supported methods:\n    - `get_user_by_id`: Retrieve a user by their ID\n    - `list_users`: List all users with pagination\n    - `create_user`: Create a new user\n    - `delete_user`: Delete a user by their ID\n    - `invite_user_by_email`: Send an invite link to a user's email\n    - `generate_link`: Generate an email link for various authentication purposes\n    - `update_user_by_id`: Update user attributes by ID\n    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)\n\n#### Why use Auth Admin SDK instead of raw SQL queries?\n\nThe Auth Admin SDK provides several key advantages over direct SQL manipulation:\n- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)\n- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas\n- **Simplicity**: Offers clear methods with proper validation and error handling\n\n  - Response format:\n    - All methods return structured Python objects instead of raw dictionaries\n    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user[\"id\"]`)\n  - Edge cases and limitations:\n    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors\n    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project\n    - Link types: When generating links, different link types have different requirements:\n      - `signup` links don't require the user to exist\n      - `magiclink` and `recovery` links require the user to already exist in the system\n    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface\n    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK\n\n### Logs & Analytics\n\nThe server provides access to Supabase logs and analytics data, making it easier to monitor and troubleshoot your applications:\n\n- **Available Tool**: `retrieve_logs` - Access logs from any Supabase service\n\n- **Log Collections**:\n  - `postgres`: Database server logs\n  - `api_gateway`: API gateway requests\n  - `auth`: Authentication events\n  - `postgrest`: RESTful API service logs\n  - `pooler`: Connection pooling logs\n  - `storage`: Object storage operations\n  - `realtime`: WebSocket subscription logs\n  - `edge_functions`: Serverless function executions\n  - `cron`: Scheduled job logs\n  - `pgbouncer`: Connection pooler logs\n\n- **Features**: Filter by time, search text, apply field filters, or use custom SQL queries\n\nSimplifies debugging across your Supabase stack without switching between interfaces or writing complex queries.\n\n### Automatic Versioning of Database Changes\n\n\"With great power comes great responsibility.\" While `execute_postgresql` tool coupled with aptly named `live_dangerously` tool provide a powerful and simple way to manage your Supabase database, it also means that dropping a table or modifying one is one chat message away. In order to reduce the risk of irreversible changes, since v0.3.8 the server supports:\n- automatic creation of migration scripts for all write & destructive sql operations executed on the database\n- improved safety mode of query execution, in which all queries are categorized in:\n  - `safe` type: always allowed. Includes all read-only ops.\n  - `write`type: requires `write` mode to be enabled by the user.\n  - `destructive` type: requires `write` mode to be enabled by the user AND a 2-step confirmation of query execution for clients that do not execute tools automatically.\n\n### Universal Safety Mode\nSince v0.3.8 Safety Mode has been standardized across all services (database, API, SDK) using a universal safety manager. This provides consistent risk management and a unified interface for controlling safety settings across the entire MCP server.\n\nAll operations (SQL queries, API requests, SDK methods) are categorized into risk levels:\n- `Low` risk: Read-only operations that don't modify data or structure (SELECT queries, GET API requests)\n- `Medium` risk: Write operations that modify data but not structure (INSERT/UPDATE/DELETE, most POST/PUT API requests)\n- `High` risk: Destructive operations that modify database structure or could cause data loss (DROP/TRUNCATE, DELETE API endpoints)\n- `Extreme` risk: Operations with severe consequences that are blocked entirely (deleting projects)\n\nSafety controls are applied based on risk level:\n- Low risk operations are always allowed\n- Medium risk operations require unsafe mode to be enabled\n- High risk operations require unsafe mode AND explicit confirmation\n- Extreme risk operations are never allowed\n\n#### How confirmation flow works\n\nAny high-risk operations (be it a postgresql or api request) will be blocked even in `unsafe` mode.\n![Every high-risk operation is blocked](https://github.com/user-attachments/assets/c0df79c2-a879-4b1f-a39d-250f9965c36a)\nYou will have to confirm and approve every high-risk operation explicitly in order for it to be executed.\n![Explicit approval is always required](https://github.com/user-attachments/assets/5cd7a308-ec2a-414e-abe2-ff2f3836dd8b)\n\n\n## Changelog\n\n- 📦 Simplified installation via package manager - ✅ (v0.2.0)\n- 🌎 Support for different Supabase regions - ✅ (v0.2.2)\n- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)\n- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)\n- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)\n- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)\n- 🔍 Stronger SQL query validation ✅ (v0.3.8)\n- 📝 Automatic versioning of database changes ✅ (v0.3.8)\n- 📖 Radically improved knowledge and tools of api spec ✅ (v0.3.8)\n- ✍️ Improved consistency of migration-related tools for a more organized database vcs ✅ (v0.3.10)\n\n\nFor a more detailed roadmap, please see this [discussion](https://github.com/alexander-zuev/supabase-mcp-server/discussions/46) on GitHub.\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)\n\n---\n\nEnjoy! ☺️",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase mcp",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "AnuragRai017--database-updater-MCP-Server": {
      "owner": "AnuragRai017",
      "name": "database-updater-MCP-Server",
      "url": "https://github.com/AnuragRai017/database-updater-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/AnuragRai017.webp",
      "description": "Update databases by importing data from CSV and Excel files, with capabilities for managing documentation through built-in note-taking features. Supports PostgreSQL, MySQL, MongoDB, and SQLite for flexible database type management.",
      "stars": 1,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-08-19T00:32:17Z",
      "readme_content": "# database-updater MCP Server\n\nA Model Context Protocol server for updating databases from CSV and Excel files.\n\n## Features\n\n### Tools\n- `update_database` - Update database from CSV/Excel files\n  - Supports CSV and Excel (.xlsx, .xls) file formats\n  - Compatible with multiple database types (PostgreSQL, MySQL, MongoDB, SQLite)\n  - Configurable connection settings and table mapping\n  \n- `create_note` - Create and manage notes (for documentation)\n  - Store important information about database updates\n  - Track changes and modifications\n\n## Usage\n\n### Update Database\nUse the `update_database` tool with the following parameters:\n```json\n{\n  \"filePath\": \"/path/to/your/file.csv\",\n  \"databaseType\": \"PostgreSQL\",\n  \"connectionString\": \"postgresql://user:pass@localhost:5432/db\",\n  \"tableName\": \"target_table\"\n}\n```\n\n### Supported Database Types\n- PostgreSQL\n- MySQL\n- MongoDB\n- SQLite\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"database-updater\": {\n      \"command\": \"/path/to/database-updater/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "postgresql",
        "secure database",
        "databases secure",
        "database updater"
      ],
      "category": "databases"
    },
    "Atomzzm--mcp-mysql-server": {
      "owner": "Atomzzm",
      "name": "mcp-mysql-server",
      "url": "https://github.com/Atomzzm/mcp-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/Atomzzm.webp",
      "description": "Facilitates interaction with MySQL databases through a standardized interface, enabling operations such as querying, inserting, updating, and deleting data.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-02-22T00:54:00Z",
      "readme_content": "# @f4ww4z/mcp-mysql-server\n[![smithery badge](https://smithery.ai/badge/@f4ww4z/mcp-mysql-server)](https://smithery.ai/server/@f4ww4z/mcp-mysql-server)\n\nA Model Context Protocol server that provides MySQL database operations. This server enables AI models to interact with MySQL databases through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/qma33al6ie\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/qma33al6ie/badge\" alt=\"mcp-mysql-server MCP server\" /></a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install MySQL Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@f4ww4z/mcp-mysql-server):\n\n```bash\nnpx -y @smithery/cli install @f4ww4z/mcp-mysql-server --client claude\n```\n\n### Manual Installation\n```bash\nnpx @f4ww4z/mcp-mysql-server\n```\n\n## Configuration\n\nThe server requires the following environment variables to be set in your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@f4ww4z/mcp-mysql-server\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your_host\",\n        \"MYSQL_USER\": \"your_user\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. connect_db\nEstablish connection to MySQL database using provided credentials.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"connect_db\",\n  arguments: {\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n### 2. query\nExecute SELECT queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_tables\nList all tables in the connected database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"list_tables\",\n  arguments: {}\n});\n```\n\n### 5. describe_table\nGet the structure of a specific table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n## Features\n\n- Secure connection handling with automatic cleanup\n- Prepared statement support for query parameters\n- Comprehensive error handling and validation\n- TypeScript support\n- Automatic connection management\n\n## Security\n\n- Uses prepared statements to prevent SQL injection\n- Supports secure password handling through environment variables\n- Validates queries before execution\n- Automatically closes connections when done\n\n## Error Handling\n\nThe server provides detailed error messages for common issues:\n- Connection failures\n- Invalid queries\n- Missing parameters\n- Database errors\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/f4ww4z/mcp-mysql-server\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "CDataSoftware--adp-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "adp-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/adp-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live ADP data via a read-only MCP interface, integrating ADP data into AI clients for real-time insights and data retrieval. Utilizes CData's JDBC driver technology to provide access to complex data sources without requiring SQL knowledge.",
      "stars": 2,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-09-10T08:20:24Z",
      "readme_content": "# adp-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for ADP\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for ADP (beta)](https://www.cdata.com/download/download.aspx?sku=JDZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data ADP supported by the [CData JDBC Driver for ADP](https://www.cdata.com/drivers/adp/jdbc).\n\nCData JDBC Driver connects to ADP by exposing them as relational SQL models.\n\nThis server wraps that driver and makes ADP data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/adp-mcp-server-by-cdata.git\n      cd adp-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/adp/download/jdbc](https://www.cdata.com/drivers/adp/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for ADP\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for ADP/`\n    * Run the command `java -jar cdata.jdbc.adp.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.adp.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `adp.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.adp.ADPDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=adp\n      ServerName=CDataADP\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.adp.jar\n      DriverClass=cdata.jdbc.adp.ADPDriver\n      JdbcUrl=jdbc:adp:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\adp.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/adp.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### adp_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"adp_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### adp_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"adp_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### adp_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"adp_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cdatasoftware",
        "jdbc",
        "cdata",
        "cdata jdbc",
        "cdatasoftware adp",
        "adp data"
      ],
      "category": "databases"
    },
    "CDataSoftware--amazon-redshift-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "amazon-redshift-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/amazon-redshift-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Amazon Redshift data without the need for SQL, facilitating seamless data retrieval from Redshift through a simple MCP interface. Connects LLM-powered clients to query and explore Redshift data with built-in tools for schemas and execution.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:11:33Z",
      "readme_content": "# amazon-redshift-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Amazon Redshift\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Amazon Redshift (beta)](https://www.cdata.com/download/download.aspx?sku=FRZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Amazon Redshift supported by the [CData JDBC Driver for Amazon Redshift](https://www.cdata.com/drivers/redshift/jdbc).\n\nCData JDBC Driver connects to Amazon Redshift by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Amazon Redshift data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/amazon-redshift-mcp-server-by-cdata.git\n      cd amazon-redshift-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/redshift/download/jdbc](https://www.cdata.com/drivers/redshift/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Amazon Redshift\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Amazon Redshift/`\n    * Run the command `java -jar cdata.jdbc.redshift.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.redshift.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `amazon-redshift.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.redshift.RedshiftDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=redshift\n      ServerName=CDataRedshift\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.redshift.jar\n      DriverClass=cdata.jdbc.redshift.RedshiftDriver\n      JdbcUrl=jdbc:redshift:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\amazon-redshift.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/amazon-redshift.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### amazon_redshift_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"amazon_redshift_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### amazon_redshift_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"amazon_redshift_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### amazon_redshift_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"amazon_redshift_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "cdatasoftware",
        "secure database",
        "databases secure",
        "enables querying"
      ],
      "category": "databases"
    },
    "CDataSoftware--cdata-jdbc-mcp-server": {
      "owner": "CDataSoftware",
      "name": "cdata-jdbc-mcp-server",
      "url": "https://github.com/CDataSoftware/cdata-jdbc-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Queries live data from over 300 sources using CData JDBC Drivers through a Model Context Protocol interface, enabling natural language access without requiring SQL knowledge. It facilitates integration with AI clients to retrieve real-time information from various data sources.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-10-03T22:32:35Z",
      "readme_content": "# cdata-jdbc-mcp-server (read-only)\nOur generic Model Context Protocol (MCP) Server for CData JDBC Drivers (read-only)\n\n❗**Note:** This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Servers (beta)](https://www.cdata.com/solutions/mcp).\n\n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data from any of over 300+ sources ([listed below](#supported-sources)) supported by [CData JDBC Drivers](https://www.cdata.com/jdbc).\n\nCData JDBC Drivers connect to SaaS apps, NoSQL stores, and APIs by exposing them as relational SQL models.\n\nThis server wraps those drivers and makes their data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\nIn the guide below, `{data source}` refers to the back-end data source (e.g. Salesforce). For code snippets and commands, Salesforce is used as an example, but the patterns apply to any of our JDBC Drivers.\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/cdata-jdbc-mcp-server.git\n      cd cdata-jdbc-mcp-server\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install a CData JDBC Driver: [https://www.cdata.com/jdbc](https://www.cdata.com/jdbc)\n3. License the CData JDBC Driver (Salesforce as an example):\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for {data source}\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for {data source}/`\n    * Run the command `java -jar cdata.jdbc.salesforce.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.salesforce.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `Salesforce.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.salesforce.SalesforceDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=salesforce\n      ServerName=CDataSalesforce\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.salesforce.jar\n      DriverClass=cdata.jdbc.salesforce.SalesforceDriver\n      JdbcUrl=jdbc:salesforce:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"salesforce\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\Salesforce.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"salesforce\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/Salesforce.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `salesforce` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### {servername}_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"salesforce_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### {servername}_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"salesforce_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### {servername}_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"salesforce_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbc",
        "databases",
        "database",
        "cdata jdbc",
        "database access",
        "secure database"
      ],
      "category": "databases"
    },
    "CDataSoftware--databricks-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "databricks-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/databricks-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Connects LLMs to live Databricks data through a read-only MCP interface, facilitating natural language querying without the need for SQL. It enables seamless retrieval of up-to-date information using exposed tables and columns from Databricks via the CData JDBC Driver.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:03Z",
      "readme_content": "# databricks-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Databricks\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Databricks (beta)](https://www.cdata.com/download/download.aspx?sku=LKZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Databricks supported by the [CData JDBC Driver for Databricks](https://www.cdata.com/drivers/databricks/jdbc).\n\nCData JDBC Driver connects to Databricks by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Databricks data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/databricks-mcp-server-by-cdata.git\n      cd databricks-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/databricks/download/jdbc](https://www.cdata.com/drivers/databricks/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Databricks\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Databricks/`\n    * Run the command `java -jar cdata.jdbc.databricks.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.databricks.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `databricks.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.databricks.DatabricksDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=databricks\n      ServerName=CDataDatabricks\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.databricks.jar\n      DriverClass=cdata.jdbc.databricks.DatabricksDriver\n      JdbcUrl=jdbc:databricks:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\databricks.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/databricks.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### databricks_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"databricks_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### databricks_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"databricks_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### databricks_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"databricks_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databricks",
        "databases",
        "database",
        "cdatasoftware databricks",
        "databricks cdata",
        "live databricks"
      ],
      "category": "databases"
    },
    "CDataSoftware--dynamics-365-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "dynamics-365-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/dynamics-365-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Dynamics 365 data through an MCP interface, facilitating read-only access to data without requiring SQL knowledge. Utilizes the CData JDBC Driver to provide a simple way for LLMs to retrieve up-to-date insights from Dynamics 365.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:10Z",
      "readme_content": "# dynamics-365-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Dynamics 365\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Dynamics 365 (beta)](https://www.cdata.com/download/download.aspx?sku=LJZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Dynamics 365 supported by the [CData JDBC Driver for Dynamics 365](https://www.cdata.com/drivers/dynamics365/jdbc).\n\nCData JDBC Driver connects to Dynamics 365 by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Dynamics 365 data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/dynamics-365-mcp-server-by-cdata.git\n      cd dynamics-365-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/dynamics365/download/jdbc](https://www.cdata.com/drivers/dynamics365/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Dynamics 365\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Dynamics 365/`\n    * Run the command `java -jar cdata.jdbc.dyanmics365.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.dyanmics365.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `dynamics-365.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.dyanmics365.Dyanmics365Driver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=dyanmics365\n      ServerName=CDataDyanmics365\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.dyanmics365.jar\n      DriverClass=cdata.jdbc.dyanmics365.Dyanmics365Driver\n      JdbcUrl=jdbc:dyanmics365:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\dynamics-365.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/dynamics-365.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### dynamics_365_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"dynamics_365_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### dynamics_365_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"dynamics_365_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### dynamics_365_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"dynamics_365_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cdatasoftware",
        "databases",
        "jdbc",
        "cdata jdbc",
        "cdatasoftware dynamics",
        "utilizes cdata"
      ],
      "category": "databases"
    },
    "CDataSoftware--excel-online-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "excel-online-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/excel-online-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live data from Excel Online through a Model Context Protocol (MCP) interface, facilitating integration of Excel data into AI workflows. Provides read-only access to Excel Online data via a JDBC driver without requiring SQL knowledge.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:18Z",
      "readme_content": "# excel-online-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Excel Online\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Excel Online (beta)](https://www.cdata.com/download/download.aspx?sku=FXZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Excel Online supported by the [CData JDBC Driver for Excel Online](https://www.cdata.com/drivers/excelonline/jdbc).\n\nCData JDBC Driver connects to Excel Online by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Excel Online data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/excel-online-mcp-server-by-cdata.git\n      cd excel-online-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/excelonline/download/jdbc](https://www.cdata.com/drivers/excelonline/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Excel Online\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Excel Online/`\n    * Run the command `java -jar cdata.jdbc.excelonline.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.excelonline.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `excel-online.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.excelonline.ExcelOnlineDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=excelonline\n      ServerName=CDataExcelOnline\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.excelonline.jar\n      DriverClass=cdata.jdbc.excelonline.ExcelOnlineDriver\n      JdbcUrl=jdbc:excelonline:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\excel-online.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/excel-online.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### excel_online_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"excel_online_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### excel_online_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"excel_online_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### excel_online_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"excel_online_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "jdbc",
        "cdatasoftware excel",
        "access excel",
        "data jdbc"
      ],
      "category": "databases"
    },
    "CDataSoftware--google-bigquery-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "google-bigquery-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/google-bigquery-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Google BigQuery data, facilitating access to datasets without requiring SQL knowledge. Integrates real-time data insights with AI clients through a simple interface.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:27Z",
      "readme_content": "# google-bigquery-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Google BigQuery\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Google BigQuery (beta)](https://www.cdata.com/download/download.aspx?sku=DBZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Google BigQuery supported by the [CData JDBC Driver for Google BigQuery](https://www.cdata.com/drivers/bigquery/jdbc).\n\nCData JDBC Driver connects to Google BigQuery by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Google BigQuery data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/google-bigquery-mcp-server-by-cdata.git\n      cd google-bigquery-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/bigquery/download/jdbc](https://www.cdata.com/drivers/bigquery/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Google BigQuery\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Google BigQuery/`\n    * Run the command `java -jar cdata.jdbc.googlebigquery.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.googlebigquery.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `google-bigquery.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.googlebigquery.GoogleBigQueryDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=googlebigquery\n      ServerName=CDataGoogleBigQuery\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.googlebigquery.jar\n      DriverClass=cdata.jdbc.googlebigquery.GoogleBigQueryDriver\n      JdbcUrl=jdbc:googlebigquery:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\google-bigquery.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/google-bigquery.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### google_bigquery_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"google_bigquery_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### google_bigquery_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"google_bigquery_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### google_bigquery_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"google_bigquery_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "bigquery",
        "bigquery data",
        "google bigquery",
        "enables querying"
      ],
      "category": "databases"
    },
    "CDataSoftware--hubspot-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "hubspot-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/hubspot-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live HubSpot data via a read-only MCP interface without requiring SQL knowledge. Utilizes the CData JDBC Driver to access and retrieve real-time HubSpot information for LLMs.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:42Z",
      "readme_content": "# hubspot-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for HubSpot\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for HubSpot (beta)](https://www.cdata.com/download/download.aspx?sku=DHZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data HubSpot supported by the [CData JDBC Driver for HubSpot](https://www.cdata.com/drivers/hubspot/jdbc).\n\nCData JDBC Driver connects to HubSpot by exposing them as relational SQL models.\n\nThis server wraps that driver and makes HubSpot data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/hubspot-mcp-server-by-cdata.git\n      cd hubspot-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/hubspot/download/jdbc](https://www.cdata.com/drivers/hubspot/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for HubSpot\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for HubSpot/`\n    * Run the command `java -jar cdata.jdbc.hubspot.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.hubspot.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `hubspot.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.hubspot.HubSpotDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=hubspot\n      ServerName=CDataHubSpot\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.hubspot.jar\n      DriverClass=cdata.jdbc.hubspot.HubSpotDriver\n      JdbcUrl=jdbc:hubspot:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\hubspot.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/hubspot.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### hubspot_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"hubspot_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### hubspot_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"hubspot_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### hubspot_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"hubspot_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cdatasoftware",
        "hubspot",
        "databases",
        "cdatasoftware hubspot",
        "hubspot data",
        "cdata jdbc"
      ],
      "category": "databases"
    },
    "CDataSoftware--intacct-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "intacct-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/intacct-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Provides a read-only interface for querying live Intacct data using natural language, enabling seamless integration with large language models (LLMs) to access up-to-date financial and operational information.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:46Z",
      "readme_content": "# intacct-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Intacct\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Intacct (beta)](https://www.cdata.com/download/download.aspx?sku=CTZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Intacct supported by the [CData JDBC Driver for Intacct](https://www.cdata.com/drivers/intacct/jdbc).\n\nCData JDBC Driver connects to Intacct by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Intacct data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/intacct-mcp-server-by-cdata.git\n      cd intacct-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/intacct/download/jdbc](https://www.cdata.com/drivers/intacct/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Intacct\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Intacct/`\n    * Run the command `java -jar cdata.jdbc.sageintacct.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.sageintacct.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `intacct.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.sageintacct.SageIntacctDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=sageintacct\n      ServerName=CDataSageIntacct\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.sageintacct.jar\n      DriverClass=cdata.jdbc.sageintacct.SageIntacctDriver\n      JdbcUrl=jdbc:sageintacct:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\intacct.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/intacct.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### intacct_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"intacct_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### intacct_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"intacct_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### intacct_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"intacct_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "cdatasoftware",
        "cdatasoftware intacct",
        "database access",
        "intacct data"
      ],
      "category": "databases"
    },
    "CDataSoftware--jira-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "jira-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/jira-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Connects to live Jira data using a read-only Model Context Protocol interface, enabling natural language queries to retrieve current information without SQL knowledge.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:12:48Z",
      "readme_content": "# jira-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Jira\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Jira (beta)](https://www.cdata.com/download/download.aspx?sku=BJZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Jira supported by the [CData JDBC Driver for Jira](https://www.cdata.com/drivers/jira/jdbc).\n\nCData JDBC Driver connects to Jira by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Jira data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/jira-mcp-server-by-cdata.git\n      cd jira-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/jira/download/jdbc](https://www.cdata.com/drivers/jira/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Jira\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Jira/`\n    * Run the command `java -jar cdata.jdbc.jira.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.jira.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `jira.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.jira.JIRADriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=jira\n      ServerName=CDataJIRA\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.jira.jar\n      DriverClass=cdata.jdbc.jira.JIRADriver\n      JdbcUrl=jdbc:jira:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\jira.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/jira.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### jira_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"jira_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### jira_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"jira_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### jira_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"jira_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "jira",
        "database",
        "jira data",
        "cdatasoftware jira",
        "database access"
      ],
      "category": "databases"
    },
    "CDataSoftware--microsoft-sql-server-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "microsoft-sql-server-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/microsoft-sql-server-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Facilitates natural language querying of live Microsoft SQL Server data without requiring SQL code. It provides a read-only MCP interface for AI clients to access and integrate Microsoft SQL Server data using language models like Claude Desktop.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-05-05T21:11:56Z",
      "readme_content": "# microsoft-sql-server-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Microsoft SQL Server\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Microsoft SQL Server (beta)](https://www.cdata.com/download/download.aspx?sku=RUZK-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Microsoft SQL Server supported by the [CData JDBC Driver for Microsoft SQL Server](https://www.cdata.com/drivers/sql/jdbc).\n\nCData JDBC Driver connects to Microsoft SQL Server by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Microsoft SQL Server data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/microsoft-sql-server-mcp-server-by-cdata.git\n      cd microsoft-sql-server-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for Microsoft SQL Server: [https://www.cdata.com/drivers/sql/download/jdbc](https://www.cdata.com/drivers/sql/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Microsoft SQL Server\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Microsoft SQL Server/`\n    * Run the command `java -jar cdata.jdbc.sql.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.sql.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `microsoft-sql-server.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.sql.SQLDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=sql\n      ServerName=CDataSQL\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.sql.jar\n      DriverClass=cdata.jdbc.sql.SQLDriver\n      JdbcUrl=jdbc:sql:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"sql\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\microsoft-sql-server.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"sql\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/microsoft-sql-server.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `sql` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "cdatasoftware",
        "database access",
        "sql server",
        "microsoft sql"
      ],
      "category": "databases"
    },
    "CDataSoftware--mysql-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "mysql-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/mysql-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Connect to MySQL databases to perform natural language queries, retrieve table and column metadata, and execute SQL SELECT queries through a simple MCP interface. Designed for LLM clients to access live MySQL data seamlessly.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:06:23Z",
      "readme_content": "# mysql-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for MySQL\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for MySQL (beta)](https://www.cdata.com/download/download.aspx?sku=DMZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data MySQL supported by the [CData JDBC Driver for MySQL](https://www.cdata.com/drivers/mysql/jdbc).\n\nCData JDBC Driver connects to MySQL by exposing them as relational SQL models.\n\nThis server wraps that driver and makes MySQL data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/mysql-mcp-server-by-cdata.git\n      cd mysql-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/mysql/download/jdbc](https://www.cdata.com/drivers/mysql/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for MySQL\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for MySQL/`\n    * Run the command `java -jar cdata.jdbc.mysql.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.mysql.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `mysql.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.mysql.MySQLDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=mysql\n      ServerName=CDataMySQL\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.mysql.jar\n      DriverClass=cdata.jdbc.mysql.MySQLDriver\n      JdbcUrl=jdbc:mysql:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\mysql.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/mysql.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### mysql_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"mysql_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### mysql_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"mysql_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### mysql_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"mysql_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "cdatasoftware mysql",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "CDataSoftware--netsuite-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "netsuite-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/netsuite-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live NetSuite data via a read-only MCP interface, utilizing the CData JDBC Driver to expose NetSuite data as relational SQL models.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-09-16T02:55:04Z",
      "readme_content": "# netsuite-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for NetSuite\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for NetSuite (beta)](https://www.cdata.com/download/download.aspx?sku=DNZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data NetSuite supported by the [CData JDBC Driver for NetSuite](https://www.cdata.com/drivers/netsuite/jdbc).\n\nCData JDBC Driver connects to NetSuite by exposing them as relational SQL models.\n\nThis server wraps that driver and makes NetSuite data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/netsuite-mcp-server-by-cdata.git\n      cd netsuite-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/netsuite/download/jdbc](https://www.cdata.com/drivers/netsuite/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for NetSuite\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for NetSuite/`\n    * Run the command `java -jar cdata.jdbc.netsuite.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.netsuite.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `netsuite.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.netsuite.NetSuiteDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=netsuite\n      ServerName=CDataNetSuite\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.netsuite.jar\n      DriverClass=cdata.jdbc.netsuite.NetSuiteDriver\n      JdbcUrl=jdbc:netsuite:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\netsuite.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/netsuite.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### netsuite_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"netsuite_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### netsuite_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"netsuite_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### netsuite_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"netsuite_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbc",
        "databases",
        "cdatasoftware",
        "cdatasoftware netsuite",
        "cdata jdbc",
        "netsuite data"
      ],
      "category": "databases"
    },
    "CDataSoftware--postgresql-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "postgresql-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/postgresql-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Connects LLMs to PostgreSQL databases for natural language querying of live data without requiring SQL. Provides a read-only interface for retrieving insights from PostgreSQL databases using the CData JDBC Driver.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:13:37Z",
      "readme_content": "# postgresql-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for PostgreSQL\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for PostgreSQL (beta)](https://www.cdata.com/download/download.aspx?sku=FPZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data PostgreSQL supported by the [CData JDBC Driver for PostgreSQL](https://www.cdata.com/drivers/postgresql/jdbc).\n\nCData JDBC Driver connects to PostgreSQL by exposing them as relational SQL models.\n\nThis server wraps that driver and makes PostgreSQL data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/postgresql-mcp-server-by-cdata.git\n      cd postgresql-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/postgresql/download/jdbc](https://www.cdata.com/drivers/postgresql/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for PostgreSQL\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for PostgreSQL/`\n    * Run the command `java -jar cdata.jdbc.postgresql.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.postgresql.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `postgresql.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.postgresql.PostgreSQLDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=postgresql\n      ServerName=CDataPostgreSQL\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.postgresql.jar\n      DriverClass=cdata.jdbc.postgresql.PostgreSQLDriver\n      JdbcUrl=jdbc:postgresql:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\postgresql.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/postgresql.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### postgresql_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"postgresql_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### postgresql_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"postgresql_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### postgresql_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"postgresql_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "jdbc",
        "postgresql",
        "cdatasoftware postgresql",
        "cdata jdbc",
        "postgresql databases"
      ],
      "category": "databases"
    },
    "CDataSoftware--snowflake-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "snowflake-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/snowflake-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Snowflake data through a read-only MCP interface, simplifying data exploration and analysis without the need for SQL. Integrates directly with LLMs to access and retrieve real-time data from Snowflake using a JDBC driver.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-08-06T15:14:30Z",
      "readme_content": "# snowflake-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Snowflake\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Snowflake (beta)](https://www.cdata.com/download/download.aspx?sku=OWZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Snowflake supported by the [CData JDBC Driver for Snowflake](https://www.cdata.com/drivers/snowflake/jdbc).\n\nCData JDBC Driver connects to Snowflake by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Snowflake data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/snowflake-mcp-server-by-cdata.git\n      cd snowflake-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/snowflake/download/jdbc](https://www.cdata.com/drivers/snowflake/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Snowflake\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Snowflake/`\n    * Run the command `java -jar cdata.jdbc.snowflake.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.snowflake.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `snowflake.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.snowflake.SnowflakeDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=snowflake\n      ServerName=CDataSnowflake\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.snowflake.jar\n      DriverClass=cdata.jdbc.snowflake.SnowflakeDriver\n      JdbcUrl=jdbc:snowflake:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\snowflake.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/snowflake.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### snowflake_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"snowflake_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### snowflake_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"snowflake_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### snowflake_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"snowflake_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbc",
        "databases",
        "database",
        "cdatasoftware snowflake",
        "snowflake data",
        "data snowflake"
      ],
      "category": "databases"
    },
    "CDataSoftware--veeva-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "veeva-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/veeva-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Veeva data through a straightforward MCP interface, allowing real-time information retrieval without requiring SQL knowledge. Interfaces with the CData JDBC Driver for easy access to Veeva data models.",
      "stars": 3,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-09-29T18:31:39Z",
      "readme_content": "# veeva-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Veeva\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Veeva (beta)](https://www.cdata.com/download/download.aspx?sku=SVZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Veeva supported by the [CData JDBC Driver for Veeva](https://www.cdata.com/drivers/veeva/jdbc).\n\nCData JDBC Driver connects to Veeva by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Veeva data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/veeva-mcp-server-by-cdata.git\n      cd veeva-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/veeva/download/jdbc](https://www.cdata.com/drivers/veeva/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Veeva\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Veeva/`\n    * Run the command `java -jar cdata.jdbc.veevavault.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.veevavault.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `veeva.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.veevavault.VeevaVaultDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=veevavault\n      ServerName=CDataVeevaVault\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.veevavault.jar\n      DriverClass=cdata.jdbc.veevavault.VeevaVaultDriver\n      JdbcUrl=jdbc:veevavault:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\veeva.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/veeva.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### veeva_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"veeva_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### veeva_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"veeva_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### veeva_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"veeva_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "jdbc",
        "cdatasoftware",
        "cdata jdbc",
        "cdatasoftware veeva",
        "veeva data"
      ],
      "category": "databases"
    },
    "CDataSoftware--workday-mcp-server-by-cdata": {
      "owner": "CDataSoftware",
      "name": "workday-mcp-server-by-cdata",
      "url": "https://github.com/CDataSoftware/workday-mcp-server-by-cdata",
      "imageUrl": "/freedevtools/mcp/pfp/CDataSoftware.webp",
      "description": "Enables natural language querying of live Workday data through a read-only MCP interface, allowing LLMs to retrieve up-to-date information without needing SQL knowledge. Utilizes the CData JDBC Driver to connect and expose Workday data as relational SQL models.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-10-03T22:48:53Z",
      "readme_content": "# workday-mcp-server-by-cdata\nCData's Model Context Protocol (MCP) Server for Workday\n\n:heavy_exclamation_mark: This project builds a read-only MCP server. For full read, write, update, delete, and action capabilities and a simplified setup, check out our free [CData MCP Server for Workday (beta)](https://www.cdata.com/download/download.aspx?sku=JWZM-V&type=beta). \n## Purpose\nWe created this read-only MCP Server to allow LLMs (like Claude Desktop) to query live data Workday supported by the [CData JDBC Driver for Workday](https://www.cdata.com/drivers/workday/jdbc).\n\nCData JDBC Driver connects to Workday by exposing them as relational SQL models.\n\nThis server wraps that driver and makes Workday data available through a simple MCP interface, so LLMs can retrieve live information by asking natural language questions — no SQL required.\n\n## Setup Guide\n1. Clone the repository:\n      ```bash\n      git clone https://github.com/cdatasoftware/workday-mcp-server-by-cdata.git\n      cd workday-mcp-server-by-cdata\n      ```\n2. Build the server:\n      ```bash\n      mvn clean install\n      ``` \n      This creates the JAR file: CDataMCP-jar-with-dependencies.jar\n2. Download and install the CData JDBC Driver for {source}: [https://www.cdata.com/drivers/workday/download/jdbc](https://www.cdata.com/drivers/workday/download/jdbc)\n3. License the CData JDBC Driver:\n    * Navigate to the `lib` folder in the installation directory, typically:\n        * (Windows) `C:\\Program Files\\CData\\CData JDBC Driver for Workday\\`\n        * (Mac/Linux) `/Applications/CData JDBC Driver for Workday/`\n    * Run the command `java -jar cdata.jdbc.workday.jar --license`\n    * Enter your name, email, and \"TRIAL\" (or your license key).\n4. Configure your connection to the data source (Salesforce as an example):\n    * Run the command `java -jar cdata.jdbc.workday.jar` to open the Connection String utility.\n      \n      \n    * Configure the connection string and click \"Test Connection\"\n      > **Note:** If the data sources uses OAuth, you will need to authenticate in your browser.\n    * Once successful, copy the connection string for use later.\n5. Create a `.prp` file for your JDBC connection (e.g. `workday.prp`) using the following properties and format:\n    * **Prefix** - a prefix to be used for the tools exposed\n    * **ServerName** - a name for your server\n    * **ServerVersion** - a version for your server\n    * **DriverPath** - the full path to the JAR file for your JDBC driver\n    * **DriverClass** - the name of the JDBC Driver Class (e.g. cdata.jdbc.workday.WorkdayDriver)\n    * **JdbcUrl** - the JDBC connection string to use with the CData JDBC Driver to connect to your data (copied from above)\n    * **Tables** - leave blank to access all data, otherwise you can explicitly declare the tables you wish to create access for\n      ```env\n      Prefix=workday\n      ServerName=CDataWorkday\n      ServerVersion=1.0\n      DriverPath=PATH\\TO\\cdata.jdbc.workday.jar\n      DriverClass=cdata.jdbc.workday.WorkdayDriver\n      JdbcUrl=jdbc:workday:InitiateOAuth=GETANDREFRESH;\n      Tables=\n      ```\n\n## Using the Server with Claude Desktop\n1. Create the config file for Claude Desktop ( claude_desktop_config.json) to add the new MCP server, using the format below. If the file already exists, add the entry to the `mcpServers` in the config file.\n\n      **Windows**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"PATH\\\\TO\\\\java.exe\",\n            \"args\": [\n              \"-jar\",\n              \"PATH\\\\TO\\\\CDataMCP-jar-with-dependencies.jar\",\n              \"PATH\\\\TO\\\\workday.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      \n      **Linux/Mac**\n      ```json\n      {\n        \"mcpServers\": {\n          \"{classname_dash}\": {\n            \"command\": \"/PATH/TO/java\",\n            \"args\": [\n              \"-jar\",\n              \"/PATH/TO/CDataMCP-jar-with-dependencies.jar\",\n              \"/PATH/TO/workday.prp\"\n            ]\n          },\n          ...\n        }\n      }\n      ```\n      If needed, copy the config file to the appropriate directory (Claude Desktop as the example).\n      **Windows**\n      ```bash\n      cp C:\\PATH\\TO\\claude_desktop_config.json %APPDATA%\\Claude\\claude_desktop_config.json\n      ```\n      **Linux/Mac**\n      ```bash\n      cp /PATH/TO/claude_desktop_config.json /Users/{user}/Library/Application\\ Support/Claude/claude_desktop_config.json'\n      ```\n2. Run or refresh your client (Claude Desktop).\n   \n> **Note:** You may need to fully exit or quit your Claude Desktop client and re-open it for the MCP Servers to appear.\n\n## Running the Server\n1. Run the follow the command to run the MCP Server on its own\n      ```bash\n      java -jar /PATH/TO/CDataMCP-jar-with-dependencies.jar /PATH/TO/Salesforce.prp\n> **Note:** The server uses `stdio` so can only be used with clients that run on the same machine as the server.\n## Usage Details\nOnce the MCP Server is configured, the AI client will be able to use the built-in tools to read, write, update, and delete the underlying data. In general, you do not need to call the tools explicitly. Simply ask the client to answer questions about the underlying data system. For example:\n* \"What is the correlation between my closed won opportunities and the account industry?\"\n* \"How many open tickets do I have in the SUPPORT project?\"\n* \"Can you tell me what calendar events I have today?\"\n\nThe list of tools available and their descriptions follow:\n### Tools & Descriptions\nIn the definitions below, `{servername}` refers to the name of the MCP Server in the config file (e.g. `{classname_dash}` above).\n* `{servername}_get_tables` - Retrieves a list of tables available in the data source. Use the `{servername}_get_columns` tool to list available columns on a table. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_get_columns` - Retrieves a list of columns for a table. Use the `{servername}_get_tables` tool to get a list of available tables. The output of the tool will be returned in CSV format, with the first line containing column headers.\n* `{servername}_run_query` - Execute a SQL SELECT query\n\n## JSON-RPC Request Examples\nIf you are scripting out the requests sent to the MCP Server instead of using an AI Client (e.g. Claude), then you can refer to the JSON payload examples below – following the JSON-RPC 2.0 specification - when calling the available tools. \n\n#### workday_get_tables\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"workday_get_tables\",\n        \"arguments\": {}\n    }\n}\n```\n\n#### workday_get_columns\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"workday_get_columns\",\n        \"arguments\": {\n            \"table\":  \"Account\"\n        }\n    }\n}\n```\n\n#### workday_run_query\n```json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n        \"name\": \"workday_run_query\",\n        \"arguments\": {\n            \"sql\":  \"SELECT * FROM [Account] WHERE [IsDeleted] = true\"\n        }\n    }\n}\n```\n\n## Troubleshooting\n1. If you cannot see your CData MCP Server in Claude Desktop, be sure that you have fully quit Claude Desktop (Windows: use the Task Manager, Mac: use the Activity Monitor)\n2. If Claude Desktop is unable to retrieve data, be sure that you have configured your connection properly. Use the Connection String builder to create the connection string (see above) and copy the connection string into the property (.prp) file.\n3. If you are having trouble connecting to your data source, contact the [CData Support Team](https://www.cdata.com/support/submit.aspx).\n4. If you are having trouble using the MCP server, or have any other feedback, join the [CData Community](https://community.cdata.com).\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [LICENSE](./LICENSE) file in the project repository.\n\n## All Supported Sources\n<table>\n<tr><td>Access</td><td>Act CRM</td><td>Act-On</td><td>Active Directory</td></tr>\n<tr><td>ActiveCampaign</td><td>Acumatica</td><td>Adobe Analytics</td><td>Adobe Commerce</td></tr>\n<tr><td>ADP</td><td>Airtable</td><td>AlloyDB</td><td>Amazon Athena</td></tr>\n<tr><td>Amazon DynamoDB</td><td>Amazon Marketplace</td><td>Amazon S3</td><td>Asana</td></tr>\n<tr><td>Authorize.Net</td><td>Avalara AvaTax</td><td>Avro</td><td>Azure Active Directory</td></tr>\n<tr><td>Azure Analysis Services</td><td>Azure Data Catalog</td><td>Azure Data Lake Storage</td><td>Azure DevOps</td></tr>\n<tr><td>Azure Synapse</td><td>Azure Table</td><td>Basecamp</td><td>BigCommerce</td></tr>\n<tr><td>BigQuery</td><td>Bing Ads</td><td>Bing Search</td><td>Bitbucket</td></tr>\n<tr><td>Blackbaud FE NXT</td><td>Box</td><td>Bullhorn CRM</td><td>Cassandra</td></tr>\n<tr><td>Certinia</td><td>Cloudant</td><td>CockroachDB</td><td>Confluence</td></tr>\n<tr><td>Cosmos DB</td><td>Couchbase</td><td>CouchDB</td><td>CSV</td></tr>\n<tr><td>Cvent</td><td>Databricks</td><td>DB2</td><td>DocuSign</td></tr>\n<tr><td>Dropbox</td><td>Dynamics 365</td><td>Dynamics 365 Business Central</td><td>Dynamics CRM</td></tr>\n<tr><td>Dynamics GP</td><td>Dynamics NAV</td><td>eBay</td><td>eBay Analytics</td></tr>\n<tr><td>Elasticsearch</td><td>Email</td><td>EnterpriseDB</td><td>Epicor Kinetic</td></tr>\n<tr><td>Exact Online</td><td>Excel</td><td>Excel Online</td><td>Facebook</td></tr>\n<tr><td>Facebook Ads</td><td>FHIR</td><td>Freshdesk</td><td>FTP</td></tr>\n<tr><td>GitHub</td><td>Gmail</td><td>Google Ad Manager</td><td>Google Ads</td></tr>\n<tr><td>Google Analytics</td><td>Google Calendar</td><td>Google Campaign Manager 360</td><td>Google Cloud Storage</td></tr>\n<tr><td>Google Contacts</td><td>Google Data Catalog</td><td>Google Directory</td><td>Google Drive</td></tr>\n<tr><td>Google Search</td><td>Google Sheets</td><td>Google Spanner</td><td>GraphQL</td></tr>\n<tr><td>Greenhouse</td><td>Greenplum</td><td>HarperDB</td><td>HBase</td></tr>\n<tr><td>HCL Domino</td><td>HDFS</td><td>Highrise</td><td>Hive</td></tr>\n<tr><td>HubDB</td><td>HubSpot</td><td>IBM Cloud Data Engine</td><td>IBM Cloud Object Storage</td></tr>\n<tr><td>IBM Informix</td><td>Impala</td><td>Instagram</td><td>JDBC-ODBC Bridge</td></tr>\n<tr><td>Jira</td><td>Jira Assets</td><td>Jira Service Management</td><td>JSON</td></tr>\n<tr><td>Kafka</td><td>Kintone</td><td>LDAP</td><td>LinkedIn</td></tr>\n<tr><td>LinkedIn Ads</td><td>MailChimp</td><td>MariaDB</td><td>Marketo</td></tr>\n<tr><td>MarkLogic</td><td>Microsoft Dataverse</td><td>Microsoft Entra ID</td><td>Microsoft Exchange</td></tr>\n<tr><td>Microsoft OneDrive</td><td>Microsoft Planner</td><td>Microsoft Project</td><td>Microsoft Teams</td></tr>\n<tr><td>Monday.com</td><td>MongoDB</td><td>MYOB AccountRight</td><td>MySQL</td></tr>\n<tr><td>nCino</td><td>Neo4J</td><td>NetSuite</td><td>OData</td></tr>\n<tr><td>Odoo</td><td>Office 365</td><td>Okta</td><td>OneNote</td></tr>\n<tr><td>Oracle</td><td>Oracle Eloqua</td><td>Oracle Financials Cloud</td><td>Oracle HCM Cloud</td></tr>\n<tr><td>Oracle Sales</td><td>Oracle SCM</td><td>Oracle Service Cloud</td><td>Outreach.io</td></tr>\n<tr><td>Parquet</td><td>Paylocity</td><td>PayPal</td><td>Phoenix</td></tr>\n<tr><td>PingOne</td><td>Pinterest</td><td>Pipedrive</td><td>PostgreSQL</td></tr>\n<tr><td>Power BI XMLA</td><td>Presto</td><td>Quickbase</td><td>QuickBooks</td></tr>\n<tr><td>QuickBooks Online</td><td>QuickBooks Time</td><td>Raisers Edge NXT</td><td>Reckon</td></tr>\n<tr><td>Reckon Accounts Hosted</td><td>Redis</td><td>Redshift</td><td>REST</td></tr>\n<tr><td>RSS</td><td>Sage 200</td><td>Sage 300</td><td>Sage 50 UK</td></tr>\n<tr><td>Sage Cloud Accounting</td><td>Sage Intacct</td><td>Salesforce</td><td>Salesforce Data Cloud</td></tr>\n<tr><td>Salesforce Financial Service Cloud</td><td>Salesforce Marketing</td><td>Salesforce Marketing Cloud Account Engagement</td><td>Salesforce Pardot</td></tr>\n<tr><td>Salesloft</td><td>SAP</td><td>SAP Ariba Procurement</td><td>SAP Ariba Source</td></tr>\n<tr><td>SAP Business One</td><td>SAP BusinessObjects BI</td><td>SAP ByDesign</td><td>SAP Concur</td></tr>\n<tr><td>SAP Fieldglass</td><td>SAP HANA</td><td>SAP HANA XS Advanced</td><td>SAP Hybris C4C</td></tr>\n<tr><td>SAP Netweaver Gateway</td><td>SAP SuccessFactors</td><td>SAS Data Sets</td><td>SAS xpt</td></tr>\n<tr><td>SendGrid</td><td>ServiceNow</td><td>SFTP</td><td>SharePoint</td></tr>\n<tr><td>SharePoint Excel Services</td><td>ShipStation</td><td>Shopify</td><td>SingleStore</td></tr>\n<tr><td>Slack</td><td>Smartsheet</td><td>Snapchat Ads</td><td>Snowflake</td></tr>\n<tr><td>Spark</td><td>Splunk</td><td>SQL Analysis Services</td><td>SQL Server</td></tr>\n<tr><td>Square</td><td>Stripe</td><td>Sugar CRM</td><td>SuiteCRM</td></tr>\n<tr><td>SurveyMonkey</td><td>Sybase</td><td>Sybase IQ</td><td>Tableau CRM Analytics</td></tr>\n<tr><td>Tally</td><td>TaxJar</td><td>Teradata</td><td>Tier1</td></tr>\n<tr><td>TigerGraph</td><td>Trello</td><td>Trino</td><td>Twilio</td></tr>\n<tr><td>Twitter</td><td>Twitter Ads</td><td>Veeva CRM</td><td>Veeva Vault</td></tr>\n<tr><td>Wave Financial</td><td>WooCommerce</td><td>WordPress</td><td>Workday</td></tr>\n<tr><td>xBase</td><td>Xero</td><td>XML</td><td>YouTube Analytics</td></tr>\n<tr><td>Zendesk</td><td>Zoho Books</td><td>Zoho Creator</td><td>Zoho CRM</td></tr>\n<tr><td>Zoho Inventory</td><td>Zoho Projects</td><td>Zuora</td><td>... Dozens More</td></tr>\n</table>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "jdbc",
        "database",
        "cdata jdbc",
        "cdatasoftware workday",
        "utilizes cdata"
      ],
      "category": "databases"
    },
    "Canner--wren-engine": {
      "owner": "Canner",
      "name": "wren-engine",
      "url": "https://github.com/Canner/wren-engine",
      "imageUrl": "",
      "description": "The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents",
      "stars": 451,
      "forks": 126,
      "license": "Apache License 2.0",
      "language": "Java",
      "updated_at": "2025-10-03T22:27:23Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://getwren.ai\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"./misc/wrenai_logo.png\">\n      \n    </picture>\n    <h1 align=\"center\">Wren Engine</h1>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a aria-label=\"Follow us\" href=\"https://x.com/getwrenai\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-@getwrenai-blue?style=for-the-badge&logo=x&logoColor=white&labelColor=gray&logoWidth=20\">\n  </a>\n  <a aria-label=\"License\" href=\"https://github.com/Canner/wren-engine/blob/main/LICENSE\">\n    <img alt=\"\" src=\"https://img.shields.io/github/license/canner/wren-engine?color=blue&style=for-the-badge\">\n  </a>\n  <a aria-label=\"Join the community on GitHub\" href=\"https://discord.gg/5DvshJqG8Z\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-JOIN%20THE%20COMMUNITY-blue?style=for-the-badge&logo=discord&logoColor=white&labelColor=grey&logoWidth=20\">\n  </a>\n  <a aria-label=\"Canner\" href=\"https://cannerdata.com/\">\n    <img alt=\"Made_by_Canner_blue_style_for_the_badge\" src=\"https://img.shields.io/badge/%F0%9F%A7%A1-Made%20by%20Canner-blue?style=for-the-badge\">\n  </a>\n</p>\n\n> Wren Engine is the Semantic Engine for MCP Clients and AI Agents. \n> [Wren AI](https://github.com/Canner/WrenAI) GenBI AI Agent is based on Wren Engine.\n\n\n\n## 🔌 Supported Data Sources\n- [BigQuery](https://docs.getwren.ai/oss/wren_engine_api#tag/BigQueryConnectionInfo)\n- [Google Cloud Storage](https://docs.getwren.ai/oss/wren_engine_api#tag/GcsFileConnectionInfo)\n- [Local Files](https://docs.getwren.ai/oss/wren_engine_api#tag/LocalFileConnectionInfo)\n- [MS SQL Server](https://docs.getwren.ai/oss/wren_engine_api#tag/MSSqlConnectionInfo)\n- [Minio](https://docs.getwren.ai/oss/wren_engine_api#tag/MinioFileConnectionInfo)\n- [MySQL Server](https://docs.getwren.ai/oss/wren_engine_api#tag/MySqlConnectionInfo)\n- [Oracle Server](https://docs.getwren.ai/oss/wren_engine_api#tag/OracleConnectionInfo)\n- [PostgreSQL Server](https://docs.getwren.ai/oss/wren_engine_api#tag/PostgresConnectionInfo)\n- [Amazon S3](https://docs.getwren.ai/oss/wren_engine_api#tag/S3FileConnectionInfo)\n- [Snowflake](https://docs.getwren.ai/oss/wren_engine_api#tag/SnowflakeConnectionInfo)\n- [Trino](https://docs.getwren.ai/oss/wren_engine_api#tag/TrinoConnectionInfo)\n\n## 😫 Challenge Today\n\nAt the enterprise level, the stakes - and the complexity - are much higher. Businesses run on structured data stored in cloud warehouses, relational databases, and secure filesystems. From BI dashboards to CRM updates and compliance workflows, AI must not only execute commands but also **understand and retrieve the right data, with precision and in context**.\n\nWhile many community and official MCP servers already support connections to major databases like PostgreSQL, MySQL, SQL Server, and more, there's a problem: **raw access to data isn't enough**.\n\nEnterprises need:\n- Accurate semantic understanding of their data models\n- Trusted calculations and aggregations in reporting\n- Clarity on business terms, like \"active customer,\" \"net revenue,\" or \"churn rate\"\n- User-based permissions and access control\n\n<p align=\"center\">\n  <img width=\"920\" height=\"638\" alt=\"without_wren_engine\" src=\"https://github.com/user-attachments/assets/3295dde5-ce41-4e56-a8ad-daff6a0c3459\" />\n</p>\n\nNatural language alone isn't enough to drive complex workflows across enterprise data systems. You need a layer that interprets intent, maps it to the correct data, applies calculations accurately, and ensures security.\n\n## 🎯 Our Mission\n\nWren Engine is on a mission to power the future of MCP clients and AI agents through the Model Context Protocol (MCP) — a new open standard that connects LLMs with tools, databases, and enterprise systems.\n\nAs part of the MCP ecosystem, Wren Engine provides a **semantic engine** powered the next generation semantic layer that enables AI agents to access business data with accuracy, context, and governance. \n\nBy building the semantic layer directly into MCP clients, such as Claude, Cline, Cursor, etc. Wren Engine empowers AI Agents with precise business context and ensures accurate data interactions across diverse enterprise environments.\n\nWe believe the future of enterprise AI lies in **context-aware, composable systems**. That’s why Wren Engine is designed to be:\n\n- 🔌 **Embeddable** into any MCP client or AI agentic workflow\n- 🔄 **Interoperable** with modern data stacks (PostgreSQL, MySQL, Snowflake, etc.)\n- 🧠 **Semantic-first**, enabling AI to “understand” your data model and business logic\n- 🔐 **Governance-ready**, respecting roles, access controls, and definitions\n\n<p align=\"center\">\n  <img width=\"1267\" height=\"705\" alt=\"with_wren_engine\" src=\"https://github.com/user-attachments/assets/3a6531fe-4731-4f21-ae9a-786b219f3c0e\" />\n</p>\n\nWith Wren Engine, you can scale AI adoption across teams — not just with better automation, but with better understanding.\n\n***Check our full article***\n\n🤩 [Our Mission - Fueling the Next Wave of AI Agents: Building the Foundation for Future MCP Clients and Enterprise Data Access](https://getwren.ai/post/fueling-the-next-wave-of-ai-agents-building-the-foundation-for-future-mcp-clients-and-enterprise-data-access)\n\n## 🚀 Get Started with MCP \n[MCP Server README](mcp-server/README.md)\n\nhttps://github.com/user-attachments/assets/dab9b50f-70d7-4eb3-8fc8-2ab55dc7d2ec\n\n\n👉 Blog Post Tutorial: [Powering AI-driven workflows with Wren Engine and Zapier via the Model Context Protocol (MCP)](https://getwren.ai/post/powering-ai-driven-workflows-with-wren-engine-and-zapier-via-the-model-context-protocol-mcp?utm_campaign=10904457-MCP&utm_content=330804773&utm_medium=social&utm_source=linkedin&hss_channel=lcp-89794921)\n\n## 🤔 Concepts\n\n- [Powering Semantic SQL for AI Agents with Apache DataFusion](https://getwren.ai/post/powering-semantic-sql-for-ai-agents-with-apache-datafusion)\n- [Quick start with Wren Engine](https://docs.getwren.ai/oss/engine/get_started/quickstart)\n- [What is semantics?](https://docs.getwren.ai/oss/engine/concept/what_is_semantics)\n- [What is Modeling Definition Language (MDL)?](https://docs.getwren.ai/oss/engine/concept/what_is_mdl)\n- [Benefits of Wren Engine with LLMs](https://docs.getwren.ai/oss/engine/concept/benefits_llm)\n\n## 🚧 Project Status\nWren Engine is currently in the beta version. The project team is actively working on progress and aiming to release new versions at least biweekly.\n\n## 🛠️ Developer Guides\nThe project consists of 4 main modules:\n1. [ibis-server](./ibis-server/): the Web server of Wren Engine powered by FastAPI and Ibis\n2. [wren-core](./wren-core): the semantic core written in Rust powered by [Apache DataFusion](https://github.com/apache/datafusion)\n3. [wren-core-py](./wren-core-py): the Python binding for wren-core\n4. [mcp-server](./mcp-server/): the MCP server of Wren Engine powered by [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n\n## ⭐️ Community\n\n- Welcome to our [Discord server](https://discord.gg/5DvshJqG8Z) to give us feedback!\n- If there is any issues, please visit [Github Issues](https://github.com/Canner/wren-engine/issues).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "access schema",
        "semantic engine",
        "secure database"
      ],
      "category": "databases"
    },
    "Cappahccino--SB-MCP": {
      "owner": "Cappahccino",
      "name": "SB-MCP",
      "url": "https://github.com/Cappahccino/SB-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/Cappahccino.webp",
      "description": "Seamlessly interact with Supabase to perform CRUD operations on Postgres tables, optimizing database management tasks and enabling real-time data access for applications.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-27T09:04:02Z",
      "readme_content": "# Supabase MCP Server\n\nA Model Context Protocol (MCP) server that allows Claude and other LLMs to interact with Supabase to perform CRUD operations on Postgres tables.\n\n## Features\n\n- Database operations:\n  - Query data with filters\n  - Insert data\n  - Update data\n  - Delete data\n  - List tables\n\n## Prerequisites\n\n- Node.js (v16 or newer)\n- npm or yarn\n- Supabase project with API keys\n\n## Installation\n\n### Option 1: Install from npm (recommended)\n\nThe package is published on npm! You can install it globally with:\n\n```bash\nnpm install -g supabase-mcp\n```\n\nOr locally in your project:\n\n```bash\nnpm install supabase-mcp\n```\n\n### Option 2: Clone the repository\n\n```bash\ngit clone https://github.com/Cappahccino/SB-MCP.git\ncd SB-MCP\nnpm install\nnpm run build\n```\n\n## Configuration\n\nCreate a `.env` file with your Supabase credentials:\n\n```\n# Supabase credentials\nSUPABASE_URL=your_supabase_project_url\nSUPABASE_ANON_KEY=your_supabase_anon_key\nSUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key\n\n# MCP server configuration\nMCP_SERVER_PORT=3000\nMCP_SERVER_HOST=localhost\nMCP_API_KEY=your_secret_api_key\n```\n\n## Usage with Claude\n\nClaude requires a specific transport mode for compatibility. This package provides a dedicated binary for Claude integration:\n\n### In Claude Desktop MCP Config\n\n```json\n\"supabase\": {\n  \"command\": \"npx\",\n  \"args\": [\n    \"-y\",\n    \"supabase-mcp@latest\",\n    \"supabase-mcp-claude\"\n  ],\n  \"env\": {\n    \"SUPABASE_URL\": \"your_supabase_project_url\",\n    \"SUPABASE_ANON_KEY\": \"your_supabase_anon_key\", \n    \"SUPABASE_SERVICE_ROLE_KEY\": \"your_service_role_key\",\n    \"MCP_API_KEY\": \"your_secret_api_key\"\n  }\n}\n```\n\nMake sure you set the required environment variables in the configuration. Claude will use the stdio transport for communication.\n\n### Manual Testing with Claude Binary\n\nFor testing outside of Claude, you can run:\n\n```bash\nnpm run start:claude\n```\n\nOr if installed globally:\n\n```bash\nsupabase-mcp-claude\n```\n\n## Usage as a Standalone Server\n\nAfter installing globally:\n\n```bash\nsupabase-mcp\n```\n\nThis will start the MCP server at http://localhost:3000 (or the port specified in your .env file).\n\n## Usage in Your Code\n\nYou can also use supabase-mcp as a library in your own Node.js projects:\n\n```javascript\nimport { createServer, mcpConfig, validateConfig } from 'supabase-mcp';\n\n// Validate configuration\nvalidateConfig();\n\n// Create the server\nconst app = createServer();\n\n// Start the server\napp.listen(mcpConfig.port, mcpConfig.host, () => {\n  console.log(`Supabase MCP server running at http://${mcpConfig.host}:${mcpConfig.port}`);\n});\n```\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n#### 1. \"Port XXXX is already in use\"\nThe HTTP server attempts to find an available port automatically. You can manually specify a different port in your `.env` file by changing the `MCP_SERVER_PORT` value.\n\n#### 2. \"Missing required environment variables\"\nMake sure you have a proper `.env` file with all the required values or that you've set the environment variables in your system.\n\n#### 3. \"TypeError: Class constructor Server cannot be invoked without 'new'\"\nIf you see this error, you may be running an older version of the package. Update to the latest version:\n```bash\nnpm install -g supabase-mcp@latest\n```\n\n#### 4. JSON parsing errors with Claude\nMake sure you're using the Claude-specific binary (`supabase-mcp-claude`) instead of the regular HTTP server (`supabase-mcp`).\n\n#### 5. Request timed out with Claude\nThis usually means Claude initiated the connection but the server was unable to respond in time. Check:\n- Are your Supabase credentials correct?\n- Is your server setup properly and running?\n- Is there anything blocking the connection?\n\n## Tools Reference\n\n### Database Tools\n\n1. **queryDatabase**\n   - Parameters:\n     - `table` (string): Name of the table to query\n     - `select` (string, optional): Comma-separated list of columns (default: \"*\")\n     - `query` (object, optional): Filter conditions\n\n2. **insertData**\n   - Parameters:\n     - `table` (string): Name of the table\n     - `data` (object or array of objects): Data to insert\n\n3. **updateData**\n   - Parameters:\n     - `table` (string): Name of the table\n     - `data` (object): Data to update as key-value pairs\n     - `query` (object): Filter conditions for the update\n\n4. **deleteData**\n   - Parameters:\n     - `table` (string): Name of the table\n     - `query` (object): Filter conditions for deletion\n\n5. **listTables**\n   - Parameters: None\n\n## Version History\n\n- 1.0.0: Initial release\n- 1.0.1: Added automatic port selection\n- 1.0.2: Fixed protocol compatibility issues\n- 1.0.3: Added JSON-RPC support\n- 1.1.0: Complete rewrite using official MCP SDK\n- 1.2.0: Added separate Claude transport and fixed port conflict issues\n- 1.3.0: Updated for improved compatibility with TypeScript projects\n- 1.4.0: Fixed Claude stdio transport integration based on Supabase community best practices\n- 1.5.0: Removed Edge Function support to improve stability and focus on database operations\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "access cappahccino"
      ],
      "category": "databases"
    },
    "Chakra-Network--mcp-server": {
      "owner": "Chakra-Network",
      "name": "mcp-server",
      "url": "https://github.com/Chakra-Network/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Chakra-Network.webp",
      "description": "Interact with databases using natural language queries, enabling the creation, updating, and deletion of tables. Manage real-time data insights through subscribed data shares.",
      "stars": 12,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-20T19:13:20Z",
      "readme_content": "# Chakra MCP Server\n\n[![PyPI version](https://badge.fury.io/py/chakra-mcp.svg)](https://badge.fury.io/py/chakra-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Python versions](https://img.shields.io/pypi/pyversions/chakra-mcp.svg)](https://pypi.org/project/chakra-mcp/)\n\n![mcp](https://github.com/user-attachments/assets/2c9e2b54-2691-43c7-928b-bd6e33cc5f73)\n\n\nA native integration with Anthropic's [Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol). Allows you to interact with your database and subscribed data shares using natural language.\n\n## Features\n- **Natural Language Queries**: Query your database using natural language.\n- **Data Share Interactions**: Interact with subscribed data shares. For example, if you have subscribed to a financial data share, you can ask questions like \"What is the stock price of Tesla?\"\n- **Database Management**: Create, update, and delete tables.\n\n## Demo\nhttps://github.com/user-attachments/assets/0d1b3588-4dec-4fae-8396-d1794177a23c\n\n## Prerequisites\n- Python 3.11+\n- [uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods). On MacOS, you can install it using Homebrew: `brew install uv`.\n- Claude Desktop\n- Chakra Account - sign up [here](https://console.chakra.dev/)\n\n## Finding your DB Session Key\n\n1. Login to the [Chakra Console](https://console.chakra.dev/)\n2. Select Settings\n3. Navigate to the releveant database and copy the DB Session Key (not the access key or secret access key)\n\nhttps://github.com/user-attachments/assets/9f1c1ab8-cb87-42a1-8627-184617bbb7d7\n\n## Installation\n\n### Automated Using OpenTools (Easier)\n\nInstall [OpenTools](https://opentools.com/docs/registry/quickstart#prerequisites) prerequisites. \n\nThen run:\n```bash\nnpx opentools@latest i chakra\n```\n\n\n### Manual Setup (More Work)\n\nAdd the following to your `claude_desktop_config.json` file:\n- On MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"chakra\": {\n      \"command\": \"uvx\",\n      \"args\": [\"chakra-mcp\"],\n      \"env\": {\n        \"db_session_key\": \"YOUR_DB_SESSION_KEY\"\n      }\n    }\n  }\n}\n\n```\n\n## Architecture\n\n<img width=\"1004\" alt=\"architecture\" src=\"https://github.com/user-attachments/assets/0984e717-afc5-4599-b2c0-eefa33d40441\" />\n\n## Disclaimers \n\n- MCP is extremely early. The experience in Claude Desktop is suboptimal - every time you use the server, you have to grant access explicitly. This is a design decision on Anthropic's part and is not yet configurable.\n- Setup is rough around the edges. We have worked closely with the folks at OpenTools to make this as seamless as possible, but there is room for improvement. We are looking forward to an MCP GUI experience in the future, but for now, users must use the command-line. \n- Today, the server runs on the user's local machine. Anthropic's roadmap includes a [hosted server option](https://modelcontextprotocol.io/development/roadmap#remote-mcp-support), which we will support. This will make authentication, setup, and performance much better. \n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Support\n\nFor support and questions, please open an issue in the GitHub repository or reach out to us on [Discord](https://discord.gg/chakra-ai).\n\n## Contributing\n\nCreating a new build:\n\n```bash\nuv build\n```\n\nPublishing a new version:\n\n```bash\nuv publish\n```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "chakra",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "ClickHouse--mcp-clickhouse": {
      "owner": "ClickHouse",
      "name": "mcp-clickhouse",
      "url": "https://github.com/ClickHouse/mcp-clickhouse",
      "imageUrl": "",
      "description": "ClickHouse database integration with schema inspection and query capabilities",
      "stars": 550,
      "forks": 111,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T15:15:03Z",
      "readme_content": "# ClickHouse MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-clickhouse)](https://pypi.org/project/mcp-clickhouse)\n\nAn MCP server for ClickHouse.\n\n<a href=\"https://glama.ai/mcp/servers/yvjy4csvo1\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/yvjy4csvo1/badge\" alt=\"mcp-clickhouse MCP server\" /></a>\n\n## Features\n\n### ClickHouse Tools\n\n* `run_select_query`\n  * Execute SQL queries on your ClickHouse cluster.\n  * Input: `sql` (string): The SQL query to execute.\n  * All ClickHouse queries are run with `readonly = 1` to ensure they are safe.\n\n* `list_databases`\n  * List all databases on your ClickHouse cluster.\n\n* `list_tables`\n  * List all tables in a database.\n  * Input: `database` (string): The name of the database.\n\n### chDB Tools\n\n* `run_chdb_select_query`\n  * Execute SQL queries using [chDB](https://github.com/chdb-io/chdb)'s embedded ClickHouse engine.\n  * Input: `sql` (string): The SQL query to execute.\n  * Query data directly from various sources (files, URLs, databases) without ETL processes.\n\n### Health Check Endpoint\n\nWhen running with HTTP or SSE transport, a health check endpoint is available at `/health`. This endpoint:\n- Returns `200 OK` with the ClickHouse version if the server is healthy and can connect to ClickHouse\n- Returns `503 Service Unavailable` if the server cannot connect to ClickHouse\n\nExample:\n```bash\ncurl http://localhost:8000/health\n# Response: OK - Connected to ClickHouse 24.3.1\n```\n\n## Configuration\n\nThis MCP server supports both ClickHouse and chDB. You can enable either or both depending on your needs.\n\n1. Open the Claude Desktop configuration file located at:\n   * On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   * On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-clickhouse\",\n        \"--python\",\n        \"3.10\",\n        \"mcp-clickhouse\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"<clickhouse-host>\",\n        \"CLICKHOUSE_PORT\": \"<clickhouse-port>\",\n        \"CLICKHOUSE_USER\": \"<clickhouse-user>\",\n        \"CLICKHOUSE_PASSWORD\": \"<clickhouse-password>\",\n        \"CLICKHOUSE_SECURE\": \"true\",\n        \"CLICKHOUSE_VERIFY\": \"true\",\n        \"CLICKHOUSE_CONNECT_TIMEOUT\": \"30\",\n        \"CLICKHOUSE_SEND_RECEIVE_TIMEOUT\": \"30\"\n      }\n    }\n  }\n}\n```\n\nUpdate the environment variables to point to your own ClickHouse service.\n\nOr, if you'd like to try it out with the [ClickHouse SQL Playground](https://sql.clickhouse.com/), you can use the following config:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-clickhouse\",\n        \"--python\",\n        \"3.10\",\n        \"mcp-clickhouse\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"sql-clickhouse.clickhouse.com\",\n        \"CLICKHOUSE_PORT\": \"8443\",\n        \"CLICKHOUSE_USER\": \"demo\",\n        \"CLICKHOUSE_PASSWORD\": \"\",\n        \"CLICKHOUSE_SECURE\": \"true\",\n        \"CLICKHOUSE_VERIFY\": \"true\",\n        \"CLICKHOUSE_CONNECT_TIMEOUT\": \"30\",\n        \"CLICKHOUSE_SEND_RECEIVE_TIMEOUT\": \"30\"\n      }\n    }\n  }\n}\n```\n\nFor chDB (embedded ClickHouse engine), add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-clickhouse\",\n        \"--python\",\n        \"3.10\",\n        \"mcp-clickhouse\"\n      ],\n      \"env\": {\n        \"CHDB_ENABLED\": \"true\",\n        \"CLICKHOUSE_ENABLED\": \"false\",\n        \"CHDB_DATA_PATH\": \"/path/to/chdb/data\"\n      }\n    }\n  }\n}\n```\n\nYou can also enable both ClickHouse and chDB simultaneously:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-clickhouse\",\n        \"--python\",\n        \"3.10\",\n        \"mcp-clickhouse\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"<clickhouse-host>\",\n        \"CLICKHOUSE_PORT\": \"<clickhouse-port>\",\n        \"CLICKHOUSE_USER\": \"<clickhouse-user>\",\n        \"CLICKHOUSE_PASSWORD\": \"<clickhouse-password>\",\n        \"CLICKHOUSE_SECURE\": \"true\",\n        \"CLICKHOUSE_VERIFY\": \"true\",\n        \"CLICKHOUSE_CONNECT_TIMEOUT\": \"30\",\n        \"CLICKHOUSE_SEND_RECEIVE_TIMEOUT\": \"30\",\n        \"CHDB_ENABLED\": \"true\",\n        \"CHDB_DATA_PATH\": \"/path/to/chdb/data\"\n      }\n    }\n  }\n}\n```\n\n3. Locate the command entry for `uv` and replace it with the absolute path to the `uv` executable. This ensures that the correct version of `uv` is used when starting the server. On a mac, you can find this path using `which uv`.\n\n4. Restart Claude Desktop to apply the changes.\n\n### Running Without uv (Using System Python)\n\nIf you prefer to use the system Python installation instead of uv, you can install the package from PyPI and run it directly:\n\n1. Install the package using pip:\n   ```bash\n   python3 -m pip install mcp-clickhouse\n   ```\n\n   To upgrade to the latest version:\n   ```bash\n   python3 -m pip install --upgrade mcp-clickhouse\n   ```\n\n2. Update your Claude Desktop configuration to use Python directly:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\",\n        \"mcp_clickhouse.main\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"<clickhouse-host>\",\n        \"CLICKHOUSE_PORT\": \"<clickhouse-port>\",\n        \"CLICKHOUSE_USER\": \"<clickhouse-user>\",\n        \"CLICKHOUSE_PASSWORD\": \"<clickhouse-password>\",\n        \"CLICKHOUSE_SECURE\": \"true\",\n        \"CLICKHOUSE_VERIFY\": \"true\",\n        \"CLICKHOUSE_CONNECT_TIMEOUT\": \"30\",\n        \"CLICKHOUSE_SEND_RECEIVE_TIMEOUT\": \"30\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use the installed script directly:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"mcp-clickhouse\",\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"<clickhouse-host>\",\n        \"CLICKHOUSE_PORT\": \"<clickhouse-port>\",\n        \"CLICKHOUSE_USER\": \"<clickhouse-user>\",\n        \"CLICKHOUSE_PASSWORD\": \"<clickhouse-password>\",\n        \"CLICKHOUSE_SECURE\": \"true\",\n        \"CLICKHOUSE_VERIFY\": \"true\",\n        \"CLICKHOUSE_CONNECT_TIMEOUT\": \"30\",\n        \"CLICKHOUSE_SEND_RECEIVE_TIMEOUT\": \"30\"\n      }\n    }\n  }\n}\n```\n\nNote: Make sure to use the full path to the Python executable or the `mcp-clickhouse` script if they are not in your system PATH. You can find the paths using:\n- `which python3` for the Python executable\n- `which mcp-clickhouse` for the installed script\n\n## Development\n\n1. In `test-services` directory run `docker compose up -d` to start the ClickHouse cluster.\n\n2. Add the following variables to a `.env` file in the root of the repository.\n\n*Note: The use of the `default` user in this context is intended solely for local development purposes.*\n\n```bash\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_PORT=8123\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=clickhouse\n```\n\n3. Run `uv sync` to install the dependencies. To install `uv` follow the instructions [here](https://docs.astral.sh/uv/). Then do `source .venv/bin/activate`.\n\n4. For easy testing with the MCP Inspector, run `fastmcp dev mcp_clickhouse/mcp_server.py` to start the MCP server.\n\n5. To test with HTTP transport and the health check endpoint:\n   ```bash\n   # Using default port 8000\n   CLICKHOUSE_MCP_SERVER_TRANSPORT=http python -m mcp_clickhouse.main\n\n   # Or with a custom port\n   CLICKHOUSE_MCP_SERVER_TRANSPORT=http CLICKHOUSE_MCP_BIND_PORT=4200 python -m mcp_clickhouse.main\n\n   # Then in another terminal:\n   curl http://localhost:8000/health  # or http://localhost:4200/health for custom port\n   ```\n\n### Environment Variables\n\nThe following environment variables are used to configure the ClickHouse and chDB connections:\n\n#### ClickHouse Variables\n\n##### Required Variables\n\n* `CLICKHOUSE_HOST`: The hostname of your ClickHouse server\n* `CLICKHOUSE_USER`: The username for authentication\n* `CLICKHOUSE_PASSWORD`: The password for authentication\n\n> [!CAUTION]\n> It is important to treat your MCP database user as you would any external client connecting to your database, granting only the minimum necessary privileges required for its operation. The use of default or administrative users should be strictly avoided at all times.\n\n##### Optional Variables\n\n* `CLICKHOUSE_PORT`: The port number of your ClickHouse server\n  * Default: `8443` if HTTPS is enabled, `8123` if disabled\n  * Usually doesn't need to be set unless using a non-standard port\n* `CLICKHOUSE_SECURE`: Enable/disable HTTPS connection\n  * Default: `\"true\"`\n  * Set to `\"false\"` for non-secure connections\n* `CLICKHOUSE_VERIFY`: Enable/disable SSL certificate verification\n  * Default: `\"true\"`\n  * Set to `\"false\"` to disable certificate verification (not recommended for production)\n* `CLICKHOUSE_CONNECT_TIMEOUT`: Connection timeout in seconds\n  * Default: `\"30\"`\n  * Increase this value if you experience connection timeouts\n* `CLICKHOUSE_SEND_RECEIVE_TIMEOUT`: Send/receive timeout in seconds\n  * Default: `\"300\"`\n  * Increase this value for long-running queries\n* `CLICKHOUSE_DATABASE`: Default database to use\n  * Default: None (uses server default)\n  * Set this to automatically connect to a specific database\n* `CLICKHOUSE_MCP_SERVER_TRANSPORT`: Sets the transport method for the MCP server.\n  * Default: `\"stdio\"`\n  * Valid options: `\"stdio\"`, `\"http\"`, `\"sse\"`. This is useful for local development with tools like MCP Inspector.\n* `CLICKHOUSE_MCP_BIND_HOST`: Host to bind the MCP server to when using HTTP or SSE transport\n  * Default: `\"127.0.0.1\"`\n  * Set to `\"0.0.0.0\"` to bind to all network interfaces (useful for Docker or remote access)\n  * Only used when transport is `\"http\"` or `\"sse\"`\n* `CLICKHOUSE_MCP_BIND_PORT`: Port to bind the MCP server to when using HTTP or SSE transport\n  * Default: `\"8000\"`\n  * Only used when transport is `\"http\"` or `\"sse\"`\n* `CLICKHOUSE_ENABLED`: Enable/disable ClickHouse functionality\n  * Default: `\"true\"`\n  * Set to `\"false\"` to disable ClickHouse tools when using chDB only\n\n#### chDB Variables\n\n* `CHDB_ENABLED`: Enable/disable chDB functionality\n  * Default: `\"false\"`\n  * Set to `\"true\"` to enable chDB tools\n* `CHDB_DATA_PATH`: The path to the chDB data directory\n  * Default: `\":memory:\"` (in-memory database)\n  * Use `:memory:` for in-memory database\n  * Use a file path for persistent storage (e.g., `/path/to/chdb/data`)\n\n#### Example Configurations\n\nFor local development with Docker:\n\n```env\n# Required variables\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=clickhouse\n\n# Optional: Override defaults for local development\nCLICKHOUSE_SECURE=false  # Uses port 8123 automatically\nCLICKHOUSE_VERIFY=false\n```\n\nFor ClickHouse Cloud:\n\n```env\n# Required variables\nCLICKHOUSE_HOST=your-instance.clickhouse.cloud\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=your-password\n\n# Optional: These use secure defaults\n# CLICKHOUSE_SECURE=true  # Uses port 8443 automatically\n# CLICKHOUSE_DATABASE=your_database\n```\n\nFor ClickHouse SQL Playground:\n\n```env\nCLICKHOUSE_HOST=sql-clickhouse.clickhouse.com\nCLICKHOUSE_USER=demo\nCLICKHOUSE_PASSWORD=\n# Uses secure defaults (HTTPS on port 8443)\n```\n\nFor chDB only (in-memory):\n\n```env\n# chDB configuration\nCHDB_ENABLED=true\nCLICKHOUSE_ENABLED=false\n# CHDB_DATA_PATH defaults to :memory:\n```\n\nFor chDB with persistent storage:\n\n```env\n# chDB configuration\nCHDB_ENABLED=true\nCLICKHOUSE_ENABLED=false\nCHDB_DATA_PATH=/path/to/chdb/data\n```\n\nFor MCP Inspector or remote access with HTTP transport:\n\n```env\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=clickhouse\nCLICKHOUSE_MCP_SERVER_TRANSPORT=http\nCLICKHOUSE_MCP_BIND_HOST=0.0.0.0  # Bind to all interfaces\nCLICKHOUSE_MCP_BIND_PORT=4200  # Custom port (default: 8000)\n```\n\nWhen using HTTP transport, the server will run on the configured port (default 8000). For example, with the above configuration:\n- MCP endpoint: `http://localhost:4200/mcp`\n- Health check: `http://localhost:4200/health`\n\nYou can set these variables in your environment, in a `.env` file, or in the Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-clickhouse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-clickhouse\",\n        \"--python\",\n        \"3.10\",\n        \"mcp-clickhouse\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"<clickhouse-host>\",\n        \"CLICKHOUSE_USER\": \"<clickhouse-user>\",\n        \"CLICKHOUSE_PASSWORD\": \"<clickhouse-password>\",\n        \"CLICKHOUSE_DATABASE\": \"<optional-database>\",\n        \"CLICKHOUSE_MCP_SERVER_TRANSPORT\": \"stdio\",\n        \"CLICKHOUSE_MCP_BIND_HOST\": \"127.0.0.1\",\n        \"CLICKHOUSE_MCP_BIND_PORT\": \"8000\"\n      }\n    }\n  }\n}\n```\n\nNote: The bind host and port settings are only used when transport is set to \"http\" or \"sse\".\n\n### Running tests\n\n```bash\nuv sync --all-extras --dev # install dev dependencies\nuv run ruff check . # run linting\n\ndocker compose up -d test_services # start ClickHouse\nuv run pytest -v tests\nuv run pytest -v tests/test_tool.py # ClickHouse only\nuv run pytest -v tests/test_chdb_tool.py # chDB only\n```\n\n## YouTube Overview\n\n[![YouTube](http://i.ytimg.com/vi/y9biAm_Fkqw/hqdefault.jpg)](https://www.youtube.com/watch?v=y9biAm_Fkqw)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "clickhouse database",
        "database integration",
        "databases secure"
      ],
      "category": "databases"
    },
    "Couchbase-Ecosystem--mcp-server-couchbase": {
      "owner": "Couchbase-Ecosystem",
      "name": "mcp-server-couchbase",
      "url": "https://github.com/Couchbase-Ecosystem/mcp-server-couchbase",
      "imageUrl": "/freedevtools/mcp/pfp/Couchbase-Ecosystem.webp",
      "description": "Enables natural language querying and CRUD operations on Couchbase clusters. Supports SQL++ queries, document retrieval, updates, and deletions in specified scopes and collections.",
      "stars": 20,
      "forks": 21,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-01T15:25:19Z",
      "readme_content": "# Couchbase MCP Server\n\nAn [MCP](https://modelcontextprotocol.io/) server implementation of Couchbase that allows LLMs to directly interact with Couchbase clusters.\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/) [![PyPI version](https://badge.fury.io/py/couchbase-mcp-server.svg)](https://pypi.org/project/couchbase-mcp-server/) [![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/13fce476-0e74-4b1e-ab82-1df2a3204809) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Couchbase-Ecosystem/mcp-server-couchbase)](https://archestra.ai/mcp-catalog/couchbase-ecosystem__mcp-server-couchbase)\n\n<a href=\"https://glama.ai/mcp/servers/@Couchbase-Ecosystem/mcp-server-couchbase\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Couchbase-Ecosystem/mcp-server-couchbase/badge\" alt=\"Couchbase Server MCP server\" />\n</a>\n\n## Features\n\n- Get a list of all the buckets in the cluster\n- Get a list of all the scopes and collections in the specified bucket\n- Get a list of all the scopes in the specified bucket\n- Get a list of all the collections in a specified scope and bucket. Note that this tool requires the cluster to have Query service.\n- Get the structure for a collection\n- Get a document by ID from a specified scope and collection\n- Upsert a document by ID to a specified scope and collection\n- Delete a document by ID from a specified scope and collection\n- Run a [SQL++ query](https://www.couchbase.com/sqlplusplus/) on a specified scope\n  - There is an option in the MCP server, `CB_MCP_READ_ONLY_QUERY_MODE` that is set to true by default to disable running SQL++ queries that change the data or the underlying collection structure. Note that the documents can still be updated by ID.\n- Get the status of the MCP server\n- Check the cluster credentials by connecting to the cluster\n\n## Prerequisites\n\n- Python 3.10 or higher.\n- A running Couchbase cluster. The easiest way to get started is to use [Capella](https://docs.couchbase.com/cloud/get-started/create-account.html#getting-started) free tier, which is fully managed version of Couchbase server. You can follow [instructions](https://docs.couchbase.com/cloud/clusters/data-service/import-data-documents.html#import-sample-data) to import one of the sample datasets or import your own.\n- [uv](https://docs.astral.sh/uv/) installed to run the server.\n- An [MCP client](https://modelcontextprotocol.io/clients) such as [Claude Desktop](https://claude.ai/download) installed to connect the server to Claude. The instructions are provided for Claude Desktop and Cursor. Other MCP clients could be used as well.\n\n## Configuration\n\nThe MCP server can be run either from the prebuilt PyPI package or the source using uv.\n\n### Running from PyPI\n\nWe publish a pre built [PyPI package](https://pypi.org/project/couchbase-mcp-server/) for the MCP server.\n\n#### Server Configuration using Pre built Package for MCP Clients\n\n#### Basic Authentication\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase\": {\n      \"command\": \"uvx\",\n      \"args\": [\"couchbase-mcp-server\"],\n      \"env\": {\n        \"CB_CONNECTION_STRING\": \"couchbases://connection-string\",\n        \"CB_USERNAME\": \"username\",\n        \"CB_PASSWORD\": \"password\"\n      }\n    }\n  }\n}\n```\n\nor\n\n#### mTLS\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase\": {\n      \"command\": \"uvx\",\n      \"args\": [\"couchbase-mcp-server\"],\n      \"env\": {\n        \"CB_CONNECTION_STRING\": \"couchbases://connection-string\",\n        \"CB_CLIENT_CERT_PATH\": \"/path/to/client-certificate.pem\",\n        \"CB_CLIENT_KEY_PATH\": \"/path/to/client.key\"\n      }\n    }\n  }\n}\n```\n\n> Note: If you have other MCP servers in use in the client, you can add it to the existing `mcpServers` object.\n\n### Running from Source\n\nThe MCP server can be run from the source using this repository.\n\n#### Clone the repository to your local machine.\n\n```bash\ngit clone https://github.com/Couchbase-Ecosystem/mcp-server-couchbase.git\n```\n\n#### Server Configuration using Source for MCP Clients\n\nThis is the common configuration for the MCP clients such as Claude Desktop, Cursor, Windsurf Editor.\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/cloned/repo/mcp-server-couchbase/\",\n        \"run\",\n        \"src/mcp_server.py\"\n      ],\n      \"env\": {\n        \"CB_CONNECTION_STRING\": \"couchbases://connection-string\",\n        \"CB_USERNAME\": \"username\",\n        \"CB_PASSWORD\": \"password\"\n      }\n    }\n  }\n}\n```\n\n> Note: `path/to/cloned/repo/mcp-server-couchbase/` should be the path to the cloned repository on your local machine. Don't forget the trailing slash at the end!\n\n> Note: If you have other MCP servers in use in the client, you can add it to the existing `mcpServers` object.\n\n### Additional Configuration for MCP Server\n\nThe server can be configured using environment variables or command line arguments:\n| Environment Variable | CLI Argument | Description | Default |\n|--------------------------------|--------------------------|---------------------------------------------------------------------------------------------|------------------------------------------|\n| `CB_CONNECTION_STRING` | `--connection-string` | Connection string to the Couchbase cluster | **Required** |\n| `CB_USERNAME` | `--username` | Username with access to required buckets for basic authentication | **Required (or Client Certificate and Key needed for mTLS)** |\n| `CB_PASSWORD` | `--password` | Password for basic authentication | **Required (or Client Certificate and Key needed for mTLS)** |\n| `CB_CLIENT_CERT_PATH` | `--client-cert-path` | Path to the client certificate file for mTLS authentication| **Required if using mTLS (or Username and Password required)** |\n| `CB_CLIENT_KEY_PATH` | `--client-key-path` | Path to the client key file for mTLS authentication| **Required if using mTLS (or Username and Password required)** |\n| `CB_CA_CERT_PATH` | `--ca-cert-path` | Path to server root certificate for TLS if server is configured with a self-signed/untrusted certificate. This will not be required if you are connecting to Capella | |\n| `CB_MCP_READ_ONLY_QUERY_MODE` | `--read-only-query-mode` | Prevent data modification queries | `true` |\n| `CB_MCP_TRANSPORT` | `--transport` | Transport mode: `stdio`, `http`, `sse` | `stdio` |\n| `CB_MCP_HOST` | `--host` | Host for HTTP/SSE transport modes | `127.0.0.1` |\n| `CB_MCP_PORT` | `--port` | Port for HTTP/SSE transport modes | `8000` |\n\n> Note: For authentication, you need either the Username and Password or the Client Certificate and key paths. Optionally, you can specify the CA root certificate path that will be used to validate the server certificates.\n> If both the Client Certificate & key path and the username and password are specified, the client certificates will be used for authentication.\n\nYou can also check the version of the server using:\n\n```bash\nuvx couchbase-mcp-server --version\n```\n\n#### Client Specific Configuration\n\n<details>\n<summary>Claude Desktop</summary>\n\nFollow the steps below to use Couchbase MCP server with Claude Desktop MCP client\n\n1. The MCP server can now be added to Claude Desktop by editing the configuration file. More detailed instructions can be found on the [MCP quickstart guide](https://modelcontextprotocol.io/quickstart/user).\n\n   - On Mac, the configuration file is located at `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows, the configuration file is located at `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   Open the configuration file and add the [configuration](#configuration) to the `mcpServers` section.\n\n2. Restart Claude Desktop to apply the changes.\n\n3. You can now use the server in Claude Desktop to run queries on the Couchbase cluster using natural language and perform CRUD operations on documents.\n\nLogs\n\nThe logs for Claude Desktop can be found in the following locations:\n\n- MacOS: ~/Library/Logs/Claude\n- Windows: %APPDATA%\\Claude\\Logs\n\nThe logs can be used to diagnose connection issues or other problems with your MCP server configuration. For more details, refer to the [official documentation](https://modelcontextprotocol.io/quickstart/user#troubleshooting).\n\n</details>\n\n<details>\n<summary>Cursor</summary>\n\nFollow steps below to use Couchbase MCP server with Cursor:\n\n1. Install [Cursor](https://cursor.sh/) on your machine.\n\n2. In Cursor, go to Cursor > Cursor Settings > Tools & Integrations > MCP Tools. Also, checkout the docs on [setting up MCP server configuration](https://docs.cursor.com/en/context/mcp#configuring-mcp-servers) from Cursor.\n\n3. Specify the same [configuration](#configuration). You may need to add the server configuration under a parent key of mcpServers.\n\n4. Save the configuration.\n\n5. You will see couchbase as an added server in MCP servers list. Refresh to see if server is enabled.\n\n6. You can now use the Couchbase MCP server in Cursor to query your Couchbase cluster using natural language and perform CRUD operations on documents.\n\nFor more details about MCP integration with Cursor, refer to the [official Cursor MCP documentation](https://docs.cursor.com/en/context/mcp).\n\nLogs\n\nIn the bottom panel of Cursor, click on \"Output\" and select \"Cursor MCP\" from the dropdown menu to view server logs. This can help diagnose connection issues or other problems with your MCP server configuration.\n\n</details>\n\n<details>\n<summary>Windsurf Editor</summary>\n\nFollow the steps below to use the Couchbase MCP server with [Windsurf Editor](https://windsurf.com/).\n\n1. Install [Windsurf Editor](https://windsurf.com/download) on your machine.\n\n2. In Windsurf Editor, navigate to Command Palette > Windsurf MCP Configuration Panel or Windsurf - Settings > Advanced > Cascade > Model Context Protocol (MCP) Servers. For more details on the configuration, please refer to the [official documentation](https://docs.windsurf.com/windsurf/cascade/mcp#adding-a-new-mcp-plugin).\n\n3. Click on Add Server and then Add custom server. On the configuration that opens in the editor, add the Couchbase MCP Server [configuration](#configuration) from above.\n\n4. Save the configuration.\n\n5. You will see couchbase as an added server in MCP Servers list under Advanced Settings. Refresh to see if server is enabled.\n\n6. You can now use the Couchbase MCP server in Windsurf Editor to query your Couchbase cluster using natural language and perform CRUD operations on documents.\n\nFor more details about MCP integration with Windsurf Editor, refer to the official [Windsurf MCP documentation](https://docs.windsurf.com/windsurf/cascade/mcp).\n\n</details>\n\n## Streamable HTTP Transport Mode\n\nThe MCP Server can be run in [Streamable HTTP](https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#streamable-http) transport mode which allows multiple clients to connect to the same server instance via HTTP.\nCheck if your [MCP client](https://modelcontextprotocol.io/clients) supports streamable http transport before attempting to connect to MCP server in this mode.\n\n> Note: This mode does not include authorization support.\n\n### Usage\n\nBy default, the MCP server will run on port 8000 but this can be configured using the `--port` or `CB_MCP_PORT` environment variable.\n\n```bash\nuvx couchbase-mcp-server \\\n  --connection-string='<couchbase_connection_string>' \\\n  --username='<database_username>' \\\n  --password='<database_password>' \\\n  --read-only-query-mode=true \\\n  --transport=http\n```\n\nThe server will be available on http://localhost:8000/mcp. This can be used in MCP clients supporting streamable http transport mode such as Cursor.\n\n### MCP Client Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase-http\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n## SSE Transport Mode\n\nThere is an option to run the MCP server in [Server-Sent Events (SSE)](https://modelcontextprotocol.io/specification/2024-11-05/basic/transports#http-with-sse) transport mode.\n\n> Note: SSE mode has been [deprecated](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse-deprecated) by MCP. We have support for [Streamable HTTP](#streamable-http-transport-mode).\n\n### Usage\n\nBy default, the MCP server will run on port 8000 but this can be configured using the `--port` or `CB_MCP_PORT` environment variable.\n\n```bash\nuvx couchbase-mcp-server \\\n  --connection-string='<couchbase_connection_string>' \\\n  --username='<database_username>' \\\n  --password='<database_password>' \\\n  --read-only-query-mode=true \\\n  --transport=sse\n```\n\nThe server will be available on http://localhost:8000/sse. This can be used in MCP clients supporting SSE transport mode such as Cursor.\n\n### MCP Client Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase-sse\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n## Docker Image\n\nThe MCP server can also be built and run as a Docker container. Prebuilt images can be found on [DockerHub](https://hub.docker.com/r/couchbaseecosystem/mcp-server-couchbase).\n\nAlternatively, we are part of the [Docker MCP Catalog](https://hub.docker.com/mcp/server/couchbase/overview).\n\n### Building Image\n\n```bash\ndocker build -t mcp/couchbase .\n```\n\n<details>\n<summary>Building with Arguments</summary>\nIf you want to build with the build arguments for commit hash and the build time, you can build using:\n\n```bash\ndocker build --build-arg GIT_COMMIT_HASH=$(git rev-parse HEAD) \\\n  --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \\\n  -t mcp/couchbase .\n```\n\n**Alternatively, use the provided build script:**\n\n```bash\n./build.sh\n```\n\nThis script automatically:\n\n- Generates git commit hash and build timestamp\n- Creates multiple useful tags (`latest`, `<short-commit>`)\n- Shows build information and results\n- Uses the same arguments as CI/CD builds\n\n**Verify image labels:**\n\n```bash\n# View git commit hash in image\ndocker inspect --format='{{index .Config.Labels \"org.opencontainers.image.revision\"}}' mcp/couchbase:latest\n\n# View all metadata labels\ndocker inspect --format='{{json .Config.Labels}}' mcp/couchbase:latest\n```\n\n</details>\n\n### Running\n\nThe MCP server can be run with the environment variables being used to configure the Couchbase settings. The environment variables are the same as described in the [Configuration section](#server-configuration-for-mcp-clients).\n\n#### Independent Docker Container\n\n```bash\ndocker run --rm -i \\\n  -e CB_CONNECTION_STRING='<couchbase_connection_string>' \\\n  -e CB_USERNAME='<database_user>' \\\n  -e CB_PASSWORD='<database_password>' \\\n  -e CB_MCP_TRANSPORT='<http|sse|stdio>' \\\n  -e CB_MCP_READ_ONLY_QUERY_MODE='<true|false>' \\\n  -e CB_MCP_PORT=9001 \\\n  -p 9001:9001 \\\n  mcp/couchbase\n```\n\nThe `CB_MCP_PORT` environment variable is only applicable in the case of HTTP transport modes like http and sse.\n\n#### MCP Client Configuration\n\nThe Docker image can be used in `stdio` transport mode with the following configuration.\n\n```json\n{\n  \"mcpServers\": {\n    \"couchbase-mcp-docker\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"CB_CONNECTION_STRING=<couchbase_connection_string>\",\n        \"-e\",\n        \"CB_USERNAME=<database_user>\",\n        \"-e\",\n        \"CB_PASSWORD=<database_password>\",\n        \"mcp/couchbase\"\n      ]\n    }\n  }\n}\n```\n\nNotes\n\n- The `couchbase_connection_string` value depends on whether the Couchbase server is running on the same host machine, in another Docker container, or on a remote host. If your Couchbase server is running on your host machine, your connection string would likely be of the form `couchbase://host.docker.internal`. For details refer to the [docker documentation](https://docs.docker.com/desktop/features/networking/#i-want-to-connect-from-a-container-to-a-service-on-the-host).\n- You can specify the container's networking using the `--network=<your_network>` option. The network you choose depends on your environment; the default is `bridge`. For details, refer to [network drivers in docker](https://docs.docker.com/engine/network/drivers/).\n\n### Risks Associated with LLMs\n\n- The use of large language models and similar technology involves risks, including the potential for inaccurate or harmful outputs.\n- Couchbase does not review or evaluate the quality or accuracy of such outputs, and such outputs may not reflect Couchbase's views.\n- You are solely responsible for determining whether to use large language models and related technology, and for complying with any license terms, terms of use, and your organization's policies governing your use of the same.\n\n### Managed MCP Server\n\nThe Couchbase MCP server can also be used as a managed server in your agentic applications via [Smithery.ai](https://smithery.ai/server/@Couchbase-Ecosystem/mcp-server-couchbase).\n\n## Troubleshooting Tips\n\n- Ensure the path to your MCP server repository is correct in the configuration if running from source.\n- Verify that your Couchbase connection string, database username, password or the path to the certificates are correct.\n- If using Couchbase Capella, ensure that the cluster is [accessible](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) from the machine where the MCP server is running.\n- Check that the database user has proper permissions to access at least one bucket.\n- Confirm that the `uv` package manager is properly installed and accessible. You may need to provide absolute path to `uv`/`uvx` in the `command` field in the configuration.\n- Check the logs for any errors or warnings that may indicate issues with the MCP server. The location of the logs depend on your MCP client.\n- If you are observing issues running your MCP server from source after updating your local MCP server repository, try running `uv sync` to update the [dependencies](https://docs.astral.sh/uv/concepts/projects/sync/#syncing-the-environment).\n\n---\n\n## 👩‍💻 Contributing\n\nWe welcome contributions from the community! Whether you want to fix bugs, add features, or improve documentation, your help is appreciated.\n\nIf you need help, have found a bug, or want to contribute improvements, the best place to do that is right here — by [opening a GitHub issue](https://github.com/Couchbase-Ecosystem/mcp-server-couchbase/issues).\n\n### For Developers\n\nIf you're interested in contributing code or setting up a development environment:\n\n📖 **See [CONTRIBUTING.md](CONTRIBUTING.md)** for comprehensive developer setup instructions, including:\n\n- Development environment setup with `uv`\n- Code linting and formatting with Ruff\n- Pre-commit hooks installation\n- Project structure overview\n- Development workflow and practices\n\n### Quick Start for Contributors\n\n```bash\n# Clone and setup\ngit clone https://github.com/Couchbase-Ecosystem/mcp-server-couchbase.git\ncd mcp-server-couchbase\n\n# Install with development dependencies\nuv sync --extra dev\n\n# Install pre-commit hooks\nuv run pre-commit install\n\n# Run linting\n./scripts/lint.sh\n```\n\n---\n\n## 📢 Support Policy\n\nWe truly appreciate your interest in this project!\nThis project is **Couchbase community-maintained**, which means it's **not officially supported** by our support team. However, our engineers are actively monitoring and maintaining this repo and will try to resolve issues on a best-effort basis.\n\nOur support portal is unable to assist with requests related to this project, so we kindly ask that all inquiries stay within GitHub.\n\nYour collaboration helps us all move forward together — thank you!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "couchbase",
        "databases",
        "database",
        "couchbase ecosystem",
        "couchbase clusters",
        "couchbase enables"
      ],
      "category": "databases"
    },
    "DanielRSnell--postgres-mcp": {
      "owner": "DanielRSnell",
      "name": "postgres-mcp",
      "url": "https://github.com/DanielRSnell/postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DanielRSnell.webp",
      "description": "Interact with PostgreSQL databases, execute read-only SQL queries, and inspect database schemas to facilitate data-driven applications.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-30T11:36:47Z",
      "readme_content": "# PostgreSQL\n\nA Model Context Protocol server that provides read-only access to PostgreSQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas** (`postgres://<host>/<table>/schema`)\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n### Docker\n\n* when running docker on macos, use host.docker.internal if the server is running on the host network (eg localhost)\n* username/password can be added to the postgresql url with `postgresql://user:password@host:port/db-name`\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"mcp/postgres\", \n        \"postgresql://host.docker.internal:5432/mydb\"]\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-postgres\",\n        \"postgresql://localhost/mydb\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/mydb` with your database name.\n\n## Building\n\nDocker:\n\n```sh\ndocker build -t mcp/postgres -f src/postgres/Dockerfile . \n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "DynamicEndpoints--advanced-pocketbase-mcp-server": {
      "owner": "DynamicEndpoints",
      "name": "advanced-pocketbase-mcp-server",
      "url": "https://github.com/DynamicEndpoints/advanced-pocketbase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Interact with PocketBase databases, enabling advanced database operations and schema management, including custom collection handling and index management.",
      "stars": 55,
      "forks": 19,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T13:16:04Z",
      "readme_content": "# Advanced PocketBase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/pocketbase-server)](https://smithery.ai/server/pocketbase-server)\n\n[![Deploy to Cloudflare Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/your-username/advanced-pocketbase-mcp-server)\n\nA comprehensive MCP server that provides sophisticated tools for interacting with PocketBase databases. This server enables advanced database operations, schema management, and data manipulation through the Model Context Protocol (MCP). **Now with full Cloudflare Workers support and Durable Objects for serverless deployment!**\n\n<a href=\"https://glama.ai/mcp/servers/z2xjuegxxh\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/z2xjuegxxh/badge\" alt=\"pocketbase-mcp-server MCP server\" /></a>\n\n## Changelog\n\n### v4.0.0 (June 30, 2025) - Cloudflare Workers & Durable Objects Support\n\n#### Added - Serverless Deployment & Production Readiness\n- **🚀 Cloudflare Workers Support**: Complete serverless deployment capability\n  - `worker.ts`: Main Cloudflare Worker entry point with routing and request handling\n  - `durable-object.ts`: Advanced Durable Object implementation for stateful MCP sessions\n  - `agent-worker-compatible.ts`: Worker-optimized PocketBase MCP agent\n  - Full WebSocket support for real-time MCP connections over Durable Objects\n- **🔧 Production Deployment Tools**: Ready-to-deploy configuration\n  - `wrangler.toml`: Complete Cloudflare Workers configuration\n  - `Dockerfile` and `Dockerfile.test`: Docker support for development and testing\n  - `tsconfig.worker.json`: Worker-specific TypeScript configuration\n- **🛡️ Super Admin Authentication**: Runtime admin privilege escalation\n  - `pocketbase_super_admin_auth` tool: Authenticate as super admin during runtime\n  - Enables admin operations (collection creation, schema changes) programmatically\n  - Comprehensive security analysis and capability testing\n- **📊 Advanced Diagnostics**: Production monitoring and debugging tools\n  - `debug_pocketbase_auth`: Authentication and connection testing\n  - `check_pocketbase_write_permissions`: Write operation capability analysis\n  - `analyze_pocketbase_capabilities`: Complete security model documentation\n  - Production vs development environment detection and guidance\n\n#### Enhanced - Serverless Architecture\n- **🌐 Multiple Deployment Options**: \n  - Traditional Node.js server (existing)\n  - Cloudflare Workers with Durable Objects (new)\n  - Docker containerization support (new)\n- **⚡ Performance Optimizations**: \n  - Durable Object hibernation for cost efficiency\n  - Connection pooling and session management\n  - Automatic retry logic with exponential backoff\n- **🔐 Enterprise Security**: \n  - Production security mode detection\n  - Admin operation restrictions with bypass capability\n  - Comprehensive audit logging and session tracking\n\n#### Documentation\n- **📖 Complete Deployment Guides**: \n  - `CLOUDFLARE_DEPLOYMENT.md`: Step-by-step Cloudflare deployment\n  - `SUPER_ADMIN_AUTH.md`: Super admin authentication usage guide\n  - `OPERATION_CAPABILITIES.md`: Production security model explanation\n- **🔄 Migration Support**: `CLOUDFLARE_AGENT.md` for transitioning to serverless\n\n#### Technical Improvements\n- Full TypeScript compatibility across Node.js and Cloudflare Workers\n- Environment variable management for multiple deployment targets\n- Comprehensive error handling for network and authentication failures\n- Resource cleanup and memory management for long-running sessions\n\nThis major release transforms the Advanced PocketBase MCP Server into a production-ready, serverless-capable solution that can be deployed on Cloudflare's global edge network while maintaining full compatibility with traditional deployments.\n\n## Changelog\n\n### v3.0.0 (June 10, 2025)\n\n#### Added - Complete Full-Stack SaaS Backend Integration\n- **Email Service Integration**: Complete email functionality with SMTP and SendGrid support\n  - 10 comprehensive email MCP tools: create/update/delete templates, send templated/custom emails\n  - Email logging and template management system\n  - Connection testing and default template setup\n- **Enhanced Stripe Service**: Advanced payment processing capabilities\n  - 10 additional Stripe MCP tools for complete payment management\n  - Payment intent creation, customer management, subscription handling\n  - Full webhook processing and product synchronization\n- **Full-Stack SaaS Automation**: 5 complete workflow automation tools\n  - `register_user_with_automation`: Complete user registration with email and Stripe customer creation\n  - `create_subscription_flow`: End-to-end subscription setup with email notifications\n  - `process_payment_webhook_with_email`: Webhook processing with automated email notifications\n  - `setup_complete_saas_backend`: One-click SaaS backend initialization\n  - `cancel_subscription_with_email`: Subscription cancellation with customer notifications\n- **Production-Ready Monitoring**: Backend status monitoring and health checks\n  - `get_saas_backend_status`: Comprehensive status reporting for production readiness\n  - Service health checks, collection validation, template verification\n  - Production readiness assessment and recommendations\n\n#### Enhanced Services\n- **EmailService**: Added `updateTemplate()` and `testConnection()` methods\n- **StripeService**: Added `createPaymentIntent()`, `retrieveCustomer()`, `updateCustomer()`, `cancelSubscription()` methods\n- **Advanced Collections**: Automated setup for `stripe_products`, `stripe_customers`, `stripe_subscriptions`, `stripe_payments`, `email_templates`, `email_logs`\n\n#### Fixed\n- **TypeScript Syntax Errors**: Resolved all compilation errors in index.ts\n- **Import Statements**: Fixed malformed import in email.ts service\n- **Tool Registration**: Corrected MCP tool registration syntax and structure\n\n#### Technical Improvements\n- Complete type safety across all new services and tools\n- Comprehensive error handling for all email and payment operations\n- Modular service architecture with proper separation of concerns\n- Environment-based configuration for all external services\n\nThis release transforms the Advanced PocketBase MCP Server into a complete full-stack SaaS backend solution, providing everything needed for user management, payment processing, email communications, and business automation through the Model Context Protocol.\n\n## Changelog\n\n### v2.3.0 (June 12, 2025)\n\n#### Added - SDK Compatibility & Modernization\n- **Complete SDK Compatibility**: Full compatibility with latest PocketBase JavaScript SDK v0.26.1\n- **Modern Type Definitions**: Completely rewrote `src/types/pocketbase.d.ts` to match actual SDK API\n  - Added correct interfaces for CollectionService, RecordService, FileService, HealthService, RealtimeService\n  - Updated AuthStore, AuthData, AuthMethodsList with proper method signatures\n  - Removed incompatible features that don't exist in current SDK version\n- **Authentication Method Modernization**: Updated all authentication tools to use current SDK patterns\n  - Fixed `authenticate_with_otp` to use `requestOTP()` for initiating OTP flow\n  - Updated `authenticate_with_oauth2` to use `authWithOAuth2Code()` with proper parameters\n  - Corrected method casing from `authWithOtp` to `authWithOTP` to match SDK\n  - Fixed all AuthStore references from deprecated `model` property to correct `record` property\n\n#### Fixed - SDK Compatibility Issues\n- **Removed Incompatible Features**: Cleaned up tools using non-existent SDK methods\n  - Removed `get_collection_scaffolds` tool (used non-existent `collections.getScaffolds()`)\n  - Removed `import_collections` tool (used non-existent `collections.import()`)\n  - Replaced `createBatch()` API calls with sequential execution in batch operation tools\n- **Interface Cleanup**: Removed `ExtendedPocketBase` interface, using standard `PocketBase` type directly\n- **Syntax Corrections**: Fixed various syntax errors including missing parentheses and semicolons\n- **Build System**: Successfully compiled TypeScript project without errors, server starts properly\n\n#### Enhanced\n- **Tool Registration**: All MCP tool registrations now follow correct patterns with modern SDK capabilities\n- **Error Handling**: Improved error handling throughout all authentication and data operations\n- **Type Safety**: Enhanced TypeScript support with accurate type definitions matching SDK v0.26.1\n- **Documentation**: Created comprehensive CHANGELOG.md documenting all changes and breaking changes\n\n#### Technical Improvements\n- Verified compatibility with MCP TypeScript SDK v1.12.1\n- Ensured all tool implementations use actual PocketBase SDK v0.26.1 methods\n- Replaced batch operations with sequential execution to work within SDK limitations\n- Improved overall code stability and maintainability\n\nThis release ensures the Advanced PocketBase MCP Server is fully compatible with the latest SDK versions and follows modern development patterns.\n\n### v2.2.0 (June 7, 2025)\n\n#### Added\n- **SSE Transport Support**: Added Server-Sent Events transport for real-time streaming capabilities\n- **Multiple Transport Options**: Now supports stdio, HTTP, and SSE transports\n- **Real-time Streaming**: Enhanced `stream_collection_changes` tool with MCP notification system\n- **HTTP Server Mode**: New HTTP server with health check endpoint\n- **Express Integration**: Added Express.js for HTTP/SSE server functionality\n- **Streamable HTTP Protocol**: Support for latest MCP protocol version 2025-03-26\n- **Backward Compatibility**: Maintains support for legacy HTTP+SSE protocol 2024-11-05\n\n#### Updated\n- **MCP SDK**: Updated to latest version 1.12.1\n- **PocketBase SDK**: Updated to latest version 0.26.1\n- **TypeScript Support**: Enhanced type definitions and error handling\n- **Package Scripts**: Added new npm scripts for different server modes\n\n#### Enhanced\n- **Documentation**: Comprehensive README updates with SSE examples\n- **Error Handling**: Improved error messages and type safety\n- **Development Experience**: Better TypeScript integration and debugging\n\n#### Technical Improvements\n- Added Express types for better TypeScript support\n- Enhanced session management for SSE connections\n- Improved transport lifecycle management\n- Better resource cleanup on server shutdown\n\n## Changelog\n\n### v2.1.0 (April 3, 2025)\n\n#### Added\n- Added `batch_update_records` tool for updating multiple records at once.\n- Added `batch_delete_records` tool for deleting multiple records at once.\n- Added `subscribe_to_collection` tool for real-time event subscriptions (requires `eventsource` polyfill).\n\n#### Fixed\n- Corrected schema for `authenticate_user` to allow admin authentication via environment variables without explicit email/password.\n- Added `eventsource` dependency and polyfill to enable real-time subscriptions in Node.js.\n\n### v2.0.0 (April 2, 2025)\n\n#### Added\n- Enhanced admin authentication support with environment variables\n- Added support for admin impersonation via the `impersonate_user` tool\n- Improved error handling for authentication operations\n- Added comprehensive TypeScript type definitions for better development experience\n- Added support for Cline integration\n\n#### Fixed\n- Fixed TypeScript errors in the PocketBase client implementation\n- Improved schema field handling with proper type annotations\n- Fixed issues with optional schema field properties\n\n#### Changed\n- Updated the authentication flow to support multiple authentication methods\n- Improved documentation with more detailed examples\n- Enhanced environment variable configuration options\n\n## 🚀 Deployment Options\n\n### Smithery Platform (Managed Hosting) ⭐ **Recommended for Beginners**\n\nDeploy to Smithery's managed platform for hosted MCP servers with zero infrastructure management:\n\n[![Deploy to Smithery](https://smithery.ai/badge/deploy)](https://smithery.ai/server/pocketbase-server)\n\n**Benefits:**\n- 🌐 Hosted MCP server with interactive web playground\n- 🔧 Zero infrastructure or deployment complexity\n- 🔍 Built-in testing and discovery tools\n- 📊 Usage analytics and monitoring dashboard\n- 🛡️ Automatic security updates and maintenance\n\n**Quick Setup:**\n1. Visit [Smithery PocketBase Server](https://smithery.ai/server/pocketbase-server)\n2. Click \"Deploy\" and connect your GitHub account\n3. Configure your PocketBase URL and optional admin credentials\n4. Start using immediately with the web playground\n\n**Configuration Options:**\n- `pocketbaseUrl`: Your PocketBase instance URL (required)\n- `adminEmail`: Admin email for elevated operations (optional)\n- `adminPassword`: Admin password for elevated operations (optional)\n- `debug`: Enable debug logging (optional, default: false)\n\n### Cloudflare Workers (Production Scale)\n\nDeploy to Cloudflare's global edge network with Durable Objects for stateful MCP sessions:\n\n[![Deploy to Cloudflare Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/your-username/advanced-pocketbase-mcp-server)\n\n**Quick Deploy:**\n```bash\n# Clone and deploy\ngit clone https://github.com/your-username/advanced-pocketbase-mcp-server\ncd advanced-pocketbase-mcp-server\nnpm install\nnpm run build\nnpx wrangler deploy\n```\n\n**Benefits:**\n- ⚡ Global edge deployment with sub-100ms latency\n- 💰 Pay-per-use pricing (free tier available)\n- 🔄 Automatic scaling and load balancing\n- 🛡️ Built-in security and DDoS protection\n- 📊 Advanced diagnostics and monitoring tools\n\n### Traditional Node.js Server\n\nStandard deployment for development and traditional hosting:\n\n```bash\nnpm install\nnpm run build\nnpm start\n```\n\n### Docker Deployment\n\nContainerized deployment for any platform:\n\n```bash\ndocker build -t pocketbase-mcp-server .\ndocker run -p 3000:3000 -e POCKETBASE_URL=your_url pocketbase-mcp-server\n```\n\n## Features\n\n### Collection Management\n- Create and manage collections with custom schemas\n- Migrate collection schemas with data preservation\n- Advanced index management (create, delete, list)\n- Schema validation and type safety\n- Retrieve collection schemas and metadata\n\n### Record Operations\n- CRUD operations for records\n- Advanced querying with filtering, sorting, and aggregation\n- Batch import/export capabilities\n- Relationship expansion support\n- Pagination and cursor-based navigation\n\n### User Management\n- User authentication and token management\n- User account creation and management\n- Password management\n- Role-based access control\n- Session handling\n\n### Database Operations\n- Database backup and restore\n- Multiple export formats (JSON/CSV)\n- Data migration tools\n- Index optimization\n- Batch operations\n\n## Available Tools\n\n### Collection Management\n- `create_collection`: Create a new collection with custom schema\n- `get_collection_schema`: Get schema details for a collection\n- `migrate_collection`: Migrate collection schema with data preservation\n- `manage_indexes`: Create, delete, or list collection indexes\n\n### Record Operations\n- `create_record`: Create a new record in a collection\n- `list_records`: List records with optional filters and pagination\n- `update_record`: Update an existing record\n- `delete_record`: Delete a record\n- `query_collection`: Advanced query with filtering, sorting, and aggregation\n- `batch_update_records`: Update multiple records in a single call\n- `batch_delete_records`: Delete multiple records in a single call\n- `subscribe_to_collection`: Subscribe to real-time changes in a collection (requires `eventsource` package in Node.js environment)\n- `import_data`: Import data into a collection with create/update/upsert modes\n\n### User Management\n- `authenticate_user`: Authenticate a user and get auth token\n- `create_user`: Create a new user account\n- `list_auth_methods`: List all available authentication methods\n- `authenticate_with_oauth2`: Authenticate a user with OAuth2\n- `authenticate_with_otp`: Authenticate a user with one-time password\n- `auth_refresh`: Refresh authentication token\n- `request_verification`: Request email verification\n- `confirm_verification`: Confirm email verification with token\n- `request_password_reset`: Request password reset\n- `confirm_password_reset`: Confirm password reset with token\n- `request_email_change`: Request email change\n- `confirm_email_change`: Confirm email change with token\n- `impersonate_user`: Impersonate another user (admin only)\n\n### Database Operations\n- `backup_database`: Create a backup of the PocketBase database with format options\n- `import_data`: Import data with various modes (create/update/upsert)\n\n### 🔧 Production Diagnostics & Admin Tools\n- `debug_pocketbase_auth`: Test authentication and connection status\n- `check_pocketbase_write_permissions`: Analyze write operation capabilities\n- `analyze_pocketbase_capabilities`: Document available vs restricted operations\n- `pocketbase_super_admin_auth`: **Authenticate as super admin at runtime**\n- `get_server_status`: Comprehensive server status and configuration\n- `health_check`: Simple health check endpoint\n\n### 🛡️ Super Admin Operations\nAfter using `pocketbase_super_admin_auth`, these admin-level operations become available:\n- Collection creation and schema modifications\n- User management and authentication settings  \n- System configuration changes\n- Database administration tasks\n\n> **Note**: Admin operations may be restricted in production environments for security. Use the diagnostic tools to understand your deployment's security model.\n\n## Configuration\n\n### Smithery Platform (Managed Hosting)\nConfigure through Smithery's web interface when deploying:\n\n**Required:**\n- `pocketbaseUrl`: Your PocketBase instance URL\n\n**Optional:**\n- `adminEmail`: Admin email for super admin authentication\n- `adminPassword`: Admin password for elevated operations\n- `debug`: Enable debug logging for troubleshooting\n\n### Node.js Deployment\nRequired environment variables:\n- `POCKETBASE_URL`: URL of your PocketBase instance (e.g., \"http://127.0.0.1:8090\")\n\nOptional environment variables:\n- `POCKETBASE_ADMIN_EMAIL`: Admin email for certain operations\n- `POCKETBASE_ADMIN_PASSWORD`: Admin password\n- `POCKETBASE_DATA_DIR`: Custom data directory path\n\n### Cloudflare Workers Deployment\nConfigure in `wrangler.toml` or through Cloudflare dashboard:\n\n```toml\n[env.production.vars]\nPOCKETBASE_URL = \"https://your-pocketbase-instance.com\"\nPOCKETBASE_ADMIN_EMAIL = \"admin@example.com\"\n\n[env.production.secrets]\nPOCKETBASE_ADMIN_PASSWORD = \"your-super-secure-password\"\n```\n\n**Environment-specific considerations:**\n- **Development**: Use local PocketBase instance with full admin access\n- **Production**: Use hosted PocketBase with potential admin restrictions\n- **Edge**: Cloudflare Workers provide global deployment with Durable Objects\n\n### Production Security & Super Admin Authentication\n- Admin credentials enable the `pocketbase_super_admin_auth` tool\n- Production environments may restrict admin API access for security\n- Use diagnostic tools (`analyze_pocketbase_capabilities`) to understand your deployment\n- The super admin tool bypasses production restrictions when credentials are valid\n\n## Usage Examples\n\n### Collection Management\n```typescript\n// Create a new collection\nawait mcp.use_tool(\"pocketbase\", \"create_collection\", {\n  name: \"posts\",\n  schema: [\n    {\n      name: \"title\",\n      type: \"text\",\n      required: true\n    },\n    {\n      name: \"content\",\n      type: \"text\",\n      required: true\n    }\n  ]\n});\n\n// Manage indexes\nawait mcp.use_tool(\"pocketbase\", \"manage_indexes\", {\n  collection: \"posts\",\n  action: \"create\",\n  index: {\n    name: \"title_idx\",\n    fields: [\"title\"],\n    unique: true\n  }\n});\n```\n\n### Advanced Querying\n```typescript\n// Query with filtering, sorting, and aggregation\nawait mcp.use_tool(\"pocketbase\", \"query_collection\", {\n  collection: \"posts\",\n  filter: \"created >= '2024-01-01'\",\n  sort: \"-created\",\n  aggregate: {\n    totalLikes: \"sum(likes)\",\n    avgRating: \"avg(rating)\"\n  },\n  expand: \"author,categories\"\n});\n```\n\n### Data Import/Export\n```typescript\n// Import data with upsert mode\nawait mcp.use_tool(\"pocketbase\", \"import_data\", {\n  collection: \"posts\",\n  data: [\n    {\n      title: \"First Post\",\n      content: \"Hello World\"\n    },\n    {\n      title: \"Second Post\",\n      content: \"More content\"\n    }\n  ],\n  mode: \"upsert\"\n});\n\n// Backup database\nawait mcp.use_tool(\"pocketbase\", \"backup_database\", {\n  format: \"json\" // or \"csv\"\n});\n```\n\n### Schema Migration\n```typescript\n// Migrate collection schema\nawait mcp.use_tool(\"pocketbase\", \"migrate_collection\", {\n  collection: \"posts\",\n  newSchema: [\n    {\n      name: \"title\",\n      type: \"text\",\n      required: true\n    },\n    {\n      name: \"content\",\n      type: \"text\",\n      required: true\n    },\n    {\n      name: \"tags\",\n      type: \"json\",\n      required: false\n    }\n  ],\n  dataTransforms: {\n    // Optional field transformations during migration\n    tags: \"JSON.parse(oldTags)\"\n  }\n});\n```\n\n### Batch & Real-time Operations\n```typescript\n// Batch update records\nawait mcp.use_tool(\"pocketbase\", \"batch_update_records\", {\n  collection: \"products\",\n  records: [\n    { id: \"record_id_1\", data: { price: 19.99 } },\n    { id: \"record_id_2\", data: { status: \"published\" } }\n  ]\n});\n\n// Batch delete records\nawait mcp.use_tool(\"pocketbase\", \"batch_delete_records\", {\n  collection: \"products\",\n  recordIds: [\"record_id_3\", \"record_id_4\"]\n});\n\n// Subscribe to collection changes (logs events to server console)\n// Note: Requires 'eventsource' package installed in the Node.js environment running the server.\nawait mcp.use_tool(\"pocketbase\", \"subscribe_to_collection\", {\n  collection: \"products\"\n});\n\n// Subscribe to a specific record\nawait mcp.use_tool(\"pocketbase\", \"subscribe_to_collection\", {\n  collection: \"products\",\n  recordId: \"specific_product_id\"\n});\n```\n\n### Authentication Methods\n```typescript\n// List available authentication methods\nawait mcp.use_tool(\"pocketbase\", \"list_auth_methods\", {\n  collection: \"users\"\n});\n\n// Authenticate with password\nawait mcp.use_tool(\"pocketbase\", \"authenticate_user\", {\n  email: \"user@example.com\",\n  password: \"securepassword\",\n  collection: \"users\"\n});\n\n// Authenticate with OAuth2\nawait mcp.use_tool(\"pocketbase\", \"authenticate_with_oauth2\", {\n  provider: \"google\",\n  code: \"auth_code_from_provider\",\n  codeVerifier: \"code_verifier_from_pkce\",\n  redirectUrl: \"https://your-app.com/auth/callback\",\n  collection: \"users\"\n});\n\n// Request password reset\nawait mcp.use_tool(\"pocketbase\", \"request_password_reset\", {\n  email: \"user@example.com\",\n  collection: \"users\"\n});\n\n// Confirm password reset\nawait mcp.use_tool(\"pocketbase\", \"confirm_password_reset\", {\n  token: \"verification_token\",\n  password: \"new_password\",\n  passwordConfirm: \"new_password\",\n  collection: \"users\"\n});\n\n// Refresh authentication token\nawait mcp.use_tool(\"pocketbase\", \"auth_refresh\", {\n  collection: \"users\"\n});\n```\n\n## Error Handling\n\nAll tools include comprehensive error handling with detailed error messages. Errors are properly typed and include:\n- Invalid request errors\n- Authentication errors\n- Database operation errors\n- Schema validation errors\n- Network errors\n\n## Type Safety\n\nThe server includes TypeScript definitions for all operations, ensuring type safety when using the tools. Each tool's input schema is strictly typed and validated.\n\n## Best Practices\n\n1. Always use proper error handling with try/catch blocks\n2. Validate data before performing operations\n3. Use appropriate indexes for better query performance\n4. Regularly backup your database\n5. Use migrations for schema changes\n6. Follow security best practices for user management\n7. Monitor and optimize database performance\n\n## Development\n\n### Smithery Platform Development\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Install Smithery CLI: `npm install -g @smithery/cli` \n4. Start development server: `npm run smithery:dev`\n5. Open the auto-generated playground URL to test\n\n### Local Development (Node.js)\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Copy `.env.example` to `.env` and configure\n4. Build: `npm run build`\n5. Start your PocketBase instance\n6. Run: `npm start`\n\n### Cloudflare Workers Development\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Configure `wrangler.toml` with your settings\n4. Build: `npm run build`\n5. Deploy: `npx wrangler deploy`\n6. Test with: `npx wrangler tail` for real-time logs\n\n### Testing Super Admin Features\n```bash\n# Test the super admin authentication tool\nnode test-super-admin-tool.js\n\n# Run all diagnostic tools to verify setup\n# Use your MCP client to call:\n# - debug_pocketbase_auth\n# - check_pocketbase_write_permissions  \n# - analyze_pocketbase_capabilities\n# - pocketbase_super_admin_auth\n```\n\n### File Structure\n```\nsrc/\n├── smithery-entry.ts             # Smithery platform entry point\n├── worker.ts                     # Cloudflare Worker entry point\n├── durable-object.ts             # Durable Object implementation\n├── agent-worker-compatible.ts    # Worker-optimized MCP agent\n├── main.ts                       # Node.js server entry point\n├── index.ts                      # Legacy Node.js entry point\n└── services/                     # Email, Stripe services\n```\n\n## Installing via Smithery\n\n### Complete Advanced PocketBase Server with 100+ Tools\n\nThe Smithery deployment now includes the **complete comprehensive agent** with all advanced features:\n\n### 🎯 All Available Tool Categories (100+ Tools Total):\n- **🗃️ PocketBase Collections Management** (30+ tools): Create, manage, and migrate collections with full schema support\n- **📊 PocketBase Records CRUD** (20+ tools): Complete record operations with advanced querying and batch processing  \n- **🔐 PocketBase Authentication** (15+ tools): User management, OAuth2, OTP, admin operations, and super admin authentication\n- **⚡ PocketBase Real-time & WebSocket** (10+ tools): Live data streaming, subscriptions, and real-time updates\n- **💳 Stripe Payment Processing** (25+ tools): Complete payment infrastructure with customers, products, subscriptions, and webhooks\n- **📧 Email & Communication** (15+ tools): SMTP, SendGrid, template management, and automated email workflows\n- **🤖 SaaS Automation Workflows** (10+ tools): End-to-end business process automation\n- **🔧 Utility & Diagnostic Tools** (10+ tools): Health checks, monitoring, and troubleshooting\n\n### Option 1: Direct Installation (Recommended)\nTo install the **complete Advanced PocketBase Server** with **100+ tools** for Claude Desktop automatically via [Smithery](https://smithery.ai/server/pocketbase-server):\n\n```bash\nnpx -y @smithery/cli install pocketbase-server --client claude\n```\n\n### What You Get with Smithery Deployment\n- 🗄️ **PocketBase CRUD Operations** (30+ tools) - Complete database management\n- 🔐 **Admin & Authentication Tools** (20+ tools) - User management and security  \n- ⚡ **Real-time & WebSocket Tools** (10+ tools) - Live data streaming\n- 💳 **Stripe Payment Processing** (25+ tools) - Complete payment workflows\n- 📧 **Email & Communication Tools** (15+ tools) - Email templates and notifications\n- 🛠️ **Utility & Diagnostic Tools** (10+ tools) - System monitoring and debugging\n- 📚 **Resources & Prompts** - Enhanced AI interactions with examples\n\n### Option 2: Web Platform Deployment\n1. Visit [Smithery PocketBase Server](https://smithery.ai/server/pocketbase-server)\n2. Click the \"Deploy\" button\n3. Connect your GitHub account and configure settings\n4. Use the web playground to test your server\n\n### Option 3: Development with Smithery CLI\nFor developers who want to modify the server:\n\n```bash\n# Install Smithery CLI\nnpm install -g @smithery/cli\n\n# Clone and develop\ngit clone https://github.com/your-username/advanced-pocketbase-mcp-server\ncd advanced-pocketbase-mcp-server\nnpm install\n\n# Start development server with hot reload\nnpm run smithery:dev\n\n# Build for production\nnpm run smithery:build\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pocketbase",
        "databases",
        "database",
        "pocketbase databases",
        "advanced pocketbase",
        "pocketbase mcp"
      ],
      "category": "databases"
    },
    "DynamicEndpoints--supabase-mcp": {
      "owner": "DynamicEndpoints",
      "name": "supabase-mcp",
      "url": "https://github.com/DynamicEndpoints/supabase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Connects to Supabase databases, enabling database operations, storage management for files and assets, and invocation of edge functions.",
      "stars": 44,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-30T03:25:19Z",
      "readme_content": "# Supabase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/supabase-server)](https://smithery.ai/server/supabase-server)\nA Model Context Protocol (MCP) server that provides comprehensive tools for interacting with Supabase databases, storage, and edge functions. This server enables seamless integration between Supabase services and MCP-compatible applications.\n\n<a href=\"https://glama.ai/mcp/servers/vwi6nt8i80\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/vwi6nt8i80/badge\" alt=\"supabase-mcp MCP server\" /></a>\n\n## Overview\n\nThe Supabase MCP server acts as a bridge between MCP clients and Supabase's suite of services, providing:\n\n- Database operations with rich querying capabilities\n- Storage management for files and assets\n- Edge function invocation\n- Project and organization management\n- User authentication and management\n- Role-based access control\n\n## Architecture\n\nThe server is built using TypeScript and follows a modular architecture:\n\n```\nsupabase-server/\n├── src/\n│   ├── index.ts              # Main server implementation\n│   └── types/\n│       └── supabase.d.ts     # Type definitions\n├── package.json\n├── tsconfig.json\n├── config.json.example       # Example configuration file\n└── .env.example             # Environment variables template\n```\n\n### Key Components\n\n- **Server Class**: Implements the MCP server interface and handles all client requests\n- **Type Definitions**: Comprehensive TypeScript definitions for all operations\n- **Environment Configuration**: Secure configuration management via environment variables\n- **Error Handling**: Robust error handling with detailed error messages\n\n## Prerequisites\n\n- Node.js 16.x or higher\n- A Supabase project with:\n  - Project URL\n  - Service Role Key (for admin operations)\n  - Access Token (for management operations)\n- MCP-compatible client\n\n## Installation\n\n### Installing via Smithery\n\nTo install Supabase Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/supabase-server):\n\n```bash\nnpx -y @smithery/cli install supabase-server --client claude\n```\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/DynamicEndpoints/supabase-mcp.git\ncd supabase-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create environment configuration:\n```bash\ncp .env.example .env\n```\n\n4. Configure environment variables:\n```bash\nSUPABASE_URL=your_project_url_here\nSUPABASE_KEY=your_service_role_key_here\nSUPABASE_ACCESS_TOKEN=your_access_token_here  # Required for management operations\n```\n\n5. Create server configuration:\n```bash\ncp config.json.example config.json\n```\n\n6. Build the server:\n```bash\nnpm run build\n```\n\n## Configuration\n\nThe server supports extensive configuration through both environment variables and a config.json file. Here's a detailed breakdown of the configuration options:\n\n### Server Configuration\n```json\n{\n  \"server\": {\n    \"name\": \"supabase-server\",    // Server name\n    \"version\": \"0.1.0\",           // Server version\n    \"port\": 3000,                 // Port number (if running standalone)\n    \"host\": \"localhost\"           // Host address (if running standalone)\n  }\n}\n```\n\n### Supabase Configuration\n```json\n{\n  \"supabase\": {\n    \"project\": {\n      \"url\": \"your_project_url\",\n      \"key\": \"your_service_role_key\",\n      \"accessToken\": \"your_access_token\"\n    },\n    \"storage\": {\n      \"defaultBucket\": \"public\",           // Default storage bucket\n      \"maxFileSize\": 52428800,            // Max file size in bytes (50MB)\n      \"allowedMimeTypes\": [               // Allowed file types\n        \"image/*\",\n        \"application/pdf\",\n        \"text/*\"\n      ]\n    },\n    \"database\": {\n      \"maxConnections\": 10,               // Max DB connections\n      \"timeout\": 30000,                   // Query timeout in ms\n      \"ssl\": true                         // SSL connection\n    },\n    \"auth\": {\n      \"autoConfirmUsers\": false,          // Auto-confirm new users\n      \"disableSignup\": false,             // Disable public signups\n      \"jwt\": {\n        \"expiresIn\": \"1h\",               // Token expiration\n        \"algorithm\": \"HS256\"              // JWT algorithm\n      }\n    }\n  }\n}\n```\n\n### Logging Configuration\n```json\n{\n  \"logging\": {\n    \"level\": \"info\",                      // Log level\n    \"format\": \"json\",                     // Log format\n    \"outputs\": [\"console\", \"file\"],       // Output destinations\n    \"file\": {\n      \"path\": \"logs/server.log\",          // Log file path\n      \"maxSize\": \"10m\",                   // Max file size\n      \"maxFiles\": 5                       // Max number of files\n    }\n  }\n}\n```\n\n### Security Configuration\n```json\n{\n  \"security\": {\n    \"cors\": {\n      \"enabled\": true,\n      \"origins\": [\"*\"],\n      \"methods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"],\n      \"allowedHeaders\": [\"Content-Type\", \"Authorization\"]\n    },\n    \"rateLimit\": {\n      \"enabled\": true,\n      \"windowMs\": 900000,                 // 15 minutes\n      \"max\": 100                          // Max requests per window\n    }\n  }\n}\n```\n\n### Monitoring Configuration\n```json\n{\n  \"monitoring\": {\n    \"enabled\": true,\n    \"metrics\": {\n      \"collect\": true,\n      \"interval\": 60000                   // Collection interval in ms\n    },\n    \"health\": {\n      \"enabled\": true,\n      \"path\": \"/health\"                   // Health check endpoint\n    }\n  }\n}\n```\n\nSee `config.json.example` for a complete example configuration file.\n\n## MCP Integration\n\nAdd the server to your MCP settings (cline_mcp_settings.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/supabase-server/build/index.js\"],\n      \"env\": {\n        \"SUPABASE_URL\": \"your_project_url\",\n        \"SUPABASE_KEY\": \"your_service_role_key\",\n        \"SUPABASE_ACCESS_TOKEN\": \"your_access_token\"\n      },\n      \"config\": \"path/to/config.json\"  // Optional: path to configuration file\n    }\n  }\n}\n```\n\n## Available Tools\n\n### Database Operations\n\n#### create_record\nCreate a new record in a table with support for returning specific fields.\n\n```typescript\n{\n  table: string;\n  data: Record<string, any>;\n  returning?: string[];\n}\n```\n\nExample:\n```typescript\n{\n  table: \"users\",\n  data: {\n    name: \"John Doe\",\n    email: \"john@example.com\"\n  },\n  returning: [\"id\", \"created_at\"]\n}\n```\n\n#### read_records\nRead records with advanced filtering, joins, and field selection.\n\n```typescript\n{\n  table: string;\n  select?: string[];\n  filter?: Record<string, any>;\n  joins?: Array<{\n    type?: 'inner' | 'left' | 'right' | 'full';\n    table: string;\n    on: string;\n  }>;\n}\n```\n\nExample:\n```typescript\n{\n  table: \"posts\",\n  select: [\"id\", \"title\", \"user.name\"],\n  filter: { published: true },\n  joins: [{\n    type: \"left\",\n    table: \"users\",\n    on: \"posts.user_id=users.id\"\n  }]\n}\n```\n\n#### update_record\nUpdate records with filtering and returning capabilities.\n\n```typescript\n{\n  table: string;\n  data: Record<string, any>;\n  filter?: Record<string, any>;\n  returning?: string[];\n}\n```\n\nExample:\n```typescript\n{\n  table: \"users\",\n  data: { status: \"active\" },\n  filter: { email: \"john@example.com\" },\n  returning: [\"id\", \"status\", \"updated_at\"]\n}\n```\n\n#### delete_record\nDelete records with filtering and returning capabilities.\n\n```typescript\n{\n  table: string;\n  filter?: Record<string, any>;\n  returning?: string[];\n}\n```\n\nExample:\n```typescript\n{\n  table: \"posts\",\n  filter: { status: \"draft\" },\n  returning: [\"id\", \"title\"]\n}\n```\n\n### Storage Operations\n\n#### upload_file\nUpload files to Supabase Storage with configurable options.\n\n```typescript\n{\n  bucket: string;\n  path: string;\n  file: File | Blob;\n  options?: {\n    cacheControl?: string;\n    contentType?: string;\n    upsert?: boolean;\n  };\n}\n```\n\nExample:\n```typescript\n{\n  bucket: \"avatars\",\n  path: \"users/123/profile.jpg\",\n  file: imageBlob,\n  options: {\n    contentType: \"image/jpeg\",\n    upsert: true\n  }\n}\n```\n\n#### download_file\nDownload files from Supabase Storage.\n\n```typescript\n{\n  bucket: string;\n  path: string;\n}\n```\n\nExample:\n```typescript\n{\n  bucket: \"documents\",\n  path: \"reports/annual-2023.pdf\"\n}\n```\n\n### Edge Functions\n\n#### invoke_function\nInvoke Supabase Edge Functions with parameters and custom options.\n\n```typescript\n{\n  function: string;\n  params?: Record<string, any>;\n  options?: {\n    headers?: Record<string, string>;\n    responseType?: 'json' | 'text' | 'arraybuffer';\n  };\n}\n```\n\nExample:\n```typescript\n{\n  function: \"process-image\",\n  params: {\n    url: \"https://example.com/image.jpg\",\n    width: 800\n  },\n  options: {\n    responseType: \"json\"\n  }\n}\n```\n\n### User Management\n\n#### list_users\nList users with pagination support.\n\n```typescript\n{\n  page?: number;\n  per_page?: number;\n}\n```\n\n#### create_user\nCreate a new user with metadata.\n\n```typescript\n{\n  email: string;\n  password: string;\n  data?: Record<string, any>;\n}\n```\n\n#### update_user\nUpdate user details.\n\n```typescript\n{\n  user_id: string;\n  email?: string;\n  password?: string;\n  data?: Record<string, any>;\n}\n```\n\n#### delete_user\nDelete a user.\n\n```typescript\n{\n  user_id: string;\n}\n```\n\n#### assign_user_role\nAssign a role to a user.\n\n```typescript\n{\n  user_id: string;\n  role: string;\n}\n```\n\n#### remove_user_role\nRemove a role from a user.\n\n```typescript\n{\n  user_id: string;\n  role: string;\n}\n```\n\n## Error Handling\n\nThe server provides detailed error messages for common scenarios:\n\n- Invalid parameters\n- Authentication failures\n- Permission issues\n- Rate limiting\n- Network errors\n- Database constraints\n\nErrors are returned in a standardized format:\n\n```typescript\n{\n  code: ErrorCode;\n  message: string;\n  details?: any;\n}\n```\n\n## Development\n\n### Running Tests\n```bash\nnpm test\n```\n\n### Building\n```bash\nnpm run build\n```\n\n### Linting\n```bash\nnpm run lint\n```\n\n### Running evals \n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/index.ts\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nMIT License - see LICENSE for details\n\n## Support\n\nFor support, please:\n\n1. Check the [issues](https://github.com/DynamicEndpoints/supabase-mcp/issues) for existing problems/solutions\n2. Create a new issue with detailed reproduction steps\n3. Include relevant error messages and environment details",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase databases",
        "supabase mcp",
        "connects supabase"
      ],
      "category": "databases"
    },
    "EdenYavin--OSV-MCP": {
      "owner": "EdenYavin",
      "name": "OSV-MCP",
      "url": "https://github.com/EdenYavin/OSV-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/EdenYavin.webp",
      "description": "Fetch and manage vulnerability data for software packages, providing detailed information on CVEs, affected versions, and fixes to improve security measures.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-23T17:22:44Z",
      "readme_content": "# MCP Server For OSV \n\nA lightweight MCP (Model Context Protocol) server for OSV Database API.\n\nExample:\n\n[](https://github.com/user-attachments/assets/e074c1d2-c6b6-4c9f-b9da-ffb27bfe90a7)\n\n\n---\n## Tools Provided\n\n### Overview\n|name|description|\n|---|---|\n|query_package_cve|List all the CVE IDs for a specific package. Specific version can be passed as well for more narrow scope CVE IDs.|\n|query_for_cve_affected|Query the OSV database for a CVE and return all affected versions of the package.|\n|query_for_cve_fix_versions|Query the OSV database for a CVE and return all versions that fix the vulnerability.|\n|get_ecosystems|Query the MCP for current supported ecosystems.\n\n### Detailed Description\n\n- **query_package_cve**\n  - Query the OSV database for a package and return the CVE IDs.\n  - Input parameters:\n    - `package` (string, required): The package name to query\n    - `version` (string, optional): The version of the package to query. If not specified, queries all versions\n    - `ecosystem` (string, optional): The ecosystem of the package. Defaults to \"PyPI\" for Python packages\n  - Returns a list of CVE IDs with their details\n\n- **query_for_cve_affected**\n  - Query the OSV database for a CVE and return all affected versions.\n  - Input parameters:\n    - `cve` (string, required): The CVE ID to query (e.g., \"CVE-2018-1000805\")\n  - Returns a list of affected version strings\n\n- **query_for_cve_fix_versions**\n  - Query the OSV database for a CVE and return all versions that fix the vulnerability.\n  - Input parameters:\n    - `cve` (string, required): The CVE ID to query (e.g., \"CVE-2018-1000805\")\n  - Returns a list of fixed version strings\n\n- **get_ecosystems**\n  - Query for all current supported ecosystems by the MCP servers.\n  - Return a dict with the key being the ecosystem name and the value the programming language / OS.\n\n---\n\n## Prerequisites\n\n1. **Python 3.11 or higher**: This project requires Python 3.11 or newer.\n   ```bash\n   # Check your Python version\n   python --version\n   ```\n\n2. **Install uv**: A fast Python package installer and resolver.\n   ```bash\n   pip install uv\n   ```\n   Or use Homebrew:\n   ```bash\n   brew install uv\n   ```\n\n---\n\n## Tested on\n\n- [X] Cursor\n- [X] Claude\n\n---\n## Installation\n\n\n1. Via [Smithery](https://smithery.ai/server/@EdenYavin/OSV-MCP):\n```bash\nnpx -y @smithery/cli install @EdenYavin/OSV-MCP --client claude\n```\n\n2. Locally:\n\n    1. Clone the repo: ```https://github.com/EdenYavin/OSV-MCP.git```\n    2. Configure your MCP Host (Cusrsor / Claude Desktop etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"osv-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path-to/OSV-MCP\", \"run\", \"osv-server\"],\n      \"env\": {}\n    }\n  }\n}\n\n```\n\n---\n\n**Leave a review on [VibeApp](https://www.vibeapp.store/app/vulnerability-osv-mcp-server)\nif you enjoyed it :)!**",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "osv",
        "database",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "FalkorDB--FalkorDB-MCPServer": {
      "owner": "FalkorDB",
      "name": "FalkorDB-MCPServer",
      "url": "https://github.com/FalkorDB/FalkorDB-MCPServer",
      "imageUrl": "/freedevtools/mcp/pfp/FalkorDB.webp",
      "description": "Connects AI models with FalkorDB graph databases to facilitate querying and interaction through the Model Context Protocol. Translates and routes requests to FalkorDB while formatting responses per MCP standards.",
      "stars": 19,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T00:46:46Z",
      "readme_content": "# FalkorDB MCP Server\n\nA Model Context Protocol (MCP) server for FalkorDB, allowing AI models to query and interact with graph databases.\n\n## Overview\n\nThis project implements a server that follows the Model Context Protocol (MCP) specification to connect AI models with FalkorDB graph databases. The server translates and routes MCP requests to FalkorDB and formats the responses according to the MCP standard.\n\n## Prerequisites\n\n* Node.js (v16 or later)\n* npm or yarn\n* FalkorDB instance (can be run locally or remotely)\n\n## Installation\n\n1. Clone this repository:\n\n   ```bash\n   git clone https://github.com/falkordb/falkordb-mcpserver.git\n   cd falkordb-mcpserver\n   ```\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n3. Copy the example environment file and configure it:\n\n   ```bash\n   cp .env.example .env\n   ```\n\n   Edit `.env` with your configuration details.\n\n## Configuration\n\nConfiguration is managed through environment variables in the `.env` file:\n\n* `PORT`: Server port (default: 3000)\n* `NODE_ENV`: Environment (development, production)\n* `FALKORDB_HOST`: FalkorDB host (default: localhost)\n* `FALKORDB_PORT`: FalkorDB port (default: 6379)\n* `FALKORDB_USERNAME`: Username for FalkorDB authentication (if required)\n* `FALKORDB_PASSWORD`: Password for FalkorDB authentication (if required)\n* `MCP_API_KEY`: API key for authenticating MCP requests\n\n## Usage\n\n### Development\n\nStart the development server with hot-reloading:\n\n```bash\nnpm run dev\n```\n\n### Production\n\nBuild and start the server:\n\n```bash\nnpm run build\nnpm start\n```\n\n## API Endpoints\n\n* `GET /api/mcp/metadata`: Get metadata about the FalkorDB instance and available capabilities\n* `POST /api/mcp/context`: Execute queries against FalkorDB\n* `GET /api/mcp/health`: Check server health\n* `GET /api/mcp/graphs`: Returns the list of Graphs\n* \n\n## MCP Configuration\n\nTo use this server with MCP clients, you can add it to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"falkordb\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-p\", \"3000:3000\",\n        \"--env-file\", \".env\",\n        \"falkordb-mcpserver\",\n        \"falkordb://host.docker.internal:6379\"\n      ]\n    }\n  }\n}\n```\n\nFor client-side configuration:\n\n```json\n{\n  \"defaultServer\": \"falkordb\",\n  \"servers\": {\n    \"falkordb\": {\n      \"url\": \"http://localhost:3000/api/mcp\",\n      \"apiKey\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "falkordb",
        "access falkordb",
        "databases facilitate",
        "secure database"
      ],
      "category": "databases"
    },
    "FocusSearch--focus_mcp_sql": {
      "owner": "FocusSearch",
      "name": "focus_mcp_sql",
      "url": "https://github.com/FocusSearch/focus_mcp_sql",
      "imageUrl": "/freedevtools/mcp/pfp/FocusSearch.webp",
      "description": "Converts natural language into SQL statements using a two-step generation process to minimize inaccuracies and ensure reliability for non-technical users.",
      "stars": 33,
      "forks": 4,
      "license": "Apache License 2.0",
      "language": "Java",
      "updated_at": "2025-08-26T06:01:07Z",
      "readme_content": "# FOCUS DATA MCP Server [[中文](./README_CN.md)]\n\nA Model Context Protocol (MCP) server enables artificial intelligence assistants to convert natural language into SQL statements.\n\n# There are already so many Text-to-SQL frameworks. Why do we still need another one?\n\nIn simple terms, focus_mcp_sql adopts a two-step SQL generation solution, which enables control over the hallucinations of LLM and truly builds the trust of non-technical users in the generated SQL results.\n\nBelow is the comparison table between focus_mcp_sql and others:\n\n#### Comparison Analysis Table  \nHere’s a side-by-side comparison of focus_mcp_sql with other LLM-based frameworks:\n\n| Feature            | Traditional LLM Frameworks | focus_mcp_sql              |\n|--------------------|----------------------------|----------------------------|\n| Generation Process | Black box, direct SQL generation | Transparent, two-step (keywords + SQL) |\n| Hallucination Risk | High, depends on model quality | Low, controllable (keyword verification) |\n| Speed              | Slow, relies on large model inference | Fast, deterministic keyword-to-SQL |\n| Cost               | High, requires advanced models | Low, reduces reliance on large models |\n| Non-Technical User Friendliness | Low, hard to verify results | High, easy keyword checking |\n\n## Features\n\n-Initialize the model\n-Convert natural language to SQL statements\n\n## Prerequisites\n\n- jdk 23 or higher. Download [jdk](https://www.oracle.com/java/technologies/downloads/)\n- gradle 8.12 or higher. Download [gradle](https://gradle.org/install/)\n- register [Datafocus](https://www.datafocus.ai/) to obtain bearer token: \n    1. Register an account in [Datafocus](https://www.datafocus.ai/)\n    2. Create an application\n    3. Enter the application\n    4. Admin -> Interface authentication -> Bearer Token -> New Bearer Token\n       \n\n## Installation\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/FocusSearch/focus_mcp_sql.git\ncd focus_mcp_sql\n```\n\n2. Build the server:\n\n```bash\ngradle clean\ngradle bootJar\n\nThe jar path: build/libs/focus_mcp_sql.jar\n```\n\n## MCP Configuration\n\nAdd the server to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"focus_mcp_data\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-jar\",\n        \"path/to/focus_mcp_sql/focus_mcp_sql.jar\"\n      ],\n      \"autoApprove\": [\n        \"gptText2sqlStart\",\n        \"gptText2sqlChat\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. gptText2sqlStart\n\ninitial model.\n\n**Parameters:**\n\n- `model` (required): table model\n- `bearer` (required): bearer token\n- `language` (optional): language ['english','chinese']\n\n**Example:**\n\n```json\n{\n  \"model\": {\n    \"tables\": [\n      {\n        \"columns\": [\n          {\n            \"columnDisplayName\": \"name\",\n            \"dataType\": \"string\",\n            \"aggregation\": \"\",\n            \"columnName\": \"name\"\n          },\n          {\n            \"columnDisplayName\": \"address\",\n            \"dataType\": \"string\",\n            \"aggregation\": \"\",\n            \"columnName\": \"address\"\n          },\n          {\n            \"columnDisplayName\": \"age\",\n            \"dataType\": \"int\",\n            \"aggregation\": \"SUM\",\n            \"columnName\": \"age\"\n          },\n          {\n            \"columnDisplayName\": \"date\",\n            \"dataType\": \"timestamp\",\n            \"aggregation\": \"\",\n            \"columnName\": \"date\"\n          }\n        ],\n        \"tableDisplayName\": \"test\",\n        \"tableName\": \"test\"\n      }\n    ],\n    \"relations\": [\n\n    ],\n    \"type\": \"mysql\",\n    \"version\": \"8.0\"\n  },\n  \"bearer\": \"ZTllYzAzZjM2YzA3NDA0ZGE3ZjguNDJhNDjNGU4NzkyYjY1OTY0YzUxYWU5NmU=\"\n}\n```\n\nmodel 参数说明：\n\n|名称|位置|类型|必选|说明|\n|---|---|---|---|---|\n| model|body|object| 是 |none|\n|» type|body|string| 是 |数据库类型|\n|» version|body|string| 是 |数据库版本|\n|» tables|body|[object]| 是 |表结构列表|\n|»» tableDisplayName|body|string| 否 |表显示名|\n|»» tableName|body|string| 否 |表原始名|\n|»» columns|body|[object]| 否 |表列列表|\n|»»» columnDisplayName|body|string| 是 |列显示名|\n|»»» columnName|body|string| 是 |列原始名|\n|»»» dataType|body|string| 是 |列数据类型|\n|»»» aggregation|body|string| 是 |列聚合方式|\n|» relations|body|[object]| 是 |表关联关系列表|\n|»» conditions|body|[object]| 否 |关联条件|\n|»»» dstColName|body|string| 否 |dimension 表关联列原始名|\n|»»» srcColName|body|string| 否 |fact 表关联列原始名|\n|»» dimensionTable|body|string| 否 |dimension 表原始名|\n|»» factTable|body|string| 否 |fact 表原始名|\n|»» joinType|body|string| 否 |关联类型|\n\n### 2. gptText2sqlChat\n\nConvert natural language to SQL.\n\n**Parameters:**\n\n- `chatId` (required): chat id\n- `input` (required): Natural language\n- `bearer` (required): bearer token\n\n**Example:**\n\n```json\n{\n  \"chatId\": \"03975af5de4b4562938a985403f206d4\",\n  \"input\": \"what is the max age\",\n  \"bearer\": \"ZTllYzAzZjM2YzA3NDA0ZGE3ZjguNDJhNDjNGU4NzkyYjY1OTY0YzUxYWU5NmU=\"\n}\n```\n\n## Response Format\n\nAll tools return responses in the following format:\n\n```json\n{\n  \"errCode\": 0,\n  \"exception\": \"\",\n  \"msgParams\": null,\n  \"promptMsg\": null,\n  \"success\": true,\n  \"data\": {\n  }\n}\n```\n\n## Visual Studio Code Cline Sample\n\n1. vsCode install cline plugin\n2. mcp server config\n   \n3. use\n    1. initial model\n       \n       \n    2. transfer: what is the max age\n       \n\n## Contact：\n[https://discord.gg/mFa3yeq9](https://discord.gg/AVufPnpaad )",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "focus_mcp_sql",
        "focussearch",
        "databases",
        "focussearch focus_mcp_sql",
        "enables querying",
        "focus_mcp_sql converts"
      ],
      "category": "databases"
    },
    "FreePeak--db-mcp-server": {
      "owner": "FreePeak",
      "name": "db-mcp-server",
      "url": "https://github.com/FreePeak/db-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/FreePeak.webp",
      "description": "Connect and interact with multiple databases to execute SQL queries, manage transactions, and analyze performance through a unified interface for AI applications.",
      "stars": 295,
      "forks": 42,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-30T02:31:25Z",
      "readme_content": "<div align=\"center\">\n\n\n\n# Multi Database MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Go Report Card](https://goreportcard.com/badge/github.com/FreePeak/db-mcp-server)](https://goreportcard.com/report/github.com/FreePeak/db-mcp-server)\n[![Go Reference](https://pkg.go.dev/badge/github.com/FreePeak/db-mcp-server.svg)](https://pkg.go.dev/github.com/FreePeak/db-mcp-server)\n[![Contributors](https://img.shields.io/github/contributors/FreePeak/db-mcp-server)](https://github.com/FreePeak/db-mcp-server/graphs/contributors)\n\n<h3>A powerful multi-database server implementing the Model Context Protocol (MCP) to provide AI assistants with structured access to databases.</h3>\n\n<div class=\"toc\">\n  <a href=\"#overview\">Overview</a> •\n  <a href=\"#core-concepts\">Core Concepts</a> •\n  <a href=\"#features\">Features</a> •\n  <a href=\"#supported-databases\">Supported Databases</a> •\n  <a href=\"#deployment-options\">Deployment Options</a> •\n  <a href=\"#configuration\">Configuration</a> •\n  <a href=\"#available-tools\">Available Tools</a> •\n  <a href=\"#examples\">Examples</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#contributing\">Contributing</a>\n</div>\n\n</div>\n\n## Overview\n\nThe DB MCP Server provides a standardized way for AI models to interact with multiple databases simultaneously. Built on the [FreePeak/cortex](https://github.com/FreePeak/cortex) framework, it enables AI assistants to execute SQL queries, manage transactions, explore schemas, and analyze performance across different database systems through a unified interface.\n\n## Core Concepts\n\n### Multi-Database Support\n\nUnlike traditional database connectors, DB MCP Server can connect to and interact with multiple databases concurrently:\n\n```json\n{\n  \"connections\": [\n    {\n      \"id\": \"mysql1\",\n      \"type\": \"mysql\",\n      \"host\": \"localhost\",\n      \"port\": 3306,\n      \"name\": \"db1\",\n      \"user\": \"user1\",\n      \"password\": \"password1\"\n    },\n    {\n      \"id\": \"postgres1\",\n      \"type\": \"postgres\",\n      \"host\": \"localhost\",\n      \"port\": 5432,\n      \"name\": \"db2\",\n      \"user\": \"user2\",\n      \"password\": \"password2\"\n    }\n  ]\n}\n```\n\n### Dynamic Tool Generation\n\nFor each connected database, the server automatically generates specialized tools:\n\n```go\n// For a database with ID \"mysql1\", these tools are generated:\nquery_mysql1       // Execute SQL queries\nexecute_mysql1     // Run data modification statements\ntransaction_mysql1 // Manage transactions\nschema_mysql1      // Explore database schema\nperformance_mysql1 // Analyze query performance\n```\n\n### Clean Architecture\n\nThe server follows Clean Architecture principles with these layers:\n\n1. **Domain Layer**: Core business entities and interfaces\n2. **Repository Layer**: Data access implementations\n3. **Use Case Layer**: Application business logic\n4. **Delivery Layer**: External interfaces (MCP tools)\n\n## Features\n\n- **Simultaneous Multi-Database Support**: Connect to multiple MySQL and PostgreSQL databases concurrently\n- **Database-Specific Tool Generation**: Auto-creates specialized tools for each connected database\n- **Clean Architecture**: Modular design with clear separation of concerns\n- **OpenAI Agents SDK Compatibility**: Full compatibility for seamless AI assistant integration\n- **Dynamic Database Tools**: Execute queries, run statements, manage transactions, explore schemas, analyze performance\n- **Unified Interface**: Consistent interaction patterns across different database types\n- **Connection Management**: Simple configuration for multiple database connections\n\n## Supported Databases\n\n| Database   | Status                    | Features                                                     |\n| ---------- | ------------------------- | ------------------------------------------------------------ |\n| MySQL      | ✅ Full Support           | Queries, Transactions, Schema Analysis, Performance Insights |\n| PostgreSQL | ✅ Full Support (v9.6-17) | Queries, Transactions, Schema Analysis, Performance Insights |\n| TimescaleDB| ✅ Full Support           | Hypertables, Time-Series Queries, Continuous Aggregates, Compression, Retention Policies |\n\n## Deployment Options\n\nThe DB MCP Server can be deployed in multiple ways to suit different environments and integration needs:\n\n### Docker Deployment\n\n```bash\n# Pull the latest image\ndocker pull freepeak/db-mcp-server:latest\n\n# Run with mounted config file\ndocker run -p 9092:9092 \\\n  -v $(pwd)/config.json:/app/my-config.json \\\n  -e TRANSPORT_MODE=sse \\\n  -e CONFIG_PATH=/app/my-config.json \\\n  freepeak/db-mcp-server\n```\n\n> **Note**: Mount to `/app/my-config.json` as the container has a default file at `/app/config.json`.\n\n### STDIO Mode (IDE Integration)\n\n```bash\n# Run the server in STDIO mode\n./bin/server -t stdio -c config.json\n```\n\nFor Cursor IDE integration, add to `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"stdio-db-mcp-server\": {\n      \"command\": \"/path/to/db-mcp-server/server\",\n      \"args\": [\"-t\", \"stdio\", \"-c\", \"/path/to/config.json\"]\n    }\n  }\n}\n```\n\n### SSE Mode (Server-Sent Events)\n\n```bash\n# Default configuration (localhost:9092)\n./bin/server -t sse -c config.json\n\n# Custom host and port\n./bin/server -t sse -host 0.0.0.0 -port 8080 -c config.json\n```\n\nClient connection endpoint: `http://localhost:9092/sse`\n\n### Source Code Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/FreePeak/db-mcp-server.git\ncd db-mcp-server\n\n# Build the server\nmake build\n\n# Run the server\n./bin/server -t sse -c config.json\n```\n\n## Configuration\n\n### Database Configuration File\n\nCreate a `config.json` file with your database connections:\n\n```json\n{\n  \"connections\": [\n    {\n      \"id\": \"mysql1\",\n      \"type\": \"mysql\",\n      \"host\": \"mysql1\",\n      \"port\": 3306,\n      \"name\": \"db1\",\n      \"user\": \"user1\",\n      \"password\": \"password1\",\n      \"query_timeout\": 60,\n      \"max_open_conns\": 20,\n      \"max_idle_conns\": 5,\n      \"conn_max_lifetime_seconds\": 300,\n      \"conn_max_idle_time_seconds\": 60\n    },\n    {\n      \"id\": \"postgres1\",\n      \"type\": \"postgres\",\n      \"host\": \"postgres1\",\n      \"port\": 5432,\n      \"name\": \"db1\",\n      \"user\": \"user1\",\n      \"password\": \"password1\"\n    }\n  ]\n}\n```\n\n### Command-Line Options\n\n```bash\n# Basic syntax\n./bin/server -t <transport> -c <config-file>\n\n# SSE transport options\n./bin/server -t sse -host <hostname> -port <port> -c <config-file>\n\n# Inline database configuration\n./bin/server -t stdio -db-config '{\"connections\":[...]}'\n\n# Environment variable configuration\nexport DB_CONFIG='{\"connections\":[...]}'\n./bin/server -t stdio\n```\n\n## Available Tools\n\nFor each connected database, DB MCP Server automatically generates these specialized tools:\n\n### Query Tools\n\n| Tool Name | Description |\n|-----------|-------------|\n| `query_<db_id>` | Execute SELECT queries and get results as a tabular dataset |\n| `execute_<db_id>` | Run data manipulation statements (INSERT, UPDATE, DELETE) |\n| `transaction_<db_id>` | Begin, commit, and rollback transactions |\n\n### Schema Tools\n\n| Tool Name | Description |\n|-----------|-------------|\n| `schema_<db_id>` | Get information about tables, columns, indexes, and foreign keys |\n| `generate_schema_<db_id>` | Generate SQL or code from database schema |\n\n### Performance Tools\n\n| Tool Name | Description |\n|-----------|-------------|\n| `performance_<db_id>` | Analyze query performance and get optimization suggestions |\n\n### TimescaleDB Tools\n\nFor PostgreSQL databases with TimescaleDB extension, these additional specialized tools are available:\n\n| Tool Name | Description |\n|-----------|-------------|\n| `timescaledb_<db_id>` | Perform general TimescaleDB operations |\n| `create_hypertable_<db_id>` | Convert a standard table to a TimescaleDB hypertable |\n| `list_hypertables_<db_id>` | List all hypertables in the database |\n| `time_series_query_<db_id>` | Execute optimized time-series queries with bucketing |\n| `time_series_analyze_<db_id>` | Analyze time-series data patterns |\n| `continuous_aggregate_<db_id>` | Create materialized views that automatically update |\n| `refresh_continuous_aggregate_<db_id>` | Manually refresh continuous aggregates |\n\nFor detailed documentation on TimescaleDB tools, see [TIMESCALEDB_TOOLS.md](docs/TIMESCALEDB_TOOLS.md).\n\n## Examples\n\n### Querying Multiple Databases\n\n```sql\n-- Query the first database\nquery_mysql1(\"SELECT * FROM users LIMIT 10\")\n\n-- Query the second database in the same context\nquery_postgres1(\"SELECT * FROM products WHERE price > 100\")\n```\n\n### Managing Transactions\n\n```sql\n-- Start a transaction\ntransaction_mysql1(\"BEGIN\")\n\n-- Execute statements within the transaction\nexecute_mysql1(\"INSERT INTO orders (customer_id, product_id) VALUES (1, 2)\")\nexecute_mysql1(\"UPDATE inventory SET stock = stock - 1 WHERE product_id = 2\")\n\n-- Commit or rollback\ntransaction_mysql1(\"COMMIT\")\n-- OR\ntransaction_mysql1(\"ROLLBACK\")\n```\n\n### Exploring Database Schema\n\n```sql\n-- Get all tables in the database\nschema_mysql1(\"tables\")\n\n-- Get columns for a specific table\nschema_mysql1(\"columns\", \"users\")\n\n-- Get constraints\nschema_mysql1(\"constraints\", \"orders\")\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **Connection Failures**: Verify network connectivity and database credentials\n- **Permission Errors**: Ensure the database user has appropriate permissions\n- **Timeout Issues**: Check the `query_timeout` setting in your configuration\n\n### Logs\n\nEnable verbose logging for troubleshooting:\n\n```bash\n./bin/server -t sse -c config.json -v\n```\n\n## Contributing\n\nWe welcome contributions to the DB MCP Server project! To contribute:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'feat: add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease see our [CONTRIBUTING.md](docs/CONTRIBUTING.md) file for detailed guidelines.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "db",
        "freepeak db",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "Gabriel-Maxsy--MCP-Car": {
      "owner": "Gabriel-Maxsy",
      "name": "MCP-Car",
      "url": "https://github.com/Gabriel-Maxsy/MCP-Car",
      "imageUrl": "/freedevtools/mcp/pfp/Gabriel-Maxsy.webp",
      "description": "Interact with a SQLite database to search for cars based on various criteria such as make, model, year, color, and price. This system facilitates efficient database queries to help users find the desired vehicle information.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-10T01:29:38Z",
      "readme_content": "# Projeto MCP\r\n\r\nEste é um projeto finalizado que utiliza o protocolo MCP (Model Context Protocol) para comunicação entre cliente e servidor. O projeto inclui a criação de um banco de dados SQLite e a implementação de um cliente que consulta esse banco.\r\n\r\n### Estrutura de pastas\r\n\r\n📁 mcp-car   \r\n│-- 📂 app  \r\n│   ├── server.py  # Arquivo do servidor MCP   \r\n│   ├── database.py  # Script responsável pela criaçãodo banco de dados  \r\n│-- 📂 client  \r\n│   ├── client.py  # Código principal do cliente onde ocorre interação  \r\n│-- 📂 utils  \r\n│   ├── create_cars.py  # Gera dados fictícios para o banco   \r\n│-- README.md  # Documentação do projeto\r\n\r\n### Como rodar o projeto\r\n\r\n1. **Criar e configurar o ambiente virtual**\r\n\r\n   - Para garantir que todas as dependências do projeto sejam instaladas corretamente, é recomendado criar um ambiente virtual. \r\n   - Na raiz do seu projeto, execute o seguinte comando para criar um ambiente virtual:\r\n\r\n      `python -m venv venv`\r\n   \r\n      Em seguida:\r\n      \r\n      `.\\venv\\Scripts\\activate`\r\n\r\n      Então para baixar as dependências:\r\n\r\n      `pip install -r requirements.txt`\r\n   - Isso instalará todas as bibliotecas que o projeto necessita para funcionar corretamente.\r\n\r\nAgora você pode seguir com o restante da configuração do projeto, já com o ambiente virtual pronto para uso.\r\n\r\n\r\n2. **Criar o banco de dados**\r\n   - Navegue até a pasta `app`.\r\n   - Abra o arquivo `database.py` e execute-o para criar o banco de dados no formato SQLite dentro da sua pasta \"data\". Este script criará a estrutura necessária para armazenar os dados dos carros.\r\n\r\n3. **Preencher o banco de dados com dados fictícios**\r\n   - Acesse a pasta `utils`.\r\n   - Abra o arquivo `create_cars.py` e execute-o para popular o banco de dados com 100 registros de carros fictícios. Esse passo é necessário para ter dados no banco antes de rodar o sistema.\r\n\r\n4. **Executar o cliente e consultar os dados**\r\n   - Com o banco de dados preenchido, vá até a pasta `client`.\r\n   - Execute o arquivo `client.py` para interagir com o sistema. O agente permitirá que você insira filtros (como marca, modelo, ano, etc.) para procurar carros no banco de dados.\r\n\r\n### Exemplo de uso\r\n\r\nAo rodar o cliente (`client.py`), você será solicitado a informar critérios de busca, como:\r\n\r\n- Marca\r\n- Modelo\r\n- Ano\r\n- Cor\r\n- Preço máximo\r\n\r\nO cliente enviará a consulta para o servidor, que realizará a busca no banco de dados e retornará os carros que atendem aos critérios fornecidos.\r\n\r\nVocê pode interromper a busca digitando **\"sair\"** a qualquer momento.\r\n\r\n---\r\n\r\n## Contribuições\r\n\r\nSinta-se à vontade para explorar e modificar o projeto conforme necessário. Caso tenha dúvidas ou queira sugerir melhorias, envie um pull request ou entre em contato.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "sqlite",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "GongRzhe--REDIS-MCP-Server": {
      "owner": "GongRzhe",
      "name": "REDIS-MCP-Server",
      "url": "https://github.com/GongRzhe/REDIS-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/GongRzhe.webp",
      "description": "Interact with Redis databases through standardized tools to facilitate communication between LLMs and Redis key-value stores.",
      "stars": 27,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T23:36:40Z",
      "readme_content": "# Redis MCP Server (@gongrzhe/server-redis-mcp@1.0.0)\n\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n[![smithery badge](https://smithery.ai/badge/@gongrzhe/server-redis-mcp)](https://smithery.ai/server/@gongrzhe/server-redis-mcp)\n\nA Redis Model Context Protocol (MCP) server implementation for interacting with Redis databases. This server enables LLMs to interact with Redis key-value stores through a set of standardized tools.\n\n## Update\n62 Redis MCP tools in https://github.com/GongRzhe/REDIS-MCP-Server/tree/redis-plus\n\n## Installation & Usage\n\n### Installing via Smithery\n\nTo install Redis MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gongrzhe/server-redis-mcp):\n\n```bash\nnpx -y @smithery/cli install @gongrzhe/server-redis-mcp --client claude\n```\n\n### Installing Manually\n```bash\n# Using npx with specific version (recommended)\nnpx @gongrzhe/server-redis-mcp@1.0.0 redis://your-redis-host:port\n\n# Example:\nnpx @gongrzhe/server-redis-mcp@1.0.0 redis://localhost:6379\n```\n\nOr install globally:\n\n```bash\n# Install specific version globally\nnpm install -g @gongrzhe/server-redis-mcp@1.0.0\n\n# Run after global installation\n@gongrzhe/server-redis-mcp redis://your-redis-host:port\n```\n\n## Components\n\n### Tools\n\n- **set**\n  - Set a Redis key-value pair with optional expiration\n  - Input:\n    - `key` (string): Redis key\n    - `value` (string): Value to store\n    - `expireSeconds` (number, optional): Expiration time in seconds\n\n- **get**\n  - Get value by key from Redis\n  - Input: `key` (string): Redis key to retrieve\n\n- **delete**\n  - Delete one or more keys from Redis\n  - Input: `key` (string | string[]): Key or array of keys to delete\n\n- **list**\n  - List Redis keys matching a pattern\n  - Input: `pattern` (string, optional): Pattern to match keys (default: *)\n\n## Configuration\n\n### Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@gongrzhe/server-redis-mcp@1.0.0\",\n        \"redis://localhost:6379\"\n      ]\n    }\n  }\n}\n```\n\nAlternatively, you can use the node command directly if you have the package installed:\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"path/to/build/index.js\",\n        \"redis://10.1.210.223:6379\"\n      ]\n    }\n  }\n}\n```\n\n### Docker Usage\n\nWhen using Docker:\n* For macOS, use `host.docker.internal` if the Redis server is running on the host network\n* Redis URL can be specified as an argument, defaults to \"redis://localhost:6379\"\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"mcp/redis\", \n        \"redis://host.docker.internal:6379\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n### Building from Source\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n### Docker Build\n\n```bash\ndocker build -t mcp/redis .\n```\n\n## License\n\nThis MCP server is licensed under the ISC License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "redis",
        "databases",
        "database",
        "redis databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "GreptimeTeam--greptimedb-mcp-server": {
      "owner": "GreptimeTeam",
      "name": "greptimedb-mcp-server",
      "url": "https://github.com/GreptimeTeam/greptimedb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/GreptimeTeam.webp",
      "description": "Enable secure exploration and analysis of GreptimeDB databases by listing tables, reading data, and executing SQL queries through a controlled interface.",
      "stars": 23,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-20T04:38:13Z",
      "readme_content": "# greptimedb-mcp-server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/greptimedb-mcp-server)](https://pypi.org/project/greptimedb-mcp-server/)\n![build workflow](https://github.com/GreptimeTeam/greptimedb-mcp-server/actions/workflows/python-app.yml/badge.svg)\n[![MIT License](https://img.shields.io/badge/license-MIT-green)](LICENSE.md)\n\nA Model Context Protocol (MCP) server implementation for [GreptimeDB](https://github.com/GreptimeTeam/greptimedb).\n\nThis server provides AI assistants with a secure and structured way to explore and analyze databases. It enables them to list tables, read data, and execute SQL queries through a controlled interface, ensuring responsible database access.\n\n# Project Status\nThis is an experimental project that is still under development. Data security and privacy issues have not been specifically addressed, so please use it with caution.\n\n# Capabilities\n\n* `list_resources` to list tables\n* `read_resource` to read table data\n* `list_tools` to list tools\n* `call_tool` to execute an SQL\n* `list_prompts` to list prompts\n* `get_prompt` to get the prompt by name\n\n# Installation\n\n```\npip install greptimedb-mcp-server\n```\n\n\n# Configuration\n\nSet the following environment variables:\n\n```bash\nGREPTIMEDB_HOST=localhost    # Database host\nGREPTIMEDB_PORT=4002         # Optional: Database MySQL port (defaults to 4002 if not specified)\nGREPTIMEDB_USER=root\nGREPTIMEDB_PASSWORD=\nGREPTIMEDB_DATABASE=public\nGREPTIMEDB_TIMEZONE=UTC\n```\n\nOr via command-line args:\n\n* `--host` the database host, `localhost` by default,\n* `--port` the database port, must be MySQL protocol port,  `4002` by default,\n* `--user` the database username, empty by default,\n* `--password` the database password, empty by default,\n* `--database` the database name, `public` by default.\n* `--timezone` the session time zone, empty by default(using server default time zone).\n\n# Usage\n\n## Claude Desktop Integration\n\nConfigure the MCP server in Claude Desktop's configuration file:\n\n#### MacOS\n\nLocation: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n#### Windows\n\nLocation: `%APPDATA%/Claude/claude_desktop_config.json`\n\n\n```json\n{\n  \"mcpServers\": {\n    \"greptimedb\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/greptimedb-mcp-server\",\n        \"run\",\n        \"-m\",\n        \"greptimedb_mcp_server.server\"\n      ],\n      \"env\": {\n        \"GREPTIMEDB_HOST\": \"localhost\",\n        \"GREPTIMEDB_PORT\": \"4002\",\n        \"GREPTIMEDB_USER\": \"root\",\n        \"GREPTIMEDB_PASSWORD\": \"\",\n        \"GREPTIMEDB_DATABASE\": \"public\",\n        \"GREPTIMEDB_TIMEZONE\": \"\"\n      }\n    }\n  }\n}\n```\n\n# License\n\nMIT License - see LICENSE.md file for details.\n\n# Contribute\n\n## Prerequisites\n- Python with `uv` package manager\n- GreptimeDB installation\n- MCP server dependencies\n\n## Development\n\n```\n# Clone the repository\ngit clone https://github.com/GreptimeTeam/greptimedb-mcp-server.git\ncd greptimedb-mcp-server\n\n# Create virtual environment\nuv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install development dependencies\nuv sync\n\n# Run tests\npytest\n```\n\nUse [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for debugging:\n\n```bash\nnpx @modelcontextprotocol/inspector uv \\\n  --directory \\\n  /path/to/greptimedb-mcp-server \\\n  run \\\n  -m \\\n  greptimedb_mcp_server.server\n```\n\n# Acknowledgement\nThis library's implementation was inspired by the following two repositories and incorporates their code, for which we express our gratitude：\n\n* [ktanaka101/mcp-server-duckdb](https://github.com/ktanaka101/mcp-server-duckdb)\n* [designcomputer/mysql_mcp_server](https://github.com/designcomputer/mysql_mcp_server)\n* [mikeskarl/mcp-prompt-templates](https://github.com/mikeskarl/mcp-prompt-templates)\n\nThanks!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "greptimedb",
        "databases",
        "database",
        "greptimedb databases",
        "analysis greptimedb",
        "databases secure"
      ],
      "category": "databases"
    },
    "Guanxinyuan--neo4j": {
      "owner": "Guanxinyuan",
      "name": "neo4j",
      "url": "https://github.com/Guanxinyuan/neo4j",
      "imageUrl": "/freedevtools/mcp/pfp/Guanxinyuan.webp",
      "description": "Leverage natural language to interact with Neo4j databases and manage knowledge graphs effortlessly. Transform natural language queries into Cypher commands and store knowledge graph memory in Neo4j or a file.",
      "stars": 2,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-25T06:03:34Z",
      "readme_content": "# Neo4j MCP Clients & Servers\n\nModel Context Protocol (MCP) is a [standardized protocol](https://modelcontextprotocol.io/introduction) for managing context between large language models (LLMs) and external systems. \n\nThis lets you use Claude Desktop, or any MCP Client, to use natural language to accomplish things with Neo4j and your Aura account, e.g.:\n\n* `What is in this graph?`\n\n## Servers\n\n### `mcp-neo4j-cypher` - natural language to Cypher queries\n\n### `mcp-neo4j-memory` - knowledge graph memory stored in Neo4j\n\n### `mcp-json-memory` - knowledge graph memory stored in a file\n\nA reference server for modeling memory as a knowledge graph.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neo4j",
        "databases",
        "database",
        "neo4j databases",
        "interact neo4j",
        "memory neo4j"
      ],
      "category": "databases"
    },
    "HarjjotSinghh--mcp-server-postgres-multi-schema": {
      "owner": "HarjjotSinghh",
      "name": "mcp-server-postgres-multi-schema",
      "url": "https://github.com/HarjjotSinghh/mcp-server-postgres-multi-schema",
      "imageUrl": "/freedevtools/mcp/pfp/HarjjotSinghh.webp",
      "description": "Enables interaction with PostgreSQL databases by providing read-only access to user-defined tables across multiple schemas with strict schema isolation. Facilitates cross-schema discovery and enhanced metadata management while ensuring security through access controls.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-22T07:32:05Z",
      "readme_content": "# PostgreSQL Multi-Schema MCP Server\n\nA Model Context Protocol server that provides read-only access to PostgreSQL databases with enhanced multi-schema support. This server enables LLMs to inspect database schemas across multiple namespaces and execute read-only queries while maintaining schema isolation.\n\n## Key Features\n\n- **Multi-Schema Support**: Explicitly specify which schemas to expose through command-line configuration\n- **Schema Isolation**: Strict access control to only authorized schemas listed during server startup\n- **Cross-Schema Discovery**: Unified view of tables across multiple schemas while maintaining schema boundaries\n- **Metadata Security**: Filters system catalogs to only expose user-defined tables in specified schemas\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n  - Schema context maintained through search_path restriction\n\n### Resources\n\nThe server provides schema information for each table across authorized schemas:\n\n- **Table Schemas** (`postgres://<host>/<db_schema>/<table>/schema`)\n  - JSON schema information for each table\n  - Includes column names, data types, and type modifiers\n  - Automatically discovered from database metadata\n  - Multi-schema support with explicit schema allow-list\n\n## Usage\n\nThe server requires a database URL and accepts a comma-separated list of schemas to expose:\n\n```\nnpx -y mcp-server-postgres-multi-schema <database-url> [schemas]\n```\n\n- **database-url**: PostgreSQL connection string (e.g., `postgresql://localhost/mydb`)\n- **schemas**: Comma-separated list of schemas to expose (defaults to 'public' if not specified)\n\n### Examples\n\n```bash\n# Connect with default public schema\nnpx -y mcp-server-postgres-multi-schema postgresql://localhost/mydb\n\n# Connect with multiple schemas\nnpx -y mcp-server-postgres-multi-schema postgresql://localhost/mydb public,analytics,staging\n```\n\n## Usage with Claude Desktop\n\nConfigure the \"mcpServers\" section in your `claude_desktop_config.json`:\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-postgres-multi-schema\",\n        \"postgresql://localhost/mydb\",\n        \"public,audit\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\nThis multi-schema MCP server is licensed under the MIT License. You may use, modify, and distribute the software according to the terms in the LICENSE file.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "secure database",
        "access schema"
      ],
      "category": "databases"
    },
    "HenkDz--postgresql-mcp-server": {
      "owner": "HenkDz",
      "name": "postgresql-mcp-server",
      "url": "https://github.com/HenkDz/postgresql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/HenkDz.webp",
      "description": "Manage and optimize PostgreSQL databases by analyzing configurations, monitoring performance metrics, assessing security, and providing recommendations for improvements. Streamline database operations with tools for schema management and data migration.",
      "stars": 124,
      "forks": 20,
      "license": "GNU Affero General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-10-01T09:28:54Z",
      "readme_content": "# PostgreSQL MCP Server\n[![smithery badge](https://smithery.ai/badge/@HenkDz/postgresql-mcp-server)](https://smithery.ai/server/@HenkDz/postgresql-mcp-server)\n\nA Model Context Protocol (MCP) server that provides comprehensive PostgreSQL database management capabilities for AI assistants.\n\n**🚀 What's New**: This server has been completely redesigned from 46 individual tools to 17 intelligent tools through consolidation (34→8 meta-tools) and enhancement (+4 new tools), providing better AI discovery while adding powerful data manipulation and comment management capabilities.\n\n## Quick Start\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=postgresql-mcp&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBoZW5rZXkvcG9zdGdyZXMtbWNwLXNlcnZlciIsIi0tY29ubmVjdGlvbi1zdHJpbmciLCJwb3N0Z3Jlc3FsOi8vdXNlcjpwYXNzd29yZEBob3N0OnBvcnQvZGF0YWJhc2UiXX0=)\n\n### Option 1: npm (Recommended)\n```bash\n# Install globally\nnpm install -g @henkey/postgres-mcp-server\n\n# Or run directly with npx (no installation)\nnpx @henkey/postgres-mcp-server --connection-string \"postgresql://user:pass@localhost:5432/db\"\n```\n\nAdd to your MCP client configuration:\n```json\n{\n  \"mcpServers\": {\n    \"postgresql-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@henkey/postgres-mcp-server\",\n        \"--connection-string\", \"postgresql://user:password@host:port/database\"\n      ]\n    }\n  }\n}\n```\n\n### Option 2: Install via Smithery\n```bash\nnpx -y @smithery/cli install @HenkDz/postgresql-mcp-server --client claude\n```\n\n### Option 3: Manual Installation (Development)\n```bash\ngit clone <repository-url>\ncd postgresql-mcp-server\nnpm install\nnpm run build\n```\n\nAdd to your MCP client configuration:\n```json\n{\n  \"mcpServers\": {\n    \"postgresql-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/postgresql-mcp-server/build/index.js\",\n        \"--connection-string\", \"postgresql://user:password@host:port/database\"\n      ]\n    }\n  }\n}\n```\n\n## What's Included\n\n**17 powerful tools** organized into three categories:\n- **🔄 Consolidation**: 34 original tools consolidated into 8 intelligent meta-tools\n- **🔧 Specialized**: 5 tools kept separate for complex operations  \n- **🆕 Enhancement**: 4 brand new tools (not in original 46)\n\n### 📊 **Consolidated Meta-Tools** (8 tools)\n- **Schema Management** - Tables, columns, ENUMs, constraints\n- **User & Permissions** - Create users, grant/revoke permissions  \n- **Query Performance** - EXPLAIN plans, slow queries, statistics\n- **Index Management** - Create, analyze, optimize indexes\n- **Functions** - Create, modify, manage stored functions\n- **Triggers** - Database trigger management\n- **Constraints** - Foreign keys, checks, unique constraints\n- **Row-Level Security** - RLS policies and management\n\n### 🚀 **Enhancement Tools** (4 NEW tools) \n*Brand new capabilities not available in the original 46 tools*\n- **Execute Query** - SELECT operations with count/exists support\n- **Execute Mutation** - INSERT/UPDATE/DELETE/UPSERT operations  \n- **Execute SQL** - Arbitrary SQL execution with transaction support\n- **Comments Management** - Comprehensive comment management for all database objects\n\n### 🔧 **Specialized Tools** (5 tools)\n- **Database Analysis** - Performance and configuration analysis\n- **Debug Database** - Troubleshoot connection, performance, locks\n- **Data Export/Import** - JSON/CSV data migration\n- **Copy Between Databases** - Cross-database data transfer  \n- **Real-time Monitoring** - Live database metrics and alerts\n\n## Example Usage\n\n```typescript\n// Analyze database performance\n{ \"analysisType\": \"performance\" }\n\n// Create a table with constraints\n{\n  \"operation\": \"create_table\",\n  \"tableName\": \"users\", \n  \"columns\": [\n    { \"name\": \"id\", \"type\": \"SERIAL PRIMARY KEY\" },\n    { \"name\": \"email\", \"type\": \"VARCHAR(255) UNIQUE NOT NULL\" }\n  ]\n}\n\n// Query data with parameters\n{\n  \"operation\": \"select\",\n  \"query\": \"SELECT * FROM users WHERE created_at > $1\",\n  \"parameters\": [\"2024-01-01\"],\n  \"limit\": 100\n}\n\n// Insert new data\n{\n  \"operation\": \"insert\",\n  \"table\": \"users\",\n  \"data\": {\"name\": \"John Doe\", \"email\": \"john@example.com\"},\n  \"returning\": \"*\"\n}\n\n// Find slow queries\n{\n  \"operation\": \"get_slow_queries\",\n  \"limit\": 5,\n  \"minDuration\": 100\n}\n\n// Manage database object comments\n{\n  \"operation\": \"set\",\n  \"objectType\": \"table\",\n  \"objectName\": \"users\",\n  \"comment\": \"Main user account information table\"\n}\n```\n\n## 📚 Documentation\n\n**📋 [Complete Tool Schema Reference](./TOOL_SCHEMAS.md)** - All 18 tool parameters & examples in one place\n\nFor additional information, see the [`docs/`](./docs/) folder:\n\n- **[📖 Usage Guide](./docs/USAGE.md)** - Comprehensive tool usage and examples\n- **[🛠️ Development Guide](./docs/DEVELOPMENT.md)** - Setup and contribution guide  \n- **[⚙️ Technical Details](./docs/TECHNICAL.md)** - Architecture and implementation\n- **[👨‍💻 Developer Reference](./docs/DEVELOPER.md)** - API reference and advanced usage\n- **[📋 Documentation Index](./docs/INDEX.md)** - Complete documentation overview\n\n## Features Highlights\n\n### **🔄 Consolidation Achievements**\n✅ **34→8 meta-tools** - Intelligent consolidation for better AI discovery  \n✅ **Multiple operations per tool** - Unified schemas with operation parameters  \n✅ **Smart parameter validation** - Clear error messages and type safety\n\n### **🆕 Enhanced Data Capabilities** \n✅ **Complete CRUD operations** - INSERT/UPDATE/DELETE/UPSERT with parameterized queries  \n✅ **Flexible querying** - SELECT with count/exists support and safety limits\n✅ **Arbitrary SQL execution** - Transaction support for complex operations\n\n### **🔧 Production Ready**\n✅ **Flexible connection** - CLI args, env vars, or per-tool configuration  \n✅ **Security focused** - SQL injection prevention, parameterized queries  \n✅ **Robust architecture** - Connection pooling, comprehensive error handling\n\n## Prerequisites\n\n- Node.js ≥ 18.0.0\n- PostgreSQL server access\n- Valid connection credentials\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Create a Pull Request\n\nSee [Development Guide](./docs/DEVELOPMENT.md) for detailed setup instructions.\n\n## License\n\nAGPLv3 License - see [LICENSE](./LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "postgresql databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "HenkDz--selfhosted-supabase-mcp": {
      "owner": "HenkDz",
      "name": "selfhosted-supabase-mcp",
      "url": "https://github.com/HenkDz/selfhosted-supabase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/HenkDz.webp",
      "description": "Facilitates interaction with self-hosted Supabase instances by enabling database management, schema introspection, user authentication, and real-time data handling from development environments.",
      "stars": 83,
      "forks": 22,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T06:52:52Z",
      "readme_content": "# Self-Hosted Supabase MCP Server\r\n\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n[![smithery badge](https://smithery.ai/badge/@HenkDz/selfhosted-supabase-mcp)](https://smithery.ai/server/@HenkDz/selfhosted-supabase-mcp)\r\n\r\n## Overview\r\n\r\nThis project provides a [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/specification) server designed specifically for interacting with **self-hosted Supabase instances**. It bridges the gap between MCP clients (like IDE extensions) and your local or privately hosted Supabase projects, enabling database introspection, management, and interaction directly from your development environment.\r\n\r\nThis server was built from scratch, drawing lessons from adapting the official Supabase cloud MCP server, to provide a minimal, focused implementation tailored for the self-hosted use case.\r\n\r\n## Purpose\r\n\r\nThe primary goal of this server is to enable developers using self-hosted Supabase installations to leverage MCP-based tools for tasks such as:\r\n\r\n*   Querying database schemas and data.\r\n*   Managing database migrations.\r\n*   Inspecting database statistics and connections.\r\n*   Managing authentication users.\r\n*   Interacting with Supabase Storage.\r\n*   Generating type definitions.\r\n\r\nIt avoids the complexities of the official cloud server related to multi-project management and cloud-specific APIs, offering a streamlined experience for single-project, self-hosted environments.\r\n\r\n## Features (Implemented Tools)\r\n\r\nThe server exposes the following tools to MCP clients:\r\n\r\n*   **Schema & Migrations**\r\n    *   `list_tables`: Lists tables in the database schemas.\r\n    *   `list_extensions`: Lists installed PostgreSQL extensions.\r\n    *   `list_migrations`: Lists applied Supabase migrations.\r\n    *   `apply_migration`: Applies a SQL migration script.\r\n*   **Database Operations & Stats**\r\n    *   `execute_sql`: Executes an arbitrary SQL query (via RPC or direct connection).\r\n    *   `get_database_connections`: Shows active database connections (`pg_stat_activity`).\r\n    *   `get_database_stats`: Retrieves database statistics (`pg_stat_*`).\r\n*   **Project Configuration & Keys**\r\n    *   `get_project_url`: Returns the configured Supabase URL.\r\n    *   `get_anon_key`: Returns the configured Supabase anon key.\r\n    *   `get_service_key`: Returns the configured Supabase service role key (if provided).\r\n    *   `verify_jwt_secret`: Checks if the JWT secret is configured and returns a preview.\r\n*   **Development & Extension Tools**\r\n    *   `generate_typescript_types`: Generates TypeScript types from the database schema.\r\n    *   `rebuild_hooks`: Attempts to restart the `pg_net` worker (if used).\r\n*   **Auth User Management**\r\n    *   `list_auth_users`: Lists users from `auth.users`.\r\n    *   `get_auth_user`: Retrieves details for a specific user.\r\n    *   `create_auth_user`: Creates a new user (Requires direct DB access, insecure password handling).\r\n    *   `delete_auth_user`: Deletes a user (Requires direct DB access).\r\n    *   `update_auth_user`: Updates user details (Requires direct DB access, insecure password handling).\r\n*   **Storage Insights**\r\n    *   `list_storage_buckets`: Lists all storage buckets.\r\n    *   `list_storage_objects`: Lists objects within a specific bucket.\r\n*   **Realtime Inspection**\r\n    *   `list_realtime_publications`: Lists PostgreSQL publications (often `supabase_realtime`).\r\n\r\n*(Note: `get_logs` was initially planned but skipped due to implementation complexities in a self-hosted environment).*\r\n\r\n## Setup and Installation\r\n\r\n### Installing via Smithery\r\n\r\nTo install Self-Hosted Supabase MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HenkDz/selfhosted-supabase-mcp):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @HenkDz/selfhosted-supabase-mcp --client claude\r\n```\r\n\r\n### Prerequisites\r\n\r\n*   Node.js (Version 18.x or later recommended)\r\n*   npm (usually included with Node.js)\r\n*   Access to your self-hosted Supabase instance (URL, keys, potentially direct DB connection string).\r\n\r\n### Steps\r\n\r\n1.  **Clone the repository:**\r\n    ```bash\r\n    git clone <repository-url>\r\n    cd selfhosted-supabase-mcp\r\n    ```\r\n2.  **Install dependencies:**\r\n    ```bash\r\n    npm install\r\n    ```\r\n3.  **Build the project:**\r\n    ```bash\r\n    npm run build\r\n    ```\r\n    This compiles the TypeScript code to JavaScript in the `dist` directory.\r\n\r\n## Configuration\r\n\r\nThe server requires configuration details for your Supabase instance. These can be provided via command-line arguments or environment variables. CLI arguments take precedence.\r\n\r\n**Required:**\r\n\r\n*   `--url <url>` or `SUPABASE_URL=<url>`: The main HTTP URL of your Supabase project (e.g., `http://localhost:8000`).\r\n*   `--anon-key <key>` or `SUPABASE_ANON_KEY=<key>`: Your Supabase project's anonymous key.\r\n\r\n**Optional (but Recommended/Required for certain tools):**\r\n\r\n*   `--service-key <key>` or `SUPABASE_SERVICE_ROLE_KEY=<key>`: Your Supabase project's service role key. Needed for operations requiring elevated privileges, like attempting to automatically create the `execute_sql` helper function if it doesn't exist.\r\n*   `--db-url <url>` or `DATABASE_URL=<url>`: The direct PostgreSQL connection string for your Supabase database (e.g., `postgresql://postgres:password@localhost:5432/postgres`). Required for tools needing direct database access or transactions (`apply_migration`, Auth tools, Storage tools, querying `pg_catalog`, etc.).\r\n*   `--jwt-secret <secret>` or `SUPABASE_AUTH_JWT_SECRET=<secret>`: Your Supabase project's JWT secret. Needed for tools like `verify_jwt_secret`.\r\n*   `--tools-config <path>`: Path to a JSON file specifying which tools to enable (whitelist). If omitted, all tools defined in the server are enabled. The file should have the format `{\"enabledTools\": [\"tool_name_1\", \"tool_name_2\"]}`.\r\n\r\n### Important Notes:\r\n\r\n*   **`execute_sql` Helper Function:** Many tools rely on a `public.execute_sql` function within your Supabase database for secure and efficient SQL execution via RPC. The server attempts to check for this function on startup. If it's missing *and* a `service-key` (or `SUPABASE_SERVICE_ROLE_KEY`) *and* `db-url` (or `DATABASE_URL`) are provided, it will attempt to create the function and grant necessary permissions. If creation fails or keys aren't provided, tools relying solely on RPC may fail.\r\n*   **Direct Database Access:** Tools interacting directly with privileged schemas (`auth`, `storage`) or system catalogs (`pg_catalog`) generally require the `DATABASE_URL` to be configured for a direct `pg` connection.\r\n\r\n## Usage\r\n\r\nRun the server using Node.js, providing the necessary configuration:\r\n\r\n```bash\r\n# Using CLI arguments (example)\r\nnode dist/index.js --url http://localhost:8000 --anon-key <your-anon-key> --db-url postgresql://postgres:password@localhost:5432/postgres [--service-key <your-service-key>]\r\n\r\n# Example with tool whitelisting via config file\r\nnode dist/index.js --url http://localhost:8000 --anon-key <your-anon-key> --tools-config ./mcp-tools.json\r\n\r\n# Or configure using environment variables and run:\r\n# export SUPABASE_URL=http://localhost:8000\r\n# export SUPABASE_ANON_KEY=<your-anon-key>\r\n# export DATABASE_URL=postgresql://postgres:password@localhost:5432/postgres\r\n# export SUPABASE_SERVICE_ROLE_KEY=<your-service-key>\r\n# The --tools-config option MUST be passed as a CLI argument if used\r\nnode dist/index.js\r\n\r\n# Using npm start script (if configured in package.json to pass args/read env)\r\nnpm start -- --url ... --anon-key ...\r\n```\r\n\r\nThe server communicates via standard input/output (stdio) and is designed to be invoked by an MCP client application (e.g., an IDE extension like Cursor). The client will connect to the server's stdio stream to list and call the available tools.\r\n\r\n## Client Configuration Examples\r\n\r\nBelow are examples of how to configure popular MCP clients to use this self-hosted server. \r\n\r\n**Important:** \r\n*   Replace placeholders like `<your-supabase-url>`, `<your-anon-key>`, `<your-db-url>`, `<path-to-dist/index.js>` etc., with your actual values.\r\n*   Ensure the path to the compiled server file (`dist/index.js`) is correct for your system.\r\n*   Be cautious about storing sensitive keys directly in configuration files, especially if committed to version control. Consider using environment variables or more secure methods where supported by the client.\r\n\r\n### Cursor\r\n\r\n1.  Create or open the file `.cursor/mcp.json` in your project root.\r\n2.  Add the following configuration:\r\n\r\n    ```json\r\n    {\r\n      \"mcpServers\": {\r\n        \"selfhosted-supabase\": { \r\n          \"command\": \"node\",\r\n          \"args\": [\r\n            \"<path-to-dist/index.js>\", // e.g., \"F:/Projects/mcp-servers/self-hosted-supabase-mcp/dist/index.js\"\r\n            \"--url\",\r\n            \"<your-supabase-url>\", // e.g., \"http://localhost:8000\"\r\n            \"--anon-key\",\r\n            \"<your-anon-key>\",\r\n            // Optional - Add these if needed by the tools you use\r\n            \"--service-key\",\r\n            \"<your-service-key>\",\r\n            \"--db-url\",\r\n            \"<your-db-url>\", // e.g., \"postgresql://postgres:password@host:port/postgres\"\r\n            \"--jwt-secret\",\r\n            \"<your-jwt-secret>\",\r\n            // Optional - Whitelist specific tools\r\n            \"--tools-config\",\r\n            \"<path-to-your-mcp-tools.json>\" // e.g., \"./mcp-tools.json\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n    ```\r\n\r\n### Visual Studio Code (Copilot)\r\n\r\nVS Code Copilot allows using environment variables populated via prompted inputs, which is more secure for keys.\r\n\r\n1.  Create or open the file `.vscode/mcp.json` in your project root.\r\n2.  Add the following configuration:\r\n\r\n    ```json\r\n    {\r\n      \"inputs\": [\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-url\", \"description\": \"Self-Hosted Supabase URL\", \"default\": \"http://localhost:8000\" },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-anon-key\", \"description\": \"Self-Hosted Supabase Anon Key\", \"password\": true },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-service-key\", \"description\": \"Self-Hosted Supabase Service Key (Optional)\", \"password\": true, \"required\": false },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-db-url\", \"description\": \"Self-Hosted Supabase DB URL (Optional)\", \"password\": true, \"required\": false },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-jwt-secret\", \"description\": \"Self-Hosted Supabase JWT Secret (Optional)\", \"password\": true, \"required\": false },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-server-path\", \"description\": \"Path to self-hosted-supabase-mcp/dist/index.js\" },\r\n        { \"type\": \"promptString\", \"id\": \"sh-supabase-tools-config\", \"description\": \"Path to tools config JSON (Optional, e.g., ./mcp-tools.json)\", \"required\": false }\r\n      ],\r\n      \"servers\": {\r\n        \"selfhosted-supabase\": {\r\n          \"command\": \"node\",\r\n          // Arguments are passed via environment variables set below OR direct args for non-env options\r\n          \"args\": [\r\n            \"${input:sh-supabase-server-path}\",\r\n            // Use direct args for options not easily map-able to standard env vars like tools-config\r\n            // Check if tools-config input is provided before adding the argument\r\n            [\"--tools-config\", \"${input:sh-supabase-tools-config}\"] \r\n            // Alternatively, pass all as args if simpler:\r\n            // \"--url\", \"${input:sh-supabase-url}\",\r\n            // \"--anon-key\", \"${input:sh-supabase-anon-key}\",\r\n            // ... etc ... \r\n           ],\r\n          \"env\": {\r\n            \"SUPABASE_URL\": \"${input:sh-supabase-url}\",\r\n            \"SUPABASE_ANON_KEY\": \"${input:sh-supabase-anon-key}\",\r\n            \"SUPABASE_SERVICE_ROLE_KEY\": \"${input:sh-supabase-service-key}\",\r\n            \"DATABASE_URL\": \"${input:sh-supabase-db-url}\",\r\n            \"SUPABASE_AUTH_JWT_SECRET\": \"${input:sh-supabase-jwt-secret}\"\r\n            // The server reads these environment variables as fallbacks if CLI args are missing\r\n          }\r\n        }\r\n      }\r\n    }\r\n    ```\r\n3.  When you use Copilot Chat in Agent mode (@workspace), it should detect the server. You will be prompted to enter the details (URL, keys, path) when the server is first invoked.\r\n\r\n### Other Clients (Windsurf, Cline, Claude)\r\n\r\nAdapt the configuration structure shown for Cursor or the official Supabase documentation, replacing the `command` and `args` with the `node` command and the arguments for this server, similar to the Cursor example:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"selfhosted-supabase\": { \r\n      \"command\": \"node\",\r\n      \"args\": [\r\n        \"<path-to-dist/index.js>\", \r\n        \"--url\", \"<your-supabase-url>\", \r\n        \"--anon-key\", \"<your-anon-key>\", \r\n        // Optional args...\r\n        \"--service-key\", \"<your-service-key>\", \r\n        \"--db-url\", \"<your-db-url>\", \r\n        \"--jwt-secret\", \"<your-jwt-secret>\",\r\n        // Optional tools config\r\n        \"--tools-config\", \"<path-to-your-mcp-tools.json>\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\nConsult the specific documentation for each client on where to place the `mcp.json` or equivalent configuration file.\r\n\r\n## Development\r\n\r\n*   **Language:** TypeScript\r\n*   **Build:** `tsc` (TypeScript Compiler)\r\n*   **Dependencies:** Managed via `npm` (`package.json`)\r\n*   **Core Libraries:** `@supabase/supabase-js`, `pg` (node-postgres), `zod` (validation), `commander` (CLI args), `@modelcontextprotocol/sdk` (MCP server framework).\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License. See the LICENSE file for details. \r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "hosted supabase",
        "selfhosted supabase",
        "secure database"
      ],
      "category": "databases"
    },
    "InfluxData--influxdb3_mcp_server": {
      "owner": "InfluxData",
      "name": "influxdb3_mcp_server",
      "url": "https://github.com/influxdata/influxdb3_mcp_server",
      "imageUrl": "",
      "description": "Official MCP server for InfluxDB 3 Core/Enterprise/Cloud Dedicated",
      "stars": 18,
      "forks": 5,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-09-22T02:29:19Z",
      "readme_content": "# InfluxDB MCP Server\n\nModel Context Protocol (MCP) server for InfluxDB 3 integration. Provides tools, resources, and prompts for interacting with InfluxDB v3 (Core/Enterprise/Cloud Dedicated) via MCP clients.\n\n---\n\n## Prerequisites\n\n- **InfluxDB 3 Instance**: URL and token (Core/Enterprise) or Cluster ID and tokens (Cloud Dedicated)\n- **Node.js**: v18 or newer (for npm/npx usage)\n- **npm**: v9 or newer (for npm/npx usage)\n- **Docker**: (for Docker-based setup)\n\n---\n\n## Available Tools\n\n| Tool Name                     | Description                                                    | Availability         |\n| ----------------------------- | -------------------------------------------------------------- | -------------------- |\n| `get_help`                    | Get help and troubleshooting guidance for InfluxDB operations  | All versions         |\n| `write_line_protocol`         | Write data using InfluxDB line protocol                        | All versions         |\n| `create_database`             | Create a new database (with Cloud Dedicated config options)    | All versions         |\n| `update_database`             | Update database configuration (maxTables, retention, etc.)     | Cloud Dedicated only |\n| `delete_database`             | Delete a database by name (irreversible)                       | All versions         |\n| `execute_query`               | Run a SQL query against a database (supports multiple formats) | All versions         |\n| `get_measurements`            | List all measurements (tables) in a database                   | All versions         |\n| `get_measurement_schema`      | Get schema (columns/types) for a measurement/table             | All versions         |\n| `create_admin_token`          | Create a new admin token (full permissions)                    | Core/Enterprise only |\n| `list_admin_tokens`           | List all admin tokens (with optional filtering)                | Core/Enterprise only |\n| `create_resource_token`       | Create a resource token for specific DBs and permissions       | Core/Enterprise only |\n| `list_resource_tokens`        | List all resource tokens (with filtering and ordering)         | Core/Enterprise only |\n| `delete_token`                | Delete a token by name                                         | Core/Enterprise only |\n| `regenerate_operator_token`   | Regenerate the operator token (dangerous/irreversible)         | Core/Enterprise only |\n| `cloud_list_database_tokens`  | List all database tokens for Cloud-Dedicated cluster           | Cloud Dedicated only |\n| `cloud_get_database_token`    | Get details of a specific database token by ID                 | Cloud Dedicated only |\n| `cloud_create_database_token` | Create a new database token for Cloud-Dedicated cluster        | Cloud Dedicated only |\n| `cloud_update_database_token` | Update an existing database token                              | Cloud Dedicated only |\n| `cloud_delete_database_token` | Delete a database token from Cloud-Dedicated cluster           | Cloud Dedicated only |\n| `list_databases`              | List all available databases in the instance                   | All versions         |\n| `health_check`                | Check InfluxDB connection and health status                    | All versions         |\n\n---\n\n## Available Resources\n\n| Resource Name      | Description                                |\n| ------------------ | ------------------------------------------ |\n| `influx-config`    | Read-only access to InfluxDB configuration |\n| `influx-status`    | Real-time connection and health status     |\n| `influx-databases` | List of all databases in the instance      |\n\n---\n\n## Setup & Integration Guide\n\n### 1. Environment Variables\n\n#### For Core/Enterprise InfluxDB:\n\nYou must provide:\n\n- `INFLUX_DB_INSTANCE_URL` (e.g. `http://localhost:8181/`)\n- `INFLUX_DB_TOKEN`\n- `INFLUX_DB_PRODUCT_TYPE` (`core` or `enterprise`)\n\nExample `.env`:\n\n```env\nINFLUX_DB_INSTANCE_URL=http://localhost:8181/\nINFLUX_DB_TOKEN=your_influxdb_token_here\nINFLUX_DB_PRODUCT_TYPE=core\n```\n\n#### For Cloud Dedicated InfluxDB:\n\nYou must provide `INFLUX_DB_PRODUCT_TYPE=cloud-dedicated` and `INFLUX_DB_CLUSTER_ID`, plus one of these token combinations:\n\n**Option 1: Database Token Only** (Query/Write operations only):\n\n```env\nINFLUX_DB_PRODUCT_TYPE=cloud-dedicated\nINFLUX_DB_CLUSTER_ID=your_cluster_id_here\nINFLUX_DB_TOKEN=your_database_token_here\n```\n\n**Option 2: Management Token Only** (Database management only):\n\n```env\nINFLUX_DB_PRODUCT_TYPE=cloud-dedicated\nINFLUX_DB_CLUSTER_ID=your_cluster_id_here\nINFLUX_DB_ACCOUNT_ID=your_account_id_here\nINFLUX_DB_MANAGEMENT_TOKEN=your_management_token_here\n```\n\n**Option 3: Both Tokens** (Full functionality):\n\n```env\nINFLUX_DB_PRODUCT_TYPE=cloud-dedicated\nINFLUX_DB_CLUSTER_ID=your_cluster_id_here\nINFLUX_DB_ACCOUNT_ID=your_account_id_here\nINFLUX_DB_TOKEN=your_database_token_here\nINFLUX_DB_MANAGEMENT_TOKEN=your_management_token_here\n```\n\nSee `env.cloud-dedicated.example` for detailed configuration options and comments.\n\n---\n\n### 2. Integration with MCP Clients\n\n#### A. Local (npm install & run)\n\n1. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n2. **Build the server:**\n   ```bash\n   npm run build\n   ```\n3. **Configure your MCP client** to use the built server. Example (see `example-local.mcp.json`):\n   ```json\n   {\n     \"mcpServers\": {\n       \"influxdb\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/influx-mcp-standalone/build/index.js\"],\n         \"env\": {\n           \"INFLUX_DB_INSTANCE_URL\": \"http://localhost:8181/\",\n           \"INFLUX_DB_TOKEN\": \"<YOUR_INFLUXDB_TOKEN>\",\n           \"INFLUX_DB_PRODUCT_TYPE\": \"core\"\n         }\n       }\n     }\n   }\n   ```\n\n#### B. Local (npx, no install/build required)\n\n1. **Run directly with npx** (after publishing to npm, won't work yet):\n   ```json\n   {\n     \"mcpServers\": {\n       \"influxdb\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@modelcontextprotocol/server-influxdb\"],\n         \"env\": {\n           \"INFLUX_DB_INSTANCE_URL\": \"http://localhost:8181/\",\n           \"INFLUX_DB_TOKEN\": \"<YOUR_INFLUXDB_TOKEN>\",\n           \"INFLUX_DB_PRODUCT_TYPE\": \"core\"\n         }\n       }\n     }\n   }\n   ```\n\n#### C. Docker\n\nBefore running the Docker integration, you must build the Docker image:\n\n```bash\n# Option 1: Use docker compose (recommended)\ndocker compose build\n# Option 2: Use npm script\nnpm run docker:build\n```\n\n**a) Docker with remote InfluxDB instance** (see `example-docker.mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"influxdb\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"INFLUX_DB_INSTANCE_URL\",\n        \"-e\",\n        \"INFLUX_DB_TOKEN\",\n        \"-e\",\n        \"INFLUX_DB_PRODUCT_TYPE\",\n        \"mcp/influxdb\"\n      ],\n      \"env\": {\n        \"INFLUX_DB_INSTANCE_URL\": \"http://remote-influxdb-host:8181/\",\n        \"INFLUX_DB_TOKEN\": \"<YOUR_INFLUXDB_TOKEN>\",\n        \"INFLUX_DB_PRODUCT_TYPE\": \"core\"\n      }\n    }\n  }\n}\n```\n\n**b) Docker with InfluxDB running in Docker on the same machine** (see `example-docker.mcp.json`):\n\nUse `host.docker.internal` as the InfluxDB URL so the MCP server container can reach the InfluxDB container:\n\n```json\n{\n  \"mcpServers\": {\n    \"influxdb\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--add-host=host.docker.internal:host-gateway\",\n        \"-e\",\n        \"INFLUX_DB_INSTANCE_URL\",\n        \"-e\",\n        \"INFLUX_DB_TOKEN\",\n        \"-e\",\n        \"INFLUX_DB_PRODUCT_TYPE\",\n        \"influxdb-mcp-server\"\n      ],\n      \"env\": {\n        \"INFLUX_DB_INSTANCE_URL\": \"http://host.docker.internal:8181/\",\n        \"INFLUX_DB_TOKEN\": \"<YOUR_INFLUXDB_TOKEN>\",\n        \"INFLUX_DB_PRODUCT_TYPE\": \"enterprise\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## Example Usage\n\n- Use your MCP client to call tools, resources, or prompts as described above.\n- See the `example-*.mcp.json` files for ready-to-use configuration templates:\n  - `example-local.mcp.json` - Local development setup\n  - `example-npx.mcp.json` - NPX-based setup\n  - `example-docker.mcp.json` - Docker-based setup\n  - `example-cloud-dedicated.mcp.json` - Cloud Dedicated with all variables\n- See the `env.example` and `env.cloud-dedicated.example` files for environment variable templates.\n\n---\n\n## Support & Troubleshooting\n\n- Use the `get_help` tool for built-in help and troubleshooting.\n- For connection issues, check your environment variables and InfluxDB instance status.\n- For advanced configuration, see the comments in the example `.env` and MCP config files.\n\n---\n\n## License\n\n[MIT](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "influxdb",
        "influxdb3_mcp_server",
        "influxdata",
        "server influxdb",
        "influxdb3_mcp_server official",
        "influxdb core"
      ],
      "category": "databases"
    },
    "JexinSam--mssql_mcp_server": {
      "owner": "JexinSam",
      "name": "mssql_mcp_server",
      "url": "https://github.com/JexinSam/mssql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/JexinSam.webp",
      "description": "Enables secure and structured interaction with Microsoft SQL Server databases, allowing users to list tables, read table contents, and execute SQL queries with controlled access.",
      "stars": 48,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T23:28:25Z",
      "readme_content": "![Tests](https://github.com/JexinSam/mssql_mcp_server/actions/workflows/test.yml/badge.svg)\n\n# MSSQL MCP Server\n\nMSSQL MCP Server is a **Model Context Protocol (MCP) server** that enables secure and structured interaction with **Microsoft SQL Server (MSSQL)** databases. It allows AI assistants to:\n- List available tables\n- Read table contents\n- Execute SQL queries with controlled access\n\nThis ensures safer database exploration, strict permission enforcement, and logging of database interactions.\n\n## Features\n\n- **Secure MSSQL Database Access** through environment variables\n- **Controlled Query Execution** with error handling\n- **Table Listing & Data Retrieval**\n- **Comprehensive Logging** for monitoring queries and operations\n\n## Installation\n\n```bash\npip install mssql-mcp-server\n```\n\n## Configuration\n\nSet the following environment variables to configure database access:\n\n```bash\nMSSQL_DRIVER=mssql_driver\nMSSQL_HOST=localhost\nMSSQL_USER=your_username\nMSSQL_PASSWORD=your_password\nMSSQL_DATABASE=your_database\n#optional\nTrustServerCertificate=yes\nTrusted_Connection=no\n```\n\n## Usage\n\n### With Claude Desktop\n\nTo integrate with **Claude Desktop**, add this configuration to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/mssql_mcp_server\",\n        \"run\",\n        \"mssql_mcp_server\"\n      ],\n      \"env\": {\n        \"MSSQL_DRIVER\": \"mssql_driver\",\n        \"MSSQL_HOST\": \"localhost\",\n        \"MSSQL_USER\": \"your_username\",\n        \"MSSQL_PASSWORD\": \"your_password\",\n        \"MSSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### Running as a Standalone Server\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Run the server\npython -m mssql_mcp_server\n```\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mssql_mcp_server.git\ncd mssql_mcp_server\n\n# Set up a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements-dev.txt\n\n# Run tests\npytest\n```\n\n## Security Considerations\n\n- **Use a dedicated MSSQL user** with minimal privileges.\n- **Never use root credentials** or full administrative accounts.\n- **Restrict database access** to only necessary operations.\n- **Enable logging and auditing** for security monitoring.\n- **Regularly review permissions** to ensure least privilege access.\n\n## Security Best Practices\n\nFor a secure setup:\n\n1. **Create a dedicated MSSQL user** with restricted permissions.\n2. **Avoid hardcoding credentials**—use environment variables instead.\n3. **Restrict access** to necessary tables and operations only.\n4. **Enable SQL Server logging and monitoring** for auditing.\n5. **Review database access regularly** to prevent unauthorized access.\n\nFor detailed instructions, refer to the **[MSSQL Security Configuration Guide](https://github.com/JexinSam/mssql_mcp_server/blob/main/SECURITY.md)**.\n\n⚠️ **IMPORTANT:** Always follow the **Principle of Least Privilege** when configuring database access.\n\n## License\n\nThis project is licensed under the **MIT License**. See the `LICENSE` file for details.\n\n## Contributing\n\nWe welcome contributions! To contribute:\n\n1. Fork the repository.\n2. Create a feature branch: `git checkout -b feature/amazing-feature`\n3. Commit your changes: `git commit -m 'Add amazing feature'`\n4. Push to the branch: `git push origin feature/amazing-feature`\n5. Open a **Pull Request**.\n\n---\n\n### Need Help?\nFor any questions or issues, feel free to open a GitHub **[Issue](https://github.com/JexinSam/mssql_mcp_server/issues)** or reach out to the maintainers.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mssql_mcp_server",
        "database",
        "secure database",
        "databases secure",
        "jexinsam mssql_mcp_server"
      ],
      "category": "databases"
    },
    "Jimmy974--mcp-server-qdrant": {
      "owner": "Jimmy974",
      "name": "mcp-server-qdrant",
      "url": "https://github.com/Jimmy974/mcp-server-qdrant",
      "imageUrl": "/freedevtools/mcp/pfp/Jimmy974.webp",
      "description": "Store and retrieve text information with associated metadata using a Qdrant vector database. Supports semantic search capabilities and integrates with FastEmbed for enhanced text embeddings, all configurable via environment variables.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Shell",
      "updated_at": "2025-03-18T14:44:18Z",
      "readme_content": "# MCP Server for Qdrant\n\nA Machine Control Protocol (MCP) server for storing and retrieving information from a Qdrant vector database.\n\n## Features\n\n- Store text information with optional metadata in Qdrant\n- Semantic search for stored information\n- FastEmbed integration for text embeddings\n- Environment-based configuration\n- Docker support\n\n## Installation\n\n### Using pip\n\n```bash\npip install mcp-server-qdrant\n```\n\n### From source\n\n```bash\ngit clone https://github.com/your-org/mcp-server-qdrant.git\ncd mcp-server-qdrant\nmake setup\n```\n\n## Configuration\n\nConfiguration is done through environment variables. You can create a `.env` file based on the `.env.example` file:\n\n```bash\ncp .env.example .env\n```\n\nEdit the `.env` file to configure the server:\n\n```\n# Qdrant configuration\nQDRANT_URL=http://localhost:6333\nQDRANT_API_KEY=your-api-key\n\n# Collection name\nCOLLECTION_NAME=memories\n\n# Embedding provider configuration\nEMBEDDING_PROVIDER=fastembed\nEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n```\n\n## Usage\n\n### Running locally\n\n```bash\npython -m mcp_server_qdrant.main\n```\n\nOr using the make command:\n\n```bash\nmake run\n```\n\n### Docker\n\n```bash\ndocker-compose up\n```\n\n## Tools\n\nThe MCP server provides the following tools:\n\n### qdrant-store\n\nStores information in the Qdrant database.\n\n```\ninformation: The text to store\nmetadata: Optional JSON metadata to associate with the text\n```\n\n### qdrant-find\n\nSearches for information in the Qdrant database using semantic search.\n\n```\nquery: The search query\n```\n\n## Development\n\n### Testing\n\n```bash\nmake test\n```\n\n### Formatting\n\n```bash\nmake format\n```\n\n### Linting\n\n```bash\nmake lint\n```\n\n### Building\n\n```bash\nmake build\n```\n\n## License\n\nApache License 2.0 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "database supports",
        "enables querying"
      ],
      "category": "databases"
    },
    "KashiwaByte--vikingdb-mcp-server": {
      "owner": "KashiwaByte",
      "name": "vikingdb-mcp-server",
      "url": "https://github.com/KashiwaByte/vikingdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/KashiwaByte.webp",
      "description": "Store and retrieve vector embeddings in VikingDB, a high-performance vector database. Supports vector similarity searching and information upserting for later retrieval.",
      "stars": 3,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-18T16:22:15Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kashiwabyte-vikingdb-mcp-server-badge.jpg)](https://mseep.ai/app/kashiwabyte-vikingdb-mcp-server)\n\n# VikingDB MCP server\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/KashiwaByte/vikingdb-mcp-server)](https://archestra.ai/mcp-catalog/kashiwabyte__vikingdb-mcp-server)\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-vikingdb)](https://smithery.ai/server/mcp-server-vikingdb)\nan mcp server for vikingdb store and search\n\n## What is VikingDB\nVikingDB is a high-performance vector database developed by ByteDance. \n\nYou can easily use it following the doc bellow:\nhttps://www.volcengine.com/docs/84313/1254444\n\n\n\n### Tools\n\nThe server implements the following tools:\n\n- vikingdb-colleciton-intro: introduce the collection of vikingdb\n\n- vikingdb-index-intro: introduce the index of vikingdb\n\n- vikingdb-upsert-information: upsert information to vikingdb for later use\n\n- vikingdb-search-information: searche for information in the VikingDB\n  \n  \n## Configuration\n\n- vikingdb_host: The host to use for the VikingDB server.\n\n- vikingdb_region: The region to use for the VikingDB server.\n \n - vikingdb_ak: The Access Key to use for the VikingDB server.\n\n - vikingdb_sk: The Secret Key to use for the VikingDB server.\n \n- collection_name: The name of the collection to use.\n\n- index_name: The name of the index to use.\n\n\n## Quickstart\n\n### Install\n\n### Installing via Smithery\n\nTo install VikingDB MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-vikingdb):\n\n```bash\nnpx -y @smithery/cli install mcp-server-vikingdb --client claude\n```\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n\nDevelopment/Unpublished Servers Configuration\n```\n{\n  \"mcpServers\": {\n    \"mcp-server-vikingdb\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"dir to mcp-server-vikingdb\",\n        \"run\",\n        \"mcp-server-vikingdb\",\n        \"--vikingdb-host\", \n        \"your host\",\n        \"--vikingdb-region\", \n        \"your region\",\n        \"--vikingdb-ak\", \n        \"your access key\",\n        \"--vikingdb-sk\", \n        \"your secret key\",\n        \"--collection-name\",\n        \"your collection name\",\n        \"--index-name\",\n        \"your index name\"\n      ]\n    }\n  }\n}\n\n  ```\n\nPublished Servers Configuration\n  ```\n{\n    \"mcpServers\": {\n      \"mcp-server-vikingdb\": {\n        \"command\": \"uvx\",\n        \"args\": [\n          \"mcp-server-vikingdb\",\n          \"--vikingdb-host\", \n          \"your host\",\n          \"--vikingdb-region\", \n          \"your region\",\n          \"--vikingdb-ak\", \n          \"your access key\",\n          \"--vikingdb-sk\", \n          \"your secret key\",\n          \"--collection-name\",\n          \"your collection name\",\n          \"--index-name\",\n          \"your index name\"\n      ]\n     }\n    }\n  } \n  ```\n\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory dir_to_mcp_server_vikingdb run mcp-server-vikingdb --vikingdb-host your_host --vikingdb-region your_region --vikingdb-ak your_access_key --vikingdb-sk your_secret_key --collection-name your_collection_name --index-name your_index_name\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vikingdb",
        "databases",
        "database",
        "embeddings vikingdb",
        "vector database",
        "secure database"
      ],
      "category": "databases"
    },
    "Kekniskd--cursor-IDE-API": {
      "owner": "Kekniskd",
      "name": "cursor-IDE-API",
      "url": "https://github.com/Kekniskd/cursor-IDE-API",
      "imageUrl": "/freedevtools/mcp/pfp/Kekniskd.webp",
      "description": "Manage posts with functionalities for creating, reading, updating, and deleting entries while enforcing user authentication and maintaining detailed logs. Implements paginated responses and integrates with an SQLite database for robust content management.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-02T14:04:26Z",
      "readme_content": "# Post Management API\n\nA simple FastAPI application for managing posts with SQLite backend, user authentication, and comprehensive logging.\n\n## Features\n\n- User Management with JWT Authentication\n- CRUD operations for posts (Create, Read, Update, Delete)\n- Post ownership and authorization controls\n- Paginated API responses\n- SQLite database integration with SQLAlchemy\n- Comprehensive logging system with file rotation\n- Request timing and monitoring\n- Input validation using Pydantic models\n- Modern FastAPI practices with lifespan management\n\n## API Flow Diagram\n\n```mermaid\nflowchart TD\n    A[<font color=black>Client</font>]\n    B[<font color=black>FastAPI App</font>]\n    C[<font color=black>Router Layer</font>]\n    D[<font color=black>Database Layer</font>]\n    E[<font color=black>Logging System</font>]\n    F[<font color=black>SQLite DB</font>]\n    K[<font color=black>Auth Layer</font>]\n\n    A -->|HTTP Request| B\n    B -->|Authenticate| K\n    K -->|Validate| D\n    B -->|Route| C\n    C -->|Query| D\n    D -->|Store/Retrieve| F\n    B -->|Log Request| E\n    C -->|Log Operation| E\n    D -->|Log DB Event| E\n\n    subgraph Operations\n        G[<font color=black>Create Post</font>]\n        H[<font color=black>Read Post</font>]\n        I[<font color=black>Update Post</font>]\n        J[<font color=black>Delete Post</font>]\n        L[<font color=black>User Auth</font>]\n    end\n\n    C --> Operations\n\n    classDef client fill:#FFD700,stroke:#333,stroke-width:2px;\n    classDef api fill:#98FB98,stroke:#333,stroke-width:2px;\n    classDef data fill:#87CEEB,stroke:#333,stroke-width:2px;\n    classDef logs fill:#FFA07A,stroke:#333,stroke-width:2px;\n    classDef ops fill:#DDA0DD,stroke:#333,stroke-width:2px;\n    classDef auth fill:#FF69B4,stroke:#333,stroke-width:2px;\n\n    class A client;\n    class B,C api;\n    class D,F data;\n    class E logs;\n    class G,H,I,J,L ops;\n    class K auth;\n```\n\n## Installation\n\n1. Ensure Python 3.8+ is installed\n2. Clone the repository\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Project Structure\n```\n.\n├── logs/                    # Application logs directory\n│   └── app_YYYYMMDD.log    # Daily rotating log files\n├── src/\n│   ├── database/\n│   │   ├── __init__.py\n│   │   ├── config.py       # Database configuration and session\n│   │   └── models.py       # SQLAlchemy database models\n│   ├── models/\n│   │   ├── __init__.py\n│   │   ├── post.py         # Pydantic models for posts\n│   │   └── user.py         # Pydantic models for users\n│   ├── router/\n│   │   ├── __init__.py\n│   │   ├── post_router.py  # Post CRUD endpoints\n│   │   └── user_router.py  # User management endpoints\n│   └── utils/\n│       ├── __init__.py\n│       ├── auth.py         # Authentication utilities\n│       └── logger.py       # Logging configuration\n├── main.py                 # FastAPI application entry point\n├── requirements.txt        # Project dependencies\n└── README.md              # Project documentation\n```\n\n## API Endpoints\n\n### Authentication\n- `POST /users/register` - Register a new user\n- `POST /users/login` - Login and get access token\n- `GET /users/me` - Get current user information\n\n### Posts\n- `GET /posts` - List all posts (paginated)\n  - Query parameters:\n    - `skip`: Number of posts to skip (default: 0)\n    - `limit`: Number of posts per page (default: 10, max: 100)\n  - Returns:\n    - `items`: List of posts\n    - `total`: Total number of posts\n    - `skip`: Current skip value\n    - `limit`: Current limit value\n- `GET /posts/{id}` - Get a specific post\n- `POST /posts` - Create a new post (requires authentication)\n- `PUT /posts/{id}` - Update an existing post (requires authentication, owner only)\n- `DELETE /posts/{id}` - Delete a post (requires authentication, owner only)\n\n## Running the Application\n\n```bash\npython main.py\n```\n\nThe API will be available at `http://127.0.0.1:8000`\n\n## Authentication\n\nThe API uses JWT (JSON Web Tokens) for authentication:\n1. Register a new user with username and password\n2. Login to receive an access token\n3. Include the token in subsequent requests:\n   ```\n   Authorization: Bearer <your_access_token>\n   ```\n\n## Logging\n\nLogs are stored in the `logs` directory with:\n- Daily rotation\n- 1MB file size limit\n- 5 backup files retained\n- Both file and console output\n- Request timing information\n- Operation tracking for all CRUD operations\n- Authentication events logging\n\n## API Documentation\n\n- Swagger UI: `http://127.0.0.1:8000/docs`\n- ReDoc: `http://127.0.0.1:8000/redoc`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlite",
        "database",
        "secure database",
        "databases secure",
        "sqlite database"
      ],
      "category": "databases"
    },
    "LeonMelamud--mysql-mcp": {
      "owner": "LeonMelamud",
      "name": "mysql-mcp",
      "url": "https://github.com/LeonMelamud/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/LeonMelamud.webp",
      "description": "Interact with MySQL databases by executing SQL queries, managing notes, and exploring database structures. Create, list, and search database content seamlessly within AI workflows.",
      "stars": 6,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-27T11:24:58Z",
      "readme_content": "# MySQL MCP Server\n\nA powerful MCP server that provides direct access to MySQL databases. This server enables AI agents to interact with MySQL databases, execute SQL queries, and manage database content through a simple interface.\n\n## Features\n\n### Resources\n- Access notes stored in the database via `note:///{id}` URIs\n- Each note has a title and content\n- Plain text mime type for simple content access\n\n### Tools\n- `create_note` - Create new text notes in the database\n  - Takes title and content as required parameters\n  - Stores note in the MySQL database\n- `list_tables` - List all tables in the connected database\n- `count_tables` - Get the total number of tables in the database\n- `search_tables` - Search for tables using LIKE pattern\n- `describe_table` - Get the structure of a specific table\n- `execute_sql` - Execute custom SQL queries\n\n## Prerequisites\n\n- Node.js 18 or higher\n- MySQL server installed and running\n- A database with appropriate permissions\n\n## Setup\n\n1. Clone this repository:\n   ```bash\n   git clone git@github.com:LeonMelamud/mysql-mcp.git\n   cd mysql-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file in the root directory with your MySQL connection details:\n   ```\n   MYSQL_HOST=localhost\n   MYSQL_USER=your_username\n   MYSQL_PASSWORD=your_password\n   MYSQL_DATABASE=your_database\n   ```\n\n4. Build the server:\n   ```bash\n   npm run build\n   ```\n\n## Installation\n\n### For Claude Desktop\n\nAdd the server config to your Claude Desktop configuration file:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mysql-server/build/index.js\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### For Cline\n\nAdd the server config to your Cline MCP settings file:\n\nOn MacOS: `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\nOn Windows: `%APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mysql-server/build/index.js\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Usage Examples\n\nOnce installed, you can use the MySQL MCP server in your conversations with Claude:\n\n### List all tables in the database\n```\nPlease list all the tables in my MySQL database.\n```\n\n### Execute a SQL query\n```\nRun this SQL query: SELECT * FROM users LIMIT 5\n```\n\n### Create a note\n```\nCreate a note titled \"Meeting Notes\" with the content \"Discussed project timeline and assigned tasks.\"\n```\n\n## Development\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Debugging\n\nUse the MCP Inspector to debug the server:\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "LiusCraft--superset-mcp-server": {
      "owner": "LiusCraft",
      "name": "superset-mcp-server",
      "url": "https://github.com/LiusCraft/superset-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/LiusCraft.webp",
      "description": "Provides basic database querying capabilities through the Apache Superset REST API, enabling execution of SQL commands and retrieval of data from various database sources.",
      "stars": 5,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-09T11:35:29Z",
      "readme_content": "# WIP: MCP Server Superset\n\n基于 Apache Superset REST API 构建的 Model Context Protocol (MCP) 服务器端应用。\n\n## 项目简介\n\n这是一个基于 Apache Superset RESTAPI 在MCP上实现了通过大模型来让它进行基本的查询能力。\n\n## 功能特性\n\n- 查询数据库\n- 查询表\n- 查询字段\n- 执行sql\n\n## 环境要求\n\n- Node.js >= 14.0.0\n\n## 快速开始\n\n### 直接使用\n\n```bash\nnpx -y https://github.com/LiusCraft/superset-mcp-server\n\nSUPERSET_URL\nSUPERSET_USERNAME\nSUPERSET_PASSWORD\n\n鉴权方式：ladp\n```\n\n### 安装依赖\n\n```bash\n# 安装 Node.js 依赖\nnpm install\n```\n\n### 启动服务\n\n```bash\n# api client 测试\nnpm run src/examples/superset-example.ts\n\n# 调试环境\nnpm run inspector\n\n# 生产环境\nnpm run build\nnpm start\n```\n\n## 配置说明\n\n项目配置文件位于 `config` 目录下，包括：\n\n- 数据库配置\n- API 配置\n- 安全配置\n\n## API 文档\n\n参考superset官方 swagger文档\n\n## 开发指南\n\n### 目录结构\n\n```\n.\n├── src/          # 源代码目录\n├── src/examples          # 封装的api客户端测试代码\n├── src/services          # 封装的api函数\n├── src/utils             # 封装的superset baseHttpClient\n├── src/index.ts          # 定义mcp接口\n```\n\n### 开发规范\n\n- 遵循 ESLint 规范\n- 使用 TypeScript 进行开发\n- 遵循 Git Flow 工作流\n\n## 部署\n\n### mcp 部署\n\n1. build the project\n\n2. set mcp config:\n```bash\nnode currentFolder/build/index.js\n```\n\n3. use the mcp\n\n## 贡献指南\n\n1. Fork 本仓库\n2. 创建特性分支\n3. 提交变更\n4. 发起 Pull Request\n\n## 许可证\n\n[Apache License 2.0](LICENSE)\n\n## 联系方式\n\n如有问题，请提交 Issue 或联系项目维护者。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "superset",
        "database",
        "apache superset",
        "liuscraft superset",
        "secure database"
      ],
      "category": "databases"
    },
    "LucasHild--mcp-server-bigquery": {
      "owner": "LucasHild",
      "name": "mcp-server-bigquery",
      "url": "https://github.com/LucasHild/mcp-server-bigquery",
      "imageUrl": "/freedevtools/mcp/pfp/LucasHild.webp",
      "description": "Provides access to BigQuery for inspecting database schemas and executing SQL queries against datasets.",
      "stars": 120,
      "forks": 32,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T12:38:17Z",
      "readme_content": "# BigQuery MCP server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-bigquery)](https://smithery.ai/server/mcp-server-bigquery)\n\nA Model Context Protocol server that provides access to BigQuery. This server enables LLMs to inspect database schemas and execute queries.\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n\n- `execute-query`: Executes a SQL query using BigQuery dialect\n- `list-tables`: Lists all tables in the BigQuery database\n- `describe-table`: Describes the schema of a specific table\n\n## Configuration\n\nThe server can be configured either with command line arguments or environment variables.\n\n| Argument     | Environment Variable | Required | Description                                                                                                                                                                                                                                                                                                                                                    |\n| ------------ | -------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `--project`  | `BIGQUERY_PROJECT`   | Yes      | The GCP project ID.                                                                                                                                                                                                                                                                                                                                            |\n| `--location` | `BIGQUERY_LOCATION`  | Yes      | The GCP location (e.g. `europe-west9`).                                                                                                                                                                                                                                                                                                                        |\n| `--dataset`  | `BIGQUERY_DATASETS`  | No       | Only take specific BigQuery datasets into consideration. Several datasets can be specified by repeating the argument (e.g. `--dataset my_dataset_1 --dataset my_dataset_2`) or by joining them with a comma in the environment variable (e.g. `BIGQUERY_DATASETS=my_dataset_1,my_dataset_2`). If not provided, all datasets in the project will be considered. |\n| `--key-file` | `BIGQUERY_KEY_FILE`  | No       | Path to a service account key file for BigQuery. If not provided, the server will use the default credentials.                                                                                                                                                                                                                                                 |\n\n## Quickstart\n\n### Install\n\n#### Installing via Smithery\n\nTo install BigQuery Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-bigquery):\n\n```bash\nnpx -y @smithery/cli install mcp-server-bigquery --client claude\n```\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n##### Development/Unpublished Servers Configuration</summary>\n\n```json\n\"mcpServers\": {\n  \"bigquery\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"{{PATH_TO_REPO}}\",\n      \"run\",\n      \"mcp-server-bigquery\",\n      \"--project\",\n      \"{{GCP_PROJECT_ID}}\",\n      \"--location\",\n      \"{{GCP_LOCATION}}\"\n    ]\n  }\n}\n```\n\n##### Published Servers Configuration\n\n```json\n\"mcpServers\": {\n  \"bigquery\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-bigquery\",\n      \"--project\",\n      \"{{GCP_PROJECT_ID}}\",\n      \"--location\",\n      \"{{GCP_LOCATION}}\"\n    ]\n  }\n}\n```\n\nReplace `{{PATH_TO_REPO}}`, `{{GCP_PROJECT_ID}}`, and `{{GCP_LOCATION}}` with the appropriate values.\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Increase the version number in `pyproject.toml`\n\n2. Sync dependencies and update lockfile:\n\n```bash\nuv sync\n```\n\n3. Build package distributions:\n\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n4. Publish to PyPI:\n\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory {{PATH_TO_REPO}} run mcp-server-bigquery\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bigquery",
        "databases",
        "database",
        "bigquery provides",
        "bigquery inspecting",
        "access bigquery"
      ],
      "category": "databases"
    },
    "MadeByNando--mcp-postgres-server": {
      "owner": "MadeByNando",
      "name": "mcp-postgres-server",
      "url": "https://github.com/MadeByNando/mcp-postgres-server",
      "imageUrl": "/freedevtools/mcp/pfp/MadeByNando.webp",
      "description": "Leverage PostgreSQL databases for executing read-only SQL queries and exploring database schemas. Integrate powerful database capabilities directly into workflows for enhanced application functionality.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-02-28T16:01:25Z",
      "readme_content": "# MCP Postgres Server\n\nCe serveur implémente le protocole MCP (Model Context Protocol) pour Cursor, permettant d'utiliser une base de données PostgreSQL comme stockage pour les contextes de modèle.\n\n## Prérequis\n\n- Docker\n- Docker Compose\n\n## Installation et démarrage\n\n1. Clonez ce dépôt\n2. Démarrez le serveur avec Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n## Configuration dans Cursor\n\n1. Ouvrez Cursor\n2. Allez dans Paramètres > MCP\n3. Ajoutez une nouvelle connexion avec les paramètres suivants:\n   - Nom: MCP Postgres Server\n   - Type: command\n   - Commande: `docker exec -i mcp-postgres-server node dist/index.js`\n\n## Résolution des problèmes\n\nSi le serveur ne démarre pas correctement:\n\n1. Vérifiez les logs du conteneur:\n\n   ```bash\n   docker logs mcp-postgres-server\n   ```\n\n2. Pour redémarrer le serveur:\n\n   ```bash\n   docker-compose restart\n   ```\n\n## Fonctionnalités du serveur MCP\n\nLe serveur MCP PostgreSQL expose les outils suivants pour Cursor:\n\n1. `postgres_query` - Exécuter une requête SQL en lecture seule\n2. `postgres_list_tables` - Lister toutes les tables de la base de données\n3. `postgres_describe_table` - Obtenir le schéma d'une table spécifique\n\nCes outils permettent à Cursor d'explorer et d'interroger la base de données de manière sécurisée.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "Maxim2324--mcp-server-test": {
      "owner": "Maxim2324",
      "name": "mcp-server-test",
      "url": "https://github.com/Maxim2324/mcp-server-test",
      "imageUrl": "/freedevtools/mcp/pfp/Maxim2324.webp",
      "description": "Connect to PostgreSQL databases to execute read-only SQL queries, explore schemas, and visualize data relationships for analysis. Utilize pre-built prompts for common data analysis tasks.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-15T07:30:11Z",
      "readme_content": "# MCP Server Test Collection\n[![License](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT)\n[![Maintenance](https://img.shields.io/badge/Maintained-Yes-brightgreen)](https://github.com/OpenBrokerRemover)\n[![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-brightgreen)](https://github.com/OpenBrokerRemover/pulls)\n\nThis repository contains a collection of tests and implementations for various Model-Controller-Provider (MCP) server architectures. Each implementation focuses on different aspects of MCP pattern and database interactions.\n\n## Current Implementations\n\n### 1. MCP PostgreSQL Server (`mcp-psql/`)\nA PostgreSQL-specific implementation that:\n- Provides safe database access through a structured API\n- Enables AI systems to interact with databases\n- Implements read-only query validation\n- Includes pre-built analysis templates\n- Supports schema exploration and data analysis\n\n### 2. MCP Figma Server (`mcp-figma/`)\nA Figma-specific implementation that:\n- Enables AI systems to interact with Figma designs\n- Provides structured access to Figma files and components\n- Implements design analysis and manipulation capabilities\n- Supports AI-driven design suggestions and modifications\n- Includes pre-built design templates and patterns\n\n## Purpose\n\nThis repository serves as:\n- A testing ground for different MCP implementations\n- A reference for MCP pattern best practices\n- A collection of database and design tool interaction patterns\n- A showcase of AI integration approaches with various systems\n\n## Getting Started\n\nEach implementation in this collection has its own setup instructions and documentation. Please refer to the specific implementation's README for details.\n\n## Contributing\n\nNew MCP implementations and test cases are welcome! Please follow these guidelines:\n1. Create a new directory for your implementation\n2. Include comprehensive documentation\n3. Follow the existing project structure\n4. Add appropriate tests\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "Medsaad--mcp-db-navigator": {
      "owner": "Medsaad",
      "name": "mcp-db-navigator",
      "url": "https://github.com/Medsaad/mcp-db-navigator",
      "imageUrl": "/freedevtools/mcp/pfp/Medsaad.webp",
      "description": "Facilitate navigation and management of MySQL and MariaDB databases via a secure and type-safe MCP server. Execute SQL queries, retrieve schema information, and manage connections efficiently.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-14T14:01:33Z",
      "readme_content": "# MySQL Navigator MCP\n\nA powerful MySQL/MariaDB database navigation tool using MCP (Model Control Protocol) for easy database querying and management.\n\n## Features\n\n- Connect to MySQL/MariaDB databases\n- Switch between different databases dynamically\n- Execute SQL queries with type safety\n- Retrieve database schema information\n- Pydantic model validation for query parameters\n- Secure credential management\n- Comprehensive logging system\n- Connection pooling and retry mechanisms\n- SSL/TLS support for secure connections\n\n## Log File Location (Cross-Platform)\n\nBy default, all logs are written to:\n\n- **Windows:** `C:\\Users\\<YourUsername>\\.mcp\\mcp-db.log`\n- **macOS/Linux:** `/home/<yourusername>/.mcp/mcp-db.log` or `/Users/<yourusername>/.mcp/mcp-db.log`\n\nIf the `.mcp` folder does not exist in your home directory, the application will automatically create it. If you run into any issues, you can manually create the folder:\n\n**Windows:**\n```powershell\nmkdir $env:USERPROFILE\\.mcp\n```\n**macOS/Linux:**\n```bash\nmkdir -p ~/.mcp\n```\n\n## Installation\n\n### From PyPI (recommended for most users):\n```bash\npip install mcp-db-navigator\n```\n\n### From source (for development):\n```bash\ngit clone <your-repo-url>\ncd mcp-db\npip install -e .\n```\n\n3. Create a `.env` file with your database credentials:\n```env\nDB_HOST=your_host\nDB_PORT=your_port\nDB_NAME=your_database_name\nDB_USER=your_username\nDB_PASSWORD=your_password\nDB_SSL_CA=/path/to/ssl/ca.pem  # Optional: for SSL/TLS connections\nDB_MAX_RETRIES=3  # Optional: number of connection retries\nDB_RETRY_DELAY=1.0  # Optional: delay between retries in seconds\n```\n\n## Usage Examples\n\n### 1. Command Line\nRun the MCP server directly from your terminal:\n```bash\nmcp-db --config /path/to/your/project/.env\n```\n\n### 2. In Cursor\nTo use this MCP server in [Cursor](https://www.cursor.so):\n- Open Cursor settings and add a new MCP server.\n- Use the following configuration (example):\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql-navigator\": {\n      \"command\": \"mcp-db\",\n      \"args\": [\n        \"--config\",\n        \"/absolute/path/to/your/.env\"\n      ]\n    }\n  }\n}\n```\n- Make sure the path to your `.env` file is absolute.\n\n### 3. In Claude Desktop\nIf Claude Desktop supports MCP servers:\n- Add a new MCP server and point it to the `mcp-db` command with the `--config` argument as above.\n- Refer to Claude Desktop's documentation for details on adding custom MCP servers.\n\n## Query Parameters\n\nThe query dictionary supports the following parameters:\n\n- `table_name` (required): Name of the table to query\n- `select_fields` (optional): List of fields to select (defaults to [\"*\"])\n- `where_conditions` (optional): Dictionary of field-value pairs for WHERE clause\n- `order_by` (optional): List of fields to order by\n- `order_direction` (optional): Sort direction \"ASC\" or \"DESC\" (default: \"ASC\")\n- `limit` (optional): Number of records to return\n- `offset` (optional): Number of records to skip\n- `group_by` (optional): List of fields to group by\n- `having` (optional): Dictionary of field-value pairs for HAVING clause\n- `join_table` (optional): Name of the table to join with\n- `join_type` (optional): Type of JOIN operation (default: \"INNER\")\n- `join_conditions` (optional): Dictionary of join conditions\n\n## Security Features\n\n- Database credentials are managed through a config file\n- Passwords are stored as SecretStr in Pydantic models\n- Input validation for all query parameters\n- SQL injection prevention through parameterized queries\n- SSL/TLS support for encrypted connections\n- Connection string sanitization\n- Rate limiting for queries\n- Query parameter sanitization\n\n## Production Features\n\n### Error Handling\n- Comprehensive error handling for database operations\n- Connection timeout handling\n- Automatic retry mechanism for failed connections\n- Input validation for all parameters\n\n### Performance\n- Connection pooling for optimal resource usage\n- Query execution time logging\n- Connection pool statistics\n- Performance metrics collection\n\n### Monitoring\n- Structured logging with different log levels\n- Query execution tracking\n- Connection state monitoring\n- Error rate tracking\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mariadb",
        "database",
        "databases secure",
        "db navigator",
        "secure database"
      ],
      "category": "databases"
    },
    "Mineru98--mysql-mcp-server": {
      "owner": "Mineru98",
      "name": "mysql-mcp-server",
      "url": "https://github.com/Mineru98/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Mineru98.webp",
      "description": "Interact with MySQL databases to perform operations such as creating tables and executing queries. Integrate database functionalities directly into workflows seamlessly.",
      "stars": 28,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T20:35:29Z",
      "readme_content": "# mysql-mcp-server\n\n<a href=\"https://glama.ai/mcp/servers/6y836dz8o5\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/6y836dz8o5/badge\" />\n</a>\n\n[한국어 README.md](https://github.com/Mineru98/mysql-mcp-server/blob/main/README.ko.md)\n\n## 0. Execution\n\n### Running with Docker\n\n> Change the database connection information as needed.\n\n```\ndocker run -d --name mcp-mysql \\\n  -e MYSQL_HOST=localhost \\\n  -e MYSQL_PORT=3306 \\\n  -e MYSQL_USER=root \\\n  -e MYSQL_PASSWORD=mcpTest1234!!! \\\n  -e MYSQL_DATABASE=mcp_test \\\n  -e MCP_PORT=8081 \\\n  -p 3306:3306 mineru/mcp-mysql:1.0.0\n```\n\n### Running with Docker Compose\n\n> This will proceed with a pre-configured setup.\n\n```\ndocker-compose up -d\n```\n\n### Running directly with Python\n\n```\npip install -r requirements.txt\npython mysql_mcp_server/main.py run\n```\n\n### Cursor Configuration\n\n> MCP functionality is available from Cursor version 0.46 and above.\n>\n> Additionally, the MCP feature is only accessible to Cursor Pro account users.\n\n\n\n### Tool Addition Tips\n\n- Adding a Tool\n  - `execute` functions implement the actual logic execution (Service Layer).\n  - The `@tool` decorator helps register the tool with MCP (Controller Layer).\n- Explanation\n  - Each file under `mysql_mcp_server/executors` represents a single tool.\n  - If a new tool is added, it must be imported in `mysql_mcp_server/executors/__init__.py` and included in the `__all__` array.\n  - This ensures the module is automatically registered in the `TOOLS_DEFINITION` variable.\n  \n```mermaid\nflowchart LR;\n    A[AI Model] -->|Request tool list| B[MCP Server]\n    B -->|Return available tools| A\n\n    A -->|Request specific tool execution| B\n    B -->|Call the corresponding executor| C[Executors]\n    \n    subgraph Executors\n        C1[execute_create_table] -->|Create table| D\n        C2[execute_desc_table] -->|View table schema| D\n        C3[execute_explain] -->|Query execution plan| D\n        C4[execute_insert_query] -->|Execute INSERT query| D\n        C5[execute_insight_starter] -->|Checking the schema for building reports| D\n        C6[execute_invoke_viz_pro] -->|Visualization chart recommendations| D\n        C7[execute_select_query] -->|Execute SELECT query| D\n        C8[execute_show_tables] -->|Retrieve table list| D\n    end\n\n    D[DatabaseManager] -->|Connect to MySQL| E[MySQL 8.0]\n\n    E -->|Return results| D\n    D -->|Send results| C\n    C -->|Return results| B\n    B -->|Return execution results| A\n```\n\n## 🚧 Development Roadmap 🚧\n\n- ⚙️ Parameter Options  \n  - [ ] 🔧 Enable/Disable Switch for Each Tool: Provide a function to reduce Input Context costs 💰  \n  - [ ] 🔒 Query Security Level Setting: Offer optional control over functions that could damage asset value, such as DROP, DELETE, UPDATE 🚫  \n\n- ✨ Features  \n  - [x] 📊 Data Analysis Report Generation: Provide a report generation function optimized for the model to appropriately select various charts based on user requests 📈  \n      - [x] 📝 Reporting capabilities for prescribed forms  \n      - [ ] 🖌️ Diversify report templates  \n  - [ ] 🗄️ Extended Text2SQL Support  \n  - [ ] 🌐 SSH Connection Support: Enable secure remote access via SSH for advanced operations 🔑  \n  - [ ] 📥 File Extraction Function  \n    - [ ] 📄 CSV  \n    - [ ] 📑 JSON  \n    - [ ] 📉 Excel  \n\n## 1. Overview\n\nMCP MySQL Server is a server application for MySQL database operations based on MCP (Model Context Protocol). This server provides tools that allow AI models to interact with the MySQL database.\n\n## 2. System Configuration\n\n### 2.1 Key Components\n\n- **MCP Server**: A FastMCP server that communicates with AI models\n- **MySQL Database**: Manages and stores data\n- **Tools**: Executors that perform database operations\n\n### 2.2 Tech Stack\n\n- **Language**: Python\n- **Database**: MySQL 8.0\n- **Key Libraries**:\n  - mcp: Implements Model Context Protocol for AI communication\n  - PyMySQL: Connects to MySQL and executes queries\n  - pandas: Processes and analyzes data\n  - python-dotenv: Manages environment variables\n  - fire: Implements command-line interfaces\n\n### 2.3 Deployment Environment\n\n- Containerized deployment via Docker and Docker Compose\n- Ports: 8081 (MCP Server), 3306 (MySQL)\n\n## 3. Directory Structure\n\n```\nMCPBoilerPlate/\n├── mysql_mcp_server/           # Main application directory\n│   ├── executors/              # Database operation executors\n│   │   ├── create_table.py     # Tool for creating tables\n│   │   ├── desc_table.py       # Tool for viewing table schema\n│   │   ├── explain.py          # Tool for query execution plans\n│   │   ├── insert_query.py     # Tool for INSERT query execution\n│   │   ├── insight_starter.py  # Schema verification tools for write reports\n│   │   ├── invoke_viz_pro.py   # Tool for Visualization chart recommendation\n│   │   ├── select_query.py     # Tool for SELECT query execution\n│   │   └── show_tables.py      # Tool for retrieving table lists\n│   ├── helper/                 # Utility modules\n│   │   ├── db_conn_helper.py   # Manages database connections\n│   │   ├── logger_helper.py    # Logging utilities\n│   │   └── tool_decorator.py   # Tool decorator\n│   └── main.py                 # Application entry point\n├── docker-compose.yml          # Docker Compose configuration\n├── Dockerfile                  # Docker image build settings\n├── requirements.txt            # Dependency package list\n└── .env.example                # Example environment variables file\n```\n\n## 4. Architecture Design\n\n### 4.1 Layered Structure\n\n1. **Interface Layer**: MCP Server (FastMCP)\n2. **Business Logic Layer**: Handlers and Executors\n3. **Data Access Layer**: Database connection and query execution\n\n### 4.2 Key Classes and Modules\n\n- **MySQLMCPServer**: Main server class that initializes and runs the MCP server\n- **DatabaseManager**: Singleton pattern-based database connection manager\n- **Executors**: Collection of tools for database operations\n  - execute_create_table: Creates tables\n  - execute_desc_table: Checks table schema\n  - execute_explain: Provides query execution plans\n  - execute_insert_query: Executes INSETR queries\n  - execute_select_query: Executes SELECT queries\n  - execute_show_tables: Retrieves table lists\n\n### 4.3 Communication Flow\n\n1. AI model requests a list of available tools from the MCP server.\n2. The server returns the available tools list.\n3. The AI model requests the execution of a specific tool.\n4. The server calls the corresponding executor to perform the database operation.\n5. The execution results are returned to the AI model.\n\n## 5. Scalability and Maintenance\n\n- **Adding Tools**: Implement new tools in the `executors` directory and register them in `__init__.py`.\n- **Environment Configuration**: Manage environment variables via the `.env` file.\n- **Logging**: Ensure consistent logging using `logger_helper`.\n\n## 6. Deployment and Execution\n\n### 6.1 Local Execution\n\n```bash\n# Setup environment\ncp .env.example .env\n# Modify .env file as needed\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run the server\npython mysql_mcp_server/main.py run\n```\n\n### 6.2 Docker Deployment\n\n```bash\n# Start database using Docker Compose\ndocker-compose up -d db\n# Build and run mysql-mcp-server with Docker Compose (including rebuilds)\ndocker-compose up -d --build mysql-mcp-server\n```\n\n## 7. Security Considerations\n\n- Manage database credentials via environment variables.\n- Use strong passwords in production environments.\n- Consider implementing SSL/TLS encryption for database connections when necessary.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "Moonlight-CL--redshift-mcp-server": {
      "owner": "Moonlight-CL",
      "name": "redshift-mcp-server",
      "url": "https://github.com/Moonlight-CL/redshift-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Moonlight-CL.webp",
      "description": "Enable interaction with Amazon Redshift databases by listing schemas and tables, executing SQL queries, analyzing tables, and obtaining execution plans. Retrieve table DDL scripts and statistics for efficient data management.",
      "stars": 1,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-18T09:02:50Z",
      "readme_content": "# Redshift MCP Server\n\nA Model Context Protocol (MCP) server for Amazon Redshift that enables AI assistants to interact with Redshift databases.\n\n## Introduction\n\nRedshift MCP Server is a Python-based implementation of the [Model Context Protocol](https://github.com/modelcontextprotocol/mcp) that provides tools and resources for interacting with Amazon Redshift databases. It allows AI assistants to:\n\n- List schemas and tables in a Redshift database\n- Retrieve table DDL (Data Definition Language) scripts\n- Get table statistics\n- Execute SQL queries\n- Analyze tables to collect statistics information\n- Get execution plans for SQL queries\n\n## Installation\n\n### Prerequisites\n\n- Python 3.13 or higher\n- Amazon Redshift cluster\n- Redshift credentials (host, port, username, password, database)\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/Moonlight-CL/redshift-mcp-server.git\ncd redshift-mcp-server\n\n# Install dependencies\nuv sync\n```\n\n## Configuration\n\nThe server requires the following environment variables to connect to your Redshift cluster:\n\n```\nRS_HOST=your-redshift-cluster.region.redshift.amazonaws.com\nRS_PORT=5439\nRS_USER=your_username\nRS_PASSWORD=your_password\nRS_DATABASE=your_database\nRS_SCHEMA=your_schema  # Optional, defaults to \"public\"\n```\n\nYou can set these environment variables directly or use a `.env` file.\n\n## Usage\n\n### Starting the server\n\n```bash\n# Start the server\nuv run --with mcp python-dotenv redshift-connector mcp\nmcp run src/redshift_mcp_server/server.py\n```\n\n### Integrating with AI assistants\n\nTo use this server with an AI assistant that supports MCP, add the following configuration to your MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"redshift\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"src/redshift_mcp_server\", \"run\", \"server.py\"],\n      \"env\": {\n        \"RS_HOST\": \"your-redshift-cluster.region.redshift.amazonaws.com\",\n        \"RS_PORT\": \"5439\",\n        \"RS_USER\": \"your_username\",\n        \"RS_PASSWORD\": \"your_password\",\n        \"RS_DATABASE\": \"your_database\",\n        \"RS_SCHEMA\": \"your_schema\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n### Resources\n\nThe server provides the following resources:\n\n- `rs:///schemas` - Lists all schemas in the database\n- `rs:///{schema}/tables` - Lists all tables in a specific schema\n- `rs:///{schema}/{table}/ddl` - Gets the DDL script for a specific table\n- `rs:///{schema}/{table}/statistic` - Gets statistics for a specific table\n\n### Tools\n\nThe server provides the following tools:\n\n- `execute_sql` - Executes a SQL query on the Redshift cluster\n- `analyze_table` - Analyzes a table to collect statistics information\n- `get_execution_plan` - Gets the execution plan with runtime statistics for a SQL query\n\n## Examples\n\n### Listing schemas\n\n```\naccess_mcp_resource(\"redshift-mcp-server\", \"rs:///schemas\")\n```\n\n### Listing tables in a schema\n\n```\naccess_mcp_resource(\"redshift-mcp-server\", \"rs:///public/tables\")\n```\n\n### Getting table DDL\n\n```\naccess_mcp_resource(\"redshift-mcp-server\", \"rs:///public/users/ddl\")\n```\n\n### Executing SQL\n\n```\nuse_mcp_tool(\"redshift-mcp-server\", \"execute_sql\", {\"sql\": \"SELECT * FROM public.users LIMIT 10\"})\n```\n\n### Analyzing a table\n\n```\nuse_mcp_tool(\"redshift-mcp-server\", \"analyze_table\", {\"schema\": \"public\", \"table\": \"users\"})\n```\n\n### Getting execution plan\n\n```\nuse_mcp_tool(\"redshift-mcp-server\", \"get_execution_plan\", {\"sql\": \"SELECT * FROM public.users WHERE user_id = 123\"})\n```\n\n## Development\n\n### Project structure\n\n```\nredshift-mcp-server/\n├── src/\n│   └── redshift_mcp_server/\n│       ├── __init__.py\n│       └── server.py\n├── pyproject.toml\n└── README.md\n```\n\n### Dependencies\n\n- `mcp[cli]>=1.5.0` - Model Context Protocol SDK\n- `python-dotenv>=1.1.0` - For loading environment variables from .env files\n- `redshift-connector>=2.1.5` - Python connector for Amazon Redshift\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "redshift",
        "redshift databases",
        "amazon redshift",
        "databases secure"
      ],
      "category": "databases"
    },
    "Muzain187--TG_MCP": {
      "owner": "Muzain187",
      "name": "TG_MCP",
      "url": "https://github.com/Muzain187/TG_MCP",
      "imageUrl": "/freedevtools/mcp/pfp/Muzain187.webp",
      "description": "Expose TigerGraph graph database operations as structured tools and URI-based resources for MCP agents, facilitating schema introspection, query execution, and vertex/edge upsert through a Python interface.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-04T21:33:32Z",
      "readme_content": "# TG_MCP\r\n![Integration](https://github.com/user-attachments/assets/cea6c4a3-1293-4bac-8c20-d1ecd7f0e866)\r\n\r\n\r\nA lightweight Python interface that exposes TigerGraph operations (queries, schema, vertices, edges, UDFs) as structured tools and URI-based resources for MCP agents.\r\n\r\n## Table of Contents\r\n\r\n1. [Features](#features)  \r\n2. [Project Structure](#project-structure)  \r\n3. [Installation](#installation)  \r\n4. [Configuration](#configuration)  \r\n5. [Connecting to Claude](#connecting-to-claude)\r\n6. [Examples](#examples)  \r\n7. [Contributing](#contributing)  \r\n8. [License](#license)  \r\n\r\n## Features\r\n\r\n- **Schema Introspection**  \r\n  Retrieve full graph schema (vertex & edge types).\r\n\r\n- **Query Execution**  \r\n  Run installed GSQL queries or raw GSQL strings with parameters.\r\n\r\n- **Vertex & Edge Upsert**  \r\n  Create or update vertices and edges programmatically.\r\n\r\n- **Resource URIs**  \r\n  Access graph objects through `tgraph://vertex/...` and `tgraph://query/...` URIs.\r\n\r\n- **UDF & Algorithm Listing**  \r\n  Fetch installed user-defined functions and GDS algorithm catalogs.\r\n\r\n## Project Structure\r\n\r\n```\r\nTG_MCP/\r\n├── config.py            # Environment config (HOST, GRAPH, SECRET)\r\n├── tg_client.py         # Encapsulates TigerGraphConnection and core operations\r\n├── tg_tools.py          # `@mcp.tool` definitions exposing client methods\r\n├── tg_resources.py      # `@mcp.resource` URI handlers\r\n├── main.py              # MCP app bootstrap (`mcp.run()`)\r\n├── pyproject.toml       # Project metadata & dependencies\r\n├── LICENSE              # MIT License\r\n└── .gitignore           # OS/Python ignore rules\r\n```\r\n\r\n## Installation\r\n\r\n1. **Clone the repo**  \r\n   ```bash\r\n   git clone https://github.com/Muzain187/TG_MCP.git\r\n   cd TG_MCP\r\n   ```\r\n\r\n2. **Create & activate a virtual environment**  \r\n   ```bash\r\n   python3 -m venv venv\r\n   source venv/bin/activate\r\n   ```\r\n\r\n3. **Install dependencies**  \r\n   ```bash\r\n   pip install .\r\n   ```\r\n   > Requires `mcp[cli]>=1.6.0` and `pyTigerGraph>=1.8.6`.\r\n\r\n## Configuration\r\n\r\nSet the following environment variables before running:\r\n\r\n```bash\r\nexport TG_HOST=https://<your-tigergraph-host>\r\nexport TG_GRAPH=<your-graph-name>\r\nexport TG_SECRET=<your-api-secret>\r\n```\r\n\r\nThese are read by `config.py`.\r\n\r\n\r\n## Connecting to Claude\r\n\r\nThis MCP server can be installed into the **Claude Desktop** client so that Claude can invoke your TigerGraph tools directly:\r\n\r\n```bash\r\nuv run mcp install main.py\r\n```\r\n\r\nAfter running the above, restart Claude Desktop and you’ll see your MCP tools available via the hammer 🛠 icon.\r\n\r\n## Examples:\r\n![image](https://github.com/user-attachments/assets/3ba65cc2-8e24-45d5-8f12-c4b76739fb39)\r\n\r\n![image](https://github.com/user-attachments/assets/032b85b9-4021-438e-9380-1ac96ae6c601)\r\n\r\n\r\n## Contributing\r\n\r\n1. Fork the repository  \r\n2. Create a feature branch  \r\n   ```bash\r\n   git checkout -b feature/YourFeature\r\n   ```\r\n3. Commit your changes  \r\n   ```bash\r\n   git commit -m \"Add YourFeature\"\r\n   ```\r\n4. Push to branch  \r\n   ```bash\r\n   git push origin feature/YourFeature\r\n   ```\r\n5. Open a Pull Request  \r\n\r\nPlease ensure all new code is covered by tests and follows PEP-8 style.\r\n\r\n## License\r\n\r\nThis project is licensed under the **MIT License**.  \r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "tigergraph",
        "database",
        "graph database",
        "access schema",
        "databases secure"
      ],
      "category": "databases"
    },
    "NetanelBollag--simple-psql-mcp": {
      "owner": "NetanelBollag",
      "name": "simple-psql-mcp",
      "url": "https://github.com/NetanelBollag/simple-psql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/NetanelBollag.webp",
      "description": "Run SQL queries and manage data within a PostgreSQL database. Connect AI for advanced data manipulation and analysis through simple command execution.",
      "stars": 31,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-06T18:53:26Z",
      "readme_content": "# Simple PostgreSQL MCP Server\n\nThis is a template project for those looking to build their own MCP servers. I designed it to be dead simple to understand and adapt - the code is straightforward with MCP docs attached so you can quickly get up to speed.\n\n## What is MCP?\n\n*TL;DR - It's a way to write plugins for AI*\n\nModel Context Protocol (MCP) is a standard way for LLMs to interact with external tools and data. In a nutshell:\n\n- **Tools** allow the LLM to execute commands (like running a database query)\n- **Resources** are data you can attach to conversations (like attaching a file to a prompt)\n- **Prompts** are templates that generate consistent LLM instructions\n\n## Features\n\nThis PostgreSQL MCP server implements:\n\n1. **Tools**\n   - `execute_query` - Run SQL queries against your database\n   - `test_connection` - Verify the database connection is working\n\n2. **Resources**\n   - `db://tables` - List of all tables in the schema\n   - `db://tables/{table_name}` - Schema information for a specific table\n   - `db://schema` - Complete schema information for all tables in the database\n\n3. **Prompts**\n   - Query generation templates\n   - Analytical query builders\n   - Based on the templates in this repo\n\n## Prerequisites\n\n- Python 3.8+\n- [uv](https://github.com/astral-sh/uv) - Modern Python package manager and installer\n- npx (included with Node.js)\n- PostgreSQL database you can connect to\n\n## Quick Setup\n\n1. **Create a virtual environment and install dependencies:**\n   ```bash\n   # Create a virtual environment with uv\n   uv venv\n   \n   # Activate the virtual environment\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   \n   # Install dependencies\n   uv pip install -r requirements.txt\n   ```\n\n2. **Run the server with the MCP Inspector:**\n   ```bash\n   # Replace with YOUR actual database credentials\n   npx @modelcontextprotocol/inspector uv --directory . run postgres -e DSN=postgresql://username:password@hostname:port/database -e SCHEMA=public\n   ```\n\n   > Note: If this is your first time running npx, you'll be prompted to approve the installation. Type 'y' to proceed.\n\n   After running this command, you'll see the MCP Inspector interface launched in your browser. You should see a message like:\n   ```\n   MCP Inspector is up and running at http://localhost:5173\n   ```\n\n   If the browser doesn't open automatically, copy and paste the URL into your browser. You should see something like this:\n   \n3. **Using the Inspector:**\n   - Click the \"Connect\" button in the interface (unless there's an error message in the console on the bottom left)\n   - Explore the \"Tools\", \"Resources\", and \"Prompts\" tabs to see available functionality\n   - Try clicking on listed commands or typing resource names to retrieve resources and prompts\n   - The interface allows you to test queries and see how the MCP server responds\n\n4. **Take a look at the official docs**\n\n   Official server developers guide: https://modelcontextprotocol.io/quickstart/server\n\n   More on the inspector: https://modelcontextprotocol.io/docs/tools/inspector\n\n## Connect Your AI Tool to the Server\n\nYou can configure the MCP server for your AI assistant by creating an MCP configuration file:\n\n```json\n{\n   \"mcpServers\": {\n      \"postgres\": {\n         \"command\": \"/path/to/uv\",\n         \"args\": [\n            \"--directory\",\n            \"/path/to/simple-psql-mcp\",\n            \"run\",\n            \"postgres\"\n         ],\n         \"env\": {\n            \"DSN\": \"postgresql://username:password@localhost:5432/my-db\",\n            \"SCHEMA\": \"public\"\n         }\n      }\n   }\n}\n```\n\nAlternatively, you can generate this config file using the included script:\n\n```bash\n# Make the script executable\nchmod +x generate_mcp_config.sh\n\n# Run the configuration generator\n./generate_mcp_config.sh\n```\n\nWhen prompted, enter your PostgreSQL DSN and schema name.\n\n### How to use it\n\nYou can now ask the LLM questions about your data in natural language:\n- \"What are all the tables in my database?\"\n- \"Show me the top 5 users by creation date\"\n- \"Count addresses by state\"\n\nFor testing, Claude Desktop supports MCP natively and works with all features (tools, resources, and prompts) right out of the box.\n\n## Example Database (Optional)\n\nIf you don't have a database ready or encounter connection issues, you can use the included example database:\n\n```bash\n# Make the script executable\nchmod +x example-db/create-db.sh\n\n# Run the database setup script\n./example-db/create-db.sh\n```\n\nThis script creates a Docker container with a PostgreSQL database pre-populated with sample users and addresses tables. After running, you can connect using:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory . run postgres -e DSN=postgresql://postgres:postgres@localhost:5432/user_database -e SCHEMA=public\n```\n\n## Next Steps\n\nTo extend this project with your own MCP servers:\n\n1. Create a new directory under `/src` (e.g., `/src/my-new-mcp`)\n2. Implement your MCP server following the PostgreSQL example\n3. Add your new MCP to `pyproject.toml`:\n\n```toml\n[project.scripts]\npostgres = \"src.postgres:main\"\nmy-new-mcp = \"src.my-new-mcp:main\"\n```\n\nYou can then run your new MCP with:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory . run my-new-mcp\n```\n\n## Documentation\n\n- MCP docs included for easy LLM development\n- Based on the approach at: https://modelcontextprotocol.io/tutorials/building-mcp-with-llms\n\n## Security\n\nThis is an experimental project meant to empower developers to create their own MCP server. I did minimum to make sure it won't die immediately when you try it, but be careful - it's very easy to run SQL injections with this tool. The server will check if the query starts with SELECT, but beyond that nothing is guaranteed. TL;DR - don't run in production unless you're the founder and there are no paying clients.\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "postgresql",
        "database",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "NightTrek--Supabase-MCP": {
      "owner": "NightTrek",
      "name": "Supabase-MCP",
      "url": "https://github.com/NightTrek/Supabase-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/NightTrek.webp",
      "description": "Interact with Supabase databases, executing complex queries and generating TypeScript types. Simplify database interactions and enhance type management through a seamless interface.",
      "stars": 13,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-29T02:33:25Z",
      "readme_content": "# Supabase MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Supabase databases. This server provides tools for querying tables and generating TypeScript types through the MCP interface.\n\n## Features\n\n- **Query Tables**: Execute queries on any table with support for:\n  - Schema selection\n  - Column filtering\n  - Where clauses with multiple operators\n  - Pagination\n  - Error handling\n\n- **Type Generation**: Generate TypeScript types for your database:\n  - Support for any schema (public, auth, api, etc.)\n  - Works with both local and remote Supabase projects\n  - Direct output to console\n  - Automatic project reference detection\n\n## Prerequisites\n\n1. Node.js (v16 or higher)\n2. A Supabase project (either local or hosted)\n3. Supabase CLI (for type generation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/supabase-mcp-server.git\ncd supabase-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Install the Supabase CLI (required for type generation):\n```bash\n# Using npm\nnpm install -g supabase\n\n# Or using Homebrew on macOS\nbrew install supabase/tap/supabase\n```\n\n## Configuration\n\n1. Get your Supabase credentials:\n   - For hosted projects:\n     1. Go to your Supabase project dashboard\n     2. Navigate to Project Settings > API\n     3. Copy the Project URL and service_role key (NOT the anon key)\n   \n   - For local projects:\n     1. Start your local Supabase instance\n     2. Use the local URL (typically http://localhost:54321)\n     3. Use your local service_role key\n\n2. Configure environment variables:\n```bash\n# Create a .env file (this will be ignored by git)\necho \"SUPABASE_URL=your_project_url\nSUPABASE_KEY=your_service_role_key\" > .env\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n## Integration with Claude Desktop\n\n1. Open Claude Desktop settings:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/supabase-mcp-server/build/index.js\"],\n      \"env\": {\n        \"SUPABASE_URL\": \"your_project_url\",\n        \"SUPABASE_KEY\": \"your_service_role_key\"\n      }\n    }\n  }\n}\n```\n\n## Integration with VSCode Extension\n\n1. Open VSCode settings:\n   - macOS: `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n   - Windows: `%APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n   - Linux: `~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n\n2. Add the server configuration (same format as Claude Desktop).\n\n## Usage Examples\n\n### Querying Tables\n\n```typescript\n// Query with schema selection and where clause\n<use_mcp_tool>\n<server_name>supabase</server_name>\n<tool_name>query_table</tool_name>\n<arguments>\n{\n  \"schema\": \"public\",\n  \"table\": \"users\",\n  \"select\": \"id,name,email\",\n  \"where\": [\n    {\n      \"column\": \"is_active\",\n      \"operator\": \"eq\",\n      \"value\": true\n    }\n  ]\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Generating Types\n\n```typescript\n// Generate types for public schema\n<use_mcp_tool>\n<server_name>supabase</server_name>\n<tool_name>generate_types</tool_name>\n<arguments>\n{\n  \"schema\": \"public\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Available Tools\n\n### query_table\nQuery a specific table with schema selection and where clause support.\n\nParameters:\n- `schema` (optional): Database schema (defaults to public)\n- `table` (required): Name of the table to query\n- `select` (optional): Comma-separated list of columns\n- `where` (optional): Array of conditions with:\n  - `column`: Column name\n  - `operator`: One of: eq, neq, gt, gte, lt, lte, like, ilike, is\n  - `value`: Value to compare against\n\n### generate_types\nGenerate TypeScript types for your Supabase database schema.\n\nParameters:\n- `schema` (optional): Database schema (defaults to public)\n\n## Troubleshooting\n\n### Type Generation Issues\n\n1. Ensure Supabase CLI is installed:\n```bash\nsupabase --version\n```\n\n2. For local projects:\n   - Make sure your local Supabase instance is running\n   - Verify your service_role key is correct\n\n3. For hosted projects:\n   - Confirm your project ref is correct (extracted from URL)\n   - Verify you're using the service_role key, not the anon key\n\n### Query Issues\n\n1. Check your schema and table names\n2. Verify column names in select and where clauses\n3. Ensure your service_role key has necessary permissions\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch: `git checkout -b feature/my-feature`\n3. Commit your changes: `git commit -am 'Add my feature'`\n4. Push to the branch: `git push origin feature/my-feature`\n5. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase databases",
        "nighttrek supabase",
        "interact supabase"
      ],
      "category": "databases"
    },
    "OpenLinkSoftware--mcp-odbc-server": {
      "owner": "OpenLinkSoftware",
      "name": "mcp-odbc-server",
      "url": "https://github.com/OpenLinkSoftware/mcp-odbc-server",
      "imageUrl": "/freedevtools/mcp/pfp/OpenLinkSoftware.webp",
      "description": "Provides access to ODBC data sources for executing SQL queries and interacting with databases through a standardized interface. Facilitates seamless database management and data retrieval for AI models using ODBC-compatible drivers.",
      "stars": 10,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T00:08:05Z",
      "readme_content": "# OpenLink MCP Server for ODBC\n\nThis document covers the set up and use of a generic ODBC server for the Model Context Protocol (MCP), referred to as an `mcp-odbc` server. It has been developed to provide Large Language Models with transparent access to ODBC-accessible data sources via a Data Source Name configured for a specific ODBC Connector (also called an ODBC Driver).\n\n![mcp-client-and-servers|648x499](https://www.openlinksw.com/data/gifs/mcp-client-and-servers.gif)\n\n## Server Implementation\n\nThis **MCP Server for ODBC** is a small TypeScript layer built on top of `node-odbc`. It routes calls to the host system's local ODBC Driver Manager via `node.js` (specifically using `npx` for TypeScript).\n\n## Operating Environment Set Up & Prerequisites\n\nWhile the examples that follow are oriented toward the Virtuoso ODBC Connector, this guide will also work with other ODBC Connectors. We *strongly* encourage code contributions and submissions of usage demos related to other database management systems (DBMS) for incorporation into this project.\n\n### Key System Components\n\n1. Check the `node.js` version. If it's not `21.1.0` or higher, upgrade or install explicitly using:\n   ```sh\n   nvm install v21.1.0\n   ```\n2. Install MCP components using: \n   ```sh\n   npm install @modelcontextprotocol/sdk zod tsx odbc dotenv\n   ```\n3. Set the `nvm` version using: \n   ```sh\n   nvm alias default 21.1.0\n   ```\n\n### Installation\n\n1. Run \n   ```sh\n   git clone https://github.com/OpenLinkSoftware/mcp-odbc-server.git\n   ```\n2. Change directory \n   ```sh\n   cd mcp-odbc-server\n   ```\n3. Run \n   ```sh\n   npm init -y\n   ```\n4. Run \n   ```sh\n   npm install @modelcontextprotocol/sdk zod tsx odbc dotenv\n   ```\n\n### unixODBC Runtime Environment Checks\n\n1. Check installation configuration (i.e., location of key INI files) by running: \n   ```sh\n   odbcinst -j\n   ```\n2. List available data source names (DSNs) by running: \n   ```sh\n   odbcinst -q -s\n   ```\n\n### Environment Variables\nAs good security practice, you should use the `.env` file situated in the same directory as the `mcp-ser` to set bindings for the ODBC Data Source Name (`ODBC_DSN`), the User (`ODBC_USER`), the Password (`ODBC_PWD`), the ODBC INI (`ODBCINI`), and, if you want to use the OpenLink AI Layer (OPAL) via ODBC, the target Large Language Model (LLM) API Key (`API_KEY`).\n\n```sh\nAPI_KEY=sk-xxx\nODBC_DSN=Local Virtuoso\nODBC_USER=dba\nODBC_PASSWORD=dba\nODBCINI=/Library/ODBC/odbc.ini \n```\n\n# Usage\n\n## Tools\nAfter successful installation, the following tools will be available to MCP client applications.\n\n### Overview\n\n|name                 |description|\n|:---                 |:---|\n|`get_schemas`        |List database schemas accessible to connected database management system (DBMS).|\n|`get_tables`         |List tables associated with a selected database schema.|\n|`describe_table`     |Provide the description of a table associated with a designated database schema. This includes information about column names, data types, null handling, autoincrement, primary key, and foreign keys|\n|`filter_table_names` |List tables associated with a selected database schema, based on a substring pattern from the `q` input field.|\n|`query_database`     |Execute a SQL query and return results in JSON Lines (JSONL) format.|\n|`execute_query`      |Execute a SQL query and return results in JSON Lines (JSONL) format.|\n|`execute_query_md`   |Execute a SQL query and return results in Markdown table format.|\n|`spasql_query`       |Execute a SPASQL query and return results.|\n|`sparql_query`       |Execute a SPARQL query and return results.|\n|`virtuoso_support_ai`|Interact with the Virtuoso Support Assistant/Agent — a Virtuoso-specific feature for interacting with LLMs|\n\n### Detailed Description\n\n- **`get_schemas`**\n  - Retrieve and return a list of all schema names from the connected database.\n  - Input parameters:\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string array of schema names.\n\n- **`get_tables`**\n  - Retrieve and return a list containing information about tables in a specified schema. If no schema is provided, uses the connection's default schema.\n  - Input parameters:\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string containing table information (e.g., `TABLE_CAT`, `TABLE_SCHEM`, `TABLE_NAME`, `TABLE_TYPE`).\n\n- **`filter_table_names`**\n  - Filters and returns information about tables whose names contain a specific substring.\n  - Input parameters:\n    - `q` (string, required): The substring to search for within table names.\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string containing information for matching tables.\n\n- **`describe_table`**\n  - Retrieve and return detailed information about the columns of a specific table.\n  - Input parameters:\n    - `schema` (string, required): The database schema name containing the table.\n    - `table` (string, required): The name of the table to describe.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string describing the table's columns (e.g., `COLUMN_NAME`, `TYPE_NAME`, `COLUMN_SIZE`, `IS_NULLABLE`).\n\n- **`query_database`**\n  - Execute a standard SQL query and return the results in JSON format.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a JSON string.\n\n- **`query_database_md`**\n  - Execute a standard SQL query and return the results formatted as a Markdown table.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a Markdown table string.\n\n- **`query_database_jsonl`**\n  - Execute a standard SQL query and return the results in JSON Lines (JSONL) format (one JSON object per line).\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a JSONL string.\n\n- **`spasql_query`**\n  - Execute a SPASQL (SQL/SPARQL hybrid) query return results. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `query` (string, required): The SPASQL query string.\n    - `max_rows` (number, optional): Maximum number of rows to return. Defaults to `20`.\n    - `timeout` (number, optional): Query timeout in milliseconds. Defaults to `30000`, i.e., 30 seconds.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns the result from the underlying stored procedure call (e.g., `Demo.demo.execute_spasql_query`).\n\n- **`sparql_query`**\n  - Execute a SPARQL query and return results. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `query` (string, required): The SPARQL query string.\n    - `format` (string, optional): Desired result format. Defaults to `'json'`.\n    - `timeout` (number, optional): Query timeout in milliseconds. Defaults to `30000`, i.e., 30 seconds.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns the result from the underlying function call (e.g., `\"UB\".dba.\"sparqlQuery\"`).\n\n- **`virtuoso_support_ai`**\n  - Utilizes a Virtuoso-specific AI Assistant function, passing a prompt and optional API key. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `prompt` (string, required): The prompt text for the AI function.\n    - `api_key` (string, optional): API key for the AI service. Defaults to `\"none\"`.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns the result from the AI Support Assistant function call (e.g., `DEMO.DBA.OAI_VIRTUOSO_SUPPORT_AI`).\n\n## Basic Installation Testing & Troubleshooting\n\n### MCP Inspector Tool\n\n#### Canonical MCP Inspector Tool Edition\n\n1. Start the inspector from the mcp-server directory/folder using the following command:\n    ```sh\n    ODBCINI=/Library/ODBC/odbc.ini npx -y @modelcontextprotocol/inspector npx tsx ./src/main.ts \n    ```\n2. Click on the \"Connect\" button, then click on the \"Tools\" tab to get started.\n\n    [![MCP Inspector](https://www.openlinksw.com/data/screenshots/mcp-server-inspector-demo-1.png)](https://www.openlinksw.com/data/screenshots/mcp-server-inspector-demo-1.png)\n\n#### OpenLink MCP Inspector Tool Edition\n\nThis is a fork of the canonical edition that includes a JSON handling bug fix related to use with this MCP Server.\n\n1. run\n   ```sh\n   git clone git@github.com:OpenLinkSoftware/inspector.git\n   cd inspector\n   ```\n2. run\n   ```sh\n   npm run start\n   ```\n3. Provide the following value in the `Arguments` input field of MCP Inspectors UI from http://localhost:6274\n   ```sh\n   tsx /path/to/mcp-odbc-server/src/main.ts\n   ```\n4. Click on the `Connect` button to initialize your session with the designated MCP Server\n\n\n### Apple Silicon (ARM64) Compatibility with MCP ODBC Server Issues\n\n#### Node x86_64 vs arm64 Conflict Issue\n\nThe x86_64 rather than arm64 edition of `node` may be in place, but the ODBC bridge and MCP server are arm64-based components.\n\nYou can solve this problem by performing the following steps:\n\n1. Uninstall the x86_64 edition of `node` by running:\n   ```sh\n    nvm uninstall 21.1.0\n   ```\n2. Run the following command to confirm your current shell is in arm64 mode:\n   ```sh\n   arch\n   ```\n   - if that returns x86_64, then run the following command to change the active mode:\n     ```\n     arch arm64\n     ```\n3. Install the arm64 edition of `node` by running:\n   ```sh\n   nvm install 21.1.0\n   ```\n\n#### Node to ODBC Bridge Layer Incompatibility\n\nWhen attempting to use a Model Context Protocol (MCP) ODBC Server on Apple Silicon machines, you may encounter architecture mismatch errors. These occur because the `Node.js` ODBC native module (`odbc.node`) is compiled for ARM64 architecture, but the x86_64-based edition of the unixODBC runtime is being loaded.\n\nTypical error message:\n\n```\nError: dlopen(...odbc.node, 0x0001): tried: '...odbc.node' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))\n```\n\nYou solve this problem by performing the following steps:\n\n1. Verify your `Node.js` is running in ARM64 mode:\n\n   ```bash\n   node -p \"process.arch\"  # Should output: `arm64`\n   ```\n\n2. Install unixODBC for ARM64:\n\n   ```bash\n   # Verify Homebrew is running in ARM64 mode\n   which brew  # Should point to /opt/homebrew/bin/brew\n   \n   # Remove existing unixODBC\n   brew uninstall --force unixodbc\n   \n   # Install ARM64 version\n   arch -arm64 brew install unixodbc\n   ```\n\n3. Rebuild the Node.js ODBC module for ARM64:\n\n   ```bash\n   # Navigate to your project\n   cd /path/to/mcp-odbc-server\n   \n   # Remove existing module\n   rm -rf node_modules/odbc\n   \n   # Set architecture environment variable\n   export npm_config_arch=arm64\n   \n   # Reinstall with force build\n   npm install odbc --build-from-source\n   ```\n\n4. Verify the module is now ARM64:\n\n   ```bash\n   file node_modules/odbc/lib/bindings/napi-v8/odbc.node\n   # Should show \"arm64\" instead of \"x86_64\"\n   ```\n\n#### Key Points\n\n- Both unixODBC and the `Node.js` ODBC module must be ARM64-compatible\n- Using environment variables (`export npm_config_arch=arm64`) is more reliable than `npm config` commands\n- Always verify architecture with the `file` command or `node -p \"process.arch\"`\n- When using Homebrew on Apple Silicon, commands can be prefixed with `arch -arm64` to force use of ARM64 binaries\n\n## MCP Application Usage\n\n### Claude Desktop Configuration\n\nThe path for this config file is: `~{username}/Library/Application Support/Claude/claude_desktop_config.json`.\n\n```json\n{\n    \"mcpServers\": {\n        \"ODBC\": {\n            \"command\": \"/path/to/.nvm/versions/node/v21.1.0/bin/node\",\n            \"args\": [\n                \"/path/to/mcp-odbc-server/node_modules/.bin/tsx\",\n                \"/path/to/mcp-odbc-server/src/main.ts\"\n            ],\n            \"env\": {\n                \"ODBCINI\": \"/Library/ODBC/odbc.ini\",\n                \"NODE_VERSION\": \"v21.1.0\",\n                \"PATH\": \"~/.nvm/versions/node/v21.1.0/bin:${PATH}\"\n            },\n            \"disabled\": false,\n            \"autoApprove\": []\n        }\n    }\n}\n```\n\n### Claude Desktop Usage\n\n1. Start the application.\n2. Apply configuration (from above) via Settings | Developer user interface.\n3. Ensure you have a working ODBC connection to a Data Source Name (DSN).\n4. Present a prompt requesting query execution, e.g.,\n   ```\n   Execute the following query: SELECT TOP * from Demo..Customers\n   ```\n\n    [![Claude Desktop](https://www.openlinksw.com/data/screenshots/claude-desktp-mcp-odbc-server-demo-1.png)](https://www.openlinksw.com/data/screenshots/claude-desktp-mcp-odbc-server-demo-1.png)\n\n### Cline (Visual Studio Extension) Configuration\n\nThe path for this config file is: `~{username}/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"ODBC\": {\n      \"command\": \"/path/to/.nvm/versions/node/v21.1.0/bin/node\",\n      \"args\": [\n        \"/path/to/mcp-odbc-server/node_modules/.bin/tsx\",\n        \"/path/to/mcp-odbc-server/src/main.ts\"\n      ],\n      \"env\": {\n        \"ODBCINI\": \"/Library/ODBC/odbc.ini\",\n        \"NODE_VERSION\": \"v21.1.0\",\n        \"PATH\": \"/path/to/.nvm/versions/node/v21.1.0/bin:${PATH}\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Cline (Visual Studio Extension) Usage\n\n1. Use Shift+Command+`P` to open the Command Palette.\n2. Type in: `Cline`.\n3. Select: `Cline View`, which opens the Cline UI in the VSCode sidebar.\n4. Use the four-squares icon to access the UI for installing and configuring MCP servers.\n6. Apply the Cline Config (from above).\n7. Return to the extension's main UI and start a new task requesting processing of the following prompt:\n   ```\n   \"Execute the following query: SELECT TOP 5 * from Demo..Customers\"\n   ```\n\n    [![Cline Extension](https://www.openlinksw.com/data/screenshots/cline-extension-mcp-server-odbc-demo-1.png)](https://www.openlinksw.com/data/screenshots/cline-extension-mcp-server-odbc-demo-1.png)\n\n### Cursor Configuration\n\nUse the settings gear to open the configuration menu that includes the MCP menu item for registering and configuring `mcp servers`.\n\n### Cursor Usage\n\n1. Use the Command+`I` or Control+`I` key combination to open the Chat Interface.\n2. Select `Agent` from the drop-down at the bottom left of the UI, where the default is `Ask`.\n3. Enter your prompt, qualifying the use of the `mcp-server for odbc` using the pattern: `@odbc {rest-of-prompt}`.\n4. Click on \"Accept\" to execute the prompt.\n   \n   [![Cursor Editor](https://www.openlinksw.com/data/screenshots/cursor-editor-mcp-config-for-odbc-server-1.png)](https://www.openlinksw.com/data/screenshots/cursor-editor-mcp-config-for-odbc-server-1.png)\n\n# Related\n\n* [MCP Inspector Usage Screencast](https://www.openlinksw.com/data/screencasts/mcp-inspector-odbc-sparql-spasql-demo-1.mp4)\n* [Basic Claude Desktop Usage Screencast](https://www.openlinksw.com/data/screencasts/claude-odbc-mcp-sql-spasql-demo-1.mp4)\n* [Basic Cline Visual Studio Code Extension Usage Screencast](https://www.openlinksw.com/data/screencasts/cline-vscode-mcp-odbc-sql-spasql-1.mp4)\n* [Basic Cursor Editor Usage Screencast](https://www.openlinksw.com/data/screencasts/cursor-odbc-mcp-sql-spasql-demo-1.mp4)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "odbc",
        "databases",
        "database",
        "access odbc",
        "using odbc",
        "odbc compatible"
      ],
      "category": "databases"
    },
    "OpenLinkSoftware--mcp-pyodbc-server": {
      "owner": "OpenLinkSoftware",
      "name": "mcp-pyodbc-server",
      "url": "https://github.com/OpenLinkSoftware/mcp-pyodbc-server",
      "imageUrl": "/freedevtools/mcp/pfp/OpenLinkSoftware.webp",
      "description": "Connect to databases using ODBC, enabling real-time data fetching, schema exploration, and query execution. Provides functionalities for managing schemas, tables, and stored procedures across various database management systems.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-22T20:48:34Z",
      "readme_content": "---\n# OpenLink MCP Server for ODBC via PyODBC\n\nA lightweight MCP (Model Context Protocol) server for ODBC built with **FastAPI** and **pyodbc**. This server is compatible with Virtuoso DBMS and any other DBMS backend that has an ODBC driver.\n\n![mcp-client-and-servers|648x499](https://www.openlinksw.com/data/gifs/mcp-client-and-servers.gif)\n\n---\n\n## Features\n\n- **Get Schemas**: Fetch and list all schema names from the connected database.\n- **Get Tables**: Retrieve table information for specific schemas or all schemas.\n- **Describe Table**: Generate a detailed description of table structures, including:\n  - Column names and data types\n  - Nullable attributes\n  - Primary and foreign keys\n- **Search Tables**: Filter and retrieve tables based on name substrings.\n- **Execute Stored Procedures**: When connected to Virtuoso, execute stored procedures and retrieve results.\n- **Execute Queries**:\n  - JSONL result format: Optimized for structured responses.\n  - Markdown table format: Ideal for reporting and visualization.\n\n---\n\n## Prerequisites\n\n1. **Install uv**:\n   ```bash\n   pip install uv\n   ```\n   Or use Homebrew:\n   ```bash\n   brew install uv\n   ```\n\n2. **unixODBC Runtime Environment Checks**:\n\n3. Check installation configuration (i.e., location of key INI files) by running: `odbcinst -j`\n\n4. List available data source names by running: `odbcinst -q -s`\n   \n5. **ODBC DSN Setup**: Configure your ODBC Data Source Name (typically in `~/.odbc.ini`) for the target database. Example for Virtuoso DBMS:\n   ```\n   [VOS]\n   Description = OpenLink Virtuoso\n   Driver = /path/to/virtodbcu_r.so\n   Database = Demo\n   Address = localhost:1111\n   WideAsUTF16 = Yes\n   ```\n\n---\n\n## Installation\n\nClone this repository:\n\n```bash\ngit clone https://github.com/OpenLinkSoftware/mcp-pyodbc-server.git\ncd mcp-pyodbc-server\n```\n\n## Environment Variables \n\nUpdate your `.env` by overriding the defaults to match your preferences.\n```\nODBC_DSN=VOS\nODBC_USER=dba\nODBC_PASSWORD=dba\nAPI_KEY=xxx\n```\n---\n\n## Configuration\n\nFor **Claude Desktop** users:\n\nAdd the following to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"my_database\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/mcp-pyodbc-server\", \"run\", \"mcp-pyodbc-server\"],\n      \"env\": {\n        \"ODBC_DSN\": \"dsn_name\",\n        \"ODBC_USER\": \"username\",\n        \"ODBC_PASSWORD\": \"password\",\n        \"API_KEY\": \"sk-xxx\"\n      }\n    }\n  }\n}\n```\n---\n## Usage\n\n### Tools Provided\nAfter successful installation, the following tools will be available to MCP client applications.\n\n#### Overview\n|name|description|\n|---|---|\n|`podbc_get_schemas`|List database schemas accessible to connected database management system (DBMS).|\n|`podbc_get_tables`|List tables associated with a selected database schema.|\n|`podbc_describe_table`|Provide the description of a table associated with a designated database schema. This includes information about column names, data types, null handling, autoincrement, primary keys, and foreign keys|\n|`podbc_filter_table_names`|List tables, based on a substring pattern from the `q` input field, associated with a selected database schema.|\n|`podbc_query_database`|Execute a SQL query and return results in JSONL format.|\n|`podbc_execute_query`|Execute a SQL query and return results in JSONL format.|\n|`podbc_execute_query_md`|Execute a SQL query and return results in Markdown table format.|\n|`podbc_spasql_query`|Execute a SPASQL query and return results.|\n|`podbc_virtuoso_support_ai`|Interact with the Virtuoso Support Assistant/Agent -- a Virtuoso-specific feature for interacting with LLMs|\n\n#### Detailed Description\n\n- **`podbc_get_schemas`**\n  - Retrieve and return a list of all schema names from the connected database.\n  - Input parameters:\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string array of schema names.\n\n- **`podbc_get_tables`**\n  - Retrieve and return a list containing information about tables in a specified schema. If no schema is provided, it uses the connection's default schema.\n  - Input parameters:\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string containing table information (e.g., `TABLE_CAT`, `TABLE_SCHEM`, `TABLE_NAME`, `TABLE_TYPE`).\n\n- **`podbc_filter_table_names`**\n  - Filters and returns information about tables whose names contain a specific substring.\n  - Input parameters:\n    - `q` (string, required): The substring to search for within table names.\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string containing information for matching tables.\n\n- **`podbc_describe_table`**\n  - Retrieve and return detailed information about the columns of a specific table.\n  - Input parameters:\n    - `schema` (string, required): The database schema name containing the table.\n    - `table` (string, required): The name of the table to describe.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns a JSON string describing the table's columns (e.g., `COLUMN_NAME`, `TYPE_NAME`, `COLUMN_SIZE`, `IS_NULLABLE`).\n\n- **`podbc_query_database`**\n  - Execute a standard SQL query and return the results in JSON format.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a JSON string.\n\n- **`podbc_query_database_md`**\n  - Execute a standard SQL query and return the results formatted as a Markdown table.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a Markdown table string.\n\n- **`podbc_query_database_jsonl`**\n  - Execute a standard SQL query and return the results in JSON Lines (JSONL) format (one JSON object per line).\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns query results as a JSONL string.\n\n- **`podbc_spasql_query`**\n  - Execute a SPASQL (SQL/SPARQL hybrid) query return results. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `query` (string, required): The SPASQL query string.\n    - `max_rows` (number, optional): Maximum number of rows to return. Defaults to `20`.\n    - `timeout` (number, optional): Query timeout in milliseconds. Defaults to `30000`.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns the result from the underlying stored procedure call (e.g., `Demo.demo.execute_spasql_query`).\n\n- **`podbc_virtuoso_support_ai`**\n  - Utilizes a Virtuoso-specific AI Assistant function, passing a prompt and optional API key. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `prompt` (string, required): The prompt text for the AI function.\n    - `api_key` (string, optional): API key for the AI service. Defaults to `\"none\"`.\n    - `user` (string, optional): Database username. Defaults to `\"demo\"`.\n    - `password` (string, optional): Database password. Defaults to `\"demo\"`.\n    - `dsn` (string, optional): ODBC data source name. Defaults to `\"Local Virtuoso\"`.\n  - Returns the result from the AI Support Assistant function call (e.g., `DEMO.DBA.OAI_VIRTUOSO_SUPPORT_AI`).\n\n---\n\n## Troubleshooting\n\nFor easier troubleshooting:\n1. Install the MCP Inspector:\n   ```bash\n   npm install -g @modelcontextprotocol/inspector\n   ```\n\n2. Start the inspector:\n   ```bash\n   npx @modelcontextprotocol/inspector uv --directory /path/to/mcp-pyodbc-server run mcp-pyodbc-server\n   ```\n\nAccess the provided URL to troubleshoot server interactions.\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/498645ed-425b-4a6e-bfea-14fa11457da6)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "odbc",
        "pyodbc",
        "databases",
        "pyodbc server",
        "using odbc",
        "odbc enabling"
      ],
      "category": "databases"
    },
    "OpenLinkSoftware--mcp-sqlalchemy-server": {
      "owner": "OpenLinkSoftware",
      "name": "mcp-sqlalchemy-server",
      "url": "https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server",
      "imageUrl": "/freedevtools/mcp/pfp/OpenLinkSoftware.webp",
      "description": "Connect to various databases using ODBC and SQLAlchemy, facilitating data management tasks such as retrieving schemas, tables, and executing queries. It enhances database interactions through a lightweight server built with FastAPI.",
      "stars": 15,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T00:08:10Z",
      "readme_content": "---\n\n# MCP Server ODBC via SQLAlchemy\n\nA lightweight MCP (Model Context Protocol) server for ODBC built with **FastAPI**, **pyodbc**, and **SQLAlchemy**. This server is compatible with Virtuoso DBMS and other DBMS backends that implement a SQLAlchemy provider.\n\n![mcp-client-and-servers|648x499](https://www.openlinksw.com/data/gifs/mcp-client-and-servers.gif)\n\n---\n\n## Features\n\n- **Get Schemas**: Fetch and list all schema names from the connected database.\n- **Get Tables**: Retrieve table information for specific schemas or all schemas.\n- **Describe Table**: Generate a detailed description of table structures, including:\n  - Column names and data types\n  - Nullable attributes\n  - Primary and foreign keys\n- **Search Tables**: Filter and retrieve tables based on name substrings.\n- **Execute Stored Procedures**: In the case of Virtuoso, execute stored procedures and retrieve results.\n- **Execute Queries**:\n  - JSONL result format: Optimized for structured responses.\n  - Markdown table format: Ideal for reporting and visualization.\n\n---\n\n## Prerequisites\n\n1. **Install uv**:\n   ```bash\n   pip install uv\n   ```\n   Or use Homebrew:\n   ```bash\n   brew install uv\n   ```\n\n2. **unixODBC Runtime Environment Checks**:\n\n1. Check installation configuration (i.e., location of key INI files) by running: `odbcinst -j`\n2. List available data source names by running: `odbcinst -q -s`\n   \n3. **ODBC DSN Setup**: Configure your ODBC Data Source Name (`~/.odbc.ini`) for the target database. Example for Virtuoso DBMS:\n   ```\n   [VOS]\n   Description = OpenLink Virtuoso\n   Driver = /path/to/virtodbcu_r.so\n   Database = Demo\n   Address = localhost:1111\n   WideAsUTF16 = Yes\n   ```\n\n3. **SQLAlchemy URL Binding**: Use the format:\n   ```\n   virtuoso+pyodbc://user:password@VOS\n   ```\n\n---\n\n## Installation\n\nClone this repository:\n```bash\ngit clone https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server.git\ncd mcp-sqlalchemy-server\n```\n## Environment Variables \nUpdate your `.env`by overriding the defaults to match your preferences\n```\nODBC_DSN=VOS\nODBC_USER=dba\nODBC_PASSWORD=dba\nAPI_KEY=xxx\n```\n---\n\n## Configuration\n\nFor **Claude Desktop** users:\nAdd the following to `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"my_database\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/mcp-sqlalchemy-server\", \"run\", \"mcp-sqlalchemy-server\"],\n      \"env\": {\n        \"ODBC_DSN\": \"dsn_name\",\n        \"ODBC_USER\": \"username\",\n        \"ODBC_PASSWORD\": \"password\",\n        \"API_KEY\": \"sk-xxx\"\n      }\n    }\n  }\n}\n```\n---\n# Usage \n## Database Management System (DBMS) Connection URLs \nHere are the pyodbc URL examples for connecting to DBMS systems that have been tested using this mcp-server.\n\n| Database      | URL Format                                    |\n|---------------|-----------------------------------------------|\n| Virtuoso DBMS | `virtuoso+pyodbc://user:password@ODBC_DSN`    |\n| PostgreSQL    | `postgresql://user:password@localhost/dbname` |\n| MySQL         | `mysql+pymysql://user:password@localhost/dbname` |\n| SQLite        | `sqlite:///path/to/database.db`               |\nOnce connected, you can interact with your WhatsApp contacts through Claude, leveraging Claude's AI capabilities in your WhatsApp conversations.\n\n## Tools Provided\n\n### Overview\n|name|description|\n|---|---|\n|podbc_get_schemas|List database schemas accessible to connected database management system (DBMS).|\n|podbc_get_tables|List tables associated with a selected database schema.|\n|podbc_describe_table|Provide the description of a table associated with a designated database schema. This includes information about column names, data types, nulls handling, autoincrement, primary key, and foreign keys|\n|podbc_filter_table_names|List tables, based on a substring pattern from the `q` input field, associated with a selected database schema.|\n|podbc_query_database|Execute a SQL query and return results in JSONL format.|\n|podbc_execute_query|Execute a SQL query and return results in JSONL format.|\n|podbc_execute_query_md|Execute a SQL query and return results in Markdown table format.|\n|podbc_spasql_query|Execute a SPASQL query and return results.|\n|podbc_sparql_query|Execute a SPARQL query and return results.|\n|podbc_virtuoso_support_ai|Interact with the Virtuoso Support Assistant/Agent -- a Virtuoso-specific feature for interacting with LLMs|\n\n### Detailed Description\n\n- **podbc_get_schemas**\n  - Retrieve and return a list of all schema names from the connected database.\n  - Input parameters:\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns a JSON string array of schema names.\n\n- **podbc_get_tables**\n  - Retrieve and return a list containing information about tables in a specified schema. If no schema is provided, uses the connection's default schema.\n  - Input parameters:\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns a JSON string containing table information (e.g., TABLE_CAT, TABLE_SCHEM, TABLE_NAME, TABLE_TYPE).\n\n- **podbc_filter_table_names**\n  - Filters and returns information about tables whose names contain a specific substring.\n  - Input parameters:\n    - `q` (string, required): The substring to search for within table names.\n    - `schema` (string, optional): Database schema to filter tables. Defaults to connection default.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns a JSON string containing information for matching tables.\n\n- **podbc_describe_table**\n  - Retrieve and return detailed information about the columns of a specific table.\n  - Input parameters:\n    - `schema` (string, required): The database schema name containing the table.\n    - `table` (string, required): The name of the table to describe.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns a JSON string describing the table's columns (e.g., COLUMN_NAME, TYPE_NAME, COLUMN_SIZE, IS_NULLABLE).\n\n- **podbc_query_database**\n  - Execute a standard SQL query and return the results in JSON format.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns query results as a JSON string.\n\n- **podbc_query_database_md**\n  - Execute a standard SQL query and return the results formatted as a Markdown table.\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns query results as a Markdown table string.\n\n- **podbc_query_database_jsonl**\n  - Execute a standard SQL query and return the results in JSON Lines (JSONL) format (one JSON object per line).\n  - Input parameters:\n    - `query` (string, required): The SQL query string to execute.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns query results as a JSONL string.\n\n- **podbc_spasql_query**\n  - Execute a SPASQL (SQL/SPARQL hybrid) query return results. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `query` (string, required): The SPASQL query string.\n    - `max_rows` (number, optional): Maximum number of rows to return. Defaults to 20.\n    - `timeout` (number, optional): Query timeout in milliseconds. Defaults to 30000.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns the result from the underlying stored procedure call (e.g., `Demo.demo.execute_spasql_query`).\n\n- **podbc_sparql_query**\n  - Execute a SPARQL query and return results. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `query` (string, required): The SPARQL query string.\n    - `format` (string, optional): Desired result format. Defaults to 'json'.\n    - `timeout` (number, optional): Query timeout in milliseconds. Defaults to 30000.\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns the result from the underlying function call (e.g., `\"UB\".dba.\"sparqlQuery\"`).\n\n- **podbc_virtuoso_support_ai**\n  - Utilizes a Virtuoso-specific AI Assistant function, passing a prompt and optional API key. This is a Virtuoso-specific feature.\n  - Input parameters:\n    - `prompt` (string, required): The prompt text for the AI function.\n    - `api_key` (string, optional): API key for the AI service. Defaults to \"none\".\n    - `user` (string, optional): Database username. Defaults to \"demo\".\n    - `password` (string, optional): Database password. Defaults to \"demo\".\n    - `dsn` (string, optional): ODBC data source name. Defaults to \"Local Virtuoso\".\n  - Returns the result from the AI Support Assistant function call (e.g., `DEMO.DBA.OAI_VIRTUOSO_SUPPORT_AI`).\n\n---\n\n## Troubleshooting\n\nFor easier troubleshooting:\n1. Install the MCP Inspector:\n   ```bash\n   npm install -g @modelcontextprotocol/inspector\n   ```\n\n2. Start the inspector:\n   ```bash\n   npx @modelcontextprotocol/inspector uv --directory /path/to/mcp-sqlalchemy-server run mcp-sqlalchemy-server\n   ```\n\nAccess the provided URL to troubleshoot server interactions.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlalchemy",
        "database",
        "odbc sqlalchemy",
        "database access",
        "mcp sqlalchemy"
      ],
      "category": "databases"
    },
    "PuroDelphi--mcpFirebird": {
      "owner": "PuroDelphi",
      "name": "mcpFirebird",
      "url": "https://github.com/PuroDelphi/mcpFirebird",
      "imageUrl": "/freedevtools/mcp/pfp/PuroDelphi.webp",
      "description": "Connect and manipulate Firebird databases using AI, allowing for data analysis and optimization through natural language and SQL commands.",
      "stars": 32,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T01:58:39Z",
      "readme_content": "[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/1cb2bb07-00f4-4579-b535-1b9de9b451e9)\n\n# MCP Firebird\n\n[![smithery badge](https://smithery.ai/badge/@PuroDelphi/mcpFirebird)](https://smithery.ai/server/@PuroDelphi/mcpFirebird)\n\nImplementation of Anthropic's MCP (Model Context Protocol) for Firebird databases.\n\n## Example Usage\n\nhttps://github.com/user-attachments/assets/e68e873f-f87b-4afd-874f-157086e223af\n\n## What is MCP Firebird?\n\nMCP Firebird is a server that implements Anthropic's [Model Context Protocol (MCP)](https://github.com/anthropics/anthropic-cookbook/tree/main/model_context_protocol) for [Firebird SQL databases](https://firebirdsql.org/). It allows Large Language Models (LLMs) like Claude to access, analyze, and manipulate data in Firebird databases securely and in a controlled manner.\n\n## Key Features\n\n- **SQL Queries**: Execute SQL queries on Firebird databases\n- **Schema Analysis**: Get detailed information about tables, columns, and relationships\n- **Database Management**: Perform backup, restore, and validation operations\n- **Performance Analysis**: Analyze query performance and suggest optimizations\n- **Multiple Transports**: Supports STDIO, SSE (Server-Sent Events), and Streamable HTTP transports\n- **Modern Protocol Support**: Full support for MCP Streamable HTTP (2025-03-26) and legacy SSE\n- **Unified Server**: Automatic protocol detection and backwards compatibility\n- **Claude Integration**: Works seamlessly with Claude Desktop and other MCP clients\n- **VSCode Integration**: Works with GitHub Copilot in Visual Studio Code\n- **Session Management**: Robust session handling with automatic cleanup and configurable timeouts\n- **Security**: Includes SQL query validation and security configuration options\n\n### Manual Installation\n\n#### Stable Version\n```bash\n# Global installation (stable)\nnpm install -g mcp-firebird\n\n# Run the server\nnpx mcp-firebird --database /path/to/database.fdb\n\n# Or use specific stable version\nnpm install -g mcp-firebird@2.2.3\n```\n\n**Stable Features (v2.2.3):**\n- 🐛 **FIXED**: SSE JSON parsing bug - resolves \"Invalid message: [object Object]\" errors\n- ✨ Streamable HTTP transport support (MCP 2025-03-26)\n- 🔄 Unified server with automatic protocol detection\n- 📊 Enhanced session management and monitoring\n- 🛠️ Modern MCP SDK integration (v1.13.2)\n- 🔧 Improved error handling and logging\n- 🧪 Comprehensive test suite with 9+ tests for SSE functionality\n\n#### Alpha Version (Latest Features)\n```bash\n# Install alpha version with latest features\nnpm install -g mcp-firebird@alpha\n\n# Or use specific alpha version\nnpm install -g mcp-firebird@2.3.0-alpha.1\n```\n\n**Alpha Features (v2.3.0-alpha.1):**\n- 🐛 **FIXED**: SSE JSON parsing bug - resolves \"Invalid message: [object Object]\" errors\n- ✨ Streamable HTTP transport support (MCP 2025-03-26)\n- 🔄 Unified server with automatic protocol detection\n- 📊 Enhanced session management and monitoring\n- 🛠️ Modern MCP SDK integration (v1.13.2)\n- 🔧 Improved error handling and logging\n- 🧪 Comprehensive test suite with 9+ tests for SSE functionality\n- 📚 Enhanced documentation with troubleshooting guides\n\nFor backup/restore operations, you'll need to install the Firebird client tools. See [Complete Installation](./docs/installation.md) for more details.\n\nFor VSCode and GitHub Copilot integration, see [VSCode Integration](./docs/vscode-integration.md).\n\n## Basic Usage\n\n### With Claude Desktop\n\n1. Edit the Claude Desktop configuration:\n   ```bash\n   code $env:AppData\\Claude\\claude_desktop_config.json  # Windows\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json  # macOS\n   ```\n\n2. Add the MCP Firebird configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-firebird\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"mcp-firebird\",\n           \"--host\",\n           \"localhost\",\n           \"--port\",\n           \"3050\",\n           \"--database\",\n           \"C:\\\\path\\\\to\\\\database.fdb\",\n           \"--user\",\n           \"SYSDBA\",\n           \"--password\",\n           \"masterkey\"\n         ],\n         \"type\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n3. Restart Claude Desktop\n\n## Transport Configuration\n\nMCP Firebird supports multiple transport protocols to accommodate different client needs and deployment scenarios.\n\n### STDIO Transport (Default)\n\nThe STDIO transport is the standard method for Claude Desktop integration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-firebird\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-firebird\",\n        \"--database\", \"C:\\\\path\\\\to\\\\database.fdb\",\n        \"--user\", \"SYSDBA\",\n        \"--password\", \"masterkey\"\n      ],\n      \"type\": \"stdio\"\n    }\n  }\n}\n```\n\n### SSE Transport (Server-Sent Events)\n\nSSE transport allows the server to run as a web service, useful for web applications and remote access:\n\n#### Basic SSE Configuration\n\n```bash\n# Start SSE server on default port 3003\nnpx mcp-firebird --transport-type sse --database /path/to/database.fdb\n\n# Custom port and full configuration\nnpx mcp-firebird \\\n  --transport-type sse \\\n  --sse-port 3003 \\\n  --database /path/to/database.fdb \\\n  --host localhost \\\n  --port 3050 \\\n  --user SYSDBA \\\n  --password masterkey\n```\n\n#### Environment Variables for SSE\n\n```bash\n# Set environment variables\nexport TRANSPORT_TYPE=sse\nexport SSE_PORT=3003\nexport DB_HOST=localhost\nexport DB_PORT=3050\nexport DB_DATABASE=/path/to/database.fdb\nexport DB_USER=SYSDBA\nexport DB_PASSWORD=masterkey\n\n# Start server\nnpx mcp-firebird\n```\n\n#### SSE Client Connection\n\nOnce the SSE server is running, clients can connect to:\n- **SSE Endpoint**: `http://localhost:3003/sse`\n- **Messages Endpoint**: `http://localhost:3003/messages`\n- **Health Check**: `http://localhost:3003/health`\n\n### Streamable HTTP Transport (Modern)\n\nThe latest MCP protocol supporting bidirectional communication:\n\n```bash\n# Start with Streamable HTTP\nnpx mcp-firebird --transport-type http --http-port 3003 --database /path/to/database.fdb\n```\n\n### Unified Transport (Recommended)\n\nSupports both SSE and Streamable HTTP protocols simultaneously with automatic detection:\n\n```bash\n# Start unified server (supports both SSE and Streamable HTTP)\nnpx mcp-firebird --transport-type unified --http-port 3003 --database /path/to/database.fdb\n```\n\n#### Unified Server Endpoints\n\n- **SSE (Legacy)**: `http://localhost:3003/sse`\n- **Streamable HTTP (Modern)**: `http://localhost:3003/mcp`\n- **Auto-Detection**: `http://localhost:3003/mcp-auto`\n- **Health Check**: `http://localhost:3003/health`\n\n### Configuration Examples\n\n#### Development Setup (SSE)\n```bash\nnpx mcp-firebird \\\n  --transport-type sse \\\n  --sse-port 3003 \\\n  --database ./dev-database.fdb \\\n  --user SYSDBA \\\n  --password masterkey\n```\n\n#### Production Setup (Unified)\n```bash\nnpx mcp-firebird \\\n  --transport-type unified \\\n  --http-port 3003 \\\n  --database /var/lib/firebird/production.fdb \\\n  --host db-server \\\n  --port 3050 \\\n  --user APP_USER \\\n  --password $DB_PASSWORD\n```\n\n#### Docker with SSE\n```bash\ndocker run -d \\\n  --name mcp-firebird \\\n  -p 3003:3003 \\\n  -e TRANSPORT_TYPE=sse \\\n  -e SSE_PORT=3003 \\\n  -e DB_DATABASE=/data/database.fdb \\\n  -v /path/to/database:/data \\\n  purodelhi/mcp-firebird:latest\n```\n\n### Advanced SSE Configuration\n\n#### Session Management\n\nConfigure session timeouts and limits:\n\n```bash\n# Environment variables for session management\nexport SSE_SESSION_TIMEOUT_MS=1800000    # 30 minutes\nexport MAX_SESSIONS=1000                 # Maximum concurrent sessions\nexport SESSION_CLEANUP_INTERVAL_MS=60000 # Cleanup every minute\n\nnpx mcp-firebird --transport-type sse\n```\n\n#### CORS Configuration\n\nFor web applications, configure CORS settings:\n\n```bash\n# Allow specific origins\nexport CORS_ORIGIN=\"https://myapp.com,https://localhost:3000\"\nexport CORS_METHODS=\"GET,POST,OPTIONS\"\nexport CORS_HEADERS=\"Content-Type,mcp-session-id\"\n\nnpx mcp-firebird --transport-type sse\n```\n\n#### SSL/TLS Support\n\nFor production deployments, use a reverse proxy like nginx:\n\n```nginx\nserver {\n    listen 443 ssl;\n    server_name mcp-firebird.yourdomain.com;\n\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n\n    location / {\n        proxy_pass http://localhost:3003;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Troubleshooting SSE\n\n#### Common Issues\n\n1. **Connection Refused**\n   ```bash\n   # Check if server is running\n   curl http://localhost:3003/health\n\n   # Check port availability\n   netstat -an | grep 3003\n   ```\n\n2. **Session Timeout**\n   ```bash\n   # Increase session timeout\n   export SSE_SESSION_TIMEOUT_MS=3600000  # 1 hour\n   ```\n\n3. **CORS Errors**\n   ```bash\n   # Allow all origins (development only)\n   export CORS_ORIGIN=\"*\"\n   ```\n\n4. **Memory Issues**\n   ```bash\n   # Reduce max sessions\n   export MAX_SESSIONS=100\n\n   # Enable more frequent cleanup\n   export SESSION_CLEANUP_INTERVAL_MS=30000\n   ```\n\n5. **JSON Parsing Issues (Fixed in v2.3.0-alpha.1+)**\n   ```bash\n   # If experiencing \"Invalid message: [object Object]\" errors,\n   # upgrade to the latest alpha version:\n   npm install mcp-firebird@alpha\n\n   # Or use the latest alpha directly:\n   npx mcp-firebird@alpha --transport-type sse\n   ```\n\n   **Note**: Versions prior to 2.3.0-alpha.1 had a bug where POST requests to `/messages`\n   endpoint failed to parse JSON body correctly. This has been fixed with improved\n   middleware handling for both `application/json` and `text/plain` content types.\n\n#### Monitoring and Logging\n\n```bash\n# Enable debug logging\nexport LOG_LEVEL=debug\n\n# Monitor server health\ncurl http://localhost:3003/health | jq\n\n# Check active sessions\ncurl http://localhost:3003/health | jq '.sessions'\n```\n\n## Quick Installation via Smithery\n\nTo install MCP Firebird for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PuroDelphi/mcpFirebird):\n\n```bash\nnpx -y @smithery/cli install @PuroDelphi/mcpFirebird --client claude\n```\n\n\n## Documentation\n\nFor more detailed information, check the following documents:\n\n### Getting Started\n- [Complete Installation](./docs/installation.md)\n- [Configuration Options](./docs/configuration.md)\n- [Available Tools](./docs/tools.md)\n\n### Transport Protocols\n- [SSE Transport Configuration](./docs/sse-transport.md)\n- [Streamable HTTP Setup](./docs/streamable-http.md)\n- [Transport Comparison](./docs/transport-comparison.md)\n\n### Integration Guides\n- [Claude Desktop Integration](./docs/claude-integration.md)\n- [VSCode Integration](./docs/vscode-integration.md)\n- [Docker Configuration](./docs/docker.md)\n- [Usage from Different Languages](./docs/clients.md)\n\n### Advanced Topics\n- [Session Management](./docs/session-management.md)\n- [Security](./docs/security.md)\n- [Performance Tuning](./docs/performance.md)\n- [Troubleshooting](./docs/troubleshooting.md)\n- [SSE JSON Parsing Fix](./docs/sse-json-parsing-fix.md) - Details about the v2.3.0-alpha.1 bug fix\n\n### Examples and Use Cases\n- [Use Cases and Examples](./docs/use-cases.md)\n- [MCP Updates Summary](./docs/mcp-updates-summary.md)\n\n\n## Support the Project\n\n### Donations\n\nIf you find MCP Firebird useful for your work or projects, please consider supporting its development through a donation. Your contributions help maintain and improve this tool.\n\n- **GitHub Sponsors**: [Sponsor @PuroDelphi](https://github.com/sponsors/PuroDelphi)\n- **PayPal**: [Donate via PayPal](https://www.paypal.com/donate/?hosted_button_id=KBAUBYYDNHQNQ)\n\n![image](https://github.com/user-attachments/assets/d04cf0eb-32a8-48a7-9324-c02af5269370)\n\n\n### Hire Our AI Agents\n\nAnother great way to support this project is by hiring our AI agents through [Asistentes Autónomos](https://asistentesautonomos.com). We offer specialized AI assistants for various business needs, helping you automate tasks and improve productivity.\n\n### Priority Support\n\n⭐ **Donors, sponsors, and clients receive priority support and assistance** with issues, feature requests, and implementation guidance. While we strive to help all users, those who support the project financially will receive faster response times and dedicated assistance.\n\nYour support is greatly appreciated and helps ensure the continued development of MCP Firebird!\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcpfirebird",
        "firebird",
        "databases",
        "firebird databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "QuantGeekDev--mongo-mcp": {
      "owner": "QuantGeekDev",
      "name": "mongo-mcp",
      "url": "https://github.com/QuantGeekDev/mongo-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/QuantGeekDev.webp",
      "description": "Enables LLMs to interact directly with MongoDB databases using natural language, facilitating schema inspection, document querying, and data management.",
      "stars": 168,
      "forks": 32,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:26:18Z",
      "readme_content": "# 🗄️ MongoDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/mongo-mcp)](https://smithery.ai/server/mongo-mcp)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with MongoDB databases. Query collections, inspect schemas, and manage data seamlessly through natural language.\n\n## ✨ Features\n\n- 🔍 Collection schema inspection\n- 📊 Document querying and filtering\n- 📈 Index management\n- 📝 Document operations (insert, update, delete)\n\n## Demo Video\n\n\nhttps://github.com/user-attachments/assets/2389bf23-a10d-49f9-bca9-2b39a1ebe654\n\n\n\n\n## 🚀 Quick Start\n\nTo get started, find your mongodb connection url and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mongo-mcp\",\n        \"mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install MongoDB MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mongo-mcp):\n\n```bash\nnpx -y @smithery/cli install mongo-mcp --client claude\n```\n\n### Prerequisites\n\n- Node.js 18+\n- npx\n- Docker and Docker Compose (for local sandbox testing only)\n- MCP Client (Claude Desktop App for example)\n\n### Test Sandbox Setup\n\nIf you don't have a mongo db server to connect to and want to create a sample sandbox, follow these steps\n\n1. Start MongoDB using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n2. Seed the database with test data:\n\n```bash\nnpm run seed\n```\n\n### Configure Claude Desktop\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"dist/index.js\",\n        \"mongodb://root:example@localhost:27017/test?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Test Sandbox Data Structure\n\nThe seed script creates three collections with sample data:\n\n#### Users\n\n- Personal info (name, email, age)\n- Nested address with coordinates\n- Arrays of interests\n- Membership dates\n\n#### Products\n\n- Product details (name, SKU, category)\n- Nested specifications\n- Price and inventory info\n- Tags and ratings\n\n#### Orders\n\n- Order details with items\n- User references\n- Shipping and payment info\n- Status tracking\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n### Basic Operations\n\n```plaintext\n\"What collections are available in the database?\"\n\"Show me the schema for the users collection\"\n\"Find all users in San Francisco\"\n```\n\n### Advanced Queries\n\n```plaintext\n\"Find all electronics products that are in stock and cost less than $1000\"\n\"Show me all orders from the user john@example.com\"\n\"List the products with ratings above 4.5\"\n```\n\n### Index Management\n\n```plaintext\n\"What indexes exist on the users collection?\"\n\"Create an index on the products collection for the 'category' field\"\n\"List all indexes across all collections\"\n```\n\n### Document Operations\n\n```plaintext\n\"Insert a new product with name 'Gaming Laptop' in the products collection\"\n\"Update the status of order with ID X to 'shipped'\"\n\"Find and delete all products that are out of stock\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for database interaction:\n\n### Query Tools\n\n- `find`: Query documents with filtering and projection\n- `listCollections`: List available collections\n- `insertOne`: Insert a single document\n- `updateOne`: Update a single document\n- `deleteOne`: Delete a single document\n\n### Index Tools\n\n- `createIndex`: Create a new index\n- `dropIndex`: Remove an index\n- `indexes`: List indexes for a collection\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "mongo",
        "databases",
        "mongodb databases",
        "quantgeekdev mongo",
        "directly mongodb"
      ],
      "category": "databases"
    },
    "Quegenx--supabase-mcp-server": {
      "owner": "Quegenx",
      "name": "supabase-mcp-server",
      "url": "https://github.com/Quegenx/supabase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Quegenx.webp",
      "description": "Manage a Supabase PostgreSQL database using natural language commands, enabling efficient database management through conversational interactions. Integrates with Cursor's Composer and Codeium's Cascade for enhanced productivity.",
      "stars": 13,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-13T10:34:12Z",
      "readme_content": "# Supabase MCP Server 🚀\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Supabase](https://img.shields.io/badge/Supabase-3ECF8E?style=for-the-badge&logo=supabase&logoColor=white)](https://supabase.com/)\n[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)\n[![Node.js](https://img.shields.io/badge/Node.js-43853D?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)\n[![MCP](https://img.shields.io/badge/MCP-Cursor-blue?style=for-the-badge)](https://cursor.sh/)\n[![Windsurf](https://img.shields.io/badge/Windsurf-Cascade-purple?style=for-the-badge)](https://www.codeium.com/cascade)\n\n> 🔥 A powerful Model Context Protocol (MCP) server that provides full administrative control over your Supabase PostgreSQL database through both Cursor's Composer and Codeium's Cascade. This tool enables seamless database management with comprehensive features for table operations, record management, schema modifications, and more.\n\n<div align=\"center\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pnSzmFJRCJztS7tkSJXYuQ.jpeg\" alt=\"Supabase\" width=\"600\"/>\n</div>\n\n## 📚 Table of Contents\n- [Prerequisites](#-prerequisites)\n- [Quick Start](#-quick-start)\n- [Integrations](#-integrations)\n- [Features](#-features)\n- [Usage](#-usage)\n- [Security Notes](#-security-notes)\n- [Troubleshooting](#-troubleshooting)\n- [Contributing](#-contributing)\n- [License](#-license)\n\n## 🔧 Prerequisites\n\n- Node.js >= 16.x\n- npm >= 8.x\n- A Supabase project with:\n  - Project ID\n  - Database password\n  - PostgreSQL connection string\n- Cursor IDE or Codeium's Cascade (for paying users)\n\n## 🚀 Quick Start\n\n### 📥 Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Quegenx/supabase-mcp-server.git\ncd supabase-mcp-server\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n### ⚙️ Configuration\n\n1. Install dependencies and build the project:\n   ```bash\n   npm install\n   npm run build\n   ```\n\n2. In Cursor's MCP settings, add the server with this command:\n   ```bash\n   /opt/homebrew/bin/node /path/to/dist/index.js postgresql://postgres.[PROJECT-ID]:[PASSWORD]@aws-0-eu-central-1.pooler.supabase.com:5432/postgres\n   ```\n\n   Replace:\n   - `/path/to/dist/index.js` with your actual path\n   - `[PROJECT-ID]` with your Supabase project ID\n   - `[PASSWORD]` with your database password\n\nNote: Keep your database credentials secure and never commit them to version control.\n\n## 🎯 Integrations\n\n### Cursor MCP Integration\n\nThe Model Context Protocol (MCP) allows you to provide custom tools to agentic LLMs in Cursor. This server can be integrated with Cursor's Composer feature, providing direct access to all database management tools through natural language commands.\n\n#### Setting up in Cursor\n\n1. Open Cursor Settings > Features > MCP\n2. Click the \"+ Add New MCP Server\" button\n3. Fill in the modal form:\n   - Name: \"Supabase MCP\" (or any nickname you prefer)\n   - Type: `command` (stdio transport)\n   - Command: Your full command string with connection details\n\n4. Build the project first:\n   ```bash\n   npm install\n   npm run build\n   ```\n\n5. Get your Node.js path:\n   ```bash\n   # On Mac/Linux\n   which node\n   # On Windows\n   where node\n   ```\n\n6. Add the server command:\n   ```bash\n   /path/to/node /path/to/dist/index.js postgresql://postgres.[PROJECT-ID]:[PASSWORD]@aws-0-eu-central-1.pooler.supabase.com:5432/postgres\n   ```\n\n   Replace:\n   - `/path/to/node` with your actual Node.js path (from step 5)\n   - `/path/to/dist/index.js` with your actual path to the built JavaScript file\n   - `[PROJECT-ID]` with your Supabase project ID\n   - `[PASSWORD]` with your database password\n\n7. Click \"Add Server\" and then click the refresh button in the top right corner\n\n#### Using the Tools in Cursor\n\nThe Composer Agent will automatically detect and use relevant tools when you describe your database tasks. For example:\n\n- \"List all tables in my database\"\n- \"Create a new users table\"\n- \"Add an index to the email column\"\n\nWhen the agent uses a tool, you'll see:\n1. A prompt to approve/deny the tool call\n2. The tool call arguments (expandable)\n3. The response after approval\n\nNote: For stdio servers like this one, the command should be a valid shell command. If you need environment variables, consider using a wrapper script.\n\n### Windsurf/Cascade Integration\n\nThis MCP server also supports Codeium's Cascade (Windsurf) integration. Note that this feature is currently only available for paying individual users (not available for Teams or Enterprise users).\n\n#### Setting up with Cascade\n\n1. Create or edit `~/.codeium/windsurf/mcp_config.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase-mcp\": {\n         \"command\": \"/path/to/node\",\n         \"args\": [\n           \"/path/to/dist/index.js\",\n           \"postgresql://postgres.[PROJECT-ID]:[PASSWORD]@aws-0-eu-central-1.pooler.supabase.com:5432/postgres\"\n         ]\n       }\n     }\n   }\n   ```\n\n2. Quick access to config:\n   - Find the toolbar above the Cascade input\n   - Click the hammer icon\n   - Click \"Configure\" to open mcp_config.json\n\n3. Replace in the configuration:\n   - `/path/to/node` with your actual Node.js path\n   - `/path/to/dist/index.js` with your actual path\n   - `[PROJECT-ID]` with your Supabase project ID\n   - `[PASSWORD]` with your database password\n\n4. In Cascade:\n   - Click the hammer icon in the toolbar\n   - Click \"Configure\" to verify your setup\n   - Click \"Refresh\" to load the MCP server\n   - Click the server name to see available tools\n\n#### Important Notes for Cascade Users\n\n- Only tools functionality is supported (no prompts or resources)\n- MCP tool calls will consume credits regardless of success or failure\n- Image output is not supported\n- Only stdio transport type is supported\n- Tool calls can invoke code written by arbitrary server implementers\n- Cascade does not assume liability for MCP tool call failures\n\n## ✨ Features\n\n### 🎯 Available Database Tools\n\n#### Table Management\n- Tables: `list_tables`, `create_table`, `drop_table`, `rename_table`\n- Columns: `add_column`, `drop_column`, `alter_column`\n- Records: `fetch_records`, `create_record`, `update_record`, `delete_record`\n\n#### Indexes & Constraints\n- Indexes: `list_indexes`, `create_index`, `delete_index`, `update_index`\n- Constraints: `list_constraints`, `add_constraint`, `remove_constraint`, `update_constraint`\n\n#### Database Functions & Triggers\n- Functions: `list_functions`, `create_function`, `update_function`, `delete_function`\n- Triggers: `list_triggers`, `create_trigger`, `update_trigger`, `delete_trigger`\n\n#### Security & Access Control\n- Policies: `list_policies`, `create_policy`, `update_policy`, `delete_policy`\n- Roles: `list_roles`, `create_role`, `update_role`, `delete_role`\n\n#### Storage Management\n- Buckets: `list_buckets`, `create_bucket`, `delete_bucket`\n- Files: `delete_file`, `bulk_delete_files`\n- Folders: `list_folders`\n\n#### Data Types & Publications\n- Enumerated Types: `list_enumerated_types`, `create_enumerated_type`, `update_enumerated_type`, `delete_enumerated_type`\n- Publications: `list_publications`, `create_publication`, `update_publication`, `delete_publication`\n\n#### Realtime Features\n- Policies: `list_realtime_policies`, `create_realtime_policy`, `update_realtime_policy`, `delete_realtime_policy`\n- Channels: `list_realtime_channels`, `manage_realtime_channels`, `send_realtime_message`, `get_realtime_messages`\n- Management: `manage_realtime_status`, `manage_realtime_views`\n\n#### User Management\n- Auth: `list_users`, `create_user`, `update_user`, `delete_user`\n\n#### Direct SQL Access\n- Query: `query` - Execute custom SQL queries\n\n### 🚀 Key Benefits\n\n- **Natural Language Control**: Manage your Supabase database through simple conversational commands\n- **Comprehensive Coverage**: Full suite of tools covering tables, records, indexes, functions, security, and more\n- **Seamless Integration**: Works directly within Cursor's Composer and Codeium's Cascade\n- **Developer Friendly**: Reduces context switching between IDE and database management tools\n- **Secure Access**: Maintains your database security with proper authentication\n\n## 📁 Project Structure\n\n```\nsupabase-mcp-server/\n├── dist/                    # Compiled JavaScript files\n│   ├── index.d.ts          # TypeScript declarations\n│   └── index.js            # Main JavaScript file\n├── src/                    # Source code\n│   └── index.ts           # Main TypeScript file\n├── package.json           # Project configuration\n├── package-lock.json      # Dependency lock file\n└── tsconfig.json         # TypeScript configuration\n```\n\n## 💡 Usage\n\nOnce configured, the MCP server provides all database management tools through Cursor's Composer. Simply describe what you want to do with your database, and the AI will use the appropriate commands.\n\nExamples:\n- 📋 \"Show me all tables in my database\"\n- ➕ \"Create a new users table with id, name, and email columns\"\n- 🔍 \"Add an index on the email column of the users table\"\n\n## 🔒 Security Notes\n\n- 🔐 Keep your database connection string secure\n- ⚠️ Never commit sensitive credentials to version control\n- 👮 Use appropriate access controls and permissions\n- 🛡️ Validate and sanitize all inputs to prevent SQL injection\n\n## 🛠️ Troubleshooting\n\n### Common Connection Issues\n\n1. **Node.js Path Issues**\n   - Ensure you're using the correct Node.js path\n   - On Mac/Linux: Use `which node` to find the correct path\n   - On Windows: Use `where node` to find the correct path\n   - Replace `/usr/local/bin/node` with your actual Node.js path\n\n2. **File Path Issues**\n   - Use absolute paths instead of relative paths\n   - On Mac/Linux: Use `pwd` in the project directory to get the full path\n   - On Windows: Use `cd` to get the full path\n   - Example: `/Users/username/projects/supabase-mcp-server/dist/index.js`\n\n3. **MCP Not Detecting Tools**\n   - Click the refresh button in Cursor's MCP settings\n   - Ensure the server is running (no error messages)\n   - Check if your connection string is correct\n   - Verify your Supabase credentials are valid\n\n4. **Permission Issues**\n   - Make sure the `dist` directory exists (run `npm run build`)\n   - Check file permissions (`chmod +x` on Unix systems)\n   - Run `npm install` with appropriate permissions\n\n### Debug Mode\n\nAdd `DEBUG=true` before your command to see detailed logs:\n\n```bash\nDEBUG=true /usr/local/bin/node /path/to/dist/index.js [connection-string]\n```\n\n### Platform-Specific Notes\n\n#### Windows Users\n```bash\n# Use this format for the command\n\"C:\\\\Program Files\\\\nodejs\\\\node.exe\" \"C:\\\\path\\\\to\\\\dist\\\\index.js\" \"postgresql://...\"\n```\n\n#### Linux Users\n```bash\n# Find Node.js path\nwhich node\n\n# Make script executable\nchmod +x /path/to/dist/index.js\n```\n\nIf you're still experiencing issues, please [open an issue](https://github.com/Quegenx/supabase-mcp-server/issues) with:\n- Your operating system\n- Node.js version (`node --version`)\n- Full error message\n- Steps to reproduce\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\n---\n\n<div align=\"center\">\n  <p>Built with ❤️ for the Cursor community</p>\n  <p>\n    <a href=\"https://cursor.sh\">Cursor</a> •\n    <a href=\"https://supabase.com\">Supabase</a> •\n    <a href=\"https://github.com/Quegenx\">GitHub</a>\n  </p>\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "supabase",
        "database",
        "supabase postgresql",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "RafaelCartenet--mcp-databricks-server": {
      "owner": "RafaelCartenet",
      "name": "mcp-databricks-server",
      "url": "https://github.com/RafaelCartenet/mcp-databricks-server",
      "imageUrl": "/freedevtools/mcp/pfp/RafaelCartenet.webp",
      "description": "Execute SQL queries against Databricks, retrieve data, list available schemas, and describe table structures to enhance data operations. Integrates effectively with Agent mode for performing complex task automation.",
      "stars": 24,
      "forks": 12,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T13:11:07Z",
      "readme_content": "# Databricks MCP Server\n\n- [Motivation](#motivation)\n- [Overview](#overview)\n- [Practical Benefits of UC Metadata for AI Agents](#practical-benefits-of-uc-metadata-for-ai-agents)\n- [Available Tools and Features](#available-tools-and-features)\n- [Setup](#setup)\n  - [System Requirements](#system-requirements)\n  - [Installation](#installation)\n- [Permissions Requirements](#permissions-requirements)\n- [Running the Server](#running-the-server)\n  - [Standalone Mode](#standalone-mode)\n  - [Using with Cursor](#using-with-cursor)\n- [Example Usage Workflow (for an LLM Agent)](#example-usage-workflow-for-an-llm-agent)\n- [Managing Metadata as Code with Terraform](#managing-metadata-as-code-with-terraform)\n- [Handling Long-Running Queries](#handling-long-running-queries)\n- [Dependencies](#dependencies)\n\n## Motivation\n\nDatabricks Unity Catalog (UC) allows for detailed documentation of your data assets, including catalogs, schemas, tables, and columns. Documenting these assets thoroughly requires an investment of time. One common question is: what are the practical benefits of this detailed metadata entry?\n\nThis MCP server provides a strong justification for that effort. It enables Large Language Models (LLMs) to directly access and utilize this Unity Catalog metadata. The more comprehensively your data is described in UC, the more effectively an LLM agent can understand your Databricks environment. This deeper understanding is crucial for the agent to autonomously construct more intelligent and accurate SQL queries to fulfill data requests.\n\n## Overview\n\nThis Model Context Protocol (MCP) server is designed to interact with Databricks, with a strong focus on leveraging Unity Catalog (UC) metadata and enabling comprehensive data lineage exploration. The primary goal is to equip an AI agent with a comprehensive set of tools, enabling it to become independent in answering questions about your data. By autonomously exploring UC, understanding data structures, analyzing data lineage (including notebook and job dependencies), and executing SQL queries, the agent can fulfill data requests without direct human intervention for each step.\n\nBeyond traditional catalog browsing, this server enables agents to discover and analyze the actual code that processes your data. Through enhanced lineage capabilities, agents can identify notebooks and jobs that read from or write to tables, then examine the actual transformation logic, business rules, and data quality checks implemented in those notebooks. This creates a powerful feedback loop where agents not only understand *what* data exists, but also *how* it's processed and transformed.\n\nWhen used in an Agent mode, it can successfully iterate over a number of requests to perform complex tasks, including data discovery, impact analysis, and code exploration.\n\n## Practical Benefits of UC Metadata for AI Agents\n\nThe tools provided by this MCP server are designed to parse and present the descriptions you've added to Unity Catalog, while also enabling deep exploration of your data processing code. This offers tangible advantages for LLM-based agents, directly impacting their ability to generate useful SQL and understand your data ecosystem:\n\n*   **Clearer Data Context**: Agents can quickly understand the purpose of tables and columns, reducing ambiguity. This foundational understanding is the first step towards correct query formulation.\n*   **More Accurate Query Generation**: Access to descriptions, data types, and relationships helps agents construct SQL queries with greater precision and semantic correctness.\n*   **Efficient Data Exploration for Query Planning**: Metadata enables agents to navigate through catalogs and schemas more effectively, allowing them to identify the correct tables and columns to include in their SQL queries.\n*   **Comprehensive Data Lineage**: Beyond table-to-table relationships, agents can discover notebooks and jobs that process data, enabling impact analysis and debugging of data pipeline issues.\n*   **Code-Level Understanding**: Through notebook content exploration, agents can analyze actual transformation logic, business rules, and data quality checks, providing deeper insights into how data is processed and transformed.\n*   **End-to-End Data Flow Analysis**: Agents can trace data from raw ingestion through transformation pipelines to final consumption, understanding both the structure and the processing logic at each step.\n\nWell-documented metadata in Unity Catalog, when accessed via this server, allows an LLM agent to operate with better information and make more informed decisions, culminating in the generation of more effective SQL queries. For instance, schema descriptions help the agent identify relevant data sources for a query:\n\n\n*Fig 1: A schema in Unity Catalog with user-provided descriptions. This MCP server makes this information directly accessible to an LLM, informing its query strategy.*\n\nSimilarly, detailed comments at the column level clarify the semantics of each field, which is crucial for constructing accurate SQL conditions and selections:\n\n\n*Fig 2: Column-level descriptions in Unity Catalog. These details are passed to the LLM, aiding its understanding of the data structure for precise SQL generation.*\n\n## Available Tools and Features\n\nThis MCP server provides a suite of tools designed to empower an LLM agent interacting with Databricks:\n\n**Core Capabilities:**\n\n*   **Execute SQL Queries**: Run arbitrary SQL queries using the Databricks SDK via the `execute_sql_query(sql: str)` tool. This is ideal for targeted data retrieval or complex operations.\n*   **LLM-Focused Output**: All descriptive tools return information in Markdown format, optimized for consumption by Large Language Models, making it easier for agents to parse and understand the context.\n\n**Unity Catalog Exploration Tools:**\n\nThe server provides the following tools for navigating and understanding your Unity Catalog assets. These are designed to be used by an LLM agent to gather context before constructing queries or making decisions, in an agentic way.\n\n1.  `list_uc_catalogs() -> str`\n    *   **Description**: Lists all available Unity Catalogs with their names, descriptions, and types.\n    *   **When to use**: As a starting point to discover available data sources when you don't know specific catalog names. It provides a high-level overview of all accessible catalogs in the workspace.\n\n2.  `describe_uc_catalog(catalog_name: str) -> str`\n    *   **Description**: Provides a summary of a specific Unity Catalog, listing all its schemas with their names and descriptions.\n    *   **When to use**: When you know the catalog name and need to discover the schemas within it. This is often a precursor to describing a specific schema or table.\n    *   **Args**:\n        *   `catalog_name`: The name of the Unity Catalog to describe (e.g., `prod`, `dev`, `system`).\n\n3.  `describe_uc_schema(catalog_name: str, schema_name: str, include_columns: Optional[bool] = False) -> str`\n    *   **Description**: Provides detailed information about a specific schema within a Unity Catalog. Returns all tables in the schema, optionally including their column details.\n    *   **When to use**: To understand the contents of a schema, primarily its tables. Set `include_columns=True` to get column information, crucial for query construction but makes the output longer. If `include_columns=False`, only table names and descriptions are shown, useful for a quicker overview.\n    *   **Args**:\n        *   `catalog_name`: The name of the catalog containing the schema.\n        *   `schema_name`: The name of the schema to describe.\n        *   `include_columns`: If True, lists tables with their columns. Defaults to False for a briefer summary.\n\n4.  `describe_uc_table(full_table_name: str, include_lineage: Optional[bool] = False) -> str`\n    *   **Description**: Provides a detailed description of a specific Unity Catalog table with comprehensive lineage capabilities.\n    *   **When to use**: To understand the structure (columns, data types, partitioning) of a single table. This is essential before constructing SQL queries against the table. Optionally, it can include comprehensive lineage information that goes beyond traditional table-to-table dependencies:\n        *   **Table Lineage**: Upstream tables (tables this table reads from) and downstream tables (tables that read from this table)\n        *   **Notebook & Job Lineage**: Notebooks that read from or write to this table, including notebook name, workspace path, associated Databricks job information (job name, ID, task details)\n        *   **Code Discovery**: The lineage provides notebook paths that enable the an agent to directly read notebook files within the current repo/workspace, allowing analysis of actual data transformation logic\n    *   **Args**:\n        *   `full_table_name`: The fully qualified three-part name of the table (e.g., `catalog.schema.table`).\n        *   `include_lineage`: Set to True to fetch comprehensive lineage (tables, notebooks, jobs). Defaults to False. May take longer to retrieve but provides rich context for understanding data dependencies and enabling code exploration.\n\n5.  `execute_sql_query(sql: str) -> str`\n    *   **Note**: This is the same tool listed under \"Core Capabilities\" but is repeated here in the context of a typical agent workflow involving UC exploration followed by querying.\n    *   **Description**: Executes a given SQL query against the Databricks SQL warehouse and returns the formatted results.\n    *   **When to use**: When you need to run specific SQL queries, such as SELECT, SHOW, or other DQL statements.\n    *   **Args**:\n        *   `sql`: The complete SQL query string to execute.\n\n## Setup\n\n### System Requirements\n\n-   Python 3.10+\n-   If you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1)\n\n### Installation\n\n1.  Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\nOr if using `uv`:\n\n```bash\nuv pip install -r requirements.txt\n```\n\n2.  Set up your environment variables:\n\n    Option 1: Using a `.env` file (recommended)\n\n    Create a `.env` file in the root directory of this project with your Databricks credentials:\n\n    ```env\n    DATABRICKS_HOST=\"your-databricks-instance.cloud.databricks.com\"\n    DATABRICKS_TOKEN=\"your-databricks-personal-access-token\"\n    DATABRICKS_SQL_WAREHOUSE_ID=\"your-sql-warehouse-id\"\n    ```\n\n    Option 2: Setting environment variables directly\n\n    ```bash\n    export DATABRICKS_HOST=\"your-databricks-instance.cloud.databricks.com\"\n    export DATABRICKS_TOKEN=\"your-databricks-personal-access-token\"\n    export DATABRICKS_SQL_WAREHOUSE_ID=\"your-sql-warehouse-id\"\n    ```\n\n    You can find your SQL Warehouse ID in the Databricks UI under \"SQL Warehouses\".\n    The `DATABRICKS_SQL_WAREHOUSE_ID` is primarily used for fetching table lineage and executing SQL queries via the `execute_sql_query` tool.\n    Metadata browsing tools (listing/describing catalogs, schemas, tables) use the Databricks SDK's general UC APIs and do not strictly require a SQL Warehouse ID unless lineage is requested.\n\n## Permissions Requirements\n\nBefore using this MCP server, ensure that the identity associated with the `DATABRICKS_TOKEN` (e.g., a user or service principal) has the necessary permissions:\n\n1.  **Unity Catalog Permissions**: \n    -   `USE CATALOG` on catalogs to be accessed.\n    -   `USE SCHEMA` on schemas to be accessed.\n    -   `SELECT` on tables to be queried or described in detail (including column information).\n    -   To list all catalogs, appropriate metastore-level permissions might be needed or it will list catalogs where the user has at least `USE CATALOG`.\n2.  **SQL Warehouse Permissions** (for `execute_sql_query` and lineage fetching):\n    -   `CAN_USE` permission on the SQL Warehouse specified by `DATABRICKS_SQL_WAREHOUSE_ID`.\n3.  **Token Permissions**: \n    -   The personal access token or service principal token should have the minimum necessary scopes. For Unity Catalog operations, this typically involves workspace access. For SQL execution, it involves SQL permissions.\n    -   It is strongly recommended to use a service principal with narrowly defined permissions for production or automated scenarios.\n\nFor security best practices, consider regularly rotating your access tokens and auditing query history and UC audit logs to monitor usage.\n\n## Running the Server\n\n### Standalone Mode\n\nTo run the server in standalone mode (e.g., for testing with Agent Composer):\n\n```bash\npython main.py\n```\n\nThis will start the MCP server using stdio transport, which can be used with Agent Composer or other MCP clients.\n\n### Using with Cursor\n\nTo use this MCP server with [Cursor](https://cursor.sh/), configure it in your Cursor settings (`~/.cursor/mcp.json`):\n\n1. Create a `.cursor` directory in your home directory if it doesn't already exist\n2. Create or edit the `mcp.json` file in that directory:\n\n```bash\nmkdir -p ~/.cursor\ntouch ~/.cursor/mcp.json\n```\n\n3. Add the following configuration to the `mcp.json` file, replacing the directory path with the actual path to where you've installed this server:\n\n```json\n{\n    \"mcpServers\": {\n        \"databricks\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/your/mcp-databricks-server\",\n                \"run\",\n                \"main.py\"\n            ]\n        }\n    }\n}\n```\n\nExample using `python`:\n```json\n{\n    \"mcpServers\": {\n        \"databricks\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"/path/to/your/mcp-databricks-server/main.py\"\n            ]\n        }\n    }\n}\n```\nRestart Cursor to apply the changes. You can then use the `databricks` agent in Cursor.\n\n## Example Usage Workflow (for an LLM Agent)\n\nThis MCP server empowers an LLM agent to autonomously navigate your Databricks environment. The following screenshot illustrates a typical interaction where the agent iteratively explores schemas and tables, adapting its approach even when initial queries don't yield results, until it successfully retrieves the requested data.\n\n\n*Fig 3: An LLM agent using the Databricks MCP tools, demonstrating iterative exploration and query refinement to locate specific page view data.*\n\nAn agent might follow this kind of workflow:\n\n1.  **Discover available catalogs**: `list_uc_catalogs()`\n    *   *Agent decides `prod_catalog` is relevant from the list.* \n2.  **Explore a specific catalog**: `describe_uc_catalog(catalog_name=\"prod_catalog\")`\n    *   *Agent sees `sales_schema` and `inventory_schema`.*\n3.  **Explore a specific schema (quick view)**: `describe_uc_schema(catalog_name=\"prod_catalog\", schema_name=\"sales_schema\")`\n    *   *Agent sees table names like `orders`, `customers`.* \n4.  **Get detailed table structure (including columns for query building)**: `describe_uc_schema(catalog_name=\"prod_catalog\", schema_name=\"sales_schema\", include_columns=True)`\n    *   *Alternatively, if a specific table is of interest:* `describe_uc_table(full_table_name=\"prod_catalog.sales_schema.orders\")`\n5.  **Analyze data lineage and discover processing code**: `describe_uc_table(full_table_name=\"prod_catalog.sales_schema.orders\", include_lineage=True)`\n    *   *Agent discovers upstream tables, downstream dependencies, and notebooks that process this data*\n    *   *For example, sees that `/Repos/production/etl/sales_processing.py` writes to this table*\n6.  **Examine data transformation logic**: *Agent directly reads the notebook file `/Repos/production/etl/sales_processing.py` within the IDE/repo*\n    *   *Agent analyzes the actual Python/SQL code to understand business rules, data quality checks, and transformation logic*\n7.  **Construct and execute a query**: `execute_sql_query(sql=\"SELECT customer_id, order_date, SUM(order_total) FROM prod_catalog.sales_schema.orders WHERE order_date > '2023-01-01' GROUP BY customer_id, order_date ORDER BY order_date DESC LIMIT 100\")`\n\n## Managing Metadata as Code with Terraform\n\nWhile manually entering metadata through the Databricks UI is an option, a more robust and scalable approach is to define your Unity Catalog metadata as code. Tools like Terraform allow you to declaratively manage your data governance objects, including catalogs and schemas. This brings several advantages:\n\n*   **Version Control**: Your metadata definitions can be stored in Git, tracked, and versioned alongside your other infrastructure code.\n*   **Repeatability and Consistency**: Ensure consistent metadata across environments (dev, staging, prod).\n*   **Automation**: Integrate metadata management into your CI/CD pipelines.\n*   **Easier Maintenance for Core Assets**: While defining every new table as code might be complex due to their dynamic nature, core assets like catalogs and schemas are often more stable and benefit significantly from this approach. Maintaining their definitions and comments as code ensures a durable and well-documented foundation for your data landscape.\n\nHere's an example of how you might define a catalog and its schemas using the Databricks provider for Terraform:\n\n```terraform\nresource \"databricks_catalog\" \"prod_catalog\" {\n  name          = \"prod\"\n  comment       = \"Main production catalog for all enterprise data.\"\n  storage_root  = var.default_catalog_storage_root\n  force_destroy = false\n}\n\n# Schemas within the 'prod' catalog\nresource \"databricks_schema\" \"prod_raw\" {\n  catalog_name = databricks_catalog.prod_catalog.name\n  name         = \"raw\"\n  comment      = \"Raw data for all different projects, telemetry, game data etc., before any transformations. No schema enforcement.\"\n}\n\nresource \"databricks_schema\" \"prod_bi_conformed\" {\n  catalog_name = databricks_catalog.prod_catalog.name\n  name         = \"bi_conformed\"\n  comment      = \"Conformed (silver) schema for Business Intelligence, cleaned and well-formatted. Schema enforced.\"\n}\n\nresource \"databricks_schema\" \"prod_bi_modeled\" {\n  catalog_name = databricks_catalog.prod_catalog.name\n  name         = \"bi_modeled\"\n  comment      = \"Modeled (gold) schema for Business Intelligence, aggregated and ready for consumption. Schema enforced.\"\n}\n```\n\nFear not if you already have existing catalogs and schemas in Unity Catalog. You don't need to recreate them to manage their metadata as code. Terraform provides the `terraform import` command, which allows you to bring existing infrastructure (including Unity Catalog assets) under its management. Once imported, you can define the resource in your Terraform configuration and selectively update attributes like the `comment` field without affecting the asset itself. For example, after importing an existing schema, you could add or update its `comment` in your `.tf` file, and `terraform apply` would only apply that change.\n\nAdopting a metadata-as-code strategy, especially for foundational elements like catalogs and schemas, greatly enhances the quality and reliability of the metadata that this MCP server leverages. This, in turn, further improves the effectiveness of AI agents interacting with your Databricks data.\n\nFor more details on using Terraform with Databricks Unity Catalog, refer to the official documentation:\n*   Databricks Provider: Catalog Resource ([https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog))\n*   Databricks Provider: Schemas Data Source ([https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/schemas](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/schemas))\n\n## Handling Long-Running Queries\n\nThe `execute_sql_query` tool utilizes the Databricks SDK's `execute_statement` method. The `wait_timeout` parameter in the underlying `databricks_sdk_utils.execute_databricks_sql` function is set to '50s'. If a query runs longer than this, the SDK may return a statement ID for polling, but the current implementation of the tool effectively waits up to this duration for a synchronous-like response. For very long-running queries, this timeout might be reached.\n\n## Dependencies\n\n-   `databricks-sdk`: For interacting with the Databricks REST APIs and Unity Catalog.\n-   `python-dotenv`: For loading environment variables from a `.env` file.\n-   `mcp[cli]`: The Model Context Protocol library.\n-   `asyncio`: For asynchronous operations within the MCP server.\n-   `httpx` (typically a sub-dependency of `databricks-sdk` or `mcp`): For making HTTP requests.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databricks",
        "databases",
        "database",
        "queries databricks",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "RathodDarshil--mcp-postgres-query-server": {
      "owner": "RathodDarshil",
      "name": "mcp-postgres-query-server",
      "url": "https://github.com/RathodDarshil/mcp-postgres-query-server",
      "imageUrl": "/freedevtools/mcp/pfp/RathodDarshil.webp",
      "description": "Provides a secure, read-only interface for executing validated SQL SELECT queries on a PostgreSQL database and returns structured JSON responses. Ensures safe data access with query timeout protection and seamless integration with MCP clients like Claude Desktop.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-08T17:55:15Z",
      "readme_content": "# MCP Postgres Query Server\n\nA Model Context Protocol (MCP) server implementation for querying a PostgreSQL database in read-only mode, designed to work with Claude Desktop and other MCP clients.\n\n## Overview\n\nThis project implements a Model Context Protocol (MCP) server that provides:\n\n1. A secure, read-only interface to a PostgreSQL database\n2. Integration with Claude Desktop through the MCP protocol\n3. SQL query validation to ensure only SELECT queries are executed\n4. Query timeout protection (10 seconds)\n\n## Prerequisites\n\n-   Node.js (v14 or later)\n-   npm (comes with Node.js)\n-   PostgreSQL database (connection details provided via command line)\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/RathodDarshil/mcp-postgres-query-server.git\ncd mcp-postgres-query-server\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Connecting to Claude Desktop\n\nYou can configure Claude Desktop to automatically launch and connect to the MCP server:\n\n1. Access the Claude Desktop configuration file:\n\n    - Open Claude Desktop\n    - Go to Settings > Developer > Edit Config\n    - This will open the configuration file in your default text editor\n\n2. Add the postgres-query-server to the `mcpServers` section of your `claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"postgres-query\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/path/to/your/mcp-postgres-query-server/dist/index.js\",\n                \"postgresql://username:password@hostname:port/database\"\n            ]\n        }\n    }\n}\n```\n\n3. Replace `/path/to/your/` with the actual path to your project directory.\n4. Replace the PostgreSQL connection string with your actual database credentials.\n5. Save the file and restart Claude Desktop. The MCP server should now appear in the MCP server selection dropdown in Settings.\n\n### Example Configuration\n\nHere's a complete example of a configuration file with postgres-query:\n\n```json\n{\n    \"mcpServers\": {\n        \"postgres-query\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Users/darshilrathod/mcp-servers/mcp-postgres-query-server/dist/index.js\",\n                \"postgresql://user:password@localhost:5432/mydatabase\"\n            ]\n        }\n    }\n}\n```\n\n### Updating Configuration\n\nTo update your Claude Desktop configuration:\n\n1. Open Claude Desktop\n2. Go to Settings > Developer > Edit Config\n3. Make your changes to the configuration file\n4. Save the file\n5. Restart Claude Desktop for the changes to take effect\n6. If you've updated the MCP server code, make sure to rebuild it with `npm run build` before restarting\n\n## Features\n\n-   **Read-Only Database Access**: Only SELECT queries are permitted for security\n-   **Query Validation**: Prevents potentially harmful SQL operations\n-   **Timeout Protection**: Queries running longer than 10 seconds are automatically terminated\n-   **MCP Protocol Support**: Complete implementation of the Model Context Protocol\n-   **JSON Response Formatting**: Query results are returned in structured JSON format\n\n## API\n\n### Tools\n\n#### query-postgres\n\nExecutes a read-only SQL query against the configured PostgreSQL database.\n\nParameters:\n\n-   `query` (string): A SQL SELECT query to execute\n\nResponse:\n\n-   JSON object containing:\n    -   `rows`: The result set rows\n    -   `rowCount`: Number of rows returned\n    -   `fields`: Column metadata\n\nExample:\n\n```\nquery-postgres: SELECT * FROM users LIMIT 5\n```\n\n## Development\n\nThe main server implementation is in `src/index.ts`. Key components:\n\n-   PostgreSQL connection pool setup\n-   Query validation logic\n-   MCP server configuration\n-   Tool and resource definitions\n\nTo modify the server's behavior, you can:\n\n-   Edit the query validation logic in `isReadOnlyQuery()`\n-   Add additional tools or resources to the MCP server\n-   Modify the query timeout duration (currently 10 seconds)\n\n## Security Considerations\n\n-   The server validates all queries to ensure they are read-only\n-   Connection to the database uses SSL\n-   Query timeout prevents resource exhaustion\n-   No write operations are permitted\n-   Database credentials are passed directly via command line arguments, not stored in files\n\n## License\n\nISC\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "mcp postgres",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "RichardHan--mssql_mcp_server": {
      "owner": "RichardHan",
      "name": "mssql_mcp_server",
      "url": "https://github.com/RichardHan/mssql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/RichardHan.webp",
      "description": "Enables secure interaction with Microsoft SQL Server databases, allowing users to list tables, read data, and execute SQL queries through a controlled interface. Provides structured access for database exploration and analysis.",
      "stars": 240,
      "forks": 63,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T13:43:12Z",
      "readme_content": "# Microsoft SQL Server MCP Server\n\n[![PyPI](https://img.shields.io/pypi/v/microsoft_sql_server_mcp)](https://pypi.org/project/microsoft_sql_server_mcp/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<a href=\"https://glama.ai/mcp/servers/29cpe19k30\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/29cpe19k30/badge\" alt=\"Microsoft SQL Server MCP server\" />\n</a>\n\nA Model Context Protocol (MCP) server for secure SQL Server database access through Claude Desktop.\n\n## Features\n\n- 🔍 List database tables\n- 📊 Execute SQL queries (SELECT, INSERT, UPDATE, DELETE)\n- 🔐 Multiple authentication methods (SQL, Windows, Azure AD)\n- 🏢 LocalDB and Azure SQL support\n- 🔌 Custom port configuration\n\n## Quick Start\n\n### Install with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"uvx\",\n      \"args\": [\"microsoft_sql_server_mcp\"],\n      \"env\": {\n        \"MSSQL_SERVER\": \"localhost\",\n        \"MSSQL_DATABASE\": \"your_database\",\n        \"MSSQL_USER\": \"your_username\",\n        \"MSSQL_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Basic SQL Authentication\n```bash\nMSSQL_SERVER=localhost          # Required\nMSSQL_DATABASE=your_database    # Required\nMSSQL_USER=your_username        # Required for SQL auth\nMSSQL_PASSWORD=your_password    # Required for SQL auth\n```\n\n### Windows Authentication\n```bash\nMSSQL_SERVER=localhost\nMSSQL_DATABASE=your_database\nMSSQL_WINDOWS_AUTH=true         # Use Windows credentials\n```\n\n### Azure SQL Database\n```bash\nMSSQL_SERVER=your-server.database.windows.net\nMSSQL_DATABASE=your_database\nMSSQL_USER=your_username\nMSSQL_PASSWORD=your_password\n# Encryption is automatic for Azure\n```\n\n### Optional Settings\n```bash\nMSSQL_PORT=1433                 # Custom port (default: 1433)\nMSSQL_ENCRYPT=true              # Force encryption\n```\n\n## Alternative Installation Methods\n\n### Using pip\n```bash\npip install microsoft_sql_server_mcp\n```\n\nThen in `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mssql_mcp_server\"],\n      \"env\": { ... }\n    }\n  }\n}\n```\n\n### Development\n```bash\ngit clone https://github.com/RichardHan/mssql_mcp_server.git\ncd mssql_mcp_server\npip install -e .\n```\n\n## Security\n\n- Create a dedicated SQL user with minimal permissions\n- Never use admin/sa accounts\n- Use Windows Authentication when possible\n- Enable encryption for sensitive data\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql_mcp_server",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "RyanLisse--lancedb_mcp": {
      "owner": "RyanLisse",
      "name": "lancedb_mcp",
      "url": "https://github.com/RyanLisse/lancedb_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RyanLisse.webp",
      "description": "Manage and search vector embeddings efficiently with capabilities for creating vector tables, adding data, and performing similarity searches. Supports the storage of embeddings along with associated metadata.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-11T10:43:36Z",
      "readme_content": "# LanceDB MCP Server\n\n## Overview\nA Model Context Protocol (MCP) server implementation for LanceDB vector database operations. This server enables efficient vector storage, similarity search, and management of vector embeddings with associated metadata.\n\n## Components\n\n### Resources\nThe server exposes vector database tables as resources:\n- `table://{name}`: A vector database table that stores embeddings and metadata\n  - Configurable vector dimensions\n  - Text metadata support\n  - Efficient similarity search capabilities\n\n### API Endpoints\n\n#### Table Management\n- `POST /table`\n   - Create a new vector table\n   - Input:\n     ```python\n     {\n       \"name\": \"my_table\",      # Table name\n       \"dimension\": 768         # Vector dimension\n     }\n     ```\n\n#### Vector Operations\n- `POST /table/{table_name}/vector`\n   - Add vector data to a table\n   - Input:\n     ```python\n     {\n       \"vector\": [0.1, 0.2, ...],  # Vector data\n       \"text\": \"associated text\"    # Metadata\n     }\n     ```\n\n- `POST /table/{table_name}/search`\n   - Search for similar vectors\n   - Input:\n     ```python\n     {\n       \"vector\": [0.1, 0.2, ...],  # Query vector\n       \"limit\": 10                  # Number of results\n     }\n     ```\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/lancedb_mcp.git\ncd lancedb_mcp\n\n# Install dependencies using uv\nuv pip install -e .\n```\n\n## Usage with Claude Desktop\n\n```bash\n# Add the server to your claude_desktop_config.json\n\"mcpServers\": {\n  \"lancedb\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"run\",\n      \"python\",\n      \"-m\",\n      \"lancedb_mcp\",\n      \"--db-path\",\n      \"~/.lancedb\"\n    ]\n  }\n}\n```\n\n## Development\n\n```bash\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\npytest\n\n# Format code\nblack .\nruff .\n```\n\n## Environment Variables\n\n- `LANCEDB_URI`: Path to LanceDB storage (default: \".lancedb\")\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "enables querying"
      ],
      "category": "databases"
    },
    "SarimSiddd--xano_mcp": {
      "owner": "SarimSiddd",
      "name": "xano_mcp",
      "url": "https://github.com/SarimSiddd/xano_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SarimSiddd.webp",
      "description": "Interacts with the Xano API to manage database operations, providing tools for secure authentication, workspace management, and table content operations. Supports type-safe API interactions using TypeScript and offers improved error handling with detailed messages.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-21T21:01:25Z",
      "readme_content": "# Xano MCP Server\n\nA Model Context Protocol (MCP) server implementation for interacting with the Xano API. This server provides tools and resources for managing Xano database operations through the MCP interface.\n\n## Features\n\n- Secure authentication with Xano API\n- Type-safe API interactions using TypeScript\n- Environment-based configuration\n- MCP-compliant interface\n- Workspace management tools\n- Table content operations (create, read, update)\n- Improved error handling with detailed messages\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone [your-repo-url]\ncd xano_mcp\n\n# Install dependencies\nnpm install\n```\n\n## Configuration\n\n1. Copy the example environment file:\n```bash\ncp .env.example .env\n```\n\n2. Update the `.env` file with your Xano credentials:\n```env\nXANO_API_KEY=your_api_key_here\nXANO_API_URL=your_xano_api_url\nNODE_ENV=development\nAPI_TIMEOUT=10000\n```\n\n## Development\n\n```bash\n# Build the project\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Start the server\nnpm start\n```\n\n## Project Structure\n\n```\nxano_mcp/\n├── src/\n│   ├── api/\n│   │   └── xano/\n│   │       ├── client/       # API client implementation\n│   │       ├── models/       # Data models and types\n│   │       ├── services/     # API service implementations\n│   │       └── utils/        # Utility functions\n│   ├── mcp/\n│   │   ├── server/          # MCP server implementation\n│   │   ├── tools/           # MCP tool implementations\n│   │   └── types/           # Tool-specific types\n│   ├── config.ts            # Configuration management\n│   └── index.ts             # Main entry point\n├── .env                     # Environment variables (not in git)\n├── .env.example            # Example environment variables\n└── tsconfig.json           # TypeScript configuration\n```\n\n## Available MCP Tools\n\n### Workspace Tools\n- `get_workspaces`: List all available workspaces\n\n### Table Tools\n- `create_table`: Create a new table in a workspace\n- `get_table_content`: Get content from a table with pagination support\n- `add_table_content`: Add new content to a table\n- `update_table_content`: Update existing content in a table\n- `get_all_tables`: List all tables in a workspace with detailed information\n\n## Usage Examples\n\n### Working with Workspaces\n```typescript\n// List available workspaces\nconst result = await mcp.use_tool(\"get_workspaces\", {});\nconsole.log('Workspaces:', result);\n```\n\n### Managing Tables\n```typescript\n// Create a new table\nconst createResult = await mcp.use_tool(\"create_table\", {\n  workspaceId: 123,\n  name: \"MyTable\"\n});\n\n// Add content to a table\nconst addResult = await mcp.use_tool(\"add_table_content\", {\n  workspaceId: 123,\n  tableId: 456,\n  content: {\n    created_at: \"2024-01-22T17:07:00.000Z\"\n  }\n});\n\n// Get table content with pagination\nconst getResult = await mcp.use_tool(\"get_table_content\", {\n  workspaceId: 123,\n  tableId: 456,\n  pagination: {\n    page: 1,\n    items: 50\n  }\n});\n\n// Update table content\nconst updateResult = await mcp.use_tool(\"update_table_content\", {\n  workspaceId: 123,\n  tableId: 456,\n  contentId: \"789\",\n  content: {\n    created_at: \"2024-01-22T17:07:00.000Z\"\n  }\n});\n\n// List all tables in a workspace\nconst tables = await mcp.use_tool(\"get_all_tables\", {\n  workspaceId: 123\n});\nconsole.log('Tables:', tables);\n// Returns an array of tables with their details:\n// [\n//   {\n//     id: number,\n//     name: string,\n//     description: string,\n//     created_at: string,\n//     updated_at: string,\n//     guid: string,\n//     auth: boolean,\n//     tag: string[],\n//     workspaceId: number\n//   },\n//   ...\n// ]\n```\n\n## Environment Variables\n\n| Variable | Description | Required | Default |\n|----------|-------------|----------|---------|\n| XANO_API_KEY | Your Xano API authentication key | Yes | - |\n| XANO_API_URL | Xano API endpoint URL | Yes | - |\n| NODE_ENV | Environment (development/production) | No | development |\n| API_TIMEOUT | API request timeout in milliseconds | No | 10000 |\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Invalid parameters\n- Authentication failures\n- API request failures\n- Content validation errors\n- Unknown tool requests\n\n## Security\n\n- Environment variables are used for sensitive configuration\n- TruffleHog configuration is included to prevent secret leaks\n- API keys and sensitive data are never committed to the repository\n\n## Contributing\n\n1. Create a feature branch\n2. Make your changes\n3. Submit a pull request\n\n## License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xano_mcp",
        "xano",
        "databases",
        "xano api",
        "sarimsiddd xano_mcp",
        "interacts xano"
      ],
      "category": "databases"
    },
    "Snowflake-Labs--mcp": {
      "owner": "Snowflake-Labs",
      "name": "mcp",
      "url": "https://github.com/Snowflake-Labs/mcp",
      "imageUrl": "",
      "description": "Open-source MCP server for Snowflake from official Snowflake-Labs supports prompting Cortex Agents, querying structured & unstructured data, object management, SQL execution, semantic view querying, and more. RBAC, fine-grained CRUD controls, and all authentication methods supported.",
      "stars": 140,
      "forks": 39,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T17:30:56Z",
      "readme_content": "# Snowflake Cortex AI Model Context Protocol (MCP) Server\n\n<a href=\"https://emerging-solutions-toolbox.streamlit.app/\">\n    <img src=\"https://github.com/user-attachments/assets/aa206d11-1d86-4f32-8a6d-49fe9715b098\" alt=\"image\" width=\"150\" align=\"right\";\">\n</a>\n\nThis Snowflake MCP server provides tooling for Snowflake Cortex AI, object management, and SQL orchestration, bringing these capabilities to the MCP ecosystem. When connected to an MCP Client (e.g. [Claude for Desktop](https://claude.ai/download), [fast-agent](https://fast-agent.ai/), [Agentic Orchestration Framework](https://github.com/Snowflake-Labs/orchestration-framework/blob/main/README.md)), users can leverage these features.\n\nThe MCP server currently supports the below capabilities:\n- **[Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview)**: Query unstructured data in Snowflake as commonly used in Retrieval Augmented Generation (RAG) applications.\n- **[Cortex Analyst](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst)**: Query structured data in Snowflake via rich semantic modeling.\n- **[Cortex Agent](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-agents)**: Agentic orchestrator across structured and unstructured data retrieval\n- **Object Management**: Perform basic operations against Snowflake's most common objects such as creation, dropping, updating, and more.\n- **SQL Execution**: Run LLM-generated SQL managed by user-configured permissions.\n- **[Semantic View Querying](https://docs.snowflake.com/en/user-guide/views-semantic/overview)**: Discover and query Snowflake Semantic Views\n\n# Getting Started\n\n## Service Configuration\n\nA simple configuration file is used to drive all tooling. An example can be seen at [services/configuration.yaml](services/configuration.yaml) and a template is below. The path to this configuration file will be passed to the server and the contents used to create MCP server tools at startup.\n\n**Cortex Services**\n\nMany Cortex Agent, Search, and Analyst services can be added. Ideal descriptions are both highly descriptive and mutually exclusive.\nOnly the explicitly listed Cortex services will be available as tools in the MCP client.\n\n**Other Services**\n\nOther services include tooling for [object management](object-management), [query execution](sql-execution), and [semantic view usage](semantic-view-querying).\nThese groups of tools can be enabled by setting them to True in the `other_services` section of the configuration file.\n\n**SQL Statement Permissions**\n\nThe `sql_statement_permissions` section ensures that only approved statements are executed across any tools with access to change Snowflake objects.\nThe list contains SQL expression types. Those marked with True are permitted while those marked with False are not permitted. Please see [SQL Execution](#sql-execution) for examples of each expression type.\n\n```\nagent_services: # List all Cortex Agent services\n  - service_name: <service_name>\n    description: > # Describe contents of the agent service\n      <Agent service that ...>\n    database_name: <database_name>\n    schema_name: <schema_name>\n  - service_name: <service_name>\n    description: > # Describe contents of the agent service\n      <Agent service that ...>\n    database_name: <database_name>\n    schema_name: <schema_name>\nsearch_services: # List all Cortex Search services\n  - service_name: <service_name>\n    description: > # Describe contents of the search service\n      <Search services that ...>\n    database_name: <database_name>\n    schema_name: <schema_name>\n  - service_name: <service_name>\n    description: > # Describe contents of the search service\n      <Search services that ...>\n    database_name: <database_name>\n    schema_name: <schema_name>\nanalyst_services: # List all Cortex Analyst semantic models/views\n  - service_name: <service_name> # Create descriptive name for the service\n    semantic_model: <semantic_yaml_or_view> # Fully-qualify semantic YAML model or Semantic View\n    description: > # Describe contents of the analyst service\n      <Analyst service that ...>\n  - service_name: <service_name> # Create descriptive name for the service\n    semantic_model: <semantic_yaml_or_view> # Fully-qualify semantic YAML model or Semantic View\n    description: > # Describe contents of the analyst service\n      <Analyst service that ...>\nother_services: # Set desired tool groups to True to enable tools for that group\n  object_manager: True # Perform basic operations against Snowflake's most common objects such as creation, dropping, updating, and more.\n  query_manager: True # Run LLM-generated SQL managed by user-configured permissions.\n  semantic_manager: True # Discover and query Snowflake Semantic Views and their components.\nsql_statement_permissions: # List SQL statements to explicitly allow (True) or disallow (False).\n  # - All: True # To allow everything, uncomment and set All: True.\n  - Alter: True\n  - Command: True\n  - Comment: True\n  - Commit: True\n  - Create: True\n  - Delete: True\n  - Describe: True\n  - Drop: True\n  - Insert: True\n  - Merge: True\n  - Rollback: True\n  - Select: True\n  - Transaction: True\n  - TruncateTable: True\n  - Unknown: False # To allow unknown or unmapped statement types, set Unknown: True.\n  - Update: True\n  - Use: True\n```\n\n> [!NOTE]\n> Previous versions of the configuration file supported specifying explicit values for columns and limit for each Cortex Search service. Instead, these are now exclusively dynamic based on user prompt. If not specified, a search service's default search_columns will be returned with a limit of 10.\n\n## Connecting to Snowflake\n\nThe MCP server uses the [Snowflake Python Connector](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect) for all authentication and connection methods. **Please refer to the official Snowflake documentation for comprehensive authentication options and best practices.**\n\n**The MCP server honors the RBAC permissions assigned to the specified role (as passed in the connection parameters) or default role of the user (if no role is passed to connect).**\n\nConnection parameters can be passed as CLI arguments and/or environment variables. The server supports all authentication methods available in the Snowflake Python Connector, including:\n\n- Username/password authentication\n- Key pair authentication\n- OAuth authentication\n- Single Sign-On (SSO)\n- Multi-factor authentication (MFA)\n\n### Connection Parameters\n\nConnection parameters can be passed as CLI arguments and/or environment variables:\n\n| Parameter | CLI Arguments | Environment Variable | Description |\n|-----------|--------------|---------------------|-------------|\n| Account | --account | SNOWFLAKE_ACCOUNT | Account identifier (e.g. xy12345.us-east-1) |\n| Host | --host | SNOWFLAKE_HOST | Snowflake host URL |\n| User | --user, --username | SNOWFLAKE_USER | Username for authentication |\n| Password | --password | SNOWFLAKE_PASSWORD | Password or programmatic access token |\n| Role | --role | SNOWFLAKE_ROLE | Role to use for connection |\n| Warehouse | --warehouse | SNOWFLAKE_WAREHOUSE | Warehouse to use for queries |\n| Passcode in Password | --passcode-in-password | - | Whether passcode is embedded in password |\n| Passcode | --passcode | SNOWFLAKE_PASSCODE | MFA passcode for authentication |\n| Private Key | --private-key | SNOWFLAKE_PRIVATE_KEY | Private key for key pair authentication |\n| Private Key File | --private-key-file | SNOWFLAKE_PRIVATE_KEY_FILE | Path to private key file |\n| Private Key Password | --private-key-file-pwd | SNOWFLAKE_PRIVATE_KEY_FILE_PWD | Password for encrypted private key |\n| Authenticator | --authenticator | - | Authentication type (default: snowflake) |\n| Connection Name | --connection-name | - | Name of connection from connections.toml (or config.toml) file |\n\n> [!WARNING]\n> **Deprecation Notice**: The CLI arguments `--account-identifier` and `--pat`, as well as the environment variable `SNOWFLAKE_PAT`, are deprecated and will be removed in a future release. Please use `--account` and `--password` (or `SNOWFLAKE_ACCOUNT` and `SNOWFLAKE_PASSWORD`) instead.\n\n# Transport Configuration\n\nThe MCP server supports multiple transport mechanisms. For detailed information about MCP transports, see [FastMCP Transport Protocols](https://gofastmcp.com/deployment/running-server#transport-protocols).\n\n| Transport | Description | Use Case |\n|-----------|-------------|----------|\n| `stdio` | Standard input/output (default) | Local development, MCP client integration |\n| `sse` (legacy) | Server-Sent Events | Streaming applications |\n| `streamable-http` | Streamable HTTP transport | Container deployments, remote servers |\n\n## Usage\n\n```bash\n# Default stdio transport\nuvx snowflake-labs-mcp --service-config-file config.yaml\n\n# HTTP transport with custom endpoint\nuvx snowflake-labs-mcp --service-config-file config.yaml --transport streamable-http --endpoint /my-endpoint\n\n# For containers (uses streamable-http on port 9000)\nuvx snowflake-labs-mcp --service-config-file config.yaml --transport streamable-http --endpoint /snowflake-mcp\n```\n\n# Use environment variable for endpoint\n\n```bash\nexport SNOWFLAKE_MCP_ENDPOINT=\"/my-mcp\"\nuvx snowflake-labs-mcp --service-config-file config.yaml --transport streamable-http\n```\n\n# Using with MCP Clients\n\nThe MCP server is client-agnostic and will work with most MCP Clients that support basic functionality for MCP tools and (optionally) resources. Below are examples for local installation. For connecting to containerized deployments, see [Connecting MCP Clients to Containers](#connecting-mcp-clients-to-containers).\n\n## [Claude Desktop](https://support.anthropic.com/en/articles/10065433-installing-claude-for-desktop)\n\nTo integrate this server with Claude Desktop as the MCP Client, add the following to your app's server configuration. By default, this is located at:\n- macOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n- Windows: %APPDATA%\\Claude\\claude_desktop_config.json\n\nSet the path to the service configuration file and configure your connection method:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-snowflake\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"snowflake-labs-mcp\",\n        \"--service-config-file\",\n        \"<path_to_file>/tools_config.yaml\",\n        \"--connection-name\",\n        \"default\"\n      ]\n    }\n  }\n}\n```\n\n## [Cursor](https://www.cursor.com/)\n\nRegister the MCP server in Cursor by opening Cursor and navigating to Settings -> Cursor Settings -> MCP. Add the below:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-snowflake\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"snowflake-labs-mcp\",\n        \"--service-config-file\",\n        \"<path_to_file>/tools_config.yaml\",\n        \"--connection-name\",\n        \"default\"\n      ]\n    }\n  }\n}\n```\n\nAdd the MCP server as context in the chat.\n\n<img alt=\"Cursor\" src=\"https://sfquickstarts.s3.us-west-1.amazonaws.com/misc/mcp/Cursor.gif\" width=\"800\"/>\n\nFor troubleshooting Cursor server issues, view the logs by opening the Output panel and selecting Cursor MCP from the dropdown menu.\n\n## [fast-agent](https://fast-agent.ai/)\n\nUpdate the `fastagent.config.yaml` mcp server section with the configuration file path and connection name:\n\n```yaml\n# MCP Servers\nmcp:\n    servers:\n        mcp-server-snowflake:\n            command: \"uvx\"\n            args: [\"snowflake-labs-mcp\", \"--service-config-file\", \"<path_to_file>/tools_config.yaml\", \"--connection-name\", \"default\"]\n```\n\n<img alt=\"fast_agent\" src=\"https://sfquickstarts.s3.us-west-1.amazonaws.com/misc/mcp/fast-agent.gif\" width=\"800\"/>\n\n## Microsoft Visual Studio Code + GitHub Copilot\n\nFor prerequisites, environment setup, step-by-step guide and instructions, please refer to this [blog](https://medium.com/snowflake/build-a-natural-language-data-assistant-in-vs-code-with-copilot-mcp-and-snowflake-cortex-ai-04a22a3b0f17).\n\n<img alt=\"dash_dark_mcp_copilot\" src=\"https://sfquickstarts.s3.us-west-1.amazonaws.com/misc/mcp/dash-dark-mcp-copilot.gif\"/>\n\n\n## [Codex](https://github.com/openai/codex)\nRegister the MCP server in codex by adding the following to `~/.codex/config.toml`\n```toml\n[mcp_servers.mcp-server-snowflake]\ncommand = \"uvx\"\nargs = [\n    \"snowflake-labs-mcp\",\n    \"--service-config-file\",\n    \"<path_to_file>/tools_config.yaml\",\n    \"--connection-name\",\n    \"default\"\n]\n```\nAfter editing, the snowflake mcp should appear in the output of `codex mcp list` run from the terminal.\n\n# Container Deployment\n\nDeploy the MCP server as a container for remote access or production environments. This guide provides step-by-step instructions for both Docker and Docker Compose deployments.\n\n## Docker Deployment\n\nFollow these steps to deploy the MCP server using Docker:\n\n### Step 1: Prepare Configuration File\nCreate a directory for MCP configuration and copy the template:\n```bash\nmkdir -p ${HOME}/.mcp/\ncp services/configuration.yaml ${HOME}/.mcp/tools_config.yaml\n```\n\n### Step 2: Configure Services\nEdit the configuration file to match your environment:\n```bash\n# Edit the configuration file as needed\n# Update service names, database/schema references, and enable desired features\nnano ${HOME}/.mcp/tools_config.yaml\n```\n\n### Step 3: Build Container Image\nBuild the Docker image from the provided Dockerfile:\n```bash\ndocker build -f docker/server/Dockerfile -t mcp-server-snowflake .\n```\n\n### Step 4: Set Environment Variables\nConfigure your Snowflake connection parameters. Choose one of the following authentication methods:\n\n**Username/Password Authentication:**\n```bash\nexport SNOWFLAKE_ACCOUNT=<your_account>\nexport SNOWFLAKE_USER=<your_username>\nexport SNOWFLAKE_PASSWORD=<your_password>\n```\n\n**Key Pair Authentication:**\n```bash\nexport SNOWFLAKE_ACCOUNT=<your_account>\nexport SNOWFLAKE_USER=<your_username>\nexport SNOWFLAKE_PRIVATE_KEY=\"$(cat <path_to_private_key.p8>)\"\nexport SNOWFLAKE_PRIVATE_KEY_FILE_PWD=<your_key_password>\n```\n\n### Step 5: Run Container\nStart the container with your configuration and environment variables:\n\n**For Username/Password Authentication:**\n```bash\ndocker run -d \\\n  --name mcp-server-snowflake \\\n  -p 9000:9000 \\\n  -e SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT} \\\n  -e SNOWFLAKE_USER=${SNOWFLAKE_USER} \\\n  -e SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD} \\\n  -v ${HOME}/.mcp/tools_config.yaml:/app/services/tools_config.yaml:ro \\\n  mcp-server-snowflake\n```\n\n**For Key Pair Authentication:**\n```bash\ndocker run -d \\\n  --name mcp-server-snowflake \\\n  -p 9000:9000 \\\n  -e SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT} \\\n  -e SNOWFLAKE_USER=${SNOWFLAKE_USER} \\\n  -e SNOWFLAKE_PRIVATE_KEY=\"${SNOWFLAKE_PRIVATE_KEY}\" \\\n  -e SNOWFLAKE_PRIVATE_KEY_FILE_PWD=${SNOWFLAKE_PRIVATE_KEY_FILE_PWD} \\\n  -v ${HOME}/.mcp/tools_config.yaml:/app/services/tools_config.yaml:ro \\\n  mcp-server-snowflake\n```\n\n### Step 6: Verify Deployment\nCheck that the container is running and accessible:\n```bash\n# Check container status\ndocker ps\n\n# Check container logs\ndocker logs mcp-server-snowflake\n\n# Test endpoint (should return MCP server info)\ncurl http://localhost:9000/snowflake-mcp\n```\n\n## Docker Compose Deployment\n\nFollow these steps for a simplified deployment using Docker Compose:\n\n### Step 1: Prepare Configuration File\nCreate the configuration directory and copy the template:\n```bash\nmkdir -p ${HOME}/.mcp/\ncp services/configuration.yaml ${HOME}/.mcp/tools_config.yaml\n```\n\n### Step 2: Configure Services\nEdit the configuration file to match your environment:\n```bash\n# Update service configurations as needed\nnano ${HOME}/.mcp/tools_config.yaml\n```\n\n### Step 3: Set Environment Variables\nConfigure your Snowflake connection parameters:\n```bash\nexport SNOWFLAKE_ACCOUNT=<your_account>\nexport SNOWFLAKE_USER=<your_username>\n# For username/password auth:\nexport SNOWFLAKE_PASSWORD=<your_password>\n# For key pair auth, also set:\n# export SNOWFLAKE_PRIVATE_KEY=\"$(cat <path_to_private_key.p8>)\"\n# export SNOWFLAKE_PRIVATE_KEY_FILE_PWD=<your_key_password>\n```\n\n### Step 4: Start Services\nLaunch the container using Docker Compose:\n```bash\ndocker-compose up -d\n```\n\n### Step 5: Verify Deployment\nCheck that the services are running:\n```bash\n# Check service status\ndocker-compose ps\n\n# View logs\ndocker-compose logs\n\n# Test endpoint\ncurl http://localhost:9000/snowflake-mcp\n```\n\n## Connecting MCP Clients to Containers\n\nOnce your MCP server is running in a container, you can connect various MCP clients to it. The connection configuration is the same across all clients - only the configuration format differs.\n\n**Connection URL Format:**\n- Local deployment: `http://localhost:9000/snowflake-mcp`\n- Remote deployment: `http://<hostname>:<port>/snowflake-mcp`\n\n### Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-snowflake\": {\n      \"url\": \"http://localhost:9000/snowflake-mcp\"\n    }\n  }\n}\n```\n\n### Cursor\nAdd this to your MCP settings in Cursor (Settings -> Cursor Settings -> MCP):\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-snowflake\": {\n      \"url\": \"http://localhost:9000/snowflake-mcp\"\n    }\n  }\n}\n```\n\n### fast-agent\nAdd this to your `fastagent.config.yaml`:\n```yaml\n# MCP Servers\nmcp:\n    servers:\n        mcp-server-snowflake:\n            url: \"http://localhost:9000/snowflake-mcp\"\n```\n\n**Notes:**\n- For remote deployments, replace `localhost:9000` with your server's hostname and port\n- Ensure your firewall allows connections on port 9000 (or your configured port)\n- For production deployments, consider using HTTPS and proper authentication\n\n# Cortex Services\n\nInstances of Cortex Agent (in `agent_services` section), Cortex Search (in `search_services` section), and Cortex Analyst (in `analyst_services` section) of the configuration file will be served as tools. Leave these sections blank to omit such tools.\n\nOnly Cortex Agent objects are supported in the MCP server. That is, only Cortex Agent objects pre-configured in Snowflake can be leveraged as tools. See [Cortex Agent Run API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-agents-run#streaming-responses) for more details.\n\nEnsure all services have accurate context names for service name, database, schema, etc. Ideal descriptions are both highly descriptive and mutually exclusive.\n\nThe `semantic_model` value in analyst services should be a fully-qualified semantic view OR semantic YAML file in a Snowflake stage:\n- For a semantic view: `MY_DATABASE.MY_SCHEMA.MY_SEMANTIC_VIEW`\n- For a semantic YAML file: `@MY_DATABASE.MY_SCHEMA.MY_STAGE/my_semantic_file.yaml` (**Note the `@`.**)\n\n# Object Management\n\nThe MCP server includes dozens of tools narrowly scoped to fulfill basic operation management. It is recommended to use Snowsight directly for advanced object management.\n\nThe MCP server currently supports **creating**, **dropping**, **creating or altering**, **describing**, and **listing** the below object types.\n**To enable these tools, set `object_manager` to True in the configuration file under `other_services`.**\n\n```\n- Database\n- Schema\n- Table\n- View\n- Warehouse\n- Compute Pool\n- Role\n- Stage\n- User\n- Image Repository\n```\n\nPlease note that these tools are also governed by permissions captured in the configuration file under `sql_statement_permissions`.\nObject management tools to create and create or alter objects are governed by the `Create` permission. Object dropping is governed by the `Drop` permission.\n\nIt is likely that more actions and objects will be included in future releases.\n\n# SQL Execution\n\nThe general SQL tool will provide a way to execute generic SQL statements generated by the MCP client. Users have full control over the types of SQL statement that are approved in the configuration file.\n\nListed in the configuration file under `sql_statement_permissions` are [sqlglot expression types](https://sqlglot.com/sqlglot/expressions.html). Those marked as False will be stopped before execution. Those marked with True will be executed (or prompt the user for execution based on the MCP client settings).\n\n**To enable the SQL execution tool, set `query_manager` to True in the configuration file under `other_services`.**\n**To allow all SQL expressions to pass the additional validation, set `All` to True.**\n\nNot all Snowflake SQL commands are mapped in sqlglot and you may find some obscure commands have yet to be captured in the configuration file.\n**Setting `Unknown` to True will allow these uncaptured commands to pass the additional validation.** You may also add new expression types directly to honor specific ones.\n\nBelow are some examples of sqlglot expression types with accompanying Snowflake SQL command examples:\n\n| SQLGlot Expression Type | SQL Command |\n|------------------------|-------------|\n| Alter | `ALTER TABLE my_table ADD COLUMN new_column VARCHAR(50);` |\n| Command | `CALL my_procedure('param1_value', 123);`<br/>`GRANT ROLE analyst TO USER user1;`<br/>`SHOW TABLES IN SCHEMA my_database.my_schema;` |\n| Comment | `COMMENT ON TABLE my_table IS 'This table stores customer data.';` |\n| Commit | `COMMIT;` |\n| Create | `CREATE TABLE my_table ( id INT, name VARCHAR(255), email VARCHAR(255) );`<br/>`CREATE OR ALTER VIEW my_schema.my_new_view AS SELECT id, name, created_at FROM my_schema.my_table WHERE created_at >= '2023-01-01';` |\n| Delete | `DELETE FROM my_table WHERE id = 101;` |\n| Describe | `DESCRIBE TABLE my_table;` |\n| Drop | `DROP TABLE my_table;` |\n| Error | `COPY INTO my_table FROM @my_stage/data/customers.csv FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1 FIELD_DELIMITER = ',');`<br/>`REVOKE ROLE analyst FROM USER user1;`<br/>`UNDROP TABLE my_table;` |\n| Insert | `INSERT INTO my_table (id, name, email) VALUES (102, 'Jane Doe', 'jane.doe@example.com');` |\n| Merge | `MERGE INTO my_table AS target USING (SELECT 103 AS id, 'John Smith' AS name, 'john.smith@example.com' AS email) AS source ON target.id = source.id WHEN MATCHED THEN UPDATE SET target.name = source.name, target.email = source.email WHEN NOT MATCHED THEN INSERT (id, name, email) VALUES (source.id, source.name, source.email);` |\n| Rollback | `ROLLBACK;` |\n| Select | `SELECT id, name FROM my_table WHERE id < 200 ORDER BY name;` |\n| Transaction | `BEGIN;` |\n| TruncateTable | `TRUNCATE TABLE my_table;` |\n| Update | `UPDATE my_table SET email = 'new.email@example.com' WHERE name = 'Jane Doe';` |\n| Use | `USE DATABASE my_database;` |\n\n# Semantic View Querying\n\nSeveral tools support the discovery and querying of [Snowflake Semantic Views](https://docs.snowflake.com/en/user-guide/views-semantic/overview) and their components.\nSemantic Views can be **listed** and **described**. In addition, you can **list their metrics and dimensions**.\nLastly, you can **[query Semantic Views](https://docs.snowflake.com/en/user-guide/views-semantic/querying)** directly.\n\n**To enable these tools, set `semantic_manager` to True in the configuration file under `other_services`.**\n\n# Troubleshooting\n\n## Running MCP Inspector\n\nThe [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) is a powerful debugging tool that provides a web interface to interact with your MCP server directly. It's essential for troubleshooting configuration issues, testing tools, and validating your setup.\n\n### Basic Inspector Usage\n\nLaunch the inspector with your MCP server configuration:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx snowflake-labs-mcp --service-config-file <path_to_file>/tools_config.yaml --connection-name \"default\"\n```\n\n### What the Inspector Shows You\n\nOnce launched, the inspector will open a web interface where you can:\n\n1. **View Available Tools**: See all MCP tools loaded from your configuration file\n2. **Test Tool Execution**: Call tools directly with custom parameters to verify they work\n3. **Inspect Resources**: View any resources exposed by the server\n4. **Debug Connection Issues**: See detailed error messages if connection fails\n5. **Validate Configuration**: Ensure your service configurations are properly loaded\n\n### Common Troubleshooting Scenarios\n\n**Configuration File Issues:**\n- If tools don't appear, check your `tools_config.yaml` syntax\n- Verify that service names and database/schema references are correct\n- Ensure `other_services` are set to `True` for the tool groups you want\n\n**Connection Problems:**\n- Verify your Snowflake connection parameters are correct\n- Check that your role has the necessary permissions for the services you've configured\n- For key pair authentication, ensure your private key is properly formatted\n\n**Tool Execution Errors:**\n- Use the inspector to test individual tools with known-good parameters\n- Check the server logs for detailed error messages\n- Verify that the underlying Snowflake objects (databases, schemas, services) exist\n\n### Alternative Debugging Methods\n\n**Using Cursor MCP Logs:**\n- Open Output panel in Cursor\n- Select \"Cursor MCP\" from the dropdown\n- View real-time logs as you interact with the MCP server\n\n**Command Line Debugging:**\nAdd verbose logging to see detailed connection and execution information:\n```bash\nuvx snowflake-labs-mcp --service-config-file <path_to_file>/tools_config.yaml --connection-name \"default\" --verbose\n```\n\n# FAQs\n\n#### How do I connect to Snowflake?\n\n- The MCP server supports all connection methods supported by the Snowflake Python Connector.\nSee [Connecting to Snowflake with the Python Connector](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect) for more information.\n\n#### I'm receiving a tool limit error/warning.\n\n- While LLMs' support for more tools will likely grow, you can hide tool groups by setting them to False in the configuration file.\nOnly listed Cortex services will be made into tools as well.\n\n#### Can I use a Programmatic Access Token (PAT) instead of a password?\n\n- Yes. Pass it to the CLI flag --password or set as environment variable SNOWFLAKE_PASSWORD.\n\n#### How do I try this?\n\n- The MCP server is intended to be used as one part of the MCP ecosystem. Think of it as a collection of tools. You'll need an MCP Client to act as an orchestrator. See the [MCP Introduction](https://modelcontextprotocol.io/introduction) for more information.\n\n#### Where is this deployed? Is this in Snowpark Container Services?\n\n- All tools in this MCP server are managed services, accessible via REST API. No separate remote service deployment is necessary. Instead, the current version of the server is intended to be started by the MCP client, such as Claude Desktop, Cursor, fast-agent, etc. By configuring these MCP client with the server, the application will spin up the server service for you. Future versions of the MCP server may be deployed as a remote service in the future.\n\n#### I'm receiving permission errors from my tool calls.\n\n- If using a Programmatic Access Tokens, note that they do not evaluate secondary roles. When creating them, please select a single role that has access to all services and their underlying objects OR select any role. A new PAT will need to be created to alter this property.\n\n#### How many Cortex Search or Cortex Analysts can I add?\n\n- You may add multiple instances of both services. The MCP Client will determine the appropriate one(s) to use based on the user's prompt.\n\n#### Help! I'm getting an SSLError?\n\n- If your account name contains underscores, try using the dashed version of the URL.\n  - Account identifier with underscores: `acme-marketing_test_account`\n  - Account identifier with dashes: `acme-marketing-test-account`\n\n#### How do I run the MCP server in a container for multiple users?\n\n- Deploy using Docker or Docker Compose as shown in the [Container Deployment](#container-deployment) section. The containerized server runs on HTTP and can handle multiple concurrent MCP client connections. Configure your environment variables for authentication and mount your configuration file as a read-only volume.\n\n#### Why aren't my Cortex services showing up as tools?\n\n- Verify that your configuration file syntax is correct (use MCP Inspector to validate)\n- Ensure the service names, database names, and schema names match exactly what exists in Snowflake\n- Check that your role has access to the specified databases and schemas\n- Confirm that the Cortex services actually exist in the specified locations\n\n#### Can I use different authentication methods for different environments?\n\n- Yes. You can set environment variables differently for each deployment, use different connection names in your connections.toml file, or pass different CLI arguments. The server supports all Snowflake Python Connector authentication methods including username/password, key pairs, OAuth, and SSO.\n\n#### How do I limit which SQL statements can be executed?\n\n- Use the `sql_statement_permissions` section in your configuration file. Set specific statement types to `True` (allow) or `False` (deny). For maximum security, only enable the statement types you actually need. Set `Unknown` to `False` to block unrecognized statement types.\n\n#### The MCP server is slow to start up. Is this normal?\n\n- Initial startup can take a few seconds as the server connects to Snowflake and validates your configuration. Subsequent tool calls should be much faster. If startup takes more than 30 seconds, check your network connection to Snowflake and verify your authentication credentials.\n\n# Bug Reports, Feedback, or Other Questions\n\nPlease add issues to the GitHub repository.\n\n<!-- mcp-name: io.github.Snowflake-Labs/mcp -->\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "snowflake labs"
      ],
      "category": "databases"
    },
    "StarRocks--mcp-server-starrocks": {
      "owner": "StarRocks",
      "name": "mcp-server-starrocks",
      "url": "https://github.com/StarRocks/mcp-server-starrocks",
      "imageUrl": "/freedevtools/mcp/pfp/StarRocks.webp",
      "description": "Connects AI assistants to StarRocks databases for executing SQL queries and exploring database schemas with ease. Provides capabilities for data visualization and detailed data overview retrieval without complex setups.",
      "stars": 120,
      "forks": 37,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-29T10:00:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/starrocks-mcp-server-starrocks)\n\n# StarRocks Official MCP Server\n\nThe StarRocks MCP Server acts as a bridge between AI assistants and StarRocks databases. It allows for direct SQL execution, database exploration, data visualization via charts, and retrieving detailed schema/data overviews without requiring complex client-side setup.\n\n<a href=\"https://glama.ai/mcp/servers/@StarRocks/mcp-server-starrocks\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@StarRocks/mcp-server-starrocks/badge\" alt=\"StarRocks Server MCP server\" />\n</a>\n\n## Features\n\n- **Direct SQL Execution:** Run `SELECT` queries (`read_query`) and DDL/DML commands (`write_query`).\n- **Database Exploration:** List databases and tables, retrieve table schemas (`starrocks://` resources).\n- **System Information:** Access internal StarRocks metrics and states via the `proc://` resource path.\n- **Detailed Overviews:** Get comprehensive summaries of tables (`table_overview`) or entire databases (`db_overview`), including column definitions, row counts, and sample data.\n- **Data Visualization:** Execute a query and generate a Plotly chart directly from the results (`query_and_plotly_chart`).\n- **Intelligent Caching:** Table and database overviews are cached in memory to speed up repeated requests. Cache can be bypassed when needed.\n- **Flexible Configuration:** Set connection details and behavior via environment variables.\n\n## Configuration\n\nThe MCP server is typically run via an MCP host. Configuration is passed to the host, specifying how to launch the StarRocks MCP server process.\n\n**Using Streamable HTTP (recommended):**\n\nTo start the server in Streamable HTTP mode:\n\nFirst test connect is ok:\n```\n$ STARROCKS_URL=root:@localhost:8000 uv run mcp-server-starrocks --test\n```\n\nStart the server:\n\n```\nuv run mcp-server-starrocks --mode streamable-http --port 8000\n```\n\nThen config the MCP like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-starrocks\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n\n**Using `uv` with installed package (individual environment variables):**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-starrocks\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--with\", \"mcp-server-starrocks\", \"mcp-server-starrocks\"],\n      \"env\": {\n        \"STARROCKS_HOST\": \"default localhost\",\n        \"STARROCKS_PORT\": \"default 9030\",\n        \"STARROCKS_USER\": \"default root\",\n        \"STARROCKS_PASSWORD\": \"default empty\",\n        \"STARROCKS_DB\": \"default empty\"\n      }\n    }\n  }\n}\n```\n\n**Using `uv` with installed package (connection URL):**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-starrocks\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--with\", \"mcp-server-starrocks\", \"mcp-server-starrocks\"],\n      \"env\": {\n        \"STARROCKS_URL\": \"root:password@localhost:9030/my_database\"\n      }\n    }\n  }\n}\n```\n\n**Using `uv` with local directory (for development):**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-starrocks\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/mcp-server-starrocks\", // <-- Update this path\n        \"run\",\n        \"mcp-server-starrocks\"\n      ],\n      \"env\": {\n        \"STARROCKS_HOST\": \"default localhost\",\n        \"STARROCKS_PORT\": \"default 9030\",\n        \"STARROCKS_USER\": \"default root\",\n        \"STARROCKS_PASSWORD\": \"default empty\",\n        \"STARROCKS_DB\": \"default empty\"\n      }\n    }\n  }\n}\n```\n\n**Using `uv` with local directory and connection URL:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-starrocks\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/mcp-server-starrocks\", // <-- Update this path\n        \"run\",\n        \"mcp-server-starrocks\"\n      ],\n      \"env\": {\n        \"STARROCKS_URL\": \"root:password@localhost:9030/my_database\"\n      }\n    }\n  }\n}\n```\n\n**Command-line Arguments:**\n\nThe server supports the following command-line arguments:\n\n```bash\nuv run mcp-server-starrocks --help\n```\n\n- `--mode {stdio,sse,http,streamable-http}`: Transport mode (default: stdio or MCP_TRANSPORT_MODE env var)\n- `--host HOST`: Server host for HTTP modes (default: localhost)\n- `--port PORT`: Server port for HTTP modes\n- `--test`: Run in test mode to verify functionality\n\nExamples:\n\n```bash\n# Start in streamable HTTP mode on custom host/port\nuv run mcp-server-starrocks --mode streamable-http --host 0.0.0.0 --port 8080\n\n# Start in stdio mode (default)\nuv run mcp-server-starrocks --mode stdio\n\n# Run test mode\nuv run mcp-server-starrocks --test\n```\n\n- The `url` field should point to the Streamable HTTP endpoint of your MCP server (adjust host/port as needed).\n- With this configuration, clients can interact with the server using standard JSON over HTTP POST requests. No special SDK is required.\n- All tool APIs accept and return standard JSON as described above.\n\n> **Note:**\n> The `sse` (Server-Sent Events) mode is deprecated and no longer maintained. Please use Streamable HTTP mode for all new integrations.\n\n**Environment Variables:**\n\n### Connection Configuration\n\nYou can configure StarRocks connection using either individual environment variables or a single connection URL:\n\n**Option 1: Individual Environment Variables**\n\n- `STARROCKS_HOST`: (Optional) Hostname or IP address of the StarRocks FE service. Defaults to `localhost`.\n- `STARROCKS_PORT`: (Optional) MySQL protocol port of the StarRocks FE service. Defaults to `9030`.\n- `STARROCKS_USER`: (Optional) StarRocks username. Defaults to `root`.\n- `STARROCKS_PASSWORD`: (Optional) StarRocks password. Defaults to empty string.\n- `STARROCKS_DB`: (Optional) Default database to use if not specified in tool arguments or resource URIs. If set, the connection will attempt to `USE` this database. Tools like `table_overview` and `db_overview` will use this if the database part is omitted in their arguments. Defaults to empty (no default database).\n\n**Option 2: Connection URL (takes precedence over individual variables)**\n\n- `STARROCKS_URL`: (Optional) A connection URL string that contains all connection parameters in a single variable. Format: `[<schema>://]user:password@host:port/database`. The schema part is optional. When this variable is set, it takes precedence over the individual `STARROCKS_HOST`, `STARROCKS_PORT`, `STARROCKS_USER`, `STARROCKS_PASSWORD`, and `STARROCKS_DB` variables.\n\n  Examples:\n  - `root:mypass@localhost:9030/test_db`\n  - `mysql://admin:secret@db.example.com:9030/production`  \n  - `starrocks://user:pass@192.168.1.100:9030/analytics`\n\n### Additional Configuration\n\n- `STARROCKS_OVERVIEW_LIMIT`: (Optional) An _approximate_ character limit for the _total_ text generated by overview tools (`table_overview`, `db_overview`) when fetching data to populate the cache. This helps prevent excessive memory usage for very large schemas or numerous tables. Defaults to `20000`.\n\n- `STARROCKS_MYSQL_AUTH_PLUGIN`: (Optional) Specifies the authentication plugin to use when connecting to the StarRocks FE service. For example, set to `mysql_clear_password` if your StarRocks deployment requires clear text password authentication (such as when using certain LDAP or external authentication setups). Only set this if your environment specifically requires it; otherwise, the default auth_plugin is used.\n\n- `MCP_TRANSPORT_MODE`: (Optional) Communication mode that specifies how the MCP Server exposes its services. Available options:\n  - `stdio` (default): Communicates through standard input/output, suitable for MCP Host hosting.\n  - `streamable-http` (Streamable HTTP): Starts as a Streamable HTTP Server, supporting RESTful API calls.\n  - `sse`: **(Deprecated, not recommended)** Starts in Server-Sent Events (SSE) streaming mode, suitable for scenarios requiring streaming responses. **Note: SSE mode is no longer maintained, it is recommended to use Streamable HTTP mode uniformly.**\n\n## Components\n\n### Tools\n\n- `read_query`\n\n  - **Description:** Execute a SELECT query or other commands that return a ResultSet (e.g., `SHOW`, `DESCRIBE`).\n  - **Input:** \n    ```json\n    {\n      \"query\": \"SQL query string\",\n      \"db\": \"database name (optional, uses default database if not specified)\"\n    }\n    ```\n  - **Output:** Text content containing the query results in a CSV-like format, including a header row and a row count summary. Returns an error message on failure.\n\n- `write_query`\n\n  - **Description:** Execute a DDL (`CREATE`, `ALTER`, `DROP`), DML (`INSERT`, `UPDATE`, `DELETE`), or other StarRocks command that does not return a ResultSet.\n  - **Input:** \n    ```json\n    {\n      \"query\": \"SQL command string\",\n      \"db\": \"database name (optional, uses default database if not specified)\"\n    }\n    ```\n  - **Output:** Text content confirming success (e.g., \"Query OK, X rows affected\") or reporting an error. Changes are committed automatically on success.\n\n- `analyze_query`\n\n  - **Description:** Analyze a query and get analyze result using query profile or explain analyze.\n  - **Input:**\n    ```json\n    {\n      \"uuid\": \"Query ID, a string composed of 32 hexadecimal digits formatted as 8-4-4-4-12\",\n      \"sql\": \"Query SQL to analyze\",\n      \"db\": \"database name (optional, uses default database if not specified)\"\n    }\n    ```\n  - **Output:** Text content containing the query analysis results. Uses `ANALYZE PROFILE FROM` if uuid is provided, otherwise uses `EXPLAIN ANALYZE` if sql is provided.\n\n- `query_and_plotly_chart`\n\n  - **Description:** Executes a SQL query, loads the results into a Pandas DataFrame, and generates a Plotly chart using a provided Python expression. Designed for visualization in supporting UIs.\n  - **Input:**\n    ```json\n    {\n      \"query\": \"SQL query to fetch data\",\n      \"plotly_expr\": \"Python expression string using 'px' (Plotly Express) and 'df' (DataFrame). Example: 'px.scatter(df, x=\\\"col1\\\", y=\\\"col2\\\")'\",\n      \"db\": \"database name (optional, uses default database if not specified)\"\n    }\n    ```\n  - **Output:** A list containing:\n    1.  `TextContent`: A text representation of the DataFrame and a note that the chart is for UI display.\n    2.  `ImageContent`: The generated Plotly chart encoded as a base64 PNG image (`image/png`). Returns text error message on failure or if the query yields no data.\n\n- `table_overview`\n\n  - **Description:** Get an overview of a specific table: columns (from `DESCRIBE`), total row count, and sample rows (`LIMIT 3`). Uses an in-memory cache unless `refresh` is true.\n  - **Input:**\n    ```json\n    {\n      \"table\": \"Table name, optionally prefixed with database name (e.g., 'db_name.table_name' or 'table_name'). If database is omitted, uses STARROCKS_DB environment variable if set.\",\n      \"refresh\": false // Optional, boolean. Set to true to bypass the cache. Defaults to false.\n    }\n    ```\n  - **Output:** Text content containing the formatted overview (columns, row count, sample data) or an error message. Cached results include previous errors if applicable.\n\n- `db_overview`\n  - **Description:** Get an overview (columns, row count, sample rows) for _all_ tables within a specified database. Uses the table-level cache for each table unless `refresh` is true.\n  - **Input:**\n    ```json\n    {\n      \"db\": \"database_name\", // Optional if default database is set.\n      \"refresh\": false // Optional, boolean. Set to true to bypass the cache for all tables in the DB. Defaults to false.\n    }\n    ```\n  - **Output:** Text content containing concatenated overviews for all tables found in the database, separated by headers. Returns an error message if the database cannot be accessed or contains no tables.\n\n### Resources\n\n#### Direct Resources\n\n- `starrocks:///databases`\n  - **Description:** Lists all databases accessible to the configured user.\n  - **Equivalent Query:** `SHOW DATABASES`\n  - **MIME Type:** `text/plain`\n\n#### Resource Templates\n\n- `starrocks:///{db}/{table}/schema`\n\n  - **Description:** Gets the schema definition of a specific table.\n  - **Equivalent Query:** `SHOW CREATE TABLE {db}.{table}`\n  - **MIME Type:** `text/plain`\n\n- `starrocks:///{db}/tables`\n\n  - **Description:** Lists all tables within a specific database.\n  - **Equivalent Query:** `SHOW TABLES FROM {db}`\n  - **MIME Type:** `text/plain`\n\n- `proc:///{+path}`\n  - **Description:** Accesses StarRocks internal system information, similar to Linux `/proc`. The `path` parameter specifies the desired information node.\n  - **Equivalent Query:** `SHOW PROC '/{path}'`\n  - **MIME Type:** `text/plain`\n  - **Common Paths:**\n    - `/frontends` - Information about FE nodes.\n    - `/backends` - Information about BE nodes (for non-cloud native deployments).\n    - `/compute_nodes` - Information about CN nodes (for cloud native deployments).\n    - `/dbs` - Information about databases.\n    - `/dbs/<DB_ID>` - Information about a specific database by ID.\n    - `/dbs/<DB_ID>/<TABLE_ID>` - Information about a specific table by ID.\n    - `/dbs/<DB_ID>/<TABLE_ID>/partitions` - Partition information for a table.\n    - `/transactions` - Transaction information grouped by database.\n    - `/transactions/<DB_ID>` - Transaction information for a specific database ID.\n    - `/transactions/<DB_ID>/running` - Running transactions for a database ID.\n    - `/transactions/<DB_ID>/finished` - Finished transactions for a database ID.\n    - `/jobs` - Information about asynchronous jobs (Schema Change, Rollup, etc.).\n    - `/statistic` - Statistics for each database.\n    - `/tasks` - Information about agent tasks.\n    - `/cluster_balance` - Load balance status information.\n    - `/routine_loads` - Information about Routine Load jobs.\n    - `/colocation_group` - Information about Colocation Join groups.\n    - `/catalog` - Information about configured catalogs (e.g., Hive, Iceberg).\n\n### Prompts\n\nNone defined by this server.\n\n## Caching Behavior\n\n- The `table_overview` and `db_overview` tools utilize an in-memory cache to store the generated overview text.\n- The cache key is a tuple of `(database_name, table_name)`.\n- When `table_overview` is called, it checks the cache first. If a result exists and the `refresh` parameter is `false` (default), the cached result is returned immediately. Otherwise, it fetches the data from StarRocks, stores it in the cache, and then returns it.\n- When `db_overview` is called, it lists all tables in the database and then attempts to retrieve the overview for _each table_ using the same caching logic as `table_overview` (checking cache first, fetching if needed and `refresh` is `false` or cache miss). If `refresh` is `true` for `db_overview`, it forces a refresh for _all_ tables in that database.\n- The `STARROCKS_OVERVIEW_LIMIT` environment variable provides a _soft target_ for the maximum length of the overview string generated _per table_ when populating the cache, helping to manage memory usage.\n- Cached results, including any error messages encountered during the original fetch, are stored and returned on subsequent cache hits.\n\n## Debug\n\nAfter starting mcp server, you can use inspector to debug:\n```\nnpx @modelcontextprotocol/inspector\n```\n\n## Demo",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "starrocks",
        "database",
        "starrocks databases",
        "access starrocks",
        "server starrocks"
      ],
      "category": "databases"
    },
    "StevenFengLi--mcp-oceanbase": {
      "owner": "StevenFengLi",
      "name": "mcp-oceanbase",
      "url": "https://github.com/StevenFengLi/mcp-oceanbase",
      "imageUrl": "/freedevtools/mcp/pfp/StevenFengLi.webp",
      "description": "Facilitate secure interaction with OceanBase databases via a Model Context Protocol server, enabling listing of tables, reading data, and executing SQL queries in a structured manner.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-21T07:26:58Z",
      "readme_content": "# mcp-oceanbase\n\nMCP Server for OceanBase database and its tools\n\nEnglish | [简体中文](README_CN.md)\n\n## Features\n\nThis repository contains MCP Servers as following:\n\n| MCP Server           | Description                                                                                     | Document                           |\n|----------------------|-------------------------------------------------------------------------------------------------|------------------------------------|\n| OceanBase MCP Server | A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases. | [Doc](doc/oceanbase_mcp_server.md) |\n\n## Community\n\nDon’t hesitate to ask!\n\nContact the developers and community at [https://ask.oceanbase.com](https://ask.oceanbase.com) if you need any help.\n\n[Open an issue](https://github.com/oceanbase/mcp-oceanbase/issues) if you found a bug.\n\n## Licensing\n\nSee [LICENSE](LICENSE) for more information.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oceanbase",
        "databases",
        "database",
        "oceanbase databases",
        "oceanbase facilitate",
        "mcp oceanbase"
      ],
      "category": "databases"
    },
    "Swayingleaves--cockroachdb-mcp-server": {
      "owner": "Swayingleaves",
      "name": "cockroachdb-mcp-server",
      "url": "https://github.com/Swayingleaves/cockroachdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Swayingleaves.webp",
      "description": "Connects directly to a CockroachDB database to execute SQL queries and retrieve table structures, while managing connections for stability and providing detailed logging for troubleshooting.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-22T14:51:43Z",
      "readme_content": "# CockroachDB MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Swayingleaves/cockroachdb-mcp-server)](https://smithery.ai/server/@Swayingleaves/cockroachdb-mcp-server)\n[English](README.md) | [简体中文](README_zh.md)\n\nThis is a CockroachDB MCP server for Cursor, implemented based on the Model Context Protocol (MCP) specification, allowing you to interact directly with CockroachDB database in Cursor.\n\n## Features\n\n- Connect to CockroachDB database\n- Get all tables from the database\n- Get table structure information\n- Execute SQL queries\n- Provide database status resources\n- Provide SQL query templates\n- Automatic reconnection mechanism to ensure connection stability\n- Connection keep-alive mechanism to prevent connection timeout\n- Graceful process exit handling\n- Detailed logging for troubleshooting\n- Support manual disconnection\n\n## Installation\n\n### Installing via Smithery\n\nTo install CockroachDB MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Swayingleaves/cockroachdb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @Swayingleaves/cockroachdb-mcp-server --client claude\n```\n\n1. Clone the repository and enter the project directory\n2. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n3. Install uv\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n## Using in Cursor\n\n```json\n{\n    \"mcpServers\": {\n        \"cockroachdb-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/Users/local/cockroachdb-mcp\",\n                \"run\",\n                \"server.py\"\n            ],\n            \"jdbc_url\": \"jdbc:postgresql://localhost:26257/defaultdb\",\n            \"username\": \"root\",\n            \"password\": \"root\"\n        }\n    }\n  }\n```\n\n## MCP Function Description\n\n### Tools\n\n#### connect_database\n\nConnect to CockroachDB database.\n\nParameters:\n- `jdbc_url`: JDBC connection URL (e.g., jdbc:postgresql://localhost:26257/defaultdb)\n- `username`: Database username\n- `password`: Database password\n\n#### initialize_connection\n\nInitialize database connection, can be called immediately after connecting to the MCP server to establish a database connection.\n\nParameters:\n- `jdbc_url`: JDBC connection URL (e.g., jdbc:postgresql://localhost:26257/defaultdb)\n- `username`: Database username\n- `password`: Database password\n\n#### disconnect_database\n\nManually disconnect from the database.\n\nNo parameters.\n\n#### get_tables\n\nGet all tables from the database.\n\nNo parameters.\n\n#### get_table_schema\n\nGet structure information of a specified table.\n\nParameters:\n- `table_name`: Table name\n\n#### execute_query\n\nExecute SQL query.\n\nParameters:\n- `query`: SQL query statement\n\n### Resources\n\n#### db://status\n\nGet database connection status.\n\nReturns:\n- When not connected: `\"Not connected\"`\n- When connected: `\"Connected - [Database version]\"`\n- When connection error: `\"Connection error - [Error message]\"`\n\n### Prompts\n\n#### sql_query_template\n\nSQL query template to help users write SQL queries.\n\n## Logs\n\nServer logs are saved in `logs/cockroachdb_mcp.log` file. You can check this file to understand the server's running status and detailed logs.\n\nThe log file uses a rotating log mechanism, with each log file maximum size of 10MB and keeping up to 5 backup files to prevent excessive disk space usage.\n\n## Special Character Handling\n\nThis server uses psycopg2 to connect directly to CockroachDB database, which automatically handles special characters in usernames and passwords without additional URL encoding. This ensures correct database connection even when passwords contain special characters (such as `@`, `%`, `&`, etc.).\n\n## TCP Keep-alive Settings\n\nThe server is configured with TCP keep-alive mechanism by default to prevent connections from being closed due to long periods of inactivity:\n\n- `keepalives=1`: Enable TCP keepalive\n- `keepalives_idle=30`: Send keepalive after 30 seconds of idle time\n- `keepalives_interval=10`: Send keepalive every 10 seconds\n- `keepalives_count=5`: Give up after 5 attempts\n\n## Troubleshooting\n\nIf you encounter problems, please check the log file `logs/cockroachdb_mcp.log`, which will help you understand the server's running status and potential issues.\n\n### Common Issues\n\n1. **Connection Refused**: Ensure the CockroachDB server is running and accessible from your machine.\n2. **Authentication Failed**: Check if the username and password are correct.\n3. **Connection Timeout**: Check if the network connection is stable, especially when connecting to a remote database.\n4. **Database Server Issues**: Check if the CockroachDB server is running properly. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cockroachdb",
        "databases",
        "database",
        "cockroachdb database",
        "directly cockroachdb",
        "cockroachdb mcp"
      ],
      "category": "databases"
    },
    "T1nker-1220--aws-postgress-mcp-server": {
      "owner": "T1nker-1220",
      "name": "aws-postgress-mcp-server",
      "url": "https://github.com/T1nker-1220/aws-postgress-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/T1nker-1220.webp",
      "description": "Provides secure, read-only SQL query access to an AWS PostgreSQL database through an MCP interface. Facilitates retrieving data safely while preventing modifications to the database.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-24T02:00:02Z",
      "readme_content": "# AWS PostgreSQL MCP Server\n\nA Model Context Protocol (MCP) server providing read-only SQL query access to an AWS PostgreSQL database via the `query` tool. Configuration uses environment variables.\n\n## Setup\n\n1.  **Clone:**\n    ```bash\n    git clone https://github.com/T1nker-1220/aws-postgress-mcp-server.git\n    cd aws-postgress-mcp-server\n    ```\n2.  **Install & Build:**\n    ```bash\n    pnpm install\n    pnpm run build\n    ```\n\n## Configuration (for Cline/Windsurf)\n\nAdd this server to your MCP client's settings file (e.g., `c:\\Users\\<User>\\AppData\\Roaming\\Windsurf\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    // ... other servers ...\n\n    \"aws-postgres-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        // Full path to the built index.js\n        \"C:\\\\Users\\\\NATH\\\\Documents\\\\Cline\\\\MCP\\\\aws-postgress-mcp-server\\\\build\\\\index.js\" \n      ],\n      // Database credentials go in the 'env' object\n      \"env\": {\n        \"DB_HOST\": \"YOUR_HOST.rds.amazonaws.com\",\n        \"DB_PORT\": \"5432\",\n        \"DB_NAME\": \"YOUR_DB_NAME\",\n        \"DB_USER\": \"YOUR_DB_USER\",\n        \"DB_PASSWORD\": \"YOUR_PASSWORD\"\n      },\n      \"transportType\": \"stdio\",\n      \"disabled\": false,\n      \"autoApprove\": [] \n    }\n    // ... other servers ...\n  }\n}\n```\n\n**-> Replace the placeholder values in the `env` object with your actual credentials.**\n\n## Usage\n\nOnce configured, the client will start the server. Use the `query` tool:\n\n```xml\n<use_mcp_tool>\n  <server_name>aws-postgres-mcp-server</server_name>\n  <tool_name>query</tool_name>\n  <arguments>\n  {\n    \"sql\": \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\"\n  }\n  </arguments>\n</use_mcp_tool>\n```\n\n## Notes\n\n*   The server only allows read-only queries (SELECT, SHOW, etc.).\n*   To configure clients using `npx @t1nker-1220/aws-postgres-mcp-server ...`, the package must first be published to npm. The configuration would still use the `env` object for credentials.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "secure database",
        "databases secure",
        "aws postgresql"
      ],
      "category": "databases"
    },
    "Teradata--teradata-mcp-server": {
      "owner": "Teradata",
      "name": "teradata-mcp-server",
      "url": "https://github.com/Teradata/teradata-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Teradata.webp",
      "description": "Enables integration with Teradata databases, facilitating execution of SQL queries, database management, and interactive exploration of database objects. Supports standard input/output and server-sent events for communication with AI agents and tools.",
      "stars": 22,
      "forks": 38,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T22:58:55Z",
      "readme_content": "<p align=\"center\">\n  <!-- Optional: replace with a logo if you have one -->\n  <!-- <img src=\"docs/media/logo.svg\" alt=\"Teradata MCP Server\" width=\"120\"> -->\n  \n</p>\n\n<h1 align=\"center\">Teradata MCP Server</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Teradata/teradata-mcp-server/blob/main/docs/README.md\">\n    <img alt=\"docs\" src=\"https://img.shields.io/badge/docs-readme-555?logo=readthedocs\">\n  </a>\n  <a href=\"https://github.com/Teradata/teradata-mcp-server/releases\">\n    <img alt=\"release\" src=\"https://img.shields.io/github/v/release/Teradata/teradata-mcp-server?display_name=tag&sort=semver\">\n  </a>\n  <a href=\"https://pypi.org/project/teradata-mcp-server/\">\n    <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/teradata-mcp-server\">\n  </a>\n  <a href=\"https://pypi.org/project/teradata-mcp-server/\">\n    <img alt=\"downloads\" src=\"https://img.shields.io/pypi/dm/teradata-mcp-server?label=downloads&color=2ea44f\">\n  </a>\n</p>\n\n<p align=\"center\">\n  Model Context Protocol (MCP) server for Teradata\n </p>\n\n<p align=\"center\">\n  ✨ <a href=\"https://github.com/Teradata/teradata-mcp-server?tab=readme-ov-file#quick-start-with-claude-desktop-no-installation\">Quickstart with Claude Desktop </a> or <a href=\"https://github.com/Teradata/teradata-mcp-server/blob/main/docs/README.md#-quick-start\"> your favorite tool</a> in <5 minute ✨\n</p>\n\n## Overview\nThe Teradata MCP server provides sets of tools and prompts, grouped as modules for interacting with Teradata databases. Enabling AI agents and users to query, analyze, and manage their data efficiently. \n\n![Getting Started](https://raw.githubusercontent.com/Teradata/teradata-mcp-server/main/docs/media/client-server-platform.png)\n\n## Key features\n\n### Available tools and prompts\n\nWe are providing groupings of tools and associated helpful prompts to support all type of agentic applications on the data platform.\n\n![Teradata MCP Server diagram](https://raw.githubusercontent.com/Teradata/teradata-mcp-server/main/docs/media/teradata-mcp-server.png)\n\n- **Search** tools, prompts and resources to search and manage vector stores.\n  - [RAG Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/rag/README.md) rapidly build RAG applications.\n- **Query** tools, prompts and resources to query and navigate your Teradata platform:\n  - [Base Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/base/README.md)\n- **Table** tools, to efficiently and predictably access structured data models:\n  - [Feature Store Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/fs/README.md) to access and manage the Teradata Enterprise Feature Store.\n  - [Semantic layer definitions](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/server_guide/CUSTOMIZING.md) to easily implement domain-specific tools, prompts and resources for your own business data models. \n- **Data Quality** tools, prompts and resources accelerate exploratory data analysis:\n  - [Data Quality Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/qlty/README.md)\n- **DBA** tools, prompts and resources to facilitate your platform administration tasks:\n  - [DBA Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/dba/README.md)\n  - [Security Tools](https://github.com/Teradata/teradata-mcp-server/blob/main/src/teradata_mcp_server/tools/sec/README.md)\n\n## Quick start with Claude Desktop (no installation)\n> Prefer to use other tools? Check out our Quick Starts for [VS Code/Copilot](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/server_guide/QUICK_START_VSCODE.md), [Open WebUI](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/server_guide/QUICK_START_OPEN_WEBUI.md), or dive into [simple code examples](https://github.com/Teradata/teradata-mcp-server/blob/main/examples/README.md#client-applications)!\nYou can use Claude Desktop to give the  Teradata MCP server a quick try, Claude can manage the server in the background using `uv`. No permanent installation needed.\n\n**Pre-requisites**\n1. Get your Teradata database credentials or create a free sandbox at [Teradata Clearscape Experience](https://www.teradata.com/getting-started/demos/clearscape-analytics).\n2. Install [Claude Desktop](https://claude.ai/download).\n3. Install [uv](https://docs.astral.sh/uv/getting-started/installation/). If you are on MacOS, Use Homebrew: `brew install uv`, on Windows you may use `pip install uv` as an alternative to the installer.\n\nConfigure the claude_desktop_config.json (Settings>Developer>Edit Config) by adding the configuration below, updating the database username, password and URL:\n\n```json\n{\n  \"mcpServers\": {\n    \"teradata\": {\n      \"command\": \"uvx\",\n      \"args\": [\"teradata-mcp-server\"],\n      \"env\": {\n        \"DATABASE_URI\": \"teradata://<USERNAME>:<PASSWORD>@<HOST_URL>:1025/<USERNAME>\"\n      }\n    }\n  }\n}\n```\n\n## Installation Instructions\n\nFollow this process to install your server, connect it to your Teradata platform and integrated your tools.\n\n**Step 1.** - Identify the running Teradata System, you need username, password and host details. If you do not have a Teradata system to connect to, then leverage [Teradata Clearscape Experience](https://www.teradata.com/getting-started/demos/clearscape-analytics)\n\n**Step 2.** - To install, configure and run the MCP server, refer to the [Teradata MCP Server Documentation](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/README.md).\n\n**Step 3.** - There are many client options available, the [Client Guide](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/README.md#-client-guide) explains how to configure and run a sample of different clients.\n\n<br>\n\nCheck out our libraries of [curated examples](https://github.com/Teradata/teradata-mcp-server/blob/main/examples/) or [video guides](https://github.com/Teradata/teradata-mcp-server/blob/doc-v1.4/docs/server_guide/VIDEO_LIBRARY.md).\n\n<br>\n\n\n\n## Contributing\nPlease refer to the [Contributing](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/developer_guide/CONTRIBUTING.md) guide and the [Developer Guide](https://github.com/Teradata/teradata-mcp-server/blob/main/docs/developer_guide/DEVELOPER_GUIDE.md).\n\n\n---------------------------------------------------------------------\n## Certification\n<a href=\"https://glama.ai/mcp/servers/@Teradata/teradata-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Teradata/teradata-mcp-server/badge\" alt=\"Teradata Server MCP server\" />\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "teradata",
        "databases",
        "database",
        "teradata databases",
        "access teradata",
        "teradata mcp"
      ],
      "category": "databases"
    },
    "TheRaLabs--legion-mcp": {
      "owner": "TheRaLabs",
      "name": "legion-mcp",
      "url": "https://github.com/TheRaLabs/legion-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/TheRaLabs.webp",
      "description": "Access and query multiple databases using the Legion Query Runner, allowing seamless execution of SQL queries, schema retrieval, and query optimization via a unified API.",
      "stars": 71,
      "forks": 18,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-09-30T15:17:18Z",
      "readme_content": "# Multi-Database MCP Server (by Legion AI)\n\nA server that helps people access and query data in databases using the Legion Query Runner with integration of the Model Context Protocol (MCP) Python SDK.\n\n# Start Generation Here\nThis tool is provided by [Legion AI](https://thelegionai.com/). To use the full-fledged and fully powered AI data analytics tool, please visit the site. Email us if there is one database you want us to support.\n# End Generation Here\n\n## Why Choose Database MCP\n\nDatabase MCP stands out from other database access solutions for several compelling reasons:\n\n- **Unified Multi-Database Interface**: Connect to PostgreSQL, MySQL, SQL Server, and other databases through a single consistent API - no need to learn different client libraries for each database type.\n- **AI-Ready Integration**: Built specifically for AI assistant interactions through the Model Context Protocol (MCP), enabling natural language database operations.\n- **Zero-Configuration Schema Discovery**: Automatically discovers and exposes database schemas without manual configuration or mapping.\n- **Database-Agnostic Tools**: Find tables, explore schemas, and execute queries with the same set of tools regardless of the underlying database technology.\n- **Secure Credential Management**: Handles database authentication details securely, separating credentials from application code.\n- **Simple Deployment**: Works with modern AI development environments like LangChain, FastAPI, and others with minimal setup.\n- **Extensible Design**: Easily add custom tools and prompts to enhance functionality for specific use cases.\n\nWhether you're building AI agents that need database access or simply want a unified interface to multiple databases, Database MCP provides a streamlined solution that dramatically reduces development time and complexity.\n\n## Features\n\n- Multi-database support - connect to multiple databases simultaneously\n- Database access via Legion Query Runner\n- Model Context Protocol (MCP) support for AI assistants\n- Expose database operations as MCP resources, tools, and prompts\n- Multiple deployment options (standalone MCP server, FastAPI integration)\n- Query execution and result handling\n- Flexible configuration via environment variables, command-line arguments, or MCP settings JSON\n- User-driven database selection for multi-database setups\n\n## Supported Databases\n\n| Database | DB_TYPE code |\n|----------|--------------|\n| PostgreSQL | pg |\n| Redshift | redshift |\n| CockroachDB | cockroach |\n| MySQL | mysql |\n| RDS MySQL | rds_mysql |\n| Microsoft SQL Server | mssql |\n| Big Query | bigquery |\n| Oracle DB | oracle |\n| SQLite | sqlite |\n\nWe use Legion Query Runner library as connectors. You can find more info on their [api doc](https://theralabs.github.io/legion-database/docs/category/query-runners).\n\n## What is MCP?\n\nThe Model Context Protocol (MCP) is a specification for maintaining context in AI applications. This server uses the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) to:\n\n- Expose database operations as tools for AI assistants\n- Provide database schemas and metadata as resources\n- Generate useful prompts for database operations\n- Enable stateful interactions with databases\n\n## Installation & Configuration\n\n### Required Parameters\n\nFor single database configuration:\n- **DB_TYPE**: The database type code (see table above)\n- **DB_CONFIG**: A JSON configuration string for database connection\n\nFor multi-database configuration:\n- **DB_CONFIGS**: A JSON array of database configurations, each containing:\n  - **db_type**: The database type code\n  - **configuration**: Database connection configuration\n  - **description**: A human-readable description of the database\n\nThe configuration format varies by database type. See the [API documentation](https://theralabs.github.io/legion-database/docs/category/query-runners) for database-specific configuration details.\n\n### Installation Methods\n\n#### Option 1: Using UV (Recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/), no specific installation is needed. We will use [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *database-mcp*.\n\n**UV Configuration Example (Single Database):**\n\n```json\nREPLACE DB_TYPE and DB_CONFIG with your connection info.\n{\n    \"mcpServers\": {\n      \"database-mcp\": {\n        \"command\": \"uvx\",\n        \"args\": [\n          \"database-mcp\"\n        ],\n        \"env\": {\n          \"DB_TYPE\": \"pg\",\n          \"DB_CONFIG\": \"{\\\"host\\\":\\\"localhost\\\",\\\"port\\\":5432,\\\"user\\\":\\\"user\\\",\\\"password\\\":\\\"pw\\\",\\\"dbname\\\":\\\"dbname\\\"}\"\n        },\n        \"disabled\": true,\n        \"autoApprove\": []\n      }\n    }\n}\n```\n\n**UV Configuration Example (Multiple Databases):**\n\n```json\n{\n    \"mcpServers\": {\n      \"database-mcp\": {\n        \"command\": \"uvx\",\n        \"args\": [\n          \"database-mcp\"\n        ],\n        \"env\": {\n          \"DB_CONFIGS\": \"[{\\\"id\\\":\\\"pg_main\\\",\\\"db_type\\\":\\\"pg\\\",\\\"configuration\\\":{\\\"host\\\":\\\"localhost\\\",\\\"port\\\":5432,\\\"user\\\":\\\"user\\\",\\\"password\\\":\\\"pw\\\",\\\"dbname\\\":\\\"postgres\\\"},\\\"description\\\":\\\"PostgreSQL Database\\\"},{\\\"id\\\":\\\"mysql_data\\\",\\\"db_type\\\":\\\"mysql\\\",\\\"configuration\\\":{\\\"host\\\":\\\"localhost\\\",\\\"port\\\":3306,\\\"user\\\":\\\"root\\\",\\\"password\\\":\\\"pass\\\",\\\"database\\\":\\\"mysql\\\"},\\\"description\\\":\\\"MySQL Database\\\"}]\"\n        },\n        \"disabled\": true,\n        \"autoApprove\": []\n      }\n    }\n}\n```\n\n#### Option 2: Using PIP\n\nInstall via pip:\n\n```bash\npip install database-mcp\n```\n\n**PIP Configuration Example (Single Database):**\n\n```json\n{\n  \"mcpServers\": {\n    \"database\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\", \"database_mcp\", \n        \"--repository\", \"path/to/git/repo\"\n      ],\n      \"env\": {\n        \"DB_TYPE\": \"pg\",\n        \"DB_CONFIG\": \"{\\\"host\\\":\\\"localhost\\\",\\\"port\\\":5432,\\\"user\\\":\\\"user\\\",\\\"password\\\":\\\"pw\\\",\\\"dbname\\\":\\\"dbname\\\"}\"\n      }\n    }\n  }\n}\n```\n\n## Running the Server\n\n### Production Mode\n\n```bash\npython mcp_server.py\n```\n\n### Configuration Methods\n\n#### Environment Variables (Single Database)\n\n```bash\nexport DB_TYPE=\"pg\"  # or mysql, postgresql, etc.\nexport DB_CONFIG='{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"}'\nuv run src/database_mcp/mcp_server.py\n```\n\n#### Environment Variables (Multiple Databases)\n\n```bash\nexport DB_CONFIGS='[{\"id\":\"pg_main\",\"db_type\":\"pg\",\"configuration\":{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"},\"description\":\"PostgreSQL Database\"},{\"id\":\"mysql_users\",\"db_type\":\"mysql\",\"configuration\":{\"host\":\"localhost\",\"port\":3306,\"user\":\"root\",\"password\":\"pass\",\"database\":\"mysql\"},\"description\":\"MySQL Database\"}]'\nuv run src/database_mcp/mcp_server.py\n```\n\nIf you don't specify an ID, the system will generate one automatically based on the database type and description:\n\n```bash\nexport DB_CONFIGS='[{\"db_type\":\"pg\",\"configuration\":{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"},\"description\":\"PostgreSQL Database\"},{\"db_type\":\"mysql\",\"configuration\":{\"host\":\"localhost\",\"port\":3306,\"user\":\"root\",\"password\":\"pass\",\"database\":\"mysql\"},\"description\":\"MySQL Database\"}]'\n# IDs will be generated as something like \"pg_postgres_0\" and \"my_mysqldb_1\"\nuv run src/database_mcp/mcp_server.py\n```\n\n#### Command Line Arguments (Single Database)\n\n```bash\npython mcp_server.py --db-type pg --db-config '{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"}'\n```\n\n#### Command Line Arguments (Multiple Databases)\n\n```bash\npython mcp_server.py --db-configs '[{\"id\":\"pg_main\",\"db_type\":\"pg\",\"configuration\":{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"},\"description\":\"PostgreSQL Database\"},{\"id\":\"mysql_users\",\"db_type\":\"mysql\",\"configuration\":{\"host\":\"localhost\",\"port\":3306,\"user\":\"root\",\"password\":\"pass\",\"database\":\"mysql\"},\"description\":\"MySQL Database\"}]'\n```\n\nNote that you can specify custom IDs for each database using the `id` field, or let the system generate them based on database type and description.\n\n## Multi-Database Support\n\nWhen connecting to multiple databases, you need to specify which database to use for each query:\n\n1. Use the `list_databases` tool to see available databases with their IDs\n2. Use `get_database_info` to view schema details of databases\n3. Use `find_table` to locate a table across all databases\n4. Provide the `db_id` parameter to tools like `execute_query`, `get_table_columns`, etc.\n\nDatabase connections are managed internally as a dictionary of `DbConfig` objects, with each database having a unique ID. Schema information is represented as a list of table objects, where each table contains its name and column information.\n\nThe `select_database` prompt guides users through the database selection process.\n\n## Schema Representation\n\nDatabase schemas are represented as a list of table objects, with each table containing information about its columns:\n\n```json\n[\n  {\n    \"name\": \"users\",\n    \"columns\": [\n      {\"name\": \"id\", \"type\": \"integer\"},\n      {\"name\": \"username\", \"type\": \"varchar\"},\n      {\"name\": \"email\", \"type\": \"varchar\"}\n    ]\n  },\n  {\n    \"name\": \"orders\",\n    \"columns\": [\n      {\"name\": \"id\", \"type\": \"integer\"},\n      {\"name\": \"user_id\", \"type\": \"integer\"},\n      {\"name\": \"product_id\", \"type\": \"integer\"},\n      {\"name\": \"quantity\", \"type\": \"integer\"}\n    ]\n  }\n]\n```\n\nThis representation makes it easy to programmatically access table and column information while keeping a clean hierarchical structure.\n\n## Exposed MCP Capabilities\n\n### Resources\n\n| Resource | Description |\n|----------|-------------|\n| `resource://schema/{database_id}` | Get the schemas for one or all configured databases |\n\n### Tools\n\n| Tool | Description |\n|------|-------------|\n| `execute_query` | Execute a SQL query and return results as a markdown table |\n| `execute_query_json` | Execute a SQL query and return results as JSON |\n| `get_table_columns` | Get column names for a specific table |\n| `get_table_types` | Get column types for a specific table |\n| `get_query_history` | Get the recent query history |\n| `list_databases` | List all available database connections |\n| `get_database_info` | Get detailed information about a database including schema |\n| `find_table` | Find which database contains a specific table |\n| `describe_table` | Get detailed description of a table including column names and types |\n| `get_table_sample` | Get a sample of data from a table |\n\nAll database-specific tools (like `execute_query`, `get_table_columns`, etc.) require a `db_id` parameter to specify which database to use.\n\n### Prompts\n\n| Prompt | Description |\n|--------|-------------|\n| `sql_query` | Create an SQL query against the database |\n| `explain_query` | Explain what a SQL query does |\n| `optimize_query` | Optimize a SQL query for better performance |\n| `select_database` | Help user select which database to use |\n\n## Development\n\n### Using MCP Inspector\n\nrun this to start the inspector\n```bash\nnpx @modelcontextprotocol/inspector uv run src/database_mcp/mcp_server.py\n```\n\nthen in the command input field, set something like\n```\nrun src/database_mcp/mcp_server.py --db-type pg --db-config '{\"host\":\"localhost\",\"port\":5432,\"user\":\"username\",\"password\":\"password\",\"dbname\":\"database_name\"}'\n```\n\n### Testing\n\n```bash\nuv pip install -e \".[dev]\"\npytest\n```\n\n### Publishing\n\n```bash\n# Clean up build artifacts\nrm -rf dist/ build/ \n# Remove any .egg-info directories if they exist\nfind . -name \"*.egg-info\" -type d -exec rm -rf {} + 2>/dev/null || true\n# Build the package\nuv run python -m build\n# Upload to PyPI\nuv run python -m twine upload dist/*\n```\n\n## License\n\nThis repository is licensed under GPL",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "legion",
        "database",
        "legion query",
        "database access",
        "using legion"
      ],
      "category": "databases"
    },
    "ThomAub--clickhouse_mcp_server": {
      "owner": "ThomAub",
      "name": "clickhouse_mcp_server",
      "url": "https://github.com/ThomAub/clickhouse_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/ThomAub.webp",
      "description": "Integrate ClickHouse databases with Large Language Models and AI applications, enabling users to list databases, retrieve table schemas, and execute SQL queries efficiently.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2024-12-18T19:48:51Z",
      "readme_content": "# ClickHouse MCP Server\n\nThis project implements a Model Context Protocol (MCP) server for ClickHouse, allowing seamless integration of ClickHouse databases with Large Language Models (LLMs) and other AI applications.\n\n## Features\n\n- List ClickHouse databases and tables as resources\n- Retrieve table schemas\n- Execute SELECT queries on ClickHouse databases\n- Secure and efficient communication using the MCP protocol\n\n## Requirements\n\n- Python 3.10+\n- ClickHouse server\n\n## Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/ThomAub/clickhouse_mcp_server.git\n   cd clickhouse_mcp_server\n   ```\n\n2. Install the required packages:\n   ```\n   uv sync --all-extras\n   ```\n\n3. Set up your ClickHouse connection details in environment variables or update the `get_clickhouse_client` function in `server.py`.\n\n## Usage\n\nRun the server:\n\n```\npython clickhouse_mcp_server/server.py\n```\n\nThe server will start and listen for MCP requests.\n\n## Testing\n\nRun the tests using pytest:\n\n```\npytest tests/\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schemas",
        "clickhouse databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "TristanLib--mcp_server_mysql_windows": {
      "owner": "TristanLib",
      "name": "mcp_server_mysql_windows",
      "url": "https://github.com/TristanLib/mcp_server_mysql_windows",
      "imageUrl": "/freedevtools/mcp/pfp/TristanLib.webp",
      "description": "Connect to and manipulate a local MySQL database through a RESTful API, supporting parameterized queries to enhance security. Includes real-time data push capabilities via Server-Sent Events.",
      "stars": 4,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-22T00:35:26Z",
      "readme_content": "# MCP MySQL 本地数据库服务\n\nMCP MySQL服务是一个轻量级的个人使用服务程序，用于连接和操作本地MySQL数据库。此服务可作为Cursor的MCP服务使用，通过API接口使Cursor能够轻松地执行各种数据库操作。\n\n## 特性\n\n- 连接本地MySQL数据库\n- 提供RESTful API进行数据库操作\n- 支持参数化查询防止SQL注入\n- 支持SSE (Server-Sent Events) 推送能力\n- 支持作为Cursor MCP服务集成\n\n## 快速开始\n\n### 前置条件\n\n- Node.js (v14+)\n- MySQL服务器\n\n### 安装\n\n1. 克隆此仓库\n2. 安装依赖\n   ```\n   npm install\n   ```\n3. 创建并配置`.env`文件\n   ```\n   # 服务器配置\n   PORT=3000\n   NODE_ENV=development\n\n   # MySQL数据库配置\n   DB_HOST=localhost\n   DB_PORT=3306\n   DB_USER=你的用户名\n   DB_PASSWORD=你的密码\n   DB_NAME=你的数据库名\n\n   # API配置\n   API_KEY=你的API密钥\n   ```\n\n### 运行\n\n```\nnpm start\n```\n\n开发模式（自动重启）:\n```\nnpm run dev\n```\n\n## API接口\n\n### 获取所有数据库\n```\nGET /api/databases\n```\n\n### 获取数据库的所有表\n```\nGET /api/databases/:database/tables\n```\n\n### 获取表结构\n```\nGET /api/databases/:database/tables/:table/structure\n```\n\n### 执行查询\n```\nPOST /api/query\nContent-Type: application/json\n\n{\n  \"sql\": \"SELECT * FROM users WHERE age > ?\",\n  \"params\": [18],\n  \"limit\": 10,\n  \"offset\": 0\n}\n```\n\n### SSE连接\n```\nGET /api/sse?apiKey=your-api-key\n```\n\n## 在Cursor中使用\n\n### SSE方式\n```json\n{\n  \"name\": \"MySQL数据库服务\",\n  \"url\": \"http://localhost:3000/api/sse\",\n  \"type\": \"sse\"\n}\n```\n\n### Command方式\n```json\n{\n  \"name\": \"MySQL数据库服务\",\n  \"command\": \"node /path/to/mcp_server_mysql/src/app.js\",\n  \"type\": \"command\"\n}\n```\n\n## 安全性考虑\n\n- 此服务仅限本地使用，不建议暴露到公网\n- 使用API密钥保护接口\n- 默认只允许执行SELECT查询\n\n## 许可证\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "VictoriaMetrics-Community--mcp-victorialogs": {
      "owner": "VictoriaMetrics-Community",
      "name": "mcp-victorialogs",
      "url": "https://github.com/VictoriaMetrics-Community/mcp-victorialogs",
      "imageUrl": "",
      "description": "Provides comprehensive integration with your [VictoriaLogs instance APIs](https://docs.victoriametrics.com/victorialogs/querying/#http-api) and [documentation](https://docs.victoriametrics.com/victorialogs/) for working with logs, investigating and debugging tasks related to your VictoriaLogs instances.",
      "stars": 22,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-01T00:44:50Z",
      "readme_content": "# VictoriaLogs MCP Server\n\n[![Latest Release](https://img.shields.io/github/v/release/VictoriaMetrics-Community/mcp-victorialogs?sort=semver&label=&filter=!*-victorialogs&logo=github&labelColor=gray&color=gray&link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics-Community%2Fmcp-victorialogs%2Freleases%2Flatest)](https://github.com/VictoriaMetrics-Community/mcp-victorialogs/releases)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/VictoriaMetrics-Community/mcp-victorialogs)](https://archestra.ai/mcp-catalog/victoriametrics-community__mcp-victorialogs)\n[![smithery badge](https://smithery.ai/badge/@VictoriaMetrics-Community/mcp-victorialogs)](https://smithery.ai/server/@VictoriaMetrics-Community/mcp-victorialogs)\n![License](https://img.shields.io/github/license/VictoriaMetrics-Community/mcp-victorialogs?labelColor=green&label=&link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics-Community%2Fmcp-victorialogs%2Fblob%2Fmain%2FLICENSE)\n![Slack](https://img.shields.io/badge/Join-4A154B?logo=slack&link=https%3A%2F%2Fslack.victoriametrics.com)\n![X](https://img.shields.io/twitter/follow/VictoriaMetrics?style=flat&label=Follow&color=black&logo=x&labelColor=black&link=https%3A%2F%2Fx.com%2FVictoriaMetrics)\n![Reddit](https://img.shields.io/reddit/subreddit-subscribers/VictoriaMetrics?style=flat&label=Join&labelColor=red&logoColor=white&logo=reddit&link=https%3A%2F%2Fwww.reddit.com%2Fr%2FVictoriaMetrics)\n\nThe implementation of [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server for [VictoriaLogs](https://docs.victoriametrics.com/victorialogs/).\n\nThis provides access to your VictoriaLogs instance and seamless integration with [VictoriaLogs APIs](https://docs.victoriametrics.com/victorialogs/querying/#http-api) and [documentation](https://docs.victoriametrics.com/victorialogs/).\nIt can give you a comprehensive interface for logs, observability, and debugging tasks related to your VictoriaLogs instances, enable advanced automation and interaction capabilities for engineers and tools.\n\n## Features\n\nThis MCP server allows you to use almost all read-only APIs of VictoriaLogs, i.e. all functions available in [Web UI](https://docs.victoriametrics.com/victorialogs/querying/#web-ui):\n\n- Querying logs and exploring logs data\n- Showing parameters of your VictoriaLogs instance\n- Listing available streams, fields, field values\n- Query statistics for the logs as metrics\n \nIn addition, the MCP server contains embedded up-to-date documentation and is able to search it without online access.\n\nMore details about the exact available tools and prompts can be found in the [Usage](#usage) section.\n\nYou can combine functionality of tools, docs search in your prompts and invent great usage scenarios for your VictoriaLogs instance.\nAnd please note the fact that the quality of the MCP Server and its responses depends very much on the capabilities of your client and the quality of the model you are using.\n\nYou can also combine the MCP server with other observability or doc search related MCP Servers and get even more powerful results.\n\n## Try without installation\n\nThere is a publicly available instance of the VictoriaMetrics MCP Server that you can use to test the features without installing it: \n\n```\nhttps://play-vmlogs-mcp.victoriametrics.com/mcp\n```\n\nIt's available in [Streamable HTTP Mode](#modes) mode and configured to work with [Public VictoriaLogs Playground](https://play-vmlogs.victoriametrics.com).\n\nHere is example of configuration for [Claude Desktop](https://claude.ai/download):\n\n![image](https://github.com/user-attachments/assets/938d9eb9-f188-42f1-8377-a283be454ac7)\n\n## Requirements\n\n- [VictoriaLogs](https://docs.victoriametrics.com/victorialogs/) instance: ([single-node](https://docs.victoriametrics.com/victorialogs/) or [cluster](https://docs.victoriametrics.com/victorialogs/cluster/))\n- Go 1.24 or higher (if you want to build from source)\n\n## Installation\n\n### Go\n\n```bash\ngo install github.com/VictoriaMetrics-Community/mcp-victorialogs/cmd/mcp-victorialogs@latest\n```\n\n### Binaries\n\nJust download the latest release from [Releases](https://github.com/VictoriaMetrics-Community/mcp-victorialogs/releases) page and put it to your PATH.\n\nExample for Linux x86_64 (note that other architectures and platforms are also available):\n\n```bash\nlatest=$(curl -s https://api.github.com/repos/VictoriaMetrics-Community/mcp-victorialogs/releases/latest | grep 'tag_name' | cut -d\\\" -f4)\nwget https://github.com/VictoriaMetrics-Community/mcp-victorialogs/releases/download/$latest/mcp-victorialogs_Linux_x86_64.tar.gz\ntar axvf mcp-victorialogs_Linux_x86_64.tar.gz\n```\n\n### Docker\n\nYou can run VictoriaLogs MCP Server using Docker.\n\nThis is the easiest way to get started without needing to install Go or build from source.\n\n```bash\ndocker run -d --name mcp-victorialogs \\\n  -e MCP_SERVER_MODE=sse \\\n  -e VL_INSTANCE_ENTRYPOINT=https://play-vmlogs.victoriametrics.com \\\n  ghcr.io/victoriametrics-community/mcp-victorialogs\n```\n\nYou should replace environment variables with your own parameters.\n\nNote that the `MCP_SERVER_MODE=sse` flag is used to enable Server-Sent Events mode, which used by MCP clients to connect.\nAlternatively, you can use `MCP_SERVER_MODE=http` to enable Streamable HTTP mode. More details about server modes can be found in the [Configuration](#configuration) section.\n\nSee available docker images in [github registry](https://github.com/orgs/VictoriaMetrics-Community/packages/container/package/mcp-victorialogs).\n\nAlso see [Using Docker instead of binary](#using-docker-instead-of-binary) section for more details about using Docker with MCP server with clients in stdio mode.\n\n\n### Source Code\n\nFor building binary from source code you can use the following approach:\n\n- Clone repo:\n\n  ```bash\n  git clone https://github.com/VictoriaMetrics-Community/mcp-victorialogs.git\n  cd mcp-victorialogs\n  ```\n- Build binary from cloned source code:\n\n  ```bash\n  make build\n  # after that you can find binary mcp-victorialogs and copy this file to your PATH or run inplace\n  ```\n- Build image from cloned source code:\n\n  ```bash\n  docker build -t mcp-victorialogs .\n  # after that you can use docker image mcp-victorialogs for running or pushing\n  ```\n\n### Smithery\n\nTo install VictoriaLogs MCP Server for your client automatically via [Smithery](https://smithery.ai/server/@VictoriaMetrics-Community/mcp-victorialogs), yo can use the following commands:\n\n```bash\n# Get the list of supported MCP clients\nnpx -y @smithery/cli list clients\n#Available clients:\n#  claude\n#  cline\n#  windsurf\n#  roocode\n#  witsy\n#  enconvo\n#  cursor\n#  vscode\n#  vscode-insiders\n#  boltai\n#  amazon-bedrock\n\n# Install VictoriaLogs MCP server for your client\nnpx -y @smithery/cli install @VictoriaMetrics-Community/mcp-victorialogs --client <YOUR-CLIENT-NAME>\n# and follow the instructions\n```\n\n## Configuration\n\nMCP Server for VictoriaLogs is configured via environment variables:\n\n| Variable                   | Description                                             | Required | Default          | Allowed values         |\n|----------------------------|---------------------------------------------------------|----------|------------------|------------------------|\n| `VL_INSTANCE_ENTRYPOINT`   | URL to VictoriaLogs instance                            | Yes      | -                | -                      |\n| `VL_INSTANCE_BEARER_TOKEN` | Authentication token for VictoriaLogs API               | No       | -                | -                      |\n| `VL_INSTANCE_HEADERS`      | Custom HTTP headers to send with requests (comma-separated key=value pairs) | No       | -                | -                      |\n| `MCP_SERVER_MODE`          | Server operation mode. See [Modes](#modes) for details. | No       | `stdio`          | `stdio`, `sse`, `http` |\n| `MCP_LISTEN_ADDR`          | Address for SSE or HTTP server to listen on             | No       | `localhost:8081` | -                      |\n| `MCP_DISABLED_TOOLS`       | Comma-separated list of tools to disable                | No       | -                | -                      |\n| `MCP_HEARTBEAT_INTERVAL`   | Defines the heartbeat interval for the streamable-http protocol. <br /> It means the MCP server will send a heartbeat to the client through the GET connection, <br /> to keep the connection alive from being closed by the network infrastructure (e.g. gateways) | No | `30s`  | -   |\n\n### Modes\n\nMCP Server supports the following modes of operation (transports):\n\n- `stdio` - Standard input/output mode, where the server reads commands from standard input and writes responses to standard output. This is the default mode and is suitable for local servers.\n- `sse` - Server-Sent Events. Server will expose the `/sse` and `/message` endpoints for SSE connections.\n- `http` - Streamable HTTP. Server will expose the `/mcp` endpoint for HTTP connections.\n\nMore info about traqnsports you can find in MCP docs:\n\n- [Core concepts -> Transports](https://modelcontextprotocol.io/docs/concepts/transports)\n- [Specifications -> Transports](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports)\n\n### Сonfiguration examples\n\n```bash\n# For a public playground\nexport VL_INSTANCE_ENTRYPOINT=\"https://play-vmlogs.victoriametrics.com\"\n\n# Custom headers for authentication (e.g., behind a reverse proxy)\n# Expected syntax is key=value separated by commas\nexport VL_INSTANCE_HEADERS=\"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n\n# Server mode\nexport MCP_SERVER_MODE=\"sse\"\nexport MCP_SSE_ADDR=\"0.0.0.0:8081\"\nexport MCP_DISABLED_TOOLS=\"hits,facets\"\n```\n\n## Endpoints\n\nIn SSE and HTTP modes the MCP server provides the following endpoints:\n\n| Endpoint            | Description                                                                                      |\n|---------------------|--------------------------------------------------------------------------------------------------|\n| `/sse` + `/message` | Endpoints for messages in SSE mode (for MCP clients that support SSE)                            |\n| `/mcp`              | HTTP endpoint for streaming messages in HTTP mode (for MCP clients that support Streamable HTTP) |\n| `/metrics`          | Metrics in Prometheus format for monitoring the MCP server                                       |\n| `/health/liveness`  | Liveness check endpoint to ensure the server is running                                          |\n| `/health/readiness` | Readiness check endpoint to ensure the server is ready to accept requests                        |\n\n## Setup in clients\n\n### Cursor\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server` and paste the following configuration into your Cursor `~/.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"victorialogs\": {\n      \"command\": \"/path/to/mcp-victorialogs\",\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n### Claude Desktop\n\nAdd this to your Claude Desktop `claude_desktop_config.json` file (you can find it if open `Settings` -> `Developer` -> `Edit config`):\n\n```json\n{\n  \"mcpServers\": {\n    \"victorialogs\": {\n      \"command\": \"/path/to/mcp-victorialogs\",\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n### Claude Code\n\nRun the command:\n\n```sh\nclaude mcp add victorialogs -- /path/to/mcp-victorialogs \\\n  -e VL_INSTANCE_ENTRYPOINT=<YOUR_VL_INSTANCE> \\\n  -e VL_INSTANCE_BEARER_TOKEN=<YOUR_VL_BEARER_TOKEN> \\\n  -e VL_INSTANCE_HEADERS=\"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n```\n\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp) for more info.\n\n### Visual Studio Code\n\nAdd this to your VS Code MCP config file:\n\n```json\n{\n  \"servers\": {\n    \"victorialogs\": {\n      \"type\": \"stdio\",\n      \"command\": \"/path/to/mcp-victorialogs\",\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n### Zed\n\nAdd the following to your Zed config file:\n\n```json\n  \"context_servers\": {\n    \"victorialogs\": {\n      \"command\": {\n        \"path\": \"/path/to/mcp-victorialogs\",\n        \"args\": [],\n        \"env\": {\n                  \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\nSee [Zed MCP docs](https://zed.dev/docs/ai/mcp) for more info.\n\n### JetBrains IDEs\n\n- Open `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`.\n- Click `Add (+)`\n- Select `As JSON`\n- Put the following to the input field:\n\n```json\n{\n  \"mcpServers\": {\n    \"victorialogs\": {\n      \"command\": \"/path/to/mcp-victorialogs\",\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following to your Windsurf MCP config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"victorialogs\": {\n      \"command\": \"/path/to/mcp-victorialogs\",\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Windsurf MCP docs](https://docs.windsurf.com/windsurf/mcp) for more info.\n\n### Using Docker instead of binary\n\nYou can run VictoriaLogs MCP Server using Docker instead of local binary.\n\nYou should replace run command in configuration examples above in the following way:\n\n```\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\", \"--rm\",\n          \"-e\", \"VL_INSTANCE_ENTRYPOINT\",\n          \"-e\", \"VL_INSTANCE_BEARER_TOKEN\",\n          \"-e\", \"VL_INSTANCE_HEADERS\",\n          \"ghcr.io/victoriametrics-community/mcp-victorialogs\",\n        ],\n      \"env\": {\n        \"VL_INSTANCE_ENTRYPOINT\": \"<YOUR_VL_INSTANCE>\",\n        \"VL_INSTANCE_BEARER_TOKEN\": \"<YOUR_VL_BEARER_TOKEN>\",\n        \"VL_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nAfter [installing](#installation) and [configuring](#setup-in-clients) the MCP server, you can start using it with your favorite MCP client.\n\nYou can start dialog with AI assistant from the phrase:\n\n```\nUse MCP VictoriaLogs in the following answers\n```\n\nBut it's not required, you can just start asking questions and the assistant will automatically use the tools and documentation to provide you with the best answers.\n\n### Toolset\n\nMCP VictoriaLogs provides numerous tools for interacting with your VictoriaLogs instance.\n\nHere's a list of available tools:\n\n| Tool                 | Description                                           |\n|----------------------|-------------------------------------------------------|\n| `documentation`      | Search in embedded VictoriaLogs documentation         |\n| `facets`             | Most frequent values per each log field               |\n| `field_names`        | List of field names for the query                     |\n| `field_values`       | List of field values for the query                    |\n| `flags`              | View non-default flags of the VictoriaLogs instance   |\n| `hits`               | The number of matching log entries grouped by buckets |\n| `query`              | Execute LogsQL queries                                |\n| `stats_query`        | Querying log stats for the given time                 |\n| `stats_query_range`  | Querying log stats on the given time range            |\n| `stream_field_names` | List of stream fields for the query                   |\n| `stream_field_values` | List of stream field values for the query             |\n| `stream_ids`         | List of stream IDs for the query                      |\n| `streams`            | List of streams for the query                         |\n\n### Prompts\n\nThe server includes pre-defined prompts for common tasks.\n\nThese are just examples at the moment, the prompt library will be added to in the future:\n\n| Prompt | Description                                           |\n|--------|-------------------------------------------------------|\n| `documentation` | Search VictoriaLogs documentation for specific topics |\n\n## Roadmap\n\n- [ ] Support \"Explain query\" tool\n- [ ] Support optional integration with [VictoriaMetrics Cloud](https://victoriametrics.com/products/cloud/)  \n- [ ] Add some extra knowledge to server in addition to current documentation tool:\n  - [x] [VictoriaMetrics blog](https://victoriametrics.com/blog/) posts\n  - [ ] Github issues\n  - [ ] Public slack chat history\n  - [ ] CRD schemas\n- [ ] Implement multitenant version of MCP (that will support several deployments)\n- [ ] Add flags/configs validation tool\n- [x] Enabling/disabling tools via configuration\n\n## Disclaimer\n\nAI services and agents along with MCP servers like this cannot guarantee the accuracy, completeness and reliability of results.\nYou should double check the results obtained with AI.\n\nThe quality of the MCP Server and its responses depends very much on the capabilities of your client and the quality of the model you are using.\n\n## Contributing\n\nContributions to the MCP VictoriaLogs project are welcome! \n\nPlease feel free to submit issues, feature requests, or pull requests.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "victorialogs",
        "database",
        "databases secure",
        "secure database",
        "victorialogs provides"
      ],
      "category": "databases"
    },
    "XGenerationLab--xiyan_mcp_server": {
      "owner": "XGenerationLab",
      "name": "xiyan_mcp_server",
      "url": "https://github.com/XGenerationLab/xiyan_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/XGenerationLab.webp",
      "description": "Enables natural language queries to MySQL and PostgreSQL databases using advanced text-to-SQL techniques. Supports both cloud and local operating modes for seamless database interaction and data retrieval.",
      "stars": 213,
      "forks": 40,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-28T03:47:50Z",
      "readme_content": "<h1 align=\"center\">XiYan MCP Server</h1>\n<p align=\"center\">\n  <a href=\"https://github.com/XGenerationLab/XiYan-SQL\"><img alt=\"MCP Playwright\" src=\"https://raw.githubusercontent.com/XGenerationLab/XiYan-SQL/main/xiyanGBI.png\" height=\"60\"/></a>\n</p>\n<p align=\"center\">\n  <b>A Model Context Protocol (MCP) server that enables natural language queries to databases</b><br/>\n  <sub>powered by <a href=\"https://github.com/XGenerationLab/XiYan-SQL\" >XiYan-SQL</a>, SOTA of text-to-sql on open benchmarks</sub>\n</p>\n<p align=\"center\">\n💻 <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server\" >XiYan-mcp-server</a> | \n🌐 <a href=\"https://github.com/XGenerationLab/XiYan-SQL\" >XiYan-SQL</a> |\n📖 <a href=\"https://arxiv.org/abs/2507.04701\"> Arxiv</a> | \n🏆 <a href=\"https://github.com/XGenerationLab/XiYanSQL-QwenCoder\" >XiYanSQL Model</a> |\n📄 <a href=\"https://paperswithcode.com/paper/xiyan-sql-a-multi-generator-ensemble\" >PapersWithCode</a>\n🤗 <a href=\"https://huggingface.co/collections/XGenerationLab/xiyansql-models-67c9844307b49f87436808fc\">HuggingFace</a> |\n🤖 <a href=\"https://modelscope.cn/collections/XiYanSQL-Models-4483337b614241\" >ModelScope</a> |\n🌕 <a href=\"https://bailian.console.aliyun.com/xiyan\">析言GBI</a> \n<br />\n<img src=\"https://badge.mcpx.dev/?type=server%20%27MCP%20Server%27\" alt=\"MCP Server\" />\n<a href=\"https://arxiv.org/abs/2411.08599\"></a>\n<a href=\"https://opensource.org/licenses/Apache-2.0\">\n  <img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"License: Apache 2.0\" />\n</a>\n<a href=\"https://pepy.tech/projects/xiyan-mcp-server\"><img src=\"https://static.pepy.tech/badge/xiyan-mcp-server\" alt=\"PyPI Downloads\"></a>\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/XGenerationLab/xiyan_mcp_server)](https://archestra.ai/mcp-catalog/xgenerationlab__xiyan_mcp_server)\n  <a href=\"https://smithery.ai/server/@XGenerationLab/xiyan_mcp_server\"><img alt=\"Smithery Installs\" src=\"https://smithery.ai/badge/@XGenerationLab/xiyan_mcp_server\" height=\"20\"/></a>\n<a href=\"https://github.com/XGenerationLab/xiyan_mcp_server\" target=\"_blank\">\n    <img src=\"https://img.shields.io/github/stars/XGenerationLab/xiyan_mcp_server?style=social\" alt=\"GitHub stars\" />\n</a>\n<br />\n<a href=\"https://github.com/XGenerationLab/xiyan_mcp_server\" >English</a> | <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/README_zh.md\"> 中文 </a> | <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/README_ja.md\"> 日本語 </a><br />\n<a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/imgs/dinggroup_out.png\">Ding Group钉钉群</a>｜ \n<a href=\"https://weibo.com/u/2540915670\" target=\"_blank\">Follow me on Weibo</a>\n</p>\n\n\n## Table of Contents\n\n- [Features](#features)\n- [Preview](#preview)\n  - [Architecture](#architecture)\n  - [Best Practice](#best-practice)\n  - [Tools Preview](#tools-preview)\n- [Installation](#installation)\n  - [Installing from pip](#installing-from-pip)\n  - [Installing from Smithery.ai](#installing-from-smitheryai)\n- [Configuration](#configuration)\n  - [LLM Configuration](#llm-configuration)\n    - [General LLMs](#general-llms)\n    - [Text-to-SQL SOTA model](#text-to-sql-sota-model)\n    - [Local Model](#local-model)\n  - [Database Configuration](#database-configuration)\n    - [MySQL](#mysql)\n    - [PostgreSQL](#postgresql)\n- [Launch](#launch)\n  - [Claude Desktop](#claude-desktop)\n  - [Cline](#cline)\n  - [Goose](#goose)\n  - [Cursor](#cursor)\n- [It Does Not Work](#it-does-not-work)\n- [Citation](#citation)\n\n\n## Features\n- 🌐 Fetch data by natural language through [XiYanSQL](https://github.com/XGenerationLab/XiYan-SQL)\n- 🤖 Support general LLMs (GPT,qwenmax), Text-to-SQL SOTA model\n- 💻 Support pure local mode (high security!)\n- 📝 Support MySQL and PostgreSQL. \n- 🖱️ List available tables as resources\n- 🔧 Read table contents\n\n## Preview\n### Architecture\nThere are two ways to integrate this server in your project, as shown below:\nThe left is remote mode, which is the default mode. It requires an API key to access the xiyanSQL-qwencoder-32B model from service provider (see [Configuration](#Configuration)).\nAnother mode is local mode, which is more secure. It does not require the API key.\n\n\n### Best practice and reports\n\n[\"Build a local data assistant using MCP + Modelscope API-Inference without writing a single line of code\"](https://mp.weixin.qq.com/s/tzDelu0W4w6t9C0_yYRbHA)\n\n[\"Xiyan MCP on Modelscope\"](https://modelscope.cn/headlines/article/1142)\n\n### Evaluation on MCPBench\nThe following figure illustrates the performance of the XiYan MCP server as measured by the MCPBench benchmark. The XiYan MCP server demonstrates superior performance compared to both the MySQL MCP server and the PostgreSQL MCP server, achieving a lead of 2-22 percentage points. The detailed experiment results can be found at [MCPBench](https://github.com/modelscope/MCPBench) and the report [\"Evaluation Report on MCP Servers\"](https://arxiv.org/abs/2504.11094).\n\n\n\n### Tools Preview\n - The tool ``get_data`` provides a natural language interface for retrieving data from a database. This server will convert the input natural language into SQL using a built-in model and call the database to return the query results.\n\n - The ``{dialect}://{table_name}`` resource allows obtaining a portion of sample data from the database for model reference when a specific table_name is specified. \n- The ``{dialect}://`` resource will list the names of the current databases\n\n## Installation\n### Installing from pip\n\nPython 3.11+ is required. \nYou can install the server through pip, and it will install the latest version:\n\n```shell\npip install xiyan-mcp-server\n```\n\nIf you want to install the development version from source, you can install from source code on github:\n```shell\npip install git+https://github.com/XGenerationLab/xiyan_mcp_server.git\n```\n\n### Installing from Smithery.ai\nSee [@XGenerationLab/xiyan_mcp_server](https://smithery.ai/server/@XGenerationLab/xiyan_mcp_server)\n\nNot fully tested.\n\n## Configuration\n\nYou need a YAML config file to configure the server.\nA default config file is provided in config_demo.yml which looks like this:\n\n```yaml\nmcp:\n  transport: \"stdio\"\nmodel:\n  name: \"XGenerationLab/XiYanSQL-QwenCoder-32B-2412\"\n  key: \"\"\n  url: \"https://api-inference.modelscope.cn/v1/\"\ndatabase:\n  host: \"localhost\"\n  port: 3306\n  user: \"root\"\n  password: \"\"\n  database: \"\"\n```\n\n### MCP Configuration\nYou can set the transport protocol to ``stdio`` or ``sse``.\n#### STDIO\nFor stdio protocol, you can set just like this:\n```yaml\nmcp:\n  transport: \"stdio\"\n```\n#### SSE\nFor sse protocol, you can set mcp config as below:\n```yaml\nmcp:\n  transport: \"sse\"\n  port: 8000\n  log_level: \"INFO\"\n```\nThe default port is `8000`. You can change the port if needed. \nThe default log level is `ERROR`. We recommend to set log level to `INFO` for more detailed information.\n\nOther configurations like `debug`, `host`, `sse_path`, `message_path` can be customized as well, but normally you don't need to modify them.\n\n### LLM Configuration\n``Name`` is the name of the model to use, ``key`` is the API key of the model, ``url`` is the API url of the model. We support following models.\n\n| versions | general LLMs(GPT,qwenmax)                                             | SOTA model by Modelscope                   | SOTA model by Dashscope                                   | Local LLMs            |\n|----------|-------------------------------|--------------------------------------------|-----------------------------------------------------------|-----------------------|\n| description| basic, easy to use | best performance, stable, recommand        | best performance, for trial                               | slow, high-security   |\n| name     | the official model name (e.g. gpt-3.5-turbo,qwen-max)                 | XGenerationLab/XiYanSQL-QwenCoder-32B-2412 | xiyansql-qwencoder-32b                                    | xiyansql-qwencoder-3b |\n| key      | the API key of the service provider (e.g. OpenAI, Alibaba Cloud)      | the API key of modelscope                  | the API key via email                                     | \"\"                    |\n| url      | the endpoint of the service provider (e.g.\"https://api.openai.com/v1\") | https://api-inference.modelscope.cn/v1/    | https://xiyan-stream.biz.aliyun.com/service/api/xiyan-sql | http://localhost:5090 |\n\n#### General LLMs\nIf you want to use the general LLMs, e.g. gpt3.5, you can directly config like this:\n```yaml\nmodel:\n  name: \"gpt-3.5-turbo\"\n  key: \"YOUR KEY \"\n  url: \"https://api.openai.com/v1\"\ndatabase:\n```\n\nIf you want to use Qwen from Alibaba, e.g. Qwen-max, you can use following config:\n```yaml\nmodel:\n  name: \"qwen-max\"\n  key: \"YOUR KEY \"\n  url: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\ndatabase:\n```\n#### Text-to-SQL SOTA model\nWe recommend the XiYanSQL-qwencoder-32B (https://github.com/XGenerationLab/XiYanSQL-QwenCoder), which is the SOTA model in text-to-sql, see [Bird benchmark](https://bird-bench.github.io/).\nThere are two ways to use the model. You can use either of them.\n(1) [Modelscope](https://www.modelscope.cn/models/XGenerationLab/XiYanSQL-QwenCoder-32B-2412),  (2) Alibaba Cloud DashScope.\n\n\n##### (1) Modelscope version\nYou need to apply a ``key`` of API-inference from Modelscope, https://www.modelscope.cn/docs/model-service/API-Inference/intro\nThen you can use the following config:\n```yaml\nmodel:\n  name: \"XGenerationLab/XiYanSQL-QwenCoder-32B-2412\"\n  key: \"\"\n  url: \"https://api-inference.modelscope.cn/v1/\"\n```\n\nRead our [model description](https://www.modelscope.cn/models/XGenerationLab/XiYanSQL-QwenCoder-32B-2412) for more details. \n\n##### (2) Dashscope version\n\nWe deployed the model on Alibaba Cloud DashScope, so you need to set the following environment variables:\nSend me your email to get the ``key``. ( godot.lzl@alibaba-inc.com )\nIn the email, please attach the following information:\n```yaml\nname: \"YOUR NAME\",\nemail: \"YOUR EMAIL\",\norganization: \"your college or Company or Organization\"\n```\nWe will send you a ``key`` according to your email. And you can fill the ``key`` in the yml file.\nThe ``key`` will be expired by  1 month or 200 queries or other legal restrictions.\n\n\n```yaml\nmodel:\n  name: \"xiyansql-qwencoder-32b\"\n  key: \"KEY\"\n  url: \"https://xiyan-stream.biz.aliyun.com/service/api/xiyan-sql\"\n```\n\nNote: this model service is just for trial, if you need to use it in production, please contact us.\n\n##### (3) Local version\nAlternatively, you can also deploy the model [XiYanSQL-qwencoder-32B](https://github.com/XGenerationLab/XiYanSQL-QwenCoder) on your own server.\nSee [Local Model](src/xiyan_mcp_server/local_model/README.md) for more details.\n\n\n### Database Configuration\n``host``, ``port``, ``user``, ``password``, ``database`` are the connection information of the database.\n\nYou can use local or any remote databases. Now we support MySQL and PostgreSQL(more dialects soon).\n\n#### MySQL\n\n```yaml\ndatabase:\n  host: \"localhost\"\n  port: 3306\n  user: \"root\"\n  password: \"\"\n  database: \"\"\n```\n#### PostgreSQL\nStep 1: Install Python packages\n```bash\npip install psycopg2\n```\nStep 2: prepare the config.yml like this:\n```yaml\ndatabase:\n  dialect: \"postgresql\"\n  host: \"localhost\"\n  port: 5432\n  user: \"\"\n  password: \"\"\n  database: \"\"\n```\n\nNote that ``dialect`` should be ``postgresql`` for postgresql.\n## Launch\n\n### Server Launch\n\nIf you want to launch server with `sse`, you have to run the following command in a terminal:\n```shell\nYML=path/to/yml python -m xiyan_mcp_server\n```\nThen you should see the information on http://localhost:8000/sse in your browser. (Defaultly, change if your mcp server runs on other host/port)\n\nOtherwise, if you use `stdio` transport protocol, you usually declare the mcp server command in specific mcp application instead of launching it in a terminal.\nHowever, you can still debug with this command if needed.\n\n### Client Setting\n\n#### Claude Desktop\nAdd this in your Claude Desktop config file, ref <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/imgs/claude_desktop.jpg\">Claude Desktop config example</a>\n```json\n{\n    \"mcpServers\": {\n        \"xiyan-mcp-server\": {\n            \"command\": \"/xxx/python\",\n            \"args\": [\n                \"-m\",\n                \"xiyan_mcp_server\"\n            ],\n            \"env\": {\n                \"YML\": \"PATH/TO/YML\"\n            }\n        }\n    }\n}\n```\n**Please note that the Python command here requires the complete path to the Python executable (`/xxx/python`); otherwise, the Python interpreter cannot be found. You can determine this path by using the command `which python`. The same applies to other applications as well.**\n\nClaude Desktop currently does not support the SSE transport protocol.\n\n#### Cline\nPrepare the config like [Claude Desktop](#claude-desktop)\n\n#### Goose\nIf you use `stdio`, add following command in the config, ref <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/imgs/goose.jpg\">Goose config example</a>\n```shell\nenv YML=path/to/yml /xxx/python -m xiyan_mcp_server\n```\nOtherwise, if you use `sse`, change Type to `SSE` and set the endpoint to `http://127.0.0.1:8000/sse`\n#### Cursor\nUse the similar command as follows.\n\nFor `stdio`:\n```json\n{\n  \"mcpServers\": {\n    \"xiyan-mcp-server\": {\n      \"command\": \"/xxx/python\",\n      \"args\": [\n        \"-m\",\n        \"xiyan_mcp_server\"\n      ],\n      \"env\": {\n        \"YML\": \"path/to/yml\"\n      }\n    }\n  }\n}\n```\nFor `sse`:\n```json\n{\n  \"mcpServers\": {\n    \"xiyan_mcp_server_1\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n\n#### Witsy\nAdd following in command:\n```shell\n/xxx/python -m xiyan_mcp_server\n```\nAdd an env: key is YML and value is the path to your yml file.\nRef <a href=\"https://github.com/XGenerationLab/xiyan_mcp_server/blob/main/imgs/witsy.jpg\">Witsy config example</a>\n\n\n## Contact us:\nIf you are interested in our research or products, please feel free to contact us.\n\n#### Contact Information:\n\nYifu Liu, zhencang.lyf@alibaba-inc.com\n\n#### Join Our DingTalk Group\n\n<a href=\"https://github.com/XGenerationLab/XiYan-SQL/blob/main/xiyansql_dingding.png\">Ding Group钉钉群</a> \n\n\n## Other Related Links\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/xgenerationlab-xiyan-mcp-server-badge.png)](https://mseep.ai/app/xgenerationlab-xiyan-mcp-server)\n\n\n\n\n## Citation\nIf you find our work helpful, feel free to give us a cite.\n```bibtex\n@article{XiYanSQL,\n      title={XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL}, \n      author={Yifu Liu and Yin Zhu and Yingqi Gao and Zhiling Luo and Xiaoxia Li and Xiaorong Shi and Yuntao Hong and Jinyang Gao and Yu Li and Bolin Ding and Jingren Zhou},\n      year={2025},\n      eprint={2507.04701},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2507.04701}, \n}\n```\n```bibtex\n@article{xiyansql_pre,\n      title={A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL}, \n      author={Yingqi Gao and Yifu Liu and Xiaoxia Li and Xiaorong Shi and Yin Zhu and Yiming Wang and Shiqi Li and Wei Li and Yuntao Hong and Zhiling Luo and Jinyang Gao and Liyu Mou and Yu Li},\n      year={2024},\n      journal={arXiv preprint arXiv:2411.08599},\n      url={https://arxiv.org/abs/2411.08599},\n      primaryClass={cs.AI}\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "postgresql",
        "secure database",
        "databases secure",
        "databases using"
      ],
      "category": "databases"
    },
    "Zhwt--go-mcp-mysql": {
      "owner": "Zhwt",
      "name": "go-mcp-mysql",
      "url": "https://github.com/Zhwt/go-mcp-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/Zhwt.webp",
      "description": "Zero burden Model Context Protocol (MCP) server designed for CRUD operations on MySQL databases and tables. Supports a read-only mode and the option to check query plans with an `EXPLAIN` statement before execution.",
      "stars": 44,
      "forks": 9,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-18T11:48:41Z",
      "readme_content": "# go-mcp-mysql\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Zhwt/go-mcp-mysql)](https://archestra.ai/mcp-catalog/zhwt__go-mcp-mysql)\n\n## Overview\n\nZero burden, ready-to-use Model Context Protocol (MCP) server for interacting with MySQL and automation. No Node.js or Python environment needed. This server provides tools to do CRUD operations on MySQL databases and tables, and a read-only mode to prevent surprise write operations. You can also make the MCP server check the query plan by using a `EXPLAIN` statement before executing the query by adding a `--with-explain-check` flag.\n\nPlease note that this is a work in progress and may not yet be ready for production use.\n\n## Installation\n\n1. Get the latest [release](https://github.com/Zhwt/go-mcp-mysql/releases) and put it in your `$PATH` or somewhere you can easily access.\n\n2. Or if you have Go installed, you can build it from source:\n\n```sh\ngo install -v github.com/Zhwt/go-mcp-mysql@latest\n```\n\n## Usage\n\n### Method A: Using Command Line Arguments\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"go-mcp-mysql\",\n      \"args\": [\n        \"--host\", \"localhost\",\n        \"--user\", \"root\",\n        \"--pass\", \"password\",\n        \"--port\", \"3306\",\n        \"--db\", \"mydb\"\n      ]\n    }\n  }\n}\n```\n\n### Method B: Using DSN With Custom Options\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"go-mcp-mysql\",\n      \"args\": [\n        \"--dsn\", \"username:password@tcp(localhost:3306)/mydb?parseTime=true&loc=Local\"\n      ]\n    }\n  }\n}\n```\n\nPlease refer to [MySQL DSN](https://github.com/go-sql-driver/mysql#dsn-data-source-name) for more details.\n\nNote: For those who put the binary outside of your `$PATH`, you need to replace `go-mcp-mysql` with the full path to the binary: e.g.: if you put the binary in the **Downloads** folder, you may use the following path:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"C:\\\\Users\\\\<username>\\\\Downloads\\\\go-mcp-mysql.exe\",\n      \"args\": [\n        ...\n      ]\n    }\n  }\n}\n```\n\n### Optional Flags\n\n- Add a `--read-only` flag to enable read-only mode. In this mode, only tools beginning with `list`, `read_` and `desc_` are available. Make sure to refresh/restart the MCP server after adding this flag.\n- By default, CRUD queries will be first executed with a `EXPLAIN ?` statement to check whether the generated query plan matches the expected pattern. Add a `--with-explain-check` flag to disable this behavior.\n\n## Tools\n\n### Schema Tools\n\n1. `list_database`\n\n    - List all databases in the MySQL server.\n    - Parameters: None\n    - Returns: A list of matching database names.\n\n2. `list_table`\n\n    - List all tables in the MySQL server.\n    - Parameters:\n        - `name`: If provided, list tables with the specified name, same as SQL `SHOW TABLES LIKE '%name%'`. Otherwise, list all tables.\n    - Returns: A list of matching table names.\n\n3. `create_table`\n\n    - Create a new table in the MySQL server.\n    - Parameters:\n        - `query`: The SQL query to create the table.\n    - Returns: x rows affected.\n\n4. `alter_table`\n\n    - Alter an existing table in the MySQL server. The LLM is informed not to drop an existing table or column.\n    - Parameters:\n        - `query`: The SQL query to alter the table.\n    - Returns: x rows affected.\n\n5. `desc_table`\n\n    - Describe the structure of a table.\n    - Parameters:\n        - `name`: The name of the table to describe.\n    - Returns: The structure of the table.\n\n### Data Tools\n\n1. `read_query`\n\n    - Execute a read-only SQL query.\n    - Parameters:\n        - `query`: The SQL query to execute.\n    - Returns: The result of the query.\n\n2. `write_query`\n\n    - Execute a write SQL query.\n    - Parameters:\n        - `query`: The SQL query to execute.\n    - Returns: x rows affected, last insert id: <last_insert_id>.\n\n3. `update_query`\n\n    - Execute an update SQL query.\n    - Parameters:\n        - `query`: The SQL query to execute.\n    - Returns: x rows affected.\n\n4. `delete_query`\n\n    - Execute a delete SQL query.\n    - Parameters:\n        - `query`: The SQL query to execute.\n    - Returns: x rows affected.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "databases secure",
        "mcp mysql"
      ],
      "category": "databases"
    },
    "a21071--mcp-postgres": {
      "owner": "a21071",
      "name": "mcp-postgres",
      "url": "https://github.com/a21071/mcp-postgres",
      "imageUrl": "/freedevtools/mcp/pfp/a21071.webp",
      "description": "Perform seamless CRUD operations on a PostgreSQL database with type safety, allowing efficient management of user data and post entities. Utilize a robust MCP-compatible tool interface for reliable database interactions.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-26T05:39:52Z",
      "readme_content": "# MCP PostgreSQL Server\n\n-\n\nA Model Context Protocol (MCP) server that provides PostgreSQL database operations through MCP tools.\n\n## Features\n\n- CRUD operations for User and Post entities\n- Type-safe database operations using Prisma\n- MCP-compatible tool interface\n- Built with TypeScript for type safety\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\ngit clone https://github.com/a21071/mcp-postgres.git\ncd mcp-postgres\nnpm install\n```\n\n3. Set up PostgreSQL database:\n\n```bash\ndocker-compose up -d\n```\n\n4. Run database migrations:\n\n```bash\nnpx prisma migrate dev\n```\n\n5. Build the project:\n\n```bash\nnpm run build\n```\n\n## Usage\n\nRun the server:\n\n```bash\nnpm start\n```\n\n### Available MCP Tools\n\n- **getData**: Retrieve user data from PostgreSQL\n\n  ```json\n  {\n    \"tableName\": \"user\"\n  }\n  ```\n\n- **addUserData**: Add new user to database\n\n  ```json\n  {\n    \"email\": \"user@example.com\",\n    \"name\": \"John Doe\",\n    \"age\": 30\n  }\n  ```\n\n- **deleteUserData**: Delete user by ID, email or name\n\n  ```json\n  {\n    \"id\": \"clxyz...\",\n    \"email\": \"user@example.com\",\n    \"name\": \"John Doe\"\n  }\n  ```\n\n- **updateUserData**: Update user information\n  ```json\n  {\n    \"id\": \"clxyz...\",\n    \"email\": \"new@example.com\",\n    \"name\": \"New Name\"\n  }\n  ```\n\n## Database Schema\n\nThe server uses the following Prisma schema:\n\n```prisma\nmodel User {\n  id        String   @id @default(cuid())\n  email     String   @unique\n  name      String?\n  age       Int?\n  createdAt DateTime @default(now())\n  posts     Post[]\n}\n\n```\n\n## Development\n\n- Watch mode:\n\n```bash\nnpm run watch\n```\n\n## Dependencies\n\n- [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk) - MCP server SDK\n- [Prisma](https://www.prisma.io/) - Type-safe database client\n- [TypeScript](https://www.typescriptlang.org/) - Type checking\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "secure database",
        "databases secure",
        "postgresql database"
      ],
      "category": "databases"
    },
    "abel9851--mcp-server-mariadb": {
      "owner": "abel9851",
      "name": "mcp-server-mariadb",
      "url": "https://github.com/abel9851/mcp-server-mariadb",
      "imageUrl": "/freedevtools/mcp/pfp/abel9851.webp",
      "description": "Retrieve and manage database schemas from a MariaDB instance while executing read-only queries. Simplifies database interactions with a standardized protocol for seamless data access.",
      "stars": 19,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-19T15:58:50Z",
      "readme_content": "# mcp-server-mariadb\n\nAn MCP server implementation for retrieving data from mariadb\n\n## Features\n\n### Resources\n\nExpose schema list in database\n\n### Tools\n\n- query_database\n  - Execute read-only operations against MariDB\n\n## dependency\n\n### install mariadb\n\n- mac\n  - when install mariadb,\nmaybe raise os error below.\nyou can resolve by installing mariadb-connector-c.\n\n```bash\n\nOSError: mariadb_config not found.\n\n      This error typically indicates that MariaDB Connector/C, a dependency which\n      must be preinstalled, is not found.\n      If MariaDB Connector/C is not installed, see installation instructions\n      If MariaDB Connector/C is installed, either set the environment variable\n      MARIADB_CONFIG or edit the configuration file 'site.cfg' to set the\n       'mariadb_config' option to the file location of the mariadb_config utility.\n\n\n```\n\n1. execute `brew install mariadb-connector-c`\n2. execute `echo 'export PATH=\"/opt/homebrew/opt/mariadb-connector-c/bin:$PATH\"' >> ~/.bashrc`\n3. set environment variable `export MARIADB_CONFIG=$(brew --prefix mariadb-connector-c)/bin/mariadb_config`\n4. execute `uv add mariadb` again.\n\n## Usage with Claude Desktop\n\n### Configuration File\n\nPaths to Claude Desktop config file:\n\n- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n<!-- markdownlint-disable MD033 -->\n<details>\n<summary>Add this configuration to enable published servers</summary>\n\n```json\n\n{\n    \"mcpServers\": {\n        \"mcp_server_mariadb\": {\n            \"command\": \"/PATH/TO/uvx\"\n            \"args\": [\n                \"mcp-server-mariadb\",\n                \"--host\",\n                \"${DB_HOST}\",\n                \"--port\",\n                \"${DB_PORT}\",\n                \"--user\",\n                \"${DB_USER}\",\n                \"--password\",\n                \"${DB_PASSWORD}\",\n                \"--database\",\n                \"${DB_NAME}\"\n            ]\n        }\n    }\n}\n\n```\n\n**Note**: Replace these placeholders with actual paths:\n\n- `/PATH/TO/uvx`: Full path to uvx executable\n\n</details>\n\n<details>\n<summary>Add this configuration to enable development/unpublished servers</summary>\n\n```json\n{\n    \"mcpServers\": {\n        \"mcp_server_mariadb\": {\n            \"command\": \"/PATH/TO/uv\",\n            \"args\": [\n                \"--directory\",\n                \"/YOUR/SOURCE/PATH/mcp-server-mariadb/src/mcp_server_mariadb\",\n                \"run\",\n                \"server.py\"\n            ],\n            \"env\": {\n                \"MARIADB_HOST\": \"127.0.0.1\",\n                \"MARIADB_USER\": \"USER\",\n                \"MARIADB_PASSWORD\": \"PASSWORD\",\n                \"MARIADB_DATABASE\": \"DATABASE\",\n                \"MARIADB_PORT\": \"3306\"\n            }\n        }\n    }\n}\n```\n\n**Note**: Replace these placeholders with actual paths:\n\n- `/PATH/TO/uv`: Full path to UV executable\n- `/YOUR/SOURCE/PATH/mcp-server-mariadb/src/mcp_server_mariadb`: Path to server source code\n\n</details>\n\n## License\n\nThis mcp server is licensed under the MIT license.  please see the LICENSE file in the repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mariadb",
        "databases",
        "database",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "adiletD--feature-request-collection-mcp": {
      "owner": "adiletD",
      "name": "feature-request-collection-mcp",
      "url": "https://github.com/adiletD/feature-request-collection-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/adiletD.webp",
      "description": "Connect to Supabase to efficiently query and manage feature suggestions. Retrieve user feedback and feature requests directly from a Supabase database in real-time.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-17T14:12:14Z",
      "readme_content": "# Supabase MCP Server\n\nThis is a Model Context Protocol (MCP) server that connects to Supabase and allows you to query the feature_suggestions table.\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm\n- Supabase project with credentials\n\n## Setup\n\n1. Make sure your `.env` file contains the following Supabase credentials:\n\n   ```\n   SUPABASE_URL=your_supabase_url\n   SUPABASE_ANON_KEY=your_supabase_anon_key\n   ```\n\n2. Install the required dependencies:\n   ```\n   npm install\n   ```\n\n## Running the Server\n\nRun the MCP server using:\n\n```bash\nnpx tsx mcp-server.ts\n```\n\nOr use the npm script:\n\n```bash\nnpm run dev\n```\n\n## Connecting to AI Tools\n\n### Cursor\n\n1. Open Cursor and navigate to **Cursor Settings**.\n2. Under the **Features** tab, tap **+ Add new MCP server** under the **MCP Servers** section.\n3. Enter the following details:\n   - **Name**: Supabase\n   - **Type**: command\n   - **Command**: `npx tsx /path/to/mcp-server.ts`\n4. You should see a green active status after the server is successfully connected.\n\n### Claude Desktop\n\n1. Open Claude desktop and navigate to **Settings**.\n2. Under the **Developer** tab, tap **Edit Config** to open the configuration file.\n3. Add the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"npx\",\n         \"args\": [\"tsx\", \"/path/to/mcp-server.ts\"]\n       }\n     }\n   }\n   ```\n4. Save the configuration file and restart Claude desktop.\n\n## Available Tools\n\n### query_feature_suggestions\n\nQuery the feature_suggestions table in your Supabase database.\n\nParameters:\n\n- `limit` (number, optional): Maximum number of records to return (default: 100)\n\nExample usage in AI tool:\n\n```\nCan you show me feature suggestions from the database?\n```\n\nOr with a limit:\n\n```\nCan you show me the top 10 feature suggestions?\n```\n\n## Troubleshooting\n\n- If you encounter connection issues, make sure your Supabase credentials are correct.\n- Check the console output for any error messages.\n- Ensure that the feature_suggestions table exists in your Supabase database.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase database",
        "supabase efficiently",
        "directly supabase"
      ],
      "category": "databases"
    },
    "alexander-zuev--supabase-mcp-server": {
      "owner": "alexander-zuev",
      "name": "supabase-mcp-server",
      "url": "https://github.com/alexander-zuev/supabase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/alexander-zuev.webp",
      "description": "Manage Supabase databases, execute SQL queries, handle schema changes, and utilize the Supabase Management API and Auth Admin SDK with built-in safety controls.",
      "stars": 802,
      "forks": 98,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:50Z",
      "readme_content": "# Query | MCP server for Supabase\n\n> 🌅 More than 17k installs via pypi and close to 30k downloads on Smithery.ai — in short, this was fun! 🥳\n> Thanks to everyone who has been using this server for the past few months, and I hope it was useful for you.\n> Since Supabase has released their own [official MCP server](https://github.com/supabase-community/supabase-mcp),\n> I've decided to no longer actively maintain this one. The official MCP server is as feature-rich, and many more\n> features will be added in the future. Check it out!\n\n\n<p class=\"center-text\">\n  <strong>Query MCP is an open-source MCP server that lets your IDE safely run SQL, manage schema changes, call the Supabase Management API, and use Auth Admin SDK — all with built-in safety controls.</strong>\n</p>\n\n\n<p class=\"center-text\">\n  <a href=\"https://pypi.org/project/supabase-mcp-server/\"><img src=\"https://img.shields.io/pypi/v/supabase-mcp-server.svg\" alt=\"PyPI version\" /></a>\n  <a href=\"https://github.com/alexander-zuev/supabase-mcp-server/actions\"><img src=\"https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg\" alt=\"CI Status\" /></a>\n  <a href=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server\"><img src=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server/branch/main/graph/badge.svg\" alt=\"Code Coverage\" /></a>\n  <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.12%2B-blue.svg\" alt=\"Python 3.12+\" /></a>\n  <a href=\"https://github.com/astral-sh/uv\"><img src=\"https://img.shields.io/badge/uv-package%20manager-blueviolet\" alt=\"uv package manager\" /></a>\n  <a href=\"https://pepy.tech/project/supabase-mcp-server\"><img src=\"https://static.pepy.tech/badge/supabase-mcp-server\" alt=\"PyPI Downloads\" /></a>\n  <a href=\"https://smithery.ai/server/@alexander-zuev/supabase-mcp-server\"><img src=\"https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server\" alt=\"Smithery.ai Downloads\" /></a>\n  <a href=\"https://modelcontextprotocol.io/introduction\"><img src=\"https://img.shields.io/badge/MCP-Server-orange\" alt=\"MCP Server\" /></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n</p>    \n\n## Table of contents\n\n<p class=\"center-text\">\n  <a href=\"#getting-started\">Getting started</a> •\n  <a href=\"#feature-overview\">Feature overview</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#changelog\">Changelog</a>\n</p>\n\n## ✨ Key features\n- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol\n- 🔐 Control read-only and read-write modes of SQL query execution\n- 🔍 Runtime SQL query validation with risk level assessment\n- 🛡️ Three-tier safety system for SQL operations: safe, write, and destructive\n- 🔄 Robust transaction handling for both direct and pooled database connections\n- 📝 Automatic versioning of database schema changes\n- 💻 Manage your Supabase projects with Supabase Management API\n- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK\n- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively\n- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)\n\n\n## Getting Started\n\n### Prerequisites\nInstalling the server requires the following on your system:\n- Python 3.12+\n\nIf you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).\n\n### PostgreSQL Installation\nPostgreSQL installation is no longer required for the MCP server itself, as it now uses asyncpg which doesn't depend on PostgreSQL development libraries.\n\nHowever, you'll still need PostgreSQL if you're running a local Supabase instance:\n\n**MacOS**\n```bash\nbrew install postgresql@16\n```\n\n**Windows**\n  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/\n  - Ensure \"PostgreSQL Server\" and \"Command Line Tools\" are selected during installation\n\n### Step 1. Installation\n\nSince v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:\n\n```bash\n# if pipx is installed (recommended)\npipx install supabase-mcp-server\n\n# if uv is installed\nuv pip install supabase-mcp-server\n```\n\n`pipx` is recommended because it creates isolated environments for each package.\n\nYou can also install the server manually by cloning the repository and running `pipx install -e .` from the root directory.\n\n#### Installing from source\nIf you would like to install from source, for example for local development:\n```bash\nuv venv\n# On Mac\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n# Install package in editable mode\nuv pip install -e .\n```\n\n#### Installing via Smithery.ai\n\nYou can find the full instructions on how to use Smithery.ai to connect to this MCP server [here](https://smithery.ai/server/@alexander-zuev/supabase-mcp-server).\n\n\n### Step 2. Configuration\n\nThe Supabase MCP server requires configuration to connect to your Supabase database, access the Management API, and use the Auth Admin SDK. This section explains all available configuration options and how to set them up.\n\n> 🔑 **Important**: Since v0.4 MCP server requires an API key which you can get for free at [thequery.dev](https://thequery.dev) to use this MCP server.\n\n#### Environment Variables\n\nThe server uses the following environment variables:\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `SUPABASE_PROJECT_REF` | Yes | `127.0.0.1:54322` | Your Supabase project reference ID (or local host:port) |\n| `SUPABASE_DB_PASSWORD` | Yes | `postgres` | Your database password |\n| `SUPABASE_REGION` | Yes* | `us-east-1` | AWS region where your Supabase project is hosted |\n| `SUPABASE_ACCESS_TOKEN` | No | None | Personal access token for Supabase Management API |\n| `SUPABASE_SERVICE_ROLE_KEY` | No | None | Service role key for Auth Admin SDK |\n| `QUERY_API_KEY` | Yes | None | API key from thequery.dev (required for all operations) |\n\n> **Note**: The default values are configured for local Supabase development. For remote Supabase projects, you must provide your own values for `SUPABASE_PROJECT_REF` and `SUPABASE_DB_PASSWORD`.\n\n> 🚨 **CRITICAL CONFIGURATION NOTE**: For remote Supabase projects, you MUST specify the correct region where your project is hosted using `SUPABASE_REGION`. If you encounter a \"Tenant or user not found\" error, this is almost certainly because your region setting doesn't match your project's actual region. You can find your project's region in the Supabase dashboard under Project Settings.\n\n#### Connection Types\n\n##### Database Connection\n- The server connects to your Supabase PostgreSQL database using the transaction pooler endpoint\n- Local development uses a direct connection to `127.0.0.1:54322`\n- Remote projects use the format: `postgresql://postgres.[project_ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres`\n\n> ⚠️ **Important**: Session pooling connections are not supported. The server exclusively uses transaction pooling for better compatibility with the MCP server architecture.\n\n##### Management API Connection\n- Requires `SUPABASE_ACCESS_TOKEN` to be set\n- Connects to the Supabase Management API at `https://api.supabase.com`\n- Only works with remote Supabase projects (not local development)\n\n##### Auth Admin SDK Connection\n- Requires `SUPABASE_SERVICE_ROLE_KEY` to be set\n- For local development, connects to `http://127.0.0.1:54321`\n- For remote projects, connects to `https://[project_ref].supabase.co`\n\n#### Configuration Methods\n\nThe server looks for configuration in this order (highest to lowest priority):\n\n1. **Environment Variables**: Values set directly in your environment\n2. **Local `.env` File**: A `.env` file in your current working directory (only works when running from source)\n3. **Global Config File**:\n   - Windows: `%APPDATA%\\supabase-mcp\\.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n4. **Default Settings**: Local development defaults (if no other config is found)\n\n> ⚠️ **Important**: When using the package installed via pipx or uv, local `.env` files in your project directory are **not** detected. You must use either environment variables or the global config file.\n\n#### Setting Up Configuration\n\n##### Option 1: Client-Specific Configuration (Recommended)\n\nSet environment variables directly in your MCP client configuration (see client-specific setup instructions in Step 3). Most MCP clients support this approach, which keeps your configuration with your client settings.\n\n##### Option 2: Global Configuration\n\nCreate a global `.env` configuration file that will be used for all MCP server instances:\n\n```bash\n# Create config directory\n# On macOS/Linux\nmkdir -p ~/.config/supabase-mcp\n# On Windows (PowerShell)\nmkdir -Force \"$env:APPDATA\\supabase-mcp\"\n\n# Create and edit .env file\n# On macOS/Linux\nnano ~/.config/supabase-mcp/.env\n# On Windows (PowerShell)\nnotepad \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nAdd your configuration values to the file:\n\n```\nQUERY_API_KEY=your-api-key\nSUPABASE_PROJECT_REF=your-project-ref\nSUPABASE_DB_PASSWORD=your-db-password\nSUPABASE_REGION=us-east-1\nSUPABASE_ACCESS_TOKEN=your-access-token\nSUPABASE_SERVICE_ROLE_KEY=your-service-role-key\n```\n\n##### Option 3: Project-Specific Configuration (Source Installation Only)\n\nIf you're running the server from source (not via package), you can create a `.env` file in your project directory with the same format as above.\n\n#### Finding Your Supabase Project Information\n\n- **Project Reference**: Found in your Supabase project URL: `https://supabase.com/dashboard/project/<project-ref>`\n- **Database Password**: Set during project creation or found in Project Settings → Database\n- **Access Token**: Generate at https://supabase.com/dashboard/account/tokens\n- **Service Role Key**: Found in Project Settings → API → Project API keys\n\n#### Supported Regions\n\nThe server supports all Supabase regions:\n\n- `us-west-1` - West US (North California)\n- `us-east-1` - East US (North Virginia) - default\n- `us-east-2` - East US (Ohio)\n- `ca-central-1` - Canada (Central)\n- `eu-west-1` - West EU (Ireland)\n- `eu-west-2` - West Europe (London)\n- `eu-west-3` - West EU (Paris)\n- `eu-central-1` - Central EU (Frankfurt)\n- `eu-central-2` - Central Europe (Zurich)\n- `eu-north-1` - North EU (Stockholm)\n- `ap-south-1` - South Asia (Mumbai)\n- `ap-southeast-1` - Southeast Asia (Singapore)\n- `ap-northeast-1` - Northeast Asia (Tokyo)\n- `ap-northeast-2` - Northeast Asia (Seoul)\n- `ap-southeast-2` - Oceania (Sydney)\n- `sa-east-1` - South America (São Paulo)\n\n#### Limitations\n\n- **No Self-Hosted Support**: The server only supports official Supabase.com hosted projects and local development\n- **No Connection String Support**: Custom connection strings are not supported\n- **No Session Pooling**: Only transaction pooling is supported for database connections\n- **API and SDK Features**: Management API and Auth Admin SDK features only work with remote Supabase projects, not local development\n\n### Step 3. Usage\n\nIn general, any MCP client that supports `stdio` protocol should work with this MCP server. This server was explicitly tested to work with:\n- Cursor\n- Windsurf\n- Cline\n- Claude Desktop\n\nAdditionally, you can also use smithery.ai to install this server a number of clients, including the ones above.\n\nFollow the guides below to install this MCP server in your client.\n\n#### Cursor\nGo to Settings -> Features -> MCP Servers and add a new server with this configuration:\n```bash\n# can be set to any name\nname: supabase\ntype: command\n# if you installed with pipx\ncommand: supabase-mcp-server\n# if you installed with uv\ncommand: uv run supabase-mcp-server\n# if the above doesn't work, use the full path (recommended)\ncommand: /full/path/to/supabase-mcp-server  # Find with 'which supabase-mcp-server' (macOS/Linux) or 'where supabase-mcp-server' (Windows)\n```\n\nIf configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.\n![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)\n\n#### Windsurf\nGo to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n          \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n        }\n      }\n    }\n}\n```\nIf configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)\n\n#### Claude Desktop\nClaude Desktop also supports MCP servers through a JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Claude Desktop:\n   - Open Claude Desktop\n   - Go to Settings → Developer -> Edit Config MCP Servers\n   - Add a new configuration with the following JSON:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\n> ⚠️ **Important**: Unlike Windsurf and Cursor, Claude Desktop requires the **full absolute path** to the executable. Using just the command name (`supabase-mcp-server`) will result in a \"spawn ENOENT\" error.\n\nIf configuration is correct, you should see the Supabase MCP server listed as available in Claude Desktop.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/500bcd40-6245-40a7-b23b-189827ed2923)\n\n#### Cline\nCline also supports MCP servers through a similar JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Cline:\n   - Open Cline in VS Code\n   - Click on the \"MCP Servers\" tab in the Cline sidebar\n   - Click \"Configure MCP Servers\"\n   - This will open the `cline_mcp_settings.json` file\n   - Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\nIf configuration is correct, you should see a green indicator next to the Supabase MCP server in the Cline MCP Servers list, and a message confirming \"supabase MCP server connected\" at the bottom of the panel.\n\n![How successful configuration in Cline looks like](https://github.com/user-attachments/assets/6c4446ad-7a58-44c6-bf12-6c82222bbe59)\n\n### Troubleshooting\n\nHere are some tips & tricks that might help you:\n- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.\n- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.\n- **\"No tools found\" error** - If you see \"Client closed - no tools available\" in Cursor despite the package being installed:\n  - Find the full path to the executable by running `which supabase-mcp-server` (macOS/Linux) or `where supabase-mcp-server` (Windows)\n  - Use the full path in your MCP server configuration instead of just `supabase-mcp-server`\n  - For example: `/Users/username/.local/bin/supabase-mcp-server` or `C:\\Users\\username\\.local\\bin\\supabase-mcp-server.exe`\n- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\\supabase-mcp\\.env` on Windows).\n- **Accessing logs** - The MCP server writes detailed logs to a file:\n  - Log file location:\n    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`\n    - Windows: `%USERPROFILE%\\.local\\share\\supabase-mcp\\mcp_server.log`\n  - Logs include connection status, configuration details, and operation results\n  - View logs using any text editor or terminal commands:\n    ```bash\n    # On macOS/Linux\n    cat ~/.local/share/supabase-mcp/mcp_server.log\n\n    # On Windows (PowerShell)\n    Get-Content \"$env:USERPROFILE\\.local\\share\\supabase-mcp\\mcp_server.log\"\n    ```\n\nIf you are stuck or any of the instructions above are incorrect, please raise an issue.\n\n### MCP Inspector\nA super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.\n> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the coming release.\n\n## Feature Overview\n\n### Database query tools\n\nSince v0.3+ server provides comprehensive database management capabilities with built-in safety controls:\n\n- **SQL Query Execution**: Execute PostgreSQL queries with risk assessment\n  - **Three-tier safety system**:\n    - `safe`: Read-only operations (SELECT) - always allowed\n    - `write`: Data modifications (INSERT, UPDATE, DELETE) - require unsafe mode\n    - `destructive`: Schema changes (DROP, CREATE) - require unsafe mode + confirmation\n\n- **SQL Parsing and Validation**:\n  - Uses PostgreSQL's parser (pglast) for accurate analysis and provides clear feedback on safety requirements\n\n- **Automatic Migration Versioning**:\n  - Database-altering operations operations are automatically versioned\n  - Generates descriptive names based on operation type and target\n\n\n- **Safety Controls**:\n  - Default SAFE mode allows only read-only operations\n  - All statements run in transaction mode via `asyncpg`\n  - 2-step confirmation for high-risk operations\n\n- **Available Tools**:\n  - `get_schemas`: Lists schemas with sizes and table counts\n  - `get_tables`: Lists tables, foreign tables, and views with metadata\n  - `get_table_schema`: Gets detailed table structure (columns, keys, relationships)\n  - `execute_postgresql`: Executes SQL statements against your database\n  - `confirm_destructive_operation`: Executes high-risk operations after confirmation\n  - `retrieve_migrations`: Gets migrations with filtering and pagination options\n  - `live_dangerously`: Toggles between safe and unsafe modes\n\n### Management API tools\n\nSince v0.3.0 server provides secure access to the Supabase Management API with built-in safety controls:\n\n- **Available Tools**:\n  - `send_management_api_request`: Sends arbitrary requests to Supabase Management API with auto-injection of project ref\n  - `get_management_api_spec`: Gets the enriched API specification with safety information\n    - Supports multiple query modes: by domain, by specific path/method, or all paths\n    - Includes risk assessment information for each endpoint\n    - Provides detailed parameter requirements and response formats\n    - Helps LLMs understand the full capabilities of the Supabase Management API\n  - `get_management_api_safety_rules`: Gets all safety rules with human-readable explanations\n  - `live_dangerously`: Toggles between safe and unsafe operation modes\n\n- **Safety Controls**:\n  - Uses the same safety manager as database operations for consistent risk management\n  - Operations categorized by risk level:\n    - `safe`: Read-only operations (GET) - always allowed\n    - `unsafe`: State-changing operations (POST, PUT, PATCH, DELETE) - require unsafe mode\n    - `blocked`: Destructive operations (delete project, etc.) - never allowed\n  - Default safe mode prevents accidental state changes\n  - Path-based pattern matching for precise safety rules\n\n**Note**: Management API tools only work with remote Supabase instances and are not compatible with local Supabase development setups.\n\n### Auth Admin tools\n\nI was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.\n\nSince v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:\n  - Includes the following tools:\n    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods\n    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling\n  - Supported methods:\n    - `get_user_by_id`: Retrieve a user by their ID\n    - `list_users`: List all users with pagination\n    - `create_user`: Create a new user\n    - `delete_user`: Delete a user by their ID\n    - `invite_user_by_email`: Send an invite link to a user's email\n    - `generate_link`: Generate an email link for various authentication purposes\n    - `update_user_by_id`: Update user attributes by ID\n    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)\n\n#### Why use Auth Admin SDK instead of raw SQL queries?\n\nThe Auth Admin SDK provides several key advantages over direct SQL manipulation:\n- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)\n- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas\n- **Simplicity**: Offers clear methods with proper validation and error handling\n\n  - Response format:\n    - All methods return structured Python objects instead of raw dictionaries\n    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user[\"id\"]`)\n  - Edge cases and limitations:\n    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors\n    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project\n    - Link types: When generating links, different link types have different requirements:\n      - `signup` links don't require the user to exist\n      - `magiclink` and `recovery` links require the user to already exist in the system\n    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface\n    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK\n\n### Logs & Analytics\n\nThe server provides access to Supabase logs and analytics data, making it easier to monitor and troubleshoot your applications:\n\n- **Available Tool**: `retrieve_logs` - Access logs from any Supabase service\n\n- **Log Collections**:\n  - `postgres`: Database server logs\n  - `api_gateway`: API gateway requests\n  - `auth`: Authentication events\n  - `postgrest`: RESTful API service logs\n  - `pooler`: Connection pooling logs\n  - `storage`: Object storage operations\n  - `realtime`: WebSocket subscription logs\n  - `edge_functions`: Serverless function executions\n  - `cron`: Scheduled job logs\n  - `pgbouncer`: Connection pooler logs\n\n- **Features**: Filter by time, search text, apply field filters, or use custom SQL queries\n\nSimplifies debugging across your Supabase stack without switching between interfaces or writing complex queries.\n\n### Automatic Versioning of Database Changes\n\n\"With great power comes great responsibility.\" While `execute_postgresql` tool coupled with aptly named `live_dangerously` tool provide a powerful and simple way to manage your Supabase database, it also means that dropping a table or modifying one is one chat message away. In order to reduce the risk of irreversible changes, since v0.3.8 the server supports:\n- automatic creation of migration scripts for all write & destructive sql operations executed on the database\n- improved safety mode of query execution, in which all queries are categorized in:\n  - `safe` type: always allowed. Includes all read-only ops.\n  - `write`type: requires `write` mode to be enabled by the user.\n  - `destructive` type: requires `write` mode to be enabled by the user AND a 2-step confirmation of query execution for clients that do not execute tools automatically.\n\n### Universal Safety Mode\nSince v0.3.8 Safety Mode has been standardized across all services (database, API, SDK) using a universal safety manager. This provides consistent risk management and a unified interface for controlling safety settings across the entire MCP server.\n\nAll operations (SQL queries, API requests, SDK methods) are categorized into risk levels:\n- `Low` risk: Read-only operations that don't modify data or structure (SELECT queries, GET API requests)\n- `Medium` risk: Write operations that modify data but not structure (INSERT/UPDATE/DELETE, most POST/PUT API requests)\n- `High` risk: Destructive operations that modify database structure or could cause data loss (DROP/TRUNCATE, DELETE API endpoints)\n- `Extreme` risk: Operations with severe consequences that are blocked entirely (deleting projects)\n\nSafety controls are applied based on risk level:\n- Low risk operations are always allowed\n- Medium risk operations require unsafe mode to be enabled\n- High risk operations require unsafe mode AND explicit confirmation\n- Extreme risk operations are never allowed\n\n#### How confirmation flow works\n\nAny high-risk operations (be it a postgresql or api request) will be blocked even in `unsafe` mode.\n![Every high-risk operation is blocked](https://github.com/user-attachments/assets/c0df79c2-a879-4b1f-a39d-250f9965c36a)\nYou will have to confirm and approve every high-risk operation explicitly in order for it to be executed.\n![Explicit approval is always required](https://github.com/user-attachments/assets/5cd7a308-ec2a-414e-abe2-ff2f3836dd8b)\n\n\n## Changelog\n\n- 📦 Simplified installation via package manager - ✅ (v0.2.0)\n- 🌎 Support for different Supabase regions - ✅ (v0.2.2)\n- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)\n- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)\n- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)\n- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)\n- 🔍 Stronger SQL query validation ✅ (v0.3.8)\n- 📝 Automatic versioning of database changes ✅ (v0.3.8)\n- 📖 Radically improved knowledge and tools of api spec ✅ (v0.3.8)\n- ✍️ Improved consistency of migration-related tools for a more organized database vcs ✅ (v0.3.10)\n- 🥳 Query MCP is released (v0.4.0)\n\n\nFor a more detailed roadmap, please see this [discussion](https://github.com/alexander-zuev/supabase-mcp-server/discussions/46) on GitHub.\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)\n\n---\n\nEnjoy! ☺️\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase databases",
        "supabase management",
        "manage supabase"
      ],
      "category": "databases"
    },
    "alexanderzuev--supabase-mcp-server": {
      "owner": "alexanderzuev",
      "name": "supabase-mcp-server",
      "url": "https://github.com/alexander-zuev/supabase-mcp-server",
      "imageUrl": "",
      "description": "Supabase MCP Server with support for SQL query execution and database exploration tools",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "databases secure",
        "supabase mcp",
        "secure database"
      ],
      "category": "databases"
    },
    "aliyun--alibabacloud-adb-mysql-mcp-server": {
      "owner": "aliyun",
      "name": "alibabacloud-adb-mysql-mcp-server",
      "url": "https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/aliyun.webp",
      "description": "Facilitates communication between AI Agents and Adb MySQL databases, enabling retrieval of database metadata and execution of SQL operations for enhanced data access. Supports real-time interactions with AnalyticDB for MySQL databases via a universal interface.",
      "stars": 16,
      "forks": 12,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-02T14:18:09Z",
      "readme_content": "# AnalyticDB for MySQL MCP Server\n\nAnalyticDB for MySQL MCP Server serves as a universal interface between AI Agents and [AnalyticDB for MySQL](https://www.alibabacloud.com/en/product/analyticdb-for-mysql) databases. It enables seamless communication between AI Agents and AnalyticDB for MySQL, helping AI Agents\nretrieve AnalyticDB for MySQL database metadata and execute SQL operations.\n\n## 1. MCP Client Configuration\n\n### Mode 1: Using Local File\n\n- #### Download the GitHub repository\n\n```shell\ngit clone https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server\n```\n\n- #### MCP Integration\n\nAdd the following configuration to the MCP client configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"adb-mysql-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/alibabacloud-adb-mysql-mcp-server\",\n        \"run\",\n        \"adb-mysql-mcp-server\"\n      ],\n      \"env\": {\n        \"ADB_MYSQL_HOST\": \"host\",\n        \"ADB_MYSQL_PORT\": \"port\",\n        \"ADB_MYSQL_USER\": \"database_user\",\n        \"ADB_MYSQL_PASSWORD\": \"database_password\",\n        \"ADB_MYSQL_DATABASE\": \"database\"\n      }\n    }\n  }\n}\n```\n\n### Mode 2: Using PIP Mode\n\n- #### Installation\n\nInstall MCP Server using the following package:\n\n```bash\npip install adb-mysql-mcp-server\n```\n\n-  #### MCP Integration\n\nAdd the following configuration to the MCP client configuration file:\n\n```json\n {\n  \"mcpServers\": {\n    \"adb-mysql-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"adb-mysql-mcp-server\",\n        \"adb-mysql-mcp-server\"\n      ],\n      \"env\": {\n        \"ADB_MYSQL_HOST\": \"host\",\n        \"ADB_MYSQL_PORT\": \"port\",\n        \"ADB_MYSQL_USER\": \"database_user\",\n        \"ADB_MYSQL_PASSWORD\": \"database_password\",\n        \"ADB_MYSQL_DATABASE\": \"database\"\n      }\n    }\n  }\n}\n```\n\n## 2. Develop your own AnalyticDB for MySQL MCP server\n\nIf you want to develop your own AnalyticDB for MySQL MCP Server, you can install the python dependency packages using the following command:\n\n1. Download the [source code from GitHub](https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server).\n2. Install  [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager.\n3. Install [Node.js](https://nodejs.org/en/download) which provides a node package tool whose name is `npx`\n4. Install the python dependencies in the root diretory of the project using the following command:\n\n```shell\nuv pip install -r pyproject.toml \n```\n\n5. If you want to debug the mcp server locally, you could start up an [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) using the following command:\n\n```shell\nnpx @modelcontextprotocol/inspector  \\\n-e ADB_MYSQL_HOST=your_host \\\n-e ADB_MYSQL_PORT=your_port \\\n-e ADB_MYSQL_USER=your_username \\\n-e ADB_MYSQL_PASSWORD=your_password \\\n-e ADB_MYSQL_DATABASE=your_database \\\nuv --directory /path/to/alibabacloud-adb-mysql-mcp-server run adb-mysql-mcp-server \n```\n\n## 3. Introduction to the components of AnalyticDB for MySQL MCP Server\n\n- ### Tools\n\n    - `execute_sql`: Execute a SQL query in the AnalyticDB for MySQL Cluster\n\n    - `get_query_plan`: Get the query plan for a SQL query\n\n    - `get_execution_plan`: Get the actual execution plan with runtime statistics for a SQL query\n\n- ### Resources\n\n    - #### Built-in Resources\n\n        - `adbmysql:///databases`: Get all the databases in the analytic for mysql cluster\n\n    - #### Resource Templates\n\n        - `adbmysql:///{schema}/tables`: Get all the tables in a specific database\n\n        - `adbmysql:///{database}/{table}/ddl`: Get the DDL script of a table in a specific database\n\n        - `adbmysql:///{config}/{key}/value`: Get the value for a config key in the cluster\n\n- ### Prompts\n\nNot provided at the present moment.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "analyticdb",
        "database access",
        "adb mysql",
        "secure database"
      ],
      "category": "databases"
    },
    "aliyun--alibabacloud-hologres-mcp-server": {
      "owner": "aliyun",
      "name": "alibabacloud-hologres-mcp-server",
      "url": "https://github.com/aliyun/alibabacloud-hologres-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/aliyun.webp",
      "description": "Facilitates communication between AI Agents and Hologres databases, enabling retrieval of database metadata and execution of SQL operations.",
      "stars": 23,
      "forks": 9,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-02T06:27:00Z",
      "readme_content": "English | [中文](README_ZH.md)\n\n# Hologres MCP Server\n\nHologres MCP Server serves as a universal interface between AI Agents and Hologres databases. It enables seamless communication between AI Agents and Hologres, helping AI Agents retrieve Hologres database metadata and execute SQL operations.\n\n## Configuration\n\n### Mode 1: Using Local File\n\n#### Download\n\nDownload from Github\n\n```bash\ngit clone https://github.com/aliyun/alibabacloud-hologres-mcp-server.git\n```\n\n#### MCP Integration\n\nAdd the following configuration to the MCP client configuration file:\n\n```json\n{\n    \"mcpServers\": {\n        \"hologres-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/alibabacloud-hologres-mcp-server\",\n                \"run\",\n                \"hologres-mcp-server\"\n            ],\n            \"env\": {\n                \"HOLOGRES_HOST\": \"host\",\n                \"HOLOGRES_PORT\": \"port\",\n                \"HOLOGRES_USER\": \"access_id\",\n                \"HOLOGRES_PASSWORD\": \"access_key\",\n                \"HOLOGRES_DATABASE\": \"database\"\n            }\n        }\n    }\n}\n```\n\n### Mode 2: Using PIP Mode\n\n#### Installation\n\nInstall MCP Server using the following package:\n\n```bash\npip install hologres-mcp-server\n```\n\n#### MCP Integration\n\nAdd the following configuration to the MCP client configuration file:\n\nUse uv mode\n\n```json\n{\n    \"mcpServers\": {\n        \"hologres-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"run\",\n                \"--with\",\n                \"hologres-mcp-server\",\n                \"hologres-mcp-server\"\n            ],\n            \"env\": {\n                \"HOLOGRES_HOST\": \"host\",\n                \"HOLOGRES_PORT\": \"port\",\n                \"HOLOGRES_USER\": \"access_id\",\n                \"HOLOGRES_PASSWORD\": \"access_key\",\n                \"HOLOGRES_DATABASE\": \"database\"\n            }\n        }\n    }\n}\n```\nUse uvx mode\n\n```json\n{\n    \"mcpServers\": {\n        \"hologres-mcp-server\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"hologres-mcp-server\"\n            ],\n            \"env\": {\n                \"HOLOGRES_HOST\": \"host\",\n                \"HOLOGRES_PORT\": \"port\",\n                \"HOLOGRES_USER\": \"access_id\",\n                \"HOLOGRES_PASSWORD\": \"access_key\",\n                \"HOLOGRES_DATABASE\": \"database\"\n            }\n        }\n    }\n}\n```\n\n## Components\n\n### Tools\n\n* `execute_hg_select_sql`: Execute a SELECT SQL query in Hologres database\n* `execute_hg_select_sql_with_serverless`: Execute a SELECT SQL query in Hologres database with serverless computing\n* `execute_hg_dml_sql`: Execute a DML (INSERT, UPDATE, DELETE) SQL query in Hologres database\n* `execute_hg_ddl_sql`: Execute a DDL (CREATE, ALTER, DROP, COMMENT ON) SQL query in Hologres database\n* `gather_hg_table_statistics`: Collect table statistics in Hologres database\n* `get_hg_query_plan`: Get query plan in Hologres database\n* `get_hg_execution_plan`: Get execution plan in Hologres database\n* `call_hg_procedure`: Invoke a procedure in Hologres database\n* `create_hg_maxcompute_foreign_table`: Create MaxCompute foreign tables in Hologres database.\n\nSince some Agents do not support resources and resource templates, the following tools are provided to obtain the metadata of schemas, tables, views, and external tables.\n* `list_hg_schemas`: Lists all schemas in the current Hologres database, excluding system schemas.\n* `list_hg_tables_in_a_schema`: Lists all tables in a specific schema, including their types (table, view, external table, partitioned table).\n* `show_hg_table_ddl`: Show the DDL script of a table, view, or external table in the Hologres database.\n\n### Resources\n\n#### Built-in Resources\n\n* `hologres:///schemas`: Get all schemas in Hologres database\n\n#### Resource Templates\n\n* `hologres:///{schema}/tables`: List all tables in a schema in Hologres database\n* `hologres:///{schema}/{table}/partitions`: List all partitions of a partitioned table in Hologres database\n* `hologres:///{schema}/{table}/ddl`: Get table DDL in Hologres database\n* `hologres:///{schema}/{table}/statistic`: Show collected table statistics in Hologres database\n* `system:///{+system_path}`:\n  System paths include:\n\n  * `hg_instance_version` - Shows the hologres instance version.\n  * `guc_value/<guc_name>` - Shows the guc (Grand Unified Configuration) value.\n  * `missing_stats_tables` - Shows the tables that are missing statistics.\n  * `stat_activity` - Shows the information of current running queries.\n  * `query_log/latest/<row_limits>` - Get recent query log history with specified number of rows.\n  * `query_log/user/<user_name>/<row_limits>` - Get query log history for a specific user with row limits.\n  * `query_log/application/<application_name>/<row_limits>` - Get query log history for a specific application with row limits.\n  * `query_log/failed/<interval>/<row_limits>` - Get failed query log history with interval and specified number of rows.\n\n### Prompts\n\nNone at this time\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "security",
        "hologres databases",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "aliyun--alibabacloud-tablestore-mcp-server": {
      "owner": "aliyun",
      "name": "alibabacloud-tablestore-mcp-server",
      "url": "https://github.com/aliyun/alibabacloud-tablestore-mcp-server",
      "imageUrl": "",
      "description": "MCP service for Tablestore, features include adding documents, semantic search for documents based on vectors and scalars, RAG-friendly, and serverless.",
      "stars": 149,
      "forks": 37,
      "license": "Apache License 2.0",
      "language": "Java",
      "updated_at": "2025-09-25T17:36:30Z",
      "readme_content": "# [Tablestore](https://www.aliyun.com/product/ots) MCP servers.\n\n## 实现列表\n\n## 1. Java\n1. [入门示例: tablestore-java-mcp-server](https://github.com/aliyun/alibabacloud-tablestore-mcp-server/blob/master/tablestore-java-mcp-server/README.md)\n2. [基于 MCP 架构实现知识库答疑系统: tablestore-java-mcp-server-rag](https://github.com/aliyun/alibabacloud-tablestore-mcp-server/blob/master/tablestore-java-mcp-server-rag/README.md)\n   - 实现一个目前最常见的一类 AI 应用即答疑系统，支持基于私有知识库的问答，会对知识库构建和 RAG 做一些优化。\n\n## 2. Python \n1. [入门示例: tablestore-python-mcp-server](https://github.com/aliyun/alibabacloud-tablestore-mcp-server/blob/master/tablestore-python-mcp-server/README.md)\n\n\n## 技术支持\n\n欢迎加入我们的钉钉公开群，与我们一起探讨 AI 技术。钉钉群号：36165029092",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tablestore",
        "databases",
        "database",
        "tablestore mcp",
        "alibabacloud tablestore",
        "tablestore features"
      ],
      "category": "databases"
    },
    "amineelkouhen--mcp-cockroachdb": {
      "owner": "amineelkouhen",
      "name": "mcp-cockroachdb",
      "url": "https://github.com/amineelkouhen/mcp-cockroachdb",
      "imageUrl": "",
      "description": "A Model Context Protocol server for managing, monitoring, and querying data in [CockroachDB](https://cockroachlabs.com).",
      "stars": 5,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T00:11:24Z",
      "readme_content": "# CockroachDB MCP Server\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.13%2B-blue)](https://www.python.org/downloads/)\n[![smithery badge](https://smithery.ai/badge/@amineelkouhen/mcp-cockroachdb)](https://smithery.ai/server/@amineelkouhen/mcp-cockroachdb)\n[![MCP Compatible](https://img.shields.io/badge/MCP-compatible-blue)](https://mcp.so/server/cockroachdb-mcp-server/cockroachdb)\n\n## Overview\n\nThe CockroachDB MCP Server is a **natural language interface** designed for LLMs and agentic applications to manage, monitor, and query data in CockroachDB. It integrates seamlessly with **MCP (Model Content Protocol)** clients, such as Claude Desktop or Cursor, enabling AI-driven workflows to interact directly with your database. \n\n## Table of Contents\n- [Overview](#overview)\n- [Features](#features)\n- [Tools](#tools)\n  - [Cluster Monitoring](#cluster-monitoring)\n  - [Database Operations](#database-operations)\n  - [Table Management](#table-management)\n  - [Query Engine](#query-engine)\n- [Installation](#installation)\n  - [Quick Start with uvx](#quick-start-with-uvx)\n  - [Development Installation](#development-installation)\n  - [With Docker](#with-docker)\n- [Configuration](#configuration)\n  - [Configuration via command line arguments](#configuration-via-command-line-arguments)\n  - [Configuration via Environment Variables](#configuration-via-environment-variables)\n- [Integrations](#integrations)\n  - [OpenAI Agents SDK](#openai-agents-sdk)\n  - [Augment](#augment)\n  - [Claude Desktop](#claude-desktop)\n  - [VS Code with GitHub Copilot](#vs-code-with-github-copilot)\n  - [Cursor](#cursor)\n- [Testing](#testing)\n- [Contributing](#contributing)\n- [License](#license)\n- [Quality Badge](#quality-badge)\n- [Contact](#contact)\n\n## Features\n- **Natural Language Queries**: Enables AI agents to query and create transactions using natural language, supporting complex workflows.\n- **Search & Filtering**: Supports efficient data retrieval and searching in CockroachDB.\n- **Cluster Monitoring**: Check and monitor the CockroachDB cluster status, including node health and replication.\n- **Database Operations**: Perform all operations related to databases, such as creation, deletion, and configuration.\n- **Table Management**: Handle tables, indexes, and schemas for flexible data modeling.\n- **Seamless MCP Integration**: Works with any **MCP client** for smooth communication.\n- **Scalable & Lightweight**: Designed for **high-performance** data operations.\n\n## Tools\n\nThe CockroachDB MCP Server Server provides tools to manage the data stored in CockroachDB. \n\n![architecture](https://github.com/user-attachments/assets/36a121d9-48b7-4840-9317-002a38441b8d)\n\nThe tools are organized into four main categories:\n\n### Cluster Monitoring\n\nPurpose:\nProvides tools for monitoring and managing CockroachDB clusters.\n\nSummary:\n- Get cluster health and node status.\n- Show currently running queries.\n- Analyze query performance statistics.\n- Retrieve replication and distribution status for tables or the whole database.\n\n### Database Operations\n\nPurpose:\nHandles database-level operations and connection management.\n\nSummary:\n- Connect to a CockroachDB database.\n- List, create, drop, and switch databases.\n- Get connection status and active sessions.\n- Retrieve database settings.\n\n### Table Management\n\nPurpose:\nProvides tools for managing tables, indexes, views, and schema relationships in CockroachDB.\n\nSummary:\n- Create, drop, and describe tables and views.\n- Bulk import data into tables.\n- Manage indexes (create/drop).\n- List tables, views, and table relationships.\n- Analyze schema structure and metadata.\n\n### Query Engine\n\nPurpose:\nExecutes and manages SQL queries and transactions.\n\nSummary:\n- Execute SQL queries with formatting options (JSON, CSV, table).\n- Run multi-statement transactions.\n- Explain query plans for optimization.\n- Track and retrieve query history.\n\n## Installation\n\nThe CockroachDB MCP Server supports the `stdio` [transport](https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio). Support for the `streamable-http` transport will be added in a future release.\n\n### Quick Start with uvx \n\nThe easiest way to use the CockroachDB MCP Server is with `uvx`, which allows you to run it directly from GitHub (from a branch, or use a tagged release). It is recommended to use a tagged release. The `main` branch is under active development and may contain breaking changes. As an example, you can execute the following command to run the `0.1.0` release:\n\n```commandline\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git@0.1.0 cockroachdb-mcp-server --url postgresql://localhost:26257/defaultdb\n```\n\nCheck the release notes for the latest version in the [Releases](https://github.com/amineelkouhen/mcp-cockroachdb/releases) section.\nAdditional examples are provided below.\n\n```sh\n# Run with CockroachDB URI\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server --url postgresql://localhost:26257/defaultdb\n\n# Run with individual parameters\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server --host localhost --port 26257 --database defaultdb --user root --password mypassword\n\n# See all options\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server --help\n```\n\n### Development Installation\n\nFor development or if you prefer to clone the repository:\n\n```sh\n# Clone the repository\ngit clone https://github.com/amineelkouhen/mcp-cockroachdb.git\ncd mcp-cockroachdb\n\n# Install dependencies using uv\nuv venv\nsource .venv/bin/activate\nuv sync\n\n# Run with CLI interface\nuv run cockroachdb-mcp-server --help\n\n# Or run the main file directly (uses environment variables)\nuv run src/main.py\n```\n\nOnce you cloned the repository, installed the dependencies and verified you can run the server, you can configure Claude Desktop or any other MCP Client to use this MCP Server running the main file directly (it uses environment variables). This is usually preferred for development.\nThe following example is for Claude Desktop, but the same applies to any other MCP Client.\n\n1. Specify your CockroachDB credentials and TLS configuration\n2. Retrieve your `uv` command full path (e.g. `which uv`)\n3. Edit the `claude_desktop_config.json` configuration file\n   - on a MacOS, at `~/Library/Application Support/Claude/`\n\n```json\n{\n    \"mcpServers\": {\n        \"cockroach\": {\n            \"command\": \"<full_path_uv_command>\",\n            \"args\": [\n                \"--directory\",\n                \"<your_mcp_server_directory>\",\n                \"run\",\n                \"src/main.py\"\n            ],\n            \"env\": {\n                \"CRDB_HOST\": \"<your_cockroachdb_hostname>\",\n                \"CRDB_PORT\": \"<your_cockroachdb_port>\",\n                \"CRDB_DATABASE\": \"<your_cockroach_database>\",\n                \"CRDB_USERNAME\": \"<your_cockroachdb_user>\",\n                \"CRDB_PWD\": \"<your_cockroachdb_password>\",\n                \"CRDB_SSL_MODE\": \"disable|allow|prefer|require|verify-ca|verify-full\",\n                \"CRDB_SSL_CA_PATH\": \"<your_cockroachdb_ca_path>\",\n                \"CRDB_SSL_KEYFILE\": \"<your_cockroachdb_keyfile_path>\",\n                \"CRDB_SSL_CERTFILE\": \"<your_cockroachdb_certificate_path>\",\n            }\n        }\n    }\n}\n```\n\nYou can troubleshoot problems by tailing the log file.\n\n```commandline\ntail -f ~/Library/Logs/Claude/mcp-server-cockroach.log\n```\n\n### With Docker\n\nYou can use a dockerized deployment of this server. You can either build your image or use the official [CockroachDB MCP Docker](https://hub.docker.com/r/mcp/cockroachdb) image.\n\nIf you'd like to build your image, the CockroachDB MCP Server provides a Dockerfile. Build this server's image with:\n\n```commandline\ndocker build -t mcp-cockroachdb .\n```\n\nFinally, configure the client to create the container at start-up. An example for Claude Desktop is provided below. Edit the `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"cockroach\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\",\n                \"--rm\",\n                \"--name\",\n                \"cockroachdb-mcp-server\",\n                \"-e\", \"CRDB_HOST=<cockroachdb_host>\",\n                \"-e\", \"CRDB_PORT=<cockroachdb_port>\",\n                \"-e\", \"CRDB_DATABASE=<cockroachdb_database>\",\n                \"-e\", \"CRDB_USERNAME=<cockroachdb_user>\",\n                \"mcp-cockroachdb\"]\n    }\n  }\n}\n```\n\nTo use the [CockroachDB MCP Docker](https://hub.docker.com/mcp/server/cockroachdb) image, just replace your image name (`mcp-cockroachdb` in the example above) with `mcp/cockroachdb`.\n\n## Configuration\n\nThe CockroachDB MCP Server can be configured in two ways: either via command-line arguments or via environment variables.\nThe precedence is: CLI arguments > environment variables > default values.\n\n### Configuration via command line arguments\n\nWhen using the CLI interface, you can configure the server with command line arguments:\n\n```sh\n# Basic CockroachDB connection\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server \\\n  --host localhost \\\n  --port 26257 \\\n  --db defaultdb \\\n  --user root \\\n  --password mypassword\n\n# Using CockroachDB URI (simpler)\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server \\\n  --url postgresql://root@localhost:26257/defaultdb\n\n# SSL connection\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server \\\n  --url postgresql://user:pass@cockroach.example.com:26257/defaultdb?sslmode=verify-full&sslrootcert=path/to/ca.crt&sslcert=path/to/client.username.crt&sslkey=path/to/client.username.key\n\n# See all available options\nuvx --from git+https://github.com/amineelkouhen/mcp-cockroachdb.git cockroachdb-mcp-server --help\n```\n\n**Available CLI Options:**\n- `--url` - CockroachDB connection URI (postgresql://user:pass@host:port/db)\n- `--host` - CockroachDB hostname \n- `--port` - CockroachDB port (default: 26257)\n- `--db` - CockroachDB database name (default: defaultdb)\n- `--user` - CockroachDB username\n- `--password` - CockroachDB password\n- `--ssl-mode` - SSL mode - Possible values: require, verify-ca, verify-full, disable (default)\n- `--ssl-key` - Path to SSL Client key file\n- `--ssl-cert` - Path to SSL Client certificate file\n- `--ssl-ca-cert` - Path to CA (Root) certificate file'\n\n### Configuration via Environment Variables\n\nIf desired, you can use environment variables. Defaults are provided for all variables.\n\n| Name                 | Description                                                                    | Default Value    |\n|----------------------|--------------------------------------------------------------------------------|------------------|\n| `CRDB_HOST`          | The host name or address of a CockroachDB node or load balancer.               | 127.0.0.1        |\n| `CRDB_PORT`          | The port number of the SQL interface of the CockroachDB node or load balancer. | 26257            |\n| `CRDB_DATABASE`      | A database name to use as the current database.                                | defaultdb        |\n| `CRDB_USERNAME`      | The SQL user that will own the client session.                                 | root             |\n| `CRDB_PWD`           | The user's password.                                                           | None             |\n| `CRDB_SSL_MODE`      | Which type of secure connection to use.                                        | disable          |\n| `CRDB_SSL_CA_PATH`   | Path to the CA certificate, when sslmode is not `disable`.                     | None             |\n| `CRDB_SSL_CERTFILE`  | Path to the client certificate, when sslmode is not `disable`.                 | None             |\n| `CRDB_SSL_KEYFILE`   | Path to the client private key, when sslmode is not `disable`.                 | None             |\n\nThere are several ways to set environment variables:\n\n1. **Using a `.env` File**:  \nPlace a `.env` file in your project directory with key-value pairs for each environment variable. Tools like `python-dotenv`, `pipenv`, and `uv` can automatically load these variables when running your application. This is a convenient and secure way to manage configuration, as it keeps sensitive data out of your shell history and version control (if `.env` is in `.gitignore`).\nFor example, create a `.env` file with the following content from the `.env.example` file provided in the repository:\n\n```bash\ncp .env.example .env\n```\n\nThen edit the `.env` file to set your CockroachDB configuration:\n\nOR,\n\n2. **Setting Variables in the Shell**:  \nYou can export environment variables directly in your shell before running your application. For example:\n\n```sh\nexport CRDB_URL= postgresql://root@127.0.0.1:26257/defaultdb\n```\n\nThis method is helpful for temporary overrides or quick testing.\n\n## Integrations\n\nIntegrating this MCP Server with development frameworks like OpenAI Agents SDK or using tools like Claude Desktop, VS Code, or Augment is described in the following sections.\n\n### OpenAI Agents SDK\n\nIntegrate this MCP Server with the OpenAI Agents SDK. Read the [documents](https://openai.github.io/openai-agents-python/mcp/) to learn more about the integration of the SDK with MCP.\n\nInstall the Python SDK.\n\n```commandline\npip install openai-agents\n```\n\nConfigure the OpenAI token:\n\n```commandline\nexport OPENAI_API_KEY=\"<openai_token>\"\n```\n\nAnd run the [application](./examples/cockroachdb_assistant.py).\n\n```commandline\npython3 examples/cockroachdb_assistant.py\n```\n\nYou can troubleshoot your agent workflows using the [OpenAI dashboard](https://platform.openai.com/traces/).\n\n### Augment\n\nYou can configure the CockroachDB MCP Server in Augment by importing the server via JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"CockroachDB MCP Server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/cockroachdb/mcp-cockroachdb.git\",\n        \"cockroachdb-mcp-server\",\n        \"--url\",\n        \"postgresql://root@localhost:26257/defaultdb\"\n      ]\n    }\n  }\n}\n```\n\n### Claude Desktop\n\nThe simplest way to configure MCP clients is using `uvx`. Add the following JSON to your `claude_desktop_config.json`, remember to provide the full path to `uvx`.\n\n```json\n{\n    \"mcpServers\": {\n        \"cockroach-mcp-server\": {\n            \"type\": \"stdio\",\n            \"command\": \"/opt/homebrew/bin/uvx\",\n            \"args\": [\n                \"--from\", \"git+https://github.com/amineelkouhen/mcp-cockroachdb.git\",\n                \"cockroachdb-mcp-server\",\n                \"--url\", \"postgresql://localhost:26257/defaultdb\"\n            ]\n        }\n    }\n}\n```\n\nIf you'd like to test the [CockroachDB MCP Server](https://smithery.ai/server/@amineelkouhen/mcp-cockroachdb) via Smithery, you can configure Claude Desktop automatically:\n\n```bash\nnpx -y @smithery/cli install @amineelkouhen/mcp-cockroachdb --client claude\n```\n\nPlease follow the prompt and give the details to configure the server and connect to CockroachDB (e.g., using a managed CockroachDB instance).\nThe procedure will create the proper configuration in the `claude_desktop_config.json` configuration file.\n\n### VS Code with GitHub Copilot\n\nTo use the CockroachDB MCP Server with VS Code, you must enable the [agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode) tools. Add the following to your `settings.json`:\n\n```json\n{\n  \"chat.agent.enabled\": true\n}\n```\n\nYou can start the GitHub desired version of the CockroachDB MCP server using `uvx` by adding the following JSON to your `settings.json`:\n\n```json\n\"mcp\": {\n    \"servers\": {\n        \"CockroachDB MCP Server\": {\n        \"type\": \"stdio\",\n        \"command\": \"uvx\", \n        \"args\": [\n            \"--from\", \"git+https://github.com/amineelkouhen/mcp-cockroachdb.git\",\n            \"cockroachdb-mcp-server\",\n            \"--url\", \"postgresql://root@localhost:26257/defaultdb\"\n        ]\n        },\n    }\n},\n```\n\nAlternatively, you can start the server using `uv` and configure your `mcp.json` or `settings.json`. This is usually desired for development.\n\n```json\n{\n  \"servers\": {\n    \"cockroach\": {\n      \"type\": \"stdio\",\n      \"command\": \"<full_path_uv_command>\",\n      \"args\": [\n        \"--directory\",\n        \"<your_mcp_server_directory>\",\n        \"run\",\n        \"src/main.py\"\n      ],\n      \"env\": {\n        \"CRDB_HOST\": \"<your_cockroachdb_hostname>\",\n        \"CRDB_PORT\": \"<your_cockroachdb_port>\",\n        \"CRDB_DATABASE\": \"<your_cockroach_database>\",\n        \"CRDB_USERNAME\": \"<your_cockroachdb_user>\",\n        \"CRDB_PWD\": \"<your_cockroachdb_password>\"\n      }\n    }\n  }\n}\n```\n\nFor more information, see the [VS Code documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n### Cursor\n\nRead the configuration options [here](#configuration-via-environment-variables) and input your selections with this link:\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=cockroachdb&config=JTdCJTIyY29tbWFuZCUyMiUzQSUyMmRvY2tlciUyMHJ1biUyMC1pJTIwLS1ybSUyMC1lJTIwQ1JEQl9IT1NUJTIwLWUlMjBDUkRCX1BPUlQlMjAtZSUyMENSREJfREFUQUJBU0UlMjAtZSUyMENSREJfVVNFUk5BTUUlMjAtZSUyMENSREJfU1NMX01PREUlMjAtZSUyMENSREJfU1NMX0NBX1BBVEglMjAtZSUyMENSREJfU1NMX0tFWUZJTEUlMjAtZSUyMENSREJfU1NMX0NFUlRGSUxFJTIwLWUlMjBDUkRCX1BXRCUyMG1jcCUyRmNvY2tyb2FjaGRiJTIyJTJDJTIyZW52JTIyJTNBJTdCJTIyQ1JEQl9IT1NUJTIyJTNBJTIyMTI3LjAuMC4xJTIyJTJDJTIyQ1JEQl9QT1JUJTIyJTNBJTIyMjYyNTclMjIlMkMlMjJDUkRCX0RBVEFCQVNFJTIyJTNBJTIyZGVmYXVsdGRiJTIyJTJDJTIyQ1JEQl9VU0VSTkFNRSUyMiUzQSUyMnJvb3QlMjIlMkMlMjJDUkRCX1NTTF9NT0RFJTIyJTNBJTIyZGlzYWJsZSUyMiUyQyUyMkNSREJfU1NMX0NBX1BBVEglMjIlM0ElMjIlMjIlMkMlMjJDUkRCX1NTTF9LRVlGSUxFJTIyJTNBJTIyJTIyJTJDJTIyQ1JEQl9TU0xfQ0VSVEZJTEUlMjIlM0ElMjIlMjIlMkMlMjJDUkRCX1BXRCUyMiUzQSUyMiUyMiU3RCU3RA%3D%3D)\n\n## Testing\n\nYou can use the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for visual debugging of this MCP Server.\n\n```sh\nnpx @modelcontextprotocol/inspector uv run src/main.py\n```\n\n## Contributing\n1. Fork the repository\n2. Create a new branch (`feature-branch`)\n3. Commit your changes\n4. Push to your branch and submit a pull request.\n\n## License\nThis project is licensed under the **MIT License**.\n\n## Quality Badge\n\n<a href=\"https://glama.ai/mcp/servers/@amineelkouhen/mcp-cockroach\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@amineelkouhen/mcp-cockroach/badge\" />\n</a>\n\n## Contact\nIf you have any questions or need support, please feel free to contact us through [GitHub Issues](https://github.com/amineelkouhen/mcp-cockroachdb/issues).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cockroachdb",
        "databases",
        "database",
        "data cockroachdb",
        "mcp cockroachdb",
        "cockroachdb model"
      ],
      "category": "databases"
    },
    "amornpan--py-mcp-mssql": {
      "owner": "amornpan",
      "name": "py-mcp-mssql",
      "url": "https://github.com/amornpan/py-mcp-mssql",
      "imageUrl": "/freedevtools/mcp/pfp/amornpan.webp",
      "description": "Provides access to Microsoft SQL Server databases through a standardized Model Context Protocol interface, enabling inspection of table schemas and execution of SQL queries.",
      "stars": 23,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T13:20:00Z",
      "readme_content": "# Python MSSQL MCP Server\n\n[![Version](https://img.shields.io/badge/version-1.0.1-blue.svg)](https://github.com/amornpan/py-mcp-mssql)\n[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org)\n[![MCP](https://img.shields.io/badge/MCP-1.2.0-green.svg)](https://github.com/modelcontextprotocol)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-teal.svg)](https://fastapi.tiangolo.com)\n[![License](https://img.shields.io/badge/license-MIT-yellow.svg)](LICENSE)\n\nA Model Context Protocol server implementation in Python that provides access to Microsoft SQL Server databases. This server enables Language Models to inspect table schemas and execute SQL queries through a standardized interface.\n\n## Features\n\n### Core Functionality\n* Asynchronous operation using Python's `asyncio`\n* Environment-based configuration using `python-dotenv`\n* Comprehensive logging system\n* Connection pooling and management via pyodbc\n* Error handling and recovery\n* FastAPI integration for API endpoints\n* Pydantic models for data validation\n* MSSQL connection handling with ODBC Driver\n\n## Prerequisites\n\n* Python 3.x\n* Required Python packages:\n  * pyodbc\n  * pydantic\n  * python-dotenv\n  * mcp-server\n* ODBC Driver 17 for SQL Server\n\n## Installation\n\n```bash\ngit clone https://github.com/amornpan/py-mcp-mssql.git\ncd py-mcp-mssql\npip install -r requirements.txt\n```\n\n## Screenshots\n\n\n\nThe screenshot above demonstrates the server being used with Claude to analyze and visualize SQL data.\n\n## Project Structure\n\n```\nPY-MCP-MSSQL/\n├── src/\n│   └── mssql/\n│       ├── __init__.py\n│       └── server.py\n├── tests/\n│   ├── __init__.py\n│   ├── test_mssql.py\n│   └── test_packages.py\n├── .env\n├── .env.example\n├── .gitignore\n├── README.md\n└── requirements.txt\n```\n\n### Directory Structure Explanation\n* `src/mssql/` - Main source code directory\n  * `__init__.py` - Package initialization\n  * `server.py` - Main server implementation\n* `tests/` - Test files directory\n  * `__init__.py` - Test package initialization\n  * `test_mssql.py` - MSSQL functionality tests\n  * `test_packages.py` - Package dependency tests\n* `.env` - Environment configuration file (not in git)\n* `.env.example` - Example environment configuration\n* `.gitignore` - Git ignore rules\n* `README.md` - Project documentation\n* `requirements.txt` - Project dependencies\n\n## Configuration\n\nCreate a `.env` file in the project root:\n\n```env\nMSSQL_SERVER=your_server\nMSSQL_DATABASE=your_database\nMSSQL_USER=your_username\nMSSQL_PASSWORD=your_password\nMSSQL_DRIVER={ODBC Driver 17 for SQL Server}\n```\n\n## API Implementation Details\n\n### Resource Listing\n```python\n@app.list_resources()\nasync def list_resources() -> list[Resource]\n```\n* Lists all available tables in the database\n* Returns table names with URIs in the format `mssql://<table_name>/data`\n* Includes table descriptions and MIME types\n\n### Resource Reading\n```python\n@app.read_resource()\nasync def read_resource(uri: AnyUrl) -> str\n```\n* Reads data from specified table\n* Accepts URIs in the format `mssql://<table_name>/data`\n* Returns first 100 rows in CSV format\n* Includes column headers\n\n### SQL Execution\n```python\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]\n```\n* Executes SQL queries\n* Supports both SELECT and modification queries\n* Returns results in CSV format for SELECT queries\n* Returns affected row count for modification queries\n\n## Usage with Claude Desktop\n\nAdd to your Claude Desktop configuration:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"server.py\"\n      ],\n      \"env\": {\n        \"MSSQL_SERVER\": \"your_server\",\n        \"MSSQL_DATABASE\": \"your_database\",\n        \"MSSQL_USER\": \"your_username\",\n        \"MSSQL_PASSWORD\": \"your_password\",\n        \"MSSQL_DRIVER\": \"{ODBC Driver 17 for SQL Server}\"\n      }\n    }\n  }\n}\n```\n\n## Error Handling\n\nThe server implements comprehensive error handling for:\n* Database connection failures\n* Invalid SQL queries\n* Resource access errors\n* URI validation\n* Tool execution errors\n\nAll errors are logged and returned with appropriate error messages.\n\n## Security Features\n\n* Environment variable based configuration\n* Connection string security\n* Result set size limits\n* Input validation through Pydantic\n* Proper SQL query handling\n\n## Contact Information\n\n### Amornpan Phornchaicharoen\n\n[![Email](https://img.shields.io/badge/Email-amornpan%40gmail.com-red?style=flat-square&logo=gmail)](mailto:amornpan@gmail.com)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Amornpan-blue?style=flat-square&logo=linkedin)](https://www.linkedin.com/in/amornpan/)\n[![HuggingFace](https://img.shields.io/badge/🤗%20Hugging%20Face-amornpan-yellow?style=flat-square)](https://huggingface.co/amornpan)\n[![GitHub](https://img.shields.io/badge/GitHub-amornpan-black?style=flat-square&logo=github)](https://github.com/amornpan)\n\nFeel free to reach out to me if you have any questions about this project or would like to collaborate!\n\n---\n*Made with ❤️ by Amornpan Phornchaicharoen*\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\nAmornpan Phornchaicharoen\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## Requirements\n\nCreate a `requirements.txt` file with:\n\n```\nfastapi>=0.104.1\npydantic>=2.10.6\nuvicorn>=0.34.0 \npython-dotenv>=1.0.1\npyodbc>=4.0.35\nanyio>=4.5.0\nmcp==1.2.0\n```\n\nThese versions have been tested and verified to work together. The key components are:\n* `fastapi` and `uvicorn` for the API server\n* `pydantic` for data validation\n* `pyodbc` for SQL Server connectivity\n* `mcp` for Model Context Protocol implementation\n* `python-dotenv` for environment configuration\n* `anyio` for asynchronous I/O support\n\n## Acknowledgments\n\n* Microsoft SQL Server team for ODBC drivers\n* Python pyodbc maintainers\n* Model Context Protocol community\n* Contributors to the python-dotenv project",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mssql",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "andreagroferreira--gateway": {
      "owner": "andreagroferreira",
      "name": "gateway",
      "url": "https://github.com/andreagroferreira/gateway",
      "imageUrl": "/freedevtools/mcp/pfp/andreagroferreira.webp",
      "description": "Creates secure APIs for structured data using the Model Context Protocol, allowing seamless integration with AI agents and ensuring compliance and performance. Automates the generation of APIs and enhances AI applications with built-in security features.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-03-31T21:19:09Z",
      "readme_content": "<div align=\"center\">\n\n![Build Binaries](https://github.com/centralmind/gateway/actions/workflows/build-binaries.yml/badge.svg) &nbsp; <a href=\"https://discord.gg/XFhaUG4F5x\"><img alt=\"XFhaUG4F5x\" src=\"https://dcbadge.limes.pink/api/server/https://discord.gg/XFhaUG4F5x\" height=\"20\"></a> &nbsp;&nbsp;<a href=\"https://t.me/+TM3T1SikjzA4ZWVi\"><img alt=\"telegram_252850_style_plastic_logo_telegram\" src=\"https://img.shields.io/badge/telegram-%E2%9D%A4%EF%B8%8F-252850?style=plastic&logo=telegram\" height=20></a> &nbsp;&nbsp; <a href=\"https://docs.centralmind.ai\"><img alt=\"Full_Documentation_blue_style_for_the_badge_logo_rocket_logoColor_white\" src=\"https://img.shields.io/badge/Full%20Documentation-blue?style=for-the-badge&logo=rocket&logoColor=white\" height=\"20\"></a>\n\n</div>\n\n\n<h2 align=\"center\">CentralMind Gateway: Create API or MCP Server in Minutes</h2>\n\n\n🚀 Interactive Demo via GitHub Codespaces\n\n[![Deploy with GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/centralmind/sample_databases)\n\n## What is Centralmind/Gateway\n\nSimple way to expose your database to AI-Agent via MCP or OpenAPI 3.1 protocols.\n\n```bash\ndocker run --platform linux/amd64 -p 9090:9090 \\\n  ghcr.io/centralmind/gateway:v0.2.6 start \\\n  --connection-string \"postgres://db-user:db-password@db-host/db-name?sslmode=require\"\n```\n\nThis will run for you an API:\n\n```shell\nINFO Gateway server started successfully!         \nINFO MCP SSE server for AI agents is running at: http://localhost:9090/sse \nINFO REST API with Swagger UI is available at: http://localhost:9090/ \n```\n\nWhich you can use inside your AI Agent:\n\n\n\nGateway will generate AI optimized API.\n\n\n## Why Centralmind/Gateway\n\nAI agents and LLM-powered applications need fast, secure access to data, but traditional APIs and databases aren't built for this purpose. We're building an API layer that automatically generates secure, LLM-optimized APIs for your structured data.\n\nOur solution:\n\n- Filters out PII and sensitive data to ensure compliance with GDPR, CPRA, SOC 2, and other regulations\n- Adds traceability and auditing capabilities, ensuring AI applications aren't black boxes and security teams maintain control\n- Optimizes for AI workloads, supporting Model Context Protocol (MCP) with enhanced meta information to help AI agents understand APIs, along with built-in caching and security features\n\nOur primary users are companies deploying AI agents for customer support, analytics, where they need models to access the data without direct SQL access to databases elemenating security, compliance and peformance risks.\n\n\n\n## Features\n\n- ⚡ **Automatic API Generation** – Creates APIs automatically using LLM based on table schema and sampled data\n- 🗄️ **Structured Database Support** – Supports <a href=\"https://docs.centralmind.ai/connectors/postgres/\">PostgreSQL</a>, <a href=\"https://docs.centralmind.ai/connectors/mysql/\">MySQL</a>, <a href=\"https://docs.centralmind.ai/connectors/clickhouse/\">ClickHouse</a>, <a href=\"https://docs.centralmind.ai/connectors/snowflake/\">Snowflake</a>, <a href=\"https://docs.centralmind.ai/connectors/mssql/\">MSSQL</a>, <a href=\"https://docs.centralmind.ai/connectors/bigquery/\">BigQuery</a>, <a href=\"https://docs.centralmind.ai/connectors/oracle/\">Oracle Database</a>, <a href=\"https://docs.centralmind.ai/connectors/sqlite/\">SQLite</a>, <a href=\"https://docs.centralmind.ai/connectors/sqlite/\">ElasticSearch</a>\n- 🌍 **Multiple Protocol Support** – Provides APIs as REST or MCP Server including SSE mode\n- 📜 **API Documentation** – Auto-generated Swagger documentation and OpenAPI 3.1.0 specification\n- 🔒 **PII Protection** – Implements <a href=\"https://docs.centralmind.ai/plugins/pii_remover/\">regex plugin</a> or <a href=\"https://docs.centralmind.ai/plugins/presidio_anonymizer/\">Microsoft Presidio plugin</a> for PII and sensitive data redaction\n- ⚡ **Flexible Configuration** – Easily extensible via YAML configuration and plugin system\n- 🐳 **Deployment Options** – Run as a binary or Docker container with ready-to-use <a href=\"https://docs.centralmind.ai/helm/gateway/\">Helm chart</a>\n- 🤖 **Multiple AI Providers Support** - Support for [OpenAI](https://docs.centralmind.ai/providers/openai), [Anthropic](https://docs.centralmind.ai/providers/anthropic), [Amazon Bedrock](https://docs.centralmind.ai/providers/bedrock), [Google Gemini](https://docs.centralmind.ai/providers/gemini) & [Google VertexAI](https://docs.centralmind.ai/providers/anthropic-vertexai)\n- 📦 **Local & On-Premises** – Support for <a href=\"https://docs.centralmind.ai/providers/local-models/\">self-hosted LLMs</a> through configurable AI endpoints and models\n- 🔑 **Row-Level Security (RLS)** – Fine-grained data access control using <a href=\"https://docs.centralmind.ai/plugins/lua_rls/\">Lua scripts</a>\n- 🔐 **Authentication Options** – Built-in support for <a href=\"https://docs.centralmind.ai/plugins/api_keys/\">API keys</a> and <a href=\"https://docs.centralmind.ai/plugins/oauth/\">OAuth</a>\n- 👀 **Comprehensive Monitoring** – Integration with <a href=\"https://docs.centralmind.ai/plugins/otel/\">OpenTelemetry (OTel)</a> for request tracking and audit trails\n- 🏎️ **Performance Optimization** – Implements time-based and <a href=\"https://docs.centralmind.ai/plugins/lru_cache/\">LRU caching</a> strategies\n\n## How it Works\n\n<div align=\"center\">\n\n\n\n</div>\n\n### 1. Connect & Discover\n\nGateway connects to your structured databases like PostgreSQL and automatically analyzes the schema and data samples\nto generate an optimized API structure based on your prompt. LLM is used only on discovery stage to produce API configuration.\nThe tool uses [AI Providers](https://docs.centralmind.ai/providers) to generate the API configuration while ensuring security\nthrough PII detection.\n\n### 2. Deploy\n\nGateway supports multiple deployment options from standalone binary, docker or <a href=\"https://docs.centralmind.ai/example/k8s/\">Kubernetes</a>.\nCheck our <a href=\"https://docs.centralmind.ai/docs/content/getting-started/launching-api/\">launching guide</a> for detailed\ninstructions. The system uses YAML configuration and plugins for easy customization.\n\n### 3. Use & Integrate\n\nAccess your data through REST APIs or Model Context Protocol (MCP) with built-in security features.\nGateway seamlessly integrates with AI models and applications like <a href=\"https://docs.centralmind.ai/docs/content/integration/langchain/\">LangChain</a>,\n<a href=\"https://docs.centralmind.ai/docs/content/integration/chatgpt/\">OpenAI</a> and\n<a href=\"https://docs.centralmind.ai/docs/content/integration/claude-desktop/\">Claude Desktop</a> using function calling\nor <a href=\"https://docs.centralmind.ai/docs/content/integration/cursor/\">Cursor</a> through MCP. You can also <a href=\"https://docs.centralmind.ai/plugins/otel/\">setup telemetry</a> to local or remote destination in otel format.\n\n## Documentation\n\n### Getting Started\n\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/quickstart/\">Quickstart Guide</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/installation/\">Installation Instructions</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/generating-api/\">API Generation Guide</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/launching-api/\">API Launch Guide</a>\n\n### Additional Resources\n\n- <a href=\"https://docs.centralmind.ai/docs/content/integration/chatgpt/\">ChatGPT Integration Guide</a>\n- <a href=\"https://docs.centralmind.ai/connectors/\">Database Connector Documentation</a>\n- <a href=\"https://docs.centralmind.ai/plugins/\">Plugin Documentation</a>\n\n## How to Build\n\n```shell\n# Clone the repository\ngit clone https://github.com/centralmind/gateway.git\n\n# Navigate to project directory\ncd gateway\n\n# Install dependencies\ngo mod download\n\n# Build the project\ngo build .\n```\n\n## API Generation\n\nGateway uses LLM models to generate your API configuration. Follow these steps:\n\n\n1. Choose one of our supported AI providers:\n\n- [OpenAI](https://docs.centralmind.ai/providers/openai) and all OpenAI-compatible providers\n- [Anthropic](https://docs.centralmind.ai/providers/anthropic)\n- [Amazon Bedrock](https://docs.centralmind.ai/providers/bedrock)\n- [Google Vertex AI (Anthropic)](https://docs.centralmind.ai/providers/anthropic-vertexai)\n- [Google Gemini](https://docs.centralmind.ai/providers/gemini)\n\n[Google Gemini](https://docs.centralmind.ai/providers/gemini) provides a generous **free tier**. You can obtain an API key by visiting Google AI Studio:\n\n- [Google AI Studio](https://aistudio.google.com/apikey)\n\nOnce logged in, you can create an API key in the API section of AI Studio. The free tier includes a generous monthly token allocation, making it accessible for development and testing purposes.\n\nConfigure AI provider authorization. For Google Gemini, set an API key.\n\n```bash\nexport GEMINI_API_KEY='yourkey'\n```\n\n2. Run the discovery command:\n\n```shell\n./gateway discover \\\n  --ai-provider gemini \\\n  --connection-string \"postgresql://neondb_owner:MY_PASSWORD@MY_HOST.neon.tech/neondb?sslmode=require\" \\\n  --prompt \"Generate for me awesome readonly API\"\n```\n\n3. Monitor the generation process:\n\n```shell\nINFO 🚀 API Discovery Process\nINFO Step 1: Read configs\nINFO ✅ Step 1 completed. Done.\n\nINFO Step 2: Discover data\nINFO Discovered Tables:\nINFO   - payment_dim: 3 columns, 39 rows\nINFO   - fact_table: 9 columns, 1000000 rows\nINFO ✅ Step 2 completed. Done.\n\n# Additional steps and output...\n\nINFO ✅ All steps completed. Done.\n\nINFO --- Execution Statistics ---\nINFO Total time taken: 1m10s\nINFO Tokens used: 16543 (Estimated cost: $0.0616)\nINFO Tables processed: 6\nINFO API methods created: 18\nINFO Total number of columns with PII data: 2\n```\n\n4. Review the generated configuration in `gateway.yaml`:\n\n```yaml\napi:\n  name: Awesome Readonly API\n  description: ''\n  version: '1.0'\ndatabase:\n  type: postgres\n  connection: YOUR_CONNECTION_INFO\n  tables:\n    - name: payment_dim\n      columns: # Table columns\n      endpoints:\n        - http_method: GET\n          http_path: /some_path\n          mcp_method: some_method\n          summary: Some readable summary\n          description: 'Some description'\n          query: SQL Query with params\n          params: # Query parameters\n```\n\n## Running the API\n\n### Run locally\n\n```shell\n./gateway start --config gateway.yaml rest\n```\n\n### Docker Compose\n\n```shell\ndocker compose -f ./example/simple/docker-compose.yml up\n```\n\n### MCP Protocol Integration\n\nGateway implements the MCP protocol for seamless integration with Claude and other tools. For detailed setup instructions, see our <a href=\"https://docs.centralmind.ai/docs/content/integration/claude-desktop/\">Claude integration guide</a>.\n\n1. Build the gateway binary:\n\n```shell\ngo build .\n```\n\n2. Configure Claude Desktop tool configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"gateway\": {\n      \"command\": \"PATH_TO_GATEWAY_BINARY\",\n      \"args\": [\"start\", \"--config\", \"PATH_TO_GATEWAY_YAML_CONFIG\", \"mcp-stdio\"]\n    }\n  }\n}\n```\n\n## Roadmap\n\nIt is always subject to change, and the roadmap will highly depend on user feedback. At this moment,\nwe are planning the following features:\n\n#### Database and Connectivity\n\n- 🗄️ **Extended Database Integrations** - Redshift, S3 (Iceberg and Parquet), Oracle DB, Microsoft SQL Server, Elasticsearch\n- 🔑 **SSH tunneling** - ability to use jumphost or ssh bastion to tunnel connections\n\n#### Enhanced Functionality\n\n- 🔍 **Advanced Query Capabilities** - Complex filtering syntax and Aggregation functions as parameters\n- 🔐 **Enhanced MCP Security** - API key and OAuth authentication\n\n#### Platform Improvements\n\n- 📦 **Schema Management** - Automated schema evolution and API versioning\n- 🚦 **Advanced Traffic Management** - Intelligent rate limiting, Request throttling\n- ✍️ **Write Operations Support** - Insert, Update operations",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "secure apis"
      ],
      "category": "databases"
    },
    "andreagroferreira--servers": {
      "owner": "andreagroferreira",
      "name": "servers",
      "url": "https://github.com/andreagroferreira/servers",
      "imageUrl": "/freedevtools/mcp/pfp/andreagroferreira.webp",
      "description": "Enables interaction with PostgreSQL databases through executing read-only SQL queries and inspecting database schemas. Provides structured data access while maintaining data integrity with read-only transactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-31T21:06:59Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the TypeScript and Python SDKs.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Redis](src/redis)** - Interact with Redis key-value stores\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg\" alt=\"Adfin Logo\" /> **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).\n- <img height=\"12\" width=\"12\" src=\"https://www.agentql.com/favicon/favicon.png\" alt=\"AgentQL Logo\" /> **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).\n- <img height=\"12\" width=\"12\" src=\"https://agentrpc.com/favicon.ico\" alt=\"AgentRPC Logo\" /> **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).\n- <img height=\"12\" width=\"12\" src=\"https://aiven.io/favicon.ico\" alt=\"Aiven Logo\" /> **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQL®, Apache Kafka®, ClickHouse® and OpenSearch® services\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png\" alt=\"APIMatic Logo\" /> **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic’s API.\n- <img height=\"12\" width=\"12\" src=\"https://resources.audiense.com/hubfs/favicon-1.png\" alt=\"Audiense Logo\" /> **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://www.bankless.com/favicon.ico\" alt=\"Bankless Logo\" /> **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.\n- <img height=\"12\" width=\"12\" src=\"https://www.box.com/favicon.ico\" alt=\"Box Logo\" /> **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.chargebee.com/static/resources/brand/favicon.png\" /> **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).\n- <img alt=\"chroma_logo_ae2d6e4b\" height=\"12\" width=\"12\" src=\"https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg\" /> **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database\n- <img height=\"12\" width=\"12\" src=\"https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico\" alt=\"Chronulus AI Logo\" /> **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.\n- <img height=\"12\" width=\"12\" src=\"https://clickhouse.com/favicon.ico\" alt=\"ClickHouse Logo\" /> **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://www.comet.com/favicon.ico\" alt=\"Comet Logo\" /> **[Comet Opik](https://github.com/comet-ml/opik-mcp)** - Query and analyze your [Opik](https://github.com/comet-ml/opik) logs, traces, prompts and all other telemtry data from your LLMs in natural language.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.convex.dev/favicon.ico\" /> **[Convex](https://stack.convex.dev/convex-mcp-server)** - Introspect and query your apps deployed to Convex.\n- <img height=\"12\" width=\"12\" src=\"http://app.itsdart.com/static/img/favicon.png\" alt=\"Dart Logo\" /> **[Dart](https://github.com/its-dart/dart-mcp-server)** - Interact with task, doc, and project data in [Dart](https://itsdart.com), an AI-native project management tool\n- <img height=\"12\" width=\"12\" src=\"https://www.devhub.com/img/upload/favicon-196x196-dh.png\" alt=\"DevHub Logo\" /> **[DevHub](https://github.com/devhub/devhub-cms-mcp)** - Manage and utilize website content within the [DevHub](https://www.devhub.com) CMS platform\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://static.edubase.net/media/brand/favicon/favicon-32x32.png\" alt=\"EduBase Logo\" /> **[EduBase](https://github.com/EduBase/MCP)** - Interact with [EduBase](https://www.edubase.net), a comprehensive e-learning platform with advanced quizzing, exam management, and content organization capabilities\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fewsats.com/favicon.svg\" alt=\"Fewsats Logo\" /> **[Fewsats](https://github.com/Fewsats/fewsats-mcp)** - Enable AI Agents to purchase anything in a secure way using [Fewsats](https://fewsats.com)\n- <img height=\"12\" width=\"12\" src=\"https://fibery.io/favicon.svg\" alt=\"Fibery Logo\" /> **[Fibery](https://github.com/Fibery-inc/fibery-mcp-server)** - Perform queries and entity operations in your [Fibery](https://fibery.io) workspace.\n- <img height=\"12\" width=\"12\" src=\"https://financialdatasets.ai/favicon.ico\" alt=\"Financial Datasets Logo\" /> **[Financial Datasets](https://github.com/financial-datasets/mcp-server)** - Stock market API made for AI agents\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/mendableai/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://gitee.com/favicon.ico\" alt=\"Gitee Logo\" /> **[Gitee](https://github.com/oschina/mcp-gitee)** - Gitee API integration, repository, issue, and pull request management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6605a2979ff17b2cd1939cd4/6605a460de47e7596ed84f06_icon256.png\" alt=\"gotoHuman Logo\" /> **[gotoHuman](https://github.com/gotohuman/gotohuman-mcp-server)** - Human-in-the-loop platform - Allow AI agents and automations to send requests for approval to your [gotoHuman](https://www.gotohuman.com) inbox.\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/KCOWBYLKunDff1Dr452y6EfjiU.png\" alt=\"Graphlit Logo\" /> **[Graphlit](https://github.com/graphlit/graphlit-mcp-server)** - Ingest anything from Slack to Gmail to podcast feeds, in addition to web crawling, into a searchable [Graphlit](https://www.graphlit.com) project.\n- <img height=\"12\" width=\"12\" src=\"https://greptime.com/favicon.ico\" alt=\"Greptime Logo\" /> **[GreptimeDB](https://github.com/GreptimeTeam/greptimedb-mcp-server)** - Provides AI assistants with a secure and structured way to explore and analyze data in [GreptimeDB](https://github.com/GreptimeTeam/greptimedb).\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN01d9qrry1i6lTNa2BRa_!!6000000004364-2-tps-218-200.png\" alt=\"Hologres Logo\" /> **[Hologres](https://github.com/aliyun/alibabacloud-hologres-mcp-server)** - Connect to a [Hologres](https://www.alibabacloud.com/en/product/hologres) instance, get table metadata, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://hyperbrowser-assets-bucket.s3.us-east-1.amazonaws.com/Hyperbrowser-logo.png\" alt=\"Hyperbrowsers23 Logo\" /> **[Hyperbrowser](https://github.com/hyperbrowserai/mcp)** - [Hyperbrowser](https://www.hyperbrowser.ai/) is the next-generation platform empowering AI agents and enabling effortless, scalable browser automation.\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://forevervm.com/icon.png\" alt=\"ForeverVM Logo\" /> **[ForeverVM](https://github.com/jamsocket/forevervm/tree/main/javascript/mcp-server)** - Run Python in a code sandbox.\n- <img height=\"12\" width=\"12\" src=\"https://www.getinboxzero.com/icon.png\" alt=\"Inbox Zero Logo\" /> **[Inbox Zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server)** - AI personal assistant for email [Inbox Zero](https://www.getinboxzero.com)\n-  **[Inkeep](https://github.com/inkeep/mcp-server-python)** - RAG Search over your content powered by [Inkeep](https://inkeep.com)\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers.\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://connection.keboola.com/favicon.ico\" alt=\"Keboola Logo\" /> **[Keboola](https://github.com/keboola/keboola-mcp-server)** - Build robust data workflows, integrations, and analytics on a single intuitive platform.\n- <img height=\"12\" width=\"12\" src=\"https://logfire.pydantic.dev/favicon.ico\" alt=\"Logfire Logo\" /> **[Logfire](https://github.com/pydantic/logfire-mcp)** - Provides access to OpenTelemetry traces and metrics through Logfire.\n- <img height=\"12\" width=\"12\" src=\"https://langfuse.com/favicon.ico\" alt=\"Langfuse Logo\" /> **[Langfuse Prompt Management](https://github.com/langfuse/mcp-server-langfuse)** - Open-source tool for collaborative editing, versioning, evaluating, and releasing prompts.\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://www.mailgun.com/favicon.ico\" alt=\"Mailgun Logo\" /> **[Mailgun](https://github.com/mailgun/mailgun-mcp-server)** - Interact with Mailgun API.\n- <img height=\"12\" width=\"12\" src=\"https://www.make.com/favicon.ico\" alt=\"Make Logo\" /> **[Make](https://github.com/integromat/make-mcp-server)** - Turn your [Make](https://www.make.com/) scenarios into callable tools for AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img alt=\"favicon_32x32\" height=\"12\" width=\"12\" src=\"https://milvus.io/favicon-32x32.png\" /> **[Milvus](https://github.com/zilliztech/mcp-server-milvus)** - Search, Query and interact with data in your Milvus Vector Database.\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/183852044?s=48&v=4\" alt=\"Neon Logo\" /> **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/82347605?s=48&v=4\" alt=\"OceanBase Logo\" /> **[OceanBase](https://github.com/oceanbase/mcp-oceanbase)** - MCP Server for OceanBase database and its tools\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[Octagon](https://github.com/OctagonAI/octagon-mcp-server)** - Deliver real-time investment research with extensive private and public market data.\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img height=\"12\" width=\"12\" src=\"https://www.perplexity.ai/favicon.ico\" alt=\"Perplexity Logo\" /> **[Perplexity](https://github.com/ppl-ai/modelcontextprotocol)** - An MCP server that connects to Perplexity's Sonar API, enabling real-time web-wide research in conversational AI.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.ramp.com/favicon.ico\" /> **[Ramp](https://github.com/ramp-public/ramp-mcp)** - Interact with [Ramp](https://ramp.com)'s Developer API to run analysis on your spend and gain insights leveraging LLMs\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://www.rember.com/favicon.ico\" alt=\"Rember Logo\" /> **[Rember](https://github.com/rember/rember-mcp)** - Create spaced repetition flashcards in [Rember](https://rember.com) to remember anything you learn in your chats\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://screenshotone.com/favicon.ico\" alt=\"ScreenshotOne Logo\" /> **[ScreenshotOne](https://github.com/screenshotone/mcp/)** - Render website screenshots with [ScreenshotOne](https://screenshotone.com/)\n- <img height=\"12\" width=\"12\" src=\"https://semgrep.dev/favicon.ico\" alt=\"Semgrep Logo\" /> **[Semgrep](https://github.com/semgrep/mcp)** - Enable AI agents to secure code with [Semgrep](https://semgrep.dev/).\n- <img alt=\"favicon_32x32_png_v_277b9cbbe31e8bc416504cf3b902d430\" height=\"12\" width=\"12\" src=\"https://www.singlestore.com/favicon-32x32.png?v=277b9cbbe31e8bc416504cf3b902d430\"/> **[SingleStore](https://github.com/singlestore-labs/mcp-server-singlestore)** - Interact with the SingleStore database platform\n- <img height=\"12\" width=\"12\" src=\"https://www.starrocks.io/favicon.ico\" alt=\"StarRocks Logo\" /> **[StarRocks](https://github.com/StarRocks/mcp-server-starrocks)** - Interact with [StarRocks](https://www.starrocks.io/)\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://thirdweb.com/favicon.ico\" alt=\"Thirdweb Logo\" /> **[Thirdweb](https://github.com/thirdweb-dev/ai/tree/main/python/thirdweb-mcp)** - Read/write to over 2k blockchains, enabling data querying, contract analysis/deployment, and transaction execution, powered by [Thirdweb](https://thirdweb.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://unifai.network/favicon.ico\" alt=\"UnifAI Logo\" /> **[UnifAI](https://github.com/unifai-network/unifai-mcp-server)** - Dynamically search and call tools using [UnifAI Network](https://unifai.network)\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/plcQevjrOYnyriuGw90NfQBPoQ.jpg\" alt=\"Unstructured Logo\" /> **[Unstructured](https://github.com/Unstructured-IO/UNS-MCP)** - Set up and interact with your unstructured data processing workflows in [Unstructured Platform](https://unstructured.io)\n- **[Vectorize](https://github.com/vectorize-io/vectorize-mcp-server/)** - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n- <img height=\"12\" width=\"12\" src=\"https://www.veyrax.com/favicon.ico\" alt=\"VeyraX Logo\" /> **[VeyraX](https://github.com/VeyraX/veyrax-mcp)** - Single tool to control all 100+ API integrations, and UI components\n- <img height=\"12\" width=\"12\" src=\"https://www.xero.com/favicon.ico\" alt=\"Xero Logo\" /> **[Xero](https://github.com/XeroAPI/xero-mcp-server)** - Interact with the accounting data in your business using our official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://cdn.zapier.com/zapier/images/favicon.ico\" alt=\"Zapier Logo\" /> **[Zapier](https://zapier.com/mcp)** - Connect your AI Agents to 8,000 apps instantly.\n- **[ZenML](https://github.com/zenml-io/mcp-zenml)** - Interact with your MLOps and LLMOps pipelines through your [ZenML](https://www.zenml.io) MCP server\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n- **[Ableton Live](https://github.com/Simon-Kansara/ableton-live-mcp-server)** - an MCP server to control Ableton Live.\n- **[Airbnb](https://github.com/openbnb-org/mcp-server-airbnb)** - Provides tools to search Airbnb and get listing details.\n- **[Algorand](https://github.com/GoPlausible/algorand-mcp)** - A comprehensive MCP server for tooling interactions (40+) and resource accessibility (60+) plus many useful prompts for interacting with the Algorand blockchain.\n- **[Airflow](https://github.com/yangkyeongmo/mcp-server-apache-airflow)** - A MCP Server that connects to [Apache Airflow](https://airflow.apache.org/) using official python client.\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Apple Calendar](https://github.com/Omar-v2/mcp-ical)** - An MCP server that allows you to interact with your MacOS Calendar through natural language, including features such as event creation, modification, schedule listing, finding free time slots etc.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[Arduino](https://github.com/vishalmysore/choturobo)** - MCP Server that enables AI-powered robotics using Claude AI and Arduino (ESP32) for real-world automation and interaction with robots.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM.\n- **[AWS Athena](https://github.com/lishenxydlgzs/aws-athena-mcp)** - A MCP server for AWS Athena to run SQL queries on Glue Catalog.\n- **[AWS Cost Explorer](https://github.com/aarora79/aws-cost-explorer-mcp-server)** - Optimize your AWS spend (including Amazon Bedrock spend) with this MCP server by examining spend across regions, services, instance types and foundation models ([demo video](https://www.youtube.com/watch?v=WuVOmYLRFmI&feature=youtu.be)).\n- **[AWS Resources Operations](https://github.com/baryhuang/mcp-server-aws-resources-python)** - Run generated python code to securely query or modify any AWS resources supported by boto3.\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents.\n- **[Azure ADX](https://github.com/pab1it0/adx-mcp-server)** - Query and analyze Azure Data Explorer databases.\n- **[Azure DevOps](https://github.com/Vortiago/mcp-azure-devops)** - An MCP server that provides a bridge to Azure DevOps services, enabling AI assistants to query and manage work items.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n* **[Basic Memory](https://github.com/basicmachines-co/basic-memory)** - Local-first knowledge management system that builds a semantic graph from Markdown files, enabling persistent memory across conversations with LLMs.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Bing Web Search API](https://github.com/leehanchung/bing-search-mcp)** (by hanchunglee) - Server implementation for Microsoft Bing Web Search API.\n- **[Bitable MCP](https://github.com/lloydzhou/bitable-mcp)** (by lloydzhou) - MCP server provides access to Lark Bitable through the Model Context Protocol. It allows users to interact with Bitable tables using predefined tools.\n- **[Blender](https://github.com/ahujasid/blender-mcp)** (by ahujasid) - Blender integration allowing prompt enabled 3D scene creation, modeling and manipulation.\n- **[Bsc-mcp](https://github.com/TermiX-official/bsc-mcp)** The first MCP server that serves as the bridge between AI and BNB Chain, enabling AI agents to execute complex on-chain operations through seamless integration with the BNB Chain, including transfer, swap, launch, security check on any token and even more.\n- **[Calculator](https://github.com/githejie/mcp-server-calculator)** - This server enables LLMs to use calculator for precise numerical calculations.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[code-assistant](https://github.com/stippi/code-assistant)** - A coding assistant MCP server that allows to explore a code-base and make changes to code. Should be used with trusted repos only (insufficient protection against prompt injections).\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[crypto-feargreed-mcp](https://github.com/kukapay/crypto-feargreed-mcp)**  -  Providing real-time and historical Crypto Fear & Greed Index data.\n- **[cryptopanic-mcp-server](https://github.com/kukapay/cryptopanic-mcp-server)** - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.\n- **[Dappier](https://github.com/DappierAI/dappier-mcp)** - Connect LLMs to real-time, rights-cleared, proprietary data from trusted sources. Access specialized models for Real-Time Web Search, News, Sports, Financial Data, Crypto, and premium publisher content. Explore data models at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n- **[Databricks](https://github.com/JordiNeil/mcp-databricks-server)** - Allows LLMs to run SQL queries, list and get details of jobs executions in a Databricks account.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DBHub](https://github.com/bytebase/dbhub/)** - Universal database MCP server connecting to MySQL, PostgreSQL, SQLite, DuckDB and etc.\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[deepseek-thinker-mcp](https://github.com/ruixingshi/deepseek-thinker-mcp)** - A MCP (Model Context Protocol) provider Deepseek reasoning content to MCP-enabled AI Clients, like Claude Desktop. Supports access to Deepseek's thought processes from the Deepseek API service or from a local Ollama server.\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dicom](https://github.com/ChristianHinge/dicom-mcp)** - An MCP server to query and retrieve medical images and for parsing and reading dicom-encapsulated documents (pdf etc.). \n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discord](https://github.com/v-3/discordmcp)** - A MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Discord](https://github.com/SaseQ/discord-mcp)** - A MCP server, which connects to Discord through a bot, and provides comprehensive integration with Discord.\n- **[Discourse](https://github.com/AshDevFr/discourse-mcp-server)** - A MCP server to search Discourse posts on a Discourse forum.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[dune-analytics-mcp](https://github.com/kukapay/dune-analytics-mcp)** -  A mcp server that bridges Dune Analytics data to AI agents.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Ergo Blockchain MCP](https://github.com/marctheshark3/ergo-mcp)** -An MCP server to integrate Ergo Blockchain Node and Explorer APIs for checking address balances, analyzing transactions, viewing transaction history, performing forensic analysis of addresses, searching for tokens, and monitoring network status.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[EVM MCP Server](https://github.com/mcpdotdirect/evm-mcp-server)** - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[Excel](https://github.com/haris-musa/excel-mcp-server)** - Excel manipulation including data reading/writing, worksheet management, formatting, charts, and pivot table.\n- **[Fantasy PL](https://github.com/rishijatia/fantasy-pl-mcp)** - Give your coding agent direct access to up-to date Fantasy Premier League data\n- **[fastn.ai – Unified API MCP Server](https://github.com/fastnai/mcp-fastn)** - A remote, dynamic MCP server with a unified API that connects to 1,000+ tools, actions, and workflows, featuring built-in authentication and monitoring.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[Fingertip](https://github.com/fingertip-com/fingertip-mcp)** - MCP server for Fingertip.com to search and create new sites.\n- **[Figma](https://github.com/GLips/Figma-Context-MCP)** - Give your coding agent direct access to Figma file data, helping it one-shot design implementation.\n- **[Firebase](https://github.com/gannonh/firebase-mcp)** - Server to interact with Firebase services including Firebase Authentication, Firestore, and Firebase Storage.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Ghost](https://github.com/MFYDev/ghost-mcp)** - A Model Context Protocol (MCP) server for interacting with Ghost CMS through LLM interfaces like Claude.\n- **[Github Actions](https://github.com/ko1ynnky/github-actions-mcp-server)** - A Model Context Protocol (MCP) server for interacting with Github Actions.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Gmail Headless](https://github.com/baryhuang/mcp-headless-gmail)** - Remote hostable MCP server that can get and send Gmail messages without local credential or file system setup.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[GOAT](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol)** - Run more than +200 onchain actions on any blockchain including Ethereum, Solana and Base.\n- **[Godot](https://github.com/Coding-Solo/godot-mcp)** - A MCP server providing comprehensive Godot engine integration for project editing, debugging, and scene management.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Goodnews](https://github.com/VectorInstitute/mcp-goodnews)** - A simple MCP server that delivers curated positive and uplifting news stories.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[GraphQL Schema](https://github.com/hannesj/mcp-graphql-schema)** - Allow LLMs to explore large GraphQL schemas without bloating the context.\n- **[HDW LinkedIn](https://github.com/horizondatawave/hdw-mcp-server)** - Access to profile data and management of user account with [HorizonDataWave.ai](https://horizondatawave.ai/).\n- **[Heurist Mesh Agent](https://github.com/heurist-network/heurist-mesh-mcp-server)** - Access specialized web3 AI agents for blockchain analysis, smart contract security, token metrics, and blockchain interactions through the [Heurist Mesh network](https://github.com/heurist-network/heurist-agent-framework/tree/main/mesh).\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[Home Assistant](https://github.com/voska/hass-mcp)** - Docker-ready MCP server for Home Assistant with entity management, domain summaries, automation support, and guided conversations. Includes pre-built container images for easy installation.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Hyperliquid](https://github.com/mektigboy/server-hyperliquid)** - An MCP server implementation that integrates the Hyperliquid SDK for exchange data.\n- **[iFlytek Workflow](https://github.com/iflytek/ifly-workflow-mcp-server)** - Connect to iFlytek Workflow via the MCP server and run your own Agent.\n- **[Image Generation](https://github.com/GongRzhe/Image-Generation-MCP-Server)** - This MCP server provides image generation capabilities using the Replicate Flux model.\n- **[InfluxDB](https://github.com/idoru/influxdb-mcp-server)** - Run queries against InfluxDB OSS API v2.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Intercom](https://github.com/raoulbia-ai/mcp-server-for-intercom)** - An MCP-compliant server for retrieving customer support tickets from Intercom. This tool enables AI assistants like Claude Desktop and Cline to access and analyze your Intercom support tickets.\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[JavaFX](https://github.com/mcpso/mcp-server-javafx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, sqllite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[KiCad MCP](https://github.com/lamaalrajih/kicad-mcp)** - MCP server for KiCad on Mac, Windows, and Linux.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Kubernetes and OpenShift](https://github.com/manusa/kubernetes-mcp-server)** - A powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- **[Langflow-DOC-QA-SERVER](https://github.com/GongRzhe/Langflow-DOC-QA-SERVER)** - A Model Context Protocol server for document Q&A powered by Langflow. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[Linear (Go)](https://github.com/geropl/linear-mcp-go)** - Allows LLM to interact with Linear's API via a single static binary.\n- **[LINE](https://github.com/amornpan/py-mcp-line)** (by amornpan) - Implementation for LINE Bot integration that enables Language Models to read and analyze LINE conversations through a standardized interface. Features asynchronous operation, comprehensive logging, webhook event handling, and support for various message types.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[mac-messages-mcp](https://github.com/carterlasalle/mac_messages_mcp)** - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- **[MariaDB](https://github.com/abel9851/mcp-server-mariadb)** - MariaDB database integration with configurable access controls in Python.\n- **[Maton](https://github.com/maton-ai/agent-toolkit/tree/main/modelcontextprotocol)** - Connect to your SaaS tools like HubSpot, Salesforce, and more.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Create](https://github.com/tesla0225/mcp-create)** - A dynamic MCP server management service that creates, runs, and manages Model Context Protocol servers on-the-fly.\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-local-rag](https://github.com/nkapila6/mcp-local-rag)** - \"primitive\" RAG-like web search model context protocol (MCP) server that runs locally using Google's MediaPipe Text Embedder and DuckDuckGo Search. ✨ no APIs required ✨.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[mem0-mcp](https://github.com/mem0ai/mem0-mcp)** - A Model Context Protocol server for Mem0, which helps with managing coding preferences.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[MSSQL-MCP](https://github.com/daobataotie/mssql-mcp)** (by daobataotie) - MSSQL MCP that refer to the official website's SQLite MCP for modifications to adapt to MSSQL\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Mindmap](https://github.com/YuChenSSR/mindmap-mcp-server)** (by YuChenSSR) - A server that generates mindmaps from input containing markdown code.\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MongoDB Lens](https://github.com/furey/mongodb-lens)** - Full Featured MCP Server for MongoDB Databases.\n- **[Monday.com](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[Multicluster-MCP-Sever](https://github.com/yanmxa/multicluster-mcp-server)** - The gateway for GenAI systems to interact with multiple Kubernetes clusters.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[n8n](https://github.com/leonardsellem/n8n-mcp-server)** - This MCP server provides tools and resources for AI assistants to manage n8n workflows and executions, including listing, creating, updating, and deleting workflows, as well as monitoring their execution status.\n- **[NASA](https://github.com/ProgramComputer/NASA-MCP-server)** (by ProgramComputer) - Access to a unified gateway of NASA's data sources including but not limited to APOD, NEO, EPIC, GIBS.\n- **[National Parks](https://github.com/KyrieTangSheng/mcp-server-nationalparks)** - The server provides latest information of park details, alerts, visitor centers, campgrounds, hiking trails, and events for U.S. National Parks.\n- **[NAVER](https://github.com/pfldy2850/py-mcp-naver)** (by pfldy2850) - This MCP server provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp)** (by teddyzxcv) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OceanBase](https://github.com/yuanoOo/oceanbase_mcp_server)** - (by yuanoOo) A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases.\n- **[Okta](https://github.com/kapilduraphe/okta-mcp-server)** - Interact with Okta API.\n- **[OneNote](https://github.com/rajvirtual/MCP-Servers/tree/master/onenote)** - (by Rajesh Vijay) An MCP server that connects to Microsoft OneNote using the Microsoft Graph API. Reading notebooks, sections, and pages from OneNote,Creating new notebooks, sections, and pages in OneNote.\n- **[OpenAI WebSearch MCP](https://github.com/ConechoAI/openai-websearch-mcp)** - This is a Python-based MCP server that provides OpenAI `web_search` build-in tool.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenAPI AnyApi](https://github.com/baryhuang/mcp-server-any-openapi)** - Interact with large [OpenAPI](https://www.openapis.org/) docs using built-in semantic search for endpoints. Allows for customizing the MCP server prefix.\n- **[OpenAPI Schema](https://github.com/hannesj/mcp-openapi-schema)** - Allow LLMs to explore large [OpenAPI](https://www.openapis.org/) schemas without bloating the context.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenDota](https://github.com/asusevski/opendota-mcp-server)** - Interact with OpenDota API to retrieve Dota 2 match data, player statistics, and more.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Productboard](https://github.com/kenjihikmatullah/productboard-mcp)** - Integrate the Productboard API into agentic workflows via MCP.\n- **[Prometheus](https://github.com/pab1it0/prometheus-mcp-server)** - Query and analyze Prometheus - open-source monitoring system.\n- **[Pulumi](https://github.com/dogukanakkaya/pulumi-mcp-server)** - MCP Server to Interact with Pulumi API, creates and lists Stacks\n- **[Pushover](https://github.com/ashiknesin/pushover-mcp)** - Send instant notifications to your devices using [Pushover.net](https://pushover.net/)\n- **[QGIS](https://github.com/jjsantos01/qgis_mcp)** - connects QGIS to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.\n- **[QuickChart](https://github.com/GongRzhe/Quickchart-MCP-Server)** - A Model Context Protocol server for generating charts using QuickChart.io\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Rquest](https://github.com/xxxbrian/mcp-rquest)** - An MCP server providing realistic browser-like HTTP request capabilities with accurate TLS/JA3/JA4 fingerprints for bypassing anti-bot measures.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[scrapling-fetch](https://github.com/cyberchitta/scrapling-fetch-mcp)** - Access text content from bot-protected websites. Fetches HTML/markdown from sites with anti-automation measures using Scrapling.\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[ServiceNow](https://github.com/osomai/servicenow-mcp)** - A MCP server to interact with a ServiceNow instance\n- **[Siri Shortcuts](https://github.com/dvcrn/mcp-server-siri-shortcuts)** - MCP to interact with Siri Shortcuts on macOS. Exposes all Shortcuts as MCP tools.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server)** - This MCP server enables LLMs to interact with the Solana blockchain with help of Solana Agent Kit by SendAI, allowing for 40+ protcool actions and growing\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Starwind UI](https://github.com/Boston343/starwind-ui-mcp/)** - This MCP provides relevant commands, documentation, and other information to allow LLMs to take full advantage of Starwind UI's open source Astro components.\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Telegram](https://github.com/chigwell/telegram-mcp)** - An MCP server that provides paginated chat reading, message retrieval, and message sending capabilities for Telegram through Telethon integration.\n- **[Terminal-Control](https://github.com/GongRzhe/terminal-controller-mcp)** - A MCP server that enables secure terminal command execution, directory navigation, and file system operations through a standardized interface.\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Typesense](https://github.com/suhail-ak-s/mcp-typesense-server)** - A Model Context Protocol (MCP) server implementation that provides AI models with access to Typesense search capabilities. This server enables LLMs to discover, search, and analyze data stored in Typesense collections.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Unity Catalog](https://github.com/ognis1205/mcp-server-unitycatalog)** - An MCP server that enables LLMs to interact with Unity Catalog AI, supporting CRUD operations on Unity Catalog Functions and executing them as MCP tools.\n- **[Unity3d Game Engine](https://github.com/CoderGamester/mcp-unity)** - An MCP server that enables LLMs to interact with Unity3d Game Engine, supporting access to a variety of the Unit's Editor engine tools (e.g. Console Logs, Test Runner logs, Editor functions, hierarchy state, etc) and executing them as MCP tools or gather them as resources.\n- **[Unity Integration (Advanced)](https://github.com/quazaai/UnityMCPIntegration)** - Advanced Unity3d Game Engine MCP which supports ,Execution of Any Editor Related Code Directly Inside of Unity, Fetch Logs, Get Editor State and Allow File Access of the Project making it much more useful in Script Editing or asset creation.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[Virtual location (Google Street View,etc.)](https://github.com/mfukushim/map-traveler-mcp)** - Integrates Google Map, Google Street View, PixAI, Stability.ai, ComfyUI API and Bluesky to provide a virtual location simulation in LLM (written in Effect.ts)\n- **[VolcEngine TOS](https://github.com/dinghuazhou/sample-mcp-server-tos)** - A sample MCP server for VolcEngine TOS that flexibly get objects from TOS.\n- **[Wanaku MCP Router](https://github.com/wanaku-ai/wanaku/)** - The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.\n- **[Webflow](https://github.com/kapilduraphe/webflow-mcp-server)** - Interfact with the Webflow APIs\n- **[whale-tracker-mcp](https://github.com/kukapay/whale-tracker-mcp)**  -  A mcp server for tracking cryptocurrency whale transactions. \n- **[Whois MCP](https://github.com/bharathvaj-ganesan/whois-mcp)** - MCP server that performs whois lookup against domain, IP, ASN and TLD. \n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[xcodebuild](https://github.com/ShenghaiWang/xcodebuild)**  - 🍎 Build iOS Xcode workspace/project and feed back errors to llm.\n- **[Xero-mcp-server](https://github.com/john-zhang-dev/xero-mcp)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[XiYan](https://github.com/XGenerationLab/xiyan_mcp_server)** - 🗄️ An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n- **[FastAPI to MCP auto generator](https://github.com/tadata-org/fastapi_mcp)** – A zero-configuration tool for automatically exposing FastAPI endpoints as MCP tools by **[Tadata](https://tadata.com/)**\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[Higress MCP Server Hosting](https://github.com/alibaba/higress/tree/main/plugins/wasm-go/mcp-servers)** - A solution for hosting MCP Servers by extending the API Gateway (based on Envoy) with wasm plugins.\n* **[MCP-Framework](https://mcp-framework.com)** Build MCP servers with elegance and speed in Typescript. Comes with a CLI to create your project with `mcp create app`. Get started with your first server in under 5 minutes by **[Alex Andru](https://github.com/QuantGeekDev)**\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n* **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Discord Server (ModelContextProtocol)](https://discord.gg/jHEGxQu2a5)** – Connect with developers, share insights, and collaborate on projects in an active Discord community dedicated to the Model Context Protocol by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-guardian](https://github.com/eqtylab/mcp-guardian)** - GUI application + tools for proxying / managing control of MCP servers by **[EQTY Lab](https://eqtylab.io)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[mcp-dockmaster](https://mcp-dockmaster.com)** - An Open-Sourced UI to install and manage MCP servers for Windows, Linux and MacOS.\n- <img height=\"12\" width=\"12\" src=\"https://mkinf.io/favicon-lilac.png\" alt=\"mkinf Logo\" /> **[mkinf](https://mkinf.io)** - An Open Source registry of hosted MCP Servers to accelerate AI agent workflows.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[r/modelcontextprotocol](https://www.reddit.com/r/modelcontextprotocol)** – A Model Context Protocol community Reddit page - discuss ideas, get answers to your questions, network with like-minded people, and showcase your projects! by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "postgresql",
        "database",
        "databases secure",
        "secure database",
        "postgresql databases"
      ],
      "category": "databases"
    },
    "anishksk--MCP-Server": {
      "owner": "anishksk",
      "name": "MCP-Server",
      "url": "https://github.com/anishksk/MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/anishksk.webp",
      "description": "Manage serverless databases via a REST API, enabling the creation, listing, and execution of queries. Integrate with GitHub for version control and utilize Smithery.ai for deployment management.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-28T21:46:30Z",
      "readme_content": "# SkySQL MCP Integration\n\n[![smithery badge](https://smithery.ai/badge/@anishksk/mcp-server)](https://smithery.ai/server/@anishksk/mcp-server)\n\nThis project provides a REST API interface for managing SkySQL databases, with integration capabilities for GitHub and Smithery.ai.\n\n## Features\n\n- Create and manage serverless databases in SkySQL\n- RESTful API endpoints for database operations\n- GitHub integration for version control\n- Smithery.ai deployment support\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- npm or yarn\n- SkySQL API key\n- GitHub account\n- Smithery.ai account\n\n## Setup\n\n1. Clone the repository:\n```bash\ngit clone <your-repo-url>\ncd skysql-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create a `.env` file with your credentials:\n```\nSKYSQL_API_KEY=your_api_key\nSKYSQL_HOST=your_host\nSKYSQL_USER=your_user\nSKYSQL_PASSWORD=your_password\nSKYSQL_DATABASE=your_database\n```\n\n4. Start the development server:\n```bash\nnpm run dev\n```\n\n## API Endpoints\n\n- `POST /api/databases` - Create a new database\n- `GET /api/databases` - List all databases\n- `GET /api/databases/:id` - Get database status\n- `DELETE /api/databases/:id` - Delete a database\n- `POST /api/query` - Execute SQL queries\n- `GET /health` - Health check endpoint\n\n## Deployment\n\nThis project is configured for deployment on Smithery.ai. Follow these steps:\n\n1. Push your code to GitHub\n2. Connect your GitHub repository to Smithery.ai\n3. Configure your environment variables in Smithery.ai\n4. Deploy your application\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "serverless",
        "secure database",
        "databases secure",
        "serverless databases"
      ],
      "category": "databases"
    },
    "anpy-j--mcp-oracle": {
      "owner": "anpy-j",
      "name": "mcp-oracle",
      "url": "https://github.com/anpy-j/mcp-oracle",
      "imageUrl": "/freedevtools/mcp/pfp/anpy-j.webp",
      "description": "Facilitates access to Oracle databases through a Model Context Protocol server, enabling efficient and secure querying and interaction with Oracle data for AI applications. Simplifies integration of Oracle data into AI workflows with minimal configuration.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-30T07:06:09Z",
      "readme_content": "# mcp-server-oracle\nModel Context Protocol server to access oracle\n\n[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Demos\n\n\nhttps://github.com/user-attachments/assets/dc4e377b-4efb-43e6-85fa-93ed852fe21f\n\n\n\n## Quickstart\n\nTo try this in Claude Desktop app, add this to your claude config files:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-oracle\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-oracle\"\n      ],\n      \"env\": {\n        \"ORACLE_CONNECTION_STRING\": \"username/password@hostname:password/service_name\"\n      }\n    }\n  }\n}\n```\n\n### Prerequisites\n\n- UV (pacakge manager)\n- Python 3.12+\n- Claude Desktop\n\n### Installation\n\n#### Claude Desktop Configuration\n\nAdd the server configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n\n## Contributing\n\n1. Fork the repository from [mcp-server-oracle](https://github.com/hdcola/mcp-server-oracle)\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oracle",
        "databases",
        "database",
        "oracle databases",
        "mcp oracle",
        "access oracle"
      ],
      "category": "databases"
    },
    "antonorlov--mcp-postgres-server": {
      "owner": "antonorlov",
      "name": "mcp-postgres-server",
      "url": "https://github.com/antonorlov/mcp-postgres-server",
      "imageUrl": "/freedevtools/mcp/pfp/antonorlov.webp",
      "description": "Interact with PostgreSQL databases, execute queries, manage connections, and retrieve data through a standardized interface for AI model integration.",
      "stars": 7,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-23T09:31:47Z",
      "readme_content": "# MCP PostgreSQL Server\n\nA Model Context Protocol server that provides PostgreSQL database operations. This server enables AI models to interact with PostgreSQL databases through a standardized interface.\n\n## Installation\n\n### Manual Installation\n\n```bash\nnpm install mcp-postgres-server\n```\n\nOr run directly with:\n\n```bash\nnpx mcp-postgres-server\n```\n\n## Configuration\n\nThe server requires the following environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-postgres-server\"],\n      \"env\": {\n        \"PG_HOST\": \"your_host\",\n        \"PG_PORT\": \"5432\",\n        \"PG_USER\": \"your_user\",\n        \"PG_PASSWORD\": \"your_password\",\n        \"PG_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. connect_db\n\nEstablish connection to PostgreSQL database using provided credentials.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"connect_db\",\n  arguments: {\n    host: \"localhost\",\n    port: 5432,\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n### 2. query\n\nExecute SELECT queries with optional prepared statement parameters. Supports both PostgreSQL-style ($1, $2) and MySQL-style (?) parameter placeholders.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = $1\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\n\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters. Supports both PostgreSQL-style ($1, $2) and MySQL-style (?) parameter placeholders.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES ($1, $2)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_schemas\n\nList all schemas in the connected database.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"list_schemas\",\n  arguments: {}\n});\n```\n\n### 5. list_tables\n\nList tables in the connected database. Accepts an optional schema parameter (defaults to 'public').\n\n```javascript\n// List tables in the 'public' schema (default)\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"list_tables\",\n  arguments: {}\n});\n\n// List tables in a specific schema\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"list_tables\",\n  arguments: {\n    schema: \"my_schema\"\n  }\n});\n```\n\n### 6. describe_table\n\nGet the structure of a specific table. Accepts an optional schema parameter (defaults to 'public').\n\n```javascript\n// Describe a table in the 'public' schema (default)\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n\n// Describe a table in a specific schema\nuse_mcp_tool({\n  server_name: \"postgres\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\",\n    schema: \"my_schema\"\n  }\n});\n```\n\n## Features\n\n* Secure connection handling with automatic cleanup\n* Prepared statement support for query parameters\n* Support for both PostgreSQL-style ($1, $2) and MySQL-style (?) parameter placeholders\n* Comprehensive error handling and validation\n* TypeScript support\n* Automatic connection management\n* Supports PostgreSQL-specific syntax and features\n* Multi-schema support for database operations\n\n## Security\n\n* Uses prepared statements to prevent SQL injection\n* Supports secure password handling through environment variables\n* Validates queries before execution\n* Automatically closes connections when done\n\n## Error Handling\n\nThe server provides detailed error messages for common issues:\n\n* Connection failures\n* Invalid queries\n* Missing parameters\n* Database errors\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "apache--iotdb-mcp-server": {
      "owner": "apache",
      "name": "iotdb-mcp-server",
      "url": "https://github.com/apache/iotdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/apache.webp",
      "description": "Enable secure interaction with IoTDB databases to run SQL queries and explore database schemas through a structured interface.",
      "stars": 28,
      "forks": 14,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-17T08:29:43Z",
      "readme_content": "# IoTDB MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@apache/iotdb-mcp-server)](https://smithery.ai/server/@apache/iotdb-mcp-server)\n\nEnglish | [中文](README-zh.md)\n\n## Overview\n\nA Model Context Protocol (MCP) server implementation that provides database interaction and business intelligence capabilities through IoTDB. This server enables running SQL queries and interacting with IoTDB using different SQL dialects (Tree Model and Table Model).\n\n## Components\n\n### Resources\n\nThe server doesn't expose any resources.\n\n### Prompts\n\nThe server doesn't provide any prompts.\n\n### Tools\n\nThe server offers different tools for IoTDB Tree Model and Table Model. You can choose between them by setting the \"IOTDB_SQL_DIALECT\" configuration to either \"tree\" or \"table\".\n\n#### Tree Model\n\n- `metadata_query`\n  - Execute SHOW/COUNT queries to read metadata from the database\n  - Input:\n    - `query_sql` (string): The SHOW/COUNT SQL query to execute\n  - Supported query types:\n    - SHOW DATABASES [path]\n    - SHOW TIMESERIES [path]\n    - SHOW CHILD PATHS [path]\n    - SHOW CHILD NODES [path]\n    - SHOW DEVICES [path]\n    - COUNT TIMESERIES [path]\n    - COUNT NODES [path]\n    - COUNT DEVICES [path]\n  - Returns: Query results as array of objects\n- `select_query`\n  - Execute SELECT queries to read data from the database\n  - Input:\n    - `query_sql` (string): The SELECT SQL query to execute (using TREE dialect, time using ISO 8601 format, e.g. 2017-11-01T00:08:00.000)\n  - Supported functions:\n    - SUM, COUNT, MAX_VALUE, MIN_VALUE, AVG, VARIANCE, MAX_TIME, MIN_TIME, etc.\n  - Returns: Query results as array of objects\n- `export_query`\n  - Execute a query and export the results to a CSV or Excel file\n  - Input:\n    - `query_sql` (string): The SQL query to execute (using TREE dialect)\n    - `format` (string): Export format, either \"csv\" or \"excel\" (default: \"csv\")\n    - `filename` (string): Optional filename for the exported file. If not provided, a unique filename will be generated.\n  - Returns: Information about the exported file and a preview of the data (max 10 rows)\n\n#### Table Model\n\n##### Query Tools\n\n- `read_query`\n  - Execute SELECT queries to read data from the database\n  - Input:\n    - `query_sql` (string): The SELECT SQL query to execute (using TABLE dialect, time using ISO 8601 format, e.g. 2017-11-01T00:08:00.000)\n  - Returns: Query results as array of objects\n\n##### Schema Tools\n\n- `list_tables`\n\n  - Get a list of all tables in the database\n  - No input required\n  - Returns: Array of table names\n\n- `describe_table`\n\n  - View schema information for a specific table\n  - Input:\n    - `table_name` (string): Name of table to describe\n  - Returns: Array of column definitions with names and types\n\n- `export_table_query`\n  - Execute a query and export the results to a CSV or Excel file\n  - Input:\n    - `query_sql` (string): The SQL query to execute (using TABLE dialect)\n    - `format` (string): Export format, either \"csv\" or \"excel\" (default: \"csv\")\n    - `filename` (string): Optional filename for the exported file. If not provided, a unique filename will be generated.\n  - Returns: Information about the exported file and a preview of the data (max 10 rows)\n\n## Configuration Options\n\nIoTDB MCP Server supports the following configuration options, which can be set via environment variables or command-line arguments:\n\n| Option        | Environment Variable | Default Value | Description                      |\n| ------------- | -------------------- | ------------- | -------------------------------- |\n| --host        | IOTDB_HOST           | 127.0.0.1     | IoTDB host address               |\n| --port        | IOTDB_PORT           | 6667          | IoTDB port                       |\n| --user        | IOTDB_USER           | root          | IoTDB username                   |\n| --password    | IOTDB_PASSWORD       | root          | IoTDB password                   |\n| --database    | IOTDB_DATABASE       | test          | IoTDB database name              |\n| --sql-dialect | IOTDB_SQL_DIALECT    | table         | SQL dialect: tree or table       |\n| --export-path | IOTDB_EXPORT_PATH    | /tmp          | Path for exporting query results |\n\n## Performance Optimizations\n\nIoTDB MCP Server includes the following performance optimization features:\n\n1. **Session Pool Management**: Uses optimized session pool configurations, supporting up to 100 concurrent sessions\n2. **Optimized Fetch Size**: For queries, a fetch size of 1024 is set\n3. **Connection Retry**: Configured automatic retry mechanism for connection failures\n4. **Timeout Management**: Session wait timeout set to 5000 milliseconds for improved reliability\n5. **Export Functionality**: Support for exporting query results to CSV or Excel formats\n\n## Prerequisites\n\n- Python environment\n- `uv` package manager\n- IoTDB installation\n- MCP server dependencies\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/apache/iotdb-mcp-server.git\ncd iotdb-mcp-server\n\n# Create virtual environment\nuv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install development dependencies\nuv sync\n```\n\n## Claude Desktop Integration\n\nConfigure the MCP server in Claude Desktop's configuration file:\n\n#### macOS\n\nLocation: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n#### Windows\n\nLocation: `%APPDATA%/Claude/claude_desktop_config.json`\n\n**You may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS/Linux or `where uv` on Windows.**\n\n### Claude Desktop Configuration Example\n\nAdd the following configuration to Claude Desktop's configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"iotdb\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/your_username/iotdb-mcp-server/src/iotdb_mcp_server\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"IOTDB_HOST\": \"127.0.0.1\",\n        \"IOTDB_PORT\": \"6667\",\n        \"IOTDB_USER\": \"root\",\n        \"IOTDB_PASSWORD\": \"root\",\n        \"IOTDB_DATABASE\": \"test\",\n        \"IOTDB_SQL_DIALECT\": \"table\",\n        \"IOTDB_EXPORT_PATH\": \"/path/to/export/folder\"\n      }\n    }\n  }\n}\n```\n\n> **Note**: Make sure to replace the `--directory` parameter's path with your actual repository clone path.\n\n## Error Handling and Logging\n\nIoTDB MCP Server includes comprehensive error handling and logging capabilities:\n\n1. **Log Level**: Logging level is set to INFO, allowing you to view server status in the console\n2. **Exception Handling**: All database operations include exception handling to ensure graceful handling and meaningful error messages when errors occur\n3. **Session Management**: Automatic closure of used sessions to prevent resource leaks\n4. **Parameter Validation**: Basic validation of user-input SQL queries to ensure only allowed query types are executed\n\n## Docker Support\n\nYou can build a container image for the IoTDB MCP Server using the `Dockerfile` in the project root:\n\n```bash\n# Build Docker image\ndocker build -t iotdb-mcp-server .\n\n# Run container\ndocker run -e IOTDB_HOST=<your-iotdb-host> -e IOTDB_PORT=<your-iotdb-port> -e IOTDB_USER=<your-iotdb-user> -e IOTDB_PASSWORD=<your-iotdb-password> iotdb-mcp-server\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "iotdb",
        "databases",
        "database",
        "iotdb databases",
        "apache iotdb",
        "secure database"
      ],
      "category": "databases"
    },
    "aqara-docs--mysql-mcp-server": {
      "owner": "aqara-docs",
      "name": "mysql-mcp-server",
      "url": "https://github.com/aqara-docs/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/aqara-docs.webp",
      "description": "Enables secure interaction with MySQL databases by listing available tables, reading data, and executing SQL queries through a structured interface. Provides mechanisms for safe database access, comprehensive logging, and error handling.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-02T23:43:34Z",
      "readme_content": "\n# Smithery AI에 등록하는 방법을 알기 위해서 다음 사이트의 코드를 한글로만 바꿔서 올렸습니다.\n#https://github.com/designcomputer/mysql_mcp_server\n\n# MySQL MCP 서버\nMySQL 데이터베이스와의 안전한 상호작용을 가능하게 하는 Model Context Protocol (MCP) 구현체입니다. 이 서버 컴포넌트는 AI 애플리케이션(호스트/클라이언트)과 MySQL 데이터베이스 간의 통신을 용이하게 하여, 제어된 인터페이스를 통해 데이터베이스 탐색 및 분석을 더 안전하고 구조화된 방식으로 수행할 수 있게 합니다.\n\n> **참고**: MySQL MCP 서버는 독립 실행형 서버로 사용하도록 설계되지 않았으며, AI 애플리케이션과 MySQL 데이터베이스 간의 통신 프로토콜 구현체로 사용됩니다.\n\n## 주요 기능\n- 사용 가능한 MySQL 테이블을 리소스로 나열\n- 테이블 내용 읽기\n- 적절한 오류 처리가 포함된 SQL 쿼리 실행\n- 환경 변수를 통한 안전한 데이터베이스 접근\n- 포괄적인 로깅\n\n## 설치\n### 수동 설치\n```bash\npip install mysql-mcp-server\n```\n\n### Smithery를 통한 설치\n[Smithery](https://smithery.ai/server/mysql-mcp-server)를 통해 Claude Desktop용 MySQL MCP 서버를 자동으로 설치하려면:\n```bash\nnpx -y @smithery/cli install mysql-mcp-server --client claude\n```\n\n## 설정\n다음 환경 변수를 설정하세요:\n```bash\nMYSQL_HOST=localhost     # 데이터베이스 호스트\nMYSQL_PORT=3306         # 선택사항: 데이터베이스 포트 (지정하지 않으면 기본값 3306)\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=your_database\n```\n\n## 사용법\n### Claude Desktop과 함께 사용\n`claude_desktop_config.json`에 다음을 추가하세요:\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"path/to/mysql_mcp_server\",\n        \"run\",\n        \"mysql_mcp_server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### Visual Studio Code와 함께 사용\n`mcp.json`에 다음을 추가하세요:\n```json\n{\n  \"servers\": {\n      \"mysql\": {\n            \"type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"mysql-mcp-server\",\n                \"mysql_mcp_server\"\n            ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n  }\n}\n```\n참고: 이 기능을 사용하려면 uv를 설치해야 합니다.\n\n### MCP Inspector로 디버깅\nMySQL MCP 서버는 독립 실행형으로 또는 Python을 통해 직접 명령줄에서 실행하도록 설계되지 않았지만, MCP Inspector를 사용하여 디버깅할 수 있습니다.\n\nMCP Inspector는 MCP 구현을 테스트하고 디버깅하는 편리한 방법을 제공합니다:\n\n```bash\n# 의존성 설치\npip install -r requirements.txt\n# 디버깅을 위해 MCP Inspector 사용 (Python으로 직접 실행하지 마세요)\n```\n\nMySQL MCP 서버는 Claude Desktop과 같은 AI 애플리케이션과 통합되도록 설계되었으며, 독립 실행형 Python 프로그램으로 직접 실행해서는 안 됩니다.\n\n## 개발\n```bash\n# 저장소 클론\ngit clone https://github.com/yourusername/mysql_mcp_server.git\ncd mysql_mcp_server\n# 가상 환경 생성\npython -m venv venv\nsource venv/bin/activate  # Windows에서는 `venv\\Scripts\\activate`\n# 개발 의존성 설치\npip install -r requirements-dev.txt\n# 테스트 실행\npytest\n```\n\n## 보안 고려사항\n- 환경 변수나 자격 증명을 절대 커밋하지 마세요\n- 최소한의 필요한 권한만 가진 데이터베이스 사용자를 사용하세요\n- 프로덕션 사용을 위해 쿼리 화이트리스팅 구현을 고려하세요\n- 모든 데이터베이스 작업을 모니터링하고 로깅하세요\n\n## 보안 모범 사례\n이 MCP 구현은 기능을 위해 데이터베이스 접근이 필요합니다. 보안을 위해:\n1. **최소 권한을 가진 전용 MySQL 사용자 생성**\n2. **루트 자격 증명이나 관리자 계정을 절대 사용하지 않기**\n3. **필요한 작업에만 데이터베이스 접근 제한**\n4. **감사 목적으로 로깅 활성화**\n5. **데이터베이스 접근에 대한 정기적인 보안 검토**\n\n자세한 지침은 [MySQL 보안 설정 가이드](https://github.com/designcomputer/mysql_mcp_server/blob/main/SECURITY.md)를 참조하세요:\n- 제한된 MySQL 사용자 생성\n- 적절한 권한 설정\n- 데이터베이스 접근 모니터링\n- 보안 모범 사례\n\n⚠️ 중요: 데이터베이스 접근을 구성할 때 항상 최소 권한 원칙을 따르세요.\n\n## 라이선스\nMIT 라이선스 - 자세한 내용은 LICENSE 파일을 참조하세요.\n\n## 기여하기\n1. 저장소를 포크하세요\n2. 기능 브랜치를 생성하세요 (`git checkout -b feature/amazing-feature`)\n3. 변경사항을 커밋하세요 (`git commit -m 'Add some amazing feature'`)\n4. 브랜치에 푸시하세요 (`git push origin feature/amazing-feature`)\n5. Pull Request를 열어주세요\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "databases secure",
        "mysql databases"
      ],
      "category": "databases"
    },
    "aqaranewbiz--mcp-mysql": {
      "owner": "aqaranewbiz",
      "name": "mcp-mysql",
      "url": "https://github.com/aqaranewbiz/mcp-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/aqaranewbiz.webp",
      "description": "Connect to MySQL databases for secure read-only queries and database exploration. Perform operations like listing databases and tables, retrieving schemas, and executing read-only SQL commands while ensuring built-in validation to prevent unsafe actions.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-25T11:50:47Z",
      "readme_content": "# MySQL MCP Server for Smithery (Python)\n\nA MySQL connector for Smithery that allows you to connect to your MySQL database directly from Smithery, built with Python.\n\n## Features\n\n- **Connect to MySQL Databases**: Configure and connect to MySQL databases\n- **List Databases**: View all accessible databases\n- **List Tables**: View all tables in a specified database\n- **Describe Tables**: Get detailed schema information for tables\n- **Execute Queries**: Run read-only SQL queries (SELECT, SHOW, DESCRIBE, EXPLAIN)\n- **Security**: Built-in query validation ensures only read-only operations are allowed\n\n## Installation in Smithery\n\nAfter adding the MCP server in Smithery, you'll be able to enter your MySQL database credentials:\n\n- **Host**: Database server hostname (default: localhost)\n- **Port**: Database server port (default: 3306)\n- **User**: Your MySQL username\n- **Password**: Your MySQL password\n- **Database**: (Optional) The specific database to connect to\n\n## Manual Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/aqaralife/mysql-mcp-python-server.git\n```\n\n2. Install dependencies:\n```bash\ncd mysql-mcp-python-server\nnpm install\npip install -r requirements.txt\n```\n\n3. Make the scripts executable (Unix/Linux/Mac):\n```bash\nchmod +x mcp_server.py run.js\n```\n\n## Manual Usage\n\nTo start the server:\n```bash\nnode run.js\n```\n\nOr, directly run the Python script:\n```bash\npython mcp_server.py\n```\n\n## Available Tools\n\n### connect_db\nEstablishes a connection to a MySQL database.\n\n**Parameters:**\n- **host**: Database server hostname\n- **port**: Database server port\n- **user**: Database username\n- **password**: Database password\n- **database**: (Optional) Database name\n\n### list_databases\nLists all accessible databases.\n\n**Parameters:** None\n\n### list_tables\nLists all tables in a database.\n\n**Parameters:**\n- **database**: (Optional) Database name, uses default if connected\n\n### describe_table\nShows the schema for a table.\n\n**Parameters:**\n- **table**: Table name\n- **database**: (Optional) Database name, uses default if connected\n\n### execute_query\nExecutes a read-only SQL query.\n\n**Parameters:**\n- **query**: SQL query (only SELECT, SHOW, DESCRIBE, and EXPLAIN allowed)\n- **database**: (Optional) Database name, uses default if connected\n\n## Security\n\nThe server includes built-in validation to ensure only read-only operations are permitted:\n\n- Only SELECT, SHOW, DESCRIBE, and EXPLAIN queries are allowed\n- Queries containing SQL commands like INSERT, UPDATE, DELETE, DROP, etc. are automatically rejected\n- Multiple statements in a single query (separated by semicolons) are not allowed\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Python Not Found**: The server will automatically detect `python3` or `python`. If neither works, ensure Python is installed and in your PATH.\n\n2. **Missing Modules**: The server will attempt to install required packages automatically. If this fails, manually run:\n   ```bash\n   pip install mysql-connector-python>=8.0.0\n   ```\n\n3. **Connection Issues**: Verify your database credentials and ensure the MySQL server is running and accessible.\n\n4. **Smithery Connection Issues**: Make sure the settings in Smithery are correctly configured with your database credentials.\n\n5. **Server Unresponsive**: Check the log output in Smithery's console for errors.\n\n## License\n\nMIT\n\n## Contact\n\nIf you have any questions, please create an issue. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "mysql databases"
      ],
      "category": "databases"
    },
    "aqaranewbiz--mcp-mysql-server": {
      "owner": "aqaranewbiz",
      "name": "mcp-mysql-server",
      "url": "https://github.com/aqaranewbiz/mcp-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/aqaranewbiz.webp",
      "description": "Connect securely to MySQL databases and perform read-only queries. Retrieve detailed schema information and list databases and tables with built-in validation for safe operations.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-25T10:48:20Z",
      "readme_content": "# MySQL MCP Server for Smithery (Python)\n\nA MySQL connector for Smithery that allows you to connect to your MySQL database directly from Smithery, built with Python.\n\n## Features\n\n- **Connect to MySQL Databases**: Configure and connect to MySQL databases\n- **List Databases**: View all accessible databases\n- **List Tables**: View all tables in a specified database\n- **Describe Tables**: Get detailed schema information for tables\n- **Execute Queries**: Run read-only SQL queries (SELECT, SHOW, DESCRIBE, EXPLAIN)\n- **Security**: Built-in query validation ensures only read-only operations are allowed\n\n## Installation in Smithery\n\nAfter adding the MCP server in Smithery, you'll be able to enter your MySQL database credentials:\n\n- **Host**: Database server hostname (default: localhost)\n- **Port**: Database server port (default: 3306)\n- **User**: Your MySQL username\n- **Password**: Your MySQL password\n- **Database**: (Optional) The specific database to connect to\n\n## Manual Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/aqaralife/mysql-mcp-python-server.git\n```\n\n2. Install dependencies:\n```bash\ncd mysql-mcp-python-server\nnpm install\npip install -r requirements.txt\n```\n\n3. Make the scripts executable (Unix/Linux/Mac):\n```bash\nchmod +x mcp_server.py run.js\n```\n\n## Manual Usage\n\nTo start the server:\n```bash\nnode run.js\n```\n\nOr, directly run the Python script:\n```bash\npython mcp_server.py\n```\n\n## Available Tools\n\n### connect_db\nEstablishes a connection to a MySQL database.\n\n**Parameters:**\n- **host**: Database server hostname\n- **port**: Database server port\n- **user**: Database username\n- **password**: Database password\n- **database**: (Optional) Database name\n\n### list_databases\nLists all accessible databases.\n\n**Parameters:** None\n\n### list_tables\nLists all tables in a database.\n\n**Parameters:**\n- **database**: (Optional) Database name, uses default if connected\n\n### describe_table\nShows the schema for a table.\n\n**Parameters:**\n- **table**: Table name\n- **database**: (Optional) Database name, uses default if connected\n\n### execute_query\nExecutes a read-only SQL query.\n\n**Parameters:**\n- **query**: SQL query (only SELECT, SHOW, DESCRIBE, and EXPLAIN allowed)\n- **database**: (Optional) Database name, uses default if connected\n\n## Security\n\nThe server includes built-in validation to ensure only read-only operations are permitted:\n\n- Only SELECT, SHOW, DESCRIBE, and EXPLAIN queries are allowed\n- Queries containing SQL commands like INSERT, UPDATE, DELETE, DROP, etc. are automatically rejected\n- Multiple statements in a single query (separated by semicolons) are not allowed\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Python Not Found**: The server will automatically detect `python3` or `python`. If neither works, ensure Python is installed and in your PATH.\n\n2. **Missing Modules**: The server will attempt to install required packages automatically. If this fails, manually run:\n   ```bash\n   pip install mysql-connector-python>=8.0.0\n   ```\n\n3. **Connection Issues**: Verify your database credentials and ensure the MySQL server is running and accessible.\n\n4. **Smithery Connection Issues**: Make sure the settings in Smithery are correctly configured with your database credentials.\n\n5. **Server Unresponsive**: Check the log output in Smithery's console for errors.\n\n## License\n\nMIT\n\n## Contact\n\nIf you have any questions, please create an issue. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "securely mysql",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "aqaranewbiz--mysql-aqara": {
      "owner": "aqaranewbiz",
      "name": "mysql-aqara",
      "url": "https://github.com/aqaranewbiz/mysql-aqara",
      "imageUrl": "/freedevtools/mcp/pfp/aqaranewbiz.webp",
      "description": "Directly interact with MySQL databases through a standardized MCP interface to perform operations such as connecting, querying, modifying tables, and listing schema details. Streamlines database management and integration within LLM applications.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-17T04:52:18Z",
      "readme_content": "# MySQL MCP Server for Smithery\n\nA MySQL connector for Smithery that allows you to connect to your MySQL database directly from Smithery.\n\n## One-Click Installation\n\n### Global Installation\n```bash\nnpm install -g mysql-aqara\n```\n\n### Local Installation\n```bash\nnpm install mysql-aqara\n```\n\n## Manual Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/aqaranewbiz/mysql-aqara.git\n```\n\n2. Install dependencies:\n```bash\ncd mysql-aqara\nnpm install\npip install -r requirements.txt\n```\n\n3. Make the run.js file executable (Unix/Linux/Mac):\n```bash\nchmod +x run.js\n```\n\n## Usage\n\n### Using Global Installation\n```bash\nmysql-aqara\n```\n\n### Using Local Installation\n```bash\nnpx mysql-aqara\n```\n\n### Direct Execution\n```bash\nnode run.js\n```\n\n## Features\n\n- **Smart Path Detection**: Automatically finds the Python script in various locations\n- **Cross-Platform Support**: Works on Windows, macOS, and Linux\n- **Automatic Python Detection**: Uses `python3` or `python` depending on your system\n- **Automatic Requirements Installation**: Installs required Python packages on startup\n- **Improved Error Handling**: Better feedback for troubleshooting\n\n## Configuration\n\nNo environment variables required! When connecting to a database, you'll need to provide:\n\n- **host**: Database server hostname or IP address\n- **user**: Database username\n- **password**: Database password\n- **database**: Database name\n\n## Available Tools\n\n### connect_db\nEstablishes a connection to the MySQL database.\n\n**Parameters:**\n- **host**: Database server hostname\n- **user**: Database username\n- **password**: Database password\n- **database**: Database name\n\n### create_or_modify_table\nCreates a new table or modifies an existing one.\n\n**Parameters:**\n- **table_name**: Name of the table\n- **columns**: Array of column definitions\n\n### execute_query\nExecutes a SELECT query on the database.\n\n**Parameters:**\n- **query**: SQL SELECT query\n- **params** (optional): Parameters for the query\n\n### execute_command\nExecutes an INSERT, UPDATE, or DELETE query.\n\n**Parameters:**\n- **command**: SQL command to execute\n- **params** (optional): Parameters for the command\n\n### list_tables\nLists all tables in the connected database.\n\n**Parameters:** None\n\n### describe_table\nGets the structure of a specific table.\n\n**Parameters:**\n- **table_name**: Name of the table to describe\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Python Not Found**: The server will automatically detect `python3` or `python`. If neither works, ensure Python is installed and in your PATH.\n\n2. **Missing Modules**: The server will attempt to install required packages automatically. If this fails, manually run:\n   ```bash\n   pip install mysql-connector-python>=8.0.0\n   ```\n\n3. **Connection Issues**: Verify your database credentials and ensure the MySQL server is running and accessible.\n\n4. **Script Path Issues**: The server checks multiple locations for the Python script. If it can't find it, ensure the `mcp_server.py` file is in the same directory as `index.js` or in the current working directory.\n\n## License\n\nMIT\n\n## Contact\n\nIf you have any questions, please create an issue. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "aqaranewbiz mysql",
        "mysql databases",
        "secure database"
      ],
      "category": "databases"
    },
    "aqaranewbiz--mysql-aqaranewbiz": {
      "owner": "aqaranewbiz",
      "name": "mysql-aqaranewbiz",
      "url": "https://github.com/aqaranewbiz/mysql-aqaranewbiz",
      "imageUrl": "/freedevtools/mcp/pfp/aqaranewbiz.webp",
      "description": "Connect and execute queries on a MySQL database using the MCP protocol for efficient interaction. Provides a standardized API for seamless database operations.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-14T13:53:42Z",
      "readme_content": "# MySQL MCP 서버\n[![smithery badge](https://smithery.ai/badge/@aqaranewbiz/mysql-aqaranewbiz)](https://smithery.ai/server/@aqaranewbiz/mysql-aqaranewbiz)\n이 프로젝트는 Smithery의 Model Context Protocol (MCP)을 사용하여 MySQL 데이터베이스와 상호작용하는 서버입니다.\n\n## Installation\n\n### Installing via Smithery\nTo install MySQL Server for Claude Desktop automatically via Smithery:\n\n```bash\nnpx -y @smithery/cli install @aqaranewbiz/mysql-aqaranewbiz --client claude\n```\n\n### Manual Installation\n```bash\nnpx @aqaranewbiz/mysql-aqaranewbiz\n```\n\n## Configuration\n\nThe server requires the following environment variables to be set in your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@aqaranewbiz/mysql-aqaranewbiz\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your_host\",\n        \"MYSQL_USER\": \"your_user\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. connect_db\nEstablish connection to MySQL database using provided credentials.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"connect_db\",\n  arguments: {\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n### 2. query\nExecute SELECT queries with optional prepared statement parameters.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_tables\nList all tables in the connected database.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"list_tables\",\n  arguments: {}\n});\n```\n\n### 5. describe_table\nGet the structure of a specific table.\n\n```javascript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n## 주요 기능\n\n- MySQL 데이터베이스 연결 및 쿼리 실행\n- MCP 프로토콜을 통한 표준화된 API 제공\n- FastAPI 기반의 RESTful API 엔드포인트\n- 환경 변수를 통한 설정 관리\n\n## 시작하기\n\n### 필수 요구사항\n\n- Python 3.11 이상\n- MySQL 서버\n- Docker (선택사항)\n\n### 환경 설정\n\n1. `.env` 파일 생성:\n```env\nMYSQL_HOST=your_mysql_host\nMYSQL_USER=your_mysql_user\nMYSQL_PASSWORD=your_mysql_password\nMYSQL_DATABASE=your_database_name\n```\n\n### 설치 방법\n\n#### 로컬 설치 (권장)\n\n1. Python 가상환경 생성 및 활성화:\n```bash\n# Windows\npython -m venv venv\nvenv\\Scripts\\activate\n\n# macOS/Linux\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n2. 의존성 설치:\n```bash\npip install -r requirements.txt\n```\n\n3. 서버 실행:\n```bash\npython mcp_server.py\n```\n\n#### Docker 설치\n\n1. Docker 이미지 빌드:\n```bash\ndocker build -t mysql-mcp-server .\n```\n\n2. 컨테이너 실행:\n```bash\ndocker run -e MYSQL_HOST=host -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db mysql-mcp-server\n```\n\n### 로컬 개발 설정\n\n1. 개발 환경 설정:\n```bash\n# 개발용 의존성 설치\npip install -r requirements-dev.txt  # 필요한 경우 생성\n\n# 코드 포맷팅 및 린팅 설정\npip install black flake8\n```\n\n2. 코드 실행:\n```bash\n# 개발 모드로 실행\npython mcp_server.py --dev\n```\n\n3. 테스트 실행:\n```bash\n# 테스트 실행\npython -m pytest tests/\n```\n\n## API 엔드포인트\n\n### 서버 정보 조회\n```\nGET /status\n```\n서버의 상태와 사용 가능한 도구 목록을 반환합니다.\n\n### 쿼리 실행\n```\nPOST /execute\n```\nMySQL 쿼리를 실행하고 결과를 반환합니다.\n\n## 개발 가이드\n\n### 프로젝트 구조\n```\n@MCP-Server-for-Smithery/\n├── mcp_server.py      # 메인 서버 코드\n├── requirements.txt   # Python 의존성\n├── Dockerfile        # Docker 설정\n├── .env              # 환경 변수 (템플릿)\n└── tests/            # 테스트 코드\n```\n\n### 새로운 기능 추가\n\n1. `mcp_server.py`에 새로운 도구 추가\n2. 필요한 의존성 `requirements.txt`에 추가\n3. 테스트 코드 작성\n4. Docker 이미지 재빌드 (Docker 사용 시)\n\n## 문제 해결\n\n### 일반적인 문제\n\n1. 연결 오류:\n   - MySQL 서버가 실행 중인지 확인\n   - 환경 변수가 올바르게 설정되었는지 확인\n   - 로컬 설치 시 MySQL 클라이언트 라이브러리가 설치되어 있는지 확인\n\n2. 쿼리 실행 오류:\n   - SQL 구문 검사\n   - 데이터베이스 권한 확인\n   - 로컬 설치 시 MySQL 커넥터 버전 확인\n\n### 로깅\n\n서버는 기본적으로 로그를 표준 출력에 기록합니다. 로컬 설치 시 로그 레벨을 조정하려면:\n```bash\npython mcp_server.py --log-level DEBUG\n```\n\nDocker를 사용하는 경우 로그를 확인하려면:\n```bash\ndocker logs [container-id]\n```\n\n## 기여하기\n\n1. 이슈 생성\n2. 브랜치 생성 (`git checkout -b feature/AmazingFeature`)\n3. 변경사항 커밋 (`git commit -m 'Add some AmazingFeature'`)\n4. 브랜치 푸시 (`git push origin feature/AmazingFeature`)\n5. Pull Request 생성\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스 하에 배포됩니다. 자세한 내용은 `LICENSE` 파일을 참조하세요.\n\n## 연락처\n\n문의사항이 있으시면 이슈를 생성해주세요. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "aqaranewbiz mysql",
        "databases secure"
      ],
      "category": "databases"
    },
    "arjshiv--blaze-sql-mcp-server": {
      "owner": "arjshiv",
      "name": "blaze-sql-mcp-server",
      "url": "https://github.com/arjshiv/blaze-sql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/arjshiv.webp",
      "description": "Enables natural language querying of databases through the BlazeSQL API, transforming plain English queries into SQL and JSON results. Facilitates seamless integration with MCP-compatible clients for enhanced data interaction.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-07T02:06:27Z",
      "readme_content": "# BlazeSQL MCP Server\n\nThis project implements a Model Context Protocol (MCP) server using the `@modelcontextprotocol/sdk` that acts as a proxy to the BlazeSQL Natural Language Query API. It allows MCP-compatible clients (like Cursor, Claude 3 with tool use, the MCP Inspector, etc.) to interact with BlazeSQL using natural language.\n\n## Features\n\n*   Built using the modern `McpServer` helper class from the MCP SDK.\n*   Exposes the BlazeSQL Natural Language Query API as an MCP tool named `blazesql_query`.\n*   Uses `zod` for robust validation of tool input parameters.\n*   Handles API key authentication securely via environment variables.\n*   Communicates with clients using the standard MCP stdio transport.\n\n## Workflow Diagram\n\nThis diagram shows the sequence of interactions when a client uses the `blazesql_query` tool (Note: The internal server logic now uses `McpServer` which simplifies tool registration compared to the low-level handlers shown in the diagram):\n\n```mermaid\nsequenceDiagram\n    participant Client as MCP Client (e.g., Cursor)\n    participant Server as BlazeSQL MCP Server (index.ts)\n    participant Env as Environment (.env)\n    participant BlazeAPI as BlazeSQL API\n\n    Client->>Server: ListTools Request (via stdio)\n    Server-->>Client: ListTools Response (tools: [blazesql_query]) (via stdio)\n\n    Client->>Server: CallTool Request (blazesql_query, db_id, nl_request) (via stdio)\n    Server->>Env: Read BLAZE_API_KEY\n    Env-->>Server: BLAZE_API_KEY\n    Server->>BlazeAPI: POST /natural_language_query_api (apiKey, db_id, nl_request)\n    BlazeAPI->>BlazeAPI: Process Query (NL->SQL, Execute)\n    BlazeAPI-->>Server: HTTPS Response (JSON: agent_response, query, data_result OR error)\n    Server->>Server: Format Response (Agent response, SQL, and data into single text block)\n    Server-->>Client: CallTool Response (content: [{type: text, text: formattedMarkdown}]) (via stdio)\n\n```\n\n## Prerequisites\n\n*   [Node.js](https://nodejs.org/) (LTS version recommended)\n*   [Yarn](https://yarnpkg.com/) (Classic or Berry)\n*   A BlazeSQL account with an API Key (Team Advanced subscription required for the API).\n*   At least one database connection configured in your BlazeSQL account.\n*   BlazeSQL Natural Language Query API Documentation: [https://help.blazesql.com/en/article/natural-language-query-api-1fgx4au/](https://help.blazesql.com/en/article/natural-language-query-api-1fgx4au/)\n\n## Setup\n\n1.  **Clone the Repository:**\n    ```bash\n    git clone <repository-url>\n    cd blaze-sql-mcp-server\n    ```\n\n2.  **Install Dependencies:**\n    ```bash\n    yarn install\n    ```\n    This will install all necessary dependencies, including the `@modelcontextprotocol/sdk`, `dotenv`, and `zod`.\n\n3.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.sample .env\n        ```\n    *   Edit the `.env` file:\n        ```dotenv\n        # .env\n        BLAZE_API_KEY=YOUR_BLAZESQL_API_KEY_HERE\n        ```\n        Replace `YOUR_BLAZESQL_API_KEY_HERE` with your actual API key obtained from your BlazeSQL account settings.\n\n## Running the Server\n\n1.  **Build the Server:**\n    Compile the TypeScript code to JavaScript:\n    ```bash\n    yarn build\n    ```\n\n2.  **Run the Server:**\n    Execute the compiled code:\n    ```bash\n    node build/index.js\n    ```\n    The server will start and log messages to `stderr` (you might see \"API Key loaded successfully...\" etc.). It is now listening for an MCP client connection via standard input/output (stdio).\n\n## Connecting an MCP Client\n\nThis server uses the **stdio** transport mechanism.\n\n### Using the MCP Inspector (Recommended for Testing)\n\n1.  Make sure the server is **not** already running separately.\n2.  Run the Inspector, telling it to launch your server:\n    ```bash\n    npx @modelcontextprotocol/inspector node build/index.js\n    ```\n3.  The Inspector UI will launch, automatically connecting to your server.\n4.  Navigate to the \"Tools\" tab to interact with the `blazesql_query` tool.\n\n### Using Integrated Clients (Cursor, Claude 3, etc.)\n\n1.  **Start the server** in a terminal:\n    ```bash\n    node build/index.js\n    ```\n2.  **Configure the client:** In your MCP client's settings, you need to add a custom server configuration.\n    *   **Transport:** Select `stdio`.\n    *   **Command:** Specify the exact command used to run the server. You need to provide the **absolute path** to node and the **absolute path** to the `build/index.js` file.\n        *   Example (macOS/Linux - adjust paths as needed):\n            `/usr/local/bin/node /Users/your_username/path/to/blaze-sql-mcp-server/build/index.js`\n        *   You can find the path to node using `which node` in your terminal.\n        *   You can find the path to the project using `pwd` inside the project directory.\n    *   Save the configuration.\n3.  The client should now be able to connect to your locally running server and list/use its tools.\n\n## Using the `blazesql_query` Tool\n\nOnce connected, the client can call the `blazesql_query` tool.\n\n*   **Tool Name:** `blazesql_query`\n*   **Arguments:**\n    *   `db_id` (string, required): The ID of the target database connection in your BlazeSQL account. You can find this ID in the BlazeSQL web application when managing your database connections.\n    *   `natural_language_request` (string, required): The query you want to execute, written in plain English (e.g., \"show me the total number of users\").\n    *(Input is validated using `zod`)*\n\n*   **Example Call (using `mcp test` syntax for illustration):**\n    ```bash\n    call-tool blazesql_query --db_id \"db_your_actual_db_id\" --natural_language_request \"What were the total sales last month?\"\n    ```\n\n*   **Output:**\n    If successful, the tool returns a single `text` content block containing:\n    *   The natural language response from the BlazeSQL agent.\n    *   The generated SQL query within a Markdown code fence (```sql ... ```).\n    *   The data results formatted as JSON within a Markdown code fence (```json ... ```).\n\n    Example structure within the `text` block:\n    ```markdown\n    **Agent Response:**\n    The total sales last month were $12345.67.\n\n    **Generated SQL:**\n    ```sql\n    SELECT sum(sales_amount) FROM sales WHERE sale_date >= date('now', '-1 month');\n    ```\n\n    **Data Result (JSON):**\n    ```json\n    [\n      {\n        \"sum(sales_amount)\": 12345.67\n      }\n    ]\n    ```\n    ```\n\n    If unsuccessful, it returns a `text` content block containing the error message from the BlazeSQL API and marks the response as an error (`isError: true`).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "blazesql",
        "databases",
        "database",
        "blaze sql",
        "secure database",
        "querying databases"
      ],
      "category": "databases"
    },
    "arjunkmrm--elasticsearch-mcp-server": {
      "owner": "arjunkmrm",
      "name": "elasticsearch-mcp-server",
      "url": "https://github.com/arjunkmrm/elasticsearch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/arjunkmrm.webp",
      "description": "Enables natural language interaction with Elasticsearch and OpenSearch clusters for managing indices, documents, aliases, and cluster health. Facilitates searches, document creation or deletion, and retrieval of cluster statistics through natural language commands.",
      "stars": 1,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-08-19T03:43:47Z",
      "readme_content": "# Elasticsearch/OpenSearch MCP Server\n\n[![smithery badge](https://smithery.ai/badge/elasticsearch-mcp-server)](https://smithery.ai/server/elasticsearch-mcp-server)\n\n## Overview\n\nA Model Context Protocol (MCP) server implementation that provides Elasticsearch and OpenSearch interaction. This server enables searching documents, analyzing indices, and managing cluster through a set of tools.\n\n<a href=\"https://glama.ai/mcp/servers/b3po3delex\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/b3po3delex/badge\" alt=\"Elasticsearch MCP Server\" /></a>\n\n## Demo\n\nhttps://github.com/user-attachments/assets/f7409e31-fac4-4321-9c94-b0ff2ea7ff15\n\n## Features\n\n### General Operations\n\n- `general_api_request`: Perform a general HTTP API request. Use this tool for any Elasticsearch/OpenSearch API that does not have a dedicated tool.\n\n### Index Operations\n\n- `list_indices`: List all indices.\n- `get_index`: Returns information (mappings, settings, aliases) about one or more indices.\n- `create_index`: Create a new index.\n- `delete_index`: Delete an index.\n\n### Document Operations\n\n- `search_documents`: Search for documents.\n- `index_document`: Creates or updates a document in the index.\n- `get_document`: Get a document by ID.\n- `delete_document`: Delete a document by ID.\n- `delete_by_query`: Deletes documents matching the provided query.\n\n### Cluster Operations\n\n- `get_cluster_health`: Returns basic information about the health of the cluster.\n- `get_cluster_stats`: Returns high-level overview of cluster statistics.\n\n### Alias Operations\n\n- `list_aliases`: List all aliases.\n- `get_alias`: Get alias information for a specific index.\n- `put_alias`: Create or update an alias for a specific index.\n- `delete_alias`: Delete an alias for a specific index.\n\n## Configure Environment Variables\n\nCopy the `.env.example` file to `.env` and update the values accordingly.\n\n## Start Elasticsearch/OpenSearch Cluster\n\nStart the Elasticsearch/OpenSearch cluster using Docker Compose:\n\n```bash\n# For Elasticsearch\ndocker-compose -f docker-compose-elasticsearch.yml up -d\n\n# For OpenSearch\ndocker-compose -f docker-compose-opensearch.yml up -d\n```\n\nThe default Elasticsearch username is `elastic` and password is `test123`. The default OpenSearch username is `admin` and password is `admin`.\n\nYou can access Kibana/OpenSearch Dashboards from http://localhost:5601.\n\n## Usage with Claude Desktop\n\n### Option 1: Installing via Smithery\n\nTo install Elasticsearch Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/elasticsearch-mcp-server):\n\n```bash\nnpx -y @smithery/cli install elasticsearch-mcp-server --client claude\n```\n\n### Option 2: Using uvx\n\nUsing `uvx` will automatically install the package from PyPI, no need to clone the repository locally. Add the following configuration to Claude Desktop's config file `claude_desktop_config.json`.\n\n```json\n// For Elasticsearch\n{\n  \"mcpServers\": {\n    \"elasticsearch-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"elasticsearch-mcp-server\"\n      ],\n      \"env\": {\n        \"ELASTICSEARCH_HOSTS\": \"https://localhost:9200\",\n        \"ELASTICSEARCH_USERNAME\": \"elastic\",\n        \"ELASTICSEARCH_PASSWORD\": \"test123\"\n      }\n    }\n  }\n}\n\n// For OpenSearch\n{\n  \"mcpServers\": {\n    \"opensearch-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"opensearch-mcp-server\"\n      ],\n      \"env\": {\n        \"OPENSEARCH_HOSTS\": \"https://localhost:9200\",\n        \"OPENSEARCH_USERNAME\": \"admin\",\n        \"OPENSEARCH_PASSWORD\": \"admin\"\n      }\n    }\n  }\n}\n```\n\n### Option 3: Using uv with local development\n\nUsing `uv` requires cloning the repository locally and specifying the path to the source code. Add the following configuration to Claude Desktop's config file `claude_desktop_config.json`.\n\n```json\n// For Elasticsearch\n{\n  \"mcpServers\": {\n    \"elasticsearch-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/src/elasticsearch_mcp_server\",\n        \"run\",\n        \"elasticsearch-mcp-server\"\n      ],\n      \"env\": {\n        \"ELASTICSEARCH_HOSTS\": \"https://localhost:9200\",\n        \"ELASTICSEARCH_USERNAME\": \"elastic\",\n        \"ELASTICSEARCH_PASSWORD\": \"test123\"\n      }\n    }\n  }\n}\n\n// For OpenSearch\n{\n  \"mcpServers\": {\n    \"opensearch-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/src/elasticsearch_mcp_server\",\n        \"run\",\n        \"opensearch-mcp-server\"\n      ],\n      \"env\": {\n        \"OPENSEARCH_HOSTS\": \"https://localhost:9200\",\n        \"OPENSEARCH_USERNAME\": \"admin\",\n        \"OPENSEARCH_PASSWORD\": \"admin\"\n      }\n    }\n  }\n}\n```\n\n- On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nRestart Claude Desktop to load the new MCP server.\n\nNow you can interact with your Elasticsearch/OpenSearch cluster through Claude using natural language commands like:\n- \"List all indices in the cluster\"\n- \"How old is the student Bob?\"\n- \"Show me the cluster health status\"\n\n## Usage with Anthropic MCP Client\n\n```python\nuv run mcp_client/client.py src/server.py\n```\n\n## License\n\nThis project is licensed under the Apache License Version 2.0 - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elasticsearch",
        "opensearch",
        "databases",
        "elasticsearch mcp",
        "arjunkmrm elasticsearch",
        "elasticsearch opensearch"
      ],
      "category": "databases"
    },
    "arjunkmrm--mcp-redis": {
      "owner": "arjunkmrm",
      "name": "mcp-redis",
      "url": "https://github.com/arjunkmrm/mcp-redis",
      "imageUrl": "/freedevtools/mcp/pfp/arjunkmrm.webp",
      "description": "Manage and search data in Redis using natural language queries, enabling efficient data operations like caching, indexing, and real-time event processing for AI applications.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-19T03:42:32Z",
      "readme_content": "# Redis MCP Server\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.13%2B-blue)](https://www.python.org/downloads/)\n[![smithery badge](https://smithery.ai/badge/@redis/mcp-redis)](https://smithery.ai/server/@redis/mcp-redis)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/70102150-efe0-4705-9f7d-87980109a279)\n\n## Overview\nThe Redis MCP Server is a **natural language interface** designed for agentic applications to efficiently manage and search data in Redis. It integrates seamlessly with **MCP (Model Content Protocol) clients**, enabling AI-driven workflows to interact with structured and unstructured data in Redis. Using this MCP Server, you can ask questions like:\n\n- \"Store the entire conversation in a stream\"\n- \"Cache this item\"\n- \"Store the session with an expiration time\"\n- \"Index and search this vector\"\n\n## Table of Contents\n- [Overview](#overview)\n- [Features](#features)\n- [Tools](#tools)\n- [Installation](#installation)\n  - [Quick Start with uvx](#quick-start-with-uvx)\n  - [Development Installation](#development-installation)\n  - [With Docker](#with-docker)\n- [Configuration](#configuration)\n  - [Redis ACL](#redis-acl)\n  - [Configuration via command line arguments](#configuration-via-command-line-arguments)\n  - [Configuration via Environment Variables](#configuration-via-environment-variables)\n- [Integrations](#integrations)\n  - [OpenAI Agents SDK](#openai-agents-sdk)\n  - [Augment](#augment)\n  - [Claude Desktop](#claude-desktop)\n  - [VS Code with GitHub Copilot](#vs-code-with-github-copilot)\n- [Testing](#testing)\n- [Example Use Cases](#example-use-cases)\n- [Contributing](#contributing)\n- [License](#license)\n- [Badges](#badges)\n- [Contact](#contact)\n\n\n## Features\n- **Natural Language Queries**: Enables AI agents to query and update Redis using natural language.\n- **Seamless MCP Integration**: Works with any **MCP client** for smooth communication.\n- **Full Redis Support**: Handles **hashes, lists, sets, sorted sets, streams**, and more.\n- **Search & Filtering**: Supports efficient data retrieval and searching in Redis.\n- **Scalable & Lightweight**: Designed for **high-performance** data operations.\n\n## Tools\n\nThis MCP Server provides tools to manage the data stored in Redis.\n\n- `string` tools to set, get strings with expiration. Useful for storing simple configuration values, session data, or caching responses.\n- `hash` tools to store field-value pairs within a single key. The hash can store vector embeddings. Useful for representing objects with multiple attributes, user profiles, or product information where fields can be accessed individually.\n- `list` tools with common operations to append and pop items. Useful for queues, message brokers, or maintaining a list of most recent actions.\n- `set` tools to add, remove and list set members. Useful for tracking unique values like user IDs or tags, and for performing set operations like intersection.\n- `sorted set` tools to manage data for e.g. leaderboards, priority queues, or time-based analytics with score-based ordering.\n- `pub/sub` functionality to publish messages to channels and subscribe to receive them. Useful for real-time notifications, chat applications, or distributing updates to multiple clients.\n- `streams` tools to add, read, and delete from data streams. Useful for event sourcing, activity feeds, or sensor data logging with consumer groups support.\n- `JSON` tools to store, retrieve, and manipulate JSON documents in Redis. Useful for complex nested data structures, document databases, or configuration management with path-based access.\n\nAdditional tools.\n\n- `query engine` tools to manage vector indexes and perform vector search\n- `server management` tool to retrieve information about the database\n\n## Installation\n\nThe Redis MCP Server supports the `stdio` [transport](https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio). Support to the `stremable-http` transport will be added in the future.\n\n> No PyPi package is available at the moment.\n\n### Quick Start with uvx \n\nThe easiest way to use the Redis MCP Server is with `uvx`, which allows you to run it directly from GitHub (from a branch, or use a tagged release). It is recommended to use a tagged release, the `main` branch is under active development and may contain breaking changes. As an example, you can execute the following command to run the `0.2.0` release:\n\n```commandline\nuvx --from git+https://github.com/redis/mcp-redis.git@0.2.0 redis-mcp-server --url redis://localhost:6379/0\n```\n\nCheck the release notes for the latest version in the [Releases](https://github.com/redis/mcp-redis/releases) section.\nAdditional examples are provided below.\n\n```sh\n# Run with Redis URI\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --url redis://localhost:6379/0\n\n# Run with Redis URI and SSL \nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --url \"rediss://<USERNAME>:<PASSWORD>@<HOST>:<PORT>?ssl_cert_reqs=required&ssl_ca_certs=<PATH_TO_CERT>\"\n\n# Run with individual parameters\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --host localhost --port 6379 --password mypassword\n\n# See all options\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --help\n```\n\n### Development Installation\n\nFor development or if you prefer to clone the repository:\n\n```sh\n# Clone the repository\ngit clone https://github.com/redis/mcp-redis.git\ncd mcp-redis\n\n# Install dependencies using uv\nuv venv\nsource .venv/bin/activate\nuv sync\n\n# Run with CLI interface\nuv run redis-mcp-server --help\n\n# Or run the main file directly (uses environment variables)\nuv run src/main.py\n```\n\nOnce you cloned the repository, installed the dependencies and verified you can run the server, you can configure Claude Desktop or any other MCP Client to use this MCP Server running the main file directly (it uses environment variables). This is usually preferred for development.\nThe following example is for Claude Desktop, but the same applies to any other MCP Client.\n\n1. Specify your Redis credentials and TLS configuration\n2. Retrieve your `uv` command full path (e.g. `which uv`)\n3. Edit the `claude_desktop_config.json` configuration file\n   - on a MacOS, at `~/Library/Application\\ Support/Claude/`\n\n```json\n{\n    \"mcpServers\": {\n        \"redis\": {\n            \"command\": \"<full_path_uv_command>\",\n            \"args\": [\n                \"--directory\",\n                \"<your_mcp_server_directory>\",\n                \"run\",\n                \"src/main.py\"\n            ],\n            \"env\": {\n                \"REDIS_HOST\": \"<your_redis_database_hostname>\",\n                \"REDIS_PORT\": \"<your_redis_database_port>\",\n                \"REDIS_PWD\": \"<your_redis_database_password>\",\n                \"REDIS_SSL\": True|False,\n                \"REDIS_CA_PATH\": \"<your_redis_ca_path>\",\n                \"REDIS_CLUSTER_MODE\": True|False\n            }\n        }\n    }\n}\n```\n\nYou can troubleshoot problems by tailing the log file.\n\n```commandline\ntail -f ~/Library/Logs/Claude/mcp-server-redis.log\n```\n\n### With Docker\n\nYou can use a dockerized deployment of this server. You can either build your own image or use the official [Redis MCP Docker](https://hub.docker.com/r/mcp/redis) image.\n\nIf you'd like to build your own image, the Redis MCP Server provides a Dockerfile. Build this server's image with:\n\n```commandline\ndocker build -t mcp-redis .\n```\n\nFinally, configure the client to create the container at start-up. An example for Claude Desktop is provided below. Edit the `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\",\n                \"--rm\",\n                \"--name\",\n                \"redis-mcp-server\",\n                \"-i\",\n                \"-e\", \"REDIS_HOST=<redis_hostname>\",\n                \"-e\", \"REDIS_PORT=<redis_port>\",\n                \"-e\", \"REDIS_USERNAME=<redis_username>\",\n                \"-e\", \"REDIS_PWD=<redis_password>\",\n                \"mcp-redis\"]\n    }\n  }\n}\n```\n\nTo use the official [Redis MCP Docker](https://hub.docker.com/r/mcp/redis) image, just replace your image name (`mcp-redis` in the example above) with `mcp/redis`.\n\n## Configuration\n\nThe Redis MCP Server can be configured in two ways: via command line arguments or via environment variables.\nThe precedence is: command line arguments > environment variables > default values.\n\n### Redis ACL\n\nYou can configure Redis ACL to restrict the access to the Redis database. For example, to create a read-only user:\n\n```\n127.0.0.1:6379> ACL SETUSER readonlyuser on >mypassword ~* +@read -@write\n```\n\nConfigure the user via command line arguments or environment variables.\n\n### Configuration via command line arguments\n\nWhen using the CLI interface, you can configure the server with command line arguments:\n\n```sh\n# Basic Redis connection\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server \\\n  --host localhost \\\n  --port 6379 \\\n  --password mypassword\n\n# Using Redis URI (simpler)\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server \\\n  --url redis://user:pass@localhost:6379/0\n\n# SSL connection\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server \\\n  --url rediss://user:pass@redis.example.com:6379/0\n\n# See all available options\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --help\n```\n\n**Available CLI Options:**\n- `--url` - Redis connection URI (redis://user:pass@host:port/db)\n- `--host` - Redis hostname (default: 127.0.0.1)\n- `--port` - Redis port (default: 6379)\n- `--db` - Redis database number (default: 0)\n- `--username` - Redis username\n- `--password` - Redis password\n- `--ssl` - Enable SSL connection\n- `--ssl-ca-path` - Path to CA certificate file\n- `--ssl-keyfile` - Path to SSL key file\n- `--ssl-certfile` - Path to SSL certificate file\n- `--ssl-cert-reqs` - SSL certificate requirements (default: required)\n- `--ssl-ca-certs` - Path to CA certificates file\n- `--cluster-mode` - Enable Redis cluster mode\n\n### Configuration via Environment Variables\n\nIf desired, you can use environment variables. Defaults are provided for all variables.\n\n| Name                 | Description                                               | Default Value |\n|----------------------|-----------------------------------------------------------|---------------|\n| `REDIS_HOST`         | Redis IP or hostname                                      | `\"127.0.0.1\"` |\n| `REDIS_PORT`         | Redis port                                                | `6379`        |\n| `REDIS_DB`           | Database                                                  | 0             |\n| `REDIS_USERNAME`     | Default database username                                 | `\"default\"`   |\n| `REDIS_PWD`          | Default database password                                 | \"\"            |\n| `REDIS_SSL`          | Enables or disables SSL/TLS                               | `False`       |\n| `REDIS_CA_PATH`      | CA certificate for verifying server                       | None          |\n| `REDIS_SSL_KEYFILE`  | Client's private key file for client authentication       | None          |\n| `REDIS_SSL_CERTFILE` | Client's certificate file for client authentication       | None          |\n| `REDIS_CERT_REQS`    | Whether the client should verify the server's certificate | `\"required\"`  |\n| `REDIS_CA_CERTS`     | Path to the trusted CA certificates file                  | None          |\n| `REDIS_CLUSTER_MODE` | Enable Redis Cluster mode                                 | `False`       |\n\n\nThere are several ways to set environment variables:\n\n1. **Using a `.env` File**:  \nPlace a `.env` file in your project directory with key-value pairs for each environment variable. Tools like `python-dotenv`, `pipenv`, and `uv` can automatically load these variables when running your application. This is a convenient and secure way to manage configuration, as it keeps sensitive data out of your shell history and version control (if `.env` is in `.gitignore`).\nFor example, create a `.env` file with the following content from the `.env.example` file provided in the repository:\n\n```bash\ncp .env.example .env\n```\n\nThen edit the `.env` file to set your Redis configuration:\n\nOR,\n\n2. **Setting Variables in the Shell**:  \nYou can export environment variables directly in your shell before running your application. For example:\n\n```sh\nexport REDIS_HOST=your_redis_host\nexport REDIS_PORT=6379\n# Other variables will be set similarly...\n```\n\nThis method is useful for temporary overrides or quick testing.\n\n\n## Integrations\n\nIntegrating this MCP Server to development frameworks like OpenAI Agents SDK, or with tools like Claude Desktop, VS Code, or Augment is described in the following sections.\n\n### OpenAI Agents SDK\n\nIntegrate this MCP Server with the OpenAI Agents SDK. Read the [documents](https://openai.github.io/openai-agents-python/mcp/) to learn more about the integration of the SDK with MCP.\n\nInstall the Python SDK.\n\n```commandline\npip install openai-agents\n```\n\nConfigure the OpenAI token:\n\n```commandline\nexport OPENAI_API_KEY=\"<openai_token>\"\n```\n\nAnd run the [application](./examples/redis_assistant.py).\n\n```commandline\npython3.13 redis_assistant.py\n```\n\nYou can troubleshoot your agent workflows using the [OpenAI dashboard](https://platform.openai.com/traces/).\n\n### Augment\n\nYou can configure the Redis MCP Server in Augment by importing the server via JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"Redis MCP Server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/redis/mcp-redis.git\",\n        \"redis-mcp-server\",\n        \"--url\",\n        \"redis://localhost:6379/0\"\n      ]\n    }\n  }\n}\n```\n\n### Claude Desktop\n\nThe simplest way to configure MCP clients is using `uvx`. Add the following JSON to your `claude_desktop_config.json`, remember to provide the full path to `uvx`.\n\n```json\n{\n    \"mcpServers\": {\n        \"redis-mcp-server\": {\n            \"type\": \"stdio\",\n            \"command\": \"/Users/mortensi/.local/bin/uvx\",\n            \"args\": [\n                \"--from\", \"git+https://github.com/redis/mcp-redis.git\",\n                \"redis-mcp-server\",\n                \"--url\", \"redis://localhost:6379/0\"\n            ]\n        }\n    }\n}\n```\n\nIf you'd like to test the [Redis MCP Server](https://smithery.ai/server/@redis/mcp-redis) via Smithery, you can configure Claude Desktop automatically:\n\n```bash\nnpx -y @smithery/cli install @redis/mcp-redis --client claude\n```\n\nFollow the prompt and provide the details to configure the server and connect to Redis (e.g. using a Redis Cloud database).\nThe procedure will create the proper configuration in the `claude_desktop_config.json` configuration file.\n\n### VS Code with GitHub Copilot\n\nTo use the Redis MCP Server with VS Code, you must nable the [agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode) tools. Add the following to your `settings.json`:\n\n```json\n{\n  \"chat.agent.enabled\": true\n}\n```\n\nYou can start the GitHub desired version of the Redis MCP server using `uvx` by adding the following JSON to your `settings.json`:\n\n```json\n\"mcp\": {\n    \"servers\": {\n        \"Redis MCP Server\": {\n        \"type\": \"stdio\",\n        \"command\": \"uvx\", \n        \"args\": [\n            \"--from\", \"git+https://github.com/redis/mcp-redis.git\",\n            \"redis-mcp-server\",\n            \"--url\", \"redis://localhost:6379/0\"\n        ]\n        },\n    }\n},\n```\n\nAlternatively, you can start the server using `uv` and configure your `mcp.json` or `settings.json`. This is usually desired for development.\n\n```json\n{\n  \"servers\": {\n    \"redis\": {\n      \"type\": \"stdio\",\n      \"command\": \"<full_path_uv_command>\",\n      \"args\": [\n        \"--directory\",\n        \"<your_mcp_server_directory>\",\n        \"run\",\n        \"src/main.py\"\n      ],\n      \"env\": {\n        \"REDIS_HOST\": \"<your_redis_database_hostname>\",\n        \"REDIS_PORT\": \"<your_redis_database_port>\",\n        \"REDIS_USERNAME\": \"<your_redis_database_username>\",\n        \"REDIS_PWD\": \"<your_redis_database_password>\",\n      }\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"redis\": {\n        \"type\": \"stdio\",\n        \"command\": \"<full_path_uv_command>\",\n        \"args\": [\n          \"--directory\",\n          \"<your_mcp_server_directory>\",\n          \"run\",\n          \"src/main.py\"\n        ],\n        \"env\": {\n          \"REDIS_HOST\": \"<your_redis_database_hostname>\",\n          \"REDIS_PORT\": \"<your_redis_database_port>\",\n          \"REDIS_USERNAME\": \"<your_redis_database_username>\",\n          \"REDIS_PWD\": \"<your_redis_database_password>\",\n        }\n      }\n    }\n  }\n}\n```\n\nFor more information, see the [VS Code documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n\n## Testing\n\nYou can use the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for visual debugging of this MCP Server.\n\n```sh\nnpx @modelcontextprotocol/inspector uv run src/main.py\n```\n\n## Example Use Cases\n- **AI Assistants**: Enable LLMs to fetch, store, and process data in Redis.\n- **Chatbots & Virtual Agents**: Retrieve session data, manage queues, and personalize responses.\n- **Data Search & Analytics**: Query Redis for **real-time insights and fast lookups**.\n- **Event Processing**: Manage event streams with **Redis Streams**.\n\n## Contributing\n1. Fork the repo\n2. Create a new branch (`feature-branch`)\n3. Commit your changes\n4. Push to your branch and submit a PR!\n\n## License\nThis project is licensed under the **MIT License**.\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@redis/mcp-redis\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@redis/mcp-redis/badge\" alt=\"Redis Server MCP server\" />\n</a>\n\n## Contact\nFor questions or support, reach out via [GitHub Issues](https://github.com/redis/mcp-redis/issues).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "redis",
        "database",
        "data redis",
        "redis using",
        "enables querying"
      ],
      "category": "databases"
    },
    "asirulnik--mcp-law-office-db": {
      "owner": "asirulnik",
      "name": "mcp-law-office-db",
      "url": "https://github.com/asirulnik/mcp-law-office-db",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Manage legal client records, case files, time tracking, and invoicing with specialized tools tailored for law firms. Streamline legal billing workflows and enforce business rules for accurate invoice generation. Enhance your law office operations with a dedicated SQLite database interface.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sqlite",
        "databases",
        "database",
        "sqlite database",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "awesimon--elasticsearch-mcp": {
      "owner": "awesimon",
      "name": "elasticsearch-mcp",
      "url": "https://github.com/awesimon/elasticsearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/awesimon.webp",
      "description": "Connects to Elasticsearch data, enabling users to manage indices, conduct searches, and handle mappings through natural language interactions.",
      "stars": 16,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T14:32:54Z",
      "readme_content": "# Elasticsearch MCP Server\n\n[English](./README.md) | [中文](./README.zh-CN.md)\n\n[![smithery badge](https://smithery.ai/badge/@awesimon/elasticsearch-mcp)](https://smithery.ai/server/@awesimon/elasticsearch-mcp)\n\nMCP Server for connecting to your Elasticsearch cluster directly from any MCP Client (like Claude Desktop, Cursor).\n\nThis server connects agents to your Elasticsearch data using the Model Context Protocol. It allows you to interact with your Elasticsearch indices through natural language conversations.\n\n\n## Demo\n\n[![Elasticsearch MCP Demo](https://img.youtube.com/vi/Wqw1XL8de5A/0.jpg)](https://www.youtube.com/watch?v=Wqw1XL8de5A \"Elasticsearch MCP Demo\")\n\n## Feature Overview\n\n### Available Features\n\n#### Cluster Management\n* `elasticsearch_health`: Get Elasticsearch cluster health status, optionally including index-level details\n\n#### Index Operations\n* `list_indices`: List available Elasticsearch indices, support regex\n* `create_index`: Create Elasticsearch index with optional settings and mappings\n* `reindex`: Reindex data from a source index to a target index with optional query and script\n\n#### Mapping Management\n* `get_mappings`: Get field mappings for a specific Elasticsearch index\n* `create_mapping`: Create or update mapping structure for an Elasticsearch index\n\n#### Search & Data Operations\n* `search`: Perform an Elasticsearch search with the provided query DSL\n* `bulk`: Bulk data into an Elasticsearch index\n\n#### Template Management\n* `create_index_template`: Create or update an index template\n* `get_index_template`: Get information about index templates\n* `delete_index_template`: Delete an index template\n\n### How It Works\n\n1. The MCP Client analyzes your request and determines which Elasticsearch operations are needed.\n2. The MCP server carries out these operations (listing indices, fetching mappings, performing searches).\n3. The MCP Client processes the results and presents them in a user-friendly format.\n\n## Getting Started\n\n### Prerequisites\n\n* An Elasticsearch instance\n* Elasticsearch authentication credentials (API key or username/password)\n* MCP Client (e.g. Claude Desktop, Cursor)\n\n### Installation & Setup\n\n#### Using the Published NPM Package\n\n> [!TIP]\n> The easiest way to use Elasticsearch MCP Server is through the published npm package.\n\n1. **Configure MCP Client**\n   - Open your MCP Client. See the [list of MCP Clients](https://modelcontextprotocol.io/clients), here we are configuring Claude Desktop.\n   - Go to **Settings > Developer > MCP Servers**\n   - Click `Edit Config` and add a new MCP Server with the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"elasticsearch-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"@awesome-ai/elasticsearch-mcp\"\n         ],\n         \"env\": {\n           \"ES_HOST\": \"your-elasticsearch-host\",\n           \"ES_API_KEY\": \"your-api-key\"\n         }\n       }\n     }\n   }\n   ```\n\n2. **Start a Conversation**\n   - Open a new conversation in your MCP Client.\n   - The MCP server should connect automatically.\n   - You can now ask questions about your Elasticsearch data.\n\n### Configuration Options\n\nThe Elasticsearch MCP Server supports configuration options to connect to your Elasticsearch:\n\n> [!NOTE]\n> You must provide either an API key or both username and password for authentication.\n\n| Environment Variable | Description | Required |\n|---------------------|-------------|----------|\n| `ES_HOST` | Your Elasticsearch instance URL(s) - supports single URL or comma-separated multiple URLs (also supports legacy `HOST`) | Yes |\n| `ES_API_KEY` | Elasticsearch API key for authentication (also supports legacy `API_KEY`) | No |\n| `ES_USERNAME` | Elasticsearch username for basic authentication (also supports legacy `USERNAME`) | No |\n| `ES_PASSWORD` | Elasticsearch password for basic authentication (also supports legacy `PASSWORD`) | No |\n| `ES_CA_CERT` | Path to custom CA certificate for Elasticsearch SSL/TLS (also supports legacy `CA_CERT`) | No |\n\n### Multiple URLs Configuration\n\nYou can configure multiple Elasticsearch nodes for high availability and load balancing:\n\n```json\n{\n  \"mcpServers\": {\n    \"elasticsearch-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@awesome-ai/elasticsearch-mcp\"\n      ],\n      \"env\": {\n        \"ES_HOST\": \"https://es-node1:9200,https://es-node2:9200,https://es-node3:9200\",\n        \"ES_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\nThe client will automatically handle failover and load balancing between the configured nodes.\n\n## Local Development\n\n> [!NOTE]\n> If you want to modify or extend the MCP Server, follow these local development steps.\n\n1. **Use the correct Node.js version**\n   ```bash\n   nvm use\n   ```\n\n2. **Install Dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Build the Project**\n   ```bash\n   npm run build\n   ```\n\n4. **Run locally in Claude Desktop App**\n   - Open **Claude Desktop App**\n   - Go to **Settings > Developer > MCP Servers**\n   - Click `Edit Config` and add a new MCP Server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"elasticsearch-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/project/dist/index.js\"\n         ],\n         \"env\": {\n           \"ES_HOST\": \"your-elasticsearch-host\",\n           \"ES_API_KEY\": \"your-api-key\"\n         }\n       }\n     }\n   }\n   ```\n\n5. **Run locally in Cursor Editor**\n   - Open **Cursor Editor**\n   - Go to **Cursor Settings > MCP**\n   - Click `Add new global MCP Server` and add a new MCP Server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"elasticsearch-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/project/dist/index.js\"\n         ],\n         \"env\": {\n           \"ES_HOST\": \"your-elasticsearch-host\",\n           \"ES_API_KEY\": \"your-api-key\"\n         }\n       }\n     }\n   }\n   ```\n\n6. **Debugging with MCP Inspector**\n   ```bash\n   ES_HOST=your-elasticsearch-url ES_API_KEY=your-api-key npm run inspector\n   ```\n\n   This will start the MCP Inspector, allowing you to debug and analyze requests. You should see:\n\n   ```bash\n   Starting MCP inspector...\n   ⚙️ Proxy server listening on port 6277\n   🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n   ```\n\n## Example Queries\n\n> [!TIP]\n> Here are some natural language queries you can try with your MCP Client.\n\n#### Cluster Management\n* \"What is the health status of my Elasticsearch cluster?\"\n* \"How many active nodes are in my cluster?\"\n\n#### Index Operations\n* \"What indices do I have in my Elasticsearch cluster?\"\n* \"Create a new index called 'users' with 3 shards and 1 replica.\"\n* \"Reindex data from 'old_index' to 'new_index'.\"\n\n#### Mapping Management\n* \"Show me the field mappings for the 'products' index.\"\n* \"Add a keyword type field called 'tags' to the 'products' index.\"\n\n#### Search & Data Operations\n* \"Find all orders over $500 from last month.\"\n* \"Which products received the most 5-star reviews?\"\n* \"Bulk import these customer records into the 'customers' index.\"\n\n#### Template Management\n* \"Create an index template for logs with pattern 'logs-*'.\"\n* \"Show me all my index templates.\"\n* \"Delete the 'outdated_template' index template.\"\n\nIf you encounter issues, feel free to open an issue on the GitHub repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elasticsearch",
        "databases",
        "schema",
        "awesimon elasticsearch",
        "elasticsearch data",
        "elasticsearch mcp"
      ],
      "category": "databases"
    },
    "b0ttle-neck--mcp-steampipe": {
      "owner": "b0ttle-neck",
      "name": "mcp-steampipe",
      "url": "https://github.com/b0ttle-neck/mcp-steampipe",
      "imageUrl": "/freedevtools/mcp/pfp/b0ttle-neck.webp",
      "description": "Bridge AI models with the Steampipe tool to enable seamless execution of SQL queries on various data sources. Retrieve structured results and simplify data access for AI-driven insights within workflows.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-09T14:26:17Z",
      "readme_content": "# Steampipe MCP\r\n\r\nThis is a simple steampipe MCP server. This acts as a bridge between your AI model and Steampipe tool.\r\n\r\n## Pre-requisites\r\n- Python 3.10+ installed.\r\n- uv installed (my fav) and mcp[cli]\r\n- Steampipe installed and working.\r\n- Steampipe plugin configured (e.g., github) with necessary credentials (e.g., token in ~/.steampipe/config/github.spc).\r\n- Any LLM supporting MCP. I am using Claude Here.\r\n- Node.js and npx installed (required for the MCP Inspector and potentially for running some MCP servers).\r\n\r\n\r\n## Running MCP Interceptor\r\nThis is an awesome tool for testing your if your MCP server is working as expected\r\n- Running the Interceptor\r\n```npx -y @modelcontextprotocol/inspector uv --directory . run steampipe_mcp_server.py```\r\n- A browser window should open with the MCP Inspector UI (usually at http://localhost:XXXX).\r\n- Wait for the \"Connected\" status on the left panel.\r\n- Go to the Tools tab.\r\n- You should see the run_steampipe_query tool listed with its description.\r\n- Click on the tool name.\r\n- In the \"Arguments\" JSON input field, enter a valid Steampipe query:\r\n```\r\n{\r\n  \"query\": \"select name, fork_count from github_my_repository \"\r\n}\r\n```\r\n- execute and view the json results\r\n\r\n## Running the tool\r\nPretty straightforward. Just run the interceptor and make sure the tool is working from the directory. Then add the server configuration to the respective LLM and select the tool from the LLM. \r\n![Screenshot 2025-04-06 at 11 53 23 PM](https://github.com/user-attachments/assets/f119615e-115f-4ab0-b32d-57dbfbd0cfb1)\r\n![Screenshot 2025-04-06 at 11 55 21 PM](https://github.com/user-attachments/assets/9f268531-2538-4232-857d-37d1d067aefc)\r\n\r\n## TroubleShooting\r\n\r\n- If the tool is not found in the interceptor then that means @mcp.tool() decorator has some issue.\r\n- Execution error - Look at the \"Result\" in the Inspector and the server logs (stderr) in your terminal. Did Steampipe run? Was there a SQL error? A timeout? A JSON parsing error? Adjust the Python script accordingly.\r\n```\r\ntail -f ~/Library/Logs/Claude/mcp.log\r\ntail -f ~/Library/Logs/Claude/mcp-server-steampipe.log\r\n```\r\n**Security Risk**\r\nClaude blindly executes your sql query in this POC so there is possibility to generate and execute arbitary SQL Queries via Steampipe using your configured credentials. \r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "workflows",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "benborla--mcp-server-mysql": {
      "owner": "benborla",
      "name": "mcp-server-mysql",
      "url": "https://github.com/benborla/mcp-server-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/benborla.webp",
      "description": "Provides read-only access to MySQL databases, enabling inspection of database schemas and execution of read-only SQL queries.",
      "stars": 840,
      "forks": 123,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:00Z",
      "readme_content": "# MCP Server for MySQL - Claude Code Edition\n\n> **🚀 This is a modified version optimized for Claude Code with SSH tunnel support**  \n> **Original Author:** [@benborla29](https://github.com/benborla)  \n> **Original Repository:** https://github.com/benborla/mcp-server-mysql  \n> **License:** MIT  \n\n# MCP Server for MySQL based on NodeJS\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/benborla/mcp-server-mysql)](https://archestra.ai/mcp-catalog/benborla__mcp-server-mysql)\n\n\n### Key Features of This Fork:\n- ✅ **Claude Code Integration** - Optimized for use with Anthropic's Claude Code CLI\n- ✅ **SSH Tunnel Support** - Built-in support for SSH tunnels to remote databases\n- ✅ **Auto-start/stop Hooks** - Automatic tunnel management with Claude start/stop\n- ✅ **DDL Operations** - Added `MYSQL_DISABLE_READ_ONLY_TRANSACTIONS` for CREATE TABLE support\n- ✅ **Multi-Project Setup** - Easy configuration for multiple projects with different databases\n\n### Quick Start for Claude Code Users:\n1. **Read the Setup Guide**: See [PROJECT_SETUP_GUIDE.md](PROJECT_SETUP_GUIDE.md) for detailed instructions\n2. **Configure SSH Tunnels**: Set up automatic SSH tunnels for remote databases\n3. **Use with Claude**: Integrated MCP server works seamlessly with Claude Code\n\nA Model Context Protocol server that provides access to MySQL databases through SSH tunnels. This server enables Claude and other LLMs to inspect database schemas and execute SQL queries securely.\n\n## Table of Contents\n\n- [Requirements](#requirements)\n- [Installation](#installation)\n  - [Smithery](#using-smithery)\n  - [Clone to Local Repository](#running-from-local-repository)\n  - [Remote mode](#run-in-remote-mode)\n- [Components](#components)\n- [Configuration](#configuration)\n- [Environment Variables](#environment-variables)\n- [Multi-DB Mode](#multi-db-mode)\n- [Schema-Specific Permissions](#schema-specific-permissions)\n- [Testing](#testing)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Requirements\n\n- Node.js v20 or higher\n- MySQL 5.7 or higher (MySQL 8.0+ recommended)\n- MySQL user with appropriate permissions for the operations you need\n- For write operations: MySQL user with INSERT, UPDATE, and/or DELETE privileges\n\n## Installation\n\n### Using Smithery\n\nThere are several ways to install and configure the MCP server but the most common would be checking this website [https://smithery.ai/server/@benborla29/mcp-server-mysql](https://smithery.ai/server/@benborla29/mcp-server-mysql)\n\n### Cursor\n\nFor Cursor IDE, you can install this MCP server with the following command in your project:\n\n1. Visit [https://smithery.ai/server/@benborla29/mcp-server-mysql](https://smithery.ai/server/@benborla29/mcp-server-mysql)\n2. Follow the instruction for Cursor\n\nMCP Get provides a centralized registry of MCP servers and simplifies the installation process.\n\n### Claude Code\n\n#### Option 1: Import from Claude Desktop (Recommended if already configured)\n\nIf you already have this MCP server configured in Claude Desktop, you can import it automatically:\n\n```bash\nclaude mcp add-from-claude-desktop\n```\n\nThis will show an interactive dialog where you can select your `mcp_server_mysql` server to import with all existing configuration.\n\n#### Option 2: Manual Configuration\n\n**Using NPM/PNPM Global Installation:**\n\nFirst, install the package globally:\n\n```bash\n# Using npm\nnpm install -g @benborla29/mcp-server-mysql\n\n# Using pnpm\npnpm add -g @benborla29/mcp-server-mysql\n```\n\nThen add the server to Claude Code:\n\n```bash\nclaude mcp add mcp_server_mysql \\\n  -e MYSQL_HOST=\"127.0.0.1\" \\\n  -e MYSQL_PORT=\"3306\" \\\n  -e MYSQL_USER=\"root\" \\\n  -e MYSQL_PASS=\"your_password\" \\\n  -e MYSQL_DB=\"your_database\" \\\n  -e ALLOW_INSERT_OPERATION=\"false\" \\\n  -e ALLOW_UPDATE_OPERATION=\"false\" \\\n  -e ALLOW_DELETE_OPERATION=\"false\" \\\n  -- npx @benborla29/mcp-server-mysql\n```\n\n**Using Local Repository (for development):**\n\nIf you're running from a cloned repository:\n\n```bash\nclaude mcp add mcp_server_mysql \\\n  -e MYSQL_HOST=\"127.0.0.1\" \\\n  -e MYSQL_PORT=\"3306\" \\\n  -e MYSQL_USER=\"root\" \\\n  -e MYSQL_PASS=\"your_password\" \\\n  -e MYSQL_DB=\"your_database\" \\\n  -e ALLOW_INSERT_OPERATION=\"false\" \\\n  -e ALLOW_UPDATE_OPERATION=\"false\" \\\n  -e ALLOW_DELETE_OPERATION=\"false\" \\\n  -e PATH=\"/path/to/node/bin:/usr/bin:/bin\" \\\n  -e NODE_PATH=\"/path/to/node/lib/node_modules\" \\\n  -- /path/to/node /full/path/to/mcp-server-mysql/dist/index.js\n```\n\nReplace:\n\n- `/path/to/node` with your Node.js binary path (find with `which node`)\n- `/full/path/to/mcp-server-mysql` with the full path to your cloned repository\n- Update MySQL credentials to match your environment\n\n**Using Unix Socket Connection:**\n\nFor local MySQL instances using Unix sockets:\n\n```bash\nclaude mcp add mcp_server_mysql \\\n  -e MYSQL_SOCKET_PATH=\"/tmp/mysql.sock\" \\\n  -e MYSQL_USER=\"root\" \\\n  -e MYSQL_PASS=\"your_password\" \\\n  -e MYSQL_DB=\"your_database\" \\\n  -e ALLOW_INSERT_OPERATION=\"false\" \\\n  -e ALLOW_UPDATE_OPERATION=\"false\" \\\n  -e ALLOW_DELETE_OPERATION=\"false\" \\\n  -- npx @benborla29/mcp-server-mysql\n```\n\n#### Choosing the Right Scope\n\nConsider which scope to use based on your needs:\n\n```bash\n# Local scope (default) - only available in current project\nclaude mcp add mcp_server_mysql [options...]\n\n# User scope - available across all your projects\nclaude mcp add mcp_server_mysql -s user [options...]\n\n# Project scope - shared with team members via .mcp.json\nclaude mcp add mcp_server_mysql -s project [options...]\n```\n\nFor database servers with credentials, **local** or **user** scope is recommended to keep credentials private.\n\n#### Verification\n\nAfter adding the server, verify it's configured correctly:\n\n```bash\n# List all configured servers\nclaude mcp list\n\n# Get details for your MySQL server\nclaude mcp get mcp_server_mysql\n\n# Check server status within Claude Code\n/mcp\n```\n\n#### Multi-Database Configuration\n\nFor multi-database mode, omit the `MYSQL_DB` environment variable:\n\n```bash\nclaude mcp add mcp_server_mysql_multi \\\n  -e MYSQL_HOST=\"127.0.0.1\" \\\n  -e MYSQL_PORT=\"3306\" \\\n  -e MYSQL_USER=\"root\" \\\n  -e MYSQL_PASS=\"your_password\" \\\n  -e MULTI_DB_WRITE_MODE=\"false\" \\\n  -- npx @benborla29/mcp-server-mysql\n```\n\n#### Advanced Configuration\n\nFor advanced features, add additional environment variables:\n\n```bash\nclaude mcp add mcp_server_mysql \\\n  -e MYSQL_HOST=\"127.0.0.1\" \\\n  -e MYSQL_PORT=\"3306\" \\\n  -e MYSQL_USER=\"root\" \\\n  -e MYSQL_PASS=\"your_password\" \\\n  -e MYSQL_DB=\"your_database\" \\\n  -e MYSQL_POOL_SIZE=\"10\" \\\n  -e MYSQL_QUERY_TIMEOUT=\"30000\" \\\n  -e MYSQL_CACHE_TTL=\"60000\" \\\n  -e MYSQL_RATE_LIMIT=\"100\" \\\n  -e MYSQL_SSL=\"true\" \\\n  -e ALLOW_INSERT_OPERATION=\"false\" \\\n  -e ALLOW_UPDATE_OPERATION=\"false\" \\\n  -e ALLOW_DELETE_OPERATION=\"false\" \\\n  -e MYSQL_ENABLE_LOGGING=\"true\" \\\n  -- npx @benborla29/mcp-server-mysql\n```\n\n#### Troubleshooting Claude Code Setup\n\n1. **Server Connection Issues**: Use `/mcp` command in Claude Code to check server status and authenticate if needed.\n\n2. **Path Issues**: If using a local repository, ensure Node.js paths are correctly set:\n\n   ```bash\n   # Find your Node.js path\n   which node\n\n   # For PATH environment variable\n   echo \"$(which node)/../\"\n\n   # For NODE_PATH environment variable\n   echo \"$(which node)/../../lib/node_modules\"\n   ```\n\n3. **Permission Errors**: Ensure your MySQL user has appropriate permissions for the operations you've enabled.\n\n4. **Server Not Starting**: Check Claude Code logs or run the server directly to debug:\n\n   ```bash\n   # Test the server directly\n   npx @benborla29/mcp-server-mysql\n   ```\n\n### Using NPM/PNPM\n\nFor manual installation:\n\n```bash\n# Using npm\nnpm install -g @benborla29/mcp-server-mysql\n\n# Using pnpm\npnpm add -g @benborla29/mcp-server-mysql\n```\n\nAfter manual installation, you'll need to configure your LLM application to use the MCP server (see Configuration section below).\n\n### Running from Local Repository\n\nIf you want to clone and run this MCP server directly from the source code, follow these steps:\n\n1. **Clone the repository**\n\n   ```bash\n   git clone https://github.com/benborla/mcp-server-mysql.git\n   cd mcp-server-mysql\n   ```\n\n2. **Install dependencies**\n\n   ```bash\n   npm install\n   # or\n   pnpm install\n   ```\n\n3. **Build the project**\n\n   ```bash\n   npm run build\n   # or\n   pnpm run build\n   ```\n\n4. **Configure Claude Desktop**\n\n   Add the following to your Claude Desktop configuration file (`claude_desktop_config.json`):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp_server_mysql\": {\n         \"command\": \"/path/to/node\",\n         \"args\": [\n           \"/full/path/to/mcp-server-mysql/dist/index.js\"\n         ],\n         \"env\": {\n           \"MYSQL_HOST\": \"127.0.0.1\",\n           \"MYSQL_PORT\": \"3306\",\n           \"MYSQL_USER\": \"root\",\n           \"MYSQL_PASS\": \"your_password\",\n           \"MYSQL_DB\": \"your_database\",\n           \"ALLOW_INSERT_OPERATION\": \"false\",\n           \"ALLOW_UPDATE_OPERATION\": \"false\",\n           \"ALLOW_DELETE_OPERATION\": \"false\",\n           \"PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/bin:/usr/bin:/bin\", // <--- Important to add the following, run in your terminal `echo \"$(which node)/../\"` to get the path\n           \"NODE_PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/lib/node_modules\" // <--- Important to add the following, run in your terminal `echo \"$(which node)/../../lib/node_modules\"`\n         }\n       }\n     }\n   }\n   ```\n\n   Replace:\n   - `/path/to/node` with the full path to your Node.js binary (find it with `which node`)\n   - `/full/path/to/mcp-server-mysql` with the full path to where you cloned the repository\n   - Set the MySQL credentials to match your environment\n\n5. **Test the server**\n\n   ```bash\n   # Run the server directly to test\n   node dist/index.js\n   ```\n\n   If it connects to MySQL successfully, you're ready to use it with Claude Desktop.\n\n### Run in remote mode\n\nTo run in remote mode, you'll need to provide [environment variables](https://github.com/benborla/mcp-server-mysql?tab=readme-ov-file#environment-variables) to the npx script.\n\n1. Create env file in preferred directory\n\n   ```bash\n   # create .env file\n   touch .env\n   ```\n\n2. Copy-paste [example file](https://github.com/benborla/mcp-server-mysql/blob/main/.env) from this repository\n3. Set the MySQL credentials to match your environment\n4. Set `IS_REMOTE_MCP=true`\n5. Set `REMOTE_SECRET_KEY` to a secure string.\n6. Provide custom `PORT` if needed. Default is 3000.\n7. Load variables in current session:\n\n   ```bash\n   source .env\n   ```\n\n8. Run the server\n\n   ```bash\n   npx @benborla29/mcp-server-mysql\n   ```\n\n9. Configure your agent to connect to the MCP with the next configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"mysql\": {\n         \"url\": \"http://your-host:3000/mcp\",\n         \"type\": \"streamableHttp\",\n         \"headers\": {\n           \"Authorization\": \"Bearer <REMOTE_SECRET_KEY>\"\n         }\n       }\n     }\n   }\n   ```\n\n## Components\n\n### Tools\n\n- **mysql_query**\n  - Execute SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - By default, limited to READ ONLY operations\n  - Optional write operations (when enabled via configuration):\n    - INSERT: Add new data to tables (requires `ALLOW_INSERT_OPERATION=true`)\n    - UPDATE: Modify existing data (requires `ALLOW_UPDATE_OPERATION=true`)\n    - DELETE: Remove data (requires `ALLOW_DELETE_OPERATION=true`)\n  - All operations are executed within a transaction with proper commit/rollback handling\n  - Supports prepared statements for secure parameter handling\n  - Configurable query timeouts and result pagination\n  - Built-in query execution statistics\n\n### Resources\n\nThe server provides comprehensive database information:\n\n- **Table Schemas**\n  - JSON schema information for each table\n  - Column names and data types\n  - Index information and constraints\n  - Foreign key relationships\n  - Table statistics and metrics\n  - Automatically discovered from database metadata\n\n### Security Features\n\n- SQL injection prevention through prepared statements\n- Query whitelisting/blacklisting capabilities\n- Rate limiting for query execution\n- Query complexity analysis\n- Configurable connection encryption\n- Read-only transaction enforcement\n\n### Performance Optimizations\n\n- Optimized connection pooling\n- Query result caching\n- Large result set streaming\n- Query execution plan analysis\n- Configurable query timeouts\n\n### Monitoring and Debugging\n\n- Comprehensive query logging\n- Performance metrics collection\n- Error tracking and reporting\n- Health check endpoints\n- Query execution statistics\n\n## Configuration\n\n### Automatic Configuration with Smithery\n\nIf you installed using Smithery, your configuration is already set up. You can view or modify it with:\n\n```bash\nsmithery configure @benborla29/mcp-server-mysql\n```\n\nWhen reconfiguring, you can update any of the MySQL connection details as well as the write operation settings:\n\n- **Basic connection settings**:\n  - MySQL Host, Port, User, Password, Database\n  - SSL/TLS configuration (if your database requires secure connections)\n\n- **Write operation permissions**:\n  - Allow INSERT Operations: Set to true if you want to allow adding new data\n  - Allow UPDATE Operations: Set to true if you want to allow updating existing data\n  - Allow DELETE Operations: Set to true if you want to allow deleting data\n\nFor security reasons, all write operations are disabled by default. Only enable these settings if you specifically need Claude to modify your database data.\n\n### Advanced Configuration Options\n\nFor more control over the MCP server's behavior, you can use these advanced configuration options:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"/path/to/npx/binary/npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ],\n      \"env\": {\n        // Basic connection settings\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\",\n        \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\",\n\n        // Performance settings\n        \"MYSQL_POOL_SIZE\": \"10\",\n        \"MYSQL_QUERY_TIMEOUT\": \"30000\",\n        \"MYSQL_CACHE_TTL\": \"60000\",\n\n        // Security settings\n        \"MYSQL_RATE_LIMIT\": \"100\",\n        \"MYSQL_MAX_QUERY_COMPLEXITY\": \"1000\",\n        \"MYSQL_SSL\": \"true\",\n\n        // Monitoring settings\n        \"ENABLE_LOGGING\": \"true\",\n        \"MYSQL_LOG_LEVEL\": \"info\",\n        \"MYSQL_METRICS_ENABLED\": \"true\",\n\n        // Write operation flags\n        \"ALLOW_INSERT_OPERATION\": \"false\",\n        \"ALLOW_UPDATE_OPERATION\": \"false\",\n        \"ALLOW_DELETE_OPERATION\": \"false\"\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n### Basic Connection\n\n- `MYSQL_SOCKET_PATH`: Unix socket path for local connections (e.g., \"/tmp/mysql.sock\")\n- `MYSQL_HOST`: MySQL server host (default: \"127.0.0.1\") - ignored if MYSQL_SOCKET_PATH is set\n- `MYSQL_PORT`: MySQL server port (default: \"3306\") - ignored if MYSQL_SOCKET_PATH is set\n- `MYSQL_USER`: MySQL username (default: \"root\")\n- `MYSQL_PASS`: MySQL password\n- `MYSQL_DB`: Target database name (leave empty for multi-DB mode)\n\n### Performance Configuration\n\n- `MYSQL_POOL_SIZE`: Connection pool size (default: \"10\")\n- `MYSQL_QUERY_TIMEOUT`: Query timeout in milliseconds (default: \"30000\")\n- `MYSQL_CACHE_TTL`: Cache time-to-live in milliseconds (default: \"60000\")\n\n### Security Configuration\n\n- `MYSQL_RATE_LIMIT`: Maximum queries per minute (default: \"100\")\n- `MYSQL_MAX_QUERY_COMPLEXITY`: Maximum query complexity score (default: \"1000\")\n- `MYSQL_SSL`: Enable SSL/TLS encryption (default: \"false\")\n- `ALLOW_INSERT_OPERATION`: Enable INSERT operations (default: \"false\")\n- `ALLOW_UPDATE_OPERATION`: Enable UPDATE operations (default: \"false\")\n- `ALLOW_DELETE_OPERATION`: Enable DELETE operations (default: \"false\")\n- `ALLOW_DDL_OPERATION`: Enable DDL operations (default: \"false\")\n- `MYSQL_DISABLE_READ_ONLY_TRANSACTIONS`: **[NEW]** Disable read-only transaction enforcement (default: \"false\") ⚠️ **Security Warning:** Only enable this if you need full write capabilities and trust the LLM with your database\n- `SCHEMA_INSERT_PERMISSIONS`: Schema-specific INSERT permissions\n- `SCHEMA_UPDATE_PERMISSIONS`: Schema-specific UPDATE permissions\n- `SCHEMA_DELETE_PERMISSIONS`: Schema-specific DELETE permissions\n- `SCHEMA_DDL_PERMISSIONS`: Schema-specific DDL permissions\n- `MULTI_DB_WRITE_MODE`: Enable write operations in multi-DB mode (default: \"false\")\n\n### Monitoring Configuration\n\n- `MYSQL_ENABLE_LOGGING`: Enable query logging (default: \"false\")\n- `MYSQL_LOG_LEVEL`: Logging level (default: \"info\")\n- `MYSQL_METRICS_ENABLED`: Enable performance metrics (default: \"false\")\n\n### Remote MCP Configuration\n\n- `IS_REMOTE_MCP`: Enable remote MCP mode (default: \"false\")\n- `REMOTE_SECRET_KEY`: Secret key for remote MCP authentication (default: \"\"). If not provided, remote MCP mode will be disabled.\n- `PORT`: Port number for the remote MCP server (default: 3000)\n\n## Multi-DB Mode\n\nMCP-Server-MySQL supports connecting to multiple databases when no specific database is set. This allows the LLM to query any database the MySQL user has access to. For full details, see [README-MULTI-DB.md](./README-MULTI-DB.md).\n\n### Enabling Multi-DB Mode\n\nTo enable multi-DB mode, simply leave the `MYSQL_DB` environment variable empty. In multi-DB mode, queries require schema qualification:\n\n```sql\n-- Use fully qualified table names\nSELECT * FROM database_name.table_name;\n\n-- Or use USE statements to switch between databases\nUSE database_name;\nSELECT * FROM table_name;\n```\n\n## Schema-Specific Permissions\n\nFor fine-grained control over database operations, MCP-Server-MySQL now supports schema-specific permissions. This allows different databases to have different levels of access (read-only, read-write, etc.).\n\n### Configuration Example\n\n```txt\nSCHEMA_INSERT_PERMISSIONS=development:true,test:true,production:false\nSCHEMA_UPDATE_PERMISSIONS=development:true,test:true,production:false\nSCHEMA_DELETE_PERMISSIONS=development:false,test:true,production:false\nSCHEMA_DDL_PERMISSIONS=development:false,test:true,production:false\n```\n\nFor complete details and security recommendations, see [README-MULTI-DB.md](./README-MULTI-DB.md).\n\n## Testing\n\n### Database Setup\n\nBefore running tests, you need to set up the test database and seed it with test data:\n\n1. **Create Test Database and User**\n\n   ```sql\n   -- Connect as root and create test database\n   CREATE DATABASE IF NOT EXISTS mcp_test;\n\n   -- Create test user with appropriate permissions\n   CREATE USER IF NOT EXISTS 'mcp_test'@'localhost' IDENTIFIED BY 'mcp_test_password';\n   GRANT ALL PRIVILEGES ON mcp_test.* TO 'mcp_test'@'localhost';\n   FLUSH PRIVILEGES;\n   ```\n\n2. **Run Database Setup Script**\n\n   ```bash\n   # Run the database setup script\n   pnpm run setup:test:db\n   ```\n\n   This will create the necessary tables and seed data. The script is located in `scripts/setup-test-db.ts`\n\n3. **Configure Test Environment**\n   Create a `.env.test` file in the project root (if not existing):\n\n   ```env\n   MYSQL_HOST=127.0.0.1\n   MYSQL_PORT=3306\n   MYSQL_USER=mcp_test\n   MYSQL_PASS=mcp_test_password\n   MYSQL_DB=mcp_test\n   ```\n\n4. **Update package.json Scripts**\n   Add these scripts to your package.json:\n\n   ```json\n   {\n     \"scripts\": {\n       \"setup:test:db\": \"ts-node scripts/setup-test-db.ts\",\n       \"pretest\": \"pnpm run setup:test:db\",\n       \"test\": \"vitest run\",\n       \"test:watch\": \"vitest\",\n       \"test:coverage\": \"vitest run --coverage\"\n     }\n   }\n   ```\n\n### Running Tests\n\nThe project includes a comprehensive test suite to ensure functionality and reliability:\n\n```bash\n# First-time setup\npnpm run setup:test:db\n\n# Run all tests\npnpm test\n```\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found at [MCP Evals](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts index.ts\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection Issues**\n   - Verify MySQL server is running and accessible\n   - Check credentials and permissions\n   - Ensure SSL/TLS configuration is correct if enabled\n   - Try connecting with a MySQL client to confirm access\n\n2. **Performance Issues**\n   - Adjust connection pool size\n   - Configure query timeout values\n   - Enable query caching if needed\n   - Check query complexity settings\n   - Monitor server resource usage\n\n3. **Security Restrictions**\n   - Review rate limiting configuration\n   - Check query whitelist/blacklist settings\n   - Verify SSL/TLS settings\n   - Ensure the user has appropriate MySQL permissions\n\n4. **Path Resolution**\n   If you encounter an error \"Could not connect to MCP server mcp-server-mysql\", explicitly set the path of all required binaries:\n\n   ```json\n   {\n     \"env\": {\n       \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\"\n     }\n   }\n   ```\n\n   *Where can I find my `node` bin path*\n   Run the following command to get it:\n\n   For **PATH**\n\n   ```bash\n   echo \"$(which node)/../\"\n   ```\n\n   For **NODE_PATH**\n\n   ```bash\n   echo \"$(which node)/../../lib/node_modules\"\n   ```\n\n5. **Claude Desktop Specific Issues**\n   - If you see \"Server disconnected\" logs in Claude Desktop, check the logs at `~/Library/Logs/Claude/mcp-server-mcp_server_mysql.log`\n   - Ensure you're using the absolute path to both the Node binary and the server script\n   - Check if your `.env` file is being properly loaded; use explicit environment variables in the configuration\n   - Try running the server directly from the command line to see if there are connection issues\n   - If you need write operations (INSERT, UPDATE, DELETE), set the appropriate flags to \"true\" in your configuration:\n\n     ```json\n     \"env\": {\n       \"ALLOW_INSERT_OPERATION\": \"true\",  // Enable INSERT operations\n       \"ALLOW_UPDATE_OPERATION\": \"true\",  // Enable UPDATE operations\n       \"ALLOW_DELETE_OPERATION\": \"true\"   // Enable DELETE operations\n     }\n     ```\n\n   - Ensure your MySQL user has the appropriate permissions for the operations you're enabling\n   - For direct execution configuration, use:\n\n     ```json\n     {\n       \"mcpServers\": {\n         \"mcp_server_mysql\": {\n           \"command\": \"/full/path/to/node\",\n           \"args\": [\n             \"/full/path/to/mcp-server-mysql/dist/index.js\"\n           ],\n           \"env\": {\n             \"MYSQL_HOST\": \"127.0.0.1\",\n             \"MYSQL_PORT\": \"3306\",\n             \"MYSQL_USER\": \"root\",\n             \"MYSQL_PASS\": \"your_password\",\n             \"MYSQL_DB\": \"your_database\"\n           }\n         }\n       }\n     }\n     ```\n\n6. **Authentication Issues**\n   - For MySQL 8.0+, ensure the server supports the `caching_sha2_password` authentication plugin\n   - Check if your MySQL user is configured with the correct authentication method\n   - Try creating a user with legacy authentication if needed:\n\n     ```sql\n     CREATE USER 'user'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';\n     ```\n\n     @lizhuangs\n\n7. I am encountering `Error [ERR_MODULE_NOT_FOUND]: Cannot find package 'dotenv' imported from` error\n   try this workaround:\n\n   ```bash\n   npx -y -p @benborla29/mcp-server-mysql -p dotenv mcp-server-mysql\n   ```\n\n   Thanks to @lizhuangs\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to\n[https://github.com/benborla/mcp-server-mysql](https://github.com/benborla/mcp-server-mysql)\n\n## Many Thanks to the following Contributors\n\n[![Contributors](https://contrib.rocks/image?repo=benborla/mcp-server-mysql)](https://github.com/benborla/mcp-server-mysql/graphs/contributors)\n\n### Development Setup\n\n1. Clone the repository\n2. Install dependencies: `pnpm install`\n3. Build the project: `pnpm run build`\n4. Run tests: `pnpm test`\n\n### Project Roadmap\n\nWe're actively working on enhancing this MCP server. Check our [CHANGELOG.md](./CHANGELOG.md) for details on planned features, including:\n\n- Enhanced query capabilities with prepared statements\n- Advanced security features\n- Performance optimizations\n- Comprehensive monitoring\n- Expanded schema information\n\nIf you'd like to contribute to any of these areas, please check the issues on GitHub or open a new one to discuss your ideas.\n\n### Submitting Changes\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/your-feature-name`\n3. Commit your changes: `git commit -am 'Add some feature'`\n4. Push to the branch: `git push origin feature/your-feature-name`\n5. Submit a pull request\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "database access",
        "databases secure"
      ],
      "category": "databases"
    },
    "benborla29--mcp-server-mysql": {
      "owner": "benborla29",
      "name": "mcp-server-mysql",
      "url": "https://github.com/benborla/mcp-server-mysql",
      "imageUrl": "",
      "description": "MySQL database integration in NodeJS with configurable access controls and schema inspection",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "nodejs",
        "secure database",
        "database integration",
        "databases secure"
      ],
      "category": "databases"
    },
    "bigdata-coss--agent_mcp": {
      "owner": "bigdata-coss",
      "name": "agent_mcp",
      "url": "https://github.com/bigdata-coss/agent_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bigdata-coss.webp",
      "description": "Enables querying and manipulation of ontology data through GraphDB's SPARQL endpoint while integrating various AI models like Ollama, OpenAI, and Google Gemini. Facilitates executing SPARQL queries, completing AI model tasks, and performing HTTP requests via a unified interface.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-14T14:18:21Z",
      "readme_content": "﻿# Ontology MCP\r\n\r\nOntology MCP는 GraphDB의 SPARQL 엔드포인트와 Ollama 모델을 Claude와 연결하는 Model Context Protocol (MCP) 서버입니다. 이 도구를 사용하면 Claude가 온톨로지 데이터를 쿼리하고 조작하며, 다양한 AI 모델을 활용할 수 있습니다.\r\n\r\n\r\n\r\n## 주요 기능\r\n\r\n### SPARQL 관련 기능\r\n- SPARQL 쿼리 실행 (`mcp_sparql_execute_query`)\r\n- SPARQL 업데이트 쿼리 실행 (`mcp_sparql_update`)\r\n- 리포지토리 목록 조회 (`mcp_sparql_list_repositories`)\r\n- 그래프 목록 조회 (`mcp_sparql_list_graphs`)\r\n- 리소스 정보 조회 (`mcp_sparql_get_resource_info`)\r\n\r\n### Ollama 모델 관련 기능\r\n- 모델 실행 (`mcp_ollama_run`)\r\n- 모델 정보 확인 (`mcp_ollama_show`)\r\n- 모델 다운로드 (`mcp_ollama_pull`)\r\n- 모델 목록 조회 (`mcp_ollama_list`)\r\n- 모델 삭제 (`mcp_ollama_rm`)\r\n- 채팅 완성 (`mcp_ollama_chat_completion`)\r\n- 컨테이너 상태 확인 (`mcp_ollama_status`)\r\n\r\n### OpenAI 관련 기능\r\n- 채팅 완성 (`mcp_openai_chat`)\r\n- 이미지 생성 (`mcp_openai_image`)\r\n- 텍스트-음성 변환 (`mcp_openai_tts`)\r\n- 음성-텍스트 변환 (`mcp_openai_transcribe`)\r\n- 임베딩 생성 (`mcp_openai_embedding`)\r\n\r\n### Google Gemini 관련 기능\r\n- 텍스트 생성 (`mcp_gemini_generate_text`)\r\n- 채팅 완성 (`mcp_gemini_chat_completion`)\r\n- 모델 목록 조회 (`mcp_gemini_list_models`)\r\n- ~~이미지 생성 (`mcp_gemini_generate_images`) - Imagen 모델 활용~~ (현재 비활성화)\r\n- ~~비디오 생성 (`mcp_gemini_generate_videos`) - Veo 모델 활용~~ (현재 비활성화)\r\n- ~~멀티모달 콘텐츠 생성 (`mcp_gemini_generate_multimodal_content`)~~ (현재 비활성화)\r\n\r\n> **참고**: Gemini의 이미지 생성, 비디오 생성 및 멀티모달 콘텐츠 생성 기능은 현재 API 호환성 문제로 인해 비활성화되어 있습니다.\r\n\r\n#### 지원하는 Gemini 모델\r\n| 모델 변형 | 입력 | 출력 | 최적화 목표 |\r\n|----------|------|------|------------|\r\n| **Gemini 2.5 Flash Preview** <br>`gemini-2.5-flash-preview-04-17` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 적응적 사고, 비용 효율성 |\r\n| **Gemini 2.5 Pro 미리보기** <br>`gemini-2.5-pro-preview-03-25` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 향상된 사고 및 추론, 멀티모달 이해, 고급 코딩 |\r\n| **Gemini 2.0 Flash** <br>`gemini-2.0-flash` | 오디오, 이미지, 동영상, 텍스트 | 텍스트, 이미지 (실험용), 오디오 (출시 예정) | 차세대 기능, 속도, 사고, 실시간 스트리밍, 멀티모달 생성 |\r\n| **Gemini 2.0 Flash-Lite** <br>`gemini-2.0-flash-lite` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 비용 효율성 및 낮은 지연 시간 |\r\n| **Gemini 1.5 Flash** <br>`gemini-1.5-flash` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 다양한 작업에서 빠르고 다재다능한 성능 |\r\n| **Gemini 1.5 Flash-8B** <br>`gemini-1.5-flash-8b` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 대용량 및 낮은 인텔리전스 태스크 |\r\n| **Gemini 1.5 Pro** <br>`gemini-1.5-pro` | 오디오, 이미지, 동영상, 텍스트 | 텍스트 | 더 많은 지능이 필요한 복잡한 추론 작업 |\r\n| **Gemini 삽입** <br>`gemini-embedding-exp` | 텍스트 | 텍스트 임베딩 | 텍스트 문자열의 관련성 측정 |\r\n| **Imagen 3** <br>`imagen-3.0-generate-002` | 텍스트 | 이미지 | Google의 가장 고급 이미지 생성 모델 |\r\n| **Veo 2** <br>`veo-2.0-generate-001` | 텍스트, 이미지 | 동영상 | 고화질 동영상 생성 |\r\n| **Gemini 2.0 Flash 실시간** <br>`gemini-2.0-flash-live-001` | 오디오, 동영상, 텍스트 | 텍스트, 오디오 | 지연 시간이 짧은 양방향 음성 및 동영상 상호작용 |\r\n\r\n### HTTP 요청 기능\r\n- HTTP 요청 실행 (`mcp_http_request`) - GET, POST, PUT, DELETE 등 다양한 HTTP 메서드를 사용하여 외부 API와 통신\r\n\r\n## 시작하기\r\n\r\n### 1. 저장소 클론\r\n\r\n```bash\r\ngit clone https://github.com/bigdata-coss/agent_mcp.git\r\ncd agent_mcp\r\n```\r\n\r\n### 2. GraphDB Docker 컨테이너 실행\r\n\r\n프로젝트 루트 디렉토리에서 다음 명령어를 실행하여 GraphDB 서버를 시작합니다:\r\n\r\n```bash\r\ndocker-compose up -d\r\n```\r\n\r\nGraphDB 웹 인터페이스가 [http://localhost:7200](http://localhost:7200)에서 실행됩니다.\r\n\r\n### 3. MCP 서버 빌드 및 실행\r\n\r\n```bash\r\n# 의존성 설치\r\nnpm install\r\n\r\n# 프로젝트 빌드\r\nnpm run build\r\n\r\n# 서버 실행 (테스트용, Claude Desktop에서는 필요 없음)\r\nnode build/index.js\r\n```\r\n\r\n### 4. RDF 데이터 가져오기\r\n\r\nGraphDB 웹 인터페이스([http://localhost:7200](http://localhost:7200))에 접속하여 다음 단계를 수행합니다:\r\n\r\n1. 리포지토리 생성:\r\n   - \"Setup\" → \"Repositories\" → \"Create new repository\"\r\n   - Repository ID: `schemaorg-current-https` (또는 원하는 이름)\r\n   - Repository title: \"Schema.org\"\r\n   - \"Create\" 클릭\r\n\r\n2. 예제 데이터 가져오기:\r\n   - 생성한 리포지토리를 선택\r\n   - \"Import\" → \"RDF\" → \"Upload RDF files\"\r\n   - `imports` 디렉토리의 예제 파일 업로드 (예: `imports/example.ttl`)\r\n   - \"Import\" 클릭\r\n\r\n> **참고**: 프로젝트에는 `imports` 디렉토리에 예제 RDF 파일이 포함되어 있습니다.\r\n\r\n### 5. Claude Desktop 설정\r\n\r\nClaude Desktop에서 Ontology MCP를 사용하려면 MCP 설정 파일을 업데이트해야 합니다:\r\n\r\n1. Claude Desktop 설정 파일 열기:\r\n   - Windows: `%AppData%\\Claude\\claude_desktop_config.json`\r\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\r\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\r\n\r\n2. 다음 설정 추가:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"a2a-ontology-mcp\": {\r\n      \"command\": \"node\",\r\n      \"args\": [\"E:\\\\codes\\\\a2a_mcp\\\\build\"],\r\n      \"env\": {\r\n        \"SPARQL_ENDPOINT\": \"http://localhost:7200\",\r\n        \"OPENAI_API_KEY\": \"your-api-key\",\r\n        \"GEMINI_API_KEY\" : \"your-api-key\"\r\n      },\r\n      \"disabled\": false,\r\n      \"autoApprove\": []\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n> **중요**: `args'의 경로를 를 프로젝트 빌드 디렉토리의 실제 절대 경로로 변경하세요.\r\n\r\n3. Claude Desktop 재시작\r\n\r\n## 라이센스\r\n\r\n이 프로젝트는 MIT 라이센스 하에 제공됩니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bigdata",
        "graphdb",
        "databases",
        "graphdb sparql",
        "access bigdata",
        "bigdata coss"
      ],
      "category": "databases"
    },
    "birdy22--mysql_mcp_server": {
      "owner": "birdy22",
      "name": "mysql_mcp_server",
      "url": "https://github.com/birdy22/mysql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/birdy22.webp",
      "description": "Facilitates secure interaction with MySQL databases for AI applications, enabling structured communication for tasks such as listing tables, reading data, and executing SQL queries. Enhances database exploration and analysis through a controlled interface.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-03T07:38:01Z",
      "readme_content": "![Tests](https://github.com/designcomputer/mysql_mcp_server/actions/workflows/test.yml/badge.svg)\n[![smithery badge](https://smithery.ai/badge/mysql-mcp-server)](https://smithery.ai/server/mysql-mcp-server)\n# MySQL MCP Server\nA Model Context Protocol (MCP) implementation that enables secure interaction with MySQL databases. This server component facilitates communication between AI applications (hosts/clients) and MySQL databases, making database exploration and analysis safer and more structured through a controlled interface.\n\n> **Note**: MySQL MCP Server is not designed to be used as a standalone server, but rather as a communication protocol implementation between AI applications and MySQL databases.\n\n## Features\n- List available MySQL tables as resources\n- Read table contents\n- Execute SQL queries with proper error handling\n- Secure database access through environment variables\n- Comprehensive logging\n\n## Installation\n### Manual Installation\n```bash\npip install mysql-mcp-server\n```\n\n### Installing via Smithery\nTo install MySQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mysql-mcp-server):\n```bash\nnpx -y @smithery/cli install mysql-mcp-server --client claude\n```\n\n## Configuration\nSet the following environment variables:\n```bash\nMYSQL_HOST=localhost     # Database host\nMYSQL_PORT=3306         # Optional: Database port (defaults to 3306 if not specified)\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=your_database\n```\n\n## Usage\n### With Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"path/to/mysql_mcp_server\",\n        \"run\",\n        \"mysql_mcp_server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### Debugging with MCP Inspector\nWhile MySQL MCP Server isn't intended to be run standalone or directly from the command line with Python, you can use the MCP Inspector to debug it.\n\nThe MCP Inspector provides a convenient way to test and debug your MCP implementation:\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n# Use the MCP Inspector for debugging (do not run directly with Python)\n```\n\nThe MySQL MCP Server is designed to be integrated with AI applications like Claude Desktop and should not be run directly as a standalone Python program.\n\n## Development\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mysql_mcp_server.git\ncd mysql_mcp_server\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n# Install development dependencies\npip install -r requirements-dev.txt\n# Run tests\npytest\n```\n\n## Security Considerations\n- Never commit environment variables or credentials\n- Use a database user with minimal required permissions\n- Consider implementing query whitelisting for production use\n- Monitor and log all database operations\n\n## Security Best Practices\nThis MCP implementation requires database access to function. For security:\n1. **Create a dedicated MySQL user** with minimal permissions\n2. **Never use root credentials** or administrative accounts\n3. **Restrict database access** to only necessary operations\n4. **Enable logging** for audit purposes\n5. **Regular security reviews** of database access\n\nSee [MySQL Security Configuration Guide](https://github.com/designcomputer/mysql_mcp_server/blob/main/SECURITY.md) for detailed instructions on:\n- Creating a restricted MySQL user\n- Setting appropriate permissions\n- Monitoring database access\n- Security best practices\n\n⚠️ IMPORTANT: Always follow the principle of least privilege when configuring database access.\n\n## License\nMIT License - see LICENSE file for details.\n\n## Contributing\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "bram2w--baserow": {
      "owner": "bram2w",
      "name": "baserow",
      "url": "https://github.com/bram2w/baserow",
      "imageUrl": "",
      "description": "Baserow database integration with table search, list, and row create, read, update, and delete capabilities.",
      "stars": 2894,
      "forks": 388,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-10-02T14:50:04Z",
      "readme_content": "## Baserow is an open-source no-code database tool and an Airtable alternative.\n\nCreate your own online database without technical experience. Our user-friendly no-code\ntool gives you the powers of a developer without leaving your browser.\n\n* A spreadsheet database hybrid combining ease of use and powerful data organization.\n* Easily self-hosted with no storage restrictions or sign-up on https://baserow.io to\n  get started immediately.\n* Alternative to Airtable.\n* Open-core with all non-premium and non-enterprise features under\n  the [MIT License](https://choosealicense.com/licenses/mit/) allowing commercial and\n  private use.\n* Headless and API first.\n* Uses popular frameworks and tools like [Django](https://www.djangoproject.com/),\n  [Vue.js](https://vuejs.org/) and [PostgreSQL](https://www.postgresql.org/).\n\n[![Deploy to Heroku](https://www.herokucdn.com/deploy/button.svg)](https://www.heroku.com/deploy/?template=https://github.com/bram2w/baserow/tree/master)\n\n```bash\ndocker run -v baserow_data:/baserow/data -p 80:80 -p 443:443 baserow/baserow:1.35.2\n```\n\n\n\n## Get Involved\n\n**We're hiring remotely**! More information at https://baserow.io/jobs.\n\nJoin our forum at https://community.baserow.io/. See\n[CONTRIBUTING.md](./CONTRIBUTING.md) on how to become a contributor.\n\n## Installation\n\n* [**Docker**](docs/installation/install-with-docker.md)\n* [**Ubuntu**](docs/installation/install-on-ubuntu.md)\n* [**Docker Compose** ](docs/installation/install-with-docker-compose.md)\n* [**Heroku**: Easily install and scale up Baserow on Heroku.](docs/installation/install-on-heroku.md)\n* [**Render**: Easily install and scale up Baserow on Render.](docs/installation/install-on-render.md)\n* [**Digital Ocean**: Easily install and scale up Baserow on Digital Ocean.](docs/installation/install-on-digital-ocean.md)\n* [**Cloudron**: Install and update Baserow on your own Cloudron server.](docs/installation/install-on-cloudron.md)\n* [**Railway**: Install Baserow via Railway.](docs/installation/install-on-railway.md)\n* [**Elestio**: Fully managed by Elestio.](https://elest.io/open-source/baserow)\n\n## Official documentation\n\nThe official documentation can be found on the website at https://baserow.io/docs/index\nor [here](./docs/index.md) inside the repository. The API docs can be found here at\nhttps://api.baserow.io/api/redoc/ or if you are looking for the OpenAPI schema here\nhttps://api.baserow.io/api/schema.json.\n\n## Become a sponsor\n\nIf you would like to get new features faster, then you might want to consider becoming a\nsponsor. By becoming a sponsor we can spend more time on Baserow which means faster\ndevelopment.\n\n[Become a GitHub Sponsor](https://github.com/sponsors/bram2w)\n\n## Development environment\n\nIf you want to contribute to Baserow you can setup a development environment like so:\n\n```\n$ git clone https://gitlab.com/baserow/baserow.git\n$ cd baserow\n$ ./dev.sh --build\n```\n\nThe Baserow development environment is now running.\nVisit [http://localhost:3000](http://localhost:3000) in your browser to see a working\nversion in development mode with hot code reloading and other dev features enabled.\n\nMore detailed instructions and more information about the development environment can be\nfound\nat [https://baserow.io/docs/development/development-environment](./docs/development/development-environment.md)\n.\n\n## Plugin development\n\nBecause of the modular architecture of Baserow it is possible to create plugins. Make\nyour own fields, views, applications, pages, or endpoints. We also have a plugin\nboilerplate to get you started right away. More information can be found in the\n[plugin introduction](./docs/plugins/introduction.md) and in the\n[plugin boilerplate docs](./docs/plugins/boilerplate.md).\n\n## Meta\n\nCreated by Baserow B.V. - bram@baserow.io.\n\nDistributes under the MIT license. See `LICENSE` for more information.\n\nVersion: 1.35.2\n\nThe official repository can be found at https://gitlab.com/baserow/baserow.\n\nThe changelog can be found [here](./changelog.md).\n\nBecome a GitHub Sponsor [here](https://github.com/sponsors/bram2w).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "baserow",
        "databases secure",
        "secure database",
        "baserow database"
      ],
      "category": "databases"
    },
    "burakdirin--clickhouse-mcp-server": {
      "owner": "burakdirin",
      "name": "clickhouse-mcp-server",
      "url": "https://github.com/burakdirin/clickhouse-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/burakdirin.webp",
      "description": "Enables interaction with Clickhouse databases, allowing query execution and secure database connections in a read-only mode. Supports multi-query execution and provides JSON format results.",
      "stars": 2,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-22T08:20:36Z",
      "readme_content": "# Clickhouse MCP server\n[![smithery badge](https://smithery.ai/badge/@burakdirin/clickhouse-mcp-server)](https://smithery.ai/server/@burakdirin/clickhouse-mcp-server)\n\nA Clickhouse database MCP server project.\n\n## Installation\n\nYou can install the package using `uv`:\n\n```bash\nuv pip install clickhouse-mcp-server\n```\n\nOr using `pip`:\n\n```bash\npip install clickhouse-mcp-server\n```\n\n## Components\n\n### Tools\n\nThe server provides two tools:\n- `connect_database`: Connects to a specific Clickhouse database\n  - `database` parameter: Name of the database to connect to (string)\n  - Returns a confirmation message when connection is successful\n\n- `execute_query`: Executes Clickhouse queries\n  - `query` parameter: SQL query/queries to execute (string)\n  - Returns query results in JSON format\n  - Multiple queries can be sent separated by semicolons\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `CLICKHOUSE_HOST`: Clickhouse server address (default: \"localhost\")\n- `CLICKHOUSE_USER`: Clickhouse username (default: \"root\") \n- `CLICKHOUSE_PASSWORD`: Clickhouse password (default: \"\")\n- `CLICKHOUSE_DATABASE`: Initial database (optional)\n- `CLICKHOUSE_READONLY`: Read-only mode (set to 1/true to enable, default: false)\n\n## Quickstart\n\n### Installation\n\n#### Claude Desktop\n\nMacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Server Configuration</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"clickhouse-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/burakdirin/Projects/clickhouse-mcp-server\",\n        \"run\",\n        \"clickhouse-mcp-server\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"localhost\",\n        \"CLICKHOUSE_USER\": \"root\",\n        \"CLICKHOUSE_PASSWORD\": \"password\",\n        \"CLICKHOUSE_DATABASE\": \"[optional]\",\n        \"CLICKHOUSE_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n  <summary>Published Server Configuration</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"clickhouse-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"clickhouse-mcp-server\"\n      ],\n      \"env\": {\n        \"CLICKHOUSE_HOST\": \"localhost\",\n        \"CLICKHOUSE_USER\": \"root\",\n        \"CLICKHOUSE_PASSWORD\": \"password\",\n        \"CLICKHOUSE_DATABASE\": \"[optional]\",\n        \"CLICKHOUSE_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n</details>\n\n### Installing via Smithery\n\nTo install Clickhouse Database Integration Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@burakdirin/clickhouse-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @burakdirin/clickhouse-mcp-server --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/burakdirin/Projects/clickhouse-mcp-server run clickhouse-mcp-server\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "querying",
        "clickhouse databases",
        "enables querying",
        "databases secure"
      ],
      "category": "databases"
    },
    "burakdirin--mysqldb-mcp-server": {
      "owner": "burakdirin",
      "name": "mysqldb-mcp-server",
      "url": "https://github.com/burakdirin/mysqldb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/burakdirin.webp",
      "description": "Enables interaction with MySQL databases, allowing secure connections, execution of queries, and support for read-only mode and multiple queries.",
      "stars": 7,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-12T17:22:09Z",
      "readme_content": "# mysqldb-mcp-server MCP server\n[![smithery badge](https://smithery.ai/badge/@burakdirin/mysqldb-mcp-server)](https://smithery.ai/server/@burakdirin/mysqldb-mcp-server)\n\nA MySQL database MCP server project.\n\n## Installation\n\nYou can install the package using `uv`:\n\n```bash\nuv pip install mysqldb-mcp-server\n```\n\nOr using `pip`:\n\n```bash\npip install mysqldb-mcp-server\n```\n\n## Components\n\n### Tools\n\nThe server provides two tools:\n- `connect_database`: Connects to a specific MySQL database\n  - `database` parameter: Name of the database to connect to (string)\n  - Returns a confirmation message when connection is successful\n\n- `execute_query`: Executes MySQL queries\n  - `query` parameter: SQL query/queries to execute (string)\n  - Returns query results in JSON format\n  - Multiple queries can be sent separated by semicolons\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `MYSQL_HOST`: MySQL server address (default: \"localhost\")\n- `MYSQL_USER`: MySQL username (default: \"root\") \n- `MYSQL_PASSWORD`: MySQL password (default: \"\")\n- `MYSQL_DATABASE`: Initial database (optional)\n- `MYSQL_READONLY`: Read-only mode (set to 1/true to enable, default: false)\n\n## Quickstart\n\n### Installation\n\n#### Claude Desktop\n\nMacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Server Configuration</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"mysqldb-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/burakdirin/Projects/mysqldb-mcp-server\",\n        \"run\",\n        \"mysqldb-mcp-server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASSWORD\": \"password\",\n        \"MYSQL_DATABASE\": \"[optional]\",\n        \"MYSQL_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n  <summary>Published Server Configuration</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"mysqldb-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mysqldb-mcp-server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASSWORD\": \"password\",\n        \"MYSQL_DATABASE\": \"[optional]\",\n        \"MYSQL_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n</details>\n\n### Installing via Smithery\n\nTo install MySQL Database Integration Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@burakdirin/mysqldb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @burakdirin/mysqldb-mcp-server --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/burakdirin/Projects/mysqldb-mcp-server run mysqldb-mcp-server\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mysqldb",
        "database",
        "burakdirin mysqldb",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "bytebase--dbhub": {
      "owner": "bytebase",
      "name": "dbhub",
      "url": "https://github.com/bytebase/dbhub",
      "imageUrl": "/freedevtools/mcp/pfp/bytebase.webp",
      "description": "Connects to various databases and executes read-only SQL queries with safety checks, providing a unified interface for database management.",
      "stars": 1357,
      "forks": 123,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T07:47:27Z",
      "readme_content": "> [!NOTE]  \n> Brought to you by [Bytebase](https://www.bytebase.com/), open-source database DevSecOps platform.\n\n<p align=\"center\">\n<a href=\"https://dbhub.ai/\" target=\"_blank\">\n<picture>\n  <img alt=\"logo_full\" src=\"https://raw.githubusercontent.com/bytebase/dbhub/main/resources/images/logo-full.webp\" width=\"50%\">\n</picture>\n</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/BjEkZpsJzn\"><img src=\"https://img.shields.io/badge/%20-Hang%20out%20on%20Discord-5865F2?style=for-the-badge&logo=discord&labelColor=EEEEEE\" alt=\"Join our Discord\" height=\"32\" /></a>\n</p>\n\n<p>\nAdd to Cursor by copying the below link to browser\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=dbhub&config=eyJjb21tYW5kIjoibnB4IEBieXRlYmFzZS9kYmh1YiIsImVudiI6eyJUUkFOU1BPUlQiOiJzdGRpbyIsIkRTTiI6InBvc3RncmVzOi8vdXNlcjpwYXNzd29yZEBsb2NhbGhvc3Q6NTQzMi9kYm5hbWU%2Fc3NsbW9kZT1kaXNhYmxlIiwiUkVBRE9OTFkiOiJ0cnVlIn19\n```\n\n</p>\n\nDBHub is a universal database gateway implementing the Model Context Protocol (MCP) server interface. This gateway allows MCP-compatible clients to connect to and explore different databases.\n\n```bash\n +------------------+    +--------------+    +------------------+\n |                  |    |              |    |                  |\n |                  |    |              |    |                  |\n |  Claude Desktop  +--->+              +--->+    PostgreSQL    |\n |                  |    |              |    |                  |\n |  Claude Code     +--->+              +--->+    SQL Server    |\n |                  |    |              |    |                  |\n |  Cursor          +--->+    DBHub     +--->+    SQLite        |\n |                  |    |              |    |                  |\n |  Other Clients   +--->+              +--->+    MySQL         |\n |                  |    |              |    |                  |\n |                  |    |              +--->+    MariaDB       |\n |                  |    |              |    |                  |\n |                  |    |              |    |                  |\n +------------------+    +--------------+    +------------------+\n      MCP Clients           MCP Server             Databases\n```\n\n## Supported Matrix\n\n### Database Resources\n\n| Resource Name               | URI Format                                             | PostgreSQL | MySQL | MariaDB | SQL Server | SQLite |\n| --------------------------- | ------------------------------------------------------ | :--------: | :---: | :-----: | :--------: | :----: |\n| schemas                     | `db://schemas`                                         |     ✅     |  ✅   |   ✅    |     ✅     |   ✅   |\n| tables_in_schema            | `db://schemas/{schemaName}/tables`                     |     ✅     |  ✅   |   ✅    |     ✅     |   ✅   |\n| table_structure_in_schema   | `db://schemas/{schemaName}/tables/{tableName}`         |     ✅     |  ✅   |   ✅    |     ✅     |   ✅   |\n| indexes_in_table            | `db://schemas/{schemaName}/tables/{tableName}/indexes` |     ✅     |  ✅   |   ✅    |     ✅     |   ✅   |\n| procedures_in_schema        | `db://schemas/{schemaName}/procedures`                 |     ✅     |  ✅   |   ✅    |     ✅     |   ❌   |\n| procedure_details_in_schema | `db://schemas/{schemaName}/procedures/{procedureName}` |     ✅     |  ✅   |   ✅    |     ✅     |   ❌   |\n\n### Database Tools\n\n| Tool        | Command Name  | Description                                                         | PostgreSQL | MySQL | MariaDB | SQL Server | SQLite |\n| ----------- | ------------- | ------------------------------------------------------------------- | :--------: | :---: | :-----: | :--------: | ------ |\n| Execute SQL | `execute_sql` | Execute single or multiple SQL statements (separated by semicolons) |     ✅     |  ✅   |   ✅    |     ✅     | ✅     |\n\n### Prompt Capabilities\n\n| Prompt              | Command Name   | PostgreSQL | MySQL | MariaDB | SQL Server | SQLite |\n| ------------------- | -------------- | :--------: | :---: | :-----: | :--------: | ------ |\n| Generate SQL        | `generate_sql` |     ✅     |  ✅   |   ✅    |     ✅     | ✅     |\n| Explain DB Elements | `explain_db`   |     ✅     |  ✅   |   ✅    |     ✅     | ✅     |\n\n## Installation\n\n### Docker\n\n```bash\n# PostgreSQL example\ndocker run --rm --init \\\n   --name dbhub \\\n   --publish 8080:8080 \\\n   bytebase/dbhub \\\n   --transport http \\\n   --port 8080 \\\n   --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n```\n\n```bash\n# Demo mode with sqlite sample employee database\ndocker run --rm --init \\\n   --name dbhub \\\n   --publish 8080:8080 \\\n   bytebase/dbhub \\\n   --transport http \\\n   --port 8080 \\\n   --demo\n```\n\n**Docker Compose Setup:**\n\nIf you're using Docker Compose for development, add DBHub to your `docker-compose.yml`:\n\n```yaml\ndbhub:\n  image: bytebase/dbhub:latest\n  container_name: dbhub\n  ports:\n    - \"8080:8080\"\n  environment:\n    - DBHUB_LOG_LEVEL=info\n  command:\n    - --transport\n    - http\n    - --port\n    - \"8080\"\n    - --dsn\n    - \"postgres://user:password@database:5432/dbname\"\n  depends_on:\n    - database\n```\n\n### NPM\n\n```bash\n# PostgreSQL example\nnpx @bytebase/dbhub --transport http --port 8080 --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n\n# Demo mode with sqlite sample employee database\nnpx @bytebase/dbhub --transport http --port 8080 --demo\n```\n\n```bash\n# Demo mode with sample employee database\nnpx @bytebase/dbhub --transport http --port 8080 --demo\n```\n\n> Note: The demo mode includes a bundled SQLite sample \"employee\" database with tables for employees, departments, salaries, and more.\n\n### Claude Desktop\n\n![claude-desktop](https://raw.githubusercontent.com/bytebase/dbhub/main/resources/images/claude-desktop.webp)\n\n- Claude Desktop only supports `stdio` transport https://github.com/orgs/modelcontextprotocol/discussions/16\n\n```json\n// claude_desktop_config.json\n{\n  \"mcpServers\": {\n    \"dbhub-postgres-docker\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"bytebase/dbhub\",\n        \"--transport\",\n        \"stdio\",\n        \"--dsn\",\n        // Use host.docker.internal as the host if connecting to the local db\n        \"postgres://user:password@host.docker.internal:5432/dbname?sslmode=disable\"\n      ]\n    },\n    \"dbhub-postgres-npx\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@bytebase/dbhub\",\n        \"--transport\",\n        \"stdio\",\n        \"--dsn\",\n        \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n      ]\n    },\n    \"dbhub-demo\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@bytebase/dbhub\", \"--transport\", \"stdio\", \"--demo\"]\n    }\n  }\n}\n```\n\n### Claude Code\n\nCheck https://docs.anthropic.com/en/docs/claude-code/mcp\n\n### Cursor\n\n<p>\nAdd to Cursor by copying the below link to browser\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=dbhub&config=eyJjb21tYW5kIjoibnB4IEBieXRlYmFzZS9kYmh1YiIsImVudiI6eyJUUkFOU1BPUlQiOiJzdGRpbyIsIkRTTiI6InBvc3RncmVzOi8vdXNlcjpwYXNzd29yZEBsb2NhbGhvc3Q6NTQzMi9kYm5hbWU%2Fc3NsbW9kZT1kaXNhYmxlIiwiUkVBRE9OTFkiOiJ0cnVlIn19\n```\n\n</p>\n\n![cursor](https://raw.githubusercontent.com/bytebase/dbhub/main/resources/images/cursor.webp)\n\n- Cursor supports both `stdio` and `http`.\n- Follow [Cursor MCP guide](https://docs.cursor.com/context/model-context-protocol) and make sure to use [Agent](https://docs.cursor.com/chat/agent) mode.\n\n### VSCode + Copilot\n\nCheck https://code.visualstudio.com/docs/copilot/customization/mcp-servers\n\nVSCode with GitHub Copilot can connect to DBHub via both `stdio` and `http` transports. This enables AI agents to interact with your development database through a secure interface.\n\n- VSCode supports both `stdio` and `http` transports\n- Configure MCP server in `.vscode/mcp.json`:\n\n**Stdio Transport:**\n\n```json\n{\n  \"servers\": {\n    \"dbhub\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@bytebase/dbhub\",\n        \"--transport\",\n        \"stdio\",\n        \"--dsn\",\n        \"postgres://user:password@localhost:5432/dbname\"\n      ]\n    }\n  },\n  \"inputs\": []\n}\n```\n\n**HTTP Transport:**\n\n```json\n{\n  \"servers\": {\n    \"dbhub\": {\n      \"url\": \"http://localhost:8080/message\",\n      \"type\": \"http\"\n    }\n  },\n  \"inputs\": []\n}\n```\n\n**Copilot Instructions:**\n\nYou can provide Copilot with context by creating `.github/copilot-instructions.md`:\n\n```markdown\n## Database Access\n\nThis project provides an MCP server (DBHub) for secure SQL access to the development database.\n\nAI agents can execute SQL queries. In read-only mode (recommended for production):\n\n- `SELECT * FROM users LIMIT 5;`\n- `SHOW TABLES;`\n- `DESCRIBE table_name;`\n\nIn read-write mode (development/testing):\n\n- `INSERT INTO users (name, email) VALUES ('John', 'john@example.com');`\n- `UPDATE users SET status = 'active' WHERE id = 1;`\n- `CREATE TABLE test_table (id INT PRIMARY KEY);`\n\nUse `--readonly` flag to restrict to read-only operations for safety.\n```\n\n## Usage\n\n### Read-only Mode\n\nYou can run DBHub in read-only mode, which restricts SQL query execution to read-only operations:\n\n```bash\n# Enable read-only mode\nnpx @bytebase/dbhub --readonly --dsn \"postgres://user:password@localhost:5432/dbname\"\n```\n\nIn read-only mode, only [readonly SQL operations](https://github.com/bytebase/dbhub/blob/main/src/utils/allowed-keywords.ts) are allowed.\n\nThis provides an additional layer of security when connecting to production databases.\n\n### Row Limiting\n\nYou can limit the number of rows returned from SELECT queries using the `--max-rows` parameter. This helps prevent accidentally retrieving too much data from large tables:\n\n```bash\n# Limit SELECT queries to return at most 1000 rows\nnpx @bytebase/dbhub --dsn \"postgres://user:password@localhost:5432/dbname\" --max-rows 1000\n```\n\n- Row limiting is only applied to SELECT statements, not INSERT/UPDATE/DELETE\n- If your query already has a `LIMIT` or `TOP` clause, DBHub uses the smaller value\n\n### SSL Connections\n\nYou can specify the SSL mode using the `sslmode` parameter in your DSN string:\n\n| Database   | `sslmode=disable` | `sslmode=require` |   Default SSL Behavior   |\n| ---------- | :---------------: | :---------------: | :----------------------: |\n| PostgreSQL |        ✅         |        ✅         | Certificate verification |\n| MySQL      |        ✅         |        ✅         | Certificate verification |\n| MariaDB    |        ✅         |        ✅         | Certificate verification |\n| SQL Server |        ✅         |        ✅         | Certificate verification |\n| SQLite     |        ❌         |        ❌         |     N/A (file-based)     |\n\n**SSL Mode Options:**\n\n- `sslmode=disable`: All SSL/TLS encryption is turned off. Data is transmitted in plaintext.\n- `sslmode=require`: Connection is encrypted, but the server's certificate is not verified. This provides protection against packet sniffing but not against man-in-the-middle attacks. You may use this for trusted self-signed CA.\n\nWithout specifying `sslmode`, most databases default to certificate verification, which provides the highest level of security.\n\nExample usage:\n\n```bash\n# Disable SSL\npostgres://user:password@localhost:5432/dbname?sslmode=disable\n\n# Require SSL without certificate verification\npostgres://user:password@localhost:5432/dbname?sslmode=require\n\n# Standard SSL with certificate verification (default)\npostgres://user:password@localhost:5432/dbname\n```\n\n### SSH Tunnel Support\n\nDBHub supports connecting to databases through SSH tunnels, enabling secure access to databases in private networks or behind firewalls.\n\n#### Using SSH Config File (Recommended)\n\nDBHub can read SSH connection settings from your `~/.ssh/config` file. Simply use the host alias from your SSH config:\n\n```bash\n# If you have this in ~/.ssh/config:\n# Host mybastion\n#   HostName bastion.example.com\n#   User ubuntu\n#   IdentityFile ~/.ssh/id_rsa\n\nnpx @bytebase/dbhub \\\n  --dsn \"postgres://dbuser:dbpass@database.internal:5432/mydb\" \\\n  --ssh-host mybastion\n```\n\nDBHub will automatically use the settings from your SSH config, including hostname, user, port, and identity file. If no identity file is specified in the config, DBHub will try common default locations (`~/.ssh/id_rsa`, `~/.ssh/id_ed25519`, etc.).\n\n#### SSH with Password Authentication\n\n```bash\nnpx @bytebase/dbhub \\\n  --dsn \"postgres://dbuser:dbpass@database.internal:5432/mydb\" \\\n  --ssh-host bastion.example.com \\\n  --ssh-user ubuntu \\\n  --ssh-password mypassword\n```\n\n#### SSH with Private Key Authentication\n\n```bash\nnpx @bytebase/dbhub \\\n  --dsn \"postgres://dbuser:dbpass@database.internal:5432/mydb\" \\\n  --ssh-host bastion.example.com \\\n  --ssh-user ubuntu \\\n  --ssh-key ~/.ssh/id_rsa\n```\n\n#### SSH with Private Key and Passphrase\n\n```bash\nnpx @bytebase/dbhub \\\n  --dsn \"postgres://dbuser:dbpass@database.internal:5432/mydb\" \\\n  --ssh-host bastion.example.com \\\n  --ssh-port 2222 \\\n  --ssh-user ubuntu \\\n  --ssh-key ~/.ssh/id_rsa \\\n  --ssh-passphrase mykeypassphrase\n```\n\n#### Using Environment Variables\n\n```bash\nexport SSH_HOST=bastion.example.com\nexport SSH_USER=ubuntu\nexport SSH_KEY=~/.ssh/id_rsa\nnpx @bytebase/dbhub --dsn \"postgres://dbuser:dbpass@database.internal:5432/mydb\"\n```\n\n**Note**: When using SSH tunnels, the database host in your DSN should be the hostname/IP as seen from the SSH server (bastion host), not from your local machine.\n\n### Configure your database connection\n\nYou can use DBHub in demo mode with a sample employee database for testing:\n\n```bash\nnpx @bytebase/dbhub  --demo\n```\n\n> [!WARNING]\n> If your user/password contains special characters, you have two options:\n>\n> 1. Escape them in the DSN (e.g. `pass#word` should be escaped as `pass%23word`)\n> 2. Use the individual database parameters method below (recommended)\n\nFor real databases, you can configure the database connection in two ways:\n\n#### Method 1: Database Source Name (DSN)\n\n- **Command line argument** (highest priority):\n\n  ```bash\n  npx @bytebase/dbhub  --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n  ```\n\n- **Environment variable** (second priority):\n\n  ```bash\n  export DSN=\"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n  npx @bytebase/dbhub\n  ```\n\n- **Environment file** (third priority):\n  - For development: Create `.env.local` with your DSN\n  - For production: Create `.env` with your DSN\n  ```\n  DSN=postgres://user:password@localhost:5432/dbname?sslmode=disable\n  ```\n\n#### Method 2: Individual Database Parameters\n\nIf your password contains special characters that would break URL parsing, use individual environment variables instead:\n\n- **Environment variables**:\n\n  ```bash\n  export DB_TYPE=postgres\n  export DB_HOST=localhost\n  export DB_PORT=5432\n  export DB_USER=myuser\n  export DB_PASSWORD='my@complex:password/with#special&chars'\n  export DB_NAME=mydatabase\n  npx @bytebase/dbhub\n  ```\n\n- **Environment file**:\n  ```\n  DB_TYPE=postgres\n  DB_HOST=localhost\n  DB_PORT=5432\n  DB_USER=myuser\n  DB_PASSWORD=my@complex:password/with#special&chars\n  DB_NAME=mydatabase\n  ```\n\n**Supported DB_TYPE values**: `postgres`, `mysql`, `mariadb`, `sqlserver`, `sqlite`\n\n**Default ports** (when DB_PORT is omitted):\n\n- PostgreSQL: `5432`\n- MySQL/MariaDB: `3306`\n- SQL Server: `1433`\n\n**For SQLite**: Only `DB_TYPE=sqlite` and `DB_NAME=/path/to/database.db` are required.\n\n> [!TIP]\n> Use the individual parameter method when your password contains special characters like `@`, `:`, `/`, `#`, `&`, `=` that would break DSN parsing.\n\n> [!WARNING]\n> When running in Docker, use `host.docker.internal` instead of `localhost` to connect to databases running on your host machine. For example: `mysql://user:password@host.docker.internal:3306/dbname`\n\nDBHub supports the following database connection string formats:\n\n| Database   | DSN Format                                               | Example                                                                                                        |\n| ---------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n| MySQL      | `mysql://[user]:[password]@[host]:[port]/[database]`     | `mysql://user:password@localhost:3306/dbname?sslmode=disable`                                                  |\n| MariaDB    | `mariadb://[user]:[password]@[host]:[port]/[database]`   | `mariadb://user:password@localhost:3306/dbname?sslmode=disable`                                                |\n| PostgreSQL | `postgres://[user]:[password]@[host]:[port]/[database]`  | `postgres://user:password@localhost:5432/dbname?sslmode=disable`                                               |\n| SQL Server | `sqlserver://[user]:[password]@[host]:[port]/[database]` | `sqlserver://user:password@localhost:1433/dbname?sslmode=disable`                                              |\n| SQLite     | `sqlite:///[path/to/file]` or `sqlite:///:memory:`       | `sqlite:///path/to/database.db`, `sqlite:C:/Users/YourName/data/database.db (windows)` or `sqlite:///:memory:` |\n\n#### SQL Server\n\nExtra query parameters:\n\n#### authentication\n\n- `authentication=azure-active-directory-access-token`. Only applicable when running from Azure. See [DefaultAzureCredential](https://learn.microsoft.com/en-us/azure/developer/javascript/sdk/authentication/credential-chains#use-defaultazurecredential-for-flexibility).\n\n### Transport\n\n- **stdio** (default) - for direct integration with tools like Claude Desktop:\n\n  ```bash\n  npx @bytebase/dbhub --transport stdio --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n  ```\n\n- **http** - for browser and network clients:\n  ```bash\n  npx @bytebase/dbhub --transport http --port 5678 --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n  ```\n\n### Command line options\n\n| Option         | Environment Variable | Description                                                           | Default                      |\n| -------------- | -------------------- | --------------------------------------------------------------------- | ---------------------------- |\n| dsn            | `DSN`                | Database connection string                                            | Required if not in demo mode |\n| N/A            | `DB_TYPE`            | Database type: `postgres`, `mysql`, `mariadb`, `sqlserver`, `sqlite`  | N/A                          |\n| N/A            | `DB_HOST`            | Database server hostname (not needed for SQLite)                      | N/A                          |\n| N/A            | `DB_PORT`            | Database server port (uses default if omitted, not needed for SQLite) | N/A                          |\n| N/A            | `DB_USER`            | Database username (not needed for SQLite)                             | N/A                          |\n| N/A            | `DB_PASSWORD`        | Database password (not needed for SQLite)                             | N/A                          |\n| N/A            | `DB_NAME`            | Database name or SQLite file path                                     | N/A                          |\n| transport      | `TRANSPORT`          | Transport mode: `stdio` or `http`                                     | `stdio`                      |\n| port           | `PORT`               | HTTP server port (only applicable when using `--transport=http`)      | `8080`                       |\n| readonly       | `READONLY`           | Restrict SQL execution to read-only operations                        | `false`                      |\n| max-rows       | N/A                  | Limit the number of rows returned from SELECT queries                 | No limit                     |\n| demo           | N/A                  | Run in demo mode with sample employee database                        | `false`                      |\n| ssh-host       | `SSH_HOST`           | SSH server hostname for tunnel connection                             | N/A                          |\n| ssh-port       | `SSH_PORT`           | SSH server port                                                       | `22`                         |\n| ssh-user       | `SSH_USER`           | SSH username                                                          | N/A                          |\n| ssh-password   | `SSH_PASSWORD`       | SSH password (for password authentication)                            | N/A                          |\n| ssh-key        | `SSH_KEY`            | Path to SSH private key file                                          | N/A                          |\n| ssh-passphrase | `SSH_PASSPHRASE`     | Passphrase for SSH private key                                        | N/A                          |\n\nThe demo mode uses an in-memory SQLite database loaded with the [sample employee database](https://github.com/bytebase/dbhub/tree/main/resources/employee-sqlite) that includes tables for employees, departments, titles, salaries, department employees, and department managers. The sample database includes SQL scripts for table creation, data loading, and testing.\n\n## Development\n\n1. Install dependencies:\n\n   ```bash\n   pnpm install\n   ```\n\n1. Run in development mode:\n\n   ```bash\n   pnpm dev\n   ```\n\n1. Build for production:\n   ```bash\n   pnpm build\n   pnpm start --transport stdio --dsn \"postgres://user:password@localhost:5432/dbname?sslmode=disable\"\n   ```\n\n### Testing\n\nThe project uses Vitest for comprehensive unit and integration testing:\n\n- **Run all tests**: `pnpm test`\n- **Run tests in watch mode**: `pnpm test:watch`\n- **Run integration tests**: `pnpm test:integration`\n\n#### Integration Tests\n\nDBHub includes comprehensive integration tests for all supported database connectors using [Testcontainers](https://testcontainers.com/). These tests run against real database instances in Docker containers, ensuring full compatibility and feature coverage.\n\n##### Prerequisites\n\n- **Docker**: Ensure Docker is installed and running on your machine\n- **Docker Resources**: Allocate sufficient memory (recommended: 4GB+) for multiple database containers\n- **Network Access**: Ability to pull Docker images from registries\n\n##### Running Integration Tests\n\n**Note**: This command runs all integration tests in parallel, which may take 5-15 minutes depending on your system resources and network speed.\n\n```bash\n# Run all database integration tests\npnpm test:integration\n```\n\n```bash\n# Run only PostgreSQL integration tests\npnpm test src/connectors/__tests__/postgres.integration.test.ts\n# Run only MySQL integration tests\npnpm test src/connectors/__tests__/mysql.integration.test.ts\n# Run only MariaDB integration tests\npnpm test src/connectors/__tests__/mariadb.integration.test.ts\n# Run only SQL Server integration tests\npnpm test src/connectors/__tests__/sqlserver.integration.test.ts\n# Run only SQLite integration tests\npnpm test src/connectors/__tests__/sqlite.integration.test.ts\n# Run JSON RPC integration tests\npnpm test src/__tests__/json-rpc-integration.test.ts\n```\n\nAll integration tests follow these patterns:\n\n1. **Container Lifecycle**: Start database container → Connect → Setup test data → Run tests → Cleanup\n2. **Shared Test Utilities**: Common test patterns implemented in `IntegrationTestBase` class\n3. **Database-Specific Features**: Each database includes tests for unique features and capabilities\n4. **Error Handling**: Comprehensive testing of connection errors, invalid SQL, and edge cases\n\n##### Troubleshooting Integration Tests\n\n**Container Startup Issues:**\n\n```bash\n# Check Docker is running\ndocker ps\n\n# Check available memory\ndocker system df\n\n# Pull images manually if needed\ndocker pull postgres:15-alpine\ndocker pull mysql:8.0\ndocker pull mariadb:10.11\ndocker pull mcr.microsoft.com/mssql/server:2019-latest\n```\n\n**SQL Server Timeout Issues:**\n\n- SQL Server containers require significant startup time (3-5 minutes)\n- Ensure Docker has sufficient memory allocated (4GB+ recommended)\n- Consider running SQL Server tests separately if experiencing timeouts\n\n**Network/Resource Issues:**\n\n```bash\n# Run tests with verbose output\npnpm test:integration --reporter=verbose\n\n# Run single database test to isolate issues\npnpm test:integration -- --testNamePattern=\"PostgreSQL\"\n\n# Check Docker container logs if tests fail\ndocker logs <container_id>\n```\n\n#### Pre-commit Hooks (for Developers)\n\nThe project includes pre-commit hooks to run tests automatically before each commit:\n\n1. After cloning the repository, set up the pre-commit hooks:\n\n   ```bash\n   ./scripts/setup-husky.sh\n   ```\n\n2. This ensures the test suite runs automatically whenever you create a commit, preventing commits that would break tests.\n\n### Debug with [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n\n![mcp-inspector](https://raw.githubusercontent.com/bytebase/dbhub/main/resources/images/mcp-inspector.webp)\n\n#### stdio\n\n```bash\n# PostgreSQL example\nTRANSPORT=stdio DSN=\"postgres://user:password@localhost:5432/dbname?sslmode=disable\" npx @modelcontextprotocol/inspector node /path/to/dbhub/dist/index.js\n```\n\n#### HTTP\n\n```bash\n# Start DBHub with HTTP transport\npnpm dev --transport=http --port=8080\n\n# Start the MCP Inspector in another terminal\nnpx @modelcontextprotocol/inspector\n```\n\nConnect to the DBHub server `/message` endpoint\n\n## Contributors\n\n<a href=\"https://github.com/bytebase/dbhub/graphs/contributors\">\n  <img alt=\"dbhub\" src=\"https://contrib.rocks/image?repo=bytebase/dbhub\" />\n</a>\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=bytebase/dbhub&type=Date)](https://www.star-history.com/#bytebase/dbhub&Date)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "bytebase",
        "dbhub",
        "bytebase dbhub",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "c0h1b4--mssql-mcp-server": {
      "owner": "c0h1b4",
      "name": "mssql-mcp-server",
      "url": "https://github.com/c0h1b4/mssql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/c0h1b4.webp",
      "description": "Connects to Microsoft SQL Server databases and executes SQL queries, providing management tools for database connections.",
      "stars": 14,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-20T03:45:11Z",
      "readme_content": "# MSSQL MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@c0h1b4/mssql-mcp-server)](https://smithery.ai/server/@c0h1b4/mssql-mcp-server)\n\nA Model Context Protocol (MCP) server for connecting to Microsoft SQL Server databases. This server provides tools for executing SQL queries and managing database connections.\n\n**Version Notice:** This project has been upgraded to use Model Context Protocol SDK 1.9.0. See [UPGRADE.md](UPGRADE.md) for details.\n\n## Installation\n\n### Installing via Smithery\n\nTo install MSSQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@c0h1b4/mssql-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @c0h1b4/mssql-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install mssql-mcp-server\n```\n\n## Usage\n\nAdd the server to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"mssql-mcp-server\",\n      \"env\": {\n        \"MSSQL_CONNECTION_STRING\": \"Server=localhost;Database=master;User Id=sa;Password=yourpassword;\",\n        // Or individual connection parameters:\n        \"MSSQL_HOST\": \"localhost\",\n        \"MSSQL_PORT\": \"1433\",\n        \"MSSQL_DATABASE\": \"master\",\n        \"MSSQL_USER\": \"sa\",\n        \"MSSQL_PASSWORD\": \"yourpassword\",\n        \"MSSQL_ENCRYPT\": \"false\",\n        \"MSSQL_TRUST_SERVER_CERTIFICATE\": \"true\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\n### query\n\nExecute a SQL query on a MSSQL database.\n\n#### Parameters\n\n- `connectionString` (string, optional): Full connection string (alternative to individual parameters)\n- `host` (string, optional): Database server hostname\n- `port` (number, optional): Database server port (default: 1433)\n- `database` (string, optional): Database name (default: master)\n- `username` (string, optional): Database username\n- `password` (string, optional): Database password\n- `query` (string, required): SQL query to execute\n- `encrypt` (boolean, optional): Enable encryption (default: false)\n- `trustServerCertificate` (boolean, optional): Trust server certificate (default: true)\n\nEither `connectionString` OR (`host` + `username` + `password`) must be provided.\n\n#### Example\n\n```typescript\nconst result = await use_mcp_tool({\n  server_name: 'mssql',\n  tool_name: 'query',\n  arguments: {\n    host: 'localhost',\n    username: 'sa',\n    password: 'yourpassword',\n    query: 'SELECT * FROM Users',\n  },\n});\n```\n\n## Running the Server\n\n### Local Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build\nnpm run build\n\n# Run the built server\nnpm start\n```\n\n### Using Docker\n\n```bash\n# Build and start services (SQL Server + MCP server)\ndocker-compose up\n\n# Or just build the Docker image\ndocker build -t mssql-mcp-server .\n```\n\n## Testing\n\n```bash\n# Run tests\nnpm test\n\n# Run tests with coverage\nnpm run test:coverage\n```\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts examples/simple-server.ts\n```\n\n## Security\n\nThe server includes safeguards against dangerous SQL operations:\n\n- Blocks potentially harmful commands like DROP, TRUNCATE, ALTER, CREATE, EXEC, etc.\n- Validates all input parameters and database names\n- Sets reasonable limits on query length and timeout\n- Uses connection pooling for better performance and security\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql",
        "databases",
        "database",
        "databases secure",
        "mssql mcp",
        "secure database"
      ],
      "category": "databases"
    },
    "c4pt0r--mcp-server-tidb": {
      "owner": "c4pt0r",
      "name": "mcp-server-tidb",
      "url": "https://github.com/c4pt0r/mcp-server-tidb",
      "imageUrl": "/freedevtools/mcp/pfp/c4pt0r.webp",
      "description": "Effortlessly connect applications to a serverless TiDB database, enabling management and querying of data through a streamlined interface. Focus on enhancing data-driven projects and simplifying database interactions.",
      "stars": 22,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-14T06:51:32Z",
      "readme_content": "# mcp-server-tidb\n\nMCP server implementation for TiDB (serverless) database.\n\n## Prerequisites\n\n- uv (Python package installer)\n\n## Installation\n\n```\n# Clone the repository\ngit clone https://github.com/c4pt0r/mcp-server-tidb\ncd mcp-server-tidb\n\n# Install the package and dependencies using uv\nuv venv\nuv pip install -e .\n```\n\n## Configuration\n\nGo [tidbcloud.com](https://tidbcloud.com) to create a free TiDB database cluster\n\nConfiguration can be provided through environment variables, or using .env:\n- `TIDB_HOST` - TiDB host address, e.g. 'gateway01.us-east-1.prod.aws.tidbcloud.com'\n- `TIDB_PORT` - TiDB port (default: 4000)\n- `TIDB_USERNAME` - Database username, e.g.  'xxxxxxxxxx.\\<username\\>'\n- `TIDB_PASSWORD` - Database password\n- `TIDB_DATABASE` - Database name, default is test\n\n## Run with Claude Desktop\n\nConfig Claude Desktop, [HOWTO](https://modelcontextprotocol.io/quickstart/user)\n\n`claude_desktop_config.json`:\n\n```\n{\n  \"mcpServers\": {\n      \"tidb\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/path/to/mcp-server-tidb\",\n              \"run\",\n              \"src/main.py\"\n          ]\n      }\n  }\n}\n```\n\n\nIf you're running mcp-server-tidb in WSL, the `claude_desktop_config.json` should look like this:\n\n```\n{\n  \"mcpServers\": {\n    \"tool-with-env-vars\": {\n      \"command\": \"wsl.exe\",\n      \"args\": [\n        \"bash\",\n        \"-c\",\n        \"/path/to/uv --directory /path/to/mcp-server-tidb run python src/main.py\"\n      ]\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "tidb",
        "tidb database",
        "server tidb",
        "serverless tidb"
      ],
      "category": "databases"
    },
    "caicongyang--mcp-demo": {
      "owner": "caicongyang",
      "name": "mcp-demo",
      "url": "https://github.com/caicongyang/mcp-demo",
      "imageUrl": "/freedevtools/mcp/pfp/caicongyang.webp",
      "description": "Enable database interactions through the execution of SQL queries, the creation of tables, and exploration of schema information for efficient data management.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-12T08:25:36Z",
      "readme_content": "## 项目结构\n\n本项目包含三个主要文件夹，分别实现了不同功能：\n\n- **doc**: MCP (Model Context Protocol) 的详细解读文档和说明\n- **mysql**: 使用 stdio 模式实现的 MySQL MCP 服务\n- **weather**: 使用 HTTP SSE 模式实现的天气 MCP 服务示例\n\n## MCP Inspector 界面展示",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "caretdev--mcp-server-iris": {
      "owner": "caretdev",
      "name": "mcp-server-iris",
      "url": "https://github.com/caretdev/mcp-server-iris",
      "imageUrl": "/freedevtools/mcp/pfp/caretdev.webp",
      "description": "Automate and interact with InterSystems IRIS databases, streamlining data manipulation and retrieval processes. Enhance application development by leveraging the Model Context Protocol for efficient database interactions.",
      "stars": 7,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T13:42:01Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/caretdev-mcp-server-iris-badge.png)](https://mseep.ai/app/caretdev-mcp-server-iris)\n\n# mcp-server-iris: An InterSystems IRIS MCP server\n\n<a href=\"https://glama.ai/mcp/servers/@caretdev/mcp-server-iris\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@caretdev/mcp-server-iris/badge\" />\n</a>\n\n# mcp-server-iris: An InterSystems IRIS MCP server\n\n## Overview\n\nA [Model Context Protocol](https://modelcontextprotocol.io/introduction) server for InterSystems IRIS database interaction and automation.\n\n## Configure Claude\n\n- [Claude Desktop](https://claude.ai/download)\n- [uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n```json\n{\n  \"mcpServers\": {\n    \"iris\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-iris\"\n      ],\n      \"env\": {\n        \"IRIS_HOSTNAME\": \"localhost\",\n        \"IRIS_PORT\": \"1972\",\n        \"IRIS_NAMESPACE\": \"USER\",\n        \"IRIS_USERNAME\": \"_SYSTEM\",\n        \"IRIS_PASSWORD\": \"SYS\"\n      }\n    }\n  }\n}\n```\n\n![ClaudeIRISInteroperability](https://github.com/user-attachments/assets/ec5b90e6-1cd3-467a-8875-72a13606a747)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "iris",
        "iris databases",
        "server iris",
        "database access"
      ],
      "category": "databases"
    },
    "centralmind--gateway": {
      "owner": "centralmind",
      "name": "gateway",
      "url": "https://github.com/centralmind/gateway",
      "imageUrl": "/freedevtools/mcp/pfp/centralmind.webp",
      "description": "Universal MCP server tailored for databases, facilitating secure and compliant interactions optimized for large language models and AI agents. It supports a variety of database systems including PostgreSQL, MySQL, and ElasticSearch among others.",
      "stars": 484,
      "forks": 59,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-04T02:28:21Z",
      "readme_content": "<div align=\"center\">\n\n![Build Binaries](https://github.com/centralmind/gateway/actions/workflows/build-binaries.yml/badge.svg) &nbsp; <a href=\"https://discord.gg/XFhaUG4F5x\"><img alt=\"XFhaUG4F5x\" src=\"https://dcbadge.limes.pink/api/server/https://discord.gg/XFhaUG4F5x\" height=\"20\"></a> &nbsp;&nbsp;<a href=\"https://t.me/+TM3T1SikjzA4ZWVi\"><img alt=\"telegram_252850_style_plastic_logo_telegram\" src=\"https://img.shields.io/badge/telegram-%E2%9D%A4%EF%B8%8F-252850?style=plastic&logo=telegram\" height=20></a> &nbsp;&nbsp; <a href=\"https://docs.centralmind.ai\"><img alt=\"Full_Documentation_blue_style_for_the_badge_logo_rocket_logoColor_white\" src=\"https://img.shields.io/badge/Full%20Documentation-blue?style=for-the-badge&logo=rocket&logoColor=white\" height=\"20\"></a>&nbsp;&nbsp; <a href=\"https://cursor.com/install-mcp?name=CentralMind%20Database%20Gateway&config=eyJjb21tYW5kIjoiZG9ja2VyIHJ1biAtaSAtLXBsYXRmb3JtIGxpbnV4L2FtZDY0IGdoY3IuaW8vY2VudHJhbG1pbmQvZ2F0ZXdheTp2MC4yLjE4IC0tY29ubmVjdGlvbi1zdHJpbmcgcG9zdGdyZXNxbDovL215X3VzZXI6bXlfcGFzc0Bsb2NhbGhvc3Q6NTQzMi9teWRiIHN0YXJ0IHN0ZGlvIn0%3D\"><img alt=\"mcp_install_dark\" height=\"21\" src=\"https://cursor.com/deeplink/mcp-install-dark.svg\"></a>\n\n\n</div>\n\n\n<h2 align=\"center\">CentralMind Gateway: Create API or MCP Server in Minutes</h2>\n\n🚀 Interactive Demo avialable here: https://centralmind.ai\n\n## What is Centralmind/Gateway\n\nSimple way to expose your database to AI-Agent via MCP or OpenAPI 3.1 protocols.\n\n```bash\ndocker run --platform linux/amd64 -p 9090:9090 \\\n  ghcr.io/centralmind/gateway:v0.2.18 start \\\n  --connection-string \"postgres://db-user:db-password@db-host/db-name?sslmode=require\"\n```\n\nThis will run for you an API:\n\n```shell\nINFO Gateway server started successfully!         \nINFO MCP SSE server for AI agents is running at: http://localhost:9090/sse \nINFO REST API with Swagger UI is available at: http://localhost:9090/ \n```\n\nWhich you can use inside your AI Agent:\n\n\n\nGateway will generate AI optimized API.\n\n\n## Why Centralmind/Gateway\n\nAI agents and LLM-powered applications need fast, secure access to data. We're building an API layer that automatically generates secure, LLM-optimized APIs for your structured data.\n- Quickly start with MCP or OpenAPI, or use Direct/Raw SQL APIs\n- Filters out PII and sensitive data to ensure compliance with GDPR, CPRA, SOC 2, and other regulations\n- Adds traceability and auditing capabilities, ensuring AI applications aren't black boxes and allowing security teams to maintain control\n- Optimized for AI workloads: supports the Model Context Protocol (MCP) with enhanced metadata to help AI agents understand APIs, along with built-in caching and security features\n\nIt can be useful during development, when an LLM needs to create, adjust, or query data from your database.\nIn analytical scenarios, it enables you to chat with your database or data warehouse.\nEnrich your AI agents with data from your database using remote function/tool calling.\n\n\n\n## Features\n\n- ⚡ **Automatic API Generation** – Creates APIs automatically using LLM based on table schema and sampled data\n- 🗄️ **Structured Database Support** – Supports <a href=\"https://docs.centralmind.ai/connectors/postgres/\">PostgreSQL</a>, <a href=\"https://docs.centralmind.ai/connectors/mysql/\">MySQL</a>, <a href=\"https://docs.centralmind.ai/connectors/clickhouse/\">ClickHouse</a>, <a href=\"https://docs.centralmind.ai/connectors/snowflake/\">Snowflake</a>, <a href=\"https://docs.centralmind.ai/connectors/mssql/\">MSSQL</a>, <a href=\"https://docs.centralmind.ai/connectors/bigquery/\">BigQuery</a>, <a href=\"https://docs.centralmind.ai/connectors/oracle/\">Oracle Database</a>, <a href=\"https://docs.centralmind.ai/connectors/sqlite/\">SQLite</a>, <a href=\"https://docs.centralmind.ai/connectors/sqlite/\">ElasticSearch</a>\n- 🌍 **Multiple Protocol Support** – Provides APIs as REST or MCP Server including SSE mode\n- 🔐 **Authentication Options** – Built-in support for <a href=\"https://docs.centralmind.ai/plugins/api_keys/\">API keys</a> and <a href=\"https://docs.centralmind.ai/plugins/oauth/\">OAuth</a>\n- 🔒 **PII Protection** – Implements <a href=\"https://docs.centralmind.ai/plugins/pii_remover/\">regex plugin</a> or <a href=\"https://docs.centralmind.ai/plugins/presidio_anonymizer/\">Microsoft Presidio plugin</a> for PII and sensitive data redaction\n- 👀 **Comprehensive Monitoring** – Integration with <a href=\"https://docs.centralmind.ai/plugins/otel/\">OpenTelemetry (OTel)</a> for request tracking and audit trails\n- 📦 **Local & On-Premises** – Support for <a href=\"https://docs.centralmind.ai/providers/local-models/\">self-hosted LLMs</a> through configurable AI endpoints and models\n- 🤖 **Multiple AI Providers Support** - Support for [OpenAI](https://docs.centralmind.ai/providers/openai), [Anthropic](https://docs.centralmind.ai/providers/anthropic), [Amazon Bedrock](https://docs.centralmind.ai/providers/bedrock), [Google Gemini](https://docs.centralmind.ai/providers/gemini) & [Google VertexAI](https://docs.centralmind.ai/providers/anthropic-vertexai)\n- ⚡ **Flexible Configuration** – Easily extensible via YAML configuration and plugin system\n- 📜 **API Documentation** – Auto-generated Swagger documentation and OpenAPI 3.1.0 specification\n- 🔑 **Row-Level Security (RLS)** – Fine-grained data access control using <a href=\"https://docs.centralmind.ai/plugins/lua_rls/\">Lua scripts</a>\n- 🏎️ **Performance Optimization** – Implements time-based and <a href=\"https://docs.centralmind.ai/plugins/lru_cache/\">LRU caching</a> strategies\n\n## How it Works\n\n<div align=\"center\">\n\n\n\n</div>\n\n### 1. Connect & Discover\n\nGateway connects to your structured databases like PostgreSQL and automatically analyzes the schema and data samples\nto generate an optimized API structure based on your prompt. LLM is used only on discovery stage to produce API configuration.\nThe tool uses [AI Providers](https://docs.centralmind.ai/providers) to generate the API configuration while ensuring security\nthrough PII detection.\n\n### 2. Deploy\n\nGateway supports multiple deployment options from standalone binary, docker or <a href=\"https://docs.centralmind.ai/example/k8s/\">Kubernetes</a>.\nCheck our <a href=\"https://docs.centralmind.ai/docs/content/getting-started/launching-api/\">launching guide</a> for detailed\ninstructions. The system uses YAML configuration and plugins for easy customization.\n\n### 3. Use & Integrate\n\nAccess your data through REST APIs or Model Context Protocol (MCP) with built-in security features.\nGateway seamlessly integrates with AI models and applications like <a href=\"https://docs.centralmind.ai/docs/content/integration/langchain/\">LangChain</a>,\n<a href=\"https://docs.centralmind.ai/docs/content/integration/chatgpt/\">OpenAI</a> and\n<a href=\"https://docs.centralmind.ai/docs/content/integration/claude-desktop/\">Claude Desktop</a> using function calling\nor <a href=\"https://docs.centralmind.ai/docs/content/integration/cursor/\">Cursor</a> through MCP. You can also <a href=\"https://docs.centralmind.ai/plugins/otel/\">setup telemetry</a> to local or remote destination in otel format.\n\n## Documentation\n\n### Getting Started\n\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/quickstart/\">Quickstart Guide</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/installation/\">Installation Instructions</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/generating-api/\">API Generation Guide</a>\n- <a href=\"https://docs.centralmind.ai/docs/content/getting-started/launching-api/\">API Launch Guide</a>\n\n### Additional Resources\n\n- <a href=\"https://docs.centralmind.ai/docs/content/integration/chatgpt/\">ChatGPT Integration Guide</a>\n- <a href=\"https://docs.centralmind.ai/connectors/\">Database Connector Documentation</a>\n- <a href=\"https://docs.centralmind.ai/plugins/\">Plugin Documentation</a>\n\n## How to Build\n\n```shell\n# Clone the repository\ngit clone https://github.com/centralmind/gateway.git\n\n# Navigate to project directory\ncd gateway\n\n# Install dependencies\ngo mod download\n\n# Build the project\ngo build .\n```\n\n## API Generation\n\nGateway uses LLM models to generate your API configuration. Follow these steps:\n\n\nChoose one of our supported AI providers:\n- [OpenAI](https://docs.centralmind.ai/providers/openai) and all OpenAI-compatible providers\n- [Anthropic](https://docs.centralmind.ai/providers/anthropic)\n- [Amazon Bedrock](https://docs.centralmind.ai/providers/bedrock)\n- [Google Vertex AI (Anthropic)](https://docs.centralmind.ai/providers/anthropic-vertexai)\n- [Google Gemini](https://docs.centralmind.ai/providers/gemini)\n\nGoogle Gemini provides a generous **free tier**. You can obtain an API key by visiting Google AI Studio:\n\n- [Google AI Studio](https://aistudio.google.com/apikey)\n\nOnce logged in, you can create an API key in the API section of AI Studio. The free tier includes a generous monthly token allocation, making it accessible for development and testing purposes.\n\nConfigure AI provider authorization. For Google Gemini, set an API key.\n\n```bash\nexport GEMINI_API_KEY='yourkey'\n```\n\n2. Run the discovery command:\n\n```shell\n./gateway discover \\\n  --ai-provider gemini \\\n  --connection-string \"postgresql://neondb_owner:MY_PASSWORD@MY_HOST.neon.tech/neondb?sslmode=require\" \\\n  --prompt \"Generate for me awesome readonly API\"\n```\n\n3. Enjoy the generation process:\n\n```shell\nINFO 🚀 API Discovery Process\nINFO Step 1: Read configs\nINFO ✅ Step 1 completed. Done.\n\nINFO Step 2: Discover data\nINFO Discovered Tables:\nINFO   - payment_dim: 3 columns, 39 rows\nINFO   - fact_table: 9 columns, 1000000 rows\nINFO ✅ Step 2 completed. Done.\n\n# Additional steps and output...\n\nINFO ✅ All steps completed. Done.\n\nINFO --- Execution Statistics ---\nINFO Total time taken: 1m10s\nINFO Tokens used: 16543 (Estimated cost: $0.0616)\nINFO Tables processed: 6\nINFO API methods created: 18\nINFO Total number of columns with PII data: 2\n```\n\n4. Review the generated configuration in `gateway.yaml`:\n\n```yaml\napi:\n  name: Awesome Readonly API\n  description: ''\n  version: '1.0'\ndatabase:\n  type: postgres\n  connection: YOUR_CONNECTION_INFO\n  tables:\n    - name: payment_dim\n      columns: # Table columns\n      endpoints:\n        - http_method: GET\n          http_path: /some_path\n          mcp_method: some_method\n          summary: Some readable summary\n          description: 'Some description'\n          query: SQL Query with params\n          params: # Query parameters\n```\n\n## Running the API\n\n### Run locally\n\n```shell\n./gateway start --config gateway.yaml\n```\n\n### Docker Compose\n\n```shell\ndocker compose -f ./example/simple/docker-compose.yml up\n```\n\n### MCP Protocol Integration\n\nGateway implements the MCP protocol for seamless integration with Claude and other tools. For detailed setup instructions, see our <a href=\"https://docs.centralmind.ai/docs/content/integration/claude-desktop/\">Claude integration guide</a>.\n\nTo add MCP Tool to Claude Desktop just adjust Claude's config :\n\n```json\n{\n  \"mcpServers\": {\n    \"gateway\": {\n      \"command\": \"PATH_TO_GATEWAY_BINARY\",\n      \"args\": [\"start\", \"--config\", \"PATH_TO_GATEWAY_YAML_CONFIG\", \"mcp-stdio\"]\n    }\n  }\n}\n```\n\n## Roadmap\n\nIt is always subject to change, and the roadmap will highly depend on user feedback. At this moment,\nwe are planning the following features:\n\n#### Database and Connectivity\n- 🗄️ **Extended Database Integrations** - Databricks, Redshift, S3 (Iceberg and Parquet), Oracle DB, Microsoft SQL Server, Elasticsearch\n- 🔑 **SSH tunneling** - ability to use jumphost or ssh bastion to tunnel connections\n\n#### Enhanced Functionality\n- 🔍 **Advanced Query Capabilities** - Complex filtering syntax and Aggregation functions as parameters\n- 🔐 **Enhanced MCP Security** - API key and OAuth authentication\n\n#### Platform Improvements\n- 📦 **Schema Management** - Automated schema evolution and API versioning\n- 🚦 **Advanced Traffic Management** - Intelligent rate limiting, Request throttling\n- ✍️ **Write Operations Support** - Insert, Update operations\n\n## Database Gateway in MCP Registries  \n- https://mcpreview.com/mcp-servers/centralmind/gateway\n- https://mcp.so/server/gateway/centralmind\n- https://smithery.ai/server/@centralmind/gateway\n- https://www.pulsemcp.com/servers/centralmind-database-gateway",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "centralmind",
        "databases",
        "database",
        "centralmind gateway",
        "access centralmind",
        "secure database"
      ],
      "category": "databases"
    },
    "chenxiaohui--ydb": {
      "owner": "chenxiaohui",
      "name": "ydb",
      "url": "https://github.com/chenxiaohui/ydb",
      "imageUrl": "/freedevtools/mcp/pfp/chenxiaohui.webp",
      "description": "Distributed SQL database offering high availability and scalability with support for ACID transactions and strict consistency. Suitable for building interactive web services with automatic disaster recovery and horizontal scalability to accommodate varying workloads.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2022-04-21T08:31:55Z",
      "readme_content": "<br/>\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ydb-platform/ydb/blob/main/LICENSE)\n[![PyPI version](https://badge.fury.io/py/ydb.svg)](https://badge.fury.io/py/ydb)\n[![Telegram](https://img.shields.io/badge/chat-on%20Telegram-2ba2d9.svg)](https://t.me/YDBPlatform)\n\n## YDB\n\n[Website](https://ydb.tech) |\n[Documentation](https://ydb.tech/docs) |\n[Official Repository](https://github.com/ydb-platform/ydb) |\n[YouTube Channel](https://www.youtube.com/channel/UCHrVUvA1cRakxRP3iwA-yyw)\n\nYDB is an open-source Distributed SQL Database that combines high availability and scalability with strict consistency and ACID transactions.\n\n[](https://youtu.be/bxZRUtMAlFI)\n\n## Main YDB Advantages\n\nYDB is designed from scratch as a response to growing demand for scalable interactive web services. Scalability, strict consistency and effective cross-row transactions were a must for such OLTP-like workload. YDB is built by people with strong background in databases and distributed systems, who had an experience of developing No-SQL database and the Map-Reduce system for one of the largest search engines in the world.\nWe found that YDB's flexible design allows us to build more services on top of it including persistent queues and virtual block devices.\n\nBasic YDB features:\n\n  - Fault-tolerant configuration that survive disk, node, rack or even datacenter outage;\n  - Horizontal scalability;\n  - Automatic disaster recovery with minimum latency disruptions for applications;\n  - SQL dialect (YQL) for data manipulation and scheme definition;\n  - ACID transactions across multiple nodes and tables with strict consistency.\n\n### Fault-tolerant Configurations\n\nYDB could be deployed in three availability zones. Cluster remains available for both reads and writes during complete outage of a single zone. Availability zones and regions are covered in more detail [in documentation](https://ydb.tech/en/docs/concepts/databases#regions-az).\n\n### Horizontal Scalability\n\nUnlike traditional relational databases YDB [scales out](https://en.wikipedia.org/wiki/Scalability#Horizontal_or_scale_out) providing developers with capability to simply extend cluster with computation or storage resources to handle increasing load. YDB has desaggregated storage and compute layers which allow you to scale storage and compute resources independently.\n\nCurrent production installations have more than 10,000 nodes, store petabytes of data and handle millions distributed transactions per second.\n\n### Automatic Disaster Recovery\n\nYDB has built-in automatic recovery support to survive a hardware failure. After unpredictable disk, node, rack or even datacenter failure YDB remains fully available for reads and writes and restores required data redundancy automatically.\n\n### Multitenant and Serverless Database\nYDB has support for multitenant and serverless setups. A user can run a YDB cluster and create several databases that share one pool of storage and have different compute nodes. Alternatively a user can run several serverless databases that share one pool of compute resources to utilize them effectively.\n\n## Supported Platforms\n\n### Minimal system requirements\n\nYDB runs on x86 64bit platforms with minimum 8 GB of RAM.\n\n### Operating Systems\n\nWe have major experience running production systems on 64-bit x86 machines working under Ubuntu Linux.\n\nFor development purposes we test that YDB could be built and run under latest versions of MacOS and Microsoft Windows on a regular basis.\n\n## Getting Started\n\n1. Install YDB using [pre-built executables](https://ydb.tech/en/docs/getting_started/self_hosted/ydb_local), [build it from source](BUILD.md) or [use Docker container](https://ydb.tech/en/docs/getting_started/self_hosted/ydb_docker).\n1. Install [command line interace](https://ydb.tech/en/docs/getting_started/cli) tool to work with scheme and run queries.\n1. Start [local cluster](https://ydb.tech/en/docs/getting_started/self_hosted/ydb_local) or container and run [YQL query](https://ydb.tech/en/docs/yql/reference/) via [YDB CLI](https://ydb.tech/en/docs/getting_started/cli).\n1. Access [Embedded UI](https://ydb.tech/en/docs/maintenance/embedded_monitoring/) via browser for schema navigation, query execution and other database development related tasks.\n1. Run available [example application](https://ydb.tech/en/docs/reference/ydb-sdk/example/go/).\n1. Develop an application using [YDB SDK](https://ydb.tech/en/docs/reference/ydb-sdk/).\n\n\n## How to Build from Source Code\n* Build server (ydbd) and client (ydb) binaries [from source code](BUILD.md).\n\n## How to Deploy\n\n* Deploy a cluster [using Kubernetes](https://ydb.tech/en/docs/deploy/orchestrated/concepts).\n* Deploy a cluster using [pre-built executables](https://ydb.tech/en/docs/getting_started/self_hosted/ydb_local).\n\n## How to Contribute\n\nWe are glad to welcome new contributors!\n\n1. Please read [contributor's guide](CONTRIBUTING).\n2. We can accept your work to YDB after you have read contributor's license agreement (aka CLA).\n3. Please don't forget to add a note to your pull request, that you agree to the terms of the CLA.\n\nMore information can be found in [CONTRIBUTING](CONTRIBUTING) file.\n\n## Success Stories\n\nTake a look at YDB [web site](https://ydb.tech/) for the latest success stories and user scenarios.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "sql",
        "secure database",
        "databases secure",
        "distributed sql"
      ],
      "category": "databases"
    },
    "christian561--gel-mcp-server": {
      "owner": "christian561",
      "name": "gel-mcp-server",
      "url": "https://github.com/christian561/gel-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/christian561.webp",
      "description": "Automate schema learning, query validation, and execution for Gel databases using natural language interactions. Integrates with EdgeQL for efficient database management and facilitates seamless connections with LLMs to enhance user interactions.",
      "stars": 11,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T15:13:34Z",
      "readme_content": "# Gel Database MCP Server \n\nA TypeScript-based Model Context Protocol (MCP) server designed to streamline Gel database operations with EdgeQL queries. This project provides Tools for LLM Agents (Cursor Agent, Claude Code, etc) to automate learning about your schema, and writing, validating, and executing database queries. Easily interact with your Gel database through natural language. Vibe coders rejoice! \n\n\nNote: Query generation is not included since LLMs can write more flexible queries. Tested this with Cursor agent using Claude-3.7-sonnet-thinking and had good results after providing Gel docs by linking the relevant webpages. \n\n\n\n## Quick Start Guide\n\n```bash\n# 1. Install dependencies\nyarn install\n\n# 2. Copy your dbschema folder into the project if you have one already \n# cp -r /path/to/your/dbschema ./\n# or just copy and paste\n\n# 3. Initialize a Gel project\nnpx gel project init\n# Follow prompts to set up a new project \n# Can point to an existing gel instance by providing the name of your instance\n#   -Import migrations if it asks\n\n# 4. Generate EdgeQL JavaScript query builder files\nnpx @gel/generate edgeql-js\n# Note: Re-run this command after any schema changes\n\n# 5. Update connection settings\n# Edit src/index_gel.ts lines 19-25 with your database, host, port, user, password\n# Edit src/index_gel.ts line 37 with your branch name\n\n# 6. Build the project\nyarn build\n\n# 7. (optional) Test the server runs without errors\nnode build/index.js\n\n# 7.1 (if you have errors) Test server with a UI that provides more clear error logs using: \nnpx @modelcontextprotocol/inspector node build/index.js\n\n# 8. (Recommended) Include the gel_llm.txt documentation file\n# Download the Gel documentation file and place it in your project root\n# This allows both the search tool and direct file access for your LLM agent\n# curl -o gel_llm.txt https://raw.githubusercontent.com/yourorg/gel-docs/main/gel_llm.txt\n# Note: Replace the URL with the actual source of your gel_llm.txt file\n```\n# Connect MCP Server in Cursor\n1. Click on the gear icon on the top right > MCP > +Add a new server\n2. Name it whatever you want\n3. Select type: Command\n4. Enter this: node your/full/path/to/build/index.js\n\n\n\n**Note:** While this server has been primarily tested with Cursor's agent, it should work with other agents and LLMs that support the Model Context Protocol. If you test with other agents, please feel free to contribute your findings!\n\n\n## Available Tools\n\nThe Gel Database MCP Server provides the following tools:\n\n### describe-schema\nThis helps your LLM agent learn and understand your database structure without having to manually inspect the code. The agent can discover available entity types, their properties, relationships, and constraints to generate more accurate queries.\n\n**When to use:** When your agent needs to understand the structure of a database entity before querying it.\n![image](https://github.com/user-attachments/assets/e48b0da7-cd95-4416-820a-2a5c870c8e73)\n\n### validate-query\nThis helps your LLM agent verify raw EdgeQL query syntax without executing it, allowing safe validation of generated queries before they're run against your database.\n\n**When to use:** During query development to check syntax without risking execution side effects.\n![image](https://github.com/user-attachments/assets/1d54c8a5-6f5c-4f7c-904c-93f664e23718)\n\n### execute-edgeql\nThis helps your LLM agent directly interact with your database by running raw EdgeQL queries, retrieving data, and performing operations based on your instructions. Your LLM can generate EdgeQL queries and execute them autonomously.\n\n**Example:**\n```edgeql\nSELECT Product { name, price } FILTER .price > 100;\n```\n![image](https://github.com/user-attachments/assets/79bbabab-aa3e-42e8-bd9f-92ba03cd18c0)\n\n### search-gel-docs\nThis tool allows your LLM agent to search through the Gel documentation to find relevant information about EdgeQL syntax, features, or examples. It returns comprehensive results with context to help the agent better understand Gel database concepts.\n\n**When to use:** When your agent needs to learn about specific Gel/EdgeQL features, understand syntax, or find examples for implementing database operations.\n\n**Example:**\n```\nsearch_term: \"for loop\"\ncontext_lines: 10  # Optional: Number of context lines to show (default: 5)\nmatch_all_terms: true  # Optional: Require all terms to match (default: false)\n```\n\n**Note on Documentation Hybrid Approach:** For optimal results, we recommend both:\n1. Including the `gel_llm.txt` file in your project root (for direct file access)\n2. Using the search-gel-docs tool for targeted queries\n\nThis hybrid approach gives your LLM agent the flexibility to search for specific terms while also accessing the complete documentation when needed for broader context.\n\n### execute-typescript\nSimilar to execute-edgeql but can use this for testing and running Typescript Gel queries made with the query builder syntax. \n\nInstructions are included in the tool, but still a good idea to ask the agent what instructions it has so it loads them up in context. This makes sure it doesn't skip them. \n\nNote: General JavaScript syntax errors can crash the server, so if the connection is appearing as closed you will have to refresh the crashed server in Cursor MCP settings or restart the server. \n\n**Tell the LLM these are the Best practices:**\n- Use `await gelClient.query()` with console.log to display results\n- Use ORDER BY with THEN, not commas (e.g., ORDER BY .field1 THEN .field2)\n- Keep code simple and focused on a single operation\n\n**Example:**\n```typescript\nconsole.log(await gelClient.query(`\n  SELECT Product { \n    name, \n    price \n  } \n  FILTER .price > 100 \n  ORDER BY .price DESC \n  LIMIT 5;\n`));\n```\n\n**When to use:** For complex queries that require programmatic logic or when you need to process query results with JavaScript.\n\n![image](https://github.com/user-attachments/assets/aed79dc8-d2ba-45d5-830b-1d73c04a5614)\n\n## Learn More\n\nFor more information about the Model Context Protocol, visit [modelcontextprotocol.io/quickstart](https://modelcontextprotocol.io/quickstart).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "edgeql",
        "secure database",
        "gel databases",
        "databases secure"
      ],
      "category": "databases"
    },
    "cnosdb--cnosdb-mcp-server": {
      "owner": "cnosdb",
      "name": "cnosdb-mcp-server",
      "url": "https://github.com/cnosdb/cnosdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cnosdb.webp",
      "description": "Execute SQL queries, manage CnosDB databases, and inspect table schemas through an MCP interface. Streamline database interactions with tools for listing databases and tables.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-10T09:33:22Z",
      "readme_content": "# CnosDB MCP Server\n\n[![Python 3.8](https://img.shields.io/badge/python-3.12-blue?logo=python&logoColor=white)](https://docs.python.org/3.12/)\n[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)\n\nAn MCP server for CnosDB.\n\n## Features\n\n - query\n\n    Execute query (automatically identifies SQL) \n\n - list_databases\n\n    List all databases\n\n - list_tables\n\n    List tables in database\n\n - describe_table\n\n    Display table schema for [table_name]\n\n\n## Development\n\n```shell\n# Clone the repository\ngit clone https://github.com/cnosdb/cnosdb-mcp-server.git\ncd cnosdb-mcp-server\n\n# Create virtual environment\nuv .venv\nsource .venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install development dependencies\npip install -r requirements.txt\n```\n\n\n### Configuration\n\n> For alternative MCP clients, see: https://github.com/punkpeye/awesome-mcp-clients\n\n1. Open the Claude Desktop configuration file located at:\n\n   - On macOS: ~/Library/Application Support/Claude/claude_desktop_config.json\n\n   - On Windows: %APPDATA%/Claude/claude_desktop_config.json\n\n2. Add the following:\n\n```json\n{\n  \"name\": \"CnosDB\",\n  \"key\": \"CnosDBMCPServer\",\n  \"command\": \"uv\",\n  \"args\": [\n    \"--directory\",\n    \"REPO_PATH/cnosdb-mcp-server\",\n    \"run\",\n    \"server.py\"\n  ],\n  \"env\": {\n    \"CNOSDB_HOST\": \"127.0.0.1\",\n    \"CNOSDB_PORT\": \"8902\",\n    \"CNOSDB_USERNAME\": \"root\",\n    \"CNOSDB_PASSWORD\": \"CnosDB#!\"\n  }\n}\n```\nUpdate the environment variables to point to your own CnosDB service.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cnosdb",
        "databases",
        "database",
        "cnosdb databases",
        "access cnosdb",
        "cnosdb mcp"
      ],
      "category": "databases"
    },
    "crate--cratedb-mcp": {
      "owner": "crate",
      "name": "cratedb-mcp",
      "url": "https://github.com/crate/cratedb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/crate.webp",
      "description": "Enables LLMs to interact directly with CrateDB for optimized query handling and data insights, facilitating the ability to answer questions about data and assist with debugging and optimizing queries.",
      "stars": 8,
      "forks": 2,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-16T16:55:51Z",
      "readme_content": "# CrateDB MCP Server\n\n[![Status][badge-status]][project-pypi]\n[![CI][badge-ci]][project-ci]\n[![Coverage][badge-coverage]][project-coverage]\n[![Downloads per month][badge-downloads-per-month]][project-downloads]\n\n[![License][badge-license]][project-license]\n[![Release Notes][badge-release-notes]][project-release-notes]\n[![PyPI Version][badge-package-version]][project-pypi]\n[![Python Versions][badge-python-versions]][project-pypi]\n\n» [Documentation]\n| [Releases]\n| [Issues]\n| [Source code]\n| [License]\n| [CrateDB]\n| [Community Forum]\n| [Bluesky]\n\n## About\n\nThe CrateDB MCP Server for natural-language Text-to-SQL and documentation\nretrieval specializes in CrateDB database clusters.\n\nThe Model Context Protocol ([MCP]) is a protocol that standardizes providing\ncontext to language models and AI assistants.\n\n### Introduction\n\nThe CrateDB Model Context Protocol (MCP) Server connects AI assistants directly\nto your CrateDB clusters and the CrateDB knowledge base, enabling seamless\ninteraction through natural language.\n\nIt serves as a bridge between AI tools and your analytics database,\nallowing you to analyze data, the cluster state, troubleshoot issues, and\nperform operations using conversational prompts.\n\n**Experimental:** Please note that the CrateDB MCP Server is an experimental\nfeature provided as-is without warranty or support guarantees. Enterprise\ncustomers should use this feature at their own discretion.\n\n### Quickstart Guide\n\nThe CrateDB MCP Server is compatible with AI assistants that support the Model\nContext Protocol (MCP), either using standard input/output (`stdio`),\nserver-sent events (`sse`), or HTTP Streams (`http`, earlier `streamable-http`).\n\nTo use the MCP server, you need a [client that supports][MCP clients] the\nprotocol. The most notable ones are ChatGPT, Claude, Cline Bot, Cursor,\nGitHub Copilot, Mistral AI, OpenAI Agents SDK, Windsurf, and others.\n\nThe `uvx` launcher command is provided by the [uv] package manager.\nThe [installation docs](#install) section includes guidelines on how to\ninstall it on your machine.\n\n#### Claude, Cline, Cursor, Roo Code, Windsurf\nAdd the following configuration to your AI assistant's settings to enable the\nCrateDB MCP Server.\n- Claude: [`claude_desktop_config.json`](https://modelcontextprotocol.io/quickstart/user)\n- Cline: [`cline_mcp_settings.json`](https://docs.cline.bot/mcp/configuring-mcp-servers)\n- Cursor: [`~/.cursor/mcp.json` or `.cursor/mcp.json`](https://docs.cursor.com/context/model-context-protocol)\n- Roo Code: [`mcp_settings.json` or `.roo/mcp.json`](https://docs.roocode.com/features/mcp/using-mcp-in-roo/)\n- Windsurf: [`~/.codeium/windsurf/mcp_config.json`](https://docs.windsurf.com/windsurf/cascade/mcp)\n```json\n{\n  \"mcpServers\": {\n    \"cratedb-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"cratedb-mcp\", \"serve\"],\n      \"env\": {\n        \"CRATEDB_CLUSTER_URL\": \"http://localhost:4200/\",\n        \"CRATEDB_MCP_TRANSPORT\": \"stdio\"\n      },\n      \"alwaysAllow\": [\n        \"get_cluster_health\",\n        \"get_table_metadata\",\n        \"query_sql\",\n        \"get_cratedb_documentation_index\",\n        \"fetch_cratedb_docs\"\n      ],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n#### VS Code\n[Add an MCP server to your VS Code user settings] to enable the MCP server\nacross all workspaces in your `settings.json` file.\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"cratedb-mcp\": {\n        \"command\": \"uvx\",\n        \"args\": [\"cratedb-mcp\", \"serve\"],\n        \"env\": {\n          \"CRATEDB_CLUSTER_URL\": \"http://localhost:4200/\",\n          \"CRATEDB_MCP_TRANSPORT\": \"stdio\"\n        }\n      }\n    }\n  },\n  \"chat.mcp.enabled\": true\n}\n```\n[Add an MCP server to your VS Code workspace] to configure an MCP server for a\nspecific workspace per `.vscode/mcp.json` file. In this case, omit the\ntop-level `mcp` element, and start from `servers` instead.\n\nAlternatively, VS Code can automatically detect and reuse MCP servers that\nyou defined in other tools, such as Claude Desktop.\nSee also [Automatic discovery of MCP servers].\n```json\n{\n  \"chat.mcp.discovery.enabled\": true\n}\n```\n\n#### Goose\nConfigure `extensions` in your `~/.config/goose/config.yaml`.\nSee also [using Goose extensions].\n```yaml\nextensions:\n  cratedb-mcp:\n    name: CrateDB MCP\n    type: stdio\n    cmd: uvx\n    args:\n      - cratedb-mcp\n      - serve\n    enabled: true\n    envs:\n      CRATEDB_CLUSTER_URL: \"http://localhost:4200/\"\n      CRATEDB_MCP_TRANSPORT: \"stdio\"\n    timeout: 300\n```\n\n#### LibreChat\nConfigure `mcpServers` in your `librechat.yaml`.\nSee also [LibreChat and MCP] and [LibreChat MCP examples].\n```yaml\nmcpServers:\n  cratedb-mcp:\n    type: stdio\n    command: uvx\n    args:\n      - cratedb-mcp\n      - serve\n    env:\n      CRATEDB_CLUSTER_URL: \"http://localhost:4200/\"\n      CRATEDB_MCP_TRANSPORT: \"stdio\"\n```\n\n#### OCI\nIf you prefer to deploy the MCP server using Docker or Podman, your command/args\nconfiguration snippet may look like this.\n```json\n{\n  \"mcpServers\": {\n    \"cratedb-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"CRATEDB_CLUSTER_URL\",\n        \"ghcr.io/crate/cratedb-mcp:latest\"\n      ],\n      \"env\": {\n        \"CRATEDB_CLUSTER_URL\": \"http://cratedb.example.org:4200/\",\n        \"CRATEDB_MCP_TRANSPORT\": \"stdio\"\n      }\n    }\n  }\n}\n```\n\n## Handbook\n\nThis section includes detailed information about how to configure and\noperate the CrateDB MCP Server, and to learn about the [MCP tools] it\nprovides.\n\nTools are a powerful primitive in the Model Context Protocol (MCP) that enable\nservers to expose executable functionality to clients. Through tools, LLMs can\ninteract with external systems, perform computations, and take actions in the\nreal world.\n\n### What's inside\n\nThe CrateDB MCP Server provides two families of tools.\n\nThe **Text-to-SQL tools** talk to a CrateDB database cluster to inquire database\nand table metadata, and table content.\n<br>\nTool names are: `query_sql`, `get_table_columns`, `get_table_metadata`\n\nThe **documentation server tools** looks up guidelines specific to CrateDB topics,\nto provide the most accurate information possible.\nRelevant information is pulled from <https://cratedb.com/docs>, curated per\n[cratedb-outline.yaml] through the [cratedb-about] package.\n<br>\nTool names are: `get_cratedb_documentation_index`, `fetch_cratedb_docs`\n\nHealth inquiry tool: `get_cluster_health`\n\n### Install package\n\nThe configuration snippets for AI assistants are using the `uvx` launcher\nof the [uv] package manager to start the application after installing it,\nlike the `npx` launcher is doing it for JavaScript and TypeScript applications.\nThis section uses `uv tool install` to install the application persistently.\n\n```shell\nuv tool install --upgrade cratedb-mcp\n```\nNotes:\n- We recommend using the [uv] package manager to install the `cratedb-mcp`\n  package, like many other MCP servers are doing it.\n  ```shell\n  {apt,brew,pipx,zypper} install uv\n  ```\n- We recommend using `uv tool install` to install the program \"user\"-wide\n  into your environment so you can invoke it from anywhere across your terminal\n  sessions or MCP client programs / AI assistants.\n- If you are unable to use `uv tool install`, you can use `uvx cratedb-mcp`\n  to acquire the package and run the application ephemerally.\n\n### Install OCI\n\nOCI images for Docker or Podman are available on GHCR per [CrateDB MCP server OCI images].\nThere is a standard OCI image and an MCPO image suitable for Open WebUI.\n\n- `ghcr.io/crate/cratedb-mcp`\n\n  See also [Docker Hub MCP Server] and [mcp hub].\n\n- `ghcr.io/crate/cratedb-mcpo`\n\n  For integrating Open WebUI, the project provides an OCI MCPO image which wraps\n  the MCP server using the `mcpo` proxy. See also [MCP support for Open WebUI] and\n  [MCP-to-OpenAPI proxy server (mcpo)].\n\nProbe invocation:\n```shell\ndocker run --rm -it --entrypoint=\"\" ghcr.io/crate/cratedb-mcp cratedb-mcp --version\n```\n\n### Configure database connectivity\n\nConfigure the `CRATEDB_CLUSTER_URL` environment variable to match your CrateDB instance.\nFor example, when connecting to CrateDB Cloud, use a value like\n`https://admin:dZ...6LqB@testdrive.eks1.eu-west-1.aws.cratedb.net:4200/`.\nWhen connecting to CrateDB on localhost, use `http://localhost:4200/`.\n```shell\nexport CRATEDB_CLUSTER_URL=\"https://<username>:<password>@<example>.aks1.westeurope.azure.cratedb.net:4200\"\n```\n```shell\nexport CRATEDB_CLUSTER_URL=\"http://crate:crate@localhost:4200/\"\n```\n\nThe `CRATEDB_MCP_HTTP_TIMEOUT` environment variable (default: 30.0) defines\nthe timeout for HTTP requests to CrateDB and its documentation resources\nin seconds.\n\nThe `CRATEDB_MCP_DOCS_CACHE_TTL` environment variable (default: 3600) defines\nthe cache lifetime for documentation resources in seconds.\n\n### Configure transport\n\nMCP servers can be started using different transport modes. The default transport\nis `stdio`, you can select another one of `{\"stdio\", \"http\", \"sse\", \"streamable-http\"}`\nand supply it to the invocation like this:\n```shell\ncratedb-mcp serve --transport=stdio\n```\nNB: The `http` transport was called `streamable-http` in earlier spec iterations.\n\nWhen using any of the HTTP-based options for serving the MCP interface, you can\nuse the CLI options `--host`, `--port` and `--path` to specify the listening address.\nThe default values are `localhost:8000`, where the SSE server responds to `/sse/`\nand `/messages/` and the HTTP server responds to `/mcp/` by default.\n\nAlternatively, you can use environment variables instead of CLI options.\n```shell\nexport CRATEDB_MCP_TRANSPORT=http\nexport CRATEDB_MCP_HOST=0.0.0.0\nexport CRATEDB_MCP_PORT=8000\n```\n```shell\nexport CRATEDB_MCP_PATH=/path/in/url\n```\n\n### Security considerations\n\nIf you want to prevent agents from modifying data, i.e., permit `SELECT` statements\nonly, it is recommended to [create a read-only database user by using \"GRANT DQL\"].\n```sql\nCREATE USER \"read-only\" WITH (password = 'YOUR_PASSWORD');\nGRANT DQL TO \"read-only\";\n```\nThen, include relevant access credentials in the cluster URL.\n```shell\nexport CRATEDB_CLUSTER_URL=\"https://read-only:YOUR_PASSWORD@example.aks1.westeurope.azure.cratedb.net:4200\"\n```\nThe MCP Server also prohibits non-SELECT statements on the application level.\nAll other operations will raise a `PermissionError` exception, unless the\n`CRATEDB_MCP_PERMIT_ALL_STATEMENTS` environment variable is set to a\ntruthy value.\n\n### System prompt customizations\n\nThe CrateDB MCP server allows users to adjust the system prompt by either\nredefining the baseline instructions or extending them with custom conventions.\nAdditional conventions can capture domain-specific details—such as information\nrequired for particular ER data models —- or any other guidelines you develop\nover time.\n\nIf you want to **add** custom conventions to the system prompt,\nuse the `--conventions` option.\n```shell\ncratedb-mcp serve --conventions=\"conventions-custom.md\"\n```\n\nIf you want to **replace** the standard built-in instructions prompt completely,\nuse the `--instructions` option.\n```shell\ncratedb-mcp serve --instructions=\"instructions-custom.md\"\n```\n\nAlternatively, use the `CRATEDB_MCP_INSTRUCTIONS` and `CRATEDB_MCP_CONVENTIONS`\nenvironment variables instead of the CLI options.\n\nTo retrieve the standard system prompt, use the `show-prompt` subcommand. By\nredirecting the output to a file, you can subsequently edit its contents and\nreuse it with the MCP server using the command outlined above.\n```shell\ncratedb-mcp show-prompt > instructions-custom.md\n```\n\nInstruction and convention fragments can be loaded from the following sources:\n\n- HTTP(S) URLs\n- Local file paths\n- Standard input (when fragment is \"-\")\n- Direct string content\n\nBecause LLMs understand Markdown well, you should also use it for writing\npersonal instructions or conventions.\n\n### Operate standalone\n\nStart MCP server with `stdio` transport (default).\n```shell\ncratedb-mcp serve --transport=stdio\n```\nStart MCP server with `sse` transport.\n```shell\ncratedb-mcp serve --transport=sse\n```\nStart MCP server with `http` transport (ex. `streamable-http`).\n```shell\ncratedb-mcp serve --transport=http\n```\nAlternatively, use the `CRATEDB_MCP_TRANSPORT` environment variable instead of\nthe `--transport` option.\n\n### Operate OCI Standard\n\nRun CrateDB database.\n```shell\ndocker network create demo\n```\n```shell\ndocker run --rm --name=cratedb --network=demo \\\n  -p 4200:4200 -p 5432:5432 \\\n  -e CRATE_HEAP_SIZE=2g \\\n  crate:latest -Cdiscovery.type=single-node\n```\n\nConfigure and run CrateDB MCP server.\n```shell\nexport CRATEDB_MCP_TRANSPORT=streamable-http\nexport CRATEDB_MCP_HOST=0.0.0.0\nexport CRATEDB_MCP_PORT=8000\nexport CRATEDB_CLUSTER_URL=http://crate:crate@cratedb:4200/\n```\n```shell\ndocker run --rm --name=cratedb-mcp --network=demo \\\n  -p 8000:8000 \\\n  -e CRATEDB_MCP_TRANSPORT -e CRATEDB_MCP_HOST -e CRATEDB_MCP_PORT -e CRATEDB_CLUSTER_URL \\\n  ghcr.io/crate/cratedb-mcp\n```\n\n### Operate OCI MCPO\nInvoke the CrateDB MCPO server for Open WebUI.\n```shell\ndocker run --rm --name=cratedb-mcpo --network=demo \\\n  -p 8000:8000 \\\n  -e CRATEDB_CLUSTER_URL ghcr.io/crate/cratedb-mcpo\n```\n\n### Operate OCI on GHA\nIf you need instances of CrateDB and CrateDB MCP on a CI environment on GitHub Actions,\nusing this section might be handy, as it includes all relevant configuration options\nin one go.\n```yaml\nservices:\n  cratedb:\n    image: crate/crate:latest\n    ports:\n      - 4200:4200\n      - 5432:5432\n    env:\n      CRATE_HEAP_SIZE: 2g\n  cratedb-mcp:\n    image: ghcr.io/crate/cratedb-mcp:latest\n    ports:\n      - 8000:8000\n    env:\n      CRATEDB_MCP_TRANSPORT: streamable-http\n      CRATEDB_MCP_HOST: 0.0.0.0\n      CRATEDB_MCP_PORT: 8000\n      CRATEDB_CLUSTER_URL: http://crate:crate@cratedb:4200/\n```\n\n### Use\n\nTo connect to the MCP server using any of the available [MCP clients], use one\nof the AI assistant applications, or refer to the programs in the [examples folder].\n\nWe collected a few example questions that have been tested and validated by\nthe team, so you may also want to try them to get started. Please remember\nthat LLMs can still hallucinate and give incorrect answers.\n\n- Optimize this query: \"SELECT * FROM movies WHERE release_date > '2012-12-1' AND revenue\"\n- Tell me about the health of the cluster\n- What is the storage consumption of my tables, give it in a graph.\n- How can I format a timestamp column to '2019 Jan 21'?\n\nPlease also explore the [example questions] from another shared collection.\n\n\n## Project information\n\n### Acknowledgements\nKudos to the authors of all the many software components and technologies\nthis project is building upon.\n\n### Contributing\nThe `cratedb-mcp` package is an open-source project, and is [managed on\nGitHub]. Contributions of any kind are welcome and appreciated.\nTo learn how to set up a development sandbox, please refer to the\n[development documentation].\n\n### Status\nThe software is in the alpha stage, so breaking changes may happen.\nVersion pinning is strongly recommended, especially if you use it as a library.\n\n\n[Add an MCP server to your VS Code user settings]: https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server-to-your-user-settings\n[Add an MCP server to your VS Code workspace]: https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server-to-your-workspace\n[Automatic discovery of MCP servers]: https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_automatic-discovery-of-mcp-servers\n[CrateDB]: https://cratedb.com/database\n[CrateDB MCP server OCI images]: https://github.com/orgs/crate/packages?repo_name=cratedb-mcp\n[cratedb-about]: https://pypi.org/project/cratedb-about/\n[cratedb-outline.yaml]: https://github.com/crate/about/blob/v0.0.4/src/cratedb_about/outline/cratedb-outline.yaml\n[create a read-only database user by using \"GRANT DQL\"]: https://community.cratedb.com/t/create-read-only-database-user-by-using-grant-dql/2031\n[development documentation]: https://github.com/crate/cratedb-mcp/blob/main/DEVELOP.md\n[Docker Hub MCP Server]: https://www.docker.com/blog/introducing-docker-hub-mcp-server/\n[example questions]: https://github.com/crate/about/blob/v0.0.4/src/cratedb_about/query/model.py#L17-L44\n[examples folder]: https://github.com/crate/cratedb-mcp/tree/main/examples\n[LibreChat and MCP]: https://www.librechat.ai/docs/features/agents#model-context-protocol-mcp\n[LibreChat MCP examples]: https://www.librechat.ai/docs/configuration/librechat_yaml/object_structure/mcp_servers\n[MCP]: https://modelcontextprotocol.io/introduction\n[MCP clients]: https://modelcontextprotocol.io/clients\n[mcp hub]: https://hub.docker.com/mcp\n[MCP support for Open WebUI]: https://docs.openwebui.com/openapi-servers/mcp/\n[MCP-to-OpenAPI proxy server (mcpo)]: https://github.com/open-webui/mcpo\n[MCP tools]: https://modelcontextprotocol.io/docs/concepts/tools\n[using Goose extensions]: https://block.github.io/goose/docs/getting-started/using-extensions/\n[uv]: https://docs.astral.sh/uv/\n\n[Bluesky]: https://bsky.app/search?q=cratedb\n[Community Forum]: https://community.cratedb.com/\n[Documentation]: https://github.com/crate/cratedb-mcp\n[Issues]: https://github.com/crate/cratedb-mcp/issues\n[License]: https://github.com/crate/cratedb-mcp/blob/main/LICENSE\n[managed on GitHub]: https://github.com/crate/cratedb-mcp\n[Source code]: https://github.com/crate/cratedb-mcp\n[Releases]: https://github.com/surister/cratedb-mcp/releases\n\n[badge-ci]: https://github.com/crate/cratedb-mcp/actions/workflows/tests.yml/badge.svg\n[badge-bluesky]: https://img.shields.io/badge/Bluesky-0285FF?logo=bluesky&logoColor=fff&label=Follow%20%40CrateDB\n[badge-coverage]: https://codecov.io/gh/crate/cratedb-mcp/branch/main/graph/badge.svg\n[badge-downloads-per-month]: https://pepy.tech/badge/cratedb-mcp/month\n[badge-license]: https://img.shields.io/github/license/crate/cratedb-mcp\n[badge-package-version]: https://img.shields.io/pypi/v/cratedb-mcp.svg\n[badge-python-versions]: https://img.shields.io/pypi/pyversions/cratedb-mcp.svg\n[badge-release-notes]: https://img.shields.io/github/release/crate/cratedb-mcp?label=Release+Notes\n[badge-status]: https://img.shields.io/pypi/status/cratedb-mcp.svg\n[project-ci]: https://github.com/crate/cratedb-mcp/actions/workflows/tests.yml\n[project-coverage]: https://app.codecov.io/gh/crate/cratedb-mcp\n[project-downloads]: https://pepy.tech/project/cratedb-mcp/\n[project-license]: https://github.com/crate/cratedb-mcp/blob/main/LICENSE\n[project-pypi]: https://pypi.org/project/cratedb-mcp\n[project-release-notes]: https://github.com/crate/cratedb-mcp/releases\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cratedb",
        "databases",
        "database",
        "directly cratedb",
        "cratedb mcp",
        "cratedb optimized"
      ],
      "category": "databases"
    },
    "crystaldba--postgres-mcp": {
      "owner": "crystaldba",
      "name": "postgres-mcp",
      "url": "https://github.com/crystaldba/postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/crystaldba.webp",
      "description": "Postgres MCP Pro provides database health checks, index tuning, and safe SQL execution, facilitating robust software development. It offers tools like explain plans for query optimization and monitoring for overall database performance.",
      "stars": 1264,
      "forks": 144,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T09:24:19Z",
      "readme_content": "<div align=\"center\">\n\n\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![PyPI - Version](https://img.shields.io/pypi/v/postgres-mcp)](https://pypi.org/project/postgres-mcp/)\n[![Discord](https://img.shields.io/discord/1336769798603931789?label=Discord)](https://discord.gg/4BEHC7ZM)\n[![Twitter Follow](https://img.shields.io/twitter/follow/auto_dba?style=flat)](https://x.com/auto_dba)\n[![Contributors](https://img.shields.io/github/contributors/crystaldba/postgres-mcp)](https://github.com/crystaldba/postgres-mcp/graphs/contributors)\n\n<h3>A Postgres MCP server with index tuning, explain plans, health checks, and safe sql execution.</h3>\n\n<div class=\"toc\">\n  <a href=\"#overview\">Overview</a> •\n  <a href=\"#demo\">Demo</a> •\n  <a href=\"#quick-start\">Quick Start</a> •\n  <a href=\"#technical-notes\">Technical Notes</a> •\n  <a href=\"#mcp-server-api\">MCP API</a> •\n  <a href=\"#related-projects\">Related Projects</a> •\n  <a href=\"#frequently-asked-questions\">FAQ</a>\n</div>\n\n</div>\n\n## Overview\n\n**Postgres MCP Pro** is an open source Model Context Protocol (MCP) server built to support you and your AI agents throughout the **entire development process**—from initial coding, through testing and deployment, and to production tuning and maintenance.\n\nPostgres MCP Pro does much more than wrap a database connection.\n\nFeatures include:\n\n- **🔍 Database Health** - analyze index health, connection utilization, buffer cache, vacuum health, sequence limits, replication lag, and more.\n- **⚡ Index Tuning** - explore thousands of possible indexes to find the best solution for your workload, using industrial-strength algorithms.\n- **📈 Query Plans** - validate and optimize performance by reviewing EXPLAIN plans and simulating the impact of hypothetical indexes.\n- **🧠 Schema Intelligence** - context-aware SQL generation based on detailed understanding of the database schema.\n- **🛡️ Safe SQL Execution** - configurable access control, including support for read-only mode and safe SQL parsing, making it usable for both development and production.\n\nPostgres MCP Pro supports both the [Standard Input/Output (stdio)](https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio) and [Server-Sent Events (SSE)](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse) transports, for flexibility in different environments.\n\nFor additional background on why we built Postgres MCP Pro, see [our launch blog post](https://www.crystaldba.ai/blog/post/announcing-postgres-mcp-server-pro).\n\n## Demo\n\n*From Unusable to Lightning Fast*\n- **Challenge:** We generated a movie app using an AI assistant, but the SQLAlchemy ORM code ran painfully slow.\n- **Solution:** Using Postgres MCP Pro with Cursor, we fixed the performance issues in minutes.\n\nWhat we did:\n- 🚀 Fixed performance - including ORM queries, indexing, and caching\n- 🛠️ Fixed a broken page - by prompting the agent to explore the data, fix queries, and add related content.\n- 🧠 Improved the top movies - by exploring the data and fixing the ORM query to surface more relevant results.\n\nSee the video below or read the [play-by-play](examples/movie-app.md).\n\nhttps://github.com/user-attachments/assets/24e05745-65e9-4998-b877-a368f1eadc13\n\n\n\n\n## Quick Start\n\n### Prerequisites\n\nBefore getting started, ensure you have:\n1. Access credentials for your database.\n2. Docker *or* Python 3.12 or higher.\n\n#### Access Credentials\n You can confirm your access credentials are valid by using `psql` or a GUI tool such as [pgAdmin](https://www.pgadmin.org/).\n\n\n#### Docker or Python\n\nThe choice to use Docker or Python is yours.\nWe generally recommend Docker because Python users can encounter more environment-specific issues.\nHowever, it often makes sense to use whichever method you are most familiar with.\n\n\n### Installation\n\nChoose one of the following methods to install Postgres MCP Pro:\n\n#### Option 1: Using Docker\n\nPull the Postgres MCP Pro MCP server Docker image.\nThis image contains all necessary dependencies, providing a reliable way to run Postgres MCP Pro in a variety of environments.\n\n```bash\ndocker pull crystaldba/postgres-mcp\n```\n\n\n#### Option 2: Using Python\n\nIf you have `pipx` installed you can install Postgres MCP Pro with:\n\n```bash\npipx install postgres-mcp\n```\n\nOtherwise, install Postgres MCP Pro with `uv`:\n\n```bash\nuv pip install postgres-mcp\n```\n\nIf you need to install `uv`, see the [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/).\n\n\n### Configure Your AI Assistant\n\nWe provide full instructions for configuring Postgres MCP Pro with Claude Desktop.\nMany MCP clients have similar configuration files, you can adapt these steps to work with the client of your choice.\n\n#### Claude Desktop Configuration\n\nYou will need to edit the Claude Desktop configuration file to add Postgres MCP Pro.\nThe location of this file depends on your operating system:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nYou can also use `Settings` menu item in Claude Desktop to locate the configuration file.\n\nYou will now edit the `mcpServers` section of the configuration file.\n\n##### If you are using Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DATABASE_URI\",\n        \"crystaldba/postgres-mcp\",\n        \"--access-mode=unrestricted\"\n      ],\n      \"env\": {\n        \"DATABASE_URI\": \"postgresql://username:password@localhost:5432/dbname\"\n      }\n    }\n  }\n}\n```\n\nThe Postgres MCP Pro Docker image will automatically remap the hostname `localhost` to work from inside of the container.\n\n- MacOS/Windows: Uses `host.docker.internal` automatically\n- Linux: Uses `172.17.0.1` or the appropriate host address automatically\n\n\n##### If you are using `pipx`\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"postgres-mcp\",\n      \"args\": [\n        \"--access-mode=unrestricted\"\n      ],\n      \"env\": {\n        \"DATABASE_URI\": \"postgresql://username:password@localhost:5432/dbname\"\n      }\n    }\n  }\n}\n```\n\n\n##### If you are using `uv`\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"postgres-mcp\",\n        \"--access-mode=unrestricted\"\n      ],\n      \"env\": {\n        \"DATABASE_URI\": \"postgresql://username:password@localhost:5432/dbname\"\n      }\n    }\n  }\n}\n```\n\n\n##### Connection URI\n\nReplace `postgresql://...` with your [Postgres database connection URI](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING-URIS).\n\n\n##### Access Mode\n\nPostgres MCP Pro supports multiple *access modes* to give you control over the operations that the AI agent can perform on the database:\n- **Unrestricted Mode**: Allows full read/write access to modify data and schema. It is suitable for development environments.\n- **Restricted Mode**: Limits operations to read-only transactions and imposes constraints on resource utilization (presently only execution time). It is suitable for production environments.\n\nTo use restricted mode, replace `--access-mode=unrestricted` with `--access-mode=restricted` in the configuration examples above.\n\n\n#### Other MCP Clients\n\nMany MCP clients have similar configuration files to Claude Desktop, and you can adapt the examples above to work with the client of your choice.\n\n- If you are using Cursor, you can use navigate from the `Command Palette` to `Cursor Settings`, then open the `MCP` tab to access the configuration file.\n- If you are using Windsurf, you can navigate to from the `Command Palette` to `Open Windsurf Settings Page` to access the configuration file.\n- If you are using Goose run `goose configure`, then select `Add Extension`.\n\n## SSE Transport\n\nPostgres MCP Pro supports the [SSE transport](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse), which allows multiple MCP clients to share one server, possibly a remote server.\nTo use the SSE transport, you need to start the server with the `--transport=sse` option.\n\nFor example, with Docker run:\n\n```bash\ndocker run -p 8000:8000 \\\n  -e DATABASE_URI=postgresql://username:password@localhost:5432/dbname \\\n  crystaldba/postgres-mcp --access-mode=unrestricted --transport=sse\n```\n\nThen update your MCP client configuration to call the the MCP server.\nFor example, in Cursor's `mcp.json` or Cline's `cline_mcp_settings.json` you can put:\n\n```json\n{\n    \"mcpServers\": {\n        \"postgres\": {\n            \"type\": \"sse\",\n            \"url\": \"http://localhost:8000/sse\"\n        }\n    }\n}\n```\n\nFor Windsurf, the format in `mcp_config.json` is slightly different:\n\n```json\n{\n    \"mcpServers\": {\n        \"postgres\": {\n            \"type\": \"sse\",\n            \"serverUrl\": \"http://localhost:8000/sse\"\n        }\n    }\n}\n```\n\n## Postgres Extension Installation (Optional)\n\nTo enable index tuning and comprehensive performance analysis you need to load the `pg_statements` and `hypopg` extensions on your database.\n\n- The `pg_statements` extension allows Postgres MCP Pro to analyze query execution statistics.\nFor example, this allows it to understand which queries are running slow or consuming significant resources.\n- The `hypopg` extension allows Postgres MCP Pro to simulate the behavior of the Postgres query planner after adding indexes.\n\n### Installing extensions on AWS RDS, Azure SQL, or Google Cloud SQL\n\nIf your Postgres database is running on a cloud provider managed service, the `pg_statements` and `hypopg` extensions should already be available on the system.\nIn this case, you can just run `CREATE EXTENSION` commands using a role with sufficient privileges:\n\n```sql\nCREATE EXTENSION IF NOT EXISTS pg_statements;\nCREATE EXTENSION IF NOT EXISTS hypopg;\n```\n\n### Installing extensions on self-managed Postgres\n\nIf you are managing your own Postgres installation, you may need to do additional work.\nBefore loading the `pg_statements` extension you must ensure that it is listed in the `shared_preload_libraries` in the Postgres configuration file.\nThe `hypopg` extension may also require additional system-level installation (e.g., via your package manager) because it does not always ship with Postgres.\n\n## Usage Examples\n\n### Get Database Health Overview\n\nAsk:\n> Check the health of my database and identify any issues.\n\n### Analyze Slow Queries\n\nAsk:\n> What are the slowest queries in my database? And how can I speed them up?\n\n### Get Recommendations On How To Speed Things Up\n\nAsk:\n> My app is slow. How can I make it faster?\n\n### Generate Index Recommendations\n\nAsk:\n> Analyze my database workload and suggest indexes to improve performance.\n\n### Optimize a Specific Query\n\nAsk:\n> Help me optimize this query: SELECT \\* FROM orders JOIN customers ON orders.customer_id = customers.id WHERE orders.created_at > '2023-01-01';\n\n## MCP Server API\n\nThe [MCP standard](https://modelcontextprotocol.io/) defines various types of endpoints: Tools, Resources, Prompts, and others.\n\nPostgres MCP Pro provides functionality via [MCP tools](https://modelcontextprotocol.io/docs/concepts/tools) alone.\nWe chose this approach because the [MCP client ecosystem](https://modelcontextprotocol.io/clients) has widespread support for MCP tools.\nThis contrasts with the approach of other Postgres MCP servers, including the [Reference Postgres MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres), which use [MCP resources](https://modelcontextprotocol.io/docs/concepts/resources) to expose schema information.\n\n\nPostgres MCP Pro Tools:\n\n| Tool Name | Description |\n|-----------|-------------|\n| `list_schemas` | Lists all database schemas available in the PostgreSQL instance. |\n| `list_objects` | Lists database objects (tables, views, sequences, extensions) within a specified schema. |\n| `get_object_details` | Provides information about a specific database object, for example, a table's columns, constraints, and indexes. |\n| `execute_sql` | Executes SQL statements on the database, with read-only limitations when connected in restricted mode. |\n| `explain_query` | Gets the execution plan for a SQL query describing how PostgreSQL will process it and exposing the query planner's cost model. Can be invoked with hypothetical indexes to simulate the behavior after adding indexes. |\n| `get_top_queries` | Reports the slowest SQL queries based on total execution time using `pg_stat_statements` data. |\n| `analyze_workload_indexes` | Analyzes the database workload to identify resource-intensive queries, then recommends optimal indexes for them. |\n| `analyze_query_indexes` | Analyzes a list of specific SQL queries (up to 10) and recommends optimal indexes for them. |\n| `analyze_db_health` | Performs comprehensive health checks including: buffer cache hit rates, connection health, constraint validation, index health (duplicate/unused/invalid), sequence limits, and vacuum health. |\n\n\n## Related Projects\n\n**Postgres MCP Servers**\n- [Query MCP](https://github.com/alexander-zuev/supabase-mcp-server). An MCP server for Supabase Postgres with a three-tier safety architecture and Supabase management API support.\n- [PG-MCP](https://github.com/stuzero/pg-mcp-server). An MCP server for PostgreSQL with flexible connection options, explain plans, extension context, and more.\n- [Reference PostgreSQL MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres). A simple MCP Server implementation exposing schema information as MCP resources and executing read-only queries.\n- [Supabase Postgres MCP Server](https://github.com/supabase-community/supabase-mcp). This MCP Server provides Supabase management features and is actively maintained by the Supabase community.\n- [Nile MCP Server](https://github.com/niledatabase/nile-mcp-server). An MCP server providing access to the management API for the Nile's multi-tenant Postgres service.\n- [Neon MCP Server](https://github.com/neondatabase-labs/mcp-server-neon). An MCP server providing access to the management API for Neon's serverless Postgres service.\n- [Wren MCP Server](https://github.com/Canner/wren-engine). Provides a semantic engine powering business intelligence for Postgres and other databases.\n\n**DBA Tools (including commercial offerings)**\n- [Aiven Database Optimizer](https://aiven.io/solutions/aiven-ai-database-optimizer). A tool that provides holistic database workload analysis, query optimizations, and other performance improvements.\n- [dba.ai](https://www.dba.ai/). An AI-powered database administration assistant that integrates with GitHub to resolve code issues.\n- [pgAnalyze](https://pganalyze.com/). A comprehensive monitoring and analytics platform for identifying performance bottlenecks, optimizing queries, and real-time alerting.\n- [Postgres.ai](https://postgres.ai/). An interactive chat experience combining an extensive Postgres knowledge base and GPT-4.\n- [Xata Agent](https://github.com/xataio/agent). An open-source AI agent that automatically monitors database health, diagnoses issues, and provides recommendations using LLM-powered reasoning and playbooks.\n\n**Postgres Utilities**\n- [Dexter](https://github.com/DexterDB/dexter). A tool for generating and testing hypothetical indexes on PostgreSQL.\n- [PgHero](https://github.com/ankane/pghero). A performance dashboard for Postgres, with recommendations.\nPostgres MCP Pro incorporates health checks from PgHero.\n- [PgTune](https://github.com/le0pard/pgtune?tab=readme-ov-file). Heuristics for tuning Postgres configuration.\n\n## Frequently Asked Questions\n\n*How is Postgres MCP Pro different from other Postgres MCP servers?*\nThere are many MCP servers allow an AI agent to run queries against a Postgres database.\nPostgres MCP Pro does that too, but also adds tools for understanding and improving the performance of your Postgres database.\nFor example, it implements a version of the [Anytime Algorithm of Database Tuning Advisor for Microsoft SQL Server](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/06/Anytime-Algorithm-of-Database-Tuning-Advisor-for-Microsoft-SQL-Server.pdf), a modern industrial-strength algorithm for automatic index tuning.\n\n| Postgres MCP Pro | Other Postgres MCP Servers |\n|--------------|----------------------------|\n| ✅ Deterministic database health checks | ❌ Unrepeatable LLM-generated health queries |\n| ✅ Principled indexing search strategies | ❌ Gen-AI guesses at indexing improvements |\n| ✅ Workload analysis to find top problems | ❌ Inconsistent problem analysis |\n| ✅ Simulates performance improvements | ❌ Try it yourself and see if it works |\n\nPostgres MCP Pro complements generative AI by adding deterministic tools and classical optimization algorithms\nThe combination is both reliable and flexible.\n\n\n*Why are MCP tools needed when the LLM can reason, generate SQL, etc?*\nLLMs are invaluable for tasks that involve ambiguity, reasoning, or natural language.\nWhen compared to procedural code, however, they can be slow, expensive, non-deterministic, and sometimes produce unreliable results.\nIn the case of database tuning, we have well established algorithms, developed over decades, that are proven to work.\nPostgres MCP Pro lets you combine the best of both worlds by pairing LLMs with classical optimization algorithms and other procedural tools.\n\n*How do you test Postgres MCP Pro?*\nTesting is critical to ensuring that Postgres MCP Pro is reliable and accurate.\nWe are building out a suite of AI-generated adversarial workloads designed to challenge Postgres MCP Pro and ensure it performs under a broad variety of scenarios.\n\n*What Postgres versions are supported?*\nOur testing presently focuses on Postgres 15, 16, and 17.\nWe plan to support Postgres versions 13 through 17.\n\n*Who created this project?*\nThis project is created and maintained by [Crystal DBA](https://www.crystaldba.ai).\n\n## Roadmap\n\n*TBD*\n\nYou and your needs are a critical driver for what we build.\nTell us what you want to see by opening an [issue](https://github.com/crystaldba/postgres-mcp/issues) or a [pull request](https://github.com/crystaldba/postgres-mcp/pulls).\nYou can also contact us on [Discord](https://discord.gg/4BEHC7ZM).\n\n## Technical Notes\n\nThis section includes a high-level overview technical considerations that influenced the design of Postgres MCP Pro.\n\n### Index Tuning\n\nDevelopers know that missing indexes are one of the most common causes of database performance issues.\nIndexes provide access methods that allow Postgres to quickly locate data that is required to execute a query.\nWhen tables are small, indexes make little difference, but as the size of the data grows, the difference in algorithmic complexity between a table scan and an index lookup becomes significant (typically *O*(*n*) vs *O*(*log* *n*), potentially more if joins on multiple tables are involved).\n\nGenerating suggested indexes in Postgres MCP Pro proceeds in several stages:\n\n1. *Identify SQL queries in need of tuning*.\n    If you know you are having a problem with a specific SQL query you can provide it.\n    Postgres MCP Pro can also analyze the workload to identify index tuning targets.\n    To do this, it relies on the `pg_stat_statements` extension, which records the runtime and resource consumption of each query.\n\n    A query is a candidate for index tuning if it is a top resource consumer, either on a per-execution basis or in aggregate.\n    At present, we use execution time as a proxy for cumulative resource consumption, but it may also make sense to look at specifics resources, e.g., the number of blocks accessed or the number of blocks read from disk.\n    The `analyze_query_workload` tool focuses on slow queries, using the mean time per execution with thresholds for execution count and mean execution time.\n    Agents may also call `get_top_queries`, which accepts a parameter for mean vs. total execution time, then pass these queries `analyze_query_indexes` to get index recommendations.\n\n    Sophisticated index tuning systems use \"workload compression\" to produce a representative subset of queries that reflects the characteristics of the workload as a whole, reducing the problem for downstream algorithms.\n    Postgres MCP Pro performs a limited form of workload compression by normalizing queries so that those generated from the same template appear as one.\n    It weights each query equally, a simplification that works when the benefits to indexing are large.\n\n2. *Generate candidate indexes*\n    Once we have a list of SQL queries that we want to improve through indexing, we generate a list of indexes that we might want to add.\n    To do this, we parse the SQL and identify any columns used in filters, joins, grouping, or sorting.\n\n    To generate all possible indexes we need to consider combinations of these columns, because Postgres supports [multicolumn indexes](https://www.postgresql.org/docs/current/indexes-multicolumn.html).\n    In the present implementation, we include only one permutation of each possible multicolumn index, which is selected at random.\n    We make this simplification to reduce the search space because permutations often have equivalent performance.\n    However, we hope to improve in this area.\n\n3. *Search for the optimal index configuration*.\n    Our objective is to find the combination of indexes that optimally balances the performance benefits against the costs of storing and maintaining those indexes.\n    We estimate the performance improvement by using the \"what if?\" capabilities provided by the `hypopg` extension.\n    This simulates how the Postgres query optimizer will execute a query after the addition of indexes, and reports changes based on the actual Postgres cost model.\n\n    One challenge is that generating query plans generally requires knowledge of the specific parameter values used in the query.\n    Query normalization, which is necessary to reduce the queries under consideration, removes parameter constants.\n    Parameter values provided via bind variables are similarly not available to us.\n\n    To address this problem, we produce realistic constants that we can provide as parameters by sampling from the table statistics.\n    In version 16, Postgres added [generic explain plan functionality](https://www.postgresql.org/docs/current/sql-explain.html), but it has limitations, for example around `LIKE` clauses, which our implementation does not have.\n\n    Search strategy is critical because evaluating all possible index combinations feasible only in simple situations.\n    This is what most sets apart various indexing approaches.\n    Adapting the approach of Microsoft's Anytime algorithm, we employ a greedy search strategy, i.e., find the best one-index solution, then find the best index to add to that to produce a two-index solution.\n    Our search terminates when the time budget is exhausted or when a round of exploration fails to produce any gains above the minimum improvement threshold of 10%.\n\n4. *Cost-benefit analysis*.\n    When posed with two indexing alternatives, one which produces better performance and one which requires more space, how do we decide which to choose?\n    Traditionally, index advisors ask for a storage budget and optimize performance with respect to that storage budget.\n    We also take a storage budget, but perform a cost-benefit analysis throughout the optimization.\n\n    We frame this as the problem of selecting a point along the [Pareto front](https://en.wikipedia.org/wiki/Pareto_front)—the set of choices for which improving one quality metric necessarily worsens another.\n    In an ideal world, we might want to assess the cost of the storage and the benefit of improved performance in monetary terms.\n    However, there is a simpler and more practical approach: to look at the changes in relative terms.\n    Most people would agree that a 100x performance improvement is worth it, even if the storage cost is 2x.\n    In our implementation, we use a configurable parameter to set this threshold.\n    By default, we require the change in the log (base 10) of the performance improvement to be 2x the difference in the log of the space cost.\n    This works out to allowing a maximum 10x increase in space for a 100x performance improvement.\n\nOur implementation is most closely related to the [Anytime Algorithm](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/06/Anytime-Algorithm-of-Database-Tuning-Advisor-for-Microsoft-SQL-Server.pdf) found in Microsoft SQL Server.\nCompared to [Dexter](https://github.com/ankane/dexter/), an automatic indexing tool for Postgres, we search a larger space and use different heuristics.\nThis allows us to generate better solutions at the cost of longer runtime.\n\nWe also show the work done in each round of the search, including a comparison of the query plans before and after the addition of each index.\nThis give the LLM additional context that it can use when responding to the indexing recommendations.\n\n### Experimental: Index Tuning by LLM\n\nPostgres MCP Pro includes an experimental index tuning feature based on [Optimization by LLM](https://arxiv.org/abs/2309.03409).\nInstead of using heuristics to explore possible index configurations, we provide the database schema and query plans to an LLM and ask it to propose index configurations.\nWe then use `hypopg` to predict performance with the proposed indexes, then feed those results back into the LLM to produce a new set of suggestions.\nWe repeat this process until multiple rounds of iteration produce no further improvements.\n\nIndex optimization by LLM is has advantages when the index search space is large, or when indexes with many columns need to be considered.\nLike traditional search-based approaches, it relies on the accuracy of the `hypopg` performance predictions.\n\nIn order to perform index optimization by LLM, you must provide an OpenAI API key by setting the `OPENAI_API_KEY` environment variable.\n\n\n### Database Health\n\nDatabase health checks identify tuning opportunities and maintenance needs before they lead to critical issues.\nIn the present release, Postgres MCP Pro adapts the database health checks directly from [PgHero](https://github.com/ankane/pghero).\nWe are working to fully validate these checks and may extend them in the future.\n\n- *Index Health*. Looks for unused indexes, duplicate indexes, and indexes that are bloated. Bloated indexes make inefficient use of database pages.\n  Postgres autovacuum cleans up index entries pointing to dead tuples, and marks the entries as reusable. However, it does not compact the index pages and, eventually, index pages may contain few live tuple references.\n- *Buffer Cache Hit Rate*. Measures the proportion of database reads that are served from the buffer cache instead of disk.\n  A low buffer cache hit rate must be investigated as it is often not cost-optimal and leads to degraded application performance.\n- *Connection Health*. Checks the number of connections to the database and reports on their utilization.\n  The biggest risk is running out of connections, but a high number of idle or blocked connections can also indicate issues.\n- *Vacuum Health*. Vacuum is important for many reasons.\n  A critical one is preventing transaction id wraparound, which can cause the database to stop accepting writes.\n  The Postgres multi-version concurrency control (MVCC) mechanism requires a unique transaction id for each transaction.\n  However, because Postgres uses a 32-bit signed integer for transaction ids, it needs to reuse transaction ids after after a maximum of 2 billion transactions.\n  To do this it \"freezes\" the transaction ids of historical transactions, setting them all to a special value that indicates distant past.\n  When records first go to disk, they are written visibility for a range of transaction ids.\n  Before re-using these transaction ids, Postgres must update any on-disk records, \"freezing\" them to remove the references to the transaction ids to be reused.\n  This check looks for tables that require vacuuming to prevent transaction id wraparound.\n- *Replication Health*. Checks replication health by monitoring lag between primary and replicas, verifying replication status, and tracking usage of replication slots.\n- *Constraint Health*. During normal operation, Postgres rejects any transactions that would cause a constraint violation.\n  However, invalid constraints may occur after loading data or in recovery scenarios. This check looks for any invalid constraints.\n- *Sequence Health*. Looks for sequences that are at risk of exceeding their maximum value.\n\n\n### Postgres Client Library\n\nPostgres MCP Pro uses [psycopg3](https://www.psycopg.org/) to connect to Postgres using asynchronous I/O.\nUnder the hood, psycopg3 uses the [libpq](https://www.postgresql.org/docs/current/libpq.html) library to connect to Postgres, providing access to the full Postgres feature set and an underlying implementation fully supported by the Postgres community.\n\nSome other Python-based MCP servers use [asyncpg](https://github.com/MagicStack/asyncpg), which may simplify installation by eliminating the `libpq` dependency.\nAsyncpg is also probably [faster](https://fernandoarteaga.dev/blog/psycopg-vs-asyncpg/) than psycopg3, but we have not validated this ourselves.\n[Older benchmarks](https://gistpreview.github.io/?0ed296e93523831ea0918d42dd1258c2) report a larger performance gap, suggesting that the newer psycopg3 has closed the gap as it matures.\n\nBalancing these considerations, we selected `psycopg3` over `asyncpg`.\nWe remain open to revising this decision in the future.\n\n\n### Connection Configuration\n\nLike the [Reference PostgreSQL MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres), Postgres MCP Pro takes Postgres connection information at startup.\nThis is convenient for users who always connect to the same database but can be cumbersome when users switch databases.\n\nAn alternative approach, taken by [PG-MCP](https://github.com/stuzero/pg-mcp-server), is provide connection details via MCP tool calls at the time of use.\nThis is more convenient for users who switch databases, and allows a single MCP server to simultaneously support multiple end-users.\n\nThere must be a better approach than either of these.\nBoth have security weaknesses—few MCP clients store the MCP server configuration securely (an exception is Goose), and credentials provided via MCP tools are passed through the LLM and stored in the chat history.\nBoth also have usability issues in some scenarios.\n\n\n### Schema Information\n\nThe purpose of the schema information tool is to provide the calling AI agent with the information it needs to generate correct and performant SQL.\nFor example, suppose a user asks, \"How many flights took off from San Francisco and landed in Paris during the past year?\"\nThe AI agent needs to find the table that stores the flights, the columns that store the origin and destinations, and perhaps a table that maps between airport codes and airport locations.\n\n\n*Why provide schema information tools when LLMs are generally capable of generating the SQL to retrieve this information from Postgres directly?*\n\nOur experience using Claude indicates that the calling LLM is very good at generating SQL to explore the Postgres schema by querying the [Postgres system catalog](https://www.postgresql.org/docs/current/catalogs.html) and the [information schema](https://www.postgresql.org/docs/current/information-schema.html) (an ANSI-standardized database metadata view).\nHowever, we do not know whether other LLMs do so as reliably and capably.\n\n*Would it be better to provide schema information using [MCP resources](https://modelcontextprotocol.io/docs/concepts/resources) rather than [MCP tools](https://modelcontextprotocol.io/docs/concepts/tools)?*\n\nThe [Reference PostgreSQL MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) uses resources to expose schema information rather than tools.\nNavigating resources is similar to navigating a file system, so this approach is natural in many ways.\nHowever, resource support is less widespread than tool support in the MCP client ecosystem (see [example clients](https://modelcontextprotocol.io/clients)).\nIn addition, while the MCP standard says that resources can be accessed by either AI agents or end-user humans, some clients only support human navigation of the resource tree.\n\n\n### Protected SQL Execution\n\nAI amplifies longstanding challenges of protecting databases from a range of threats, ranging from simple mistakes to sophisticated attacks by malicious actors.\nWhether the threat is accidental or malicious, a similar security framework applies, with aims that fall into three categories: confidentiality, integrity, and availability.\nThe familiar tension between convenience and safety is also evident and pronounced.\n\nPostgres MCP Pro's protected SQL execution mode focuses on integrity.\nIn the context of MCP, we are most concerned with LLM-generated SQL causing damage—for example, unintended data modification or deletion, or other changes that might circumvent an organization's change management process.\n\nThe simplest way to provide integrity is to ensure that all SQL executed against the database is read-only.\nOne way to do this is by creating a database user with read-only access permissions.\nWhile this is a good approach, many find this cumbersome in practice.\nPostgres does not provide a way to place a connection or session into read-only mode, so Postgres MCP Pro uses a more complex approach to ensure read-only SQL execution on top of a read-write connection.\n\nPostgres MCP Provides a read-only transaction mode that prevents data and schema modifications.\nLike the [Reference PostgreSQL MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres), we use read-only transactions to provide protected SQL execution.\n\nTo make this mechanism robust, we need to ensure that the SQL does not somehow circumvent the read-only transaction mode, say by issuing a `COMMIT` or `ROLLBACK` statement and then beginning a new transaction.\n\nFor example, the LLM can circumvent the read-only transaction mode by issuing a `ROLLBACK` statement and then beginning a new transaction.\nFor example:\n```sql\nROLLBACK; DROP TABLE users;\n```\n\nTo prevent cases like this, we parse the SQL before execution using the [pglast](https://pglast.readthedocs.io/) library.\nWe reject any SQL that contains `commit` or `rollback` statements.\nHelpfully, the popular Postgres stored procedure languages, including PL/pgSQL and PL/Python, do not allow for `COMMIT` or `ROLLBACK` statements.\nIf you have unsafe stored procedure languages enabled on your database, then our read-only protections could be circumvented.\n\nAt present, Postgres MCP Pro provides two levels of protection for the database, one at either extreme of the convenience/safety spectrum.\n- \"Unrestricted\" provides maximum flexibility.\nIt is suitable for development environments where speed and flexibility are paramount, and where there is no need to protect valuable or sensitive data.\n- \"Restricted\" provides a balance between flexibility and safety.\nIt is suitable for production environments where the database is exposed to untrusted users, and where it is important to protect valuable or sensitive data.\n\nUnrestricted mode aligns with the approach of [Cursor's auto-run mode](https://docs.cursor.com/chat/tools#auto-run), where the AI agent operates with limited human oversight or approvals.\nWe expect auto-run to be deployed in development environments where the consequences of mistakes are low, where databases do not contain valuable or sensitive data, and where they can be recreated or restored from backups when needed.\n\nWe designed restricted mode to be conservative, erring on the side of safety even though it may be inconvenient.\nRestricted mode is limited to read-only operations, and we limit query execution time to prevent long-running queries from impacting system performance.\nWe may add measures in the future to make sure that restricted mode is safe to use with production databases.\n\n\n## Postgres MCP Pro Development\n\nThe instructions below are for developers who want to work on Postgres MCP Pro, or users who prefer to install Postgres MCP Pro from source.\n\n### Local Development Setup\n\n1. **Install uv**:\n\n   ```bash\n   curl -sSL https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Clone the repository**:\n\n   ```bash\n   git clone https://github.com/crystaldba/postgres-mcp.git\n   cd postgres-mcp\n   ```\n\n3. **Install dependencies**:\n\n   ```bash\n   uv pip install -e .\n   uv sync\n   ```\n\n4. **Run the server**:\n   ```bash\n   uv run postgres-mcp \"postgres://user:password@localhost:5432/dbname\"\n   ```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "postgres",
        "crystaldba",
        "crystaldba postgres",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "cuongtl1992--mcp-dbs": {
      "owner": "cuongtl1992",
      "name": "mcp-dbs",
      "url": "https://github.com/cuongtl1992/mcp-dbs",
      "imageUrl": "/freedevtools/mcp/pfp/cuongtl1992.webp",
      "description": "Connect to various database systems to execute queries, explore schemas, and manage data through a simple interface.",
      "stars": 19,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T17:15:24Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/cuongtl1992-mcp-dbs)\n\n# MCP Database Server\n\nA Model Context Protocol (MCP) implementation for connecting to and working with various database systems.\n\n<a href=\"https://glama.ai/mcp/servers/tvpshb3f1n\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/tvpshb3f1n/badge\" />\n</a>\n\n## Supported Databases\n\n- SQLite\n- PostgreSQL\n- Microsoft SQL Server\n- MongoDB\n\n## Installation\n\n```bash\nnpm install -g mcp-dbs\n```\n\n## Usage\n\nThe MCP Database Server can be used in two modes:\n\n### SSE Mode (Default)\n\nBy default, the server runs in SSE (Server-Sent Events) mode on port 3001:\n\n```bash\nnpx mcp-dbs\n```\n\nThis will start an HTTP server with an SSE endpoint at `http://localhost:3001/mcp`.\n\n#### Custom Port\n\nYou can specify a custom port using the `--port` option:\n\n```bash\nnpx mcp-dbs --port 8080\n```\n\n### STDIO Mode\n\nFor tools that communicate over standard input/output, you can use the `--stdio` option:\n\n```bash\nnpx mcp-dbs --stdio\n```\n\n## Claude Desktop Integration\n\nYou can integrate mcp-dbs with Claude Desktop by adding it to your Claude configuration file. \n\n### Configuration Steps\n\n1. Open or create your Claude Desktop configuration file\n2. Add the mcp-dbs configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-dbs\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/mcp-dbs/dist/cli.js\",\n        \"--stdio\"\n      ],\n      \"env\": {\n        \"MCP_MONGODB_URI\": \"mongodb://localhost:27017\",\n        \"MCP_MONGODB_DATABASE\": \"your-database-name\"\n      }\n    }\n  }\n}\n```\n\nReplace the environment variables with your own database connection details. \n\n### Notes\n- The `command` should be `node`\n- In `args`, provide the absolute path to the cli.js file in your mcp-dbs installation\n- Configure the appropriate environment variables for your database type (see the environment variables section below)\n- You can use environment variables for any of the supported databases (SQLite, PostgreSQL, SQL Server, or MongoDB)\n\n### Using with Claude\n\nOnce configured, Claude will be able to access your database using the MCP tools described below. You can ask Claude to:\n- Connect to your database\n- Execute queries and get results\n- Explore your database schema\n- Work with tables and data\n\n## Tools\n\n- **connect-database**: Connect to a database\n- **disconnect-database**: Disconnect from a database\n- **execute-query**: Execute a query and return results\n- **execute-update**: Execute a query without returning results\n\n## Resources\n\n- **database-schema**: Get the full database schema\n- **table-schema**: Get the schema for a specific table\n- **tables-list**: Get a list of all tables\n\n## Using environment variables for configuration\n\nYou can configure your database connections using environment variables:\n\n#### SQLite\n\n```bash\n# Set these environment variables before connecting\nexport MCP_SQLITE_FILENAME=\"path/to/database.db\"\nexport MCP_SQLITE_CREATE_IF_NOT_EXISTS=\"true\"\n```\n\n#### PostgreSQL\n\n```bash\n# Set these environment variables before connecting\nexport MCP_POSTGRES_HOST=\"your-postgres-host\"\nexport MCP_POSTGRES_PORT=\"5432\"\nexport MCP_POSTGRES_DATABASE=\"your-database-name\"\nexport MCP_POSTGRES_USER=\"your-username\"\nexport MCP_POSTGRES_PASSWORD=\"your-password\"\nexport MCP_POSTGRES_SSL=\"false\"\n```\n\n#### SQL Server\n\n```bash\n# Set these environment variables before connecting\nexport MCP_MSSQL_SERVER=\"your-server-address\"\nexport MCP_MSSQL_PORT=\"1433\"\nexport MCP_MSSQL_DATABASE=\"your-database-name\"\nexport MCP_MSSQL_USER=\"your-username\"\nexport MCP_MSSQL_PASSWORD=\"your-password\"\nexport MCP_MSSQL_ENCRYPT=\"true\"\nexport MCP_MSSQL_TRUST_SERVER_CERTIFICATE=\"true\"\n```\n\n#### MongoDB\n\n```bash\n# Set these environment variables before connecting\nexport MCP_MONGODB_URI=\"mongodb://localhost:27017\"\nexport MCP_MONGODB_DATABASE=\"your-database-name\"\nexport MCP_MONGODB_MAX_POOL_SIZE=\"10\"\nexport MCP_MONGODB_USE_UNIFIED_TOPOLOGY=\"true\"\n```\n\nThese environment variables will take precedence over any configuration passed to the connect-database tool.\n\n## MCP Tools\n\nThe server exposes the following MCP tools:\n\n### connect-database\n\nConnect to a database.\n\nParameters:\n- `connectionId`: A unique identifier for the connection\n- `type`: Database type (`sqlite`, `postgres`, `mssql`, or `mongodb`)\n\nExample for SQLite:\n```json\n{\n  \"connectionId\": \"my-sqlite-db\",\n  \"type\": \"sqlite\"\n}\n```\n\nExample for PostgreSQL:\n```json\n{\n  \"connectionId\": \"my-postgres-db\",\n  \"type\": \"postgres\"\n}\n```\n\nExample for SQL Server:\n```json\n{\n  \"connectionId\": \"my-mssql-db\",\n  \"type\": \"mssql\"\n}\n```\n\nExample for MongoDB:\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"type\": \"mongodb\"\n}\n```\n\n### disconnect-database\n\nDisconnect from a database.\n\nParameters:\n- `connectionId`: The connection ID to disconnect\n\n### execute-query\n\nExecute a query that returns results.\n\nParameters:\n- `connectionId`: The connection ID\n- `query`: SQL query or MongoDB aggregation pipeline (as JSON string)\n- `params`: (Optional) Array of parameters for the query. For MongoDB, the first parameter is the collection name.\n\nExample for SQL:\n```json\n{\n  \"connectionId\": \"my-postgres-db\",\n  \"query\": \"SELECT * FROM users WHERE age > $1\",\n  \"params\": [21]\n}\n```\n\nExample for MongoDB:\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"[{\\\"$match\\\": {\\\"age\\\": {\\\"$gt\\\": 21}}}, {\\\"$sort\\\": {\\\"name\\\": 1}}]\",\n  \"params\": [\"users\"]\n}\n```\n\nExample for MongoDB (new format with embedded collection):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"{\\\"collection\\\": \\\"users\\\", \\\"pipeline\\\": [{\\\"$match\\\": {\\\"age\\\": {\\\"$gt\\\": 21}}}, {\\\"$sort\\\": {\\\"name\\\": 1}}]}\"\n}\n```\n\nExample for MongoDB (shell syntax):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"db.getCollection('users').find({\\\"age\\\": {\\\"$gt\\\": 21}})\"\n}\n```\n\nExample for MongoDB (direct collection reference shell syntax):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"db.users.find({\\\"age\\\": {\\\"$gt\\\": 21}})\"\n}\n```\n\nExample for MongoDB (raw command):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\", \n  \"query\": \"{\\\"find\\\": \\\"users\\\", \\\"filter\\\": {\\\"age\\\": {\\\"$gt\\\": 21}}}\"\n}\n```\n\n### execute-update\n\nExecute a query that doesn't return results (INSERT, UPDATE, DELETE).\n\nParameters:\n- `connectionId`: The connection ID\n- `query`: SQL query or MongoDB command (as JSON string)\n- `params`: (Optional) Array of parameters for the query. For MongoDB, the first parameter is the collection name.\n\nExample for SQL:\n```json\n{\n  \"connectionId\": \"my-postgres-db\",\n  \"query\": \"INSERT INTO users (name, age) VALUES ($1, $2)\",\n  \"params\": [\"John Doe\", 30]\n}\n```\n\nExample for MongoDB:\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"{\\\"insertOne\\\": {\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30}}\",\n  \"params\": [\"users\"]\n}\n```\n\nExample for MongoDB (new format with embedded collection):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"{\\\"collection\\\": \\\"users\\\", \\\"operation\\\": {\\\"insertOne\\\": {\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30}}}\"\n}\n```\n\nExample for MongoDB (shell syntax):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"db.getCollection('users').insertOne({\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30})\"\n}\n```\n\nExample for MongoDB (direct collection reference shell syntax):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"db.users.insertOne({\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30})\"\n}\n```\n\nExample for MongoDB (raw command):\n```json\n{\n  \"connectionId\": \"my-mongodb-db\",\n  \"query\": \"{\\\"insert\\\": \\\"users\\\", \\\"documents\\\": [{\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30}]}\"\n}\n```\n\n## MCP Resources\n\nThe server exposes the following MCP resources:\n\n### Database Schema\n\nURI: `database://{connectionId}/schema`\n\nReturns schema information about the database, including all tables and their columns.\n\n### Table Schema\n\nURI: `database://{connectionId}/tables/{tableName}`\n\nReturns schema information about a specific table, including its columns.\n\n### Tables List\n\nURI: `database://{connectionId}/tables`\n\nReturns a list of all tables in the database.\n\n## Development\n\n### Testing\n\nRun the tests:\n\n```bash\nnpm test\n```\n\n## Support the Project\n\nIf you find this project helpful, consider buying me a coffee!\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/cuongtl1992/mcp-dbs/main/assets/bmc_qr.png\" alt=\"Buy Me A Coffee QR Code\" width=\"200\">\n</p>\n\nScan the QR code above or [click here](https://www.buymeacoffee.com/cuongtl1992) to support the development of this project.\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "dbs",
        "mcp dbs",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "cv-cat--LarkAgentX": {
      "owner": "cv-cat",
      "name": "LarkAgentX",
      "url": "https://github.com/cv-cat/LarkAgentX",
      "imageUrl": "/freedevtools/mcp/pfp/cv-cat.webp",
      "description": "Enables function calls and message handling within the Lark ecosystem, utilizing a user's Lark account as an AI assistant. It automatically analyzes user input to invoke relevant functions and stores messages in a MySQL database for chat management.",
      "stars": 120,
      "forks": 22,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-01T14:40:59Z",
      "readme_content": "# Lark Agentx - 你的飞书 AI 助手 🚀\n\n[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)\n[![Node.js Version](https://img.shields.io/badge/nodejs-18%2B-blue)](https://nodejs.org/zh-cn/)\n\n一个基于飞书(Lark)的AI Agent，实现大模型通过飞书进行函数调用和消息处理。\n\n\n**无需配置飞书机器人，你的飞书账号即是AI助手。**\n\n\n**只需定义函数和注释，你的飞书机器人会自动根据场景调用。**\n\n\n## 项目概述 🌟\n\nLark Agentx是一个现代化的Python应用程序，能够:\n\n- 📊 逆向飞书Protobuf格式传输的Websockets和API，监听并记录消息\n- 🤖 提供自定义函数供大模型调用\n- 🔄 实现基于MCP (Model Context Protocol) 的函数调用框架\n- 💾 使用SQLAlchemy将消息存储到MySQL数据库\n\n## 效果图🧸\n\n<div align=\"center\">\n  \n  <br>\n  <em>图1: 后台日志</em>\n</div>\n\n\n<div align=\"center\">\n  \n  <br>\n  <em>图2: 聊天数据库查询</em>\n</div>\n\n<div align=\"center\">\n   \n  <br>\n  <em>图3: 天气查询</em>\n</div>\n\n<div align=\"center\">\n   \n  <br>\n  <em>图4: 简单注册函数，只需定义函数和注释</em>\n</div>\n\n## ✨ 功能特点\n\n- **函数注册机制**: 简单直观的函数注册装饰器\n- **消息自动处理**: 记录所有接收到的消息（私聊和群聊）\n- **异步处理**: 采用async/await模式进行异步通信\n- **数据持久化**: 使用SQLAlchemy将消息存储在MySQL数据库中\n- **灵活配置**: 通过环境变量进行配置\n- **容器化部署**: 支持Docker快速部署\n- **智能函数调用**: AI会根据用户输入的文字自动分析并调用最匹配的函数，开发者只需添加函数及其注释描述\n\n## 📦 当前支持的函数\n\n项目目前内置了以下函数供大模型调用:\n\n| 函数名 | 描述 |\n|-------|------|\n| `tell_joke` | 讲一个随机笑话 |\n| `get_time` | 获取当前时间 |\n| `fortune` | 抽取一个随机运势 |\n| `get_weather` | 获取城市天气 |\n| `count_daily_speakers` | 获取今天发言的人数统计 |\n| `get_top_speaker_today` | 获取今天发言最多的用户 |\n| `send_message` | 给指定用户发送消息 |\n| `list_tools` | 列出所有可用的工具及其描述 |\n| `extra_order_from_content` | 提取文字中的订单信息，包括订单号、商品名称、数量等 |\n\n\n你可以通过在飞书中输入触发指令后跟要执行的操作来调用这些功能，例如: `/run 讲个笑话`\n\n## 📂 项目结构\n\n```\nproject/\n├── app/                    # 应用程序模块\n│   ├── api/                # API相关模块\n│   │   ├── auth.py         # 认证模块\n│   │   └── lark_client.py  # 飞书客户端\n│   ├── config/             # 配置模块\n│   │   └── settings.py     # 应用配置\n│   ├── core/               # 核心业务逻辑\n│   │   ├── mcp_server.py   # MCP服务器（函数注册和处理）\n│   │   ├── llm_service.py  # LLM服务\n│   │   └── message_service.py  # 消息处理服务\n│   ├── db/                 # 数据库相关\n│   │   ├── models.py       # 数据模型\n│   │   └── session.py      # 数据库会话管理\n│   └── utils/              # 工具函数\n├── builder/                # 请求构建器\n├── extension/              # 扩展功能\n│   └── weather_api/        # 天气API集成\n├── static/                 # 静态资源\n│   ├── resource/           # 图片资源\n│   ├── proto_pb2.py        # 协议定义\n│   └── lark_decrypt.js     # 飞书解密工具\n├── .env                    # 环境变量\n├── main.py                 # 应用入口\n├── requirements.txt        # 项目依赖\n├── docker-compose.yml      # Docker Compose配置\n└── Dockerfile              # Docker配置\n```\n\n## 🛠️ 自定义函数开发\n\n在 `app/core/mcp_server.py` 文件中，您可以使用 `@register_tool` 装饰器添加您自己的自定义函数:\n\n```python\n@register_tool(name=\"tell_joke\", description=\"讲一个随机笑话\")\ndef tell_joke() -> str:\n    jokes = [\n        \"为什么程序员都喜欢黑色？因为他们不喜欢 bug 光。\",\n        \"Python 和蛇有什么共同点？一旦缠上你就放不下了。\",\n        \"为什么 Java 开发者很少被邀去派对？因为他们总是抛出异常。\",\n    ]\n    return random.choice(jokes)\n\n@register_tool(name=\"send_message\", description=\"给指定用户发送消息 {user:用户名称 content:消息内容}\")\ndef send_message(user: str, content: str) -> str:\n    \"\"\"给指定用户发送私信\"\"\"\n    lark_client = LarkClient(get_auth())\n    # ... 实现逻辑 ...\n    return f\"成功向 {user} 发送了私信: '{content}'\"\n```\n\n**重要**: 只需添加函数和对应的描述，AI会根据用户的文字自动分析并调用最匹配的函数，无需手动实现函数匹配逻辑。\n\n## 🔧 环境要求\n\n- Python 3.10+\n- Node.js 18+\n- MySQL数据库\n\n## 📦 安装方法\n\n### 使用本地环境\n\n1. 安装依赖:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. Windows用户注意:\n   Windows系统需要额外安装以下依赖:\n   ```bash\n   pip install win-inet-pton==1.1.0\n   ```\n\n### 使用Docker\n\n方法一：单独构建镜像\n```bash\n# 构建镜像 \ndocker build -t feishuapp .\n\n# 运行容器 需要外部mysql 通过docker网关连接宿主机mysql 推荐--env-file\ndocker run -it feishuapp bash\n```\n\n方法二：使用Docker Compose（推荐）\n```bash\n# 启动所有服务（应用和数据库）\ndocker-compose up -d\n\n# 查看日志\ndocker-compose logs -f\n\n# 停止所有服务\ndocker-compose down\n```\n\n使用Docker Compose可以一键启动整个应用环境，包括MySQL数据库和应用服务，更加方便和高效。\n\n## 🛠️ 配置说明\n\n复制`.env.example`文件命名为`.env`文件，包含以下配置:\n\n```\n# 数据库设置\nDB_HOST=localhost\nDB_PORT=3306\nDB_USER=root\nDB_PASSWORD=123456\nDB_NAME=lark_messages\n\n# 飞书的Cookie设置 - 只需配置LARK_COOKIE即可，告别飞书机器人\nLARK_COOKIE=\"\"\n\n# 调用函数的触发前缀 （以FUNCTION_TRIGGER_FLAG开头的消息会被大模型解析，所有消息都会被记录到数据库，无论是否以该前缀开头）\nFUNCTION_TRIGGER_FLAG=\"/run\"\n\n# 机器人发言前缀 （暂未使用）\nAI_BOT_PREFIX=\"Lark AI Bot:\"\n\n# OpenAI API配置 默认是通义千问的，满足OpenAI的大模型厂商都可以\nOPENAI_API_KEY=\"\"\nOPENAI_API_BASE_URL=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\nOPENAI_API_MODEL=\"qwen-plus\"\n```\n\n## 🚀 使用指南\n\n### 运行应用程序\n\n方法一：直接运行\n```bash\npython main.py\n```\n\n方法二：使用Docker Compose\n```bash\ndocker-compose up -d\n```\n\n应用程序将:\n1. 初始化MCP服务器\n2. 连接到飞书API并使用你的飞书账号作为AI助手\n3. 监听传入的消息\n4. 处理并执行大模型通过飞书发起的函数调用\n5. 将消息存储在MySQL数据库中\n\n\n## 🗄️ 数据库结构\n\n应用程序将消息存储在`messages`表中，该表具有以下结构:\n\n| 列名           | 类型           | 描述                      |\n|----------------|---------------|---------------------------|\n| id             | INT (PK)      | 主键                      |\n| user_name      | VARCHAR(255)  | 消息发送者的名称           |\n| user_id        | VARCHAR(255)  | 发送者的飞书用户ID         |\n| content        | TEXT          | 消息内容                  |\n| is_group_chat  | BOOLEAN       | 消息是否来自群聊           |\n| group_name     | VARCHAR(255)  | 群聊名称（如适用）         |\n| chat_id        | VARCHAR(255)  | 聊天ID                    |\n| message_time   | DATETIME      | 消息发送时间               |\n| created_at     | DATETIME      | 记录创建时间               |\n\n## 🤝 贡献指南\n\n欢迎贡献！请随时提交Pull Request。\n\n1. Fork这个仓库\n2. 创建您的特性分支 (`git checkout -b feature/amazing-feature`)\n3. 提交您的更改 (`git commit -m '添加一些很棒的特性'`)\n4. 推送到分支 (`git push origin feature/amazing-feature`)\n5. 打开Pull Request\n\n## 🐛 问题与支持\n\n如果您遇到任何问题或有疑问，请[提交issue](https://github.com/cv-cat/LarkAgentX/issues)或访问我们的[讨论论坛](https://github.com/cv-cat/LarkAgentX/discussions)。\n\n## 📈 Star 趋势\n<a href=\"https://www.star-history.com/#cv-cat/LarkAgentX&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=cv-cat/LarkAgentX&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=cv-cat/LarkAgentX&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=cv-cat/LarkAgentX&type=Date\" />\n </picture>\n</a>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "larkagentx",
        "database",
        "databases",
        "secure database",
        "database chat",
        "databases secure"
      ],
      "category": "databases"
    },
    "cwilby--mcp-node-mssql": {
      "owner": "cwilby",
      "name": "mcp-node-mssql",
      "url": "https://github.com/cwilby/mcp-node-mssql",
      "imageUrl": "/freedevtools/mcp/pfp/cwilby.webp",
      "description": "Integrate SQL Server databases with AI assistants by facilitating secure and efficient database querying capabilities through the Model Context Protocol. Enable LLM applications to easily access and manipulate Microsoft SQL Server data.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-22T19:12:42Z",
      "readme_content": "# mcp-node-mssql\n\n## Usage\n\n### Cursor\n\nSee the [official Cursor docs](https://docs.cursor.com/context/model-context-protocol) for more information.\n\n1. Open (or create) the `mcp.json` file (it should be in `~/.cursor/mcp.json` or `<project-root>/.cursor/mcp.json`, but see Cursor docs for more details).\n2. Add the following details and save the file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-node-mssql\"\n      ],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"1433\",\n        \"DB_USERNAME\": \"<username>\",\n        \"DB_PASSWORD\": \"<password>\",\n        \"DB_DATABASE\": \"<database>\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nSee the [official Windsurf docs](https://codeium.com/docs/windsurf/mcp) for more information.\n\n1. Open the `Windsurf MCP Configuration Panel`\n2. Click `Add custom server`.\n3. Add the following details and save the file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-node-mssql\"\n      ],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"1433\",\n        \"DB_USERNAME\": \"<username>\",\n        \"DB_PASSWORD\": \"<password>\",\n        \"DB_DATABASE\": \"<database>\"\n      }\n    }\n  }\n}\n```\n\n\n### Claude Code\n\nSee the [official Claude Code docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp) for more information.\n\n_You can add a new MCP server from the Claude Code CLI. But modifying the json file directly is simpler!_\n\n1. Open the Claude Code configuration file (it should be in `~/.claude.json`).\n2. Find the `projects` > `mcpServers` section and add the following details and save the file:\n\n```json\n{\n  \"projects\": {\n    \"mcpServers\": {\n      \"mssql\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"mcp-node-mssql\"\n        ],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"1433\",\n          \"DB_USERNAME\": \"<username>\",\n          \"DB_PASSWORD\": \"<password>\",\n          \"DB_DATABASE\": \"<database>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Issues and Troubleshooting\n\nBefore doing anything else, please make sure you are running the latest version!\n\nIf you run into problems using this MCP server, please open an issue on [GitHub](https://github.com/cwilby/mcp-node-mssql/issues)!\n\n## Development\n\n### Installation\n\n```bash\nnpm install\n```\n\n### Build\n\n```bash\nnpm run build\n```\n\n### Running the Development Server Locally\n\nTo test your local development version of the MCP server rather than using the published package, follow these steps:\n\n1. Build the project:\n```bash\nnpm run build\n```\n\n2. Create or modify your `mcp.json` file to reference your local build:\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/local/mcp-node-mssql/dist/index.js\"\n      ],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"1433\",\n        \"DB_USERNAME\": \"<username>\",\n        \"DB_PASSWORD\": \"<password>\",\n        \"DB_DATABASE\": \"<database>\"\n      }\n    }\n  }\n}\n```\n\n3. Place this `mcp.json` file in one of the following locations:\n   - For Cursor: In your home directory (`~/.cursor/mcp.json`) or in your project directory (`.cursor/mcp.json`)\n   - For Windsurf: Use the MCP Configuration Panel to add the custom server\n\n4. Restart your AI assistant (Cursor or Windsurf) to load the new configuration.\n\nThis allows you to instantly test changes to the MCP server without having to publish a new version.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "danielbushman--MCP-Quickbase": {
      "owner": "danielbushman",
      "name": "MCP-Quickbase",
      "url": "https://github.com/danielbushman/MCP-Quickbase",
      "imageUrl": "/freedevtools/mcp/pfp/danielbushman.webp",
      "description": "Interact with Quickbase data using natural language, facilitating the management of apps, tables, and records. Leverages AI for efficient data access and manipulation through the Quickbase JSON RESTful API.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-23T18:41:24Z",
      "readme_content": "# Quickbase MCP Server\n\nA TypeScript-based Model Context Protocol (MCP) server for Quickbase, designed for seamless integration with Claude Desktop and other AI assistants.\n\n> **📋 Community Project Notice**  \n> This is a community-developed integration that is not an official Quickbase product. While it uses Quickbase's public APIs, it is not officially supported by Quickbase, Inc. This project is provided \"as is\" and maintained by the community. For official Quickbase products and support, please visit [quickbase.com](https://www.quickbase.com).\n\n## 🚀 Quick Start for Claude Desktop\n\n### One-Line Setup Check\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/danielbushman/MCP-Quickbase/main/check_dependencies.sh | bash\n```\n\n### Configure Claude Desktop\n\nAdd this to your Claude Desktop configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"quickbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-quickbase\"],\n      \"env\": {\n        \"QUICKBASE_REALM_HOST\": \"your-realm.quickbase.com\",\n        \"QUICKBASE_USER_TOKEN\": \"your-user-token\",\n        \"QUICKBASE_APP_ID\": \"your-app-id\"\n      }\n    }\n  }\n}\n```\n\n**That's it!** Restart Claude Desktop and you can start using Quickbase tools.\n\n---\n\n## 📦 Installation Options\n\n### Option 1: NPM (Recommended)\n\n```bash\n# Use directly with npx (no installation needed)\nnpx -y mcp-quickbase\n\n# Or install globally\nnpm install -g mcp-quickbase\n```\n\n### Option 2: From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/danielbushman/MCP-Quickbase.git\ncd MCP-Quickbase\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nFor source installation, use this Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"quickbase\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/MCP-Quickbase/dist/mcp-stdio-server.js\"],\n      \"env\": {\n        \"QUICKBASE_REALM_HOST\": \"your-realm.quickbase.com\",\n        \"QUICKBASE_USER_TOKEN\": \"your-user-token\",\n        \"QUICKBASE_APP_ID\": \"your-app-id\"\n      }\n    }\n  }\n}\n```\n\n## 🔧 Configuration\n\nThe server can start without environment variables configured, but tools will not be functional until proper configuration is provided. Use the `check_configuration` tool to verify your setup.\n\n### Required Environment Variables\n\n- **`QUICKBASE_REALM_HOST`** - Your Quickbase realm (e.g., `company.quickbase.com`)\n- **`QUICKBASE_USER_TOKEN`** - Your Quickbase API token ([Get one here](https://help.quickbase.com/en/articles/8672050))\n\n### Optional Environment Variables\n\n- **`QUICKBASE_APP_ID`** - Default application ID\n\n### Optional Settings\n\n- **`QUICKBASE_CACHE_ENABLED`** - Enable caching (`true`/`false`, default: `true`)\n- **`QUICKBASE_CACHE_TTL`** - Cache duration in seconds (default: `3600`)\n- **`DEBUG`** - Enable debug logging (`true`/`false`, default: `false`)\n- **`LOG_LEVEL`** - Logging level (`DEBUG`/`INFO`/`WARN`/`ERROR`, default: `INFO`)\n\n## 🛠️ Available Tools\n\n### Connection & Configuration\n- **`check_configuration`** - Check if Quickbase configuration is properly set up\n- **`test_connection`** - Test connection to Quickbase\n- **`configure_cache`** - Configure caching behavior\n\n### Application Management\n- **`create_app`** - Create new Quickbase applications\n- **`update_app`** - Update existing applications\n- **`list_tables`** - List all tables in an application\n\n### Table Operations\n- **`create_table`** - Create new tables\n- **`update_table`** - Update table properties\n- **`get_table_fields`** - Get field information for a table\n\n### Field Management\n- **`create_field`** - Create new fields in tables\n- **`update_field`** - Update field properties\n\n### Record Operations\n- **`query_records`** - Query records with filtering and sorting\n- **`create_record`** - Create single records\n- **`update_record`** - Update existing records\n- **`bulk_create_records`** - Create multiple records\n- **`bulk_update_records`** - Update multiple records\n\n### File Operations\n- **`upload_file`** - Upload files to file attachment fields\n- **`download_file`** - Download files from records\n\n### Reporting\n- **`run_report`** - Execute Quickbase reports\n\n## 📚 Usage Examples\n\n### Basic Record Query\n```\nQuery all customers from the Customers table\n```\n\n### Create a New Record\n```\nCreate a new customer record with name \"Acme Corp\" and status \"Active\"\n```\n\n### Upload a File\n```\nUpload invoice.pdf to the Documents field in record 123\n```\n\n## 🔒 Security\n\n- API tokens are handled securely and never logged\n- All file operations are sandboxed to the working directory\n- Supports field-level permissions and access controls\n\n## 📋 Requirements\n\n- Node.js 18 or higher\n- Valid Quickbase account with API access\n- Claude Desktop (for MCP integration)\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🔗 Links\n\n- [Quickbase API Documentation](https://developer.quickbase.com/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quickbase",
        "databases",
        "database",
        "quickbase data",
        "quickbase json",
        "quickbase interact"
      ],
      "category": "databases"
    },
    "daobataotie--mssql-mcp": {
      "owner": "daobataotie",
      "name": "mssql-mcp",
      "url": "https://github.com/daobataotie/mssql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/daobataotie.webp",
      "description": "Provides capabilities for database interaction, allowing execution of SQL queries, data analysis, and automatic generation of business insight memos.",
      "stars": 38,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-01T03:56:57Z",
      "readme_content": "# MSSQL MCP Server\n\n[English](/README_en.md) | [中文](/README_zh.md) \n\n## Overview\n\nMSSQL MCP Server,  provides database interaction and business intelligence capabilities. This server enables running SQL queries, analyzing business data, and automatically generating business insight memos.  \nRefer to the official website's SQLite for modifications to adapt to MSSQL\n\n## Components\n\n- `read_query`\n   - Execute SELECT queries to read data from the database\n- `write_query`\n   - Execute INSERT, UPDATE, or DELETE queries\n- `create_table`\n   - Create new tables in the database\n- `list_tables`\n   - Get a list of all tables in the database\n- `describe-table`\n   - View schema information for a specific table\n- `append_insight`\n   - Add new business insights to the memo resource\n\n## Demo\nThe database table is as follows. The column names are not standardized, and AI will match them on its own. Errors during SQL execution will self correct.\n\n\n\nThe following is the demo.\n\n\n   \n## Operating environment\n\n- `Python 3.x`\n- `Packages`\n   - pyodbc>=4.0.39\n   - pydantic>=2.0.0\n   - mcp>=0.1.0 \n- `ODBC Driver 17 for SQL Server`\n\n## Usage \n\n### Install packages\n\n```bash\nCD /d ~/mssql-mcp  \npip install -r requirements.txt  \n```\n\n### config\n\n```bash\n#with server.py same folder create config.json，add：    \n{\n    \"database\": {\n        \"driver\": \"ODBC Driver 17 for SQL Server\",\n        \"server\": \"server ip\",\n        \"database\": \"db name\",\n        \"username\": \"username\",\n        \"password\": \"password\",\n        \"trusted_connection\": false\n    },\n    \"server\": {\n        \"name\": \"mssql-manager\",\n        \"version\": \"0.1.0\"\n    }\n}\n```\n\n### Claude Desktop 、 Windsurf\n\n```bash\n# add to claude_desktop_config.json. Note：use your path  \n{\n    \"mcpServers\": {\n        \"mssql\": {\n            \"command\": \"python\",\n            \"args\": [\n                # your path，e.g.：\"C:\\\\mssql-mcp\\\\src\\\\server.py\"\n                \"~/server.py\"\n            ]\n        }\n    }\n}\n```\n\n### Cursor\n\n```bash\n# Add according to the following diagram Cursor MCP. Note：use your path  \n```\n\n\nNote：The new version of cursor has also been changed to JSON configuration, please refer to the previous section\n\n### MCP Inspector\n\n```bash\n# Note：use your path  \nnpx -y @modelcontextprotocol/inspector python C:\\\\mssql-mcp\\\\src\\\\server.py\n```\n## Project Structure\n\n```\nmssql-mcp\n├── .git\n├── .gitignore\n├── LICENSE\n├── README.md\n├── README_en.md\n├── README_zh.md\n├── imgs\n│   ├── cursor_config.png\n│   ├── table.png\n│   └── demo.gif\n├── requirements.txt\n└── src\n    ├── __init__.py\n    └── server.py\n```\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mssql",
        "database",
        "daobataotie mssql",
        "database access",
        "databases secure"
      ],
      "category": "databases"
    },
    "datastax--astra-db-mcp": {
      "owner": "datastax",
      "name": "astra-db-mcp",
      "url": "https://github.com/datastax/astra-db-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/datastax.webp",
      "description": "Interact with Astra DB to perform operations such as creating, updating, and deleting collections and records directly from Large Language Models. Enables seamless data interaction and enhances applications with database capabilities.",
      "stars": 31,
      "forks": 21,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-30T17:45:14Z",
      "readme_content": "# Astra DB MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Astra DB. MCP extends the capabilities of Large Language Models (LLMs) by allowing them to interact with external systems as agents.\n\n## Prerequisites\n\nYou need to have a running Astra DB database. If you don't have one, you can create a free database [here](https://astra.datastax.com/register). From there, you can get two things you need:\n\n1. An Astra DB Application Token\n2. The Astra DB API Endpoint\n\nTo learn how to get these, please [read the getting started docs](https://docs.datastax.com/en/astra-db-serverless/api-reference/dataapiclient.html#set-environment-variables).\n\n## Adding to an MCP client\n\nHere's how you can add this server to your MCP client.\n\n### Claude Desktop\n\n![Claude Desktop](https://github.com/datastax/astra-db-mcp/raw/main/docs/img/claude-settings.png)\n\nTo add this to [Claude Desktop](https://claude.ai/download), go to Preferences -> Developer -> Edit Config and add this JSON blob to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"astra-db-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@datastax/astra-db-mcp\"],\n      \"env\": {\n        \"ASTRA_DB_APPLICATION_TOKEN\": \"your_astra_db_token\",\n        \"ASTRA_DB_API_ENDPOINT\": \"your_astra_db_endpoint\"\n      }\n    }\n  }\n}\n```\n\n**Optional Keyspace Configuration:**\nBy default, this server uses the keyspace configured in the underlying Astra DB library (typically `default_keyspace`). If you need to connect to a specific keyspace, you can add the `ASTRA_DB_KEYSPACE` variable to the `env` object above, like so:\n\n```json\n\"env\": {\n  \"ASTRA_DB_APPLICATION_TOKEN\": \"your_astra_db_token\",\n  \"ASTRA_DB_API_ENDPOINT\": \"your_astra_db_endpoint\",\n  \"ASTRA_DB_KEYSPACE\": \"your_desired_keyspace\"\n}\n```\n\n**Windows PowerShell Users:**\n`npx` is a batch command so modify the JSON as follows:\n\n```json\n  \"command\": \"cmd\",\n  \"args\": [\"/k\", \"npx\", \"-y\", \"@datastax/astra-db-mcp\"],\n```\n\n### Cursor\n\n![Cursor](https://github.com/datastax/astra-db-mcp/raw/main/docs/img/cursor-settings.png)\n\nTo add this to [Cursor](https://www.cursor.com/), go to Settings -> Cursor Settings -> MCP\n\nFrom there, you can add the server by clicking the \"+ Add New MCP Server\" button, where you should be brought to an `mcp.json` file.\n\n> **Tip**: there is a `~/.cursor/mcp.json` that represents your Global MCP settings, and a project-specific `.cursor/mcp.json` file\n> that is specific to the project. You probably want to install this MCP server into the project-specific file.\n\nAdd the same JSON as indiciated in the Claude Desktop instructions.\n\nAlternatively you may be presented with a wizard, where you can enter the following values (for Unix-based systems):\n\n- Name: Whatever you want\n- Type: Command\n- Command:\n\n```sh\nenv ASTRA_DB_APPLICATION_TOKEN=your_astra_db_token ASTRA_DB_API_ENDPOINT=your_astra_db_endpoint npx -y @datastax/astra-db-mcp\n```\n\n*Note: `ASTRA_DB_KEYSPACE` is optional. If omitted, the default keyspace configured in the Astra DB library will be used.*\n\nOnce added, your editor will be fully connected to your Astra DB database.\n\n## Available Tools\n\nThe server provides the following tools for interacting with Astra DB:\n\n- `GetCollections`: Get all collections in the database\n- `CreateCollection`: Create a new collection in the database\n- `UpdateCollection`: Update an existing collection in the database\n- `DeleteCollection`: Delete a collection from the database\n- `ListRecords`: List records from a collection in the database\n- `GetRecord`: Get a specific record from a collection by ID\n- `CreateRecord`: Create a new record in a collection\n- `UpdateRecord`: Update an existing record in a collection\n- `DeleteRecord`: Delete a record from a collection\n- `FindRecord`: Find records in a collection by field value\n- `BulkCreateRecords`: Create multiple records in a collection at once\n- `BulkUpdateRecords`: Update multiple records in a collection at once\n- `BulkDeleteRecords`: Delete multiple records from a collection at once\n- `OpenBrowser`: Open a web browser for authentication and setup\n- `HelpAddToClient`: Get assistance with adding Astra DB client to your MCP client\n- `EstimateDocumentCount`: Get estimate of the number of documents in a collection\n\n## Changelog\nAll notable changes to this project will be documented in [this file](./CHANGELOG.md).\nThe format is based on [Keep a Changelog](https://keepachangelog.com), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts tools.ts\n```\n## ❤️ Contributors\n\n[![astra-db-mcp contributors](https://contrib.rocks/image?repo=datastax/astra-db-mcp)](https://github.com/datastax/astra-db-mcp/graphs/contributors)\n\n## Badges\n[![Astra DB MCP Server on Glama.ai](https://glama.ai/mcp/servers/tigix0yf4b/badge)](https://glama.ai/mcp/servers/tigix0yf4b)\n\n[![MseeP.ai Security Assessment](https://mseep.net/pr/datastax-astra-db-mcp-badge.png)](https://mseep.ai/app/datastax-astra-db-mcp)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/932eb437-ab8e-4cf4-bbb5-1b3dbdb9f0aa)\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "datastax",
        "astra db",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "datastrato--mcp-server-gravitino": {
      "owner": "datastrato",
      "name": "mcp-server-gravitino",
      "url": "https://github.com/datastrato/mcp-server-gravitino",
      "imageUrl": "/freedevtools/mcp/pfp/datastrato.webp",
      "description": "Integrates with Apache Gravitino services to manage metadata including catalogs, schemas, tables, tags, and user roles through an intuitive interface. Facilitates optimized metadata retrieval and management, enhancing data governance workflows.",
      "stars": 19,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-06T07:38:38Z",
      "readme_content": "# MCP Server for Apache Gravitino\n\n[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n\nMCP server providing Gravitino APIs - A FastMCP integration for Apache Gravitino services.\n\n## Features\n\n* Seamless integration with [FastMCP](https://github.com/jlowin/fastmcp) for Gravitino APIs\n* Simplified interface for metadata interaction\n* Supports metadata operations for catalogs, schemas, tables, models, users, tags, and user-role management\n\n## Installation\n\nThis project uses [uv](https://github.com/astral-sh/uv) as the dependency and virtual environment management tool. Please ensure `uv` is installed on your system.\n\n1. Clone the repository:\n\n   ```bash\n   git clone git@github.com:datastrato/mcp-server-gravitino.git\n   ```\n\n2. Navigate into the project directory:\n\n   ```bash\n   cd mcp-server-gravitino\n   ```\n\n3. Create a virtual environment:\n\n   ```bash\n   uv venv\n   ```\n\n4. Activate the virtual environment:\n\n   ```bash\n   source .venv/bin/activate\n   ```\n\n5. Install dependencies:\n\n   ```bash\n   uv install\n   ```\n\n## Configuration\n\n### Common Configuration\n\nRegardless of the Authorization, the following environment variables need to be set:\n\n```bash\nGRAVITINO_METALAKE=<YOUR_METALAKE> # default: \"metalake_demo\"\nGRAVITINO_URI=<YOUR_GRAVITINO_URI>\n```\n\n* `GRAVITINO_URI`: The base URL of your Gravitino server.\n* `GRAVITINO_METALAKE`: The name of the metakube to use.\n\n### Authorization\n\n`mcp-server-gravitino` supports both token-based and basic authentication methods. These mechanisms allow secure access to MCP tools and prompts and are suitable for integration with external systems.\n\n#### Token Authentication\n\nSet the following environment variables:\n\n```bash\nGRAVITINO_JWT_TOKEN=<YOUR_GRAVITINO_JWT_TOKEN>\n```\n\n`GRAVITINO_JWT_TOKEN`: The JWT token for authentication.\n\n#### Basic Authentication\n\nAlternatively, you can use basic authentication:\n\n```bash\nGRAVITINO_USERNAME=<YOUR_GRAVITINO_USERNAME>\nGRAVITINO_PASSWORD=<YOUR_GRAVITINO_PASSWORD>\n```\n\n* `GRAVITINO_USERNAME`: The username for Gravitino authentication.\n* `GRAVITINO_PASSWORD`: The corresponding password.\n\n### Tool Activation\n\nTool activation is currently based on method names (e.g., `get_list_of_table`). You can specify which tools to activate by setting the optional environment variable `GRAVITINO_ACTIVE_TOOLS`. The default value is `*`, which activates all tools. If just want to activate `get_list_of_roles` tool, you can set the environment variable as follows:\n\n```bash\nGRAVITINO_ACTIVE_TOOLS=get_list_of_roles\n```\n\n## Usage\n\nTo launch the Gravitino MCP Server, run the following command:\n\n```bash\nuv \\\n--directory /path/to/mcp-gravitino \\\nrun \\\n--with fastmcp \\\n--with httpx \\\n--with mcp-server-gravitino \\\npython -m mcp_server_gravitino.server\n```\n\nThe meaning of each argument is as follows:\n\n| Argument                                | Description                                                             |\n| --------------------------------------- | ----------------------------------------------------------------------- |\n| `uv`                                    | Launches the [UV](https://github.com/astral-sh/uv) CLI tool             |\n| `--directory /path/to/mcp-gravitino`    | Specifies the working project directory with `pyproject.toml`           |\n| `run`                                   | Indicates that a command will be executed in the managed environment    |\n| `--with fastmcp`                        | Adds `fastmcp` to the runtime environment without altering project deps |\n| `--with httpx`                          | Adds `httpx` dependency for async HTTP functionality                    |\n| `--with mcp-server-gravitino`           | Adds the local module as a runtime dependency                           |\n| `python -m mcp_server_gravitino.server` | Starts the MCP server using the package's entry module                  |\n\n### Goose Client Example\n\nExample configuration to run the server using Goose:\n\n```json\n{\n  \"mcpServers\": {\n    \"Gravitino\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/user/workspace/mcp-server-gravitino\",\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"httpx\",\n        \"--with\",\n        \"mcp-server-gravitino\",\n        \"python\",\n        \"-m\",\n        \"mcp_server_gravitino.server\"\n      ],\n      \"env\": {\n        \"GRAVITINO_URI\": \"http://localhost:8090\",\n        \"GRAVITINO_USERNAME\": \"admin\",\n        \"GRAVITINO_PASSWORD\": \"admin\",\n        \"GRAVITINO_METALAKE\": \"metalake_demo\"\n      }\n    }\n  }\n}\n```\n\n## Tool List\n\n`mcp-server-gravitino` does not expose all Gravitino APIs, but provides a selected set of optimized tools:\n\n### Table Tools\n\n* `get_list_of_catalogs`: Retrieve a list of catalogs\n* `get_list_of_schemas`: Retrieve a list of schemas\n* `get_list_of_tables`: Retrieve a paginated list of tables\n* `get_table_by_fqn`: Fetch detailed information for a specific table\n* `get_table_columns_by_fqn`: Retrieve column information for a table\n\n### Tag Tools\n\n* `get_list_of_tags`: Retrieve all tags\n* `associate_tag_to_entity`: Attach a tag to a table or column\n* `list_objects_by_tag`: List objects associated with a specific tag\n\n### User Role Tools\n\n* `get_list_of_roles`: Retrieve all roles\n* `get_list_of_users`: Retrieve all users\n* `grant_role_to_user`: Assign a role to a user\n* `revoke_role_from_user`: Revoke a user's role\n\n### Model Tools\n\n* `get_list_of_models`: Retrieve a list of models\n* `get_list_of_model_versions_by_fqn`: Get versions of a model by fully qualified name\n\nEach tool is designed to return concise and relevant metadata to stay within LLM token limits while maintaining semantic integrity.\n\n## License\n\nThis project is licensed under the [Apache License Version 2.0](LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "gravitino",
        "server gravitino",
        "gravitino services",
        "databases secure"
      ],
      "category": "databases"
    },
    "datawiz168--mcp-snowflake-service": {
      "owner": "datawiz168",
      "name": "mcp-snowflake-service",
      "url": "https://github.com/datawiz168/mcp-snowflake-service",
      "imageUrl": "/freedevtools/mcp/pfp/datawiz168.webp",
      "description": "Controls database access rights using database users, enabling seamless execution of SQL queries on Snowflake databases. Manages database connections and handles query results and errors securely, ensuring robust error handling and automatic connection management.",
      "stars": 42,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:31:59Z",
      "readme_content": "A Model Context Protocol (MCP) server that provides Claude access to Snowflake databases. / 一个为 Claude 提供 Snowflake 数据库访问能力的 MCP (模型上下文协议) 服务器。\n\n![GitHub Stars](https://img.shields.io/github/stars/datawiz168/mcp-snowflake-service?style=social)\n[![Smithery Badge](https://smithery.ai/badge/@datawiz168/mcp-service-snowflake)](https://smithery.ai/server/@datawiz168/mcp-service-snowflake)\n\nThis server implements the Model Context Protocol to allow Claude to:\n- Execute SQL queries on Snowflake databases\n- Automatically handle database connection lifecycle (connect, reconnect on timeout, close)\n- Handle query results and errors\n- Perform database operations safely\n\n此服务器实现了模型上下文协议，使 Claude 能够：\n- 在 Snowflake 数据库上执行 SQL 查询\n- 自动管理数据库连接生命周期（连接创建、超时重连、连接关闭）\n- 处理查询结果和错误\n- 安全地执行数据库操作\n\n## Installation / 安装\n\n### Installing via Smithery\n\nTo install mcp-service-snowflake for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@datawiz168/mcp-service-snowflake):\n\n```bash\nnpx -y @smithery/cli install @datawiz168/mcp-service-snowflake --client claude\n```\n\n### Manual Installation\n1. Clone this repository / 克隆此仓库\n```bash\ngit clone https://github.com/datawiz168/mcp-snowflake-service.git\n```\n\n2. Install dependencies / 安装依赖\n```bash\npip install -r requirements.txt\n```\n\n## Configuration / 配置说明\n\n### MCP Client Configuration Example / MCP 客户端配置示例\n\nAdd the following configuration to `claude_desktop_config.json` / 在 `claude_desktop_config.json` 中添加配置:\n\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"C:\\\\Users\\\\K\\\\anaconda3\\\\envs\\\\regular310\\\\python.exe\",\n      \"args\": [\"D:\\\\tools\\\\mcp-snowflake\\\\server.py\"]\n    }\n  }\n}\n```\n\nConfiguration parameters / 配置参数说明:\n- `command`: Full path to your Python interpreter. Please modify this according to your Python installation location. / Python 解释器的完整路径，请根据您的 Python 安装位置进行修改。\n- `args`: Full path to the server script. Please modify this according to where you cloned the repository. / 服务器脚本的完整路径，请根据您克隆仓库的位置进行修改。\n\nExample paths for different operating systems / 不同操作系统的路径示例:\n\nWindows:\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\anaconda3\\\\python.exe\",\n      \"args\": [\"C:\\\\Path\\\\To\\\\mcp-snowflake\\\\server.py\"]\n    }\n  }\n}\n```\n\nMacOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"/usr/bin/python3\",\n      \"args\": [\"/path/to/mcp-snowflake/server.py\"]\n    }\n  }\n}\n```\n\n### Snowflake Configuration / Snowflake 配置\n\nCreate a `.env` file in the project root directory and add the following configuration / 在项目根目录下创建 `.env` 文件，添加以下配置：\n\n```env\nSNOWFLAKE_USER=your_username      # Your username / 您的用户名\nSNOWFLAKE_PASSWORD=your_password  # Your password / 您的密码\nSNOWFLAKE_ACCOUNT=NRB18479.US-WEST-2    # Example: NRB18479.US-WEST-2 / 示例: NRB18479.US-WEST-2\nSNOWFLAKE_DATABASE=your_database  # Your database / 您的数据库\nSNOWFLAKE_WAREHOUSE=your_warehouse # Your warehouse / 您的数据仓库\n```\n\n## Connection Management / 连接管理\n\nThe server provides automatic connection management features / 服务器提供以下自动连接管理功能：\n\n- Automatic connection initialization / 自动初始化连接\n  - Creates connection when first query is received / 首次查询时自动创建连接\n  - Validates connection parameters / 验证连接参数\n\n- Connection maintenance / 连接维护\n  - Keeps track of connection state / 跟踪连接状态\n  - Handles connection timeouts / 处理连接超时\n  - Automatically reconnects if connection is lost / 连接断开时自动重连\n\n- Connection cleanup / 连接清理\n  - Properly closes connections when server stops / 服务器停止时正确关闭连接\n  - Releases resources appropriately / 适当释放资源\n\n## Usage / 使用说明\n\nThe server will start automatically with the Claude Desktop client. No manual startup is required. Once the server is running, Claude will be able to execute Snowflake queries. / 服务器会随 Claude Desktop 客户端自动启动，无需手动运行。服务器启动后，Claude 将能够执行 Snowflake 查询。\n\nFor development testing, you can start the server manually using / 如果需要单独启动服务器进行测试，可以使用以下命令:\n\n```bash\npython server.py\n```\n\nNote: Manual server startup is not needed for normal use. The Claude Desktop client will automatically manage server startup and shutdown based on the configuration. / 注意：正常使用时无需手动启动服务器，Claude Desktop 客户端会根据配置自动管理服务器的启动和停止。\n\n## Features / 功能\n\n- Secure Snowflake database access / 安全的 Snowflake 数据库访问\n- Robust error handling and reporting / 健壮的错误处理和报告机制\n- Automatic connection management / 自动连接管理\n- Query execution and result processing / 查询执行和结果处理\n\n## Development / 开发\n\nTo contribute code or report issues / 要贡献代码或报告问题，请:\n\n1. Fork this repository / Fork 此仓库\n2. Create your feature branch / 创建您的功能分支 (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes / 提交您的更改 (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch / 推送到分支 (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request / 开启一个 Pull Request\n\n## notes / 备注\nmcp‑server‑snowflake controls database access rights precisely by way of database users. If you only need to read data, just assign a user with read‑only database permissions./\nmcp-server-snowflake 控制数据库访问权限，是通过数据库用户来精准实现的。如果你只想读取数据，给个只有读取数据库权限的用户就行了。\n\n## License / 许可\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThis project is licensed under the [MIT License](LICENSE).\n\n此项目采用 [MIT 许可证](LICENSE)。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "datawiz168",
        "snowflake databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "dave-wind--mysql-mcp-server": {
      "owner": "dave-wind",
      "name": "mysql-mcp-server",
      "url": "https://github.com/dave-wind/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dave-wind.webp",
      "description": "Enables AI models to securely access and inspect MySQL database schemas while executing read-only SQL queries. Ensures data integrity through validation and read-only transactions, facilitating database exploration without modification risks.",
      "stars": 9,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-25T22:43:14Z",
      "readme_content": "# @davewind/mysql-mcp-server\n\n\nA Model Context Protocol server that provides read-only access to Mysql databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n### Key Features\n1.Read-Only Database Access: Enforces read-only operations through SQL validation and READ ONLY transactions  \n2.Schema Discovery: Automatically identifies and exposes database table structures  \n3.SQL Query Execution: Provides a query tool that accepts and executes SELECT statements  \n4.Model Context Protocol Compliance: Implements the MCP specification for seamless integration with compatible LLMs  \n5.Simple Configuration: Easy setup with minimal configuration required  \n\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas** (`mysql://user:password@localhost:3306/database`)\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n\n\n### Install\n```bash\nnpm install @davewind/mysql-mcp-server -g\n```\n\n## Configuration\nMCP settings configuration file:\n\n> recommended use\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@davewind/mysql-mcp-server\", \"mysql://user:password@localhost:port/database\"],\n    }\n  }\n}\n```\n\n\n\n### Test\n>  Replace mysql://user:password@localhost:port/  and npm run inspector\n```js\n  \"scripts\": {\n    \"inspector\": \"npx @modelcontextprotocol/inspector@0.10.2 build/index.js mysql://user:password@localhost:port/database\n  }\n```\n\n\n### Env\n\n```js\n\nnode v18 +\n\n```\n\n### System Architecture\n> The MySQL MCP Server acts as an intermediary between LLMs and MySQL databases, processing requests according to the Model Context Protocol.\n<p>\n  \n</p>\n\n\n### Component Interaction\n<p>\n  \n</p>\n\n\n### Component Interaction\n<p>\n  \n</p>\n\n### Security Model\n> The MySQL MCP Server implements a strict security model to ensure that database access is read-only.\n<p>\n  \n</p>\n\nSecurity measures include:\n\n1.SQL query validation to allow only SELECT statements\n2.Execution of all queries within READ ONLY transactions\n3.No support for data modification operations (INSERT, UPDATE, DELETE, etc.)\n4. No support for database schema modification (CREATE, ALTER, DROP, etc.)\n\n\n### Integration with LLMs\n> The MySQL MCP Server is designed to work with any LLM system that supports the Model Context Protocol. It communicates through JSON-RPC over stdio, following the MCP specification.\n<p>\n  \n</p>\n\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "davesbits--supabase-mcp-server": {
      "owner": "davesbits",
      "name": "supabase-mcp-server",
      "url": "https://github.com/davesbits/supabase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/davesbits.webp",
      "description": "Connects to SQL databases for executing queries, managing data, and accessing user management APIs with built-in safety controls.",
      "stars": 1,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-14T17:59:44Z",
      "readme_content": "# Query MCP (Supabase MCP Server)\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/4a363bcd-7c15-47fa-a72a-d159916517f7\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" />\n    <img alt=\"Supabase\" src=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" height=\"40\" />\n  </picture>\n  &nbsp;&nbsp;\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/38db1bcd-50df-4a49-a106-1b5afd924cb2\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" />\n    <img alt=\"MCP\" src=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" height=\"40\" />\n  </picture>\n</p>\n\n<p align=\"center\">\n  <strong>Enable your favorite IDE to safely execute SQL queries, manage your database end-to-end, access Management API, and handle user authentication with built-in safety controls.</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://thequery.dev\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/supabase-mcp-server/\"><img src=\"https://img.shields.io/pypi/v/supabase-mcp-server.svg\" alt=\"PyPI version\" /></a>\n  <a href=\"https://github.com/alexander-zuev/supabase-mcp-server/actions\"><img src=\"https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg\" alt=\"CI Status\" /></a>\n  <a href=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server\"><img src=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server/branch/main/graph/badge.svg\" alt=\"Code Coverage\" /></a>\n  <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.12%2B-blue.svg\" alt=\"Python 3.12+\" /></a>\n  <a href=\"https://github.com/astral-sh/uv\"><img src=\"https://img.shields.io/badge/uv-package%20manager-blueviolet\" alt=\"uv package manager\" /></a>\n  <a href=\"https://pepy.tech/project/supabase-mcp-server\"><img src=\"https://static.pepy.tech/badge/supabase-mcp-server\" alt=\"PyPI Downloads\" /></a>\n  <a href=\"https://smithery.ai/server/@alexander-zuev/supabase-mcp-server\"><img src=\"https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server\" alt=\"Smithery.ai Downloads\" /></a>\n  <a href=\"https://modelcontextprotocol.io/introduction\"><img src=\"https://img.shields.io/badge/MCP-Server-orange\" alt=\"MCP Server\" /></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n</p>\n\n\n## 🎉 The Future of Supabase MCP Server -> Query MCP\n\n**I'm thrilled to announce that Supabase MCP Server is evolving into [thequery.dev](https://thequery.dev)!**\n\nWhile I have big plans for the future, I want to make these commitments super clear:\n- **The core tool will stay free forever** - free & open-source software is how I got into coding\n- **Premium features will be added on top** - enhancing capabilities without limiting existing functionality\n- **First 2,000 early adopters will get special perks** - join early for an exclusive treat!\n\n**🚀 BIG v4 Launch Coming Soon!**\n\n[**👉 Join Early Access at thequery.dev**](https://thequery.dev)\n\n## Table of contents\n<p align=\"center\">\n  <a href=\"#getting-started\">Getting started</a> •\n  <a href=\"#feature-overview\">Feature overview</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#changelog\">Changelog</a>\n</p>\n\n## ✨ Key features\n- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol\n- 🔐 Control read-only and read-write modes of SQL query execution\n- 🔍 Runtime SQL query validation with risk level assessment\n- 🛡️ Three-tier safety system for SQL operations: safe, write, and destructive\n- 🔄 Robust transaction handling for both direct and pooled database connections\n- 📝 Automatic versioning of database schema changes\n- 💻 Manage your Supabase projects with Supabase Management API\n- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK\n- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively\n- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)\n\n\n## Getting Started\n\n### Prerequisites\nInstalling the server requires the following on your system:\n- Python 3.12+\n\nIf you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).\n\n### PostgreSQL Installation\nPostgreSQL installation is no longer required for the MCP server itself, as it now uses asyncpg which doesn't depend on PostgreSQL development libraries.\n\nHowever, you'll still need PostgreSQL if you're running a local Supabase instance:\n\n**MacOS**\n```bash\nbrew install postgresql@16\n```\n\n**Windows**\n  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/\n  - Ensure \"PostgreSQL Server\" and \"Command Line Tools\" are selected during installation\n\n### Step 1. Installation\n\nSince v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:\n\n```bash\n# if pipx is installed (recommended)\npipx install supabase-mcp-server\n\n# if uv is installed\nuv pip install supabase-mcp-server\n```\n\n`pipx` is recommended because it creates isolated environments for each package.\n\nYou can also install the server manually by cloning the repository and running `pipx install -e .` from the root directory.\n\n#### Installing from source\nIf you would like to install from source, for example for local development:\n```bash\nuv venv\n# On Mac\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n# Install package in editable mode\nuv pip install -e .\n```\n\n#### Installing via Smithery.ai\n\nYou can find the full instructions on how to use Smithery.ai to connect to this MCP server [here](https://smithery.ai/server/@alexander-zuev/supabase-mcp-server).\n\n\n### Step 2. Configuration\n\nThe Supabase MCP server requires configuration to connect to your Supabase database, access the Management API, and use the Auth Admin SDK. This section explains all available configuration options and how to set them up.\n\n#### Environment Variables\n\nThe server uses the following environment variables:\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `SUPABASE_PROJECT_REF` | Yes | `127.0.0.1:54322` | Your Supabase project reference ID (or local host:port) |\n| `SUPABASE_DB_PASSWORD` | Yes | `postgres` | Your database password |\n| `SUPABASE_REGION` | Yes* | `us-east-1` | AWS region where your Supabase project is hosted |\n| `SUPABASE_ACCESS_TOKEN` | No | None | Personal access token for Supabase Management API |\n| `SUPABASE_SERVICE_ROLE_KEY` | No | None | Service role key for Auth Admin SDK |\n\n> **Note**: The default values are configured for local Supabase development. For remote Supabase projects, you must provide your own values for `SUPABASE_PROJECT_REF` and `SUPABASE_DB_PASSWORD`.\n\n> 🚨 **CRITICAL CONFIGURATION NOTE**: For remote Supabase projects, you MUST specify the correct region where your project is hosted using `SUPABASE_REGION`. If you encounter a \"Tenant or user not found\" error, this is almost certainly because your region setting doesn't match your project's actual region. You can find your project's region in the Supabase dashboard under Project Settings.\n\n#### Connection Types\n\n##### Database Connection\n- The server connects to your Supabase PostgreSQL database using the transaction pooler endpoint\n- Local development uses a direct connection to `127.0.0.1:54322`\n- Remote projects use the format: `postgresql://postgres.[project_ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres`\n\n> ⚠️ **Important**: Session pooling connections are not supported. The server exclusively uses transaction pooling for better compatibility with the MCP server architecture.\n\n##### Management API Connection\n- Requires `SUPABASE_ACCESS_TOKEN` to be set\n- Connects to the Supabase Management API at `https://api.supabase.com`\n- Only works with remote Supabase projects (not local development)\n\n##### Auth Admin SDK Connection\n- Requires `SUPABASE_SERVICE_ROLE_KEY` to be set\n- For local development, connects to `http://127.0.0.1:54321`\n- For remote projects, connects to `https://[project_ref].supabase.co`\n\n#### Configuration Methods\n\nThe server looks for configuration in this order (highest to lowest priority):\n\n1. **Environment Variables**: Values set directly in your environment\n2. **Local `.env` File**: A `.env` file in your current working directory (only works when running from source)\n3. **Global Config File**:\n   - Windows: `%APPDATA%\\supabase-mcp\\.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n4. **Default Settings**: Local development defaults (if no other config is found)\n\n> ⚠️ **Important**: When using the package installed via pipx or uv, local `.env` files in your project directory are **not** detected. You must use either environment variables or the global config file.\n\n#### Setting Up Configuration\n\n##### Option 1: Client-Specific Configuration (Recommended)\n\nSet environment variables directly in your MCP client configuration (see client-specific setup instructions in Step 3). Most MCP clients support this approach, which keeps your configuration with your client settings.\n\n##### Option 2: Global Configuration\n\nCreate a global `.env` configuration file that will be used for all MCP server instances:\n\n```bash\n# Create config directory\n# On macOS/Linux\nmkdir -p ~/.config/supabase-mcp\n# On Windows (PowerShell)\nmkdir -Force \"$env:APPDATA\\supabase-mcp\"\n\n# Create and edit .env file\n# On macOS/Linux\nnano ~/.config/supabase-mcp/.env\n# On Windows (PowerShell)\nnotepad \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nAdd your configuration values to the file:\n\n```\nSUPABASE_PROJECT_REF=your-project-ref\nSUPABASE_DB_PASSWORD=your-db-password\nSUPABASE_REGION=us-east-1\nSUPABASE_ACCESS_TOKEN=your-access-token\nSUPABASE_SERVICE_ROLE_KEY=your-service-role-key\n```\n\n##### Option 3: Project-Specific Configuration (Source Installation Only)\n\nIf you're running the server from source (not via package), you can create a `.env` file in your project directory with the same format as above.\n\n#### Finding Your Supabase Project Information\n\n- **Project Reference**: Found in your Supabase project URL: `https://supabase.com/dashboard/project/<project-ref>`\n- **Database Password**: Set during project creation or found in Project Settings → Database\n- **Access Token**: Generate at https://supabase.com/dashboard/account/tokens\n- **Service Role Key**: Found in Project Settings → API → Project API keys\n\n#### Supported Regions\n\nThe server supports all Supabase regions:\n\n- `us-west-1` - West US (North California)\n- `us-east-1` - East US (North Virginia) - default\n- `us-east-2` - East US (Ohio)\n- `ca-central-1` - Canada (Central)\n- `eu-west-1` - West EU (Ireland)\n- `eu-west-2` - West Europe (London)\n- `eu-west-3` - West EU (Paris)\n- `eu-central-1` - Central EU (Frankfurt)\n- `eu-central-2` - Central Europe (Zurich)\n- `eu-north-1` - North EU (Stockholm)\n- `ap-south-1` - South Asia (Mumbai)\n- `ap-southeast-1` - Southeast Asia (Singapore)\n- `ap-northeast-1` - Northeast Asia (Tokyo)\n- `ap-northeast-2` - Northeast Asia (Seoul)\n- `ap-southeast-2` - Oceania (Sydney)\n- `sa-east-1` - South America (São Paulo)\n\n#### Limitations\n\n- **No Self-Hosted Support**: The server only supports official Supabase.com hosted projects and local development\n- **No Connection String Support**: Custom connection strings are not supported\n- **No Session Pooling**: Only transaction pooling is supported for database connections\n- **API and SDK Features**: Management API and Auth Admin SDK features only work with remote Supabase projects, not local development\n\n### Step 3. Usage\n\nIn general, any MCP client that supports `stdio` protocol should work with this MCP server. This server was explicitly tested to work with:\n- Cursor\n- Windsurf\n- Cline\n- Claude Desktop\n\nAdditionally, you can also use smithery.ai to install this server a number of clients, including the ones above.\n\nFollow the guides below to install this MCP server in your client.\n\n#### Cursor\nGo to Settings -> Features -> MCP Servers and add a new server with this configuration:\n```bash\n# can be set to any name\nname: supabase\ntype: command\n# if you installed with pipx\ncommand: supabase-mcp-server\n# if you installed with uv\ncommand: uv run supabase-mcp-server\n# if the above doesn't work, use the full path (recommended)\ncommand: /full/path/to/supabase-mcp-server  # Find with 'which supabase-mcp-server' (macOS/Linux) or 'where supabase-mcp-server' (Windows)\n```\n\nIf configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.\n![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)\n\n#### Windsurf\nGo to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n          \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n        }\n      }\n    }\n}\n```\nIf configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)\n\n#### Claude Desktop\nClaude Desktop also supports MCP servers through a JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Claude Desktop:\n   - Open Claude Desktop\n   - Go to Settings → Developer -> Edit Config MCP Servers\n   - Add a new configuration with the following JSON:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\n> ⚠️ **Important**: Unlike Windsurf and Cursor, Claude Desktop requires the **full absolute path** to the executable. Using just the command name (`supabase-mcp-server`) will result in a \"spawn ENOENT\" error.\n\nIf configuration is correct, you should see the Supabase MCP server listed as available in Claude Desktop.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/500bcd40-6245-40a7-b23b-189827ed2923)\n\n#### Cline\nCline also supports MCP servers through a similar JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Cline:\n   - Open Cline in VS Code\n   - Click on the \"MCP Servers\" tab in the Cline sidebar\n   - Click \"Configure MCP Servers\"\n   - This will open the `cline_mcp_settings.json` file\n   - Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\nIf configuration is correct, you should see a green indicator next to the Supabase MCP server in the Cline MCP Servers list, and a message confirming \"supabase MCP server connected\" at the bottom of the panel.\n\n![How successful configuration in Cline looks like](https://github.com/user-attachments/assets/6c4446ad-7a58-44c6-bf12-6c82222bbe59)\n\n### Troubleshooting\n\nHere are some tips & tricks that might help you:\n- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.\n- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.\n- **\"No tools found\" error** - If you see \"Client closed - no tools available\" in Cursor despite the package being installed:\n  - Find the full path to the executable by running `which supabase-mcp-server` (macOS/Linux) or `where supabase-mcp-server` (Windows)\n  - Use the full path in your MCP server configuration instead of just `supabase-mcp-server`\n  - For example: `/Users/username/.local/bin/supabase-mcp-server` or `C:\\Users\\username\\.local\\bin\\supabase-mcp-server.exe`\n- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\\supabase-mcp\\.env` on Windows).\n- **Accessing logs** - The MCP server writes detailed logs to a file:\n  - Log file location:\n    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`\n    - Windows: `%USERPROFILE%\\.local\\share\\supabase-mcp\\mcp_server.log`\n  - Logs include connection status, configuration details, and operation results\n  - View logs using any text editor or terminal commands:\n    ```bash\n    # On macOS/Linux\n    cat ~/.local/share/supabase-mcp/mcp_server.log\n\n    # On Windows (PowerShell)\n    Get-Content \"$env:USERPROFILE\\.local\\share\\supabase-mcp\\mcp_server.log\"\n    ```\n\nIf you are stuck or any of the instructions above are incorrect, please raise an issue.\n\n### MCP Inspector\nA super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.\n> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the coming release.\n\n## Feature Overview\n\n### Database query tools\n\nSince v0.3+ server provides comprehensive database management capabilities with built-in safety controls:\n\n- **SQL Query Execution**: Execute PostgreSQL queries with risk assessment\n  - **Three-tier safety system**:\n    - `safe`: Read-only operations (SELECT) - always allowed\n    - `write`: Data modifications (INSERT, UPDATE, DELETE) - require unsafe mode\n    - `destructive`: Schema changes (DROP, CREATE) - require unsafe mode + confirmation\n\n- **SQL Parsing and Validation**:\n  - Uses PostgreSQL's parser (pglast) for accurate analysis and provides clear feedback on safety requirements\n\n- **Automatic Migration Versioning**:\n  - Database-altering operations operations are automatically versioned\n  - Generates descriptive names based on operation type and target\n\n\n- **Safety Controls**:\n  - Default SAFE mode allows only read-only operations\n  - All statements run in transaction mode via `asyncpg`\n  - 2-step confirmation for high-risk operations\n\n- **Available Tools**:\n  - `get_schemas`: Lists schemas with sizes and table counts\n  - `get_tables`: Lists tables, foreign tables, and views with metadata\n  - `get_table_schema`: Gets detailed table structure (columns, keys, relationships)\n  - `execute_postgresql`: Executes SQL statements against your database\n  - `confirm_destructive_operation`: Executes high-risk operations after confirmation\n  - `retrieve_migrations`: Gets migrations with filtering and pagination options\n  - `live_dangerously`: Toggles between safe and unsafe modes\n\n### Management API tools\n\nSince v0.3.0 server provides secure access to the Supabase Management API with built-in safety controls:\n\n- **Available Tools**:\n  - `send_management_api_request`: Sends arbitrary requests to Supabase Management API with auto-injection of project ref\n  - `get_management_api_spec`: Gets the enriched API specification with safety information\n    - Supports multiple query modes: by domain, by specific path/method, or all paths\n    - Includes risk assessment information for each endpoint\n    - Provides detailed parameter requirements and response formats\n    - Helps LLMs understand the full capabilities of the Supabase Management API\n  - `get_management_api_safety_rules`: Gets all safety rules with human-readable explanations\n  - `live_dangerously`: Toggles between safe and unsafe operation modes\n\n- **Safety Controls**:\n  - Uses the same safety manager as database operations for consistent risk management\n  - Operations categorized by risk level:\n    - `safe`: Read-only operations (GET) - always allowed\n    - `unsafe`: State-changing operations (POST, PUT, PATCH, DELETE) - require unsafe mode\n    - `blocked`: Destructive operations (delete project, etc.) - never allowed\n  - Default safe mode prevents accidental state changes\n  - Path-based pattern matching for precise safety rules\n\n**Note**: Management API tools only work with remote Supabase instances and are not compatible with local Supabase development setups.\n\n### Auth Admin tools\n\nI was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.\n\nSince v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:\n  - Includes the following tools:\n    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods\n    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling\n  - Supported methods:\n    - `get_user_by_id`: Retrieve a user by their ID\n    - `list_users`: List all users with pagination\n    - `create_user`: Create a new user\n    - `delete_user`: Delete a user by their ID\n    - `invite_user_by_email`: Send an invite link to a user's email\n    - `generate_link`: Generate an email link for various authentication purposes\n    - `update_user_by_id`: Update user attributes by ID\n    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)\n\n#### Why use Auth Admin SDK instead of raw SQL queries?\n\nThe Auth Admin SDK provides several key advantages over direct SQL manipulation:\n- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)\n- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas\n- **Simplicity**: Offers clear methods with proper validation and error handling\n\n  - Response format:\n    - All methods return structured Python objects instead of raw dictionaries\n    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user[\"id\"]`)\n  - Edge cases and limitations:\n    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors\n    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project\n    - Link types: When generating links, different link types have different requirements:\n      - `signup` links don't require the user to exist\n      - `magiclink` and `recovery` links require the user to already exist in the system\n    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface\n    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK\n\n### Logs & Analytics\n\nThe server provides access to Supabase logs and analytics data, making it easier to monitor and troubleshoot your applications:\n\n- **Available Tool**: `retrieve_logs` - Access logs from any Supabase service\n\n- **Log Collections**:\n  - `postgres`: Database server logs\n  - `api_gateway`: API gateway requests\n  - `auth`: Authentication events\n  - `postgrest`: RESTful API service logs\n  - `pooler`: Connection pooling logs\n  - `storage`: Object storage operations\n  - `realtime`: WebSocket subscription logs\n  - `edge_functions`: Serverless function executions\n  - `cron`: Scheduled job logs\n  - `pgbouncer`: Connection pooler logs\n\n- **Features**: Filter by time, search text, apply field filters, or use custom SQL queries\n\nSimplifies debugging across your Supabase stack without switching between interfaces or writing complex queries.\n\n### Automatic Versioning of Database Changes\n\n\"With great power comes great responsibility.\" While `execute_postgresql` tool coupled with aptly named `live_dangerously` tool provide a powerful and simple way to manage your Supabase database, it also means that dropping a table or modifying one is one chat message away. In order to reduce the risk of irreversible changes, since v0.3.8 the server supports:\n- automatic creation of migration scripts for all write & destructive sql operations executed on the database\n- improved safety mode of query execution, in which all queries are categorized in:\n  - `safe` type: always allowed. Includes all read-only ops.\n  - `write`type: requires `write` mode to be enabled by the user.\n  - `destructive` type: requires `write` mode to be enabled by the user AND a 2-step confirmation of query execution for clients that do not execute tools automatically.\n\n### Universal Safety Mode\nSince v0.3.8 Safety Mode has been standardized across all services (database, API, SDK) using a universal safety manager. This provides consistent risk management and a unified interface for controlling safety settings across the entire MCP server.\n\nAll operations (SQL queries, API requests, SDK methods) are categorized into risk levels:\n- `Low` risk: Read-only operations that don't modify data or structure (SELECT queries, GET API requests)\n- `Medium` risk: Write operations that modify data but not structure (INSERT/UPDATE/DELETE, most POST/PUT API requests)\n- `High` risk: Destructive operations that modify database structure or could cause data loss (DROP/TRUNCATE, DELETE API endpoints)\n- `Extreme` risk: Operations with severe consequences that are blocked entirely (deleting projects)\n\nSafety controls are applied based on risk level:\n- Low risk operations are always allowed\n- Medium risk operations require unsafe mode to be enabled\n- High risk operations require unsafe mode AND explicit confirmation\n- Extreme risk operations are never allowed\n\n#### How confirmation flow works\n\nAny high-risk operations (be it a postgresql or api request) will be blocked even in `unsafe` mode.\n![Every high-risk operation is blocked](https://github.com/user-attachments/assets/c0df79c2-a879-4b1f-a39d-250f9965c36a)\nYou will have to confirm and approve every high-risk operation explicitly in order for it to be executed.\n![Explicit approval is always required](https://github.com/user-attachments/assets/5cd7a308-ec2a-414e-abe2-ff2f3836dd8b)\n\n\n## Changelog\n\n- 📦 Simplified installation via package manager - ✅ (v0.2.0)\n- 🌎 Support for different Supabase regions - ✅ (v0.2.2)\n- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)\n- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)\n- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)\n- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)\n- 🔍 Stronger SQL query validation ✅ (v0.3.8)\n- 📝 Automatic versioning of database changes ✅ (v0.3.8)\n- 📖 Radically improved knowledge and tools of api spec ✅ (v0.3.8)\n- ✍️ Improved consistency of migration-related tools for a more organized database vcs ✅ (v0.3.10)\n\n\nFor a more detailed roadmap, please see this [discussion](https://github.com/alexander-zuev/supabase-mcp-server/discussions/46) on GitHub.\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)\n\n---\n\nEnjoy! ☺️",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase mcp",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "davewind--mysql-mcp-server": {
      "owner": "davewind",
      "name": "mysql-mcp-server",
      "url": "https://github.com/dave-wind/mysql-mcp-server",
      "imageUrl": "",
      "description": "friendly read-only mysql mcp server for cursor and n8n...",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "davidamom--snowflake-mcp": {
      "owner": "davidamom",
      "name": "snowflake-mcp",
      "url": "https://github.com/davidamom/snowflake-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/davidamom.webp",
      "description": "Connect and execute SQL queries on Snowflake databases while managing database connections and securely handling query results for AI applications.",
      "stars": 4,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T13:27:31Z",
      "readme_content": "# Snowflake MCP Service\n\nA Model Context Protocol (MCP) server that provides access to Snowflake databases for any MCP-compatible client.\n\n![GitHub repo](https://img.shields.io/badge/GitHub-snowflake--mcp-blue)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n\nThis server implements the Model Context Protocol to allow any MCP client to:\n- Execute SQL queries on Snowflake databases\n- Automatically handle database connection lifecycle (connect, reconnect on timeout, close)\n- Handle query results and errors\n- Perform database operations safely\n- Connect using either password or key pair authentication\n\n## Architecture Overview\n\n### What is MCP (Model Context Protocol)?\n\nMCP is a standard protocol that allows applications to communicate with AI models and external services. It enables AI models to access tools and data sources beyond their training data, expanding their capabilities through a standardized communication interface. Key features include:\n\n- Based on stdio communication (standard input/output)\n- Structured tool definition and discovery\n- Standardized tool call mechanism\n- Structured results transmission\n\n### System Components\n\nThe Snowflake-MCP server consists of several key components:\n\n1. **MCP Server** - Central component that implements the MCP protocol and handles client requests\n2. **Snowflake Connection Manager** - Manages database connections, including creation, maintenance, and cleanup\n3. **Query Processor** - Executes SQL queries on Snowflake and processes the results\n4. **Authentication Manager** - Handles different authentication methods (password or private key)\n\n\n\n### Communication Flow\n\nThe system works through the following communication flow:\n\n1. An MCP Client (such as Claude or other MCP-compatible application) sends a request to the MCP Server\n2. The MCP Server authenticates with Snowflake using credentials from the `.env` file\n3. The MCP Server executes SQL queries on Snowflake\n4. Snowflake returns results to the MCP Server\n5. The MCP Server formats and sends the results back to the MCP Client\n\n\n\nThis architecture allows for seamless integration between AI applications and Snowflake databases while maintaining security and efficient connection management.\n\n## Installation\n\n1. Clone this repository\n```bash\ngit clone https://github.com/davidamom/snowflake-mcp.git\n```\n\n2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n## Configuration\n\n### MCP Client Configuration Example\n\nBelow is an example configuration for Claude Desktop, but this server works with any MCP-compatible client. Each client may have its own configuration method:\n\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\path\\\\to\\\\python.exe\",\n      \"args\": [\"C:\\\\path\\\\to\\\\snowflake-mcp\\\\server.py\"]\n    }\n  }\n}\n```\n\nConfiguration parameters:\n- `command`: Full path to your Python interpreter. Please modify this according to your Python installation location.\n- `args`: Full path to the server script. Please modify this according to where you cloned the repository.\n\nExample paths for different operating systems:\n\nWindows:\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\anaconda3\\\\python.exe\",\n      \"args\": [\"C:\\\\Path\\\\To\\\\snowflake-mcp\\\\server.py\"]\n    }\n  }\n}\n```\n\nMacOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"/usr/bin/python3\",\n      \"args\": [\"/path/to/snowflake-mcp/server.py\"]\n    }\n  }\n}\n```\n\n### Snowflake Configuration\n\nCreate a `.env` file in the project root directory and add the following configuration:\n\n```env\n# Snowflake Configuration - Basic Info\nSNOWFLAKE_USER=your_username          # Your Snowflake username\nSNOWFLAKE_ACCOUNT=YourAccount.Region  # Example: MyOrg.US-WEST-2\nSNOWFLAKE_DATABASE=your_database      # Your database\nSNOWFLAKE_WAREHOUSE=your_warehouse    # Your warehouse\nSNOWFLAKE_ROLE=your_role              # Your role\n\n# Authentication - Choose one method\n```\n\n#### Authentication Options\n\nThis MCP server supports two authentication methods:\n\n1. **Password Authentication**\n   ```env\n   SNOWFLAKE_PASSWORD=your_password      # Your Snowflake password\n   ```\n\n2. **Key Pair Authentication**\n   ```env\n   SNOWFLAKE_PRIVATE_KEY_FILE=/path/to/rsa_key.p8     # Path to private key file \n   SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=your_passphrase   # Optional: passphrase if key is encrypted\n   ```\n\n   For key pair authentication, you must first set up key pair authentication with Snowflake:\n   - Generate a key pair and register the public key with Snowflake\n   - Store the private key file securely on your machine\n   - Provide the full path to the private key file in the configuration\n\n   For instructions on setting up key pair authentication, refer to [Snowflake documentation on key pair authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth).\n\nIf both authentication methods are configured, the server will prioritize key pair authentication.\n\n## Connection Management\n\nThe server provides automatic connection management features:\n\n- Automatic connection initialization\n  - Creates connection when first query is received\n  - Validates connection parameters\n\n- Connection maintenance\n  - Keeps track of connection state\n  - Handles connection timeouts\n  - Automatically reconnects if connection is lost\n\n- Connection cleanup\n  - Properly closes connections when server stops\n  - Releases resources appropriately\n\n## Usage\n\n### Standard Usage\n\nThe server will start automatically when configured with your MCP client. No manual startup is required in normal operation. Once the server is running, your MCP client will be able to execute Snowflake queries.\n\nFor development testing, you can start the server manually using:\n\n```bash\npython server.py\n```\n\nNote: Manual server startup is not needed for normal use. The MCP client will typically manage server startup and shutdown based on the configuration.\n\n### Docker Usage\n\nYou can also run the server using Docker. This method is recommended for production environments and ensures consistent execution across different platforms.\n\n1. Build the Docker image:\n```bash\ndocker build -t snowflake-mcp .\n```\n\n2. Configure your MCP client to use Docker. Example configuration:\n```json\n{\n  \"mcpServers\": {\n    \"snowflake-docker\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"snowflake-mcp\"\n      ],\n      \"env\": {\n        \"SNOWFLAKE_USER\": \"your_username\",\n        \"SNOWFLAKE_ACCOUNT\": \"your_account\",\n        \"SNOWFLAKE_DATABASE\": \"your_database\",\n        \"SNOWFLAKE_WAREHOUSE\": \"your_warehouse\",\n        \"SNOWFLAKE_PASSWORD\": \"your_password\",\n        \"SNOWFLAKE_ROLE\": \"your_role\"\n        \n      }\n    }\n  }\n}\n```\n\nNote: The Docker implementation uses stdio for communication, so no ports need to be exposed.\n\nIf using key pair authentication with Docker, you'll need to mount your private key file:\n```bash\ndocker run -i -v /path/to/your/key.p8:/app/rsa_key.p8:ro snowflake-mcp\n```\n\nAnd update your configuration accordingly:\n```json\n{\n  \"mcpServers\": {\n    \"Snowflake-Docker\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"-v\",\n        \"/path/to/your/key.p8:/app/rsa_key.p8:ro\",\n        //optional\n        \"-v\",\n        \"/path/to/export/dir/:/export/\"\n        \"snowflake-mcp\"\n      ],\n      \"env\": {\n        \"SNOWFLAKE_USER\": \"your_username\",\n        \"SNOWFLAKE_ACCOUNT\": \"your_account\",\n        \"SNOWFLAKE_DATABASE\": \"your_database\",\n        \"SNOWFLAKE_WAREHOUSE\": \"your_warehouse\",\n        \"SNOWFLAKE_ROLE\": \"your_role\",\n        \"SNOWFLAKE_PRIVATE_KEY_FILE\": \"path_for_your_private_key\",\n        \"SNOWFLAKE_PRIVATE_KEY_PASSPHRASE\": \"your_password_for_private_key\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- Secure Snowflake database access\n- Flexible authentication (password or key pair authentication)\n- Robust error handling and reporting\n- Automatic connection management\n- Query execution and result processing\n- Compatible with any MCP-compliant client\n\n## Technical Details\n\n### Core Components\n\nThe implementation consists of several key classes and modules:\n\n- **server.py** - The main entry point containing the MCP server implementation.\n- **SnowflakeConnection** - Class that handles all Snowflake database operations, including:\n  - Connection establishment and reconnection\n  - Query execution and transaction management\n  - Connection maintenance and cleanup\n- **SnowflakeMCPServer** - The main server class that implements the MCP protocol:\n  - Registers available tools with the MCP framework\n  - Handles tool call requests from clients\n  - Manages the lifecycle of connections\n\n### Connection Lifecycle\n\nThe connection lifecycle is carefully managed to ensure reliability:\n\n1. **Initialization** - Connections are created lazily when the first query is received\n2. **Validation** - Connection parameters are validated before attempting to connect\n3. **Monitoring** - Connections are regularly tested for validity\n4. **Recovery** - Automatic reconnection if the connection is lost or times out\n5. **Cleanup** - Proper resource release when the server shuts down\n\n### MCP Tool Interface\n\nThe server exposes the following tool to MCP clients:\n\n- **execute_query** - Executes a SQL query on Snowflake and returns the results\n  - Input: SQL query string\n  - Output: Query results in a structured format\n\n- **export_to_csv** - Executes a SQL query on Snowflake and returns the results\n  - Input: SQL query string\n  - Output: Num rows exported. File path of the output file\n\nThis implementation follows best practices for both MCP protocol implementation and Snowflake database interaction.\n\n## License\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThis project is licensed under the [MIT License](LICENSE). See the [LICENSE](LICENSE) file for details.\n\nCopyright (c) 2025 David Amom",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "queries",
        "snowflake databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "dbcodeio--public": {
      "owner": "dbcodeio",
      "name": "public",
      "url": "https://github.com/dbcodeio/public",
      "imageUrl": "/freedevtools/mcp/pfp/dbcodeio.webp",
      "description": "Connect to over 20 different databases for data management tasks. Query, visualize, and edit data without writing SQL, utilizing AI-powered features for enhanced productivity.",
      "stars": 195,
      "forks": 10,
      "license": "Other",
      "language": "",
      "updated_at": "2025-10-03T23:12:35Z",
      "readme_content": "# DBCode - Data Beside Code\n\n<p align=\"center\">\n\t<a href=\"https://dbcode.io/docs\" target=\"_blank\">DOCS</a> | <a href=\"https://dbcode.io/roadmap\" target=\"_blank\">ROADMAP</a> | <a href=\"https://dbcode.io/changelog\" target=\"_blank\">CHANGELOG</a>  | <a href=\"https://discord.gg/FvAzEAHb9w\" target=\"_blank\">DISCORD</a>\n</p>\n\nFor devs who'd rather ship than <strike>fumble</strike> alt-tab around.\n\n## Database Support \n\nConnect to 20+ databases including:\n\n<img width=\"100%\" alt=\"data-editor\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/logo_panel.png\">\n\n\n[See more](https://dbcode.io)\n\n## Data Viewing & Editing\n\n<img width=\"100%\" alt=\"data-editor\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/videos/editing.gif\">\n\n<ul>\n\t<li>⚡ Filter, Sort, and Group data with a few clicks</li>\n\t<li>🔄 CRUD operations without writing SQL (I know, sometimes it feels like cheating)</li>\n\t<li>✅ Changes verified before execution - because we all have that production horror story</li>\n</ul>\n\n\n## Copilot Integration\n\n<img width=\"100%\" alt=\"copilot\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/videos/copilot.gif\">\n\n<p>Query your data with natural language. Ask schema questions, generate queries, create tables - because sometimes typing \"SELECT * FROM\" for the 100th time is just too much effort.</p>\n\n\n## Entity Relationship Diagrams\n\n<img width=\"100%\" alt=\"erd\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/videos/erd.gif\">\n\n<p>Auto-generated diagrams that map your database structure. Your DBA will think you spent hours on this.</p>\n\n## Data Exploration\n\n<img width=\"100%\" alt=\"erd\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/videos/relationships.gif\">\n\n<p>Navigate foreign key relationships with a click. No JOINs required.</p>\n\n## Custom SQL\n\n<img width=\"100%\" alt=\"custom SQL\" src=\"https://github.com/dbcodeio/public/assets/1918994/650632b0-da26-4b98-9f66-5138a7db1e7e\">\n\n<p>Write and execute your own queries right within VS Code. For when you need to flex those SQL muscles.</p>\n\n\n## Inline SQL Help\n\n<img width=\"100%\" alt=\"signature\" src=\"https://github.com/dbcodeio/public/assets/1918994/1efd912b-7750-47d0-a2e9-7aaaff0b0c52\">\n\n<ul>\n\t<li>🔍 Database-specific SQL keywords - no more forgetting dialect differences</li>\n\t<li>📊 Table/view/procedure intellisense with data types - because remembering every column name is for computers</li>\n</ul>\n\n\n## Secure Report Sharing\n\n<img  height=\"300\" alt=\"share\" src=\"https://raw.githubusercontent.com/dbcodeio/public/main/public/share.png\">\n\n<p>Share reports securely with encryption - no credentials or data exposed. Your security team will finally stop giving you that look.</p>\n\n## Notebooks\n\n<img width=\"100%\" alt=\"notebook\" src=\"https://github.com/dbcodeio/public/assets/1918994/6295585e-f13f-46b7-98e1-80e850485db1\">\n\n<p>Use VS Code Notebooks for database work. Query, analyze, and document in one place.</p>\n\n## Stored Procedures and Functions\n\n<img width=\"100%\" alt=\"stored-procedure\" src=\"https://github.com/dbcodeio/public/assets/1918994/dae92d32-13b7-4f90-8bcb-67116a5468cd\">\n\n<p>Edit database logic in your favorite code editor. Say goodbye to those prehistoric database IDEs.</p>\n\n\n## Data Visualization\n\n<img width=\"100%\" alt=\"chart\" src=\"https://github.com/dbcodeio/public/assets/1918994/d1d33ee9-9b3b-408f-9477-f208cf2adf87\">\n\n<p>Transform query results into charts and graphs. Impress stakeholders with minimal effort (I won't tell).</p>\n\n\n## Additional Features\n\n- **Import**: Easily import data from CSV files or other database tables\n- **Export**: Get your data out when you need it elsewhere\n- **Multi-DB Sessions**: Query across databases simultaneously\n- **Query Parameters**: Reuse queries with different inputs (less copy-paste, more productivity)\n- **Custom Colors**: Color-code your connections - production remains red, naturally\n- **Result Pinning**: Keep important query results around\n- **SSL Auto Config**: Automatic SSL setup for [known hosts](https://dbcode.io/docs/connections/auto-ssl)\n- **SSH Tunnels**: Manual config or auto-discovery from SSH config\n- **Theme Support**: Full compatibility with VS Code themes\n- **Localization**: Available in all VS Code supported languages\n- **Fuzzy Table Search**: Find that table even when you can't remember its exact name\n- **Quick Query History**: Revisit recent queries without retyping - perfect for iterative development\n- **Command Palette Integration**: All database actions at your fingertips, keyboard warriors rejoice\n- **[More Features](https://dbcode.io/features/)**\n\n## Pricing\n\nCore features are free, forever. Some advanced features require a subscription. See our [Pricing](https://dbcode.io/pricing) page.\n\n## Contributors\n\n- [Pradeep Kumar](https://www.linkedin.com/in/pradeep-kumar-1722b6123/): Documentation, Testing\n- [Lanterns](https://github.com/L4nterns): Translation\n- [snickerjp](https://github.com/snickerjp): Translation\n- [intervisionlord](https://github.com/intervisionlord): Translation\n\nA heartfelt thank you to these incredible contributors who have generously donated their time and expertise. DBCode wouldn't be the same without their input.\n\n## Telemetry\n\nWe collect anonymous usage data when VS Code telemetry is enabled. This helps us improve the extension.\n\nTelemetry respects your VS Code settings - if disabled there, we collect nothing. See our [privacy policy](https://dbcode.io/legal/privacy-policy/).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dbcodeio",
        "databases",
        "database",
        "dbcodeio public",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "dennismartis--sql_mcp_server": {
      "owner": "dennismartis",
      "name": "sql_mcp_server",
      "url": "https://github.com/dennismartis/sql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/dennismartis.webp",
      "description": "Interact with SQL databases using natural language for executing queries, managing tables, and retrieving information seamlessly through a conversational interface.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-24T21:39:00Z",
      "readme_content": "# MCP SQL Server\n\nA FastMCP server that provides SQL database interaction tools via a conversational AI interface.\n\n## Overview\n\nThis project creates a server that exposes MS SQL Server operations through a conversational AI interface. It uses the FastMCP framework to provide tools for querying and manipulating SQL data, allowing users to interact with databases using natural language.\n\n## Features\n\n- Execute SQL queries and view results\n- List available tables in the database\n- Describe table structure with column information\n- Execute non-query operations (INSERT, UPDATE, DELETE)\n- List available ODBC drivers on the system\n- View database information and server details\n\n## Requirements\n\n- Python 3.7+\n- pyodbc\n- asyncio\n- FastMCP framework\n- Microsoft SQL Server\n- ODBC Driver 17 for SQL Server\n\n## Installation\n\n1. Install Python dependencies:\n\n```bash\npip install pyodbc asyncio fastmcp\n```\n\n2. Ensure you have Microsoft SQL Server installed and the ODBC Driver 17 for SQL Server.\n\n3. Configure the connection settings in the script:\n\n```python\n# Connection parameters\nSERVER = \"server\\\\instance\"  # Change to your SQL Server instance\nDATABASE = \"db_name\"              # Change to your database name\n```\n\n## Usage\n\nRun the server:\n\n```bash\npython mcp_sql_server.py\n```\n\nThe server will initialize and establish a connection to the specified SQL Server database.\n\n## Available Tools\n\n### query_sql\n\nExecute a SQL query and return the results.\n\n```\nquery_sql(query: str = None) -> str\n```\n\n- If no query is provided, it defaults to `SELECT * FROM [dbo].[Table_1]`\n- Returns query results as a formatted string\n\n### list_tables\n\nList all tables available in the database.\n\n```\nlist_tables() -> str\n```\n\n- Returns a list of table names as a string\n\n### describe_table\n\nGet the structure of a specific table.\n\n```\ndescribe_table(table_name: str) -> str\n```\n\n- `table_name`: Name of the table to describe\n- Returns column information including names and data types\n\n### execute_nonquery\n\nExecute INSERT, UPDATE, DELETE or other non-query SQL statements.\n\n```\nexecute_nonquery(sql: str) -> str\n```\n\n- `sql`: The SQL statement to execute\n- Returns operation results, including number of affected rows\n- Automatically handles transactions (commit/rollback)\n\n### list_odbc_drivers\n\nList all available ODBC drivers on the system.\n\n```\nlist_odbc_drivers() -> str\n```\n\n- Returns a comma-separated list of installed ODBC drivers\n\n### database_info\n\nGet general information about the connected database.\n\n```\ndatabase_info() -> str\n```\n\n- Returns server name, database name, SQL Server version, current server time, and table count\n\n## Architecture\n\nThe server uses an asynchronous architecture to avoid blocking operations:\n\n1. **Lifecycle Management**: The `app_lifespan` context manager handles database connection setup and teardown.\n\n2. **Non-blocking Operations**: Database operations run in a separate thread using `asyncio.get_event_loop().run_in_executor()` to prevent blocking the main event loop.\n\n3. **Error Handling**: All operations include comprehensive error handling with useful error messages.\n\n## Error Handling\n\nThe server handles various error conditions:\n\n- Database connection failures\n- SQL query syntax errors\n- Table not found errors\n- Permission-related issues\n\nAll errors are logged and appropriate error messages are returned to the client.\n\n## Customization\n\nTo add new database tools or modify existing ones, follow the pattern used in the existing tools:\n\n```python\n@mcp.tool()\nasync def your_new_tool(ctx: Context, param1: str) -> str:\n    \"\"\"Documentation for your tool\"\"\"\n    try:\n        conn = ctx.request_context.lifespan_context[\"conn\"]\n        \n        if conn is None:\n            return \"Database connection is not available.\"\n            \n        def your_db_operation():\n            # Your database operations here\n            pass\n            \n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(None, your_db_operation)\n        \n        # Process and return results\n        return \"Your result\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n## Security Considerations\n\n- The server uses Windows Authentication (\"Trusted_Connection=yes\")\n- Consider implementing input validation for SQL queries to prevent SQL injection\n- Restrict database user permissions based on the principle of least privilege\n\n## Troubleshooting\n\nCommon issues:\n\n1. **Connection errors**: Verify the SQL Server instance name and ensure it's running\n2. **ODBC driver errors**: Confirm ODBC Driver 17 for SQL Server is installed\n3. **Permission errors**: Check that the Windows user running the application has appropriate SQL Server permissions\n\n## License\n\n[Your License Information]\n\n## Contact\n\n[Your Contact Information]\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sql_mcp_server",
        "database",
        "databases secure",
        "secure database",
        "sql databases"
      ],
      "category": "databases"
    },
    "denniswon--intel-tdx-zk-prover": {
      "owner": "denniswon",
      "name": "intel-tdx-zk-prover",
      "url": "https://github.com/denniswon/intel-tdx-zk-prover",
      "imageUrl": "/freedevtools/mcp/pfp/denniswon.webp",
      "description": "Provides Intel TDX DCAP attestation verification capabilities via a REST API, managing agents, requests, and attestations while leveraging zero knowledge proofs for enhanced security. Facilitates cryptographic proofs of attestation verification in applications.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Rust",
      "updated_at": "2025-05-06T21:58:31Z",
      "readme_content": "# TDX Prover\n\nTDX Prover is a Rust Rest Api service built with Rust's Axum framework for providing Intel TDX DCAP attestation verification capabilities including DCAP verification zero knowledge proof (Groth16) using sp1 zkvm.\n\nThe service manages three main entities: agents, attestation requests, and attestations. It uses the Axum\nweb framework and SQLx for database access, with a PostgreSQL backend. The primary function is to verify TDX\nDCAP attestations and generate cryptographic proofs of verification.\n\n## Features\n\nThis project uses Axum framework and SQLx for DB access layer for storing agent, request, and attestation data. It includes three basic routes: agent, request, and attestation.\n\n## Prerequisites\n\n- [Rust toolchain](https://rustup.rs/)\n- [Cargo Lambda](https://github.com/cargo-lambda/cargo-lambda)\n- PostgreSQL\n- [sqlx-cli](https://crates.io/crates/sqlx-cli)\n\n## Deployment Prerequisites\n\n- [cross-rs](https://github.com/cross-rs/cross)\n- [Docker](https://docs.docker.com/engine/install/)\n- [AWS CLI](https://aws.amazon.com/cli/)\n- [SAM](https://aws.amazon.com/serverless/sam/)\n- [Zig](https://ziglang.org/)\n\n## Routes\n\n### Agent\n\n- POST `/agent/register` - Register a new agent\n- GET `/agent/{id}` - Get agent by id\n- PUT `/agent/{id}` - Update a agent\n- DELETE `/agent/{id}` - Delete a agent\n\n### Request\n\n- POST `/request/register` - Register a new request\n- GET `/request/{id}` - Get request by id\n- PUT `/request/{id}` - Update a request\n- DELETE `/request/{id}` - Delete a request\n\n### Attestation\n\n- POST `/attestation/register` - Register a new attestation\n- GET `/attestation/{id}` - Get attestation by id\n- GET `/attestation/verify_dcap/{id}` - Verify attestation with DCAP\n\n- GET `/attestation/prove/{id}` - Generate zero knowledge proof of attestation\n- POST `/attestation/verify` - Verify zero knowledge proof of attestation\n- POST `/attestation/submit_proof` - Submit zero knowledge proof of attestation\n\n## Development\n\n1. Clone the project\n2. Update `.env` file with the DB credentials\n3. Install `sqlx-cli` or run `cargo sqlx database create` to create your DB\n4. Run the migration file using `cargo sqlx migrate run`. This will run the migration file that exists in the migration folder in the root of the project.\n5. Build the project and dependencies using `cargo build`\n6. Run the project using `cargo run -- up`\n\n## Database\n\n- Create: `cargo sqlx database create`\n- Migrate: `cargo sqlx migrate run`\n- Offline: `cargo sqlx prepare -- --merged`\n\n## Deploy\n\n1. Install `cross-rs` for cross platform build\n2. Build the project using `cross build`\n\n## Lint\n\n- Lint: `cargo clippy`\n\n## Test\n\n- Test: `cargo test [test_name]` (to run a specific test)\n\n## Code Style Guidelines\n\n- **Formatting**: Follow Rust standard style (rustfmt defaults)\n- **Imports**: Group by external crates then internal modules\n- **Naming**:\n  - Use snake_case for files, modules, functions, variables\n  - Use CamelCase for types, structs, enums\n  - Always use descriptive variable names\n- **Error Handling**:\n  - Use thiserror for domain-specific errors\n  - Implement IntoResponse for API errors\n  - Use ? operator for error propagation\n- **Types**: Prefer strong typing with explicit types\n- **Documentation**: Document public API functions\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "attestation",
        "databases",
        "attestations",
        "attestation verification",
        "dcap attestation",
        "secure database"
      ],
      "category": "databases"
    },
    "deploya-labs--mcp-supabase": {
      "owner": "deploya-labs",
      "name": "mcp-supabase",
      "url": "https://github.com/deploya-labs/mcp-supabase",
      "imageUrl": "/freedevtools/mcp/pfp/deploya-labs.webp",
      "description": "Manage Supabase databases and execute SQL queries autonomously and securely while utilizing safety controls. Provides an interface for efficient database management and interaction through Cursor and Windsurf.",
      "stars": 10,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-07-07T13:10:19Z",
      "readme_content": "# Supabase MCP Server\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/4a363bcd-7c15-47fa-a72a-d159916517f7\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" />\n    <img alt=\"Supabase\" src=\"https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df\" height=\"40\" />\n  </picture>\n  &nbsp;&nbsp;\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/38db1bcd-50df-4a49-a106-1b5afd924cb2\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" />\n    <img alt=\"MCP\" src=\"https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926\" height=\"40\" />\n  </picture>\n</p>\n\n<p align=\"center\">\n  <strong>Let Cursor & Windsurf manage your Supabase and run SQL queries. Autonomously. In a safe way.</strong>\n</p>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/supabase-mcp-server/\"><img src=\"https://img.shields.io/pypi/v/supabase-mcp-server.svg\" alt=\"PyPI version\" /></a>\n  <a href=\"https://github.com/alexander-zuev/supabase-mcp-server/actions\"><img src=\"https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg\" alt=\"CI Status\" /></a>\n  <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.12%2B-blue.svg\" alt=\"Python 3.12+\" /></a>\n  <a href=\"https://github.com/astral-sh/uv\"><img src=\"https://img.shields.io/badge/uv-package%20manager-blueviolet\" alt=\"uv package manager\" /></a>\n  <a href=\"https://pepy.tech/project/supabase-mcp-server\"><img src=\"https://static.pepy.tech/badge/supabase-mcp-server\" alt=\"PyPI Downloads\" /></a>\n  <a href=\"https://modelcontextprotocol.io/introduction\"><img src=\"https://img.shields.io/badge/MCP-Server-orange\" alt=\"MCP Server\" /></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n</p>\n\n\nA feature-rich MCP server that enables Cursor and Windsurf to safely interact with Supabase databases. It provides tools for database management, SQL query execution, and Supabase Management API access with built-in safety controls.\n\n## Table of contents\n<p align=\"center\">\n  <a href=\"#getting-started\">Getting started</a> •\n  <a href=\"#feature-overview\">Feature overview</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#roadmap\">Roadmap</a>\n</p>\n\n## ✨ Key features\n- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol\n- 🔐 Control read-only and read-write modes of SQL query execution\n- 🔄 Robust transaction handling for both direct and pooled database connections\n- 💻 Manage your Supabase projects with Supabase Management API\n- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK\n- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively\n- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)\n\n## Getting Started\n\n### Prerequisites\nInstalling the server requires the following on your system:\n- Python 3.12+\n- PostgresSQL 16+\n\nIf you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).\n\n### PostgreSQL Installation\n> ⚠️ **Important**: PostgreSQL must be installed BEFORE installing project dependencies, as psycopg2 requires PostgreSQL development libraries during compilation.\n\n**MacOS**\n```bash\nbrew install postgresql@16\n```\n\n**Windows**\n  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/\n  - Ensure \"PostgreSQL Server\" and \"Command Line Tools\" are selected during installation\n\n### Step 1. MCP Server Installation\n\nSince v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:\n\n```bash\n# if pipx is installed (recommended)\npipx install supabase-mcp-server\n\n# if uv is installed\nuv pip install supabase-mcp-server\n```\n\n`pipx` is recommended because it creates isolated environments for each package.\n\nYou can also install the server manually by cloning the repository and running `pipx` install -editable . from the root directory.\n\n> ⚠️ If you run into psycopg2 compilation issues, you might be missing PostgreSQL development packages. See above.\n\n#### Installing from source\nIf you would like to install from source, for example for local development:\n```bash\nuv venv\n# On Mac\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n# Install package in editable mode\nuv pip install -e .\n```\n\n#### Installing via Smithery.ai\nPlease report any issues with Smithery, as I haven't tested it yet.\n\nTo install Supabase MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@alexander-zuev/supabase-mcp):\n\n```bash\nnpx -y @smithery/cli install @alexander-zuev/supabase-mcp --client claude\n```\n\n### Step 2. Configuration\n\nAfter installing the package, you'll need to configure your database connection settings. The server supports both local and remote Supabase instances.\n\n#### Local Supabase instance (Default)\n Server is pre-configured to connect to the local Supabase instance using default settings:\n- `Host`: 127.0.0.1:54322\n- `Password`: postgres\n\n>💡 As long as you didn't modify the default settings and you want to connect to the local instance, you don't need to set environment variables.\n\n#### Remote Supabase instance\n\n> ⚠️ **IMPORTANT WARNING**: Session pooling connections are not supported and there are no plans to support it yet. Let me know if you feel there is a use case for supporting this in an MCP server\n\nFor remote Supabase projects, you need to configure:\n- `SUPABASE_PROJECT_REF` - Your project reference (found in project URL)\n- `SUPABASE_DB_PASSWORD` - Your database password\n- `SUPABASE_REGION` - (Optional) Defaults to `us-east-1`\n- `SUPABASE_ACCESS_TOKEN` - (Optional) For Management API access\n\nYou can get your SUPABASE_PROJECT_REF from your project's dashboard URL:\n- `https://supabase.com/dashboard/project/<supabase-project-ref>`\n\nThe server supports all Supabase regions:\n- `us-west-1` - West US (North California)\n- `us-east-1` - East US (North Virginia) - default\n- `us-east-2` - East US (Ohio)\n- `ca-central-1` - Canada (Central)\n- `eu-west-1` - West EU (Ireland)\n- `eu-west-2` - West Europe (London)\n- `eu-west-3` - West EU (Paris)\n- `eu-central-1` - Central EU (Frankfurt)\n- `eu-central-2` - Central Europe (Zurich)\n- `eu-north-1` - North EU (Stockholm)\n- `ap-south-1` - South Asia (Mumbai)\n- `ap-southeast-1` - Southeast Asia (Singapore)\n- `ap-northeast-1` - Northeast Asia (Tokyo)\n- `ap-northeast-2` - Northeast Asia (Seoul)\n- `ap-southeast-2` - Oceania (Sydney)\n- `sa-east-1` - South America (São Paulo)\n\nMethod of MCP configuration differs between Cursor and Windsurf. Read the relevant section to understand how to configure connection.\n\n##### Cursor\nSince v0.46 there are two ways to configure MCP servers in Cursor:\n- per project basis -> create `mcp.json` in your project / repo folder and `.env` to configure connection\n- globally -> create an MCP server in Settings and configure using `.env` which is supported by this MCP server only\n\n\nYou can create project-specific MCP by:\n- creating .cursor folder in your repo, if doesn't exist\n- creating or updating `mcp.json` file with the following settings\n\n> ⚠ **Environment variables**: If you are configuring MCP server on a per-project basis you still need to create .env file for connection settings to be picked up. I wasn't able to configure mcp.json to pick up my env vars 😔\n\n```json\n{\n\t\"mcpServers\": {\n\t  \"filesystem\": {\n\t\t\"command\": \"supabase-mcp-server\",\n\t  }\n\t}\n  }\n```\n\nAlternatively, if you want to configure MCP servers globally (i.e. not for each project), you can use configure connection settings by updating an `.env` file in a global config folder by running the following commands:\n```bash\n# Create config directory and navigate to it\n# On macOS/Linux\nmkdir -p ~/.config/supabase-mcp\ncd ~/.config/supabase-mcp\n\n# On Windows (in PowerShell)\nmkdir -Force \"$env:APPDATA\\supabase-mcp\"\ncd \"$env:APPDATA\\supabase-mcp\"\n```\nThis creates the necessary config folder where your environment file will be stored.\n\n```bash\n# Create and edit .env file\n# On macOS/Linux\nnano ~/.config/supabase-mcp/.env\n\n# On Windows (PowerShell)\nnotepad \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nThis will open the .env file. Once the file is open, copy & paste the following:\n```bash\nSUPABASE_PROJECT_REF=your-project-ref\nSUPABASE_DB_PASSWORD=your-db-password\nSUPABASE_REGION=us-east-1  # optional, defaults to us-east-1\nSUPABASE_ACCESS_TOKEN=your-access-token  # optional, for management API\n```\n\nVerify the file exists - you should see the values you have just set:\n```bash\n# On macOS/Linux\ncat ~/.config/supabase-mcp/.env\n\n# On Windows (PowerShell)\nGet-Content \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nYou can find global config file:\n   - Windows: `%APPDATA%/supabase-mcp/.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n\n\n##### Windsurf\nWindsurf supports de facto standard .json format for MCP Servers configuration. You can configure the server in mcp_config.json file:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\"  // optional, for management API\n        }\n      }\n    }\n}\n```\n> 💡 **Finding the server path**:\n> - macOS/Linux: Run `which supabase-mcp-server`\n> - Windows: Run `where supabase-mcp-server`\n\n#### Configuration Precedence\nThe server looks for configuration in this order:\n1. Environment variables (highest priority)\n2. Local `.env` file in current directory\n3. Global config file:\n   - Windows: `%APPDATA%/supabase-mcp/.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n4. Default settings (local development)\n\n### Step 3. Running MCP Server in Cursor/Windsurf\n\nIn general, any MCP client that supports `stdio` protocol should work with this MCP server (Cline, for example) but I haven't tested it with anything except Cursor/Windsurf.\n\n#### Cursor\nGo to Settings -> Features -> MCP Servers and add a new server with this configuration:\n```bash\n# can be set to any name\nname: supabase\ntype: command\n# if you installed with pipx\ncommand: supabase-mcp-server\n# if you installed with uv\ncommand: uv run supabase-mcp-server\n```\n\nIf configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.\n![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)\n\n#### Windsurf\nGo to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\"  // optional, for management API\n        }\n      }\n    }\n}\n```\nIf configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)\n\n### Troubleshooting\n\nHere are some tips & tricks that might help you:\n- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.\n- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.\n- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\\supabase-mcp\\.env` on Windows).\n- **Accessing logs** - The MCP server writes detailed logs to a file:\n  - Log file location:\n    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`\n    - Windows: `%USERPROFILE%\\.local\\share\\supabase-mcp\\mcp_server.log`\n  - Logs include connection status, configuration details, and operation results\n  - View logs using any text editor or terminal commands:\n    ```bash\n    # On macOS/Linux\n    cat ~/.local/share/supabase-mcp/mcp_server.log\n\n    # On Windows (PowerShell)\n    Get-Content \"$env:USERPROFILE\\.local\\share\\supabase-mcp\\mcp_server.log\"\n    ```\n\nIf you are stuck or any of the instructions above are incorrect, please raise an issue.\n\n### MCP Inspector\nA super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.\n> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the cominng release.\n\n## Feature Overview\n\n### Database query tools\n\nSince v0.3.0 server supports both read-only and data modification operations:\n\n- **Read operations**: SELECT queries for data retrieval\n- **Data Manipulation Language (DML)**: INSERT, UPDATE, DELETE operations for data changes\n- **Data Definition Language (DDL)**: CREATE, ALTER, DROP operations for schema changes*\n\n*Note: DDL operations require:\n1. Read-write mode enabled via `live_dangerously`\n2. Sufficient permissions for the connected database role\n\n#### Transaction Handling\n\nThe server supports two approaches for executing write operations:\n\n1. **Explicit Transaction Control** (Recommended):\n   ```sql\n   BEGIN;\n   CREATE TABLE public.test_table (id SERIAL PRIMARY KEY, name TEXT);\n   COMMIT;\n   ```\n\n2. **Single Statements**:\n   ```sql\n   CREATE TABLE public.test_table (id SERIAL PRIMARY KEY, name TEXT);\n   ```\n\nFor DDL operations (CREATE/ALTER/DROP), tool description appropriately guides Cursor/Windsurft to use explicit transaction control with BEGIN/COMMIT blocks.\n\n#### Connection Types\n\nThis MCP server uses::\n- **Direct Database Connection**: when connecting to a local Supabase instance\n- **Transaction Pooler Connections**: when connecting to a remote Supabase instance\n\n\nWhen connecting via Supabase's Transaction Pooler, some complex transaction patterns may not work as expected. For schema changes in these environments, use explicit transaction blocks or consider using Supabase migrations or the SQL Editor in the dashboard.\n\nAvailable database tools:\n- `get_db_schemas` - Lists all database schemas with their sizes and table counts\n- `get_tables` - Lists all tables in a schema with their sizes, row counts, and metadata\n- `get_table_schema` - Gets detailed table structure including columns, keys, and relationships\n- `execute_sql_query` - Executes raw SQL queries with comprehensive support for all PostgreSQL operations:\n  - Supports all query types (SELECT, INSERT, UPDATE, DELETE, CREATE, ALTER, DROP, etc.)\n  - Handles transaction control statements (BEGIN, COMMIT, ROLLBACK)\n\n\n- Supported modes:\n  - `read-only` - only read-only queries are allowed (default mode)\n  - `read-write` - all SQL operations are allowed when explicitly enabled\n- Safety features:\n  - Starts in read-only mode by default\n  - Requires explicit mode switch for write operations\n  - Automatically resets to read-only mode after write operations\n  - Intelligent transaction state detection to prevent errors\n  - SQL query validation [TODO]\n\n### Management API tools\nSince v0.3.0 server supports sending arbitrary requests to Supabase Management API with auto-injection of project ref and safety mode control:\n  - Includes the following tools:\n    - `send_management_api_request` to send arbitrary requests to Supabase Management API, with auto-injection of project ref and safety mode control\n    - `get_management_api_spec` to get the enriched API specification with safety information\n    - `get_management_api_safety_rules` to get all safety rules including blocked and unsafe operations with human-readable explanations\n    - `live_dangerously` to switch between safe and unsafe modes\n  - Safety features:\n    - Divides API methods into `safe`, `unsafe` and `blocked` categories based on the risk of the operation\n    - Allows to switch between safe and unsafe modes dynamically\n    - Blocked operations (delete project, delete database) are not allowed regardless of the mode\n\n### Auth Admin tools\nI was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.\n\nSince v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:\n  - Includes the following tools:\n    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods\n    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling\n  - Supported methods:\n    - `get_user_by_id`: Retrieve a user by their ID\n    - `list_users`: List all users with pagination\n    - `create_user`: Create a new user\n    - `delete_user`: Delete a user by their ID\n    - `invite_user_by_email`: Send an invite link to a user's email\n    - `generate_link`: Generate an email link for various authentication purposes\n    - `update_user_by_id`: Update user attributes by ID\n    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)\n\n#### Why use Auth Admin SDK instead of raw SQL queries?\n\nThe Auth Admin SDK provides several key advantages over direct SQL manipulation:\n- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)\n- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas\n- **Simplicity**: Offers clear methods with proper validation and error handling\n\n  - Response format:\n    - All methods return structured Python objects instead of raw dictionaries\n    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user[\"id\"]`)\n  - Edge cases and limitations:\n    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors\n    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project\n    - Link types: When generating links, different link types have different requirements:\n      - `signup` links don't require the user to exist\n      - `magiclink` and `recovery` links require the user to already exist in the system\n    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface\n    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK\n\n## Roadmap\n\n- 📦 Simplified installation via package manager - ✅ (v0.2.0)\n- 🌎 Support for different Supabase regions - ✅ (v0.2.2)\n- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)\n- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)\n- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)\n- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)\n- 🔍 Stronger SQL query validation (read vs write operations)\n- 📝 Automatic versioning of DDL queries(?)\n- 🪵 Tools / resources to more easily access database, edge functions logs (?) \n- 👨‍💻 Supabase CLI integration (?)\n\n\n\n### Connect to Supabase logs\n\nI'm planning to research, if it's possible to connect to Supabase db logs which might be useful for debugging (if not already supported.)\n\n\n---\n\nEnjoy! ☺️\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase databases",
        "supabase manage",
        "manage supabase"
      ],
      "category": "databases"
    },
    "designcomputer--mysql_mcp_server": {
      "owner": "designcomputer",
      "name": "mysql_mcp_server",
      "url": "https://github.com/designcomputer/mysql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/designcomputer.webp",
      "description": "Facilitates secure interaction with MySQL databases, enabling AI assistants to list tables, read data, and execute SQL queries through a controlled interface.",
      "stars": 899,
      "forks": 194,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T18:57:45Z",
      "readme_content": "![Tests](https://github.com/designcomputer/mysql_mcp_server/actions/workflows/test.yml/badge.svg)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/mysql-mcp-server)\n[![smithery badge](https://smithery.ai/badge/mysql-mcp-server)](https://smithery.ai/server/mysql-mcp-server)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/designcomputer-mysql-mcp-server)\n# MySQL MCP Server\nA Model Context Protocol (MCP) implementation that enables secure interaction with MySQL databases. This server component facilitates communication between AI applications (hosts/clients) and MySQL databases, making database exploration and analysis safer and more structured through a controlled interface.\n\n> **Note**: MySQL MCP Server is not designed to be used as a standalone server, but rather as a communication protocol implementation between AI applications and MySQL databases.\n\n## Features\n- List available MySQL tables as resources\n- Read table contents\n- Execute SQL queries with proper error handling\n- Secure database access through environment variables\n- Comprehensive logging\n\n## Installation\n### Manual Installation\n```bash\npip install mysql-mcp-server\n```\n\n### Installing via Smithery\nTo install MySQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mysql-mcp-server):\n```bash\nnpx -y @smithery/cli install mysql-mcp-server --client claude\n```\n\n## Configuration\nSet the following environment variables:\n```bash\nMYSQL_HOST=localhost     # Database host\nMYSQL_PORT=3306         # Optional: Database port (defaults to 3306 if not specified)\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=your_database\n```\n\n## Usage\n### With Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/mysql_mcp_server\",\n        \"run\",\n        \"mysql_mcp_server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### With Visual Studio Code\nAdd this to your `mcp.json`:\n```json\n{\n  \"servers\": {\n      \"mysql\": {\n            \"type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"mysql-mcp-server\",\n                \"mysql_mcp_server\"\n            ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\nNote: Will need to install uv for this to work\n\n### Debugging with MCP Inspector\nWhile MySQL MCP Server isn't intended to be run standalone or directly from the command line with Python, you can use the MCP Inspector to debug it.\n\nThe MCP Inspector provides a convenient way to test and debug your MCP implementation:\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n# Use the MCP Inspector for debugging (do not run directly with Python)\n```\n\nThe MySQL MCP Server is designed to be integrated with AI applications like Claude Desktop and should not be run directly as a standalone Python program.\n\n## Development\n```bash\n# Clone the repository\ngit clone https://github.com/designcomputer/mysql_mcp_server.git\ncd mysql_mcp_server\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n# Install development dependencies\npip install -r requirements-dev.txt\n# Run tests\npytest\n```\n\n## Security Considerations\n- Never commit environment variables or credentials\n- Use a database user with minimal required permissions\n- Consider implementing query whitelisting for production use\n- Monitor and log all database operations\n\n## Security Best Practices\nThis MCP implementation requires database access to function. For security:\n1. **Create a dedicated MySQL user** with minimal permissions\n2. **Never use root credentials** or administrative accounts\n3. **Restrict database access** to only necessary operations\n4. **Enable logging** for audit purposes\n5. **Regular security reviews** of database access\n\nSee [MySQL Security Configuration Guide](https://github.com/designcomputer/mysql_mcp_server/blob/main/SECURITY.md) for detailed instructions on:\n- Creating a restricted MySQL user\n- Setting appropriate permissions\n- Monitoring database access\n- Security best practices\n\n⚠️ IMPORTANT: Always follow the principle of least privilege when configuring database access.\n\n## License\nMIT License - see LICENSE file for details.\n\n## Contributing\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql_mcp_server",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "devlimelabs--meilisearch-ts-mcp": {
      "owner": "devlimelabs",
      "name": "meilisearch-ts-mcp",
      "url": "https://github.com/devlimelabs/meilisearch-ts-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/devlimelabs.webp",
      "description": "Enables interaction with Meilisearch through a standardized interface for managing indexes, documents, search capabilities, and system operations.",
      "stars": 9,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-15T12:49:02Z",
      "readme_content": "# Meilisearch MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@devlimelabs/meilisearch-ts-mcp)](https://smithery.ai/server/@devlimelabs/meilisearch-ts-mcp)\n\nA Model Context Protocol (MCP) server implementation for Meilisearch, enabling AI assistants to interact with Meilisearch through a standardized interface.\n\n## Features\n\n- **Index Management**: Create, update, and delete indexes\n- **Document Management**: Add, update, and delete documents\n- **Search Capabilities**: Perform searches with various parameters and filters\n- **Settings Management**: Configure index settings\n- **Task Management**: Monitor and manage asynchronous tasks\n- **System Operations**: Health checks, version information, and statistics\n- **Vector Search**: Experimental vector search capabilities\n\n## Installation\n\n### Installing via Smithery\n\nTo install Meilisearch MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@devlimelabs/meilisearch-ts-mcp):\n\n```bash\nnpx -y @smithery/cli install @devlimelabs/meilisearch-ts-mcp --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/devlimelabs/meilisearch-ts-mcp.git\n   cd meilisearch-ts-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file based on the example:\n   ```bash\n   cp .env.example .env\n   ```\n   \n4. Edit the `.env` file to configure your Meilisearch connection.\n\n## Docker Setup\n\nThe Meilisearch MCP Server can be run in a Docker container for easier deployment and isolation.\n\n### Using Docker Compose\n\nThe easiest way to get started with Docker is to use Docker Compose:\n\n```bash\n# Start the Meilisearch MCP Server\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f\n\n# Stop the server\ndocker-compose down\n```\n\n### Building and Running the Docker Image Manually\n\nYou can also build and run the Docker image manually:\n\n```bash\n# Build the Docker image\ndocker build -t meilisearch-ts-mcp .\n\n# Run the container\ndocker run -p 3000:3000 --env-file .env meilisearch-ts-mcp\n```\n\n## Development Setup\n\nFor developers who want to contribute to the Meilisearch MCP Server, we provide a convenient setup script:\n\n```bash\n# Clone the repository\ngit clone https://github.com/devlimelabs-ts-mcp/meilisearch-ts-mcp.git\ncd meilisearch-ts-mcp\n\n# Run the development setup script\n./scripts/setup-dev.sh\n```\n\nThe setup script will:\n1. Create a `.env` file from `.env.example` if it doesn't exist\n2. Install dependencies\n3. Build the project\n4. Run tests to ensure everything is working correctly\n\nAfter running the setup script, you can start the server in development mode:\n\n```bash\nnpm run dev\n```\n\n## Usage\n\n### Building the Project\n\n```bash\nnpm run build\n```\n\n### Running the Server\n\n```bash\nnpm start\n```\n\n### Development Mode\n\n```bash\nnpm run dev\n```\n\n## Claude Desktop Integration\n\nThe Meilisearch MCP Server can be integrated with Claude for Desktop, allowing you to interact with your Meilisearch instance directly through Claude.\n\n### Automated Setup\n\nWe provide a setup script that automatically configures Claude for Desktop to work with the Meilisearch MCP Server:\n\n```bash\n# First build the project\nnpm run build\n\n# Then run the setup script\nnode scripts/claude-desktop-setup.js\n```\n\nThe script will:\n1. Detect your operating system and locate the Claude for Desktop configuration file\n2. Read your Meilisearch configuration from the `.env` file\n3. Generate the necessary configuration for Claude for Desktop\n4. Provide instructions for updating your Claude for Desktop configuration\n\n### Manual Setup\n\nIf you prefer to manually configure Claude for Desktop:\n\n1. Locate your Claude for Desktop configuration file:\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the following configuration (adjust paths as needed):\n\n```json\n{\n  \"mcpServers\": {\n    \"meilisearch\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/meilisearch-ts-mcp/dist/index.js\"],\n      \"env\": {\n        \"MEILISEARCH_HOST\": \"http://localhost:7700\",\n        \"MEILISEARCH_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n3. Restart Claude for Desktop to apply the changes.\n\n4. In Claude, type: \"I want to use the Meilisearch MCP server\" to activate the integration.\n\n## Cursor Integration\n\nThe Meilisearch MCP Server can also be integrated with [Cursor](https://cursor.com), an AI-powered code editor.\n\n### Setting Up MCP in Cursor\n\n1. Install and set up the Meilisearch MCP Server:\n   ```bash\n   git clone https://github.com/devlimelabs/meilisearch-ts-mcp.git\n   cd meilisearch-ts-mcp\n   npm install\n   npm run build\n   ```\n\n2. Start the MCP server:\n   ```bash\n   npm start\n   ```\n\n3. In Cursor, open the Command Palette (Cmd/Ctrl+Shift+P) and search for \"MCP: Connect to MCP Server\".\n\n4. Select \"Connect to a local MCP server\" and enter the following details:\n   - **Name**: Meilisearch\n   - **Command**: node\n   - **Arguments**: /absolute/path/to/meilisearch-ts-mcp/dist/index.js\n   - **Environment Variables**: \n     ```\n     MEILISEARCH_HOST=http://localhost:7700\n     MEILISEARCH_API_KEY=your-api-key\n     ```\n\n5. Click \"Connect\" to establish the connection.\n\n6. You can now interact with your Meilisearch instance through Cursor by typing commands like \"Search my Meilisearch index for documents about...\"\n\n## Available Tools\n\nThe Meilisearch MCP Server provides the following tools:\n\n### Index Tools\n- `create-index`: Create a new index\n- `get-index`: Get information about an index\n- `list-indexes`: List all indexes\n- `update-index`: Update an index\n- `delete-index`: Delete an index\n\n### Document Tools\n- `add-documents`: Add documents to an index\n- `get-document`: Get a document by ID\n- `get-documents`: Get multiple documents\n- `update-documents`: Update documents\n- `delete-document`: Delete a document by ID\n- `delete-documents`: Delete multiple documents\n- `delete-all-documents`: Delete all documents in an index\n\n### Search Tools\n- `search`: Search for documents\n- `multi-search`: Perform multiple searches in a single request\n\n### Settings Tools\n- `get-settings`: Get index settings\n- `update-settings`: Update index settings\n- `reset-settings`: Reset index settings to default\n- Various specific settings tools (synonyms, stop words, ranking rules, etc.)\n\n### Task Tools\n- `list-tasks`: List tasks with optional filtering\n- `get-task`: Get information about a specific task\n- `cancel-tasks`: Cancel tasks based on provided filters\n- `wait-for-task`: Wait for a specific task to complete\n\n### System Tools\n- `health`: Check the health status of the Meilisearch server\n- `version`: Get version information\n- `info`: Get system information\n- `stats`: Get statistics about indexes\n\n### Vector Tools (Experimental)\n- `enable-vector-search`: Enable vector search\n- `get-experimental-features`: Get experimental features status\n- `update-embedders`: Configure embedders\n- `get-embedders`: Get embedders configuration\n- `reset-embedders`: Reset embedders configuration\n- `vector-search`: Perform vector search\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "meilisearch",
        "enables querying",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "dhartunian--cockroachdb-mcp-server": {
      "owner": "dhartunian",
      "name": "cockroachdb-mcp-server",
      "url": "https://github.com/dhartunian/cockroachdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dhartunian.webp",
      "description": "Connects to a CockroachDB instance to execute SQL queries and access database schemas and cluster metadata for enhanced data analysis. Provides tools for query optimization and execution plan analysis.",
      "stars": 4,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-14T19:51:52Z",
      "readme_content": "# CockroachDB MCP Server\n\nThis MCP server connects to a CockroachDB instance, exposing database and table schemas as resources, running SQL queries as tools, and providing prompts for query analysis.\n\n## Features\n\n### Resources\n\n- `postgres://{host}/databases/{database}` - Get information about a specific database\n- `postgres://{host}/databases/{database}/tables/{table}/schema` - Get the schema for a specific table\n- `postgres://{host}/cluster-metadata/{resource}` - Get cluster metadata (requires auth token)\n  - Currently supports: `nodes` - Information about cluster nodes\n\n### Tools\n\n- `query` - Execute a SQL query with options for execution plan analysis\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n3. Build the project:\n   ```bash\n   npx tsc\n   ```\n\n> **Note:** You must build the project with `tsc` before using the MCP server locally.\n\n## Configuration\n\nThe server requires a database URL as a command-line argument and optionally accepts an auth token for accessing admin UI endpoints:\n\n```bash\nnode dist/server.js postgres://user:password@host:port/database [auth_token]\n```\n\nThe auth token is required for accessing cluster metadata resources.\n\n## Using with Claude for Desktop\n\n1. Open your Claude for Desktop App configuration:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add your server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"cockroachdb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/cockroachdb-mcp-server/dist/server.js\", \n        \"postgres://user:password@host:port/database\",\n        \"your_auth_token\"\n      ]\n    }\n  }\n}\n```\n\n3. Restart Claude for Desktop\n\n## Using with Cline\n\n1. Open your Cline configuration file from the extension settings under \"MCP Servers\". Select \"Configure MCP Servers\".\n\n2. Add your server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"crdb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/cockroachdb-mcp-server/dist/server.js\",\n        \"postgres://root@127.0.0.1:26257/testdb\",\n        \"your_auth_token\"\n      ]\n    }\n  }\n}\n```\n\n3. Restart Cline or start a new session\n\n## Example Queries\n\nHere are some example queries you can ask Claude:\n\n1. \"What databases are available in my CockroachDB instance?\"\n2. \"Can you show me the schema for the 'users' table in the 'testdb' database?\"\n3. \"Run this query on my database: SELECT * FROM users LIMIT 10\"\n4. \"Debug this query and suggest improvements: SELECT * FROM orders WHERE customer_id = 123\"\n5. \"Show me information about all nodes in my CockroachDB cluster\"\n\n## Security Considerations\n\n- Be careful when configuring database access. Consider using a read-only user for the connection if you only need to query data.\n- The auth token is used to access the CockroachDB admin UI API. Make sure to keep this token secure.\n\n## Troubleshooting\n\n- If you encounter connection issues, verify your database credentials and ensure the CockroachDB instance is accessible from your machine.\n- For SQL errors, check the server logs for detailed error messages.\n- If Claude can't see the server, verify the configuration file is properly formatted and the path to the server.js file is correct.\n- For cluster metadata resources, ensure you've provided a valid auth token and that the admin UI is accessible on port 8080.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cockroachdb",
        "databases",
        "database",
        "connects cockroachdb",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "diez7lm--firestore-advanced-mcp": {
      "owner": "diez7lm",
      "name": "firestore-advanced-mcp",
      "url": "https://github.com/diez7lm/firestore-advanced-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/diez7lm.webp",
      "description": "Interact with Firebase Firestore databases using advanced features such as CRUD operations, complex queries, and transaction management. Supports special data types, TTL management, and intelligent index optimization for efficient data handling.",
      "stars": 4,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-14T09:00:28Z",
      "readme_content": "# 🔥 Firestore Advanced MCP\n\n![Version](https://img.shields.io/badge/version-1.0.0-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![Node](https://img.shields.io/badge/node-%3E%3D16.0.0-brightgreen)\n\nServeur MCP (Model Context Protocol) avancé pour Firebase Firestore, permettant aux grands modèles de langage comme Claude d'interagir de façon complète avec vos bases de données Firebase.\n\n## ✨ Fonctionnalités\n\n- 📝 **Support complet de Firestore** : CRUD, requêtes composées, filtres multiples\n- ⚡ **Opérations avancées** : Transactions, opérations atomiques, mise à jour par lot\n- 🔄 **Types de données spéciaux** : GeoPoint, références de documents, horodatages\n- ⏱️ **Gestion TTL** : Configuration du Time-To-Live pour les documents\n- 🔍 **Détection intelligente des index manquants** : Instructions automatiques pour créer les index nécessaires\n- 🎯 **Recherche avancée** : Requêtes sur groupes de collections, filtres complexes\n\n## 📋 Prérequis\n\n- Node.js >= 16.0.0\n- Un projet Firebase avec Firestore activé\n- Une clé de compte de service Firebase (fichier JSON)\n\n## 🚀 Installation\n\n### Via npm\n\n```bash\nnpm install -g firestore-advanced-mcp\n```\n\n### Via GitHub\n\n```bash\ngit clone https://github.com/diez7lm/firestore-advanced-mcp.git\ncd firestore-advanced-mcp\nnpm install\n```\n\n## 🔧 Configuration\n\n1. **Obtenir votre clé de compte de service Firebase** :\n   - Allez sur la [console Firebase](https://console.firebase.google.com/)\n   - Sélectionnez votre projet\n   - Paramètres du projet > Comptes de service\n   - Générez une nouvelle clé privée et téléchargez le fichier JSON\n\n2. **Définir la variable d'environnement** :\n\n```bash\nexport SERVICE_ACCOUNT_KEY_PATH=\"/chemin/vers/votre/serviceAccountKey.json\"\n```\n\n## 🖥️ Utilisation\n\n### Avec npm global\n\n```bash\nSERVICE_ACCOUNT_KEY_PATH=\"/chemin/vers/votre/serviceAccountKey.json\" firestore-advanced-mcp\n```\n\n### Avec npx\n\n```bash\nSERVICE_ACCOUNT_KEY_PATH=\"/chemin/vers/votre/serviceAccountKey.json\" npx firestore-advanced-mcp\n```\n\n### Depuis le répertoire cloné\n\n```bash\nSERVICE_ACCOUNT_KEY_PATH=\"/chemin/vers/votre/serviceAccountKey.json\" node index.js\n```\n\n### Configuration dans Claude\n\nPour utiliser ce serveur MCP avec Claude, ajoutez la configuration suivante dans votre fichier `claude_desktop_config.json` :\n\n```json\n\"firebase-mcp\": {\n  \"command\": \"npx\",\n  \"args\": [\"firestore-advanced-mcp\"],\n  \"env\": {\n    \"SERVICE_ACCOUNT_KEY_PATH\": \"/chemin/vers/votre/serviceAccountKey.json\"\n  }\n}\n```\n\nOu pour une version installée localement :\n\n```json\n\"firebase-mcp\": {\n  \"command\": \"node\",\n  \"args\": [\"/chemin/vers/firestore-advanced-mcp/index.js\"],\n  \"env\": {\n    \"SERVICE_ACCOUNT_KEY_PATH\": \"/chemin/vers/votre/serviceAccountKey.json\"\n  }\n}\n```\n\n## 🛠️ Outils disponibles\n\nLe serveur fournit les outils suivants à Claude :\n\n### Opérations de base\n- `firestore_get` - Récupérer un document\n- `firestore_create` - Créer un nouveau document\n- `firestore_update` - Mettre à jour un document existant\n- `firestore_delete` - Supprimer un document\n- `firestore_query` - Exécuter une requête avec filtres\n- `firestore_list_collections` - Lister les collections disponibles\n\n### Requêtes avancées\n- `firestore_collection_group_query` - Requête sur groupes de collections\n- `firestore_composite_query` - Requête avec filtres et tris multiples\n- `firestore_count_documents` - Compter les documents sans tout récupérer\n\n### Types spéciaux et fonctionnalités avancées\n- `firestore_special_data_types` - Gérer les GeoPoints et références\n- `firestore_set_ttl` - Configurer l'expiration automatique des documents\n- `firestore_transaction` - Exécuter une transaction composée de multiples opérations\n- `firestore_batch` - Exécuter des opérations par lot\n- `firestore_field_operations` - Opérations atomiques (increment, arrayUnion, etc.)\n- `firestore_full_text_search` - Recherche textuelle dans les documents\n\n## 📝 Exemples\n\n### Récupérer un document\n```json\n{\n  \"collection\": \"users\",\n  \"id\": \"user123\"\n}\n```\n\n### Créer un document avec référence à un autre document\n```json\n{\n  \"collection\": \"orders\",\n  \"data\": {\n    \"product\": \"Laptop\",\n    \"price\": 999.99,\n    \"fields\": [\n      {\n        \"fieldPath\": \"user\",\n        \"type\": \"reference\",\n        \"value\": \"users/user123\"\n      }\n    ]\n  }\n}\n```\n\n### Configurer TTL sur un document\n```json\n{\n  \"collection\": \"temporaryData\",\n  \"id\": \"session123\",\n  \"expiresIn\": 86400000,\n  \"fieldName\": \"expires_at\"\n}\n```\n\n### Exécuter une requête avec filtres multiples\n```json\n{\n  \"collection\": \"products\",\n  \"filters\": [\n    {\n      \"field\": \"category\",\n      \"operator\": \"==\",\n      \"value\": \"electronics\"\n    },\n    {\n      \"field\": \"price\",\n      \"operator\": \"<\",\n      \"value\": 1000\n    }\n  ],\n  \"orderBy\": {\n    \"field\": \"price\",\n    \"direction\": \"asc\"\n  },\n  \"limit\": 10\n}\n```\n\n## 📄 Licence\n\nCe projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de détails.\n\n## 👨🏽‍💻 Auteur\n\n- Diez7lm\n\n## 🙏 Remerciements\n\n- [Anthropic](https://www.anthropic.com/) pour Claude et le Model Context Protocol\n- [Firebase](https://firebase.google.com/) pour Firestore et les outils de développement\n\n## 🦾 Contribution\n\nLes contributions sont les bienvenues ! N'hésitez pas à soumettre une pull request ou à signaler des problèmes via les issues GitHub.\n\n## 📚 Documentation supplémentaire\n\nPour plus d'informations sur l'utilisation de Firestore avec Firebase, consultez la [documentation officielle de Firebase](https://firebase.google.com/docs/firestore).\n\nPour en savoir plus sur le Model Context Protocol (MCP) et son utilisation avec Claude, consultez la [documentation d'Anthropic](https://docs.anthropic.com/claude/docs/model-context-protocol).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firestore",
        "firebase",
        "databases",
        "firestore databases",
        "firebase firestore",
        "firestore advanced"
      ],
      "category": "databases"
    },
    "direkt--mcp-test": {
      "owner": "direkt",
      "name": "mcp-test",
      "url": "https://github.com/direkt/mcp-test",
      "imageUrl": "/freedevtools/mcp/pfp/direkt.webp",
      "description": "Create and interact with an SQLite database derived from compressed log files, enabling efficient analysis of log data through structured queries. Gain insights into application behavior and performance seamlessly using the Model Context Protocol.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-07T23:12:15Z",
      "readme_content": "# Log Analysis with SQLite MCP Server\n\nThis project provides tools to create an SQLite database from compressed log files and interact with it using the Model Context Protocol (MCP) SQLite server.\n\n## Install instructions\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\n```\n\nPlace log files in the folder as .gz files, then run:\n```bash\npython3 create_log_db.py \n```\n## MCP SQLite Server\n\nTo configure the MCP SQLite server in Cursor-\n\n- Cursor Settings\n- MCP \n- Add New MCP Server\n- Name `SQLlite`\n- Set the type to `command`\n- Put this in the command box \n```bash\nnpx -y @smithery/cli@latest run mcp-server-sqlite-npx --config \"{\\\"databasePath\\\":\\\"/path/to/thedatbase/logs.db\\\"}\"\n```\n\n\n## Contents\n\n- `create_log_db.py`: Script to extract and parse log files into an SQLite database\n- `query_logs.py`: Script to directly query the SQLite database\n- `logs.db`: SQLite database containing parsed log data\n\n## Database Structure\n\nThe database contains the following tables:\n\n### `logs` Table\n\n- `id`: Unique identifier for each log entry\n- `timestamp`: Timestamp of the log entry\n- `thread`: Thread that generated the log\n- `level`: Log level (INFO, WARN, ERROR, DEBUG)\n- `module`: Module that generated the log\n- `message`: Log message content\n- `source_file`: Source log file\n- `raw_log`: Raw log entry\n\n### `stack_traces` Table\n\n- `id`: Unique identifier for each stack trace\n- `log_id`: Reference to the log entry this stack trace belongs to\n- `stack_trace`: Full stack trace text\n\n### `parsing_errors` Table\n\n- `id`: Unique identifier for each parsing error\n- `line`: The line that couldn't be parsed\n- `source_file`: Source log file\n- `error_message`: Error message explaining why parsing failed\n- `timestamp`: When the parsing error occurred\n\nYou can query the database directly using the `query_logs.py` script:\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlite",
        "database",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "dkmaker--mcp-azure-tablestorage": {
      "owner": "dkmaker",
      "name": "mcp-azure-tablestorage",
      "url": "https://github.com/dkmaker/mcp-azure-tablestorage",
      "imageUrl": "/freedevtools/mcp/pfp/dkmaker.webp",
      "description": "Enables interaction with Azure Table Storage for querying and managing data directly through Cline. It supports OData filters, provides access to table schemas, and lists available tables in the storage account.",
      "stars": 5,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-13T01:13:36Z",
      "readme_content": "# Azure TableStore MCP Server\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA TypeScript-based MCP server that enables interaction with Azure Table Storage directly through Cline. This tool allows you to query and manage data in Azure Storage Tables.\n\n<a href=\"https://glama.ai/mcp/servers/8kah8zukke\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/8kah8zukke/badge?refresh=1\" alt=\"mcp-azure-tablestorage MCP server\" /></a>\n\n## Features\n\n- Query Azure Storage Tables with OData filter support\n- Get table schemas to understand data structure\n- List all tables in the storage account\n- Detailed error handling and response information\n- Simple configuration through connection string\n\n## Installation\n\n### Local Development Setup\n\n1. Clone the repository:\n```powershell\ngit clone https://github.com/dkmaker/mcp-azure-tablestorage.git\ncd mcp-azure-tablestorage\n```\n\n2. Install dependencies:\n```powershell\nnpm install\n```\n\n3. Build the server:\n```powershell\nnpm run build\n```\n\n### NPM Installation\n\nYou can install the package globally via npm:\n\n```bash\nnpm install -g dkmaker-mcp-server-tablestore\n```\n\nOr run it directly with npx:\n\n```bash\nnpx dkmaker-mcp-server-tablestore\n```\n\nNote: When using npx or global installation, you'll still need to configure the AZURE_STORAGE_CONNECTION_STRING environment variable.\n\n### Installing in Cline\n\nTo use the Azure TableStore server with Cline, you need to add it to your MCP settings configuration. The configuration file is located at:\n\nWindows: `%APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n\nAdd the following to your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"tablestore\": {\n      \"command\": \"node\",\n      \"args\": [\"C:/path/to/your/mcp-azure-tablestorage/build/index.js\"],\n      \"env\": {\n        \"AZURE_STORAGE_CONNECTION_STRING\": \"your_connection_string_here\"  // Required: Your Azure Storage connection string\n      }\n    }\n  }\n}\n```\n\nReplace `C:/path/to/your/mcp-azure-tablestorage` with the actual path where you cloned the repository.\n\n## Configuration\n\nThe server requires the following environment variable:\n\n- `AZURE_STORAGE_CONNECTION_STRING`: Your Azure Storage account connection string\n\n## Usage in Cline\n\n⚠️ **IMPORTANT SAFETY NOTE**: The query_table tool returns a limited subset of results (default: 5 items) to protect the LLM's context window. DO NOT increase this limit unless explicitly confirmed by the user, as larger result sets can overwhelm the context window.\n\nOnce installed, you can use the Azure TableStore server through Cline. Here are some examples:\n\n1. Querying a table:\n```\nQuery the Users table where PartitionKey is 'ACTIVE'\n```\n\nCline will use the query_table tool with:\n```json\n{\n  \"tableName\": \"Users\",\n  \"filter\": \"PartitionKey eq 'ACTIVE'\",\n  \"limit\": 5  // Optional: Defaults to 5 items. WARNING: Do not increase without user confirmation\n}\n```\n\nThe response will include:\n- Total number of items that match the query (without limit)\n- Limited subset of items (default 5) for safe LLM processing\n- Applied limit value\n\nFor example:\n```json\n{\n  \"totalItems\": 25,\n  \"limit\": 5,\n  \"items\": [\n    // First 5 matching items\n  ]\n}\n```\n\nThis design allows the LLM to understand the full scope of the data while working with a manageable subset. The default limit of 5 items protects against overwhelming the LLM's context window - this limit should only be increased when explicitly confirmed by the user.\n\n2. Getting table schema:\n```\nShow me the schema for the Orders table\n```\n\nCline will use the get_table_schema tool with:\n```json\n{\n  \"tableName\": \"Orders\"\n}\n```\n\n3. Listing tables:\n```\nList all tables in the storage account\n```\n\nCline will use the list_tables tool with:\n```json\n{}\n```\n\n## Project Structure\n\n- `src/index.ts`: Main server implementation with Azure Table Storage interaction logic\n- `build/`: Compiled JavaScript output\n- `package.json`: Project dependencies and scripts\n\n## Dependencies\n\n- @azure/data-tables: Azure Table Storage client library\n- @modelcontextprotocol/sdk: MCP server implementation toolkit\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. This means you can use, modify, distribute, and sublicense the code freely, provided you include the original copyright notice and license terms.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "azure",
        "tablestorage",
        "azure tablestorage",
        "azure table",
        "tablestorage enables"
      ],
      "category": "databases"
    },
    "domdomegg--airtable-mcp-server": {
      "owner": "domdomegg",
      "name": "airtable-mcp-server",
      "url": "https://github.com/domdomegg/airtable-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/domdomegg.webp",
      "description": "Provides read and write access to Airtable databases, enabling interaction with database schemas and record manipulation.",
      "stars": 312,
      "forks": 99,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T03:06:22Z",
      "readme_content": "# airtable-mcp-server\n\nA Model Context Protocol server that provides read and write access to Airtable databases. This server enables LLMs to inspect database schemas, then read and write records.\n\nhttps://github.com/user-attachments/assets/c8285e76-d0ed-4018-94c7-20535db6c944\n\n## Installation\n\n**Step 1**: [Create an Airtable personal access token by clicking here](https://airtable.com/create/tokens/new). Details:\n- Name: Anything you want e.g. 'Airtable MCP Server Token'.\n- Scopes: `schema.bases:read`, `data.records:read`, and optionally `schema.bases:write` and `data.records:write`.\n- Access: The bases you want to access. If you're not sure, select 'Add all resources'.\n\nKeep the token handy, you'll need it in the next step. It should look something like `pat123.abc123` (but longer).\n\n**Step 2**: Follow the instructions below for your preferred client:\n\n- [Claude Desktop](#claude-desktop)\n- [Cursor](#cursor)\n- [Cline](#cline)\n\n### Claude Desktop\n\n#### (Recommended) Via the extensions browser\n\n1. Open Claude Desktop and go to Settings → Extensions\n2. Click 'Browse Extensions' and find 'Airtable MCP Server'\n3. Click 'Install' and paste in your API key\n\n#### (Advanced) Alternative: Via manual .mcpb installation\n\n1. Find the latest mcpb build in [the GitHub Actions history](https://github.com/domdomegg/airtable-mcp-server/actions/workflows/mcpb.yaml?query=branch%3Amaster) (the top one)\n2. In the 'Artifacts' section, download the `airtable-mcp-server-mcpb` file\n3. Rename the `.zip` file to `.mcpb`\n4. Double-click the `.mcpb` file to open with Claude Desktop\n5. Click \"Install\" and configure with your API key\n\n#### (Advanced) Alternative: Via JSON configuration\n\n1. Install [Node.js](https://nodejs.org/en/download)\n2. Open Claude Desktop and go to Settings → Developer\n3. Click \"Edit Config\" to open your `claude_desktop_config.json` file\n4. Add the following configuration to the \"mcpServers\" section, replacing `pat123.abc123` with your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"airtable-mcp-server\"\n      ],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"pat123.abc123\",\n      }\n    }\n  }\n}\n```\n\n5. Save the file and restart Claude Desktop\n\n### Cursor\n\n#### (Recommended) Via one-click install\n\n1. Click [![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=airtable&config=JTdCJTIyY29tbWFuZCUyMiUzQSUyMm5weCUyMC15JTIwYWlydGFibGUtbWNwLXNlcnZlciUyMiUyQyUyMmVudiUyMiUzQSU3QiUyMkFJUlRBQkxFX0FQSV9LRVklMjIlM0ElMjJwYXQxMjMuYWJjMTIzJTIyJTdEJTdE)\n2. Edit your `mcp.json` file to insert your API key\n\n#### (Advanced) Alternative: Via JSON configuration\n\nCreate either a global (`~/.cursor/mcp.json`) or project-specific (`.cursor/mcp.json`) configuration file, replacing `pat123.abc123` with your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"airtable-mcp-server\"],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"pat123.abc123\"\n      }\n    }\n  }\n}\n```\n\n### Cline\n\n#### (Recommended) Via marketplace\n\n1. Click the \"MCP Servers\" icon in the Cline extension\n2. Search for \"Airtable\" and click \"Install\"\n3. Follow the prompts to install the server\n\n#### (Advanced) Alternative: Via JSON configuration\n\n1. Click the \"MCP Servers\" icon in the Cline extension\n2. Click on the \"Installed\" tab, then the \"Configure MCP Servers\" button at the bottom\n3. Add the following configuration to the \"mcpServers\" section, replacing `pat123.abc123` with your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"airtable-mcp-server\"],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"pat123.abc123\"\n      }\n    }\n  }\n}\n```\n\n## Components\n\n### Tools\n\n- **list_records**\n  - Lists records from a specified Airtable table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to query\n    - `maxRecords` (number, optional): Maximum number of records to return. Defaults to 100.\n    - `filterByFormula` (string, optional): Airtable formula to filter records\n\n- **search_records**\n  - Search for records containing specific text\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to query\n    - `searchTerm` (string, required): Text to search for in records\n    - `fieldIds` (array, optional): Specific field IDs to search in. If not provided, searches all text-based fields.\n    - `maxRecords` (number, optional): Maximum number of records to return. Defaults to 100.\n\n- **list_bases**\n  - Lists all accessible Airtable bases\n  - No input parameters required\n  - Returns base ID, name, and permission level\n\n- **list_tables**\n  - Lists all tables in a specific base\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `detailLevel` (string, optional): The amount of detail to get about the tables (`tableIdentifiersOnly`, `identifiersOnly`, or `full`)\n  - Returns table ID, name, description, fields, and views (to the given `detailLevel`)\n\n- **describe_table**\n  - Gets detailed information about a specific table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to describe\n    - `detailLevel` (string, optional): The amount of detail to get about the table (`tableIdentifiersOnly`, `identifiersOnly`, or `full`)\n  - Returns the same format as list_tables but for a single table\n  - Useful for getting details about a specific table without fetching information about all tables in the base\n\n- **get_record**\n  - Gets a specific record by ID\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `recordId` (string, required): The ID of the record to retrieve\n\n- **create_record**\n  - Creates a new record in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `fields` (object, required): The fields and values for the new record\n\n- **update_records**\n  - Updates one or more records in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `records` (array, required): Array of objects containing record ID and fields to update\n\n- **delete_records**\n  - Deletes one or more records from a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `recordIds` (array, required): Array of record IDs to delete\n\n- **create_table**\n  - Creates a new table in a base\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `name` (string, required): Name of the new table\n    - `description` (string, optional): Description of the table\n    - `fields` (array, required): Array of field definitions (name, type, description, options)\n\n- **update_table**\n  - Updates a table's name or description\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `name` (string, optional): New name for the table\n    - `description` (string, optional): New description for the table\n\n- **create_field**\n  - Creates a new field in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `name` (string, required): Name of the new field\n    - `type` (string, required): Type of the field\n    - `description` (string, optional): Description of the field\n    - `options` (object, optional): Field-specific options\n\n- **update_field**\n  - Updates a field's name or description\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `fieldId` (string, required): The ID of the field\n    - `name` (string, optional): New name for the field\n    - `description` (string, optional): New description for the field\n\n### Resources\n\nThe server provides schema information for Airtable bases and tables:\n\n- **Table Schemas** (`airtable://<baseId>/<tableId>/schema`)\n  - JSON schema information for each table\n  - Includes:\n    - Base id and table id\n    - Table name and description\n    - Primary field ID\n    - Field definitions (ID, name, type, description, options)\n    - View definitions (ID, name, type)\n  - Automatically discovered from Airtable's metadata API\n\n## Contributing\n\nPull requests are welcomed on GitHub! To get started:\n\n1. Install Git and Node.js\n2. Clone the repository\n3. Install dependencies with `npm install`\n4. Run `npm run test` to run tests\n5. Build with `npm run build`\n  - You can use `npm run build:watch` to automatically build after editing [`src/index.ts`](./src/index.ts). This means you can hit save, reload Claude Desktop (with Ctrl/Cmd+R), and the changes apply.\n\n## Releases\n\nVersions follow the [semantic versioning spec](https://semver.org/).\n\nTo release:\n\n1. Use `npm version <major | minor | patch>` to bump the version\n2. Run `git push --follow-tags` to push with tags\n3. Wait for GitHub Actions to publish to the NPM registry.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "airtable",
        "airtable databases",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "donghao1393--mcp-dbutils": {
      "owner": "donghao1393",
      "name": "mcp-dbutils",
      "url": "https://github.com/donghao1393/mcp-dbutils",
      "imageUrl": "/freedevtools/mcp/pfp/donghao1393.webp",
      "description": "DButils provides a unified interface for data analysis across multiple database types, including sqlite, mysql, and postgres, with secure connection options. It simplifies connection management and data handling for AI applications.",
      "stars": 81,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-26T12:08:03Z",
      "readme_content": "# MCP 数据库工具\n\n<!-- 项目状态徽章 -->\n[![构建状态](https://img.shields.io/github/workflow/status/donghao1393/mcp-dbutils/Quality%20Assurance?label=tests)](https://github.com/donghao1393/mcp-dbutils/actions)\n[![覆盖率](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/donghao1393/bdd0a63ec2a816539ff8c136ceb41e48/raw/coverage.json)](https://github.com/donghao1393/mcp-dbutils/actions)\n[![质量门禁状态](https://sonarcloud.io/api/project_badges/measure?project=donghao1393_mcp-dbutils&metric=alert_status)](https://sonarcloud.io/dashboard?id=donghao1393_mcp-dbutils)\n\n<!-- 版本和安装徽章 -->\n[![PyPI 版本](https://img.shields.io/pypi/v/mcp-dbutils)](https://pypi.org/project/mcp-dbutils/)\n[![PyPI 下载量](https://img.shields.io/pypi/dm/mcp-dbutils)](https://pypi.org/project/mcp-dbutils/)\n[![Smithery](https://smithery.ai/badge/@donghao1393/mcp-dbutils)](https://smithery.ai/server/@donghao1393/mcp-dbutils)\n\n<!-- 技术规格徽章 -->\n[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)\n[![许可证](https://img.shields.io/github/license/donghao1393/mcp-dbutils)](LICENSE)\n[![GitHub 星标](https://img.shields.io/github/stars/donghao1393/mcp-dbutils?style=social)](https://github.com/donghao1393/mcp-dbutils/stargazers)\n\n[English](README_EN.md) | [Français](README_FR.md) | [Español](README_ES.md) | [العربية](README_AR.md) | [Русский](README_RU.md) | [文档导航](#文档导航)\n\n![Image](https://github.com/user-attachments/assets/26c4f1a1-7b19-4bdd-b9fd-34ad198b0ce3)\n\n## 简介\n\nMCP Database Utilities 是一个多功能的 MCP 服务，它使您的 AI 能够通过统一的连接配置安全地访问各种类型的数据库（SQLite、MySQL、PostgreSQL 等）进行数据分析。\n\n您可以将其视为 AI 系统和数据库之间的安全桥梁，允许 AI 在不直接访问数据库或冒数据修改风险的情况下读取和分析您的数据。\n\n### 核心特性\n\n- **安全优先**：严格只读操作，无直接数据库访问，隔离连接，按需连接，自动超时\n- **隐私保障**：本地处理，最小数据暴露，凭证保护，敏感数据屏蔽\n- **多数据库支持**：使用相同的接口连接 SQLite、MySQL、PostgreSQL\n- **简单配置**：所有数据库连接使用单个 YAML 文件\n- **高级功能**：表格浏览、架构分析和查询执行\n\n> 🔒 **安全说明**：MCP 数据库工具采用安全优先的架构设计，非常适合注重数据保护的企业、初创公司和个人用户。详细了解我们的[安全架构](docs/zh/technical/security.md)。\n\n## 快速入门\n\n我们提供了多种安装方式，包括 uvx、Docker 和 Smithery。详细的安装和配置步骤请参阅[安装指南](docs/zh/installation.md)。\n\n### 基本步骤\n\n1. **安装**：选择适合您的安装方式（[详细说明](docs/zh/installation.md)）\n2. **配置**：创建包含数据库连接信息的 YAML 文件（[配置指南](docs/zh/configuration.md)）\n3. **连接**：将配置添加到您的 AI 客户端\n4. **使用**：开始与您的数据库交互（[使用指南](docs/zh/usage.md)）\n\n### 示例交互\n\n**您**：\"能否列出我的数据库中的所有表？\"\n\n**AI**：\"以下是您的数据库中的表：\n- customers（客户）\n- products（产品）\n- orders（订单）\n- inventory（库存）\"\n\n**您**：\"customers 表的结构是什么样的？\"\n\n**AI**：\"customers 表有以下结构：\n- id（整数，主键）\n- name（文本）\n- email（文本）\n- registration_date（日期）\"\n\n## 文档导航\n\n### 入门指南\n- [安装指南](docs/zh/installation.md) - 详细的安装步骤和配置说明\n- [平台特定安装指南](docs/zh/installation-platform-specific.md) - 针对不同操作系统的安装说明\n- [配置指南](docs/zh/configuration.md) - 数据库连接配置示例和最佳实践\n- [使用指南](docs/zh/usage.md) - 基本操作流程和常见使用场景\n\n### 技术文档\n- [架构设计](docs/zh/technical/architecture.md) - 系统架构和组件说明\n- [安全架构](docs/zh/technical/security.md) - 安全特性和保护机制\n- [开发指南](docs/zh/technical/development.md) - 代码质量和开发流程\n- [测试指南](docs/zh/technical/testing.md) - 测试框架和最佳实践\n- [SonarCloud 集成](docs/zh/technical/sonarcloud-integration.md) - SonarCloud 与 AI 集成指南\n\n### 示例文档\n- [SQLite 示例](docs/zh/examples/sqlite-examples.md) - SQLite 数据库操作示例\n- [PostgreSQL 示例](docs/zh/examples/postgresql-examples.md) - PostgreSQL 数据库操作示例\n- [MySQL 示例](docs/zh/examples/mysql-examples.md) - MySQL 数据库操作示例\n- [高级 LLM 交互示例](docs/zh/examples/advanced-llm-interactions.md) - 与各类 LLM 的高级交互示例\n\n### 多语言文档\n- **英语** - [English Documentation](docs/en/)\n- **法语** - [Documentation Française](docs/fr/)\n- **西班牙语** - [Documentación en Español](docs/es/)\n- **阿拉伯语** - [التوثيق باللغة العربية](docs/ar/)\n- **俄语** - [Документация на русском](docs/ru/)\n\n### 支持与反馈\n- [GitHub Issues](https://github.com/donghao1393/mcp-dbutils/issues) - 报告问题或请求功能\n- [Smithery](https://smithery.ai/server/@donghao1393/mcp-dbutils) - 简化安装和更新\n\n## 可用工具\n\nMCP 数据库工具提供了多种工具，使 AI 能够与您的数据库交互：\n\n- **dbutils-list-connections**：列出配置中的所有可用数据库连接，包括数据库类型、主机、端口和数据库名称等详细信息，同时隐藏密码等敏感信息。\n- **dbutils-list-tables**：列出指定数据库连接中的所有表，包括表名、URI和可用的表描述，按数据库类型分组以便于识别。\n- **dbutils-run-query**：执行只读SQL查询（仅SELECT），支持包括JOIN、GROUP BY和聚合函数在内的复杂查询，返回包含列名和数据行的结构化结果。\n- **dbutils-describe-table**：提供表结构的详细信息，包括列名、数据类型、是否可为空、默认值和注释，以易于阅读的格式呈现。\n- **dbutils-get-ddl**：获取创建指定表的完整DDL（数据定义语言）语句，包括所有列定义、约束和索引。\n- **dbutils-list-indexes**：列出指定表上的所有索引，包括索引名称、类型（唯一/非唯一）、索引方法和包含的列，按索引名称分组。\n- **dbutils-get-stats**：获取表的统计信息，包括估计行数、平均行长度、数据大小和索引大小。\n- **dbutils-list-constraints**：列出表上的所有约束，包括主键、外键、唯一约束和检查约束，对于外键约束还显示引用的表和列。\n- **dbutils-explain-query**：获取SQL查询的执行计划，显示数据库引擎将如何处理查询，包括访问方法、连接类型和估计成本。\n- **dbutils-get-performance**：获取数据库连接的性能指标，包括查询计数、平均执行时间、内存使用情况和错误统计。\n- **dbutils-analyze-query**：分析SQL查询的性能特性，提供执行计划、实际执行时间和具体的优化建议。\n\n有关这些工具的详细说明和使用示例，请参阅[使用指南](docs/zh/usage.md)。\n\n## 星标历史\n\n[![星标历史图表](https://starchart.cc/donghao1393/mcp-dbutils.svg?variant=adaptive)](https://starchart.cc/donghao1393/mcp-dbutils)\n\n## 许可证\n\n本项目采用 MIT 许可证 - 有关详细信息，请参阅 [LICENSE](LICENSE) 文件。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "dbutils",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "dperussina--mssql-mcp-server": {
      "owner": "dperussina",
      "name": "mssql-mcp-server",
      "url": "https://github.com/dperussina/mssql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dperussina.webp",
      "description": "Query and explore Microsoft SQL Server databases using natural language, enabling users to view table structures and execute read-only SQL queries effortlessly. Provides a safe and user-friendly interface for AI assistants to interact with databases without coding experience.",
      "stars": 64,
      "forks": 31,
      "license": "GNU General Public License v3.0",
      "language": "JavaScript",
      "updated_at": "2025-09-20T06:00:50Z",
      "readme_content": "# MS SQL MCP Server 1.1\n\nAn easy-to-use bridge that lets AI assistants like Claude directly query and explore Microsoft SQL Server databases. No coding experience required!\n\n## What Does This Tool Do?\n\nThis tool allows AI assistants to:\n1. **Discover** tables in your SQL Server database\n2. **View** table structures (columns, data types, etc.)\n3. **Execute** read-only SQL queries safely\n4. **Generate** SQL queries from natural language requests\n\n## 🌟 Why You Need This Tool\n\n### Bridge the Gap Between Your Data and AI\n- **No Coding Required**: Give Claude and other AI assistants direct access to your SQL Server databases without writing complex integration code\n- **Maintain Control**: All queries are read-only by default, ensuring your data remains safe\n- **Private & Secure**: Your database credentials stay local and are never sent to external services\n\n### Practical Benefits\n- **Save Hours of Manual Work**: No more copy-pasting data or query results to share with AI\n- **Deeper Analysis**: AI can navigate your entire database schema and provide insights across multiple tables\n- **Natural Language Interface**: Ask questions about your data in plain English\n- **End the Context Limit Problem**: Access large datasets that would exceed normal AI context windows\n\n### Perfect For\n- **Data Analysts** who want AI help interpreting SQL data without sharing credentials\n- **Developers** looking for a quick way to explore database structure through natural conversation\n- **Business Analysts** who need insights without SQL expertise\n- **Database Administrators** who want to provide controlled access to AI tools\n\n## 🚀 Quick Start Guide\n\n### Step 1: Install Prerequisites\n- Install [Node.js](https://nodejs.org/) (version 14 or higher)\n- Have access to a Microsoft SQL Server database (on-premises or Azure)\n\n### Step 2: Clone and Setup\n```bash\n# Clone this repository\ngit clone https://github.com/dperussina/mssql-mcp-server.git\n\n# Navigate to the project directory\ncd mssql-mcp-server\n\n# Install dependencies\nnpm install\n\n# Copy the example environment file\ncp .env.example .env\n```\n\n### Step 3: Configure Your Database Connection\nEdit the `.env` file with your database credentials:\n```\nDB_USER=your_username\nDB_PASSWORD=your_password\nDB_SERVER=your_server_name_or_ip\nDB_DATABASE=your_database_name\nPORT=3333\nHOST=0.0.0.0                    # Host for the server to listen on, e.g., 'localhost' or '0.0.0.0'\nTRANSPORT=stdio\nSERVER_URL=http://localhost:3333\nDEBUG=false                     # Set to 'true' for detailed logging (helpful for troubleshooting)\nQUERY_RESULTS_PATH=/path/to/query_results  # Directory where query results will be saved as JSON files\n```\n\n### Step 4: Start the Server\n```bash\n# Start with default stdio transport\nnpm start\n\n# OR start with HTTP/SSE transport for network access\nnpm run start:sse\n```\n\n### Step 5: Try it out!\n```bash\n# Run the interactive client\nnpm run client\n```\n\n## 📊 Example Use Cases\n\n1. **Explore your database structure without writing SQL**\n   ```javascript\n   mcp_SQL_mcp_discover_database()\n   ```\n\n2. **Get detailed information about a specific table**\n   ```javascript\n   mcp_SQL_mcp_table_details({ tableName: \"Customers\" })\n   ```\n\n3. **Run a safe query**\n   ```javascript\n   mcp_SQL_mcp_execute_query({ sql: \"SELECT TOP 10 * FROM Customers\", returnResults: true })\n   ```\n\n4. **Find tables by name pattern**\n   ```javascript\n   mcp_SQL_mcp_discover_tables({ namePattern: \"%user%\" })\n   ```\n\n5. **Use pagination to navigate large result sets**\n   ```javascript\n   // First page\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT * FROM Users ORDER BY Username OFFSET 0 ROWS FETCH NEXT 10 ROWS ONLY\", \n     returnResults: true \n   })\n   \n   // Next page\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT * FROM Users ORDER BY Username OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY\", \n     returnResults: true \n   })\n   ```\n\n6. **Cursor-based pagination for optimal performance**\n   ```javascript\n   // First page\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT TOP 10 * FROM Users ORDER BY Username\", \n     returnResults: true \n   })\n   \n   // Next page using the last value as cursor\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT TOP 10 * FROM Users WHERE Username > 'last_username' ORDER BY Username\", \n     returnResults: true \n   })\n   ```\n\n7. **Ask natural language questions**\n   ```\n   \"Show me the top 5 customers with the most orders in the last month\"\n   ```\n\n## 💡 Real-World Applications\n\n### For Business Intelligence\n- **Sales Performance Analysis**: \"Show me monthly sales trends for the past year and identify our top-performing products by region.\"\n- **Customer Segmentation**: \"Analyze our customer base by purchase frequency, average order value, and geographical location.\"\n- **Financial Reporting**: \"Create a quarterly profit and loss report comparing this year to last year.\"\n\n### For Database Management\n- **Schema Optimization**: \"Help me identify tables with missing indexes by examining query performance data.\"\n- **Data Quality Auditing**: \"Find all customer records with incomplete information or invalid values.\"\n- **Usage Analysis**: \"Show me which tables are most frequently accessed and what queries are most resource-intensive.\"\n\n### For Development\n- **API Exploration**: \"I'm building an API - help me analyze the database schema to design appropriate endpoints.\"\n- **Query Optimization**: \"Review this complex query and suggest performance improvements.\"\n- **Database Documentation**: \"Create comprehensive documentation of our database structure with explanations of relationships.\"\n\n## 🖥️ Interactive Client Features\n\nThe bundled client provides an easy menu-driven interface:\n\n1. **List available resources** - See what information is available\n2. **List available tools** - See what actions you can perform\n3. **Execute SQL query** - Run a read-only SQL query\n4. **Get table details** - View structure of any table\n5. **Read database schema** - See all tables and their relationships\n6. **Generate SQL query** - Convert natural language to SQL\n\n## 🧠 Effective Prompting & Tool Usage Guide\n\nWhen working with Claude or other AI assistants through this MCP server, the way you phrase your requests significantly impacts the results. Here's how to help the AI use the database tools effectively:\n\n### Basic Tool Call Format\n\nWhen prompting an AI to use this tool, follow this structure:\n\n```\nCan you use the SQL MCP tools to [your goal]?\n\nFor example:\n- Check what tables exist in my database\n- Query the Customers table and show me the first 10 records\n- Find all orders from the past month\n```\n\n### Essential Commands & Syntax\n\nHere are the main tools and their correct syntax:\n\n```javascript\n// Discover the database structure\nmcp_SQL_mcp_discover_database()\n\n// Get detailed information about a specific table\nmcp_SQL_mcp_table_details({ tableName: \"YourTableName\" })\n\n// Execute a query and return results\nmcp_SQL_mcp_execute_query({ \n  sql: \"SELECT * FROM YourTable WHERE Condition\", \n  returnResults: true \n})\n\n// Find tables by name pattern\nmcp_SQL_mcp_discover_tables({ namePattern: \"%pattern%\" })\n\n// Access saved query results (for large result sets)\nmcp_SQL_mcp_get_query_results({ uuid: \"provided-uuid-here\" })\n```\n\n**When to use each tool:**\n- **Database Discovery**: Start with this when the AI is unfamiliar with your database structure.\n- **Table Details**: Use when focusing on a specific table before writing queries.\n- **Query Execution**: When you need to retrieve or analyze actual data.\n- **Table Discovery by Pattern**: When looking for tables related to a specific domain.\n\n### Effective Prompting Patterns\n\n#### Step-by-Step Workflows\nFor complex tasks, guide the AI through a series of steps:\n\n```\nI'd like to analyze our sales data. Please:\n1. First use mcp_SQL_mcp_discover_tables to find tables related to sales\n2. Use mcp_SQL_mcp_table_details to examine the structure of relevant tables\n3. Create a query with mcp_SQL_mcp_execute_query that shows monthly sales by product category\n```\n\n#### Structure First, Then Query\n```\nFirst, discover what tables exist in my database. Then, look at the structure\nof the Customers table. Finally, show me the top 10 customers by total purchase amount.\n```\n\n#### Ask for Explanations\n```\nQuery the top 5 underperforming products based on sales vs. forecasts,\nand explain your approach to writing this query.\n```\n\n### SQL Server Dialect Notes\n\nRemind the AI about SQL Server's specific syntax:\n\n```\nPlease use SQL Server syntax for pagination:\n- For offset/fetch: \"OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY\"\n- For cursor-based: \"WHERE ID > last_id ORDER BY ID\"\n```\n\n### Correcting Tool Usage\n\nIf the AI uses incorrect syntax, you can help it with:\n\n```\nThat's not quite right. Please use this format for the tool call:\nmcp_SQL_mcp_execute_query({ \n  sql: \"SELECT * FROM Customers WHERE Region = 'West'\",\n  returnResults: true\n})\n```\n\n### Troubleshooting Through Prompts\n\nIf the AI is struggling with a database task, try these approaches:\n\n1. **Be more specific about tables:** \"Before writing that query, please check if the CustomerOrders table exists and what columns it has.\"\n\n2. **Break complex tasks into steps:** \"Let's approach this step by step. First, look at the Products table structure. Then, check the Orders table...\"\n\n3. **Ask for intermediate results:** \"Run a simple query on that table first so we can verify the data format before trying more complex analysis.\"\n\n4. **Request query explanations:** \"After writing this query, explain what each part does so I can verify it's doing what I need.\"\n\n## 🔎 Advanced Query Capabilities\n\n### Table Discovery & Exploration\n\nThe MCP Server provides powerful tools for exploring your database structure:\n\n- **Pattern-based table discovery**: Find tables matching specific patterns\n  ```javascript\n  mcp_SQL_mcp_discover_tables({ namePattern: \"%order%\" })\n  ```\n\n- **Schema overview**: Get a high-level view of tables by schema\n  ```javascript\n  mcp_SQL_mcp_execute_query({ \n    sql: \"SELECT TABLE_SCHEMA, COUNT(*) AS TableCount FROM INFORMATION_SCHEMA.TABLES GROUP BY TABLE_SCHEMA\" \n  })\n  ```\n\n- **Column exploration**: Examine column metadata for any table\n  ```javascript\n  mcp_SQL_mcp_table_details({ tableName: \"dbo.Users\" })\n  ```\n\n### Pagination Techniques\n\nThe server supports multiple pagination methods for handling large datasets:\n\n1. **Offset/Fetch Pagination**: Standard SQL pagination using OFFSET and FETCH\n   ```javascript\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT * FROM Users ORDER BY Username OFFSET 0 ROWS FETCH NEXT 10 ROWS ONLY\" \n   })\n   ```\n\n2. **Cursor-Based Pagination**: More efficient for large datasets\n   ```javascript\n   // Get first page\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT TOP 10 * FROM Users ORDER BY Username\" \n   })\n   \n   // Get next page using last value as cursor\n   mcp_SQL_mcp_execute_query({ \n     sql: \"SELECT TOP 10 * FROM Users WHERE Username > 'last_username' ORDER BY Username\" \n   })\n   ```\n\n3. **Count with Data**: Retrieve total count alongside paginated data\n   ```javascript\n   mcp_SQL_mcp_execute_query({ \n     sql: \"WITH TotalCount AS (SELECT COUNT(*) AS Total FROM Users) SELECT TOP 10 u.*, t.Total FROM Users u CROSS JOIN TotalCount t ORDER BY Username\" \n   })\n   ```\n\n### Complex Joins & Relationships\n\nExplore relationships between tables with join operations:\n\n```javascript\nmcp_SQL_mcp_execute_query({ \n  sql: \"SELECT u.Username, u.Email, r.RoleName FROM Users u JOIN UserRoles ur ON u.Username = ur.Username JOIN Roles r ON ur.RoleId = r.RoleId ORDER BY u.Username\"\n})\n```\n\n### Analytical Queries\n\nRun aggregations and analytical queries to gain insights:\n\n```javascript\nmcp_SQL_mcp_execute_query({ \n  sql: \"SELECT UserType, COUNT(*) AS UserCount, SUM(CASE WHEN IsActive = 1 THEN 1 ELSE 0 END) AS ActiveUsers FROM Users GROUP BY UserType\"\n})\n```\n\n### Using SQL Server Features\n\nThe MCP server supports SQL Server-specific features:\n\n- **Common Table Expressions (CTEs)**\n- **Window functions**\n- **JSON operations**\n- **Hierarchical queries**\n- **Full-text search** (when configured in your database)\n\n## 🔗 Integration Options\n\n### Claude Desktop Integration\n\nConnect this tool directly to Claude Desktop in a few easy steps:\n\n1. Install Claude Desktop from [anthropic.com](https://www.anthropic.com/)\n2. Edit Claude's configuration file:\n   - Location: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add this configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"mssql\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/FULL/PATH/TO/mssql-mcp-server/server.mjs\"\n            ]\n        }\n    }\n}\n```\n3. Replace `/FULL/PATH/TO/` with the actual path to where you cloned this repository\n4. Restart Claude Desktop\n5. Look for the tools icon in Claude Desktop - you can now use database commands directly!\n\n### Connecting with Cursor IDE\n\nCursor is an AI-powered code editor that can leverage this tool for advanced database interactions. Here's how to set it up:\n\n#### Setup in Cursor\n\n1. Open Cursor IDE (download from [cursor.sh](https://cursor.sh) if you don't have it)\n2. Start the MS SQL MCP Server using the HTTP/SSE transport:\n   ```bash\n   npm run start:sse\n   ```\n3. Create a new workspace or open an existing project in Cursor\n4. Enter Cursor Settings\n5. Click MCP\n6. Add new MCP server\n7. Name your MCP server, select type: sse\n8. Enter server URL as: localhost:3333/sse (or the port you have it running on)\n\n#### Using Database Commands in Cursor\n\nOnce connected, you can use MCP commands directly in Cursor's AI chat:\n\n1. Ask Claude in Cursor to explore your database:\n   ```\n   Can you show me the tables in my database?\n   ```\n\n2. Execute specific queries:\n   ```\n   Query the top 10 records from the Customers table\n   ```\n\n3. Generate and run complex queries:\n   ```\n   Find all orders from the last month with a value over $1000\n   ```\n\n#### Troubleshooting Cursor Connection\n\n- Make sure the MS SQL MCP Server is running with the HTTP/SSE transport\n- Check that the port is correct and matches what's in your .env file\n- Ensure your firewall isn't blocking the connection\n- If using a different IP/hostname, update the SERVER_URL in your .env file\n\n## 🔄 Transport Methods Explained\n\n### Option 1: stdio Transport (Default)\nBest for: Using directly with Claude Desktop or the bundled client\n```bash\nnpm start\n```\n\n### Option 2: HTTP/SSE Transport\nBest for: Network access or when used with web applications\n```bash\nnpm run start:sse\n```\n\n## 🛡️ Security Features\n\n- **Read-only by default**: No risk of data modification\n- **Private credentials**: Database connection details stay in your `.env` file\n- **SQL injection protection**: Built-in validation for SQL queries\n\n## 🔎 Troubleshooting for New Users\n\n### \"Cannot connect to database\"\n- Check your `.env` file for correct database credentials\n- Make sure your SQL Server is running and accepting connections\n- For Azure SQL, verify your IP is allowed in the firewall settings\n\n### \"Module not found\" errors\n- Run `npm install` again to ensure all dependencies are installed\n- Make sure you're using Node.js version 14 or higher\n\n### \"Transport error\" or \"Connection refused\"\n- For HTTP/SSE transport, verify the PORT in your .env is available\n- Make sure no firewall is blocking the connection\n\n### Claude Desktop can't connect\n- Double-check the path in your `claude_desktop_config.json`\n- Ensure you're using absolute paths, not relative ones\n- Restart Claude Desktop completely after making changes\n\n## 📚 Understanding SQL Server Basics\n\nIf you're new to SQL Server, here are some key concepts:\n\n- **Tables**: Store your data in rows and columns\n- **Schemas**: Logical groupings of tables (like folders)\n- **Queries**: Commands to retrieve or analyze data\n- **Views**: Pre-defined queries saved for easy access\n\nThis tool helps you explore all of these without needing to be a SQL expert!\n\n## 🏗️ Architecture & Core Modules\n\nThe MS SQL MCP Server is built with a modular architecture that separates concerns for maintainability and extensibility:\n\n### Core Modules\n\n#### `database.mjs` - Database Connectivity\n- Manages SQL Server connection pooling\n- Provides query execution with retry logic and error handling\n- Handles database connections, transactions, and configuration\n- Includes utilities for sanitizing SQL and formatting errors\n\n#### `tools.mjs` - Tool Registration\n- Registers all database tools with the MCP server\n- Implements tool validation and parameter checking\n- Provides core functionality for SQL queries, table exploration, and database discovery\n- Maps tool calls to database operations\n\n#### `resources.mjs` - Database Resources\n- Exposes database metadata through resource endpoints\n- Provides schema information, table listings, and procedure documentation\n- Formats database structure information for AI consumption\n- Includes discovery utilities for database exploration\n\n#### `pagination.mjs` - Results Navigation\n- Implements cursor-based pagination for large result sets\n- Provides utilities for generating next/previous page cursors\n- Transforms SQL queries to support pagination\n- Handles SQL Server's OFFSET/FETCH pagination syntax\n\n#### `errors.mjs` - Error Handling\n- Defines custom error types for different failure scenarios\n- Implements JSON-RPC error formatting\n- Provides human-readable error messages\n- Includes middleware for global error handling\n\n#### `logger.mjs` - Logging System\n- Configures Winston logging with multiple transports\n- Provides context-aware request logging\n- Handles log rotation and formatting\n- Captures uncaught exceptions and unhandled rejections\n\n### How These Modules Work Together\n\n1. When a tool call is received, the MCP server routes it to the appropriate handler in `tools.mjs`\n2. The tool handler validates parameters and constructs a database query\n3. The query is executed via functions in `database.mjs`, with possible pagination from `pagination.mjs`\n4. Results are formatted and returned to the client\n5. Any errors are caught and processed through `errors.mjs`\n6. All operations are logged via `logger.mjs`\n\nThis architecture ensures:\n- Clean separation of concerns\n- Consistent error handling\n- Comprehensive logging\n- Efficient database connection management\n- Scalable query execution\n\n## ⚙️ Environment Configuration Explained\n\nThe `.env` file controls how the MS SQL MCP Server connects to your database and operates. Here's a detailed explanation of each setting:\n\n```\n# Database Connection Settings\nDB_USER=your_username           # SQL Server username\nDB_PASSWORD=your_password       # SQL Server password\nDB_SERVER=your_server_name_or_ip\nDB_DATABASE=your_database_name\n\n# Server Configuration\nPORT=3333                       # Port for the HTTP/SSE server to listen on\nHOST=0.0.0.0                    # Host for the server to listen on, e.g., 'localhost' or '0.0.0.0'\nTRANSPORT=stdio                 # Connection method: 'stdio' (for Claude Desktop) or 'sse' (for network connections)\nSERVER_URL=http://localhost:3333 # Base URL when using SSE transport. If HOST is '0.0.0.0', external clients use http://<your-machine-ip>:${PORT}\n\n# Advanced Settings\nDEBUG=false                     # Set to 'true' for detailed logging (helpful for troubleshooting)\nQUERY_RESULTS_PATH=/path/to/query_results  # Directory where query results will be saved as JSON files\n```\n\n### Connection Types Explained\n\n#### stdio Transport\n- Use when connecting directly with Claude Desktop\n- Communication happens through standard input/output streams\n- Set `TRANSPORT=stdio` in your .env file\n- Run with `npm start`\n\n#### HTTP/SSE Transport\n- Use when connecting over a network (like with Cursor IDE)\n- Uses Server-Sent Events (SSE) for real-time communication\n- Set `TRANSPORT=sse` in your .env file\n- Configure `SERVER_URL` to match your server address\n- Run with `npm run start:sse`\n\n### SQL Server Connection Examples\n\n#### Local SQL Server\n```\nDB_USER=sa\nDB_PASSWORD=YourStrongPassword\nDB_SERVER=localhost\nDB_DATABASE=AdventureWorks\n```\n\n#### Azure SQL Database\n```\nDB_USER=azure_admin@myserver\nDB_PASSWORD=YourStrongPassword\nDB_SERVER=myserver.database.windows.net\nDB_DATABASE=AdventureWorks\n```\n\n### Query Results Storage\n\nQuery results are saved as JSON files in the directory specified by `QUERY_RESULTS_PATH`. This prevents large result sets from overwhelming the conversation. You can:\n\n- Leave this blank to use the default `query-results` directory in the project\n- Set a custom path like `/Users/username/Documents/query-results`\n- Access saved results using the provided UUID in the tool response\n\n## 📝 License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mssql",
        "database",
        "dperussina mssql",
        "database access",
        "databases secure"
      ],
      "category": "databases"
    },
    "dpflucas--mysql-mcp-server": {
      "owner": "dpflucas",
      "name": "mysql-mcp-server",
      "url": "https://github.com/dpflucas/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dpflucas.webp",
      "description": "Provides read-only access to MySQL databases, enabling users to list databases, tables, and execute read-only SQL queries while ensuring no data modifications are possible.",
      "stars": 44,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T15:56:51Z",
      "readme_content": "<a href=\"https://glama.ai/mcp/servers/@dpflucas/mysql-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@dpflucas/mysql-mcp-server/badge\" alt=\"mysql-mcp-server MCP server\" />\n</a>\n\n[![npm version](https://img.shields.io/npm/v/mysql-mcp-server?color=blue)](https://www.npmjs.com/package/mysql-mcp-server) [![smithery badge](https://smithery.ai/badge/@dpflucas/mysql-mcp-server)](https://smithery.ai/server/@dpflucas/mysql-mcp-server)\n\n\n# MySQL Database Access MCP Server\n\nThis MCP server provides read-only access to MySQL databases. It allows you to:\n\n- List available databases\n- List tables in a database\n- Describe table schemas\n- Execute read-only SQL queries\n\n## Security Features\n\n- **Read-only access**: Only SELECT, SHOW, DESCRIBE, and EXPLAIN statements are allowed\n- **Query validation**: Prevents SQL injection and blocks any data modification attempts\n- **Query timeout**: Prevents long-running queries from consuming resources\n- **Row limit**: Prevents excessive data return\n\n## Installation\n\n### 1. Install using one of these methods:\n\n#### Install from NPM\n\n```bash\n# Install globally\nnpm install -g mysql-mcp-server\n\n# Or install locally in your project\nnpm install mysql-mcp-server\n```\n\n#### Build from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/dpflucas/mysql-mcp-server.git\ncd mysql-mcp-server\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\n#### Install via Smithery\n\nTo install MySQL Database Access MCP Server for Claude AI automatically via [Smithery](https://smithery.ai/server/@dpflucas/mysql-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @dpflucas/mysql-mcp-server --client claude\n```\n\n### 2. Configure environment variables\n\nThe server requires the following environment variables:\n\n- `MYSQL_HOST`: Database server hostname\n- `MYSQL_PORT`: Database server port (default: 3306)\n- `MYSQL_USER`: Database username\n- `MYSQL_PASSWORD`: Database password (optional, but recommended for secure connections)\n- `MYSQL_DATABASE`: Default database name (optional)\n\n### 3. Add to MCP settings\n\nAdd the following configuration to your MCP settings file:\n\nIf you installed via npm (Option 1):\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"mysql-mcp-server\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your-mysql-host\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your-mysql-user\",\n        \"MYSQL_PASSWORD\": \"your-mysql-password\",\n        \"MYSQL_DATABASE\": \"your-default-database\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nIf you built from source (Option 2):\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mysql-mcp-server/build/index.js\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your-mysql-host\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your-mysql-user\",\n        \"MYSQL_PASSWORD\": \"your-mysql-password\",\n        \"MYSQL_DATABASE\": \"your-default-database\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Available Tools\n\n### list_databases\n\nLists all accessible databases on the MySQL server.\n\n**Parameters**: None\n\n**Example**:\n```json\n{\n  \"server_name\": \"mysql\",\n  \"tool_name\": \"list_databases\",\n  \"arguments\": {}\n}\n```\n\n### list_tables\n\nLists all tables in a specified database.\n\n**Parameters**:\n- `database` (optional): Database name (uses default if not specified)\n\n**Example**:\n```json\n{\n  \"server_name\": \"mysql\",\n  \"tool_name\": \"list_tables\",\n  \"arguments\": {\n    \"database\": \"my_database\"\n  }\n}\n```\n\n### describe_table\n\nShows the schema for a specific table.\n\n**Parameters**:\n- `database` (optional): Database name (uses default if not specified)\n- `table` (required): Table name\n\n**Example**:\n```json\n{\n  \"server_name\": \"mysql\",\n  \"tool_name\": \"describe_table\",\n  \"arguments\": {\n    \"database\": \"my_database\",\n    \"table\": \"my_table\"\n  }\n}\n```\n\n### execute_query\n\nExecutes a read-only SQL query.\n\n**Parameters**:\n- `query` (required): SQL query (only SELECT, SHOW, DESCRIBE, and EXPLAIN statements are allowed)\n- `database` (optional): Database name (uses default if not specified)\n\n**Example**:\n```json\n{\n  \"server_name\": \"mysql\",\n  \"tool_name\": \"execute_query\",\n  \"arguments\": {\n    \"database\": \"my_database\",\n    \"query\": \"SELECT * FROM my_table LIMIT 10\"\n  }\n}\n```\n\n## Advanced Connection Pool Configuration\n\nFor more control over the MySQL connection pool behavior, you can configure additional parameters:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"mysql-mcp-server\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your-mysql-host\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your-mysql-user\",\n        \"MYSQL_PASSWORD\": \"your-mysql-password\",\n        \"MYSQL_DATABASE\": \"your-default-database\",\n        \n        \"MYSQL_CONNECTION_LIMIT\": \"10\",\n        \"MYSQL_QUEUE_LIMIT\": \"0\",\n        \"MYSQL_CONNECT_TIMEOUT\": \"10000\",\n        \"MYSQL_IDLE_TIMEOUT\": \"60000\",\n        \"MYSQL_MAX_IDLE\": \"10\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nThese advanced options allow you to:\n\n- `MYSQL_CONNECTION_LIMIT`: Control the maximum number of connections in the pool (default: 10)\n- `MYSQL_QUEUE_LIMIT`: Set the maximum number of connection requests to queue (default: 0, unlimited)\n- `MYSQL_CONNECT_TIMEOUT`: Adjust the connection timeout in milliseconds (default: 10000)\n- `MYSQL_IDLE_TIMEOUT`: Configure how long a connection can be idle before being released (in milliseconds)\n- `MYSQL_MAX_IDLE`: Set the maximum number of idle connections to keep in the pool\n\n\n## Testing\n\nThe server includes test scripts to verify functionality with your MySQL setup:\n\n### 1. Setup Test Database\n\nThis script creates a test database, table, and sample data:\n\n```bash\n# Set your MySQL credentials as environment variables\nexport MYSQL_HOST=localhost\nexport MYSQL_PORT=3306\nexport MYSQL_USER=your_username\nexport MYSQL_PASSWORD=your_password\n\n# Run the setup script\nnpm run test:setup\n```\n\n### 2. Test MCP Tools\n\nThis script tests each of the MCP tools against the test database:\n\n```bash\n# Set your MySQL credentials as environment variables\nexport MYSQL_HOST=localhost\nexport MYSQL_PORT=3306\nexport MYSQL_USER=your_username\nexport MYSQL_PASSWORD=your_password\nexport MYSQL_DATABASE=mcp_test_db\n\n# Run the tools test script\nnpm run test:tools\n```\n\n### 3. Run All Tests\n\nTo run both setup and tool tests:\n\n```bash\n# Set your MySQL credentials as environment variables\nexport MYSQL_HOST=localhost\nexport MYSQL_PORT=3306\nexport MYSQL_USER=your_username\nexport MYSQL_PASSWORD=your_password\n\n# Run all tests\nnpm test\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check the server logs for error messages\n2. Verify your MySQL credentials and connection details\n3. Ensure your MySQL user has appropriate permissions\n4. Check that your query is read-only and properly formatted\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](../LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "dpflucas mysql",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "dreamme7--mcp-oceanbase": {
      "owner": "dreamme7",
      "name": "mcp-oceanbase",
      "url": "https://github.com/dreamme7/mcp-oceanbase",
      "imageUrl": "/freedevtools/mcp/pfp/dreamme7.webp",
      "description": "Enable secure and structured interactions with OceanBase databases, facilitating listing of tables, reading data, and executing SQL queries through a controlled interface.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-21T03:53:37Z",
      "readme_content": "# mcp-oceanbase\n\nMCP Server for OceanBase database and its tools\n\nEnglish | [简体中文](README_CN.md)\n\n## Features\n\nThis repository contains MCP Servers as following:\n\n| MCP Server           | Description                                                                                     | Document                           |\n|----------------------|-------------------------------------------------------------------------------------------------|------------------------------------|\n| OceanBase MCP Server | A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases. | [Doc](doc/oceanbase_mcp_server.md) |\n\n## Community\n\nDon’t hesitate to ask!\n\nContact the developers and community at [https://ask.oceanbase.com](https://ask.oceanbase.com) if you need any help.\n\n[Open an issue](https://github.com/oceanbase/mcp-oceanbase/issues) if you found a bug.\n\n## Licensing\n\nSee [LICENSE](LICENSE) for more information.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oceanbase",
        "databases",
        "database",
        "oceanbase databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "edwinbernadus--nocodb-mcp-server": {
      "owner": "edwinbernadus",
      "name": "nocodb-mcp-server",
      "url": "https://github.com/edwinbernadus/nocodb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/edwinbernadus.webp",
      "description": "Seamlessly interact with a Nocodb database to perform CRUD operations on its tables using the Model Context Protocol. Manage data efficiently with functionalities for adding, updating, deleting, and retrieving records.",
      "stars": 50,
      "forks": 20,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T14:26:59Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/edwinbernadus-nocodb-mcp-server-badge.jpg)](https://mseep.ai/app/edwinbernadus-nocodb-mcp-server)\n\n# Nocodb MCP Server\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/edwinbernadus/nocodb-mcp-server)](https://archestra.ai/mcp-catalog/edwinbernadus__nocodb-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@edwinbernadus/nocodb-mcp-server)](https://smithery.ai/server/@edwinbernadus/nocodb-mcp-server)\n\n## Introduction\n\nThe NocoDB MCP Server enables seamless interaction with your NocoDB database using the Model Context Protocol (MCP). This server makes it easy to perform CRUD (Create, Read, Update, Delete) operations on NocoDB tables through natural language commands.\n\n## Example Prompt\n\n```text\n[Get Records]\nget data from nocodb, table: Shinobi\n\n[Create Record]\nadd new row, with name: sasuke-2\nadd other row, with name: naruto-2\n\n[Update Record]\nupdate all rows, remove suffix -\n\n[Delete Record]\ndelete all rows with name naruto\n\n[Add Column]\nadd column with name: Age\n\nupdate all rows, set Age to 18\n\n[Delete Column]\ndelete column with name: Age\n```\n\n## Example Prompt - Upload File\n\n```text\n[Create table]\nfrom the json files\nput on nocodb database\ntable name is TableShinobi\n```\nJSON location file in: [example_upload.json](example_upload.json)\n\n## Example Prompt - Bulk Create Records and Bulk Delete Records\n\n\n![bulk_sample1](https://raw.githubusercontent.com/edwinbernadus/nocodb-mcp-server/refs/heads/main/docs/sample-bulk/bulk-screen1.png)\n![bulk_sample2](https://raw.githubusercontent.com/edwinbernadus/nocodb-mcp-server/refs/heads/main/docs/sample-bulk/bulk-screen2.png)\n![bulk_sample3](https://raw.githubusercontent.com/edwinbernadus/nocodb-mcp-server/refs/heads/main/docs/sample-bulk/bulk-screen3.png)\n\n## About This Fork\n\nThis repository is a TypeScript-based fork of [NocoDB-MCP-Server](https://github.com/granthooks/Nocodb-MCP-Server). It retains the core functionality while improving maintainability and compatibility with modern TypeScript development practices.\n\n## Setup\n\nEnsure that Node.js and TypeScript are installed, then execute:\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\nDefine the required environment variables in a `.env` file:\n\n```env\nNOCODB_URL=https://your-nocodb-instance.com\nNOCODB_API_TOKEN=your_api_token_here\nNOCODB_BASE_ID=your_base_id_here\n```\n\n**Tip:** You can copy the template from [env.example](env.example) and fill in your values.\n\n### How to Obtain NOCODB_BASE_ID\n\nTo find your `NOCODB_BASE_ID`, check the URL of your Nocodb instance.  \nFor example:\nhttps://app.nocodb.com/#/wi6evls6/pqmob3ammcknma5/maty9c5xkmf4012  \nIn this URL format:\n\n```text\nhttps://app.nocodb.com/#/{USERNAME}/{NOCODB_BASE_ID}/{TABLE_ID}\n```\n\n## Integration with Claude Desktop\n\nModify `claude_desktop_config.json` to include:\n\n```json\n{\n  \"mcpServers\": {\n    \"nocodb\": {\n      \"command\": \"node\",\n      \"args\": [\"{working_folder}/dist/start.js\"],\n      \"env\": {\n        \"NOCODB_URL\": \"https://your-nocodb-instance.com\",\n        \"NOCODB_BASE_ID\": \"your_base_id_here\",\n        \"NOCODB_API_TOKEN\": \"your_api_token_here\"\n      }\n    }\n  }\n}\n```\n\n## Direct call from CLI\n\nYou can directly call the MCP server from the command line:  \nNOCODB_URL, NOCODB_API_TOKEN, and NOCODB_BASE_ID are required parameters.  \n`NOCODB_URL=https://app.nocodb.com` if you are using NocoDB cloud.\n\n```bash\nnpx -y nocodb-mcp-server {NOCODB_URL} {NOCODB_BASE_ID} {NOCODB_API_TOKEN} \n```\n\n## Testing CLI\n\nTo run the tests, execute:\n\n```bash\nnpx -y @wong2/mcp-cli npx nocodb-mcp-server {NOCODB_URL} {NOCODB_BASE_ID} {NOCODB_API_TOKEN} \n```\n\n## API Functions\n\nFor detailed information about available API functions, please refer to [API_FUNCTION.md](API_FUNCTION.md).\n\n## Project Structure\n\n```text\n/project-root\n  ├── src/            # TypeScript source files\n  ├── dist/           # Compiled JavaScript output\n  ├── .env            # Environment variable configurations\n  ├── package.json    # Project dependencies and scripts\n  ├── tsconfig.json   # TypeScript settings\n```\n\n## Contribution Guidelines\n\nContributions are encouraged! Feel free to open issues or submit pull requests.\n\n## License\n\nThis project is distributed under MIT.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nocodb",
        "databases",
        "database",
        "nocodb database",
        "nocodb mcp",
        "interact nocodb"
      ],
      "category": "databases"
    },
    "elber-code--database-tools": {
      "owner": "elber-code",
      "name": "database-tools",
      "url": "https://github.com/elber-code/database-tools",
      "imageUrl": "/freedevtools/mcp/pfp/elber-code.webp",
      "description": "Query MySQL databases directly using Claude AI, executing SQL queries and retrieving formatted results for user-friendly reading. Access metadata about tables, including their size and structure.",
      "stars": 4,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T18:17:49Z",
      "readme_content": "# Database Tools for Claude AI\n\n[![smithery badge](https://smithery.ai/badge/@elber-code/database-tools)](https://smithery.ai/server/@elber-code/database-tools)\n\nThis is an MCP (Model Context Protocol) server that allows Claude AI to interact directly with MySQL databases.\n\n## Features\n\n- Query MySQL databases through Claude\n- Execute any valid SQL query\n- Get information about tables, including size and structure\n- Formatted results for easy reading in Claude\n\n## Installation\n\n### Installing via Smithery\n\nTo install Database Tools for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@elber-code/database-tools):\n\n```bash\nnpx -y @smithery/cli install @elber-code/database-tools --client claude\n```\n\nTo install and use this tool, follow these steps:\n\n1. **Clone or download the repository**\n   ```\n   git clone [repository-url]\n   ```\n   or download and extract the ZIP file.\n\n2. **Install dependencies**\n   Navigate to the project directory and run:\n   ```\n   npm install\n   ```\n\n## Configuration\n\nFor Claude to use this tool, you need to add the configuration to your `claude_desktop_config.json` file, which is typically located at:\n\n```json\nC:\\Users\\YOUR_USER\\AppData\\Roaming\\Claude\\claude_desktop_config.json\n```\n\nWith the following structure:\n\n```json\n{\n  \"mcpServers\": {\n    // Other existing configurations...\n    \"database-tools\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\path\\\\to\\\\index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nOnce configured, you can interact with your MySQL databases from Claude with commands like:\n\n1. **List all databases**  \n   \"Execute query in MySQL to show me the databases.\"\n\n2. **View tables in a database**  \n   \"Execute query in MySQL to show me the table `name_table`.\"\n\n3. **Query the size of a table**  \n   \"Execute query in MySQL to show me the size of the table `name_table`.\"\n\n4. **Execute custom queries**  \n   \"Execute query in MySQL: 'The description of what you want your query to do.'\"\n\n## Security\n\nThis tool runs with the permissions configured in the `mysql.js` file. Make sure the credentials provided have only the necessary permissions for the operations you want to allow.\n\n## Troubleshooting\n\nIf you have connection problems, check:\n- That MySQL is running\n- That the credentials in `mysql.js` are correct\n- That the path in the Claude configuration file is correct\n\n## Implementation\n\nTo query databases, simply ask Claude something like:\n\"Show me all databases in my MySQL\" or \"What is the size of the users table?\"\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database tools"
      ],
      "category": "databases"
    },
    "endaoment--endaoment-postgres-mcp": {
      "owner": "endaoment",
      "name": "endaoment-postgres-mcp",
      "url": "https://github.com/endaoment/endaoment-postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/endaoment.webp",
      "description": "Connects AI models to a PostgreSQL database, executing SQL queries and retrieving schema information through a standardized protocol. Enables real-time data access and handles connection errors gracefully.",
      "stars": 1,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-03T06:55:21Z",
      "readme_content": "# Model Context Protocol PostgreSQL Server\n\nThis project implements a Model Context Protocol (MCP) server that connects to a PostgreSQL database. It allows AI models to interact with your database through a standardized protocol.\n\n## Features\n\n- Connects to a PostgreSQL database using connection pooling\n- Implements the Model Context Protocol for AI model interaction\n- Provides database schema information as resources\n- Allows executing SQL queries with retry logic\n- Handles connection errors gracefully\n\n## Prerequisites\n\n- Node.js 20 or higher\n- PostgreSQL database\n- Access credentials for the database\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n## Configuration\n\nThe server reads database credentials from a `.env` file in the project root directory. You need to add your database credentials as a JSON string in the `DB_CREDENTIALS` environment variable:\n\n1. Create a `.env` file in the project root:\n\n```bash\ntouch .env\n```\n\n2. Add the following line with your actual database credentials:\n\n```bash\nexport DB_CREDENTIALS='{\"DB_USER\":\"your-username\",\"DB_PASSWORD\":\"your-password\",\"DB_HOST\":\"your-host\",\"DB_PORT\":\"5433\",\"DB_NAME\":\"your-database\"}'\n```\n\n### Fallback to Shell Config Files\n\nIf the `.env` file is not present or the credentials variable is not found, the server will automatically look for the credentials in your shell configuration files in the following order:\n\n1. `~/.zshrc`\n2. `~/.bashrc` \n3. `~/.bash_profile`\n4. `~/.profile`\n\nThis is especially useful in environments where shell config files are not automatically sourced, such as the Cursor MCP environment.\n\nTo set up credentials in any of your shell config files:\n\n1. Open your preferred shell config file, for example:\n\n```bash\nnano ~/.zshrc\n# or\nnano ~/.bashrc\n```\n\n2. Add the following line with your actual database credentials:\n\n```bash\nexport DB_CREDENTIALS='{\"DB_USER\":\"your-username\",\"DB_PASSWORD\":\"your-password\",\"DB_HOST\":\"your-host\",\"DB_PORT\":\"5433\",\"DB_NAME\":\"your-database\"}'\n```\n\nThe server will automatically detect and use these credentials when the `.env` file is not available.\n\n### Custom Credentials Variable\n\nYou can also use a custom environment variable name instead of `DB_CREDENTIALS` by using the `--credentials-var` flag when starting the server:\n\n```bash\nnode server.js --credentials-var MY_CUSTOM_DB_CREDS\n```\n\nIn this case, you would define `MY_CUSTOM_DB_CREDS` in your `.env` file instead.\n\n### Combining Options\n\nYou can combine different command-line options as needed:\n\n```bash\n# Use custom credentials and enable verbose mode\nnode server.js --credentials-var MY_CUSTOM_DB_CREDS --verbose\n\n# Short form also works\nnode server.js -c MY_CUSTOM_DB_CREDS -v\n```\n\n## Usage\n\nStart the MCP server:\n\n```bash\n# Directly with Node.js\nnode server.js\n\n# Or with npm\nnpm start\n```\n\n### Logging Options\n\nBy default, the server runs in silent mode, displaying only error messages. If you want to see all log messages, you can use the verbose flag:\n\n```bash\n# With verbose logging\nnode server.js --verbose\n\n# Or with npm\nnpm start -- --verbose\n```\n\nYou can also use the short flag `-v`:\n\n```bash\nnode server.js -v\n```\n\nThe server will:\n1. Test the database connection\n2. Start the MCP server using stdio transport\n3. Handle requests from AI models\n\n## Integration with Cursor\n\nThis server supports the Model Context Protocol (MCP) and integrates with Cursor AI. \n\n### Automatic Configuration\n\nThis project includes a pre-configured `.cursor/mcp.json` file for automatic setup within Cursor.\n\n### Manual Configuration\n\nTo manually add this server to Cursor:\n\n1. Go to Cursor Settings → Features → MCP\n2. Click \"+ Add New MCP Server\"\n3. Enter the following details:\n   - **Name**: Postgres MCP\n   - **Type**: stdio\n   - **Command**: `node /full/path/to/server.js`\n   \nFor more information on MCP integration with Cursor, see the [official documentation](https://cursor.sh/docs/mcp).\n\n## Available Tools\n\nThe server provides the following tools to AI models:\n\n- `query`: Execute SQL queries with retry logic\n\n## Resources\n\nThe server exposes database tables as resources, allowing AI models to:\n\n- List all tables in the database\n- View schema information for each table\n\n## Error Handling\n\nThe server includes:\n\n- Connection retry logic\n- Detailed error logging\n- Graceful shutdown handling\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Database Connection Failed**\n   - Check if PostgreSQL is running: `pg_isready -h localhost -p 5433`\n   - Verify your credentials in the `.env` file are correct\n   - Make sure your IP address has access to the database (check pg_hba.conf)\n   - Try connecting with another tool like `psql` to verify credentials\n\n2. **Environment Variable Problems**\n   - Make sure your `.env` file is in the project root directory\n   - Check that the JSON structure in `DB_CREDENTIALS` is valid\n   - Verify there are no extra spaces or line breaks in the JSON string\n   - Test with: `node -e \"console.log(JSON.parse(process.env.DB_CREDENTIALS))\" < .env`\n\n3. **Node.js Version Issues**\n   - Check your Node.js version: `node -v`\n   - This server requires Node.js 20+\n   - If using an older version, install Node.js 20: `nvm install 20 && nvm use 20`\n\n### Cursor Integration\n\n1. **Server Not Showing in Cursor**\n   - Make sure the `.cursor/mcp.json` file exists and is properly formatted\n   - Try restarting Cursor to detect the project-specific configuration\n   - Check Cursor logs for any error messages\n\n2. **\"Failed to create client\" Error**\n   - This usually indicates the server crashed during startup\n   - Run the server manually with verbose logging to see the error: `node server.js -v`\n   - Check if the database credentials are accessible in the Cursor environment\n\n3. **No Tools Available in Cursor**\n   - Ensure the server is running properly (check logs)\n   - Try clicking the refresh button in the MCP tool panel\n   - Restart Cursor and try again\n\n### PostgreSQL Specific Issues\n\n1. **Permission Denied Errors**\n   - Make sure the database user has appropriate permissions for the tables\n   - Try granting required permissions: `GRANT SELECT ON ALL TABLES IN SCHEMA public TO username;`\n\n2. **\"Relation does not exist\" Errors**\n   - Verify that the table exists: `\\dt tablename` in psql\n   - Check if you're connecting to the correct database\n   - Ensure the user has access to the schema where the table is located\n\n3. **Performance Issues**\n   - Large query results may cause lag, consider adding LIMIT clauses\n   - Check if your database needs optimization (indexes, vacuuming)\n\nFor additional help, you can run the server with verbose logging (`-v` flag) to see detailed error messages and operation logs.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "postgresql",
        "postgres",
        "secure database",
        "databases secure",
        "endaoment postgres"
      ],
      "category": "databases"
    },
    "enemyrr--mcp-mysql-server": {
      "owner": "enemyrr",
      "name": "mcp-mysql-server",
      "url": "https://github.com/enemyrr/mcp-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/enemyrr.webp",
      "description": "Enable interaction with MySQL databases through a standardized interface, facilitating operations such as querying, inserting, updating, and deleting records.",
      "stars": 27,
      "forks": 11,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-21T22:59:48Z",
      "readme_content": "# @enemyrr/mcp-mysql-server\n\n[![smithery badge](https://smithery.ai/badge/@enemyrr/mcp-mysql-server)](https://smithery.ai/server/@enemyrr/mcp-mysql-server)\n\nA Model Context Protocol server that provides MySQL database operations. This server enables AI models to interact with MySQL databases through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/hcqqd3qi8q\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/hcqqd3qi8q/badge\" alt=\"MCP-MySQL Server MCP server\" /></a>\n\n## Installation & Setup for Cursor IDE\n\n### Installing via Smithery\n\nTo install MySQL Database Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@enemyrr/mcp-mysql-server):\n\n```bash\nnpx -y @smithery/cli install @enemyrr/mcp-mysql-server --client claude\n```\n\n### Installing Manually\n1. Clone and build the project:\n```bash\ngit clone https://github.com/enemyrr/mcp-mysql-server.git\ncd mcp-mysql-server\nnpm install\nnpm run build\n```\n\n2. Add the server in Cursor IDE settings:\n   - Open Command Palette (Cmd/Ctrl + Shift + P)\n   - Search for \"MCP: Add Server\"\n   - Fill in the fields:\n     - Name: `mysql`\n     - Type: `command`\n     - Command: `node /absolute/path/to/mcp-mysql-server/build/index.js`\n\n> **Note**: Replace `/absolute/path/to/` with the actual path where you cloned and built the project.\n\n## Database Configuration\n\nYou can configure the database connection in three ways:\n\n1. **Database URL in .env** (Recommended):\n```env\nDATABASE_URL=mysql://user:password@host:3306/database\n```\n\n2. **Individual Parameters in .env**:\n```env\nDB_HOST=localhost\nDB_USER=your_user\nDB_PASSWORD=your_password\nDB_DATABASE=your_database\n```\n\n3. **Direct Connection via Tool**:\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"connect_db\",\n  arguments: {\n    url: \"mysql://user:password@host:3306/database\"\n    // OR\n    workspace: \"/path/to/your/project\" // Will use project's .env\n    // OR\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n## Available Tools\n\n### 1. connect_db\nConnect to MySQL database using URL, workspace path, or direct credentials.\n\n### 2. query\nExecute SELECT queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_tables\nList all tables in the connected database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"list_tables\"\n});\n```\n\n### 5. describe_table\nGet the structure of a specific table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n### 6. create_table\nCreate a new table with specified fields and indexes.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"create_table\",\n  arguments: {\n    table: \"users\",\n    fields: [\n      {\n        name: \"id\",\n        type: \"int\",\n        autoIncrement: true,\n        primary: true\n      },\n      {\n        name: \"email\",\n        type: \"varchar\",\n        length: 255,\n        nullable: false\n      }\n    ],\n    indexes: [\n      {\n        name: \"email_idx\",\n        columns: [\"email\"],\n        unique: true\n      }\n    ]\n  }\n});\n```\n\n### 7. add_column\nAdd a new column to an existing table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"add_column\",\n  arguments: {\n    table: \"users\",\n    field: {\n      name: \"phone\",\n      type: \"varchar\",\n      length: 20,\n      nullable: true\n    }\n  }\n});\n```\n\n## Features\n\n- Multiple connection methods (URL, workspace, direct)\n- Secure connection handling with automatic cleanup\n- Prepared statement support for query parameters\n- Schema management tools\n- Comprehensive error handling and validation\n- TypeScript support\n- Automatic workspace detection\n\n## Security\n\n- Uses prepared statements to prevent SQL injection\n- Supports secure password handling through environment variables\n- Validates queries before execution\n- Automatically closes connections when done\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Connection failures\n- Invalid queries or parameters\n- Missing configuration\n- Database errors\n- Schema validation errors\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/enemyrr/mcp-mysql-server\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "ergut--mcp-bigquery-server": {
      "owner": "ergut",
      "name": "mcp-bigquery-server",
      "url": "https://github.com/ergut/mcp-bigquery-server",
      "imageUrl": "",
      "description": "Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities",
      "stars": 124,
      "forks": 31,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T17:15:12Z",
      "readme_content": "# BigQuery MCP Server\n[![smithery badge](https://smithery.ai/badge/@ergut/mcp-bigquery-server)](https://smithery.ai/protocol/@ergut/mcp-bigquery-server)\n<div align=\"center\">\n  \n</div>\n\n## What is this? 🤔\n\nThis is a server that lets your LLMs (like Claude) talk directly to your BigQuery data! Think of it as a friendly translator that sits between your AI assistant and your database, making sure they can chat securely and efficiently.\n\n### Quick Example\n```text\nYou: \"What were our top 10 customers last month?\"\nClaude: *queries your BigQuery database and gives you the answer in plain English*\n```\n\nNo more writing SQL queries by hand - just chat naturally with your data!\n\n## How Does It Work? 🛠️\n\nThis server uses the Model Context Protocol (MCP), which is like a universal translator for AI-database communication. While MCP is designed to work with any AI model, right now it's available as a developer preview in Claude Desktop.\n\nHere's all you need to do:\n1. Set up authentication (see below)\n2. Add your project details to Claude Desktop's config file\n3. Start chatting with your BigQuery data naturally!\n\n### What Can It Do? 📊\n\n- Run SQL queries by just asking questions in plain English\n- Access both tables and materialized views in your datasets\n- Explore dataset schemas with clear labeling of resource types (tables vs views)\n- Analyze data within safe limits (1GB query limit by default)\n- Keep your data secure (read-only access)\n\n## Quick Start 🚀\n\n### Prerequisites\n- Node.js 14 or higher\n- Google Cloud project with BigQuery enabled\n- Either Google Cloud CLI installed or a service account key file\n- Claude Desktop (currently the only supported LLM interface)\n\n### Option 1: Quick Install via Smithery (Recommended)\nTo install BigQuery MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/protocol/@ergut/mcp-bigquery-server), run this command in your terminal:\n\n```bash\nnpx @smithery/cli install @ergut/mcp-bigquery-server --client claude\n```\nThe installer will prompt you for:\n\n- Your Google Cloud project ID\n- BigQuery location (defaults to us-central1)\n\nOnce configured, Smithery will automatically update your Claude Desktop configuration and restart the application.\n\n### Option 2: Manual Setup\nIf you prefer manual configuration or need more control:\n\n1. **Authenticate with Google Cloud** (choose one method):\n   - Using Google Cloud CLI (great for development):\n     ```bash\n     gcloud auth application-default login\n     ```\n   - Using a service account (recommended for production):\n     ```bash\n     # Save your service account key file and use --key-file parameter\n     # Remember to keep your service account key file secure and never commit it to version control\n     ```\n\n2. **Add to your Claude Desktop config**\n   Add this to your `claude_desktop_config.json`:\n\n   - Basic configuration:\n     ```json\n     {\n       \"mcpServers\": {\n         \"bigquery\": {\n           \"command\": \"npx\",\n           \"args\": [\n             \"-y\",\n             \"@ergut/mcp-bigquery-server\",\n             \"--project-id\",\n             \"your-project-id\",\n             \"--location\",\n             \"us-central1\"\n           ]\n         }\n       }\n     }\n     ```\n\n   - With service account:\n     ```json\n     {\n       \"mcpServers\": {\n         \"bigquery\": {\n           \"command\": \"npx\",\n           \"args\": [\n             \"-y\",\n             \"@ergut/mcp-bigquery-server\",\n             \"--project-id\",\n             \"your-project-id\",\n             \"--location\",\n             \"us-central1\",\n             \"--key-file\",\n             \"/path/to/service-account-key.json\"\n           ]\n         }\n       }\n     }\n     ```\n     \n\n3. **Start chatting!** \n   Open Claude Desktop and start asking questions about your data.\n\n### Command Line Arguments\n\nThe server accepts the following arguments:\n- `--project-id`: (Required) Your Google Cloud project ID\n- `--location`: (Optional) BigQuery location, defaults to 'us-central1'\n- `--key-file`: (Optional) Path to service account key JSON file\n\nExample using service account:\n```bash\nnpx @ergut/mcp-bigquery-server --project-id your-project-id --location europe-west1 --key-file /path/to/key.json\n```\n\n### Permissions Needed\n\nYou'll need one of these:\n- `roles/bigquery.user` (recommended)\n- OR both:\n  - `roles/bigquery.dataViewer`\n  - `roles/bigquery.jobUser`\n\n## Developer Setup (Optional) 🔧\n\nWant to customize or contribute? Here's how to set it up locally:\n\n```bash\n# Clone and install\ngit clone https://github.com/ergut/mcp-bigquery-server\ncd mcp-bigquery-server\nnpm install\n\n# Build\nnpm run build\n```\n\nThen update your Claude Desktop config to point to your local build:\n```json\n{\n  \"mcpServers\": {\n    \"bigquery\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/clone/mcp-bigquery-server/dist/index.js\",\n        \"--project-id\",\n        \"your-project-id\",\n        \"--location\",\n        \"us-central1\",\n        \"--key-file\",\n        \"/path/to/service-account-key.json\"\n      ]\n    }\n  }\n}\n```\n\n## Current Limitations ⚠️\n\n- MCP support is currently only available in Claude Desktop (developer preview)\n- Connections are limited to local MCP servers running on the same machine\n- Queries are read-only with a 1GB processing limit\n- While both tables and views are supported, some complex view types might have limitations\n\n## Support & Resources 💬\n\n- 🐛 [Report issues](https://github.com/ergut/mcp-bigquery-server/issues)\n- 💡 [Feature requests](https://github.com/ergut/mcp-bigquery-server/issues)\n- 📖 [Documentation](https://github.com/ergut/mcp-bigquery-server)\n\n## License 📝\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## Author ✍️ \n\nSalih Ergüt\n\n## Sponsorship\n\nThis project is proudly sponsored by:\n\n<div align=\"center\">\n  <a href=\"https://www.oredata.com\">\n    \n  </a>\n</div>\n\n## Version History 📋\n\nSee [CHANGELOG.md](CHANGELOG.md) for updates and version history.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bigquery",
        "databases",
        "database",
        "bigquery database",
        "bigquery server",
        "google bigquery"
      ],
      "category": "databases"
    },
    "evansims--openfga-mcp": {
      "owner": "evansims",
      "name": "openfga-mcp",
      "url": "https://github.com/evansims/openfga-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/evansims.webp",
      "description": "Enables Large Language Models to interact with OpenFGA stores, facilitating reading, searching, and manipulation tasks. Supports fine-grained authorization through agentic AI and vibe coding.",
      "stars": 10,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "PHP",
      "updated_at": "2025-09-07T03:29:04Z",
      "readme_content": "<div align=\"center\">\n  <p><a href=\"https://openfga.dev\"></a></p>\n\n  <h1>OpenFGA MCP Server</h1>\n\n  <p>\n    <a href=\"https://codecov.io/gh/evansims/openfga-mcp\" target=\"_blank\"><img src=\"https://codecov.io/gh/evansims/openfga-mcp/graph/badge.svg?token=DG6KWF1EG6\" alt=\"codecov\" /></a>\n    <a href=\"https://shepherd.dev/github/evansims/openfga-mcp\" target=\"_blank\"><img src=\"https://shepherd.dev/github/evansims/openfga-mcp/coverage.svg\" alt=\"Psalm Type Coverage\" /></a>\n    <a href=\"https://www.bestpractices.dev/projects/10901\"><img alt=\"badge\" src=\"https://www.bestpractices.dev/projects/10901/badge\"></a>\n  </p>\n\n  <p>AI-powered authorization management for OpenFGA</p>\n</div>\n\n<p><br /></p>\n\nConnect [OpenFGA](https://openfga.dev/) and [Auth0 FGA](https://auth0.com/fine-grained-authorization) to AI agents via the Model Context Protocol.\n\n## Use Cases\n\n- **Plan & Design** - Design efficient authorization model using best practice patterns\n- **Generate Code** - Generate accurate SDK integrations with comprehensive documentation context\n- **Manage Instances** - Query and control live OpenFGA servers through AI agents\n\n## Quick Start\n\n### Offline Mode (Default)\n\nDesign models and generate code without a server:\n\n```json\n{\n  \"mcpServers\": {\n    \"OpenFGA\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--pull=always\",\n        \"evansims/openfga-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Online Mode\n\nConnect to OpenFGA for full management capabilities:\n\n```json\n{\n  \"mcpServers\": {\n    \"OpenFGA\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--pull=always\",\n        \"-e\",\n        \"OPENFGA_MCP_API_URL=http://host.docker.internal:8080\",\n        \"evansims/openfga-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n> **Safety:** Write operations are disabled by default. Set `OPENFGA_MCP_API_WRITEABLE=true` to enable.\n\n> **Docker Networking:** For your `OPENFGA_MCP_API_URL` use `host.docker.internal` when running OpenFGA on your local machine, container names for Docker networks, or full URLs for remote instances.\n\nWorks with [Claude Desktop](https://claude.ai/download), [Claude Code](https://www.anthropic.com/claude-code), [Cursor](https://cursor.sh), [Windsurf](https://windsurf.com), [Zed](https://zed.dev), and other MCP clients.\n\n## Configuration\n\n### MCP Transport\n\n| Variable                          | Default     | Description                                                                     |\n| --------------------------------- | ----------- | ------------------------------------------------------------------------------- |\n| `OPENFGA_MCP_TRANSPORT`           | `stdio`     | Supports `stdio` or `http` (Streamable HTTP.)                                   |\n| `OPENFGA_MCP_TRANSPORT_HOST`      | `127.0.0.1` | IP to listen for connections on. Only applicable when using `http` transport.   |\n| `OPENFGA_MCP_TRANSPORT_PORT`      | `9090`      | Port to listen for connections on. Only applicable when using `http` transport. |\n| `OPENFGA_MCP_TRANSPORT_SSE`       | `true`      | Enables Server-Sent Events (SSE) streams for responses.                         |\n| `OPENFGA_MCP_TRANSPORT_STATELESS` | `false`     | Enables stateless mode for session-less clients.                                |\n\n### OpenFGA\n\n| Variable                    | Default | Description                                         |\n| --------------------------- | ------- | --------------------------------------------------- |\n| `OPENFGA_MCP_API_URL`       |         | OpenFGA server URL                                  |\n| `OPENFGA_MCP_API_WRITEABLE` | `false` | Enables write operations                            |\n| `OPENFGA_MCP_API_STORE`     |         | Default requests to a specific store ID             |\n| `OPENFGA_MCP_API_MODEL`     |         | Default requests to a specific model ID             |\n| `OPENFGA_MCP_API_RESTRICT`  | `false` | Restrict requests to configured default store/model |\n\n### OpenFGA Authentication\n\n| Authentication     | Variable                        | Default | Description   |\n| ------------------ | ------------------------------- | ------- | ------------- |\n| Pre-Shared Keys    | `OPENFGA_MCP_API_TOKEN`         |         | API Token     |\n| Client Credentials | `OPENFGA_MCP_API_CLIENT_ID`     |         | Client ID     |\n|                    | `OPENFGA_MCP_API_CLIENT_SECRET` |         | Client Secret |\n|                    | `OPENFGA_MCP_API_ISSUER`        |         | Token Issuer  |\n|                    | `OPENFGA_MCP_API_AUDIENCE`      |         | API Audience  |\n\nSee [`docker-compose.example.yml`](docker-compose.example.yml) for complete examples.\n\n## Features\n\n### Management Tools\n\n- **Stores**: Create, list, get, delete stores\n- **Models**: Create models with [DSL](https://openfga.dev/docs/configuration-language), list, get, verify\n- **Permissions**: Check, grant, revoke permissions; query users and objects\n\n### SDK Documentation\n\nComprehensive documentation for accurate code generation:\n\n- All OpenFGA SDKs (PHP, Go, Python, Java, .NET, JavaScript, Laravel)\n- Class and method documentation with code examples\n- Advanced search with language filtering\n\n### AI Prompts\n\n**Design & Planning**\n\n- Domain-specific model design\n- RBAC to ReBAC migration\n- Hierarchical relationships\n- Performance optimization\n\n**Implementation**\n\n- Step-by-step model creation\n- Relationship patterns\n- Test generation\n- Security patterns\n\n**Troubleshooting**\n\n- Permission debugging\n- Security audits\n- Least privilege implementation\n\n### Resources & URIs\n\n- `openfga://stores` - List stores\n- `openfga://store/{id}/model/{modelId}` - Model details\n- `openfga://docs/{sdk}/class/{className}` - SDK documentation\n- `openfga://docs/search/{query}` - Search documentation\n\n### Smart Completions\n\nAuto-completion for store IDs, model IDs, relations, users, and objects when connected.\n\n---\n\n- [Contributing](./.github/CONTRIBUTING.md) | [Apache 2.0 License](./LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openfga",
        "databases",
        "database",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "f4ww4z--mcp-mysql-server": {
      "owner": "f4ww4z",
      "name": "mcp-mysql-server",
      "url": "https://github.com/f4ww4z/mcp-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/f4ww4z.webp",
      "description": "Provides operations for interacting with MySQL databases through a standardized interface for AI models.",
      "stars": 115,
      "forks": 24,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T03:55:07Z",
      "readme_content": "# @f4ww4z/mcp-mysql-server\n[![smithery badge](https://smithery.ai/badge/@f4ww4z/mcp-mysql-server)](https://smithery.ai/server/@f4ww4z/mcp-mysql-server)\n\nA Model Context Protocol server that provides MySQL database operations. This server enables AI models to interact with MySQL databases through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/qma33al6ie\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/qma33al6ie/badge\" alt=\"mcp-mysql-server MCP server\" /></a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install MySQL Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@f4ww4z/mcp-mysql-server):\n\n```bash\nnpx -y @smithery/cli install @f4ww4z/mcp-mysql-server --client claude\n```\n\n### Manual Installation\n```bash\nnpx @f4ww4z/mcp-mysql-server\n```\n\n## Configuration\n\nThe server requires the following environment variables to be set in your MCP settings configuration file:\n\n> recommended use\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@f4ww4z/mcp-mysql-server\", \"mysql://user:password@localhost:port/database\"],\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@f4ww4z/mcp-mysql-server\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"your_host\",\n        \"MYSQL_USER\": \"your_user\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/index.ts\n```\n## Available Tools\n\n### 1. connect_db\nEstablish connection to MySQL database using provided credentials.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"connect_db\",\n  arguments: {\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n### 2. query\nExecute SELECT queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_tables\nList all tables in the connected database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"list_tables\",\n  arguments: {}\n});\n```\n\n### 5. describe_table\nGet the structure of a specific table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n## Features\n\n- Secure connection handling with automatic cleanup\n- Prepared statement support for query parameters\n- Comprehensive error handling and validation\n- TypeScript support\n- Automatic connection management\n\n## Security\n\n- Uses prepared statements to prevent SQL injection\n- Supports secure password handling through environment variables\n- Validates queries before execution\n- Automatically closes connections when done\n\n## Error Handling\n\nThe server provides detailed error messages for common issues:\n- Connection failures\n- Invalid queries\n- Missing parameters\n- Database errors\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/f4ww4z/mcp-mysql-server\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "farhankaz--redis-mcp": {
      "owner": "farhankaz",
      "name": "redis-mcp",
      "url": "https://github.com/farhankaz/redis-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/farhankaz.webp",
      "description": "Provides access to Redis database operations, enabling interaction with various data types and commands within Redis environments.",
      "stars": 6,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-22T23:35:15Z",
      "readme_content": "# Redis MCP Server\n[![smithery badge](https://smithery.ai/badge/redis-mcp)](https://smithery.ai/server/redis-mcp)\n\nA Model Context Protocol (MCP) server that provides access to Redis database operations.\n\n<a href=\"https://glama.ai/mcp/servers/cbn7lsbp7h\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/cbn7lsbp7h/badge\" alt=\"Redis Server MCP server\" /></a>\n\n## Project Structure\n\n```\nsrc/\n├── interfaces/\n│   └── types.ts           # Shared TypeScript interfaces and types\n├── tools/\n│   ├── base_tool.ts       # Abstract base class for Redis tools\n│   ├── tool_registry.ts   # Registry managing all available Redis tools\n│   ├── hmset_tool.ts      # HMSET Redis operation\n│   ├── hget_tool.ts       # HGET Redis operation\n│   ├── hgetall_tool.ts    # HGETALL Redis operation\n│   ├── scan_tool.ts       # SCAN Redis operation\n│   ├── set_tool.ts        # SET Redis operation\n│   ├── get_tool.ts        # GET Redis operation\n│   ├── del_tool.ts        # DEL Redis operation\n│   ├── zadd_tool.ts       # ZADD Redis operation\n│   ├── zrange_tool.ts     # ZRANGE Redis operation\n│   ├── zrangebyscore_tool.ts # ZRANGEBYSCORE Redis operation\n│   └── zrem_tool.ts       # ZREM Redis operation\n└── redis_server.ts        # Main server implementation\n```\n\n## Available Tools\n\n| Tool | Type | Description | Input Schema |\n|------|------|-------------|--------------|\n| hmset | Hash Command | Set multiple hash fields to multiple values | `key`: string (Hash key)<br>`fields`: object (Field-value pairs to set) |\n| hget | Hash Command | Get the value of a hash field | `key`: string (Hash key)<br>`field`: string (Field to get) |\n| hgetall | Hash Command | Get all fields and values in a hash | `key`: string (Hash key) |\n| scan | Key Command | Scan Redis keys matching a pattern | `pattern`: string (Pattern to match, e.g., \"user:*\")<br>`count`: number, optional (Number of keys to return) |\n| set | String Command | Set string value with optional NX and PX options | `key`: string (Key to set)<br>`value`: string (Value to set)<br>`nx`: boolean, optional (Only set if not exists)<br>`px`: number, optional (Expiry in milliseconds) |\n| get | String Command | Get string value | `key`: string (Key to get) |\n| del | Key Command | Delete a key | `key`: string (Key to delete) |\n| zadd | Sorted Set Command | Add one or more members to a sorted set | `key`: string (Sorted set key)<br>`members`: array of objects with `score`: number and `value`: string |\n| zrange | Sorted Set Command | Return a range of members from a sorted set by index | `key`: string (Sorted set key)<br>`start`: number (Start index)<br>`stop`: number (Stop index)<br>`withScores`: boolean, optional (Include scores in output) |\n| zrangebyscore | Sorted Set Command | Return members from a sorted set with scores between min and max | `key`: string (Sorted set key)<br>`min`: number (Minimum score)<br>`max`: number (Maximum score)<br>`withScores`: boolean, optional (Include scores in output) |\n| zrem | Sorted Set Command | Remove one or more members from a sorted set | `key`: string (Sorted set key)<br>`members`: array of strings (Members to remove) |\n| sadd | Set Command | Add one or more members to a set | `key`: string (Set key)<br>`members`: array of strings (Members to add to the set) |\n| smembers | Set Command | Get all members in a set | `key`: string (Set key) |\n\n## Usage\n\nConfigure in your MCP client (e.g., Claude Desktop, Cline):\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"npx\",\n      \"args\": [\"redis-mcp\", \"--redis-host\", \"localhost\", \"--redis-port\", \"6379\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n## Command Line Arguments\n\n- `--redis-host`: Redis server host (default: localhost)\n- `--redis-port`: Redis server port (default: 6379)\n\n### Installing via Smithery\n\nTo install Redis Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/redis-mcp):\n\n```bash\nnpx -y @smithery/cli install redis-mcp --client claude\n```\n\n## Development\n\nTo add a new Redis tool:\n\n1. Create a new tool class in `src/tools/` extending `RedisTool`\n2. Define the tool's interface in `src/interfaces/types.ts`\n3. Register the tool in `src/tools/tool_registry.ts`\n\nExample tool implementation:\n\n```typescript\nexport class MyTool extends RedisTool {\n  name = 'mytool';\n  description = 'Description of what the tool does';\n  inputSchema = {\n    type: 'object',\n    properties: {\n      // Define input parameters\n    },\n    required: ['requiredParam']\n  };\n\n  validateArgs(args: unknown): args is MyToolArgs {\n    // Implement argument validation\n  }\n\n  async execute(args: unknown, client: RedisClientType): Promise<ToolResponse> {\n    // Implement tool logic\n  }\n}\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/tools/zrangebyscore_tool.ts\n```\n## License\n\nMIT: https://opensource.org/license/mit\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "redis",
        "databases",
        "database",
        "redis database",
        "access redis",
        "secure database"
      ],
      "category": "databases"
    },
    "felores--airtable-mcp": {
      "owner": "felores",
      "name": "airtable-mcp",
      "url": "https://github.com/felores/airtable-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/felores.webp",
      "description": "Interact programmatically with Airtable's API for managing bases, tables, fields, and records, facilitating structured data management and creation processes.",
      "stars": 69,
      "forks": 33,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2025-09-28T20:25:56Z",
      "readme_content": "# Airtable MCP Server\n\nA Model Context Protocol server that provides tools for interacting with Airtable's API. This server enables programmatic management of Airtable bases, tables, fields, and records through Claude Desktop or other MCP clients.\n\nThis MCP server features a specialized implementation that allows it to build tables in stages, leveraging Claude's agentic capabilities and minimizing the failure rate typically seen in other MCP servers for Airtable when building complex tables. It also includes [system prompt](https://github.com/felores/airtable-mcp/blob/main/prompts/system-prompt.md) and [project knowledge](https://github.com/felores/airtable-mcp/blob/main/prompts/project-knowledge.md) markdown files to provide additional guidance for the LLM when leveraging projects in Claude Desktop.\n\n## Requirements: Node.js\n\n1. Install Node.js (version 18 or higher) and npm from [nodejs.org](https://nodejs.org/)\n2. Verify installation:\n   ```bash\n   node --version\n   npm --version\n   ```\n\n⚠️ **Important**: Before running, make sure to setup your Airtable API key\n\n## Obtaining an Airtable API Key\n\n1. Log in to your Airtable account at [airtable.com](https://airtable.com)\n2. Create a personal access token at [Airtable's Builder Hub](https://airtable.com/create/tokens)\n3. In the Personal access token section select these scopes: \n     - data.records:read\n     - data.records:write\n     - schema.bases:read\n     - schema.bases:write\n4. Select the workspace or bases you want to give access to the personal access token\n5. Keep this key secure - you'll need it for configuration\n\n## Installation\n\n### Method 1: Using npx (Recommended)\n1. Navigate to the Claude configuration directory:\n\n   - Windows: `C:\\Users\\NAME\\AppData\\Roaming\\Claude`\n   - macOS: `~/Library/Application Support/Claude/`\n   \n   You can also find these directories inside the Claude Desktop app: Claude Desktop > Settings > Developer > Edit Config\n\n2. Create or edit `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\"@felores/airtable-mcp-server\"],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\nNote: For Windows paths, use double backslashes (\\\\) or forward slashes (/).\n\n### Method 2: Using mcp-installer:\nmcp-installer is a MCP server to install other MCP servers.\n1. Install [mcp-installer](https://github.com/anaisbetts/mcp-installer)\n2. Install the Airtable MCP server by prompting Claude Desktop:\n```bash\nInstall @felores/airtable-mcp-server set the environment variable AIRTABLE_API_KEY to 'your_api_key'\n```\nClaude will install the server, modify the configuration file and set the environment variable AIRTABLE_API_KEY to your Airtable API key.\n\n### Method 3: Local Development Installation\nIf you want to contribute or modify the code run this in your terminal:\n```bash\n# Clone the repository\ngit clone https://github.com/felores/airtable-mcp.git\ncd airtable-mcp\n\n# Install dependencies\nnpm install\n\n# Build the server\nnpm run build\n\n# Run locally\nnode build/index.js\n```\nThen modify the Claude Desktop configuration file to use the local installation:\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/airtable-mcp/build/index.js\"],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Verifying Installation\n\n1. Start Claude Desktop\n2. The Airtable MCP server should be listed in the \"Connected MCP Servers\" section\n3. Test with a simple command:\n```\nList all bases\n```\n\n## Features\n\n### Available Operations\n\n#### Base Management\n- `list_bases`: List all accessible Airtable bases\n- `list_tables`: List all tables in a base\n- `create_table`: Create a new table with fields\n- `update_table`: Update a table's name or description\n\n#### Field Management\n- `create_field`: Add a new field to a table\n- `update_field`: Modify an existing field\n\n#### Record Operations\n- `list_records`: Retrieve records from a table\n- `create_record`: Add a new record\n- `update_record`: Modify an existing record\n- `delete_record`: Remove a record\n- `search_records`: Find records matching criteria\n- `get_record`: Get a single record by its ID\n\n### Field Types\n- `singleLineText`: Single line text field\n- `multilineText`: Multi-line text area\n- `email`: Email address field\n- `phoneNumber`: Phone number field\n- `number`: Numeric field with optional precision\n- `currency`: Money field with currency symbol\n- `date`: Date field with format options\n- `singleSelect`: Single choice from options\n- `multiSelect`: Multiple choices from options\n\n### Field Colors\nAvailable colors for select fields:\n- `blueBright`, `redBright`, `greenBright`\n- `yellowBright`, `purpleBright`, `pinkBright`\n- `grayBright`, `cyanBright`, `orangeBright`\n- `blueDark1`, `greenDark1`\n\n## Contributing\n\nWe welcome contributions to improve the Airtable MCP server! Here's how you can contribute:\n\n1. Fork the Repository\n   - Visit https://github.com/felores/airtable-mcp\n   - Click the \"Fork\" button in the top right\n   - Clone your fork locally:\n     ```bash\n     git clone https://github.com/your-username/airtable-mcp.git\n     ```\n\n2. Create a Feature Branch\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n3. Make Your Changes\n   - Follow the existing code style\n   - Add tests if applicable\n   - Update documentation as needed\n\n4. Commit Your Changes\n   ```bash\n   git add .\n   git commit -m \"feat: add your feature description\"\n   ```\n\n5. Push to Your Fork\n   ```bash\n   git push origin feature/your-feature-name\n   ```\n\n6. Create a Pull Request\n   - Go to your fork on GitHub\n   - Click \"New Pull Request\"\n   - Select your feature branch\n   - Describe your changes in detail\n\n### Development Guidelines\n\n- Use TypeScript for new code\n- Follow semantic commit messages\n- Update documentation for new features\n- Add examples for new functionality\n- Test your changes thoroughly\n\n### Getting Help\n\n- Open an issue for bugs or feature requests\n- Join discussions in existing issues\n- Ask questions in pull requests\n\nYour contributions help make this tool better for everyone. Whether it's:\n- Adding new features\n- Fixing bugs\n- Improving documentation\n- Suggesting enhancements\n\nWe appreciate your help in making the Airtable MCP server more powerful and user-friendly!\n\n## License\n\n[MIT](LICENSE)\n\n---\n\nMade with ❤️ by the Airtable MCP community\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "airtable",
        "felores airtable",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "ferrants--memvid-mcp-server": {
      "owner": "ferrants",
      "name": "memvid-mcp-server",
      "url": "https://github.com/ferrants/memvid-mcp-server",
      "imageUrl": "",
      "description": "Python Streamable HTTP Server you can run locally to interact with [memvid](https://github.com/Olow304/memvid) storage and semantic search.",
      "stars": 4,
      "forks": 3,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-07-31T15:31:49Z",
      "readme_content": "# memvid-mcp-server\n\n\nA Streamable-HTTP MCP Server that uses [memvid](https://github.com/Olow304/memvid) to encode text data into videos that can be quickly looked up with semantic search.\n\n\nSupported Actions:\n- `add_chunks`: Adds chunks to the memory video. Note: each time you add chunks, it resets the memory.mp4. Unsure if there is a way to incrementally add.\n- `search`: queries for the top-matching chunks. Returns 5 by default, but can be changed with top_k param.\n\n## Running\n\nSet up your environment:\n```bash\npython3.11 -m venv my_env\n. ./my_env/bin/activate\npip install -r requirements.txt\n```\n\nRun the server:\n```bash\npython server.py\n```\n\nWith a custom port:\n\n```bash\nPORT=3002 python server.py\n```\n\n## Connect a Client\n\nYou can connect a client to your MCP Server once it's running. Configure per the client's configuration. There is the [mcp-config.json](/mcp-config.json) that has an example configuration that looks like this:\n```json\n{\n  \"mcpServers\": {\n    \"memvid\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"http://localhost:3000\"\n    }\n  }\n}\n```\n\n### Acknowledgements\n\n- Obviously the modelcontextprotocol and Anthropic teams for the MCP Specification. [https://modelcontextprotocol.io/introduction](https://modelcontextprotocol.io/introduction)\n- [HeyFerrante](https://heyferrante.com?ref=github-memvid-mcp-server) for enabling and sponsoring this project.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "memvid",
        "memvid storage",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "fireproof-storage--mcp-database-server": {
      "owner": "fireproof-storage",
      "name": "mcp-database-server",
      "url": "https://github.com/fireproof-storage/mcp-database-server",
      "imageUrl": "",
      "description": "Fireproof ledger database with multi-user sync",
      "stars": 27,
      "forks": 10,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2025-09-24T03:02:54Z",
      "readme_content": "# Model Context Protocol and Fireproof Demo: JSON Document Server\n\nThis is a simple example of how to use a [Fireproof](https://fireproof.storage/) database in a [Model Context Protocol](https://github.com/modelcontextprotocol) server (used for plugging code and data into A.I. systems such as [Claude Desktop](https://claude.ai/download)).\n\nThis demo server implements a basic JSON document store with CRUD operations (Create, Read, Update, Delete) and the ability to query documents sorted by any field.\n\n# Installation\n\nInstall dependencies:\n\n```bash\nnpm install\nnpm build\n```\n\n## Running the Server\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"fireproof\": {\n      \"command\": \"/path/to/fireproof-mcp/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "security",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "fnf-deepHeading--mcp-snowflake-reader": {
      "owner": "fnf-deepHeading",
      "name": "mcp-snowflake-reader",
      "url": "https://github.com/fnf-deepHeading/mcp-snowflake-reader",
      "imageUrl": "/freedevtools/mcp/pfp/fnf-deepHeading.webp",
      "description": "Provides secure, read-only access to Snowflake databases for streamlined data retrieval processes, ideal for analytics and reporting without modifying data.",
      "stars": 0,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-04T00:06:35Z",
      "readme_content": "# MCP Snowflake Reader\n\n[English](#english) | [한국어](#korean)\n\n[![smithery badge](https://smithery.ai/badge/@fnf-deepHeading/mcp-snowflake-reader)](https://smithery.ai/server/@fnf-deepHeading/mcp-snowflake-reader)\n\n## English\n\nA read-only MCP server for Snowflake databases. This server provides secure, read-only access to Snowflake databases through the MCP protocol.\n\n### Features\n\n- **Read-only Access**: Secure read-only access to Snowflake databases\n\n### Setup\n\n#### Snowflake Connection\n\nThe Snowflake connection information should be provided as a JSON string in the following format:\n\n```json\n{\n  \"account\": \"your-account\",\n  \"user\": \"your-user\",\n  \"password\": \"your-password\",\n  \"warehouse\": \"your-warehouse\",\n  \"database\": \"your-database\",\n  \"schema\": \"your-schema\",\n  \"role\": \"your-role\"\n}\n```\n\n#### MCP Client Configuration\n\nAdd the following configuration to your MCP client settings file (Cursor AI or Claude):\n\n##### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-snowflake-reader\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"mcp-snowflake-reader\",\n        \"--connection\",\n        \"{\\\"account\\\":\\\"your-account\\\",\\\"user\\\":\\\"your-user\\\",\\\"password\\\":\\\"your-password\\\",\\\"warehouse\\\":\\\"your-warehouse\\\",\\\"database\\\":\\\"your-database\\\",\\\"schema\\\":\\\"your-schema\\\",\\\"role\\\":\\\"your-role\\\"}\"\n      ]\n    }\n  }\n}\n```\n\n##### UVX\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-snowflake-reader\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-snowflake-reader\",\n        \"--connection\",\n        \"{\\\"account\\\":\\\"your-account\\\",\\\"user\\\":\\\"your-user\\\",\\\"password\\\":\\\"your-password\\\",\\\"warehouse\\\":\\\"your-warehouse\\\",\\\"database\\\":\\\"your-database\\\",\\\"schema\\\":\\\"your-schema\\\",\\\"role\\\":\\\"your-role\\\"}\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install Snowflake Reader for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@fnf-deepHeading/mcp-snowflake-reader):\n\n```bash\nnpx -y @smithery/cli install @fnf-deepHeading/mcp-snowflake-reader --client claude\n```\n\n### Limitations\n\n- Only read-only operations are allowed\n- Table names can only contain alphanumeric characters, underscores, and dots\n- The following SQL keywords are prohibited:\n  - INSERT\n  - UPDATE\n  - DELETE\n  - DROP\n  - TRUNCATE\n  - ALTER\n  - CREATE\n  - GRANT\n  - REVOKE\n  - COMMIT\n  - ROLLBACK\n\n### License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Korean\n\nSnowflake 데이터베이스의 테이블을 읽어오는 MCP(Microservice Control Protocol) 서버입니다.\n\n### 주요 기능\n\n- **읽기 전용 접근**: Snowflake 데이터베이스에 대한 안전한 읽기 전용 접근\n\n### 설정\n\n#### Snowflake 연결 정보\n\nSnowflake 연결 정보는 다음과 같은 형식으로 JSON 문자열로 제공됩니다:\n\n```json\n{\n  \"account\": \"your-account\",\n  \"user\": \"your-user\",\n  \"password\": \"your-password\",\n  \"warehouse\": \"your-warehouse\",\n  \"database\": \"your-database\",\n  \"schema\": \"your-schema\",\n  \"role\": \"your-role\"\n}\n```\n\n#### MCP 클라이언트 설정\n\nCursor AI나 Claude와 같은 MCP 클라이언트의 설정 파일에 다음 설정을 추가하세요:\n\n##### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-snowflake-reader\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"mcp-snowflake-reader\",\n        \"--connection\",\n        \"{\\\"account\\\":\\\"your-account\\\",\\\"user\\\":\\\"your-user\\\",\\\"password\\\":\\\"your-password\\\",\\\"warehouse\\\":\\\"your-warehouse\\\",\\\"database\\\":\\\"your-database\\\",\\\"schema\\\":\\\"your-schema\\\",\\\"role\\\":\\\"your-role\\\"}\"\n      ]\n    }\n  }\n}\n```\n\n##### UVX\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-snowflake-reader\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-snowflake-reader\",\n        \"--connection\",\n        \"{\\\"account\\\":\\\"your-account\\\",\\\"user\\\":\\\"your-user\\\",\\\"password\\\":\\\"your-password\\\",\\\"warehouse\\\":\\\"your-warehouse\\\",\\\"database\\\":\\\"your-database\\\",\\\"schema\\\":\\\"your-schema\\\",\\\"role\\\":\\\"your-role\\\"}\"\n      ]\n    }\n  }\n}\n```\n\n### Smithery 사용하여 설치\n\n[Smithery](https://smithery.ai/server/@fnf-deepHeading/mcp-snowflake-reader)를 통해 Claude Desktop에서 Snowflake Reader를 자동으로 설치하려면:\n\n```bash\nnpx -y @smithery/cli install @fnf-deepHeading/mcp-snowflake-reader --client claude\n```\n\n### 제한사항\n\n- 읽기 전용 작업만 허용됩니다\n- 테이블 이름은 영숫자, 언더스코어, 점만 허용됩니다\n- 다음 SQL 키워드는 금지됩니다:\n  - INSERT\n  - UPDATE\n  - DELETE\n  - DROP\n  - TRUNCATE\n  - ALTER\n  - CREATE\n  - GRANT\n  - REVOKE\n  - COMMIT\n  - ROLLBACK\n\n### 라이선스\n\n이 프로젝트는 MIT 라이선스를 따릅니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "fnf",
        "snowflake databases",
        "access fnf",
        "databases secure"
      ],
      "category": "databases"
    },
    "freema--mcp-gsheets": {
      "owner": "freema",
      "name": "mcp-gsheets",
      "url": "https://github.com/freema/mcp-gsheets",
      "imageUrl": "",
      "description": "MCP server for Google Sheets API integration with comprehensive reading, writing, formatting, and sheet management capabilities.",
      "stars": 29,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-22T13:01:34Z",
      "readme_content": "# MCP Google Sheets Server\n\n<a href=\"https://glama.ai/mcp/servers/@freema/mcp-gsheets\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@freema/mcp-gsheets/badge\" />\n</a>\n\n[![npm version](https://badge.fury.io/js/mcp-gsheets.svg)](https://www.npmjs.com/package/mcp-gsheets)\n![CI](https://github.com/freema/mcp-gsheets/workflows/CI/badge.svg)\n![Coverage](https://codecov.io/gh/freema/mcp-gsheets/branch/main/graph/badge.svg)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n![TypeScript](https://img.shields.io/badge/TypeScript-5.0%2B-007ACC?logo=typescript&logoColor=white)\n![Node](https://img.shields.io/badge/Node.js-18%2B-339933?logo=node.js&logoColor=white)\n![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?logo=prettier&logoColor=white)\n\nA Model Context Protocol (MCP) server for Google Sheets API integration. Enables reading, writing, and managing Google Sheets documents directly from your MCP client (e.g., Claude Desktop).\n\n## 🚀 Quick Start\n\n### 1. Prerequisites\n\n- Node.js v18 or higher\n- Google Cloud Project with Sheets API enabled\n- Service Account with JSON key file\n\n### 2. Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/freema/mcp-gsheets.git\n# Or using SSH\n# git clone git@github.com:freema/mcp-gsheets.git\ncd mcp-gsheets\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n### 3. Google Cloud Setup\n\n1. Go to [Google Cloud Console](https://console.cloud.google.com)\n2. Create a new project or select existing\n3. Enable Google Sheets API:\n   - Navigate to \"APIs & Services\" → \"Library\"\n   - Search for \"Google Sheets API\" and click \"Enable\"\n4. Create Service Account:\n   - Go to \"APIs & Services\" → \"Credentials\"\n   - Click \"Create Credentials\" → \"Service Account\"\n   - Download the JSON key file\n5. Share your spreadsheets:\n   - Open your Google Sheet\n   - Click Share and add the service account email (from JSON file)\n   - Grant \"Editor\" permissions\n\n### 4. Configure MCP Client\n\n#### Easy Setup (Recommended)\n\nRun the interactive setup script:\n\n```bash\nnpm run setup\n```\n\nThis will:\n- Guide you through the configuration\n- Automatically detect your Node.js installation (including nvm)\n- Find your Claude Desktop config\n- Create the proper JSON configuration\n- Optionally create a .env file for development\n\n#### Manual Setup\n\nIf you prefer manual configuration, add to your Claude Desktop config:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- Linux: `~/.config/claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-gsheets\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-gsheets/dist/index.js\"],\n      \"env\": {\n        \"GOOGLE_PROJECT_ID\": \"your-project-id\",\n        \"GOOGLE_APPLICATION_CREDENTIALS\": \"/absolute/path/to/service-account-key.json\"\n      }\n    }\n  }\n}\n```\n\n#### Alternative: JSON String Authentication\n\nInstead of using a file path, you can provide the service account credentials directly as a JSON string. This is useful for containerized environments, CI/CD pipelines, or when you want to avoid managing credential files.\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-gsheets\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-gsheets/dist/index.js\"],\n      \"env\": {\n        \"GOOGLE_PROJECT_ID\": \"your-project-id\",\n        \"GOOGLE_SERVICE_ACCOUNT_KEY\": \"{\\\"type\\\":\\\"service_account\\\",\\\"project_id\\\":\\\"your-project\\\",\\\"private_key_id\\\":\\\"...\\\",\\\"private_key\\\":\\\"-----BEGIN PRIVATE KEY-----\\\\n...\\\\n-----END PRIVATE KEY-----\\\\n\\\",\\\"client_email\\\":\\\"...@....iam.gserviceaccount.com\\\",\\\"client_id\\\":\\\"...\\\",\\\"auth_uri\\\":\\\"https://accounts.google.com/o/oauth2/auth\\\",\\\"token_uri\\\":\\\"https://oauth2.googleapis.com/token\\\",\\\"auth_provider_x509_cert_url\\\":\\\"https://www.googleapis.com/oauth2/v1/certs\\\",\\\"client_x509_cert_url\\\":\\\"...\\\"}\"\n      }\n    }\n  }\n}\n```\n\n**Note**: When using `GOOGLE_SERVICE_ACCOUNT_KEY`:\n- The entire JSON must be on a single line\n- All quotes must be escaped with backslashes\n- Newlines in the private key must be represented as `\\\\n`\n- If the JSON includes a `project_id`, you can omit `GOOGLE_PROJECT_ID`\n\nRestart Claude Desktop after adding the configuration.\n\n## 📦 Build & Development\n\n### Development Commands\n\n```bash\n# Development mode with hot reload\nnpm run dev\n\n# Build for production\nnpm run build\n\n# Type checking\nnpm run typecheck\n\n# Clean build artifacts\nnpm run clean\n\n# Run MCP inspector for debugging\nnpm run inspector\n\n# Run MCP inspector in development mode\nnpm run inspector:dev\n```\n\n### Task Runner (Alternative)\n\nIf you have [Task](https://taskfile.dev) installed:\n\n```bash\n# Install dependencies\ntask install\n\n# Build the project\ntask build\n\n# Run in development mode\ntask dev\n\n# Run linter\ntask lint\n\n# Format code\ntask fmt\n\n# Run all checks\ntask check\n```\n\n### Development Setup\n\n1. Create `.env` file for testing:\n```bash\ncp .env.example .env\n# Edit .env with your credentials:\n# GOOGLE_PROJECT_ID=your-project-id\n# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n# TEST_SPREADSHEET_ID=your-test-spreadsheet-id\n```\n\n2. Run in development mode:\n```bash\nnpm run dev  # Watch mode with auto-reload\n```\n\n## 📋 Available Tools\n\n### Reading Data\n- `sheets_get_values` - Read from a range\n- `sheets_batch_get_values` - Read from multiple ranges\n- `sheets_get_metadata` - Get spreadsheet info\n- `sheets_check_access` - Check access permissions\n\n### Writing Data\n- `sheets_update_values` - Write to a range\n- `sheets_batch_update_values` - Write to multiple ranges\n- `sheets_append_values` - Append rows to a table (**Note:** Default `insertDataOption` is `OVERWRITE`. To insert new rows, set `insertDataOption: 'INSERT_ROWS'`)\n- `sheets_clear_values` - Clear cell contents\n- `sheets_insert_rows` - Insert new rows at specific position with optional data\n\n### Sheet Management\n- `sheets_insert_sheet` - Add new sheet\n- `sheets_delete_sheet` - Remove sheet\n- `sheets_duplicate_sheet` - Copy sheet\n- `sheets_copy_to` - Copy to another spreadsheet\n- `sheets_update_sheet_properties` - Update sheet settings\n\n### Batch Operations\n- `sheets_batch_delete_sheets` - Delete multiple sheets at once\n- `sheets_batch_format_cells` - Format multiple cell ranges at once\n\n### Cell Formatting\n- `sheets_format_cells` - Format cells (colors, fonts, alignment, number formats)\n- `sheets_update_borders` - Add or modify cell borders\n- `sheets_merge_cells` - Merge cells together\n- `sheets_unmerge_cells` - Unmerge previously merged cells\n- `sheets_add_conditional_formatting` - Add conditional formatting rules\n\n### Charts\n- `sheets_create_chart` - Create various types of charts\n- `sheets_update_chart` - Modify existing charts\n- `sheets_delete_chart` - Remove charts\n\n## 🔧 Code Quality\n\n### Linting\n\n```bash\n# Run ESLint\nnpm run lint\n\n# Fix auto-fixable issues\nnpm run lint:fix\n```\n\n### Formatting\n\n```bash\n# Check formatting with Prettier\nnpm run format:check\n\n# Format code\nnpm run format\n```\n\n### Type Checking\n\n```bash\n# Run TypeScript type checking\nnpm run typecheck\n```\n\n## ❗ Troubleshooting\n\n### Common Issues\n\n**\"Authentication failed\"**\n- If using file-based auth: Verify JSON key path is absolute and correct\n- If using JSON string auth: Ensure JSON is properly escaped and valid\n- Check GOOGLE_PROJECT_ID matches your project (or is included in JSON)\n- Ensure Sheets API is enabled\n\n**\"Permission denied\"**\n- Share spreadsheet with service account email\n- Service account needs \"Editor\" role\n- Check email in JSON file (client_email field)\n\n**\"Spreadsheet not found\"**\n- Verify spreadsheet ID from URL\n- Format: `https://docs.google.com/spreadsheets/d/[SPREADSHEET_ID]/edit`\n\n**MCP Connection Issues**\n- Ensure you're using the built version (`dist/index.js`)\n- Check that Node.js path is correct in Claude Desktop config\n- Look for errors in Claude Desktop logs\n- Use `npm run inspector` to debug\n\n## 🔍 Finding IDs\n\n### Spreadsheet ID\nFrom the URL:\n```\nhttps://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/edit\n                                        ↑ This is the spreadsheet ID\n```\n\n### Sheet ID\nUse `sheets_get_metadata` to list all sheets with their IDs.\n\n## 📝 Tips\n\n1. Always test with a copy of your data\n2. Use batch operations for better performance\n3. Set appropriate permissions (read-only vs edit)\n4. Check rate limits for large operations\n5. Use `sheets_check_access` to verify permissions before operations\n\n## 📘 Tool Details\n\n### sheets_insert_rows\n\nInsert new rows at a specific position in a spreadsheet with optional data.\n\n**Parameters:**\n- `spreadsheetId` (required): The ID of the spreadsheet\n- `range` (required): A1 notation anchor point where rows will be inserted (e.g., \"Sheet1!A5\")\n- `rows` (optional): Number of rows to insert (default: 1)\n- `position` (optional): 'BEFORE' or 'AFTER' the anchor row (default: 'BEFORE')\n- `inheritFromBefore` (optional): Whether to inherit formatting from the row before (default: false)\n- `values` (optional): 2D array of values to fill the newly inserted rows\n- `valueInputOption` (optional): 'RAW' or 'USER_ENTERED' (default: 'USER_ENTERED')\n\n**Examples:**\n\n```javascript\n// Insert 1 empty row before row 5\n{\n  \"spreadsheetId\": \"your-spreadsheet-id\",\n  \"range\": \"Sheet1!A5\"\n}\n\n// Insert 3 rows after row 10 with data\n{\n  \"spreadsheetId\": \"your-spreadsheet-id\",\n  \"range\": \"Sheet1!A10\",\n  \"rows\": 3,\n  \"position\": \"AFTER\",\n  \"values\": [\n    [\"John\", \"Doe\", \"john@example.com\"],\n    [\"Jane\", \"Smith\", \"jane@example.com\"],\n    [\"Bob\", \"Johnson\", \"bob@example.com\"]\n  ]\n}\n```\n\n## 📋 Changelog\n\nSee [CHANGELOG.md](CHANGELOG.md) for a list of changes in each version.\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Run tests and linting (`npm run check`)\n4. Commit your changes (`git commit -m 'Add some amazing feature'`)\n5. Push to the branch (`git push origin feature/amazing-feature`)\n6. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gsheets",
        "databases",
        "database",
        "gsheets mcp",
        "mcp gsheets",
        "secure database"
      ],
      "category": "databases"
    },
    "furey--mongodb-lens": {
      "owner": "furey",
      "name": "mongodb-lens",
      "url": "https://github.com/furey/mongodb-lens",
      "imageUrl": "/freedevtools/mcp/pfp/furey.webp",
      "description": "Provides access to MongoDB databases using natural language, enabling users to perform queries, run aggregations, and optimize database performance.",
      "stars": 187,
      "forks": 24,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-28T08:13:22Z",
      "readme_content": "# MongoDB Lens\n\n[![License](https://img.shields.io/github/license/furey/mongodb-lens)](./LICENSE)\n[![Docker Hub Version](https://img.shields.io/docker/v/furey/mongodb-lens)](https://hub.docker.com/r/furey/mongodb-lens)\n[![NPM Version](https://img.shields.io/npm/v/mongodb-lens)](https://www.npmjs.com/package/mongodb-lens)\n[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-donate-orange.svg)](https://www.buymeacoffee.com/furey)\n\n**MongoDB Lens** is a local Model Context Protocol (MCP) server with full featured access to MongoDB databases using natural language via LLMs to perform queries, run aggregations, optimize performance, and more.\n\n## Contents\n\n- [Quick Start](#quick-start)\n- [Features](#features)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Client Setup](#client-setup)\n- [Data Protection](#data-protection)\n- [Tutorial](#tutorial)\n- [Test Suite](#test-suite)\n- [Disclaimer](#disclaimer)\n- [Support](#support)\n\n## Quick Start\n\n- [Install](#installation) MongoDB Lens\n- [Configure](#configuration) MongoDB Lens\n- [Set up](#client-setup) your MCP Client (e.g. [Claude Desktop](#client-setup-claude-desktop), [Cursor](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers), etc)\n- Explore your MongoDB databases with [natural language queries](#tutorial-4-example-queries)\n\n## Features\n\n- [Tools](#tools)\n- [Resources](#resources)\n- [Prompts](#prompts)\n- [Other](#other-features)\n\n### Tools\n\n- [`add-connection-alias`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27add-connection-alias%27%2C%2F): Add a new MongoDB connection alias\n- [`aggregate-data`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27aggregate-data%27%2C%2F): Execute aggregation pipelines\n- [`analyze-query-patterns`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27analyze-query-patterns%27%2C%2F): Analyze live queries and suggest optimizations\n- [`analyze-schema`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27analyze-schema%27%2C%2F): Automatically infer collection schemas\n- [`bulk-operations`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27bulk-operations%27%2C%2F): Perform multiple operations efficiently ([requires confirmation](#data-protection-confirmation-for-destructive-operations) for destructive operations)\n- [`clear-cache`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27clear-cache%27%2C%2F): Clear memory caches to ensure fresh data\n- [`collation-query`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27collation-query%27%2C%2F): Find documents with language-specific collation rules\n- [`compare-schemas`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27compare-schemas%27%2C%2F): Compare schemas between two collections\n- [`connect-mongodb`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27connect-mongodb%27%2C%2F): Connect to a different MongoDB URI\n- [`connect-original`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27connect-original%27%2C%2F): Connect back to the original MongoDB URI used at startup\n- [`count-documents`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27count-documents%27%2C%2F): Count documents matching specified criteria\n- [`create-collection`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27create-collection%27%2C%2F): Create new collections with custom options\n- [`create-database`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27create-database%27%2C%2F): Create a new database with option to switch to it\n- [`create-index`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27create-index%27%2C%2F): Create new indexes for performance optimization\n- [`create-timeseries`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27create-timeseries%27%2C%2F): Create time series collections for temporal data\n- [`create-user`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27create-user%27%2C%2F): Create new database users with specific roles\n- [`current-database`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27current-database%27%2C%2F): Show the current database context\n- [`delete-document`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27delete-document%27%2C%2F): Delete documents matching specified criteria ([requires confirmation](#data-protection-confirmation-for-destructive-operations))\n- [`distinct-values`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27distinct-values%27%2C%2F): Extract unique values for any field\n- [`drop-collection`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27drop-collection%27%2C%2F): Remove collections from the database ([requires confirmation](#data-protection-confirmation-for-destructive-operations))\n- [`drop-database`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27drop-database%27%2C%2F): Drop a database ([requires confirmation](#data-protection-confirmation-for-destructive-operations))\n- [`drop-index`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27drop-index%27%2C%2F): Remove indexes from collections ([requires confirmation](#data-protection-confirmation-for-destructive-operations))\n- [`drop-user`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27drop-user%27%2C%2F): Remove database users ([requires confirmation](#data-protection-confirmation-for-destructive-operations))\n- [`explain-query`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27explain-query%27%2C%2F): Analyze query execution plans\n- [`export-data`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27export-data%27%2C%2F): Export query results in JSON or CSV format\n- [`find-documents`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27find-documents%27%2C%2F): Run queries with filters, projections, and sorting\n- [`generate-schema-validator`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27generate-schema-validator%27%2C%2F): Generate JSON Schema validators\n- [`geo-query`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27geo-query%27%2C%2F): Perform geospatial queries with various operators\n- [`get-stats`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27get-stats%27%2C%2F): Retrieve database or collection statistics\n- [`gridfs-operation`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27gridfs-operation%27%2C%2F): Manage large files with GridFS buckets\n- [`insert-document`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27insert-document%27%2C%2F): Insert one or more documents into collections\n- [`list-collections`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27list-collections%27%2C%2F): Explore collections in the current database\n- [`list-connections`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27list-connections%27%2C%2F): View all available MongoDB connection aliases\n- [`list-databases`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27list-databases%27%2C%2F): View all accessible databases\n- [`rename-collection`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27rename-collection%27%2C%2F): Rename existing collections ([requires confirmation](#data-protection-confirmation-for-destructive-operations) when dropping targets)\n- [`shard-status`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27shard-status%27%2C%2F): View sharding configuration for databases and collections\n- [`text-search`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27text-search%27%2C%2F): Perform full-text search across text-indexed fields\n- [`transaction`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27transaction%27%2C%2F): Execute multiple operations in a single ACID transaction\n- [`update-document`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27update-document%27%2C%2F): Update documents matching specified criteria\n- [`use-database`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27use-database%27%2C%2F): Switch to a specific database context\n- [`validate-collection`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27validate-collection%27%2C%2F): Check for data inconsistencies\n- [`watch-changes`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.tool%5C%28%5Cs*%27watch-changes%27%2C%2F): Monitor real-time changes to collections\n\n### Resources\n\n- [`collection-indexes`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27collection-indexes%27%2C%2F): Index information for a collection\n- [`collection-schema`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27collection-schema%27%2C%2F): Schema information for a collection\n- [`collection-stats`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27collection-stats%27%2C%2F): Performance statistics for a collection\n- [`collection-validation`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27collection-validation%27%2C%2F): Validation rules for a collection\n- [`collections`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27collections%27%2C%2F): List of collections in the current database\n- [`database-triggers`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27database-triggers%27%2C%2F): Database change streams and event triggers configuration\n- [`database-users`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27database-users%27%2C%2F): Database users and roles in the current database\n- [`databases`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27databases%27%2C%2F): List of all accessible databases\n- [`performance-metrics`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27performance-metrics%27%2C%2F): Real-time performance metrics and profiling data\n- [`replica-status`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27replica-status%27%2C%2F): Replica set status and configuration\n- [`server-status`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27server-status%27%2C%2F): Server status information\n- [`stored-functions`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.resource%5C%28%5Cs*%27stored-functions%27%2C%2F): Stored JavaScript functions in the current database\n\n### Prompts\n\n- [`aggregation-builder`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27aggregation-builder%27%2C%2F): Step-by-step creation of aggregation pipelines\n- [`backup-strategy`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27backup-strategy%27%2C%2F): Customized backup and recovery recommendations\n- [`data-modeling`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27data-modeling%27%2C%2F): Expert advice on MongoDB schema design for specific use cases\n- [`database-health-check`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27database-health-check%27%2C%2F): Comprehensive database health assessment and recommendations\n- [`index-recommendation`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27index-recommendation%27%2C%2F): Get personalized index suggestions based on query patterns\n- [`migration-guide`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27migration-guide%27%2C%2F): Step-by-step MongoDB version migration plans\n- [`mongo-shell`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27mongo-shell%27%2C%2F): Generate MongoDB shell commands with explanations\n- [`multi-tenant-design`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27multi-tenant-design%27%2C%2F): Design MongoDB multi-tenant database architecture\n- [`query-builder`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27query-builder%27%2C%2F): Interactive guidance for constructing MongoDB queries\n- [`query-optimizer`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27query-optimizer%27%2C%2F): Optimization recommendations for slow queries\n- [`schema-analysis`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27schema-analysis%27%2C%2F): Detailed collection schema analysis with recommendations\n- [`schema-versioning`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27schema-versioning%27%2C%2F): Manage schema evolution in MongoDB applications\n- [`security-audit`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27security-audit%27%2C%2F): Database security analysis and improvement recommendations\n- [`sql-to-mongodb`](https://github.com/search?type=code&q=repo%3Afurey%2Fmongodb-lens+%2Fserver%5C.prompt%5C%28%5Cs*%27sql-to-mongodb%27%2C%2F): Convert SQL queries to MongoDB aggregation pipelines\n\n### Other Features\n\n- [Overview](#other-features-overview)\n- [New Database Metadata](#other-features-new-database-metadata)\n\n#### Other Features: Overview\n\nMongoDB Lens includes numerous other features:\n\n- **[Config File](#configuration-config-file)**: Custom configuration via `~/.mongodb-lens.[jsonc|json]`\n- **[Env Var Overrides](#configuration-environment-variable-overrides)**: Override config settings via `process.env.CONFIG_*`\n- **[Confirmation System](#data-protection-confirmation-for-destructive-operations)**: Two-step verification for destructive operations\n- **[Multiple Connections](#configuration-multiple-mongodb-connections)**: Define and switch between named URI aliases\n- **[Component Disabling](#disabling-tools)**: Selectively disable tools, prompts or resources\n- **Connection Resilience**: Auto-reconnection with exponential backoff\n- **Query Safeguards**: Configurable limits and performance protections\n- **Error Handling**: Comprehensive JSONRPC error codes and messages\n- **Schema Inference**: Efficient schema analysis with intelligent sampling\n- **Credential Protection**: Connection string password obfuscation in logs\n- **Memory Management**: Auto-monitoring and cleanup for large operations\n- **Smart Caching**: Optimized caching for schema, indexes, fields and collections\n- **Backwards Compatible**: Support both modern and legacy MongoDB versions\n\n#### Other Features: New Database Metadata\n\nMongoDB Lens inserts a `metadata` collection into each database it creates.\n\nThis `metadata` collection stores a single document containing contextual information serving as a permanent record of the database's origin while ensuring the new and otherwise empty database persists in MongoDB's storage system.\n\n<details>\n  <summary><strong>Example metadata document</strong></summary>\n\n```js\n{\n    \"_id\" : ObjectId(\"67d5284463788ec38aecee14\"),\n    \"created\" : {\n        \"timestamp\" : ISODate(\"2025-03-15T07:12:04.705Z\"),\n        \"tool\" : \"MongoDB Lens v5.0.7\",\n        \"user\" : \"anonymous\"\n    },\n    \"mongodb\" : {\n        \"version\" : \"3.6.23\",\n        \"connectionInfo\" : {\n            \"host\" : \"unknown\",\n            \"readPreference\" : \"primary\"\n        }\n    },\n    \"database\" : {\n        \"name\" : \"example_database\",\n        \"description\" : \"Created via MongoDB Lens\"\n    },\n    \"system\" : {\n        \"hostname\" : \"unknown\",\n        \"platform\" : \"darwin\",\n        \"nodeVersion\" : \"v22.14.0\"\n    },\n    \"lens\" : {\n        \"version\" : \"5.0.7\",\n        \"startTimestamp\" : ISODate(\"2025-03-15T07:10:06.084Z\")\n    }\n}\n```\n\n</details>\n\nOnce you've added your own collections to your new database, you can safely remove the `metadata` collection via the `drop-collection` tool:\n\n- _\"Drop the new database's metadata collection\"_<br>\n  <sup>➥ Uses `drop-collection` tool (with confirmation)</sup>\n\n## Installation\n\nMongoDB Lens can be installed and run in several ways:\n\n- [NPX](#installation-npx) (Easiest)\n- [Docker Hub](#installation-docker-hub)\n- [Node.js from Source](#installation-nodejs-from-source)\n- [Docker from Source](#installation-docker-from-source)\n- [Installation Verification](#installation-verification)\n- [Older MongoDB Versions](#installation-older-mongodb-versions)\n\n### Installation: NPX\n\n> [!NOTE]<br>\n> NPX requires [Node.js](https://nodejs.org/en/download) installed and running on your system (suggestion: use [Volta](https://volta.sh)).\n\nThe easiest way to run MongoDB Lens is using NPX.\n\nFirst, ensure Node.js is installed:\n\n```console\nnode --version # Ideally >= v22.x but MongoDB Lens is >= v18.x compatible\n```\n\nThen, run MongoDB Lens via NPX:\n\n```console\n# Using default connection string mongodb://localhost:27017\nnpx -y mongodb-lens\n\n# Using custom connection string\nnpx -y mongodb-lens mongodb://your-connection-string\n\n# Using \"@latest\" to keep the package up-to-date\nnpx -y mongodb-lens@latest\n```\n\n> [!TIP]<br>\n> If you encounter permissions errors with `npx` try running `npx clear-npx-cache` prior to running `npx -y mongodb-lens` (this clears the cache and re-downloads the package).\n\n### Installation: Docker Hub\n\n> [!NOTE]<br>\n> Docker Hub requires [Docker](https://docs.docker.com/get-started/get-docker) installed and running on your system.\n\nFirst, ensure Docker is installed:\n\n```console\ndocker --version # Ideally >= v27.x\n```\n\nThen, run MongoDB Lens via Docker Hub:\n\n```console\n# Using default connection string mongodb://localhost:27017\ndocker run --rm -i --network=host furey/mongodb-lens\n\n# Using custom connection string\ndocker run --rm -i --network=host furey/mongodb-lens mongodb://your-connection-string\n\n# Using \"--pull\" to keep the Docker image up-to-date\ndocker run --rm -i --network=host --pull=always furey/mongodb-lens\n```\n\n### Installation: Node.js from Source\n\n> [!NOTE]<br>\n> Node.js from source requires [Node.js](https://nodejs.org/en/download) installed and running on your system (suggestion: use [Volta](https://volta.sh)).\n\n1. Clone the MongoDB Lens repository:<br>\n    ```console\n    git clone https://github.com/furey/mongodb-lens.git\n    ```\n1. Navigate to the cloned repository directory:<br>\n    ```console\n    cd /path/to/mongodb-lens\n    ```\n1. Ensure Node.js is installed:<br>\n    ```console\n    node --version # Ideally >= v22.x but MongoDB Lens is >= v18.x compatible\n    ```\n1. Install Node.js dependencies:<br>\n    ```console\n    npm ci\n    ```\n1. Start the server:<br>\n    ```console\n    # Using default connection string mongodb://localhost:27017\n    node mongodb-lens.js\n\n    # Using custom connection string\n    node mongodb-lens.js mongodb://your-connection-string\n    ```\n\n### Installation: Docker from Source\n\n> [!NOTE]<br>\n> Docker from source requires [Docker](https://docs.docker.com/get-started/get-docker) installed and running on your system.\n\n1. Clone the MongoDB Lens repository:<br>\n    ```console\n    git clone https://github.com/furey/mongodb-lens.git\n    ```\n1. Navigate to the cloned repository directory:<br>\n    ```console\n    cd /path/to/mongodb-lens\n    ```\n1. Ensure Docker is installed:<br>\n    ```console\n    docker --version # Ideally >= v27.x\n    ```\n1. Build the Docker image:<br>\n    ```console\n    docker build -t mongodb-lens .\n    ```\n1. Run the container:<br>\n    ```console\n    # Using default connection string mongodb://localhost:27017\n    docker run --rm -i --network=host mongodb-lens\n\n    # Using custom connection string\n    docker run --rm -i --network=host mongodb-lens mongodb://your-connection-string\n    ```\n\n### Installation Verification\n\nTo verify the installation, paste and run the following JSONRPC message into the server's stdio:\n\n```json\n{\"method\":\"resources/read\",\"params\":{\"uri\":\"mongodb://databases\"},\"jsonrpc\":\"2.0\",\"id\":1}\n```\n\nThe server should respond with a list of databases in your MongoDB instance, for example:\n\n```json\n{\"result\":{\"contents\":[{\"uri\":\"mongodb://databases\",\"text\":\"Databases (12):\\n- admin (180.00 KB)\\n- config (108.00 KB)\\n- local (40.00 KB)\\n- sample_airbnb (51.88 MB)\\n- sample_analytics (9.46 MB)\\n- sample_geospatial (980.00 KB)\\n- sample_guides (40.00 KB)\\n- sample_mflix (108.90 MB)\\n- sample_restaurants (7.73 MB)\\n- sample_supplies (968.00 KB)\\n- sample_training (40.85 MB)\\n- sample_weatherdata (2.69 MB)\"}]},\"jsonrpc\":\"2.0\",\"id\":1}\n```\n\nMongoDB Lens is now installed and ready to accept MCP requests.\n\n### Installation: Older MongoDB Versions\n\nIf connecting to a MongoDB instance with a version `< 4.0`, the MongoDB Node.js driver used by the latest version of MongoDB Lens will not be compatible. Specifically, MongoDB Node.js driver versions `4.0.0` and above require MongoDB version `4.0` or higher.\n\nTo use MongoDB Lens with older MongoDB instances, you need to use a MongoDB Node.js driver version from the `3.x` series (e.g. `3.7.4` which is compatible with MongoDB `3.6`).\n\n#### Older MongoDB Versions: Running from Source\n\n1. Clone the MongoDB Lens repository:<br>\n    ```console\n    git clone https://github.com/furey/mongodb-lens.git\n    ```\n1. Navigate to the cloned repository directory:<br>\n    ```console\n    cd /path/to/mongodb-lens\n    ```\n1. Modify `package.json`:<br>\n    ```diff\n    \"dependencies\": {\n      ...\n    -  \"mongodb\": \"^6.15.0\",  // Or whatever newer version is listed\n    +  \"mongodb\": \"^3.7.4\",   // Or whatever 3.x version is compatible with your older MongoDB instance\n      ...\n    }\n    ```\n1. Install Node.js dependencies:<br>\n    ```console\n    npm install\n    ```\n1. Start MongoDB Lens:<br>\n    ```console\n    node mongodb-lens.js mongodb://older-mongodb-instance\n    ```\n\nThis will use the older driver version compatible with your MongoDB instance.\n\n> [!NOTE]<br>\n> You may also need to revert [this commit](https://github.com/furey/mongodb-lens/commit/603b28cbde72fcd62a15cd324afc93028380a054) to add back `useNewUrlParser` and `useUnifiedTopology` MongoDB configuration options.\n\n#### Older MongoDB Versions: Using NPX or Docker\n\nIf you prefer to use NPX or Docker, you'll need to use an older version of MongoDB Lens that was published with a compatible driver.\n\nFor example, MongoDB Lens `8.3.0` uses MongoDB Node.js driver `3.7.4` (see: [`package-lock.json`](https://github.com/furey/mongodb-lens/blob/8.3.0/package-lock.json#L944-L945)).\n\nTo run an older version of MongoDB Lens using NPX, specify the version tag:\n\n```console\nnpx -y mongodb-lens@8.3.0\n```\n\nSimilarly for Docker:\n\n```console\ndocker run --rm -i --network=host furey/mongodb-lens:8.3.0\n```\n\n## Configuration\n\n- [MongoDB Connection String](#configuration-mongodb-connection-string)\n- [Config File](#configuration-config-file)\n- [Config File Generation](#configuration-config-file-generation)\n- [Multiple MongoDB Connections](#configuration-multiple-mongodb-connections)\n- [Environment Variable Overrides](#configuration-environment-variable-overrides)\n- [Cross-Platform Environment Variables](#configuration-cross-platform-environment-variables)\n\n### Configuration: MongoDB Connection String\n\nThe server accepts a MongoDB connection string as its only argument.\n\nExample NPX usage:\n\n```console\nnpx -y mongodb-lens@latest mongodb://your-connection-string\n```\n\nMongoDB connection strings have the following format:\n\n```txt\nmongodb://[username:password@]host[:port][/database][?options]\n```\n\nExample connection strings:\n\n- Local connection:<br>\n  `mongodb://localhost:27017`\n- Connection to `mydatabase` with credentials from `admin` database:<br>\n  `mongodb://username:password@hostname:27017/mydatabase?authSource=admin`\n- Connection to `mydatabase` with various other options:<br>\n  `mongodb://hostname:27017/mydatabase?retryWrites=true&w=majority`\n\nIf no connection string is provided, the server will attempt to connect via local connection.\n\n### Configuration: Config File\n\nMongoDB Lens supports extensive customization via JSON configuration file.\n\n> [!NOTE]<br>\n> The config file is optional. MongoDB Lens will run with default settings if no config file is provided.\n\n> [!TIP]<br>\n> You only need to include the settings you want to customize in the config file. MongoDB Lens will use default settings for any omitted values.\n\n> [!TIP]<br>\n> MongoDB Lens supports both `.json` and `.jsonc` (JSON with comments) config file formats.\n\n<details>\n  <summary><strong>Example configuration file</strong></summary>\n\n```jsonc\n{\n  \"mongoUri\": \"mongodb://localhost:27017\",         // Default MongoDB connection string or object of alias-URI pairs\n  \"connectionOptions\": {\n    \"maxPoolSize\": 20,                             // Maximum number of connections in the pool\n    \"retryWrites\": false,                          // Whether to retry write operations\n    \"connectTimeoutMS\": 30000,                     // Connection timeout in milliseconds\n    \"socketTimeoutMS\": 360000,                     // Socket timeout in milliseconds\n    \"heartbeatFrequencyMS\": 10000,                 // How often to ping servers for status\n    \"serverSelectionTimeoutMS\": 30000              // Timeout for server selection\n  },\n  \"defaultDbName\": \"admin\",                        // Default database if not specified in URI\n  \"connection\": {\n    \"maxRetries\": 5,                               // Maximum number of initial connection attempts\n    \"maxRetryDelayMs\": 30000,                      // Maximum delay between retries\n    \"reconnectionRetries\": 10,                     // Maximum reconnection attempts if connection lost\n    \"initialRetryDelayMs\": 1000                    // Initial delay between retries\n  },\n  \"disabled\": {\n    \"tools\": [],                                   // Array of tools to disable or true to disable all\n    \"prompts\": [],                                 // Array of prompts to disable or true to disable all\n    \"resources\": []                                // Array of resources to disable or true to disable all\n  },\n  \"enabled\": {\n    \"tools\": true,                                 // Array of tools to enable or true to enable all\n    \"prompts\": true,                               // Array of prompts to enable or true to enable all\n    \"resources\": true                              // Array of resources to enable or true to enable all\n  },\n  \"cacheTTL\": {\n    \"stats\": 15000,                                // Stats cache lifetime in milliseconds\n    \"fields\": 30000,                               // Fields cache lifetime in milliseconds\n    \"schemas\": 60000,                              // Schema cache lifetime in milliseconds\n    \"indexes\": 120000,                             // Index cache lifetime in milliseconds\n    \"collections\": 30000,                          // Collections list cache lifetime in milliseconds\n    \"serverStatus\": 20000                          // Server status cache lifetime in milliseconds\n  },\n  \"enabledCaches\": [                               // List of caches to enable\n    \"stats\",                                       // Statistics cache\n    \"fields\",                                      // Collection fields cache\n    \"schemas\",                                     // Collection schemas cache\n    \"indexes\",                                     // Collection indexes cache\n    \"collections\",                                 // Database collections cache\n    \"serverStatus\"                                 // MongoDB server status cache\n  ],\n  \"memory\": {\n    \"enableGC\": true,                              // Whether to enable garbage collection\n    \"warningThresholdMB\": 1500,                    // Memory threshold for warnings\n    \"criticalThresholdMB\": 2000                    // Memory threshold for cache clearing\n  },\n  \"logLevel\": \"info\",                              // Log level (info or verbose)\n  \"disableDestructiveOperationTokens\": false,      // Whether to skip confirmation for destructive ops\n  \"watchdogIntervalMs\": 30000,                     // Interval for connection monitoring\n  \"defaults\": {\n    \"slowMs\": 100,                                 // Threshold for slow query detection\n    \"queryLimit\": 10,                              // Default limit for query results\n    \"allowDiskUse\": true,                          // Allow operations to use disk for large datasets\n    \"schemaSampleSize\": 100,                       // Sample size for schema inference\n    \"aggregationBatchSize\": 50                     // Batch size for aggregation operations\n  },\n  \"security\": {\n    \"tokenLength\": 4,                              // Length of confirmation tokens\n    \"tokenExpirationMinutes\": 5,                   // Expiration time for tokens\n    \"strictDatabaseNameValidation\": true           // Enforce strict database name validation\n  },\n  \"tools\": {\n    \"transaction\": {\n      \"readConcern\": \"snapshot\",                   // Read concern level for transactions\n      \"writeConcern\": {\n        \"w\": \"majority\"                            // Write concern for transactions\n      }\n    },\n    \"bulkOperations\": {\n      \"ordered\": true                              // Whether bulk operations execute in order\n    },\n    \"export\": {\n      \"defaultLimit\": -1,                          // Default limit for exports (-1 = no limit)\n      \"defaultFormat\": \"json\"                      // Default export format (json or csv)\n    },\n    \"watchChanges\": {\n      \"maxDurationSeconds\": 60,                    // Maximum duration for change streams\n      \"defaultDurationSeconds\": 10                 // Default duration for change streams\n    },\n    \"queryAnalysis\": {\n      \"defaultDurationSeconds\": 10                 // Default duration for query analysis\n    }\n  }\n}\n```\n\n</details>\n\nBy default, MongoDB Lens looks for the config file at:\n\n- `~/.mongodb-lens.jsonc` first, then falls back to\n- `~/.mongodb-lens.json` if the former doesn't exist\n\nTo customize the config file path, set the environment variable `CONFIG_PATH` to the desired file path.\n\nExample NPX usage:\n\n```console\nCONFIG_PATH='/path/to/config.json' npx -y mongodb-lens@latest\n```\n\nExample Docker Hub usage:\n\n```console\ndocker run --rm -i --network=host --pull=always -v /path/to/config.json:/root/.mongodb-lens.json furey/mongodb-lens\n```\n\n### Configuration: Config File Generation\n\nYou can generate a configuration file automatically using the `config:create` script:\n\n```console\n# NPX Usage (recommended)\nnpx -y mongodb-lens@latest config:create\n\n# Node.js Usage\nnpm run config:create\n\n# Force overwrite existing files\nnpx -y mongodb-lens@latest config:create -- --force\nnpm run config:create -- --force\n```\n\nThis script extracts the [example configuration file](#configuration-config-file) above and saves it to: `~/.mongodb-lens.jsonc`\n\n#### Config File Generation: Custom Path\n\nYou can specify a custom output location using the `CONFIG_PATH` environment variable.\n\n- If `CONFIG_PATH` has no file extension, it's treated as a directory and `.mongodb-lens.jsonc` is appended\n- If `CONFIG_PATH` ends with `.json` (not `.jsonc`) comments are removed from the generated file\n\nExample NPX usage:\n\n```console\n# With custom path\nCONFIG_PATH=/path/to/config.jsonc npx -y mongodb-lens@latest config:create\n\n# Save to directory (will append .mongodb-lens.jsonc to the path)\nCONFIG_PATH=/path/to/directory npx -y mongodb-lens@latest config:create\n\n# Save as JSON instead of JSONC\nCONFIG_PATH=/path/to/config.json npx -y mongodb-lens@latest config:create\n```\n\nExample Node.js usage:\n\n```console\n# With custom path\nCONFIG_PATH=/path/to/config.jsonc node mongodb-lens.js config:create\n\n# Save to directory (will append .mongodb-lens.jsonc to the path)\nCONFIG_PATH=/path/to/directory node mongodb-lens.js config:create\n\n# Save as JSON instead of JSONC\nCONFIG_PATH=/path/to/config.json node mongodb-lens.js config:create\n```\n\n### Configuration: Multiple MongoDB Connections\n\nMongoDB Lens supports multiple MongoDB URIs with aliases in your [config file](#configuration-config-file), allowing you to easily switch between different MongoDB instances using simple names.\n\nTo configure multiple connections, set the `mongoUri` config setting to an object with alias-URI pairs:\n\n```json\n{\n  \"mongoUri\": {\n    \"main\": \"mongodb://localhost:27017\",\n    \"backup\": \"mongodb://localhost:27018\",\n    \"atlas\": \"mongodb+srv://username:password@cluster.mongodb.net/mydb\"\n  }\n}\n```\n\nWith this configuration:\n\n- The first URI in the list (e.g. `main`) becomes the default connection at startup\n- You can switch connections using natural language: `\"Connect to backup\"` or `\"Connect to atlas\"`\n- The original syntax still works: `\"Connect to mongodb://localhost:27018\"`\n- The `list-connections` tool shows all available connection aliases\n\n> [!NOTE]<br>\n> When using the command-line argument to specify a connection, you can use either a full MongoDB URI or an alias defined in your configuration file.\n\n> [!TIP]<br>\n> To add connection aliases at runtime, use the `add-connection-alias` tool.\n\n### Configuration: Environment Variable Overrides\n\nMongoDB Lens supports environment variable overrides for configuration settings.\n\nEnvironment variables take precedence over [config file](#configuration-config-file) settings.\n\nConfig environment variables follow the naming pattern:\n\n```txt\nCONFIG_[SETTING PATH, SNAKE CASED, UPPERCASED]\n```\n\nExample overrides:\n\n| Config Setting                   | Environment Variable Override             |\n| -------------------------------- | ----------------------------------------- |\n| `mongoUri`                       | `CONFIG_MONGO_URI`                        |\n| `logLevel`                       | `CONFIG_LOG_LEVEL`                        |\n| `defaultDbName`                  | `CONFIG_DEFAULT_DB_NAME`                  |\n| `defaults.queryLimit`            | `CONFIG_DEFAULTS_QUERY_LIMIT`             |\n| `tools.export.defaultFormat`     | `CONFIG_TOOLS_EXPORT_DEFAULT_FORMAT`      |\n| `connectionOptions.maxPoolSize`  | `CONFIG_CONNECTION_OPTIONS_MAX_POOL_SIZE` |\n| `connection.reconnectionRetries` | `CONFIG_CONNECTION_RECONNECTION_RETRIES`  |\n\nFor environment variable values:\n\n- For boolean settings, use string values `'true'` or `'false'`.\n- For numeric settings, use string representations.\n- For nested objects or arrays, use JSON strings.\n\nExample NPX usage:\n\n```console\nCONFIG_DEFAULTS_QUERY_LIMIT='25' npx -y mongodb-lens@latest\n```\n\nExample Docker Hub usage:\n\n```console\ndocker run --rm -i --network=host --pull=always -e CONFIG_DEFAULTS_QUERY_LIMIT='25' furey/mongodb-lens\n```\n\n### Configuration: Cross-Platform Environment Variables\n\nFor consistent environment variable usage across Windows, macOS, and Linux, consider using `cross-env`:\n\n1. Install cross-env globally:<br>\n   ```console\n   # Using NPM\n   npm install -g cross-env\n\n   # Using Volta (see: https://volta.sh)\n   volta install cross-env\n   ```\n1. Prefix any NPX or Node.js environment variables in this document's examples:<br>\n   ```console\n   # Example NPX usage with cross-env\n   cross-env CONFIG_DEFAULTS_QUERY_LIMIT='25' npx -y mongodb-lens@latest\n\n   # Example Node.js usage with cross-env\n   cross-env CONFIG_DEFAULTS_QUERY_LIMIT='25' node mongodb-lens.js\n   ```\n\n## Client Setup\n\n- [Claude Desktop](#client-setup-claude-desktop)\n- [MCP Inspector](#client-setup-mcp-inspector)\n- [Other MCP Clients](#client-setup-other-mcp-clients)\n\n### Client Setup: Claude Desktop\n\nTo use MongoDB Lens with Claude Desktop:\n\n1. Install [Claude Desktop](https://claude.ai/download)\n1. Open `claude_desktop_config.json` (create if it doesn't exist):\n    - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n    - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n1. Add the MongoDB Lens server configuration as per [configuration options](#claude-desktop-configuration-options)\n1. Restart Claude Desktop\n1. Start a conversation with Claude about your MongoDB data\n\n#### Claude Desktop Configuration Options\n\n- [Option 1: NPX (Recommended)](#option-1-npx-recommended)\n- [Option 2: Docker Hub Image](#option-2-docker-hub-image)\n- [Option 3: Local Node.js Installation](#option-3-local-nodejs-installation)\n- [Option 4: Local Docker Image](#option-4-local-docker-image)\n\nFor each option:\n\n- Replace `mongodb://your-connection-string` with your MongoDB connection string or omit it to use the default `mongodb://localhost:27017`.\n- To use a custom config file, set [`CONFIG_PATH`](#configuration-config-file) environment variable.\n- To include environment variables:\n  - For NPX or Node.js add `\"env\": {}` with key-value pairs, for example:<br>\n    ```json\n    \"command\": \"/path/to/npx\",\n    \"args\": [\n      \"-y\",\n      \"mongodb-lens@latest\",\n      \"mongodb://your-connection-string\"\n    ],\n    \"env\": {\n      \"CONFIG_LOG_LEVEL\": \"verbose\"\n    }\n    ```\n  - For Docker add `-e` flags, for example:<br>\n    ```json\n    \"command\": \"docker\",\n    \"args\": [\n      \"run\", \"--rm\", \"-i\",\n      \"--network=host\",\n      \"--pull=always\",\n      \"-e\", \"CONFIG_LOG_LEVEL=verbose\",\n      \"furey/mongodb-lens\",\n      \"mongodb://your-connection-string\"\n    ]\n    ```\n\n##### Option 1: NPX (Recommended)\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-lens\": {\n      \"command\": \"/path/to/npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-lens@latest\",\n        \"mongodb://your-connection-string\"\n      ]\n    }\n  }\n}\n```\n\n##### Option 2: Docker Hub Image\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-lens\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"--network=host\",\n        \"--pull=always\",\n        \"furey/mongodb-lens\",\n        \"mongodb://your-connection-string\"\n      ]\n    }\n  }\n}\n```\n\n##### Option 3: Local Node.js Installation\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-lens\": {\n      \"command\": \"/path/to/node\",\n      \"args\": [\n        \"/path/to/mongodb-lens.js\",\n        \"mongodb://your-connection-string\"\n      ]\n    }\n  }\n}\n```\n\n##### Option 4: Local Docker Image\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-lens\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"--network=host\",\n        \"mongodb-lens\",\n        \"mongodb://your-connection-string\"\n      ]\n    }\n  }\n}\n```\n\n### Client Setup: MCP Inspector\n\n[MCP Inspector](https://github.com/modelcontextprotocol/inspector) is a tool designed for testing and debugging MCP servers.\n\n> [!NOTE]<br>\n> MCP Inspector starts a proxy server on port 3000 and web client on port 5173.\n\nExample NPX usage:\n\n1. Run MCP Inspector:<br>\n    ```console\n    # Using default connection string mongodb://localhost:27017\n    npx -y @modelcontextprotocol/inspector npx -y mongodb-lens@latest\n\n    # Using custom connection string\n    npx -y @modelcontextprotocol/inspector npx -y mongodb-lens@latest mongodb://your-connection-string\n\n    # Using custom ports\n    SERVER_PORT=1234 CLIENT_PORT=5678 npx -y @modelcontextprotocol/inspector npx -y mongodb-lens@latest\n    ```\n1. Open MCP Inspector: http://localhost:5173\n\nMCP Inspector should support the full range of MongoDB Lens capabilities, including autocompletion for collection names and query fields.\n\nFor more, see: [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector)\n\n### Client Setup: Other MCP Clients\n\nMongoDB Lens should be usable with any MCP-compatible client.\n\nFor more, see: [MCP Documentation: Example Clients](https://modelcontextprotocol.io/clients)\n\n## Data Protection\n\nTo protect your data while using MongoDB Lens, consider the following:\n\n- [Read-Only User Accounts](#data-protection-read-only-user-accounts)\n- [Working with Database Backups](#data-protection-working-with-database-backups)\n- [Data Flow Considerations](#data-protection-data-flow-considerations)\n- [Confirmation for Destructive Operations](#data-protection-confirmation-for-destructive-operations)\n- [Disabling Destructive Operations](#data-protection-disabling-destructive-operations)\n\n### Data Protection: Read-Only User Accounts\n\nWhen connecting MongoDB Lens to your database, the permissions granted to the user in the MongoDB connection string dictate what actions can be performed. When the use case fits, a read-only user can prevent unintended writes or deletes, ensuring MongoDB Lens can query data but not alter it.\n\nTo set this up, create a user with the `read` role scoped to the database(s) you're targeting. In MongoDB shell, you'd run something like:\n\n```js\nuse admin\n\ndb.createUser({\n  user: 'readonly',\n  pwd: 'eXaMpLePaSsWoRd',\n  roles: [{ role: 'read', db: 'mydatabase' }]\n})\n```\n\nThen, apply those credentials to your MongoDB connection string:\n\n```txt\nmongodb://readonly:eXaMpLePaSsWoRd@localhost:27017/mydatabase\n```\n\nUsing read-only credentials is a simple yet effective way to enforce security boundaries, especially when you're poking around schemas or running ad-hoc queries.\n\n### Data Protection: Working with Database Backups\n\nWhen working with MongoDB Lens, consider connecting to a backup copy of your data hosted on a separate MongoDB instance.\n\nStart by generating the backup with `mongodump`. Next, spin up a fresh MongoDB instance (e.g. on a different port like `27018`) and restore the backup there using `mongorestore`. Once it's running, point MongoDB Lens to the backup instance's connection string (e.g. `mongodb://localhost:27018/mydatabase`).\n\nThis approach gives you a sandbox to test complex or destructive operations against without risking accidental corruption of your live data.\n\n### Data Protection: Data Flow Considerations\n\n- [How Your Data Flows Through the System](#data-flow-considerations-how-your-data-flows-through-the-system)\n- [Protecting Sensitive Data with Projection](#data-flow-considerations-protecting-sensitive-data-with-projection)\n- [Connection Aliases and Passwords](#data-flow-considerations-connection-aliases-and-passwords)\n- [Local Setup for Maximum Safety](#data-flow-considerations-local-setup-for-maximum-safety)\n\n#### Data Flow Considerations: How Your Data Flows Through the System\n\nWhen using an MCP Server with a remote LLM provider (such as Anthropic via Claude Desktop) understanding how your data flows through the system is key to protecting sensitive information from unintended exposure.\n\nWhen you send a MongoDB related query through your MCP client, here’s what happens:\n\n> [!NOTE]<br>\n> While this example uses a local MongoDB instance, the same principles apply to remote MongoDB instances.\n\n```mermaid\nsequenceDiagram\n    actor User\n    box Local Machine #d4f1f9\n        participant Client as MCP Client\n        participant Lens as MongoDB Lens\n        participant MongoDB as MongoDB Instance\n    end\n    box Remote Server #ffe6cc\n        participant LLM as Remote LLM Provider\n    end\n\n    User->>Client: 1. Submit request<br>\"Show me all users older than 30\"\n    Client->>LLM: 2. User request + available tools\n    Note over LLM: Interprets request<br>Chooses appropriate tool\n    LLM->>Client: 3. Tool selection (find-documents)\n    Client->>Lens: 4. Tool run with parameters\n    Lens->>MongoDB: 5. Database query\n    MongoDB-->>Lens: 6. Database results\n    Lens-->>Client: 7. Tool results (formatted data)\n    Client->>LLM: 8. Tool results\n    Note over LLM: Processes results<br>Formats response\n    LLM-->>Client: 9. Processed response\n    Client-->>User: 10. Final answer\n```\n\n1. You submit a request<br><sup>➥ e.g. \"Show me all users older than 30\"</sup>\n1. Your client sends the request to the remote LLM<br><sup>➥ The LLM provider receives your exact words along with a list of available MCP tools and their parameters.</sup>\n1. The remote LLM interprets your request<br><sup>➥ It determines your intent and instructs the client to use a specific MCP tool with appropriate parameters.</sup>\n1. The client asks MongoDB Lens to run the tool<br><sup>➥ This occurs locally on your machine via stdio.</sup>\n1. MongoDB Lens queries your MongoDB database\n1. MongoDB Lens retrieves your MongoDB query results\n1. MongoDB Lens sends the data back to the client<br><sup>➥ The client receives results formatted by MongoDB Lens.</sup>\n1. The client forwards the data to the remote LLM<br><sup>➥ The LLM provider sees the exact data returned by MongoDB Lens.</sup>\n1. The remote LLM processes the data<br><sup>➥ It may summarize or format the results further.</sup>\n1. The remote LLM sends the final response to the client<br><sup>➥ The client displays the answer to you.</sup>\n\nThe remote LLM provider sees both your original request and the full response from MongoDB Lens. If your database includes sensitive fields (e.g. passwords, personal details, etc) this data could be unintentionally transmitted to the remote provider unless you take precautions.\n\n#### Data Flow Considerations: Protecting Sensitive Data with Projection\n\nTo prevent sensitive data from being sent to the remote LLM provider, use the concept of projection when using tools like `find-documents`, `aggregate-data`, or `export-data`. Projection allows you to specify which fields to include or exclude in query results, ensuring sensitive information stays local.\n\nExample projection usage:\n\n- _\"Show me all users older than 30, but use projection to hide their passwords.\"_<br>\n  <sup>➥ Uses `find-documents` tool with projection</sup>\n\n#### Data Flow Considerations: Connection Aliases and Passwords\n\nWhen adding new connection aliases using the `add-connection-alias` tool, avoid added aliases to URIs that contain passwords if you're using a remote LLM provider. Since your request is sent to the LLM, any passwords in the URI could be exposed. Instead, define aliases with passwords in the MongoDB Lens [config file](#configuration-multiple-mongodb-connections), where they remain local and are not transmitted to the LLM.\n\n#### Data Flow Considerations: Local Setup for Maximum Safety\n\nWhile outside the scope of this document, for the highest level of data privacy, consider using a local MCP client paired with a locally hosted LLM model. This approach keeps all requests and data within your local environment, eliminating the risk of sensitive information being sent to a remote provider.\n\n### Data Protection: Confirmation for Destructive Operations\n\nMongoDB Lens implements a token-based confirmation system for potentially destructive operations, requiring a two-step process to execute tools that may otherwise result in unchecked data loss:\n\n1. First tool invocation: Returns a 4-digit confirmation token that expires after 5 minutes\n1. Second tool invocation: Executes the operation if provided with the valid token\n\nFor an example of the confirmation process, see: [Working with Confirmation Protection](#tutorial-5-working-with-confirmation-protection)\n\nTools that require confirmation include:\n\n- `drop-user`: Remove a database user\n- `drop-index`: Remove an index (potential performance impact)\n- `drop-database`: Permanently delete a database\n- `drop-collection`: Delete a collection and all its documents\n- `delete-document`: Delete one or multiple documents\n- `bulk-operations`: When including delete operations\n- `rename-collection`: When the target collection exists and will be dropped\n\nThis protection mechanism aims to prevent accidental data loss from typos and unintended commands. It's a safety net ensuring you're aware of the consequences before proceeding with potentially harmful actions.\n\n> [!NOTE]<br>\n> If you're working in a controlled environment where data loss is acceptable, you can configure MongoDB Lens to [bypass confirmation](#bypassing-confirmation-for-destructive-operations) and perform destructive operations immediately.\n\n#### Bypassing Confirmation for Destructive Operations\n\nYou might want to bypass the token confirmation system.\n\nSet the environment variable `CONFIG_DISABLE_DESTRUCTIVE_OPERATION_TOKENS` to `true` to execute destructive operations immediately without confirmation:\n\n```console\n# Using NPX\nCONFIG_DISABLE_DESTRUCTIVE_OPERATION_TOKENS=true npx -y mongodb-lens@latest\n\n# Using Docker\ndocker run --rm -i --network=host --pull=always -e CONFIG_DISABLE_DESTRUCTIVE_OPERATION_TOKENS='true' furey/mongodb-lens\n```\n\n> [!WARNING]<br>\n> Disabling confirmation tokens removes an important safety mechanism. It's strongly recommended to only use this option in controlled environments where data loss is acceptable, such as development or testing. Disable at your own risk.\n\n### Data Protection: Disabling Destructive Operations\n\n- [Disabling Tools](#disabling-tools)\n- [High-Risk Tools](#high-risk-tools)\n- [Medium-Risk Tools](#medium-risk-tools)\n- [Read-Only Configuration](#read-only-configuration)\n- [Selective Component Enabling](#selective-component-enabling)\n\n#### Disabling Tools\n\nMongoDB Lens includes several tools that can modify or delete data. To disable specific tools, add them to the `disabled.tools` array in your [configuration file](#configuration-config-file):\n\n```json\n{\n  \"disabled\": {\n    \"tools\": [\n      \"drop-user\",\n      \"drop-index\",\n      \"drop-database\",\n      \"drop-collection\",\n      \"delete-document\",\n      \"bulk-operations\",\n      \"rename-collection\"\n    ]\n  }\n}\n```\n\n> [!NOTE]<br>\n> Resources and prompts can also be disabled via `disabled.resources` and `disabled.prompts` settings.\n\n#### High-Risk Tools\n\nThese tools can cause immediate data loss and should be considered for disabling in sensitive environments:\n\n- `drop-user`: Removes database users and their access permissions\n- `drop-index`: Removes indexes (can impact query performance)\n- `drop-database`: Permanently deletes entire databases\n- `drop-collection`: Permanently deletes collections and all their documents\n- `delete-document`: Removes documents matching specified criteria\n- `bulk-operations`: Can perform batch deletions when configured to do so\n- `rename-collection`: Can overwrite existing collections when using the drop target option\n\n#### Medium-Risk Tools\n\nThese tools can modify data but typically don't cause immediate data loss:\n\n- `create-user`: Creates users with permissions that could enable further changes\n- `transaction`: Executes multiple operations in a transaction (potential for complex changes)\n- `update-document`: Updates documents which could overwrite existing data\n\n#### Read-Only Configuration\n\nFor a complete read-only configuration, disable all potentially destructive tools:\n\n```json\n{\n  \"disabled\": {\n    \"tools\": [\n      \"drop-user\",\n      \"drop-index\",\n      \"create-user\",\n      \"transaction\",\n      \"create-index\",\n      \"drop-database\",\n      \"drop-collection\",\n      \"insert-document\",\n      \"update-document\",\n      \"delete-document\",\n      \"bulk-operations\",\n      \"create-database\",\n      \"gridfs-operation\",\n      \"create-collection\",\n      \"rename-collection\",\n      \"create-timeseries\"\n    ]\n  }\n}\n```\n\nThis configuration allows MongoDB Lens to query and analyze data while preventing any modifications, providing multiple layers of protection against accidental data loss.\n\n#### Selective Component Enabling\n\nIn addition to [disabling components](#disabling-tools), specify exactly which components should be enabled (implicitly disabling all others) using the `enabled` settings in your [configuration file](#configuration-config-file):\n\n```json\n{\n  \"enabled\": {\n    \"tools\": [\n      \"use-database\",\n      \"find-documents\",\n      \"count-documents\",\n      \"aggregate-data\"\n    ]\n  },\n  \"disabled\": {\n    \"resources\": true,\n    \"prompts\": true\n  }\n}\n```\n\n> [!IMPORTANT]<br>\n> If a component appears in both `enabled` and `disabled` lists, the `enabled` setting takes precedence.\n\n## Tutorial\n\nThis following tutorial guides you through setting up a MongoDB container with sample data, then using MongoDB Lens to interact with it through natural language queries:\n\n1. [Start Sample Data Container](#tutorial-1-start-sample-data-container)\n1. [Import Sample Data](#tutorial-2-import-sample-data)\n1. [Connect MongoDB Lens](#tutorial-3-connect-mongodb-lens)\n1. [Example Queries](#tutorial-4-example-queries)\n1. [Working With Confirmation Protection](#tutorial-5-working-with-confirmation-protection)\n\n### Tutorial: 1. Start Sample Data Container\n\n> [!NOTE]<br>\n> This tutorial assumes you have [Docker](https://docs.docker.com/get-started/get-docker/) installed and running on your system.\n\n> [!IMPORTANT]<br>\n> If Docker is already running a container on port 27017, stop it before proceeding.\n\n1. Initialise the sample data container:<br>\n    ```console\n    docker run --name mongodb-sampledata -d -p 27017:27017 mongo:6\n    ```\n1. Verify the container is running without issue:<br>\n    ```console\n    docker ps | grep mongodb-sampledata\n    ```\n\n### Tutorial: 2. Import Sample Data\n\nMongoDB provides several [sample datasets](https://www.mongodb.com/docs/atlas/sample-data/#available-sample-datasets) which we'll use to explore MongoDB Lens.\n\n1. Download the sample datasets:\n    ```console<br>\n    curl -LO https://atlas-education.s3.amazonaws.com/sampledata.archive\n    ```\n1. Copy the sample datasets into your sample data container:<br>\n    ```console\n    docker cp sampledata.archive mongodb-sampledata:/tmp/\n    ```\n1. Import the sample datasets into MongoDB:<br>\n    ```console\n    docker exec -it mongodb-sampledata mongorestore --archive=/tmp/sampledata.archive\n    ```\n\nThis will import several databases:\n\n- `sample_airbnb`: Airbnb listings and reviews\n- `sample_analytics`: Customer and account data\n- `sample_geospatial`: Geographic data\n- `sample_mflix`: Movie data\n- `sample_restaurants`: Restaurant data\n- `sample_supplies`: Supply chain data\n- `sample_training`: Training data for various applications\n- `sample_weatherdata`: Weather measurements\n\n### Tutorial: 3. Connect MongoDB Lens\n\n[Install](#installation) MongoDB Lens as per the [Quick Start](#quick-start) instructions.\n\nSet your [MCP Client](#client-setup) to connect to MongoDB Lens via: `mongodb://localhost:27017`\n\n> [!TIP]<br>\n> Omitting the connection string from your MCP Client configuration will default the connection string to `mongodb://localhost:27017`.\n\nExample [Claude Desktop configuration](#client-setup-claude-desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-lens\": {\n      \"command\": \"/path/to/npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-lens@latest\"\n      ]\n    }\n  }\n}\n```\n\n### Tutorial: 4. Example Queries\n\nWith your MCP Client running and connected to MongoDB Lens, try the following example queries:\n\n- [Example Queries: Basic Database Operations](#example-queries-basic-database-operations)\n- [Example Queries: Collection Management](#example-queries-collection-management)\n- [Example Queries: User Management](#example-queries-user-management)\n- [Example Queries: Querying Data](#example-queries-querying-data)\n- [Example Queries: Schema Analysis](#example-queries-schema-analysis)\n- [Example Queries: Data Modification](#example-queries-data-modification)\n- [Example Queries: Performance & Index Management](#example-queries-performance--index-management)\n- [Example Queries: Geospatial & Special Operations](#example-queries-geospatial--special-operations)\n- [Example Queries: Export, Administrative & Other Features](#example-queries-export-administrative--other-features)\n- [Example Queries: Connection Management](#example-queries-connection-management)\n\n#### Example Queries: Basic Database Operations\n\n- _\"List all databases\"_<br>\n  <sup>➥ Uses `list-databases` tool</sup>\n- _\"What db am I currently using?\"_<br>\n  <sup>➥ Uses `current-database` tool</sup>\n- _\"Switch to the sample_mflix database\"_<br>\n  <sup>➥ Uses `use-database` tool</sup>\n- _\"Create a new db called test_db\"_<br>\n  <sup>➥ Uses `create-database` tool</sup>\n- _\"Create another db called analytics_db and switch to it\"_<br>\n  <sup>➥ Uses `create-database` tool with switch=true</sup>\n- _\"Drop test_db\"_<br>\n  <sup>➥ Uses `drop-database` tool (with confirmation)</sup>\n\n#### Example Queries: Collection Management\n\n- _\"What collections are in the current database?\"_<br>\n  <sup>➥ Uses `list-collections` tool</sup>\n- _\"Create user_logs collection\"_<br>\n  <sup>➥ Uses `create-collection` tool</sup>\n- _\"Rename user_logs to system_logs\"_<br>\n  <sup>➥ Uses `rename-collection` tool</sup>\n- _\"Drop system_logs\"_<br>\n  <sup>➥ Uses `drop-collection` tool (with confirmation)</sup>\n- _\"Check the data consistency in the movies collection\"_<br>\n  <sup>➥ Uses `validate-collection` tool</sup>\n\n#### Example Queries: User Management\n\n- _\"Create a read-only user for analytics\"_<br>\n  <sup>➥ Uses `create-user` tool</sup>\n- _\"Drop the inactive_user account\"_<br>\n  <sup>➥ Uses `drop-user` tool (with confirmation)</sup>\n\n#### Example Queries: Querying Data\n\n- _\"Count all docs in the movies collection\"_<br>\n  <sup>➥ Uses `count-documents` tool</sup>\n- _\"Find the top 5 movies with the highest IMDB rating\"_<br>\n  <sup>➥ Uses `find-documents` tool</sup>\n- _\"Show me aggregate data for movies grouped by decade\"_<br>\n  <sup>➥ Uses `aggregate-data` tool</sup>\n- _\"List all unique countries where movies were produced\"_<br>\n  <sup>➥ Uses `distinct-values` tool</sup>\n- _\"Search for movies containing godfather in their title\"_<br>\n  <sup>➥ Uses `text-search` tool</sup>\n- _\"Find German users with last name müller using proper collation\"_<br>\n  <sup>➥ Uses `collation-query` tool</sup>\n\n#### Example Queries: Schema Analysis\n\n- _\"What's the schema structure of the movies collection?\"_<br>\n  <sup>➥ Uses `analyze-schema` tool</sup>\n- _\"Compare users and comments schemas\"_<br>\n  <sup>➥ Uses `compare-schemas` tool</sup>\n- _\"Generate a schema validator for the movies collection\"_<br>\n  <sup>➥ Uses `generate-schema-validator` tool</sup>\n- _\"Analyze common query patterns for the movies collection\"_<br>\n  <sup>➥ Uses `analyze-query-patterns` tool</sup>\n\n#### Example Queries: Data Modification\n\n- _\"Insert new movie document: \\<your field data\\>\"_<br>\n  <sup>➥ Uses `insert-document` tool</sup>\n- _\"Update all movies from 1994 to add a 'classic' flag\"_<br>\n  <sup>➥ Uses `update-document` tool</sup>\n- _\"Delete all movies with zero ratings\"_<br>\n  <sup>➥ Uses `delete-document` tool (with confirmation)</sup>\n- _\"Run these bulk operations on the movies collection: \\<your JSON data\\>\"_<br>\n  <sup>➥ Uses `bulk-operations` tool</sup>\n\n> [!TIP]<br>\n> For specialized MongoDB operations (like array operations, bitwise operations, or other complex updates), use MongoDB's native operators via the `update-document` tool's `update` and `options` parameters.\n\n#### Example Queries: Performance & Index Management\n\n- _\"Create an index on the title field in the movies collection\"_<br>\n  <sup>➥ Uses `create-index` tool</sup>\n- _\"Drop the ratings_idx index\"_<br>\n  <sup>➥ Uses `drop-index` tool (with confirmation)</sup>\n- _\"Explain the execution plan for finding movies from 1995\"_<br>\n  <sup>➥ Uses `explain-query` tool</sup>\n- _\"Get statistics for the current db\"_<br>\n  <sup>➥ Uses `get-stats` tool with target=database</sup>\n- _\"Show collection stats for the movies collection\"_<br>\n  <sup>➥ Uses `get-stats` tool with target=collection</sup>\n\n#### Example Queries: Geospatial & Special Operations\n\n- _\"Switch to sample_geospatial db, then find all shipwrecks within 10km of coordinates [-80.12, 26.46]\"_<br>\n  <sup>➥ Uses `geo-query` tool</sup>\n- _\"Switch to sample_analytics db, then execute a transaction to move funds between accounts: \\<account ids\\>\"_<br>\n  <sup>➥ Uses `transaction` tool</sup>\n- _\"Create a time series collection for sensor readings\"_<br>\n  <sup>➥ Uses `create-timeseries` tool</sup>\n- _\"Watch for changes in the users collection for 30 seconds\"_<br>\n  <sup>➥ Uses `watch-changes` tool</sup>\n- _\"List all files in the images GridFS bucket\"_<br>\n  <sup>➥ Uses `gridfs-operation` tool with operation=list</sup>\n\n#### Example Queries: Export, Administrative & Other Features\n\n- _\"Switch to sample_mflix db, then export the top 20 movies based on 'tomatoes.critic.rating' as a CSV with title, year and rating fields (output in a single code block)\"_<br>\n  <sup>➥ Uses `export-data` tool</sup>\n- _\"Switch to sample_analytics db, then check its sharding status\"_<br>\n  <sup>➥ Uses `shard-status` tool</sup>\n- _\"Clear the collections cache\"_<br>\n  <sup>➥ Uses `clear-cache` tool with target=collections</sup>\n- _\"Clear all caches\"_<br>\n  <sup>➥ Uses `clear-cache` tool</sup>\n- _\"Switch to sample_weatherdata db then generate an interactive report on its current state\"_<br>\n  <sup>➥ Uses numerous tools</sup>\n\n#### Example Queries: Connection Management\n\n- _\"Connect to mongodb://localhost:27018\"_<br>\n  <sup>➥ Uses `connect-mongodb` tool</sup>\n- _\"Connect to mongodb+srv://username:password@cluster.mongodb.net/mydb\"_<br>\n  <sup>➥ Uses `connect-mongodb` tool</sup>\n- _\"Connect back to the original mongodb instance\"_<br>\n  <sup>➥ Uses `connect-original` tool</sup>\n- _\"Connect to replica set without validating the connection: \\<replica set details\\>\"_<br>\n  <sup>➥ Uses `connect-mongodb` tool with validateConnection=false</sup>\n- _\"Add connection alias 'prod' for mongodb://username:password@prod-server:27017/mydb\"_<br>\n<sup>➥ Uses `add-connection-alias` tool</sup>\n\n### Tutorial: 5. Working With Confirmation Protection\n\nMongoDB Lens includes a safety mechanism for potentially destructive operations. Here's how it works in practice:\n\n1. Request to drop a collection:<br>\n    ```\n    \"Drop the collection named test_collection\"\n    ```\n1. MongoDB Lens responds with a warning and confirmation token:<br>\n    ```\n    ⚠️ DESTRUCTIVE OPERATION WARNING ⚠️\n\n    You've requested to drop the collection 'test_collection'.\n\n    This operation is irreversible and will permanently delete all data in this collection.\n\n    To confirm, you must type the 4-digit confirmation code EXACTLY as shown below:\n\n    Confirmation code: 9876\n\n    This code will expire in 5 minutes for security purposes.\n    ```\n1. Confirm the operation by submitting the confirmation token:<br>\n    ```\n    \"9876\"\n    ```\n1. MongoDB Lens executes the operation:<br>\n    ```\n    Collection 'test_collection' has been permanently deleted.\n    ```\n\nThis two-step process prevents accidental data loss by requiring explicit confirmation.\n\n> [!NOTE]<br>\n> If you're working in a controlled environment where data loss is acceptable, you can configure MongoDB Lens to [bypass confirmation](#bypassing-confirmation-for-destructive-operations) and perform destructive operations immediately.\n\n## Test Suite\n\nMongoDB Lens includes a [test suite](./mongodb-lens.test.js) to verify functionality across tools, resources, and prompts.\n\n- [Running Tests](#test-suite-running-tests)\n- [Command Line Options](#test-suite-command-line-options)\n- [Examples](#test-suite-examples)\n\n### Test Suite: Running Tests\n\nThe test suite requires a `CONFIG_MONGO_URI` environment variable which can be set to:\n\n- a MongoDB connection string (e.g. `mongodb://localhost:27017`)\n- `mongodb-memory-server` (for in-memory testing)\n\n```console\n# Run Tests with MongoDB Connection String\nCONFIG_MONGO_URI=mongodb://localhost:27017 node mongodb-lens.test.js\n\n# Run Tests with In-Memory MongoDB (requires mongodb-memory-server)\nCONFIG_MONGO_URI=mongodb-memory-server node mongodb-lens.test.js\n```\n\nFor convenience, the following scripts are available for running tests:\n\n```console\nnpm test                        # Fails if no CONFIG_MONGO_URI provided\nnpm run test:localhost          # Uses mongodb://localhost:27017\nnpm run test:localhost:verbose  # Runs with DEBUG=true for verbose output\nnpm run test:in-memory          # Uses mongodb-memory-server\nnpm run test:in-memory:verbose  # Runs with DEBUG=true for verbose output\n```\n\n> [!NOTE]<br>\n> The test suite creates temporary databases and collections that are cleaned up after test completion.\n\n### Test Suite: Command Line Options\n\n| Option             | Description                                          |\n| ------------------ | ---------------------------------------------------- |\n| `--list`           | List all available tests without running them        |\n| `--test=<n>`       | Run specific test(s) by name (comma-separated)       |\n| `--group=<n>`      | Run all tests in specific group(s) (comma-separated) |\n| `--pattern=<glob>` | Run tests matching pattern(s) (comma-separated)      |\n\n### Test Suite: Examples\n\n```console\n# List All Available Tests\nnpm test -- --list\n\n# Run Only Connection-Related Tests (:27017)\nnpm run test:localhost -- --group=Connection\\ Tools\n\n# Test Specific Database Operations (In-Memory)\nnpm run test:in-memory -- --test=create-database\\ Tool,drop-database\\ Tool\n\n# Test All Document-Related Tools (:27017)\nnpm run test:localhost -- --pattern=document\n\n# Run Resource Tests Only (In-Memory)\nnpm run test:in-memory -- --group=Resources\n\n# Run Specific Tests Only (In-Memory)\nnpm run test:in-memory -- --test=aggregate-data\\ Tool,find-documents\\ Tool\n```\n\n## Disclaimer\n\nMongoDB Lens:\n\n- is licensed under the [MIT License](./LICENSE).\n- is not affiliated with or endorsed by MongoDB, Inc.\n- is written with the assistance of AI and may contain errors.\n- is intended for educational and experimental purposes only.\n- is provided as-is with no warranty—please use at your own risk.\n\n## Support\n\nIf you've found MongoDB Lens helpful consider supporting my work through:\n\n[Buy Me a Coffee](https://www.buymeacoffee.com/furey) | [GitHub Sponsorship](https://github.com/sponsors/furey)\n\nContributions help me continue developing and improving this tool, allowing me to dedicate more time to add new features and ensuring it remains a valuable resource for the community.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "mongodb lens",
        "mongodb databases",
        "furey mongodb"
      ],
      "category": "databases"
    },
    "gannonh--firebase-mcp": {
      "owner": "gannonh",
      "name": "firebase-mcp",
      "url": "https://github.com/gannonh/firebase-mcp",
      "imageUrl": "",
      "description": "Firebase services including Auth, Firestore and Storage.",
      "stars": 215,
      "forks": 41,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T12:43:21Z",
      "readme_content": "# Firebase MCP\n\n\n\n\n<a href=\"https://glama.ai/mcp/servers/x4i8z2xmrq\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/x4i8z2xmrq/badge\" alt=\"Firebase MCP server\" />\n</a>\n\n[![Firebase Tests CI](https://github.com/gannonh/firebase-mcp/actions/workflows/tests.yml/badge.svg)](https://github.com/gannonh/firebase-mcp/actions/workflows/tests.yml)\n\n## Overview\n\n**Firebase MCP** enables AI assistants to work directly with Firebase services, including:\n\n- **Firestore**: Document database operations\n- **Storage**: File management with robust upload capabilities\n- **Authentication**: User management and verification\n\nThe server works with MCP client applicatios such as [Claude Desktop](https://claude.ai/download), [Augment Code](https://docs.augmentcode.com/setup-augment/mcp), [VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers), and [Cursor](https://www.cursor.com/).\n\n> ⚠️ **Known Issue**: The `firestore_list_collections` tool may return a Zod validation error in the client logs. This is an erroneous validation error in the MCP SDK, as our investigation confirmed no boolean values are present in the response. Despite the error message, the query still works correctly and returns the proper collection data. This is a log-level error that doesn't affect functionality.\n\n## ⚡ Quick Start\n\n### Prerequisites\n- Firebase project with service account credentials\n- Node.js environment\n\n### 1. Install MCP Server\n\nAdd the server configuration to your MCP settings file:\n\n- Claude Desktop: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Augment: `~/Library/Application Support/Code/User/settings.json`\n- Cursor: `[project root]/.cursor/mcp.json`\n\nMCP Servers can be installed manually or at runtime via npx (recommended). How you install determines your configuration:\n\n#### Configure for npx (recommended)\n\n   ```json\n   {\n     \"firebase-mcp\": {\n       \"command\": \"npx\",\n       \"args\": [\n         \"-y\",\n         \"@gannonh/firebase-mcp\"\n       ],\n       \"env\": {\n         \"SERVICE_ACCOUNT_KEY_PATH\": \"/absolute/path/to/serviceAccountKey.json\",\n         \"FIREBASE_STORAGE_BUCKET\": \"your-project-id.firebasestorage.app\"\n       }\n     }\n   }\n   ```\n\n#### Configure for local installation\n\n   ```json\n   {\n     \"firebase-mcp\": {\n       \"command\": \"node\",\n       \"args\": [\n         \"/absolute/path/to/firebase-mcp/dist/index.js\"\n       ],\n       \"env\": {\n         \"SERVICE_ACCOUNT_KEY_PATH\": \"/absolute/path/to/serviceAccountKey.json\",\n         \"FIREBASE_STORAGE_BUCKET\": \"your-project-id.firebasestorage.app\"\n       }\n     }\n   }\n```\n\n\n### 2. Test the Installation\n\nAsk your AI client: \"Please test all Firebase MCP tools.\"\n\n## 🛠️ Setup & Configuration\n\n### 1. Firebase Configuration\n\n1. Go to [Firebase Console](https://console.firebase.google.com) → Project Settings → Service Accounts\n2. Click \"Generate new private key\"\n3. Save the JSON file securely\n\n### 2. Environment Variables\n\n#### Required\n- `SERVICE_ACCOUNT_KEY_PATH`: Path to your Firebase service account key JSON (required)\n\n#### Optional\n- `FIREBASE_STORAGE_BUCKET`: Bucket name for Firebase Storage (defaults to `[projectId].appspot.com`)\n- `MCP_TRANSPORT`: Transport type to use (`stdio` or `http`) (defaults to `stdio`)\n- `MCP_HTTP_PORT`: Port for HTTP transport (defaults to `3000`)\n- `MCP_HTTP_HOST`: Host for HTTP transport (defaults to `localhost`)\n- `MCP_HTTP_PATH`: Path for HTTP transport (defaults to `/mcp`)\n- `DEBUG_LOG_FILE`: Enable file logging:\n  - Set to `true` to log to `~/.firebase-mcp/debug.log`\n  - Set to a file path to log to a custom location\n\n### 3. Client Integration\n\n#### Claude Desktop\nEdit: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n#### VS Code / Augment\nEdit: `~/Library/Application Support/Code/User/settings.json`\n\n#### Cursor\nEdit: `[project root]/.cursor/mcp.json`\n\n## 📚 API Reference\n\n### Firestore Tools\n\n| Tool                               | Description                    | Required Parameters        |\n| ---------------------------------- | ------------------------------ | -------------------------- |\n| `firestore_add_document`           | Add a document to a collection | `collection`, `data`       |\n| `firestore_list_documents`         | List documents with filtering  | `collection`               |\n| `firestore_get_document`           | Get a specific document        | `collection`, `id`         |\n| `firestore_update_document`        | Update an existing document    | `collection`, `id`, `data` |\n| `firestore_delete_document`        | Delete a document              | `collection`, `id`         |\n| `firestore_list_collections`       | List root collections          | None                       |\n| `firestore_query_collection_group` | Query across subcollections    | `collectionId`             |\n\n### Storage Tools\n\n| Tool                      | Description               | Required Parameters              |\n| ------------------------- | ------------------------- | -------------------------------- |\n| `storage_list_files`      | List files in a directory | None (optional: `directoryPath`) |\n| `storage_get_file_info`   | Get file metadata and URL | `filePath`                       |\n| `storage_upload`          | Upload file from content  | `filePath`, `content`            |\n| `storage_upload_from_url` | Upload file from URL      | `filePath`, `url`                |\n\n### Authentication Tools\n\n| Tool            | Description             | Required Parameters |\n| --------------- | ----------------------- | ------------------- |\n| `auth_get_user` | Get user by ID or email | `identifier`        |\n\n## 💻 Developer Guide\n\n### Installation & Building\n\n```bash\ngit clone https://github.com/gannonh/firebase-mcp\ncd firebase-mcp\nnpm install\nnpm run build\n```\n\n### Running Tests\n\nFirst, install and start Firebase emulators:\n```bash\nnpm install -g firebase-tools\nfirebase init emulators\nfirebase emulators:start\n```\n\nThen run tests:\n```bash\n# Run tests with emulator\nnpm run test:emulator\n\n# Run tests with coverage\nnpm run test:coverage:emulator\n```\n\n### Project Structure\n\n```bash\nsrc/\n├── index.ts                  # Server entry point\n├── utils/                    # Utility functions\n└── lib/\n    └── firebase/              # Firebase service clients\n        ├── authClient.ts     # Authentication operations\n        ├── firebaseConfig.ts   # Firebase configuration\n        ├── firestoreClient.ts # Firestore operations\n        └── storageClient.ts  # Storage operations\n```\n\n## 🌐 HTTP Transport\n\nFirebase MCP now supports HTTP transport in addition to the default stdio transport. This allows you to run the server as a standalone HTTP service that can be accessed by multiple clients.\n\n### Running with HTTP Transport\n\nTo run the server with HTTP transport:\n\n```bash\n# Using environment variables\nMCP_TRANSPORT=http MCP_HTTP_PORT=3000 node dist/index.js\n\n# Or with npx\nMCP_TRANSPORT=http MCP_HTTP_PORT=3000 npx @gannonh/firebase-mcp\n```\n\n### Client Configuration for HTTP\n\nWhen using HTTP transport, configure your MCP client to connect to the HTTP endpoint:\n\n```json\n{\n  \"firebase-mcp\": {\n    \"url\": \"http://localhost:3000/mcp\"\n  }\n}\n```\n\n### Session Management\n\nThe HTTP transport supports session management, allowing multiple clients to connect to the same server instance. Each client receives a unique session ID that is used to maintain state between requests.\n\n## 🔍 Troubleshooting\n\n### Common Issues\n\n#### Storage Bucket Not Found\nIf you see \"The specified bucket does not exist\" error:\n1. Verify your bucket name in Firebase Console → Storage\n2. Set the correct bucket name in `FIREBASE_STORAGE_BUCKET` environment variable\n\n#### Firebase Initialization Failed\nIf you see \"Firebase is not initialized\" error:\n1. Check that your service account key path is correct and absolute\n2. Ensure the service account has proper permissions for Firebase services\n\n#### Composite Index Required\nIf you receive \"This query requires a composite index\" error:\n1. Look for the provided URL in the error message\n2. Follow the link to create the required index in Firebase Console\n3. Retry your query after the index is created (may take a few minutes)\n\n#### Zod Validation Error with `firestore_list_collections`\nIf you see a Zod validation error with message \"Expected object, received boolean\" when using the `firestore_list_collections` tool:\n\n> ⚠️ **Known Issue**: The `firestore_list_collections` tool may return a Zod validation error in the client logs. This is an erroneous validation error in the MCP SDK, as our investigation confirmed no boolean values are present in the response. Despite the error message, the query still works correctly and returns the proper collection data. This is a log-level error that doesn't affect functionality.\n\n### Debugging\n\n#### Enable File Logging\nTo help diagnose issues, you can enable file logging:\n\n```bash\n# Log to default location (~/.firebase-mcp/debug.log)\nDEBUG_LOG_FILE=true npx @gannonh/firebase-mcp\n\n# Log to a custom location\nDEBUG_LOG_FILE=/path/to/custom/debug.log npx @gannonh/firebase-mcp\n```\n\nYou can also enable logging in your MCP client configuration:\n\n```json\n{\n  \"firebase-mcp\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@gannonh/firebase-mcp\"],\n    \"env\": {\n      \"SERVICE_ACCOUNT_KEY_PATH\": \"/path/to/serviceAccountKey.json\",\n      \"FIREBASE_STORAGE_BUCKET\": \"your-project-id.firebasestorage.app\",\n      \"DEBUG_LOG_FILE\": \"true\"\n    }\n  }\n}\n```\n\n#### Real-time Log Viewing\nTo view logs in real-time:\n\n```bash\n# Using tail to follow the log file\ntail -f ~/.firebase-mcp/debug.log\n\n# Using a split terminal to capture stderr\nnpm start 2>&1 | tee logs.txt\n```\n\n#### Using MCP Inspector\nThe MCP Inspector provides interactive debugging:\n\n```bash\n# Install MCP Inspector\nnpm install -g @mcp/inspector\n\n# Connect to your MCP server\nmcp-inspector --connect stdio --command \"node ./dist/index.js\"\n```\n\n## 📋 Response Formatting\n\n### Storage Upload Response Example\n\n```json\n{\n  \"name\": \"reports/quarterly.pdf\",\n  \"size\": \"1024000\",\n  \"contentType\": \"application/pdf\",\n  \"updated\": \"2025-04-11T15:37:10.290Z\",\n  \"downloadUrl\": \"https://storage.googleapis.com/bucket/reports/quarterly.pdf?alt=media\",\n  \"bucket\": \"your-project.appspot.com\"\n}\n```\n\nDisplayed to the user as:\n\n```markdown\n## File Successfully Uploaded! 📁\n\nYour file has been uploaded to Firebase Storage:\n\n**File Details:**\n- **Name:** reports/quarterly.pdf\n- **Size:** 1024000 bytes\n- **Type:** application/pdf\n- **Last Updated:** April 11, 2025 at 15:37:10 UTC\n\n**[Click here to download your file](https://storage.googleapis.com/bucket/reports/quarterly.pdf?alt=media)**\n```\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Implement changes with tests (80%+ coverage required)\n4. Submit a pull request\n\n## 📄 License\n\nMIT License - see [LICENSE](LICENSE) file for details\n\n## 🔗 Related Resources\n\n- [Model Context Protocol Documentation](https://github.com/modelcontextprotocol)\n- [Firebase Documentation](https://firebase.google.com/docs)\n- [Firebase Admin SDK](https://firebase.google.com/docs/admin/setup)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firebase",
        "databases",
        "database",
        "secure database",
        "databases secure",
        "gannonh firebase"
      ],
      "category": "databases"
    },
    "georgi-terziyski--database_mcp_server": {
      "owner": "georgi-terziyski",
      "name": "database_mcp_server",
      "url": "https://github.com/georgi-terziyski/database_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/georgi-terziyski.webp",
      "description": "Connect and interact with various database systems by executing SQL queries, managing schemas, and handling transactions through a unified interface. Provides support for multiple databases including SQLite, PostgreSQL, MySQL/MariaDB, and SQL Server.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-25T21:58:56Z",
      "readme_content": "# Database MCP Server\n\nA Model Context Protocol (MCP) server that provides tools for connecting to and interacting with various database systems.\n\n## Features\n\n- **Multi-Database Support**: Connect to SQLite, PostgreSQL, MySQL/MariaDB, and SQL Server databases\n- **Unified Interface**: Common tools for database operations across all supported database types\n- **Database-Specific Extensions**: Where needed, specific tools for database-specific features\n- **Schema Management**: Create, alter, and drop tables and indexes\n- **Query Execution**: Execute raw SQL queries or use structured query tools\n- **Transaction Support**: Begin, commit, and rollback transactions\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8 or higher\n- Required Python packages (installed automatically with pip):\n  - SQLAlchemy\n  - Various database drivers, depending on which databases you want to use:\n    - SQLite (included with Python)\n    - PostgreSQL: `psycopg2-binary`\n    - MySQL/MariaDB: `mysql-connector-python`\n    - SQL Server: `pyodbc`\n\n### Installing from Source\n\n```bash\n# Clone the repository\ngit clone <repository-url>\n\n# Install the package\npip install -e .\n```\n\n## Configuration\n\nThe server can be configured using environment variables, a configuration file, or by providing connection details at runtime.\n\n### Environment Variables\n\n- `DB_CONFIG_PATH`: Path to a JSON configuration file\n- `DB_CONNECTIONS`: A comma-separated list of connection IDs or a JSON string with connection details\n\n### Configuration File Format\n\n```json\n{\n  \"connections\": {\n    \"sqlite_conn\": {\n      \"type\": \"sqlite\",\n      \"db_path\": \"/path/to/database.db\"\n    },\n    \"postgres_conn\": {\n      \"type\": \"postgres\",\n      \"host\": \"localhost\",\n      \"port\": 5432,\n      \"database\": \"mydatabase\",\n      \"user\": \"myuser\",\n      \"password\": \"mypassword\"\n    }\n  }\n}\n```\n\n## Usage\n\n### Running the Server\n\n#### As an MCP Server for Claude\n\n```bash\n# Run with default settings\npython -m db_mcp_server\n\n# Specify a configuration file\npython -m db_mcp_server --config /path/to/config.json\n\n# Set logging level\npython -m db_mcp_server --log-level DEBUG\n```\n\n#### As a Standalone Web Server (for any LLM)\n\n```bash\n# Run as a web server\npython -m db_mcp_server.web_server\n\n# Specify host and port\npython -m db_mcp_server.web_server --host 0.0.0.0 --port 8000\n\n# Specify configuration file and logging level\npython -m db_mcp_server.web_server --config /path/to/config.json --log-level DEBUG\n```\n\n### Available MCP Tools\n\n#### Connection Management\n\n- `add_connection`: Add a new database connection\n- `test_connection`: Test a database connection\n- `list_connections`: List all database connections\n- `remove_connection`: Remove a database connection\n\n#### Query Execution\n\n- `execute_query`: Execute a SQL query\n- `get_records`: Get records from a table\n- `insert_record`: Insert a record into a table\n- `update_record`: Update records in a table\n- `delete_record`: Delete records from a table\n\n#### Schema Management\n\n- `list_tables`: List all tables in a database\n- `get_table_schema`: Get the schema for a table\n- `create_table`: Create a new table\n- `drop_table`: Drop a table\n- `create_index`: Create an index on a table\n- `drop_index`: Drop an index\n- `alter_table`: Alter a table structure\n\n#### Transaction Management\n\n- `begin_transaction`: Begin a transaction\n- `commit_transaction`: Commit a transaction\n- `rollback_transaction`: Rollback a transaction\n\n## Examples\n\n### Add a Connection\n\n```json\n{\n  \"connection_id\": \"my_sqlite_db\",\n  \"type\": \"sqlite\",\n  \"db_path\": \"/path/to/database.db\"\n}\n```\n\n### Execute a Query\n\n```json\n{\n  \"connection_id\": \"my_sqlite_db\",\n  \"query\": \"SELECT * FROM users WHERE age > ?\",\n  \"params\": [21]\n}\n```\n\n### Create a Table\n\n```json\n{\n  \"connection_id\": \"my_sqlite_db\",\n  \"table\": \"users\",\n  \"columns\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"INTEGER\",\n      \"primary_key\": true,\n      \"nullable\": false\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"TEXT\",\n      \"nullable\": false\n    },\n    {\n      \"name\": \"email\",\n      \"type\": \"TEXT\",\n      \"nullable\": true\n    }\n  ]\n}\n```\n\n### Insert Records\n\n```json\n{\n  \"connection_id\": \"my_sqlite_db\",\n  \"table\": \"users\",\n  \"data\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\"\n  }\n}\n```\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests\npython -m unittest discover\n\n# Run specific test file\npython -m unittest tests.test_sqlite\n```\n\n## Connecting from Other LLMs\n\nWhen running as a standalone web server, other LLMs (like Llama 3) can connect to the database MCP server via HTTP. The server exposes the following endpoints:\n\n### Endpoints\n\n- `/list_tools` - GET or POST: Returns a list of all available tools with their descriptions and input schemas\n- `/call_tool` - POST: Execute a specific database tool\n\n### Example: Calling from Another LLM\n\nTo use this server with another LLM, have the LLM generate HTTP requests to the server. Here's an example of how you could structure the prompt for an LLM like Llama 3:\n\n```\nYou can interact with a database by making HTTP requests to a database service at http://localhost:8000. \nThe service provides the following endpoints:\n\n1. To get a list of available tools:\n   Make a POST request to: http://localhost:8000/list_tools\n   \n2. To execute a database tool:\n   Make a POST request to: http://localhost:8000/call_tool\n   with a JSON body like:\n   {\n     \"name\": \"tool_name\",\n     \"arguments\": {\n       \"param1\": \"value1\",\n       \"param2\": \"value2\"\n     }\n   }\n\nFor example, to execute a SQL query, you would make a request like:\nPOST http://localhost:8000/call_tool\nContent-Type: application/json\n\n{\n  \"name\": \"execute_query\",\n  \"arguments\": {\n    \"connection_id\": \"my_db\",\n    \"query\": \"SELECT * FROM users\"\n  }\n}\n```\n\n### Sample Python Code for Client Integration\n\n```python\nimport requests\nimport json\n\n# Base URL of the database MCP server\nBASE_URL = \"http://localhost:8000\"\n\n# List available tools\ndef list_tools():\n    response = requests.post(f\"{BASE_URL}/list_tools\")\n    return response.json()\n\n# Execute a database tool\ndef call_tool(tool_name, arguments):\n    payload = {\n        \"name\": tool_name,\n        \"arguments\": arguments\n    }\n    response = requests.post(f\"{BASE_URL}/call_tool\", json=payload)\n    return response.json()\n\n# Example: List tables in a database\ndef list_tables(connection_id):\n    return call_tool(\"list_tables\", {\"connection_id\": connection_id})\n\n# Example: Execute a SQL query\ndef execute_query(connection_id, query, params=None):\n    return call_tool(\"execute_query\", {\n        \"connection_id\": connection_id,\n        \"query\": query,\n        \"params\": params\n    })\n\n# Example: Add a new connection\ndef add_connection(connection_id, db_type, **kwargs):\n    args = {\"connection_id\": connection_id, \"type\": db_type}\n    args.update(kwargs)\n    return call_tool(\"add_connection\", args)\n```\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schemas",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "get-convex--convex-backend": {
      "owner": "get-convex",
      "name": "convex-backend",
      "url": "https://github.com/get-convex/convex-backend",
      "imageUrl": "/freedevtools/mcp/pfp/get-convex.webp",
      "description": "A reactive database that facilitates dynamic, live-updating applications by managing data and business logic using TypeScript. It offers a cloud platform and self-hosting options for developers to focus on application development without infrastructure concerns.",
      "stars": 7497,
      "forks": 407,
      "license": "Other",
      "language": "Rust",
      "updated_at": "2025-10-04T08:41:53Z",
      "readme_content": "<p align=\"center\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://static.convex.dev/logo/convex-logo-light.svg\" width=\"600\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://static.convex.dev/logo/convex-logo.svg\" width=\"600\">\n  <img alt=\"Convex logo\" src=\"https://static.convex.dev/logo/convex-logo.svg\" width=\"600\">\n</picture>\n</p>\n\n[Convex](https://convex.dev) is the open-source reactive database designed to\nmake life easy for web app developers, whether human or LLM. Fetch data and\nperform business logic with strong consistency by writing pure TypeScript.\n\nConvex provides a database, a place to write your server functions, and client\nlibraries. It makes it easy to build and scale dynamic live-updating apps.\n[Read the docs to learn more](https://docs.convex.dev/understanding/).\n\nDevelopment of the Convex backend is led by the Convex team. We\n[welcome bug fixes](./CONTRIBUTING.md) and\n[love receiving feedback](https://discord.gg/convex). We keep this repository\nsynced with any internal development work within a handful of days.\n\n## Getting Started\n\nVisit our [documentation](https://docs.convex.dev/) to learn more about Convex\nand follow our getting started guides.\n\nThe easiest way to build with Convex is through our\n[cloud platform](https://www.convex.dev/plans), which includes a generous free\ntier and lets you focus on building your application without worrying about\ninfrastructure. Many small applications and side-projects can operate entirely\non the free tier with zero cost and zero maintenance.\n\n## Self Hosting\n\nThe self-hosted product includes most features of the cloud product, including\nthe dashboard and CLI. Self-hosted Convex works well with a variety of tools\nincluding Neon, Fly.io, Vercel, Netlify, RDS, Sqlite, Postgres, and more.\n\nYou can either use Docker (recommended) or a prebuilt binary to self host\nConvex. Check out our [self-hosting guide](./self-hosted/README.md) for detailed\ninstructions. Community support for self-hosting is available in the\n`#self-hosted` channel on [Discord](https://discord.gg/convex).\n\n## Community & Support\n\n- Join our [Discord community](https://discord.gg/convex) for help and\n  discussions.\n- Report issues when building and using the open source Convex backend through\n  [GitHub Issues](https://github.com/get-convex/convex-backend/issues)\n\n## Building from source\n\nSee [BUILD.md](./BUILD.md).\n\n## Disclaimers\n\n- If you choose to self-host, we recommend following the self-hosting guide. If\n  you are instead building from source, make sure to change your instance secret\n  and admin key from the defaults in the repo.\n- Convex is battle tested most thoroughly on Linux and Mac. On Windows, it has\n  less experience. If you run into issues, please message us on\n  [Discord](https://convex.dev/community) in the `#self-hosted` channel.\n- Convex self-hosted builds contain a beacon to help Convex improve the product.\n  The information is minimal and anonymous and helpful to Convex, but if you\n  really want to disable it, you can set the `--disable-beacon` flag on the\n  backend binary. The beacon's messages print in the log and only include\n  - A random identifier for your deployment (not used elsewhere)\n  - Migration version of your database\n  - Git rev of the backend\n  - Uptime of the backend\n\n## Repository layout\n\n- `crates/` contains Rust code\n\n  - Main binary\n    - `local_backend/` is an application server on top of the `Runtime`. This is\n      the serving edge for the Convex cloud.\n\n- `npm-packages/` contains both our public and internal TypeScript packages.\n  - Internal packages\n    - `udf-runtime/` sets up the user-defined functions JS environment for\n      queries and mutations\n    - `udf-tests/` is a collection of functions used in testing the isolate\n      layer\n    - `system-udfs/` contains functions used by the Convex system e.g. the CLI\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "backend",
        "secure database",
        "databases secure",
        "reactive database"
      ],
      "category": "databases"
    },
    "giorgos3215--ultimate-cursor-mcp": {
      "owner": "giorgos3215",
      "name": "ultimate-cursor-mcp",
      "url": "https://github.com/giorgos3215/ultimate-cursor-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/giorgos3215.webp",
      "description": "Integrates advanced web, code, and file operations with Supabase for database management, providing tools for web scraping, code analysis, and file handling alongside AI-powered functionalities for intelligent analysis and self-improvement based on usage patterns.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-23T16:40:11Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/giorgos3215-ultimate-cursor-mcp-badge.png)](https://mseep.ai/app/giorgos3215-ultimate-cursor-mcp)\n\n# Ultimate Self-Evolving Cursor MCP\n\nA comprehensive MCP (Model Context Protocol) implementation for Cursor, featuring advanced tools for web, code, file operations, and Supabase database management.\n\n[![Smithery.ai](https://img.shields.io/badge/Smithery.ai-Available-blue.svg)](https://smithery.ai/package/ultimate-cursor-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Features\n\n- Advanced web tools (scraping, crawling, semantic search)\n- Powerful code analysis and refactoring tools\n- File operations with batch processing and watching capabilities\n- AI-powered capabilities (LLM queries, image analysis)\n- **Full Supabase integration** for database operations and management\n- Self-improvement mechanism with usage analytics\n- Memory persistence for better context understanding\n\n## Installation\n\n### Easy Setup (Recommended)\n\nRun the setup script which will install both the Ultimate Cursor MCP and optionally the Supabase MCP:\n\n```bash\n./setup.sh\n```\n\nThe script will:\n1. Install the Ultimate Cursor MCP\n2. Ask if you want to set up Supabase integration\n3. Guide you through providing Supabase credentials if needed\n4. Configure everything automatically\n\n### Manual Installation\n\n#### Ultimate Cursor MCP\n\n```bash\npython3 tools/mcp_installer.py local .\n```\n\n#### Supabase MCP (Optional)\n\n```bash\npython3 tools/mcp_installer.py supabase --url \"https://yourproject.supabase.co\" --key \"your-api-key\"\n```\n\n### Smithery.ai Installation\n\nIf you prefer to install via smithery.ai:\n\n```bash\ncursor smithery install ultimate-cursor-mcp\n```\n\n## Supabase Integration\n\nThe Supabase integration provides:\n\n- SQL query execution with safety controls (read-only by default)\n- Database schema inspection tools\n- Management API access with safety classifications\n- Auth Admin tools for user management\n\n### Benefits of Supabase MCP\n\n- **Safety features**: Starts in read-only mode; requires explicit mode switching for write operations\n- **Comprehensive database tools**: Schema inspection, table information, detailed structure\n- **Full SQL support**: Execute any PostgreSQL query with transaction handling\n- **Advanced Management API access**: Send arbitrary requests with auto-injection of project ref\n- **Auth Admin tools**: User creation, deletion, invitation and management\n\n[Read the complete Supabase integration guide](./docs/supabase.md)\n\n## Testing\n\nAfter installation, you can test the functionality:\n\n```bash\n./test-client.js\n```\n\n## Configuration\n\nThe configuration is stored in `~/.cursor/mcp.json`. After installation, restart Cursor for the changes to take effect.\n\n## Development\n\n### Project Structure\n\n- `src/` - TypeScript implementation of the MCP server\n  - `enhanced-mcp.js` - Main MCP server\n  - `tools/` - Tool implementations\n    - `web-tools.js` - Web scraping and search tools\n    - `code-tools.js` - Code analysis tools\n    - `file-tools.js` - File operation tools\n    - `ai-tools.js` - LLM and image analysis tools\n- `tools/` - Helper scripts\n  - `mcp_installer.py` - Installation utility\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "supabase",
        "supabase database",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "gitskyflux--firestore-mcp": {
      "owner": "gitskyflux",
      "name": "firestore-mcp",
      "url": "https://github.com/gitskyflux/firestore-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/gitskyflux.webp",
      "description": "Interact with Google Firestore to manage documents efficiently. Supports creating, reading, updating, deleting, and querying documents with configurable filters and ordering.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-27T14:54:10Z",
      "readme_content": "# Firestore MCP Server\n\nAn MCP (Model Context Protocol) server for interacting with Google Firestore directly. This server provides a clean interface for creating, reading, updating, and deleting Firestore documents through Claude Desktop.\n\n## Features\n\n- Create documents in Firestore collections\n- Read documents from Firestore collections\n- Update existing documents\n- Delete documents\n- Query documents with filtering, ordering, and limits\n- List available collections\n\n## Setup\n\n1. **Install dependencies**:\n   ```\n   npm install\n   ```\n\n2. **Build the project**:\n   ```\n   npm run build\n   ```\n\n3. **Configure Claude Desktop**:\n   Add the following to your `claude_desktop_config.json`:\n\n   ```json\n   \"firestore-mcp\": {\n     \"command\": \"node\",\n     \"args\": [\n       \"/path/to/firestore-mcp/build/index.js\"\n     ],\n     \"env\": {\n       \"GOOGLE_CLOUD_PROJECTS\": \"project-id\"\n     }\n   }\n   ```\n\n   Replace the path in args with the actual path to index.js.\n\n   Define a comma-separated list of project ids in GOOGLE_CLOUD_PROJECTS.\n   Example: `google-project-id1,google-project-id2`\n   The first listed project is the default.\n\n   The application expects to find .json credential file(s) in the keys folder for each project.\n   Example: keys/google-project-id1.json, keys/google-project-id2.json\n   Ensure the cloud service account has appropriate permission to interact with Cloud Firestore, e.g. `Cloud Datastore Owner` or lesser permission(s).\n\n## Available Tools\n\n- **getDocument**: Get a document by ID from a collection\n- **createDocument**: Create a new document in a collection\n- **updateDocument**: Update an existing document\n- **deleteDocument**: Delete a document\n- **queryDocuments**: Query documents with filters, ordering, and limits\n- **listCollections**: List all available collections\n\n## Example Usage in Claude Desktop\n\nHere are examples of how to use each tool in Claude Desktop:\n\n### Get a Document\n\n```\nGet the document with ID \"user123\" from the \"users\" collection\n```\n\n### Create a Document\n\n```\nCreate a new document in the \"users\" collection with the following data:\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"age\": 30\n}\n```\n\n### Update a Document\n\n```\nUpdate the document with ID \"user123\" in the \"users\" collection to change the age to 31\n```\n\n### Delete a Document\n\n```\nDelete the document with ID \"user123\" from the \"users\" collection\n```\n\n### Query Documents\n\n```\nFind all users over 25 years old, ordered by name\n```\n\n### List Collections\n\n```\nList all available Firestore collections\n```\n\n## Development\n\n- **Watch mode**: `npm run dev`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firestore",
        "databases",
        "database",
        "firestore manage",
        "google firestore",
        "firestore mcp"
      ],
      "category": "databases"
    },
    "gldc--mcp-postgres": {
      "owner": "gldc",
      "name": "mcp-postgres",
      "url": "https://github.com/gldc/mcp-postgres",
      "imageUrl": "/freedevtools/mcp/pfp/gldc.webp",
      "description": "Interact with PostgreSQL databases through a standardized interface, enabling the execution of SQL queries, schema listing, and table structure descriptions. Provides capabilities for detailing table constraints and foreign key information to effectively leverage database data.",
      "stars": 15,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T12:34:40Z",
      "readme_content": "# PostgreSQL MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@gldc/mcp-postgres)](https://smithery.ai/server/@gldc/mcp-postgres)\n\n<a href=\"https://glama.ai/mcp/servers/@gldc/mcp-postgres\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@gldc/mcp-postgres/badge\" />\n</a>\n\nA PostgreSQL MCP server implementation using the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) Python SDK- an open protocol that enables seamless integration between LLM applications and external data sources. This server allows AI agents to interact with PostgreSQL databases through a standardized interface.\n\n## Features\n\n- List database schemas\n- List tables within schemas\n- Describe table structures\n- List table constraints and relationships\n- Get foreign key information\n- Execute SQL queries\n- Typed tools with JSON/markdown output\n- Optional table resources and guidance prompts\n\n## Quick Start\n\n```bash\n# Run the server without a DB connection (useful for Glama or inspection)\npython postgres_server.py\n\n# With a live database – pick one method:\nexport POSTGRES_CONNECTION_STRING=\"postgresql://user:pass@host:5432/db\"\npython postgres_server.py\n\n# …or…\npython postgres_server.py --conn \"postgresql://user:pass@host:5432/db\"\n\n# Or using Docker (build once, then run):\n# docker build -t mcp-postgres . && docker run -p 8000:8000 mcp-postgres\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install PostgreSQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gldc/mcp-postgres):\n\n```bash\nnpx -y @smithery/cli install @gldc/mcp-postgres --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n```bash\ngit clone <repository-url>\ncd mcp-postgres\n```\n\n2. Create and activate a virtual environment (recommended):\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows, use: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n1. Start the MCP server.\n\n   ```bash\n   # Without a connection string (server starts, DB‑backed tools will return a friendly error)\n   python postgres_server.py\n\n   # Or set the connection string via environment variable:\n   export POSTGRES_CONNECTION_STRING=\"postgresql://username:password@host:port/database\"\n   python postgres_server.py\n\n   # Or pass it using the --conn flag:\n   python postgres_server.py --conn \"postgresql://username:password@host:port/database\"\n\n   # Optional: Run over HTTP transports\n   # Streamable HTTP (recommended for streaming tool outputs)\n   python postgres_server.py --transport streamable-http --host 0.0.0.0 --port 8000\n\n   # SSE transport (server-sent events) mounted at /sse and /messages/\n   python postgres_server.py --transport sse --host 0.0.0.0 --port 8000 --mount /mcp\n   ```\n2. The server provides the following tools:\n\n- `query`: Execute SQL queries against the database\n- `list_schemas`: List all available schemas\n- `list_tables`: List all tables in a specific schema\n- `describe_table`: Get detailed information about a table's structure\n- `get_foreign_keys`: Get foreign key relationships for a table\n- `find_relationships`: Discover both explicit and implied relationships for a table\n- `db_identity`: Show current db/user/host/port, search_path, and version\n\nTyped (preferred):\n- `run_query(input)`: Execute with typed input (`sql`, `parameters`, `row_limit`, `format: 'markdown'|'json'`).\n- `run_query_json(input)`: Execute and return JSON-serializable rows.\n- `list_schemas_json(input)`: List schemas with filters (`include_system`, `include_temp`, `require_usage`, `row_limit`).\n- `list_schemas_json_page(input)`: Paginated listing with filters and `name_like` pattern.\n- `list_tables_json(input)`: List tables within a schema with filters (name pattern, case sensitivity, table_types, row_limit).\n- `list_tables_json_page(input)`: Paginated tables listing with filters.\n\nExamples:\n\n```json\n// run_query (markdown)\n{\n  \"sql\": \"SELECT * FROM information_schema.tables WHERE table_schema = %s\",\n  \"parameters\": [\"public\"],\n  \"row_limit\": 50,\n  \"format\": \"markdown\"\n}\n\n// run_query_json\n{\n  \"sql\": \"SELECT now() as ts\",\n  \"row_limit\": 1\n}\n```\n\nInspect current connection identity:\n\n```json\n// db_identity (no input)\n{}\n```\n\nList schemas (JSON) with filters:\n\n```json\n{\n  \"include_system\": false,\n  \"include_temp\": false,\n  \"require_usage\": true,\n  \"row_limit\": 10000\n}\n```\n\nPaginated list with pattern filter:\n\n```json\n{\n  \"include_system\": false,\n  \"include_temp\": false,\n  \"require_usage\": true,\n  \"page_size\": 200,\n  \"cursor\": null,\n  \"name_like\": \"sales_*\",\n  \"case_sensitive\": false\n}\n```\n\nResponse shape:\n\n```json\n{\n  \"items\": [ { \"schema_name\": \"sales_eu\", \"owner\": \"...\", \"is_system\": false, \"is_temporary\": false, \"has_usage\": true } ],\n  \"next_cursor\": \"...base64...\" // null when no more pages\n}\n```\n\nList tables with filters (JSON):\n\n```json\n{\n  \"db_schema\": \"public\",\n  \"name_like\": \"orders_*\",\n  \"case_sensitive\": false,\n  \"table_types\": [\"BASE TABLE\", \"VIEW\"],\n  \"row_limit\": 1000\n}\n```\n\nPaginated tables listing:\n\n```json\n{\n  \"db_schema\": \"public\",\n  \"page_size\": 200,\n  \"cursor\": null,\n  \"name_like\": \"orders_%\"\n}\n```\n\nResources (if supported by client):\n- `table://{schema}/{table}` for reading table rows. Fallback tools are available:\n  - `list_table_resources(schema)` → `table://...` URIs\n  - `read_table_resource(schema, table, row_limit)` → rows JSON\n\nPrompts (registered when supported; also exposed as tools):\n- `write_safe_select` / `prompt_write_safe_select_tool`\n- `explain_plan_tips` / `prompt_explain_plan_tips_tool`\n\n### Running with Docker\n\nBuild the image:\n\n```bash\ndocker build -t mcp-postgres .\n```\n\nRun the container without a database connection (the server stays inspectable):\n\n```bash\ndocker run -p 8000:8000 mcp-postgres\n```\n\nRun with a live PostgreSQL database by supplying `POSTGRES_CONNECTION_STRING`:\n\n```bash\ndocker run \\\n  -e POSTGRES_CONNECTION_STRING=\"postgresql://username:password@host:5432/database\" \\\n  -p 8000:8000 \\\n  mcp-postgres\n```\n\n*If the environment variable is omitted, the server boots normally and all database‑backed tools return a friendly “connection string is not set” message until you provide it.*\n\n### Configuration with mcp.json\n\nTo integrate this server with MCP-compatible tools (like Cursor), add it to your `~/.cursor/mcp.json`:\n\n```json\n{\n  \"servers\": {\n    \"postgres\": {\n      \"command\": \"/path/to/venv/bin/python\",\n      \"args\": [\n        \"/path/to/postgres_server.py\"\n      ],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"postgresql://username:password@host:5432/database?ssl=true\"\n      }\n    }\n  }\n}\n```\n\n### Transport Environment Variables\n- `MCP_TRANSPORT=stdio|sse|streamable-http` (default: `stdio`)\n- `MCP_HOST=0.0.0.0` and `MCP_PORT=8000` for SSE/HTTP transports\n- `MCP_SSE_MOUNT=/mcp` optional SSE mount path\n\n*If `POSTGRES_CONNECTION_STRING` is omitted, the server still starts and is fully inspectable; database‑backed tools will simply return an informative error until the variable is provided.*\n\nReplace:\n- `/path/to/venv` with your virtual environment path\n- `/path/to/postgres_server.py` with the absolute path to the server script\n\n### HTTP Client Integration\n\nRun the server with Streamable HTTP:\n\n```bash\npython postgres_server.py --transport streamable-http --host 0.0.0.0 --port 8000\n# or with Docker\ndocker run -p 8000:8000 mcp-postgres \\\n  python postgres_server.py --transport streamable-http --host 0.0.0.0 --port 8000\n```\n\nBasic reachability check (expect non-200 since MCP expects a handshake):\n\n```bash\ncurl -i http://localhost:8000/mcp\n# A 404/405/422 indicates the server is reachable; clients must speak MCP.\n```\n\nExample MCP client config (conceptual) pointing at the Streamable HTTP endpoint:\n\n```json\n{\n  \"servers\": {\n    \"postgres\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\nFor SSE instead of Streamable HTTP:\n\n```bash\npython postgres_server.py --transport sse --host 0.0.0.0 --port 8000 --mount /mcp\ncurl -N http://localhost:8000/sse  # Connects to the SSE endpoint\n```\n\n#### Python MCP Client Example (Streamable HTTP)\n\n```python\nimport asyncio\nfrom mcp.client import streamable_http\nfrom mcp.client.session import ClientSession\n\n\nasync def main():\n    url = \"http://localhost:8000/mcp\"\n    async with streamable_http.streamablehttp_client(url) as (read, write, _get_session_id):\n        session = ClientSession(read, write)\n        init = await session.initialize()\n        print(\"protocol:\", init.protocolVersion)\n\n        # List tools\n        tools = await session.list_tools()\n        print(\"tools:\", [t.name for t in tools.tools])\n\n        # Call typed tool: run_query_json\n        result = await session.call_tool(\n            \"run_query_json\",\n            {\"input\": {\"sql\": \"SELECT 1 AS n\", \"row_limit\": 1}},\n        )\n        # Prefer structuredContent if provided; fallback to text content\n        if result.structuredContent is not None:\n            print(\"structured:\", result.structuredContent)\n        else:\n            print(\"text blocks:\", [getattr(b, \"text\", None) for b in result.content])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Security\n\n- Never expose sensitive database credentials in your code\n- Use environment variables or secure configuration files for database connection strings\n- Consider using connection pooling for better resource management\n- Implement proper access controls and user authentication\n\n### Environment options\n- `POSTGRES_READONLY=true` to allow only SELECT/CTE/EXPLAIN/SHOW/VALUES\n- `POSTGRES_STATEMENT_TIMEOUT_MS=15000` to cap statement runtime\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n### Development & Tests\n- Create a venv and install runtime deps: `pip install -r requirements.txt`\n- (Optional) install test deps: `pip install -r dev-requirements.txt`\n- Run tests: `pytest -q`\n\n## Related Projects\n\n- [MCP Specification](https://github.com/modelcontextprotocol/specification)\n- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n- [MCP Servers](https://github.com/modelcontextprotocol/servers)\n\n## License\n\nMIT License\n\nCopyright (c) 2025 gldc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "goaltang--homestay3": {
      "owner": "goaltang",
      "name": "homestay3",
      "url": "https://github.com/goaltang/homestay3",
      "imageUrl": "/freedevtools/mcp/pfp/goaltang.webp",
      "description": "A comprehensive solution for managing homestay bookings, including guest check-ins, reservation tracking, and a user-friendly interface for both guests and administrators.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Vue",
      "updated_at": "2025-06-03T05:51:38Z",
      "readme_content": "# 民宿预订平台 (Homestay Booking Platform)\r\n\r\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\r\n![Version](https://img.shields.io/badge/version-1.0.0-green.svg)\r\n\r\n## 项目简介\r\n\r\n民宿预订平台是一个连接房东与旅客的综合性服务系统，致力于为旅客提供多样化的民宿选择，同时为房东提供高效的房源管理工具。系统包含三个主要部分：面向用户的前端界面、面向房东与管理员的后台管理系统，以及支撑整体业务的服务器端 API，构建了从房源发布、在线预订到服务评价的完整业务闭环。\r\n\r\n## 技术栈\r\n\r\n### 前端\r\n\r\n- Vue 3\r\n- Element Plus\r\n- Pinia (状态管理)\r\n- TypeScript\r\n- Vite (构建工具)\r\n- Axios (HTTP 请求)\r\n- ECharts (数据可视化)\r\n\r\n### 后端\r\n\r\n- Spring Boot 2.7\r\n- Java 17\r\n- MySQL 8.0\r\n- Spring Data JPA\r\n- Spring Security + JWT\r\n- Maven\r\n- SendGrid (邮件服务)\r\n- AWS S3 (文件存储)\r\n\r\n## 项目结构\r\n\r\n```\r\nhomestay3/\r\n├── homestay-front/       # 用户前端\r\n├── homestay-admin/       # 房东与管理员前端\r\n└── homestay-backend/     # 后端服务API\r\n```\r\n\r\n## 核心功能模块\r\n\r\n- **用户认证与权限管理**\r\n\r\n  - 基于 JWT 的身份验证与授权\r\n  - 用户注册与邮箱验证\r\n  - 多角色权限控制 (普通用户、房东、管理员)\r\n  - 密码重置与修改\r\n\r\n- **房源管理模块**\r\n\r\n  - 房源创建与编辑\r\n  - 多维度房源信息维护 (基本信息、价格日历、设施配置、地理位置)\r\n  - 房源状态管理 (上架、下架、审核中)\r\n  - 丰富的查询筛选机制 (地点、价格、房型、设施、可用日期等)\r\n  - 房源类型与设施分类管理\r\n\r\n- **订单管理模块**\r\n\r\n  - 完整的订单生命周期管理\r\n  - 多状态订单处理 (待付款、已确认、已入住、已完成、已取消等)\r\n  - 智能订单冲突检测\r\n  - 退款与取消政策实现\r\n  - 订单统计与导出\r\n\r\n- **评价系统模块**\r\n\r\n  - 多维度评分机制\r\n  - 用户评价与房东回复\r\n  - 评价管理与筛选\r\n  - 评价分析与展示\r\n\r\n- **通知系统**\r\n\r\n  - 系统通知与用户消息\r\n  - 实时通知提醒\r\n  - 消息状态管理 (已读/未读)\r\n  - 通知类型分类与筛选\r\n\r\n- **支付集成**\r\n  - 支付处理与订单确认\r\n  - 退款流程处理\r\n  - 交易记录与财务管理\r\n\r\n## 技术特点\r\n\r\n- RESTful API 设计\r\n- 基于 Spring Security 的安全认证\r\n- 数据库事务一致性保障\r\n- 多条件动态查询构建\r\n- 日志记录与异常处理\r\n- 数据分页与高效检索\r\n\r\n## 安装与使用\r\n\r\n### 前端环境要求\r\n\r\n- Node.js 14.18+\r\n- npm 或 yarn\r\n\r\n### 后端环境要求\r\n\r\n- JDK 17+\r\n- Maven 3.6+\r\n- MySQL 8.0+\r\n\r\n### 设置与运行\r\n\r\n**1. 克隆仓库**\r\n\r\n```bash\r\ngit clone https://github.com/yourusername/homestay3.git\r\ncd homestay3\r\n```\r\n\r\n**2. 前端设置**\r\n\r\n```bash\r\n# 用户前端\r\ncd homestay-front\r\nnpm install\r\nnpm run dev\r\n\r\n# 房东管理前端\r\ncd ../homestay-admin\r\nnpm install\r\nnpm run dev\r\n```\r\n\r\n**3. 后端设置**\r\n\r\n```bash\r\ncd ../homestay-backend\r\nmvn clean install\r\nmvn spring-boot:run\r\n```\r\n\r\n**4. 数据库设置**\r\n\r\n- 创建名为`homestay_db`的 MySQL 数据库\r\n- 配置`application.properties`中的数据库连接参数\r\n\r\n## 开发指南\r\n\r\n### 代码规范\r\n\r\n- 前端遵循 Vue 3 官方风格指南\r\n- 后端遵循 Java 代码规范和 RESTful API 设计原则\r\n\r\n### 分支管理\r\n\r\n- `main`: 稳定版本\r\n- `develop`: 开发版本\r\n- 功能分支: `feature/功能名称`\r\n- 修复分支: `bugfix/问题描述`\r\n\r\n## 贡献指南\r\n\r\n1. Fork 项目\r\n2. 创建功能分支 (`git checkout -b feature/amazing-feature`)\r\n3. 提交更改 (`git commit -m 'Add some amazing feature'`)\r\n4. 推送到分支 (`git push origin feature/amazing-feature`)\r\n5. 创建 Pull Request\r\n\r\n## 许可证\r\n\r\n本项目采用 MIT 许可证 - 详情请参阅[LICENSE](LICENSE)文件\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "homestay",
        "secure database",
        "databases secure",
        "homestay3 comprehensive"
      ],
      "category": "databases"
    },
    "googleapis--genai-toolbox": {
      "owner": "googleapis",
      "name": "genai-toolbox",
      "url": "https://github.com/googleapis/genai-toolbox",
      "imageUrl": "/freedevtools/mcp/pfp/googleapis.webp",
      "description": "Facilitates the development of AI tools that interact with databases, handling complexities like connection pooling and authentication. Includes observability features to enhance performance and security during data access.",
      "stars": 10795,
      "forks": 885,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-04T07:13:07Z",
      "readme_content": "# MCP Toolbox for Databases\n\n[![Docs](https://img.shields.io/badge/docs-MCP_Toolbox-blue)](https://googleapis.github.io/genai-toolbox/)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=flat&logo=discord&logoColor=white)](https://discord.gg/Dmm69peqjh)\n[![Medium](https://img.shields.io/badge/Medium-12100E?style=flat&logo=medium&logoColor=white)](https://medium.com/@mcp_toolbox)\n[![Go Report Card](https://goreportcard.com/badge/github.com/googleapis/genai-toolbox)](https://goreportcard.com/report/github.com/googleapis/genai-toolbox)\n\n> [!NOTE]\n> MCP Toolbox for Databases is currently in beta, and may see breaking\n> changes until the first stable release (v1.0).\n\nMCP Toolbox for Databases is an open source MCP server for databases. It enables\nyou to develop tools easier, faster, and more securely by handling the complexities\nsuch as connection pooling, authentication, and more.\n\nThis README provides a brief overview. For comprehensive details, see the [full\ndocumentation](https://googleapis.github.io/genai-toolbox/).\n\n> [!NOTE]\n> This solution was originally named “Gen AI Toolbox for Databases” as\n> its initial development predated MCP, but was renamed to align with recently\n> added MCP compatibility.\n\n<!-- TOC ignore:true -->\n## Table of Contents\n\n<!-- TOC -->\n\n- [Why Toolbox?](#why-toolbox)\n- [General Architecture](#general-architecture)\n- [Getting Started](#getting-started)\n  - [Installing the server](#installing-the-server)\n  - [Running the server](#running-the-server)\n  - [Integrating your application](#integrating-your-application)\n- [Configuration](#configuration)\n  - [Sources](#sources)\n  - [Tools](#tools)\n  - [Toolsets](#toolsets)\n- [Versioning](#versioning)\n  - [Pre-1.0.0 Versioning](#pre-100-versioning)\n  - [Post-1.0.0 Versioning](#post-100-versioning)\n- [Contributing](#contributing)\n- [Community](#community)\n\n<!-- /TOC -->\n\n## Why Toolbox?\n\nToolbox helps you build Gen AI tools that let your agents access data in your\ndatabase. Toolbox provides:\n\n- **Simplified development**: Integrate tools to your agent in less than 10\n  lines of code, reuse tools between multiple agents or frameworks, and deploy\n  new versions of tools more easily.\n- **Better performance**: Best practices such as connection pooling,\n  authentication, and more.\n- **Enhanced security**: Integrated auth for more secure access to your data\n- **End-to-end observability**: Out of the box metrics and tracing with built-in\n  support for OpenTelemetry.\n\n**⚡ Supercharge Your Workflow with an AI Database Assistant ⚡**\n\nStop context-switching and let your AI assistant become a true co-developer. By\n[connecting your IDE to your databases with MCP Toolbox][connect-ide], you can\ndelegate complex and time-consuming database tasks, allowing you to build faster\nand focus on what matters. This isn't just about code completion; it's about\ngiving your AI the context it needs to handle the entire development lifecycle.\n\nHere’s how it will save you time:\n\n- **Query in Plain English**: Interact with your data using natural language\n  right from your IDE. Ask complex questions like, *\"How many orders were\n  delivered in 2024, and what items were in them?\"* without writing any SQL.\n- **Automate Database Management**: Simply describe your data needs, and let the\n  AI assistant manage your database for you. It can handle generating queries,\n  creating tables, adding indexes, and more.\n- **Generate Context-Aware Code**: Empower your AI assistant to generate\n  application code and tests with a deep understanding of your real-time\n  database schema.  This accelerates the development cycle by ensuring the\n  generated code is directly usable.\n- **Slash Development Overhead**: Radically reduce the time spent on manual\n  setup and boilerplate. MCP Toolbox helps streamline lengthy database\n  configurations, repetitive code, and error-prone schema migrations.\n\nLearn [how to connect your AI tools (IDEs) to Toolbox using MCP][connect-ide].\n\n[connect-ide]: https://googleapis.github.io/genai-toolbox/how-to/connect-ide/\n\n## General Architecture\n\nToolbox sits between your application's orchestration framework and your\ndatabase, providing a control plane that is used to modify, distribute, or\ninvoke tools. It simplifies the management of your tools by providing you with a\ncentralized location to store and update tools, allowing you to share tools\nbetween agents and applications and update those tools without necessarily\nredeploying your application.\n\n\n\n## Getting Started\n\n### Installing the server\n\nFor the latest version, check the [releases page][releases] and use the\nfollowing instructions for your OS and CPU architecture.\n\n[releases]: https://github.com/googleapis/genai-toolbox/releases\n\n<details open>\n<summary>Binary</summary>\n\nTo install Toolbox as a binary:\n\n<!-- {x-release-please-start-version} -->\n> <details>\n> <summary>Linux (AMD64)</summary>\n>\n> To install Toolbox as a binary on Linux (AMD64):\n> ```sh\n> # see releases page for other versions\n> export VERSION=0.16.0\n> curl -L -o toolbox https://storage.googleapis.com/genai-toolbox/v$VERSION/linux/amd64/toolbox\n> chmod +x toolbox\n> ```\n>\n> </details>\n> <details>\n> <summary>macOS (Apple Silicon)</summary>\n>\n> To install Toolbox as a binary on macOS (Apple Silicon):\n> ```sh\n> # see releases page for other versions\n> export VERSION=0.16.0\n> curl -L -o toolbox https://storage.googleapis.com/genai-toolbox/v$VERSION/darwin/arm64/toolbox\n> chmod +x toolbox\n> ```\n>\n> </details>\n> <details>\n> <summary>macOS (Intel)</summary>\n>\n> To install Toolbox as a binary on macOS (Intel):\n> ```sh\n> # see releases page for other versions\n> export VERSION=0.16.0\n> curl -L -o toolbox https://storage.googleapis.com/genai-toolbox/v$VERSION/darwin/amd64/toolbox\n> chmod +x toolbox\n> ```\n>\n> </details>\n> <details>\n> <summary>Windows (AMD64)</summary>\n>\n> To install Toolbox as a binary on Windows (AMD64):\n> ```powershell\n> # see releases page for other versions\n> $VERSION = \"0.16.0\"\n> Invoke-WebRequest -Uri \"https://storage.googleapis.com/genai-toolbox/v$VERSION/windows/amd64/toolbox.exe\" -OutFile \"toolbox.exe\"\n> ```\n>\n> </details>\n</details>\n\n<details>\n<summary>Container image</summary>\nYou can also install Toolbox as a container:\n\n```sh\n# see releases page for other versions\nexport VERSION=0.16.0\ndocker pull us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:$VERSION\n```\n\n</details>\n\n<details>\n<summary>Homebrew</summary>\n\nTo install Toolbox using Homebrew on macOS or Linux:\n\n```sh\nbrew install mcp-toolbox\n```\n\n</details>\n\n<details>\n<summary>Compile from source</summary>\n\nTo install from source, ensure you have the latest version of\n[Go installed](https://go.dev/doc/install), and then run the following command:\n\n```sh\ngo install github.com/googleapis/genai-toolbox@v0.16.0\n```\n<!-- {x-release-please-end} -->\n\n</details>\n\n<details>\n<summary>Gemini CLI Extensions</summary>\n\nTo install Gemini CLI Extensions for MCP Toolbox, run the following command:\n\n```sh\ngemini extensions install https://github.com/gemini-cli-extensions/mcp-toolbox\n```\n\n</details>\n\n### Running the server\n\n[Configure](#configuration) a `tools.yaml` to define your tools, and then\nexecute `toolbox` to start the server:\n\n<details open>\n<summary>Binary</summary>\n\nTo run Toolbox from binary:\n\n```sh\n./toolbox --tools-file \"tools.yaml\"\n```\n\nⓘ **NOTE:**  \nToolbox enables dynamic reloading by default. To disable, use the\n`--disable-reload` flag.\n\n</details>\n\n<details>\n\n<summary>Container image</summary>\n\nTo run the server after pulling the [container image](#installing-the-server):\n\n```sh\nexport VERSION=0.11.0 # Use the version you pulled\ndocker run -p 5000:5000 \\\n-v $(pwd)/tools.yaml:/app/tools.yaml \\\nus-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:$VERSION \\\n--tools-file \"/app/tools.yaml\"\n```\n\nⓘ **NOTE:**  \nThe `-v` flag mounts your local `tools.yaml` into the container, and `-p` maps\nthe container's port `5000` to your host's port `5000`.\n\n</details>\n\n<details>\n\n<summary>Source</summary>\n\nTo run the server directly from source, navigate to the project root directory\nand run:\n\n```sh\ngo run .\n```\n\nⓘ **NOTE:**  \nThis command runs the project from source, and is more suitable for development\nand testing. It does **not** compile a binary into your `$GOPATH`. If you want\nto compile a binary instead, refer the [Developer\nDocumentation](./DEVELOPER.md#building-the-binary).\n\n</details>\n\n<details>\n\n<summary>Homebrew</summary>\n\nIf you installed Toolbox using [Homebrew](https://brew.sh/), the `toolbox`\nbinary is available in your system path. You can start the server with the same\ncommand:\n\n```sh\ntoolbox --tools-file \"tools.yaml\"\n```\n\n</details>\n\n<details>\n\n<summary>Gemini CLI</summary>\n\nInteract with your custom tools using natural language. Check\n[gemini-cli-extensions/mcp-toolbox](https://github.com/gemini-cli-extensions/mcp-toolbox)\nfor more information.\n\n</details>\n\nYou can use `toolbox help` for a full list of flags! To stop the server, send a\nterminate signal (`ctrl+c` on most platforms).\n\nFor more detailed documentation on deploying to different environments, check\nout the resources in the [How-to\nsection](https://googleapis.github.io/genai-toolbox/how-to/)\n\n### Integrating your application\n\nOnce your server is up and running, you can load the tools into your\napplication. See below the list of Client SDKs for using various frameworks:\n\n<details open>\n  <summary>Python (<a href=\"https://github.com/googleapis/mcp-toolbox-sdk-python\">Github</a>)</summary>\n  <br>\n  <blockquote>\n\n  <details open>\n    <summary>Core</summary>\n\n1. Install [Toolbox Core SDK][toolbox-core]:\n\n    ```bash\n    pip install toolbox-core\n    ```\n\n1. Load tools:\n\n    ```python\n    from toolbox_core import ToolboxClient\n\n    # update the url to point to your server\n    async with ToolboxClient(\"http://127.0.0.1:5000\") as client:\n\n        # these tools can be passed to your application!\n        tools = await client.load_toolset(\"toolset_name\")\n    ```\n\nFor more detailed instructions on using the Toolbox Core SDK, see the\n[project's README][toolbox-core-readme].\n\n[toolbox-core]: https://pypi.org/project/toolbox-core/\n[toolbox-core-readme]: https://github.com/googleapis/mcp-toolbox-sdk-python/tree/main/packages/toolbox-core/README.md\n\n  </details>\n  <details>\n    <summary>LangChain / LangGraph</summary>\n\n1. Install [Toolbox LangChain SDK][toolbox-langchain]:\n\n    ```bash\n    pip install toolbox-langchain\n    ```\n\n1. Load tools:\n\n    ```python\n    from toolbox_langchain import ToolboxClient\n\n    # update the url to point to your server\n    async with ToolboxClient(\"http://127.0.0.1:5000\") as client:\n\n        # these tools can be passed to your application!\n        tools = client.load_toolset()\n    ```\n\n    For more detailed instructions on using the Toolbox LangChain SDK, see the\n    [project's README][toolbox-langchain-readme].\n\n    [toolbox-langchain]: https://pypi.org/project/toolbox-langchain/\n    [toolbox-langchain-readme]: https://github.com/googleapis/mcp-toolbox-sdk-python/blob/main/packages/toolbox-langchain/README.md\n\n  </details>\n  <details>\n    <summary>LlamaIndex</summary>\n\n1. Install [Toolbox Llamaindex SDK][toolbox-llamaindex]:\n\n    ```bash\n    pip install toolbox-llamaindex\n    ```\n\n1. Load tools:\n\n    ```python\n    from toolbox_llamaindex import ToolboxClient\n\n    # update the url to point to your server\n    async with ToolboxClient(\"http://127.0.0.1:5000\") as client:\n\n        # these tools can be passed to your application!\n        tools = client.load_toolset()\n    ```\n\n    For more detailed instructions on using the Toolbox Llamaindex SDK, see the\n    [project's README][toolbox-llamaindex-readme].\n\n    [toolbox-llamaindex]: https://pypi.org/project/toolbox-llamaindex/\n    [toolbox-llamaindex-readme]: https://github.com/googleapis/genai-toolbox-llamaindex-python/blob/main/README.md\n\n  </details>\n</details>\n</blockquote>\n<details>\n  <summary>Javascript/Typescript (<a href=\"https://github.com/googleapis/mcp-toolbox-sdk-js\">Github</a>)</summary>\n  <br>\n  <blockquote>\n\n  <details open>\n    <summary>Core</summary>\n\n1. Install [Toolbox Core SDK][toolbox-core-js]:\n\n    ```bash\n    npm install @toolbox-sdk/core\n    ```\n\n1. Load tools:\n\n    ```javascript\n    import { ToolboxClient } from '@toolbox-sdk/core';\n\n    // update the url to point to your server\n    const URL = 'http://127.0.0.1:5000';\n    let client = new ToolboxClient(URL);\n\n    // these tools can be passed to your application!\n    const tools = await client.loadToolset('toolsetName');\n    ```\n\n    For more detailed instructions on using the Toolbox Core SDK, see the\n    [project's README][toolbox-core-js-readme].\n\n    [toolbox-core-js]: https://www.npmjs.com/package/@toolbox-sdk/core\n    [toolbox-core-js-readme]: https://github.com/googleapis/mcp-toolbox-sdk-js/blob/main/packages/toolbox-core/README.md\n\n  </details>\n  <details>\n    <summary>LangChain / LangGraph</summary>\n\n1. Install [Toolbox Core SDK][toolbox-core-js]:\n\n    ```bash\n    npm install @toolbox-sdk/core\n    ```\n\n2. Load tools:\n\n    ```javascript\n    import { ToolboxClient } from '@toolbox-sdk/core';\n\n    // update the url to point to your server\n    const URL = 'http://127.0.0.1:5000';\n    let client = new ToolboxClient(URL);\n\n    // these tools can be passed to your application!\n    const toolboxTools = await client.loadToolset('toolsetName');\n\n    // Define the basics of the tool: name, description, schema and core logic\n    const getTool = (toolboxTool) => tool(currTool, {\n        name: toolboxTool.getName(),\n        description: toolboxTool.getDescription(),\n        schema: toolboxTool.getParamSchema()\n    });\n\n    // Use these tools in your Langchain/Langraph applications\n    const tools = toolboxTools.map(getTool);\n    ```\n\n  </details>\n  <details>\n    <summary>Genkit</summary>\n\n1. Install [Toolbox Core SDK][toolbox-core-js]:\n\n    ```bash\n    npm install @toolbox-sdk/core\n    ```\n\n2. Load tools:\n\n    ```javascript\n    import { ToolboxClient } from '@toolbox-sdk/core';\n    import { genkit } from 'genkit';\n\n    // Initialise genkit\n    const ai = genkit({\n        plugins: [\n            googleAI({\n                apiKey: process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY\n            })\n        ],\n        model: googleAI.model('gemini-2.0-flash'),\n    });\n\n    // update the url to point to your server\n    const URL = 'http://127.0.0.1:5000';\n    let client = new ToolboxClient(URL);\n\n    // these tools can be passed to your application!\n    const toolboxTools = await client.loadToolset('toolsetName');\n\n    // Define the basics of the tool: name, description, schema and core logic\n    const getTool = (toolboxTool) => ai.defineTool({\n        name: toolboxTool.getName(),\n        description: toolboxTool.getDescription(),\n        schema: toolboxTool.getParamSchema()\n    }, toolboxTool)\n\n    // Use these tools in your Genkit applications\n    const tools = toolboxTools.map(getTool);\n    ```\n\n  </details>\n</details>\n</blockquote>\n<details>\n  <summary>Go (<a href=\"https://github.com/googleapis/mcp-toolbox-sdk-go\">Github</a>)</summary>\n  <br>\n  <blockquote>\n\n  <details open>\n    <summary>Core</summary>\n\n1. Install [Toolbox Go SDK][toolbox-go]:\n\n    ```bash\n    go get github.com/googleapis/mcp-toolbox-sdk-go\n    ```\n\n1. Load tools:\n\n    ```go\n    package main\n\n    import (\n      \"github.com/googleapis/mcp-toolbox-sdk-go/core\"\n      \"context\"\n    )\n\n    func main() {\n      // Make sure to add the error checks\n      // update the url to point to your server\n      URL := \"http://127.0.0.1:5000\";\n      ctx := context.Background()\n\n      client, err := core.NewToolboxClient(URL)\n\n      // Framework agnostic tools\n      tools, err := client.LoadToolset(\"toolsetName\", ctx)\n    }\n    ```\n\n    For more detailed instructions on using the Toolbox Go SDK, see the\n    [project's README][toolbox-core-go-readme].\n\n    [toolbox-go]: https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core\n    [toolbox-core-go-readme]: https://github.com/googleapis/mcp-toolbox-sdk-go/blob/main/core/README.md\n\n  </details>\n  <details>\n    <summary>LangChain Go</summary>\n\n1. Install [Toolbox Go SDK][toolbox-go]:\n\n    ```bash\n    go get github.com/googleapis/mcp-toolbox-sdk-go\n    ```\n\n2. Load tools:\n\n    ```go\n    package main\n\n    import (\n      \"context\"\n      \"encoding/json\"\n\n      \"github.com/googleapis/mcp-toolbox-sdk-go/core\"\n      \"github.com/tmc/langchaingo/llms\"\n    )\n\n    func main() {\n      // Make sure to add the error checks\n      // update the url to point to your server\n      URL := \"http://127.0.0.1:5000\"\n      ctx := context.Background()\n\n      client, err := core.NewToolboxClient(URL)\n\n      // Framework agnostic tool\n      tool, err := client.LoadTool(\"toolName\", ctx)\n\n      // Fetch the tool's input schema\n      inputschema, err := tool.InputSchema()\n\n      var paramsSchema map[string]any\n      _ = json.Unmarshal(inputschema, &paramsSchema)\n\n      // Use this tool with LangChainGo\n      langChainTool := llms.Tool{\n        Type: \"function\",\n        Function: &llms.FunctionDefinition{\n          Name:        tool.Name(),\n          Description: tool.Description(),\n          Parameters:  paramsSchema,\n        },\n      }\n    }\n\n    ```\n\n  </details>\n  <details>\n    <summary>Genkit</summary>\n\n1. Install [Toolbox Go SDK][toolbox-go]:\n\n    ```bash\n    go get github.com/googleapis/mcp-toolbox-sdk-go\n    ```\n\n2. Load tools:\n\n    ```go\n    package main\n    import (\n      \"context\"\n      \"encoding/json\"\n\n      \"github.com/firebase/genkit/go/ai\"\n      \"github.com/firebase/genkit/go/genkit\"\n      \"github.com/googleapis/mcp-toolbox-sdk-go/core\"\n      \"github.com/googleapis/mcp-toolbox-sdk-go/tbgenkit\"\n      \"github.com/invopop/jsonschema\"\n    )\n\n    func main() {\n      // Make sure to add the error checks\n      // Update the url to point to your server\n      URL := \"http://127.0.0.1:5000\"\n      ctx := context.Background()\n      g, err := genkit.Init(ctx)\n\n      client, err := core.NewToolboxClient(URL)\n\n      // Framework agnostic tool\n      tool, err := client.LoadTool(\"toolName\", ctx)\n\n      // Convert the tool using the tbgenkit package\n      // Use this tool with Genkit Go\n      genkitTool, err := tbgenkit.ToGenkitTool(tool, g)\n      if err != nil {\n        log.Fatalf(\"Failed to convert tool: %v\\n\", err)\n      }\n    }\n    ```\n\n  </details>\n  <details>\n    <summary>Go GenAI</summary>\n\n1. Install [Toolbox Go SDK][toolbox-go]:\n\n    ```bash\n    go get github.com/googleapis/mcp-toolbox-sdk-go\n    ```\n\n2. Load tools:\n\n    ```go\n    package main\n\n    import (\n      \"context\"\n      \"encoding/json\"\n\n      \"github.com/googleapis/mcp-toolbox-sdk-go/core\"\n      \"google.golang.org/genai\"\n    )\n\n    func main() {\n      // Make sure to add the error checks\n      // Update the url to point to your server\n      URL := \"http://127.0.0.1:5000\"\n      ctx := context.Background()\n\n      client, err := core.NewToolboxClient(URL)\n\n      // Framework agnostic tool\n      tool, err := client.LoadTool(\"toolName\", ctx)\n\n      // Fetch the tool's input schema\n      inputschema, err := tool.InputSchema()\n\n      var schema *genai.Schema\n      _ = json.Unmarshal(inputschema, &schema)\n\n      funcDeclaration := &genai.FunctionDeclaration{\n        Name:        tool.Name(),\n        Description: tool.Description(),\n        Parameters:  schema,\n      }\n\n      // Use this tool with Go GenAI\n      genAITool := &genai.Tool{\n        FunctionDeclarations: []*genai.FunctionDeclaration{funcDeclaration},\n      }\n    }\n    ```\n\n  </details>\n  <details>\n    <summary>OpenAI Go</summary>\n\n1. Install [Toolbox Go SDK][toolbox-go]:\n\n    ```bash\n    go get github.com/googleapis/mcp-toolbox-sdk-go\n    ```\n\n2. Load tools:\n\n    ```go\n    package main\n\n    import (\n      \"context\"\n      \"encoding/json\"\n\n      \"github.com/googleapis/mcp-toolbox-sdk-go/core\"\n      openai \"github.com/openai/openai-go\"\n    )\n\n    func main() {\n      // Make sure to add the error checks\n      // Update the url to point to your server\n      URL := \"http://127.0.0.1:5000\"\n      ctx := context.Background()\n\n      client, err := core.NewToolboxClient(URL)\n\n      // Framework agnostic tool\n      tool, err := client.LoadTool(\"toolName\", ctx)\n\n      // Fetch the tool's input schema\n      inputschema, err := tool.InputSchema()\n\n      var paramsSchema openai.FunctionParameters\n      _ = json.Unmarshal(inputschema, &paramsSchema)\n\n      // Use this tool with OpenAI Go\n      openAITool := openai.ChatCompletionToolParam{\n        Function: openai.FunctionDefinitionParam{\n          Name:        tool.Name(),\n          Description: openai.String(tool.Description()),\n          Parameters:  paramsSchema,\n        },\n      }\n\n    }\n    ```\n\n  </details>\n</details>\n</blockquote>\n</details>\n\n## Configuration\n\nThe primary way to configure Toolbox is through the `tools.yaml` file. If you\nhave multiple files, you can tell toolbox which to load with the `--tools-file\ntools.yaml` flag.\n\nYou can find more detailed reference documentation to all resource types in the\n[Resources](https://googleapis.github.io/genai-toolbox/resources/).\n\n### Sources\n\nThe `sources` section of your `tools.yaml` defines what data sources your\nToolbox should have access to. Most tools will have at least one source to\nexecute against.\n\n```yaml\nsources:\n  my-pg-source:\n    kind: postgres\n    host: 127.0.0.1\n    port: 5432\n    database: toolbox_db\n    user: toolbox_user\n    password: my-password\n```\n\nFor more details on configuring different types of sources, see the\n[Sources](https://googleapis.github.io/genai-toolbox/resources/sources).\n\n### Tools\n\nThe `tools` section of a `tools.yaml` define the actions an agent can take: what\nkind of tool it is, which source(s) it affects, what parameters it uses, etc.\n\n```yaml\ntools:\n  search-hotels-by-name:\n    kind: postgres-sql\n    source: my-pg-source\n    description: Search for hotels based on name.\n    parameters:\n      - name: name\n        type: string\n        description: The name of the hotel.\n    statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%';\n```\n\nFor more details on configuring different types of tools, see the\n[Tools](https://googleapis.github.io/genai-toolbox/resources/tools).\n\n### Toolsets\n\nThe `toolsets` section of your `tools.yaml` allows you to define groups of tools\nthat you want to be able to load together. This can be useful for defining\ndifferent groups based on agent or application.\n\n```yaml\ntoolsets:\n    my_first_toolset:\n        - my_first_tool\n        - my_second_tool\n    my_second_toolset:\n        - my_second_tool\n        - my_third_tool\n```\n\nYou can load toolsets by name:\n\n```python\n# This will load all tools\nall_tools = client.load_toolset()\n\n# This will only load the tools listed in 'my_second_toolset'\nmy_second_toolset = client.load_toolset(\"my_second_toolset\")\n```\n\n## Versioning\n\nThis project uses [semantic versioning](https://semver.org/) (`MAJOR.MINOR.PATCH`).\nSince the project is in a pre-release stage (version `0.x.y`), we follow the\nstandard conventions for initial  development:\n\n### Pre-1.0.0 Versioning\n\nWhile the major version is `0`, the public API should be considered unstable.\nThe version will be incremented  as follows:\n\n- **`0.MINOR.PATCH`**: The **MINOR** version is incremented when we add\n  new functionality or make breaking, incompatible API changes.\n- **`0.MINOR.PATCH`**: The **PATCH** version is incremented for\n  backward-compatible bug fixes.\n\n### Post-1.0.0 Versioning\n\nOnce the project reaches a stable `1.0.0` release, the versioning will follow\nthe more common convention:\n\n- **`MAJOR.MINOR.PATCH`**: Incremented for incompatible API changes.\n- **`MAJOR.MINOR.PATCH`**: Incremented for new, backward-compatible functionality.\n- **`MAJOR.MINOR.PATCH`**: Incremented for backward-compatible bug fixes.\n\nThe public API that this applies to is the CLI associated with Toolbox, the\ninteractions with official SDKs, and the definitions in the `tools.yaml` file.\n\n## Contributing\n\nContributions are welcome. Please, see the [CONTRIBUTING](CONTRIBUTING.md)\nto get started.\n\nPlease note that this project is released with a Contributor Code of Conduct.\nBy participating in this project you agree to abide by its terms. See\n[Contributor Code of Conduct](CODE_OF_CONDUCT.md) for more information.\n\n## Community\n\nJoin our [discord community](https://discord.gg/GQrFB3Ec3W) to connect with our developers!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "googleapis",
        "databases",
        "database",
        "googleapis genai",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "gotalab--bigquery-analysis-mcp-server": {
      "owner": "gotalab",
      "name": "bigquery-analysis-mcp-server",
      "url": "https://github.com/gotalab/bigquery-analysis-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gotalab.webp",
      "description": "Execute SQL queries against Google BigQuery with validation, ensuring that queries are safe, valid, and constrained to specific processing limits. Retrieve structured JSON results for analytics needs without modifying data.",
      "stars": 2,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-17T02:47:20Z",
      "readme_content": "# BigQuery Analysis MCP Server\n\n## Overview\nThis server is an MCP server for executing SQL queries against Google BigQuery, providing the following features:\n\n- Query validation (dry run): Verifies if a query is valid and estimates its processing size\n- Safe query execution: Only runs SELECT queries under 1TB (prevents data modifications)\n- JSON-formatted results: Returns query results in structured JSON format\n\n## Features\n\n### Tools\n- `dry_run_query` - Perform a dry run of a BigQuery query\n  - Validates the query and estimates its processing size\n  - Checks query size against the 1TB limit\n\n- `run_query_with_validation` - Run a BigQuery query with validation\n  - Detects and rejects DML statements (data modification queries)\n  - Rejects data processing over 1TB\n  - Executes queries that pass validation and returns results\n\n## Development\n\n### Prerequisites\n- Node.js (v16 or higher)\n- Google Cloud authentication setup (gcloud CLI or service account)\n\n### Install Dependencies\n```bash\nnpm install\n```\n\n### Build\n```bash\nnpm run build\n```\n\n### Development Mode (Auto-rebuild)\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server configuration:\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"bigquery-analysis-server\": {\n      \"command\": \"/path/to/bigquery-analysis-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Authentication Setup\n\nThis server uses Google Cloud authentication. Set up authentication using one of the following methods:\n\n1. Login with gcloud CLI:\n   ```bash\n   gcloud auth application-default login\n   ```\n\n2. Use a service account key:\n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n   ```\n\n## Usage Examples\n\n1. Dry run a query:\n   ```\n   dry_run_query(\"SELECT * FROM `bigquery-public-data.samples.shakespeare` LIMIT 10\")\n   ```\n\n2. Run a query with validation:\n   ```\n   run_query_with_validation(\"SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus='hamlet' LIMIT 10\")\n   ```\n\n---\n\n# BigQuery Analysis MCP Server (日本語版)\n\n## 概要\nBigQueryでSQLクエリを実行するためのMCPサーバーです。クエリの検証（ドライラン）と実行を行い、1TB以上のデータ処理や変更系クエリ（DML）を防止する安全機能を備えています。\n\n## 機能\nこのサーバーはGoogle BigQueryに対してSQLクエリを実行するためのMCPサーバーで、以下の機能を提供します：\n\n- クエリの検証（ドライラン）：クエリが有効かどうかを確認し、処理サイズを見積もる\n- 安全なクエリ実行：1TB以下のSELECTクエリのみを実行（データ変更を防止）\n- 結果のJSON形式での返却：クエリ結果を構造化されたJSONで返す\n\n## 機能\n\n### ツール\n- `dry_run_query` - BigQueryクエリのドライラン実行\n  - クエリの検証と処理サイズの見積もりを行う\n  - 1TBの制限に対してクエリサイズをチェック\n\n- `run_query_with_validation` - 検証付きでBigQueryクエリを実行\n  - DML文（データ変更クエリ）を検出して拒否\n  - 1TB以上のデータ処理を拒否\n  - 検証に通過したクエリを実行し結果を返す\n\n## 開発方法\n\n### 前提条件\n- Node.js（v16以上）\n- Google Cloud認証設定（gcloud CLIまたはサービスアカウント）\n\n### 依存関係のインストール\n```bash\nnpm install\n```\n\n### ビルド\n```bash\nnpm run build\n```\n\n### 開発モード（自動再ビルド）\n```bash\nnpm run watch\n```\n\n## インストール\n\nClaude Desktopで使用するには、サーバー設定を追加してください：\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"bigquery\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/bigquery-server/build/index.js\"]\n    }\n  }\n}\n```\n\n### デバッグ\n\nMCPサーバーは標準入出力（stdio）を介して通信するため、デバッグが難しい場合があります。[MCP Inspector](https://github.com/modelcontextprotocol/inspector)の使用をお勧めします：\n\n```bash\nnpm run inspector\n```\n\nInspectorはブラウザでデバッグツールにアクセスするためのURLを提供します。\n\n## 認証設定\n\nこのサーバーはGoogle Cloud認証情報を使用します。以下のいずれかの方法で認証を設定してください：\n\n1. gcloud CLIでログイン：\n   ```bash\n   gcloud auth application-default login\n   ```\n\n2. サービスアカウントキーを使用：\n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n   ```\n\n## 使用例\n\n1. クエリのドライラン：\n   ```\n   dry_run_query(\"SELECT * FROM `bigquery-public-data.samples.shakespeare` LIMIT 10\")\n   ```\n\n2. 検証付きクエリ実行：\n   ```\n   run_query_with_validation(\"SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus='hamlet' LIMIT 10\")\n   ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "bigquery",
        "database",
        "google bigquery",
        "bigquery analysis",
        "enables querying"
      ],
      "category": "databases"
    },
    "guoling2008--go-mcp-postgres": {
      "owner": "guoling2008",
      "name": "go-mcp-postgres",
      "url": "https://github.com/guoling2008/go-mcp-postgres",
      "imageUrl": "/freedevtools/mcp/pfp/guoling2008.webp",
      "description": "Perform CRUD operations on Postgres databases with a focus on automation and safety. Features include a read-only mode and query plan checks using the `EXPLAIN` statement before execution.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-08-27T02:09:06Z",
      "readme_content": "# go-mcp-postgres\r\n\r\n## Overview\r\n\r\nCopy code from https://github.com/Zhwt/go-mcp-mysql/ and with AI help, I change db from mysql to postgres.\r\nZero burden, ready-to-use Model Context Protocol (MCP) server for interacting with Postgres and automation. No Node.js or Python environment needed. This server provides tools to do CRUD operations on MySQL databases and tables, and a read-only mode to prevent surprise write operations. You can also make the MCP server check the query plan by using a `EXPLAIN` statement before executing the query by adding a `--with-explain-check` flag.\r\n\r\nPlease note that this is a work in progress and may not yet be ready for production use.\r\n\r\n## Installation\r\n\r\n1. Get the latest [release](https://github.com/guoling2008/go-mcp-postgres/releases) and put it in your `$PATH` or somewhere you can easily access.\r\n\r\n2. Or if you have Go installed, you can build it from source:\r\n\r\n```sh\r\ngo install -v github.com/guoling2008/go-mcp-postgres@latest\r\n```\r\n\r\n## Usage\r\n\r\n### Method A: Using Command Line Arguments for stdio mode\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"postgres\": {\r\n      \"command\": \"go-mcp-postgres\",\r\n      \"args\": [\r\n        \"--dsn\",\r\n        \"postgresql://user:pass@host:port/db\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n\r\nNote: For those who put the binary outside of your `$PATH`, you need to replace `go-mcp-postgres` with the full path to the binary: e.g.: if you put the binary in the **Downloads** folder, you may use the following path:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"postgres\": {\r\n      \"command\": \"C:\\\\Users\\\\<username>\\\\Downloads\\\\go-mcp-postgres.exe\",\r\n      \"args\": [\r\n        ...\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Method B: Using Command Line Arguments for sse mode\r\n\r\n./go-mcp-postgres --t sse --ip x.x.x.x --port nnnn --dsn postgresql://user:pass@host:port/db --lang en\r\n\r\n### Optional Flags\r\n\r\n- `--lang`: Set language option (en/zh-CN), defaults to system language\r\n- Add a `--read-only` flag to enable read-only mode. In this mode, only tools beginning with `list`, `read_` and `desc_` are available. Make sure to refresh/restart the MCP server after adding this flag.\r\n- By default, CRUD queries will be first executed with a `EXPLAIN ?` statement to check whether the generated query plan matches the expected pattern. Add a `--with-explain-check` flag to disable this behavior.\r\n\r\n## Tools\r\n\r\n_Multi-language support: All tool descriptions will automatically localize based on lang parameter_\r\n\r\nIf you want to add your own language support, please refer to the [locales](for i18n) folder.\r\nThe new locales/xxx/active-xx.toml file should be created if you want to use it in command line.\r\n\r\n### Schema Tools\r\n\r\n1. `list_database`\r\n\r\n    - ${mcp.tool.list_database.desc}\r\n    - Parameters: None\r\n    - Returns: A list of matching database names.\r\n\r\n2. `list_table`\r\n\r\n    - ${mcp.tool.list_table.desc}\r\n    - Parameters:\r\n        - `name`: If provided, list tables with the specified name, Otherwise, list all tables.\r\n    - Returns: A list of matching table names.\r\n\r\n3. `create_table`\r\n\r\n    - ${mcp.tool.create_table.desc}\r\n    - Parameters:\r\n        - `query`: The SQL query to create the table.\r\n    - Returns: x rows affected.\r\n\r\n4. `alter_table`\r\n\r\n    - Alter an existing table in the Postgres server. The LLM is informed not to drop an existing table or column.\r\n    - Parameters:\r\n        - `query`: The SQL query to alter the table.\r\n    - Returns: x rows affected.\r\n\r\n5. `desc_table`\r\n\r\n    - Describe the structure of a table.\r\n    - Parameters:\r\n        - `name`: The name of the table to describe.\r\n    - Returns: The structure of the table.\r\n  \r\n### Data Tools\r\n\r\n1. `read_query`\r\n\r\n    - Execute a read-only SQL query.\r\n    - Parameters:\r\n        - `query`: The SQL query to execute.\r\n    - Returns: The result of the query.\r\n\r\n2. `write_query`\r\n\r\n    - Execute a write SQL query.\r\n    - Parameters:\r\n        - `query`: The SQL query to execute.\r\n    - Returns: x rows affected, last insert id: <last_insert_id>.\r\n\r\n3. `update_query`\r\n\r\n    - Execute an update SQL query.\r\n    - Parameters:\r\n        - `query`: The SQL query to execute.\r\n    - Returns: x rows affected.\r\n\r\n4. `delete_query`\r\n\r\n    - Execute a delete SQL query.\r\n    - Parameters:\r\n        - `query`: The SQL query to execute.\r\n    - Returns: x rows affected.\r\n    \r\n5. `count_query`\r\n\r\n    - Query the number of rows in a certain table..\r\n    - Parameters:\r\n        - `name`: The name of the table to count.\r\n    - Returns: The row number of the table.\r\n    \r\nBig thanks to https://github.com/Zhwt/go-mcp-mysql/ again.\r\n\r\n## License\r\n\r\nMIT\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgres",
        "databases",
        "database",
        "postgres databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "hadv--wisdomforge": {
      "owner": "hadv",
      "name": "wisdomforge",
      "url": "https://github.com/hadv/wisdomforge",
      "imageUrl": "/freedevtools/mcp/pfp/hadv.webp",
      "description": "Manage and retrieve knowledge efficiently using a vector database. Capable of intelligent storage and retrieval of various knowledge types, including best practices, lessons learned, and insights.",
      "stars": 5,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-25T06:00:50Z",
      "readme_content": "# WisdomForge\n\nA powerful knowledge management system that forges wisdom from experiences, insights, and best practices. Built with Qdrant vector database for efficient knowledge storage and retrieval.\n\n## Features\n\n- Intelligent knowledge management and retrieval\n- Support for multiple knowledge types (best practices, lessons learned, insights, experiences)\n- Configurable database selection via environment variables\n- Uses Qdrant's built-in FastEmbed for efficient embedding generation\n- Domain knowledge storage and retrieval\n- Deployable to Smithery.ai platform\n\n## Prerequisites\n\n- Node.js 20.x or later (LTS recommended)\n- npm 10.x or later\n- Qdrant or Chroma vector database\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/hadv/wisdomforge\ncd wisdomforge\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create a `.env` file in the root directory based on the `.env.example` template:\n```bash\ncp .env.example .env\n```\n\n4. Configure your environment variables in the `.env` file:\n\n### Required Environment Variables\n\n#### Database Configuration\n- `DATABASE_TYPE`: Choose your vector database (`qdrant` or `chroma`)\n- `COLLECTION_NAME`: Name of your vector collection\n- `QDRANT_URL`: URL of your Qdrant instance (required if using Qdrant)\n- `QDRANT_API_KEY`: API key for Qdrant (required if using Qdrant)\n- `CHROMA_URL`: URL of your Chroma instance (required if using Chroma)\n\n#### Server Configuration\n- `HTTP_SERVER`: Set to `true` to enable HTTP server mode\n- `PORT`: Port number for local development only (default: 3000). Not used in Smithery cloud deployment.\n\nExample `.env` configuration for Qdrant:\n```env\nDATABASE_TYPE=qdrant\nCOLLECTION_NAME=wisdom_collection\nQDRANT_URL=https://your-qdrant-instance.example.com:6333\nQDRANT_API_KEY=your_api_key\nHTTP_SERVER=true\nPORT=3000  # Only needed for local development\n```\n\n5. Build the project:\n```bash\nnpm run build\n```\n\n## AI IDE Integration\n\n### Cursor AI IDE\nAdd this configuration to your `~/.cursor/mcp.json` or `.cursor/mcp.json` file:\n```json\n{\n  \"mcpServers\": { \n    \"wisdomforge\": {\n      \"command\": \"/bin/zsh\",\n      \"args\": [\n        \"/path/to/wisdomforge/run-wisdomforge-mcp.sh\"\n      ]\n    }\n  }\n}\n```\n\nReplace the following placeholders in the configuration:\n- `YOUR_API_KEY`: Your Smithery API key\n- `YOUR_COLLECTION_NAME`: Your Qdrant collection name\n- `YOUR_QDRANT_URL`: Your Qdrant instance URL\n- `YOUR_QDRANT_API_KEY`: Your Qdrant API key\n\nNote: Make sure you have Node.js installed and `npx` available in your PATH. If you're using nvm, ensure you're using the correct Node.js version by running `nvm use --lts` before starting Cursor.\n\n### Claude Desktop\nAdd this configuration in Claude's settings:\n```json\n{\n  \"processes\": {\n    \"knowledge_server\": {\n      \"command\": \"/path/to/your/project/run-mcp.sh\",\n      \"args\": []\n    }\n  },\n  \"tools\": [\n    {\n      \"name\": \"store_knowledge\",\n      \"description\": \"Store domain-specific knowledge in a vector database\",\n      \"provider\": \"process\",\n      \"process\": \"knowledge_server\"\n    },\n    {\n      \"name\": \"retrieve_knowledge_context\",\n      \"description\": \"Retrieve relevant domain knowledge from a vector database\",\n      \"provider\": \"process\",\n      \"process\": \"knowledge_server\"\n    }\n  ]\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "wisdomforge",
        "secure database",
        "hadv wisdomforge",
        "database capable"
      ],
      "category": "databases"
    },
    "hagsmand--mcp-server-starrocks": {
      "owner": "hagsmand",
      "name": "mcp-server-starrocks",
      "url": "https://github.com/hagsmand/mcp-server-starrocks",
      "imageUrl": "/freedevtools/mcp/pfp/hagsmand.webp",
      "description": "Integrate with StarRocks databases to execute queries, manipulate data, and manage database schemas through a standardized interface. Provides functionality for reading and writing operations including SELECT, INSERT, UPDATE, and DELETE.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-19T14:13:39Z",
      "readme_content": "# StarRocks MCP Server\n\nA Model Control Protocol (MCP) server for interacting with StarRocks databases. This server provides a standardized interface for AI models to query and manipulate StarRocks databases through a set of defined tools.\n\n## Overview\n\nThe StarRocks MCP Server allows AI models to:\n- Execute SELECT queries on StarRocks databases\n- List available tables\n- Describe table schemas\n- Create new tables (when not in read-only mode)\n- Execute write operations like INSERT, UPDATE, DELETE (when not in read-only mode)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8+\n- StarRocks database instance\n- SQLAlchemy\n- MCP Python library\n\n### Install from source\n```bash\ngit clone https://github.com/yourusername/mcp-server-starrocks.git\ncd mcp-server-starrocks\npip install -e .\n```\n\n### Install from Smithery\nnpm install @smithery/sdk @modelcontextprotocol/sdk\n\n### Using MCP Inspector\nnpx @modelcontextprotocol/inspector uv --directory ~/mcp-server-starrocks run mcp-server-starrocks\n\n\n## Usage\n\n### Starting the server\n```bash\npython -m mcp_server_starrocks.server --host <starrocks-host> --port <starrocks-port> --user <username> --database <database-name> [--password <password>] [--readonly]\n```\n\n\n#### Command-line arguments:\n\n- `--host`: StarRocks server host (required)\n- `--port`: StarRocks server port (default: 9030)\n- `--user`: StarRocks username (required)\n- `--database`: StarRocks database name (required)\n- `--password`: StarRocks password (if required)\n- `--readonly`: Run the server in read-only mode (optional)\n\n### Available Tools\n\nThe server provides the following tools:\n\n#### Read-only tools:\n\n- `read-query`: Execute a SELECT query on the StarRocks database\n- `list-tables`: List all tables in the StarRocks database\n- `describe-table`: Describe the schema of a specific table\n\n#### Write tools (available when not in read-only mode):\n\n- `write-query`: Execute an INSERT, UPDATE, or DELETE query\n- `create-table`: Create a new table in the StarRocks database\n\n## Examples\n\n### Listing tables\n```json\n{\n    \"name\": \"list-tables\",\n    \"arguments\": {}\n}\n```\n\n### Executing a SELECT query\n```json\n{\n    \"name\": \"read-query\",\n    \"arguments\": {\n        \"query\": \"SELECT FROM my_table LIMIT 10\"\n    }\n}\n```\n\n### Describing a table\n```json\n{\n    \"name\": \"describe-table\",\n    \"arguments\": {\n        \"table_name\": \"my_table\"\n    }\n}\n```\n\n### Creating a table (when not in read-only mode)\n```json\n{\n    \"name\": \"create-table\",\n    \"arguments\": {\n        \"query\": \"CREATE TABLE new_table (id INT, name VARCHAR(100))\"\n    }\n}\n```\n\n## License\n\n[MIT License](LICENSE)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "starrocks",
        "database",
        "starrocks databases",
        "server starrocks",
        "databases secure"
      ],
      "category": "databases"
    },
    "hannesrudolph--sqlite-explorer-fastmcp-mcp-server": {
      "owner": "hannesrudolph",
      "name": "sqlite-explorer-fastmcp-mcp-server",
      "url": "https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hannesrudolph.webp",
      "description": "Provides read-only access to SQLite databases through the Model Context Protocol (MCP), allowing for querying and data exploration with built-in safety features and query validation.",
      "stars": 86,
      "forks": 24,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T17:14:47Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hannesrudolph-sqlite-explorer-fastmcp-mcp-server-badge.png)](https://mseep.ai/app/hannesrudolph-sqlite-explorer-fastmcp-mcp-server)\n\n# SQLite Explorer MCP Server\n\nAn MCP server that provides safe, read-only access to SQLite databases through Model Context Protocol (MCP). This server is built with the FastMCP framework, which enables LLMs to explore and query SQLite databases with built-in safety features and query validation.\n\n## 📋 System Requirements\n\n- Python 3.6+\n- SQLite database file (path specified via environment variable)\n\n## 📦 Dependencies\n\nInstall all required dependencies:\n\n```bash\n# Using pip\npip install -r requirements.txt\n```\n\n### Required Packages\n- **fastmcp**: Framework for building Model Context Protocol servers\n\nAll dependencies are specified in `requirements.txt` for easy installation.\n\n## 📑 Table of Contents\n- [System Requirements](#-system-requirements)\n- [Dependencies](#-dependencies)\n- [MCP Tools](#%EF%B8%8F-mcp-tools)\n- [Getting Started](#-getting-started)\n- [Installation Options](#-installation-options)\n  - [Claude Desktop](#option-1-install-for-claude-desktop)\n  - [Cline VSCode Plugin](#option-2-install-for-cline-vscode-plugin)\n- [Safety Features](#-safety-features)\n- [Development Documentation](#-development-documentation)\n- [Environment Variables](#%EF%B8%8F-environment-variables)\n\n## 🛠️ MCP Tools\n\nThe server exposes the following tools to LLMs:\n\n### read_query\nExecute a SELECT query on the database with built-in safety validations. Features:\n- Query validation and sanitization\n- Parameter binding support\n- Row limit enforcement\n- Results formatted as dictionaries\n\n### list_tables \nList all available tables in the database with their names.\n\n### describe_table\nGet detailed schema information for a specific table, including:\n- Column names and types\n- NULL constraints\n- Default values\n- Primary key information\n\n## 🚀 Getting Started\n\nClone the repository:\n\n```bash\ngit clone https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server.git\ncd sqlite-explorer-fastmcp-mcp-server\n```\n\n## 📦 Installation Options\n\nYou can install this MCP server in either Claude Desktop or the Cline VSCode plugin. Choose the option that best suits your needs.\n\n### Option 1: Install for Claude Desktop\n\nInstall using FastMCP:\n\n```bash\nfastmcp install sqlite_explorer.py --name \"SQLite Explorer\" -e SQLITE_DB_PATH=/path/to/db\n```\n\nReplace `/path/to/db` with the path to your SQLite database file.\n\n### Option 2: Install for Cline VSCode Plugin\n\nTo use this server with the [Cline VSCode plugin](http://cline.bot):\n\n1. In VSCode, click the server icon (☰) in the Cline plugin sidebar\n2. Click the \"Edit MCP Settings\" button (✎)\n3. Add the following configuration to the settings file:\n\n```json\n{\n  \"sqlite-explorer\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"run\",\n      \"--with\",\n      \"fastmcp\",\n      \"--with\",\n      \"uvicorn\",\n      \"fastmcp\",\n      \"run\",\n      \"/path/to/repo/sqlite_explorer.py\"\n    ],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"/path/to/your/database.db\"\n    }\n  }\n}\n```\n\nReplace:\n- `/path/to/repo` with the full path to where you cloned this repository (e.g., `/Users/username/Projects/sqlite-explorer-fastmcp-mcp-server`)\n- `/path/to/your/database.db` with the full path to your SQLite database file\n\n## 🔒 Safety Features\n\n- Read-only access to SQLite databases\n- Query validation and sanitization\n- Parameter binding for safe query execution\n- Row limit enforcement\n- Progress output suppression for clean JSON responses\n\n## 📚 Development Documentation\n\nThe repository includes documentation files for development:\n\n- `mcp-documentation.txt`: Contains comprehensive documentation about the MCP server implementation and FastMCP framework usage.\n\nThis documentation serves as context when developing features and can be used with LLMs to assist in development.\n\n## ⚙️ Environment Variables\n\nThe following environment variables must be set:\n\n- `SQLITE_DB_PATH`: Full path to the SQLite database file you want to explore\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sqlite",
        "databases",
        "database",
        "sqlite databases",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "hanweg--mcp-sqlexpress": {
      "owner": "hanweg",
      "name": "mcp-sqlexpress",
      "url": "https://github.com/hanweg/mcp-sqlexpress",
      "imageUrl": "/freedevtools/mcp/pfp/hanweg.webp",
      "description": "Interact with Microsoft SQL Server Express to execute queries, manage databases, and create tables. Supports both Windows and SQL Server authentication for seamless database operations.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-30T02:55:08Z",
      "readme_content": "# SQL Server Express MCP Server\n\nAn MCP server for interacting with Microsoft SQL Server Express. Supports Windows and SQL Server authentication.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- Microsoft ODBC Driver 18 for SQL Server\n- SQL Server instance with appropriate permissions\n\n## Installation\n\nClone this repo\n\n```powershell\ncd mcp-sqlexpress\n\n# Create and activate virtual environment\nuv venv\n.venv\\Scripts\\activate\n\n# Install dependencies\nuv pip install --editable .\n```\n\n## Usage with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"sqlexpress\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"PATH\\\\TO\\\\PROJECT\\\\mcp-sqlexpress\",\n                \"run\",\n                \"mcp-server-sqlexpress\",\n                \"--server\",\n                \"server\\\\instance\",\n                \"--auth\",\n                \"windows\",\n                \"--trusted-connection\",\n                \"yes\",\n                \"--trust-server-certificate\",\n                \"yes\",\n                \"--allowed-databases\",\n                \"database1,database2\"\n            ]\n        }\n    }\n}\n```\n\n### Authentication Options\n\nFor Windows Authentication:\n- Set `--auth windows`\n- Set `--trusted-connection yes`\n\nFor SQL Server Authentication:\n- Set `--auth sql`\n- Add `--username` and `--password`\n\n## Features\n\n### Tools\n- `get_allowed_databases`: Get list of databases that are allowed to be accessed\n- `read_query`: Execute SELECT queries\n- `write_query`: Execute INSERT/UPDATE/DELETE queries\n- `create_table`: Create new tables\n- `list_tables`: List all tables in database\n- `describe_table`: Show table schema",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sqlexpress",
        "databases",
        "database",
        "mcp sqlexpress",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "henilcalagiya--google-sheets-mcp": {
      "owner": "henilcalagiya",
      "name": "google-sheets-mcp",
      "url": "https://github.com/henilcalagiya/google-sheets-mcp",
      "imageUrl": "",
      "description": "Your AI Assistant's Gateway to Google Sheets! 25 powerful tools for seamless Google Sheets automation via MCP.",
      "stars": 8,
      "forks": 3,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-10-02T23:00:08Z",
      "readme_content": "# Google Sheets MCP Server\n\n> Powerful tools for automating Google Sheets using Model Context Protocol (MCP)\n\n**mcp-name: io.github.henilcalagiya/google-sheets-mcp**\n\n## Overview\n\nGoogle Sheets MCP Server provides seamless integration of Google Sheets with any MCP-compatible client. It enables full spreadsheet automation — including creating, reading, updating, and deleting sheets — through a simple and secure API layer.\n\n## Features\n\n- Full CRUD support for Google Sheets and tables\n- Works with Continue.dev, Claude Desktop, Perplexity, and other MCP clients\n- Secure authentication via Google Service Account\n- Comprehensive tools for Google Sheets automation\n- Automatic installation via `uvx`\n\n## Requirements\n\n- Python 3.10+\n- `uv` package manager (for `uvx` command)\n- A Google Cloud project with a Service Account\n- MCP-compatible client (e.g., Continue.dev)\n\n**Install uv:**\n```bash\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows PowerShell\nirm https://astral.sh/uv/install.ps1 | iex\n```\n\n---\n\n## Quick Start\n\n### 1. Set Up Google Service Account\n\n**Step 1: Create a Google Cloud Project**\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n2. Click \"Select a project\" → \"New Project\"\n3. Enter a project name (e.g., \"my-sheets-automation\")\n4. Click \"Create\"\n\n**Step 2: Enable Required APIs**\n1. In your project, go to \"APIs & Services\" → \"Library\"\n2. Search for \"Google Sheets API\" → Click → \"Enable\"\n3. Search for \"Google Drive API\" → Click → \"Enable\"\n\n**Step 3: Create Service Account**\n1. Go to \"IAM & Admin\" → \"Service Accounts\"\n2. Click \"Create Service Account\"\n3. Enter service account name (e.g., \"sheets-mcp-service\")\n4. Click \"Create and Continue\"\n5. Skip role assignment → Click \"Continue\"\n6. Click \"Done\"\n\n**Step 4: Generate JSON Key**\n1. Click on your new service account email\n2. Go to \"Keys\" tab → \"Add Key\" → \"Create new key\"\n3. Choose \"JSON\" format → Click \"Create\"\n4. The JSON file will download automatically\n\n**Step 5: Extract Required Values**\nOpen the downloaded JSON file and note these values:\n- `project_id` (e.g., \"my-sheets-automation-123456\")\n- `private_key_id` (e.g., \"a4ae73111b11b2c3b07cc01006e71eb8230dfa29\")\n- `private_key` (the long private key starting with \"-----BEGIN PRIVATE KEY-----\")\n- `client_email` (e.g., \"sheets-mcp-service@my-sheets-automation-123456.iam.gserviceaccount.com\")\n- `client_id` (e.g., \"113227823918217958816\")\n- `client_x509_cert_url` (e.g., \"https://www.googleapis.com/robot/v1/metadata/x509/sheets-mcp-service%40my-sheets-automation-123456.iam.gserviceaccount.com\")\n\n**Example Google service account JSON structure:**\n```json\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"your-project-id\",\n  \"private_key_id\": \"your-private-key-id\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"your-service@your-project.iam.gserviceaccount.com\",\n  \"client_id\": \"your-client-id\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/your-service%40your-project.iam.gserviceaccount.com\"\n}\n```\n\n[Follow this guide if needed](https://console.cloud.google.com/apis/credentials)\n\n### 2. Configure MCP Client\n\n```json\n{\n  \"mcpServers\": {\n    \"google-sheets-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"google-sheets-mcp@latest\"],\n      \"env\": {\n        \"project_id\": \"your-project-id\",\n        \"private_key_id\": \"your-private-key-id\",\n        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\",\n        \"client_email\": \"your-service@your-project.iam.gserviceaccount.com\",\n        \"client_id\": \"your-client-id\",\n        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/your-service%40your-project.iam.gserviceaccount.com\"\n      }\n    }\n  }\n}\n```\n\n**💡 Pro Tip:** You can copy the values directly from your Google service account JSON file. The field names in the JSON file are used exactly as they are - no changes needed!\n\n**🔄 Backward Compatibility:** The server also supports the old `GOOGLE_` prefixed variable names (e.g., `GOOGLE_PROJECT_ID`) for existing configurations.\n\n### 3. Share Your Google Sheet with the Service Account\n\n- Open your target Google Spreadsheet in your web browser.\n- Click the **Share** button.\n- Enter the **service account email** (e.g., `your-service@your-project.iam.gserviceaccount.com`) and assign **Editor** access.\n- Click **Send** to provide editor permissions.\n\n**🎉 You're all set!** Your MCP client will automatically install and run the package when needed.\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Henil C Alagiya**\n\n- **GitHub**: [@henilcalagiya](https://github.com/henilcalagiya)\n- **LinkedIn**: [Henil C Alagiya](https://www.linkedin.com/in/henilcalagiya/)\n\n**Support & Contributions:**\n- 🐛 **Report Issues**: [GitHub Issues](https://github.com/henilcalagiya/google-sheets-mcp/issues)\n- 💬 **Questions**: Reach out on [LinkedIn](https://www.linkedin.com/in/henilcalagiya/)\n- 🤝 **Contributions**: Pull requests welcome! ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "automation",
        "sheets automation",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "hertzfelt--windsurf-supabase-mcp": {
      "owner": "hertzfelt",
      "name": "windsurf-supabase-mcp",
      "url": "https://github.com/hertzfelt/windsurf-supabase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrate a Supabase database with the Windsurf Editor, providing real-time query validation and optimized response formatting for UI components. The server improves error handling and SQL query processing for better interaction experiences.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase database",
        "windsurf supabase",
        "database windsurf"
      ],
      "category": "databases"
    },
    "hkk101--mcp-server-mysql": {
      "owner": "hkk101",
      "name": "mcp-server-mysql",
      "url": "https://github.com/hkk101/mcp-server-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/hkk101.webp",
      "description": "Enables LLMs to execute read-only SQL queries against MySQL databases and inspect database schemas. Retrieves schema information including column names and data types for each table.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-03T03:44:20Z",
      "readme_content": "# MCP Server for MySQL based on NodeJS\n[![smithery badge](https://smithery.ai/badge/@benborla29/mcp-server-mysql)](https://smithery.ai/server/@benborla29/mcp-server-mysql)\n\n\nA Model Context Protocol server that provides read-only access to MySQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Components\n\n### Tools\n\n- **mysql_query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas**\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\",\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\"\n      }\n\n    }\n  }\n}\n```\n\nReplace `/db_name` with your database name or leave it blank to retrieve all databases.\n\n## Troubleshooting\nIf you encounter an error \"Could not connect to MCP server mcp-server-mysql\", you may need to explicitly\nset the path of all required binaries such as the configuration below:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"/path/to/npx/binary/npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\",\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\"\n        \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\" <-- Add this\n      }\n\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "hoonzinope--pymcp-mysql": {
      "owner": "hoonzinope",
      "name": "pymcp-mysql",
      "url": "https://github.com/hoonzinope/pymcp-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/hoonzinope.webp",
      "description": "Enable seamless interaction with MySQL databases, allowing the execution of SQL queries and retrieval of results in real-time. Integrate MySQL tools into MCP workflows for data querying and analysis.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-22T09:16:37Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hoonzinope-pymcp-mysql-badge.png)](https://mseep.ai/app/hoonzinope-pymcp-mysql)\n\n# pymcp\n\n`pymcp`는 FastMCP를 기반으로 한 Python 프로젝트로, MySQL 데이터베이스와 상호작용할 수 있는 도구를 제공합니다. 이 프로젝트는 서버와 클라이언트 간의 통신을 지원하며, 다양한 도구를 통해 데이터를 조회하고 분석할 수 있습니다.\n\n## 프로젝트 구조\n\n```\npymcp/\n├── client.py          # 클라이언트 코드\n├── main.py            # 서버 실행 코드\n├── src/\n│   ├── env.py         # 로컬 환경 설정\n│   ├── env_dev.py     # 개발 환경 설정\n│   ├── mcp_instance.py # MCP 인스턴스 초기화\n│   ├── mysql_tool.py  # MySQL 관련 도구 정의\n├── pyproject.toml     # 프로젝트 메타데이터 및 의존성\n├── requirements.txt   # 의존성 목록\n└── README.md          # 프로젝트 설명\n```\n\n## 설치 및 실행\n\n### 1. 의존성 설치\n\nPython 3.13 이상이 필요합니다. 의존성을 설치하려면 아래 명령어를 실행하세요:\n\n```bash\npip install -r requirements.txt\n```\n\n### 2. 서버 실행\n\n서버를 실행하려면 `main.py`를 실행하세요:\n\n```bash\npython main.py\n```\n\n서버는 기본적으로 `0.0.0.0:8080`에서 실행됩니다.\n\n### 3. 클라이언트 실행\n\n클라이언트를 실행하려면 `client.py`를 실행하세요:\n\n```bash\npython client.py\n```\n\n클라이언트는 서버와 통신하여 MySQL 쿼리를 실행하거나 도구 목록을 조회할 수 있습니다.\n\n## 환경 설정\n\n환경에 따라 MySQL 설정이 다르게 적용됩니다:\n\n- **로컬 환경**: `src/env.py`\n- **개발 환경**: `src/env_dev.py`\n\n환경은 `APP_ENV` 환경 변수를 통해 설정할 수 있습니다. 기본값은 `local`입니다.\n\n```bash\nexport APP_ENV=dev  # 개발 환경 설정\n```\n\n## 제공 도구\n\n서버에서 제공하는 도구는 다음과 같습니다:\n\n1. **`describe_tools`**  \n   사용 가능한 도구 목록과 사용법을 설명합니다.\n\n2. **`query_mysql(sql: str)`**  \n   주어진 SQL 쿼리를 실행하고 결과를 반환합니다.  \n   예시: `query_mysql(\"SELECT * FROM users LIMIT 10;\")`\n\n## 주요 파일 설명\n\n### `main.py`\n\n서버를 실행하는 진입점입니다. MCP 인스턴스를 초기화하고 도구를 등록한 뒤 서버를 실행합니다.\n\n### `client.py`\n\n서버와 상호작용하는 클라이언트 코드입니다. 서버에 연결하여 도구를 호출할 수 있습니다.\n\n### `src/mysql_tool.py`\n\nMySQL 관련 도구를 정의한 파일입니다. `query_mysql`와 같은 도구를 통해 SQL 쿼리를 실행할 수 있습니다.\n\n### `src/env.py` 및 `src/env_dev.py`\n\nMySQL 연결 설정을 포함한 환경 변수 파일입니다. 환경에 따라 적절한 설정을 로드합니다.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "mysql tools",
        "databases secure"
      ],
      "category": "databases"
    },
    "hsinyuyen--my-postgres-mcp": {
      "owner": "hsinyuyen",
      "name": "my-postgres-mcp",
      "url": "https://github.com/hsinyuyen/my-postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hsinyuyen.webp",
      "description": "Facilitates interaction with PostgreSQL databases by enabling read-only execution of SQL queries and inspection of database schemas. Provides structured data access while maintaining data integrity through read-only transactions.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-01T00:54:20Z",
      "readme_content": "# PostgreSQL\n\nA Model Context Protocol server that provides read-only access to PostgreSQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas** (`postgres://<host>/<table>/schema`)\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n### Docker\n\n* when running docker on macos, use host.docker.internal if the server is running on the host network (eg localhost)\n* username/password can be added to the postgresql url with `postgresql://user:password@host:port/db-name`\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"mcp/postgres\", \n        \"postgresql://host.docker.internal:5432/mydb\"]\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-postgres\",\n        \"postgresql://localhost/mydb\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/mydb` with your database name.\n\n## Building\n\nDocker:\n\n```sh\ndocker build -t mcp/postgres -f src/postgres/Dockerfile . \n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "hthuong09--postgres-mcp": {
      "owner": "hthuong09",
      "name": "postgres-mcp",
      "url": "https://github.com/hthuong09/postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hthuong09.webp",
      "description": "Interact with PostgreSQL databases through a secure, read-only interface to query data and inspect database schemas.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-29T09:30:21Z",
      "readme_content": "# MCP PostgreSQL Server\n\nThis is a Model Context Protocol server for interacting with PostgreSQL databases. It provides a read-only interface to query PostgreSQL databases and inspect their schema.\n\n## Installation\n\n```bash\nnpm install -g @hthuong09/postgres-mcp\n```\n\n## Configuration\n\nThe server can be configured in multiple ways, listed in order of priority:\n\n1. **Environment Variables**\n   - `POSTGRES_URL`: Full database URL (e.g., `postgres://user:pass@host:5432/dbname`)\n   - Individual connection parameters:\n     - `POSTGRES_HOST`: Database host\n     - `POSTGRES_PORT`: Database port (default: 5432)\n     - `POSTGRES_DB`: Database name\n     - `POSTGRES_USER`: Database user\n     - `POSTGRES_PASSWORD`: Database password\n     - `POSTGRES_SSL`: Enable SSL mode (set to 'true' to enable)\n     - `POSTGRES_SCHEMA`: Database schema (default: 'public')\n   - Additional configuration:\n     - `DOTENV_PATH`: Custom path to .env file\n     - `DEBUG_MCP`: Enable debug logging (set to 'true' to enable)\n\n2. **Command Line**\n   ```bash\n   npx @hthuong09/postgres-mcp \"postgres://user:pass@host:5432/dbname\"\n   ```\n\n### Resources\n\n- Table schemas: Each table in the database is exposed as a resource\n- Resource URI format: `postgres://user@host/dbname/table_name/schema`\n- Response format: JSON array of column definitions (name and data type)\n\n## Usage Examples\n\n1. Using environment variables:\n   ```bash\n   export POSTGRES_HOST=localhost\n   export POSTGRES_DB=mydb\n   export POSTGRES_USER=myuser\n   export POSTGRES_PASSWORD=mypassword\n   npx @hthuong09/postgres-mcp\n   ```\n\n2. Using a connection URL:\n   ```bash\n   npx @hthuong09/postgres-mcp \"postgres://myuser:mypassword@localhost:5432/mydb\"\n   ```\n\n3. Using environment variables with SSL:\n   ```bash\n   export POSTGRES_HOST=db.example.com\n   export POSTGRES_DB=mydb\n   export POSTGRES_USER=myuser\n   export POSTGRES_PASSWORD=mypassword\n   export POSTGRES_SSL=true\n   npx @hthuong09/postgres-mcp\n   ```\n\n4. Using a custom .env file location:\n   ```bash\n   DOTENV_PATH=/path/to/.env npx @hthuong09/postgres-mcp\n   ```\n\n## Security Considerations\n\n- Database credentials should be kept secure\n- Use environment variables or .env files instead of command line arguments in production to avoid exposing credentials in process lists\n- Consider using SSL in production environments\n- The server only allows read-only transactions for safety\n- Passwords are automatically stripped from resource URIs\n\n## Development\n\nTo build the server locally:\n\n```bash\nnpm install\nnpm run build\n```\n\nTo run in watch mode during development:\n\n```bash\nnpm run watch\n```\n\n## Debugging\n\nSet `DEBUG_MCP=true` to enable debug logging. Logs will be written to:\n- Unix/macOS: `/tmp/postgres-mcp-debug.json`\n- Windows: `%TEMP%/postgres-mcp-debug.json`\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "databases secure",
        "secure database",
        "postgresql databases"
      ],
      "category": "databases"
    },
    "hummusonrails--couchbase-mcp-server": {
      "owner": "hummusonrails",
      "name": "couchbase-mcp-server",
      "url": "https://github.com/hummusonrails/couchbase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hummusonrails.webp",
      "description": "Execute natural language queries on Couchbase Capella clusters by transforming plain English requests into SQL++ queries and retrieving results in a user-friendly format.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-05T04:47:02Z",
      "readme_content": "# Couchbase MCP Server\n\nThe [Model Context Protocol (MCP) is a protocol](https://modelcontextprotocol.io/introduction) for handling interactions between large language models (LLMs) and external systems. This repository implements a Couchbase MCP Server using the Couchbase Node.js SDK, enabling MCP clients (e.g., Claude Desktop) to perform natural language queries on Couchbase Capella clusters.\n\nWith this server you can use commands like:\n\n* `Show me the results of SELECT * FROM my_bucket LIMIT 10`\n* `Execute this query: SELECT name, age FROM users WHERE active = true`\n* `Get me the latest 5 documents from my_bucket`\n* `Summarize the latest 5 orders from the orders bucket for me`\n\n## Example\n\nThe following screenshot shows the MCP server in action with the Claude Desktop client. The user issues a general request in plain English and the MCP server translates it into a SQL++ query that is executed against the Couchbase Capella cluster. The results are then returned to the user in a readable format.\n\n<p>\n    \n</p>\n\n## Setup\n\n1. Clone the repository and install dependencies:\n\n```bash\ngit clone git@github.com:hummusonrails/couchbase-mcp-server.git\ncd couchbase-mcp-server\nnpm install\n```\n\n2. Create a `.env` file in the root directory and add your Couchbase connection string, username, and password:\n\n```env\nCOUCHBASE_CONNECTION_STRING=couchbases://your-cluster.cloud.couchbase.com\nCOUCHBASE_USERNAME=your_username\nCOUCHBASE_PASSWORD=your_password\n```\n\nRefer to the `.env.sample` file for the required environment variables.\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Run the server using Stdio transport:\n\n```bash\nnpx couchbase-mcp-server\n```\n> [!NOTE]\n> The MCP server uses the StdioServerTransport, so it communicates over standard input/output. Ensure that your MCP client (e.g., Claude Desktop) is configured to use a local MCP server.\n> Follow the [Claude Desktop documentation](https://modelcontextprotocol.io/quickstart/user) to set up the MCP client to connect to the local server.\n\n## Features\n\n### Couchbase Query Tool\n\n* **ToolName:** `query-couchbase`\n* **Description:** Executes a SQL++ query statement on your Couchbase Capella cluster.\n* **Usage**: When invoked, the server will use the Couchbase Node.js SDK to execute the provided SQL++ query and return the results.\n\n## Developing\n\nTo work on the project locally:\n\n1. Install dependences:\n\n```bash\nnpm install\nnpm run build\n```\n\n2. Test the server using an MCP client:\n\nLaunch your MCP client (e.g., Claude Desktop) configured to connect and invoke the tool using a sample query.\n\n3. Debugging\n\nAll logging messages are sent to `stderr` to ensure that `stdout` only contains MCP protocol JSON. Check your logs for detailed connection and error messages.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a pull request or open an issue with your suggestions. For any changes, ensure you follow the project’s code style.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "couchbase",
        "databases",
        "database",
        "queries couchbase",
        "couchbase capella",
        "databases secure"
      ],
      "category": "databases"
    },
    "hydrolix--mcp-hydrolix": {
      "owner": "hydrolix",
      "name": "mcp-hydrolix",
      "url": "https://github.com/hydrolix/mcp-hydrolix",
      "imageUrl": "",
      "description": "Hydrolix time-series datalake integration providing schema exploration and query capabilities to LLM-based workflows.",
      "stars": 5,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-12T17:16:43Z",
      "readme_content": "# Hydrolix MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-hydrolix)](https://pypi.org/project/mcp-hydrolix)\n\nAn MCP server for Hydrolix.\n\n## Tools\n\n* `run_select_query`\n  * Execute SQL queries on your Hydrolix cluster.\n  * Input: `sql` (string): The SQL query to execute.\n  * All Hydrolix queries are run with `readonly = 1` to ensure they are safe.\n\n* `list_databases`\n  * List all databases on your Hydrolix cluster.\n\n* `list_tables`\n  * List all tables in a database.\n  * Input: `database` (string): The name of the database.\n\n## Effective Usage\n\nDue to the wide variety in LLM architectures, not all models will proactively use the tools above, and few will use them effectively without guidance, even with the carefully-constructed tool descriptions provided to the model. To get the best results out of your model while using the Hydrolix MCP server, we recommend the following:\n\n* Refer to your Hydrolix database by name and request tool usage in your prompts (e.g., \"Using MCP tools to access my Hydrolix database, please ...\")\n  - This encourages the model to use the MCP tools available and minimizes hallucinations.\n* Include time ranges in your prompts (e.g., \"Between December 5 2023 and January 18 2024, ...\") and specifically request that the output be ordered by timestamp.\n  - This prompts the model to write more efficient queries that take advantage of [primary key optimizations](https://hydrolix.io/blog/optimizing-latest-n-row-queries/)\n\n### Health Check Endpoint\n\nWhen running with HTTP or SSE transport, a health check endpoint is available at `/health`. This endpoint:\n- Returns `200 OK` with the Hydrolix query-head's Clickhouse version if the server is healthy and can connect to Hydrolix\n- Returns `503 Service Unavailable` if the server cannot connect to the Hydrolix query-head\n\nExample:\n```bash\ncurl http://localhost:8000/health\n# Response: OK - Connected to Hydrolix compatible with ClickHouse 24.3.1\n```\n\n## Configuration\n\nThe Hydrolix MCP server is configured using a standard MCP server entry. Consult your client's documentation for specific instructions on where to find or declare MCP servers. An example setup using Claude Desktop is documented below.\n\nThe recommended way to launch the Hydrolix MCP server is via the [`uv` project manager](https://github.com/astral-sh/uv), which will manage installing all other dependencies in an isolated environment.\n\nMCP Server definition using username and password (JSON):\n\n```json\n{\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp-hydrolix\",\n    \"--python\",\n    \"3.13\",\n    \"mcp-hydrolix\"\n  ],\n  \"env\": {\n    \"HYDROLIX_HOST\": \"<hydrolix-host>\",\n    \"HYDROLIX_USER\": \"<hydrolix-user>\",\n    \"HYDROLIX_PASSWORD\": \"<hydrolix-password>\"\n  }\n}\n```\n\nMCP Server definition using service account token (JSON):\n\n```json\n{\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp-hydrolix\",\n    \"--python\",\n    \"3.13\",\n    \"mcp-hydrolix\"\n  ],\n  \"env\": {\n    \"HYDROLIX_HOST\": \"<hydrolix-host>\",\n    \"HYDROLIX_TOKEN\": \"<hydrolix-service-account-token>\"\n  }\n}\n```\n\nMCP Server definition using username and password (YAML):\n\n```yaml\ncommand: uv\nargs:\n- run\n- --with\n- mcp-hydrolix\n- --python\n- \"3.13\"\n- mcp-hydrolix\nenv:\n  HYDROLIX_HOST: <hydrolix-host>\n  HYDROLIX_USER: <hydrolix-user>\n  HYDROLIX_PASSWORD: <hydrolix-password>\n```\n\nMCP Server definition using service account token (YAML):\n\n```yaml\ncommand: uv\nargs:\n- run\n- --with\n- mcp-hydrolix\n- --python\n- \"3.13\"\n- mcp-hydrolix\nenv:\n  HYDROLIX_HOST: <hydrolix-host>\n  HYDROLIX_TOKEN: <hydrolix-service-account-token>\n```\n\n### Configuration Example (Claude Desktop)\n\n1. Open the Claude Desktop configuration file located at:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add a `mcp-hydrolix` server entry to the `mcpServers` config block to use username and password:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hydrolix\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-hydrolix\",\n        \"--python\",\n        \"3.13\",\n        \"mcp-hydrolix\"\n      ],\n      \"env\": {\n        \"HYDROLIX_HOST\": \"<hydrolix-host>\",\n        \"HYDROLIX_USER\": \"<hydrolix-user>\",\n        \"HYDROLIX_PASSWORD\": \"<hydrolix-password>\"\n      }\n    }\n  }\n}\n```\n\nTo leverage service account use the following config block:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hydrolix\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp-hydrolix\",\n        \"--python\",\n        \"3.13\",\n        \"mcp-hydrolix\"\n      ],\n      \"env\": {\n        \"HYDROLIX_HOST\": \"<hydrolix-host>\",\n        \"HYDROLIX_TOKEN\": \"<hydrolix-service-account-token>\"\n      }\n    }\n  }\n}\n```\n\n3. Update the environment variable definitions to point to your Hydrolix cluster.\n\n4. (Recommended) Locate the command entry for `uv` and replace it with the absolute path to the `uv` executable. This ensures that the correct version of `uv` is used when starting the server. You can find this path using `which uv` or `where.exe uv`.\n\n5. Restart Claude Desktop to apply the changes. If you are using Windows, ensure Claude is stopped completely by closing the client using the system tray icon.\n\n### Environment Variables\n\nThe following variables are used to configure the Hydrolix connection. These variables may be provided via the MCP config block (as shown above), a `.env` file, or traditional environment variables.\n\n#### Required Variables\n* `HYDROLIX_HOST`: The hostname of your Hydrolix server\n* `HYDROLIX_TOKEN`: The Hydrolix service account token (omit if using username/password)\n* `HYDROLIX_USER`: The username for authentication (omit if using service account)\n* `HYDROLIX_PASSWORD`: The password for authentication (omit if using service account)\n\n**Authentication precedence:** If both `HYDROLIX_TOKEN` and `HYDROLIX_USER`/`HYDROLIX_PASSWORD` are provided, the service account token takes precedence and username/password authentication will be ignored.\n\n#### Optional Variables\n* `HYDROLIX_PORT`: The port number of your Hydrolix server\n  * Default: `8088`\n  * Usually doesn't need to be set unless using a non-standard port\n* `HYDROLIX_VERIFY`: Enable/disable SSL certificate verification\n  * Default: `\"true\"`\n  * Set to `\"false\"` to disable certificate verification (not recommended for production)\n* `HYDROLIX_DATABASE`: Default database to use\n  *Default: None (uses server default)\n  * Set this to automatically connect to a specific database\n* `HYDROLIX_MCP_SERVER_TRANSPORT`: Sets the transport method for the MCP server.\n  * Default: `\"stdio\"`\n  * Valid options: `\"stdio\"`, `\"http\"`, `\"sse\"`. This is useful for local development with tools like MCP Inspector.\n* `HYDROLIX_MCP_BIND_HOST`: Host to bind the MCP server to when using HTTP or SSE transport\n  * Default: `\"127.0.0.1\"`\n  * Set to `\"0.0.0.0\"` to bind to all network interfaces (useful for Docker or remote access)\n  * Only used when transport is `\"http\"` or `\"sse\"`\n* `HYDROLIX_MCP_BIND_PORT`: Port to bind the MCP server to when using HTTP or SSE transport\n  * Default: `\"8000\"`\n  * Only used when transport is `\"http\"` or `\"sse\"`\n\n\nFor MCP Inspector or remote access with HTTP transport:\n\n```env\nHYDROLIX_HOST=localhost\nHYDROLIX_USER=default\nHYDROLIX_PASSWORD=myPassword\nHYDROLIX_MCP_SERVER_TRANSPORT=http\nHYDROLIX_MCP_BIND_HOST=0.0.0.0  # Bind to all interfaces\nHYDROLIX_MCP_BIND_PORT=4200  # Custom port (default: 8000)\n```\n\nWhen using HTTP transport, the server will run on the configured port (default 8000). For example, with the above configuration:\n- MCP endpoint: `http://localhost:4200/mcp`\n- Health check: `http://localhost:4200/health`\n\nNote: The bind host and port settings are only used when transport is set to \"http\" or \"sse\".\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "datalake",
        "secure database",
        "databases secure",
        "datalake integration"
      ],
      "category": "databases"
    },
    "imlewc--elasticsearch7-mcp-server": {
      "owner": "imlewc",
      "name": "elasticsearch7-mcp-server",
      "url": "https://github.com/imlewc/elasticsearch7-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/imlewc.webp",
      "description": "Interact with Elasticsearch 7.x using a standardized MCP protocol to perform basic operations and advanced search functionalities. Access features like aggregation queries, highlighting, and sorting without the complexity of direct API calls.",
      "stars": 5,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-20T17:56:18Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/imlewc-elasticsearch7-mcp-server-badge.png)](https://mseep.ai/app/imlewc-elasticsearch7-mcp-server)\n\n# Elasticsearch 7.x MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@imlewc/elasticsearch7-mcp-server)](https://smithery.ai/server/@imlewc/elasticsearch7-mcp-server)\n\nAn MCP server for Elasticsearch 7.x, providing compatibility with Elasticsearch 7.x versions.\n\n<a href=\"https://glama.ai/mcp/servers/zxwxozvlme\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/zxwxozvlme/badge\" alt=\"Elasticsearch 7.x Server MCP server\" />\n</a>\n\n## Features\n\n- Provides an MCP protocol interface for interacting with Elasticsearch 7.x\n- Supports basic Elasticsearch operations (ping, info, etc.)\n- Supports complete search functionality, including aggregation queries, highlighting, sorting, and other advanced features\n- Easily access Elasticsearch functionality through any MCP client\n\n## Requirements\n\n- Python 3.10+\n- Elasticsearch 7.x (7.17.x recommended)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Elasticsearch 7.x MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@imlewc/elasticsearch7-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @imlewc/elasticsearch7-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\npip install -e .\n```\n\n## Environment Variables\n\nThe server requires the following environment variables:\n\n- `ELASTIC_HOST`: Elasticsearch host address (e.g., http://localhost:9200)\n- `ELASTIC_USERNAME`: Elasticsearch username\n- `ELASTIC_PASSWORD`: Elasticsearch password\n- `MCP_PORT`: (Optional) MCP server listening port, default 9999\n\n## Using Docker Compose\n\n1. Create a `.env` file and set `ELASTIC_PASSWORD`:\n\n```\nELASTIC_PASSWORD=your_secure_password\n```\n\n2. Start the services:\n\n```bash\ndocker-compose up -d\n```\n\nThis will start a three-node Elasticsearch 7.17.10 cluster, Kibana, and the MCP server.\n\n## Using an MCP Client\n\nYou can use any MCP client to connect to the MCP server:\n\n```python\nfrom mcp import MCPClient\n\nclient = MCPClient(\"localhost:9999\")\nresponse = client.call(\"es-ping\")\nprint(response)  # {\"success\": true}\n```\n\n## API Documentation\n\nCurrently supported MCP methods:\n\n- `es-ping`: Check Elasticsearch connection\n- `es-info`: Get Elasticsearch cluster information\n- `es-search`: Search documents in Elasticsearch index\n\n### Search API Examples\n\n#### Basic Search\n```python\n# Basic search\nsearch_response = client.call(\"es-search\", {\n    \"index\": \"my_index\",\n    \"query\": {\n        \"match\": {\n            \"title\": \"search keywords\"\n        }\n    },\n    \"size\": 10,\n    \"from\": 0\n})\n```\n\n#### Aggregation Query\n```python\n# Aggregation query\nagg_response = client.call(\"es-search\", {\n    \"index\": \"my_index\",\n    \"size\": 0,  # Only need aggregation results, no documents\n    \"aggs\": {\n        \"categories\": {\n            \"terms\": {\n                \"field\": \"category.keyword\",\n                \"size\": 10\n            }\n        },\n        \"avg_price\": {\n            \"avg\": {\n                \"field\": \"price\"\n            }\n        }\n    }\n})\n```\n\n#### Advanced Search\n```python\n# Advanced search with highlighting, sorting, and filtering\nadvanced_response = client.call(\"es-search\", {\n    \"index\": \"my_index\",\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\"match\": {\"content\": \"search term\"}}\n            ],\n            \"filter\": [\n                {\"range\": {\"price\": {\"gte\": 100, \"lte\": 200}}}\n            ]\n        }\n    },\n    \"sort\": [\n        {\"date\": {\"order\": \"desc\"}},\n        \"_score\"\n    ],\n    \"highlight\": {\n        \"fields\": {\n            \"content\": {}\n        }\n    },\n    \"_source\": [\"title\", \"date\", \"price\"]\n})\n```\n\n## Development\n\n1. Clone the repository\n2. Install development dependencies\n3. Run the server: `elasticsearch7-mcp-server`\n\n## License\n\n[License in LICENSE file]\n\n*[中文文档](README-cn.md)*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elasticsearch7",
        "elasticsearch",
        "databases",
        "elasticsearch7 mcp",
        "imlewc elasticsearch7",
        "elasticsearch using"
      ],
      "category": "databases"
    },
    "isaacgounton--sqlite-mcp-server": {
      "owner": "isaacgounton",
      "name": "sqlite-mcp-server",
      "url": "https://github.com/isaacgounton/sqlite-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/isaacgounton.webp",
      "description": "Provides standardized SQLite database operations through an MCP interface, enabling efficient execution of SQL commands, table management, and business insights memo tracking. Supports both in-memory and file-based storage configurations.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-25T00:57:28Z",
      "readme_content": "# SQLite MCP Server\n[![smithery badge](https://smithery.ai/badge/@isaacgounton/sqlite-mcp-server)](https://smithery.ai/server/@isaacgounton/sqlite-mcp-server)\n\nA Model Context Protocol (MCP) server that provides SQLite database operations through a standardized interface.\n\n## Features\n\n- In-memory SQLite database (configurable for file-based storage)\n- SQL operations (SELECT, INSERT, UPDATE, DELETE)\n- Table management (CREATE, LIST, DESCRIBE)\n- Business insights memo tracking\n- Docker support for easy deployment\n\n## Development & Deployment\n\n### Local Development\n```bash\n# Install dependencies and build\nnpm install\nnpm start\n```\n\n### Docker Deployment\n```bash\n# Build and run with Docker\ndocker build -t sqlite-mcp-server .\ndocker run -d --name sqlite-mcp sqlite-mcp-server\n```\n\n### Nixpacks Deployment\n\nThe application can be easily deployed using Nixpacks with platforms like Railway, Coolify, or Render:\n\n```bash\n# Deploy with Nixpacks\nnixpacks build . --name sqlite-mcp-server\n```\n\nNo additional configuration is needed as the project includes a Dockerfile.\n\n## Available Tools\n\n1. `read_query`: Execute SELECT queries\n2. `write_query`: Execute INSERT, UPDATE, or DELETE queries\n3. `create_table`: Create new tables\n4. `list_tables`: List all tables in the database\n5. `describe_table`: View schema information for a table\n6. `append_insight`: Add business insights to the memo\n\n## Remote Server Connection\n\nTo connect using SSE in n8n:\n\n1. Add an MCP Client node\n2. Configure SSE connection:\n   - SSE URL: `http://localhost:3000/sse`\n   - Messages Post Endpoint: `http://localhost:3000/messages`\n   - No additional headers required\n\n## Example Usage\n\n```typescript\n// Create a table\nawait callTool('create_table', {\n  query: 'CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)'\n});\n\n// Insert data\nawait callTool('write_query', {\n  query: 'INSERT INTO users (name) VALUES (\"John Doe\")'\n});\n\n// Query data\nconst result = await callTool('read_query', {\n  query: 'SELECT * FROM users'\n});\n```\n\n## Environment Variables\n\nNone required by default. If using file-based storage, modify the database path in `src/index.ts`.\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sqlite",
        "databases",
        "database",
        "sqlite mcp",
        "isaacgounton sqlite",
        "sqlite database"
      ],
      "category": "databases"
    },
    "isaacwasserman--mcp-snowflake-server": {
      "owner": "isaacwasserman",
      "name": "mcp-snowflake-server",
      "url": "https://github.com/isaacwasserman/mcp-snowflake-server",
      "imageUrl": "/freedevtools/mcp/pfp/isaacwasserman.webp",
      "description": "Provides database interaction with Snowflake, enabling SQL queries and aggregating data insights. Exposes schema context as resources for enhanced data accessibility.",
      "stars": 165,
      "forks": 75,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-10-01T06:39:50Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/isaacwasserman-mcp-snowflake-server-badge.png)](https://mseep.ai/app/isaacwasserman-mcp-snowflake-server)\n\n# Snowflake MCP Server\n---\n\n## Overview\n\nA Model Context Protocol (MCP) server implementation that provides database interaction with Snowflake. This server enables running SQL queries via tools and exposes data insights and schema context as resources.\n\n---\n\n## Components\n\n### Resources\n\n- **`memo://insights`**  \n  A continuously updated memo aggregating discovered data insights.  \n  Updated automatically when new insights are appended via the `append_insight` tool.\n\n- **`context://table/{table_name}`**  \n  (If prefetch enabled) Per-table schema summaries, including columns and comments, exposed as individual resources.\n\n---\n\n### Tools\n\nThe server exposes the following tools:\n\n#### Query Tools\n\n- **`read_query`**  \n  Execute `SELECT` queries to read data from the database.  \n  **Input:**\n\n  - `query` (string): The `SELECT` SQL query to execute  \n    **Returns:** Query results as array of objects\n\n- **`write_query`** (enabled only with `--allow-write`)  \n  Execute `INSERT`, `UPDATE`, or `DELETE` queries.  \n  **Input:**\n\n  - `query` (string): The SQL modification query  \n    **Returns:** Number of affected rows or confirmation\n\n- **`create_table`** (enabled only with `--allow-write`)  \n  Create new tables in the database.  \n  **Input:**\n  - `query` (string): `CREATE TABLE` SQL statement  \n    **Returns:** Confirmation of table creation\n\n#### Schema Tools\n\n- **`list_databases`**  \n  List all databases in the Snowflake instance.  \n  **Returns:** Array of database names\n\n- **`list_schemas`**  \n  List all schemas within a specific database.  \n  **Input:**\n\n  - `database` (string): Name of the database  \n    **Returns:** Array of schema names\n\n- **`list_tables`**  \n  List all tables within a specific database and schema.  \n  **Input:**\n\n  - `database` (string): Name of the database\n  - `schema` (string): Name of the schema  \n    **Returns:** Array of table metadata\n\n- **`describe_table`**  \n  View column information for a specific table.  \n  **Input:**\n  - `table_name` (string): Fully qualified table name (`database.schema.table`)  \n    **Returns:** Array of column definitions with names, types, nullability, defaults, and comments\n\n#### Analysis Tools\n\n- **`append_insight`**  \n  Add new data insights to the memo resource.  \n  **Input:**\n  - `insight` (string): Data insight discovered from analysis  \n    **Returns:** Confirmation of insight addition  \n    **Effect:** Triggers update of `memo://insights` resource\n\n---\n\n## Usage with Claude Desktop\n\n### Installing via Smithery\n\nTo install Snowflake Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp_snowflake_server):\n\n```bash\nnpx -y @smithery/cli install mcp_snowflake_server --client claude\n```\n\n---\n\n### Installing via UVX\n\n#### Traditional Configuration (Individual Parameters)\n\n```json\n\"mcpServers\": {\n  \"snowflake_pip\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"--python=3.12\",  // Optional: specify Python version <=3.12\n      \"mcp_snowflake_server\",\n      \"--account\", \"your_account\",\n      \"--warehouse\", \"your_warehouse\",\n      \"--user\", \"your_user\",\n      \"--password\", \"your_password\",\n      \"--role\", \"your_role\",\n      \"--database\", \"your_database\",\n      \"--schema\", \"your_schema\"\n      // Optionally: \"--private_key_path\", \"your_private_key_absolute_path\"\n      // Optionally: \"--allow_write\"\n      // Optionally: \"--log_dir\", \"/absolute/path/to/logs\"\n      // Optionally: \"--log_level\", \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n      // Optionally: \"--exclude_tools\", \"{tool_name}\", [\"{other_tool_name}\"]\n    ]\n  }\n}\n```\n\n#### TOML Configuration (Recommended)\n\n```json\n\"mcpServers\": {\n  \"snowflake_production\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"--python=3.12\",\n      \"mcp_snowflake_server\",\n      \"--connections-file\", \"/path/to/snowflake_connections.toml\",\n      \"--connection-name\", \"production\"\n      // Optionally: \"--allow_write\"\n      // Optionally: \"--log_dir\", \"/absolute/path/to/logs\"\n      // Optionally: \"--log_level\", \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n      // Optionally: \"--exclude_tools\", \"{tool_name}\", [\"{other_tool_name}\"]\n    ]\n  },\n  \"snowflake_staging\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"--python=3.12\",\n      \"mcp_snowflake_server\",\n      \"--connections-file\", \"/path/to/snowflake_connections.toml\",\n      \"--connection-name\", \"staging\"\n    ]\n  }\n}\n```\n\n---\n\n### Installing Locally\n\n1. Install [Claude AI Desktop App](https://claude.ai/download)\n\n2. Install `uv`:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n3. Create a `.env` file with your Snowflake credentials:\n\n```bash\nSNOWFLAKE_USER=\"xxx@your_email.com\"\nSNOWFLAKE_ACCOUNT=\"xxx\"\nSNOWFLAKE_ROLE=\"xxx\"\nSNOWFLAKE_DATABASE=\"xxx\"\nSNOWFLAKE_SCHEMA=\"xxx\"\nSNOWFLAKE_WAREHOUSE=\"xxx\"\nSNOWFLAKE_PASSWORD=\"xxx\"\nSNOWFLAKE_PASSWORD=\"xxx\"\nSNOWFLAKE_PRIVATE_KEY_PATH=/absolute/path/key.p8\n# Alternatively, use external browser authentication:\n# SNOWFLAKE_AUTHENTICATOR=\"externalbrowser\"\n```\n\n4. [Optional] Modify `runtime_config.json` to set exclusion patterns for databases, schemas, or tables.\n\n5. Test locally:\n\n```bash\nuv --directory /absolute/path/to/mcp_snowflake_server run mcp_snowflake_server\n```\n\n6. Add the server to your `claude_desktop_config.json`:\n\n#### Traditional Configuration (Using Environment Variables)\n\n```json\n\"mcpServers\": {\n  \"snowflake_local\": {\n    \"command\": \"/absolute/path/to/uv\",\n    \"args\": [\n      \"--python=3.12\",  // Optional\n      \"--directory\", \"/absolute/path/to/mcp_snowflake_server\",\n      \"run\", \"mcp_snowflake_server\"\n      // Optionally: \"--allow_write\"\n      // Optionally: \"--log_dir\", \"/absolute/path/to/logs\"\n      // Optionally: \"--log_level\", \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n      // Optionally: \"--exclude_tools\", \"{tool_name}\", [\"{other_tool_name}\"]\n    ]\n  }\n}\n```\n\n#### TOML Configuration (Recommended)\n\n```json\n\"mcpServers\": {\n  \"snowflake_local\": {\n    \"command\": \"/absolute/path/to/uv\",\n    \"args\": [\n      \"--python=3.12\",\n      \"--directory\", \"/absolute/path/to/mcp_snowflake_server\",\n      \"run\", \"mcp_snowflake_server\",\n      \"--connections-file\", \"/absolute/path/to/snowflake_connections.toml\",\n      \"--connection-name\", \"development\"\n      // Optionally: \"--allow_write\"\n      // Optionally: \"--log_dir\", \"/absolute/path/to/logs\"\n      // Optionally: \"--log_level\", \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n      // Optionally: \"--exclude_tools\", \"{tool_name}\", [\"{other_tool_name}\"]\n    ]\n  }\n}\n```\n\n---\n\n## Notes\n\n- By default, **write operations are disabled**. Enable them explicitly with `--allow-write`.\n- The server supports filtering out specific databases, schemas, or tables via exclusion patterns.\n- The server exposes additional per-table context resources if prefetching is enabled.\n- The `append_insight` tool updates the `memo://insights` resource dynamically.\n\n---\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "access schema",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "iskakaushik--mcp-clickhouse": {
      "owner": "iskakaushik",
      "name": "mcp-clickhouse",
      "url": "https://github.com/iskakaushik/mcp-clickhouse",
      "imageUrl": "/freedevtools/mcp/pfp/iskakaushik.webp",
      "description": "Execute SQL queries on a ClickHouse cluster, list all databases, and retrieve tables within a specified database.",
      "stars": 3,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-01-06T22:46:21Z",
      "readme_content": "# ClickHouse MCP Server\n\n# Migrated to https://github.com/ClickHouse/mcp-clickhouse\n\n\nAn MCP server for ClickHouse.\n\n## Features\n\n### Tools\n\n* `run_select_query`\n  - Execute SQL queries on your ClickHouse cluster.\n  - Input: `sql` (string): The SQL query to execute.\n  - All ClickHouse queries are run with `readonly = 1` to ensure they are safe.\n\n* `list_databases`\n  - List all databases on your ClickHouse cluster.\n\n* `list_tables`\n  - List all tables in a database.\n  - Input: `database` (string): The name of the database.\n\n## Configuration\n\n> **Note**: This is a temporary configuration process that will be significantly improved once the package is published.\n\n1. Run `uv sync` to install the dependencies. To install `uv` follow the instructions [here](https://docs.astral.sh/uv/). Then do `source .venv/bin/activate`.\n\n2. Setup the `.env.production` file with the ClickHouse credentials.\n\n```\nCLICKHOUSE_HOST=<CLICKHOUSE_HOST>\nCLICKHOUSE_PORT=<CLICKHOUSE_PORT>\nCLICKHOUSE_USER=<CLICKHOUSE_USER>\nCLICKHOUSE_PASSWORD=<CLICKHOUSE_PASSWORD>\n```\n\n3. Run `fastmcp install mcp_clickhouse/mcp_server.py -f .env.production` to install the server.\n\n4. Restart Claude Desktop.\n\n\n## Development\n\n1. In `test-services` directory run `docker compose up -d` to start the ClickHouse cluster.\n\n2. Add the following variables to a `.env` file in the root of the repository.\n\n```\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_PORT=8123\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=clickhouse\n```\n\n3. Run `uv sync` to install the dependencies. To install `uv` follow the instructions [here](https://docs.astral.sh/uv/). Then do `source .venv/bin/activate`.\n\n4. For easy testing, you can run `fastmcp dev mcp_clickhouse/mcp_server.py` to start the MCP server.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "queries",
        "databases secure",
        "database access",
        "secure database"
      ],
      "category": "databases"
    },
    "j3k0--mcp-brain-tools": {
      "owner": "j3k0",
      "name": "mcp-brain-tools",
      "url": "https://github.com/j3k0/mcp-brain-tools",
      "imageUrl": "/freedevtools/mcp/pfp/j3k0.webp",
      "description": "Utilizes a scalable knowledge graph built on Elasticsearch to manage and query large datasets, providing persistent memory for AI systems. Supports complete CRUD operations and offers advanced search capabilities for improved data handling in Model Context Protocol applications.",
      "stars": 16,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T07:07:56Z",
      "readme_content": "# MCP Memory: Persistent Memory for AI Conversations 🧠\n\n![Version](https://img.shields.io/badge/version-1.0.0-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![Elasticsearch](https://img.shields.io/badge/Elasticsearch-7.x-yellow)\n![Node](https://img.shields.io/badge/node-18+-green)\n\n> **Give your AI a memory that persists across conversations.** Never lose important context again.\n\nMCP Memory is a robust, Elasticsearch-backed knowledge graph system that gives AI models persistent memory beyond the limits of their context windows. Built for the Model Context Protocol (MCP), it ensures your LLMs remember important information forever, creating more coherent, personalized, and effective AI conversations.\n\n<p align=\"center\">\n  <img src=\"https://via.placeholder.com/800x400?text=MCP+Memory+Visualization\" alt=\"MCP Memory Visualization\" width=\"600\">\n</p>\n\n## 🌟 Why AI Models Need Persistent Memory\n\nEver experienced these frustrations with AI assistants?\n\n- Your AI forgetting crucial details from earlier conversations\n- Having to repeat the same context every time you start a new chat\n- Losing valuable insights once the conversation history fills up\n- Inability to reference past work or decisions\n\nMCP Memory solves these problems by creating a structured, searchable memory store that preserves context indefinitely. Your AI can now build meaningful, long-term relationships with users and maintain coherence across days, weeks, or months of interactions.\n\n## ✨ Key Features\n\n- **📊 Persistent Memory**: Store and retrieve information across multiple sessions\n- **🔍 Smart Search**: Find exactly what you need with powerful Elasticsearch queries\n- **📓 Contextual Recall**: AI automatically prioritizes relevant information based on the conversation\n- **🧩 Relational Understanding**: Connect concepts with relationships that mimic human associative memory\n- **🔄 Long-term / Short-term Memory**: Distinguish between temporary details and important knowledge\n- **🗂️ Memory Zones**: Organize information into separate domains (projects, clients, topics)\n- **🔒 Reliable & Scalable**: Built on Elasticsearch for enterprise-grade performance\n\n## 🚀 5-Minute Setup\n\nGetting started is incredibly simple:\n\n### Prerequisites\n\n- **Docker**: Required for running Elasticsearch (or a local Elasticsearch installation)\n- **Node.js**: Version 18 or higher\n- **npm**: For package management\n\n```bash\n# 1. Clone the repository\ngit clone https://github.com/mcp-servers/mcp-servers.git\ncd mcp-servers/memory\n\n# 2. Install dependencies\nnpm install\n\n# 3. Start Elasticsearch (uses Docker)\nnpm run es:start\n# Note: If you prefer to use your own Elasticsearch installation,\n# set the ES_NODE environment variable to point to your Elasticsearch instance\n\n# 4. Build the project\nnpm run build\n```\n\n### 🔌 Connecting to Claude Desktop\n\nMCP Memory is designed to work seamlessly with Claude Desktop, giving Claude persistent memory across all your conversations:\n\n1. **Copy and configure the launch script**:\n   \n   The repository includes a `launch.example` file that you can simply copy:\n   \n   ```bash\n   # Copy the example launch file\n   cp launch.example launch.sh\n   \n   # Edit launch.sh to add your Groq API key\n   # This is required for smart memory retrieval\n   nano launch.sh  # or use your preferred editor\n   ```\n   \n   Make the script executable:\n   ```bash\n   chmod +x launch.sh\n   ```\n\n2. **Add the command to Claude Desktop**:\n   - Open Claude Desktop Settings\n   - Navigate to the \"Commands\" section\n   - Click \"Add New Command\"\n   - Configure as follows:\n     - **Name**: MCP Memory\n     - **Command**: /path/to/mcp-servers/memory/launch.sh\n     - **Arguments**: Leave empty\n     - **Run in background**: Yes\n     - **Show in menu**: Yes\n\n3. **Verify connection**:\n   - Start the command from Claude Desktop\n   - You should see a notification that Claude is connected to MCP Memory\n   - Try asking Claude about something you discussed in a previous conversation!\n\nFor complete examples and visual guides, see the [Claude Desktop MCP Server Setup Guide](https://github.com/anthropic-claude/claude-desktop-mcp-examples) online.\n\n## 💡 How It Works\n\nMCP Memory creates a structured knowledge graph where:\n\n1. **Entities** represent people, concepts, projects, or anything worth remembering\n2. **Relations** connect entities, creating a network of associations\n3. **Observations** capture specific details about entities\n4. **Relevance scoring** determines what information to prioritize\n\nWhen integrated with an LLM, the system automatically:\n- Stores new information learned during conversations\n- Retrieves relevant context when needed\n- Builds connections between related concepts\n- Forgets unimportant details while preserving critical knowledge\n\n## 🛠️ Example: How Agents Use Memory\n\n### From the User's Perspective\n\n**Conversation 1: Initial Information**\n\n```\nUser: My daughter's name is Emma and she's turning 8 next month.\n\nClaude: That's wonderful! Emma will be turning 8 next month. Is there anything special you're planning for her birthday?\n(Behind the scenes: Claude stores \"Emma\" as an entity of type \"Person\" with observations about her age and birthday timing)\n```\n\n**Conversation 2: Days or Weeks Later**\n\n```\nUser: I'm thinking about what to get for a gift.\n\nClaude: If you're looking for gift ideas for Emma's 8th birthday coming up, have you considered what her current interests are? At that age, many children enjoy creative items, books, or educational games.\n(Behind the scenes: Claude retrieved the entity \"Emma\", her age, and the fact that her birthday is approaching)\n```\n\n**Conversation 3: After the Birthday**\n\n```\nUser: The party went really well on Saturday!\n\nClaude: I'm glad Emma's 8th birthday party was a success on Saturday! Did she enjoy the gifts she received?\n(Behind the scenes: Claude updates the observations about Emma, noting that her birthday party occurred on a Saturday and went well)\n```\n\n### How the Agent Uses Memory\n\nWhen the user mentions something important, the agent:\n\n1. **Recognizes important information** worth remembering\n2. **Stores it in memory** by creating entities, relations, and observations\n3. **Updates existing information** when new details emerge\n\nWhen the user mentions something related to stored information, the agent:\n\n1. **Searches memory** for relevant context based on the current conversation\n2. **Retrieves important details** that might be helpful\n3. **Incorporates this information** naturally into its responses\n\nThis happens automatically - the user simply has a normal conversation with the assistant, and the memory system works behind the scenes to maintain context across sessions.\n\n### Intelligent Entity Management\n\nMCP Memory includes smart handling of entity creation and updates:\n\n- When attempting to create an entity that already exists, the system returns the existing entity data with guidance on how to extend it with new information\n- The system intelligently differentiates between creating new entities and updating existing ones\n- Entity relationships are automatically maintained even when information is updated\n\n## 🧰 Admin Tools\n\nMCP Memory includes a comprehensive admin CLI for maintaining your knowledge graph:\n\n```bash\n# Search the memory\nnode dist/admin-cli.js search \"Emma birthday\"\n\n# View details about a specific entity\nnode dist/admin-cli.js entity \"Emma\"\n\n# Back up your entire memory system\nnode dist/admin-cli.js backup memory-backup.json\n```\n\n## 📚 Advanced Features\n\n### Memory Zones\n\nOrganize knowledge into separate domains:\n\n```bash\n# Create a project-specific memory zone\nnode dist/admin-cli.js zones add client-acme \"ACME Corp Project Knowledge\"\n\n# Import data into that zone\nnode dist/admin-cli.js import acme-data.json client-acme\n```\n\n### Conversational Memory Management\n\nYou can also instruct the assistant to organize memories in different zones through natural conversation:\n\n**Creating and Using Memory Zones**\n\n```\nUser: Claude, can you create a new memory zone called \"work-project\" for storing information about my current work project?\n\nClaude: I've created a new memory zone called \"work-project\". I'll store information related to your work project in this zone. What would you like me to remember about your project?\n\nUser: The project deadline is March 15th, and we're building a new inventory management system.\n\nClaude: I've stored information about your work project in the \"work-project\" zone, including the March 15th deadline and that you're building an inventory management system. I'll keep future information about this project organized in this zone.\n(Behind the scenes: Claude creates a new memory zone and stores project information specifically in that zone)\n```\n\n**Retrieving Zone-Specific Information**\n\n```\nUser: What do you remember about my work project?\n\nClaude: Based on what we've discussed about your work project, I recall that you're building an inventory management system with a deadline of March 15th.\n(Behind the scenes: Claude specifically searches the \"work-project\" zone for relevant information)\n```\n\n**Switching Between Memory Zones**\n\n```\nUser: Claude, let's switch to my \"cooking\" memory zone now. I want to discuss a recipe.\n\nClaude: I've switched to your \"cooking\" memory zone. What recipe would you like to discuss today?\n(Behind the scenes: Claude updates its memory context to prioritize searching in the \"cooking\" zone)\n\nUser: What was that pasta dish I told you about last month?\n\nClaude: From our previous conversations in your cooking zone, you shared a carbonara recipe that used pancetta, eggs, pecorino cheese, and black pepper. You mentioned it was a family favorite that you learned during your trip to Rome.\n(Behind the scenes: Claude searches specifically in the \"cooking\" zone to find this information)\n```\n\nBy organizing memory into separate zones, conversations become more relevant and focused on the current topic or project.\n\n### Search Capabilities\n\nLeverage Elasticsearch's powerful search features:\n\n```bash\n# Fuzzy search (finds \"meeting\" even with typo)\nnode dist/admin-cli.js search \"meteing notes\"\n\n# Zone-specific search\nnode dist/admin-cli.js search \"budget\" client-acme\n```\n\n## 🤝 Contributing\n\nContributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## 📝 License\n\nMIT\n\n---\n\n<p align=\"center\">\n  <b>Ready to give your AI a memory that lasts? Get started in 5 minutes!</b><br>\n  <a href=\"https://github.com/mcp-servers/mcp-servers\">GitHub</a> •\n  <a href=\"https://discord.gg/mcp-community\">Discord</a> •\n  <a href=\"https://mcp-servers.readthedocs.io\">Documentation</a>\n</p>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "elasticsearch",
        "database",
        "enables querying",
        "secure database",
        "access schema"
      ],
      "category": "databases"
    },
    "j4c0bs--mcp-server-sql-analyzer": {
      "owner": "j4c0bs",
      "name": "mcp-server-sql-analyzer",
      "url": "https://github.com/j4c0bs/mcp-server-sql-analyzer",
      "imageUrl": "/freedevtools/mcp/pfp/j4c0bs.webp",
      "description": "Provides SQL analysis, linting, and dialect conversion using SQLGlot, validating SQL syntax and facilitating conversion between different SQL dialects.",
      "stars": 25,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-28T08:17:11Z",
      "readme_content": "# mcp-server-sql-analyzer\n\nA Model Context Protocol (MCP) server that provides SQL analysis, linting, and dialect conversion capabilities using [SQLGlot](https://sqlglot.com/sqlglot.html).\n\n## Overview\n\nThe SQL Analyzer MCP server provides tools for analyzing and working with SQL queries. It helps with:\n\n- SQL syntax validation and linting\n- Converting queries between different SQL dialects (e.g., MySQL to PostgreSQL)\n- Extracting and analyzing table references and dependencies\n- Identifying column usage and relationships\n- Discovering supported SQL dialects\n\n### How Claude Uses This Server\n\nAs an AI assistant, this server enhances my ability to help users work with SQL efficiently by:\n\n1. **Query Validation**: I can instantly validate SQL syntax before suggesting it to users, ensuring I provide correct and dialect-appropriate queries.\n\n2. **Dialect Conversion**: When users need to migrate queries between different database systems, I can accurately convert the syntax while preserving the query's logic.\n\n3. **Code Analysis**: The table and column reference analysis helps me understand complex queries, making it easier to explain query structure and suggest optimizations.\n\n4. **Compatibility Checking**: By knowing the supported dialects and their specific features, I can guide users toward database-specific best practices.\n\nThis toolset allows me to provide more accurate and helpful SQL-related assistance while reducing the risk of syntax errors or dialect-specific issues.\n\n### Tips\n\nUpdate your personal preferences in Claude Desktop settings to request that generated SQL is first validated using the `lint_sql` tool.\n\n## Tools\n\n1. lint_sql\n   - Validates SQL query syntax and returns any errors\n   - Input:\n     - sql (string): SQL query to analyze\n     - dialect (string, optional): SQL dialect (e.g., 'mysql', 'postgres')\n   - Returns: ParseResult containing:\n     - is_valid (boolean): Whether the SQL is valid\n     - message (string): Error message or \"No syntax errors\"\n     - position (object, optional): Line and column of error if present\n\n2. transpile_sql\n   - Converts SQL between different dialects\n   - Inputs:\n     - sql (string): SQL statement to transpile\n     - read_dialect (string): Source SQL dialect\n     - write_dialect (string): Target SQL dialect\n   - Returns: TranspileResult containing:\n     - is_valid (boolean): Whether transpilation succeeded\n     - message (string): Error message or success confirmation\n     - sql (string): Transpiled SQL if successful\n\n3. get_all_table_references\n   - Extracts table and CTE references from SQL\n   - Inputs:\n     - sql (string): SQL statement to analyze\n     - dialect (string, optional): SQL dialect\n   - Returns: TableReferencesResult containing:\n     - is_valid (boolean): Whether analysis succeeded\n     - message (string): Status message\n     - tables (array): List of table references with type, catalog, database, table name, alias, and fully qualified name\n\n4. get_all_column_references\n   - Extracts column references with table context\n   - Inputs:\n     - sql (string): SQL statement to analyze\n     - dialect (string, optional): SQL dialect\n   - Returns: ColumnReferencesResult containing:\n     - is_valid (boolean): Whether analysis succeeded\n     - message (string): Status message\n     - columns (array): List of column references with column name, table name, and fully qualified name\n\n## Resources\n\n### SQL Dialect Discovery\n\n```\ndialects://all\n```\n\nReturns a list of all supported SQL dialects for use in all tools.\n\n## Configuration\n\n### Using uvx (recommended)\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n      \"sql-analyzer\": {\n          \"command\": \"uvx\",\n          \"args\": [\n              \"--from\",\n              \"git+https://github.com/j4c0bs/mcp-server-sql-analyzer.git\",\n              \"mcp-server-sql-analyzer\"\n          ]\n      }\n  }\n}\n```\n\n### Using uv\n\nAfter cloning this repo, add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n      \"sql-analyzer\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/path/to/mcp-server-sql-analyzer\",\n              \"run\",\n              \"mcp-server-sql-analyzer\"\n          ]\n      }\n  }\n}\n```\n\n## Development\n\nTo run the server in development mode:\n\n```bash\n# Clone the repository\ngit clone git@github.com:j4c0bs/mcp-server-sql-analyzer.git\n\n# Run the server\nnpx @modelcontextprotocol/inspector uv --directory /path/to/mcp-server-sql-analyzer run mcp-server-sql-analyzer\n```\n\nTo run unit tests:\n\n```bash\nuv run pytest .\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "analyzer",
        "sql analyzer",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "jbdamask--cursor-db-mcp": {
      "owner": "jbdamask",
      "name": "cursor-db-mcp",
      "url": "https://github.com/jbdamask/cursor-db-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jbdamask.webp",
      "description": "Access and interact with SQLite databases from Cursor IDE. Retrieve project data, chat history, and composer information for enhanced data analysis and querying.",
      "stars": 21,
      "forks": 2,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-09-12T19:32:58Z",
      "readme_content": "# Cursor DB MCP Server\n\nA Model Context Protocol (MCP) server for accessing Cursor IDE's SQLite databases. This server allows AI assistants to explore and interact with Cursor's project data, chat history, and composer information.\n\n<!-- __Claude__\n![In Claude GIF](./img/cursor-db-mcp-claude.gif) -->\n\n__Cursor__\n\n\n\n## Prerequisites\n\nCursor IDE\n<!-- Claude Desktop (if you want to use MCP in Claude) -->\n\n## Installation\n\n### Easy Installation\n\nUse the provided installation script to install all dependencies:\n\n```bash\npython install.py\n```\n\nThis script will install:\n- Basic MCP server and dependencies\n\n<!-- ### Manual Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/yourusername/cursor-db-mcp.git\ncd cursor-db-mcp\n```\n\n2. Install basic dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Install MCP CLI tools (optional, for testing):\n```bash\npip install 'mcp[cli]'  # Note the quotes around mcp[cli]\n```\n\nIf the above command fails, you can install the CLI dependencies directly:\n```bash\npip install typer rich\n``` -->\n\n<!-- ## Usage\n\n### Using with Claude Desktop\n\n1. Install the MCP server in Claude Desktop:\n```bash\nmcp install cursor-db-mcp-server.py\n```\n\n2. In Claude Desktop, you can now access your Cursor data by asking questions like:\n   - \"Show me a list of my Cursor projects\"\n   - \"What's in my chat history for project X?\"\n   - \"Find composer data for composer ID Y\"\n\n   See detailed examples below\n\nNote: If Claude shows an error connecting to this MCP it's likely because it can't find uv. To fix this, change the command value to include the fully qualified path to uv. For example:\n```\n    \"Cursor DB Manager\": {\n      \"command\": \"/Users/johndamask/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"/Users/johndamask/code/cursor-db-mcp/cursor-db-mcp-server.py\"\n      ]\n    }\n``` -->\n\n## Using with Cursor IDE\n\n1. Open Cursor and navigate to Settings->Cursor Settings->MCP. \n2. Click: Add new MCP server\n3. Name: Cursor DB MCP; Type: Command\n4. Command: \\<fully qualified path to\\>uv run --with mcp[cli] mcp run \\<fully qualified path to\\>/cursor-db-mcp-server.py \n\n\n\nNow you can ask questions about the database or retrieve info about historical chats.\n\n\n\n\n\n### Using with Claude Desktop \n\n[Installing MCP servers for Claude Desktop](https://modelcontextprotocol.io/quickstart/user)\n\nAdd this to your claude_desktop_config.json file\n```\n    \"cursor-db-mcp\": {\n      \"command\": \"<fully qualified path to >/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"<fully qualified path to >/cursor-db-mcp-server.py\"\n      ]\n    }\n```\n\n\n\n\n## Available Resources\n\n- `cursor://projects` - List all available Cursor projects\n- `cursor://projects/detailed` - List projects with detailed information\n- `cursor://projects/{project_name}/chat` - Get chat data for a specific project\n- `cursor://projects/{project_name}/composers` - Get composer IDs for a specific project\n- `cursor://composers/{composer_id}` - Get data for a specific composer\n\n## Available Tools\n\n- `query_table` - Query a specific table in a project's database\n- `refresh_databases` - Refresh the list of database paths\n\n\n<!-- # Example Usage with Claude\n\n## Listing Projects\n\n**User**: \"Can you show me a list of my Cursor projects?\"\n\n**Claude**: \n```\nI'll retrieve a list of your Cursor projects.\n\nHere are the Cursor projects I found:\n- project1 (path: /Users/username/Library/Application Support/Cursor/User/workspaceStorage/abc123/state.vscdb)\n- project2 (path: /Users/username/Library/Application Support/Cursor/User/workspaceStorage/def456/state.vscdb)\n- custom-project (path: /Users/username/code/custom-project/state.vscdb)\n```\n\n## Viewing Chat History\n\n**User**: \"Show me the chat history from my 'project1' project.\"\n\n**Claude**:\n```\nI'll retrieve the chat history from your 'project1' project.\n\nHere's the chat history I found:\n\nChat: \"Understanding React Hooks\"\n- You: \"Can you explain how useEffect works in React?\"\n- Claude: \"The useEffect hook in React is used for handling side effects in functional components...\"\n\nChat: \"Debugging API Connection\"\n- You: \"I'm getting a 401 error when connecting to the API. Here's my code...\"\n- Claude: \"It looks like you're missing the authorization header in your request...\"\n```\n\n## Querying Database Tables\n\n**User**: \"Can you search for keys related to 'settings' in the ItemTable of my 'project1' project?\"\n\n**Claude**:\n```\nI'll search for keys related to 'settings' in the ItemTable of your 'project1' project.\n\nHere are the results:\n1. Key: \"workbench.settings.editor\"\n   Value: {\"mode\": \"ui\", \"lastFocusedSettingId\": \"editor.fontSize\"}\n\n2. Key: \"settings.language\"\n   Value: \"en\"\n\n3. Key: \"settings.theme\"\n   Value: \"dark\"\n```\n\n## Analyzing Chat Data\n\n**User**: \"Can you analyze the chat data from my 'project1' project and tell me what topics I've been discussing most frequently?\"\n\n**Claude**:\n```\nI'll analyze the chat data from your 'project1' project.\n\nBased on your chat history, here are the most frequent topics you've been discussing:\n1. React Hooks and Components (5 conversations)\n2. API Integration (3 conversations)\n3. CSS Styling (2 conversations)\n4. Performance Optimization (2 conversations)\n5. Debugging (1 conversation)\n\nThe most common questions were about state management in React and handling API responses.\n``` \n\n# Architecture\n\nThe server uses the Model Context Protocol (MCP) to expose Cursor's SQLite databases to AI assistants. Key components include:\n\n1. **Lifespan Context Management**: The server uses MCP's lifespan API to efficiently manage resources throughout the server's lifecycle.\n\n2. **CursorDBManager**: Handles the detection and management of Cursor projects and their databases.\n\n3. **Resources**: Expose data from Cursor databases as MCP resources.\n\n4. **Tools**: Provide functionality to query databases and manage projects.\n\n5. **Prompts**: Define reusable templates for AI interactions. -->\n\n# How It Works\n\nThe server scans your Cursor installation directory to find project databases (state.vscdb files). It then exposes these databases through MCP resources and tools, allowing AI assistants to query and analyze the data.\n\n# Notes\n1. Cursor stores AI conversations in different places. Increasingly, chats are stored as \"composerData\" under globalStorage/state.vscdb. If you don't get results when asking about chats for recent projects, try asking for composers.\n2. This was written on a Mac. YMMV with other OS\n\n# Shameless Plug\n\n\nLike this? Try [Cursor Journal](https://medium.com/@jbdamask/building-cursor-journal-with-cursor-77445026a08c) to create DevLogs directly from Cursor chat history!\n\n# License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "sqlite",
        "databases cursor",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "jdbcx--pydbcx-mcp": {
      "owner": "jdbcx",
      "name": "pydbcx-mcp",
      "url": "https://github.com/jdbcx/pydbcx-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Python-based MCP server that facilitates communication with various data sources such as databases and web services through a JDBCX server interface.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbcx",
        "databases",
        "database",
        "jdbcx pydbcx",
        "access jdbcx",
        "jdbcx server"
      ],
      "category": "databases"
    },
    "jimpick--mcp-json-db-collection-server": {
      "owner": "jimpick",
      "name": "mcp-json-db-collection-server",
      "url": "https://github.com/jimpick/mcp-json-db-collection-server",
      "imageUrl": "/freedevtools/mcp/pfp/jimpick.webp",
      "description": "Manage multiple JSON document databases with capabilities for creating, reading, updating, and deleting documents. Sync databases to the cloud for easy access and enable collaboration on structured data.",
      "stars": 0,
      "forks": 4,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2024-12-30T03:03:47Z",
      "readme_content": "# Model Context Protocol and Fireproof Demo: JSON Document Collection Server\n\n<h1>\n\n❤️\n\n</h1>\n\nThis is an example of how to use a [Fireproof](https://fireproof.storage/) database in a [Model Context Protocol](https://github.com/modelcontextprotocol) server (used for plugging code and data into A.I. systems such as [Claude Desktop](https://claude.ai/download)).\n\nThis server:\n\n  * allows the creation of multiple \"JSON Document Databases\" (implemented using Fireproof)\n  * implements basic CRUD operations (Create, Read, Update, Delete) within any database and the ability to query documents sorted by any field.\n  * allows sharing of any database with others using the Fireproof Cloud service and dashboard\n\nPrior art: This is based on https://github.com/fireproof-storage/mcp-database-server - it adds multiple databases plus the ability to sync to the cloud.\n\n## Running the Server\n\nFirst, build it:\n\n```\nnpm install\nnpm build\n```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"json-db-collections\": {\n      \"command\": \"<base-dir>/mcp-json-db-collection-server/build/index.js\"\n    }\n  }\n}\n```\n\n# Sample Claude Desktop chat session\n\nHere is a sample session using Claude Desktop where I create 3 document databases and sync them to the cloud:\n\n## 1. Chat Session: Database of Elements\n\n**human** (Dec 29, 2024, 03:59 PM)\n\ncreate a json document database named elements\n\n**assistant** (Dec 29, 2024, 04:00 PM)\n\n I'll help you create a JSON document database named \"elements\".\n\n\n\nThe database has been created successfully. You can now use this database to store and manage JSON documents. Would you like to add any documents to it?\n\n**human** (Dec 29, 2024, 04:00 PM)\n\nadd noble gases from period table as individual docs\n\n**assistant** (Dec 29, 2024, 04:00 PM)\n\n I'll help you add the noble gases as individual documents to the elements database. I'll include key properties for each noble gas.\n\n\n\nI've added all seven noble gases (Helium, Neon, Argon, Krypton, Xenon, Radon, and Oganesson) to the database. For each element, I included:\n- Name and symbol\n- Atomic number and mass\n- Period, group, and block\n- Electron configuration\n- Melting and boiling points (in Kelvin)\n- Density (in g/cm³)\n\nNote that for Oganesson, some physical properties are marked as null since it's a synthetic element with very short half-life and its properties aren't well-determined experimentally.\n\nWould you like to query any specific information about these noble gases from the database?\n\n**human** (Dec 29, 2024, 04:01 PM)\n\nshow docs\n\n**assistant** (Dec 29, 2024, 04:01 PM)\n\n I'll query all the documents from the elements database, sorted by atomic number.\n\n\n\nHere are all the noble gases in the database, listed in order of atomic number (from highest to lowest):\n\n1. Oganesson (Og) - Element 118\n2. Radon (Rn) - Element 86\n3. Xenon (Xe) - Element 54\n4. Krypton (Kr) - Element 36\n5. Argon (Ar) - Element 18\n6. Neon (Ne) - Element 10\n7. Helium (He) - Element 2\n\nEach document contains detailed information about the element, including its atomic mass, density, melting and boiling points (where known), and electron configuration. Would you like to examine any specific properties or compare certain aspects of these elements?\n\n## 2. Chat Session: Database of Cookie Ingredients\n\n**human** (Dec 29, 2024, 04:01 PM)\n\ncreate json document db called cookie_ingredients\n\n**assistant** (Dec 29, 2024, 04:01 PM)\n\n I'll create a new JSON document database called \"cookie_ingredients\".\n\n\n\nThe database has been created successfully. Would you like to add any ingredients to it?\n\n**human** (Dec 29, 2024, 04:02 PM)\n\nadd ingredients from chocolate chip cookie recipe as individual docs\n\n**assistant** (Dec 29, 2024, 04:02 PM)\n\n I'll add common ingredients for chocolate chip cookies as individual documents with their typical measurements and properties.\n\n\n\nI've added all the essential ingredients for a classic chocolate chip cookie recipe. For each ingredient, I included:\n- Name and amount\n- Unit of measurement\n- Type (dry, wet, leavening, etc.)\n- Purpose in the recipe\n- Possible substitutes\n- Weight measurements\n- Important notes about usage\n\nWould you like to query any specific information about these ingredients from the database?\n\n**human** (Dec 29, 2024, 04:02 PM)\n\nshow ingredients as table\n\n**assistant** (Dec 29, 2024, 04:03 PM)\n\n I'll query the ingredients and format them as a markdown table.\n\n\n\n| Ingredient | Amount | Unit | Type | Purpose | Weight/Unit |\n|------------|--------|------|------|---------|------------|\n| All-Purpose Flour | 2¼ | cups | dry | structure | 120g/cup |\n| Unsalted Butter | 1 | cup | fat | texture and flavor | 227g/cup |\n| Granulated Sugar | ¾ | cup | sweetener | sweetness and texture | 200g/cup |\n| Brown Sugar | ¾ | cup | sweetener | moisture and flavor | 220g/cup |\n| Large Eggs | 2 | whole | binding | structure and richness | 50g each |\n| Vanilla Extract | 1 | teaspoon | flavoring | flavor enhancement | 5g/tsp |\n| Salt | 1 | teaspoon | seasoning | flavor enhancement | 6g/tsp |\n| Baking Soda | 1 | teaspoon | leavening | rise and spread | 4.8g/tsp |\n| Chocolate Chips | 2 | cups | mix-in | flavor and texture | 170g/cup |\n\nI've organized the ingredients by type and included their amounts, units, purposes, and weight measurements. Would you like to see any additional details about specific ingredients?\n\n## 3. Chat Session: Database of Horse Breeds\n\n**human** (Dec 29, 2024, 04:03 PM)\n\ncreate new json document db called horse_breeds\n\n**assistant** (Dec 29, 2024, 04:03 PM)\n\n I'll create a new JSON document database for horse breeds.\n\n\n\nThe database has been created successfully. Would you like to add some horse breeds to it?\n\n**human** (Dec 29, 2024, 04:03 PM)\n\nyes\n\n**assistant** (Dec 29, 2024, 04:04 PM)\n\n I'll add several popular horse breeds with their key characteristics, origins, and typical uses.\n\n\n\nI've added six distinctive horse breeds to the database:\n1. Arabian - known for endurance and refinement\n2. Thoroughbred - famous for racing and athleticism\n3. Clydesdale - a powerful draft horse\n4. Quarter Horse - America's versatile stock horse\n5. Friesian - elegant black horse from the Netherlands\n6. Appaloosa - known for spotted patterns and Native American heritage\n\nEach entry includes information about:\n- Origin and type\n- Height range (in hands - where 1 hand = 4 inches)\n- Common colors\n- Physical characteristics\n- Typical uses\n- Temperament\n- Notable traits\n\nWould you like to see the information presented in a specific way or learn more about any particular breed?\n\n## 4. Chat Session: Sync to Cloud\n\n**human** (Dec 29, 2024, 04:04 PM)\n\nlist json document dbs\n\n**assistant** (Dec 29, 2024, 04:04 PM)\n\n I'll list all the JSON document databases that have been created.\n\n\n\nWe have three JSON document databases:\n1. horse_breeds\n2. elements\n3. cookie_ingredients\n\nEach database contains documents related to its specific theme:\n- horse_breeds has information about different horse breeds\n- elements contains data about noble gases\n- cookie_ingredients has details about chocolate chip cookie ingredients\n\nWould you like to query any specific information from these databases?\n\n**human** (Dec 29, 2024, 04:04 PM)\n\nsync horse_breeds to cloud\n\n**assistant** (Dec 29, 2024, 04:04 PM)\n\n I'll help you connect the horse_breeds database to cloud sync service.\n\n\n\nThe horse_breeds database has been successfully connected to the cloud. You can access the dashboard using the URL provided above. This will allow you to view and manage your horse breeds data through the cloud interface. Would you like to sync any other databases to the cloud as well?\n\n**human** (Dec 29, 2024, 04:05 PM)\n\nshow url\n\n**assistant** (Dec 29, 2024, 04:05 PM)\n\n Here's the dashboard URL for the horse_breeds database:\n\nhttps://dashboard.fireproof.storage/fp/databases/connect?endpoint=fireproof%3A%2F%2Fcloud.fireproof.direct%2F&localName=horse_breeds&remoteName=019414e1-2db0-70aa-b9a4-2e2f78a7adce\n\nYou can use this URL to access and manage the horse breeds data through the cloud interface.\n\n**human** (Dec 29, 2024, 04:05 PM)\n\nsync elements db to cloud\n\n**assistant** (Dec 29, 2024, 04:05 PM)\n\n I'll connect the elements database to the cloud sync service.\n\n\n\nThe elements database has been successfully connected to the cloud. You can access the dashboard at:\n\nhttps://dashboard.fireproof.storage/fp/databases/connect?endpoint=fireproof%3A%2F%2Fcloud.fireproof.direct%2F&localName=elements&remoteName=019414e1-db52-75e7-b8d0-21ce605176a7\n\nWould you like to sync the cookie_ingredients database as well?\n\n**human** (Dec 29, 2024, 04:05 PM)\n\nyes\n\n**assistant** (Dec 29, 2024, 04:05 PM)\n\n I'll connect the cookie_ingredients database to the cloud sync service.\n\n\n\nHere's the dashboard URL for the cookie_ingredients database:\n\nhttps://dashboard.fireproof.storage/fp/databases/connect?endpoint=fireproof%3A%2F%2Fcloud.fireproof.direct%2F&localName=cookie_ingredients&remoteName=019414e2-1f47-72d7-bf8f-a721d29a24c7\n\nNow all three databases (horse_breeds, elements, and cookie_ingredients) are synchronized to the cloud and can be accessed through their respective dashboard URLs.\n\n## 5. Chat Session: Cleanup\n\n**human** (Dec 29, 2024, 04:06 PM)\n\ndelete all databases\n\n**assistant** (Dec 29, 2024, 04:06 PM)\n\n I'll help you delete all three databases.\n\n\n\nAll three databases have been successfully deleted:\n1. horse_breeds\n2. elements\n3. cookie_ingredients\n\nLet me verify that they're all gone.\n\n\n\nConfirmed - there are no remaining databases in the system. Would you like to create any new databases?\n\n**Converted to Markdown thanks to**: https://simonwillison.net/2024/Aug/8/convert-claude-json-to-markdown/\n\n# Future development ideas\n\n* prompts\n* resources\n* multiuser with permissions\n* indexing\n* generate static websites from Claude Desktop using Firebase npm libraries\n\n# Development\n\n```bash\npnpm install\npnpm build\npnpm run inspector\n# To reset, do: rm -rf ~/.fireproof /tmp/dist\n```\n\n`~/Library/Application\\ Support/Claude/claude_desktop_config.json`:\n```\n{\n  \"mcpServers\": {\n    \"json-db-collections\": {\n      \"command\": \"<base-dir>/mcp-json-db-collection-server/build/index.js\"\n    }\n  }\n}\n```\n\n# License\n\nMIT or Apache 2",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "db",
        "secure database",
        "databases secure",
        "databases capabilities"
      ],
      "category": "databases"
    },
    "jjikky--dynamo-readonly-mcp": {
      "owner": "jjikky",
      "name": "dynamo-readonly-mcp",
      "url": "https://github.com/jjikky/dynamo-readonly-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jjikky.webp",
      "description": "Query AWS DynamoDB databases using natural language requests, facilitating data retrieval and management through a straightforward interface. Integrate easily with LLMs to enhance data interactions.",
      "stars": 0,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-23T23:36:30Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jjikky-dynamo-readonly-mcp-badge.png)](https://mseep.ai/app/jjikky-dynamo-readonly-mcp)\n\n# DynamoDB Read-Only MCP\n\n[![npm version](https://badge.fury.io/js/dynamo-readonly-mcp.svg)](https://badge.fury.io/js/dynamo-readonly-mcp) [![smithery badge](https://smithery.ai/badge/@jjikky/dynamo-readonly-mcp)](https://smithery.ai/server/@jjikky/dynamo-readonly-mcp)\n\nA server that utilizes the Model Context Protocol (MCP) to query AWS DynamoDB databases. This server allows LLMs like Claude to query DynamoDB data through natural language requests.\n\n<a href=\"https://glama.ai/mcp/servers/@jjikky/dynamo-readonly-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@jjikky/dynamo-readonly-mcp/badge\" alt=\"DynamoDB Read-Only MCP server\" />\n</a>\n\n## Features\n\nThis MCP server provides the following features:\n\n- **Table Management Tools**:\n  - `list-tables`: View a list of all DynamoDB tables\n  - `describe-table`: View detailed information about a specific table\n- **Data Query Tools**:\n  - `scan-table`: Scan all or part of a table's data\n  - `query-table`: Search for data that matches specific conditions in a table\n  - `paginate-query-table`: Retrieve data across multiple pages that matches specific conditions\n  - `get-item`: Retrieve an item with a specific key\n  - `count-items`: Calculate the number of items in a table\n- **Resources**:\n  - `dynamodb-tables-info`: A resource that provides metadata for all tables\n  - `dynamodb-table-schema`: A resource that provides schema information for a specific table\n- **Prompts**:\n  - `dynamodb-query-help`: A help prompt for writing DynamoDB queries\n\n## Installation and Execution\n\nYou can run it without installation using the `Run with NPX` method below.\n\n### Installing via Smithery\n\nTo install DynamoDB Read-Only Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jjikky/dynamo-readonly-mcp):\n\n```bash\nnpx -y @smithery/cli install @jjikky/dynamo-readonly-mcp --client claude\n```\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/jjikky/dynamo-readonly-mcp.git\n   cd dynamo-readonly-mcp\n   ```\n\n2. Install the required packages:\n\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file and set up your AWS credentials:\n\n   ```\n   AWS_ACCESS_KEY_ID=your_access_key\n   AWS_SECRET_ACCESS_KEY=your_secret_key\n   AWS_REGION=your_region\n   ```\n\n### Build and Run\n\n```bash\nnpm run build\nnpm start\n```\n\n## Connect to Claude Desktop\n\nTo use this MCP server with Claude Desktop, you need to modify the Claude Desktop configuration file.\n\n1. Open the Claude Desktop configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n2. Add the server configuration as follows:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"dynamodb-readonly\": {\n         \"command\": \"node\",\n         \"args\": [\"/absolute-path/dynamo-readonly-mcp/dist/index.js\"],\n         \"env\": {\n           \"AWS_ACCESS_KEY_ID\": \"your_access_key\",\n           \"AWS_SECRET_ACCESS_KEY\": \"your_secret_key\",\n           \"AWS_REGION\": \"your_region\"\n         }\n       }\n     }\n   }\n   ```\n\n3. Restart Claude Desktop.\n\n## Run with NPX\n\nYou can also run this server using `npx` without a global installation:\n\n```json\n{\n  \"mcpServers\": {\n    \"dynamodb-readonly\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"dynamo-readonly-mcp\"],\n      \"env\": {\n        \"AWS_ACCESS_KEY_ID\": \"your_access_key\",\n        \"AWS_SECRET_ACCESS_KEY\": \"your_secret_key\",\n        \"AWS_REGION\": \"your_region\"\n      }\n    }\n  }\n}\n```\n\n## Usage Examples\n\nYou can ask Claude questions like:\n\n1. \"Can you tell me what tables are in DynamoDB?\"\n2. \"Explain the structure of the Users table\"\n3. \"Find the number of users in the 'Users' table where groupId is '0lxp4paxk7'\"\n\n---\n\n## Architecture\n\nThis MCP server consists of the following layered structure:\n\n1. **Client Interface (Claude Desktop)** - Interaction between user and LLM\n2. **MCP Protocol Layer** - Provides standardized message exchange method\n3. **DynamoDB Server** - Implements functions that interact with DynamoDB\n4. **AWS SDK** - Communicates with AWS DynamoDB service\n\n## Key Operation Mechanisms\n\n### 1. Initialization and Connection\n\nWhen the server starts, the following process occurs:\n\n```tsx\nasync function main() {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error('DynamoDB read-only MCP server is running...');\n}\n```\n\n- `StdioServerTransport` sets up a communication channel through standard input/output.\n- `server.connect(transport)` connects to Claude Desktop through the MCP protocol.\n- During connection, the server sends information about supported tools, resources, and prompts to the client.\n\n### 2. Tool Request Processing\n\nWhen a user asks Claude something like \"Show me the list of DynamoDB tables\":\n\n1. Claude analyzes this request and calls the `list-tables` tool.\n2. This request is sent to the server through the MCP protocol.\n3. The server executes the corresponding tool handler:\n\n```tsx\nserver.tool('list-tables', 'Gets a list of all DynamoDB tables', {}, async () => {\n  try {\n    const tables = await listTables();\n    return {\n      content: [{ type: 'text', text: JSON.stringify(tables, null, 2) }],\n    };\n  } catch (error) {\n    return { isError: true, content: [{ type: 'text', text: `Error: ${error.message}` }] };\n  }\n});\n```\n\n1. The result is returned to Claude through the MCP protocol.\n2. Claude processes this result into natural language and presents it to the user.\n\n### 3. Specific Parameter Handling\n\nWhen a user requests \"Tell me the structure of the Users table\":\n\n1. Claude determines that this request should use the `describe-table` tool.\n2. Claude configures the parameter as `{ tableName: \"Users\" }`.\n3. This information is sent to the MCP server:\n\n```tsx\nserver.tool(\n  'describe-table',\n  'Gets detailed information about a DynamoDB table',\n  {\n    tableName: z.string().describe('Name of the table to get detailed information for'),\n  },\n  async ({ tableName }) => {\n    // Query table information using the tableName parameter\n    const tableInfo = await describeTable(tableName);\n    // Return results\n  }\n);\n```\n\nHere, `z.string()` uses the Zod library to validate parameters.\n\n### 4. Resource Handling\n\nResources are another MCP feature that provides read-only data:\n\n```tsx\nserver.resource('dynamodb-tables-info', 'DynamoDB table information', async () => {\n  // Create and return resource data\n  const tables = await listTables();\n  const tablesInfo = await Promise.all(/* Query table information */);\n\n  return {\n    contents: [\n      {\n        uri: 'dynamodb://tables-info',\n        text: JSON.stringify(tablesInfo, null, 2),\n        mimeType: 'application/json',\n      },\n    ],\n  };\n});\n```\n\nClaude accesses resources and uses them as context information.\n\n### 5. Prompt Handling\n\nThe MCP server can provide prompt templates for specific tasks:\n\n```tsx\nserver.prompt(\n  'dynamodb-query-help',\n  'A prompt that helps write DynamoDB queries',\n  {\n    tableName: z.string().describe('Table name to query'),\n    queryType: z.enum(['basic', 'advanced']).default('basic'),\n  },\n  async ({ tableName, queryType }) => {\n    // Generate prompt content\n    return {\n      messages: [\n        {\n          role: 'user',\n          content: { type: 'text', text: helpContent },\n        },\n      ],\n    };\n  }\n);\n```\n\nThis prompt is used when a user requests \"Show me how to write queries for the Users table.\"\n\n## Data Flow Summary\n\n1. User makes a request to Claude in natural language\n2. Claude analyzes the request and selects the appropriate MCP tool/resource/prompt\n3. MCP client sends the request to the server in a standardized format\n4. Server processes the request and calls the AWS DynamoDB API\n5. DynamoDB returns results\n6. Server converts results to MCP format and sends them to the client\n7. Claude processes the results into natural language and presents them to the user\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dynamodb",
        "dynamo",
        "databases",
        "dynamodb databases",
        "aws dynamodb",
        "dynamo readonly"
      ],
      "category": "databases"
    },
    "johnnyoshika--mcp-server-sqlite-npx": {
      "owner": "johnnyoshika",
      "name": "mcp-server-sqlite-npx",
      "url": "https://github.com/johnnyoshika/mcp-server-sqlite-npx",
      "imageUrl": "/freedevtools/mcp/pfp/johnnyoshika.webp",
      "description": "Node.js implementation of the Model Context Protocol SQLite server offering an npx-based alternative for environments without Python's UVX runner. This server connects AI models to SQLite databases, facilitating data storage and retrieval operations.",
      "stars": 13,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-22T22:55:06Z",
      "readme_content": "# MCP SQLite Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-sqlite-npx)](https://smithery.ai/server/mcp-server-sqlite-npx) [![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/johnnyoshika-mcp-server-sqlite-npx)\n\nA Node.js implementation of the Model Context Protocol SQLite server, based on the [official Python reference](https://github.com/modelcontextprotocol/servers/tree/main/src/sqlite). This version provides an npx-based alternative for environments where Python's UVX runner is not available, such as [LibreChat](https://github.com/danny-avila/LibreChat/issues/4876#issuecomment-2561363955).\n\n## Use with Claude Desktop\n\n### Installing via Smithery\n\nTo install MCP SQLite Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-sqlite-npx):\n\n```bash\nnpx -y @smithery/cli install mcp-server-sqlite-npx --client claude\n```\n\n### Installing Manually\n\nAdd the following to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"/absolute/path/to/npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-sqlite-npx\",\n        \"/absolute/path/to/database.db\"\n      ],\n      \"env\": {\n        \"PATH\": \"/absolute/path/to/executables\",\n        \"NODE_PATH\": \"/absolute/path/to/node_modules\"\n      }\n    }\n  }\n}\n```\n\nFull example when using nvm on macoS:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"/Users/{username}/.nvm/versions/node/v22.12.0/bin/npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-sqlite-npx\",\n        \"/Users/{username}/projects/database.db\"\n      ],\n      \"env\": {\n        \"PATH\": \"/Users/{username}/.nvm/versions/node/v22.12.0/bin:/usr/local/bin:/usr/bin:/bin\",\n        \"NODE_PATH\": \"/Users/{username}/.nvm/versions/node/v22.12.0/lib/node_modules\"\n      }\n    }\n  }\n}\n```\n\nFull example when using nvm on Windows:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"C:\\\\Program Files\\\\nodejs\\\\npx.cmd\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-sqlite-npx\",\n        \"C:\\\\Users\\\\{username}\\\\projects\\\\database.db\"\n      ],\n      \"env\": {\n        \"PATH\": \"C:\\\\Program Files\\\\nodejs;%PATH%\",\n        \"NODE_PATH\": \"C:\\\\Program Files\\\\nodejs\\\\node_modules\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\n1. Install dependencies:\n\n```bash\nnpm ci\n```\n\n2. Build the TypeScript code:\n\n```bash\nnpm run build\n```\n\n### Testing with MCP Inspector\n\nYou can test the server using the [MCP Inspector tool](https://modelcontextprotocol.io/docs/tools/inspector):\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js /absolute/path/to/database.db\n```\n\n`Connect` and go to `Tools` to start using the server.\n\n### Testing with Claude Desktop\n\nAdd the following to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"/absolute/path/to/node\",\n      \"args\": [\n        \"/absolute/path/to/dist/index.js\",\n        \"/absolute/path/to/database.db\"\n      ]\n    }\n  }\n}\n```\n\nExamples:\n\n- `/absolute/path/to/node`: `/Users/{username}/.nvm/versions/node/v20.18.1/bin/node`\n- `/absolute/path/to/index.js`: `/Users/{username}/projects/mcp-server-sqlite-npx/dist/index.js`\n- `/absolute/path/to/database.db`: `/Users/{username}/projects/database.db`\n\n### Publish\n\n- Bump version in package.json\n- `npm install`\n- Commit with message: `Release {version, e.g. 0.1.6}`\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sqlite",
        "databases",
        "database",
        "sqlite npx",
        "protocol sqlite",
        "server sqlite"
      ],
      "category": "databases"
    },
    "joleyline--mcp-memory-libsql": {
      "owner": "joleyline",
      "name": "mcp-memory-libsql",
      "url": "https://github.com/joleyline/mcp-memory-libsql",
      "imageUrl": "/freedevtools/mcp/pfp/joleyline.webp",
      "description": "Leverage high-performance vector search and efficient knowledge storage to manage entities and relations. Provides semantic search capabilities and secure token-based authentication for connecting to remote libSQL databases.",
      "stars": 18,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T13:57:56Z",
      "readme_content": "# mcp-memory-libsql\n\nA high-performance, persistent memory system for the Model Context\nProtocol (MCP) powered by libSQL. This server provides vector search\ncapabilities and efficient knowledge storage using libSQL as the\nbacking store.\n\n<a href=\"https://glama.ai/mcp/servers/22lg4lq768\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/22lg4lq768/badge\" alt=\"Glama badge\" />\n</a>\n\n## Features\n\n- 🚀 High-performance vector search using libSQL\n- 💾 Persistent storage of entities and relations\n- 🔍 Semantic search capabilities\n- 🔄 Knowledge graph management\n- 🌐 Compatible with local and remote libSQL databases\n- 🔒 Secure token-based authentication for remote databases\n\n## Configuration\n\nThis server is designed to be used as part of an MCP configuration.\nHere are examples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-memory-libsql\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-memory-libsql\"],\n\t\t\t\"env\": {\n\t\t\t\t\"LIBSQL_URL\": \"file:/path/to/your/database.db\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor a detailed guide on setting up this server with Claude Desktop in\nWSL, see\n[Getting MCP Server Working with Claude Desktop in WSL](https://scottspence.com/posts/getting-mcp-server-working-with-claude-desktop-in-wsl).\n\nAdd this to your Claude Desktop configuration for WSL environments:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-memory-libsql\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && LIBSQL_URL=file:/path/to/database.db /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-memory-libsql\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Database Configuration\n\nThe server supports both local SQLite and remote libSQL databases\nthrough the LIBSQL_URL environment variable:\n\nFor local SQLite databases:\n\n```json\n{\n\t\"env\": {\n\t\t\"LIBSQL_URL\": \"file:/path/to/database.db\"\n\t}\n}\n```\n\nFor remote libSQL databases (e.g., Turso):\n\n```json\n{\n\t\"env\": {\n\t\t\"LIBSQL_URL\": \"libsql://your-database.turso.io\",\n\t\t\"LIBSQL_AUTH_TOKEN\": \"your-auth-token\"\n\t}\n}\n```\n\nNote: When using WSL, ensure the database path uses the Linux\nfilesystem format (e.g., `/home/username/...`) rather than Windows\nformat.\n\nBy default, if no URL is provided, it will use `file:/memory-tool.db`\nin the current directory.\n\n## API\n\nThe server implements the standard MCP memory interface with\nadditional vector search capabilities:\n\n- Entity Management\n  - Create/Update entities with embeddings\n  - Delete entities\n  - Search entities by similarity\n- Relation Management\n  - Create relations between entities\n  - Delete relations\n  - Query related entities\n\n## Architecture\n\nThe server uses a libSQL database with the following schema:\n\n- Entities table: Stores entity information and embeddings\n- Relations table: Stores relationships between entities\n- Vector search capabilities implemented using libSQL's built-in\n  vector operations\n\n## Development\n\n### Publishing\n\nDue to npm 2FA requirements, publishing needs to be done manually:\n\n1. Create a changeset (documents your changes):\n\n```bash\npnpm changeset\n```\n\n2. Version the package (updates version and CHANGELOG):\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm (will prompt for 2FA code):\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please read our contributing guidelines\nbefore submitting pull requests.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [libSQL](https://github.com/tursodatabase/libsql)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "libsql",
        "databases",
        "database",
        "libsql databases",
        "memory libsql",
        "secure database"
      ],
      "category": "databases"
    },
    "jonfreeland--mongodb-mcp": {
      "owner": "jonfreeland",
      "name": "mongodb-mcp",
      "url": "https://github.com/jonfreeland/mongodb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jonfreeland.webp",
      "description": "Query and analyze MongoDB databases with read-only access, enabling exploration of data while ensuring safety. Utilize powerful querying and aggregation capabilities to gain insights and suggest visualizations.",
      "stars": 7,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-08T12:13:04Z",
      "readme_content": "# MongoDB MCP Server\n\nA Model Context Protocol server that provides read-only access to MongoDB databases through standardized MCP tools and resources.\n\n<a href=\"https://glama.ai/mcp/servers/cmywezu1sn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/cmywezu1sn/badge\" alt=\"MongoDB Server MCP server\" />\n</a>\n\n## Overview\n\nThis MongoDB MCP server enables AI assistants to directly query and analyze MongoDB databases without write access, maintaining data safety while providing powerful data exploration capabilities.\n\n## Features\n\n### MongoDB Operations\n- **Database Exploration**: List databases and collections\n- **Schema Discovery**: Infer collection schemas from sample documents\n- **Querying**: Execute MongoDB queries with filtering, projection, sorting, and limiting\n- **Aggregation**: Run read-only aggregation pipelines with safety validation\n- **Text Search**: Perform full-text search on collections with text indexes\n- **Geospatial Queries**: Find locations near points, within polygons, or intersecting geometries\n- **Document Operations**: Count documents, sample random documents, find documents by IDs\n- **Data Analysis**: Get collection statistics, index information, and query execution plans\n- **Performance Insights**: Examine query execution plans to optimize performance\n- **Data Exploration**: Get distinct values, field distributions, and data samples\n- **Format Conversion**: Export query results as JSON or CSV formats\n\n### Enhanced Capabilities\n- **Schema Inference**: Automatically detect data types and structure from documents\n- **Visualization Hints**: Intelligent suggestions for data visualization based on result content\n- **Safety Validation**: Prevents write operations in aggregation pipelines\n- **Example-Rich Documentation**: Each tool includes detailed examples in its description\n\n## Requirements\n\n### Environment Variables\n- `MONGODB_URI` (required): MongoDB connection string with authentication if needed\n- `MONGODB_DEFAULT_DATABASE` (optional): Default database name when not specified in queries\n\n### Prerequisites\n- Network access to MongoDB server\n- Authentication credentials if required by MongoDB instance\n- Appropriate read permissions on target databases\n\n## Installation\n\n### Building from Source\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Integration with Claude Desktop\n\nTo use with Claude Desktop, add the server configuration:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"/path/to/mongodb-server/build/index.js\",\n      \"env\": {\n        \"MONGODB_URI\": \"mongodb://username:password@hostname:port/database\",\n        \"MONGODB_DEFAULT_DATABASE\": \"your_default_db\"\n      }\n    }\n  }\n}\n```\n\n### Integration with Claude Web\n\nFor Claude Web via the MCP Chrome extension, add configuration to Cline MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mongodb-server/build/index.js\"],\n      \"env\": {\n        \"MONGODB_URI\": \"mongodb://username:password@hostname:port/database\",\n        \"MONGODB_DEFAULT_DATABASE\": \"your_default_db\"\n      }\n    }\n  }\n}\n```\n\n### Integration with Claude Code\n\nTo use with Claude Code, use the following commands:\n\n```bash\ncd /path/to/my/project\nclaude mcp add mongo-server /path/to/mongodb-mcp/build/index.js -e \"MONGODB_URI=mongodb://user@password:27017/dbname?authSource=authDbName\" -e MONGO_DEFAULT_DATABASE=dbname \n```\n\nMake sure to replace the placeholders with your actual MongoDB connection string and default database name.\n\nIf configured correctly, you should see the following when you run `claude`:\n```bash\n╭───────────────────────────────────────────────────────╮\n│ ✻ Welcome to Claude Code research preview!            │\n│                                                       │\n│   /help for help                                      │\n│                                                       │\n│   cwd: <path-to-project-directory>                    │\n│                                                       │\n│   ─────────────────────────────────────────────────── │\n│                                                       │\n│   MCP Servers:                                        │\n│                                                       │\n│   • mongo-server                            connected │\n╰───────────────────────────────────────────────────────╯\n\n```\n\nIf you run into issues, see the Claude Code [documentation](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp-servers).\n\n\n## Security Considerations\n\n- This server provides read-only access by design\n- Connection strings may contain sensitive authentication information\n- Store connection strings securely in environment variables\n- Use a MongoDB user with read-only permissions\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. Use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "mongodb databases",
        "analyze mongodb",
        "mongodb mcp"
      ],
      "category": "databases"
    },
    "joshuarileydev--supabase-mcp-server": {
      "owner": "joshuarileydev",
      "name": "supabase-mcp-server",
      "url": "https://github.com/joshuarileydev/supabase",
      "imageUrl": "",
      "description": "Supabase MCP Server for managing and creating projects and organisations in Supabase",
      "stars": 47,
      "forks": 14,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-20T20:59:19Z",
      "readme_content": "# Supabase MCP Server\n\nA Model Context Protocol (MCP) server that provides programmatic access to the Supabase Management API. This server allows AI models and other clients to manage Supabase projects and organizations through a standardized interface.\n\n## Features\n\n### Project Management\n- List all projects\n- Get project details\n- Create new projects\n- Delete projects\n- Retrieve project API keys\n\n### Organization Management\n- List all organizations\n- Get organization details\n- Create new organizations\n\n## Installation\nAdd the following to your Claude Config JSON file\n```\n{\n  \"mcpServers\": {\n    \"supabase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"y\",\n        \"@joshuarileydev/supabase-mcp-server\"\n      ],\n      \"env\": {\n        \"SUPABASE_API_KEY\": \"API_KEY_HERE\"\n      }\n    }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "server supabase",
        "supabase mcp",
        "organisations supabase"
      ],
      "category": "databases"
    },
    "jovezhong--mcp-timeplus": {
      "owner": "jovezhong",
      "name": "mcp-timeplus",
      "url": "https://github.com/jovezhong/mcp-timeplus",
      "imageUrl": "",
      "description": "MCP server for Apache Kafka and Timeplus. Able to list Kafka topics, poll Kafka messages, save Kafka data locally and query streaming data with SQL via Timeplus",
      "stars": 10,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-29T07:04:38Z",
      "readme_content": "# Timeplus MCP Server\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-timeplus)](https://pypi.org/project/mcp-timeplus)\n\nAn MCP server for Timeplus.\n\n<a href=\"https://glama.ai/mcp/servers/9aleefsq9s\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/9aleefsq9s/badge\" alt=\"mcp-timeplus MCP server\" /></a>\n\n## Features\n\n### Prompts\n\n* `generate_sql` to give LLM more knowledge about how to query Timeplus via SQL\n\n### Tools\n\n* `run_sql`\n  - Execute SQL queries on your Timeplus cluster.\n  - Input: `sql` (string): The SQL query to execute.\n  - By default, all Timeplus queries are run with `readonly = 1` to ensure they are safe. If you want to run DDL or DML queries, you can set the environment variable `TIMEPLUS_READ_ONLY` to `false`.\n\n* `list_databases`\n  - List all databases on your Timeplus cluster.\n\n* `list_tables`\n  - List all tables in a database.\n  - Input: `database` (string): The name of the database.\n\n* `list_kafka_topics`\n  - List all topics in a Kafka cluster\n\n* `explore_kafka_topic`\n  - Show some messages in the Kafka topic\n  - Input: `topic` (string): The name of the topic. `message_count` (int): The number of messages to show, default to 1.\n\n* `create_kafka_stream`\n  - Setup a streaming ETL in Timeplus to save the Kafka messages locally\n  - Input: `topic` (string): The name of the topic.\n\n* `connect_to_apache_iceberg`\n  - Connect to a database based on Apache Iceberg. Currently this is only available via Timeplus Enterprise and it's planned to make it available for Timeplus Proton soon.\n  - Input: `iceberg_db` (string): The name of the Iceberg database. `aws_account_id` (int): The AWS account ID (12 digits). `s3_bucket` (string): The S3 bucket name. `aws_region` (string): The AWS region, default to \"us-west-2\". `is_s3_table_bucket` (bool): Whether the S3 bucket is a S3 table bucket, default to False.\n\n## Configuration\n\nFirst, ensure you have the `uv` executable installed. If not, you can install it by following the instructions [here](https://docs.astral.sh/uv/).\n\n1. Open the Claude Desktop configuration file located at:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-timeplus\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-timeplus\"],\n      \"env\": {\n        \"TIMEPLUS_HOST\": \"<timeplus-host>\",\n        \"TIMEPLUS_PORT\": \"<timeplus-port>\",\n        \"TIMEPLUS_USER\": \"<timeplus-user>\",\n        \"TIMEPLUS_PASSWORD\": \"<timeplus-password>\",\n        \"TIMEPLUS_SECURE\": \"false\",\n        \"TIMEPLUS_VERIFY\": \"true\",\n        \"TIMEPLUS_CONNECT_TIMEOUT\": \"30\",\n        \"TIMEPLUS_SEND_RECEIVE_TIMEOUT\": \"30\",\n        \"TIMEPLUS_READ_ONLY\": \"false\",\n        \"TIMEPLUS_KAFKA_CONFIG\": \"{\\\"bootstrap.servers\\\":\\\"a.aivencloud.com:28864\\\", \\\"sasl.mechanism\\\":\\\"SCRAM-SHA-256\\\",\\\"sasl.username\\\":\\\"avnadmin\\\", \\\"sasl.password\\\":\\\"thePassword\\\",\\\"security.protocol\\\":\\\"SASL_SSL\\\",\\\"enable.ssl.certificate.verification\\\":\\\"false\\\"}\"\n      }\n    }\n  }\n}\n```\n\nUpdate the environment variables to point to your own Timeplus service.\n\n3. Restart Claude Desktop to apply the changes.\n\nYou can also try this MCP server with other MCP clients, such as [5ire](https://github.com/nanbingxyz/5ire).\n\n## Development\n\n1. In `test-services` directory run `docker compose up -d` to start a Timeplus Proton server. You can also download it via `curl https://install.timeplus.com/oss | sh`, then start with `./proton server`.\n\n2. Add the following variables to a `.env` file in the root of the repository.\n\n```\nTIMEPLUS_HOST=localhost\nTIMEPLUS_PORT=8123\nTIMEPLUS_USER=default\nTIMEPLUS_PASSWORD=\nTIMEPLUS_SECURE=false\nTIMEPLUS_VERIFY=true\nTIMEPLUS_CONNECT_TIMEOUT=30\nTIMEPLUS_SEND_RECEIVE_TIMEOUT=30\nTIMEPLUS_READ_ONLY=false\nTIMEPLUS_KAFKA_CONFIG={\"bootstrap.servers\":\"a.aivencloud.com:28864\", \"sasl.mechanism\":\"SCRAM-SHA-256\",\"sasl.username\":\"avnadmin\", \"sasl.password\":\"thePassword\",\"security.protocol\":\"SASL_SSL\",\"enable.ssl.certificate.verification\":\"false\"}\n```\n\n3. Run `uv sync` to install the dependencies. Then do `source .venv/bin/activate`.\n\n4. For easy testing, you can run `mcp dev mcp_timeplus/mcp_server.py` to start the MCP server. Click the \"Connect\" button to connect the UI with the MCP server, then switch to the \"Tools\" tab to run the available tools.\n\n5. To build the Docker image, run `docker build -t mcp_timeplus .`.\n\n### Environment Variables\n\nThe following environment variables are used to configure the Timeplus connection:\n\n#### Required Variables\n* `TIMEPLUS_HOST`: The hostname of your Timeplus server\n* `TIMEPLUS_USER`: The username for authentication\n* `TIMEPLUS_PASSWORD`: The password for authentication\n\n#### Optional Variables\n* `TIMEPLUS_PORT`: The port number of your Timeplus server\n  - Default: `8443` if HTTPS is enabled, `8123` if disabled\n  - Usually doesn't need to be set unless using a non-standard port\n* `TIMEPLUS_SECURE`: Enable/disable HTTPS connection\n  - Default: `\"false\"`\n  - Set to `\"true\"` for secure connections\n* `TIMEPLUS_VERIFY`: Enable/disable SSL certificate verification\n  - Default: `\"true\"`\n  - Set to `\"false\"` to disable certificate verification (not recommended for production)\n* `TIMEPLUS_CONNECT_TIMEOUT`: Connection timeout in seconds\n  - Default: `\"30\"`\n  - Increase this value if you experience connection timeouts\n* `TIMEPLUS_SEND_RECEIVE_TIMEOUT`: Send/receive timeout in seconds\n  - Default: `\"300\"`\n  - Increase this value for long-running queries\n* `TIMEPLUS_DATABASE`: Default database to use\n  - Default: None (uses server default)\n  - Set this to automatically connect to a specific database\n* `TIMEPLUS_READ_ONLY`: Enable/disable read-only mode\n  - Default: `\"true\"`\n  - Set to `\"false\"` to enable DDL/DML\n* `TIMEPLUS_KAFKA_CONFIG`: A JSON string for the Kafka configuration. Please refer to [librdkafka configuration](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) or take the above example as a reference.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kafka",
        "databases",
        "database",
        "kafka data",
        "kafka timeplus",
        "apache kafka"
      ],
      "category": "databases"
    },
    "jparkerweb--mcp-sqlite": {
      "owner": "jparkerweb",
      "name": "mcp-sqlite",
      "url": "https://github.com/jparkerweb/mcp-sqlite",
      "imageUrl": "",
      "description": "Model Context Protocol (MCP) server that provides comprehensive SQLite database interaction capabilities.",
      "stars": 57,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:49:16Z",
      "readme_content": "# 🐇 MCP SQLite Server\r\nThis is a Model Context Protocol (MCP) server that provides comprehensive SQLite database interaction capabilities.\r\n\r\n![cursor-settings](https://raw.githubusercontent.com/jparkerweb/mcp-sqlite/refs/heads/main/.readme/mcp-sqlite.jpg)\r\n\r\n#### Maintained by\r\n<a href=\"https://www.equilllabs.com\">\r\n  <img src=\"https://raw.githubusercontent.com/jparkerweb/eQuill-Labs/refs/heads/main/src/static/images/logo-text-outline.png\" alt=\"eQuill Labs\" height=\"32\">\r\n</a>\r\n\r\n## Features\r\n- Complete CRUD operations (Create, Read, Update, Delete)\r\n- Database exploration and introspection\r\n- Execute custom SQL queries\r\n\r\n## Setup\r\n\r\nDefine the command in your IDE's MCP Server settings:\r\n\r\ne.g. `Cursor`:\r\n```json\r\n{\r\n    \"mcpServers\": {\r\n        \"MCP SQLite Server\": {\r\n            \"command\": \"npx\",\r\n            \"args\": [\r\n                \"-y\",\r\n                \"mcp-sqlite\",\r\n                \"<path-to-your-sqlite-database.db>\"\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\ne.g. `VSCode`:\r\n```json\r\n{\r\n    \"servers\": {\r\n        \"MCP SQLite Server\": {\r\n            \"type\": \"stdio\",\r\n            \"command\": \"npx\",\r\n            \"args\": [\r\n                \"-y\",\r\n                \"mcp-sqlite\",\r\n                \"<path-to-your-sqlite-database.db>\"\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n![cursor-settings](https://raw.githubusercontent.com/jparkerweb/mcp-sqlite/refs/heads/main/.readme/cursor-mcp-settings.jpg)\r\n\r\nYour database path must be provided as an argument.\r\n\r\n## Available Tools\r\n\r\n### Database Information\r\n\r\n#### db_info\r\n\r\nGet detailed information about the connected database.\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"db_info\",\r\n    \"arguments\": {}\r\n  }\r\n}\r\n```\r\n\r\n#### list_tables\r\n\r\nList all tables in the database.\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"list_tables\",\r\n    \"arguments\": {}\r\n  }\r\n}\r\n```\r\n\r\n#### get_table_schema\r\n\r\nGet detailed information about a table's schema.\r\n\r\nParameters:\r\n- `tableName` (string): Name of the table\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"get_table_schema\",\r\n    \"arguments\": {\r\n      \"tableName\": \"users\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### CRUD Operations\r\n\r\n#### create_record\r\n\r\nInsert a new record into a table.\r\n\r\nParameters:\r\n- `table` (string): Name of the table\r\n- `data` (object): Record data as key-value pairs\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"create_record\",\r\n    \"arguments\": {\r\n      \"table\": \"users\",\r\n      \"data\": {\r\n        \"name\": \"John Doe\",\r\n        \"email\": \"john@example.com\",\r\n        \"age\": 30\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n#### read_records\r\n\r\nQuery records from a table with optional filtering.\r\n\r\nParameters:\r\n- `table` (string): Name of the table\r\n- `conditions` (object, optional): Filter conditions as key-value pairs\r\n- `limit` (number, optional): Maximum number of records to return\r\n- `offset` (number, optional): Number of records to skip\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"read_records\",\r\n    \"arguments\": {\r\n      \"table\": \"users\",\r\n      \"conditions\": {\r\n        \"age\": 30\r\n      },\r\n      \"limit\": 10,\r\n      \"offset\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n#### update_records\r\n\r\nUpdate records in a table that match specified conditions.\r\n\r\nParameters:\r\n- `table` (string): Name of the table\r\n- `data` (object): New values as key-value pairs\r\n- `conditions` (object): Filter conditions as key-value pairs\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"update_records\",\r\n    \"arguments\": {\r\n      \"table\": \"users\",\r\n      \"data\": {\r\n        \"email\": \"john.updated@example.com\"\r\n      },\r\n      \"conditions\": {\r\n        \"id\": 1\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n#### delete_records\r\n\r\nDelete records from a table that match specified conditions.\r\n\r\nParameters:\r\n- `table` (string): Name of the table\r\n- `conditions` (object): Filter conditions as key-value pairs\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"delete_records\",\r\n    \"arguments\": {\r\n      \"table\": \"users\",\r\n      \"conditions\": {\r\n        \"id\": 1\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Custom Queries\r\n\r\n#### query\r\n\r\nExecute a custom SQL query against the connected SQLite database.\r\n\r\nParameters:\r\n- `sql` (string): The SQL query to execute\r\n- `values` (array, optional): Array of parameter values to use in the query\r\n\r\nExample:\r\n```json\r\n{\r\n  \"method\": \"tools/call\",\r\n  \"params\": {\r\n    \"name\": \"query\",\r\n    \"arguments\": {\r\n      \"sql\": \"SELECT * FROM users WHERE id = ?\",\r\n      \"values\": [1]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Built with\r\n\r\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/typescript-sdk)\r\n- [sqlite3](https://github.com/TryGhost/node-sqlite3)\r\n\r\n---\r\n\r\n## Appreciation\r\nIf you enjoy this library please consider sending me a tip to support my work 😀\r\n### [🍵 tip me here](https://ko-fi.com/jparkerweb)\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlite",
        "database",
        "mcp sqlite",
        "secure database",
        "jparkerweb mcp"
      ],
      "category": "databases"
    },
    "kenhuangus--mcp-vulnerable-server-demo": {
      "owner": "kenhuangus",
      "name": "mcp-vulnerable-server-demo",
      "url": "https://github.com/kenhuangus/mcp-vulnerable-server-demo",
      "imageUrl": "/freedevtools/mcp/pfp/kenhuangus.webp",
      "description": "Demonstrates security vulnerabilities in MCP servers by exposing insecure tools for educational purposes. Showcases common attack vectors like SQL injection and arbitrary SQL execution, facilitating an understanding of risks and mitigation strategies.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-10T22:57:26Z",
      "readme_content": "# Insecure MCP Demo\n\n## Overview\nThis project demonstrates a vulnerable MCP server and multiple clients, including a proof-of-concept attack client and also a good client. It is designed for educational purposes to showcase potential security vulnerabilities in MCP server.\n\n## Project Structure\n- `vuln-mcp.py`: Vulnerable MCP server exposing insecure tools.\n- `good-mcp-client.py`: Regular good client for normal interactions (insert/query records).\n- `attack-mcp-client.py`: Automated attack client that demonstrates exploitation of server vulnerabilities.\n- `requirements.txt`: Python dependencies for the project.\n\n## Features & Vulnerabilities\n### Exposed Server Tools\n1. **insert_record**\n   - Inserts a name/address record into the database.\n   - **Vulnerability:** Prone to SQL injection due to direct string interpolation of user input into SQL queries.\n2. **query_records**\n   - Lists all records in the database.\n   - **Vulnerability:** Exposes all data without authentication or access control.\n3. **execute_sql**\n   - Executes arbitrary SQL queries provided by the client.\n   - **Vulnerability:** Allows any SQL command, including destructive ones (e.g., data exfiltration, schema changes).\n4. **get_env_variable**\n   - Returns the value of any environment variable requested.\n   - **Vulnerability:** Leaks sensitive environment variables (e.g., secrets, API keys).\n\n## How to Run\n### 1. Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n### 2. Start the Server and Good Client\nIn one terminal:\n```bash\npython good-mcp-client.py vuln-mcp.py\n```\nFollow the prompts to insert/query records interactively.\n\n### 3. Run the Attack Client\nIn another terminal:\n```bash\npython attack-mcp-client.py vuln-mcp.py\n```\nThis will automatically:\n- Attempt SQL injection attacks\n- Execute arbitrary SQL queries\n- Attempt to read several common environment variables\n\n## Example Output\n- Attack client will show which payloads succeed or fail, and print out database contents and environment variable values if accessible.\n\n## Vulnerabilities Demonstrated\n- **SQL Injection:** User input is unsanitized, allowing attackers to manipulate SQL logic and insert arbitrary data.\n- **Arbitrary Code Execution:** The `execute_sql` tool allows attackers to run any SQL command, including data theft or destruction.\n- **Sensitive Data Exposure:** The `get_env_variable` tool allows attackers to read secrets and configuration values.\n- **Lack of Access Control:** Anyone can run all tools and access all data without authentication.\n\n## Mitigation Strategies\nTo secure a real-world MCP server, you should:\n\n1. **Use Parameterized Queries:**\n   - Always use parameter substitution instead of string interpolation for SQL queries to prevent injection.\n   - Example (secure):\n     ```python\n     cursor.execute(\"INSERT INTO records (name, address) VALUES (?, ?)\", (name, address))\n     ```\n2. **Restrict Dangerous Tools:**\n   - Remove or strictly limit tools like `execute_sql` and `get_env_variable`.\n   - Only expose necessary functionality.\n3. **Implement Authentication & Authorization:**\n   - Require users to authenticate and check permissions before allowing access to sensitive tools or data.\n4. **Validate and Sanitize Input:**\n   - Check and sanitize all user inputs, especially those that interact with the database or system.\n5. **Limit Environment Variable Access:**\n   - Only allow access to non-sensitive variables, or remove this tool entirely.\n6. **Audit and Monitor Usage:**\n   - Log all tool invocations and monitor for suspicious or abusive behavior.\n7. **Principle of Least Privilege:**\n   - Run the server with minimal privileges and restrict database and OS access as much as possible.\n\n## Disclaimer\nThis project is for educational and demonstration purposes only. **Do not deploy this code in production environments.**\n\n---\n\nFor questions or further improvements, please open an issue or contact the project maintainer.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "vulnerabilities",
        "database",
        "secure database",
        "databases secure",
        "vulnerabilities mcp"
      ],
      "category": "databases"
    },
    "keonchennl--mcp-graphdb": {
      "owner": "keonchennl",
      "name": "mcp-graphdb",
      "url": "https://github.com/keonchennl/mcp-graphdb",
      "imageUrl": "/freedevtools/mcp/pfp/keonchennl.webp",
      "description": "Explore RDF graphs and execute SPARQL queries on an Ontotext GraphDB instance for data retrieval and insights. It provides read-only access and enables interaction with various graphs in the repository.",
      "stars": 8,
      "forks": 7,
      "license": "GNU General Public License v3.0",
      "language": "JavaScript",
      "updated_at": "2025-07-23T14:50:59Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/keonchennl-mcp-graphdb-badge.png)](https://mseep.ai/app/keonchennl-mcp-graphdb)\n\n# GraphDB MCP Server\n\nA Model Context Protocol server that provides read-only access to Ontotext GraphDB. This server enables LLMs to explore RDF graphs and execute SPARQL queries against a GraphDB instance.\n\n## Components\n\n### Tools\n\n- **sparqlQuery**\n  - Execute SPARQL queries against the connected GraphDB repository\n  - Input:\n    - `query` (string): The SPARQL query to execute\n    - `graph` (string, optional): Specific graph IRI to target\n    - `format` (string, optional): Response format (json, xml, csv)\n  - All queries are executed in read-only mode\n\n- **listGraphs**\n  - Lists all graphs available in the repository\n  - No input parameters required\n\n### Resources\n\nThe server provides multiple views of the repository data:\n\n- **Class List** (`graphdb://<host>/repository/<repo>/classes`)\n  - Lists all RDF classes found in the repository with counts\n\n- **Predicates** (`graphdb://<host>/repository/<repo>/predicates`)\n  - Lists all predicates (properties) with usage counts\n\n- **Statistics** (`graphdb://<host>/repository/<repo>/stats`)\n  - Provides counts of subjects, predicates, objects, and triples\n\n- **Sample Data** (`graphdb://<host>/repository/<repo>/sample`)\n  - Shows a sample of triples from the repository\n\n- **Graph Content** (`graphdb://<host>/repository/<repo>/graph/<graphUri>`)\n  - Provides sample data from specific graphs along with metadata\n\n## Configuration\n\nYou can configure the server using environment variables by creating a `.env` file:\n\n```\nGRAPHDB_ENDPOINT=http://localhost:7200\nGRAPHDB_REPOSITORY=myRepository\nGRAPHDB_USERNAME=username\nGRAPHDB_PASSWORD=password\n```\n\nAlternatively, you can provide the endpoint and repository as command-line arguments:\n\n```\nnode dist/index.js http://localhost:7200 myRepository\n```\n\nThe command-line arguments take precedence over environment variables.\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"graphdb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/mcp-server-graphdb/dist/index.js\"\n      ],\n      \"env\": {\n        \"GRAPHDB_ENDPOINT\": \"http://localhost:7200\",\n        \"GRAPHDB_REPOSITORY\": \"myRepository\",\n        \"GRAPHDB_USERNAME\": \"username\",\n        \"GRAPHDB_PASSWORD\": \"password\"\n      }\n    }\n  }\n}\n```\n\nReplace the values with your specific GraphDB configuration.\n\n## Installation\n\n```sh\n# Clone the repository\ngit clone https://github.com/keonchennl/mcp-server-graphdb.git\ncd mcp-server-graphdb\n\n# Install dependencies\nyarn install\n\n# Build the project\nyarn build\n```\n\n## Example SPARQL Queries\n\nHere are some example SPARQL queries you can run with this server:\n\n1. List all classes in the ontology:\n```sparql\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nSELECT DISTINCT ?class ?label\nWHERE {\n  { ?class a rdfs:Class } UNION { ?class a owl:Class }\n  OPTIONAL { ?class rdfs:label ?label }\n}\nORDER BY ?class\n```\n\n2. List all properties for a specific class:\n```sparql\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nSELECT ?property ?label ?range\nWHERE {\n  ?property rdfs:domain <http://example.org/YourClass> .\n  OPTIONAL { ?property rdfs:label ?label }\n  OPTIONAL { ?property rdfs:range ?range }\n}\nORDER BY ?property\n```\n\n3. Count instances by class:\n```sparql\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nSELECT ?class (COUNT(?instance) AS ?count)\nWHERE {\n  ?instance a ?class\n}\nGROUP BY ?class\nORDER BY DESC(?count)\n```\n\n## License\n\nThis MCP server is licensed under the GPL-3.0 License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the GNU GPL-3.0 License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "graphdb",
        "databases",
        "database",
        "graphdb explore",
        "ontotext graphdb",
        "rdf graphs"
      ],
      "category": "databases"
    },
    "kevinbin--mcp-mysql-server": {
      "owner": "kevinbin",
      "name": "mcp-mysql-server",
      "url": "https://github.com/kevinbin/mcp-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/kevinbin.webp",
      "description": "Interact with MySQL databases through a standardized interface to perform operations such as querying, executing commands, and managing schemas securely. Provides comprehensive error handling and easy-to-use tools for database interactions.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-08T03:37:52Z",
      "readme_content": "# @enemyrr/mcp-mysql-server\n\n[![smithery badge](https://smithery.ai/badge/@enemyrr/mcp-mysql-server)](https://smithery.ai/server/@enemyrr/mcp-mysql-server)\n\nA Model Context Protocol server that provides MySQL database operations. This server enables AI models to interact with MySQL databases through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/hcqqd3qi8q\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/hcqqd3qi8q/badge\" alt=\"MCP-MySQL Server MCP server\" /></a>\n\n## Installation & Setup for Cursor IDE\n\n### Installing via Smithery\n\nTo install MySQL Database Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@enemyrr/mcp-mysql-server):\n\n```bash\nnpx -y @smithery/cli install @enemyrr/mcp-mysql-server --client claude\n```\n\n### Installing Manually\n1. Clone and build the project:\n```bash\ngit clone https://github.com/enemyrr/mcp-mysql-server.git\ncd mcp-mysql-server\nnpm install\nnpm run build\n```\n\n2. Add the server in Cursor IDE settings:\n   - Open Command Palette (Cmd/Ctrl + Shift + P)\n   - Search for \"MCP: Add Server\"\n   - Fill in the fields:\n     - Name: `mysql`\n     - Type: `command`\n     - Command: `node /absolute/path/to/mcp-mysql-server/build/index.js`\n\n> **Note**: Replace `/absolute/path/to/` with the actual path where you cloned and built the project.\n\n## Database Configuration\n\nYou can configure the database connection in three ways:\n\n1. **Database URL in .env** (Recommended):\n```env\nDATABASE_URL=mysql://user:password@host:3306/database\n```\n\n2. **Individual Parameters in .env**:\n```env\nDB_HOST=localhost\nDB_USER=your_user\nDB_PASSWORD=your_password\nDB_DATABASE=your_database\n```\n\n3. **Direct Connection via Tool**:\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"connect_db\",\n  arguments: {\n    url: \"mysql://user:password@host:3306/database\"\n    // OR\n    workspace: \"/path/to/your/project\" // Will use project's .env\n    // OR\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n## Available Tools\n\n### 1. connect_db\nConnect to MySQL database using URL, workspace path, or direct credentials.\n\n### 2. query\nExecute SELECT queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n### 3. execute\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n### 4. list_tables\nList all tables in the connected database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"list_tables\"\n});\n```\n\n### 5. describe_table\nGet the structure of a specific table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n### 6. create_table\nCreate a new table with specified fields and indexes.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"create_table\",\n  arguments: {\n    table: \"users\",\n    fields: [\n      {\n        name: \"id\",\n        type: \"int\",\n        autoIncrement: true,\n        primary: true\n      },\n      {\n        name: \"email\",\n        type: \"varchar\",\n        length: 255,\n        nullable: false\n      }\n    ],\n    indexes: [\n      {\n        name: \"email_idx\",\n        columns: [\"email\"],\n        unique: true\n      }\n    ]\n  }\n});\n```\n\n### 7. add_column\nAdd a new column to an existing table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql\",\n  tool_name: \"add_column\",\n  arguments: {\n    table: \"users\",\n    field: {\n      name: \"phone\",\n      type: \"varchar\",\n      length: 20,\n      nullable: true\n    }\n  }\n});\n```\n\n## Features\n\n- Multiple connection methods (URL, workspace, direct)\n- Secure connection handling with automatic cleanup\n- Prepared statement support for query parameters\n- Schema management tools\n- Comprehensive error handling and validation\n- TypeScript support\n- Automatic workspace detection\n\n## Security\n\n- Uses prepared statements to prevent SQL injection\n- Supports secure password handling through environment variables\n- Validates queries before execution\n- Automatically closes connections when done\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Connection failures\n- Invalid queries or parameters\n- Missing configuration\n- Database errors\n- Schema validation errors\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/enemyrr/mcp-mysql-server\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "databases secure",
        "mysql databases"
      ],
      "category": "databases"
    },
    "kevindwei--mcp-server-mysql": {
      "owner": "kevindwei",
      "name": "mcp-server-mysql",
      "url": "https://github.com/kevindwei/mcp-server-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/kevindwei.webp",
      "description": "Provides access to MySQL databases, enabling inspection of database schemas and execution of SQL queries securely. Supports read-only and configurable write operations with transaction handling and prepared statements, along with comprehensive database metadata.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-18T13:05:19Z",
      "readme_content": "# MCP Server for MySQL based on NodeJS\n[![smithery badge](https://smithery.ai/badge/@benborla29/mcp-server-mysql)](https://smithery.ai/server/@benborla29/mcp-server-mysql)\n\n\n\nA Model Context Protocol server that provides access to MySQL databases. This server enables LLMs to inspect database schemas and execute SQL queries.\n\n## Table of Contents\n- [Requirements](#requirements)\n- [Installation](#installation)\n  - [Claude Desktop](#claude-desktop)\n  - [Cursor](#cursor)\n  - [Smithery](#using-smithery)\n  - [MCP Get](#using-mcp-get)\n  - [Clone to Local Repository](#running-from-local-repository)\n- [Components](#components)\n- [Configuration](#configuration)\n- [Environment Variables](#environment-variables)\n- [Multi-DB Mode](#multi-db-mode)\n- [Schema-Specific Permissions](#schema-specific-permissions)\n- [Testing](#testing)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Requirements\n\n- Node.js v18 or higher\n- MySQL 5.7 or higher (MySQL 8.0+ recommended)\n- MySQL user with appropriate permissions for the operations you need\n- For write operations: MySQL user with INSERT, UPDATE, and/or DELETE privileges\n\n## Installation\n\nThere are several ways to install and configure the MCP server:\n\n### Claude Desktop\n\nTo manually configure the MCP server for Claude Desktop App, add the following to your `claude_desktop_config.json` file (typically located in your user directory):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"your_password\",\n        \"MYSQL_DB\": \"your_database\",\n        \"ALLOW_INSERT_OPERATION\": \"false\",\n        \"ALLOW_UPDATE_OPERATION\": \"false\",\n        \"ALLOW_DELETE_OPERATION\": \"false\",\n           \"PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/bin:/usr/bin:/bin\", // <--- Important to add the following, run in your terminal `echo \"$(which node)/../\"` to get the path\n           \"NODE_PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/lib/node_modules\" // <--- Important to add the following, run in your terminal `echo \"$(which node)/../../lib/node_modules\"`\n      }\n    }\n  }\n}\n```\n\n### Cursor\n\nFor Cursor IDE, you can install this MCP server with the following command in your project:\n\n\n```\nnpx mcprunner MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 MYSQL_USER=root MYSQL_PASS=root MYSQL_DB=demostore ALLOW_INSERT_OPERATION=true ALLOW_UPDATE_OPERATION=true ALLOW_DELETE_OPERATION=false -- npx -y @benborla29/mcp-server-mysql\n```\nDon't forget to replace the `env` values on that command. If you have the latest version (for v0.47 and above) of Cursor, just copy and paste the config below:\n\n`mcp.json`\n```json\n{\n  \"mcpServers\": {\n    \"MySQL\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcprunner\",\n        \"MYSQL_HOST=127.0.0.1\",\n        \"MYSQL_PORT=3306\",\n        \"MYSQL_USER=root\",\n        \"MYSQL_PASS=root\",\n        \"MYSQL_DB=demostore\",\n        \"ALLOW_INSERT_OPERATION=true\",\n        \"ALLOW_UPDATE_OPERATION=true\",\n        \"ALLOW_DELETE_OPERATION=false\",\n        \"--\",\n        \"npx\",\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ]\n    }\n  }\n}\n```\n\n### Using Smithery\n\nThe easiest way to install and configure this MCP server is through [Smithery](https://smithery.ai/server/@benborla29/mcp-server-mysql):\n\n```bash\nnpx -y @smithery/cli@latest install @benborla29/mcp-server-mysql --client claude\n```\n\n\nDuring configuration, you'll be prompted to enter your MySQL connection details. Smithery will automatically:\n- Set up the correct environment variables\n- Configure your LLM application to use the MCP server\n- Test the connection to your MySQL database\n- Provide helpful troubleshooting if needed\n- Configure write operation settings (INSERT, UPDATE, DELETE permissions)\n\nThe installation will ask for the following connection details:\n- MySQL Host (default: 127.0.0.1)\n- MySQL Port (default: 3306)\n- MySQL Username\n- MySQL Password\n- MySQL Database name\n- SSL Configuration (if needed)\n- Write operations permissions:\n  - Allow INSERT operations (default: false)\n  - Allow UPDATE operations (default: false)\n  - Allow DELETE operations (default: false)\n\nFor security reasons, write operations are disabled by default. Enable them only if you need Claude to modify your database data.\n\n### Using MCP Get\n\nYou can also install this package using [MCP Get](https://mcp-get.com/packages/%40benborla29%2Fmcp-server-mysql):\n\n```bash\nnpx @michaellatman/mcp-get@latest install @benborla29/mcp-server-mysql\n```\n\nMCP Get provides a centralized registry of MCP servers and simplifies the installation process.\n\n### Using NPM/PNPM\n\nFor manual installation:\n\n```bash\n# Using npm\nnpm install -g @benborla29/mcp-server-mysql\n\n# Using pnpm\npnpm add -g @benborla29/mcp-server-mysql\n```\n\nAfter manual installation, you'll need to configure your LLM application to use the MCP server (see Configuration section below).\n\n### Running from Local Repository\n\nIf you want to clone and run this MCP server directly from the source code, follow these steps:\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/benborla/mcp-server-mysql.git\n   cd mcp-server-mysql\n   ```\n\n2. **Install dependencies**\n   ```bash\n   npm install\n   # or\n   pnpm install\n   ```\n\n3. **Build the project**\n   ```bash\n   npm run build\n   # or\n   pnpm run build\n   ```\n\n4. **Configure Claude Desktop**\n\n   Add the following to your Claude Desktop configuration file (`claude_desktop_config.json`):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp_server_mysql\": {\n         \"command\": \"/path/to/node\",\n         \"args\": [\n           \"/full/path/to/mcp-server-mysql/dist/index.js\"\n         ],\n         \"env\": {\n           \"MYSQL_HOST\": \"127.0.0.1\",\n           \"MYSQL_PORT\": \"3306\",\n           \"MYSQL_USER\": \"root\",\n           \"MYSQL_PASS\": \"your_password\",\n           \"MYSQL_DB\": \"your_database\",\n           \"ALLOW_INSERT_OPERATION\": \"false\",\n           \"ALLOW_UPDATE_OPERATION\": \"false\",\n           \"ALLOW_DELETE_OPERATION\": \"false\",\n           \"PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/bin:/usr/bin:/bin\", // <--- Important to add the following, run in your terminal `echo \"$(which node)/../\"` to get the path\n           \"NODE_PATH\": \"/Users/atlasborla/Library/Application Support/Herd/config/nvm/versions/node/v22.9.0/lib/node_modules\" // <--- Important to add the following, run in your terminal `echo \"$(which node)/../../lib/node_modules\"`\n         }\n       }\n     }\n   }\n   ```\n\n   Replace:\n   - `/path/to/node` with the full path to your Node.js binary (find it with `which node`)\n   - `/full/path/to/mcp-server-mysql` with the full path to where you cloned the repository\n   - Set the MySQL credentials to match your environment\n\n5. **Test the server**\n   ```bash\n   # Run the server directly to test\n   node dist/index.js\n   ```\n\n   If it connects to MySQL successfully, you're ready to use it with Claude Desktop.\n\n## Components\n\n### Tools\n\n- **mysql_query**\n  - Execute SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - By default, limited to READ ONLY operations\n  - Optional write operations (when enabled via configuration):\n    - INSERT: Add new data to tables (requires `ALLOW_INSERT_OPERATION=true`)\n    - UPDATE: Modify existing data (requires `ALLOW_UPDATE_OPERATION=true`)\n    - DELETE: Remove data (requires `ALLOW_DELETE_OPERATION=true`)\n  - All operations are executed within a transaction with proper commit/rollback handling\n  - Supports prepared statements for secure parameter handling\n  - Configurable query timeouts and result pagination\n  - Built-in query execution statistics\n\n### Resources\n\nThe server provides comprehensive database information:\n\n- **Table Schemas**\n  - JSON schema information for each table\n  - Column names and data types\n  - Index information and constraints\n  - Foreign key relationships\n  - Table statistics and metrics\n  - Automatically discovered from database metadata\n\n### Security Features\n\n- SQL injection prevention through prepared statements\n- Query whitelisting/blacklisting capabilities\n- Rate limiting for query execution\n- Query complexity analysis\n- Configurable connection encryption\n- Read-only transaction enforcement\n\n### Performance Optimizations\n\n- Optimized connection pooling\n- Query result caching\n- Large result set streaming\n- Query execution plan analysis\n- Configurable query timeouts\n\n### Monitoring and Debugging\n\n- Comprehensive query logging\n- Performance metrics collection\n- Error tracking and reporting\n- Health check endpoints\n- Query execution statistics\n\n## Configuration\n\n### Automatic Configuration with Smithery\nIf you installed using Smithery, your configuration is already set up. You can view or modify it with:\n\n```bash\nsmithery configure @benborla29/mcp-server-mysql\n```\n\nWhen reconfiguring, you can update any of the MySQL connection details as well as the write operation settings:\n\n- **Basic connection settings**:\n  - MySQL Host, Port, User, Password, Database\n  - SSL/TLS configuration (if your database requires secure connections)\n\n- **Write operation permissions**:\n  - Allow INSERT Operations: Set to true if you want to allow adding new data\n  - Allow UPDATE Operations: Set to true if you want to allow updating existing data\n  - Allow DELETE Operations: Set to true if you want to allow deleting data\n\nFor security reasons, all write operations are disabled by default. Only enable these settings if you specifically need Claude to modify your database data.\n\n### Advanced Configuration Options\nFor more control over the MCP server's behavior, you can use these advanced configuration options:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"/path/to/npx/binary/npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ],\n      \"env\": {\n        // Basic connection settings\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\",\n        \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\",\n        \n        // Performance settings\n        \"MYSQL_POOL_SIZE\": \"10\",\n        \"MYSQL_QUERY_TIMEOUT\": \"30000\",\n        \"MYSQL_CACHE_TTL\": \"60000\",\n        \n        // Security settings\n        \"MYSQL_RATE_LIMIT\": \"100\",\n        \"MYSQL_MAX_QUERY_COMPLEXITY\": \"1000\",\n        \"MYSQL_SSL\": \"true\",\n        \n        // Monitoring settings\n        \"MYSQL_ENABLE_LOGGING\": \"true\",\n        \"MYSQL_LOG_LEVEL\": \"info\",\n        \"MYSQL_METRICS_ENABLED\": \"true\",\n        \n        // Write operation flags\n        \"ALLOW_INSERT_OPERATION\": \"false\",\n        \"ALLOW_UPDATE_OPERATION\": \"false\",\n        \"ALLOW_DELETE_OPERATION\": \"false\"\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n### Basic Connection\n- `MYSQL_HOST`: MySQL server host (default: \"127.0.0.1\")\n- `MYSQL_PORT`: MySQL server port (default: \"3306\")\n- `MYSQL_USER`: MySQL username (default: \"root\")\n- `MYSQL_PASS`: MySQL password\n- `MYSQL_DB`: Target database name (leave empty for multi-DB mode)\n\n### Performance Configuration\n- `MYSQL_POOL_SIZE`: Connection pool size (default: \"10\")\n- `MYSQL_QUERY_TIMEOUT`: Query timeout in milliseconds (default: \"30000\")\n- `MYSQL_CACHE_TTL`: Cache time-to-live in milliseconds (default: \"60000\")\n\n### Security Configuration\n- `MYSQL_RATE_LIMIT`: Maximum queries per minute (default: \"100\")\n- `MYSQL_MAX_QUERY_COMPLEXITY`: Maximum query complexity score (default: \"1000\")\n- `MYSQL_SSL`: Enable SSL/TLS encryption (default: \"false\")\n- `ALLOW_INSERT_OPERATION`: Enable INSERT operations (default: \"false\")\n- `ALLOW_UPDATE_OPERATION`: Enable UPDATE operations (default: \"false\")\n- `ALLOW_DELETE_OPERATION`: Enable DELETE operations (default: \"false\")\n- `ALLOW_DDL_OPERATION`: Enable DDL operations (default: \"false\")\n- `SCHEMA_INSERT_PERMISSIONS`: Schema-specific INSERT permissions\n- `SCHEMA_UPDATE_PERMISSIONS`: Schema-specific UPDATE permissions\n- `SCHEMA_DELETE_PERMISSIONS`: Schema-specific DELETE permissions\n- `SCHEMA_DDL_PERMISSIONS`: Schema-specific DDL permissions\n- `MULTI_DB_WRITE_MODE`: Enable write operations in multi-DB mode (default: \"false\")\n\n### Monitoring Configuration\n- `MYSQL_ENABLE_LOGGING`: Enable query logging (default: \"false\")\n- `MYSQL_LOG_LEVEL`: Logging level (default: \"info\")\n- `MYSQL_METRICS_ENABLED`: Enable performance metrics (default: \"false\")\n\n## Multi-DB Mode\n\nMCP-Server-MySQL supports connecting to multiple databases when no specific database is set. This allows the LLM to query any database the MySQL user has access to. For full details, see [README-MULTI-DB.md](./README-MULTI-DB.md).\n\n### Enabling Multi-DB Mode\n\nTo enable multi-DB mode, simply leave the `MYSQL_DB` environment variable empty. In multi-DB mode, queries require schema qualification:\n\n```sql\n-- Use fully qualified table names\nSELECT * FROM database_name.table_name;\n\n-- Or use USE statements to switch between databases\nUSE database_name;\nSELECT * FROM table_name;\n```\n\n## Schema-Specific Permissions\n\nFor fine-grained control over database operations, MCP-Server-MySQL now supports schema-specific permissions. This allows different databases to have different levels of access (read-only, read-write, etc.).\n\n### Configuration Example\n\n```\nSCHEMA_INSERT_PERMISSIONS=development:true,test:true,production:false\nSCHEMA_UPDATE_PERMISSIONS=development:true,test:true,production:false\nSCHEMA_DELETE_PERMISSIONS=development:false,test:true,production:false\nSCHEMA_DDL_PERMISSIONS=development:false,test:true,production:false\n```\n\nFor complete details and security recommendations, see [README-MULTI-DB.md](./README-MULTI-DB.md).\n\n## Testing\n\n### Database Setup\n\nBefore running tests, you need to set up the test database and seed it with test data:\n\n1. **Create Test Database and User**\n   ```sql\n   -- Connect as root and create test database\n   CREATE DATABASE IF NOT EXISTS mcp_test;\n   \n   -- Create test user with appropriate permissions\n   CREATE USER IF NOT EXISTS 'mcp_test'@'localhost' IDENTIFIED BY 'mcp_test_password';\n   GRANT ALL PRIVILEGES ON mcp_test.* TO 'mcp_test'@'localhost';\n   FLUSH PRIVILEGES;\n   ```\n\n2. **Run Database Setup Script**\n   ```bash\n   # Run the database setup script\n   pnpm run setup:test:db\n   ```\n\n   This will create the necessary tables and seed data. The script is located in `scripts/setup-test-db.ts`\n\n3. **Configure Test Environment**\n   Create a `.env.test` file in the project root (if not existing):\n   ```env\n   MYSQL_HOST=127.0.0.1\n   MYSQL_PORT=3306\n   MYSQL_USER=mcp_test\n   MYSQL_PASS=mcp_test_password\n   MYSQL_DB=mcp_test\n   ```\n\n4. **Update package.json Scripts**\n   Add these scripts to your package.json:\n   ```json\n   {\n     \"scripts\": {\n       \"setup:test:db\": \"ts-node scripts/setup-test-db.ts\",\n       \"pretest\": \"pnpm run setup:test:db\",\n       \"test\": \"vitest run\",\n       \"test:watch\": \"vitest\",\n       \"test:coverage\": \"vitest run --coverage\"\n     }\n   }\n   ```\n\n### Running Tests\n\nThe project includes a comprehensive test suite to ensure functionality and reliability:\n\n```bash\n# First-time setup\npnpm run setup:test:db\n\n# Run all tests\npnpm test\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection Issues**\n   - Verify MySQL server is running and accessible\n   - Check credentials and permissions\n   - Ensure SSL/TLS configuration is correct if enabled\n   - Try connecting with a MySQL client to confirm access\n\n2. **Performance Issues**\n   - Adjust connection pool size\n   - Configure query timeout values\n   - Enable query caching if needed\n   - Check query complexity settings\n   - Monitor server resource usage\n\n3. **Security Restrictions**\n   - Review rate limiting configuration\n   - Check query whitelist/blacklist settings\n   - Verify SSL/TLS settings\n   - Ensure the user has appropriate MySQL permissions\n\n4. **Path Resolution**\nIf you encounter an error \"Could not connect to MCP server mcp-server-mysql\", explicitly set the path of all required binaries:\n```json\n{\n  \"env\": {\n    \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\"\n  }\n}\n```\n\n*Where can I find my `node` bin path*\nRun the following command to get it:\n\nFor **PATH**\n```bash\necho \"$(which node)/../\"    \n```\n\nFor **NODE_PATH**\n```bash\necho \"$(which node)/../../lib/node_modules\"    \n```\n\n5. **Claude Desktop Specific Issues**\n   - If you see \"Server disconnected\" logs in Claude Desktop, check the logs at `~/Library/Logs/Claude/mcp-server-mcp_server_mysql.log`\n   - Ensure you're using the absolute path to both the Node binary and the server script\n   - Check if your `.env` file is being properly loaded; use explicit environment variables in the configuration\n   - Try running the server directly from the command line to see if there are connection issues\n   - If you need write operations (INSERT, UPDATE, DELETE), set the appropriate flags to \"true\" in your configuration:\n     ```json\n     \"env\": {\n       \"ALLOW_INSERT_OPERATION\": \"true\",  // Enable INSERT operations\n       \"ALLOW_UPDATE_OPERATION\": \"true\",  // Enable UPDATE operations\n       \"ALLOW_DELETE_OPERATION\": \"true\"   // Enable DELETE operations\n     }\n     ```\n   - Ensure your MySQL user has the appropriate permissions for the operations you're enabling\n   - For direct execution configuration, use:\n     ```json\n     {\n       \"mcpServers\": {\n         \"mcp_server_mysql\": {\n           \"command\": \"/full/path/to/node\",\n           \"args\": [\n             \"/full/path/to/mcp-server-mysql/dist/index.js\"\n           ],\n           \"env\": {\n             \"MYSQL_HOST\": \"127.0.0.1\",\n             \"MYSQL_PORT\": \"3306\",\n             \"MYSQL_USER\": \"root\",\n             \"MYSQL_PASS\": \"your_password\",\n             \"MYSQL_DB\": \"your_database\"\n           }\n         }\n       }\n     }\n     ```\n\n6. **Authentication Issues**\n   - For MySQL 8.0+, ensure the server supports the `caching_sha2_password` authentication plugin\n   - Check if your MySQL user is configured with the correct authentication method\n   - Try creating a user with legacy authentication if needed:\n     ```sql\n     CREATE USER 'user'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';\n     ```\n     @lizhuangs\n\n7. I am encountering `Error [ERR_MODULE_NOT_FOUND]: Cannot find package 'dotenv' imported from` error\ntry this workaround:\n```bash\nnpx -y -p @benborla29/mcp-server-mysql -p dotenv mcp-server-mysql\n```\nThanks to @lizhuangs\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to \nhttps://github.com/benborla/mcp-server-mysql\n\n### Development Setup\n\n1. Clone the repository\n2. Install dependencies: `pnpm install`\n3. Build the project: `pnpm run build`\n4. Run tests: `pnpm test`\n\n### Project Roadmap\n\nWe're actively working on enhancing this MCP server. Check our [CHANGELOG.md](./CHANGELOG.md) for details on planned features, including:\n\n- Enhanced query capabilities with prepared statements\n- Advanced security features\n- Performance optimizations\n- Comprehensive monitoring\n- Expanded schema information\n\nIf you'd like to contribute to any of these areas, please check the issues on GitHub or open a new one to discuss your ideas.\n\n### Submitting Changes\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/your-feature-name`\n3. Commit your changes: `git commit -am 'Add some feature'`\n4. Push to the branch: `git push origin feature/your-feature-name`\n5. Submit a pull request\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "databases",
        "mysql",
        "secure database",
        "databases secure",
        "mysql provides"
      ],
      "category": "databases"
    },
    "kevinwatt--mysql-mcp": {
      "owner": "kevinwatt",
      "name": "mysql-mcp",
      "url": "https://github.com/kevinwatt/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kevinwatt.webp",
      "description": "Integrate MySQL databases with language models to facilitate secure database access and operations. Supports both read and write operations including transaction management and parameterized queries for data safety.",
      "stars": 17,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T11:30:03Z",
      "readme_content": "# MySQL MCP Server\n\nAn MCP server implementation that integrates with MySQL databases, providing secure database access capabilities for LLMs.\n\n## Features\n\n* **Read Operations**\n  * Execute read-only SELECT queries\n  * List all database tables\n  * Show table structures\n  * View schema information\n* **Write Operations**\n  * Execute INSERT/UPDATE/DELETE with transaction support\n  * Parameterized queries for data safety\n  * Returns affected rows and insert IDs\n* **Security**\n  * Read-only transaction mode for SELECT queries\n  * Query length and result size limits\n  * Performance monitoring and logging\n  * Automatic transaction handling\n\n## Installation\n\n```bash\nnpm install -g @kevinwatt/mysql-mcp\n```\n\n## Usage with [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n\n1. Click \"+ Add MCP Server\" in Dive Desktop\n2. Copy and paste this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/mysql-mcp\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n3. Click \"Save\" to install the MCP server\n\n## Tool Documentation\n\n* **mysql_query**\n  * Execute read-only SELECT queries\n  * Inputs:\n    * `sql` (string): SQL SELECT query to execute\n  * Limits:\n    * Maximum query length: 4096 characters\n    * Maximum result rows: 1000\n    * Query timeout: 30 seconds\n\n* **mysql_execute**\n  * Execute data modification operations\n  * Inputs:\n    * `sql` (string): SQL statement (INSERT/UPDATE/DELETE)\n    * `params` (array, optional): Parameters for the SQL statement\n  * Features:\n    * Returns affected rows count\n    * Returns last insert ID\n    * Automatic transaction handling\n\n* **list_tables**\n  * List all tables in current database\n  * No inputs required\n\n* **describe_table**\n  * Show table structure\n  * Inputs:\n    * `table` (string): Table name to describe\n\n## Usage Examples\n\nAsk your LLM to:\n\n```\n\"Show me all tables in the database\"\n\"Describe the structure of users table\"\n\"Select all active users from the database\"\n\"Insert a new record into orders table\"\n```\n\n## Manual Start\n\nIf needed, start the server manually:\n\n```bash\nnpx @kevinwatt/mysql-mcp\n```\n\n## Requirements\n\n* Node.js 18+\n* MySQL Server\n* MCP-compatible LLM service\n\n## License\n\nMIT\n\n## Author\n\nDewei Yen\n\n## Keywords\n\n* mcp\n* mysql\n* database\n* dive\n* llm\n* ai\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "mysql databases"
      ],
      "category": "databases"
    },
    "kiliczsh--mcp-mongo-server": {
      "owner": "kiliczsh",
      "name": "mcp-mongo-server",
      "url": "https://github.com/kiliczsh/mcp-mongo-server",
      "imageUrl": "/freedevtools/mcp/pfp/kiliczsh.webp",
      "description": "Provides access to MongoDB databases for large language models, enabling inspection of collection schemas and execution of read-only queries.",
      "stars": 264,
      "forks": 48,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T14:51:07Z",
      "readme_content": "# MCP MongoDB Server\n---\n\n![NPM Version](https://img.shields.io/npm/v/mcp-mongo-server)\n![NPM Downloads](https://img.shields.io/npm/dm/mcp-mongo-server)\n![NPM License](https://img.shields.io/npm/l/mcp-mongo-server)\n[![smithery badge](https://smithery.ai/badge/mcp-mongo-server)](https://smithery.ai/server/mcp-mongo-server)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/e274a3dd-7fe6-4440-8c43-043bae668251)\n\nA Model Context Protocol server that enables LLMs to interact with MongoDB databases. This server provides capabilities for inspecting collection schemas and executing MongoDB operations through a standardized interface.\n\n## Demo\n\n[![MCP MongoDB Server Demo | Claude Desktop](https://img.youtube.com/vi/FI-oE_voCpA/0.jpg)](https://www.youtube.com/watch?v=FI-oE_voCpA)\n\n## Key Features\n\n### Smart ObjectId Handling\n- Intelligent conversion between string IDs and MongoDB ObjectId\n- Configurable with `objectIdMode` parameter:\n  - `\"auto\"`: Convert based on field names (default)\n  - `\"none\"`: No conversion\n  - `\"force\"`: Force all string ID fields to ObjectId\n\n### Flexible Configuration\n- **Environment Variables**:\n  - `MCP_MONGODB_URI`: MongoDB connection URI\n  - `MCP_MONGODB_READONLY`: Enable read-only mode when set to \"true\"\n- **Command-line Options**:\n  - `--read-only` or `-r`: Connect in read-only mode\n\n### Read-Only Mode\n- Protection against write operations (update, insert, createIndex)\n- Uses MongoDB's secondary read preference for optimal performance\n- Ideal for safely connecting to production databases\n\n### MongoDB Operations\n- **Read Operations**:\n  - Query documents with optional execution plan analysis\n  - Execute aggregation pipelines\n  - Count documents matching criteria\n  - Get collection schema information\n- **Write Operations** (when not in read-only mode):\n  - Update documents\n  - Insert new documents\n  - Create indexes\n\n### LLM Integration\n- Collection completions for enhanced LLM interaction\n- Schema inference for improved context understanding\n- Collection analysis for data insights\n\n## Installation\n\n### Global Installation\n\n```bash\nnpm install -g mcp-mongo-server\n```\n\n### For Development\n\n```bash\n# Clone repository\ngit clone https://github.com/kiliczsh/mcp-mongo-server.git\ncd mcp-mongo-server\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Development with auto-rebuild\nnpm run watch\n```\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Start server with MongoDB URI\nnpx -y mcp-mongo-server mongodb://muhammed:kilic@localhost:27017/database\n\n# Connect in read-only mode\nnpx -y mcp-mongo-server mongodb://muhammed:kilic@localhost:27017/database --read-only\n```\n\n### Environment Variables\n\nYou can configure the server using environment variables, which is particularly useful for CI/CD pipelines, Docker containers, or when you don't want to expose connection details in command arguments:\n\n```bash\n# Set MongoDB connection URI\nexport MCP_MONGODB_URI=\"mongodb://muhammed:kilic@localhost:27017/database\"\n\n# Enable read-only mode\nexport MCP_MONGODB_READONLY=\"true\"\n\n# Run server (will use environment variables if no URI is provided)\nnpx -y mcp-mongo-server\n```\n\nUsing environment variables in Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb-env\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\"\n      ],\n      \"env\": {\n        \"MCP_MONGODB_URI\": \"mongodb://muhammed:kilic@localhost:27017/database\",\n        \"MCP_MONGODB_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n\nUsing environment variables with Docker:\n\n```bash\n# Build\ndocker build -t mcp-mongo-server .\n\n# Run\ndocker run -it -d -e MCP_MONGODB_URI=\"mongodb://muhammed:kilic@localhost:27017/database\" -e MCP_MONGODB_READONLY=\"true\" mcp-mongo-server\n\n# or edit docker-compose.yml and run\ndocker-compose up -d\n```\n\n## Integration with Claude Desktop\n\n### Manual Configuration\n\nAdd the server configuration to Claude Desktop's config file:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Command-line Arguments Approach:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\"\n      ]\n    },\n    \"mongodb-readonly\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\",\n        \"--read-only\"\n      ]\n    }\n  }\n}\n```\n\n#### Environment Variables Approach:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\"\n      ],\n      \"env\": {\n        \"MCP_MONGODB_URI\": \"mongodb://muhammed:kilic@localhost:27017/database\"\n      }\n    },\n    \"mongodb-readonly\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\"\n      ],\n      \"env\": {\n        \"MCP_MONGODB_URI\": \"mongodb://muhammed:kilic@localhost:27017/database\",\n        \"MCP_MONGODB_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n\n### GitHub Package Usage:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:kiliczsh/mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\"\n      ]\n    },\n    \"mongodb-readonly\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:kiliczsh/mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\",\n        \"--read-only\"\n      ]\n    }\n  }\n}\n```\n\n## Integration with Windsurf and Cursor\n\nThe MCP MongoDB Server can be used with Windsurf and Cursor in a similar way to Claude Desktop.\n\n### Windsurf Configuration\n\nAdd the server to your Windsurf configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\"\n      ]\n    }\n  }\n}\n```\n\n### Cursor Configuration\n\nFor Cursor, add the server configuration to your settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mongo-server\",\n        \"mongodb://muhammed:kilic@localhost:27017/database\"\n      ]\n    }\n  }\n}\n```\n\nYou can also use the environment variables approach with both Windsurf and Cursor, following the same pattern shown in the Claude Desktop configuration.\n\n### Automated Installation\n\n**Using Smithery**:\n```bash\nnpx -y @smithery/cli install mcp-mongo-server --client claude\n```\n\n**Using mcp-get**:\n```bash\nnpx @michaellatman/mcp-get@latest install mcp-mongo-server\n```\n\n## Available Tools\n\n### Query Operations\n\n- **query**: Execute MongoDB queries\n  ```javascript\n  {\n    collection: \"users\",\n    filter: { age: { $gt: 30 } },\n    projection: { name: 1, email: 1 },\n    limit: 20,\n    explain: \"executionStats\"  // Optional\n  }\n  ```\n\n- **aggregate**: Run aggregation pipelines\n  ```javascript\n  {\n    collection: \"orders\",\n    pipeline: [\n      { $match: { status: \"completed\" } },\n      { $group: { _id: \"$customerId\", total: { $sum: \"$amount\" } } }\n    ],\n    explain: \"queryPlanner\"  // Optional\n  }\n  ```\n\n- **count**: Count matching documents\n  ```javascript\n  {\n    collection: \"products\",\n    query: { category: \"electronics\" }\n  }\n  ```\n\n### Write Operations\n\n- **update**: Modify documents\n  ```javascript\n  {\n    collection: \"posts\",\n    filter: { _id: \"60d21b4667d0d8992e610c85\" },\n    update: { $set: { title: \"Updated Title\" } },\n    upsert: false,\n    multi: false\n  }\n  ```\n\n- **insert**: Add new documents\n  ```javascript\n  {\n    collection: \"comments\",\n    documents: [\n      { author: \"user123\", text: \"Great post!\" },\n      { author: \"user456\", text: \"Thanks for sharing\" }\n    ]\n  }\n  ```\n\n- **createIndex**: Create collection indexes\n  ```javascript\n  {\n    collection: \"users\",\n    indexes: [\n      {\n        key: { email: 1 },\n        unique: true,\n        name: \"email_unique_idx\"\n      }\n    ]\n  }\n  ```\n\n### System Operations\n\n- **serverInfo**: Get MongoDB server details\n  ```javascript\n  {\n    includeDebugInfo: true  // Optional\n  }\n  ```\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. Use the MCP Inspector for better visibility:\n\n```bash\nnpm run inspector\n```\n\nThis will provide a URL to access the debugging tools in your browser.\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/schemas/tools.ts\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "mongo",
        "databases",
        "mongodb databases",
        "access mongodb",
        "secure database"
      ],
      "category": "databases"
    },
    "kitae-kim-Edwin--mcp-bigquery-server": {
      "owner": "kitae-kim-Edwin",
      "name": "mcp-bigquery-server",
      "url": "https://github.com/kitae-kim-Edwin/mcp-bigquery-server",
      "imageUrl": "/freedevtools/mcp/pfp/kitae-kim-Edwin.webp",
      "description": "Enable natural language querying of BigQuery datasets to retrieve data and explore schemas without writing SQL. Provides secure read-only access and query limits for efficient data interaction.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-21T13:04:30Z",
      "readme_content": "# BigQuery MCP Server\n[![smithery badge](https://smithery.ai/badge/@ergut/mcp-bigquery-server)](https://smithery.ai/protocol/@ergut/mcp-bigquery-server)\n<div align=\"center\">\n  \n</div>\n\n## What is this? 🤔\n\nThis is a server that lets your LLMs (like Claude) talk directly to your BigQuery data! Think of it as a friendly translator that sits between your AI assistant and your database, making sure they can chat securely and efficiently.\n\n### Quick Example\n```text\nYou: \"What were our top 10 customers last month?\"\nClaude: *queries your BigQuery database and gives you the answer in plain English*\n```\n\nNo more writing SQL queries by hand - just chat naturally with your data!\n\n## How Does It Work? 🛠️\n\nThis server uses the Model Context Protocol (MCP), which is like a universal translator for AI-database communication. While MCP is designed to work with any AI model, right now it's available as a developer preview in Claude Desktop.\n\nHere's all you need to do:\n1. Set up authentication (see below)\n2. Add your project details to Claude Desktop's config file\n3. Start chatting with your BigQuery data naturally!\n\n### What Can It Do? 📊\n\n- Run SQL queries by just asking questions in plain English\n- Access both tables and materialized views in your datasets\n- Explore dataset schemas with clear labeling of resource types (tables vs views)\n- Analyze data within safe limits (1GB query limit by default)\n- Keep your data secure (read-only access)\n\n## Quick Start 🚀\n\n### Prerequisites\n- Node.js 14 or higher\n- Google Cloud project with BigQuery enabled\n- Either Google Cloud CLI installed or a service account key file\n- Claude Desktop (currently the only supported LLM interface)\n\n### Option 1: Quick Install via Smithery (Recommended)\nTo install BigQuery MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/protocol/@ergut/mcp-bigquery-server), run this command in your terminal:\n\n```bash\nnpx @smithery/cli install @ergut/mcp-bigquery-server --client claude\n```\nThe installer will prompt you for:\n\n- Your Google Cloud project ID\n- BigQuery location (defaults to us-central1)\n\nOnce configured, Smithery will automatically update your Claude Desktop configuration and restart the application.\n\n### Option 2: Manual Setup\nIf you prefer manual configuration or need more control:\n\n1. **Authenticate with Google Cloud** (choose one method):\n   - Using Google Cloud CLI (great for development):\n     ```bash\n     gcloud auth application-default login\n     ```\n   - Using a service account (recommended for production):\n     ```bash\n     # Save your service account key file and use --key-file parameter\n     # Remember to keep your service account key file secure and never commit it to version control\n     ```\n\n2. **Add to your Claude Desktop config**\n   Add this to your `claude_desktop_config.json`:\n\n   - Basic configuration:\n     ```json\n     {\n       \"mcpServers\": {\n         \"bigquery\": {\n           \"command\": \"npx\",\n           \"args\": [\n             \"-y\",\n             \"@ergut/mcp-bigquery-server\",\n             \"--project-id\",\n             \"your-project-id\",\n             \"--location\",\n             \"us-central1\"\n           ]\n         }\n       }\n     }\n     ```\n\n   - With service account:\n     ```json\n     {\n       \"mcpServers\": {\n         \"bigquery\": {\n           \"command\": \"npx\",\n           \"args\": [\n             \"-y\",\n             \"@ergut/mcp-bigquery-server\",\n             \"--project-id\",\n             \"your-project-id\",\n             \"--location\",\n             \"us-central1\",\n             \"--key-file\",\n             \"/path/to/service-account-key.json\"\n           ]\n         }\n       }\n     }\n     ```\n     \n\n3. **Start chatting!** \n   Open Claude Desktop and start asking questions about your data.\n\n### Command Line Arguments\n\nThe server accepts the following arguments:\n- `--project-id`: (Required) Your Google Cloud project ID\n- `--location`: (Optional) BigQuery location, defaults to 'us-central1'\n- `--key-file`: (Optional) Path to service account key JSON file\n\nExample using service account:\n```bash\nnpx @ergut/mcp-bigquery-server --project-id your-project-id --location europe-west1 --key-file /path/to/key.json\n```\n\n### Permissions Needed\n\nYou'll need one of these:\n- `roles/bigquery.user` (recommended)\n- OR both:\n  - `roles/bigquery.dataViewer`\n  - `roles/bigquery.jobUser`\n\n## Developer Setup (Optional) 🔧\n\nWant to customize or contribute? Here's how to set it up locally:\n\n```bash\n# Clone and install\ngit clone https://github.com/ergut/mcp-bigquery-server\ncd mcp-bigquery-server\nnpm install\n\n# Build\nnpm run build\n```\n\nThen update your Claude Desktop config to point to your local build:\n```json\n{\n  \"mcpServers\": {\n    \"bigquery\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/clone/mcp-bigquery-server/dist/index.js\",\n        \"--project-id\",\n        \"your-project-id\",\n        \"--location\",\n        \"us-central1\",\n        \"--key-file\",\n        \"/path/to/service-account-key.json\"\n      ]\n    }\n  }\n}\n```\n\n## Current Limitations ⚠️\n\n- MCP support is currently only available in Claude Desktop (developer preview)\n- Connections are limited to local MCP servers running on the same machine\n- Queries are read-only with a 1GB processing limit\n- While both tables and views are supported, some complex view types might have limitations\n\n## Support & Resources 💬\n\n- 🐛 [Report issues](https://github.com/ergut/mcp-bigquery-server/issues)\n- 💡 [Feature requests](https://github.com/ergut/mcp-bigquery-server/issues)\n- 📖 [Documentation](https://github.com/ergut/mcp-bigquery-server)\n\n## License 📝\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## Author ✍️ \n\nSalih Ergüt\n\n## Sponsorship\n\nThis project is proudly sponsored by:\n\n<div align=\"center\">\n  <a href=\"https://www.oredata.com\">\n    \n  </a>\n</div>\n\n## Version History 📋\n\nSee [CHANGELOG.md](CHANGELOG.md) for updates and version history.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bigquery",
        "databases",
        "database",
        "bigquery datasets",
        "bigquery server",
        "secure database"
      ],
      "category": "databases"
    },
    "knight0zh--mssql-mcp-server": {
      "owner": "knight0zh",
      "name": "mssql-mcp-server",
      "url": "https://github.com/knight0zh/mssql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/knight0zh.webp",
      "description": "Connect to Microsoft SQL Server databases to execute SQL queries and manage database connections. Enhance data handling through direct query execution and connection management within applications.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-16T20:12:25Z",
      "readme_content": "# MSSQL MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@knight0zh/mssql-mcp-server)](https://smithery.ai/server/@knight0zh/mssql-mcp-server)\n\nA Model Context Protocol (MCP) server for connecting to Microsoft SQL Server databases. This server provides tools for executing SQL queries and managing database connections.\n\n## Installation\n\n### Installing via Smithery\n\nTo install MSSQL Database Connector for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@knight0zh/mssql-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @knight0zh/mssql-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install mssql-mcp-server\n```\n\n## Usage\n\nAdd the server to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"mssql-mcp-server\",\n      \"env\": {\n        \"MSSQL_CONNECTION_STRING\": \"Server=localhost;Database=master;User Id=sa;Password=yourpassword;\",\n        // Or individual connection parameters:\n        \"MSSQL_HOST\": \"localhost\",\n        \"MSSQL_PORT\": \"1433\",\n        \"MSSQL_DATABASE\": \"master\",\n        \"MSSQL_USER\": \"sa\",\n        \"MSSQL_PASSWORD\": \"yourpassword\",\n        \"MSSQL_ENCRYPT\": \"false\",\n        \"MSSQL_TRUST_SERVER_CERTIFICATE\": \"true\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\n### query\n\nExecute a SQL query on a MSSQL database.\n\n#### Parameters\n\n- `connectionString` (string, optional): Full connection string (alternative to individual parameters)\n- `host` (string, optional): Database server hostname\n- `port` (number, optional): Database server port (default: 1433)\n- `database` (string, optional): Database name (default: master)\n- `username` (string, optional): Database username\n- `password` (string, optional): Database password\n- `query` (string, required): SQL query to execute\n- `encrypt` (boolean, optional): Enable encryption (default: false)\n- `trustServerCertificate` (boolean, optional): Trust server certificate (default: true)\n\nEither `connectionString` OR (`host` + `username` + `password`) must be provided.\n\n#### Example\n\n```typescript\nconst result = await use_mcp_tool({\n  server_name: 'mssql',\n  tool_name: 'query',\n  arguments: {\n    host: 'localhost',\n    username: 'sa',\n    password: 'yourpassword',\n    query: 'SELECT * FROM Users',\n  },\n});\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Run linter\nnpm run lint\n\n# Format code\nnpm run format\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "krimoi45--chroma-docker-rag": {
      "owner": "krimoi45",
      "name": "chroma-docker-rag",
      "url": "https://github.com/krimoi45/chroma-docker-rag",
      "imageUrl": "/freedevtools/mcp/pfp/krimoi45.webp",
      "description": "Enables semantic similarity search and vector collection management using ChromaDB in a Dockerized environment. Supports customized document collections and search queries through a Python script.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-15T00:39:35Z",
      "readme_content": "# RAG avec ChromaDB et Docker\n\n## Prérequis\n- Docker\n- Docker Compose\n\n## Installation et Démarrage\n\n1. Clonez le dépôt\n```bash\ngit clone https://github.com/krimoi45/chroma-docker-rag.git\ncd chroma-docker-rag\n```\n\n2. Démarrez les services\n```bash\ndocker-compose up --build\n```\n\n## Architecture\n\n- ChromaDB : Base de données vectorielle\n- Python App : Script de démonstration RAG\n- Docker Compose : Orchestration des services\n\n## Fonctionnalités\n\n- Création de collections vectorielles\n- Recherche de similarité sémantique\n- Configuration dynamique avec variables d'environnement\n\n## Technologies\n\n- ChromaDB\n- Sentence Transformers\n- Docker\n- Python\n\n## Utilisation\n\nLe script démontre :\n- La création d'une collection de documents\n- La génération d'embeddings\n- La recherche de documents similaires par similarité sémantique\n\n## Personnalisation\n\nModifiez `main.py` pour ajouter vos propres documents et requêtes de recherche.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "chromadb",
        "secure database",
        "databases secure",
        "chromadb dockerized"
      ],
      "category": "databases"
    },
    "ktanaka101--mcp-server-duckdb": {
      "owner": "ktanaka101",
      "name": "mcp-server-duckdb",
      "url": "https://github.com/ktanaka101/mcp-server-duckdb",
      "imageUrl": "/freedevtools/mcp/pfp/ktanaka101.webp",
      "description": "Interact with DuckDB databases, enabling data analysis and manipulation through the Model Context Protocol (MCP). Provides functions for querying and managing database interactions effectively.",
      "stars": 165,
      "forks": 18,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T02:51:57Z",
      "readme_content": "# mcp-server-duckdb\n\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-server-duckdb)](https://pypi.org/project/mcp-server-duckdb/)\n[![PyPI - License](https://img.shields.io/pypi/l/mcp-server-duckdb)](LICENSE)\n[![smithery badge](https://smithery.ai/badge/mcp-server-duckdb)](https://smithery.ai/server/mcp-server-duckdb)\n\nA Model Context Protocol (MCP) server implementation for DuckDB, providing database interaction capabilities through MCP tools.\nIt would be interesting to have LLM analyze it. DuckDB is suitable for local analysis.\n\n<a href=\"https://glama.ai/mcp/servers/fwggl49w22\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/fwggl49w22/badge\" alt=\"mcp-server-duckdb MCP server\" /></a>\n\n## Overview\n\nThis server enables interaction with a DuckDB database through the Model Context Protocol, allowing for database operations like querying, table creation, and schema inspection.\n\n## Components\n\n### Resources\n\nCurrently, no custom resources are implemented.\n\n### Prompts\n\nCurrently, no custom prompts are implemented.\n\n### Tools\n\nThe server implements the following database interaction tool:\n\n- **query**: Execute any SQL query on the DuckDB database\n  - **Input**: `query` (string) - Any valid DuckDB SQL statement\n  - **Output**: Query results as text (or success message for operations like CREATE/INSERT)\n\n> [!NOTE]\n> The server provides a single unified `query` function rather than separate specialized functions, as modern LLMs can generate appropriate SQL for any database operation (SELECT, CREATE TABLE, JOIN, etc.) without requiring separate endpoints.\n\n> [!NOTE]\n> When the server is running in `readonly` mode, DuckDB's native readonly protection is enforced.\n> This ensures that the Language Model (LLM) cannot perform any write operations (CREATE, INSERT, UPDATE, DELETE), maintaining data integrity and preventing unintended changes.\n\n## Configuration\n\n### Required Parameters\n\n- **db-path** (string): Path to the DuckDB database file\n  - The server will automatically create the database file and parent directories if they don't exist\n  - If `--readonly` is specified and the database file doesn't exist, the server will fail to start with an error\n\n### Optional Parameters\n\n- **--readonly**: Run server in read-only mode (default: `false`)\n  - **Description**: When this flag is set, the server operates in read-only mode. This means:\n    - The DuckDB database will be opened with `read_only=True`, preventing any write operations.\n    - If the specified database file does not exist, it **will not** be created.\n    - **Security Benefit**: Prevents the Language Model (LLM) from performing any write operations, ensuring that the database remains unaltered.\n  - **Reference**: For more details on read-only connections in DuckDB, see the [DuckDB Python API documentation](https://duckdb.org/docs/api/python/dbapi.html#read_only-connections).\n- **--keep-connection**: Re-uses a single DuckDB connection mode (default: `false`)\n  - **Description**: When this flag is set, Re-uses a single DuckDB connection for the entire server lifetime. Enables TEMP objects & slightly faster queries, but can hold an exclusive lock on the file.\n\n## Installation\n\n### Installing via Smithery\n\nTo install DuckDB Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-duckdb):\n\n```bash\nnpx -y @smithery/cli install mcp-server-duckdb --client claude\n```\n\n### Claude Desktop Integration\n\nConfigure the MCP server in Claude Desktop's configuration file:\n\n#### MacOS\nLocation: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n#### Windows\nLocation: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"duckdb\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-duckdb\",\n        \"--db-path\",\n        \"~/mcp-server-duckdb/data/data.db\"\n      ]\n    }\n  }\n}\n```\n\n* Note: `~/mcp-server-duckdb/data/data.db` should be replaced with the actual path to the DuckDB database file.\n\n## Development\n\n### Prerequisites\n\n- Python with `uv` package manager\n- DuckDB Python package\n- MCP server dependencies\n\n### Debugging\n\nDebugging MCP servers can be challenging due to their stdio-based communication. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) for the best debugging experience.\n\n#### Using MCP Inspector\n\n1. Install the inspector using npm:\n```bash\nnpx @modelcontextprotocol/inspector uv --directory ~/codes/mcp-server-duckdb run mcp-server-duckdb --db-path ~/mcp-server-duckdb/data/data.db\n```\n\n2. Open the provided URL in your browser to access the debugging interface\n\nThe inspector provides visibility into:\n- Request/response communication\n- Tool execution\n- Server state\n- Error messages\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckdb",
        "databases",
        "database",
        "duckdb databases",
        "server duckdb",
        "duckdb interact"
      ],
      "category": "databases"
    },
    "kukapay--thegraph-mcp": {
      "owner": "kukapay",
      "name": "thegraph-mcp",
      "url": "https://github.com/kukapay/thegraph-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kukapay.webp",
      "description": "Powers AI agents with indexed blockchain data, enabling them to fetch subgraph schemas and execute GraphQL queries against specified subgraphs.",
      "stars": 5,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-31T09:46:46Z",
      "readme_content": "# TheGraph MCP Server\n\nAn MCP server that powers AI agents with indexed blockchain data from [The Graph](https://thegraph.com/).\n\n<a href=\"https://glama.ai/mcp/servers/@kukapay/thegraph-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kukapay/thegraph-mcp/badge\" alt=\"TheGraph Server MCP server\" />\n</a>\n\n![GitHub License](https://img.shields.io/github/license/kukapay/thegraph-mcp) \n![GitHub Last Commit](https://img.shields.io/github/last-commit/kukapay/thegraph-mcp) \n![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)\n\n## Available Tools\n\n### 1. getSubgraphSchema\nFetches the schema of a specified subgraph, providing AI agents with the context needed to generate GraphQL queries. \n\n**Parameters:**\n- `subgraphId`: The subgraph ID (e.g., \"QmZBQcF...\")\n- `asText`: Output format flag\n  - `true`: Returns human-readable GraphQL schema\n  - `false`: Returns JSON schema (default)\n\n### 2. querySubgraph\nExecutes GraphQL queries against a specified subgraph. While queries are typically generated by AI, you can also manually craft your own.\n\n**Parameters:**\n- `subgraphId`: The subgraph ID\n- `query`: GraphQL query string\n\n\n## Installation\n\n1. **Clone the Repository**\n    ```bash\n    git clone https://github.com/kukapay/thegraph-mcp.git\n    ```\n2. **Client Configuration**\n    ```json\n    {\n      \"mcpServers\": {\n        \"thegraph-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\"--directory\", \"path/to/thegraph-mcp\", \"run\", \"main.py\"],\n          \"env\": {\n            \"THEGRAPH_API_KEY\": \"your_api_key_here\"\n          }\n        }\n      }\n    }\n    ```\n\n## Example Prompts\n\nHere are some natural language prompts to trigger the tools:\n\n### Schema Queries\n- \"Show me the schema for subgraph QmZBQcF... in a readable format\"\n- \"What's the structure of the QmZBQcF... subgraph? Please display it in GraphQL format\"\n- \"I need to understand the data model of subgraph QmZBQcF..., can you fetch its schema?\"\n\n### Data Queries\n- \"Find the top 5 tokens by trading volume in the last 24 hours from subgraph QmZBQcF...\"\n- \"Show me all pairs with liquidity greater than 1 million USD in subgraph QmZBQcF...\"\n- \"Get the latest 10 swap events from the QmZBQcF... subgraph, including token symbols and amounts\"\n\n### Analysis Tasks\n- \"Analyze the trading volume of USDT pairs in the last week using subgraph QmZBQcF...\"\n- \"Compare the liquidity of ETH and USDC pairs in subgraph QmZBQcF...\"\n- \"Find unusual trading patterns in the last 24 hours from subgraph QmZBQcF...\"\n\n### Combined Tasks\n- \"First get the schema of QmZBQcF..., then help me write a query to find high-value transactions\"\n- \"Check the schema of QmZBQcF... and tell me what fields are available for querying token prices\"\n- \"Using subgraph QmZBQcF..., analyze the market impact of large trades by first understanding the schema and then querying relevant events\"\n\n## License\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "graphql",
        "databases",
        "schemas",
        "graphql queries",
        "subgraph schemas",
        "databases secure"
      ],
      "category": "databases"
    },
    "l1806858547--tidb-server": {
      "owner": "l1806858547",
      "name": "tidb-server",
      "url": "https://github.com/l1806858547/tidb-server",
      "imageUrl": "/freedevtools/mcp/pfp/l1806858547.webp",
      "description": "Execute SELECT queries on TiDB databases securely and efficiently through a Model Context Protocol server, facilitating integration of TiDB query capabilities into applications with minimal setup.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-21T03:13:18Z",
      "readme_content": "# TiDB MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@l1806858547/tidb-server)](https://smithery.ai/server/@l1806858547/tidb-server)\n\nA Model Context Protocol (MCP) server for TiDB that allows executing SELECT queries through MCP tools.\n\n<a href=\"https://glama.ai/mcp/servers/@l1806858547/tidb-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@l1806858547/tidb-server/badge\" alt=\"TiDB Server MCP server\" />\n</a>\n\n## Features\n- Execute SELECT queries on TiDB\n- Secure connection via environment variables\n- Lightweight and easy to use\n\n## Prerequisites\n- Node.js 16+\n- TiDB instance\n\n## Installation\n\n### Installing via Smithery\n\nTo install TiDB Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@l1806858547/tidb-server):\n\n```bash\nnpx -y @smithery/cli install @l1806858547/tidb-server --client claude\n```\n\n### Via npx\n```bash\nnpx tidb-mcp-server\n```\n\n### Global installation\n```bash\nnpm install -g tidb-mcp-server\ntidb-mcp-server\n```\n\n## Configuration\n\nSet these environment variables before running:\n\n```bash\nexport TIDB_HOST=\"your_tidb_host\"\nexport TIDB_PORT=\"your_tidb_port\" \nexport TIDB_USER=\"your_username\"\nexport TIDB_PASS=\"your_password\"\nexport TIDB_DB=\"your_database\"\n\n# Optional operation permissions (default: false)\nexport ALLOW_INSERT_OPERATION=\"false\"  # Set to \"true\" to allow INSERT operations\nexport ALLOW_UPDATE_OPERATION=\"false\"  # Set to \"true\" to allow UPDATE operations \nexport ALLOW_DELETE_OPERATION=\"false\"  # Set to \"true\" to allow DELETE operations\n\nWARNING: Enabling these operations may expose your database to modification risks.\nOnly enable what you need and ensure proper access controls are in place.\n```\n\n## Usage\n\n1. Start the server:\n```bash\ntidb-server\n```\n\n2. Add to MCP configuration (cline_mcp_settings.json):\n```json\n{\n  \"mcpServers\": {\n    \"tidb-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tidb-mcp-server\"], # Also adding the -y flag for consistency\n      \"env\": {\n        \"TIDB_HOST\": \"your_tidb_host\",\n        \"TIDB_PORT\": \"your_tidb_port\",\n        \"TIDB_USER\": \"your_username\",\n        \"TIDB_PASS\": \"your_password\",\n        \"TIDB_DB\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n3. Use the MCP tool:\n```javascript\nconst result = await use_mcp_tool({\n  server_name: 'tidb-server',\n  tool_name: 'tidb_query', \n  arguments: {\n    sql: 'SELECT * FROM your_table LIMIT 10'\n  }\n});\n```\n\n## Development\n\n1. Clone the repo:\n```bash\ngit clone https://github.com/l1806858547/tidb-server.git\ncd tidb-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build:\n```bash\nnpm run build\n```\n\n4. Run:\n```bash\nnode build/index.js\n```\n\n## License\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tidb",
        "databases",
        "database",
        "tidb databases",
        "tidb server",
        "queries tidb"
      ],
      "category": "databases"
    },
    "leopeng1995--mssql-mcp-server": {
      "owner": "leopeng1995",
      "name": "mssql-mcp-server",
      "url": "https://github.com/leopeng1995/mssql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/leopeng1995.webp",
      "description": "Connect to Microsoft SQL Server to manage and retrieve data efficiently within applications using a standardized protocol. Facilitate database interactions and streamline data-driven workflows.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-07T08:22:35Z",
      "readme_content": "# mssql-mcp-server\n\nmssql-mcp-server is a Model Context Protocol (MCP) server for connecting to Microsoft SQL Server.\n\n## Installation\n\n```\ngit clone https://github.com/leopeng1995/mssql-mcp-server.git\ncd mssql-mcp-server\n\nuv sync\nuv run mssql-mcp-server\n```\n\n## Configuration in Cline\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"H:/workspaces/leopeng1995/mssql-mcp-server\",\n        \"run\",\n        \"mssql-mcp-server\"\n      ],\n      \"env\": {\n        \"MSSQL_SERVER\": \"localhost\",\n        \"MSSQL_PORT\": \"1433\",\n        \"MSSQL_USER\": \"username\",\n        \"MSSQL_PASSWORD\": \"password\",\n        \"MSSQL_DATABASE\": \"database\",\n        \"MSSQL_CHARSET\": \"UTF-8\" # or CP936 ...\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n**Note:** The `MSSQL_CHARSET` value is case-sensitive.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "lishenxydlgzs--aws-athena-mcp": {
      "owner": "lishenxydlgzs",
      "name": "aws-athena-mcp",
      "url": "https://github.com/lishenxydlgzs/aws-athena-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/lishenxydlgzs.webp",
      "description": "Execute SQL queries against AWS Athena databases and retrieve the results for analysis and reporting.",
      "stars": 36,
      "forks": 12,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T15:02:36Z",
      "readme_content": "# @lishenxydlgzs/aws-athena-mcp\n[![smithery badge](https://smithery.ai/badge/@lishenxydlgzs/aws-athena-mcp)](https://smithery.ai/server/@lishenxydlgzs/aws-athena-mcp)\n\nA Model Context Protocol (MCP) server for running AWS Athena queries. This server enables AI assistants to execute SQL queries against your AWS Athena databases and retrieve results.\n\n<a href=\"https://glama.ai/mcp/servers/0i7dhkex6t\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0i7dhkex6t/badge\" alt=\"aws-athena-mcp MCP server\" />\n</a>\n\n## Usage\n\n1. Configure AWS credentials using one of the following methods:\n   - AWS CLI configuration\n   - Environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`)\n   - IAM role (if running on AWS)\n\n2. Add the server to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"athena\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@lishenxydlgzs/aws-athena-mcp\"],\n      \"env\": {\n        // Required\n        \"OUTPUT_S3_PATH\": \"s3://your-bucket/athena-results/\",\n        \n        // Optional AWS configuration\n        \"AWS_REGION\": \"us-east-1\",                    // Default: AWS CLI default region\n        \"AWS_PROFILE\": \"default\",                     // Default: 'default' profile\n        \"AWS_ACCESS_KEY_ID\": \"\",                      // Optional: AWS access key\n        \"AWS_SECRET_ACCESS_KEY\": \"\",                  // Optional: AWS secret key\n        \"AWS_SESSION_TOKEN\": \"\",                      // Optional: AWS session token\n        \n        // Optional server configuration\n        \"ATHENA_WORKGROUP\": \"default_workgroup\",      // Optional: specify the Athena WorkGroup\n        \"QUERY_TIMEOUT_MS\": \"300000\",                 // Default: 5 minutes (300000ms)\n        \"MAX_RETRIES\": \"100\",                         // Default: 100 attempts\n        \"RETRY_DELAY_MS\": \"500\"                       // Default: 500ms between retries\n      }\n    }\n  }\n}\n```\n\n3. The server provides the following tools:\n\n- `run_query`: Execute a SQL query using AWS Athena\n  - Parameters:\n    - database: The Athena database to query\n    - query: SQL query to execute\n    - maxRows: Maximum number of rows to return (default: 1000, max: 10000)\n  - Returns:\n    - If query completes within timeout: Full query results\n    - If timeout reached: Only the queryExecutionId for later retrieval\n\n- `get_status`: Check the status of a query execution\n  - Parameters:\n    - queryExecutionId: The ID returned from run_query\n  - Returns:\n    - state: Query state (QUEUED, RUNNING, SUCCEEDED, FAILED, or CANCELLED)\n    - stateChangeReason: Reason for state change (if any)\n    - submissionDateTime: When the query was submitted\n    - completionDateTime: When the query completed (if finished)\n    - statistics: Query execution statistics (if available)\n\n- `get_result`: Retrieve results for a completed query\n  - Parameters:\n    - queryExecutionId: The ID returned from run_query\n    - maxRows: Maximum number of rows to return (default: 1000, max: 10000)\n  - Returns:\n    - Full query results if the query has completed successfully\n    - Error if query failed or is still running\n\n- `list_saved_queries`: List all saved (named) queries in Athena.\n\n- Returns:\n  - An array of saved queries with `id`, `name`, and optional `description`\n  - Queries are returned from the configured `ATHENA_WORKGROUP` and `AWS_REGION`\n\n- run_saved_query: Run a previously saved query by its ID.\n- Parameters:\n  - `namedQueryId`: ID of the saved query\n  - `databaseOverride`: Optional override of the saved query's default database\n  - `maxRows`: Maximum number of rows to return (default: 1000)\n  - `timeoutMs`: Timeout in milliseconds (default: 60000)\n- Returns:\n  - Same behavior as `run_query`: full results or execution ID\n\n---\n\n## Usage Examples\n\n### Show All Databases\nMessage to AI Assistant:\n```List all databases in Athena```\n\nMCP parameter:\n```json\n{\n  \"database\": \"default\",\n  \"query\": \"SHOW DATABASES\"\n}\n```\n\n### List Tables in a Database\nMessage to AI Assistant:\n```Show me all tables in the default database```\n\nMCP parameter:\n```json\n{\n  \"database\": \"default\",\n  \"query\": \"SHOW TABLES\"\n}\n```\n\n### Get Table Schema\nMessage to AI Assistant:\n```What's the schema of the asin_sitebestimg table?```\n\nMCP parameter:\n```json\n{\n  \"database\": \"default\",\n  \"query\": \"DESCRIBE default.asin_sitebestimg\"\n}\n```\n\n### Table Rows Preview\nMessage to AI Assistant:\n```Show some rows from my_database.mytable```\n\nMCP parameter:\n```json\n{\n  \"database\": \"my_database\",\n  \"query\": \"SELECT * FROM my_table LIMIT 10\",\n  \"maxRows\": 10\n}\n```\n\n### Advanced Query with Filtering and Aggregation\nMessage to AI Assistant:\n```Find the average price by category for in-stock products```\n\nMCP parameter:\n```json\n{\n  \"database\": \"my_database\",\n  \"query\": \"SELECT category, COUNT(*) as count, AVG(price) as avg_price FROM products WHERE in_stock = true GROUP BY category ORDER BY count DESC\",\n  \"maxRows\": 100\n}\n```\n\n### Checking Query Status\n```json\n{\n  \"queryExecutionId\": \"12345-67890-abcdef\"\n}\n```\n\n### Getting Results for a Completed Query\n```json\n{\n  \"queryExecutionId\": \"12345-67890-abcdef\",\n  \"maxRows\": 10\n}\n```\n\n### Listing Saved Queries\n```json\n{\n  \"name\": \"list_saved_queries\",\n  \"arguments\": {}\n}\n```\n\n### Running a Saved Query\n```json\n{\n  \"name\": \"run_saved_query\",\n  \"arguments\": {\n    \"namedQueryId\": \"abcd-1234-efgh-5678\",\n    \"maxRows\": 100\n  }\n}\n```\n\n---\n\n## Requirements\n\n- Node.js >= 16\n- AWS credentials with appropriate Athena and S3 permissions\n- S3 bucket for query results\n- Named queries (optional) must exist in the specified `ATHENA_WORKGROUP` and `AWS_REGION`\n\n---\n\n## License\n\nMIT\n\n## Repository\n\n[GitHub Repository](https://github.com/lishenxydlgzs/aws-athena-mcp)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "athena",
        "athena databases",
        "aws athena",
        "databases secure"
      ],
      "category": "databases"
    },
    "llm-graph--postgres-mcp": {
      "owner": "llm-graph",
      "name": "postgres-mcp",
      "url": "https://github.com/llm-graph/postgres-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/llm-graph.webp",
      "description": "Connects AI agents to multiple PostgreSQL databases for secure read and write operations, schema inspection, and transaction management with detailed logging and progress reporting.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-20T08:59:18Z",
      "readme_content": "# FastPostgresMCP 🐘⚡️ (Full-Featured Multi-DB MCP Server)\n\n**This project implements a blazing fast, type-safe, and full-featured Model Context Protocol (MCP) Server designed for AI Agents (like Cursor, Claude Desktop) to interact with multiple PostgreSQL databases, including listing tables and inspecting schemas.**\n\nIt is built with Bun, TypeScript, `postgres`, and leverages advanced features of the `fastmcp` framework for building robust MCP servers.\n\n<a href=\"https://glama.ai/mcp/servers/@llm-graph/postgres-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@llm-graph/postgres-mcp/badge\" alt=\"FastPostgresMCP MCP server\" />\n</a>\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Powered by fastmcp](https://img.shields.io/badge/Powered%20by-fastmcp-blue)](https://github.com/punkpeye/fastmcp)\n[![Built with Bun](https://img.shields.io/badge/Built%20with-Bun-_000)](https://bun.sh)\n[![Uses postgres](https://img.shields.io/badge/Uses-postgres-336791)](https://github.com/porsager/postgres)\n[![TypeScript](https://img.shields.io/badge/TypeScript-Strict-blue)](https://www.typescriptlang.org/)\n[![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-green)](https://github.com/llm-graph/postgres-mcp)\n[![NPM Package](https://img.shields.io/badge/NPM-Package-red)](https://www.npmjs.com/package/postgres-mcp)\n\n## Purpose: An MCP Server for AI Agents\n\nThis is **not** a library to be imported into your code. It is a **standalone server application**. You run it as a process, and MCP clients (like AI agents) communicate with it using the JSON-based Model Context Protocol (v2.0), typically over a `stdio` connection managed by the client application (e.g., Cursor).\n\n## Troubleshooting and Development\n\n### Using the CLI for Testing\n\nThe package includes a built-in CLI command for testing the MCP server directly:\n\n```bash\n# From the project repository:\nbun run cli\n\n# This will start an interactive MCP CLI session where you can:\n# - Call any of the PostgreSQL tools (query_tool, execute_tool, etc.)\n# - View server capabilities\n# - Test queries against your configured databases\n```\n\n### Testing with Built-in MCP Inspector\n\nYou can also use the MCP Inspector to visually test and debug:\n\n```bash\n# From the project repository:\nbun run inspect\n```\n\n### Common Issues\n\nIf you see this error when running `bunx postgres-mcp`:\n```\nFastPostgresMCP started\n[warning] FastMCP could not infer client capabilities\n```\n\nfollowed by ping messages, it means:\n\n1. The MCP server started successfully\n2. The client connected successfully \n3. But the client is only sending ping requests and not properly negotiating capabilities\n\nThis usually indicates you need to use a proper MCP client. Try:\n- Using `bun run cli` to test with the MCP CLI\n- Configuring the MCP server in Cursor or Claude Desktop as described in the Installation section\n\nIf you're developing a custom MCP client, make sure it properly implements the MCP protocol including capabilities negotiation.\n\n## ✨ Core Features\n\n*   **🚀 Blazing Fast:** Built on Bun and `fastmcp`.\n*   **🔒 Type-Safe:** End-to-end TypeScript with Zod schema validation.\n*   **🐘 Multi-Database Support:** Connect to and manage interactions across several PostgreSQL instances defined in `.env`.\n*   **🛡️ Secure by Design:** Parameterized queries via `postgres` prevent SQL injection.\n*   **🔑 Optional Authentication:** Secure network-based connections (SSE/HTTP) using API Key validation (`fastmcp`'s `authenticate` hook).\n*   **📄 Database Schema via MCP Resources:**\n    *   **List Tables:** Get a list of tables in a database via `db://{dbAlias}/schema/tables`.\n    *   **Inspect Table Schema:** Get detailed column info for a specific table via `db://{dbAlias}/schema/{tableName}`.\n*   **💬 Enhanced Tool Interaction:**\n    *   **In-Tool Logging:** Tools send detailed logs back to the client (`log` context).\n    *   **Progress Reporting:** Long-running operations report progress (`reportProgress` context).\n*   **🧠 Session-Aware:** Access session information within tool execution context (`session` context).\n*   **📡 Event-Driven:** Uses `server.on` and `session.on` for connection/session event handling.\n*   **🔧 Modern Developer Experience (DX):** Clear configuration, intuitive API, easy testing with `fastmcp` tools.\n\n## What's Included (fastmcp Features Leveraged)\n\n*   `FastMCP` Server Core\n*   `server.addTool` (for `query_tool`, `execute_tool`, `schema_tool`, and `transaction_tool`)\n*   `server.addResourceTemplate` (for listing tables and inspecting table schemas)\n*   `server.start` (with `stdio` focus, adaptable for `sse`/`http`)\n*   **Optional:** `authenticate` Hook (for API Key validation)\n*   Tool Execution `context` (`log`, `reportProgress`, `session`)\n*   Zod for Parameter Schema Validation\n*   `server.on` (for connection logging)\n*   (Potentially) `session.on` for session-specific logic\n\n## 📋 Prerequisites\n\n*   **[Bun](https://bun.sh/) (v1.0 or later recommended):** Installed and in PATH.\n*   **PostgreSQL Database(s):** Access credentials and connectivity. User needs permissions to query `information_schema`.\n\n## ⚙️ Installation\n\n### Option 1: NPM Package\n\n```bash\n# Install globally\nnpm install -g postgres-mcp\n\n# Or install locally in your project\nnpm install postgres-mcp\n```\n\nThe npm package is available at [https://www.npmjs.com/package/postgres-mcp](https://www.npmjs.com/package/postgres-mcp)\n\n### Option 2: Clone Repository\n\n1.  **Clone the repository:**\n    ```bash\n    # Replace with your actual repository URL\n    git clone https://github.com/llm-graph/postgres-mcp.git\n    cd postgres-mcp\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    bun install\n    ```\n\n## 🔑 Configuration (Multi-Database & Optional Auth)\n\nConfigure via environment variables, loaded from appropriate `.env` files.\n\n1.  **Create environment files:**\n    - For production: `cp .env.example .env`\n    - For development: `cp .env.development.example .env.development`\n    \n2.  **Environment file loading order:**\n    The server loads environment variables from files in the following order of priority:\n    - `.env.<NODE_ENV>` (e.g., `.env.development`, `.env.production`, `.env.staging`)\n    - `.env.local` (for local overrides, not version controlled)\n    - `.env` (default fallback)\n    \n    This allows different configurations for different environments.\n\n3.  **Edit the environment files** to define database connections and authentication:\n    - `DB_ALIASES` - Comma-separated list of unique DB aliases\n    - `DEFAULT_DB_ALIAS` - Default alias if 'dbAlias' is omitted in tool calls\n    - Database connection details for each alias (e.g., `DB_MAIN_HOST`, `DB_REPORTING_HOST`)\n    - Optional API Key authentication (`ENABLE_AUTH`, `MCP_API_KEY`)\n\n```dotenv\n# Example .env file - Key Variables\n\n# REQUIRED: Comma-separated list of unique DB aliases\nDB_ALIASES=main,reporting\n\n# REQUIRED: Default alias if 'dbAlias' is omitted in tool calls\nDEFAULT_DB_ALIAS=main\n\n# OPTIONAL: Enable API Key auth (primarily for network transports)\nENABLE_AUTH=false\nMCP_API_KEY=your_super_secret_api_key_here # CHANGE THIS\n\n# Define DB connection details for each alias (DB_MAIN_*, DB_REPORTING_*, etc.)\nDB_MAIN_HOST=localhost\nDB_MAIN_PORT=5432\nDB_MAIN_NAME=app_prod_db\nDB_MAIN_USER=app_user\nDB_MAIN_PASSWORD=app_secret_password\nDB_MAIN_SSL=disable\n\n# Alternative: Use connection URLs\n# DB_MAIN_URL=postgres://user:password@localhost:5432/database?sslmode=require\n\n# --- Optional: Server Logging Level ---\n# LOG_LEVEL=info # debug, info, warn, error (defaults to info)\n```\n\n## 🚀 Running the Server (as a Process)\n\nRun this server directly using Bun. The AI Client (like Cursor) will typically start and manage this command for you.\n\n### Option 1: Using the globally installed package\n*   **To run manually:** `postgres-mcp`\n\n### Option 2: Using the package in your project\n*   **To run from your project:** `npx postgres-mcp`\n*   **Or import programmatically:**\n    ```javascript\n    // server.js\n    import { startServer } from 'postgres-mcp';\n    \n    // Start the MCP server\n    startServer();\n    ```\n\n### Option 3: From cloned repository\n*   **To run manually (for testing):** `bun run src/index.ts`\n*   **Manual Development Mode:** `bun run --watch src/index.ts`\n\n### Testing with `fastmcp` CLI Tools\n\n*   **Interactive Terminal:** `bunx fastmcp dev src/index.ts`\n*   **Web UI Inspector:** `bunx fastmcp inspect src/index.ts`\n\n## 💻 Using the Programmatic API (as a Library)\n\nIn addition to running as a standalone MCP server, postgres-mcp can also be used programmatically as a library in your Node.js/TypeScript applications.\n\n### Basic Usage\n\n```typescript\nimport { createPostgresMcp } from 'postgres-mcp';\n\n// Create the PostgresMcp instance\nconst postgresMcp = createPostgresMcp();\n\n// Start the server\npostgresMcp.start();\n\n// Direct database operations\nconst results = await postgresMcp.executeQuery(\n  'SELECT * FROM users WHERE role = $1',\n  ['admin'],\n  'main' // optional database alias\n);\n\n// When done, stop the server and close connections\nawait postgresMcp.stop();\n```\n\n### Direct Function Imports\n\nFor simpler use cases, you can import specific functions directly:\n\n```typescript\nimport { \n  initConnections, \n  closeConnections, \n  executeQuery, \n  executeCommand, \n  executeTransaction, \n  getTableSchema,\n  getAllTableSchemas\n} from 'postgres-mcp';\n\n// Configure database connections\nconst dbConfigs = {\n  main: {\n    host: 'localhost',\n    port: 5432,\n    database: 'my_db',\n    user: 'db_user',\n    password: 'db_password'\n  }\n};\n\n// Initialize connections\ninitConnections(dbConfigs);\n\n// Execute a query\nconst results = await executeQuery(\n  'SELECT * FROM users WHERE role = $1',\n  ['admin'],\n  'main'\n);\n\n// Get schema for a single table\nconst schema = await getTableSchema('users', 'main');\n\n// Get schema for all tables in the database\nconst allSchemas = await getAllTableSchemas('main');\n\n// Close connections when done\nawait closeConnections();\n```\n\n### Configuration Options\n\n```typescript\nconst postgresMcp = createPostgresMcp({\n  // Custom database configurations (override .env)\n  databaseConfigs: {\n    main: {\n      host: 'localhost',\n      port: 5432,\n      database: 'app_db',\n      user: 'app_user',\n      password: 'password',\n      ssl: 'disable'\n    }\n  },\n  // Server configuration\n  serverConfig: {\n    name: 'Custom PostgresMCP',\n    defaultDbAlias: 'main'\n  },\n  // Transport options: 'stdio', 'sse', or 'http'\n  transport: 'http',\n  port: 3456\n});\n```\n\nFor complete documentation on the programmatic API, see [docs/programmatic-api.md](docs/programmatic-api.md).\n\n## 🔌 Connecting with AI Clients (Cursor, Claude Desktop)\n\nConfigure your AI Agent (MCP Client) to **execute** this server script via its command/args mechanism.\n\n### Cursor AI - Detailed Example\n\n1.  Open Cursor Settings/Preferences (Cmd+, or Ctrl+,).\n2.  Navigate to \"Extensions\" -> \"MCP\".\n3.  Click \"Add MCP Server\" or edit `settings.json`.\n4.  Add the following JSON configuration:\n\n    ```json\n    // In Cursor's settings.json or MCP configuration UI\n    {\n      \"mcpServers\": {\n        \"postgres-mcp\": { // Unique name for Cursor\n          \"description\": \"MCP Server for PostgreSQL DBs (Main, Reporting)\",\n          \"command\": \"bunx\",  // Use 'bun' or provide absolute path: \"/Users/your_username/.bun/bin/bun\"\n          \"args\": [\n            \"postgres-mcp\"\n            // or\n            // *** ABSOLUTE PATH to your server's entry point ***\n            // \"/Users/your_username/projects/postgres-mcp/src/index.ts\" /\n          ],\n          \"env\": {\n            // .env file in project dir is loaded automatically by Bun.\n            // Add overrides or Cursor-specific vars here if needed.\n          },\n          \"enabled\": true\n        }\n      }\n    }\n    ```\n\n5.  **Save** and **Restart Cursor** or \"Reload MCP Servers\".\n6.  **Verify** connection in Cursor's MCP status/logs.\n\n### Claude Desktop\n\n1.  Locate and edit `config.json` (see previous README for paths).\n2.  Add a similar entry under `mcpServers`, using the **absolute path** in `args`.\n3.  Restart Claude Desktop.\n\n## 🛠️ MCP Capabilities Exposed\n\n### Authentication (Optional)\n\n*   Secures network transports (HTTP/SSE) via `X-API-Key` header matching `MCP_API_KEY` if `ENABLE_AUTH=true`.\n*   `stdio` connections (default for Cursor/Claude) generally bypass this check.\n\n### Resources\n\n#### 1. List Database Tables\n\n*   **URI Template:** `db://{dbAlias}/schema/tables`\n*   **Description:** Retrieves a list of user table names within the specified database alias (typically from the 'public' schema).\n*   **Resource Definition (`addResourceTemplate`):**\n    *   `uriTemplate`: `\"db://{dbAlias}/schema/tables\"`\n    *   `arguments`:\n        *   `dbAlias`: (string, required) - Alias of the database (from `.env`).\n    *   `load({ dbAlias })`: Connects to the database, queries `information_schema.tables` (filtered for base tables in the public schema, customizable in implementation), formats the result as a JSON string array `[\"table1\", \"table2\", ...]`, and returns `{ text: \"...\" }`.\n\n**Example Usage (AI Prompt):** \"Get the resource `db://main/schema/tables` to list tables in the main database.\"\n\n#### 2. Inspect Table Schema\n\n*   **URI Template:** `db://{dbAlias}/schema/{tableName}`\n*   **Description:** Provides detailed schema information (columns, types, nullability, defaults) for a specific table.\n*   **Resource Definition (`addResourceTemplate`):**\n    *   `uriTemplate`: `\"db://{dbAlias}/schema/{tableName}\"`\n    *   `arguments`:\n        *   `dbAlias`: (string, required) - Database alias.\n        *   `tableName`: (string, required) - Name of the table.\n    *   `load({ dbAlias, tableName })`: Connects, queries `information_schema.columns` for the specific table, formats as JSON string array of column objects, returns `{ text: \"...\" }`.\n\n**Example Usage (AI Prompt):** \"Describe the resource `db://reporting/schema/daily_sales`.\"\n\n**Example Response Content (JSON String):**\n```json\n\"[{\\\"column_name\\\":\\\"session_id\\\",\\\"data_type\\\":\\\"uuid\\\",\\\"is_nullable\\\":\\\"NO\\\",\\\"column_default\\\":\\\"gen_random_uuid()\\\"},{\\\"column_name\\\":\\\"user_id\\\",\\\"data_type\\\":\\\"integer\\\",\\\"is_nullable\\\":\\\"NO\\\",\\\"column_default\\\":null},{\\\"column_name\\\":\\\"created_at\\\",\\\"data_type\\\":\\\"timestamp with time zone\\\",\\\"is_nullable\\\":\\\"YES\\\",\\\"column_default\\\":\\\"now()\\\"},{\\\"column_name\\\":\\\"expires_at\\\",\\\"data_type\\\":\\\"timestamp with time zone\\\",\\\"is_nullable\\\":\\\"YES\\\",\\\"column_default\\\":null}]\"\n```\n\n### Tools\n\nTools receive `context` object (`log`, `reportProgress`, `session`).\n\n---\n\n#### 1. `query_tool`\n\nExecutes read-only SQL queries.\n\n*   **Description:** Safely execute read-only SQL, get results, with execution logging/progress.\n*   **Parameters:** `statement` (string), `params` (array, opt), `dbAlias` (string, opt).\n*   **Context Usage:** `log.info/debug`, optional `reportProgress`, access `session`.\n*   **Returns:** JSON string of the row array.\n\n**Example Request:**\n```json\n{\n  \"tool_name\": \"query_tool\",\n  \"arguments\": {\n    \"statement\": \"SELECT product_id, name, price FROM products WHERE category = $1 AND price < $2 ORDER BY name LIMIT 10\",\n    \"params\": [\"electronics\", 500],\n    \"dbAlias\": \"main\"\n  }\n}\n```\n\n**Example Response Content (JSON String):**\n```json\n\"[{\\\"product_id\\\":123,\\\"name\\\":\\\"Example Gadget\\\",\\\"price\\\":499.99},{\\\"product_id\\\":456,\\\"name\\\":\\\"Another Device\\\",\\\"price\\\":350.00}]\"\n```\n\n---\n\n#### 2. `execute_tool`\n\nExecutes data-modifying SQL statements.\n\n*   **Description:** Safely execute data-modifying SQL, with execution logging.\n*   **Parameters:** `statement` (string), `params` (array, opt), `dbAlias` (string, opt).\n*   **Context Usage:** `log.info/debug`, access `session`.\n*   **Returns:** String indicating rows affected.\n\n**Example Request:**\n```json\n{\n  \"tool_name\": \"execute_tool\",\n  \"arguments\": {\n    \"statement\": \"UPDATE users SET last_login = NOW() WHERE user_id = $1\",\n    \"params\": [54321]\n    // dbAlias omitted, uses DEFAULT_DB_ALIAS\n  }\n}\n```\n\n**Example Response Content (String):**\n```\n\"Rows affected: 1\"\n```\n\n---\n\n#### 3. `schema_tool`\n\nRetrieves detailed schema information for a specific table.\n\n*   **Description:** Get column definitions and details for a database table.\n*   **Parameters:** `tableName` (string), `dbAlias` (string, opt).\n*   **Context Usage:** `log.info`, access `session`.\n*   **Returns:** JSON string array of column information objects.\n\n**Example Request:**\n```json\n{\n  \"tool_name\": \"schema_tool\",\n  \"arguments\": {\n    \"tableName\": \"user_sessions\",\n    \"dbAlias\": \"main\"\n  }\n}\n```\n\n**Example Response Content (JSON String):**\n```json\n\"[{\\\"column_name\\\":\\\"session_id\\\",\\\"data_type\\\":\\\"uuid\\\",\\\"is_nullable\\\":\\\"NO\\\",\\\"column_default\\\":\\\"gen_random_uuid()\\\"},{\\\"column_name\\\":\\\"user_id\\\",\\\"data_type\\\":\\\"integer\\\",\\\"is_nullable\\\":\\\"NO\\\",\\\"column_default\\\":null},{\\\"column_name\\\":\\\"created_at\\\",\\\"data_type\\\":\\\"timestamp with time zone\\\",\\\"is_nullable\\\":\\\"YES\\\",\\\"column_default\\\":\\\"now()\\\"},{\\\"column_name\\\":\\\"expires_at\\\",\\\"data_type\\\":\\\"timestamp with time zone\\\",\\\"is_nullable\\\":\\\"YES\\\",\\\"column_default\\\":null}]\"\n```\n\n---\n\n#### 4. `transaction_tool`\n\nExecutes multiple SQL statements atomically.\n\n*   **Description:** Execute SQL sequence in a transaction, with step logging/progress.\n*   **Parameters:** `operations` (array of {statement, params}), `dbAlias` (string, opt).\n*   **Context Usage:** `log.info/debug/error`, `reportProgress`, access `session`.\n*   **Returns:** JSON string summarizing success/failure: `{\"success\": true, \"results\": [...]}` or `{\"success\": false, \"error\": ..., \"failedOperationIndex\": ...}`.\n\n**Example Request:**\n```json\n{\n  \"tool_name\": \"transaction_tool\",\n  \"arguments\": {\n    \"operations\": [\n      {\n        \"statement\": \"INSERT INTO orders (customer_id, order_date, status) VALUES ($1, NOW(), 'pending') RETURNING order_id\",\n        \"params\": [101]\n      },\n      {\n        \"statement\": \"INSERT INTO order_items (order_id, product_sku, quantity, price) VALUES ($1, $2, $3, $4)\",\n        \"params\": [9999, \"GADGET-X\", 2, 49.99]\n      },\n      {\n        \"statement\": \"UPDATE inventory SET stock_count = stock_count - $1 WHERE product_sku = $2 AND stock_count >= $1\",\n        \"params\": [2, \"GADGET-X\"]\n      }\n    ],\n    \"dbAlias\": \"main\"\n  }\n}\n```\n\n**Example Success Response Content (JSON String):**\n```json\n\"{\\\"success\\\":true,\\\"results\\\":[{\\\"operation\\\":0,\\\"rowsAffected\\\":1},{\\\"operation\\\":1,\\\"rowsAffected\\\":1},{\\\"operation\\\":2,\\\"rowsAffected\\\":1}]}\"\n```\n\n**Example Error Response Content (JSON String):**\n```json\n\"{\\\"success\\\":false,\\\"error\\\":\\\"Error executing operation 2: new row for relation \\\\\\\"inventory\\\\\\\" violates check constraint \\\\\\\"stock_count_non_negative\\\\\\\"\\\",\\\"failedOperationIndex\\\":2}\"\n```\n\n---\n\n### Server & Session Events\n\n*   Uses `server.on('connect'/'disconnect')` for logging client connections.\n*   Can use `session.on(...)` for more granular session event handling if needed.\n\n## 🔒 Security Considerations\n\n*   **SQL Injection:** Mitigated via parameterized queries. **No direct input concatenation.**\n*   **Database Permissions:** **Critical.** Assign least privilege to each `DB_<ALIAS>_USER`, including read access to `information_schema` for schema/table listing resources.\n*   **SSL/TLS:** **Essential** for production (`DB_<ALIAS>_SSL=require` or stricter).\n*   **Secrets Management:** Protect `.env` file (add to `.gitignore`). Use secure secret management for production environments (Vault, Doppler, cloud secrets).\n*   **Authentication Scope:** `authenticate` hook primarily secures network transports. `stdio` security relies on the execution environment.\n*   **Data Sensitivity:** Be aware of data accessible via connections/tools.\n*   **Resource Queries:** The queries used for listing tables (`information_schema.tables`) and schemas (`information_schema.columns`) are generally safe but rely on database permissions. Ensure the configured users have appropriate read access. Customize the table listing query (e.g., schema filtering) if needed for security or clarity.\n\n## 📜 License\n\nThis project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.\n\n## 📋 Changelog\n\n### 1.0.0\n- Initial release\n- Full-featured MCP Server for PostgreSQL\n- Support for multiple database connections\n- Tools for queries, execution, schema inspection, and transactions\n- Resources for schema introspection\n- Comprehensive documentation and examples",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgres",
        "databases",
        "postgresql",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "log6262635--mcp-mysql": {
      "owner": "log6262635",
      "name": "mcp-mysql",
      "url": "https://github.com/log6262635/mcp-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/log6262635.webp",
      "description": "Interact with MySQL databases through the MCP protocol, enabling operations such as creating, reading, updating, and deleting database tables alongside executing custom SQL queries.",
      "stars": 6,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-12T12:50:11Z",
      "readme_content": "# MySQL数据库MCP服务\n\n这是一个基于MCP（Model Context Protocol）的服务，允许通过Cursor与MySQL数据库进行交互，实现表的创建、查询、修改和删除等操作。\n\n## 功能特点\n\n- 创建和管理数据库表\n- 执行CRUD操作（创建、读取、更新、删除）\n- 执行自定义SQL查询\n- 通过MCP协议与Cursor集成\n\n## 安装和设置\n\n### 前提条件\n\n- Node.js 18.0.0 或更高版本\n- MySQL 数据库服务器\n\n### 安装步骤\n\n1. 克隆或下载本仓库\n2. 安装依赖：\n\n```bash\nnpm install\n```\n\n3. 配置环境变量：\n   - 复制 `.env.example` 文件为 `.env`\n   - 编辑 `.env` 文件，填入您的MySQL数据库连接信息\n\n```\n# 数据库配置\nDB_HOST=localhost\nDB_PORT=3306\nDB_USER=您的数据库用户名\nDB_PASSWORD=您的数据库密码\nDB_NAME=您的数据库名称\n\n# 服务器配置\nPORT=3001\n```\n\n### 启动服务\n\n```bash\nnpm start\n```\n\n服务器将在 http://localhost:3001 启动（或您在 `.env` 中指定的端口）。\n\n## 在Cursor中使用\n\n1. 在Cursor中，使用以下方法添加MCP服务：\n   - 方法1: 在命令面板中搜索 \"MCP\" 并选择添加服务\n   - 方法2: 在设置中找到 MCP 相关配置\n   - 方法3: 直接使用命令 `/connect-mcp http://localhost:3001/sse`\n\n2. 连接成功后，您可以通过资源和工具与MySQL数据库进行交互\n\n## 可用功能\n\n### 资源\n\n1. 列出所有表：\n   ```\n   mysql://tables\n   ```\n\n2. 查看表结构：\n   ```\n   mysql://schema/表名\n   ```\n\n3. 查看表数据：\n   ```\n   mysql://data/表名\n   ```\n\n### 工具\n\n1. 创建表：\n   ```\n   create-table tableName=\"表名\" schema=\"id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255)\"\n   ```\n\n2. 删除表：\n   ```\n   drop-table tableName=\"表名\"\n   ```\n\n3. 插入数据：\n   ```\n   insert-data tableName=\"表名\" data=\"{\\\"name\\\": \\\"张三\\\", \\\"age\\\": 30}\"\n   ```\n\n4. 更新数据：\n   ```\n   update-data tableName=\"表名\" data=\"{\\\"name\\\": \\\"李四\\\"}\" condition=\"{\\\"id\\\": 1}\"\n   ```\n\n5. 删除数据：\n   ```\n   delete-data tableName=\"表名\" condition=\"{\\\"id\\\": 1}\"\n   ```\n\n6. 查询数据：\n   ```\n   query-data tableName=\"表名\" fields=\"[\\\"id\\\",\\\"name\\\"]\" condition=\"{\\\"age\\\": 30}\"\n   ```\n\n7. 执行自定义SQL：\n   ```\n   execute-sql sql=\"SELECT * FROM users WHERE age > 18\" params=\"[]\"\n   ```\n\n### 提示模板\n\n1. 创建表指南：\n   ```\n   create-table-guide tableName=\"表名\"\n   ```\n\n2. 插入数据指南：\n   ```\n   insert-data-guide tableName=\"表名\"\n   ```\n\n3. 数据库操作概览：\n   ```\n   database-operations\n   ```\n\n## 示例场景\n\n### 创建用户表并添加数据\n\n1. 创建用户表：\n   ```\n   create-table tableName=\"users\" schema=\"id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255) NOT NULL, email VARCHAR(255) UNIQUE, age INT\"\n   ```\n\n2. 插入用户数据：\n   ```\n   insert-data tableName=\"users\" data=\"{\\\"name\\\": \\\"张三\\\", \\\"email\\\": \\\"zhangsan@example.com\\\", \\\"age\\\": 30}\"\n   ```\n\n3. 查询用户数据：\n   ```\n   mysql://data/users\n   ```\n\n## 项目架构\n\n```\nmcp-db-service/\n├── src/\n│   ├── db/              # 数据库连接和操作\n│   ├── resources/       # MCP资源处理\n│   ├── tools/           # MCP工具处理\n│   ├── prompts/         # MCP提示模板\n│   └── index.js         # 主程序入口\n├── .env                 # 环境变量配置\n├── .env.example         # 环境变量示例\n├── package.json         # 项目配置\n└── README.md            # 项目说明\n```\n\n## 安全注意事项\n\n- 此服务未实现身份验证和授权机制，请勿在生产环境中使用\n- 建议设置MySQL用户的权限，只允许必要的操作\n- 不要在代码或环境变量中存储敏感的数据库凭据\n\n## 许可证\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "mcp mysql",
        "secure database",
        "databases mcp"
      ],
      "category": "databases"
    },
    "lowcodelocky2--xano-mcp": {
      "owner": "lowcodelocky2",
      "name": "xano-mcp",
      "url": "https://github.com/lowcodelocky2/xano-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/lowcodelocky2.webp",
      "description": "Manage Xano databases by creating, modifying, and deleting tables while facilitating comprehensive schema editing. Provides tools for database management and API documentation extraction in JSON or Markdown format.",
      "stars": 7,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-17T03:20:21Z",
      "readme_content": "# Xano MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Xano's metadata API. This server provides tools that can be used by AI assistants (like Claude) through Cursor or other MCP-compatible clients.\n\n## Features\n\n- **Manage Tables**: Create, list, and delete tables in your Xano database\n- **Schema Operations**: View and modify table schemas with comprehensive schema editing capabilities\n- **Database Management**: Complete toolset for interacting with your Xano database structure\n- **Swagger Spec**: Extract your API group api details in either JSON or Markdown (reduced token) format\n\nNote this is an early-stage with feedback / requests welcomed.\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm or another Node.js package manager\n- A Xano account with API access\n- Cursor, Claude Desktop, Cline or another MCP client.\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/lowcodelocky2/xano-mcp.git\ncd xano-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Configure your Xano credentials:\n   - Edit `index.ts` and set your Xano credentials:\n     - `XANO_API_KEY`: Your Xano API key\n     - `XANO_WORKSPACE`: Your Xano workspace ID\n     - `XANO_API_BASE`: Your Xano instance API URL (e.g., https://your-instance.xano.io/api:meta)\n\n4. Build the project:\n```bash\nnpm run build\n```\n\n## Usage with Claude Desktop\n\nFollow this guide - https://modelcontextprotocol.io/quickstart/user\n\nUpdate your config with: \n```json\n{\n  \"mcpServers\": {\n    \"xano\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/xano-mcp\"\n      ]\n    }\n  }\n} \n```\n\nReplace `/path/to/xano-mcp` with the absolute path to your project directory.\n\n**This does not work with the claude web app, only via the desktop app - https://claude.ai/download**\n\n## Usage with Cursor\n\n1. Open Cursor\n2. Click \"Add MCP Server\"\n3. Configure the server:\n   - Name: `whatever you want to call it`\n   - Type: `command`\n   - Command: `node /path/to/xano-mcp/build/index.js`\n\nReplace `/path/to/xano-mcp` with the absolute path to your project directory.\n\nExample mac  \nnode /Users/your-user/Documents/folder-name/xano-mcp/build/index.js\n\nIf you're in your're inside your directory you can run the comman 'pwd' into your terminal to get the absolute path.\n\n## Xano MCP Tools Overview\n\nThis integration provides a comprehensive set of tools for managing your Xano workspace through the Model Context Protocol (MCP). Here's what you can do:\n\n## Database Management\n\n### Tables\n- List all tables in your workspace\n- View detailed table schemas\n- Create new tables with custom schemas\n- Delete existing tables\n- Modify table schemas (add/remove/rename columns)\n\n### Schema Operations\n- Add new columns with various data types\n- Remove columns\n- Rename columns\n- Update entire table schemas\n- Support for complex data types and relationships\n\n## API Management\n\n### API Groups\n- Create new API groups\n- List all API groups\n- Browse APIs within groups\n- Enable/disable Swagger documentation\n- Manage API group metadata (tags, branches, etc.)\n\n### Individual APIs\n- Add new APIs to groups\n- Configure HTTP methods (GET, POST, PUT, DELETE, PATCH, HEAD)\n- Set up API documentation\n- Add metadata (tags, descriptions)\n\n## Documentation\n- Generate API Group specifications in both markdown (reduced tokens) and JSO (full) formats\n- View Swagger documentation\n- Access detailed schema information\n\nThis toolset enables complete management of your Xano workspace, allowing you to build and maintain your backend infrastructure programmatically through the MCP interface. \n\n## Re-enabling the Delete Table Tool\n\nTo re-enable the delete-table functionality in this codebase, follow these step-by-step instructions:\n\n1. Open the file `src/index.ts` in your code editor\n2. Locate the commented-out section that starts with:\n   ```typescript\n   // Delete Table Tool\n   /*\n   server.tool(\n   ```\n   and ends with:\n   ```typescript\n   );\n   */\n   ```\n\n3. To uncomment this section:\n   - Delete the opening `/*` on the line after \"Delete Table Tool\"\n   - Delete the closing `*/` before \"Edit Table Schema Tool\"\n   \n   That's it! The delete-table tool will now be active again.   (After running a new build)\n\n### Example of What the Code Should Look Like After\n\n```typescript\n// Delete Table Tool\nserver.tool(\n  \"delete-table\",\n  \"Delete a table from the Xano workspace\",\n  {\n    table_id: z.string().describe(\"ID of the table to delete\")\n  },\n  async ({ table_id }) => {\n    // ... rest of the implementation\n  }\n);\n```\n\n### Verification\nAfter making these changes:\n1. Save the file\n2. Run a new build `npm run build'\n3. Restart your MCP client (Claude / Cursor)\n4. The delete-table tool should now be available in your toolset\n\n### Safety Note\nThe delete-table tool permanently removes tables from your Xano workspace. Make sure you have appropriate backups before using this functionality. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xano",
        "databases",
        "database",
        "xano databases",
        "manage xano",
        "lowcodelocky2 xano"
      ],
      "category": "databases"
    },
    "lucas-deangelis--arango-mcp-server": {
      "owner": "lucas-deangelis",
      "name": "arango-mcp-server",
      "url": "https://github.com/lucas-deangelis/arango-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/lucas-deangelis.webp",
      "description": "Connect and interact with ArangoDB databases, executing queries, managing collections, and retrieving data efficiently through a standardized protocol.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-15T11:52:17Z",
      "readme_content": "# ArangoDB MCP Server\n\nThis is an implementation of the Model Context Protocol for ArangoDB.\n\n## Overview\n\nTo be filled.\n\n## Components\n\n### Resources\n\n### Tools\n\n#### Query Tools\n\n- `readQuery`\n  - Execute read-only query on the database\n  - Input:\n    - `databaseName` (string): The database to query\n    - `aql` (string): The read-only AQL query to execute\n  - Returns: Query results as array of objects\n- `readWriteQuery`\n  - Execute query on the database\n  - Input:\n    - `databaseName` (string): The database to query\n    - `aql` (string): The AQL query to execute\n  - Returns: Query results as array of objects\n- `listDatabases`\n  - List all the databases on the ArangoDB server\n  - Returns: Array of the databases names\n- `listCollections`\n  - List all the collections in an ArangoDB database\n  - Input:\n    - `databaseName` (string): The name of the database\n  - Returns: Array of objects `{ \"name\": \"<collectionName>\" }`\n\n## Usage\n\nTo connect to an arangodb instance running on localhost:2434, to the database \"account\", add the following to your `claude_desktop_config.json`, assuming the path to this project is `/home/yourcoolname/arango-mcp-server`:\n\n```json\n{\n  \"mcpServers\": {\n    \"arangodb-account\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"arango-mcp-server\",\n        \"http://localhost:8529\",\n        \"root\",\n        \"root\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\nClone the repository.\nInstall everything.\nSetup the dev environment.\nRun the watcher.\nEdit index.ts.\n\n```sh\n$ npm install\n$ npm run dev:setup\n$ npm run dev\n```\n\nGo to http://localhost:5173/ to see the inspector.\n\n## Todo\n\n- [ ] Properly study the spec to see if the current implementation of resources actually make sense (I don't think it does)\n  - [x] The resource templates make sense\n- [ ] Change all the \"arango\" to \"arangodb\" (repo name included...)\n- [ ] Add back the arangodb password\n- [ ] Proper README\n  - [ ] Tools/resource/etc following the format of the official anthropic stuff\n- [ ] Figure out notifications\n- [ ] Health checks\n- [ ] More tools?\n- [ ] Access all the databases running on an arangodb instance\n- [ ] Release on npm somehow so it can be used with `npx`\n- [ ] `resources/subscribe` and `notifications/resources/list_changed` and `resources/unsubscribe`\n- [x] Properly document tools in the readme\n- [x] Like on the SQLite MCP client\n  - [x] `write_query` tool separated from `read_query` -> actually is `readWriteQuery`\n  - [x] `list_collections` (see `list_tables`)\n- [x] Client pool ie one client per database\n- [x] Dev environment\n- [x] `resources/read` with a template to read any document by database name, collection, id.\n- [x] Add username and passwords as parameters of the command\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "arangodb",
        "databases",
        "database",
        "arangodb databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "m4tyn0--influx_mcp": {
      "owner": "m4tyn0",
      "name": "influx_mcp",
      "url": "https://github.com/m4tyn0/influx_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/m4tyn0.webp",
      "description": "Query time-series data from InfluxDB using a standardized interface with secure read-only access through JWT authentication. Simplifies data retrieval by allowing users to list databases and execute queries easily.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-19T02:02:48Z",
      "readme_content": "# InfluxDB MCP Server\n\nA Model Context Protocol (MCP) server that provides secure, read-only access to InfluxDB 1.8 via JWT authentication.\n\n## Overview\n\nInfluxDB MCP Server allows AI assistants to query time-series data stored in InfluxDB through a standardized interface. It provides read-only access to your InfluxDB instance with authentication via JWT tokens.\n\n## Prerequisites\n\n- Docker\n- InfluxDB 1.8 instance (already running)\n- Credentials for your InfluxDB instance\n\n## Installation\n\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/m4tyn0/influx_mcp\n   cd influxdb-mcp-server\n   ```\n\n2. **Create a `.env` file** with your configuration :\n   ```bash\n   cp env.example .env\n   ```\n   ```\n   INFLUXDB_HOST=\n   INFLUXDB_PORT=8086\n   INFLUXDB_USERNAME=\n   INFLUXDB_PASSWORD=\n   INFLUXDB_SSL=false\n   INFLUXDB_VERIFY_SSL=true\n   INFLUXDB_TIMEOUT=10\n   JWT_SECRET=\n   JWT_ALGORITHM=HS256\n   ```\n\n3. **Build and run the Docker container**:\n   ```bash\n   docker build -t influxdb-mcp-server .\n   docker run -d --env-file .env -p 8000:8000 influxdb-mcp-server\n   ```\n\n## JWT Authentication\n\nThe server uses JWT tokens for authentication. You need to generate a token to authenticate with the MCP server, here is a simple script to do that:\n\n```python\nimport jwt\nimport datetime\n\n# Create a token\npayload = {\n    \"sub\": \"username\",  # Replace with the username you want to identify with\n    \"iat\": datetime.datetime.utcnow(),\n    \"exp\": datetime.datetime.utcnow() + datetime.timedelta(hours=1)\n}\ntoken = jwt.encode(payload, \"your-jwt-secret\", algorithm=\"HS256\")\nprint(token)\n```\n\nSave this token for use with your MCP client.\n\n## Using with MCP Clients\n\n1. **Authenticate** with the MCP server using the JWT token:\n   ```\n   Tool: auth\n   Arguments: {\"token\": \"your.jwt.token.here\"}\n   ```\n\n2. **Query your InfluxDB data**:\n   ```\n   Tool: list_databases\n   Arguments: {}\n   ```\n   ```\n   Tool: list_measurements\n   Arguments: {\"database\": \"your_database_name\"}\n   ```\n   ```\n   Tool: query\n   Arguments: {\n     \"database\": \"your_database_name\",\n     \"query\": \"SELECT * FROM measurement_name LIMIT 10\"\n   }\n   ```\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Connection errors to InfluxDB**:\n   - Verify the INFLUXDB_HOST is correct and accessible from the Docker container\n   - Check that your InfluxDB credentials are correct\n   - Ensure your InfluxDB instance is configured to accept connections from external hosts\n\n2. **Authentication issues**:\n   - Verify your JWT_SECRET is set correctly\n   - Ensure the token you're using matches the JWT_SECRET and hasn't expired\n   - Check that your InfluxDB credentials have read access to the databases\n\n3. **Check logs**:\n   ```bash\n   docker logs \n   ```\n\n## Security Notes\n\n- The MCP server only allows read-only access to your InfluxDB instance\n- All queries are validated to ensure they begin with SELECT\n- JWT authentication protects access to the MCP server\n- Consider running the server in a private network for additional security\n\n\n#### Thank you for working with me. If you have any issues with the code, or want more things built, hit me up: m4tyn0@gmail.com\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "influxdb",
        "databases",
        "database",
        "influxdb using",
        "data influxdb",
        "secure database"
      ],
      "category": "databases"
    },
    "madhukarkumar--singlestore-mcp-server": {
      "owner": "madhukarkumar",
      "name": "singlestore-mcp-server",
      "url": "https://github.com/madhukarkumar/singlestore-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/madhukarkumar.webp",
      "description": "Interact with SingleStore databases to execute queries, describe schemas, and generate ER diagrams. It supports SSL connections and includes error handling with TypeScript type safety.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-13T02:53:41Z",
      "readme_content": "# SingleStore MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@madhukarkumar/singlestore-mcp-server)](https://smithery.ai/server/@madhukarkumar/singlestore-mcp-server)\n\nA Model Context Protocol (MCP) server for interacting with SingleStore databases. This server provides tools for querying tables, describing schemas, and generating ER diagrams.\n\n## Features\n\n- List all tables in the database\n- Execute custom SQL queries\n- Get detailed table information including schema and sample data\n- Generate Mermaid ER diagrams of database schema\n- SSL support with automatic CA bundle fetching\n- Proper error handling and TypeScript type safety\n\n## Prerequisites\n\n- Node.js 16 or higher\n- npm or yarn\n- Access to a SingleStore database\n- SingleStore CA bundle (automatically fetched from portal)\n\n## Installation\n\n### Installing via Smithery\n\nTo install SingleStore MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@madhukarkumar/singlestore-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @madhukarkumar/singlestore-mcp-server --client claude\n```\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd mcp-server-singlestore\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n## Environment Variables\n\n### Required Environment Variables\n\nThe server requires the following environment variables for database connection:\n\n```env\nSINGLESTORE_HOST=your-host.singlestore.com\nSINGLESTORE_PORT=3306\nSINGLESTORE_USER=your-username\nSINGLESTORE_PASSWORD=your-password\nSINGLESTORE_DATABASE=your-database\n```\n\nAll these environment variables are required for the server to establish a connection to your SingleStore database. The connection uses SSL with the SingleStore CA bundle, which is automatically fetched from the SingleStore portal.\n\n### Optional Environment Variables\n\nFor SSE (Server-Sent Events) protocol support:\n\n```env\nSSE_ENABLED=true       # Enable the SSE HTTP server (default: false if not set)\nSSE_PORT=3333          # HTTP port for the SSE server (default: 3333 if not set)\n```\n\n### Setting Environment Variables\n\n1. **In Your Shell**:\n   Set the variables in your terminal before running the server:\n   ```bash\n   export SINGLESTORE_HOST=your-host.singlestore.com\n   export SINGLESTORE_PORT=3306\n   export SINGLESTORE_USER=your-username\n   export SINGLESTORE_PASSWORD=your-password\n   export SINGLESTORE_DATABASE=your-database\n   ```\n\n2. **In Client Configuration Files**:\n   Add the variables to your MCP client configuration file as shown in the integration sections below.\n\n## Usage\n\n### Protocol Support\n\nThis server supports two protocols for client integration:\n\n1. **MCP Protocol**: The standard Model Context Protocol using stdio communication, used by Claude Desktop, Windsurf, and Cursor.\n2. **SSE Protocol**: Server-Sent Events over HTTP for web-based clients and applications that need real-time data streaming.\n\nBoth protocols expose the same tools and functionality, allowing you to choose the best integration method for your use case.\n\n### Available Tools\n\n1. **list_tables**\n   - Lists all tables in the database\n   - No parameters required\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"list_tables\",\n     arguments: {}\n   })\n   ```\n\n2. **query_table**\n   - Executes a custom SQL query\n   - Parameters:\n     - query: SQL query string\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"query_table\",\n     arguments: {\n       query: \"SELECT * FROM your_table LIMIT 5\"\n     }\n   })\n   ```\n\n3. **describe_table**\n   - Gets detailed information about a table\n   - Parameters:\n     - table: Table name\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"describe_table\",\n     arguments: {\n       table: \"your_table\"\n     }\n   })\n   ```\n\n4. **generate_er_diagram**\n   - Generates a Mermaid ER diagram of the database schema\n   - No parameters required\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"generate_er_diagram\",\n     arguments: {}\n   })\n   ```\n\n5. **run_read_query**\n   - Executes a read-only (SELECT) query on the database\n   - Parameters:\n     - query: SQL SELECT query to execute\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"run_read_query\",\n     arguments: {\n       query: \"SELECT * FROM your_table LIMIT 5\"\n     }\n   })\n   ```\n\n6. **create_table**\n   - Create a new table in the database with specified columns and constraints\n   - Parameters:\n     - table_name: Name of the table to create\n     - columns: Array of column definitions\n     - table_options: Optional table configuration\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"create_table\",\n     arguments: {\n       table_name: \"new_table\",\n       columns: [\n         {\n           name: \"id\",\n           type: \"INT\",\n           nullable: false,\n           auto_increment: true\n         },\n         {\n           name: \"name\",\n           type: \"VARCHAR(255)\",\n           nullable: false\n         }\n       ],\n       table_options: {\n         shard_key: [\"id\"],\n         sort_key: [\"name\"]\n       }\n     }\n   })\n   ```\n\n7. **generate_synthetic_data**\n   - Generate and insert synthetic data into an existing table\n   - Parameters:\n     - table: Name of the table to insert data into\n     - count: Number of rows to generate (default: 100)\n     - column_generators: Custom generators for specific columns\n     - batch_size: Number of rows to insert in each batch (default: 1000)\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"generate_synthetic_data\",\n     arguments: {\n       table: \"customers\",\n       count: 1000,\n       column_generators: {\n         \"customer_id\": {\n           \"type\": \"sequence\",\n           \"start\": 1000\n         },\n         \"status\": {\n           \"type\": \"values\",\n           \"values\": [\"active\", \"inactive\", \"pending\"]\n         },\n         \"signup_date\": {\n           \"type\": \"formula\",\n           \"formula\": \"NOW() - INTERVAL FLOOR(RAND() * 365) DAY\"\n         }\n       },\n       batch_size: 500\n     }\n   })\n   ```\n\n8. **optimize_sql**\n   - Analyze a SQL query using PROFILE and provide optimization recommendations\n   - Parameters:\n     - query: SQL query to analyze and optimize\n   ```typescript\n   use_mcp_tool({\n     server_name: \"singlestore\",\n     tool_name: \"optimize_sql\",\n     arguments: {\n       query: \"SELECT * FROM customers JOIN orders ON customers.id = orders.customer_id WHERE region = 'west'\"\n     }\n   })\n   ```\n   - The response includes:\n     - Original query\n     - Performance profile summary (total runtime, compile time, execution time)\n     - List of detected bottlenecks\n     - Optimization recommendations with impact levels (high/medium/low)\n     - Suggestions for indexes, joins, memory usage, and other optimizations\n\n### Running Standalone\n\n1. Build the server:\n```bash\nnpm run build\n```\n\n2. Run the server with MCP protocol only:\n```bash\nnode build/index.js\n```\n\n3. Run the server with both MCP and SSE protocols:\n```bash\nSSE_ENABLED=true SSE_PORT=3333 node build/index.js\n```\n\n### Using the SSE Protocol\n\nWhen SSE is enabled, the server exposes the following HTTP endpoints:\n\n1. **Root Endpoint**\n   ```\n   GET /\n   ```\n   Returns server information and available endpoints.\n\n2. **Health Check**\n   ```\n   GET /health\n   ```\n   Returns status information about the server.\n\n3. **SSE Connection**\n   ```\n   GET /sse\n   ```\n   Establishes a Server-Sent Events connection for real-time updates.\n\n4. **List Tools**\n   ```\n   GET /tools\n   ```\n   Returns a list of all available tools, same as the MCP `list_tools` functionality.\n\n   Also supports POST requests for MCP Inspector compatibility:\n   ```\n   POST /tools\n   Content-Type: application/json\n   \n   {\n     \"jsonrpc\": \"2.0\",\n     \"id\": \"request-id\",\n     \"method\": \"mcp.list_tools\",\n     \"params\": {}\n   }\n   ```\n\n5. **Call Tool**\n   ```\n   POST /call-tool\n   Content-Type: application/json\n   \n   {\n     \"name\": \"tool_name\",\n     \"arguments\": {\n       \"param1\": \"value1\",\n       \"param2\": \"value2\"\n     },\n     \"client_id\": \"optional_sse_client_id_for_streaming_response\"\n   }\n   ```\n   Executes a tool with the provided arguments.\n   \n   - If `client_id` is provided, the response is streamed to that SSE client.\n   - If `client_id` is omitted, the response is returned directly in the HTTP response.\n   \n   Also supports standard MCP format for MCP Inspector compatibility:\n   ```\n   POST /call-tool\n   Content-Type: application/json\n   \n   {\n     \"jsonrpc\": \"2.0\",\n     \"id\": \"request-id\",\n     \"method\": \"mcp.call_tool\",\n     \"params\": {\n       \"name\": \"tool_name\",\n       \"arguments\": {\n         \"param1\": \"value1\",\n         \"param2\": \"value2\"\n       },\n       \"_meta\": {\n         \"client_id\": \"optional_sse_client_id_for_streaming_response\"\n       }\n     }\n   }\n   ```\n\n#### SSE Event Types\n\nWhen using SSE connections, the server sends the following event types:\n\n1. **message** (unnamed event): Sent when an SSE connection is successfully established.\n2. **open**: Additional connection established event.\n3. **message**: Used for all MCP protocol messages including tool start, result, and error events.\n\nAll events follow the JSON-RPC 2.0 format used by the MCP protocol. The system uses the standard `message` event type for compatibility with the MCP Inspector and most SSE client libraries.\n\n#### Example JavaScript Client\n\n```javascript\n// Connect to SSE endpoint\nconst eventSource = new EventSource('http://localhost:3333/sse');\nlet clientId = null;\n\n// Handle connection establishment via unnamed event\neventSource.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  if (data.type === 'connection_established') {\n    clientId = data.clientId;\n    console.log(`Connected with client ID: ${clientId}`);\n  }\n};\n\n// Handle open event\neventSource.addEventListener('open', (event) => {\n  console.log('SSE connection opened via open event');\n});\n\n// Handle all MCP messages\neventSource.addEventListener('message', (event) => {\n  const data = JSON.parse(event.data);\n  \n  if (data.jsonrpc === '2.0') {\n    if (data.result) {\n      console.log('Tool result:', data.result);\n    } else if (data.error) {\n      console.error('Tool error:', data.error);\n    } else if (data.method === 'mcp.call_tool.update') {\n      console.log('Tool update:', data.params);\n    }\n  }\n});\n\n// Call a tool with streaming response (custom format)\nasync function callTool(name, args) {\n  const response = await fetch('http://localhost:3333/call-tool', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      name: name,\n      arguments: args,\n      client_id: clientId\n    })\n  });\n  return response.json();\n}\n\n// Call a tool with streaming response (MCP format)\nasync function callToolMcp(name, args) {\n  const response = await fetch('http://localhost:3333/call-tool', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      jsonrpc: '2.0',\n      id: 'request-' + Date.now(),\n      method: 'mcp.call_tool',\n      params: {\n        name: name,\n        arguments: args,\n        _meta: {\n          client_id: clientId\n        }\n      }\n    })\n  });\n  return response.json();\n}\n\n// Example usage\ncallTool('list_tables', {})\n  .then(response => console.log('Request accepted:', response));\n```\n\n### Using with MCP Inspector\n\nThe MCP Inspector is a browser-based tool for testing and debugging MCP servers. To use it with this server:\n\n1. Start both the server and MCP inspector in one command:\n   ```bash\n   npm run inspector\n   ```\n   \n   Or start just the server with:\n   ```bash\n   npm run start:inspector\n   ```\n\n2. To install and run the MCP Inspector separately:\n   ```bash\n   npx @modelcontextprotocol/inspector\n   ```\n   \n   The inspector will open in your default browser.\n\n3. When the MCP Inspector opens:\n   \n   a. Enter the URL in the connection field:\n      ```\n      http://localhost:8081\n      ```\n      \n      Note: The actual port may vary depending on your configuration. Check the server \n      startup logs for the actual port being used. The server will output:\n      ```\n      MCP SingleStore SSE server listening on port XXXX\n      ```\n      \n   b. Make sure \"SSE\" is selected as the transport type\n   \n   c. Click \"Connect\"\n\n4. If you encounter connection issues, try these alternatives:\n   \n   a. Try connecting to a specific endpoint:\n      ```\n      http://localhost:8081/stream\n      ```\n      \n   b. Try using your machine's actual IP address:\n      ```\n      http://192.168.1.x:8081\n      ```\n      \n   c. If running in Docker:\n      ```\n      http://host.docker.internal:8081\n      ```\n\n5. **Debugging connection issues**:\n   \n   a. Verify the server is running by visiting http://localhost:8081 in your browser\n   \n   b. Check the server logs for connection attempts\n   \n   c. Try restarting both the server and inspector\n   \n   d. Make sure no other service is using port 8081\n   \n   e. Test SSE connection with the provided script:\n      ```bash\n      npm run test:sse\n      ```\n      \n      Or manually with curl:\n      ```bash\n      curl -N http://localhost:8081/sse\n      ```\n      \n   f. Verify your firewall settings allow connections to port 8081\n\n6. Once connected, the inspector will show all available tools and allow you to test them interactively.\n\n⚠️ **Note**: When using the MCP Inspector, you must use the full URL, including the `http://` prefix.\n\n## MCP Client Integration\n\n### Installing in Claude Desktop\n\n1. Add the server configuration to your Claude Desktop config file located at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"singlestore\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-server-singlestore/build/index.js\"],\n      \"env\": {\n        \"SINGLESTORE_HOST\": \"your-host.singlestore.com\",\n        \"SINGLESTORE_PORT\": \"3306\",\n        \"SINGLESTORE_USER\": \"your-username\",\n        \"SINGLESTORE_PASSWORD\": \"your-password\",\n        \"SINGLESTORE_DATABASE\": \"your-database\",\n        \"SSE_ENABLED\": \"true\",\n        \"SSE_PORT\": \"3333\"\n      }\n    }\n  }\n}\n```\n\nThe SSE_ENABLED and SSE_PORT variables are optional. Include them if you want to enable the HTTP server with SSE support alongside the standard MCP protocol.\n\n2. Restart the Claude Desktop App\n\n3. In your conversation with Claude, you can now use the SingleStore MCP server with:\n```\nuse_mcp_tool({\n  server_name: \"singlestore\",\n  tool_name: \"list_tables\",\n  arguments: {}\n})\n```\n\n### Installing in Windsurf \n\n1. Add the server configuration to your Windsurf config file located at:\n   - macOS: `~/Library/Application Support/Windsurf/config.json`\n   - Windows: `%APPDATA%\\Windsurf\\config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"singlestore\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-server-singlestore/build/index.js\"],\n      \"env\": {\n        \"SINGLESTORE_HOST\": \"your-host.singlestore.com\",\n        \"SINGLESTORE_PORT\": \"3306\",\n        \"SINGLESTORE_USER\": \"your-username\",\n        \"SINGLESTORE_PASSWORD\": \"your-password\",\n        \"SINGLESTORE_DATABASE\": \"your-database\",\n        \"SSE_ENABLED\": \"true\",\n        \"SSE_PORT\": \"3333\"\n      }\n    }\n  }\n}\n```\n\nThe SSE_ENABLED and SSE_PORT variables are optional, but enable additional functionality through the SSE HTTP server.\n\n2. Restart Windsurf\n\n3. In your conversation with Claude in Windsurf, the SingleStore MCP tools will be available automatically when Claude needs to access database information.\n\n### Installing in Cursor\n\n1. Add the server configuration to your Cursor settings:\n   - Open Cursor\n   - Go to Settings (gear icon) > Extensions > Claude AI > MCP Servers\n   - Add a new MCP server with the following configuration:\n\n```json\n{\n  \"singlestore\": {\n    \"command\": \"node\",\n    \"args\": [\"path/to/mcp-server-singlestore/build/index.js\"],\n    \"env\": {\n      \"SINGLESTORE_HOST\": \"your-host.singlestore.com\",\n      \"SINGLESTORE_PORT\": \"3306\",\n      \"SINGLESTORE_USER\": \"your-username\",\n      \"SINGLESTORE_PASSWORD\": \"your-password\",\n      \"SINGLESTORE_DATABASE\": \"your-database\",\n      \"SSE_ENABLED\": \"true\",\n      \"SSE_PORT\": \"3333\"\n    }\n  }\n}\n```\n\nThe SSE_ENABLED and SSE_PORT variables allow web applications to connect to the server via HTTP and receive real-time updates through Server-Sent Events.\n\n2. Restart Cursor\n\n3. When using Claude AI within Cursor, the SingleStore MCP tools will be available for database operations.\n\n\n## Security Considerations\n\n1. Never commit credentials to version control\n2. Use environment variables or secure configuration management\n3. Consider using a connection pooling mechanism for production use\n4. Implement appropriate access controls and user permissions in SingleStore\n5. Keep the SingleStore CA bundle up to date\n\n## Development\n\n### Project Structure\n\n```\nmcp-server-singlestore/\n├── src/\n│   └── index.ts      # Main server implementation\n├── package.json\n├── tsconfig.json\n├── README.md\n└── CHANGELOG.md\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Testing\n\n```bash\nnpm test\n```\n\n## Troubleshooting\n\n1. **Connection Issues**\n   - Verify credentials and host information in your environment variables\n   - Check SSL configuration\n   - Ensure database is accessible from your network\n   - Check your firewall settings to allow outbound connections to your SingleStore database\n\n2. **Build Issues**\n   - Clear node_modules and reinstall dependencies\n   - Verify TypeScript configuration\n   - Check Node.js version compatibility (should be 16+)\n\n3. **MCP Integration Issues**\n   - Verify the path to the server's build/index.js file is correct in your client configuration\n   - Check that all environment variables are properly set in your client configuration\n   - Restart your client application after making configuration changes\n   - Check client logs for any error messages related to the MCP server\n   - Try running the server standalone first to validate it works outside the client\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "singlestore",
        "database",
        "singlestore databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "mangooer--mysql-mcp-server-sse": {
      "owner": "mangooer",
      "name": "mysql-mcp-server-sse",
      "url": "https://github.com/mangooer/mysql-mcp-server-sse",
      "imageUrl": "/freedevtools/mcp/pfp/mangooer.webp",
      "description": "Offers real-time querying capabilities for MySQL databases with support for data transmission via SSE, designed for seamless high-concurrency database operations and includes robust security measures.",
      "stars": 83,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T17:12:56Z",
      "readme_content": "# MySQL查询服务器 / MySQL Query Server\n\n---\n\n## 1. 项目简介 / Project Introduction\n\n本项目是基于MCP框架的MySQL查询服务器，支持通过SSE协议进行实时数据库操作，具备完善的安全、日志、配置和敏感信息保护机制，适用于开发、测试和生产环境下的安全MySQL数据访问。\n\nThis project is a MySQL query server based on the MCP framework, supporting real-time database operations via SSE protocol. It features comprehensive security, logging, configuration, and sensitive information protection mechanisms, suitable for secure MySQL data access in development, testing, and production environments.\n\n---\n\n## 2. 主要特性 / Key Features\n\n- 基于FastMCP框架，异步高性能\n- 支持高并发的数据库连接池，参数灵活可调\n- 支持SSE实时推送\n- 丰富的MySQL元数据与结构查询API\n- 自动事务管理与回滚\n- 多级SQL风险控制与注入防护\n- **数据库隔离安全**：防止跨数据库访问，支持三级访问控制\n- 敏感信息自动隐藏与自定义\n- 灵活的环境变量配置\n- 完善的日志与错误处理\n- Docker支持，快速部署\n\n- Built on FastMCP framework, high-performance async\n- Connection pool for high concurrency, with flexible parameter tuning\n- SSE real-time push support\n- Rich MySQL metadata & schema query APIs\n- Automatic transaction management & rollback\n- Multi-level SQL risk control & injection protection\n- **Database Isolation Security**: Prevents cross-database access with 3-level access control\n- Automatic and customizable sensitive info masking\n- Flexible environment variable configuration\n- Robust logging & error handling\n- Docker support for quick deployment\n\n---\n\n## 3. 快速开始 / Quick Start\n\n### Docker 方式 / Docker Method\n\n```bash\n# 拉取镜像\ndocker pull mangooer/mysql-mcp-server-sse:latest\n\n# 运行容器\ndocker run -d \\\n  --name mysql-mcp-server-sse \\\n  -e HOST=0.0.0.0 \\\n  -e PORT=3000 \\\n  -e MYSQL_HOST=your_mysql_host \\\n  -e MYSQL_PORT=3306 \\\n  -e MYSQL_USER=your_mysql_user \\\n  -e MYSQL_PASSWORD=your_mysql_password \\\n  -e MYSQL_DATABASE=your_database \\\n  -p 3000:3000 \\\n  mangooer/mysql-mcp-server-sse:latest\n```\n\nWindows PowerShell 格式：\n```powershell\ndocker run -d `\n  --name mysql-mcp-server-sse `\n  -e HOST=0.0.0.0 `\n  -e PORT=3000 `\n  -e MYSQL_HOST=your_mysql_host `\n  -e MYSQL_PORT=3306 `\n  -e MYSQL_USER=your_mysql_user `\n  -e MYSQL_PASSWORD=your_mysql_password `\n  -e MYSQL_DATABASE=your_database `\n  -p 3000:3000 `\n  mangooer/mysql-mcp-server-sse:latest\n```\n\n### 源码方式 / Source Code Method\n\n#### 安装依赖 / Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n#### 配置环境变量 / Configure Environment Variables\n复制`.env.example`为`.env`，并根据实际情况修改。\nCopy `.env.example` to `.env` and modify as needed.\n\n#### 启动服务 / Start the Server\n```bash\npython -m src.server\n```\n默认监听：http://127.0.0.1:3000/sse\nDefault endpoint: http://127.0.0.1:3000/sse\n\n---\n\n## 4. 目录结构 / Project Structure\n\n```\n.\n├── src/\n│   ├── server.py           # 主服务器入口 / Main server entry\n│   ├── config.py           # 配置项定义 / Config definitions\n│   ├── validators.py       # 参数校验 / Parameter validation\n│   ├── db/\n│   │   └── mysql_operations.py # 数据库操作 / DB operations\n│   ├── security/\n│   │   ├── interceptor.py      # SQL拦截 / SQL interception\n│   │   ├── query_limiter.py    # 风险控制 / Risk control\n│   │   └── sql_analyzer.py     # SQL分析 / SQL analysis\n│   └── tools/\n│       ├── mysql_tool.py           # 基础查询 / Basic query\n│       ├── mysql_metadata_tool.py  # 元数据查询 / Metadata query\n│       ├── mysql_info_tool.py      # 信息查询 / Info query\n│       ├── mysql_schema_tool.py    # 结构查询 / Schema query\n│       └── metadata_base_tool.py   # 工具基类 / Tool base class\n├── tests/                  # 测试 / Tests\n├── .env.example            # 环境变量示例 / Env example\n└── requirements.txt        # 依赖 / Requirements\n```\n\n---\n\n## 5. 环境变量与配置 / Environment Variables & Configuration\n\n| 变量名 / Variable         | 说明 / Description                                   | 默认值 / Default |\n|--------------------------|------------------------------------------------------|------------------|\n| HOST                     | 服务器监听地址 / Server listen address                | 127.0.0.1        |\n| PORT                     | 服务器监听端口 / Server listen port                   | 3000             |\n| MYSQL_HOST               | MySQL服务器地址 / MySQL server host                   | localhost        |\n| MYSQL_PORT               | MySQL服务器端口 / MySQL server port                   | 3306             |\n| MYSQL_USER               | MySQL用户名 / MySQL username                          | root             |\n| MYSQL_PASSWORD           | MySQL密码 / MySQL password                            | (空/empty)       |\n| MYSQL_DATABASE           | 要连接的数据库名 / Database name                      | (空/empty)       |\n| DB_CONNECTION_TIMEOUT    | 连接超时时间(秒) / Connection timeout (seconds)       | 5                |\n| DB_AUTH_PLUGIN           | 认证插件类型 / Auth plugin type                       | mysql_native_password |\n| DB_POOL_ENABLED          | 是否启用连接池 / Enable connection pool (true/false)  | true             |\n| DB_POOL_MIN_SIZE         | 连接池最小连接数 / Pool min size                      | 5                |\n| DB_POOL_MAX_SIZE         | 连接池最大连接数 / Pool max size                      | 20               |\n| DB_POOL_RECYCLE          | 连接回收时间(秒) / Pool recycle time (seconds)        | 300              |\n| DB_POOL_MAX_LIFETIME     | 连接最大存活时间(秒, 0=不限制) / Max lifetime (sec)   | 0                |\n| DB_POOL_ACQUIRE_TIMEOUT  | 获取连接超时时间(秒) / Acquire timeout (seconds)      | 10.0             |\n| ENV_TYPE                 | 环境类型(development/production) / Env type           | development      |\n| ALLOWED_RISK_LEVELS      | 允许的风险等级(逗号分隔) / Allowed risk levels        | LOW,MEDIUM       |\n| ALLOW_SENSITIVE_INFO     | 允许查询敏感字段 / Allow sensitive info (true/false)  | false            |\n| SENSITIVE_INFO_FIELDS    | 自定义敏感字段模式(逗号分隔) / Custom sensitive fields | (空/empty)       |\n| MAX_SQL_LENGTH           | 最大SQL语句长度 / Max SQL length                      | 5000             |\n| BLOCKED_PATTERNS         | 阻止的SQL模式(逗号分隔) / Blocked SQL patterns        | (空/empty)       |\n| ENABLE_QUERY_CHECK       | 启用查询安全检查 / Enable query check (true/false)    | true             |\n| **ENABLE_DATABASE_ISOLATION** | **启用数据库隔离 / Enable database isolation (true/false)** | **false** |\n| **DATABASE_ACCESS_LEVEL** | **数据库访问级别 / Database access level (strict/restricted/permissive)** | **permissive** |\n| LOG_LEVEL                | 日志级别(DEBUG/INFO/...) / Log level                 | DEBUG            |\n\n> 注/Note: 部分云MySQL需指定`DB_AUTH_PLUGIN`为`mysql_native_password`。\n\n### MySQL 8.0 认证支持 / MySQL 8.0 Authentication Support\n\n本系统完全支持 MySQL 8.0 的认证机制。MySQL 8.0 默认使用 `caching_sha2_password` 认证插件，提供更高的安全性。\n\nThis system fully supports MySQL 8.0 authentication mechanisms. MySQL 8.0 uses `caching_sha2_password` by default for enhanced security.\n\n#### 认证插件对比 / Authentication Plugin Comparison\n\n| 认证插件 / Plugin | 安全性 / Security | 兼容性 / Compatibility | 依赖要求 / Dependencies |\n|------------------|-------------------|------------------------|------------------------|\n| `mysql_native_password` | 中等 / Medium | 高 / High | 无 / None |\n| `caching_sha2_password` | 高 / High | 中等 / Medium | cryptography |\n\n#### 配置建议 / Configuration Recommendations\n\n**生产环境 / Production**（推荐 / Recommended）：\n```ini\nDB_AUTH_PLUGIN=caching_sha2_password\n```\n\n**开发环境 / Development**（简化配置 / Simplified）：\n```ini\nDB_AUTH_PLUGIN=mysql_native_password\n```\n\n#### 依赖安装 / Dependency Installation\n\n使用 `caching_sha2_password` 时需要安装 `cryptography` 包（已包含在 requirements.txt 中）：\n\nWhen using `caching_sha2_password`, the `cryptography` package is required (already included in requirements.txt):\n\n```bash\npip install cryptography\n```\n\n\n### 数据库隔离安全 / Database Isolation Security\n\n本系统提供强大的数据库隔离功能，防止跨数据库访问，确保数据安全。\n\nThis system provides robust database isolation features to prevent cross-database access and ensure data security.\n\n#### 访问级别 / Access Levels\n\n| 级别 / Level | 允许访问 / Allowed Access | 适用场景 / Use Case |\n|-------------|---------------------------|-------------------|\n| **strict** | 仅指定数据库 / Only specified database | 生产环境 / Production |\n| **restricted** | 指定数据库 + 系统库 / Specified + system databases | 开发环境 / Development |\n| **permissive** | 所有数据库 / All databases | 测试环境 / Testing |\n\n#### 启用数据库隔离 / Enable Database Isolation\n\n```bash\n# Docker 启用严格模式 / Docker with strict mode\ndocker run -d \\\n  -e MYSQL_DATABASE=your_database \\\n  -e ENABLE_DATABASE_ISOLATION=true \\\n  -e DATABASE_ACCESS_LEVEL=strict \\\n  mangooer/mysql-mcp-server-sse:latest\n\n# 生产环境自动启用 / Auto-enable in production\ndocker run -d \\\n  -e ENV_TYPE=production \\\n  -e MYSQL_DATABASE=your_database \\\n  mangooer/mysql-mcp-server-sse:latest\n```\n\n**安全效果 / Security Effects**：\n- ✅ 阻止 `SHOW DATABASES` / Blocks `SHOW DATABASES`\n- ✅ 阻止 `SELECT * FROM mysql.user` / Blocks `SELECT * FROM mysql.user`\n- ✅ 阻止 `SHOW TABLES FROM other_db` / Blocks `SHOW TABLES FROM other_db`\n- ✅ 允许当前数据库操作 / Allows current database operations\n\n> 🔒 **重要**：生产环境(`ENV_TYPE=production`)会自动启用数据库隔离，使用 `restricted` 模式。\n> \n> 🔒 **Important**: Production environment (`ENV_TYPE=production`) automatically enables database isolation with `restricted` mode.\n\n---\n\n## 6. 自动化与资源管理优化 / Automation & Resource Management Enhancements\n\n### 自动化工具注册 / Automated Tool Registration\n- 所有MySQL相关API工具均采用自动注册机制：\n  - 无需手动在主入口维护注册代码，新增/删除工具只需在`src/tools/`目录下实现`register_xxx_tool(s)`函数即可。\n  - 系统启动时自动扫描并注册，极大提升可维护性和扩展性。\n- All MySQL-related API tools are registered automatically:\n  - No need to manually maintain registration code in the main entry. To add or remove a tool, simply implement a `register_xxx_tool(s)` function in the `src/tools/` directory.\n  - The system scans and registers tools automatically at startup, greatly improving maintainability and extensibility.\n\n### 连接池自动回收与资源管理 / Connection Pool Auto-Recycling & Resource Management\n- 连接池采用事件循环隔离与自动回收机制：\n  - 每个事件循环独立池，支持高并发与多环境。\n  - 定期（默认每5分钟）自动回收无效或失效的连接池，防止资源泄漏。\n  - 事件循环关闭时自动关闭对应连接池，确保资源彻底释放。\n  - 支持多数据库/多租户场景扩展。\n- 所有资源管理操作均有详细日志，便于追踪和排查。\n- The connection pool uses event loop isolation and auto-recycling:\n  - Each event loop has its own pool, supporting high concurrency and multi-environment deployment.\n  - Unused or invalid pools are automatically recycled every 5 minutes (by default), preventing resource leaks.\n  - When an event loop is closed, its pool is automatically closed to ensure complete resource release.\n  - Ready for multi-database/multi-tenant scenarios.\n- All resource management operations are logged in detail for easy tracking and troubleshooting.\n\n---\n\n## 7. 安全机制 / Security Mechanisms\n\n- 多级SQL风险等级（LOW/MEDIUM/HIGH/CRITICAL）\n- SQL注入与危险操作拦截\n- WHERE子句强制检查\n- **数据库隔离安全**：三级访问控制（strict/restricted/permissive）\n- **跨数据库访问防护**：阻止未授权的数据库访问\n- 敏感信息自动隐藏（支持自定义字段）\n- 生产环境默认只允许低风险操作\n- **生产环境自动启用数据库隔离**\n\n- Multi-level SQL risk levels (LOW/MEDIUM/HIGH/CRITICAL)\n- SQL injection & dangerous operation interception\n- Mandatory WHERE clause check\n- **Database Isolation Security**: 3-level access control (strict/restricted/permissive)\n- **Cross-database Access Protection**: Blocks unauthorized database access\n- Automatic sensitive info masking (customizable fields)\n- Production allows only low-risk operations by default\n- **Auto-enable database isolation in production**\n\n---\n\n## 8. 日志与错误处理 / Logging & Error Handling\n\n- 日志级别可配置（LOG_LEVEL）\n- 控制台与文件日志输出\n- 详细记录运行状态与错误\n- 完善的异常捕获与事务回滚\n\n- Configurable log level (LOG_LEVEL)\n- Console & file log output\n- Detailed running status & error logs\n- Robust exception capture & transaction rollback\n\n---\n\n## 9. 常见问题 / FAQ\n\n### Q: DELETE操作未执行成功？\nA: 检查是否有WHERE条件，无WHERE为高风险，需在ALLOWED_RISK_LEVELS中允许CRITICAL。\n\nQ: Why does DELETE not work?\nA: Check for WHERE clause. DELETE without WHERE is high risk (CRITICAL), must be allowed in ALLOWED_RISK_LEVELS.\n\n### Q: 如何自定义敏感字段？\nA: 设置SENSITIVE_INFO_FIELDS，如SENSITIVE_INFO_FIELDS=password,token\n\nQ: How to customize sensitive fields?\nA: Set SENSITIVE_INFO_FIELDS, e.g. SENSITIVE_INFO_FIELDS=password,token\n\n### Q: 如何启用数据库隔离？\nA: 设置ENABLE_DATABASE_ISOLATION=true和DATABASE_ACCESS_LEVEL=strict，或使用ENV_TYPE=production自动启用。\n\nQ: How to enable database isolation?\nA: Set ENABLE_DATABASE_ISOLATION=true and DATABASE_ACCESS_LEVEL=strict, or use ENV_TYPE=production for auto-enable.\n\n### Q: 数据库隔离后无法查询系统表？\nA: strict模式禁止系统表访问，可改为restricted模式，或检查是否确实需要系统表访问权限。\n\nQ: Cannot query system tables after enabling database isolation?\nA: strict mode blocks system table access. Use restricted mode or verify if system table access is actually needed.\n\n### Q: limit参数报错？\nA: limit必须为非负整数。\n\nQ: limit parameter error?\nA: limit must be a non-negative integer.\n\n---\n\n## 10. 贡献指南 / Contribution Guide\n\n欢迎通过Issue和Pull Request参与改进。\nContributions via Issue and Pull Request are welcome.\n\n---\n\n## 11. 许可证 / License\n\nMIT License\n\n本软件按\"原样\"提供，不提供任何形式的明示或暗示的保证，包括但不限于对适销性、特定用途的适用性和非侵权性的保证。在任何情况下，作者或版权持有人均不对任何索赔、损害或其他责任负责，无论是在合同诉讼、侵权行为还是其他方面，产生于、源于或与本软件有关，或与本软件的使用或其他交易有关。  \nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "mangooer mysql"
      ],
      "category": "databases"
    },
    "manpreet2000--mcp-database-server": {
      "owner": "manpreet2000",
      "name": "mcp-database-server",
      "url": "https://github.com/manpreet2000/mcp-database-server",
      "imageUrl": "/freedevtools/mcp/pfp/manpreet2000.webp",
      "description": "Enable interaction with databases via natural language, supporting operations like querying, inserting, and deleting documents. Currently integrates with MongoDB and plans to extend support to additional databases such as PostgreSQL, CockroachDB, and Redis.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-03-23T16:56:45Z",
      "readme_content": "# MCP Database Server\n\nA Model Context Protocol (MCP) server implementation that allows Large Language Models (LLMs) to interact with various databases through natural language. Currently supports MongoDB, with plans to support:\n\n- PostgreSQL\n- CockroachDB\n- Redis\n- And more...\n\n## Features\n\n- Database operations through natural language\n- Currently supports MongoDB with features:\n  - List all collections\n  - Query documents with filtering and projection\n  - Insert documents\n  - Delete documents\n  - Aggregate pipeline operations\n- Future support for other databases:\n  - PostgreSQL: SQL queries, table operations\n  - CockroachDB: Distributed SQL operations\n  - Redis: Key-value operations, caching\n\n## Prerequisites\n\n- Node.js v20.12.2 or higher\n- Database (currently MongoDB, other databases coming soon)\n- Claude Desktop Application\n\n## Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/manpreet2000/mcp-database-server.git\ncd mcp-database-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the TypeScript code:\n\n```bash\nnpm run build\n```\n\n## Configuration\n\nTo get started, you need to configure your database connection in your Claude Desktop configuration file:\n\n### MacOS\n\n```bash\n~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n### Windows\n\n```bash\n%APPDATA%/Claude/claude_desktop_config.json\n```\n\nAdd the following configuration to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"database\": {\n      \"command\": \"/path/to/node\",\n      \"args\": [\"/path/to/mcp-database/dist/index.js\"],\n      \"env\": {\n        \"MONGODB_URI\": \"your-mongodb-connection-string\"\n      }\n    }\n  }\n}\n```\n\nReplace:\n\n- `/path/to/node` with your Node.js executable path or just use `node`\n- `/path/to/mcp-database` with the absolute path to this repository\n- `your-mongodb-connection-string` with your MongoDB connection URL\n\n## Usage Examples\n\n### MongoDB Examples\n\n1. List all collections in your database:\n\n```\nCan you show me all the collections in my database?\n```\n\n2. Get specific records from a collection:\n\n```\nGive me 2 records from the chargers collection\n```\n\n3. Query with filters:\n\n```\nShow me all documents in the users collection where status is active\n```\n\n4. Insert a document:\n\n```\nAdd a new user to the users collection with name John and email john@example.com\n```\n\n5. Delete a document:\n\n```\nRemove the user with email john@example.com from the users collection\n```\n\n6. Aggregate data:\n\n```\nShow me the total count of users by status in the users collection\n```\n\n## Available Tools\n\n### 1. getCollections\n\nLists all collections in the connected database.\n\n### 2. getCollection\n\nRetrieves documents from a collection with optional query parameters:\n\n- `collectionName`: Name of the collection\n- `limit`: Maximum number of documents to return (default: 10, max: 1000)\n- `query`: MongoDB query object\n- `projection`: Fields to include/exclude\n\n### 3. insertOne\n\nInserts a single document into a collection:\n\n- `collectionName`: Name of the collection\n- `document`: Document object to insert\n\n### 4. deleteOne\n\nDeletes a single document from a collection:\n\n- `collectionName`: Name of the collection\n- `query`: Query to match the document to delete\n\n### 5. aggregate\n\nExecutes an aggregation pipeline:\n\n- `collectionName`: Name of the collection\n- `pipeline`: Array of aggregation stages\n- `options`: Optional aggregation options\n\n## Future Database Support\n\n### PostgreSQL\n\n- SQL query execution\n- Table operations\n- Schema management\n- Transaction support\n\n### CockroachDB\n\n- Distributed SQL operations\n- Multi-region support\n- Transaction management\n- Schema operations\n\n### Redis\n\n- Key-value operations\n- Caching mechanisms\n- Pub/sub operations\n- Data structure operations\n\n## Security\n\n- Never commit your database connection strings to version control\n- Use environment variables for sensitive information\n- Follow database-specific security best practices\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n\nMIT License - See [LICENSE](LICENSE) for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mongodb",
        "database",
        "secure database",
        "databases secure",
        "integrates mongodb"
      ],
      "category": "databases"
    },
    "meanands--mysql-mcp": {
      "owner": "meanands",
      "name": "mysql-mcp",
      "url": "https://github.com/meanands/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/meanands.webp",
      "description": "Interact with local MySQL databases using natural language, executing SQL queries and managing transactions for data consistency. Supports multiple database connections and all types of SQL commands.",
      "stars": 2,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-12T08:56:57Z",
      "readme_content": "# SQL MCP Server for Claude Desktop\n\nA natural language interface to your local MySQL databases through Claude Desktop. This MCP (Machine Communication Protocol) server allows Claude to execute SQL queries on your local MySQL databases, enabling you to interact with your databases using natural language.\n\n## Features\n\n- Natural language to SQL query conversion through Claude\n- Secure connection to local MySQL databases\n- Support for multiple databases\n- Transaction management for data consistency\n- Connection pooling for better performance\n- Support for all types of SQL queries (SELECT, INSERT, UPDATE, DELETE, etc.)\n\n## Prerequisites\n\n- Python 3.8 or higher\n- MySQL server installed and running\n- Claude Desktop application\n- Virtual environment (recommended)\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone git@github.com:meanands/mysql-mcp.git\ncd mysql-mcp\n```\n\n2. Create and activate a virtual environment:\n```bash\n# For macOS/Linux\npython -m venv venv\nsource venv/bin/activate\n\n# For Windows\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Create a `.env` file in the project root with your MySQL credentials:\n```env\nMYSQL_HOST=localhost\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\n```\n\n5. Update the directory path in `run.sh`:\n```bash\n# Open run.sh and replace this line:\ncd /Users/yourname/code/sql-mcp\n# with your actual project directory path, for example:\ncd /Users/yourname/projects/mysql-mcp\n```\n\n6. Make the run script executable:\n```bash\nchmod +x run.sh\n```\n\n## Configuration in Claude Desktop\n\n1. Open Claude Desktop's MCP configuration\n2. Add the following configuration:\n```json\n{\n  \"sql\": {\n    \"command\": \"/absolute/path/to/your/mysql-mcp/run.sh\"\n  }\n}\n```\nReplace `/absolute/path/to/your/mysql-mcp` with the actual absolute path to your project directory.\n\n## Usage\n\nOnce configured, you can interact with your databases through Claude Desktop using natural language. Examples:\n\n1. Selecting a database and creating a table:\n```\nUse the 'employees' database and create a table for storing employee information with fields for name, email, and department.\n```\n\n2. Inserting data:\n```\nInsert a new employee named John Doe with email john.doe@example.com in the Engineering department.\n```\n\n3. Querying data:\n```\nShow me all employees in the Engineering department.\n```\n\n## Important Notes\n\n- Always use absolute paths in the run.sh script and Claude Desktop configuration\n- Ensure MySQL server is running before using the MCP server\n- Keep your .env file secure and never commit it to version control\n- The MCP server uses connection pooling with a default pool size of 5 connections\n\n## Troubleshooting\n\n1. If you get a \"connection refused\" error, ensure your MySQL server is running\n2. If you get an authentication error, verify your credentials in the .env file\n3. For permission errors, ensure your MySQL user has appropriate privileges for the operations you're trying to perform\n\n## Security Considerations\n\n- Store sensitive credentials in the .env file\n- Use a MySQL user with appropriate permissions (avoid using root)\n- Keep your virtual environment and dependencies up to date\n- Consider network security if accessing non-localhost MySQL servers\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "databases using"
      ],
      "category": "databases"
    },
    "meilisearch--meilisearch-mcp": {
      "owner": "meilisearch",
      "name": "meilisearch-mcp",
      "url": "https://github.com/meilisearch/meilisearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/meilisearch.webp",
      "description": "Interact with Meilisearch to manage indices and documents, configure settings, monitor tasks, and handle API keys. It supports dynamic connections to multiple Meilisearch instances and offers smart search capabilities across indices.",
      "stars": 143,
      "forks": 18,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:04Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://github.com/meilisearch/meilisearch/blob/main/assets/logo.svg\" alt=\"Meilisearch\" width=\"200\" height=\"200\" />\n</div>\n\n<h1 align=\"center\">Meilisearch MCP Server</h1>\n\n<h4 align=\"center\">\n  <a href=\"https://github.com/meilisearch/meilisearch\">Meilisearch</a> |\n  <a href=\"https://www.meilisearch.com/cloud?utm_campaign=oss&utm_source=github&utm_medium=meilisearch-mcp\">Meilisearch Cloud</a> |\n  <a href=\"https://www.meilisearch.com/docs\">Documentation</a> |\n  <a href=\"https://discord.meilisearch.com\">Discord</a>\n</h4>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/meilisearch-mcp/\"><img src=\"https://img.shields.io/pypi/v/meilisearch-mcp.svg\" alt=\"PyPI version\"></a>\n  <a href=\"https://pypi.org/project/meilisearch-mcp/\"><img src=\"https://img.shields.io/pypi/pyversions/meilisearch-mcp.svg\" alt=\"Python Versions\"></a>\n  <a href=\"https://github.com/meilisearch/meilisearch-mcp/actions\"><img src=\"https://github.com/meilisearch/meilisearch-mcp/workflows/Test%20and%20Lint/badge.svg\" alt=\"Tests\"></a>\n  <a href=\"https://github.com/meilisearch/meilisearch-mcp/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-informational\" alt=\"License\"></a>\n  <a href=\"https://pypi.org/project/meilisearch-mcp/\"><img src=\"https://img.shields.io/pypi/dm/meilisearch-mcp\" alt=\"Downloads\"></a>\n</p>\n\n<p align=\"center\">⚡ Connect any LLM to Meilisearch and supercharge your AI with lightning-fast search capabilities! 🔍</p>\n\n## 🤔 What is this?\n\nThe Meilisearch MCP Server is a Model Context Protocol server that enables any MCP-compatible client (including Claude, OpenAI agents, and other LLMs) to interact with Meilisearch. This stdio-based server allows AI assistants to manage search indices, perform searches, and handle your data through natural conversation.\n\n**Why use this?**\n- 🤖 **Universal Compatibility** - Works with any MCP client, not just Claude\n- 🗣️ **Natural Language Control** - Manage Meilisearch through conversation with any LLM\n- 🚀 **Zero Learning Curve** - No need to learn Meilisearch's API\n- 🔧 **Full Feature Access** - All Meilisearch capabilities at your fingertips\n- 🔄 **Dynamic Connections** - Switch between Meilisearch instances on the fly\n- 📡 **stdio Transport** - Currently uses stdio; native Meilisearch MCP support coming soon!\n\n## ✨ Key Features\n\n- 📊 **Index & Document Management** - Create, update, and manage search indices\n- 🔍 **Smart Search** - Search across single or multiple indices with advanced filtering\n- ⚙️ **Settings Configuration** - Fine-tune search relevancy and performance\n- 📈 **Task Monitoring** - Track indexing progress and system operations\n- 🔐 **API Key Management** - Secure access control\n- 🏥 **Health Monitoring** - Keep tabs on your Meilisearch instance\n- 🐍 **Python Implementation** - [TypeScript version also available](https://github.com/devlimelabs/meilisearch-ts-mcp)\n\n## 🚀 Quick Start\n\nGet up and running in just 3 steps!\n\n### 1️⃣ Install the package\n\n```bash\n# Using pip\npip install meilisearch-mcp\n\n# Or using uvx (recommended)\nuvx -n meilisearch-mcp\n```\n\n### 2️⃣ Configure Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"meilisearch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"-n\", \"meilisearch-mcp\"]\n    }\n  }\n}\n```\n\n### 3️⃣ Start Meilisearch\n\n```bash\n# Using Docker (recommended)\ndocker run -d -p 7700:7700 getmeili/meilisearch:v1.6\n\n# Or using Homebrew\nbrew install meilisearch\nmeilisearch\n```\n\nThat's it! Now you can ask your AI assistant to search and manage your Meilisearch data! 🎉\n\n## 📚 Examples\n\n### 💬 Talk to your AI assistant naturally:\n\n```\nYou: \"Create a new index called 'products' with 'id' as the primary key\"\nAI: I'll create that index for you... ✓ Index 'products' created successfully!\n\nYou: \"Add some products to the index\"\nAI: I'll add those products... ✓ Added 5 documents to 'products' index\n\nYou: \"Search for products under $50 with 'electronics' in the category\"\nAI: I'll search for those products... Found 12 matching products!\n```\n\n### 🔍 Advanced Search Example:\n\n```\nYou: \"Search across all my indices for 'machine learning' and sort by date\"\nAI: Searching across all indices... Found 47 results from 3 indices:\n- 'blog_posts': 23 articles about ML\n- 'documentation': 15 technical guides  \n- 'tutorials': 9 hands-on tutorials\n```\n\n## 🔧 Installation\n\n### Prerequisites\n\n- Python ≥ 3.9\n- Running Meilisearch instance\n- MCP-compatible client (Claude Desktop, OpenAI agents, etc.)\n\n### From PyPI\n\n```bash\npip install meilisearch-mcp\n```\n\n### From Source (for development)\n\n```bash\n# Clone repository\ngit clone https://github.com/meilisearch/meilisearch-mcp.git\ncd meilisearch-mcp\n\n# Create virtual environment and install\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n```\n\n### Using Docker\n\nPerfect for containerized environments like n8n workflows!\n\n#### From Docker Hub\n\n```bash\n# Pull the latest image\ndocker pull getmeili/meilisearch-mcp:latest\n\n# Or a specific version\ndocker pull getmeili/meilisearch-mcp:0.5.0\n\n# Run the container\ndocker run -it \\\n  -e MEILI_HTTP_ADDR=http://your-meilisearch:7700 \\\n  -e MEILI_MASTER_KEY=your-master-key \\\n  getmeili/meilisearch-mcp:latest\n```\n\n#### Build from Source\n\n```bash\n# Build your own image\ndocker build -t meilisearch-mcp .\ndocker run -it \\\n  -e MEILI_HTTP_ADDR=http://your-meilisearch:7700 \\\n  -e MEILI_MASTER_KEY=your-master-key \\\n  meilisearch-mcp\n```\n\n#### Integration with n8n\n\nFor n8n workflows, you can use the Docker image directly in your setup:\n```yaml\nmeilisearch-mcp:\n  image: getmeili/meilisearch-mcp:latest\n  environment:\n    - MEILI_HTTP_ADDR=http://meilisearch:7700\n    - MEILI_MASTER_KEY=masterKey\n```\n\n## 🛠️ What Can You Do?\n\n<details>\n<summary><b>🔗 Connection Management</b></summary>\n\n- View current connection settings\n- Switch between Meilisearch instances dynamically\n- Update API keys on the fly\n\n</details>\n\n<details>\n<summary><b>📁 Index Operations</b></summary>\n\n- Create new indices with custom primary keys\n- List all indices with stats\n- Delete indices and their data\n- Get detailed index metrics\n\n</details>\n\n<details>\n<summary><b>📄 Document Management</b></summary>\n\n- Add or update documents\n- Retrieve documents with pagination\n- Bulk import data\n\n</details>\n\n<details>\n<summary><b>🔍 Search Capabilities</b></summary>\n\n- Search with filters, sorting, and facets\n- Multi-index search\n- Semantic search with vectors\n- Hybrid search (keyword + semantic)\n\n</details>\n\n<details>\n<summary><b>⚙️ Settings & Configuration</b></summary>\n\n- Configure ranking rules\n- Set up faceting and filtering\n- Manage searchable attributes\n- Customize typo tolerance\n\n</details>\n\n<details>\n<summary><b>🔐 Security</b></summary>\n\n- Create and manage API keys\n- Set granular permissions\n- Monitor key usage\n\n</details>\n\n<details>\n<summary><b>📊 Monitoring & Health</b></summary>\n\n- Health checks\n- System statistics\n- Task monitoring\n- Version information\n\n</details>\n\n## 🌍 Environment Variables\n\nConfigure default connection settings:\n\n```bash\nMEILI_HTTP_ADDR=http://localhost:7700  # Default Meilisearch URL\nMEILI_MASTER_KEY=your_master_key       # Optional: Default API key\n```\n\n## 💻 Development\n\n### Setting Up Development Environment\n\n1. **Start Meilisearch**:\n   ```bash\n   docker run -d -p 7700:7700 getmeili/meilisearch:v1.6\n   ```\n\n2. **Install Development Dependencies**:\n   ```bash\n   uv pip install -r requirements-dev.txt\n   ```\n\n3. **Run Tests**:\n   ```bash\n   python -m pytest tests/ -v\n   ```\n\n4. **Format Code**:\n   ```bash\n   black src/ tests/\n   ```\n\n### Testing with MCP Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector python -m src.meilisearch_mcp\n```\n\n## 🤝 Community & Support\n\nWe'd love to hear from you! Here's how to get help and connect:\n\n- 💬 [Join our Discord](https://discord.meilisearch.com) - Chat with the community\n- 🐛 [Report Issues](https://github.com/meilisearch/meilisearch-mcp/issues) - Found a bug? Let us know!\n- 💡 [Feature Requests](https://github.com/meilisearch/meilisearch-mcp/issues) - Have an idea? We're listening!\n- 📖 [Meilisearch Docs](https://www.meilisearch.com/docs) - Learn more about Meilisearch\n\n## 🤗 Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Write tests for your changes\n4. Make your changes and run tests\n5. Format your code with `black`\n6. Commit your changes (`git commit -m 'Add amazing feature'`)\n7. Push to your branch (`git push origin feature/amazing-feature`)\n8. Open a Pull Request\n\nSee our [Contributing Guidelines](#contributing-1) for more details.\n\n## 📦 Release Process\n\nThis project uses automated versioning and publishing. When the version in `pyproject.toml` changes on the `main` branch, the package is automatically published to PyPI.\n\nSee the [Release Process](#release-process-1) section for detailed instructions.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n<p align=\"center\">\n  <b>Meilisearch</b> is an open-source search engine that offers a delightful search experience.<br>\n  Learn more about Meilisearch at <a href=\"https://www.meilisearch.com\">meilisearch.com</a>\n</p>\n\n---\n\n<details>\n<summary><h2>📖 Full Documentation</h2></summary>\n\n### Available Tools\n\n#### Connection Management\n- `get-connection-settings`: View current Meilisearch connection URL and API key status\n- `update-connection-settings`: Update URL and/or API key to connect to a different instance\n\n#### Index Management\n- `create-index`: Create a new index with optional primary key\n- `list-indexes`: List all available indexes\n- `delete-index`: Delete an existing index and all its documents\n- `get-index-metrics`: Get detailed metrics for a specific index\n\n#### Document Operations\n- `get-documents`: Retrieve documents from an index with pagination\n- `add-documents`: Add or update documents in an index\n\n#### Search\n- `search`: Flexible search across single or multiple indices with filtering and sorting options\n\n#### Settings Management\n- `get-settings`: View current settings for an index\n- `update-settings`: Update index settings (ranking, faceting, etc.)\n\n#### API Key Management\n- `get-keys`: List all API keys\n- `create-key`: Create new API key with specific permissions\n- `delete-key`: Delete an existing API key\n\n#### Task Management\n- `get-task`: Get information about a specific task\n- `get-tasks`: List tasks with optional filters\n- `cancel-tasks`: Cancel pending or enqueued tasks\n- `delete-tasks`: Delete completed tasks\n\n#### System Monitoring\n- `health-check`: Basic health check\n- `get-health-status`: Comprehensive health status\n- `get-version`: Get Meilisearch version information\n- `get-stats`: Get database statistics\n- `get-system-info`: Get system-level information\n\n### Development Setup\n\n#### Prerequisites\n\n1. **Start Meilisearch server**:\n   ```bash\n   # Using Docker (recommended for development)\n   docker run -d -p 7700:7700 getmeili/meilisearch:v1.6\n   \n   # Or using brew (macOS)\n   brew install meilisearch\n   meilisearch\n   \n   # Or download from https://github.com/meilisearch/meilisearch/releases\n   ```\n\n2. **Install development tools**:\n   ```bash\n   # Install uv for Python package management\n   pip install uv\n   \n   # Install Node.js for MCP Inspector testing\n   # Visit https://nodejs.org/ or use your package manager\n   ```\n\n### Running Tests\n\nThis project includes comprehensive integration tests that verify MCP tool functionality:\n\n```bash\n# Run all tests\npython -m pytest tests/ -v\n\n# Run specific test file\npython -m pytest tests/test_mcp_client.py -v\n\n# Run tests with coverage report\npython -m pytest --cov=src tests/\n\n# Run tests in watch mode (requires pytest-watch)\npytest-watch tests/\n```\n\n**Important**: Tests require a running Meilisearch instance on `http://localhost:7700`.\n\n### Code Quality\n\n```bash\n# Format code with Black\nblack src/ tests/\n\n# Run type checking (if mypy is configured)\nmypy src/\n\n# Lint code (if flake8 is configured)\nflake8 src/ tests/\n```\n\n### Contributing Guidelines\n\n1. **Fork and clone** the repository\n2. **Set up development environment** following the Development Setup section above\n3. **Create a feature branch** from `main`\n4. **Write tests first** if adding new functionality (Test-Driven Development)\n5. **Run tests locally** to ensure all tests pass before committing\n6. **Format code** with Black and ensure code quality\n7. **Commit changes** with descriptive commit messages\n8. **Push to your fork** and create a pull request\n\n### Development Workflow\n\n```bash\n# Create feature branch\ngit checkout -b feature/your-feature-name\n\n# Make your changes, write tests first\n# Edit files...\n\n# Run tests to ensure everything works\npython -m pytest tests/ -v\n\n# Format code\nblack src/ tests/\n\n# Commit and push\ngit add .\ngit commit -m \"Add feature description\"\ngit push origin feature/your-feature-name\n```\n\n### Testing Guidelines\n\n- All new features should include tests\n- Tests should pass before submitting PRs\n- Use descriptive test names and clear assertions\n- Test both success and error cases\n- Ensure Meilisearch is running before running tests\n\n### Release Process\n\nThis project uses automated versioning and publishing to PyPI. The release process is designed to be simple and automated.\n\n#### How Releases Work\n\n1. **Automated Publishing**: When the version number in `pyproject.toml` changes on the `main` branch, a GitHub Action automatically:\n   - Builds the Python package\n   - Publishes it to PyPI using trusted publishing\n   - Creates a new release on GitHub\n\n2. **Version Detection**: The workflow compares the current version in `pyproject.toml` with the previous commit to detect changes\n\n3. **PyPI Publishing**: Uses PyPA's official publish action with trusted publishing (no manual API keys needed)\n\n#### Creating a New Release\n\nTo create a new release, follow these steps:\n\n##### 1. Determine Version Number\n\nFollow [Semantic Versioning](https://semver.org/) (MAJOR.MINOR.PATCH):\n\n- **PATCH** (e.g., 0.4.0 → 0.4.1): Bug fixes, documentation updates, minor improvements\n- **MINOR** (e.g., 0.4.0 → 0.5.0): New features, new MCP tools, significant enhancements\n- **MAJOR** (e.g., 0.5.0 → 1.0.0): Breaking changes, major API changes\n\n##### 2. Update Version and Create PR\n\n```bash\n# 1. Create a branch from latest main\ngit checkout main\ngit pull origin main\ngit checkout -b release/v0.5.0\n\n# 2. Update version in pyproject.toml\n# Edit the version = \"0.4.0\" line to your new version\n\n# 3. Commit and push\ngit add pyproject.toml\ngit commit -m \"Bump version to 0.5.0\"\ngit push origin release/v0.5.0\n\n# 4. Create PR and get it reviewed/merged\ngh pr create --title \"Release v0.5.0\" --body \"Bump version for release\"\n```\n\n##### 3. Merge to Main\n\nOnce the PR is approved and merged to `main`, the GitHub Action will automatically:\n\n1. Detect the version change\n2. Build the package  \n3. Publish to PyPI at https://pypi.org/p/meilisearch-mcp\n4. Make the new version available via `pip install meilisearch-mcp`\n\n##### 4. Verify Release\n\nAfter merging, verify the release:\n\n```bash\n# Check GitHub Action status\ngh run list --workflow=publish.yml\n\n# Verify on PyPI (may take a few minutes)\npip index versions meilisearch-mcp\n\n# Test installation of new version\npip install --upgrade meilisearch-mcp\n```\n\n### Release Workflow File\n\nThe automated release is handled by `.github/workflows/publish.yml`, which:\n\n- Triggers on pushes to `main` branch\n- Checks if `pyproject.toml` version changed\n- Uses Python 3.10 and official build tools\n- Publishes using trusted publishing (no API keys required)\n- Provides verbose output for debugging\n\n### Troubleshooting Releases\n\n**Release didn't trigger**: Check that the version in `pyproject.toml` actually changed between commits\n\n**Build failed**: Check the GitHub Actions logs for Python package build errors\n\n**PyPI publish failed**: Verify the package name and that trusted publishing is configured properly\n\n**Version conflicts**: Ensure the new version number hasn't been used before on PyPI\n\n### Development vs Production Versions\n\n- **Development**: Install from source using `pip install -e .`\n- **Production**: Install from PyPI using `pip install meilisearch-mcp`\n- **Specific version**: Install using `pip install meilisearch-mcp==0.5.0`\n\n</details>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "meilisearch",
        "databases",
        "database",
        "access meilisearch",
        "meilisearch instances",
        "meilisearch manage"
      ],
      "category": "databases"
    },
    "memgraph--mcp-memgraph": {
      "owner": "memgraph",
      "name": "mcp-memgraph",
      "url": "https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph",
      "imageUrl": "",
      "description": "Memgraph MCP Server - includes a tool to run a query against Memgraph and a schema resource.",
      "stars": 25,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:19:48Z",
      "readme_content": "> [!IMPORTANT]  \n> **This repository has been merged into the [Memgraph AI Toolkit](https://github.com/memgraph/ai-toolkit) monorepo to avoid duplicating tools.  \n> It will be deleted in one month—please follow the [MCP integration](https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph) there for all future development, and feel free to open issues or PRs in that repo.**\n\n# 🚀 Memgraph MCP Server\n\nMemgraph MCP Server is a lightweight server implementation of the Model Context Protocol (MCP) designed to connect Memgraph with LLMs.\n\n\n\n## ⚡ Quick start\n\n> 📹 [Memgraph MCP Server Quick Start video](https://www.youtube.com/watch?v=0Tjw5QWj_qY)\n\n### 1. Run Memgraph MCP Server\n\n1. Install [`uv`](https://docs.astral.sh/uv/getting-started/installation/) and create `venv` with `uv venv`. Activate virtual environment with `.venv\\Scripts\\activate`. \n2. Install dependencies: `uv add \"mcp[cli]\" httpx`\n3. Run Memgraph MCP server: `uv run server.py`.\n\n\n### 2. Run MCP Client\n1. Install [Claude for Desktop](https://claude.ai/download).\n2. Add the Memgraph server to Claude config: \n\n**MacOS/Linux**\n```\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n**Windows**\n\n```\ncode $env:AppData\\Claude\\claude_desktop_config.json\n```\n\nExample config:\n```\n{\n    \"mcpServers\": {\n      \"mpc-memgraph\": {\n        \"command\": \"/Users/katelatte/.local/bin/uv\",\n        \"args\": [\n            \"--directory\",\n            \"/Users/katelatte/projects/mcp-memgraph\",\n            \"run\",\n            \"server.py\"\n        ]\n     }\n   }\n}\n```\n> [!NOTE]  \n> You may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS/Linux or `where uv` on Windows. Make sure you pass in the absolute path to your server.\n\n### 3. Chat with the database\n1. Run Memgraph MAGE:\n   ```\n   docker run -p 7687:7687 memgraph/memgraph-mage --schema-info-enabled=True\n   ```\n   The `--schema-info-enabled` configuration setting is set to `True` to allow LLM to run `SHOW SCHEMA INFO` query.\n2. Open Claude Desktop and see the Memgraph tools and resources listed. Try it out! (You can load dummy data from [Memgraph Lab](https://memgraph.com/docs/data-visualization) Datasets)\n\n## 🔧Tools\n\n### run_query()\nRun a Cypher query against Memgraph.\n\n## 🗃️ Resources\n\n### get_schema()\nGet Memgraph schema information (prerequisite: `--schema-info-enabled=True`).\n\n## 🗺️ Roadmap\n\nThe Memgraph MCP Server is just at its beginnings. We're actively working on expanding its capabilities and making it even easier to integrate Memgraph into modern AI workflows. In the near future, we'll be releasing a TypeScript version of the server to better support JavaScript-based environments. Additionally, we plan to migrate this project into our central [AI Toolkit](https://github.com/memgraph/ai-toolkit) repository, where it will live alongside other tools and integrations for LangChain, LlamaIndex, and MCP. Our goal is to provide a unified, open-source toolkit that makes it seamless to build graph-powered applications and intelligent agents with Memgraph at the core.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memgraph",
        "databases",
        "database",
        "memgraph schema",
        "query memgraph",
        "access memgraph"
      ],
      "category": "databases"
    },
    "michael7736--mysql-mcp-server": {
      "owner": "michael7736",
      "name": "mysql-mcp-server",
      "url": "https://github.com/michael7736/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/michael7736.webp",
      "description": "Provides access to a MySQL database for executing SQL queries. Facilitates data retrieval, creation, updating, and deletion, with results returned in JSON format.",
      "stars": 7,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-02T11:29:39Z",
      "readme_content": "# MySQL MCP Server\n\nThis is a Model Context Protocol (MCP) server that provides access to a MySQL database. It allows agent to execute SQL queries against a MySQL database.\n\n## Features\n\n- Execute SQL queries against a MySQL database:\n  - Read data (SELECT statements)\n  - Create tables (CREATE TABLE statements)\n  - Insert data (INSERT INTO statements)\n  - Update data (UPDATE statements)\n  - Delete data (DELETE FROM statements)\n- Returns query results in JSON format\n- Configurable database connection settings\n- Transaction logging with unique IDs\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- MySQL server\n- MCP SDK\n\n## Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n\n```bash\ncd mysql-mcp-server\nnpm install\n```\n\n3. Build the server:\n\n```bash\nnpm run build\n```\n\n## Configuration\n\nThe MySQL MCP server uses the following environment variables for configuration:\n\n- `MYSQL_HOST`: MySQL server hostname (default: 'localhost')\n- `MYSQL_PORT`: MySQL server port (default: 3306)\n- `MYSQL_USER`: MySQL username (default: 'mcp101')\n- `MYSQL_PASSWORD`: MySQL password (default: '123qwe')\n- `MYSQL_DATABASE`: MySQL database name (default: 'mcpdb')\n\n## Database Setup\n\n1. Create a MySQL database:\n\n```sql\nCREATE DATABASE mcpdb;\n```\n\n2. Create a MySQL user with access to the database:\n\n```sql\nCREATE USER 'mcp101'@'localhost' IDENTIFIED BY '123qwe';\nGRANT ALL PRIVILEGES ON mcpdb.* TO 'mcp101'@'localhost';\nFLUSH PRIVILEGES;\n```\n\n3. Create a test table with sample data:\n\n```sql\nUSE mcpdb;\nCREATE TABLE test_users (\n  id INT AUTO_INCREMENT PRIMARY KEY,\n  name VARCHAR(100),\n  email VARCHAR(100),\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO test_users (name, email) VALUES\n  ('John Doe', 'john@example.com'),\n  ('Jane Smith', 'jane@example.com'),\n  ('Bob Johnson', 'bob@example.com');\n```\n\n## MCP Configuration\n\nAdd the MySQL MCP server to your MCP settings file:\n\n### VSCode (Claude Extension)\n\nFile: `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n\nChange the args according your MySQL configuruation\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql-mcp-server\": {\n      \"autoApprove\": [],\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/mysql-mcp-server/build/index.js\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"mcp101\",\n        \"MYSQL_PASSWORD\": \"123qwe\",\n        \"MYSQL_DATABASE\": \"mcpdb\"\n      },\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n### Claude Desktop App\n\nFile: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql-mcp-server\": {\n      \"autoApprove\": [],\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/mysql-mcp-server/build/index.js\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"mcp101\",\n        \"MYSQL_PASSWORD\": \"123qwe\",\n        \"MYSQL_DATABASE\": \"mcpdb\"\n      },\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n## Usage\n\nOnce configured, you can use the MySQL MCP server in your conversations with Claude. For example:\n\n\"Can you show me all the users in the test_users table?\"\n\nClaude will use the `run_sql_query` tool to execute:\n\n```sql\nSELECT * FROM test_users\n```\n\n## Available Tools\n\n### run_sql_query\n\nExecutes a read-only SQL query (SELECT statements only) against the MySQL database.\n\nParameters:\n- `query`: The SQL SELECT query to execute.\n\nExample:\n```json\n{\n  \"query\": \"SELECT * FROM test_users\"\n}\n```\n\n### create_table\n\nCreates a new table in the MySQL database.\n\nParameters:\n- `query`: The SQL CREATE TABLE query to execute.\n\nExample:\n```json\n{\n  \"query\": \"CREATE TABLE products (id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100), price DECIMAL(10,2))\"\n}\n```\n\n### insert_data\n\nInserts data into a table in the MySQL database.\n\nParameters:\n- `query`: The SQL INSERT INTO query to execute.\n\nExample:\n```json\n{\n  \"query\": \"INSERT INTO products (name, price) VALUES ('Laptop', 999.99), ('Smartphone', 499.99)\"\n}\n```\n\n### update_data\n\nUpdates data in a table in the MySQL database.\n\nParameters:\n- `query`: The SQL UPDATE query to execute.\n\nExample:\n```json\n{\n  \"query\": \"UPDATE products SET price = 899.99 WHERE name = 'Laptop'\"\n}\n```\n\n### delete_data\n\nDeletes data from a table in the MySQL database.\n\nParameters:\n- `query`: The SQL DELETE FROM query to execute.\n\nExample:\n```json\n{\n  \"query\": \"DELETE FROM products WHERE name = 'Smartphone'\"\n}\n```\n\n## Security Considerations\n\n- Use a dedicated MySQL user with appropriate privileges for the MCP server\n- Consider using read-only privileges if you only need to query data\n- Store sensitive information like database credentials securely\n- All operations are logged with unique transaction IDs for auditing\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "database",
        "mysql",
        "databases",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "mjftw--mcp_neo4j_knowledge_graph": {
      "owner": "mjftw",
      "name": "mcp_neo4j_knowledge_graph",
      "url": "https://github.com/mjftw/mcp_neo4j_knowledge_graph",
      "imageUrl": "/freedevtools/mcp/pfp/mjftw.webp",
      "description": "Manage and interact with knowledge graphs using Neo4j. This server provides an stdio-based interface for creating, updating, and searching entities and relationships in a graph database format.",
      "stars": 4,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T00:48:09Z",
      "readme_content": "# Neo4j MCP Server\n\nThis is a Memory Control Protocol (MCP) server implementation that uses Neo4j as the backend storage for knowledge graph management. It provides a stdio-based interface for storing and retrieving knowledge in a graph database format.\n\n## Prerequisites\n\n- Python 3.8+\n- Neo4j Database (local or remote)\n- Poetry (Python package manager)\n- Docker and Docker Compose (for running Neo4j)\n- Go Task (optional, for task automation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd neo4j_mcp_server\n```\n\n2. Install Poetry if you haven't already:\n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n```\n\n3. Install dependencies:\n```bash\npoetry install\n```\n\n## Configuration\n\n### Claude Desktop Configuration\n\nFor Ubuntu users running Claude Desktop, you can configure the MCP server by adding it to your Claude desktop configuration file at:\n```\n~/.config/Claude/claude_desktop_config.json\n```\n\nBefore configuring, you need to build the standalone executable:\n```bash\ntask build\n```\n\nThis will create a binary at `dist/neo4j_mcp_server`. Make sure to update the path in your configuration to point to this built executable.\n\nAn example configuration is provided in `example_mcp_config.json`. You can copy and modify this file:\n\n```bash\ncp example_mcp_config.json ~/.config/Claude/claude_desktop_config.json\n```\n\nThen edit the `command` path in the configuration file to point to your built executable:\n```json\n{\n  \"mcpServers\": [\n    {\n      \"name\": \"neo4j-knowledge-graph\",\n      \"command\": [\"/path/to/your/dist/neo4j_mcp_server\"],\n      ...\n    }\n  ]\n}\n```\n\nThe configuration includes:\n- Server name and description\n- Command to start the server (path to the built executable)\n- Available tools and their parameters\n- Required fields and data types\n\n## Running the Server\n\n### Using Task (Recommended)\n\nIf you have Go Task installed, you can use the provided Taskfile to manage the server:\n\n```bash\n# Show available tasks\ntask\n\n# Start everything (Docker + Server)\ntask run\n\n# Start development environment (Docker + Server + Test)\ntask dev\n\n# Stop all services\ntask down\n```\n\n### Using Docker Compose directly\n\n1. Start the Neo4j container:\n```bash\ndocker-compose up -d\n```\n\n2. Wait for Neo4j to be ready (the container will show as \"healthy\" in `docker ps`)\n\n### Running the MCP Server directly\n\nStart the server with:\n```bash\npoetry run python mcp_neo4j_knowledge_graph/mcp/server.py\n```\n\nThe server will start in stdio mode, ready to accept MCP protocol messages.\n\n## Available Tools\n\n### 1. Create Entities\nCreates new entities in the knowledge graph. Each entity must have a type and properties. The ID will be automatically set from the name property if not explicitly provided.\n\nParameters:\n- `entities`: List of entity objects, each containing:\n  - `type`: String - The type of entity (e.g., Person, Organization)\n  - `properties`: Object - Key-value pairs of entity properties (must include either 'id' or 'name')\n\nExample input:\n```json\n{\n    \"entities\": [{\n        \"type\": \"Person\",\n        \"properties\": {\n            \"name\": \"John Doe\",\n            \"occupation\": \"Developer\",\n            \"age\": 30\n        }\n    }]\n}\n```\n\n### 2. Create Relations\nCreates relationships between existing entities in the knowledge graph. All referenced entities must exist before creating relations.\n\nParameters:\n- `relations`: List of relation objects, each containing:\n  - `type`: String - The type of relation (e.g., KNOWS, WORKS_FOR)\n  - `from`: String - ID of the source entity\n  - `to`: String - ID of the target entity\n\nExample input:\n```json\n{\n    \"relations\": [{\n        \"type\": \"KNOWS\",\n        \"from\": \"john_doe\",\n        \"to\": \"jane_smith\"\n    }]\n}\n```\n\n### 3. Search Entities\nSearches for entities in the knowledge graph with powerful text matching and filtering capabilities. Can be used to search by text, list entities by type, find entities with specific properties, or any combination of these filters.\n\nParameters:\n- `search_term`: String (optional) - Text to search for in entity properties. If not provided, returns entities based on other filters.\n- `entity_type`: String (optional) - Filter results by entity type (e.g., Person, Organization). If provided alone, returns all entities of that type.\n- `properties`: List[String] (optional) - List of property names to filter by:\n  - With search_term: Searches these properties for the term\n  - Without search_term: Returns entities that have any of these properties defined\n- `include_relationships`: Boolean (optional, default: false) - Whether to include connected entities and relationships\n- `fuzzy_match`: Boolean (optional, default: true) - Whether to use case-insensitive partial matching when search_term is provided\n\nExample inputs:\n```json\n// Search by text with type filter\n{\n    \"search_term\": \"John\",\n    \"entity_type\": \"Person\",\n    \"properties\": [\"name\", \"occupation\"],\n    \"include_relationships\": true\n}\n\n// List all entities of a type\n{\n    \"entity_type\": \"Person\"\n}\n\n// Find entities with specific properties\n{\n    \"properties\": [\"email\", \"phone\"],\n    \"entity_type\": \"Contact\"\n}\n\n// Combine filters\n{\n    \"entity_type\": \"Person\",\n    \"properties\": [\"email\"],\n    \"search_term\": \"example.com\",\n    \"fuzzy_match\": true\n}\n\n// Return all entities (no filters)\n{}\n```\n\nReturns:\n```json\n{\n    \"results\": [\n        {\n            \"id\": \"john_doe\",\n            \"type\": [\"Entity\", \"Person\"],\n            \"properties\": {\n                \"name\": \"John Doe\",\n                \"email\": \"john@example.com\"\n            },\n            \"relationships\": [  // Only included if include_relationships is true\n                {\n                    \"type\": \"WORKS_AT\",\n                    \"direction\": \"outgoing\",\n                    \"node\": {\n                        \"id\": \"tech_corp\",\n                        \"type\": \"Company\",\n                        \"properties\": {\n                            \"name\": \"Tech Corp\"\n                        }\n                    }\n                }\n            ]\n        }\n    ]\n}\n```\n\nNotes:\n- When no filters are provided, returns all entities\n- Entity type filtering is exact match (not fuzzy)\n- Property existence check is done with `IS NOT NULL`\n- Text search supports case-insensitive partial matching when fuzzy_match is true\n- Empty results are returned as an empty array, not an error\n- Performance considerations:\n  - Filtering by type is more efficient than text search\n  - Property existence checks are optimized\n  - Consider using specific properties instead of searching all properties\n  - Large result sets may be paginated in future versions\n\n### 4. Update Entities\nUpdates existing entities in the knowledge graph. Supports adding/removing properties and labels.\n\nParameters:\n- `updates`: List of update objects, each containing:\n  - `id`: String (required) - ID of the entity to update\n  - `properties`: Object (optional) - Properties to update or add\n  - `remove_properties`: List[String] (optional) - Property names to remove\n  - `add_labels`: List[String] (optional) - Labels to add to the entity\n  - `remove_labels`: List[String] (optional) - Labels to remove from the entity\n\nExample input:\n```json\n{\n    \"updates\": [{\n        \"id\": \"john_doe\",\n        \"properties\": {\n            \"occupation\": \"Senior Developer\",\n            \"salary\": 100000\n        },\n        \"remove_properties\": [\"temporary_note\"],\n        \"add_labels\": [\"Verified\"],\n        \"remove_labels\": [\"Pending\"]\n    }]\n}\n```\n\n### 5. Delete Entities\nDeletes entities from the knowledge graph with optional cascade deletion of relationships.\n\nParameters:\n- `entity_ids`: List[String] (required) - List of entity IDs to delete\n- `cascade`: Boolean (optional, default: false) - Whether to delete connected relationships\n- `dry_run`: Boolean (optional, default: false) - Preview deletion impact without making changes\n\nExample input:\n```json\n{\n    \"entity_ids\": [\"john_doe\", \"jane_smith\"],\n    \"cascade\": true,\n    \"dry_run\": true\n}\n```\n\nReturns:\n- `success`: Boolean - Whether the operation was successful\n- `deleted_entities`: List of deleted entities\n- `deleted_relationships`: List of deleted relationships\n- `errors`: List of error messages (if any)\n- `impacted_entities`: List of entities that would be affected (dry_run only)\n- `impacted_relationships`: List of relationships that would be affected (dry_run only)\n\n### 6. Introspect Schema\nRetrieves comprehensive information about the Neo4j database schema, including node labels, relationship types, and their properties.\n\nParameters: None required\n\nReturns:\n- `schema`: Object containing:\n  - `node_labels`: List of all node labels in the database\n  - `relationship_types`: List of all relationship types\n  - `node_properties`: Map of label to list of property names\n  - `relationship_properties`: Map of relationship type to list of property names\n\nExample input:\n```json\n{}\n```\n\n## Testing\n\n### Test Scripts\n\nThe project includes several test scripts for different aspects of the system:\n\n1. `mcp_neo4j_knowledge_graph/test_mcp_client.py` - Tests the MCP client functionality\n   - Verifies server startup\n   - Tests tool listing\n   - Tests schema introspection\n   - Tests entity creation\n   ```bash\n   task test-client  # Run just the client test\n   ```\n\n2. `mcp_neo4j_knowledge_graph/test_mcp_config.py` - Tests the MCP configuration\n   - Validates configuration file loading\n   - Tests server connection using the official MCP SDK\n   - Verifies all required tools are available\n   ```bash\n   task test-config  # Run just the config test\n   ```\n\n3. `mcp_neo4j_knowledge_graph/test_neo4j_connection.py` - Tests the Neo4j database connection\n   - Verifies database connectivity\n   - Tests basic query functionality\n   - Checks environment configuration\n   ```bash\n   task test-db  # Run just the database test\n   ```\n\n### Running Tests\n\nYou can run the tests in several ways:\n\n1. Run all tests together:\n   ```bash\n   task test  # Runs all tests including pytest and integration tests\n   ```\n\n2. Run individual test types:\n   ```bash\n   task test-client    # Run MCP client test\n   task test-config    # Run MCP config test\n   task test-db        # Run Neo4j connection test\n   task test-integration  # Run integration tests\n   ```\n\n3. Run tests with pytest directly:\n   ```bash\n   poetry run pytest  # Run all pytest-compatible tests\n   ```\n\n## Development\n\n### Using Task\n\nThe project includes several development tasks:\n\n```bash\n# Format code\ntask format\n\n# Run linter\ntask lint\n\n# Run tests\ntask test\n\n# Start development environment\ntask dev\n```\n\n### Running directly\n\nThis project uses several development tools that are automatically installed with Poetry:\n\n- `black` for code formatting\n- `isort` for import sorting\n- `flake8` for linting\n- `pytest` for testing\n\nYou can run these tools using Poetry:\n\n```bash\n# Format code\npoetry run black .\n\n# Sort imports\npoetry run isort .\n\n# Run linter\npoetry run flake8\n\n# Run tests\npoetry run pytest\n```\n\n## Error Handling\n\nThe server includes comprehensive error handling for:\n- Database connection issues\n- Invalid queries\n- Missing nodes\n- Invalid request formats\n- Schema validation errors\n- Relationship creation failures\n- Entity update conflicts\n\nAll errors are returned with appropriate error messages in the MCP protocol format.\n\n## Docker Configuration\n\nThe Neo4j container is configured with the following settings:\n- Ports: 7474 (HTTP) and 7687 (Bolt)\n- Default credentials: neo4j/password\n- APOC plugin enabled\n- File import/export enabled\n- Health check configured\n\nYou can modify these settings in the `docker-compose.yml` file.\n\n## Task Commands Reference\n\n- `task` - Show available tasks\n- `task run` - Start Docker and MCP server\n- `task dev` - Start development environment (Docker + Server + Test)\n- `task docker` - Start Neo4j database\n- `task server` - Run the MCP server\n- `task test` - Run all tests\n- `task test-client` - Run MCP client tests\n- `task test-config` - Run MCP config tests\n- `task test-db` - Run database tests\n- `task test-integration` - Run integration tests\n- `task down` - Stop all Docker services\n- `task format` - Format code using black and isort\n- `task lint` - Run flake8 linter\n- `task help` - Show detailed help for all tasks ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_neo4j_knowledge_graph",
        "neo4j",
        "databases",
        "mcp_neo4j_knowledge_graph manage",
        "mjftw mcp_neo4j_knowledge_graph",
        "using neo4j"
      ],
      "category": "databases"
    },
    "modelcontextprotocol--server-postgres": {
      "owner": "modelcontextprotocol",
      "name": "server-postgres",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/postgres",
      "imageUrl": "",
      "description": "PostgreSQL database integration with schema inspection and query capabilities",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "secure database",
        "postgresql database",
        "databases secure"
      ],
      "category": "databases"
    },
    "modelcontextprotocol--server-sqlite": {
      "owner": "modelcontextprotocol",
      "name": "server-sqlite",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/sqlite",
      "imageUrl": "",
      "description": "SQLite database operations with built-in analysis features",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "sqlite",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "momentohq--mcp-momento": {
      "owner": "momentohq",
      "name": "mcp-momento",
      "url": "https://github.com/momentohq/mcp-momento",
      "imageUrl": "/freedevtools/mcp/pfp/momentohq.webp",
      "description": "Leverages a caching solution to efficiently store and retrieve data, enabling quick access to frequently used values. Simplifies data management with straightforward commands for setting and getting cache values.",
      "stars": 3,
      "forks": 4,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-08-18T12:40:12Z",
      "readme_content": "# Momento MCP Server\n\nA simple Model Context Protocol (MCP) server implementation for Momento Cache.\n\nAvailable on npmjs as [`@gomomento/mcp-momento`](https://www.npmjs.com/package/@gomomento/mcp-momento)\n\n## Tools\n\n- `get`\n  - Get the cache value stored for the given key.\n  - Inputs:\n    - `key` string -- the key to look up in the cache.\n    - `cacheName` string -- the name cache where the key presides (*optional*)\n  - Returns:\n    - `Hit` with the found value if the key was found.\n    - `Miss` if the key was not found.\n    - `Error` if the request failed.\n- `set`\n  - Sets the value in cache with a given Time To Live (TTL) seconds. If a value for this key is already present, it will be replaced by the new value regardless of the previous value's data type.\n  - Inputs:\n    - `key`: string -- the key to set in the cache\n    - `value`: string -- the value to set for the given key\n    - `ttl`: integer -- the number of seconds to keep this value in the cache (*optional*)\n    - `cacheName`: string -- the name of the cache to store the key in (*optional*)\n  - Returns:\n    - `Success` if the key was successfully written to the cache.\n    - `Error` if the request failed.\n- `list-caches`\n  - Lists the names of all the caches in your Momento account.\n  - Inputs:\n    - (none)\n  - Returns:\n    - `Success` with a comma separated list of cache names\n    - `Error` if the request failed\n- `create-cache`\n  - Creates a new cache in your Momento account\n  - Inputs:\n    - `name`: string - the name of the cache to create\n  - Returns:\n    - `Success` if the cache was successfully created\n    - `Error` if the request failed\n- `delete-cache`\n  - Deletes a cache from your Momento account\n  - Inputs:\n    - `name`: string - the name of the cache to delete\n  - Returns:\n    - `Success` if the cache was successfully deleted\n    - `Error` if the request failed\n\n## Quickstart\n\n1. Get a Momento API key from the [Momento Console](https://console.gomomento.com/). *Note - to run control plane tools (`list-caches`, `create-cache`, `delete-cache`), you must use a **super user API key**.*\n\n2. Set environment variables to configure the cache name and Time To Live (TTL) for items in the cache.\n    ```bash\n    # required\n    export MOMENTO_API_KEY=\"your-api-key\"\n\n    # optional\n    export MOMENTO_CACHE_NAME=\"your-cache-name\"\n    export DEFAULT_TTL_SECONDS=60\n    ```\n  If you do not set these values, it will use `mcp-momento` as the cache name and `60 seconds` for the default time to live.\n\n### Usage with MCP Inspector\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @gomomento/mcp-momento@latest\n```\n\n### Usage with NPX on Claude Desktop\n\nNote: if you're using `nodenv`, replace the plain `npx` with the path to your npx binary (e.g. `/Users/username/.nodenv/shims/npx`).\n\n```json\n{\n  \"mcpServers\": {\n    \"momento\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@gomomento/mcp-momento\"\n      ],\n      \"env\": {\n        \"MOMENTO_API_KEY\": \"your-api-key\",\n        \"MOMENTO_CACHE_NAME\": \"your-cache-name\",\n        \"DEFAULT_TTL_SECONDS\": 60\n      }\n    }\n  }\n}\n```\n\n## Setup for local development\n\n1. Install dependencies:\n    ```bash\n    npm install\n    ```\n\n2. Build the server:\n    ```bash\n    npm run build\n    ```\n\n3. Run with MCP Inspector\n    ```bash\n    export MOMENTO_API_KEY=\"your-api-key\"\n    npx @modelcontextprotocol/inspector node dist/index.js\n    ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "momentohq",
        "momento",
        "databases",
        "access momentohq",
        "mcp momento",
        "momentohq mcp"
      ],
      "category": "databases"
    },
    "mongodb-developer--mongodb-mcp-server": {
      "owner": "mongodb-developer",
      "name": "mongodb-mcp-server",
      "url": "https://github.com/mongodb-developer/mongodb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mongodb-developer.webp",
      "description": "Access MongoDB databases to execute aggregation pipelines and inspect collection schemas for data insights. Provides read-only capabilities to enhance LLM applications with database functionalities.",
      "stars": 25,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:26:33Z",
      "readme_content": "<h2 align=\"center\">\n  📢 <strong>COMMUNITY SERVER NOTICE</strong><br/>\n  This is a community-maintained MCP Server.<br/>\n  👉 For the <strong>official</strong> MongoDB MCP Server, visit  \n  <a href=\"https://github.com/mongodb-js/mongodb-mcp-server\">mongodb-js/mongodb-mcp-server</a>\n</h2>\n\n# MongoDB MCP Server\n\nA Model Context Protocol server that provides read-only access to MongoDB databases. This server enables LLMs to inspect collection schemas and execute aggregation pipelines.\n\n## Components\n\n### Tools\n\n- **aggregate**\n  - Execute MongoDB aggregation pipelines against the connected database\n  - Input:\n    - `collection` (string): The collection to query\n    - `pipeline` (array): MongoDB aggregation pipeline stages\n    - `options` (object): Optional aggregation settings\n      - `allowDiskUse` (boolean): Allow operations that require disk usage\n      - `maxTimeMS` (number): Maximum execution time in milliseconds\n      - `comment` (string): Comment to identify the operation\n  - Default limit of 1000 documents if no limit stage is specified\n  - Default timeout of 30 seconds\n\n- **explain**\n  - Get execution plans for aggregation pipelines\n  - Input:\n    - `collection` (string): The collection to analyze\n    - `pipeline` (array): MongoDB aggregation pipeline stages\n    - `verbosity` (string): Detail level of the explanation\n      - Options: \"queryPlanner\", \"executionStats\", \"allPlansExecution\"\n      - Default: \"queryPlanner\"\n\n### Resources\n\nThe server provides schema information for each collection in the database:\n\n- **Collection Schemas** (`mongodb://<host>/<collection>/schema`)\n  - Inferred JSON schema information for each collection\n  - Includes field names and data types\n  - Schema is derived from sampling collection documents\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n\"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\" ,\n        \"@pash1986/mcp-server-mongodb\"\n      ],\n     \"env\" : {\n\t\"MONGODB_URI\" : \"mongodb+srv://<yourcluster>\" // 'mongodb://localhost:27017'\n\t}\n    }\n```\n\nReplace `mydb` with your database name and adjust the connection string as needed.\n\n## Example Usage\n\n### Basic Aggregation\n\n```javascript\n{\n  \"collection\": \"users\",\n  \"pipeline\": [\n    { \"$match\": { \"age\": { \"$gt\": 21 } } },\n    { \"$group\": {\n      \"_id\": \"$city\",\n      \"avgAge\": { \"$avg\": \"$age\" },\n      \"count\": { \"$sum\": 1 }\n    }},\n    { \"$sort\": { \"count\": -1 } },\n    { \"$limit\": 10 }\n  ],\n  \"options\": {\n    \"allowDiskUse\": true,\n    \"maxTimeMS\": 60000,\n    \"comment\": \"City-wise user statistics\"\n  }\n}\n```\n\n### Query Explanation\n\n```javascript\n{\n  \"collection\": \"users\",\n  \"pipeline\": [\n    { \"$match\": { \"age\": { \"$gt\": 21 } } },\n    { \"$sort\": { \"age\": 1 } }\n  ],\n  \"verbosity\": \"executionStats\"\n}\n```\n\n## Safety Features\n\n- Automatic limit of 1000 documents if no limit is specified in the pipeline\n- Default timeout of 30 seconds for all operations\n- Read-only operations only\n- Safe schema inference from collection samples\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "mongodb databases",
        "developer mongodb",
        "access mongodb"
      ],
      "category": "databases"
    },
    "mongodb-js--mongodb-mcp-server": {
      "owner": "mongodb-js",
      "name": "mongodb-mcp-server",
      "url": "https://github.com/mongodb-js/mongodb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mongodb-js.webp",
      "description": "Connects to MongoDB databases and MongoDB Atlas for admin tasks and data operations using natural language. Supports over 30 tools for cluster management and database querying with safety controls like read-only mode and tool disabling.",
      "stars": 660,
      "forks": 127,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-04T03:15:36Z",
      "readme_content": "[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?logo=data:image/svg%2bxml;base64,PHN2ZyBmaWxsPSIjRkZGRkZGIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciICB2aWV3Qm94PSIwIDAgNDggNDgiIHdpZHRoPSIyNHB4IiBoZWlnaHQ9IjI0cHgiPjxwYXRoIGQ9Ik00NC45OTkgMTAuODd2MjYuMjFjMCAxLjAzLS41OSAxLjk3LTEuNTEgMi40Mi0yLjY4IDEuMjktOCAzLjg1LTguMzUgNC4wMS0uMTMuMDctLjM4LjItLjY3LjMxLjM1LS42LjUzLTEuMy41My0yLjAyVjYuMmMwLS43NS0uMi0xLjQ1LS41Ni0yLjA2LjA5LjA0LjE3LjA4LjI0LjExLjIuMSA1Ljk4IDIuODYgOC44IDQuMkM0NC40MDkgOC45IDQ0Ljk5OSA5Ljg0IDQ0Ljk5OSAxMC44N3pNNy40OTkgMjYuMDNjMS42IDEuNDYgMy40MyAzLjEzIDUuMzQgNC44NmwtNC42IDMuNWMtLjc3LjU3LTEuNzguNS0yLjU2LS4wNS0uNS0uMzYtMS44OS0xLjY1LTEuODktMS42NS0xLjAxLS44MS0xLjA2LTIuMzItLjExLTMuMTlDMy42NzkgMjkuNSA1LjE3OSAyOC4xMyA3LjQ5OSAyNi4wM3pNMzEuOTk5IDYuMnYxMC4xMWwtNy42MyA1LjgtNi44NS01LjIxYzQuOTgtNC41MyAxMC4wMS05LjExIDEyLjY1LTExLjUyQzMwLjg2OSA0Ljc0IDMxLjk5OSA1LjI1IDMxLjk5OSA2LjJ6TTMyIDQxLjc5OFYzMS42OUw4LjI0IDEzLjYxYy0uNzctLjU3LTEuNzgtLjUtMi41Ni4wNS0uNS4zNi0xLjg5IDEuNjUtMS44OSAxLjY1LTEuMDEuODEtMS4wNiAyLjMyLS4xMSAzLjE5IDAgMCAyMC4xNDUgMTguMzM4IDI2LjQ4NSAyNC4xMTZDMzAuODcxIDQzLjI2IDMyIDQyLjc1MyAzMiA0MS43OTh6Ii8+PC9zdmc+)](https://insiders.vscode.dev/redirect/mcp/install?name=mongodb&inputs=%5B%7B%22id%22%3A%22connection_string%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22MongoDB%20connection%20string%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22mongodb-mcp-server%22%2C%22--readOnly%22%5D%2C%22env%22%3A%7B%22MDB_MCP_CONNECTION_STRING%22%3A%22%24%7Binput%3Aconnection_string%7D%22%7D%7D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-Install_Server-1e1e1e?logo=data:image/svg%2bxml;base64,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)](https://cursor.com/install-mcp?name=MongoDB&config=eyJjb21tYW5kIjoibnB4IC15IG1vbmdvZGItbWNwLXNlcnZlciAtLXJlYWRPbmx5In0%3D)\n\n# MongoDB MCP Server\n\nA Model Context Protocol server for interacting with MongoDB Databases and MongoDB Atlas.\n\n## 📚 Table of Contents\n\n- [🚀 Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Setup](#setup)\n    - [Quick Start](#quick-start)\n- [🛠️ Supported Tools](#supported-tools)\n  - [MongoDB Atlas Tools](#mongodb-atlas-tools)\n  - [MongoDB Database Tools](#mongodb-database-tools)\n- [📄 Supported Resources](#supported-resources)\n- [⚙️ Configuration](#configuration)\n  - [Configuration Options](#configuration-options)\n  - [Atlas API Access](#atlas-api-access)\n  - [Configuration Methods](#configuration-methods)\n    - [Environment Variables](#environment-variables)\n    - [Command-Line Arguments](#command-line-arguments)\n    - [MCP Client Configuration](#mcp-configuration-file-examples)\n    - [Proxy Support](#proxy-support)\n- [🤝 Contributing](#contributing)\n\n<a name=\"getting-started\"></a>\n\n## Prerequisites\n\n- Node.js\n  - At least 20.19.0\n  - When using v22 then at least v22.12.0\n  - Otherwise any version 23+\n\n```shell\nnode -v\n```\n\n- A MongoDB connection string or Atlas API credentials, **_the Server will not start unless configured_**.\n  - **_Service Accounts Atlas API credentials_** are required to use the Atlas tools. You can create a service account in MongoDB Atlas and use its credentials for authentication. See [Atlas API Access](#atlas-api-access) for more details.\n  - If you have a MongoDB connection string, you can use it directly to connect to your MongoDB instance.\n\n## Setup\n\n### Quick Start\n\n> **🔒 Security Recommendation 1:** When using Atlas API credentials, be sure to assign only the minimum required permissions to your service account. See [Atlas API Permissions](#atlas-api-permissions) for details.\n\n> **🔒 Security Recommendation 2:** For enhanced security, we strongly recommend using environment variables to pass sensitive configuration such as connection strings and API credentials instead of command line arguments. Command line arguments can be visible in process lists and logged in various system locations, potentially exposing your secrets. Environment variables provide a more secure way to handle sensitive information.\n\nMost MCP clients require a configuration file to be created or modified to add the MCP server.\n\nNote: The configuration file syntax can be different across clients. Please refer to the following links for the latest expected syntax:\n\n- **Windsurf**: https://docs.windsurf.com/windsurf/mcp\n- **VSCode**: https://code.visualstudio.com/docs/copilot/chat/mcp-servers\n- **Claude Desktop**: https://modelcontextprotocol.io/quickstart/user\n- **Cursor**: https://docs.cursor.com/context/model-context-protocol\n\n> **Default Safety Notice:** All examples below include `--readOnly` by default to ensure safe, read-only access to your data. Remove `--readOnly` if you need to enable write operations.\n\n#### Option 1: Connection String\n\nYou can pass your connection string via environment variables, make sure to use a valid username and password.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb://localhost:27017/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nNOTE: The connection string can be configured to connect to any MongoDB cluster, whether it's a local instance or an Atlas cluster.\n\n#### Option 2: Atlas API Credentials\n\nUse your Atlas API Service Accounts credentials. Must follow all the steps in [Atlas API Access](#atlas-api-access) section.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 3: Standalone Service using environment variables and command line arguments\n\nYou can source environment variables defined in a config file or explicitly set them like we do in the example below and run the server via npx.\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the server\nnpx -y mongodb-mcp-server@latest --readOnly\n```\n\n> **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n- For a complete list of configuration options see [Configuration Options](#configuration-options)\n- To configure your Atlas Service Accounts credentials please refer to [Atlas API Access](#atlas-api-access)\n- Connection String via environment variables in the MCP file [example](#connection-string-with-environment-variables)\n- Atlas API credentials via environment variables in the MCP file [example](#atlas-api-credentials-with-environment-variables)\n\n#### Option 4: Using Docker\n\nYou can run the MongoDB MCP Server in a Docker container, which provides isolation and doesn't require a local Node.js installation.\n\n#### Run with Environment Variables\n\nYou may provide either a MongoDB connection string OR Atlas API credentials:\n\n##### Option A: No configuration\n\n```shell\ndocker run --rm -i \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n##### Option B: With MongoDB connection string\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_CONNECTION_STRING \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n> **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Option C: With Atlas API credentials\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_API_CLIENT_ID \\\n  -e MDB_MCP_API_CLIENT_SECRET \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n> **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Docker in MCP Configuration File\n\nWithout options:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-i\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\nWith connection string:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_CONNECTION_STRING\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nWith Atlas API credentials:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_ID\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_SECRET\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 5: Running as an HTTP Server\n\n> **⚠️ Security Notice:** This server now supports Streamable HTTP transport for remote connections. **HTTP transport is NOT recommended for production use without implementing proper authentication and security measures.**\n\n**Suggested Security Measures Examples:**\n\n- Implement authentication (e.g., API gateway, reverse proxy)\n- Use HTTPS/TLS encryption\n- Deploy behind a firewall or in private networks\n- Implement rate limiting\n- Never expose directly to the internet\n\nFor more details, see [MCP Security Best Practices](https://modelcontextprotocol.io/docs/concepts/transports#security-considerations).\n\nYou can run the MongoDB MCP Server as an HTTP server instead of the default stdio transport. This is useful if you want to interact with the server over HTTP, for example from a web client or to expose the server on a specific port.\n\nTo start the server with HTTP transport, use the `--transport http` option:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http\n```\n\nBy default, the server will listen on `http://127.0.0.1:3000`. You can customize the host and port using the `--httpHost` and `--httpPort` options:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http --httpHost=0.0.0.0 --httpPort=8080\n```\n\n- `--httpHost` (default: 127.0.0.1): The host to bind the HTTP server.\n- `--httpPort` (default: 3000): The port number for the HTTP server.\n\n> **Note:** The default transport is `stdio`, which is suitable for integration with most MCP clients. Use `http` transport if you need to interact with the server over HTTP.\n\n## 🛠️ Supported Tools\n\n### Tool List\n\n#### MongoDB Atlas Tools\n\n- `atlas-list-orgs` - Lists MongoDB Atlas organizations\n- `atlas-list-projects` - Lists MongoDB Atlas projects\n- `atlas-create-project` - Creates a new MongoDB Atlas project\n- `atlas-list-clusters` - Lists MongoDB Atlas clusters\n- `atlas-inspect-cluster` - Inspect a specific MongoDB Atlas cluster\n- `atlas-create-free-cluster` - Create a free MongoDB Atlas cluster\n- `atlas-connect-cluster` - Connects to MongoDB Atlas cluster\n- `atlas-inspect-access-list` - Inspect IP/CIDR ranges with access to MongoDB Atlas clusters\n- `atlas-create-access-list` - Configure IP/CIDR access list for MongoDB Atlas clusters\n- `atlas-list-db-users` - List MongoDB Atlas database users\n- `atlas-create-db-user` - Creates a MongoDB Atlas database user\n- `atlas-list-alerts` - List MongoDB Atlas Alerts for a Project\n\nNOTE: atlas tools are only available when you set credentials on [configuration](#configuration) section.\n\n#### MongoDB Database Tools\n\n- `connect` - Connect to a MongoDB instance\n- `find` - Run a find query against a MongoDB collection. The number of documents returned is limited by the `limit` parameter and the server's `maxDocumentsPerQuery` configuration, whichever is smaller. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `aggregate` - Run an aggregation against a MongoDB collection. The number of documents returned is limited by the server's `maxDocumentsPerQuery` configuration. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `count` - Get the number of documents in a MongoDB collection\n- `insert-one` - Insert a single document into a MongoDB collection\n- `insert-many` - Insert multiple documents into a MongoDB collection\n- `create-index` - Create an index for a MongoDB collection\n- `update-one` - Update a single document in a MongoDB collection\n- `update-many` - Update multiple documents in a MongoDB collection\n- `rename-collection` - Rename a MongoDB collection\n- `delete-one` - Delete a single document from a MongoDB collection\n- `delete-many` - Delete multiple documents from a MongoDB collection\n- `drop-collection` - Remove a collection from a MongoDB database\n- `drop-database` - Remove a MongoDB database\n- `list-databases` - List all databases for a MongoDB connection\n- `list-collections` - List all collections for a given database\n- `collection-indexes` - Describe the indexes for a collection\n- `collection-schema` - Describe the schema for a collection\n- `collection-storage-size` - Get the size of a collection in MB\n- `db-stats` - Return statistics about a MongoDB database\n- `export` - Export query or aggregation results to EJSON format. Creates a uniquely named export accessible via the `exported-data` resource.\n\n## 📄 Supported Resources\n\n- `config` - Server configuration, supplied by the user either as environment variables or as startup arguments with sensitive parameters redacted. The resource can be accessed under URI `config://config`.\n- `debug` - Debugging information for MongoDB connectivity issues. Tracks the last connectivity attempt and error information. The resource can be accessed under URI `debug://mongodb`.\n- `exported-data` - A resource template to access the data exported using the export tool. The template can be accessed under URI `exported-data://{exportName}` where `exportName` is the unique name for an export generated by the export tool.\n\n## Configuration\n\n> **🔒 Security Best Practice:** We strongly recommend using environment variables for sensitive configuration such as API credentials (`MDB_MCP_API_CLIENT_ID`, `MDB_MCP_API_CLIENT_SECRET`) and connection strings (`MDB_MCP_CONNECTION_STRING`) instead of command-line arguments. Environment variables are not visible in process lists and provide better security for your sensitive data.\n\nThe MongoDB MCP Server can be configured using multiple methods, with the following precedence (highest to lowest):\n\n1. Command-line arguments\n2. Environment variables\n\n### Configuration Options\n\n| CLI Option                             | Environment Variable                                | Default                                                                     | Description                                                                                                                                                                                             |\n| -------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `apiClientId`                          | `MDB_MCP_API_CLIENT_ID`                             | <not set>                                                                   | Atlas API client ID for authentication. Required for running Atlas tools.                                                                                                                               |\n| `apiClientSecret`                      | `MDB_MCP_API_CLIENT_SECRET`                         | <not set>                                                                   | Atlas API client secret for authentication. Required for running Atlas tools.                                                                                                                           |\n| `connectionString`                     | `MDB_MCP_CONNECTION_STRING`                         | <not set>                                                                   | MongoDB connection string for direct database connections. Optional, if not set, you'll need to call the `connect` tool before interacting with MongoDB data.                                           |\n| `loggers`                              | `MDB_MCP_LOGGERS`                                   | disk,mcp                                                                    | Comma separated values, possible values are `mcp`, `disk` and `stderr`. See [Logger Options](#logger-options) for details.                                                                              |\n| `logPath`                              | `MDB_MCP_LOG_PATH`                                  | see note\\*                                                                  | Folder to store logs.                                                                                                                                                                                   |\n| `disabledTools`                        | `MDB_MCP_DISABLED_TOOLS`                            | <not set>                                                                   | An array of tool names, operation types, and/or categories of tools that will be disabled.                                                                                                              |\n| `confirmationRequiredTools`            | `MDB_MCP_CONFIRMATION_REQUIRED_TOOLS`               | create-access-list,create-db-user,drop-database,drop-collection,delete-many | An array of tool names that require user confirmation before execution. **Requires the client to support [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation)**.       |\n| `readOnly`                             | `MDB_MCP_READ_ONLY`                                 | false                                                                       | When set to true, only allows read, connect, and metadata operation types, disabling create/update/delete operations.                                                                                   |\n| `indexCheck`                           | `MDB_MCP_INDEX_CHECK`                               | false                                                                       | When set to true, enforces that query operations must use an index, rejecting queries that perform a collection scan.                                                                                   |\n| `telemetry`                            | `MDB_MCP_TELEMETRY`                                 | enabled                                                                     | When set to disabled, disables telemetry collection.                                                                                                                                                    |\n| `transport`                            | `MDB_MCP_TRANSPORT`                                 | stdio                                                                       | Either 'stdio' or 'http'.                                                                                                                                                                               |\n| `httpPort`                             | `MDB_MCP_HTTP_PORT`                                 | 3000                                                                        | Port number.                                                                                                                                                                                            |\n| `httpHost`                             | `MDB_MCP_HTTP_HOST`                                 | 127.0.0.1                                                                   | Host to bind the http server.                                                                                                                                                                           |\n| `idleTimeoutMs`                        | `MDB_MCP_IDLE_TIMEOUT_MS`                           | 600000                                                                      | Idle timeout for a client to disconnect (only applies to http transport).                                                                                                                               |\n| `maxBytesPerQuery`                     | `MDB_MCP_MAX_BYTES_PER_QUERY`                       | 16777216 (16MiB)                                                            | The maximum size in bytes for results from a `find` or `aggregate` tool call. This serves as an upper bound for the `responseBytesLimit` parameter in those tools.                                      |\n| `maxDocumentsPerQuery`                 | `MDB_MCP_MAX_DOCUMENTS_PER_QUERY`                   | 100                                                                         | The maximum number of documents that can be returned by a `find` or `aggregate` tool call. For the `find` tool, the effective limit will be the smaller of this value and the tool's `limit` parameter. |\n| `notificationTimeoutMs`                | `MDB_MCP_NOTIFICATION_TIMEOUT_MS`                   | 540000                                                                      | Notification timeout for a client to be aware of diconnect (only applies to http transport).                                                                                                            |\n| `exportsPath`                          | `MDB_MCP_EXPORTS_PATH`                              | see note\\*                                                                  | Folder to store exported data files.                                                                                                                                                                    |\n| `exportTimeoutMs`                      | `MDB_MCP_EXPORT_TIMEOUT_MS`                         | 300000                                                                      | Time in milliseconds after which an export is considered expired and eligible for cleanup.                                                                                                              |\n| `exportCleanupIntervalMs`              | `MDB_MCP_EXPORT_CLEANUP_INTERVAL_MS`                | 120000                                                                      | Time in milliseconds between export cleanup cycles that remove expired export files.                                                                                                                    |\n| `atlasTemporaryDatabaseUserLifetimeMs` | `MDB_MCP_ATLAS_TEMPORARY_DATABASE_USER_LIFETIME_MS` | 14400000                                                                    | Time in milliseconds that temporary database users created when connecting to MongoDB Atlas clusters will remain active before being automatically deleted.                                             |\n\n#### Logger Options\n\nThe `loggers` configuration option controls where logs are sent. You can specify one or more logger types as a comma-separated list. The available options are:\n\n- `mcp`: Sends logs to the MCP client (if supported by the client/transport).\n- `disk`: Writes logs to disk files. Log files are stored in the log path (see `logPath` above).\n- `stderr`: Outputs logs to standard error (stderr), useful for debugging or when running in containers.\n\n**Default:** `disk,mcp` (logs are written to disk and sent to the MCP client).\n\nYou can combine multiple loggers, e.g. `--loggers disk stderr` or `export MDB_MCP_LOGGERS=\"mcp,stderr\"`.\n\n##### Example: Set logger via environment variable\n\n```shell\nexport MDB_MCP_LOGGERS=\"disk,stderr\"\n```\n\n> **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Example: Set logger via command-line argument\n\n```shell\nnpx -y mongodb-mcp-server@latest --loggers mcp stderr\n```\n\n##### Log File Location\n\nWhen using the `disk` logger, log files are stored in:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\.app-logs`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/.app-logs`\n\nYou can override the log directory with the `logPath` option.\n\n#### Disabled Tools\n\nYou can disable specific tools or categories of tools by using the `disabledTools` option. This option accepts an array of strings,\nwhere each string can be a tool name, operation type, or category.\n\nThe way the array is constructed depends on the type of configuration method you use:\n\n- For **environment variable** configuration, use a comma-separated string: `export MDB_MCP_DISABLED_TOOLS=\"create,update,delete,atlas,collectionSchema\"`.\n- For **command-line argument** configuration, use a space-separated string: `--disabledTools create update delete atlas collectionSchema`.\n\nCategories of tools:\n\n- `atlas` - MongoDB Atlas tools, such as list clusters, create cluster, etc.\n- `mongodb` - MongoDB database tools, such as find, aggregate, etc.\n\nOperation types:\n\n- `create` - Tools that create resources, such as create cluster, insert document, etc.\n- `update` - Tools that update resources, such as update document, rename collection, etc.\n- `delete` - Tools that delete resources, such as delete document, drop collection, etc.\n- `read` - Tools that read resources, such as find, aggregate, list clusters, etc.\n- `metadata` - Tools that read metadata, such as list databases, list collections, collection schema, etc.\n- `connect` - Tools that allow you to connect or switch the connection to a MongoDB instance. If this is disabled, you will need to provide a connection string through the config when starting the server.\n\n#### Require Confirmation\n\nIf your client supports [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation), you can set the MongoDB MCP server to request user confirmation before executing certain tools.\n\nWhen a tool is marked as requiring confirmation, the server will send an elicitation request to the client. The client with elicitation support will then prompt the user for confirmation and send the response back to the server. If the client does not support elicitation, the tool will execute without confirmation.\n\nYou can set the `confirmationRequiredTools` configuration option to specify the names of tools which require confirmation. By default, the following tools have this setting enabled: `drop-database`, `drop-collection`, `delete-many`, `atlas-create-db-user`, `atlas-create-access-list`.\n\n#### Read-Only Mode\n\nThe `readOnly` configuration option allows you to restrict the MCP server to only use tools with \"read\", \"connect\", and \"metadata\" operation types. When enabled, all tools that have \"create\", \"update\" or \"delete\" operation types will not be registered with the server.\n\nThis is useful for scenarios where you want to provide access to MongoDB data for analysis without allowing any modifications to the data or infrastructure.\n\nYou can enable read-only mode using:\n\n- **Environment variable**: `export MDB_MCP_READ_ONLY=true`\n- **Command-line argument**: `--readOnly`\n\n> **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen read-only mode is active, you'll see a message in the server logs indicating which tools were prevented from registering due to this restriction.\n\n#### Index Check Mode\n\nThe `indexCheck` configuration option allows you to enforce that query operations must use an index. When enabled, queries that perform a collection scan will be rejected to ensure better performance.\n\nThis is useful for scenarios where you want to ensure that database queries are optimized.\n\nYou can enable index check mode using:\n\n- **Environment variable**: `export MDB_MCP_INDEX_CHECK=true`\n- **Command-line argument**: `--indexCheck`\n\n> **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen index check mode is active, you'll see an error message if a query is rejected due to not using an index.\n\n#### Exports\n\nThe data exported by the `export` tool is temporarily stored in the configured `exportsPath` on the machine running the MCP server until cleaned up by the export cleanup process. If the `exportsPath` configuration is not provided, the following defaults are used:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\exports`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/exports`\n\nThe `exportTimeoutMs` configuration controls the time after which the exported data is considered expired and eligible for cleanup. By default, exports expire after 5 minutes (300000ms).\n\nThe `exportCleanupIntervalMs` configuration controls how frequently the cleanup process runs to remove expired export files. By default, cleanup runs every 2 minutes (120000ms).\n\n#### Telemetry\n\nThe `telemetry` configuration option allows you to disable telemetry collection. When enabled, the MCP server will collect usage data and send it to MongoDB.\n\nYou can disable telemetry using:\n\n- **Environment variable**: `export MDB_MCP_TELEMETRY=disabled`\n- **Command-line argument**: `--telemetry disabled`\n- **DO_NOT_TRACK environment variable**: `export DO_NOT_TRACK=1`\n\n> **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n### Atlas API Access\n\nTo use the Atlas API tools, you'll need to create a service account in MongoDB Atlas:\n\n> **ℹ️ Note:** For a detailed breakdown of the minimum required permissions for each Atlas operation, see the [Atlas API Permissions](#atlas-api-permissions) section below.\n\n1. **Create a Service Account:**\n   - Log in to MongoDB Atlas at [cloud.mongodb.com](https://cloud.mongodb.com)\n   - Navigate to Access Manager > Organization Access\n   - Click Add New > Applications > Service Accounts\n   - Enter name, description and expiration for your service account (e.g., \"MCP, MCP Server Access, 7 days\")\n   - **Assign only the minimum permissions needed for your use case.**\n     - See [Atlas API Permissions](#atlas-api-permissions) for details.\n   - Click \"Create\"\n\nTo learn more about Service Accounts, check the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/api/service-accounts-overview/).\n\n2. **Save Client Credentials:**\n   - After creation, you'll be shown the Client ID and Client Secret\n   - **Important:** Copy and save the Client Secret immediately as it won't be displayed again\n\n3. **Add Access List Entry:**\n   - Add your IP address to the API access list\n\n4. **Configure the MCP Server:**\n   - Use one of the configuration methods below to set your `apiClientId` and `apiClientSecret`\n\n### Atlas API Permissions\n\n> **Security Warning:** Granting the Organization Owner role is rarely necessary and can be a security risk. Assign only the minimum permissions needed for your use case.\n\n#### Quick Reference: Required roles per operation\n\n| What you want to do                  | Safest Role to Assign (where)           |\n| ------------------------------------ | --------------------------------------- |\n| List orgs/projects                   | Org Member or Org Read Only (Org)       |\n| Create new projects                  | Org Project Creator (Org)               |\n| View clusters/databases in a project | Project Read Only (Project)             |\n| Create/manage clusters in a project  | Project Cluster Manager (Project)       |\n| Manage project access lists          | Project IP Access List Admin (Project)  |\n| Manage database users                | Project Database Access Admin (Project) |\n\n- **Prefer project-level roles** for most operations. Assign only to the specific projects you need to manage or view.\n- **Avoid Organization Owner** unless you require full administrative control over all projects and settings in the organization.\n\nFor a full list of roles and their privileges, see the [Atlas User Roles documentation](https://www.mongodb.com/docs/atlas/reference/user-roles/#service-user-roles).\n\n### Configuration Methods\n\n#### Environment Variables\n\nSet environment variables with the prefix `MDB_MCP_` followed by the option name in uppercase with underscores:\n\n**Linux/macOS (bash/zsh):**\n\n```bash\n# Set Atlas API credentials (via Service Accounts)\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\nexport MDB_MCP_LOG_PATH=\"/path/to/logs\"\n```\n\n**Windows Command Prompt (cmd):**\n\n```cmd\nset \"MDB_MCP_API_CLIENT_ID=your-atlas-service-accounts-client-id\"\nset \"MDB_MCP_API_CLIENT_SECRET=your-atlas-service-accounts-client-secret\"\n\nset \"MDB_MCP_CONNECTION_STRING=mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\nset \"MDB_MCP_LOG_PATH=C:\\path\\to\\logs\"\n```\n\n**Windows PowerShell:**\n\n```powershell\n# Set Atlas API credentials (via Service Accounts)\n$env:MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\n$env:MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\n$env:MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\n$env:MDB_MCP_LOG_PATH=\"C:\\path\\to\\logs\"\n```\n\n#### MCP configuration file examples\n\n##### Connection String with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\n##### Atlas API credentials with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Command-Line Arguments\n\nPass configuration options as command-line arguments when starting the server:\n\n> **🔒 Security Note:** For sensitive configuration like API credentials and connection strings, use environment variables instead of command-line arguments.\n\n```shell\n# Set sensitive data as environment variable\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Start the server with command line arguments\nnpx -y mongodb-mcp-server@latest --logPath=/path/to/logs --readOnly --indexCheck\n```\n\n> **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n#### MCP configuration file examples\n\n##### Connection String with command-line arguments\n\n> **🔒 Security Note:** We do not recommend passing connection string as command line argument. Connection string might contain credentials which can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [connection string through environment variables](#connection-string-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--connectionString\",\n        \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n##### Atlas API credentials with command-line arguments\n\n> **🔒 Security Note:** We do not recommend passing Atlas API credentials as command line argument. The provided credentials can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [Atlas API credentials through environment variables](#atlas-api-credentials-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--apiClientId\",\n        \"your-atlas-service-accounts-client-id\",\n        \"--apiClientSecret\",\n        \"your-atlas-service-accounts-client-secret\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n### Proxy Support\n\nThe MCP Server will detect typical PROXY environment variables and use them for\nconnecting to the Atlas API, your MongoDB Cluster, or any other external calls\nto third-party services like OID Providers. The behaviour is the same as what\n`mongosh` does, so the same settings will work in the MCP Server.\n\n## 🤝Contributing\n\nInterested in contributing? Great! Please check our [Contributing Guide](CONTRIBUTING.md) for guidelines on code contributions, standards, adding new tools, and troubleshooting information.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "mongodb databases",
        "databases mongodb",
        "access mongodb"
      ],
      "category": "databases"
    },
    "morningman--mcp-doris": {
      "owner": "morningman",
      "name": "mcp-doris",
      "url": "https://github.com/morningman/mcp-doris",
      "imageUrl": "/freedevtools/mcp/pfp/morningman.webp",
      "description": "Connect to Apache Doris for efficient data management and querying, enabling seamless interactions with data sources and enhanced application functionality.",
      "stars": 15,
      "forks": 4,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-07-28T16:12:14Z",
      "readme_content": "# Apache Doris MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@morningman/mcp-doris)](https://smithery.ai/server/@morningman/mcp-doris)\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) for [Apache Doris](https://doris.apache.org/).\n\n\n\n## Usage\n\n### Cursor\n\n```\nName: doris\nType: command\nCommand: DORIS_HOST=<doris-host> DORIS_PORT=<port> DORIS_USER=<doris-user> DORIS_PASSWORD=<doris-pwd> uv run --with mcp-doris --python 3.13 mcp-doris\n```\n\n## Development\n\n### Prerequest\n\n- install [uv](https://docs.astral.sh/uv)\n\n### Run MCP Inspector\n\n```sql\ncd /path/to/mcp-doris\nuv sync\nsource .venv/bin/activate\nexport PYTHONPATH=/path/to/mcp-doris:$PYTHONPATH\nenv DORIS_HOST=<doris-host> DORIS_PORT=<port> DORIS_USER=<doris-user> DORIS_PASSWORD=<doris-pwd> mcp dev mcp_doris/mcp_server.py\n```\n\nThen visit `http://localhost:5173` in web browser.\n\n## Publish\n\n```\nuv build\nuv publish\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "querying",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "motherduckdb--mcp-server-motherduck": {
      "owner": "motherduckdb",
      "name": "mcp-server-motherduck",
      "url": "https://github.com/motherduckdb/mcp-server-motherduck",
      "imageUrl": "/freedevtools/mcp/pfp/motherduckdb.webp",
      "description": "Connect and query local DuckDB and cloud-based MotherDuck databases. Provides SQL analytics capabilities and integrates with cloud storage for data access and sharing.",
      "stars": 339,
      "forks": 44,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T19:16:19Z",
      "readme_content": "# MotherDuck's DuckDB MCP Server\n\nAn MCP server implementation that interacts with DuckDB and MotherDuck databases, providing SQL analytics capabilities to AI Assistants and IDEs.\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=DuckDB&config=eyJjb21tYW5kIjoidXZ4IG1jcC1zZXJ2ZXItbW90aGVyZHVjayAtLWRiLXBhdGggOm1lbW9yeToiLCJlbnYiOnsibW90aGVyZHVja190b2tlbiI6IiJ9fQ%3D%3D)\n\n## Resources\n- [Close the Loop: Faster Data Pipelines with MCP, DuckDB & AI (Blogpost)](https://motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai/)\n- [Faster Data Pipelines development with MCP and DuckDB (YouTube)](https://www.youtube.com/watch?v=yG1mv8ZRxcU)\n\n## Features\n\n- **Hybrid execution**: query data from local DuckDB or/and cloud-based MotherDuck databases\n- **Cloud storage integration**: access data stored in Amazon S3 or other cloud storage thanks to MotherDuck's integrations\n- **Data sharing**: create and share databases\n- **SQL analytics**: use DuckDB's SQL dialect to query any size of data directly from your AI Assistant or IDE\n- **Serverless architecture**: run analytics without needing to configure instances or clusters\n\n## Components\n\n### Prompts\n\nThe server provides one prompt:\n\n- `duckdb-motherduck-initial-prompt`: A prompt to initialize a connection to DuckDB or MotherDuck and start working with it\n\n### Tools\n\nThe server offers one tool:\n\n- `query`: Execute a SQL query on the DuckDB or MotherDuck database\n  - **Inputs**:\n    - `query` (string, required): The SQL query to execute\n\nAll interactions with both DuckDB and MotherDuck are done through writing SQL queries.\n\n## Command Line Parameters\n\nThe MCP server supports the following parameters:\n\n| Parameter | Type | Default | Description                                                                                                                                                                                                                                                    |\n|-----------|------|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `--transport` | Choice | `stdio` | Transport type. Options: `stdio`, `sse`, `stream`                                                                                                                                                                                                              |\n| `--port` | Integer | `8000` | Port to listen on for sse and stream transport mode                                                                                                                                                                                                            |\n| `--db-path` | String | `md:` | Path to local DuckDB database file, MotherDuck database, or S3 URL (e.g., `s3://bucket/path/to/db.duckdb`)                                                                                                                                                     |\n| `--motherduck-token` | String | `None` | Access token to use for MotherDuck database connections (uses `motherduck_token` env var by default)                                                                                                                                                           |\n| `--read-only` | Flag | `False` | Flag for connecting to DuckDB or MotherDuck in read-only mode. For DuckDB it uses short-lived connections to enable concurrent access                                                                                                                          |\n| `--home-dir` | String | `None` | Home directory for DuckDB (uses `HOME` env var by default)                                                                                                                                                                                                     |\n| `--saas-mode` | Flag | `False` | Flag for connecting to MotherDuck in [SaaS mode](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-saas-mode). (disables filesystem and write permissions for local DuckDB) |\n| `--json-response` | Flag | `False` | Enable JSON responses for HTTP stream. Only supported for `stream` transport                                                                                                                                                                                   |\n\n### Quick Usage Examples\n\n```bash\n# Connect to local DuckDB file in read-only mode with stream transport mode\nuvx mcp-server-motherduck --transport stream --db-path /path/to/local.db --read-only\n\n# Connect to MotherDuck with token with stream transport mode\nuvx mcp-server-motherduck --transport stream --db-path md: --motherduck-token YOUR_TOKEN\n\n# Connect to local DuckDB file in read-only mode with stream transport mode\nuvx mcp-server-motherduck --transport stream --db-path /path/to/local.db --read-only\n\n# Connect to MotherDuck in SaaS mode for enhanced security with stream transport mode\nuvx mcp-server-motherduck --transport stream --db-path md: --motherduck-token YOUR_TOKEN --saas-mode\n```\n\n## Getting Started\n\n### General Prerequisites\n\n- `uv` installed, you can install it using `pip install uv` or `brew install uv`\n\nIf you plan to use the MCP with Claude Desktop or any other MCP comptabile client, the client need to be installed.\n\n### Prerequisites for DuckDB\n\n- No prerequisites. The MCP server can create an in-memory database on-the-fly\n- Or connect to an existing local DuckDB database file , or one stored on remote object storage (e.g., AWS S3).\n\nSee [Connect to local DuckDB](#connect-to-local-duckdb).\n\n### Prerequisites for MotherDuck\n\n- Sign up for a [MotherDuck account](https://app.motherduck.com/?auth_flow=signup)\n- Generate an access token via the [MotherDuck UI](https://app.motherduck.com/settings/tokens?auth_flow=signup)\n- Store the token securely for use in the configuration\n\n### Usage with Cursor\n\n1. Install Cursor from [cursor.com/downloads](https://www.cursor.com/downloads) if you haven't already\n\n2. Open Cursor:\n\n- To set it up globally for the first time, go to Settings->MCP and click on \"+ Add new global MCP server\".\n- This will open a `mcp.json` file to which you add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"md:\",\n        \"--motherduck-token\",\n        \"<YOUR_MOTHERDUCK_TOKEN_HERE>\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\n[![Install with UV in VS Code](https://img.shields.io/badge/VS_Code-Install_with_UV-0098FF?style=plastic)](https://insiders.vscode.dev/redirect/mcp/install?name=mcp-server-motherduck&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-server-motherduck%22%2C%22--db-path%22%2C%22md%3A%22%2C%22--motherduck-token%22%2C%22%24%7Binput%3Amotherduck_token%7D%22%5D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22motherduck_token%22%2C%22description%22%3A%22MotherDuck+Token%22%2C%22password%22%3Atrue%7D%5D) [![Install with UV in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_with_UV-24bfa5?style=plastic&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=mcp-server-motherduck&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-server-motherduck%22%2C%22--db-path%22%2C%22md%3A%22%2C%22--motherduck-token%22%2C%22%24%7Binput%3Amotherduck_token%7D%22%5D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22motherduck_token%22%2C%22description%22%3A%22MotherDuck+Token%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\nFor the quickest installation, click one of the \"Install with UV\" buttons at the top.\n\n#### Manual Installation\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"motherduck_token\",\n        \"description\": \"MotherDuck Token\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"motherduck\": {\n        \"command\": \"uvx\",\n        \"args\": [\n          \"mcp-server-motherduck\",\n          \"--db-path\",\n          \"md:\",\n          \"--motherduck-token\",\n          \"${input:motherduck_token}\"\n        ]\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"motherduck_token\",\n      \"description\": \"MotherDuck Token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"md:\",\n        \"--motherduck-token\",\n        \"${input:motherduck_token}\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with Claude Desktop\n\n1. Install Claude Desktop from [claude.ai/download](https://claude.ai/download) if you haven't already\n\n2. Open the Claude Desktop configuration file:\n\n- To quickly access it or create it the first time, open the Claude Desktop app, select Settings, and click on the \"Developer\" tab, finally click on the \"Edit Config\" button.\n- Add the following configuration to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"md:\",\n        \"--motherduck-token\",\n        \"<YOUR_MOTHERDUCK_TOKEN_HERE>\"\n      ]\n    }\n  }\n}\n```\n\n**Important Notes**:\n\n- Replace `YOUR_MOTHERDUCK_TOKEN_HERE` with your actual MotherDuck token\n\n### Usage with Claude Code\n\nClaude Code supports MCP servers through CLI commands or JSON configuration. Here are two ways to set it up:\n\n#### Option 1: Using CLI Commands\n\nAdd the MotherDuck MCP server directly using the Claude Code CLI:\n\n```bash\nclaude mcp add mcp-server-motherduck uvx mcp-server-motherduck -- --db-path md: --motherduck-token <YOUR_MOTHERDUCK_TOKEN_HERE>\n```\n\n#### Option 2: Using JSON Configuration\n\nAdd the server using a JSON configuration:\n\n```bash\nclaude mcp add-json mcp-server-motherduck '{\n  \"command\": \"uvx\",\n  \"args\": [\n    \"mcp-server-motherduck\",\n    \"--db-path\",\n    \"md:\",\n    \"--motherduck-token\",\n    \"<YOUR_MOTHERDUCK_TOKEN_HERE>\"\n  ]\n}'\n```\n\n**Scoping Options**:\n- Use `--local` (default) for project-specific configuration\n- Use `--project` to share the configuration with your team via `.mcp.json`\n- Use `--user` to make the server available across all your projects\n\n**Important Notes**:\n- Replace `YOUR_MOTHERDUCK_TOKEN_HERE` with your actual MotherDuck token\n- Claude Code also supports environment variable expansion, so you can use `${MOTHERDUCK_TOKEN}` if you've set the environment variable\n\n## Securing your MCP Server when querying MotherDuck\n\nIf the MCP server is exposed to third parties and should only have read access to data, we recommend using a read scaling token and running the MCP server in SaaS mode.\n\n**Read Scaling Tokens** are special access tokens that enable scalable read operations by allowing up to 4 concurrent read replicas, improving performance for multiple end users while *restricting write capabilities*.\nRefer to the [Read Scaling documentation](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/#creating-a-read-scaling-token) to learn how to create a read-scaling token.\n\n**SaaS Mode** in MotherDuck enhances security by restricting it's access to local files, databases, extensions, and configurations, making it ideal for third-party tools that require stricter environment protection. Learn more about it in the [SaaS Mode documentation](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-saas-mode).\n\n**Secure Configuration**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"md:\",\n        \"--motherduck-token\",\n        \"<YOUR_READ_SCALING_TOKEN_HERE>\",\n        \"--saas-mode\"\n      ]\n    }\n  }\n}\n```\n\n## Connect to local DuckDB\n\nTo connect to a local DuckDB, instead of using the MotherDuck token, specify the path to your local DuckDB database file or use `:memory:` for an in-memory database.\n\nIn-memory database:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \":memory:\"\n      ]\n    }\n  }\n}\n```\n\nLocal DuckDB file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"/path/to/your/local.db\"\n      ]\n    }\n  }\n}\n```\n\nLocal DuckDB file in [readonly mode](https://duckdb.org/docs/stable/connect/concurrency.html):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"/path/to/your/local.db\",\n        \"--read-only\"\n      ]\n    }\n  }\n}\n```\n\n**Note**: readonly mode for local file-backed DuckDB connections also makes use of\nshort lived connections. Each time the query MCP tool is used a temporary,\nreaodnly connection is created + query is executed + connection is closed. This\nfeature was motivated by a workflow where [DBT](https://www.getdbt.com) was for\nmodeling data within duckdb and then an MCP client (Windsurf/Cline/Claude/Cursor)\nwas used for exploring the database. The short lived connections allow each tool\nto run and then release their connection, allowing the next tool to connect.\n\n## Connect to DuckDB on S3\n\nYou can connect to DuckDB databases stored on Amazon S3 by providing an S3 URL as the database path. The server will automatically configure the necessary S3 credentials from your environment variables.\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"s3://your-bucket/path/to/database.duckdb\"\n      ],\n      \"env\": {\n        \"AWS_ACCESS_KEY_ID\": \"<your_key>\",\n        \"AWS_SECRET_ACCESS_KEY\": \"<your_secret>\",\n        \"AWS_DEFAULT_REGION\": \"<your_region>\"\n      }\n    }\n  }\n}\n```\n\n\n**Note**: For S3 connections:\n- AWS credentials must be provided via environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and optionally `AWS_DEFAULT_REGION`)\n- The S3 database is attached to an in-memory DuckDB instance\n- The httpfs extension is automatically installed and configured for S3 access\n- Both read and write operations are supported\n\n## Example Queries\n\nOnce configured, you can e.g. ask Claude to run queries like:\n\n- \"Create a new database and table in MotherDuck\"\n- \"Query data from my local CSV file\"\n- \"Join data from my local DuckDB database with a table in MotherDuck\"\n- \"Analyze data stored in Amazon S3\"\n\n## Running in SSE mode\n\nThe server can run in SSE mode in two ways:\n\n### Direct SSE mode\n\nRun the server directly in SSE mode using the `--transport sse` flag:\n\n```bash\nuvx mcp-server-motherduck --transport sse --port 8000 --db-path md: --motherduck-token <your_motherduck_token>\n```\n\nThis will start the server listening on the specified port (default 8000) and you can point your clients directly to this endpoint.\n\n### Using supergateway\n\nAlternatively, you can run SSE mode using `supergateway`:\n\n```bash\nnpx -y supergateway --stdio \"uvx mcp-server-motherduck --db-path md: --motherduck-token <your_motherduck_token>\"\n```\n\nBoth methods allow you to point your clients such as Claude Desktop, Cursor to the SSE endpoint.\n\n## Development configuration\n\nTo run the server from a local development environment, use the following configuration:\n\n```json\n {\n  \"mcpServers\": {\n    \"mcp-server-motherduck\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/your/local/mcp-server-motherduck\",\n        \"run\",\n        \"mcp-server-motherduck\",\n        \"--db-path\",\n        \"md:\",\n        \"--motherduck-token\",\n        \"<YOUR_MOTHERDUCK_TOKEN_HERE>\"\n      ]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n- If you encounter connection issues, verify your MotherDuck token is correct\n- For local file access problems, ensure the `--home-dir` parameter is set correctly\n- Check that the `uvx` command is available in your PATH\n- If you encounter [`spawn uvx ENOENT`](https://github.com/motherduckdb/mcp-server-motherduck/issues/6) errors, try specifying the full path to `uvx` (output of `which uvx`)\n- In version previous for v0.4.0 we used environment variables, now we use parameters\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n##\nmcp-name: io.github.motherduckdb/mcp-server-motherduck\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "motherduckdb",
        "databases",
        "database",
        "motherduck databases",
        "access motherduckdb",
        "motherduckdb mcp"
      ],
      "category": "databases"
    },
    "msathiyakeerthi--mysql_mcp_server": {
      "owner": "msathiyakeerthi",
      "name": "mysql_mcp_server",
      "url": "https://github.com/msathiyakeerthi/mysql_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/msathiyakeerthi.webp",
      "description": "Enable secure and structured interaction with MySQL databases through AI applications, facilitating the listing of tables, reading data, and executing SQL queries in a controlled manner.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-21T06:16:36Z",
      "readme_content": "![Tests](https://github.com/designcomputer/mysql_mcp_server/actions/workflows/test.yml/badge.svg)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/mysql-mcp-server)\n[![smithery badge](https://smithery.ai/badge/mysql-mcp-server)](https://smithery.ai/server/mysql-mcp-server)\n# MySQL MCP Server\nA Model Context Protocol (MCP) implementation that enables secure interaction with MySQL databases. This server component facilitates communication between AI applications (hosts/clients) and MySQL databases, making database exploration and analysis safer and more structured through a controlled interface.\n\n> **Note**: MySQL MCP Server is not designed to be used as a standalone server, but rather as a communication protocol implementation between AI applications and MySQL databases.\n\n## Features\n- List available MySQL tables as resources\n- Read table contents\n- Execute SQL queries with proper error handling\n- Secure database access through environment variables\n- Comprehensive logging\n\n## Installation\n### Manual Installation\n```bash\npip install mysql-mcp-server\n```\n\n### Installing via Smithery\nTo install MySQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mysql-mcp-server):\n```bash\nnpx -y @smithery/cli install mysql-mcp-server --client claude\n```\n\n## Configuration\nSet the following environment variables:\n```bash\nMYSQL_HOST=localhost     # Database host\nMYSQL_PORT=3306         # Optional: Database port (defaults to 3306 if not specified)\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=your_database\n```\n\n## Usage\n### With Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"path/to/mysql_mcp_server\",\n        \"run\",\n        \"mysql_mcp_server\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### With Visual Studio Code\nAdd this to your `mcp.json`:\n```json\n{\n  \"servers\": {\n      \"mysql\": {\n            \"type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"mysql-mcp-server\",\n                \"mysql_mcp_server\"\n            ],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"your_username\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\"\n      }\n  }\n}\n```\nNote: Will need to install uv for this to work\n\n### Debugging with MCP Inspector\nWhile MySQL MCP Server isn't intended to be run standalone or directly from the command line with Python, you can use the MCP Inspector to debug it.\n\nThe MCP Inspector provides a convenient way to test and debug your MCP implementation:\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n# Use the MCP Inspector for debugging (do not run directly with Python)\n```\n\nThe MySQL MCP Server is designed to be integrated with AI applications like Claude Desktop and should not be run directly as a standalone Python program.\n\n## Development\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mysql_mcp_server.git\ncd mysql_mcp_server\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n# Install development dependencies\npip install -r requirements-dev.txt\n# Run tests\npytest\n```\n\n## Security Considerations\n- Never commit environment variables or credentials\n- Use a database user with minimal required permissions\n- Consider implementing query whitelisting for production use\n- Monitor and log all database operations\n\n## Security Best Practices\nThis MCP implementation requires database access to function. For security:\n1. **Create a dedicated MySQL user** with minimal permissions\n2. **Never use root credentials** or administrative accounts\n3. **Restrict database access** to only necessary operations\n4. **Enable logging** for audit purposes\n5. **Regular security reviews** of database access\n\nSee [MySQL Security Configuration Guide](https://github.com/designcomputer/mysql_mcp_server/blob/main/SECURITY.md) for detailed instructions on:\n- Creating a restricted MySQL user\n- Setting appropriate permissions\n- Monitoring database access\n- Security best practices\n\n⚠️ IMPORTANT: Always follow the principle of least privilege when configuring database access.\n\n## License\nMIT License - see LICENSE file for details.\n\n## Contributing\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "nahmanmate--postgresql-mcp-server": {
      "owner": "nahmanmate",
      "name": "postgresql-mcp-server",
      "url": "https://github.com/nahmanmate/postgresql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/nahmanmate.webp",
      "description": "Analyzes PostgreSQL database configurations, assesses performance metrics, and provides security evaluations along with optimization recommendations.",
      "stars": 16,
      "forks": 29,
      "license": "GNU Affero General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-06-15T23:50:15Z",
      "readme_content": "# PostgreSQL MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nahmanmate/postgresql-mcp-server)](https://smithery.ai/server/@nahmanmate/postgresql-mcp-server)\n\nA Model Context Protocol (MCP) server that provides PostgreSQL database management capabilities. This server assists with analyzing existing PostgreSQL setups, providing implementation guidance, and debugging database issues.\n\n<a href=\"https://glama.ai/mcp/servers/bnw58zblt1\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/bnw58zblt1/badge\" alt=\"PostgreSQL Server MCP server\" /></a>\n\n## Features\n\n### 1. Database Analysis (`analyze_database`)\nAnalyzes PostgreSQL database configuration and performance metrics:\n- Configuration analysis\n- Performance metrics\n- Security assessment\n- Recommendations for optimization\n\n```typescript\n// Example usage\n{\n  \"connectionString\": \"postgresql://user:password@localhost:5432/dbname\",\n  \"analysisType\": \"performance\" // Optional: \"configuration\" | \"performance\" | \"security\"\n}\n```\n\n### 2. Setup Instructions (`get_setup_instructions`)\nProvides step-by-step PostgreSQL installation and configuration guidance:\n- Platform-specific installation steps\n- Configuration recommendations\n- Security best practices\n- Post-installation tasks\n\n```typescript\n// Example usage\n{\n  \"platform\": \"linux\", // Required: \"linux\" | \"macos\" | \"windows\"\n  \"version\": \"15\", // Optional: PostgreSQL version\n  \"useCase\": \"production\" // Optional: \"development\" | \"production\"\n}\n```\n\n### 3. Database Debugging (`debug_database`)\nDebug common PostgreSQL issues:\n- Connection problems\n- Performance bottlenecks\n- Lock conflicts\n- Replication status\n\n```typescript\n// Example usage\n{\n  \"connectionString\": \"postgresql://user:password@localhost:5432/dbname\",\n  \"issue\": \"performance\", // Required: \"connection\" | \"performance\" | \"locks\" | \"replication\"\n  \"logLevel\": \"debug\" // Optional: \"info\" | \"debug\" | \"trace\"\n}\n```\n\n## Prerequisites\n\n- Node.js >= 18.0.0\n- PostgreSQL server (for target database operations)\n- Network access to target PostgreSQL instances\n\n## Installation\n\n### Installing via Smithery\n\nTo install PostgreSQL MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nahmanmate/postgresql-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @nahmanmate/postgresql-mcp-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n4. Add to MCP settings file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"postgresql-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/postgresql-mcp-server/build/index.js\"],\n         \"disabled\": false,\n         \"alwaysAllow\": []\n       }\n     }\n   }\n   ```\n\n## Development\n\n- `npm run dev` - Start development server with hot reload\n- `npm run lint` - Run ESLint\n- `npm test` - Run tests\n\n## Security Considerations\n\n1. Connection Security\n   - Uses connection pooling\n   - Implements connection timeouts\n   - Validates connection strings\n   - Supports SSL/TLS connections\n\n2. Query Safety\n   - Validates SQL queries\n   - Prevents dangerous operations\n   - Implements query timeouts\n   - Logs all operations\n\n3. Authentication\n   - Supports multiple authentication methods\n   - Implements role-based access control\n   - Enforces password policies\n   - Manages connection credentials securely\n\n## Best Practices\n\n1. Always use secure connection strings with proper credentials\n2. Follow production security recommendations for sensitive environments\n3. Regularly monitor and analyze database performance\n4. Keep PostgreSQL version up to date\n5. Implement proper backup strategies\n6. Use connection pooling for better resource management\n7. Implement proper error handling and logging\n8. Regular security audits and updates\n\n## Error Handling\n\nThe server implements comprehensive error handling:\n- Connection failures\n- Query timeouts\n- Authentication errors\n- Permission issues\n- Resource constraints\n\n## Running evals and tests\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can see the full documentation [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/index.ts\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nThis project is licensed under the AGPLv3 License - see LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "analyzes postgresql"
      ],
      "category": "databases"
    },
    "nbbaier--mcp-turso": {
      "owner": "nbbaier",
      "name": "mcp-turso",
      "url": "https://github.com/nbbaier/mcp-turso",
      "imageUrl": "/freedevtools/mcp/pfp/nbbaier.webp",
      "description": "Access and manage Turso-hosted LibSQL databases, including retrieving table lists, database schemas, and executing SQL queries. Enhance data interaction capabilities through efficient database management.",
      "stars": 5,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-12T16:49:30Z",
      "readme_content": "# mcp-turso\n\nA Model Context Protocol (MCP) server that provides access to the Turso-hosted LibSQL databases. Currently, the server provides the following functionality:\n\n-  Retrieving a list of tables in a database\n-  Retrieving a database's schema\n-  Retrieving the schema of a table\n-  Performing SELECT queries\n\n## Configuration\n\n### With Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n   \"mcpServers\": [\n      \"turso\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"mcp-turso\"],\n         \"env\": {\n            \"TURSO_DATABASE_URL\": \"your_url\",\n            \"TURSO_AUTH_TOKEN\": \"your_token\"\n         }\n      }\n   ]\n}\n```\n\nYou will need an existing database to continue. If you don't have one, [create one](https://docs.turso.tech/quickstart). To get the database URL via the Turso CLI, run:\n\n```bash\nturso db show --url <database-name>\n```\n\nThen get the database authentication token:\n\n```bash\nturso db tokens create <database-name>\n```\n\nAdd those values to your configuration as shown above.\n\n### With Cursor\n\nTo configure the Turso MCP server with Cursor, add the following to your Cursor settings:\n\n1. Open Cursor and go to Settings (⚙️) > Settings (JSON)\n2. Add the following configuration to your settings JSON:\n\n```json\n\"mcpServers\": {\n  \"turso\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"mcp-turso\"],\n    \"env\": {\n      \"TURSO_DATABASE_URL\": \"your_url\",\n      \"TURSO_AUTH_TOKEN\": \"your_token\"\n    }\n  }\n}\n```\n\nReplace `your_url` and `your_token` with your Turso database URL and authentication token as described in the previous section.\n\n### Logging\n\nThe server includes a custom logger for debugging outside of Claude Desktop. By default, this logger writes to `<parent-dir>/logs/mcp-turso.log`, where `<parent-dir>` is the parent directory of directory containing the `mcp-turso` script. In other words, if the path to `mcp-turso` is `~/foo/bin/mcp-turso`, the logs will be at `~/foo/logs/mcp-turso.log`. If running with NPX as above, the default logs will be:\n\n```\n~/.npm/_npx/<npx-dir-name>/node_modules/mcp-turso/logs/mcp-turso.log\n```\n\nIf you would like to specify a custom path, you can include a `--logs` flag with an **absolute posix path** in the server's configuration:\n\n```json\n{\n   \"mcpServers\": [\n      \"turso\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"mcp-turso\", \"--logs\", \"/Users/<username>/path/to/dir/mcp-logs.log\"],\n         \"env\": {\n            \"TURSO_DATABASE_URL\": \"your_url\",\n            \"TURSO_AUTH_TOKEN\": \"your_token\"\n         }\n      }\n   ]\n}\n```\n\nThe path to the log file (default or custom) is always logged to `stderr` when the server is created. For Claude desktop, this will show up in your server logs in `~/Library/Logs/Claude`.\n\n_Note_: Right now, I haven't implemented specifying a custom logging file for Windows, but this is coming.\n\n## Server Capabilities\n\nThe server provides the following tools:\n\n-  `list_tables`\n   -  Get a list of all the tables in the database\n   -  No input\n   -  Returns: an array of table names\n-  `get_db_schema`\n   -  Get the schemas of all tables in the database\n   -  No input\n   -  Returns: an array of SQL creation statements\n-  `describe_table`\n   -  View schema information for a specific table\n   -  Input:\n      -  `table_name` (string): Name of table to describe\n   -  Returns: Array of column definitions with names and types\n-  `query_database`\n   -  Execute a SELECT query to read data from the database\n   -  Input:\n      -  `sql` (string): The SELECT SQL query to execute\n   -  Returns: Query results as an object of type `{ columns: string[]; rows: Record<string, unknown>[]; rowCount: number; }`\n\n## Todo\n\n-  [ ] Add the ability to specify a custom log file on windows\n-  [ ] Add more query tools\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "libsql",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "nebula-contrib--nebulagraph-mcp-server": {
      "owner": "nebula-contrib",
      "name": "nebulagraph-mcp-server",
      "url": "https://github.com/nebula-contrib/nebulagraph-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/nebula-contrib.webp",
      "description": "Provides access to NebulaGraph 3.x for graph exploration, including schema management, query execution, and shortcut algorithms, compliant with the Model Context Protocol.",
      "stars": 20,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-08T07:14:21Z",
      "readme_content": "# Model Context Protocol Server for NebulaGraph\n\n\nA Model Context Protocol (MCP) server implementation that provides access to [NebulaGraph](https://github.com/vesoft-inc/nebula).\n\n[![PyPI - Version](https://img.shields.io/pypi/v/nebulagraph-mcp-server)](https://pypi.org/project/nebulagraph-mcp-server/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/nebulagraph-mcp-server)](https://pypi.org/project/nebulagraph-mcp-server/)\n[![Lint and Test](https://github.com/PsiACE/nebulagraph-mcp-server/actions/workflows/test.yml/badge.svg)](https://github.com/PsiACE/nebulagraph-mcp-server/actions/workflows/test.yml)\n\n## Features\n\n- Seamless access to NebulaGraph 3.x .\n- Get ready for graph exploration, you know, Schema, Query, and a few shortcut algorithms.\n- Follow Model Context Protocol, ready to integrate with LLM tooling systems.\n- Simple command-line interface with support for configuration via environment variables and .env files.\n\n\n\n## Installation\n\n```shell\npip install nebulagraph-mcp-server\n```\n\n## Usage\n\n`nebulagraph-mcp-server` will load configs from `.env`, for example:\n\n```\nNEBULA_VERSION=v3 # only v3 is supported\nNEBULA_HOST=<your-nebulagraph-server-host>\nNEBULA_PORT=<your-nebulagraph-server-port>\nNEBULA_USER=<your-nebulagraph-server-user>\nNEBULA_PASSWORD=<your-nebulagraph-server-password>\n```\n\n> It requires the value of `NEBULA_VERSION` to be equal to v3 until we are ready for v5.\n\n## Development\n\n```shell\nnpx @modelcontextprotocol/inspector \\\n  uv run nebulagraph-mcp-server\n```\n\n## Credits\n\nThe layout and workflow of this repo is copied from [mcp-server-opendal](https://github.com/Xuanwo/mcp-server-opendal).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nebulagraph",
        "nebula",
        "databases",
        "access nebulagraph",
        "access nebula",
        "nebulagraph graph"
      ],
      "category": "databases"
    },
    "neondatabase--mcp-server-neon": {
      "owner": "neondatabase",
      "name": "mcp-server-neon",
      "url": "https://github.com/neondatabase/mcp-server-neon",
      "imageUrl": "/freedevtools/mcp/pfp/neondatabase.webp",
      "description": "Interact with Neon serverless Postgres databases using natural language for tasks such as database creation and management, running SQL queries, handling migrations, and managing projects.",
      "stars": 476,
      "forks": 78,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:26:10Z",
      "readme_content": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://neon.com/brand/neon-logo-dark-color.svg\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://neon.com/brand/neon-logo-light-color.svg\">\n  <img width=\"250px\" alt=\"Neon Logo fallback\" src=\"https://neon.com/brand/neon-logo-dark-color.svg\">\n</picture>\n\n# Neon MCP Server\n\n[![Install MCP Server in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Neon&config=eyJ1cmwiOiJodHRwczovL21jcC5uZW9uLnRlY2gvbWNwIn0%3D)\n\n**Neon MCP Server** is an open-source tool that lets you interact with your Neon Postgres databases in **natural language**.\n\n[![npm version](https://img.shields.io/npm/v/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![npm downloads](https://img.shields.io/npm/dt/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThe Model Context Protocol (MCP) is a [new, standardized protocol](https://modelcontextprotocol.io/introduction) designed to manage context between large language models (LLMs) and external systems. This repository offers an installer and an MCP Server for [Neon](https://neon.tech).\n\nNeon's MCP server acts as a bridge between natural language requests and the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api). Built upon MCP, it translates your requests into the necessary API calls, enabling you to manage tasks such as creating projects and branches, running queries, and performing database migrations seamlessly.\n\nSome of the key features of the Neon MCP server include:\n\n- **Natural language interaction:** Manage Neon databases using intuitive, conversational commands.\n- **Simplified database management:** Perform complex actions without writing SQL or directly using the Neon API.\n- **Accessibility for non-developers:** Empower users with varying technical backgrounds to interact with Neon databases.\n- **Database migration support:** Leverage Neon's branching capabilities for database schema changes initiated via natural language.\n\nFor example, in Claude Desktop, or any MCP Client, you can use natural language to accomplish things with Neon, such as:\n\n- `Let's create a new Postgres database, and call it \"my-database\". Let's then create a table called users with the following columns: id, name, email, and password.`\n- `I want to run a migration on my project called \"my-project\" that alters the users table to add a new column called \"created_at\".`\n- `Can you give me a summary of all of my Neon projects and what data is in each one?`\n\n> [!WARNING]  \n> **Neon MCP Server Security Considerations**  \n> The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.\n>\n> The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.\n>\n> For more information, see [MCP security guidance →](https://neon.tech/docs/ai/neon-mcp-server#mcp-security-guidance).\n\n## Setting up Neon MCP Server\n\nYou have two options for connecting your MCP client to Neon:\n\n1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys. Additionally, you will automatically receive the latest features and improvements as soon as they are released.\n\n2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.\n\n## Prerequisites\n\n- An MCP Client application.\n- A [Neon account](https://console.neon.tech/signup).\n- **Node.js (>= v18.0.0) and npm:** Download from [nodejs.org](https://nodejs.org).\n\nFor Local MCP Server setup, you also need a Neon API key. See [Neon API Keys documentation](https://neon.tech/docs/manage/api-keys) for instructions on generating one.\n\n### Option 1. Remote Hosted MCP Server (Preview)\n\nConnect to Neon's managed MCP server using OAuth for authentication. This is the easiest setup, requires no local installation of this server, and doesn't need a Neon API key configured in the client.\n\n- Add the following \"Neon\" entry to your client's MCP server configuration file (e.g., `mcp.json`, `mcp_config.json`):\n\n  ```json\n  {\n    \"mcpServers\": {\n      \"Neon\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.neon.tech/mcp\"]\n      }\n    }\n  }\n  ```\n\n- Save the configuration file.\n- Restart or refresh your MCP client.\n- An OAuth window will open in your browser. Follow the prompts to authorize your MCP client to access your Neon account.\n\n> With OAuth base authentication, the MCP server will, by default operate on projects under your personal Neon account. To access or manage projects under organization, you must explicitly provide either the `org_id` or the `project_id` in your prompt to MCP client.\n\nRemote MCP Server also supports authentication using API key in the `Authorization` header if your client supports it\n\n```json\n{\n  \"mcpServers\": {\n    \"Neon\": {\n      \"url\": \"https://mcp.neon.tech/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <$NEON_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n> Provider organization's API key to limit access to projects under the organization only.\n\nMCP supports two remote server transports: the deprecated Server-Sent Events (SSE) and the newer, recommended Streamable HTTP. If your LLM client doesn't support Streamable HTTP yet, you can switch the endpoint from `https://mcp.neon.tech/mcp` to `https://mcp.neon.tech/sse` to use SSE instead.\n\n### Option 2. Local MCP Server\n\nRun the Neon MCP server on your local machine with your Neon API key. This method allows you to manage your Neon projects and databases without relying on a remote MCP server.\n\nAdd the following JSON configuration within the `mcpServers` section of your client's `mcp_config` file, replacing `<YOUR_NEON_API_KEY>` with your actual Neon API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"<YOUR_NEON_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf your client does not use `JSON` for configuration of MCP servers (such as older versions of Cursor), you can use the following command when prompted:\n\n```bash\nnpx -y @neondatabase/mcp-server-neon start <YOUR_NEON_API_KEY>\n```\n\n#### Troubleshooting on Windows\n\nIf you are using Windows and encounter issues while adding the MCP server, you might need to use the Command Prompt (`cmd`) or Windows Subsystem for Linux (`wsl`) to run the necessary commands. Your configuration setup may resemble the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"<YOUR_NEON_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"wsl\",\n      \"args\": [\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"<YOUR_NEON_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\n## Guides\n\n- [Neon MCP Server Guide](https://neon.tech/docs/ai/neon-mcp-server)\n- [Connect MCP Clients to Neon](https://neon.tech/docs/ai/connect-mcp-clients-to-neon)\n- [Cursor with Neon MCP Server](https://neon.tech/guides/cursor-mcp-neon)\n- [Claude Desktop with Neon MCP Server](https://neon.tech/guides/neon-mcp-server)\n- [Cline with Neon MCP Server](https://neon.tech/guides/cline-mcp-neon)\n- [Windsurf with Neon MCP Server](https://neon.tech/guides/windsurf-mcp-neon)\n- [Zed with Neon MCP Server](https://neon.tech/guides/zed-mcp-neon)\n\n# Features\n\n## Supported Tools\n\nThe Neon MCP Server provides the following actions, which are exposed as \"tools\" to MCP Clients. You can use these tools to interact with your Neon projects and databases using natural language commands.\n\n**Project Management:**\n\n- **`list_projects`**: Lists the first 10 Neon projects in your account, providing a summary of each project. If you can't find a specific project, increase the limit by passing a higher value to the `limit` parameter.\n- **`list_shared_projects`**: Lists Neon projects shared with the current user. Supports a search parameter and limiting the number of projects returned (default: 10).\n- **`describe_project`**: Fetches detailed information about a specific Neon project, including its ID, name, and associated branches and databases.\n- **`create_project`**: Creates a new Neon project in your Neon account. A project acts as a container for branches, databases, roles, and computes.\n- **`delete_project`**: Deletes an existing Neon project and all its associated resources.\n\n**Branch Management:**\n\n- **`create_branch`**: Creates a new branch within a specified Neon project. Leverages [Neon's branching](/docs/introduction/branching) feature for development, testing, or migrations.\n- **`delete_branch`**: Deletes an existing branch from a Neon project.\n- **`describe_branch`**: Retrieves details about a specific branch, such as its name, ID, and parent branch.\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, including compute ID, type, size, and autoscaling information.\n- **`list_organizations`**: Lists all organizations that the current user has access to. Optionally filter by organization name or ID using the search parameter.\n- **`reset_from_parent`**: Resets the current branch to its parent's state, discarding local changes. Automatically preserves to backup if branch has children, or optionally preserve on request with a custom name.\n\n**SQL Query Execution:**\n\n- **`get_connection_string`**: Returns your database connection string.\n- **`run_sql`**: Executes a single SQL query against a specified Neon database. Supports both read and write operations.\n- **`run_sql_transaction`**: Executes a series of SQL queries within a single transaction against a Neon database.\n- **`get_database_tables`**: Lists all tables within a specified Neon database.\n- **`describe_table_schema`**: Retrieves the schema definition of a specific table, detailing columns, data types, and constraints.\n- **`list_slow_queries`**: Identifies performance bottlenecks by finding the slowest queries in a database. Requires the pg_stat_statements extension.\n\n**Database Migrations (Schema Changes):**\n\n- **`prepare_database_migration`**: Initiates a database migration process. Critically, it creates a temporary branch to apply and test the migration safely before affecting the main branch.\n- **`complete_database_migration`**: Finalizes and applies a prepared database migration to the main branch. This action merges changes from the temporary migration branch and cleans up temporary resources.\n\n**Query Performance Optimization:**\n\n- **`explain_sql_statement`**: Provides detailed execution plans for SQL queries to help identify performance bottlenecks.\n- **`prepare_query_tuning`**: Analyzes query performance and suggests optimizations like index creation. Creates a temporary branch for safely testing these optimizations.\n- **`complete_query_tuning`**: Applies or discards query optimizations after testing. Can merge changes from the temporary branch to the main branch.\n- **`list_slow_queries`**: Identifies and analyzes slow-performing queries in your database. Requires the `pg_stat_statements` extension.\n\n**Compute Management:**\n\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, showing details like compute ID, type, size, and last active time.\n\n**Neon Auth:**\n\n- **`provision_neon_auth`**: Provisions Neon Auth for a Neon project. It allows developers to easily set up authentication infrastructure by creating an integration with Stack Auth (`@stackframe/stack`).\n\n**Query Performance Tuning:**\n\n- **`explain_sql_statement`**: Analyzes a SQL query and returns detailed execution plan information to help understand query performance.\n- **`prepare_query_tuning`**: Identifies potential performance issues in a SQL query and suggests optimizations. Creates a temporary branch for testing improvements.\n- **`complete_query_tuning`**: Finalizes and applies query optimizations after testing. Merges changes from the temporary tuning branch to the main branch.\n\n## Migrations\n\nMigrations are a way to manage changes to your database schema over time. With the Neon MCP server, LLMs are empowered to do migrations safely with separate \"Start\" (`prepare_database_migration`) and \"Commit\" (`complete_database_migration`) commands.\n\nThe \"Start\" command accepts a migration and runs it in a new temporary branch. Upon returning, this command hints to the LLM that it should test the migration on this branch. The LLM can then run the \"Commit\" command to apply the migration to the original branch.\n\n# Development\n\n## Development with MCP CLI Client\n\nThe easiest way to iterate on the MCP Server is using the `mcp-client/`. Learn more in `mcp-client/README.md`.\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\ncd mcp-client/ && NEON_API_KEY=... npm run start:mcp-server-neon\n```\n\n## Development with Claude Desktop (Local MCP Server)\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\nnode dist/index.js init $NEON_API_KEY\n```\n\nThen, **restart Claude** each time you want to test changes.\n\n# Testing\n\nTo run the tests you need to setup the `.env` file according to the `.env.example` file.\n\n```bash\nnpm run test\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neondatabase",
        "databases",
        "database",
        "access neondatabase",
        "neondatabase mcp",
        "databases secure"
      ],
      "category": "databases"
    },
    "netwrix--mcp-server-naa": {
      "owner": "netwrix",
      "name": "mcp-server-naa",
      "url": "https://github.com/netwrix/mcp-server-naa",
      "imageUrl": "/freedevtools/mcp/pfp/netwrix.webp",
      "description": "Connects to SQL Server databases for efficient access data analysis, enabling execution of SQL queries and dynamic exploration of database schemas. Facilitates identification of sensitive data and management of user permissions within the environment.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-15T22:37:38Z",
      "readme_content": "# Netwrix Access Analyzer MCP Server\n\nAn MCP server for Netwrix Access Analyzer, designed to integrate with Claude Desktop. Currently supports Active Directory and File System solutions. \n\n## Features\n\n- SQL Server integration with automatic connection on startup\n- Dynamic database schema exploration\n- SQL query execution\n- Netwrix Access Analyzer File System tools\n\n## Dependencies\n\nThis MCP server requires the following dependencies:\n\n- Python 3.12 or higher\n- MCP SDK\n- pyodbc 4.0.39 or higher (for SQL Server connectivity)\n- python-dotenv 1.0.0 or higher (for environment variable management)\n- ODBC Driver 17 for SQL Server or later (must be installed on your system)\n\n### Netwrix Access Analyzer (NAA) Dependencies\n\nThis MCP Server requires Netwrix Access Analyzer (NAA) File System or Active Directory scans to be completed.\n\n## Available Tools\n\n| Solution         | Tool Name                       | Description |\n|------------------|---------------------------------|-------------|\n| Active Directory | Get-ADEffectiveMembership       | Discovers effective group membership in AD with filters. |\n| Active Directory | Get-ADExceptions                | Retrieves AD exceptions with optional filters. |\n| Active Directory | Get-ADPermissions               | Retrieves AD permissions from the permissions view with filters. |\n| Active Directory | Get-DomainControllers           | Lists domain controllers. |\n| Active Directory | Get-CertificateVulnerabilities  | Lists certificate vulnerabilities. |\n| Active Directory | Get-ADCARights                  | Lists AD CA rights. |\n| Active Directory | Get-ADSecurityAssessment        | Retrieves AD security assessment results. |\n| Active Directory | Get-ADUsers                     | Retrieves AD user details with filters. |\n| Active Directory | Get-ADGroups                    | Retrieves AD group details with filters. |\n| Active Directory | Get-ADComputers                 | Retrieves AD computer details with filters. |\n| Database         | Connect-Database                | Connects to a specified MSSQL database. |\n| Database         | Show-ConnectionStatus           | Shows the current DB connection status. |\n| Database         | Show-TableSchema                | Shows the schema for a given table. |\n| File System      | Discover-SensitiveData          | Discovers where sensitive data exists (DLP matches). |\n| File System      | Get-OpenShares                  | Finds open shares accessible to broad groups. |\n| File System      | Get-TrusteeAccess               | Finds resources where a trustee has access. |\n| File System      | Get-TrusteePermissionSource     | Finds the source of access for a trustee/resource. |\n| File System      | Get-ResourceAccess              | Gets effective access for a resource path. |\n| File System      | Get-UnusedAccess                | Finds users with unused access to a share. |\n| File System      | Get-RunningJobs                 | Lists running Netwrix Access Auditor jobs. |\n| File System      | Get-ShadowAccess                | Retrieves details about shadow access. |\n\n## Installation Instructions (Claude Desktop)\n\n1. **Install Claude Desktop**\n   - Download and install Claude Desktop from the official website: https://claude.ai/download\n   - Follow the installation prompts for your operating system (macOS, Windows, or Linux).\n\n2. **Clone this repository**\n   ```sh\n   git clone https://github.com/netwrix/mcp-server-naa.git\n   cd mcp-server-naa\n   ```\n\n3. **Connect Claude Desktop to this Server**\n   - Add the following [`uv`](https://docs.astral.sh/uv/getting-started/installation/) configuration to your Claude Desktop MCP Configuration:\n    ```\n    \"NAA_AD\": {\n      \"command\": \"/path/to/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"pyodbc\",\n        \"fastmcp\",\n        \"run\",\n        \"/path/to/mcp-server-naa/run.py\"\n      ],\n      \"env\": {\n        \"DB_SERVER\": \"HOST OR IP\",\n        \"DB_NAME\": \"DATABASENAME\",\n        \"DB_USER\": \"USERNAME\",\n        \"DB_PASSWORD\": \"PASSWORD\",\n        \"DB_USE_WINDOWS_AUTH\": \"FALSE|TRUE\"\n      }\n    }\n    ```\n---\n\n---\n# Troubleshooting\n\n## Connection Issues\n\nIf you encounter connection issues:\n\n1. Verify your SQL Server is running and accessible from your network   \n2. Check your credentials in the .env file\n3. Ensure the ODBC driver is correctly installed\n4. Check the logs for detailed error messages\n\n## Claude Desktop Integration\n\nIf Claude Desktop can't find the uv command:\n\n1. Use the full path to uv in your configuration (use which uv or where uv to find it)\n2. Make sure you've restarted Claude Desktop after configuration changes\n3. Check the Claude logs for any error messages related to the MCP server\n\n## Community\n\nIf you need help using this MCP server or understanding your results, just visit the [Netwrix Community](https://community.netwrix.com/) - we’re here to help!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schemas",
        "database access",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "nickiiitu--MongoDB-Model-Context-Protocol-MCP-": {
      "owner": "nickiiitu",
      "name": "MongoDB-Model-Context-Protocol-MCP-",
      "url": "https://github.com/nickiiitu/MongoDB-Model-Context-Protocol-MCP-",
      "imageUrl": "/freedevtools/mcp/pfp/nickiiitu.webp",
      "description": "Enables interaction with MongoDB databases using natural language commands for querying collections, managing data, and inspecting schemas. Supports a range of document operations including inserting, updating, and deleting records.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-26T08:05:09Z",
      "readme_content": "# 🗄️ MongoDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/mongo-mcp)](https://smithery.ai/server/mongo-mcp)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with MongoDB databases. Query collections, inspect schemas, and manage data seamlessly through natural language.\n\n## ✨ Features\n\n- 🔍 Collection schema inspection\n- 📊 Document querying and filtering\n- 📈 Index management\n- 📝 Document operations (insert, update, delete)\n\n\n\n\n## 🚀 Quick Start\n\nTo get started, find your mongodb connection url and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mongo-mcp\",\n        \"mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Prerequisites\n\n- Node.js 18+\n- npx\n- Docker and Docker Compose (for local sandbox testing only)\n- MCP Client (Claude Desktop App for example)\n\n### Test Sandbox Setup\n\nIf you don't have a mongo db server to connect to and want to create a sample sandbox, follow these steps\n\n1. Start MongoDB using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n2. Seed the database with test data:\n\n```bash\nnpm run seed\n```\n\n### Configure Claude Desktop\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"dist/index.js\",\n        \"mongodb://root:example@localhost:27017/test?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Test Sandbox Data Structure\n\nThe seed script creates three collections with sample data:\n\n#### Users\n\n- Personal info (name, email, age)\n- Nested address with coordinates\n- Arrays of interests\n- Membership dates\n\n#### Products\n\n- Product details (name, SKU, category)\n- Nested specifications\n- Price and inventory info\n- Tags and ratings\n\n#### Orders\n\n- Order details with items\n- User references\n- Shipping and payment info\n- Status tracking\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n### Basic Operations\n\n```plaintext\n\"What collections are available in the database?\"\n\"Show me the schema for the users collection\"\n\"Find all users in San Francisco\"\n```\n\n### Advanced Queries\n\n```plaintext\n\"Find all electronics products that are in stock and cost less than $1000\"\n\"Show me all orders from the user john@example.com\"\n\"List the products with ratings above 4.5\"\n```\n\n### Index Management\n\n```plaintext\n\"What indexes exist on the users collection?\"\n\"Create an index on the products collection for the 'category' field\"\n\"List all indexes across all collections\"\n```\n\n### Document Operations\n\n```plaintext\n\"Insert a new product with name 'Gaming Laptop' in the products collection\"\n\"Update the status of order with ID X to 'shipped'\"\n\"Find and delete all products that are out of stock\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for database interaction:\n\n### Query Tools\n\n- `find`: Query documents with filtering and projection\n- `listCollections`: List available collections\n- `insertOne`: Insert a single document\n- `updateOne`: Update a single document\n- `deleteOne`: Delete a single document\n\n### Index Tools\n\n- `createIndex`: Create a new index\n- `dropIndex`: Remove an index\n- `indexes`: List indexes for a collection\n\n### CertifiedBy:-\n- `MCP Reviews`: https://mcpreview.com/mcp-servers/nickiiitu/MongoDB-Model-Context-Protocol-MCP-\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n# MongoDB-Model-Context-Protocol-MCP-\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "schemas",
        "mongodb databases",
        "interaction mongodb",
        "nickiiitu mongodb"
      ],
      "category": "databases"
    },
    "nsxdavid--surrealdb-mcp-server": {
      "owner": "nsxdavid",
      "name": "surrealdb-mcp-server",
      "url": "https://github.com/nsxdavid/surrealdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/nsxdavid.webp",
      "description": "Enables interaction with SurrealDB databases for querying, manipulating, and managing data through a standardized interface. Facilitates real-time database interactions within AI applications.",
      "stars": 30,
      "forks": 7,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2025-09-23T01:52:57Z",
      "readme_content": "# SurrealDB MCP Server\n\n<p align=\"center\">\n  \n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/surrealdb-mcp-server\"><img src=\"https://img.shields.io/npm/v/surrealdb-mcp-server.svg\" alt=\"npm version\"></a>\n  <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n  <a href=\"https://nodejs.org/\"><img src=\"https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen\" alt=\"Node.js Version\"></a>\n  <a href=\"https://github.com/modelcontextprotocol/typescript-sdk\"><img src=\"https://img.shields.io/badge/MCP%20SDK-%5E1.6.1-orange\" alt=\"MCP SDK\"></a>\n</p>\n\nA Model Context Protocol (MCP) server that provides a standardized interface for AI assistants to interact with a [SurrealDB](https://surrealdb.com/) database. This server enables AI systems to query and manipulate data within a configured SurrealDB instance.\n\n> **Note for AI Assistants:** If you are an AI assistant (like Claude, Cline, Copilot, etc.) reading this documentation, please refer to the [llms-install.md](llms-install.md) file for detailed instructions specifically designed for you to help users install and configure this MCP server.\n\n## Installation Guide\n\n### Which AI assistant are you using?\n\n- **📝 Using Cline VS Code Extension?** → [Go to Cline Installation](#cline-installation)\n- **🖥️ Using Claude Desktop App?** → [Go to Claude Installation](#claude-installation)\n- **🤖 Using GitHub Copilot in VS Code?** → [Go to Copilot Installation](#copilot-installation)\n- **🦘 Using Roo Code in VS Code?** → [Go to Roo Code Installation](#roo-code-installation)\n- **🌊 Using Windsurf?** → [Go to Windsurf Installation](#windsurf-installation)\n- **⚡ Using Cursor?** → [Go to Cursor Installation](#cursor-installation)\n- **🔄 Using [n8n](https://n8n.io/)?** → [Go to n8n Integration](#integration-with-n8n)\n\n## Key Terms\n\n- **MCP Server**: A server that implements the Model Context Protocol, allowing AI assistants to access external tools and resources\n- **MCP Host**: The application (like VS Code with Cline or Claude Desktop) that connects to MCP servers\n- **[SurrealDB](https://surrealdb.com/)**: A scalable, distributed, document-graph database with real-time capabilities\n\n## Available Tools\n\nThe server exposes the following tools for interacting with SurrealDB:\n\n-   `query`: Execute a raw SurrealQL query.\n-   `select`: Select records from a table (all or by specific ID).\n-   `create`: Create a single new record in a table.\n-   `update`: Update a specific record, replacing its content.\n-   `delete`: Delete a specific record by ID.\n-   `merge`: Merge data into a specific record (partial update).\n-   `patch`: Apply JSON Patch operations to a specific record.\n-   `upsert`: Create a record if it doesn't exist, or update it if it does.\n-   `insert`: Insert multiple records into a table.\n-   `insertRelation`: Create a graph relation (edge) between two records.\n\n*(Refer to the MCP host's tool listing for detailed input schemas.)*\n\n## 📝 Cline Installation\n\n### One-Click Installation for Cline VS Code Extension\n\n1. **Install the package globally:**\n\n   ```bash\n   npm install -g surrealdb-mcp-server\n   ```\n\n2. **Add to Cline settings:**\n\n   Edit the file at: `%APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n   \n   Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"surrealdb\": {\n         \"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\",\n         \"args\": [\n           \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\surrealdb-mcp-server\\\\build\\\\index.js\"\n         ],\n         \"env\": {\n           \"SURREALDB_URL\": \"ws://localhost:8000\",\n           \"SURREALDB_NS\": \"your_namespace\",\n           \"SURREALDB_DB\": \"your_database\",\n           \"SURREALDB_USER\": \"your_db_user\",\n           \"SURREALDB_PASS\": \"your_db_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   > **Important:** Replace `YOUR_USERNAME` with your actual Windows username in the path.\n\n3. **Restart VS Code**\n\n4. **Verify Installation:**\n   - Open Cline in VS Code\n   - Ask Cline to \"list available MCP servers\"\n   - You should see \"surrealdb\" in the list\n\n## 🖥️ Claude Installation\n\n### Installation for Claude Desktop App\n\n1. **Configure Claude Desktop to use the server:**\n   \n   Edit the Claude Desktop App's MCP settings file:\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n   Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"surrealdb\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"surrealdb-mcp-server\"\n         ],\n         \"env\": {\n           \"SURREALDB_URL\": \"ws://localhost:8000\",\n           \"SURREALDB_NS\": \"your_namespace\",\n           \"SURREALDB_DB\": \"your_database\",\n           \"SURREALDB_USER\": \"your_db_user\",\n           \"SURREALDB_PASS\": \"your_db_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   > **Note:** Using the `npx` command as shown above means the MCP client will automatically download and run the package from npm when needed. No manual installation is required.\n\n2. **Restart Claude Desktop App**\n\n3. **Verify Installation:**\n   - Ask Claude to \"list available MCP servers\"\n   - You should see \"surrealdb\" in the list\n\n## 🤖 Copilot Installation\n\n### Installation for GitHub Copilot in VS Code\n\n1. **Create a workspace configuration file:**\n   \n   Create a file at: `.vscode/mcp.json` in your workspace\n   \n   Add the following configuration:\n\n   ```json\n   {\n     \"inputs\": [\n       {\n         \"type\": \"promptString\",\n         \"id\": \"surrealdb-url\",\n         \"description\": \"SurrealDB URL\",\n         \"default\": \"ws://localhost:8000\"\n       },\n       {\n         \"type\": \"promptString\",\n         \"id\": \"surrealdb-ns\",\n         \"description\": \"SurrealDB Namespace\"\n       },\n       {\n         \"type\": \"promptString\",\n         \"id\": \"surrealdb-db\",\n         \"description\": \"SurrealDB Database\"\n       },\n       {\n         \"type\": \"promptString\",\n         \"id\": \"surrealdb-user\",\n         \"description\": \"SurrealDB Username\"\n       },\n       {\n         \"type\": \"promptString\",\n         \"id\": \"surrealdb-pass\",\n         \"description\": \"SurrealDB Password\",\n         \"password\": true\n       }\n     ],\n     \"servers\": {\n       \"surrealdb\": {\n         \"type\": \"stdio\",\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"surrealdb-mcp-server\"\n         ],\n         \"env\": {\n           \"SURREALDB_URL\": \"${input:surrealdb-url}\",\n           \"SURREALDB_NS\": \"${input:surrealdb-ns}\",\n           \"SURREALDB_DB\": \"${input:surrealdb-db}\",\n           \"SURREALDB_USER\": \"${input:surrealdb-user}\",\n           \"SURREALDB_PASS\": \"${input:surrealdb-pass}\"\n         }\n       }\n     }\n   }\n   ```\n\n   > **Note:** This configuration uses VS Code's input variables to securely prompt for and store your SurrealDB credentials.\n\n2. **Verify Installation:**\n   - Open GitHub Copilot Chat in VS Code\n   - Select \"Agent\" mode from the dropdown\n   - Click the \"Tools\" button to see available tools\n   - You should see SurrealDB tools in the list\n\n## 🦘 Roo Code Installation\n\n### Installation for Roo Code in VS Code\n\n1. **Access MCP Settings:**\n   \n   Click the MCP icon in the top navigation of the Roo Code pane, then select \"Edit MCP Settings\" to open the configuration file.\n\n2. **Add the SurrealDB MCP Server configuration:**\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"surrealdb\": {\n         \"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\",\n         \"args\": [\n           \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\surrealdb-mcp-server\\\\build\\\\index.js\"\n         ],\n         \"env\": {\n           \"SURREALDB_URL\": \"ws://localhost:8000\",\n           \"SURREALDB_NS\": \"your_namespace\",\n           \"SURREALDB_DB\": \"your_database\",\n           \"SURREALDB_USER\": \"your_db_user\",\n           \"SURREALDB_PASS\": \"your_db_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   > **Important:** Replace `YOUR_USERNAME` with your actual Windows username in the path.\n\n3. **Restart VS Code**\n\n4. **Verify Installation:**\n   - Open Roo Code in VS Code\n   - Click the MCP icon to see available servers\n   - You should see \"surrealdb\" in the list\n\n## 🌊 Windsurf Installation\n\n### Installation for Windsurf\n\n1. **Install the package globally:**\n\n   ```bash\n   npm install -g surrealdb-mcp-server\n   ```\n\n2. **Configure Windsurf:**\n   \n   - Open Windsurf on your system\n   - Navigate to the Settings page\n   - Go to the Cascade tab\n   - Find the Model Context Protocol (MCP) Servers section\n   - Click on \"View raw config\" to open the configuration file (typically at `~/.codeium/windsurf/mcp_config.json`)\n\n3. **Add the SurrealDB MCP Server configuration:**\n\n   ```json\n   {\n     \"servers\": [\n       {\n         \"name\": \"surrealdb\",\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/global/node_modules/surrealdb-mcp-server/build/index.js\"\n         ],\n         \"env\": {\n           \"SURREALDB_URL\": \"ws://localhost:8000\",\n           \"SURREALDB_NS\": \"your_namespace\",\n           \"SURREALDB_DB\": \"your_database\",\n           \"SURREALDB_USER\": \"your_db_user\",\n           \"SURREALDB_PASS\": \"your_db_password\"\n         }\n       }\n     ]\n   }\n   ```\n\n   > **Note:** Replace `/path/to/global/node_modules` with the actual path to your global node_modules directory.\n\n4. **Restart Windsurf**\n\n5. **Verify Installation:**\n   - Open Cascade in Windsurf\n   - You should see SurrealDB tools available in the tools list\n\n## ⚡ Cursor Installation\n\n### Installation for Cursor\n\n1. **Install the package globally:**\n\n   ```bash\n   npm install -g surrealdb-mcp-server\n   ```\n\n2. **Configure Cursor:**\n   \n   - Open Cursor\n   - Go to Settings > Cursor Settings\n   - Find the MCP Servers option and enable it\n   - Click on \"Add New MCP Server\"\n\n3. **Add the SurrealDB MCP Server configuration:**\n\n   ```json\n   {\n     \"name\": \"surrealdb\",\n     \"command\": \"node\",\n     \"args\": [\n       \"/path/to/global/node_modules/surrealdb-mcp-server/build/index.js\"\n     ],\n     \"env\": {\n       \"SURREALDB_URL\": \"ws://localhost:8000\",\n       \"SURREALDB_NS\": \"your_namespace\",\n       \"SURREALDB_DB\": \"your_database\",\n       \"SURREALDB_USER\": \"your_db_user\",\n       \"SURREALDB_PASS\": \"your_db_password\"\n     }\n   }\n   ```\n\n   > **Note:** Replace `/path/to/global/node_modules` with the actual path to your global node_modules directory.\n\n4. **Restart Cursor**\n\n5. **Verify Installation:**\n   - Open Cursor Chat\n   - You should see SurrealDB tools available in the tools list\n\n## Required Environment Variables\n\nThis server requires the following environment variables to connect to your SurrealDB instance:\n\n-   `SURREALDB_URL`: The WebSocket endpoint of your SurrealDB instance (e.g., `ws://localhost:8000` or `wss://cloud.surrealdb.com`).\n-   `SURREALDB_NS`: The target Namespace.\n-   `SURREALDB_DB`: The target Database.\n-   `SURREALDB_USER`: The username for authentication (Root, NS, DB, or Scope user).\n-   `SURREALDB_PASS`: The password for the specified user.\n\n## Troubleshooting\n\n### Common Issues\n\n#### \"Cannot find module\" Error\n\nIf you see an error like \"Cannot find module 'surrealdb-mcp-server'\", try:\n\n1. Verify the global installation: `npm list -g surrealdb-mcp-server`\n2. Check the path in your configuration matches the actual installation path\n3. Try reinstalling: `npm install -g surrealdb-mcp-server`\n\n#### Connection Errors\n\nIf you see \"Failed to connect to SurrealDB\":\n\n1. Verify SurrealDB is running: `surreal start --log debug`\n2. Check your connection URL, namespace, database, and credentials\n3. Ensure your SurrealDB instance is accessible from the path specified\n\n#### Cline-Specific Issues\n\nIf the npx approach doesn't work with Cline:\n\n1. Always use the global installation method for Cline\n2. Specify the full path to node.exe and the installed package\n3. Make sure to replace YOUR_USERNAME with your actual Windows username\n\n## Advanced Configuration\n\n### Using a Local Build\n\nIf you've cloned the repository or want to use a local build, you can use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"surrealdb\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/your/surrealdb-mcp-server/build/index.js\"],\n      \"env\": {\n        \"SURREALDB_URL\": \"ws://localhost:8000\",\n        \"SURREALDB_NS\": \"your_namespace\",\n        \"SURREALDB_DB\": \"your_database\",\n        \"SURREALDB_USER\": \"your_db_user\",\n        \"SURREALDB_PASS\": \"your_db_password\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n- Replace `/path/to/your/surrealdb-mcp-server` with the actual path where you cloned the repository\n- Replace the environment variable values with your actual SurrealDB connection details\n\n## Development\n\nIf you want to contribute to the development of this MCP server, follow these steps:\n\n### Local Development Setup\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/nsxdavid/surrealdb-mcp-server.git\n   cd surrealdb-mcp-server\n   ```\n\n2. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n\n3. **Build the project:**\n   ```bash\n   npm run build\n   ```\n\n### Running Locally\n\n```bash\n# Ensure required SURREALDB_* environment variables are set\nnpm run dev # (Note: dev script uses ts-node to run TypeScript directly)\n# Or run the built version:\nnpm start\n```\n\n### Testing\n\n```bash\nnpm test # (Note: Tests need to be implemented)\n```\n\n### Contributing\n\nContributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## Integration with n8n\n\nYou can integrate this SurrealDB MCP Server with [n8n](https://n8n.io/) using the [n8n-nodes-mcp](https://github.com/nerding-io/n8n-nodes-mcp) community node.\n\n**NOTE: Currently only the [self-hosted (Docker) version of n8n](https://docs.n8n.io/hosting/installation/docker/) supports community nodes. There is no option for MCP Servers in the n8n cloud version (yet?).**\n\n### Installation\n\n1. **Install the n8n-nodes-mcp package:**\n\n   ```bash\n   npm install n8n-nodes-mcp\n   ```\n\n2. **Configure n8n to use the custom node:**\n\n   Add the following to your n8n configuration:\n\n   ```bash\n   N8N_CUSTOM_EXTENSIONS=\"n8n-nodes-mcp\"\n   ```\n\n3. **Configure the MCP node in n8n:**\n   \n   - Add the \"MCP\" node to your workflow\n   - Configure it to connect to your SurrealDB MCP Server\n   - Select the desired operation (query, select, create, etc.)\n   - Configure the operation parameters\n\nFor more details, visit the [n8n-nodes-mcp GitHub repository](https://github.com/nerding-io/n8n-nodes-mcp).\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "surrealdb",
        "database",
        "surrealdb databases",
        "nsxdavid surrealdb",
        "secure database"
      ],
      "category": "databases"
    },
    "openlink--mcp-server-jdbc": {
      "owner": "openlink",
      "name": "mcp-server-jdbc",
      "url": "https://github.com/OpenLinkSoftware/mcp-jdbc-server",
      "imageUrl": "",
      "description": "An MCP server for generic Database Management System (DBMS) Connectivity via the Java Database Connectivity (JDBC) protocol",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbc",
        "databases",
        "dbms",
        "jdbc protocol",
        "jdbc mcp",
        "connectivity jdbc"
      ],
      "category": "databases"
    },
    "openlink--mcp-server-odbc": {
      "owner": "openlink",
      "name": "mcp-server-odbc",
      "url": "https://github.com/OpenLinkSoftware/mcp-odbc-server",
      "imageUrl": "",
      "description": "An MCP server for generic Database Management System (DBMS) Connectivity via the Open Database Connectivity (ODBC) protocol",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "odbc",
        "databases",
        "dbms",
        "odbc protocol",
        "server odbc",
        "connectivity odbc"
      ],
      "category": "databases"
    },
    "openlink--mcp-server-sqlalchemy": {
      "owner": "openlink",
      "name": "mcp-server-sqlalchemy",
      "url": "https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server",
      "imageUrl": "",
      "description": "An MCP server for generic Database Management System (DBMS) Connectivity via SQLAlchemy using Python ODBC (pyodbc)",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlalchemy",
        "database",
        "database access",
        "secure database",
        "sqlalchemy mcp"
      ],
      "category": "databases"
    },
    "oshion--python-mcp-carsearch-demo": {
      "owner": "oshion",
      "name": "python-mcp-carsearch-demo",
      "url": "https://github.com/oshion/python-mcp-carsearch-demo",
      "imageUrl": "/freedevtools/mcp/pfp/oshion.webp",
      "description": "Access a comprehensive car database to search for cars based on specific criteria, retrieve detailed information about individual vehicles, and receive tailored recommendations based on user preferences.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-06T14:22:15Z",
      "readme_content": "# Car Database MCP Server\n\n이 프로젝트는 자동차 데이터베이스를 MCP(Model Context Protocol) 서버로 노출하는 애플리케이션입니다. LLM(Large Language Model)이 자동차 데이터베이스에 접근하여 검색, 조회 및 추천 기능을 사용할 수 있게 합니다.\n\n## 데모\nhttps://nest-resource-bucket.s3.ap-northeast-2.amazonaws.com/MCP_local_demo.mp4\n\n## 기능\n\n- 조건에 맞는 자동차 검색\n- 특정 자동차의 상세 정보 조회\n- 사용 가능한 검색 파라미터 조회\n- 브랜드별 모델 목록 조회\n- 사용자 선호도에 따른 검색 파라미터 추천\n\n## 설치\n\n1. 필요한 의존성 설치:\n\n```bash\npip install -r requirements.txt",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "carsearch",
        "database",
        "car database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "pab1it0--adx-mcp-server": {
      "owner": "pab1it0",
      "name": "adx-mcp-server",
      "url": "https://github.com/pab1it0/adx-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/pab1it0.webp",
      "description": "Access Azure Data Explorer clusters and databases through standardized MCP interfaces to execute KQL queries and explore data resources. Features include querying capabilities, resource discovery, and support for authentication.",
      "stars": 48,
      "forks": 20,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-10T14:51:26Z",
      "readme_content": "# Azure Data Explorer MCP Server\n\n<a href=\"https://glama.ai/mcp/servers/1yysyd147h\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/1yysyd147h/badge\" />\n</a>\n\nA [Model Context Protocol][mcp] (MCP) server for Azure Data Explorer/Eventhouse in Microsoft Fabric.\n\nThis provides access to your Azure Data Explorer/Eventhouse clusters and databases through standardized MCP interfaces, allowing AI assistants to execute KQL queries and explore your data.\n\n[mcp]: https://modelcontextprotocol.io\n\n## Features\n\n- [x] Execute KQL queries against Azure Data Explorer\n- [x] Discover and explore database resources\n  - [x] List tables in the configured database\n  - [x] View table schemas\n  - [x] Sample data from tables\n  - [x] Get table statistics/details\n\n- [x] Authentication support\n  - [x] Token credential support (Azure CLI, MSI, etc.)\n  - [x] Workload Identity credential support for AKS\n- [x] Docker containerization support\n\n- [x] Provide interactive tools for AI assistants\n\nThe list of tools is configurable, so you can choose which tools you want to make available to the MCP client.\nThis is useful if you don't use certain functionality or if you don't want to take up too much of the context window.\n\n## Usage\n\n1. Login to your Azure account which has the permission to the ADX cluster using Azure CLI.\n\n2. Configure the environment variables for your ADX cluster, either through a `.env` file or system environment variables:\n\n```env\n# Required: Azure Data Explorer configuration\nADX_CLUSTER_URL=https://yourcluster.region.kusto.windows.net\nADX_DATABASE=your_database\n\n# Optional: Azure Workload Identity credentials \n# AZURE_TENANT_ID=your-tenant-id\n# AZURE_CLIENT_ID=your-client-id \n# ADX_TOKEN_FILE_PATH=/var/run/secrets/azure/tokens/azure-identity-token\n\n# Optional: Custom MCP Server configuration\nADX_MCP_SERVER_TRANSPORT=stdio # Choose between http/sse/stdio, default = stdio\n\n# Optional: Only relevant for non-stdio transports\nADX_MCP_BIND_HOST=127.0.0.1 # default = 127.0.0.1\nADX_MCP_BIND_PORT=8080 # default = 8080\n```\n\n#### Azure Workload Identity Support\n\nThe server now uses WorkloadIdentityCredential by default when running in Azure Kubernetes Service (AKS) environments with workload identity configured. It prioritizes the use of WorkloadIdentityCredential whenever the necessary environment variables are present.\n\nFor AKS with Azure Workload Identity, you only need to:\n1. Make sure the pod has `AZURE_TENANT_ID` and `AZURE_CLIENT_ID` environment variables set\n2. Ensure the token file is mounted at the default path or specify a custom path with `ADX_TOKEN_FILE_PATH`\n\nIf these environment variables are not present, the server will automatically fall back to DefaultAzureCredential, which tries multiple authentication methods in sequence.\n\n3. Add the server configuration to your client configuration file. For example, for Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"adx\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<full path to adx-mcp-server directory>\",\n        \"run\",\n        \"src/adx_mcp_server/main.py\"\n      ],\n      \"env\": {\n        \"ADX_CLUSTER_URL\": \"https://yourcluster.region.kusto.windows.net\",\n        \"ADX_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n> Note: if you see `Error: spawn uv ENOENT` in Claude Desktop, you may need to specify the full path to `uv` or set the environment variable `NO_UV=1` in the configuration.\n\n## Docker Usage\n\nThis project includes Docker support for easy deployment and isolation.\n\n### Building the Docker Image\n\nBuild the Docker image using:\n\n```bash\ndocker build -t adx-mcp-server .\n```\n\n### Running with Docker\n\nYou can run the server using Docker in several ways:\n\n#### Using docker run directly:\n\n```bash\ndocker run -it --rm \\\n  -e ADX_CLUSTER_URL=https://yourcluster.region.kusto.windows.net \\\n  -e ADX_DATABASE=your_database \\\n  -e AZURE_TENANT_ID=your_tenant_id \\\n  -e AZURE_CLIENT_ID=your_client_id \\\n  adx-mcp-server\n```\n\n#### Using docker-compose:\n\nCreate a `.env` file with your Azure Data Explorer credentials and then run:\n\n```bash\ndocker-compose up\n```\n\n### Running with Docker in Claude Desktop\n\nTo use the containerized server with Claude Desktop, update the configuration to use Docker with the environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"adx\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"ADX_CLUSTER_URL\",\n        \"-e\", \"ADX_DATABASE\",\n        \"-e\", \"AZURE_TENANT_ID\",\n        \"-e\", \"AZURE_CLIENT_ID\",\n        \"-e\", \"ADX_TOKEN_FILE_PATH\",\n        \"adx-mcp-server\"\n      ],\n      \"env\": {\n        \"ADX_CLUSTER_URL\": \"https://yourcluster.region.kusto.windows.net\",\n        \"ADX_DATABASE\": \"your_database\",\n        \"AZURE_TENANT_ID\": \"your_tenant_id\",\n        \"AZURE_CLIENT_ID\": \"your_client_id\",\n        \"ADX_TOKEN_FILE_PATH\": \"/var/run/secrets/azure/tokens/azure-identity-token\"\n      }\n    }\n  }\n}\n```\n\nThis configuration passes the environment variables from Claude Desktop to the Docker container by using the `-e` flag with just the variable name, and providing the actual values in the `env` object.\n\n#### Using Docker with HTTP Transport\n\nFor HTTP mode deployment, you can use the following Docker configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"adx\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-p\", \"8080:8080\",\n        \"-e\", \"ADX_CLUSTER_URL\",\n        \"-e\", \"ADX_DATABASE\", \n        \"-e\", \"ADX_MCP_SERVER_TRANSPORT\",\n        \"-e\", \"ADX_MCP_BIND_HOST\",\n        \"-e\", \"ADX_MCP_BIND_PORT\",\n        \"adx-mcp-server\"\n      ],\n      \"env\": {\n        \"ADX_CLUSTER_URL\": \"https://yourcluster.region.kusto.windows.net\",\n        \"ADX_DATABASE\": \"your_database\",\n        \"ADX_MCP_SERVER_TRANSPORT\": \"http\",\n        \"ADX_MCP_BIND_HOST\": \"0.0.0.0\",\n        \"ADX_MCP_BIND_PORT\": \"8080\"\n      }\n    }\n  }\n}\n```\n\n## Using as a Dev Container / GitHub Codespace\n\nThis repository can also be used as a development container for a seamless development experience. The dev container setup is located in the `devcontainer-feature/adx-mcp-server` folder.\n\nFor more details, check the [devcontainer README](devcontainer-feature/adx-mcp-server/README.md).\n\n\n\n## Development\n\nContributions are welcome! Please open an issue or submit a pull request if you have any suggestions or improvements.\n\nThis project uses [`uv`](https://github.com/astral-sh/uv) to manage dependencies. Install `uv` following the instructions for your platform:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nYou can then create a virtual environment and install the dependencies with:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n.venv\\Scripts\\activate     # On Windows\nuv pip install -e .\n```\n\n## Project Structure\n\nThe project has been organized with a `src` directory structure:\n\n```\nadx-mcp-server/\n├── src/\n│   └── adx_mcp_server/\n│       ├── __init__.py      # Package initialization\n│       ├── server.py        # MCP server implementation\n│       ├── main.py          # Main application logic\n├── Dockerfile               # Docker configuration\n├── docker-compose.yml       # Docker Compose configuration\n├── .dockerignore            # Docker ignore file\n├── pyproject.toml           # Project configuration\n└── README.md                # This file\n```\n\n### Testing\n\nThe project includes a comprehensive test suite that ensures functionality and helps prevent regressions.\n\nRun the tests with pytest:\n\n```bash\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run the tests\npytest\n\n# Run with coverage report\npytest --cov=src --cov-report=term-missing\n```\nTests are organized into:\n\n- Configuration validation tests\n- Server functionality tests\n- Error handling tests\n- Main application tests\n\nWhen adding new features, please also add corresponding tests.\n\n### Tools\n\n| Tool | Category | Description |\n| --- | --- | --- |\n| `execute_query` | Query | Execute a KQL query against Azure Data Explorer |\n| `list_tables` | Discovery | List all tables in the configured database |\n| `get_table_schema` | Discovery | Get the schema for a specific table |\n| `sample_table_data` | Discovery | Get sample data from a table with optional sample size |\n\n\n## License\n\nMIT\n\n---\n\n[mcp]: https://modelcontextprotocol.io\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "queries",
        "enables querying",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "peterdonaghey--snowflake-mcp": {
      "owner": "peterdonaghey",
      "name": "snowflake-mcp",
      "url": "https://github.com/peterdonaghey/snowflake-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/peterdonaghey.webp",
      "description": "Execute SQL queries on Snowflake databases using natural language through seamless integration with AI assistants. Handle query results and manage database connections securely and efficiently.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-07T10:06:09Z",
      "readme_content": "# Snowflake MCP (Model Context Protocol) Server\n\nA Model Context Protocol (MCP) server implementation that allows AI assistants like Claude to interact with Snowflake databases through natural language queries.\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## Features\n\n- Execute SQL queries on Snowflake databases via natural language\n- Automatic database connection lifecycle management (connect, reconnect, close)\n- Integration with Claude, Cursor IDE, and other MCP-compatible clients\n- Convert natural language to SQL using semantic understanding\n- Handle query results and format them for easy reading\n- Secure database operations with proper authentication\n\n## Prerequisites\n\n- Python 3.8+\n- Snowflake account with appropriate access permissions\n- MCP-compatible client (Claude, Cursor IDE, etc.)\n\n## Installation\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/yourusername/snowflake-mcp.git\ncd snowflake-mcp\n```\n\n2. Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n3. Copy the sample environment file and add your Snowflake credentials:\n\n```bash\ncp .env.sample .env\n# Edit .env with your Snowflake credentials\n```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file with your Snowflake credentials:\n\n```\nSNOWFLAKE_USER=your_username\nSNOWFLAKE_PASSWORD=your_password\nSNOWFLAKE_ACCOUNT=your_account_locator  # e.g., xy12345.us-east-2\nSNOWFLAKE_DATABASE=your_database\nSNOWFLAKE_WAREHOUSE=your_warehouse\nSNOWFLAKE_SCHEMA=your_schema  # Optional, defaults to PUBLIC\nSNOWFLAKE_ROLE=your_role  # Optional, defaults to user's default role\n```\n\n### MCP Client Configuration\n\n#### For Cursor IDE:\n\nCursor automatically discovers and integrates with MCP servers. Just make sure the server is running.\n\n#### For Claude Desktop:\n\nAdd the following to your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"snowflake\": {\n      \"command\": \"/path/to/python\",\n      \"args\": [\"/path/to/snowflake-mcp/src/server.py\"]\n    }\n  }\n}\n```\n\nReplace `/path/to/python` with the path to your Python interpreter, and `/path/to/snowflake-mcp` with the full path to where you cloned this repository.\n\n## Usage\n\nThe Snowflake MCP server implements the Model Context Protocol (MCP) specification, allowing AI systems like Claude to connect to Snowflake databases through natural language.\n\n### Using MCP CLI (Recommended)\n\nThe MCP Python SDK includes a command-line interface that makes it easy to run and manage MCP servers:\n\n1. **Development Mode with Inspector UI**:\n\n```bash\nmcp dev src/server.py\n```\n\nThis starts the server and opens an inspector interface in your browser where you can test the tools.\n\n2. **Install in Claude Desktop**:\n\n```bash\nmcp install src/server.py\n```\n\nThis installs the server in Claude Desktop, making it available for Claude to use.\n\n3. **Standard Mode**:\n\n```bash\nmcp run src/server.py\n```\n\nThis runs the server in standard mode without the inspector interface.\n\n### Available Tools\n\nThe Snowflake MCP server provides the following tools:\n\n1. **query_database** - Execute SQL or natural language queries on Snowflake\n2. **list_tables** - List all tables in the database\n3. **get_table_schema** - Get the schema of a specific table\n\nFor more examples and detailed parameter information, check the [examples/README.md](examples/README.md) file.\n\n### Running Manually\n\nYou can also run the server directly, which is useful for debugging:\n\n```bash\npython src/server.py\n```\n\nThis starts the server in standalone mode using stdio transport.\n\n## Deployment\n\n### Hosting on Smithery\n\nThis server can be hosted on Smithery.ai for easy access by other users:\n\n1. Create an account on [Smithery.ai](https://smithery.ai)\n2. Add your server to the Smithery registry\n3. Configure deployment using the Dockerfile in this repository\n4. Click \"Deploy\" on the Deployments tab on your server page\n\nThe Dockerfile is already configured to properly build and run the server with WebSocket transport support for Smithery hosting.\n\n### Security Considerations\n\nWhen hosting your Snowflake MCP server publicly:\n\n- Consider using a read-only Snowflake account\n- Restrict access to specific schemas/tables\n- Use environment variables for secure credential management\n\n## Development\n\nTo contribute to this project:\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/new-feature`\n3. Make your changes\n4. Run tests: `pytest`\n5. Commit your changes: `git commit -m 'Add new feature'`\n6. Push to the branch: `git push origin feature/new-feature`\n7. Submit a pull request\n\n## License\n\nMIT License\n\n## Acknowledgements\n\n- The [Model Context Protocol](https://modelcontextprotocol.io/) team for creating the standard\n- Snowflake for their robust database and API\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "queries",
        "snowflake databases",
        "queries snowflake",
        "databases secure"
      ],
      "category": "databases"
    },
    "prisma--mcp": {
      "owner": "prisma",
      "name": "mcp",
      "url": "https://github.com/prisma/mcp",
      "imageUrl": "",
      "description": "Gives LLMs the ability to manage Prisma Postgres databases (e.g. spin up new databases and run migrations or queries).",
      "stars": 31,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T16:20:04Z",
      "readme_content": "## Overview\n\nThe [Model-Context-Protocol](https://modelcontextprotocol.io/introduction) (MCP) gives LLMs a way to call APIs and thus access external systems in a well-defined manner. \n\nPrisma's provides two MCP servers: a _local_ and a _remote_ one. See below for specific information on each.\n\nIf you're a developer working on a local machine and want your AI agent to help with your database workflows, use the local MCP server.\n\nIf you're building an \"AI platform\" and want to give the ability to manage database to your users, use the remote MCP server.\n\n## Remote MCP server\n\nYou can start the remote MCP server as follows:\n\n```terminal\nnpx -y mcp-remote https://mcp.prisma.io/mcp\n```\n\n### Tools\n\n[Tools](https://modelcontextprotocol.io/docs/concepts/tools) represent the _capabilities_ of an MCP server. Here's the list of tools exposed by the remote MCP server:\n\n- `CreateBackupTool`: Create a new managed Prisma Postgres Backup.\n- `CreateConnectionStringTool`: Create a new Connection String for a Prisma Postgres database with the given id.\n- `CreateRecoveryTool`: Restore a Prisma Postgres Database to a new database with the given Backup id.\n- `DeleteConnectionStringTool`: Delete a Connection String with the given connection string id.\n- `DeleteDatabaseTool`: Delete a Prisma Postgres database with the given id.\n- `ListBackupsTool`: Fetch a list of available Prisma Postgres Backups for the given database id and environment id.\n- `ListConnectionStringsTool`: Fetch a list of available Prisma Postgres Database Connection Strings for the given database id and environment id.\n- `ListDatabasesTool`: Fetch a list of available Prisma Postgres Databases for user's workspace.\n- `ExecuteSqlQueryTool`: Execute a SQL query on a Prisma Postgres database with the given id.\n- `IntrospectSchemaTool`: Introspect the schema of a Prisma Postgres database with the given id.\n\nOnce you're connected to the remote MCP server, you can also always prompt your AI agent to \"List the Prisma tools\" to get a full overview of the latest supported tools.\n\n### Usage\n\nThe remote Prisma MCP server follows the standard JSON-based configuration for MCP servers. Here's what it looks like:\n\n```json\n{\n  \"mcpServers\": {\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    }\n  }\n}\n```\n\n\n> [!TIP]\n> If you want to try it the remote MCP server and explore it's capabilities, we recommend [Cloudflare's AI Playground](https://playground.ai.cloudflare.com/) for that. Add the `https://mcp.prisma.io/mcp` URL into the text field with the **Enter MCP server URL** placeholder, click **Connect**, and then authenticate with the [Prisma Console](https://console.prisma.io) in the popup window. Once connected, you can send prompts to the Playground and see what MCP tools the LLM chooses based on your prompts.\n\n### Sample prompts\n\n- \"Show me a list of all the databases in my account.\"\n- \"Create a new database in the US region for me.\"\n- \"Seed my database with real-looking data but create a backup beforehand.\"\n- \"Show me all available backups of my database.\"\n- \"Show me all customers and run an analysis over their orders.\"\n\n## Local MCP server\n\nYou can start the local MCP server as follows:\n\n```terminal\nnpx -y prisma mcp\n```\n\n\n\n> [!TIP]\n> If you're using VS Code, you can use [VS Code agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode) to enter prompts such as \"create Postgres database\" or \"apply schema migration\" directly in the chat. The VS code agent handles all underlying Prisma CLI invocations and API calls automatically. See our [VS Code Agent documentation](/postgres/integrations/vscode-extension#agent-mode) for more details.\n\n### Tools\n\n[Tools](https://modelcontextprotocol.io/docs/concepts/tools) represent the _capabilities_ of an MCP server. Here's the list of tools exposed by the local MCP server:\n\n- `migrate-status`: Checks your migration status via the `prisma migrate status` command.\n- `migrate-dev`: Creates and executes a migration via the `prisma migrate dev --name <name>` command. The LLM will provide the `<name>` option.\n- `migrate-reset`: Resets your database via the `prisma migrate reset --force` command.\n- `Prisma-Postgres-account-status`: Checks your authentication status with [Prisma Console](https://console.prisma.io) via the `platform auth show --early-access` command.\n- `Create-Prisma-Postgres-Database`: Creates a new Prisma Postgres database via the `'init --db --name' <name> '--region' <region> '--non-interactive'` command.  The LLM will provide the `<name>` and `<region>` options.\n- `Prisma-Login`: Authenticates with [Prisma Console](https://console.prisma.io) via the `platform auth login --early-access` command.\n- `Prisma-Studio`: Open Prisma Studio via the `prisma studio` command. \n\n### Usage\n\nThe local Prisma MCP server follows the standard JSON-based configuration for MCP servers. Here's what it looks like:\n\n```json\n{\n  \"mcpServers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    }\n  }\n}\n```\n\n### Sample prompts\n\nHere are some sample prompts you can use when the MCP server is running:\n- \"Log me into the Prisma Console.\"\n- \"Create a database in the US region.\"\n- \"Create a new `Product` table in my database.\"\n\n## Integrating in AI tools\n\nAI tools have different ways of integrating MCP servers. In most cases, there are dedicated configuration files in which you add the JSON configuration from above. The configuration contains a command for starting the server that'll be executed by the respective tool so that the server is available to its LLM.\n\nIn this section, we're covering the config formats of the most popular AI tools.\n\n### VS Code \n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=prisma&config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fmcp.prisma.io%2Fmcp%2F%22%7D)\n\n\nIf your browser blocks the link, [you can set it up manually](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server-to-your-workspace) by creating a `.vscode/mcp.json` file in your workspace and adding:\n\n```json file=.vscode/mcp.json\n{\n  \"servers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    },\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    }\n  }\n}\n```\n\nExplore additional Prisma features and workflows for VS Code in [our docs](https://prisma.io/docs/postgres/integrations/vscode).\n\n### Cursor\n\nTo learn more about Cursor's MCP integration, check out the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol#configuration-locations).\n\n#### Add via one-click installation\n\nYou can add the Prisma MCP server to Cursor using the [one-click installation](https://docs.cursor.com/context/model-context-protocol#one-click-installation) by clicking on the following link:\n\n<a href=\"https://cursor.com/install-mcp?name=Prisma&config=eyJjb21tYW5kIjoibnB4IC15IHByaXNtYSBtY3AifQ%3D%3D\" target=\"_blank\" rel=\"noopener noreferrer\">\n  <img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install MCP Server\" />\n</a>\n\nThis will prompt you to open the Cursor app in your browser. Once opened, you'll be guided to install the Prisma MCP server directly into your Cursor configuration.\n\n#### Add via Cursor Settings UI\n\nWhen opening the **Cursor Settings**, you can add the Prisma MCP Server as follows:\n1. Select **MCP** in the settings sidenav\n1. Click **+ Add new global MCP server**\n1. Add the `Prisma` snippet to the `mcpServers` JSON object:\n    ```json\n    {\n      \"mcpServers\": {\n        \"Prisma-Local\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"prisma\", \"mcp\"]\n        },\n        \"Prisma-Remote\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n        }\n      }\n    }\n    ```\n\n#### Global configuration\n\nAdding it via the  **Cursor Settings** settings will modify the global `~/.cursor/mcp.json` config file. In this case, the Prisma MCP server will be available in _all_ your Cursor projects:\n\n```json file=\\~/.cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    },\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    },\n    // other MCP servers\n  }\n}\n```\n\n#### Project configuration\n\nIf you want the Prisma MCP server to be available only in specific Cursor projects, add it to the Cursor config of the respective project inside the `.cursor` directory in the project's root:\n\n```json file=.cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    },\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    }\n    // other MCP servers\n  }\n}\n```\n\n### Windsurf\n\nTo learn more about Windsurf's MCP integration, check out the [Windsurf MCP docs](https://docs.codeium.com/windsurf/mcp).\n\n### Add via Windsurf MCP Plugin Store (Recommended)\n\nUse the Prisma MCP plugin from the [Windsurf MCP Plugin Store](https://docs.windsurf.com/windsurf/cascade/mcp#adding-a-new-mcp-plugin). Follow [the steps here](/orm/more/ai-tools/windsurf#add-prisma-mcp-server-via-windsurf-plugins) to add the Prisma MCP plugin in Windsurf. This is the simplest and recommended way to add the Prisma MCP server to Windsurf.\n\n#### Add via Windsurf Settings UI\n\nWhen opening the **Windsurf Settings** (via **Windsurf - Settings** > **Advanced Settings or Command Palette** > **Open Windsurf Settings Page**), you can add the Prisma MCP Server as follows:\n1. Select **Cascade** in the settings sidenav\n1. Click **Add Server**\n1. Add the `Prisma-Local` and/or `Prisma-Remote` snippets to the `mcpServers` JSON object:\n    ```json\n    {\n      \"mcpServers\": {\n        \"Prisma-Local\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"prisma\", \"mcp\"]\n        },\n        \"Prisma-Remote\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n        }\n      }\n    }\n    ```\n\n#### Global configuration\n\nAdding it via the  **Windsurf Settings** will modify the global `~/.codeium/windsurf/mcp_config.json` config file. Alternatively, you can also manually add it to that file:\n\n```json file=~/.codeium/windsurf/mcp_config.json\n{\n  \"mcpServers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    },\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    },\n    // other MCP servers\n  }\n}\n```\n\n### Warp\n\nYou can add the Prisma MCP to Warp as a globally available tool. First, [visit your MCP settings](https://docs.warp.dev/knowledge-and-collaboration/mcp#how-to-access-mcp-server-settings) and click **+ Add**. From here, you can configure the Prisma MCP server as JSON. Use the `command` and `args` properties to start the Prisma MCP server as a setup command. You can optionally configure Prisma to activate on startup using the `start_on_launch` flag:\n\n```json\n{\n  \"Prisma\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"prisma\",\n      \"mcp\"\n    ],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\nHit **Save** and ensure the MCP server is running from your MCP settings panel. Then, open a new terminal window and ask Warp to manage your Prisma database. It should reach for the Prisma MCP server automatically.\n\nTo learn more about Warp's MCP integration, visit the [Warp MCP docs](https://docs.warp.dev/knowledge-and-collaboration/mcp).\n\n### Claude Code\n\nClaude Code is a terminal-based AI tool where you can add MCP server using the `claud mcp add` command for the local MCP server:\n\n```terminal\nclaude mcp add prisma-local npx prisma mcp\n```\n\nor for the remote MCP server:\n\n```terminal\nclaude mcp add prisma-remote npx mcp-remote https://mcp.prisma.io/mcp\n```\n\nLearn more in the [Claude Code MCP docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#configure-mcp-servers).\n\n### Claude Desktop\n\nFollow the instructions in the [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server) to create the required configuration file:\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nThen add the JSON snippet to that configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"Prisma-Local\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prisma\", \"mcp\"]\n    },\n    \"Prisma-Remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n    },\n    // other MCP servers\n  }\n}\n```\n\n### OpenAI Agents SDK\n\nHere's an example for using the Prisma MCP servers in a Python script via the OpenAI Agents SDK:\n\n```python\nfrom openai import AsyncOpenAI\nfrom openai.types.beta import Assistant\nfrom openai.beta import AsyncAssistantExecutor\nfrom openai.experimental.mcp import MCPServerStdio\nfrom openai.types.beta.threads import Message, Thread\nfrom openai.types.beta.tools import ToolCall\n\nimport asyncio\n\nasync def main():\n    # Launch both MCP servers concurrently\n    async with MCPServerStdio(\n        params={\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"prisma\", \"mcp\"]\n        }\n    ) as local_server, MCPServerStdio(\n        params={\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.prisma.io/mcp\"]\n        }\n    ) as remote_server:\n        \n        # Optional: list tools from both servers\n        local_tools = await local_server.list_tools()\n        remote_tools = await remote_server.list_tools()\n        print(\"Local server tools:\", [tool.name for tool in local_tools])\n        print(\"Remote server tools:\", [tool.name for tool in remote_tools])\n\n        # Set up the assistant with both MCP servers\n        agent = Assistant(\n            name=\"Prisma Assistant\",\n            instructions=\"Use the Prisma tools to help the user with database tasks.\",\n            mcp_servers=[local_server, remote_server],\n        )\n\n        executor = AsyncAssistantExecutor(agent=agent)\n\n        # Create a thread and send a message\n        thread = Thread(messages=[Message(role=\"user\", content=\"Create a new user in the database\")])\n        response = await executor.run(thread=thread)\n\n        print(\"Agent response:\")\n        for message in response.thread.messages:\n            print(f\"{message.role}: {message.content}\")\n\n# Run the async main function\nasyncio.run(main())\n```\n---\n[Certified by MCP Review](https://mcpreview.com/mcp-servers/prisma/prisma)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prisma",
        "databases",
        "database",
        "manage prisma",
        "prisma postgres",
        "access prisma"
      ],
      "category": "databases"
    },
    "prisma--prisma": {
      "owner": "prisma",
      "name": "prisma",
      "url": "https://github.com/prisma/prisma",
      "imageUrl": "/freedevtools/mcp/pfp/prisma.webp",
      "description": "Manage databases with Prisma's ORM tools, automate schema migrations, generate type-safe query builders, and interact visually with data through a GUI. The Prisma MCP server enables AI agents to control Prisma Postgres databases by spinning up instances and running migrations.",
      "stars": 43940,
      "forks": 1878,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-04T10:02:45Z",
      "readme_content": "![Prisma](https://i.imgur.com/h6UIYTu.png)\n\n<div align=\"center\">\n  <h1>Prisma</h1>\n  <a href=\"https://www.npmjs.com/package/prisma\"><img alt=\"prisma_svg_style_flat\" src=\"https://img.shields.io/npm/v/prisma.svg?style=flat\" /></a>\n  <a href=\"https://github.com/prisma/prisma/blob/main/CONTRIBUTING.md\"><img alt=\"PRs_welcome_brightgreen\" src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\" /></a>\n  <a href=\"https://github.com/prisma/prisma/blob/main/LICENSE\"><img alt=\"license_Apache_2_blue\" src=\"https://img.shields.io/badge/license-Apache%202-blue\" /></a>\n  <a href=\"https://pris.ly/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/937751382725886062?label=Discord\"></a>\n  <br />\n  <br />\n  <a href=\"https://www.prisma.io/docs/getting-started/quickstart\">Quickstart</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://www.prisma.io/\">Website</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://www.prisma.io/docs/\">Docs</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://github.com/prisma/prisma-examples/\">Examples</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://www.prisma.io/blog\">Blog</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://pris.ly/discord?utm_source=github&utm_medium=prisma&utm_content=repo_readme\">Discord</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://pris.ly/x?utm_source=github&utm_medium=prisma&utm_content=repo_readme\">Twitter</a>\n  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n  <a href=\"https://pris.ly/youtube?utm_source=github&utm_medium=prisma&utm_content=repo_readme\">Youtube</a>\n  <br />\n  <hr />\n</div>\n\n## What is Prisma?\n\nPrisma ORM is a **next-generation ORM** that consists of these tools:\n\n- [**Prisma Client**](https://www.prisma.io/docs/concepts/components/prisma-client): Auto-generated and type-safe query builder for Node.js & TypeScript\n- [**Prisma Migrate**](https://www.prisma.io/docs/concepts/components/prisma-migrate): Declarative data modeling & migration system\n- [**Prisma Studio**](https://github.com/prisma/studio): GUI to view and edit data in your database\n\nPrisma Client can be used in _any_ Node.js or TypeScript backend application (including serverless applications and microservices). This can be a [REST API](https://www.prisma.io/docs/concepts/overview/prisma-in-your-stack/rest), a [GraphQL API](https://www.prisma.io/docs/concepts/overview/prisma-in-your-stack/graphql), a gRPC API, or anything else that needs a database.\n\n**If you need a database to use with Prisma ORM, check out [Prisma Postgres](https://www.prisma.io/docs/getting-started/quickstart-prismaPostgres?utm_source=github&utm_medium=prisma-readme) or if you are looking for our MCP Server, head [here](https://github.com/prisma/mcp).**\n\n## Getting started\n\n### Quickstart (5min)\n\nThe fastest way to get started with Prisma is by following the quickstart guides. You can choose either of two databases:\n\n- [Prisma Postgres](https://www.prisma.io/docs/getting-started/quickstart-prismaPostgres)\n- [SQLite](https://www.prisma.io/docs/getting-started/quickstart-sqlite)\n\n### Bring your own database\n\nIf you already have your own database, you can follow these guides:\n\n- [Add Prisma to an existing project](https://www.prisma.io/docs/getting-started/setup-prisma/add-to-existing-project/relational-databases-typescript-postgresql)\n- [Set up a new project with Prisma from scratch](https://www.prisma.io/docs/getting-started/setup-prisma/start-from-scratch/relational-databases-typescript-postgresql)\n\n## How Prisma ORM works\n\nThis section provides a high-level overview of how Prisma ORM works and its most important technical components. For a more thorough introduction, visit the [Prisma documentation](https://www.prisma.io/docs/).\n\n### The Prisma schema\n\nEvery project that uses a tool from the Prisma toolkit starts with a [Prisma schema file](https://www.prisma.io/docs/concepts/components/prisma-schema). The Prisma schema allows developers to define their _application models_ in an intuitive data modeling language. It also contains the connection to a database and defines a _generator_:\n\n```prisma\n// Data source\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\n// Generator\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\n// Data model\nmodel Post {\n  id        Int     @id @default(autoincrement())\n  title     String\n  content   String?\n  published Boolean @default(false)\n  author    User?   @relation(fields:  [authorId], references: [id])\n  authorId  Int?\n}\n\nmodel User {\n  id    Int     @id @default(autoincrement())\n  email String  @unique\n  name  String?\n  posts Post[]\n}\n```\n\nIn this schema, you configure three things:\n\n- **Data source**: Specifies your database connection (via an environment variable)\n- **Generator**: Indicates that you want to generate Prisma Client\n- **Data model**: Defines your application models\n\n---\n\n### The Prisma data model\n\nOn this page, the focus is on the data model. You can learn more about [Data sources](https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema/data-sources) and [Generators](https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema/generators) on the respective docs pages.\n\n#### Functions of Prisma models\n\nThe data model is a collection of [models](https://www.prisma.io/docs/concepts/components/prisma-schema/data-model#defining-models). A model has two major functions:\n\n- Represent a table in the underlying database\n- Provide the foundation for the queries in the Prisma Client API\n\n#### Getting a data model\n\nThere are two major workflows for \"getting\" a data model into your Prisma schema:\n\n- Generate the data model from [introspecting](https://www.prisma.io/docs/concepts/components/introspection) a database\n- Manually writing the data model and mapping it to the database with [Prisma Migrate](https://www.prisma.io/docs/concepts/components/prisma-migrate)\n\nOnce the data model is defined, you can [generate Prisma Client](https://www.prisma.io/docs/concepts/components/prisma-client/generating-prisma-client) which will expose CRUD and more queries for the defined models. If you're using TypeScript, you'll get full type-safety for all queries (even when only retrieving the subsets of a model's fields).\n\n---\n\n### Accessing your database with Prisma Client\n\n#### Generating Prisma Client\n\nThe first step when using Prisma Client is installing its npm package:\n\n```\nnpm install @prisma/client\n```\n\nNote that the installation of this package invokes the `prisma generate` command which reads your Prisma schema and _generates_ the Prisma Client code. The code will be located in `node_modules/.prisma/client`, which is exported by `node_modules/@prisma/client/index.d.ts`.\n\nAfter you change your data model, you'll need to manually re-generate Prisma Client to ensure the code inside `node_modules/.prisma/client` gets updated:\n\n```\nnpx prisma generate\n```\n\nRefer to the documentation for more information about [\"generating the Prisma client\"](https://www.prisma.io/docs/concepts/components/prisma-client/generating-prisma-client).\n\n#### Using Prisma Client to send queries to your database\n\nOnce the Prisma Client is generated, you can import it in your code and send queries to your database. This is what the setup code looks like.\n\n##### Import and instantiate Prisma Client\n\nYou can import and instantiate Prisma Client as follows:\n\n```ts\nimport { PrismaClient } from '@prisma/client'\n\nconst prisma = new PrismaClient()\n```\n\nor\n\n```js\nconst { PrismaClient } = require('@prisma/client')\n\nconst prisma = new PrismaClient()\n```\n\nNow you can start sending queries via the generated Prisma Client API, here are a few sample queries. Note that all Prisma Client queries return _plain old JavaScript objects_.\n\nLearn more about the available operations in the [Prisma Client docs](https://www.prisma.io/docs/concepts/components/prisma-client) or watch this [demo video](https://www.youtube.com/watch?v=LggrE5kJ75I&list=PLn2e1F9Rfr6k9PnR_figWOcSHgc_erDr5&index=4) (2 min).\n\n##### Retrieve all `User` records from the database\n\n```ts\nconst allUsers = await prisma.user.findMany()\n```\n\n##### Include the `posts` relation on each returned `User` object\n\n```ts\nconst allUsers = await prisma.user.findMany({\n  include: { posts: true },\n})\n```\n\n##### Filter all `Post` records that contain `\"prisma\"`\n\n```ts\nconst filteredPosts = await prisma.post.findMany({\n  where: {\n    OR: [{ title: { contains: 'prisma' } }, { content: { contains: 'prisma' } }],\n  },\n})\n```\n\n##### Create a new `User` and a new `Post` record in the same query\n\n```ts\nconst user = await prisma.user.create({\n  data: {\n    name: 'Alice',\n    email: 'alice@prisma.io',\n    posts: {\n      create: { title: 'Join us for Prisma Day 2021' },\n    },\n  },\n})\n```\n\n##### Update an existing `Post` record\n\n```ts\nconst post = await prisma.post.update({\n  where: { id: 42 },\n  data: { published: true },\n})\n```\n\n#### Usage with TypeScript\n\nNote that when using TypeScript, the result of this query will be _statically typed_ so that you can't accidentally access a property that doesn't exist (and any typos are caught at compile-time). Learn more about leveraging Prisma Client's generated types on the [Advanced usage of generated types](https://www.prisma.io/docs/concepts/components/prisma-client/advanced-usage-of-generated-types) page in the docs.\n\n## Community\n\nPrisma has a large and supportive [community](https://www.prisma.io/community) of enthusiastic application developers. You can join us on [Discord](https://pris.ly/discord) and here on [GitHub](https://github.com/prisma/prisma/discussions).\n\n## Badges\n\n[![Made with Prisma](http://made-with.prisma.io/dark.svg)](https://prisma.io) [![Made with Prisma](http://made-with.prisma.io/indigo.svg)](https://prisma.io)\n\nBuilt something awesome with Prisma? 🌟 Show it off with these [badges](https://github.com/prisma/presskit?tab=readme-ov-file#badges), perfect for your readme or website.\n\n```\n[![Made with Prisma](http://made-with.prisma.io/dark.svg)](https://prisma.io)\n```\n\n```\n[![Made with Prisma](http://made-with.prisma.io/indigo.svg)](https://prisma.io)\n```\n\n## Security\n\nIf you have a security issue to report, please contact us at [security@prisma.io](mailto:security@prisma.io?subject=[GitHub]%20Prisma%202%20Security%20Report%20).\n\n## Support\n\n### Ask a question about Prisma\n\nYou can ask questions and initiate [discussions](https://github.com/prisma/prisma/discussions/) about Prisma-related topics in the `prisma` repository on GitHub.\n\n👉 [**Ask a question**](https://github.com/prisma/prisma/discussions/new)\n\n### Create a bug report for Prisma\n\nIf you see an error message or run into an issue, please make sure to create a bug report! You can find [best practices for creating bug reports](https://www.prisma.io/docs/guides/other/troubleshooting-orm/creating-bug-reports) (like including additional debugging output) in the docs.\n\n👉 [**Create bug report**](https://pris.ly/prisma-prisma-bug-report)\n\n### Submit a feature request\n\nIf Prisma currently doesn't have a certain feature, be sure to check out the [roadmap](https://www.prisma.io/docs/more/roadmap) to see if this is already planned for the future.\n\nIf the feature on the roadmap is linked to a GitHub issue, please make sure to leave a 👍 reaction on the issue and ideally a comment with your thoughts about the feature!\n\n👉 [**Submit feature request**](https://github.com/prisma/prisma/issues/new?assignees=&labels=&template=feature_request.md&title=)\n\n## Contributing\n\nRefer to our [contribution guidelines](https://github.com/prisma/prisma/blob/main/CONTRIBUTING.md) and [Code of Conduct for contributors](https://github.com/prisma/prisma/blob/main/CODE_OF_CONDUCT.md).\n\n## Tests Status\n\n- Prisma Tests Status:\n  [![Prisma Tests Status](https://github.com/prisma/prisma/workflows/CI/badge.svg)](https://github.com/prisma/prisma/actions/workflows/test.yml?query=branch%3Amain)\n- Ecosystem Tests Status:\n  [![Ecosystem Tests Status](https://github.com/prisma/ecosystem-tests/workflows/test/badge.svg)](https://github.com/prisma/ecosystem-tests/actions/workflows/test.yaml?query=branch%3Adev)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prisma",
        "databases",
        "database",
        "databases prisma",
        "prisma manage",
        "access prisma"
      ],
      "category": "databases"
    },
    "pu-re--bytebase": {
      "owner": "pu-re",
      "name": "bytebase",
      "url": "https://github.com/pu-re/bytebase",
      "imageUrl": "/freedevtools/mcp/pfp/pu-re.webp",
      "description": "Streamlines the database development lifecycle by managing schema changes, enforcing policies, and ensuring data security within a unified CI/CD solution.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2025-02-10T12:56:37Z",
      "readme_content": "<h1 align=\"center\">\n  <a\n    target=\"_blank\"\n    href=\"https://bytebase.com?source=github\"\n  >\n    <img\n      align=\"center\"\n      alt=\"Bytebase\"\n      src=\"https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/banner.webp\"\n      style=\"width:100%;\"\n    />\n  </a>\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://bytebase.com/docs/get-started/install/overview\" target=\"_blank\"><b>⚙️ Install</b></a> •\n  <a href=\"https://bytebase.com/docs\"><b>📚 Docs</b></a> •\n  <a href=\"https://discord.gg/huyw7gRsyA\"><b>💬 Discord</b></a> •\n  <a href=\"https://www.bytebase.com/request-demo/\"><b>🙋‍♀️ Book Demo</b></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://goreportcard.com/report/github.com/bytebase/bytebase\">\n    <img alt=\"go report\" src=\"https://goreportcard.com/badge/github.com/bytebase/bytebase\" />\n  </a>\n  <a href=\"https://artifacthub.io/packages/search?repo=bytebase\">\n    <img alt=\"Artifact Hub\" src=\"https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/bytebase\" />\n  </a>\n    <a\n    href=\"https://github.com/bytebase/bytebase\"\n    target=\"_blank\"\n  >\n    <img alt=\"Github Stars\" src=\"https://img.shields.io/github/stars/bytebase/bytebase?logo=github\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <b> Different </b> database development tasks\n</p>\n\n<p align=\"center\">\n  <b> Multiple </b> database systems\n</p>\n\n<p align=\"center\">\n  <b> Unified </b> process\n</p>\n\n<p align=\"center\">\n  <b> Single </b> tool\n</p>\n\n<br />\n\n<p align=\"center\" >\n  \n</p>\n\n<br />\n\n<p align=\"center\">\n  <img alt=\"fish\" src=\"https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/fish.webp\" />\n</p>\n\n<br />\n\n<p align=\"center\" >\n  \n</p>\n\n<br />\n\n<p align=\"center\">🪜</p>\n<h1 align=\"center\">Change</h1>\n<p align=\"center\">\n  Want to formalize the database change process but don't know how?\n</p>\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                      |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n| <b>Standard Operating Procedure (SOP) </b><br />Standardize the database schema and data change process across different database systems, small or [large tables](https://www.bytebase.com/docs/change-database/online-schema-migration-for-mysql) and [different tenants](https://www.bytebase.com/docs/change-database/batch-change/#change-databases-from-multiple-tenants).<br /><br/><b>SQL Review</b><br />[100+ lint rules](https://www.bytebase.com/docs/sql-review/review-rules) to detect SQL anti-patterns and enforce consistent SQL style in the organization.<br /><br /><b>GitOps</b><br />[Point-and-click GitHub and GitLab integration](https://www.bytebase.com/docs/vcs-integration/overview) to enable GitOps workflow for changing database. |  |\n\n<br />\n\n<p align=\"center\">🔮</p>\n<h1 align=\"center\">Query</h1>\n<p align=\"center\">\n  Want to control the data access but don't know how?\n</p>\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                    |\n| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n| <b>All-in-one SQL Editor</b><br />Web-based IDE specifically for performing SQL specific tasks.<br /><br/><b>Data Masking</b><br />State-of-the-art [column level masking](https://www.bytebase.com/docs/sql-editor/mask-data) engine to cover complex situations like subquery, CTE.<br /><br /><b>Data Access Control</b><br />Organization level policy to centralize the [database permission](https://www.bytebase.com/docs/security/database-permission/overview). |  |\n\n<br />\n\n<p align=\"center\">🔒</p>\n<h1 align=\"center\">Secure</h1>\n<p align=\"center\">\n  Want to avoid data leakage, change outage and detect malicious behavior but don't know how?\n</p>\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                        |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n| <b>Centralize Change, Query and Admin Tasks</b><br />A single place to perform different tasks on different databases, thus enforce policy and monitor activity accordingly. <br /><br /><b>RBAC</b><br />[Two-level RBAC model](https://www.bytebase.com/docs/concepts/roles-and-permissions) mapping to the organization wide privileges and application team privileges respectively.<br /><br /><b>Anomaly Center and Audit Logging</b><br /> Capture all database [anomalies](https://www.bytebase.com/docs/administration/anomaly-center), user actions and system events and present them in a holistic view. |  |\n\n<br />\n\n<p align=\"center\">👩‍💼</p>\n<h1 align=\"center\">Govern</h1>\n<p align=\"center\">\n  Want to enforce organization policy but don't know how?\n</p>\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                           |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\n| <b>Manage Database Resources</b><br /> A single place to manage environments, database instances, database users for application development, with optional [Terraform integration](https://registry.terraform.io/providers/bytebase/bytebase/latest/docs). <br /><br /><b>Policy Enforcement</b><br />Enforce organization wide SQL Review policy, backup policy and data access policy.<br /><br/><b>SQL Editor Admin mode</b><br />[CLI like experience](https://www.bytebase.com/docs/sql-editor/admin-mode) without setting up bastion. |  |\n\n<br />\n\n# 🖖 Intro\n\n[](https://www.youtube.com/watch?v=7UE78BufSLM)\n\nBytebase is a Database CI/CD solution for the Developers and DBAs. It's the **only database CI/CD project** included by the [CNCF Landscape](https://landscape.cncf.io/?selected=bytebase) and [Platform Engineering](https://platformengineering.org/tools/bytebase). The Bytebase family consists of these tools:\n\n- [Bytebase Console](https://bytebase.com/?source=github): A web-based GUI for developers and DBAs to manage the database development lifecycle.\n- [Bytebase API](https://www.bytebase.com/docs/api/overview): Provide both gRPC and RESTful API to manipulate every aspect of Bytebase.\n- [SQL Review GitHub Action](https://github.com/bytebase/sql-review-action): The GitHub Action to detect SQL anti-patterns and enforce a consistent SQL style guide during Pull Request.\n- [Terraform Bytebase Provider](https://registry.terraform.io/providers/bytebase/bytebase/latest/docs): The Terraform\n  provider enables team to manage Bytebase resources via Terraform. A typical setup involves teams using\n  Terraform to provision database instances from Cloud vendors, followed by using Bytebase provider to\n  prepare those instances ready for application use.\n\n|     | Topic                                                               |\n| --- | :------------------------------------------------------------------ |\n| 🔧  | <b>[Installation](#-installation)</b>                               |\n| 🎮  | <b>[Demo](#-demo)</b>                                               |\n| 👩‍🏫  | <b>[Tutorials](#-tutorials)</b>                                     |\n| 💎  | <b>[Design Principles](#-design-principles)</b>                     |\n| 🧩  | <b>[Data Model](#-data-model)</b>                                   |\n| 🎭  | <b>[Roles](#-roles)</b>                                             |\n| 🕊   | <b>[Developing and Contributing](#-developing-and-contributing)</b> |\n| 🤺  | <b>[Bytebase vs Alternatives](#-bytebase-vs-alternatives)</b>       |\n\n<br />\n\n# 🔧 Installation\n\n- [Docker](https://www.bytebase.com/docs/get-started/install/deploy-with-docker)\n- [Kubernetes](https://www.bytebase.com/docs/get-started/install/deploy-to-kubernetes)\n- [Build from source](https://www.bytebase.com/docs/get-started/install/build-from-source-code)\n\n<br />\n\n# 🎮 Demo\n\nLive demo at https://demo.bytebase.com\n\nYou can also [book a 30min product walkthrough](https://cal.com/bytebase/product-walkthrough) with one of\nour product experts.\n\n<br />\n\n# 👩‍🏫 Tutorials\n\nProduct tutorials are available at https://www.bytebase.com/tutorial.\n\n## Integrations\n\n- [Manage Supabase PostgreSQL](https://www.bytebase.com/docs/how-to/integrations/supabase)\n- [Manage render PostgreSQL](https://www.bytebase.com/docs/how-to/integrations/render)\n- [Manage Neon database](https://www.bytebase.com/docs/how-to/integrations/neon)\n- [Deploy to sealos](https://www.bytebase.com/docs/get-started/install/deploy-to-sealos)\n- [Deploy to Rainbond](https://www.bytebase.com/docs/get-started/install/deploy-to-rainbond)\n\n<br />\n\n# 💎 Design Principles\n\n|     |                         |                                                                                                                                                                                                                                                                                                                                                        |\n| --- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| 🪶  | Dependency Free         | Start with a single command `./bytebase` without any external dependency. External PostgreSQL data store and others are optional.                                                                                                                                                                                                                      |\n| 🔗  | Integration First       | Solely focus on database management and leave the rest to others. We have native VCS integration with [GitHub/GitLab](https://www.bytebase.com/docs/vcs-integration/overview), [Terraform Provider](https://registry.terraform.io/providers/bytebase/bytebase/latest/docs), [webhook](https://www.bytebase.com/docs/change-database/webhook), and etc. |\n| 💂‍♀️  | Engineering Disciplined | Disciplined [bi-weekly release](https://www.bytebase.com/changelog) and [engineering practice](https://github.com/bytebase/bytebase/blob/main/docs/life-of-a-feature.md).                                                                                                                                                                              |\n\n<br />\n\n# 🧩 Data Model\n\nMore details in [Data Model Doc](https://www.bytebase.com/docs/concepts/data-model).\n\n<p align=\"center\">\n    \n</p>\n\n<br />\n\n# 🎭 Roles\n\nMore details in [Roles and Permissions Doc](https://www.bytebase.com/docs/concepts/roles-and-permissions).\n\nBytebase employs RBAC (role based access control) and provides two role sets at the workspace and project level:\n\n- Workspace roles: `Admin`, `DBA`, `Member`. The workspace role maps to the role in an organization.\n- Project roles: `Owner`, `Developer`, `Releaser`, `SQL Editor User`, `Exporter`, `Viewer`. The project level role maps to the role in a specific team or project.\n\nEvery user is assigned a workspace role, and if a particular user is involved in a particular project, then she will also be assigned a project role accordingly.\n\nBelow diagram describes a typical mapping between an engineering org and the corresponding roles in the Bytebase workspace\n\n<p align=\"center\">\n    \n</p>\n\n<br />\n\n# 🕊 Developing and Contributing\n\n<p align=\"center\">\n    \n</p>\n\n- Bytebase is built with a curated tech stack. It is optimized for **developer experience** and is very easy to start\n  working on the code:\n\n  1. It has no external dependency.\n  1. It requires zero config.\n  1. 1 command to start backend and 1 command to start frontend, both with live reload support.\n\n- Interactive code walkthrough\n\n  - [Life of a schema change](https://sourcegraph.com/github.com/bytebase/bytebase/-/blob/docs/design/life-of-a-schema-change.snb.md)\n  - [SQL Review](https://sourcegraph.com/github.com/bytebase/bytebase/-/blob/docs/design/sql-review-source-code-tour.snb.md)\n\n- Follow [Life of a Feature](https://github.com/bytebase/bytebase/blob/main/docs/life-of-a-feature.md).\n\n## Dev Environment Setup\n\n### Prerequisites\n\n- [Go](https://golang.org/doc/install)\n- [pnpm](https://pnpm.io/installation)\n\n### Steps\n\n1. Pull source.\n\n   ```bash\n   git clone https://github.com/bytebase/bytebase\n   ```\n\n1. Create an external Postgres database on localhost.\n\n   ```sql\n   CREATE USER bbdev SUPERUSER;\n   CREATE DATABASE bbdev;\n   ```\n\n1. Start backend.\n\n   ```bash\n   PG_URL=postgresql://bbdev@localhost/bbdev\n   go build -ldflags \"-w -s\" -p=16 -o ./.air/bytebase ./backend/bin/server/main.go && ./.air/bytebase --port 8080 --data . --debug --disable-sample\n   ```\n\n1. Start frontend (with live reload).\n\n   ```bash\n   cd frontend && pnpm i && pnpm dev\n   ```\n\n   Bytebase should now be running at http://localhost:3000 and change either frontend or backend code would trigger live reload.\n\n### Tips\n\n- Use [Code Inspector](https://en.inspector.fe-dev.cn/guide/start.html#method1-recommend) to locate\n  frontend code from UI. Hold `Option + Shift` on Mac or `Alt + Shift` on Windows\n\n<br />\n\n# 🤺 Bytebase vs Alternatives\n\n## Bytebase vs Flyway, Liquibase\n\n- [Bytebase vs Liquibase](https://www.bytebase.com/blog/bytebase-vs-liquibase/)\n- [Bytebase vs Flyway](https://www.bytebase.com/blog/bytebase-vs-flyway/)\n\nEither Flyway or Liquibase is a library and CLI focusing on schema change. While Bytebase is an one-stop\nsolution covering the entire database development lifecycle for Developers and DBAs to collaborate.\n\nAnother key difference is Bytebase **doesn't** support Oracle and SQL Server. This is a conscious\ndecision we make so that we can focus on supporting other databases without good tooling support.\nIn particular, many of our users tell us Bytebase is by far the best (and sometimes the only) database\ntool that can support their PostgreSQL and ClickHouse use cases.\n\n[![Star History Chart](https://api.star-history.com/svg?repos=bytebase/bytebase,liquibase/liquibase,flyway/flyway&type=Date)](https://star-history.com/#bytebase/bytebase&liquibase/liquibase&flyway/flyway&Date)\n\n## Bytebase vs Yearning, Archery\n\nEither Yearning or Archery provides a DBA operation portal. While Bytebase provides a collaboration\nworkspace for DBAs and Developers, and brings DevOps practice to the Database Change Management (DCM).\nBytebase has the similar `Project` concept seen in GitLab/GitHub and provides native GitOps integration\nwith GitLab/GitHub.\n\nAnother key difference is Yearning, Archery are open source projects maintained by the individuals part-time. While Bytebase is open-sourced, it adopts an open-core model and is a commercialized product, supported\nby a [fully staffed team](https://www.bytebase.com/about#team) [releasing new version every 2 weeks](https://www.bytebase.com/changelog).\n\n[![Star History Chart](https://api.star-history.com/svg?repos=bytebase/bytebase,cookieY/Yearning,hhyo/Archery&type=Date)](https://star-history.com/#bytebase/bytebase&cookieY/Yearning&hhyo/Archery&Date)\n\n## Bytebase vs Metabase\n\nMetabase is a data visualization and business intelligence (BI) tool. It's built for data teams and business analysts to make sense of the data.\n\nBytebase is a database development platform. It's built for the developer teams to perform database operations during the application development lifecycle.\n\n- [Bytebase vs Metabase](https://www.bytebase.com/blog/bytebase-vs-metabase/)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=bytebase/bytebase,metabase/metabase&type=Date)](https://star-history.com/#bytebase/bytebase&metabase/metabase&Date)\n\n## Bytebase vs CloudBeaver\n\nBoth have web-based SQL clients. Additionally, Bytebase offers review workflow, more collaboration and security features.\n\n- [Bytebase vs CloudBeaver](https://www.bytebase.com/blog/bytebase-vs-cloudbeaver/)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=bytebase/bytebase,dbeaver/cloudbeaver&type=Date)](https://star-history.com/#bytebase/bytebase&dbeaver/cloudbeaver&Date)\n\n## Bytebase vs DBeaver / Navicat\n\nSQL GUI Client such as MySQL Workbench, pgAdmin, DBeaver, Navicat provide a GUI to interact with the database. Bytebase not only provides a GUI client, it can also enforce centralized data access control for data security and governance.\n\n- [Bytebase vs DBeaver](https://www.bytebase.com/blog/bytebase-vs-dbeaver/)\n- [Bytebase vs Navicat](https://www.bytebase.com/blog/bytebase-vs-navicat/)\n\n## Bytebase vs Jira\n\nJira is a general-purpose issue ticketing system. Bytebase is a database domain-specific change management system. Bytebase provides an integrated experience to plan, review, and deploy database changes.\n\n- [Bytebase vs Jira](https://www.bytebase.com/blog/use-jira-for-database-change/)\n\n# 👨‍👩‍👧‍👦 Community\n\n[![Hang out on Discord](https://img.shields.io/badge/%20-Hang%20out%20on%20Discord-5865F2?style=for-the-badge&logo=discord&labelColor=EEEEEE)](https://discord.gg/huyw7gRsyA)\n\n[![Follow us on Twitter](https://img.shields.io/badge/Follow%20us%20on%20Twitter-1DA1F2?style=for-the-badge&logo=twitter&labelColor=EEEEEE)](https://twitter.com/Bytebase)\n\n<br />\n\n# 🤔 Frequently Asked Questions (FAQs)\n\nCheck out our [FAQ](https://www.bytebase.com/docs/faq).\n\n<br />\n\n# 🙋 Contact Us\n\n- Interested in joining us? Check out our [jobs page](https://bytebase.com/jobs?source=github) for openings.\n- Want to solve your schema change and database management headache? Book a [30min demo](https://cal.com/bytebase/product-walkthrough) with one of our product experts.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "bytebase",
        "database",
        "secure database",
        "databases secure",
        "pu bytebase"
      ],
      "category": "databases"
    },
    "quarkiverse--mcp-server-jdbc": {
      "owner": "quarkiverse",
      "name": "mcp-server-jdbc",
      "url": "https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc",
      "imageUrl": "",
      "description": "Connect to any JDBC-compatible database and query, insert, update, delete, and more.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jdbc",
        "databases",
        "database",
        "databases secure",
        "secure database",
        "server jdbc"
      ],
      "category": "databases"
    },
    "qwert666--mcp-server-foundry": {
      "owner": "qwert666",
      "name": "mcp-server-foundry",
      "url": "https://github.com/qwert666/mcp-server-foundry",
      "imageUrl": "/freedevtools/mcp/pfp/qwert666.webp",
      "description": "Interact with datasets, ontology objects, and functions within a Foundry instance, enabling AI assistants to perform dynamic data interactions and intelligent processing.",
      "stars": 9,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-14T02:43:40Z",
      "readme_content": "# Foundry MCP Server\n\nA Model Context Protocol server for interacting with Foundry.\nIt allows AI assistants to interact with datasets, ontology objects and functions.\n\n## Tools 🌟\n\n- list datasets\n- query datasets\n- list ontology objects\n- query ontology objects\n- list functions\n- execute functions\n\n\n## Prerequisites \n\n* Python 3.9+\n* mcp\n* pyarrow\n* pandas\n* foundry-platform-sdk\n\n# Environment Variables 🌍\n\nThe server requires few configuration variables to run:\n\n| Variable         | Description                                                          | Default     |\n|------------------|----------------------------------------------------------------------|-------------|\n| `HOSTNAME`       | Your hostname of your Foundry instance                               | *required*  |\n| `TOKEN`          | A user token that you can generate in your profile page              | *required** |\n| `CLIENT_ID`      | A service user that is created in developer console                  | *required** |\n| `CLIENT_SECRET`  | A secret associated with the service user                            | *required** |\n| `SCOPES`         | Oauth scopes                                                         | None        |\n| `ONTOLOGY_ID`    | Your ontology id                                                     | *required*  |\n\n* if token is not provided the server will try to authenticate using the oauth2 flow with client_id and client_secret\n\n## Usage\n\n### uv \n\nfirst you need to clone the repository and add the config to your app\n\n``` json\n{\n  \"mcpServers\": {\n    \"foundry\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"<path_to_mcp_server>\",\n        \"run\",\n        \"mcp-server-foundry\"\n      ],\n      \"env\": {\n        \"HOSTNAME\": \"<hostname>\",\n        \"TOKEN\": \"<token>\",\n        \"CLIENT_ID\": \"<client_id>\",\n        \"CLIENT_SECRET\": \"<client_secret>\",\n        \"SCOPES\": \"<scopes>\",\n        \"ONTOLOGY_ID\": \"<ontology_id>\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\nTo run the server in development mode:\n\n```bash\n# Clone the repository\ngit clone git@github.com:qwert666/mcp-server-foundry.git\n\n# Run the server\nnpx @modelcontextprotocol/inspector uv --directory /path/to/mcp-foundry-server run mcp-server-foundry\n```\n\n# Contributing\n- Fork the repository\n- Create your feature branch (git checkout -b feature/amazing-feature)\n- Commit your changes (git commit -m 'Add some amazing feature')\n- Push to the branch (git push origin feature/amazing-feature)\n- Open a Pull Request\n\n# License  📜\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "foundry",
        "databases secure",
        "secure database",
        "server foundry"
      ],
      "category": "databases"
    },
    "rahgadda--oracledb_mcp_server": {
      "owner": "rahgadda",
      "name": "oracledb_mcp_server",
      "url": "https://github.com/rahgadda/oracledb_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/rahgadda.webp",
      "description": "Enables interaction with Oracle Database through the generation of SQL statements and the retrieval of results using Language Model prompts, facilitating efficient data management within applications.",
      "stars": 31,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-26T15:29:04Z",
      "readme_content": "# OracleDB MCP Server\r\n\r\n[![smithery badge](https://smithery.ai/badge/@rahgadda/oracledb_mcp_server)](https://smithery.ai/server/@rahgadda/oracledb_mcp_server) [![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/rahgadda-oracledb-mcp-server)\r\n\r\n## Overview\r\n- This project will install `MCP - Model Context Protocol Server`, that provides configured Oracle Database Table/Columns as context to LLM's.\r\n- Using this we can enable LLMs to interact with Oracle Database, Generate SQL Statements and Return Results using LLM prompts.\r\n\r\n## Installation\r\n- Install package\r\n  ```bash\r\n  pip install oracledb_mcp_server\r\n  ```\r\n- Create .env in a folder with minimum value of `Oracle DB Connection String`. Sample file available [here](https://raw.githubusercontent.com/rahgadda/oracledb_mcp_server/refs/heads/main/.env)\r\n- Test `oracledb_mcp_server` server using `uv run oracledb_mcp_server` from the above folder.\r\n\r\n## Claud Desktop\r\n- Configuration details for Claud Desktop\r\n  ```json\r\n  {\r\n    \"mcpServers\": {\r\n      \"oracledb_mcp_server\":{\r\n        \"command\": \"uv\",\r\n        \"args\": [\"run\",\"oracledb_mcp_server\"],\r\n        \"env\": {\r\n            \"DEBUG\":\"True\",\r\n            \"COMMENT_DB_CONNECTION_STRING\":\"oracle+oracledb://USERNAME:PASSWORD@IP:PORT/?service_name=SERVICENAME\",\r\n            \"DB_CONNECTION_STRING\":\"oracle+oracledb://USERNAME:PASSWORD@IP:PORT/?service_name=SERVICENAME\",\r\n            \"TABLE_WHITE_LIST\":\"ACCOUNTS,CUS_ACC_RELATIONS,CUSTOMERS\",\r\n            \"COLUMN_WHITE_LIST\":\"ACCOUNTS.ACC_AAD_ID,CUS_ACC_RELATIONS.CAR_CUS_ID,CUS_ACC_RELATIONS.CAR_AAD_ID,CUSTOMERS.CUS_ID\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n  ```\r\n\r\n### Configuration\r\n- List of available environment variables\r\n  - `DEBUG`: Enable debug logging (optional default is False)\r\n  - `COMMENT_DB_CONNECTION_STRING`: Oracle DB connection String for comments. (required)\r\n  - `DB_CONNECTION_STRING`: Oracle DB connection String for execution of queries. (required)\r\n  - `TABLE_WHITE_LIST`: White Listed table names in list format [\"table1\", \"table2\"] (required)\r\n  - `COLUMN_WHITE_LIST`: White Listed table-column names in list format [\"table.column1\", \"table.column2\"] (required)\r\n  - `QUERY_LIMIT_SIZE`: Default value is 10 records if not provided(optional default is 10)\r\n\r\n## Interceptor\r\n```bash\r\nnpx @modelcontextprotocol/inspector uv --directory \"D:\\\\MyDev\\\\mcp\\\\oracledb_mcp_server\" run -m oracledb_mcp_server\r\n```\r\n\r\n## Contributing\r\nContributions are welcome.    \r\nPlease feel free to submit a Pull Request.\r\n\r\n## License\r\nThis project is licensed under the terms of the MIT license.\r\n\r\n## Demo\r\n\r\n\r\n## Github Stars\r\n[![Star History Chart](https://api.star-history.com/svg?repos=rahgadda/oracledb_mcp_server=Date)](https://star-history.com/#rahgadda/oracledb_mcp_server&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oracle",
        "databases",
        "database",
        "rahgadda oracledb_mcp_server",
        "oracle database",
        "secure database"
      ],
      "category": "databases"
    },
    "ramuzes--mcp-jena": {
      "owner": "ramuzes",
      "name": "mcp-jena",
      "url": "https://github.com/ramuzes/mcp-jena",
      "imageUrl": "/freedevtools/mcp/pfp/ramuzes.webp",
      "description": "Facilitates querying and updating RDF data in Apache Jena using SPARQL, enabling interaction with Jena Fuseki servers. Supports executing SPARQL queries and updates, listing named graphs, and integrates with AI agents through the Model Context Protocol.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T06:55:08Z",
      "readme_content": "# MCP Server for Apache Jena\n\nA Model Context Protocol (MCP) server that connects AI agents to Apache Jena for SPARQL query capabilities.\n\n## Overview\n\nThis project implements an MCP server that allows AI agents (such as Cursor, Claude for Cline, or Claude Desktop) to access and query RDF data stored in Apache Jena. The server provides tools for executing SPARQL queries and updates against a Jena Fuseki server.\n\n## Features\n\n- Execute SPARQL queries against a Jena Fuseki server\n- Execute SPARQL updates to modify RDF data\n- List available named graphs in the dataset\n- HTTP Basic authentication support for Jena Fuseki\n- Compatible with the Model Context Protocol\n\n## Prerequisites\n\n- Node.js (v16 or later)\n- Apache Jena Fuseki server running with your RDF data loaded\n- An AI agent that supports the Model Context Protocol (e.g., Cursor, Claude for Cline)\n\n## Installation\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/ramuzes/mcp-jena.git\n   cd mcp-jena\n   ```\n\n2. Install dependencies:\n   ```\n   npm install\n   ```\n\n3. Build the TypeScript code:\n   ```\n   npm run build\n   ```\n\n## Usage\n\nRun the server with default settings (localhost:3030 for Jena, 'ds' for dataset):\n\n```\nnpm start\n```\n\nOr specify custom Jena endpoint, dataset, and authentication credentials:\n\n```\nnpm start -- --endpoint http://your-jena-server:3030 --dataset your_dataset --username your_username --password your_password\n```\n\nYou can also use short flags:\n\n```\nnpm start -- -e http://your-jena-server:3030 -d your_dataset -u your_username -p your_password\n```\n\nFor development mode with automatic transpilation:\n\n```\nnpm run dev:transpile -- -e http://your-jena-server:3030 -d your_dataset -u your_username -p your_password\n```\n\n## Docker\n\nYou can run the MCP Jena server using Docker:\n\n### Building the Docker image\n\n```bash\ndocker build -t mcp-jena .\n```\n\n### Running with Docker\n\n```bash\ndocker run -e JENA_FUSEKI_URL=http://your-jena-server:3030 -e DEFAULT_DATASET=your_dataset mcp-jena\n```\n\n## Available Tools\n\nThis MCP server provides the following tools:\n\n1. **`execute_sparql_query`** - Execute a SPARQL query against the Jena dataset\n   - Includes comprehensive SPARQL syntax documentation\n   - Property path operators (/, *, +, ?, ^, |) with examples\n   - Common query patterns and templates\n   - Automatic query validation and suggestions\n\n2. **`execute_sparql_update`** - Execute a SPARQL update query to modify the dataset  \n   - Insert/Delete operations documentation\n   - Conditional updates with WHERE clauses\n   - Graph management operations\n\n3. **`list_graphs`** - List all available named graphs in the dataset\n   - Graph usage patterns and best practices\n   - Provenance and versioning examples\n\n4. **`sparql_query_templates`** - Get pre-built SPARQL query templates\n   - **exploration**: Basic data discovery and statistics\n   - **property-paths**: Complex graph navigation patterns  \n   - **statistics**: Knowledge graph metrics and analysis\n   - **validation**: Data quality and consistency checks\n   - **schema**: Structure discovery and documentation\n\n\n## Environment Variables\n\nYou can also configure the server using environment variables:\n\n- `JENA_FUSEKI_URL`: URL of your Jena Fuseki server (default: http://localhost:3030)\n- `DEFAULT_DATASET`: Default dataset name (default: ds)\n- `JENA_USERNAME`: Username for HTTP Basic authentication to Jena Fuseki\n- `JENA_PASSWORD`: Password for HTTP Basic authentication to Jena Fuseki\n- `PORT`: Port for the MCP server (for HTTP transport, default: 8080)\n- `API_KEY`: API key for MCP server authentication\n\n## Example SPARQL Queries\n\n### Basic SELECT query:\n\n```sparql\nSELECT ?subject ?predicate ?object\nWHERE {\n  ?subject ?predicate ?object\n}\nLIMIT 10\n```\n\n### Insert data with UPDATE:\n\n```sparql\nPREFIX ex: <http://example.org/>\nINSERT DATA {\n  ex:subject1 ex:predicate1 \"object1\" .\n  ex:subject2 ex:predicate2 42 .\n}\n```\n\n### Query a specific named graph:\n\n```sparql\nSELECT ?subject ?predicate ?object\nFROM NAMED <http://example.org/graph1>\nWHERE {\n  GRAPH <http://example.org/graph1> {\n    ?subject ?predicate ?object\n  }\n}\nLIMIT 10\n```\n\n## Resources\n\n- [Apache Jena](https://jena.apache.org/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [SPARQL Query Language](https://www.w3.org/TR/sparql11-query/) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sparql",
        "databases",
        "jena",
        "apache jena",
        "jena using",
        "jena facilitates"
      ],
      "category": "databases"
    },
    "rapidappio--rapidapp-mcp": {
      "owner": "rapidappio",
      "name": "rapidapp-mcp",
      "url": "https://github.com/rapidappio/rapidapp-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rapidappio.webp",
      "description": "Connect to a PostgreSQL database through the Rapidapp API, enabling AI assistants to perform operations such as creating databases, listing databases, and retrieving database details.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-13T05:16:58Z",
      "readme_content": "<h3 align=\"center\">\n  <a href=\"https://rapidapp.io\">🏠 Home page</a>\n</h4>\n\n# Rapidapp MCP Server\n\nA Node.js server implementing Model Context Protocol (MCP) for [Rapidapp](https://rapidapp.io) PostgreSQL database operations.\n\n## Overview\n\nThis MCP server allows AI assistants to do PostgreSQL database operation through the Rapidapp API.\n\n## How to Use\n\nIn Claude Desktop, or any MCP Client, you can use natural language to accomplish things with Neon, such as:\n\n- `Create a new Rapidapp PostgreSQL database called 'products'`\n- `List all my Rapidapp PostgreSQL databases`\n- `Get details of my Rapidapp PostgreSQL database with id of '123456'`\n- `Create a Spring Boot application for simple product service with CRUD operations. Use the Rapidapp PostgreSQL database 'products' as the backend. Configure the application to connect to the database.`\n\n\n#### API Key Requirement\n\n**Important:** You need a Rapidapp API key to use this MCP server. Visit https://rapidapp.io to sign up and obtain your API key.\n\n## Installation\n\n### Usage with Cursor\n\n1. Navigate to Cursor Settings > MCP\n2. Add new MCP server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"rapidapp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@rapidappio/rapidapp-mcp\"],\n         \"env\": {\n           \"RAPIDAPP_API_KEY\": \"<your-api-key>\"\n         }\n       }\n     }\n   }\n   ```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"rapidapp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@rapidappio/rapidapp-mcp\"],\n      \"env\": {\n        \"RAPIDAPP_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n### Usage with Continue.dev\n\n1. Open your Continue.dev configuration file in either format:\n\n    - YAML:\n        - MacOS/Linux: `~/.continue/config.yaml`\n        - Windows: `%USERPROFILE%\\.continue\\config.yaml`\n    - JSON:\n        - Same location as above, but named `config.json`\n\n2. Add the configuration using either format:\n\n   YAML format:\n\n   ```yaml\n   experimental:\n     modelContextProtocolServers:\n       - transport:\n           type: stdio\n           command: node\n           args: [\"-y\", \"@rapidappio/rapidapp-mcp\"]\n           env: { \"RAPIDAPP_API_KEY\": \"<your-api-key>\" }\n   ```\n\n   JSON format:\n\n   ```json\n   {\n     \"experimental\": {\n       \"modelContextProtocolServers\": [\n         {\n           \"transport\": {\n             \"type\": \"stdio\",\n             \"command\": \"npx\",\n             \"args\": [\"-y\", \"@rapidappio/rapidapp-mcp\"],\n             \"env\": { \"RAPIDAPP_API_KEY\": \"<your-api-key>\" }\n           }\n         }\n       ]\n     }\n   }\n   ```\n\n3. Save the file - Continue will automatically refresh to apply the new configuration. If the changes don't take effect immediately, try restarting your IDE.\n\n## Installing via Smithery\n\nSmithery provides the easiest way to install and configure the Rapidapp MCP across various AI assistant platforms.\n\n```\n# Claude\nnpx -y @smithery/cli@latest install @rapidappio/rapidapp-mcp --client claude\n\n# Cursor\nnpx -y @smithery/cli@latest install @rapidappio/rapidapp-mcp --client cursor\n\n# Windsurf\nnpx -y @smithery/cli@latest install@rapidappio/rapidapp-mcp --client windsurf\n```\n\nFor more information and additional integration options, visit https://smithery.ai/server/@rapidappio/rapidapp-mcp",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rapidappio",
        "rapidapp",
        "databases",
        "database rapidapp",
        "access rapidappio",
        "rapidapp api"
      ],
      "category": "databases"
    },
    "rashidazarang--airtable-mcp": {
      "owner": "rashidazarang",
      "name": "airtable-mcp",
      "url": "https://github.com/rashidazarang/airtable-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rashidazarang.webp",
      "description": "Connect to Airtable to perform operations such as querying, creating, updating, and deleting records using natural language. Supports base management, table operations, schema manipulation, record filtering, and data migration through a standardized MCP interface.",
      "stars": 39,
      "forks": 13,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T03:40:13Z",
      "readme_content": "# Airtable MCP Server\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/rashidazarang/airtable-mcp)](https://archestra.ai/mcp-catalog/rashidazarang__airtable-mcp)\n[![smithery badge](https://smithery.ai/badge/@rashidazarang/airtable-mcp)](https://smithery.ai/server/@rashidazarang/airtable-mcp)\n![Airtable](https://img.shields.io/badge/Airtable-18BFFF?style=for-the-badge&logo=Airtable&logoColor=white)\n[![MCP](https://img.shields.io/badge/MCP-3.2.4-blue)](https://github.com/rashidazarang/airtable-mcp)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue)](https://www.typescriptlang.org/)\n[![AI Agent](https://img.shields.io/badge/AI_Agent-Enhanced-purple)](https://github.com/rashidazarang/airtable-mcp)\n[![Security](https://img.shields.io/badge/Security-Enterprise-green)](https://github.com/rashidazarang/airtable-mcp)\n[![Protocol](https://img.shields.io/badge/Protocol-2024--11--05-success)](https://modelcontextprotocol.io/)\n\n🤖 **Revolutionary AI Agent v3.2.4** - Advanced AI-powered Airtable MCP server with **fixed TypeScript architecture**, world-class project organization, comprehensive intelligence capabilities, predictive analytics, and enterprise automation features.\n\n## 🚀 Latest: v3.2.4 - XSS Security Fix & Complete Protection\n\n**Major Improvements** with full backward compatibility:\n- 🔧 **TypeScript Architecture Fixed** - Resolved compilation issues, proper separation of types and runtime code\n- 📁 **World-Class Organization** - Restructured project with src/typescript, src/javascript, src/python\n- 🔒 **Security Fix Complete** - Fully resolved command injection vulnerability with comprehensive validation\n- 🔷 **TypeScript Implementation** - Complete type-safe server with strict validation\n- 📘 **Comprehensive Type Definitions** - All 33 tools and 10 AI prompts fully typed\n- 🛡️ **Compile-Time Safety** - Catch errors before runtime with advanced type checking\n- 🎯 **Developer Experience** - IntelliSense, auto-completion, and refactoring support\n- 🔄 **Dual Distribution** - Use with JavaScript or TypeScript, your choice\n\n## 🤖 AI Intelligence Suite\n\n**Complete AI-Powered Intelligence** with enterprise capabilities:\n- 🤖 **10 AI Prompt Templates** - Advanced analytics, predictions, and automation\n- 🔮 **Predictive Analytics** - Forecasting and trend analysis with confidence intervals\n- 🗣️ **Natural Language Processing** - Query your data using human language\n- 📊 **Business Intelligence** - Automated insights and recommendations\n- 🏗️ **Smart Schema Design** - AI-optimized database architecture\n- ⚡ **Workflow Automation** - Intelligent process optimization\n- 🔍 **Data Quality Auditing** - Comprehensive quality assessment and fixes\n- 📈 **Statistical Analysis** - Advanced analytics with significance testing\n\n## ✨ Features\n\n- 🔍 **Natural Language Queries** - Ask questions about your data in plain English\n- 📊 **Full CRUD Operations** - Create, read, update, and delete records\n- 🪝 **Webhook Management** - Create and manage webhooks for real-time notifications\n- 🏗️ **Advanced Schema Management** - Create tables, fields, and manage base structure\n- 🔍 **Base Discovery** - Explore all accessible bases and their schemas\n- 🔧 **Field Management** - Add, modify, and remove fields programmatically\n- 🔐 **Secure Authentication** - Uses environment variables for credentials\n- 🚀 **Easy Setup** - Multiple installation options available\n- ⚡ **Fast & Reliable** - Built with Node.js for optimal performance\n- 🎯 **33 Powerful Tools** - Complete Airtable API coverage with batch operations\n- 📎 **Attachment Management** - Upload files via URLs to attachment fields\n- ⚡ **Batch Operations** - Create, update, delete up to 10 records at once\n- 👥 **Collaboration Tools** - Manage base collaborators and shared views\n- 🤖 **AI Integration** - Prompts and sampling for intelligent data operations\n- 🔐 **Enterprise Security** - OAuth2, rate limiting, comprehensive validation\n\n## 📋 Prerequisites\n\n- Node.js 14+ installed on your system\n- An Airtable account with a Personal Access Token\n- Your Airtable Base ID\n\n## 🚀 Quick Start\n\n### Step 1: Get Your Airtable Credentials\n\n1. **Personal Access Token**: Visit [Airtable Account](https://airtable.com/account) → Create a token with the following scopes:\n   - `data.records:read` - Read records from tables\n   - `data.records:write` - Create, update, delete records\n   - `schema.bases:read` - View table schemas\n   - `schema.bases:write` - **New in v1.5.0** - Create/modify tables and fields\n   - `webhook:manage` - (Optional) For webhook features\n\n2. **Base ID**: Open your Airtable base and copy the ID from the URL:\n   ```\n   https://airtable.com/[BASE_ID]/...\n   ```\n\n### Step 2: Installation\n\nChoose one of these installation methods:\n\n#### 🔷 TypeScript Users (Recommended for Development)\n\n```bash\n# Install with TypeScript support\nnpm install -g @rashidazarang/airtable-mcp\n\n# For development with types\nnpm install --save-dev typescript @types/node\n```\n\n#### 📦 JavaScript Users (Production Ready)\n\n**Option A: Install via NPM (Recommended)**\n\n```bash\nnpm install -g @rashidazarang/airtable-mcp\n```\n\n**Option B: Clone from GitHub**\n\n```bash\ngit clone https://github.com/rashidazarang/airtable-mcp.git\ncd airtable-mcp\nnpm install\n```\n\n### Step 3: Set Up Environment Variables\n\nCreate a `.env` file in your project directory:\n\n```env\nAIRTABLE_TOKEN=your_personal_access_token_here\nAIRTABLE_BASE_ID=your_base_id_here\n```\n\n**Security Note**: Never commit `.env` files to version control!\n\n### Step 4: Configure Your MCP Client\n\n#### 🔷 TypeScript Configuration (Enhanced Developer Experience)\n\nAdd to your Claude Desktop configuration file with TypeScript binary:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%\\\\Claude\\\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable-typescript\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@rashidazarang/airtable-mcp\",\n        \"--token\",\n        \"YOUR_AIRTABLE_TOKEN\",\n        \"--base\",\n        \"YOUR_BASE_ID\"\n      ],\n      \"env\": {\n        \"NODE_ENV\": \"production\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\n#### 📦 JavaScript Configuration (Standard)\n\nAdd to your Claude Desktop configuration file:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@rashidazarang/airtable-mcp\",\n        \"--token\",\n        \"YOUR_AIRTABLE_TOKEN\",\n        \"--base\",\n        \"YOUR_BASE_ID\"\n      ]\n    }\n  }\n}\n```\n\n#### For Environment Variables (More Secure)\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\"@rashidazarang/airtable-mcp\"],\n      \"env\": {\n        \"AIRTABLE_TOKEN\": \"YOUR_AIRTABLE_TOKEN\",\n        \"AIRTABLE_BASE_ID\": \"YOUR_BASE_ID\"\n      }\n    }\n  }\n}\n```\n\n### Step 5: Restart Your MCP Client\n\nAfter configuration, restart Claude Desktop or your MCP client to load the Airtable server.\n\n## 🎯 Usage Examples\n\nOnce configured, you can interact with your Airtable data naturally:\n\n### 🔷 TypeScript Development\n\n```typescript\nimport { \n  AirtableMCPServer, \n  ListRecordsInput, \n  AnalyzeDataPrompt \n} from '@rashidazarang/airtable-mcp/types';\n\nconst server = new AirtableMCPServer();\n\n// Type-safe data operations\nconst params: ListRecordsInput = {\n  table: 'Tasks',\n  maxRecords: 10,\n  filterByFormula: \"Status = 'Active'\"\n};\n\nconst records = await server.handleToolCall('list_records', params);\n\n// Type-safe AI analytics\nconst analysis: AnalyzeDataPrompt = {\n  table: 'Sales',\n  analysis_type: 'predictive',\n  confidence_level: 0.95\n};\n\nconst insights = await server.handlePromptGet('analyze_data', analysis);\n```\n\n### 📦 Natural Language Interactions\n\n**Basic Operations**\n```\n\"Show me all records in the Projects table\"\n\"Create a new task with priority 'High' and due date tomorrow\"\n\"Update the status of task ID rec123 to 'Completed'\"\n\"Delete all records where status is 'Archived'\"\n\"What tables are in my base?\"\n\"Search for records where Status equals 'Active'\"\n```\n\n**Webhook Operations (v1.4.0+)**\n```\n\"Create a webhook for my table that notifies https://my-app.com/webhook\"\n\"List all active webhooks in my base\"\n\"Show me the recent webhook payloads\"\n\"Delete webhook ach123xyz\"\n```\n\n**Schema Management (v1.5.0+)**\n```\n\"List all my accessible Airtable bases\"\n\"Show me the complete schema for this base\"\n\"Describe the Projects table with all field details\"\n\"Create a new table called 'Tasks' with Name, Priority, and Due Date fields\"\n\"Add a Status field to the existing Projects table\"\n\"What field types are available in Airtable?\"\n```\n\n**Batch Operations & Attachments (v1.6.0+)**\n```\n\"Create 5 new records at once in the Tasks table\"\n\"Update multiple records with new status values\"\n\"Delete these 3 records in one operation\"\n\"Attach this image URL to the record's photo field\"\n\"Who are the collaborators on this base?\"\n\"Show me all shared views in this base\"\n```\n\n## 🛠️ Available Tools (33 Total)\n\n### 📊 Data Operations (7 tools)\n| Tool | Description |\n|------|-------------|\n| `list_tables` | Get all tables in your base with schema information |\n| `list_records` | Query records with optional filtering and pagination |\n| `get_record` | Retrieve a single record by ID |\n| `create_record` | Add new records to any table |\n| `update_record` | Modify existing record fields |\n| `delete_record` | Remove records from a table |\n| `search_records` | Advanced search with Airtable formulas and sorting |\n\n### 🪝 Webhook Management (5 tools)\n| Tool | Description |\n|------|-------------|\n| `list_webhooks` | View all webhooks configured for your base |\n| `create_webhook` | Set up real-time notifications for data changes |\n| `delete_webhook` | Remove webhook configurations |\n| `get_webhook_payloads` | Retrieve webhook notification history |\n| `refresh_webhook` | Extend webhook expiration time |\n\n### 🔍 Schema Discovery (6 tools) - **New in v1.5.0**\n| Tool | Description |\n|------|-------------|\n| `list_bases` | List all accessible Airtable bases with permissions |\n| `get_base_schema` | Get complete schema information for any base |\n| `describe_table` | Get detailed table info including all field specifications |\n| `list_field_types` | Reference guide for all available Airtable field types |\n| `get_table_views` | List all views for a specific table with configurations |\n\n### 🏗️ Table Management (3 tools) - **New in v1.5.0**\n| Tool | Description |\n|------|-------------|\n| `create_table` | Create new tables with custom field definitions |\n| `update_table` | Modify table names and descriptions |\n| `delete_table` | Remove tables (with safety confirmation required) |\n\n### 🔧 Field Management (3 tools) - **New in v1.5.0**\n| Tool | Description |\n|------|-------------|\n| `create_field` | Add new fields to existing tables with all field types |\n| `update_field` | Modify field properties, names, and options |\n| `delete_field` | Remove fields (with safety confirmation required) |\n\n### ⚡ Batch Operations (4 tools) - **New in v1.6.0**\n| Tool | Description |\n|------|-------------|\n| `batch_create_records` | Create up to 10 records at once for better performance |\n| `batch_update_records` | Update up to 10 records simultaneously |\n| `batch_delete_records` | Delete up to 10 records in a single operation |\n| `batch_upsert_records` | Update existing or create new records based on key fields |\n\n### 📎 Attachment Management (1 tool) - **New in v1.6.0**\n| Tool | Description |\n|------|-------------|\n| `upload_attachment` | Attach files from public URLs to attachment fields |\n\n### 👁️ Advanced Views (2 tools) - **New in v1.6.0**\n| Tool | Description |\n|------|-------------|\n| `create_view` | Create new views (grid, form, calendar, etc.) with custom configurations |\n| `get_view_metadata` | Get detailed view information including filters and sorts |\n\n### 🏢 Base Management (3 tools) - **New in v1.6.0**\n| Tool | Description |\n|------|-------------|\n| `create_base` | Create new Airtable bases with initial table structures |\n| `list_collaborators` | View base collaborators and their permission levels |\n| `list_shares` | List shared views and their public configurations |\n\n### 🤖 AI Intelligence Suite (10 prompts) - **New in v3.0.0**\n| Prompt | Description | Enterprise Features |\n|--------|-------------|-------------------|\n| `analyze_data` | Advanced statistical analysis with ML insights | Confidence intervals, anomaly detection |\n| `create_report` | Intelligent report generation with recommendations | Multi-stakeholder customization, ROI analysis |\n| `data_insights` | Business intelligence and pattern discovery | Cross-table correlations, predictive indicators |\n| `optimize_workflow` | AI-powered automation recommendations | Change management, implementation roadmaps |\n| `smart_schema_design` | Database optimization with best practices | Compliance-aware (GDPR, HIPAA), scalability planning |\n| `data_quality_audit` | Comprehensive quality assessment and fixes | Automated remediation, governance frameworks |\n| `predictive_analytics` | Forecasting and trend prediction | Multiple algorithms, uncertainty quantification |\n| `natural_language_query` | Process human questions intelligently | Context awareness, confidence scoring |\n| `smart_data_transformation` | AI-assisted data processing | Quality rules, audit trails, optimization |\n| `automation_recommendations` | Workflow optimization suggestions | Technical feasibility, cost-benefit analysis |\n\n## 🔧 Advanced Configuration\n\n### Using with Smithery Cloud\n\nFor cloud-hosted MCP servers:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@smithery/cli\",\n        \"run\",\n        \"@rashidazarang/airtable-mcp\",\n        \"--token\",\n        \"YOUR_TOKEN\",\n        \"--base\",\n        \"YOUR_BASE_ID\"\n      ]\n    }\n  }\n}\n```\n\n### Direct Node.js Execution\n\nIf you cloned the repository:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/airtable-mcp/airtable_simple.js\",\n        \"--token\",\n        \"YOUR_TOKEN\",\n        \"--base\",\n        \"YOUR_BASE_ID\"\n      ]\n    }\n  }\n}\n```\n\n## 🧪 Testing\n\n### 🔷 TypeScript Testing\n\nRun the comprehensive TypeScript test suite:\n\n```bash\n# Install dependencies first\nnpm install\n\n# Run TypeScript type checking\nnpm run test:types\n\n# Run full TypeScript test suite\nnpm run test:ts\n\n# Build and test TypeScript server\nnpm run build\nnpm run start:ts\n```\n\n### 📦 JavaScript Testing\n\nRun the comprehensive test suite to verify all 33 tools:\n\n```bash\n# Set environment variables first\nexport AIRTABLE_TOKEN=your_token\nexport AIRTABLE_BASE_ID=your_base_id\n\n# Start the server\nnode airtable_simple.js &\n\n# Run comprehensive tests (v1.6.0+)\n./test_v1.6.0_comprehensive.sh\n```\n\nThe TypeScript test suite validates:\n- **Type Safety**: Compile-time validation of all interfaces\n- **Enterprise Testing**: 33 tools with strict type checking\n- **AI Prompt Validation**: All 10 AI templates with proper typing\n- **Error Handling**: Type-safe error management\n- **Performance**: Concurrent operations with type safety\n- **Integration**: Full MCP protocol compliance\n\nThe JavaScript test suite validates:\n- All 33 tools with real API calls\n- Complete CRUD operations\n- Advanced schema management\n- Batch operations (create/update/delete multiple records)\n- Attachment management via URLs\n- Advanced view creation and metadata\n- Base management and collaboration tools\n- Webhook management\n- Error handling and edge cases\n- Security verification\n- 100% test coverage\n\n## 🐛 Troubleshooting\n\n### \"Connection Refused\" Error\n- Ensure the MCP server is running\n- Check that port 8010 is not blocked\n- Restart your MCP client\n\n### \"Invalid Token\" Error\n- Verify your Personal Access Token is correct\n- Check that the token has the required scopes\n- Ensure no extra spaces in your credentials\n\n### \"Base Not Found\" Error\n- Confirm your Base ID is correct\n- Check that your token has access to the base\n\n### Port Conflicts\nIf port 8010 is in use:\n```bash\nlsof -ti:8010 | xargs kill -9\n```\n\n## 📚 Documentation\n\n### 🔷 TypeScript Documentation\n- 📘 [TypeScript Examples](./examples/typescript/) - Complete type-safe usage examples\n- 🏗️ [Type Definitions](./types/) - Comprehensive type definitions for all features\n- 🧪 [TypeScript Testing](./src/test-suite.ts) - Enterprise-grade testing framework\n\n### 📦 General Documentation  \n- 🎆 [Release Notes v3.1.0](./RELEASE_NOTES_v3.1.0.md) - **Latest TypeScript release**\n- [Release Notes v1.6.0](./RELEASE_NOTES_v1.6.0.md) - Major feature release\n- [Release Notes v1.5.0](./RELEASE_NOTES_v1.5.0.md)\n- [Release Notes v1.4.0](./RELEASE_NOTES_v1.4.0.md)\n- [Detailed Setup Guide](./CLAUDE_INTEGRATION.md)\n- [Development Guide](./DEVELOPMENT.md)\n- [Security Notice](./SECURITY_NOTICE.md)\n\n## 📦 Version History\n\n- **v3.1.0** (2025-08-16) - 🔷 **TypeScript Support**: Enterprise-grade type safety, comprehensive type definitions, dual JS/TS distribution\n- **v3.0.0** (2025-08-16) - 🤖 **Revolutionary AI Agent**: 10 intelligent prompts, predictive analytics, natural language processing\n- **v2.2.3** (2025-08-16) - 🔒 **Security release**: Final XSS vulnerability fixes and enhanced validation\n- **v2.2.0** (2025-08-16) - 🏆 **Major release**: Complete MCP 2024-11-05 protocol implementation\n- **v1.6.0** (2025-08-15) - 🎆 **Major release**: Added batch operations & attachment management (33 total tools)\n- **v1.5.0** (2025-08-15) - Added comprehensive schema management (23 total tools)\n- **v1.4.0** (2025-08-14) - Added webhook support and enhanced CRUD operations (12 tools)\n- **v1.2.4** (2025-08-12) - Security fixes and stability improvements\n- **v1.2.3** (2025-08-11) - Bug fixes and error handling\n- **v1.2.2** (2025-08-10) - Initial stable release\n\n## 📂 Project Structure\n\n```\nairtable-mcp/\n├── src/                    # Source code\n│   ├── index.js           # Main entry point\n│   ├── typescript/        # TypeScript implementation\n│   ├── javascript/        # JavaScript implementation\n│   └── python/            # Python implementation\n├── dist/                  # Compiled TypeScript output\n├── docs/                  # Documentation\n│   ├── guides/           # User guides\n│   └── releases/         # Release notes\n├── tests/                # Test files\n├── examples/             # Usage examples\n└── types/                # TypeScript type definitions\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## 📄 License\n\nMIT License - see [LICENSE](./LICENSE) file for details\n\n## 🙏 Acknowledgments\n\n- Built for the [Model Context Protocol](https://modelcontextprotocol.io/)\n- Powered by [Airtable API](https://airtable.com/developers/web/api/introduction)\n- Compatible with [Claude Desktop](https://claude.ai/) and other MCP clients\n\n## 📮 Support\n\n- **Issues**: [GitHub Issues](https://github.com/rashidazarang/airtable-mcp/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/rashidazarang/airtable-mcp/discussions)\n\n---\n\n**Version**: 3.2.4 | **Status**: 🔷 TypeScript Fixed + 🤖 AI Agent | **MCP Protocol**: 2024-11-05 Complete | **Type Safety**: Enterprise-Grade | **Intelligence**: 10 AI Prompts | **Security**: Fully Patched (XSS Fixed) | **Last Updated**: September 9, 2025\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "airtable",
        "rashidazarang airtable",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "ravenwits--mcp-server-arangodb": {
      "owner": "ravenwits",
      "name": "mcp-server-arangodb",
      "url": "https://github.com/ravenwits/mcp-server-arangodb",
      "imageUrl": "/freedevtools/mcp/pfp/ravenwits.webp",
      "description": "Provides database interaction capabilities through ArangoDB, enabling execution of AQL queries and insertion of documents into collections while supporting parameterized queries.",
      "stars": 40,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T20:05:45Z",
      "readme_content": "# MCP Server for ArangoDB\n\n[![smithery badge](https://smithery.ai/badge/@ravenwits/mcp-server-arangodb)](https://smithery.ai/server/@ravenwits/mcp-server-arangodb)\n\nA Model Context Protocol server for ArangoDB\n\nThis is a TypeScript-based MCP server that provides database interaction capabilities through ArangoDB. It implements core database operations and allows seamless integration with ArangoDB through MCP tools. You can use it wih Claude app and also extension for VSCode that works with mcp like Cline!\n\n## Features\n\n### Tools\n\n- `arango_query` - Execute AQL queries\n\n  - Takes an AQL query string as required parameter\n  - Optionally accepts bind variables for parameterized queries\n  - Returns query results as JSON\n\n- `arango_insert` - Insert documents into collections\n\n  - Takes collection name and document object as required parameters\n  - Automatically generates document key if not provided\n  - Returns the created document metadata\n\n- `arango_update` - Update existing documents\n\n  - Takes collection name, document key, and update object as required parameters\n  - Returns the updated document metadata\n\n- `arango_remove` - Remove documents from collections\n\n  - Takes collection name and document key as required parameters\n  - Returns the removed document metadata\n\n- `arango_backup` - Backup all collections to JSON files\n\n  - Takes output directory path as required parameter\n  - Creates JSON files for each collection with current data\n  - Useful for data backup and migration purposes\n\n- `arango_list_collections` - List all collections in the database\n\n  - Returns array of collection information including names, IDs, and types\n\n- `arango_create_collection` - Create a new collection in the database\n  - Takes collection name as required parameter\n  - Optionally specify collection type (document or edge collection)\n  - Configure waitForSync behavior for write operations\n  - Returns collection information including name, type, and status\n\n## Installation\n\n### Installing via NPM\n\nTo install `arango-server` globally via NPM, run the following command:\n\n```bash\nnpm install -g arango-server\n```\n\n### Running via NPX\n\nTo run `arango-server` directly without installation, use the following command:\n\n```bash\nnpx arango-server\n```\n\n### Configuring for VSCode Agent\n\nTo use `arango-server` with the VSCode Copilot agent, you must have at least **VSCode 1.99.0 installed** and follow these steps:\n\n1. **Create or edit the MCP configuration file**:\n\n   - **Workspace-specific configuration**: Create or edit the `.vscode/mcp.json` file in your workspace.\n   - **User-specific configuration**: Optionally, specify the server in the [setting(mcp)](vscode://settings/mcp) VS Code [user settings](https://code.visualstudio.com/docs/getstarted/personalize-vscode#_configure-settings) to enable the MCP server across all workspaces.\n\n     _Tip: You can refer [here](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) to the MCP configuration documentation of VSCode for more details on how to set up the configuration file._\n\n2. **Add the following configuration**:\n\n   ```json\n   {\n   \t\"servers\": {\n   \t\t\"arango-mcp\": {\n   \t\t\t\"type\": \"stdio\",\n   \t\t\t\"command\": \"npx\",\n   \t\t\t\"args\": [\"arango-server\"],\n   \t\t\t\"env\": {\n   \t\t\t\t\"ARANGO_URL\": \"http://localhost:8529\",\n   \t\t\t\t\"ARANGO_DB\": \"v20\",\n   \t\t\t\t\"ARANGO_USERNAME\": \"app\",\n   \t\t\t\t\"ARANGO_PASSWORD\": \"75Sab@MYa3Dj8Fc\"\n   \t\t\t}\n   \t\t}\n   \t}\n   }\n   ```\n\n3. **Start the MCP server**:\n\n   - Open the Command Palette in VSCode (`Ctrl+Shift+P` or `Cmd+Shift+P` on Mac).\n   - Run the command `MCP: Start Server` and select `arango-mcp` from the list.\n\n4. **Verify the server**:\n   - Open the Chat view in VSCode and switch to Agent mode.\n   - Use the `Tools` button to verify that the `arango-server` tools are available.\n\n### Installing via Smithery\n\nTo install ArangoDB for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ravenwits/mcp-server-arangodb):\n\n```bash\nnpx -y @smithery/cli install @ravenwits/mcp-server-arangodb --client claude\n```\n\n#### To use with Claude Desktop\n\nGo to: `Settings > Developer > Edit Config` or\n\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### To use with Cline VSCode Extension\n\nGo to: `Cline Extension > MCP Servers > Edit Configuration` or\n\n- MacOS: `~/Library/Application Support/Code/User/globalStorage/cline.cline/config.json`\n- Windows: `%APPDATA%/Code/User/globalStorage/cline.cline/config.json`\n\nAdd the following configuration to the `mcpServers` section:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"arango\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"/path/to/arango-server/build/index.js\"],\n\t\t\t\"env\": {\n\t\t\t\t\"ARANGO_URL\": \"your_database_url\",\n\t\t\t\t\"ARANGO_DB\": \"your_database_name\",\n\t\t\t\t\"ARANGO_USERNAME\": \"your_username\",\n\t\t\t\t\"ARANGO_PASSWORD\": \"your_password\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variables:\n\n- `ARANGO_URL` - ArangoDB server URL (note: 8529 is the default port for ArangoDB for local development)\n- `ARANGO_DB` - Database name\n- `ARANGO_USERNAME` - Database user\n- `ARANGO_PASSWORD` - Database password\n\n## Usage\n\nYou can pretty much provide any meaningful prompt and Claude will try to execute the appropriate function.\n\nSome example propmts:\n\n- \"List all collections in the database\"\n- \"Query all users\"\n- \"Insert a new document with name 'John Doe' and email \"<john@example.com>' to the 'users' collection\"\n- \"Update the document with key '123456' or name 'Jane Doe' to change the age to 48\"\n- \"Create a new collection named 'products'\"\n\n### Usage with Claude App\n\n\n\n### Uasge with Cline VSCode extension\n\n\n\nQuery all users:\n\n```typescript\n{\n  \"query\": \"FOR user IN users RETURN user\"\n}\n```\n\nInsert a new document:\n\n```typescript\n{\n  \"collection\": \"users\",\n  \"document\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\"\n  }\n}\n```\n\nUpdate a document:\n\n```typescript\n{\n  \"collection\": \"users\",\n  \"key\": \"123456\",\n  \"update\": {\n    \"name\": \"Jane Doe\"\n  }\n}\n```\n\nRemove a document:\n\n```typescript\n{\n  \"collection\": \"users\",\n  \"key\": \"123456\"\n}\n```\n\nList all collections:\n\n```typescript\n{\n} // No parameters required\n```\n\nBackup database collections:\n\n```typescript\n{\n  \"outputDir\": \"./backup\" // Specify an absolute output directory path for the backup files (optional)\n  \"collection\": \"users\" // Specify a collection name to backup (optional) If no collection name is provided, all collections will be backed up\n  \"docLimit\": 1000 // Specify the maximum number of documents to backup per collection (optional), if not provided, all documents will be backed up (not having a limit might cause timeout for large collections)\n}\n```\n\nCreate a new collection:\n\n```typescript\n{\n  \"name\": \"products\",\n  \"type\": \"document\", // \"document\" or \"edge\" (optional, defaults to \"document\")\n  \"waitForSync\": false // Optional, defaults to false\n}\n```\n\nNote: The server is database-structure agnostic and can work with any collection names or structures as long as they follow ArangoDB's document and edge collection models.\n\n## Disclaimer\n\n### For Development Use Only\n\nThis tool is designed for local development environments only. While technically it could connect to a production database, this would create significant security risks and is explicitly discouraged. We use it exclusively with our development databases to maintain separation of concerns and protect production data.\n\n## Development\n\n1. Clone the repository\n2. Install dependencies:\n\n   ```bash\n   npm run build\n   ```\n\n3. For development with auto-rebuild:\n\n   ```bash\n   npm run watch\n   ```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. recommended debugging can be done by using [MCP Inspector](https://github.com/modelcontextprotocol/inspector) for development:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "arangodb",
        "databases",
        "database",
        "capabilities arangodb",
        "arangodb provides",
        "server arangodb"
      ],
      "category": "databases"
    },
    "rebots-online--mcp-neo4j": {
      "owner": "rebots-online",
      "name": "mcp-neo4j",
      "url": "https://github.com/rebots-online/mcp-neo4j",
      "imageUrl": "/freedevtools/mcp/pfp/rebots-online.webp",
      "description": "Manage and manipulate knowledge graphs with Neo4j, supporting operations for creating, reading, and modifying entities and relationships in graph databases.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-27T02:26:12Z",
      "readme_content": "# Neo4j MCP Server\n\nFork of the Neo4j Model Context Protocol (MCP) server with environment variable support and improved configuration options.\n\n## Features\n\n* Environment variable configuration for Neo4j connection\n* Support for custom ports and remote Neo4j instances\n* Improved error handling and logging\n* Compatible with the [Model Context Protocol](https://modelcontextprotocol.io/introduction)\n\n## Configuration\n\nThe server can be configured using the following environment variables:\n\n* `NEO4J_URL` - Neo4j connection URL (default: \"bolt://localhost:7687\")\n* `NEO4J_USER` - Neo4j username (default: \"neo4j\")\n* `NEO4J_PASSWORD` - Neo4j password (default: \"neo4j\")\n\nExample:\n```bash\nNEO4J_URL=\"bolt://192.168.0.157:28687\" \\\nNEO4J_USER=\"neo4j\" \\\nNEO4J_PASSWORD=\"your-password\" \\\nnode dist/servers/mcp-neo4j-memory/main.js\n```\n\n## Available Tools\n\n### mcp-neo4j-memory\n\nKnowledge graph memory stored in Neo4j with the following capabilities:\n\n* `create_entities` - Create multiple new entities in the knowledge graph\n* `create_relations` - Create relations between entities (in active voice)\n* `add_observations` - Add new observations to existing entities\n* `delete_entities` - Delete entities and their relations\n* `delete_observations` - Delete specific observations from entities\n* `delete_relations` - Delete specific relations\n* `read_graph` - Read the entire knowledge graph\n* `search_nodes` - Search for nodes based on a query\n* `open_nodes` - Open specific nodes by their names\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Start the server\nnpm start\n```\n\n## Changes from Upstream\n\n* Added environment variable support for Neo4j connection details\n* Improved error handling and connection management\n* Added detailed logging for debugging\n* Updated configuration to support remote Neo4j instances\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neo4j",
        "databases",
        "schema",
        "graphs neo4j",
        "neo4j manage",
        "graph databases"
      ],
      "category": "databases"
    },
    "redis--mcp-redis": {
      "owner": "redis",
      "name": "mcp-redis",
      "url": "https://github.com/redis/mcp-redis",
      "imageUrl": "/freedevtools/mcp/pfp/redis.webp",
      "description": "Manage and search data in Redis using natural language queries. It enables interaction with Redis data structures like hashes, lists, sets, and streams for efficient data operations and insights.",
      "stars": 267,
      "forks": 66,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T01:58:17Z",
      "readme_content": "# Redis MCP Server\n[![Integration](https://github.com/redis/mcp-redis/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/redis/mcp-redis/actions/workflows/ci.yml)\n[![PyPI - Version](https://img.shields.io/pypi/v/redis-mcp-server)](https://pypi.org/project/redis-mcp-server/)\n[![Python Version](https://img.shields.io/badge/python-3.13%2B-blue&logo=redis)](https://www.python.org/downloads/)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE.txt)\n[![smithery badge](https://smithery.ai/badge/@redis/mcp-redis)](https://smithery.ai/server/@redis/mcp-redis)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/70102150-efe0-4705-9f7d-87980109a279)\n[![Docker Image Version](https://img.shields.io/docker/v/mcp/redis?sort=semver&logo=docker&label=Docker)](https://hub.docker.com/r/mcp/redis)\n[![codecov](https://codecov.io/gh/redis/mcp-redis/branch/master/graph/badge.svg?token=yenl5fzxxr)](https://codecov.io/gh/redis/mcp-redis)\n\n\n[![Discord](https://img.shields.io/discord/697882427875393627.svg?style=social&logo=discord)](https://discord.gg/redis)\n[![Twitch](https://img.shields.io/twitch/status/redisinc?style=social)](https://www.twitch.tv/redisinc)\n[![YouTube](https://img.shields.io/youtube/channel/views/UCD78lHSwYqMlyetR0_P4Vig?style=social)](https://www.youtube.com/redisinc)\n[![Twitter](https://img.shields.io/twitter/follow/redisinc?style=social)](https://twitter.com/redisinc)\n[![Stack Exchange questions](https://img.shields.io/stackexchange/stackoverflow/t/mcp-redis?style=social&logo=stackoverflow&label=Stackoverflow)](https://stackoverflow.com/questions/tagged/mcp-redis)\n\n## Overview\nThe Redis MCP Server is a **natural language interface** designed for agentic applications to efficiently manage and search data in Redis. It integrates seamlessly with **MCP (Model Content Protocol) clients**, enabling AI-driven workflows to interact with structured and unstructured data in Redis. Using this MCP Server, you can ask questions like:\n\n- \"Store the entire conversation in a stream\"\n- \"Cache this item\"\n- \"Store the session with an expiration time\"\n- \"Index and search this vector\"\n\n## Table of Contents\n- [Overview](#overview)\n- [Features](#features)\n- [Tools](#tools)\n- [Installation](#installation)\n  - [From PyPI (recommended)](#from-pypi-recommended)\n  - [Testing the PyPI package](#testing-the-pypi-package)\n  - [From GitHub](#from-github)\n  - [Development Installation](#development-installation)\n  - [With Docker](#with-docker)\n- [Configuration](#configuration)\n  - [Redis ACL](#redis-acl)\n  - [Configuration via command line arguments](#configuration-via-command-line-arguments)\n  - [Configuration via Environment Variables](#configuration-via-environment-variables)\n  - [Logging](#logging)\n- [Integrations](#integrations)\n  - [OpenAI Agents SDK](#openai-agents-sdk)\n  - [Augment](#augment)\n  - [Claude Desktop](#claude-desktop)\n  - [VS Code with GitHub Copilot](#vs-code-with-github-copilot)\n- [Testing](#testing)\n- [Example Use Cases](#example-use-cases)\n- [Contributing](#contributing)\n- [License](#license)\n- [Badges](#badges)\n- [Contact](#contact)\n\n\n## Features\n- **Natural Language Queries**: Enables AI agents to query and update Redis using natural language.\n- **Seamless MCP Integration**: Works with any **MCP client** for smooth communication.\n- **Full Redis Support**: Handles **hashes, lists, sets, sorted sets, streams**, and more.\n- **Search & Filtering**: Supports efficient data retrieval and searching in Redis.\n- **Scalable & Lightweight**: Designed for **high-performance** data operations.\n- The Redis MCP Server supports the `stdio` [transport](https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio). Support to the `stremable-http` transport will be added in the future.\n\n## Tools\n\nThis MCP Server provides tools to manage the data stored in Redis.\n\n- `string` tools to set, get strings with expiration. Useful for storing simple configuration values, session data, or caching responses.\n- `hash` tools to store field-value pairs within a single key. The hash can store vector embeddings. Useful for representing objects with multiple attributes, user profiles, or product information where fields can be accessed individually.\n- `list` tools with common operations to append and pop items. Useful for queues, message brokers, or maintaining a list of most recent actions.\n- `set` tools to add, remove and list set members. Useful for tracking unique values like user IDs or tags, and for performing set operations like intersection.\n- `sorted set` tools to manage data for e.g. leaderboards, priority queues, or time-based analytics with score-based ordering.\n- `pub/sub` functionality to publish messages to channels and subscribe to receive them. Useful for real-time notifications, chat applications, or distributing updates to multiple clients.\n- `streams` tools to add, read, and delete from data streams. Useful for event sourcing, activity feeds, or sensor data logging with consumer groups support.\n- `JSON` tools to store, retrieve, and manipulate JSON documents in Redis. Useful for complex nested data structures, document databases, or configuration management with path-based access.\n\nAdditional tools.\n\n- `query engine` tools to manage vector indexes and perform vector search\n- `server management` tool to retrieve information about the database\n\n## Installation\n\nThe Redis MCP Server is available as a PyPI package and as direct installation from the GitHub repository.\n\n### From PyPI (recommended)\nConfiguring the latest Redis MCP Server version from PyPI, as an example, can be done importing the following JSON configuration in the desired framework or tool.\nThe `uvx` command will download the server on the fly (if not cached already), create a temporary environment, and then run it.\n\n```commandline\n{\n  \"mcpServers\": {\n    \"RedisMCPServer\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"redis-mcp-server@latest\",\n        \"redis-mcp-server\",\n        \"--url\",\n        \"\\\"redis://localhost:6379/0\\\"\"\n      ]\n    }\n  }\n}\n```\n\nYou will find examples for different platforms along the README.\n\n### Testing the PyPI package\n\nYou can install the package as follows:\n\n```sh\npip install redis-mcp-server\n```\n\nAnd start it using `uv` the package in your environment.\n\n```sh\nuv python install 3.13\nuv sync\nuv run redis-mcp-server --url redis://localhost:6379/0\n```\n\nHowever, starting the MCP Server is most useful when delegate to the framework or tool where this MCP Server is configured.\n\n### From GitHub\n\nYou can configure the desired Redis MCP Server version with `uvx`, which allows you to run it directly from GitHub (from a branch, or use a tagged release).\n\n> It is recommended to use a tagged release, the `main` branch is under active development and may contain breaking changes.\n\nAs an example, you can execute the following command to run the `0.2.0` release:\n\n```commandline\nuvx --from git+https://github.com/redis/mcp-redis.git@0.2.0 redis-mcp-server --url redis://localhost:6379/0\n```\n\nCheck the release notes for the latest version in the [Releases](https://github.com/redis/mcp-redis/releases) section.\nAdditional examples are provided below.\n\n```sh\n# Run with Redis URI\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --url redis://localhost:6379/0\n\n# Run with Redis URI and SSL\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --url \"rediss://<USERNAME>:<PASSWORD>@<HOST>:<PORT>?ssl_cert_reqs=required&ssl_ca_certs=<PATH_TO_CERT>\"\n\n# Run with individual parameters\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --host localhost --port 6379 --password mypassword\n\n# See all options\nuvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --help\n```\n\n### Development Installation\n\nFor development or if you prefer to clone the repository:\n\n```sh\n# Clone the repository\ngit clone https://github.com/redis/mcp-redis.git\ncd mcp-redis\n\n# Install dependencies using uv\nuv venv\nsource .venv/bin/activate\nuv sync\n\n# Run with CLI interface\nuv run redis-mcp-server --help\n\n# Or run the main file directly (uses environment variables)\nuv run src/main.py\n```\n\nOnce you cloned the repository, installed the dependencies and verified you can run the server, you can configure Claude Desktop or any other MCP Client to use this MCP Server running the main file directly (it uses environment variables). This is usually preferred for development.\nThe following example is for Claude Desktop, but the same applies to any other MCP Client.\n\n1. Specify your Redis credentials and TLS configuration\n2. Retrieve your `uv` command full path (e.g. `which uv`)\n3. Edit the `claude_desktop_config.json` configuration file\n   - on a MacOS, at `~/Library/Application\\ Support/Claude/`\n\n```json\n{\n    \"mcpServers\": {\n        \"redis\": {\n            \"command\": \"<full_path_uv_command>\",\n            \"args\": [\n                \"--directory\",\n                \"<your_mcp_server_directory>\",\n                \"run\",\n                \"src/main.py\"\n            ],\n            \"env\": {\n                \"REDIS_HOST\": \"<your_redis_database_hostname>\",\n                \"REDIS_PORT\": \"<your_redis_database_port>\",\n                \"REDIS_PWD\": \"<your_redis_database_password>\",\n                \"REDIS_SSL\": True|False,\n                \"REDIS_CA_PATH\": \"<your_redis_ca_path>\",\n                \"REDIS_CLUSTER_MODE\": True|False\n            }\n        }\n    }\n}\n```\n\nYou can troubleshoot problems by tailing the log file.\n\n```commandline\ntail -f ~/Library/Logs/Claude/mcp-server-redis.log\n```\n\n### With Docker\n\nYou can use a dockerized deployment of this server. You can either build your own image or use the official [Redis MCP Docker](https://hub.docker.com/r/mcp/redis) image.\n\nIf you'd like to build your own image, the Redis MCP Server provides a Dockerfile. Build this server's image with:\n\n```commandline\ndocker build -t mcp-redis .\n```\n\nFinally, configure the client to create the container at start-up. An example for Claude Desktop is provided below. Edit the `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\",\n                \"--rm\",\n                \"--name\",\n                \"redis-mcp-server\",\n                \"-i\",\n                \"-e\", \"REDIS_HOST=<redis_hostname>\",\n                \"-e\", \"REDIS_PORT=<redis_port>\",\n                \"-e\", \"REDIS_USERNAME=<redis_username>\",\n                \"-e\", \"REDIS_PWD=<redis_password>\",\n                \"mcp-redis\"]\n    }\n  }\n}\n```\n\nTo use the official [Redis MCP Docker](https://hub.docker.com/r/mcp/redis) image, just replace your image name (`mcp-redis` in the example above) with `mcp/redis`.\n\n## Configuration\n\nThe Redis MCP Server can be configured in two ways: via command line arguments or via environment variables.\nThe precedence is: command line arguments > environment variables > default values.\n\n### Redis ACL\n\nYou can configure Redis ACL to restrict the access to the Redis database. For example, to create a read-only user:\n\n```\n127.0.0.1:6379> ACL SETUSER readonlyuser on >mypassword ~* +@read -@write\n```\n\nConfigure the user via command line arguments or environment variables.\n\n### Configuration via command line arguments\n\nWhen using the CLI interface, you can configure the server with command line arguments:\n\n```sh\n# Basic Redis connection\nuvx --from redis-mcp-server@latest redis-mcp-server \\\n  --host localhost \\\n  --port 6379 \\\n  --password mypassword\n\n# Using Redis URI (simpler)\nuvx --from redis-mcp-server@latest redis-mcp-server \\\n  --url redis://user:pass@localhost:6379/0\n\n# SSL connection\nuvx --from redis-mcp-server@latest redis-mcp-server \\\n  --url rediss://user:pass@redis.example.com:6379/0\n\n# See all available options\nuvx --from redis-mcp-server@latest redis-mcp-server --help\n```\n\n**Available CLI Options:**\n- `--url` - Redis connection URI (redis://user:pass@host:port/db)\n- `--host` - Redis hostname (default: 127.0.0.1)\n- `--port` - Redis port (default: 6379)\n- `--db` - Redis database number (default: 0)\n- `--username` - Redis username\n- `--password` - Redis password\n- `--ssl` - Enable SSL connection\n- `--ssl-ca-path` - Path to CA certificate file\n- `--ssl-keyfile` - Path to SSL key file\n- `--ssl-certfile` - Path to SSL certificate file\n- `--ssl-cert-reqs` - SSL certificate requirements (default: required)\n- `--ssl-ca-certs` - Path to CA certificates file\n- `--cluster-mode` - Enable Redis cluster mode\n\n### Configuration via Environment Variables\n\nIf desired, you can use environment variables. Defaults are provided for all variables.\n\n| Name                 | Description                                               | Default Value |\n|----------------------|-----------------------------------------------------------|---------------|\n| `REDIS_HOST`         | Redis IP or hostname                                      | `\"127.0.0.1\"` |\n| `REDIS_PORT`         | Redis port                                                | `6379`        |\n| `REDIS_DB`           | Database                                                  | 0             |\n| `REDIS_USERNAME`     | Default database username                                 | `\"default\"`   |\n| `REDIS_PWD`          | Default database password                                 | \"\"            |\n| `REDIS_SSL`          | Enables or disables SSL/TLS                               | `False`       |\n| `REDIS_CA_PATH`      | CA certificate for verifying server                       | None          |\n| `REDIS_SSL_KEYFILE`  | Client's private key file for client authentication       | None          |\n| `REDIS_SSL_CERTFILE` | Client's certificate file for client authentication       | None          |\n| `REDIS_CERT_REQS`    | Whether the client should verify the server's certificate | `\"required\"`  |\n| `REDIS_CA_CERTS`     | Path to the trusted CA certificates file                  | None          |\n| `REDIS_CLUSTER_MODE` | Enable Redis Cluster mode                                 | `False`       |\n\n\nThere are several ways to set environment variables:\n\n1. **Using a `.env` File**:\nPlace a `.env` file in your project directory with key-value pairs for each environment variable. Tools like `python-dotenv`, `pipenv`, and `uv` can automatically load these variables when running your application. This is a convenient and secure way to manage configuration, as it keeps sensitive data out of your shell history and version control (if `.env` is in `.gitignore`).\nFor example, create a `.env` file with the following content from the `.env.example` file provided in the repository:\n\n```bash\ncp .env.example .env\n```\n\nThen edit the `.env` file to set your Redis configuration:\n\nOR,\n\n2. **Setting Variables in the Shell**:\nYou can export environment variables directly in your shell before running your application. For example:\n\n```sh\nexport REDIS_HOST=your_redis_host\nexport REDIS_PORT=6379\n# Other variables will be set similarly...\n```\n\nThis method is useful for temporary overrides or quick testing.\n\n\n### Logging\n\nThe server uses Python's standard logging and is configured at startup. By default it logs at WARNING and above. You can change verbosity with the `MCP_REDIS_LOG_LEVEL` environment variable.\n\n- Accepted values (case-insensitive): `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`, `NOTSET`\n- Aliases supported: `WARN` → `WARNING`, `FATAL` → `CRITICAL`\n- Numeric values are also accepted, including signed (e.g., `\"10\"`, `\"+20\"`)\n- Default when unset or unrecognized: `WARNING`\n\nHandler behavior\n- If the host (e.g., `uv`, VS Code, pytest) already installed console handlers, the server will NOT add its own; it only lowers overly-restrictive handler thresholds so your chosen level is not filtered out. It will never raise a handler's threshold.\n- If no handlers are present, the server adds a single stderr StreamHandler with a simple format.\n\nExamples\n```bash\n# See normal lifecycle messages\nMCP_REDIS_LOG_LEVEL=INFO uv run src/main.py\n\n# Very verbose for debugging\nMCP_REDIS_LOG_LEVEL=DEBUG uvx --from redis-mcp-server@latest redis-mcp-server --url redis://localhost:6379/0\n```\n\nIn MCP client configs that support env, add it alongside your Redis settings. For example:\n```json\n{\n  \"mcpServers\": {\n    \"redis\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"redis-mcp-server@latest\", \"redis-mcp-server\", \"--url\", \"redis://localhost:6379/0\"],\n      \"env\": {\n        \"REDIS_HOST\": \"localhost\",\n        \"REDIS_PORT\": \"6379\",\n        \"MCP_REDIS_LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\n\n## Integrations\n\nIntegrating this MCP Server to development frameworks like OpenAI Agents SDK, or with tools like Claude Desktop, VS Code, or Augment is described in the following sections.\n\n### OpenAI Agents SDK\n\nIntegrate this MCP Server with the OpenAI Agents SDK. Read the [documents](https://openai.github.io/openai-agents-python/mcp/) to learn more about the integration of the SDK with MCP.\n\nInstall the Python SDK.\n\n```commandline\npip install openai-agents\n```\n\nConfigure the OpenAI token:\n\n```commandline\nexport OPENAI_API_KEY=\"<openai_token>\"\n```\n\nAnd run the [application](./examples/redis_assistant.py).\n\n```commandline\npython3.13 redis_assistant.py\n```\n\nYou can troubleshoot your agent workflows using the [OpenAI dashboard](https://platform.openai.com/traces/).\n\n### Augment\n\nThe preferred way of configuring the Redis MCP Server in Augment is to use the [Easy MCP](https://docs.augmentcode.com/setup-augment/mcp#redis) feature.\n\nYou can also configure the Redis MCP Server in Augment manually by importing the server via JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"Redis MCP Server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"redis-mcp-server@latest\",\n        \"redis-mcp-server\",\n        \"--url\",\n        \"redis://localhost:6379/0\"\n      ]\n    }\n  }\n}\n```\n\n### Claude Desktop\n\nThe simplest way to configure MCP clients is using `uvx`. Add the following JSON to your `claude_desktop_config.json`, remember to provide the full path to `uvx`.\n\n```json\n{\n  \"mcpServers\": {\n    \"redis-mcp-server\": {\n        \"type\": \"stdio\",\n        \"command\": \"/Users/mortensi/.local/bin/uvx\",\n        \"args\": [\n            \"--from\", \"redis-mcp-server@latest\",\n            \"redis-mcp-server\",\n            \"--url\", \"redis://localhost:6379/0\"\n        ]\n    }\n  }\n}\n```\n\nIf you'd like to test the [Redis MCP Server](https://smithery.ai/server/@redis/mcp-redis) via Smithery, you can configure Claude Desktop automatically:\n\n```bash\nnpx -y @smithery/cli install @redis/mcp-redis --client claude\n```\n\nFollow the prompt and provide the details to configure the server and connect to Redis (e.g. using a Redis Cloud database).\nThe procedure will create the proper configuration in the `claude_desktop_config.json` configuration file.\n\n### VS Code with GitHub Copilot\n\nTo use the Redis MCP Server with VS Code, you must nable the [agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode) tools. Add the following to your `settings.json`:\n\n```json\n{\n  \"chat.agent.enabled\": true\n}\n```\n\nYou can start the GitHub desired version of the Redis MCP server using `uvx` by adding the following JSON to your `mcp.json` file:\n\n```json\n\"servers\": {\n  \"Redis MCP Server\": {\n    \"type\": \"stdio\",\n    \"command\": \"uvx\", \n    \"args\": [\n      \"--from\", \"redis-mcp-server@latest\",\n      \"redis-mcp-server\",\n      \"--url\", \"redis://localhost:6379/0\"\n    ]\n  },\n}\n```\n\nAlternatively, you can start the server using `uv` and configure your `mcp.json`. This is usually desired for development.\n\n```json\n{\n  \"servers\": {\n    \"redis\": {\n      \"type\": \"stdio\",\n      \"command\": \"<full_path_uv_command>\",\n      \"args\": [\n        \"--directory\",\n        \"<your_mcp_server_directory>\",\n        \"run\",\n        \"src/main.py\"\n      ],\n      \"env\": {\n        \"REDIS_HOST\": \"<your_redis_database_hostname>\",\n        \"REDIS_PORT\": \"<your_redis_database_port>\",\n        \"REDIS_USERNAME\": \"<your_redis_database_username>\",\n        \"REDIS_PWD\": \"<your_redis_database_password>\",\n      }\n    }\n  }\n}\n```\n\nFor more information, see the [VS Code documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n> **Note:** Starting with [VS Code v1.102](https://code.visualstudio.com/updates/v1_102),  \n> MCP servers are now stored in a dedicated `mcp.json` file instead of `settings.json`. \n\n## Testing\n\nYou can use the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for visual debugging of this MCP Server.\n\n```sh\nnpx @modelcontextprotocol/inspector uv run src/main.py\n```\n\n## Example Use Cases\n- **AI Assistants**: Enable LLMs to fetch, store, and process data in Redis.\n- **Chatbots & Virtual Agents**: Retrieve session data, manage queues, and personalize responses.\n- **Data Search & Analytics**: Query Redis for **real-time insights and fast lookups**.\n- **Event Processing**: Manage event streams with **Redis Streams**.\n\n## Contributing\n1. Fork the repo\n2. Create a new branch (`feature-branch`)\n3. Commit your changes\n4. Push to your branch and submit a PR!\n\n## License\nThis project is licensed under the **MIT License**.\n\n## Badges\n\n<a href=\"https://glama.ai/mcp/servers/@redis/mcp-redis\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@redis/mcp-redis/badge\" alt=\"Redis Server MCP server\" />\n</a>\n\n## Contact\nFor questions or support, reach out via [GitHub Issues](https://github.com/redis/mcp-redis/issues).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "redis",
        "databases",
        "database",
        "data redis",
        "redis data",
        "redis using"
      ],
      "category": "databases"
    },
    "rhabraken--mcp-python": {
      "owner": "rhabraken",
      "name": "mcp-python",
      "url": "https://github.com/rhabraken/mcp-python",
      "imageUrl": "/freedevtools/mcp/pfp/rhabraken.webp",
      "description": "Interact seamlessly with PostgreSQL, MySQL, MariaDB, and SQLite databases using Claude Desktop. This MCP server facilitates efficient database management and query execution through a unified interface.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-02-12T06:09:05Z",
      "readme_content": "# Talk with Your Database Using MCP\n\n\n\nThis guide explains how to set up and run your MCP server to interact with your\nPostgreSQL database using Claude Desktop. It should also work with MySQL,\nMariaDB, SQLite as it uses sqlalchemy under the hood. This project is build on\ntop of https://github.com/runekaagaard/mcp-alchemy\n\n> **Note:** This guide assumes you have a basic understanding of Docker,\n> environment variables, and CLI usage.\n\n---\n\n## Prerequisites\n\n- **Docker Compose**:\n  [Installation instructions](https://docs.docker.com/compose/install/)\n- **Claude Desktop**: [Download here](https://claude.ai/download)\n- **uv**: A modern, high-performance Python package manager. If not installed,\n  follow the instructions below.\n\n---\n\n## 1. Set Up Environment Variables\n\n1. **Copy and Rename the Environment File**  \n   Duplicate the provided `.env.example` file and rename it to `.env`:\n   ```bash\n   cp .env.example .env\n   ```\n\n---\n\n## 2. Set Up Claude Desktop\n\n1. **Download and Install Claude Desktop**  \n   Visit the [Claude Desktop download page](https://claude.ai/download) and\n   install the application.\n\n---\n\n## 3. Launch the PostgreSQL Database with Dummy Data\n\n1. **Run Docker Compose**  \n   Ensure Docker Compose is installed and run:\n   ```bash\n   docker-compose up -d\n   ```\n   - This command will launch a PostgreSQL database on `localhost:5432` and\n     populate it with dummy data.\n\n---\n\n## 4. Install `uv` (if not already installed)\n\n1. **Install `uv`**  \n   Execute the following command to install `uv`:\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n---\n\n## 5. Configure and Launch the MCP Server\n\n1. **Create/Update the MCP Server Configuration**  \n   Save the following JSON configuration in your MCP server config file (adjust\n   paths if necessary):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"my_database\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"/directory/where/this/app/is/located/app/\",\n           \"run\",\n           \"server.py\"\n         ],\n         \"env\": {\n           \"DB_URL\": \"postgresql://postgres:password@localhost:5432/mydatabase\"\n         }\n       }\n     }\n   }\n   ```\n\n   - **Tip:** If `uv` is installed in a non-standard location, update the\n     `\"command\"` value to reflect the full path to the executable.\n\n2. **Launch the MCP Server**  \n   With the configuration in place, the MCP server will automatically start each\n   time Claude Desktop is launched.\n\n---\n\n## 6. Enjoy Your Setup\n\n- Open **Claude Desktop**.\n- The tool will automatically call your MCP server, enabling you to interact\n  with your database seamlessly.\n\n---\n\n## Summary\n\n1. **Set Up Environment Variables**: Copy `.env.example` to `.env`.\n2. **Install and Run Claude Desktop**: Download from\n   [Claude Desktop](https://claude.ai/download).\n3. **Launch PostgreSQL with Docker Compose**: Run `docker-compose up` to start\n   the database with dummy data.\n4. **Install `uv`**: Run the provided installation command if necessary.\n5. **Configure MCP Server**: Update the config file and ensure paths and\n   environment variables are correct.\n6. **Launch and Enjoy**: Start Claude Desktop to begin interacting with your\n   database via MCP.\n\n---\n\nIf you encounter any issues or need further assistance, please refer to the\nrelevant documentation or contact your support team.\n\nHappy coding!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mariadb",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "rick-noya--mcp-chatbot": {
      "owner": "rick-noya",
      "name": "mcp-chatbot",
      "url": "https://github.com/rick-noya/mcp-chatbot",
      "imageUrl": "/freedevtools/mcp/pfp/rick-noya.webp",
      "description": "Generates and executes SQL queries on a Postgres database using OpenAI's GPT models, returning structured JSON responses optimized for chatbot user interfaces. Integrates with frontend applications via a FastAPI REST API, supporting deployment on AWS Lambda or local environments.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-25T03:30:05Z",
      "readme_content": "# MCP Chat Backend\n\nThis project is a serverless FastAPI backend for a chatbot that generates and executes SQL queries on a Postgres database using OpenAI's GPT models, then returns structured, UI-friendly responses. It is designed to run on AWS Lambda via AWS SAM, but can also be run locally or in Docker.\n\n## Features\n- FastAPI REST API with a single `/ask` endpoint\n- Uses OpenAI GPT models to generate and summarize SQL queries\n- Connects to a Postgres (Supabase) database\n- Returns structured JSON responses for easy frontend rendering\n- CORS enabled for frontend integration\n- Deployable to AWS Lambda (SAM), or run locally/Docker\n- Verbose logging for debugging (CloudWatch)\n\n## Project Structure\n```\n├── main.py            # Main FastAPI app and Lambda handler\n├── requirements.txt   # Python dependencies\n├── template.yaml      # AWS SAM template for Lambda deployment\n├── samconfig.toml     # AWS SAM deployment config\n├── Dockerfile         # For local/Docker deployment\n├── .gitignore         # Files to ignore in git\n└── .env               # (Not committed) Environment variables\n```\n\n## Setup\n\n### 1. Clone the repository\n```sh\ngit clone <your-repo-url>\ncd mcp-chat-3\n```\n\n### 2. Install Python dependencies\n```sh\npython -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\npip install -r requirements.txt\n```\n\n### 3. Set up environment variables\nCreate a `.env` file (not committed to git):\n```\nOPENAI_API_KEY=your-openai-key\nSUPABASE_DB_NAME=your-db\nSUPABASE_DB_USER=your-user\nSUPABASE_DB_PASSWORD=your-password\nSUPABASE_DB_HOST=your-host\nSUPABASE_DB_PORT=your-port\n```\n\n## Running Locally\n\n### With Uvicorn\n```sh\nuvicorn main:app --reload --port 8080\n```\n\n### With Docker\n```sh\ndocker build -t mcp-chat-backend .\ndocker run -p 8080:8080 --env-file .env mcp-chat-backend\n```\n\n## Deploying to AWS Lambda (SAM)\n1. Install [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)\n2. Build and deploy:\n```sh\nsam build\nsam deploy --guided\n```\n- Configure environment variables in `template.yaml` or via the AWS Console.\n- The API will be available at the endpoint shown after deployment (e.g. `https://xxxxxx.execute-api.region.amazonaws.com/Prod/ask`).\n\n## API Usage\n\n### POST /ask\n- **Body:** `{ \"question\": \"your question here\" }`\n- **Response:** Structured JSON for chatbot UI, e.g.\n```json\n{\n  \"messages\": [\n    {\n      \"type\": \"text\",\n      \"content\": \"Sample 588 has a resistance of 1.2 ohms.\",\n      \"entity\": {\n        \"entity_type\": \"sample\",\n        \"id\": \"588\"\n      }\n    },\n    {\n      \"type\": \"list\",\n      \"items\": [\"Item 1\", \"Item 2\"]\n    }\n  ]\n}\n```\n- See `main.py` for the full schema and more details.\n\n## Environment Variables\n- `OPENAI_API_KEY`: Your OpenAI API key\n- `SUPABASE_DB_NAME`, `SUPABASE_DB_USER`, `SUPABASE_DB_PASSWORD`, `SUPABASE_DB_HOST`, `SUPABASE_DB_PORT`: Your Postgres database credentials\n\n## Development Notes\n- All logs are sent to stdout (and CloudWatch on Lambda)\n- CORS is enabled for all origins by default\n- The backend expects the frontend to handle the structured response format\n\n## License\nMIT (or your license here) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatbot",
        "api",
        "databases",
        "optimized chatbot",
        "mcp chatbot",
        "secure database"
      ],
      "category": "databases"
    },
    "rileylemm--graphrag_mcp": {
      "owner": "rileylemm",
      "name": "graphrag_mcp",
      "url": "https://github.com/rileylemm/graphrag_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rileylemm.webp",
      "description": "Integrates semantic and graph-based document retrieval using a hybrid system of Neo4j and Qdrant, enabling enhanced search capabilities. Supports semantic search via document embeddings and graph context expansion based on relationships.",
      "stars": 45,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T15:18:37Z",
      "readme_content": "# GraphRAG MCP Server\n\nA Model Context Protocol server for querying a hybrid graph and vector database system, combining Neo4j (graph database) and Qdrant (vector database) for powerful semantic and graph-based document retrieval.\n\n## Overview\n\nGraphRAG MCP provides a seamless integration between large language models and a hybrid retrieval system that leverages the strengths of both graph databases (Neo4j) and vector databases (Qdrant). This enables:\n\n- Semantic search through document embeddings\n- Graph-based context expansion following relationships\n- Hybrid search combining vector similarity with graph relationships\n- Full integration with Claude and other LLMs through MCP\n\nThis project follows the [Model Context Protocol](https://github.com/modelcontextprotocol/python-sdk) specification, making it compatible with any MCP-enabled client.\n\n## Features\n\n- **Semantic search** using sentence embeddings and Qdrant\n- **Graph-based context expansion** using Neo4j\n- **Hybrid search** combining both approaches\n- **MCP tools and resources** for LLM integration\n- Full documentation of Neo4j schema and Qdrant collection information\n\n## Prerequisites\n\n- Python 3.12+\n- Neo4j running on localhost:7687 (default configuration)\n- Qdrant running on localhost:6333 (default configuration)\n- Document data indexed in both databases\n\n## Installation\n\n### Quick Start\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/rileylemm/graphrag_mcp.git\n   cd graphrag_mcp\n   ```\n\n2. Install dependencies with uv:\n   ```bash\n   uv install\n   ```\n\n3. Configure your database connections in the `.env` file:\n   ```\n   # Neo4j Configuration\n   NEO4J_URI=bolt://localhost:7687\n   NEO4J_USER=neo4j\n   NEO4J_PASSWORD=password\n\n   # Qdrant Configuration\n   QDRANT_HOST=localhost\n   QDRANT_PORT=6333\n   QDRANT_COLLECTION=document_chunks\n   ```\n\n4. Run the server:\n   ```bash\n   uv run main.py\n   ```\n\n### Detailed Setup Guide\n\nFor a detailed guide on setting up the underlying hybrid database system, please refer to the companion repository: [GraphRAG Hybrid Database](https://github.com/rileylemm/graphrag-hybrid)\n\n#### Setting up Neo4j and Qdrant\n\n1. Install and start Neo4j:\n   ```bash\n   # Using Docker\n   docker run \\\n     --name neo4j \\\n     -p 7474:7474 -p 7687:7687 \\\n     -e NEO4J_AUTH=neo4j/password \\\n     -v $HOME/neo4j/data:/data \\\n     -v $HOME/neo4j/logs:/logs \\\n     -v $HOME/neo4j/import:/import \\\n     -v $HOME/neo4j/plugins:/plugins \\\n     neo4j:latest\n   ```\n\n2. Install and start Qdrant:\n   ```bash\n   # Using Docker\n   docker run -p 6333:6333 -p 6334:6334 \\\n     -v $HOME/qdrant/storage:/qdrant/storage \\\n     qdrant/qdrant\n   ```\n\n#### Indexing Documents\n\nTo index your documents in both databases, follow these steps:\n\n1. Prepare your documents\n2. Create embeddings using sentence-transformers\n3. Store documents in Neo4j with relationship information\n4. Store document chunk embeddings in Qdrant\n\nRefer to the [GraphRAG Hybrid Database](https://github.com/rileylemm/graphrag-hybrid) repository for detailed indexing scripts and procedures.\n\n## Integration with MCP Clients\n\n### Claude Desktop / Cursor Integration\n\n1. Make the run script executable:\n   ```bash\n   chmod +x run_server.sh\n   ```\n\n2. Add the server to your MCP configuration file (`~/.cursor/mcp.json` or Claude Desktop equivalent):\n   ```json\n   {\n     \"mcpServers\": {\n       \"GraphRAG\": {\n         \"command\": \"/path/to/graphrag_mcp/run_server.sh\",\n         \"args\": []\n       }\n     }\n   }\n   ```\n\n3. Restart your MCP client (Cursor, Claude Desktop, etc.)\n\n## Usage\n\n### MCP Tools\n\nThis server provides the following tools for LLM use:\n\n1. `search_documentation` - Search for information using semantic search\n   ```python\n   # Example usage in MCP context\n   result = search_documentation(\n       query=\"How does graph context expansion work?\",\n       limit=5,\n       category=\"technical\"\n   )\n   ```\n\n2. `hybrid_search` - Search using both semantic and graph-based approaches\n   ```python\n   # Example usage in MCP context\n   result = hybrid_search(\n       query=\"Vector similarity with graph relationships\",\n       limit=10,\n       category=None,\n       expand_context=True\n   )\n   ```\n\n### MCP Resources\n\nThe server provides the following resources:\n\n1. `https://graphrag.db/schema/neo4j` - Information about the Neo4j graph schema\n2. `https://graphrag.db/collection/qdrant` - Information about the Qdrant vector collection\n\n## Troubleshooting\n\n- **Connection issues**: Ensure Neo4j and Qdrant are running and accessible\n- **Empty results**: Check that your document collection is properly indexed\n- **Missing dependencies**: Run `uv install` to ensure all packages are installed\n- **Database authentication**: Verify credentials in your `.env` file\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License\n\nCopyright (c) 2025 Riley Lemm\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n## Attribution\n\nIf you use this MCP server or adapt it for your own purposes, please provide attribution to Riley Lemm and link back to this repository (https://github.com/rileylemm/graphrag_mcp).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "graphrag_mcp",
        "databases",
        "neo4j",
        "semantic search",
        "enables querying",
        "semantic graph"
      ],
      "category": "databases"
    },
    "rioriost--homebrew-age-mcp-server": {
      "owner": "rioriost",
      "name": "homebrew-age-mcp-server",
      "url": "https://github.com/rioriost/homebrew-age-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/rioriost.webp",
      "description": "Connect applications to a powerful graph database using Apache AGE, enabling relationship and pattern analysis in data. Support for dynamic data manipulation is provided through write operations.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T07:45:52Z",
      "readme_content": "# AGE-MCP-Server\n\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n![Python](https://img.shields.io/badge/Python-3.13%2B-blue)\n\nApache AGE MCP Server\n\n[Apache AGE™](https://age.apache.org/) is a PostgreSQL Graph database compatible with PostgreSQL's distributed assets and leverages graph data structures to analyze and use relationships and patterns in data.\n\n[Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/services/postgresql/) is a managed database service that is based on the open-source Postgres database engine.\n\n[Introducing support for Graph data in Azure Database for PostgreSQL (Preview)](https://techcommunity.microsoft.com/blog/adforpostgresql/introducing-support-for-graph-data-in-azure-database-for-postgresql-preview/4275628).\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Install](#install)\n- [Usage with Claude](#usage-with-claude)\n- [Usage with Visual Studio Code Insiders](#usage-with-visual-studio-code-insiders)\n- [Write Operations](#write-operations)\n- [Release Notes](#release-notes)\n- [For More Information](#for-more-information)\n- [License](#license)\n\n## Prerequisites\n\n- Python 3.13 and above\n- This module runs on [psycopg](https://www.psycopg.org/)\n- Enable the Apache AGE extension in your Azure Database for PostgreSQL instance. Login Azure Portal, go to 'server parameters' blade, and check 'AGE\" on within 'azure.extensions' and 'shared_preload_libraries' parameters. See, above blog post for more information.\n- Load the AGE extension in your PostgreSQL database.\n\n```sql\nCREATE EXTENSION IF NOT EXISTS age CASCADE;\n```\n\n- Claude\nDownload from [Claude Desktop Client](https://claude.ai/download) or,\n\n```bash\nbrew install claude\n```\n\n- Visual Studio Code Insiders\nDownload from [Visual Studio Code](https://code.visualstudio.com/download) or,\n\n```bash\nbrew intall visual-studio-code\n```\n\n## Install\n\n- with brew\n\n```bash\nbrew tap rioriost/age-mcp-server\nbrew install age-mcp-server\n```\n\n- with uv\n\n```bash\nuv init your_project\ncd your_project\nuv venv\nsource .venv/bin/activate\nuv add age-mcp-server\n```\n\n- with python venv on macOS / Linux\n\n```bash\nmkdir your_project\ncd your_project\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install age-mcp-server\n```\n\n- with python venv on Windows\n\n```bash\nmkdir your_project\ncd your_project\npython -m venv venv\n.\\venv\\Scripts\\activate\npython -m pip install age-mcp-server\n```\n\n## Usage with Claude\n\n- on macOS\n`claude_desktop_config.json` is located in `~/Library/Application Support/Claude/`.\n\n- on Windows\nYou need to create a new `claude_desktop_config.json` under `%APPDATA%\\Claude`.\n\n- Homebrew on macOS\n\nHomebrew installs `age-mcp-server` into $PATH.\n\n```json\n{\n  \"mcpServers\": {\n    \"age-manager\": {\n      \"command\": \"age-mcp-server\",\n      \"args\": [\n        \"--pg-con-str\",\n        \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username password=your_password\",\n      ]\n    }\n  }\n}\n```\n\n- uv / Pyhon venv\n\nOn macOS:\n\n```json\n{\n  \"mcpServers\": {\n    \"age-manager\": {\n      \"command\": \"/Users/your_username/.local/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/your_project\",\n        \"run\",\n        \"age-mcp-server\",\n        \"--pg-con-str\",\n        \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username password=your_password\",\n      ]\n    }\n  }\n}\n```\n\nOn Windows:\n\n```json\n{\n  \"mcpServers\": {\n    \"age-manager\": {\n      \"command\": \"C:\\\\Users\\\\USER\\\\.local\\\\bin\\\\uv.exe\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\path\\\\to\\\\your_project\",\n        \"run\",\n        \"age-mcp-server\",\n        \"--pg-con-str\",\n        \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username password=your_password\",\n      ]\n    }\n  }\n}\n```\n\nIf you need to hide the password or to use Entra ID, you can set `--pg-con-str` as follows.\n\n```\n{\n  \"mcpServers\": {\n    \"age-manager\": {\n        ...\n        \"--pg-con-str\",\n        \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username\",\n        ...\n      ]\n    }\n  }\n}\n```\n\nAnd, you need to set `PGPASSWORD` env variable, or to [install Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) and [sign into Azure](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli) with your Azure account.\n\nAfter saving `claude_desktop_config.json`, start Claude Desktop Client.\n\n![Show me graphs on the server](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_01.png)\n![Show me a graph schema of FROM_AGEFREIGHTER](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_02.png)\n![Pick up a customer and calculate the amount of its purchase.](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_03.png)\n![Find another customer buying more than Lisa](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_04.png)\n![OK. Please make a new graph named MCP_Test](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_05.png)\n![Make a node labeled 'Person' with properties, name=Rio, age=52](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_06.png)\n![Please make an another node labeled 'Company' with properties, name=Microsoft](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_07.png)\n![Can you put a relation, \"Rio WORK at Microsoft\"?](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_08.png)\n![Delete the graph, MCP_Test](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/query_09.png)\n\n![Claude on Windows](https://raw.githubusercontent.com/rioriost/homebrew-age-mcp-server/main/images/Claude_Win.png)\n\n## Usage with Visual Studio Code\n\nAfter installing, [Preferences]->[Settings] and input `mcp` to [Search settings].\n\n\n\nEdit the settings.json as followings:\n\n```json\n{\n    \"mcp\": {\n        \"inputs\": [],\n        \"servers\": {\n            \"age-manager\": {\n            \"command\": \"/Users/your_user_name/.local/bin/uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/your_project\",\n                \"run\",\n                \"age-mcp-server\",\n                \"--pg-con-str\",\n                \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username password=your_password\",\n                \"--debug\"\n            ]\n            }\n        }\n    }\n}\n```\n\nAnd then, you'll see `start` to start the AGE MCP Server.\n\nSwitch the Chat window to `agent` mode.\n\n\n\nNow, you can play with your graph data via Visual Studio Code!\n\n\n\n## Write Operations\n\nAGE-MCP-Server prohibits write operations by default for safety. If you want to enable write operations, you can use the `--allow-write` flag.\n\n```json\n{\n  \"mcpServers\": {\n    \"age-manager\": {\n      \"command\": \"age-mcp-server\",\n      \"args\": [\n        \"--pg-con-str\",\n        \"host=your_server.postgres.database.azure.com port=5432 dbname=postgres user=your_username password=your_password\",\n        \"--allow-write\"\n      ]\n    }\n  }\n}\n```\n\n## Release Notes\n\n### 0.2.26 Release\n- Dependency Update\n\n### 0.2.25 Release\n- Dependency Update\n\n### 0.2.24 Release\n- Dependency Update\n\n### 0.2.23 Release\n- Dependency Update\n\n### 0.2.22 Release\n- Dependency Update\n\n### 0.2.21 Release\n- Dependency Update\n\n### 0.2.20 Release\n- Dependency Update\n\n### 0.2.19 Release\n- Dependency Update\n\n### 0.2.18 Release\n- Dependency Update\n\n### 0.2.17 Release\n- Dependency Update\n\n### 0.2.16 Release\n- Dependency Update\n\n### 0.2.15 Release\n- Dependency Update\n\n### 0.2.14 Release\n- Dependency Update\n\n### 0.2.13 Release\n- Dependency Update\n\n### 0.2.12 Release\n- Dependency Update\n\n### 0.2.11 Release\n- Dependency Update\n\n### 0.2.10 Release\n- Dependency Update\n\n### 0.2.9 Release\n- Dependency Update\n\n### 0.2.8 Release\n- Add support for VSCode(Stable)\n\n### 0.2.7 Release\n- Add support for VSCode Insiders\n\n### 0.2.6 Release\n- Fix a typo\n\n### 0.2.5 Release\n- Support connection with Entra ID\n\n### 0.2.4 Release\n- Dependency Update\n\n### 0.2.3 Release\n- Dependency Update\n\n### 0.2.2 Release\n- Drop a conditional test of `CREATE` operation by adding `RETURN` to the description for `write-age-cypher` tool.\n\n### 0.2.1 Release\n- Fix a bug in node/edge creation\n\n### 0.2.0 Release\n- Add multiple graph support\n- Add graph creation and deletion support\n- Obsolete `--graph-name` argument\n\n### 0.1.8 Release\n- Add `--allow-write` flag\n\n### 0.1.7 Release\n- Add Windows support\n\n### 0.1.6 Release\n- Fix parser for `RETURN` values\n\n### 0.1.5 Release\n- Draft release\n\n### 0.1.4 Release\n- Draft release\n\n### 0.1.3 Release\n- Draft release\n\n### 0.1.2 Release\n- Draft release\n\n### 0.1.1 Release\n- Draft release\n\n### 0.1.0a1 Release\n- Draft release\n\n## For More Information\n\n- Apache AGE : https://age.apache.org/\n- GitHub : https://github.com/apache/age\n- Document : https://age.apache.org/age-manual/master/index.html\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "graph database"
      ],
      "category": "databases"
    },
    "robertoamoreno--couchdb-mcp-server": {
      "owner": "robertoamoreno",
      "name": "couchdb-mcp-server",
      "url": "https://github.com/robertoamoreno/couchdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/robertoamoreno.webp",
      "description": "Manage CouchDB databases and documents through a straightforward interface, facilitating operations like creating, listing, and deleting databases.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-09T08:18:02Z",
      "readme_content": "# couchdb-mcp-server\n[![smithery badge](https://smithery.ai/badge/@robertoamoreno/couchdb-mcp-server)](https://smithery.ai/server/@robertoamoreno/couchdb-mcp-server)\n\nA Model Context Protocol server for interacting with CouchDB\n\nThis is a TypeScript-based MCP server that provides tools for managing CouchDB databases and documents. It enables AI assistants to interact with CouchDB through a simple interface.\n\n## Features\n\n### Tools\n\n#### Base Tools (All CouchDB Versions)\n- `createDatabase` - Create a new CouchDB database\n  - Takes `dbName` as a required parameter\n  - Creates the database if it doesn't exist\n  \n- `listDatabases` - List all CouchDB databases\n  - Returns an array of database names\n  \n- `deleteDatabase` - Delete a CouchDB database\n  - Takes `dbName` as a required parameter\n  - Removes the specified database and all its documents\n  \n- `createDocument` - Create a new document or update an existing document in a database\n  - Required parameters:\n    - `dbName`: Database name\n    - `docId`: Document ID\n    - `data`: Document data (JSON object)\n      - For updates, include `_rev` field with the current document revision\n  - Returns:\n    - For new documents: document ID and new revision\n    - For updates: document ID and updated revision\n  - Automatically detects if operation is create or update based on presence of `_rev` field\n  \n- `getDocument` - Get a document from a database\n  - Required parameters:\n    - `dbName`: Database name\n    - `docId`: Document ID\n  - Returns the document content\n\n#### Mango Query Tools (CouchDB 3.x+ Only)\n- `createMangoIndex` - Create a new Mango index\n  - Required parameters:\n    - `dbName`: Database name\n    - `indexName`: Name of the index\n    - `fields`: Array of field names to index\n  - Creates a new index for efficient querying\n\n- `deleteMangoIndex` - Delete a Mango index\n  - Required parameters:\n    - `dbName`: Database name\n    - `designDoc`: Design document name\n    - `indexName`: Name of the index\n  - Removes an existing Mango index\n\n- `listMangoIndexes` - List all Mango indexes in a database\n  - Required parameters:\n    - `dbName`: Database name\n  - Returns information about all indexes in the database\n\n- `findDocuments` - Query documents using Mango query\n  - Required parameters:\n    - `dbName`: Database name\n    - `query`: Mango query object\n  - Performs a query using CouchDB's Mango query syntax\n\n## Version Support\n\nThe server automatically detects the CouchDB version and enables features accordingly:\n- All versions: Basic database and document operations\n- CouchDB 3.x+: Mango query support (indexes and queries)\n\n## Configuration\n\nThe server requires a CouchDB connection URL and version. These can be provided through environment variables:\n\n```bash\nCOUCHDB_URL=http://username:password@localhost:5984\nCOUCHDB_VERSION=1.7.2\n\nYou can create a `.env` file in the project root with this configuration. If not provided, it defaults to `http://localhost:5984`.\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install couchdb-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@robertoamoreno/couchdb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @robertoamoreno/couchdb-mcp-server --client claude\n```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"couchdb-mcp-server\": {\n      \"command\": \"/path/to/couchdb-mcp-server/build/index.js\",\n      \"env\": {\n        \"COUCHDB_URL\": \"http://username:password@localhost:5984\"\n      }\n    }\n  }\n}\n```\n\n### Prerequisites\n\n- Node.js 14 or higher\n- Running CouchDB instance\n- Proper CouchDB credentials if authentication is enabled\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Error Handling\n\nThe server includes robust error handling for common scenarios:\n- Invalid database names or document IDs\n- Database already exists/doesn't exist\n- Connection issues\n- Authentication failures\n- Invalid document data\n\nAll errors are properly formatted and returned through the MCP protocol with appropriate error codes and messages.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "couchdb",
        "databases",
        "database",
        "couchdb databases",
        "manage couchdb",
        "couchdb mcp"
      ],
      "category": "databases"
    },
    "robinong79--mcp-cosmos": {
      "owner": "robinong79",
      "name": "mcp-cosmos",
      "url": "https://github.com/robinong79/mcp-cosmos",
      "imageUrl": "/freedevtools/mcp/pfp/robinong79.webp",
      "description": "Enables querying of Azure Cosmos DB data using natural language through a conversational interface. Facilitates secure and efficient interactions between AI models and the database by translating user queries into database requests.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-28T12:54:49Z",
      "readme_content": "# Azure Cosmos DB MCP  Server\n\n<div align=\"center\">\n  \n</div>\n\n## What is this? 🤔\n\nThis is a server that lets your LLMs (like Claude) talk directly to your Azure Cosmos DB data! Think of it as a friendly translator that sits between your AI assistant and your database, making sure they can chat securely and efficiently.\n\n### Quick Example\n```text\nYou: \"What were our top 10 customers last month?\"\nClaude: *queries your Azure Cosmos DB database and gives you the answer in plain English*\n```\n\n\n## How Does It Work? 🛠️\n\nThis server leverages the Model Context Protocol (MCP), a versatile framework that acts as a universal translator between AI models and databases. Although MCP is built to support any AI model, it is currently accessible as a developer preview in Claude Desktop.\n\nHere's all you need to do:\n1. Set up project (see below)\n2. Add your project details to Claude Desktop's config file\n3. Start chatting with your Azure Cosmos DB data naturally!\n\n### What Can It Do? 📊\n\n- Run Azure Cosmos DB queries by just asking questions in plain English\n\n## Quick Start 🚀\n\n### Prerequisites\n- Node.js 14 or higher\n- Azure Cosmos DB NOSQL account or Azure Cosmos DB Emulator\n- Claude Desktop \n\n### Set up project\n\n- Obtain Azure Cosmos DB NOSQL account URI and the KEY from the keys section and create an '.env' file with the below key and replace the values\n\n```\nCOSMOSDB_URI=\nCOSMOSDB_KEY= \n```\n\n### Getting Started\n\n1. **Install Dependencies**  \n   Run the following command in the root folder to install all necessary dependencies:  \n   ```bash\n   npm install\n   ```\n\n2. **Build the Project**  \n   Compile the project by running:  \n   ```bash\n   npm run build\n   ```\n\n3. **Start the Server**  \n   Navigate to the `dist` folder and start the server:  \n   ```bash\n   npm start\n   ```\n\n4. **Confirmation Message**  \n   You should see the following message:  \n   ```\n   Azure Cosmos DB Server running on stdio\n   ```\n\n### Add your project details to Claude Destkop's config file\n\nOpen Claude Desktop and Navigate to File -> Settings -> Developer -> Edit Config and open the `claude_desktop_config` file and replace with the values below,\n\n```json\n{\n  \"mcpServers\": {\n    \"cosmosdb\": {\n      \"command\": \"node\",\n      \"args\": [ \"C:/Cosmos/azure-cosmos-mcp/dist/index.js\" ] // Your Path for the Azure Cosmos DB MCP server file,\n      \"env\": {\n        \"COSMOSDB_URI\": \"Your Cosmos DB Account URI\",\n        \"COSMOSDB_KEY\": \"Your Cosmos DB KEY\"\n      }\n    }\n  }\n}\n\n```\n\nYou should now have successfully configured the MCP server for Azure Cosmos DB with Claude Desktop. This setup allows you to seamlessly interact with Azure Cosmos DB through the MCP server as shown below.\n\n\n\n\nhttps://github.com/user-attachments/assets/ae3a14f3-9ca1-415d-8645-1c8367fd6943\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cosmos",
        "databases",
        "db",
        "cosmos db",
        "azure cosmos",
        "cosmos enables"
      ],
      "category": "databases"
    },
    "roboulos--simple-xano-mcp": {
      "owner": "roboulos",
      "name": "simple-xano-mcp",
      "url": "https://github.com/roboulos/simple-xano-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/roboulos.webp",
      "description": "Integrate with Xano databases through a simple API for querying and manipulating database structures and records. Supports local deployment for AI assistants and features comprehensive logging for troubleshooting.",
      "stars": 2,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-05T16:53:27Z",
      "readme_content": "# Xano MCP Python SDK\n\nA Python-based MCP (Model Context Protocol) server for Xano that allows AI assistants like Claude to interact directly with your Xano instance. This is a standalone version optimized for local use with Claude Desktop and other MCP-compatible LLMs.\n\n## 🌟 Features\n\n- **Simple Authentication**: Connect with your Xano API token\n- **Comprehensive API**: Query and manipulate Xano instances, databases, tables, and records\n- **Local Deployment**: Run as a local MCP server for Claude Desktop or other clients\n- **Detailed Logging**: Troubleshoot issues with comprehensive logging\n- **Portable**: Works on macOS, Windows, and Linux\n\n## 🚀 Quick Start\n\n1. **Clone this repository**:\n   ```bash\n   git clone https://github.com/yourusername/xano-mcp-python.git\n   cd xano-mcp-python\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Configure Claude Desktop** (if using):\n   \n   Edit your Claude Desktop config file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   Add this configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"xano\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/path/to/xano-mcp-python/xano_mcp_sdk.py\"\n         ],\n         \"env\": {\n           \"XANO_API_TOKEN\": \"your-xano-api-token\"\n         }\n       }\n     }\n   }\n   ```\n\n4. **Run the installation script**:\n   ```bash\n   # On macOS/Linux\n   ./install.sh\n   \n   # On Windows\n   install.bat\n   ```\n\n5. **Test the installation**:\n   ```bash\n   ./test.py\n   ```\n\n## 💡 Usage Examples\n\nOnce installed, you can use it with Claude or any MCP-compatible assistant. Here are some examples:\n\n- **List your Xano instances**:\n  > What Xano instances do I have?\n\n- **Check database tables**:\n  > Show me all tables in my Xano instance \"my-instance\"\n\n- **Create a new table**:\n  > Create a new table called \"products\" in my Xano instance \"my-instance\"\n\n- **Examine table structure**:\n  > What's the schema for the \"users\" table?\n\n- **Query records**:\n  > Show me the first 5 records in the \"users\" table\n\n## 🧰 Available Tools\n\n### Instance Management\n- List instances\n- Get instance details\n- Check instance status\n\n### Database Operations\n- List databases/workspaces\n- Get workspace details\n- Database schema management\n\n### Table Operations\n- Create, update, delete tables\n- Add, modify, and remove fields\n- Index management\n\n### Record Management\n- Create, read, update, delete records\n- Bulk operations\n- Complex queries\n\n### File Operations\n- List and manage files\n- Upload and download\n\n### API Tools\n- API group management\n- API endpoint creation and configuration\n\n## 🔧 Advanced Configuration\n\n### Environment Variables\n\n- `XANO_API_TOKEN`: Your Xano API token (required)\n- `XANO_LOG_LEVEL`: Set log level (default: INFO)\n- `XANO_DEFAULT_INSTANCE`: Default instance to use when not specified\n\n### Command Line Options\n\n```bash\npython xano_mcp_sdk.py --token YOUR_TOKEN --log-level DEBUG\n```\n\n### Logging\n\nLogs are written to:\n- macOS: `~/Library/Logs/Claude/mcp*.log`\n- Windows: `%APPDATA%\\Claude\\logs\\mcp*.log`\n\nFor direct console output, run:\n```bash\npython xano_mcp_sdk.py --console-logging\n```\n\n## 🚨 Troubleshooting\n\nIf you encounter issues:\n\n1. **Check logs** for errors:\n   ```bash\n   # macOS\n   tail -n 100 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type \"%APPDATA%\\Claude\\logs\\mcp*.log\"\n   ```\n\n2. **Verify API token** is correct and has appropriate permissions\n\n3. **Check network connectivity** to Xano servers\n\n4. **Ensure Python environment** is properly set up\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🙏 Acknowledgements\n\n- Xano for their excellent database platform\n- Anthropic for the Model Context Protocol specification\n- Contributors and testers who helped refine this SDK",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xano",
        "databases",
        "database",
        "xano databases",
        "simple xano",
        "xano mcp"
      ],
      "category": "databases"
    },
    "rock913--mongo-mcp": {
      "owner": "rock913",
      "name": "mongo-mcp",
      "url": "https://github.com/rock913/mongo-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rock913.webp",
      "description": "Enables interaction with MongoDB databases, facilitating collection schema inspection, document querying, and basic data management through natural language commands. Streamlines operations like document insertion, updating, and deletion.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-18T02:15:50Z",
      "readme_content": "# 🗄️ MongoDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/mongo-mcp)](https://smithery.ai/server/mongo-mcp)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with MongoDB databases. Query collections, inspect schemas, and manage data seamlessly through natural language.\n\n## ✨ Features\n\n- 🔍 Collection schema inspection\n- 📊 Document querying and filtering\n- 📈 Index management\n- 📝 Document operations (insert, update, delete)\n\n## Demo Video\n\n\nhttps://github.com/user-attachments/assets/2389bf23-a10d-49f9-bca9-2b39a1ebe654\n\n\n\n\n## 🚀 Quick Start\n\nTo get started, find your mongodb connection url and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mongo-mcp\",\n        \"mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install MongoDB MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mongo-mcp):\n\n```bash\nnpx -y @smithery/cli install mongo-mcp --client claude\n```\n\n### Prerequisites\n\n- Node.js 18+\n- npx\n- Docker and Docker Compose (for local sandbox testing only)\n- MCP Client (Claude Desktop App for example)\n\n### Test Sandbox Setup\n\nIf you don't have a mongo db server to connect to and want to create a sample sandbox, follow these steps\n\n1. Start MongoDB using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n2. Seed the database with test data:\n\n```bash\nnpm run seed\n```\n\n### Configure Claude Desktop\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"dist/index.js\",\n        \"mongodb://root:example@localhost:27017/test?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Test Sandbox Data Structure\n\nThe seed script creates three collections with sample data:\n\n#### Users\n\n- Personal info (name, email, age)\n- Nested address with coordinates\n- Arrays of interests\n- Membership dates\n\n#### Products\n\n- Product details (name, SKU, category)\n- Nested specifications\n- Price and inventory info\n- Tags and ratings\n\n#### Orders\n\n- Order details with items\n- User references\n- Shipping and payment info\n- Status tracking\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n### Basic Operations\n\n```plaintext\n\"What collections are available in the database?\"\n\"Show me the schema for the users collection\"\n\"Find all users in San Francisco\"\n```\n\n### Advanced Queries\n\n```plaintext\n\"Find all electronics products that are in stock and cost less than $1000\"\n\"Show me all orders from the user john@example.com\"\n\"List the products with ratings above 4.5\"\n```\n\n### Index Management\n\n```plaintext\n\"What indexes exist on the users collection?\"\n\"Create an index on the products collection for the 'category' field\"\n\"List all indexes across all collections\"\n```\n\n### Document Operations\n\n```plaintext\n\"Insert a new product with name 'Gaming Laptop' in the products collection\"\n\"Update the status of order with ID X to 'shipped'\"\n\"Find and delete all products that are out of stock\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for database interaction:\n\n### Query Tools\n\n- `find`: Query documents with filtering and projection\n- `listCollections`: List available collections\n- `insertOne`: Insert a single document\n- `updateOne`: Update a single document\n- `deleteOne`: Delete a single document\n\n### Index Tools\n\n- `createIndex`: Create a new index\n- `dropIndex`: Remove an index\n- `indexes`: List indexes for a collection\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "mongo",
        "databases",
        "mongodb databases",
        "mongo mcp",
        "interaction mongodb"
      ],
      "category": "databases"
    },
    "rtcface--first_mcp": {
      "owner": "rtcface",
      "name": "first_mcp",
      "url": "https://github.com/rtcface/first_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/rtcface.webp",
      "description": "Enable structured querying and management of MongoDB collections through a standardized protocol interface, allowing users to list collections, retrieve documents with filtering and projections, and manage data operations securely with logging and error handling.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-22T23:05:34Z",
      "readme_content": "# MCP MongoDB Server\n\nThis server implements a Model Context Protocol (MCP) interface for MongoDB, allowing interaction with MongoDB collections and documents through standardized MCP requests.\n\n## Features\n\n### Resource Management\n\n- **List Collections**: Lists all MongoDB collections as MCP resources\n  - Each collection is represented with a `mongodb://` URI scheme\n  - Returns collection names and metadata in MCP resource format\n\n### Document Operations\n\n- **Read Documents**: Retrieve documents from MongoDB collections\n  - Access collections using `mongodb://collection-name` URI format\n  - Supports filtering and projection of documents\n  - Default limit of 100 documents per request\n\n### Tools\n\n- **Query Builder**: Structured querying of MongoDB collections\n  - Specify collection name\n  - Apply filters and projections\n  - Configure result limits\n\n### Security & Logging\n\n- Secure MongoDB connection handling\n- Detailed operation logging to `logs/server.log`\n- Connection error handling and reporting\n- Input validation for collection names and queries\n\n### Configuration\n\n- MongoDB connection via environment variables (`MONGODB_URI`)\n- Configurable client options for performance and security\n- Logging system with timestamp and error tracking\n\n## Technical Details\n\n- Built with `@modelcontextprotocol/sdk` version 1.10.2\n- Uses MongoDB Node.js driver version 6.16.0\n- Implements MCP server capabilities for resources and tools\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute MongoDB queries with filtering and projection\n  - Input parameters:\n    - `collection`: Name of collection to query\n    - `filter`: MongoDB query filter (optional)\n    - `projection`: Fields to include/exclude (optional)\n    - `limit`: Maximum number of documents to return (default 100)\n\n### Resources\n\nThe server provides access to MongoDB collections as resources:\n\n- **Collections** (`mongodb://<collection-name>`)\n  - Each collection is exposed as a resource\n  - Documents are returned in JSON format\n  - Supports filtering and projection via query tool\n\n## Configuration\n\n### Usage with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your `claude_desktop_config.json`:\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "management mongodb",
        "mongodb collections",
        "access schema"
      ],
      "category": "databases"
    },
    "runekaagaard--mcp-alchemy": {
      "owner": "runekaagaard",
      "name": "mcp-alchemy",
      "url": "https://github.com/runekaagaard/mcp-alchemy",
      "imageUrl": "/freedevtools/mcp/pfp/runekaagaard.webp",
      "description": "Connects Claude to databases for querying and analysis, facilitating SQL query writing, visualization of table relationships, and report generation.",
      "stars": 339,
      "forks": 53,
      "license": "Mozilla Public License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T17:14:36Z",
      "readme_content": "# MCP Alchemy\n\n<a href=\"https://www.pulsemcp.com/servers/runekaagaard-alchemy\"><img src=\"https://www.pulsemcp.com/badge/top-pick/runekaagaard-alchemy\" width=\"400\" alt=\"PulseMCP Badge\"></a>\n\n**Status: Works great and is in daily use without any known bugs.**\n\n**Status2: I just added the package to PyPI and updated the usage instructions. Please report any issues :)**\n\nLet Claude be your database expert! MCP Alchemy connects Claude Desktop directly to your databases, allowing it to:\n\n- Help you explore and understand your database structure\n- Assist in writing and validating SQL queries\n- Displays relationships between tables\n- Analyze large datasets and create reports\n- Claude Desktop Can analyse and create artifacts for very large datasets using [claude-local-files](https://github.com/runekaagaard/claude-local-files).\n\nWorks with PostgreSQL, MySQL, MariaDB, SQLite, Oracle, MS SQL Server, CrateDB, Vertica,\nand a host of other [SQLAlchemy-compatible](https://docs.sqlalchemy.org/en/20/dialects/) databases.\n\n![MCP Alchemy in action](https://raw.githubusercontent.com/runekaagaard/mcp-alchemy/refs/heads/main/screenshot.png)\n\n## Installation\n\nEnsure you have uv installed:\n```bash\n# Install uv if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n## Usage with Claude Desktop\n\nAdd to your `claude_desktop_config.json`. You need to add the appropriate database driver in the ``--with`` parameter.\n\n_Note: After a new version release there might be a period of up to 600 seconds while the cache clears locally \ncached causing uv to raise a versioning error. Restarting the MCP client once again solves the error._\n\n### SQLite (built into Python)\n```json\n{\n  \"mcpServers\": {\n    \"my_sqlite_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"sqlite:////absolute/path/to/database.db\"\n      }\n    }\n  }\n}\n```\n\n### PostgreSQL\n```json\n{\n  \"mcpServers\": {\n    \"my_postgres_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"psycopg2-binary\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"postgresql://user:password@localhost/dbname\"\n      }\n    }\n  }\n}\n```\n\n### MySQL/MariaDB\n```json\n{\n  \"mcpServers\": {\n    \"my_mysql_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"pymysql\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"mysql+pymysql://user:password@localhost/dbname\"\n      }\n    }\n  }\n}\n```\n\n### Microsoft SQL Server\n```json\n{\n  \"mcpServers\": {\n    \"my_mssql_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"pymssql\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"mssql+pymssql://user:password@localhost/dbname\"\n      }\n    }\n  }\n}\n```\n\n### Oracle\n```json\n{\n  \"mcpServers\": {\n    \"my_oracle_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"oracledb\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"oracle+oracledb://user:password@localhost/dbname\"\n      }\n    }\n  }\n}\n```\n\n### CrateDB\n```json\n{\n  \"mcpServers\": {\n    \"my_cratedb\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"sqlalchemy-cratedb>=0.42.0.dev1\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"crate://user:password@localhost:4200/?schema=testdrive\"\n      }\n    }\n  }\n}\n```\nFor connecting to CrateDB Cloud, use a URL like\n`crate://user:password@example.aks1.westeurope.azure.cratedb.net:4200?ssl=true`.\n\n### Vertica\n```json\n{\n  \"mcpServers\": {\n    \"my_vertica_db\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"mcp-alchemy==2025.8.15.91819\", \"--with\", \"vertica-python\",\n               \"--refresh-package\", \"mcp-alchemy\", \"mcp-alchemy\"],\n      \"env\": {\n        \"DB_URL\": \"vertica+vertica_python://user:password@localhost:5433/dbname\",\n        \"DB_ENGINE_OPTIONS\": \"{\\\"connect_args\\\": {\\\"ssl\\\": false}}\"\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n- `DB_URL`: SQLAlchemy [database URL](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls) (required)\n- `CLAUDE_LOCAL_FILES_PATH`: Directory for full result sets (optional)\n- `EXECUTE_QUERY_MAX_CHARS`: Maximum output length (optional, default 4000)\n- `DB_ENGINE_OPTIONS`: JSON string containing additional SQLAlchemy engine options (optional)\n\n## Connection Pooling\n\nMCP Alchemy uses connection pooling optimized for long-running MCP servers. The default settings are:\n\n- `pool_pre_ping=True`: Tests connections before use to handle database timeouts and network issues\n- `pool_size=1`: Maintains 1 persistent connection (MCP servers typically handle one request at a time)\n- `max_overflow=2`: Allows up to 2 additional connections for burst capacity\n- `pool_recycle=3600`: Refreshes connections older than 1 hour (prevents timeout issues)\n- `isolation_level='AUTOCOMMIT'`: Ensures each query commits automatically\n\nThese defaults work well for most databases, but you can override them via `DB_ENGINE_OPTIONS`:\n\n```json\n{\n  \"DB_ENGINE_OPTIONS\": \"{\\\"pool_size\\\": 5, \\\"max_overflow\\\": 10, \\\"pool_recycle\\\": 1800}\"\n}\n```\n\nFor databases with aggressive timeout settings (like MySQL's 8-hour default), the combination of `pool_pre_ping` and `pool_recycle` ensures reliable connections.\n\n## API\n\n### Tools\n\n- **all_table_names**\n  - Return all table names in the database\n  - No input required\n  - Returns comma-separated list of tables\n  ```\n  users, orders, products, categories\n  ```\n\n- **filter_table_names**\n  - Find tables matching a substring\n  - Input: `q` (string)\n  - Returns matching table names\n  ```\n  Input: \"user\"\n  Returns: \"users, user_roles, user_permissions\"\n  ```\n\n- **schema_definitions**\n  - Get detailed schema for specified tables\n  - Input: `table_names` (string[])\n  - Returns table definitions including:\n    - Column names and types\n    - Primary keys\n    - Foreign key relationships\n    - Nullable flags\n  ```\n  users:\n      id: INTEGER, primary key, autoincrement\n      email: VARCHAR(255), nullable\n      created_at: DATETIME\n      \n      Relationships:\n        id -> orders.user_id\n  ```\n\n- **execute_query**\n  - Execute SQL query with vertical output format\n  - Inputs:\n    - `query` (string): SQL query\n    - `params` (object, optional): Query parameters\n  - Returns results in clean vertical format:\n  ```\n  1. row\n  id: 123\n  name: John Doe\n  created_at: 2024-03-15T14:30:00\n  email: NULL\n\n  Result: 1 rows\n  ```\n  - Features:\n    - Smart truncation of large results\n    - Full result set access via [claude-local-files](https://github.com/runekaagaard/claude-local-files) integration\n    - Clean NULL value display\n    - ISO formatted dates\n    - Clear row separation\n\n## Claude Local Files\n\nWhen [claude-local-files](https://github.com/runekaagaard/claude-local-files) is configured:\n\n- Access complete result sets beyond Claude's context window\n- Generate detailed reports and visualizations\n- Perform deep analysis on large datasets\n- Export results for further processing\n\nThe integration automatically activates when `CLAUDE_LOCAL_FILES_PATH` is set.\n\n## Developing\n\nFirst clone the github repository, install the dependencies and your database driver(s) of choice:\n\n```\ngit clone git@github.com:runekaagaard/mcp-alchemy.git\ncd mcp-alchemy\nuv sync\nuv pip install psycopg2-binary\n```\n\nThen set this in claude_desktop_config.json:\n\n```\n...\n\"command\": \"uv\",\n\"args\": [\"run\", \"--directory\", \"/path/to/mcp-alchemy\", \"-m\", \"mcp_alchemy.server\", \"main\"],\n...\n```\n\n## My Other LLM Projects\n\n- **[MCP Redmine](https://github.com/runekaagaard/mcp-redmine)** - Let Claude Desktop manage your Redmine projects and issues.\n- **[MCP Notmuch Sendmail](https://github.com/runekaagaard/mcp-notmuch-sendmail)** - Email assistant for Claude Desktop using notmuch.\n- **[Diffpilot](https://github.com/runekaagaard/diffpilot)** - Multi-column git diff viewer with file grouping and tagging.\n- **[Claude Local Files](https://github.com/runekaagaard/claude-local-files)** - Access local files in Claude Desktop artifacts.\n\n## MCP Directory Listings\n\nMCP Alchemy is listed in the following MCP directory sites and repositories:\n\n- [PulseMCP](https://www.pulsemcp.com/servers/runekaagaard-alchemy)\n- [Glama](https://glama.ai/mcp/servers/@runekaagaard/mcp-alchemy)\n- [MCP.so](https://mcp.so/server/mcp-alchemy)\n- [MCP Archive](https://mcp-archive.com/server/mcp-alchemy)\n- [Playbooks MCP](https://playbooks.com/mcp/runekaagaard-alchemy)\n- [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers)\n\n## Contributing\n\nContributions are warmly welcomed! Whether it's bug reports, feature requests, documentation improvements, or code contributions - all input is valuable. Feel free to:\n\n- Open an issue to report bugs or suggest features\n- Submit pull requests with improvements\n- Enhance documentation or share your usage examples\n- Ask questions and share your experiences\n\nThe goal is to make database interaction with Claude even better, and your insights and contributions help achieve that.\n\n## License\n\nMozilla Public License Version 2.0\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "s2-streamstore--s2-sdk-typescript": {
      "owner": "s2-streamstore",
      "name": "s2-sdk-typescript",
      "url": "https://github.com/s2-streamstore/s2-sdk-typescript",
      "imageUrl": "",
      "description": "Official MCP server for the S2.dev serverless stream platform.",
      "stars": 13,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T22:59:24Z",
      "readme_content": "# streamstore\n\nDeveloper-friendly & type-safe TypeScript SDK specifically catered to leverage *streamstore* API.\n\n<div align=\"left\">\n    <a href=\"https://www.speakeasy.com/?utm_source=streamstore&utm_campaign=typescript\"><img alt=\"Built_By_Speakeasy_212015_style_for_the_badge_logoColor_FBE331_logo_speakeasy_labelColor_545454\" src=\"https://custom-icon-badges.demolab.com/badge/-Built%20By%20Speakeasy-212015?style=for-the-badge&logoColor=FBE331&logo=speakeasy&labelColor=545454\" /></a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n        <img alt=\"License_MIT_blue\" src=\"https://img.shields.io/badge/License-MIT-blue.svg\" style=\"width: 100px; height: 28px;\" />\n    </a>\n</div>\n\n\n<br /><br />\n<!-- Start Summary [summary] -->\n## Summary\n\nS2 API: Serverless API for streaming data backed by object storage.\n<!-- End Summary [summary] -->\n\n<!-- Start Table of Contents [toc] -->\n## Table of Contents\n<!-- $toc-max-depth=2 -->\n* [streamstore](#streamstore)\n  * [SDK Installation](#sdk-installation)\n  * [Requirements](#requirements)\n  * [SDK Example Usage](#sdk-example-usage)\n  * [Authentication](#authentication)\n  * [Available Resources and Operations](#available-resources-and-operations)\n  * [Standalone functions](#standalone-functions)\n  * [Server-sent event streaming](#server-sent-event-streaming)\n  * [Pagination](#pagination)\n  * [Retries](#retries)\n  * [Error Handling](#error-handling)\n  * [Server Selection](#server-selection)\n  * [Custom HTTP Client](#custom-http-client)\n  * [Debugging](#debugging)\n* [Development](#development)\n  * [Maturity](#maturity)\n  * [Contributions](#contributions)\n\n<!-- End Table of Contents [toc] -->\n\n<!-- Start SDK Installation [installation] -->\n## SDK Installation\n\nThe SDK can be installed with either [npm](https://www.npmjs.com/), [pnpm](https://pnpm.io/), [bun](https://bun.sh/) or [yarn](https://classic.yarnpkg.com/en/) package managers.\n\n### NPM\n\n```bash\nnpm add @s2-dev/streamstore\n```\n\n### PNPM\n\n```bash\npnpm add @s2-dev/streamstore\n```\n\n### Bun\n\n```bash\nbun add @s2-dev/streamstore\n```\n\n### Yarn\n\n```bash\nyarn add @s2-dev/streamstore\n```\n\n> [!NOTE]\n> This package is published with CommonJS and ES Modules (ESM) support.\n\n\n### Model Context Protocol (MCP) Server\n\nThis SDK is also an installable MCP server where the various SDK methods are\nexposed as tools that can be invoked by AI applications.\n\n> Node.js v20 or greater is required to run the MCP server from npm.\n\n<details>\n<summary>Claude installation steps</summary>\n\nAdd the following server definition to your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"S2\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\", \"--package\", \"@s2-dev/streamstore\",\n        \"--\",\n        \"mcp\", \"start\",\n        \"--access-token\", \"...\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Cursor installation steps</summary>\n\nCreate a `.cursor/mcp.json` file in your project root with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"S2\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\", \"--package\", \"@s2-dev/streamstore\",\n        \"--\",\n        \"mcp\", \"start\",\n        \"--access-token\", \"...\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\nYou can also run MCP servers as a standalone binary with no additional dependencies. You must pull these binaries from available Github releases:\n\n```bash\ncurl -L -o mcp-server \\\n    https://github.com/{org}/{repo}/releases/download/{tag}/mcp-server-bun-darwin-arm64 && \\\nchmod +x mcp-server\n```\n\nIf the repo is a private repo you must add your Github PAT to download a release `-H \"Authorization: Bearer {GITHUB_PAT}\"`.\n\n\n```json\n{\n  \"mcpServers\": {\n    \"Todos\": {\n      \"command\": \"./DOWNLOAD/PATH/mcp-server\",\n      \"args\": [\n        \"start\"\n      ]\n    }\n  }\n}\n```\n\nFor a full list of server arguments, run:\n\n```sh\nnpx -y --package @s2-dev/streamstore -- mcp start --help\n```\n<!-- End SDK Installation [installation] -->\n\n<!-- Start Requirements [requirements] -->\n## Requirements\n\nFor supported JavaScript runtimes, please consult [RUNTIMES.md](RUNTIMES.md).\n<!-- End Requirements [requirements] -->\n\n<!-- Start SDK Example Usage [usage] -->\n## SDK Example Usage\n\n### Example\n\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.accessTokens.listAccessTokens({});\n\n  console.log(result);\n}\n\nrun();\n\n```\n<!-- End SDK Example Usage [usage] -->\n\n<!-- Start Authentication [security] -->\n## Authentication\n\n### Per-Client Security Schemes\n\nThis SDK supports the following security scheme globally:\n\n| Name          | Type | Scheme      | Environment Variable |\n| ------------- | ---- | ----------- | -------------------- |\n| `accessToken` | http | HTTP Bearer | `S2_ACCESS_TOKEN`    |\n\nTo authenticate with the API the `accessToken` parameter must be set when initializing the SDK client instance. For example:\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.accessTokens.listAccessTokens({});\n\n  console.log(result);\n}\n\nrun();\n\n```\n<!-- End Authentication [security] -->\n\n<!-- Start Available Resources and Operations [operations] -->\n## Available Resources and Operations\n\n<details open>\n<summary>Available methods</summary>\n\n### [accessTokens](docs/sdks/accesstokens/README.md)\n\n* [listAccessTokens](docs/sdks/accesstokens/README.md#listaccesstokens) - List access tokens.\n* [issueAccessToken](docs/sdks/accesstokens/README.md#issueaccesstoken) - Issue a new access token.\n* [revokeAccessToken](docs/sdks/accesstokens/README.md#revokeaccesstoken) - Revoke an access token.\n\n### [basins](docs/sdks/basins/README.md)\n\n* [listBasins](docs/sdks/basins/README.md#listbasins) - List basins.\n* [createBasin](docs/sdks/basins/README.md#createbasin) - Create a basin.\n* [getBasinConfig](docs/sdks/basins/README.md#getbasinconfig) - Get basin configuration.\n* [createOrReconfigureBasin](docs/sdks/basins/README.md#createorreconfigurebasin) - Create or reconfigure a basin.\n* [deleteBasin](docs/sdks/basins/README.md#deletebasin) - Delete a basin.\n* [reconfigureBasin](docs/sdks/basins/README.md#reconfigurebasin) - Reconfigure a basin.\n\n### [metrics](docs/sdks/metrics/README.md)\n\n* [accountMetrics](docs/sdks/metrics/README.md#accountmetrics) - Account-level metrics.\n* [basinMetrics](docs/sdks/metrics/README.md#basinmetrics) - Basin-level metrics.\n* [streamMetrics](docs/sdks/metrics/README.md#streammetrics) - Stream-level metrics.\n\n### [records](docs/sdks/records/README.md)\n\n* [read](docs/sdks/records/README.md#read) - Read records.\n* [append](docs/sdks/records/README.md#append) - Append records.\n* [checkTail](docs/sdks/records/README.md#checktail) - Check the tail.\n\n\n### [streams](docs/sdks/streams/README.md)\n\n* [listStreams](docs/sdks/streams/README.md#liststreams) - List streams.\n* [createStream](docs/sdks/streams/README.md#createstream) - Create a stream.\n* [getStreamConfig](docs/sdks/streams/README.md#getstreamconfig) - Get stream configuration.\n* [createOrReconfigureStream](docs/sdks/streams/README.md#createorreconfigurestream) - Create or reconfigure a stream.\n* [deleteStream](docs/sdks/streams/README.md#deletestream) - Delete a stream.\n* [reconfigureStream](docs/sdks/streams/README.md#reconfigurestream) - Reconfigure a stream.\n\n</details>\n<!-- End Available Resources and Operations [operations] -->\n\n<!-- Start Standalone functions [standalone-funcs] -->\n## Standalone functions\n\nAll the methods listed above are available as standalone functions. These\nfunctions are ideal for use in applications running in the browser, serverless\nruntimes or other environments where application bundle size is a primary\nconcern. When using a bundler to build your application, all unused\nfunctionality will be either excluded from the final bundle or tree-shaken away.\n\nTo read more about standalone functions, check [FUNCTIONS.md](./FUNCTIONS.md).\n\n<details>\n\n<summary>Available standalone functions</summary>\n\n- [`accessTokensIssueAccessToken`](docs/sdks/accesstokens/README.md#issueaccesstoken) - Issue a new access token.\n- [`accessTokensListAccessTokens`](docs/sdks/accesstokens/README.md#listaccesstokens) - List access tokens.\n- [`accessTokensRevokeAccessToken`](docs/sdks/accesstokens/README.md#revokeaccesstoken) - Revoke an access token.\n- [`basinsCreateBasin`](docs/sdks/basins/README.md#createbasin) - Create a basin.\n- [`basinsCreateOrReconfigureBasin`](docs/sdks/basins/README.md#createorreconfigurebasin) - Create or reconfigure a basin.\n- [`basinsDeleteBasin`](docs/sdks/basins/README.md#deletebasin) - Delete a basin.\n- [`basinsGetBasinConfig`](docs/sdks/basins/README.md#getbasinconfig) - Get basin configuration.\n- [`basinsListBasins`](docs/sdks/basins/README.md#listbasins) - List basins.\n- [`basinsReconfigureBasin`](docs/sdks/basins/README.md#reconfigurebasin) - Reconfigure a basin.\n- [`metricsAccountMetrics`](docs/sdks/metrics/README.md#accountmetrics) - Account-level metrics.\n- [`metricsBasinMetrics`](docs/sdks/metrics/README.md#basinmetrics) - Basin-level metrics.\n- [`metricsStreamMetrics`](docs/sdks/metrics/README.md#streammetrics) - Stream-level metrics.\n- [`recordsAppend`](docs/sdks/records/README.md#append) - Append records.\n- [`recordsCheckTail`](docs/sdks/records/README.md#checktail) - Check the tail.\n- [`recordsRead`](docs/sdks/records/README.md#read) - Read records.\n- [`streamsCreateOrReconfigureStream`](docs/sdks/streams/README.md#createorreconfigurestream) - Create or reconfigure a stream.\n- [`streamsCreateStream`](docs/sdks/streams/README.md#createstream) - Create a stream.\n- [`streamsDeleteStream`](docs/sdks/streams/README.md#deletestream) - Delete a stream.\n- [`streamsGetStreamConfig`](docs/sdks/streams/README.md#getstreamconfig) - Get stream configuration.\n- [`streamsListStreams`](docs/sdks/streams/README.md#liststreams) - List streams.\n- [`streamsReconfigureStream`](docs/sdks/streams/README.md#reconfigurestream) - Reconfigure a stream.\n\n</details>\n<!-- End Standalone functions [standalone-funcs] -->\n\n<!-- Start Server-sent event streaming [eventstream] -->\n## Server-sent event streaming\n\n[Server-sent events][mdn-sse] are used to stream content from certain\noperations. These operations will expose the stream as an async iterable that\ncan be consumed using a [`for await...of`][mdn-for-await-of] loop. The loop will\nterminate when the server no longer has any events to send and closes the\nunderlying connection.\n\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.records.read({\n    stream: \"<value>\",\n    s2Basin: \"<value>\",\n  });\n\n  console.log(result);\n}\n\nrun();\n\n```\n\n[mdn-sse]: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\n[mdn-for-await-of]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of\n<!-- End Server-sent event streaming [eventstream] -->\n\n<!-- Start Pagination [pagination] -->\n## Pagination\n\nSome of the endpoints in this SDK support pagination. To use pagination, you\nmake your SDK calls as usual, but the returned response object will also be an\nasync iterable that can be consumed using the [`for await...of`][for-await-of]\nsyntax.\n\n[for-await-of]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of\n\nHere's an example of one such pagination call:\n\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.basins.listBasins({});\n\n  for await (const page of result) {\n    console.log(page);\n  }\n}\n\nrun();\n\n```\n<!-- End Pagination [pagination] -->\n\n<!-- Start Retries [retries] -->\n## Retries\n\nSome of the endpoints in this SDK support retries.  If you use the SDK without any configuration, it will fall back to the default retry strategy provided by the API.  However, the default retry strategy can be overridden on a per-operation basis, or across the entire SDK.\n\nTo change the default retry strategy for a single API call, simply provide a retryConfig object to the call:\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.accessTokens.listAccessTokens({}, {\n    retries: {\n      strategy: \"backoff\",\n      backoff: {\n        initialInterval: 1,\n        maxInterval: 50,\n        exponent: 1.1,\n        maxElapsedTime: 100,\n      },\n      retryConnectionErrors: false,\n    },\n  });\n\n  console.log(result);\n}\n\nrun();\n\n```\n\nIf you'd like to override the default retry strategy for all operations that support retries, you can provide a retryConfig at SDK initialization:\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  retryConfig: {\n    strategy: \"backoff\",\n    backoff: {\n      initialInterval: 1,\n      maxInterval: 50,\n      exponent: 1.1,\n      maxElapsedTime: 100,\n    },\n    retryConnectionErrors: false,\n  },\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.accessTokens.listAccessTokens({});\n\n  console.log(result);\n}\n\nrun();\n\n```\n<!-- End Retries [retries] -->\n\n<!-- Start Error Handling [errors] -->\n## Error Handling\n\n[`S2Error`](./src/models/errors/s2error.ts) is the base class for all HTTP error responses. It has the following properties:\n\n| Property            | Type       | Description                                                                             |\n| ------------------- | ---------- | --------------------------------------------------------------------------------------- |\n| `error.message`     | `string`   | Error message                                                                           |\n| `error.statusCode`  | `number`   | HTTP response status code eg `404`                                                      |\n| `error.headers`     | `Headers`  | HTTP response headers                                                                   |\n| `error.body`        | `string`   | HTTP body. Can be empty string if no body is returned.                                  |\n| `error.rawResponse` | `Response` | Raw HTTP response                                                                       |\n| `error.data$`       |            | Optional. Some errors may contain structured data. [See Error Classes](#error-classes). |\n\n### Example\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\nimport * as errors from \"@s2-dev/streamstore/models/errors\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  try {\n    const result = await s2.accessTokens.listAccessTokens({});\n\n    console.log(result);\n  } catch (error) {\n    // The base class for HTTP error responses\n    if (error instanceof errors.S2Error) {\n      console.log(error.message);\n      console.log(error.statusCode);\n      console.log(error.body);\n      console.log(error.headers);\n\n      // Depending on the method different errors may be thrown\n      if (error instanceof errors.ErrorResponse) {\n        console.log(error.data$.code); // string\n        console.log(error.data$.message); // string\n      }\n    }\n  }\n}\n\nrun();\n\n```\n\n### Error Classes\n**Primary errors:**\n* [`S2Error`](./src/models/errors/s2error.ts): The base class for HTTP error responses.\n  * [`ErrorResponse`](./src/models/errors/errorresponse.ts): .\n\n<details><summary>Less common errors (9)</summary>\n\n<br />\n\n**Network errors:**\n* [`ConnectionError`](./src/models/errors/httpclienterrors.ts): HTTP client was unable to make a request to a server.\n* [`RequestTimeoutError`](./src/models/errors/httpclienterrors.ts): HTTP request timed out due to an AbortSignal signal.\n* [`RequestAbortedError`](./src/models/errors/httpclienterrors.ts): HTTP request was aborted by the client.\n* [`InvalidRequestError`](./src/models/errors/httpclienterrors.ts): Any input used to create a request is invalid.\n* [`UnexpectedClientError`](./src/models/errors/httpclienterrors.ts): Unrecognised or unexpected error.\n\n\n**Inherit from [`S2Error`](./src/models/errors/s2error.ts)**:\n* [`FencingToken`](./src/models/errors/fencingtoken.ts): Fencing token did not match. The expected fencing token is returned. Status code `412`. Applicable to 1 of 21 methods.*\n* [`SeqNum`](./src/models/errors/seqnum.ts): Sequence number did not match the tail of the stream. The expected next sequence number is returned. Status code `412`. Applicable to 1 of 21 methods.*\n* [`TailResponse`](./src/models/errors/tailresponse.ts): . Status code `416`. Applicable to 1 of 21 methods.*\n* [`ResponseValidationError`](./src/models/errors/responsevalidationerror.ts): Type mismatch between the data returned from the server and the structure expected by the SDK. See `error.rawValue` for the raw value and `error.pretty()` for a nicely formatted multi-line string.\n\n</details>\n\n\\* Check [the method documentation](#available-resources-and-operations) to see if the error is applicable.\n<!-- End Error Handling [errors] -->\n\n<!-- Start Server Selection [server] -->\n## Server Selection\n\n### Override Server URL Per-Client\n\nThe default server can be overridden globally by passing a URL to the `serverURL: string` optional parameter when initializing the SDK client instance. For example:\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  serverURL: \"https://aws.s2.dev/v1\",\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.accessTokens.listAccessTokens({});\n\n  console.log(result);\n}\n\nrun();\n\n```\n\n### Override Server URL Per-Operation\n\nThe server URL can also be overridden on a per-operation basis, provided a server list was specified for the operation. For example:\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst s2 = new S2({\n  accessToken: process.env[\"S2_ACCESS_TOKEN\"] ?? \"\",\n});\n\nasync function run() {\n  const result = await s2.streams.listStreams({\n    s2Basin: \"<value>\",\n  }, {\n    serverURL: \"https://.b.aws.s2.dev/v1\",\n  });\n\n  for await (const page of result) {\n    console.log(page);\n  }\n}\n\nrun();\n\n```\n<!-- End Server Selection [server] -->\n\n<!-- Start Custom HTTP Client [http-client] -->\n## Custom HTTP Client\n\nThe TypeScript SDK makes API calls using an `HTTPClient` that wraps the native\n[Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API). This\nclient is a thin wrapper around `fetch` and provides the ability to attach hooks\naround the request lifecycle that can be used to modify the request or handle\nerrors and response.\n\nThe `HTTPClient` constructor takes an optional `fetcher` argument that can be\nused to integrate a third-party HTTP client or when writing tests to mock out\nthe HTTP client and feed in fixtures.\n\nThe following example shows how to use the `\"beforeRequest\"` hook to to add a\ncustom header and a timeout to requests and how to use the `\"requestError\"` hook\nto log errors:\n\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\nimport { HTTPClient } from \"@s2-dev/streamstore/lib/http\";\n\nconst httpClient = new HTTPClient({\n  // fetcher takes a function that has the same signature as native `fetch`.\n  fetcher: (request) => {\n    return fetch(request);\n  }\n});\n\nhttpClient.addHook(\"beforeRequest\", (request) => {\n  const nextRequest = new Request(request, {\n    signal: request.signal || AbortSignal.timeout(5000)\n  });\n\n  nextRequest.headers.set(\"x-custom-header\", \"custom value\");\n\n  return nextRequest;\n});\n\nhttpClient.addHook(\"requestError\", (error, request) => {\n  console.group(\"Request Error\");\n  console.log(\"Reason:\", `${error}`);\n  console.log(\"Endpoint:\", `${request.method} ${request.url}`);\n  console.groupEnd();\n});\n\nconst sdk = new S2({ httpClient: httpClient });\n```\n<!-- End Custom HTTP Client [http-client] -->\n\n<!-- Start Debugging [debug] -->\n## Debugging\n\nYou can setup your SDK to emit debug logs for SDK requests and responses.\n\nYou can pass a logger that matches `console`'s interface as an SDK option.\n\n> [!WARNING]\n> Beware that debug logging will reveal secrets, like API tokens in headers, in log messages printed to a console or files. It's recommended to use this feature only during local development and not in production.\n\n```typescript\nimport { S2 } from \"@s2-dev/streamstore\";\n\nconst sdk = new S2({ debugLogger: console });\n```\n\nYou can also enable a default debug logger by setting an environment variable `S2_DEBUG` to true.\n<!-- End Debugging [debug] -->\n\n<!-- Placeholder for Future Speakeasy SDK Sections -->\n\n# Development\n\n## Maturity\n\nThis SDK is in beta, and there may be breaking changes between versions without a major version update. Therefore, we recommend pinning usage\nto a specific package version. This way, you can install the same version each time without breaking changes unless you are intentionally\nlooking for the latest version.\n\n## Contributions\n\nWhile we value open-source contributions to this SDK, this library is generated programmatically. Any manual changes added to internal files will be overwritten on the next generation. \nWe look forward to hearing your feedback. Feel free to open a PR or an issue with a proof of concept and we'll do our best to include it in a future release. \n\n### SDK Created by [Speakeasy](https://www.speakeasy.com/?utm_source=streamstore&utm_campaign=)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "s2",
        "streamstore",
        "streamstore s2",
        "s2 streamstore",
        "server s2"
      ],
      "category": "databases"
    },
    "santos-404--mcp-server.sqlite": {
      "owner": "santos-404",
      "name": "mcp-server.sqlite",
      "url": "https://github.com/santos-404/mcp-server.sqlite",
      "imageUrl": "/freedevtools/mcp/pfp/santos-404.webp",
      "description": "Execute SQL queries, manage database schemas, and synthesize business insights from an SQLite database. Integrate with AI tools through a standardized protocol for enhanced data interaction.",
      "stars": 7,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-15T02:03:48Z",
      "readme_content": "# SQLite MCP Server\n\nA Model Context Protocol (MCP) server implementation using TypeScript for interacting with an SQLite database. This server provides an interactive interface for executing SQL queries, managing database schemas, and synthesizing business insights—all within an extensible protocol framework.\n\nNot familiar with MCP? Check out the [What is an MCP?](#whats-an-mcp) section below.\n\n## Features\n\n| Command | Description | Example |\n|---------|-------------|---------|\n| `list_tables` | List all tables on the SQLite database | - |\n| `read_query` | Execute SELECT queries on the SQLite database | `SELECT * FROM users WHERE age > 18` |\n\n## Installation & Setup\n\n```bash\ngit clone https://github.com/javsanmar5/mcp-server.sqlite.git\ncd mcp-server.sqlite\n```\n\nSince this hasn't been published as an npm package yet, we'll focus on the Docker installation method:\n\n### 1. Build the Docker image\n\n```bash\ndocker build -t mcp/sqlite .\n```\n\n### 2. Configure your AI client\n\nAdd the following to your AI client's configuration file:\n\n```json\n\"mcpServers\": {\n  \"sqlite\": {\n    \"command\": \"docker\",\n    \"args\": [\n      \"run\",\n      \"--rm\",\n      \"-i\",\n      \"-v\",\n      \"mcp-test:/mcp\",\n      \"mcp/sqlite\",\n      \"--db-path\",\n      \"test_db.sqlite3\"\n    ]\n  }\n}\n```\n\nIf you don't know what is that json file you might want to see the [Client Configuration Guide](#tutorial-setting-up-with-claude-desktop).\n\n### 3. Restart your AI client\n\nAfter restarting, the MCP Tools should be available in your AI client's interface.\n_On Windows, you may need to manually kill the process for the changes to take effect._\n\n## Documentation\n\n### What's an MCP?\n\nModel Context Protocol (MCP) is a standardized way for AI models to interact with external tools and services. It allows AI assistants to perform actions like running database queries, accessing external APIs, or manipulating files through a defined protocol interface.\n\nMCPs extend AI capabilities beyond conversation by providing structured access to tools and data sources without requiring direct integration into the AI model itself.\n\n### Tutorial: Setting up with Claude Desktop\n\nClaude Desktop is one of many AI clients that support MCP servers. Here's how to set it up on Windows:\n\n1. Press `Windows Key + R` to open the Run dialog\n2. Type `%appdata%\\Claude` and press Enter\n3. Create a new file called `claude_desktop_config.json` if it doesn't exist already\n4. Add the configuration from step 2 of the setup instructions above\n5. Save the file and restart Claude Desktop\n6. You should now see the SQLite tools available in your Claude interface\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Contributing\n\nThis project was created primarily for learning purposes. However, if you'd like to contribute, feel free to submit a Pull Request and I'll review it.\n\nThanks for your interest!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "sqlite",
        "database",
        "databases secure",
        "secure database",
        "insights sqlite"
      ],
      "category": "databases"
    },
    "sazboxai--MCP_MetaBase": {
      "owner": "sazboxai",
      "name": "MCP_MetaBase",
      "url": "https://github.com/sazboxai/MCP_MetaBase",
      "imageUrl": "/freedevtools/mcp/pfp/sazboxai.webp",
      "description": "Enable seamless interaction with Metabase databases by allowing exploration of database schemas, visualization of table relationships, and execution of Metabase actions through a secure API.",
      "stars": 6,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T17:26:29Z",
      "readme_content": "# Metabase MCP Server\n\nA Model Control Protocol (MCP) server that enables AI assistants to interact with Metabase databases and actions.\n\n![Metabase MCP Server]\n\n## Overview\n\nThe Metabase MCP Server provides a bridge between AI assistants and Metabase, allowing AI models to:\n\n- List and explore databases configured in Metabase\n- Retrieve detailed metadata about database schemas, tables, and fields\n- Visualize relationships between tables in a database\n- List and execute Metabase actions\n- Perform operations on Metabase data through a secure API\n\nThis server implements the [Model Control Protocol (MCP)] specification, making it compatible with AI assistants that support MCP tools.\n\n## Features\n\n- **Database Exploration**: List all databases and explore their schemas\n- **Metadata Retrieval**: Get detailed information about tables, fields, and relationships\n- **Relationship Visualization**: Generate visual representations of database relationships\n- **Action Management**: List, view details, and execute Metabase actions\n- **Secure API Key Handling**: Store API keys encrypted and prevent exposure\n- **Web Interface**: Test and debug functionality through a user-friendly web interface\n- **Docker Support**: Easy deployment with Docker and Docker Compose\n\n## Prerequisites\n\n- Metabase instance (v0.46.0 or higher recommended)\n- Metabase API key with appropriate permissions\n- Docker (for containerized deployment)\n- Python 3.10+ (for local development)\n\n## Installation\n\n### Using Docker (Recommended)\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/metabase-mcp.git\n   cd metabase-mcp\n   ```\n\n2. Build and run the Docker container:\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Access the configuration interface at http://localhost:5001\n\n### Manual Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/metabase-mcp.git\n   cd metabase-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Run the configuration interface:\n   ```bash\n   python -m src.server.web_interface\n   ```\n\n4. Access the configuration interface at http://localhost:5000\n\n## Configuration\n\n1. Open the web interface in your browser\n2. Enter your Metabase URL (e.g., http://localhost:3000)\n3. Enter your Metabase API key\n4. Click \"Save Configuration\" and test the connection\n\n### Obtaining a Metabase API Key\n\n1. Log in to your Metabase instance as an administrator\n2. Go to Settings > Admin settings > API Keys\n3. Create a new API key with appropriate permissions\n4. Copy the generated key for use in the MCP server\n\n## Usage\n\n### Running the MCP Server\n\nAfter configuration, you can run the MCP server:\n\n```bash\n# Using Docker\ndocker run -p 5001:5000 metabase-mcp\n\n# Manually\npython -m src.server.mcp_server\n```\n\n### Available Tools\n\nThe MCP server provides the following tools to AI assistants:\n\n1. **list_databases**: List all databases configured in Metabase\n2. **get_database_metadata**: Get detailed metadata for a specific database\n3. **db_overview**: Get a high-level overview of all tables in a database\n4. **table_detail**: Get detailed information about a specific table\n5. **visualize_database_relationships**: Generate a visual representation of database relationships\n6. **run_database_query**: Execute a SQL query against a database\n7. **list_actions**: List all actions configured in Metabase\n8. **get_action_details**: Get detailed information about a specific action\n9. **execute_action**: Execute a Metabase action with parameters\n\n### Testing Tools via Web Interface\n\nThe web interface provides a testing area for each tool:\n\n1. **List Databases**: View all databases configured in Metabase\n2. **Get Database Metadata**: View detailed schema information for a database\n3. **DB Overview**: View a concise list of all tables in a database\n4. **Table Detail**: View detailed information about a specific table\n5. **Visualize Database Relationships**: Generate a visual representation of table relationships\n6. **Run Query**: Execute SQL queries against databases\n7. **List Actions**: View all actions configured in Metabase\n8. **Get Action Details**: View detailed information about a specific action\n9. **Execute Action**: Test executing an action with parameters\n\n## Security Considerations\n\n- API keys are stored encrypted at rest\n- The web interface never displays API keys in plain text\n- All API requests use HTTPS when configured with a secure Metabase URL\n- The server should be deployed behind a secure proxy in production environments\n\n## Development\n\n### Project Structure\n\n```\nmetabase-mcp/\n├── src/\n│   ├── api/            # Metabase API client\n│   ├── config/         # Configuration management\n│   ├── server/         # MCP and web servers\n│   └── tools/          # Tool implementations\n├── templates/          # Web interface templates\n├── docker-compose.yml  # Docker Compose configuration\n├── Dockerfile          # Docker build configuration\n├── requirements.txt    # Python dependencies\n└── README.md           # Documentation\n```\n\n### Adding New Tools\n\nTo add a new tool:\n\n1. Implement the tool function in `src/tools/`\n2. Register the tool in `src/server/mcp_server.py`\n3. Add a testing interface in `templates/config.html` (optional)\n4. Add a route in `src/server/web_interface.py` (if adding a testing interface)\n\n## Troubleshooting\n\n### Common Issues\n\n- **Connection Failed**: Ensure your Metabase URL is correct and accessible\n- **Authentication Error**: Verify your API key has the necessary permissions\n- **Docker Network Issues**: When using Docker, ensure proper network configuration\n\n### Logs\n\nCheck the logs for detailed error information:\n\n```bash\n# Docker logs\ndocker logs metabase-mcp\n\n# Manual execution logs\n# Logs are printed to the console\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "metabase",
        "database",
        "metabase databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "schemacrawler--SchemaCrawler-MCP-Server-Usage": {
      "owner": "schemacrawler",
      "name": "SchemaCrawler-MCP-Server-Usage",
      "url": "https://github.com/schemacrawler/SchemaCrawler-MCP-Server-Usage",
      "imageUrl": "",
      "description": "[sirmews/mcp-pinecone](https://github.com/sirmews/mcp-pinecone) 🐍 ☁️ - Pinecone integration with vector search capabilities",
      "stars": 4,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-10-03T04:13:11Z",
      "readme_content": "<!-- markdownlint-disable MD041 -->\n[![Docker Pulls](https://img.shields.io/docker/pulls/schemacrawler/schemacrawler-ai?color=FFDAB9)](https://hub.docker.com/r/schemacrawler/schemacrawler-ai/)\n![GitHub Repo stars](https://img.shields.io/github/stars/schemacrawler/schemacrawler?style=social)\n\n\n# <img alt=\"schemacrawler_logo\" src=\"https://raw.githubusercontent.com/schemacrawler/SchemaCrawler/main/schemacrawler-website/src/site/resources/images/schemacrawler_logo.png\" height=\"100px\" width=\"100px\" valign=\"middle\"/> SchemaCrawler AI MCP Server: Usage\n\n> [!NOTE]  \n> * Please see the [SchemaCrawler website](https://www.schemacrawler.com/) for more details.\n> * Explore the SchemaCrawler command-line with a [live online tutorial](https://killercoda.com/schemacrawler).\n\n## About\n\nSchemaCrawler is a free database schema discovery and comprehension tool. SchemaCrawler has a good mix of useful features for data governance. You can [search for database schema objects](https://www.schemacrawler.com/schemacrawler-grep.html) using regular expressions, and output the schema and data in a readable text format.\n\nThis is a bare project that acts as an MCP client for the [SchemaCrawler AI MCP Server](https://github.com/schemacrawler/SchemaCrawler-AI) for use in \"Agent\" mode. You can find documentation on how to use the server here.\n\nThe SchemaCrawler AI MCP Server is available as a Docker image [schemacrawler/schemacrawler-ai](https://hub.docker.com/repository/docker/schemacrawler/schemacrawler-ai). It is also available from the [the Docker MCP Catalog](https://hub.docker.com/mcp/server/schemacrawler-ai/overview) as a Docker-verified image.\n\n\n## Prerequisites\n\n1. Install supporting software\n   - Docker\n   - Docker Compose\n   - Visual Studio Code\n2. Read [Use MCP servers in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers)\n3. Clone this project, and open it in Visual Studio Code\n\n\n## Getting Started\n\nRefer to the [getting started](../docs/getting-started.md) documentation.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "schemacrawler",
        "database",
        "enables querying",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "showjason--opensearch-mcp-server": {
      "owner": "showjason",
      "name": "opensearch-mcp-server",
      "url": "https://github.com/showjason/opensearch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/showjason.webp",
      "description": "Manage and interact with an OpenSearch cluster by utilizing tools for index management, cluster health monitoring, and document searching. Streamline the management of indices and clusters while seamlessly integrating into applications.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-21T12:02:15Z",
      "readme_content": "# OpenSearch MCP Server\n\nMCP (Model Context Protocol) server for OpenSearch integration which is inspired by **[elasticsearch-mcp-server](https://github.com/cr7258/elasticsearch-mcp-server)**.\n\n## Features\n\n- Index Management Tools:\n  - List all indices in OpenSearch cluster\n  - Get index mapping\n  - Get index settings\n- Cluster Management Tools:\n  - Get cluster health status\n  - Get cluster statistics\n- Document Tools:\n  - Search documents\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/your-username/opensearch-mcp-server.git\ncd opensearch-mcp-server\n\n# Create and activate a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the package\npip install uv\nuv pip install -e .\n```\n\n## Configuration\n\nCreate a `.env` file in the root directory with the following variables:\n\n```\nOPENSEARCH_HOST=https://localhost:9200\nOPENSEARCH_USERNAME=xxxx\nOPENSEARCH_PASSWORD=xxxx\n```\n\nAdjust the values to match your OpenSearch configuration.\n\n## Usage with Cursor\n\n### Run the MCP server:\n\n```bash\nuv run opensearch-mcp-server --port=<port>\n```\n### Integrate with Cursor\n```\n{\n  \"mcpServers\": {\n    \"opensearch\": {\n      \"url\": \"http://<host>:<port>>/sse\"\n    }\n  }\n}\n```\n## Usage with Claude Desktop APP\n```\n{\n  \"mcpServers\": {\n    \"opensearch\": {\n      \"command\": \"uv\",\n      \"args\": [\n          \"--directory\",\n          \"/absolute/path/to/opensearch-mcp-server\",\n          \"run\",\n          \"opensearch-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nuv pip install -e .\n\n# Run tests\nuv run pytest -vv -s test_opensearch.py\n```\n\n## License\n\n[MIT](LICENSE) \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opensearch",
        "databases",
        "database",
        "opensearch cluster",
        "opensearch mcp",
        "interact opensearch"
      ],
      "category": "databases"
    },
    "shreyansh-ghl--mysql-mcp-server": {
      "owner": "shreyansh-ghl",
      "name": "mysql-mcp-server",
      "url": "https://github.com/shreyansh-ghl/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/shreyansh-ghl.webp",
      "description": "Provides read-only access to MySQL databases, enabling execution of SQL queries and inspection of database schemas securely. Facilitates streamlined data access for enhanced application insights.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-16T10:45:45Z",
      "readme_content": "# MySQL MCP Server\n\nA Model Context Protocol server that provides read-only access to MySQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Authentication\n\nThe server supports MySQL authentication through the database URL. The URL format is:\n\n```\nmysql://username:password@host:port/database\n```\n\nExamples:\n\n- DB: `mysql://user:pass@localhost:3306/mydb`\n\n**Note**: Always ensure your credentials are properly secured and not exposed in public configurations.\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n  - Authentication is handled automatically using the provided credentials\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas** (`mysql://<host>/<table>/schema`)\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n  - Access is authenticated using the provided credentials\n\n## Installation\n\n1. Clone the repository:\n\n```sh\ngit clone https://github.com/yourusername/mysql-mcp-server.git\ncd mysql-mcp-server\n```\n\n2. Prepare and install dependencies:\n\n```sh\nnpm run prepare\nnpm install\n```\n\n3. Create a global link:\n\n```sh\nnpm link\n```\n\nNow you can use the `mysql-mcp-server` command from anywhere in your terminal:\n\n```sh\nmysql-mcp-server mysql://user:password@localhost:3306/mydb\n```\n\n## Usage with Cursor\n\n### Configuring MCP in Cursor\n\n1. Open Cursor's settings:\n\n   - Click on the gear icon (⚙️) in the bottom left corner of Cursor\n   - Or press `Shift + Cmd + J` on macOS\n\n2. Configure MCP Server:\n   - Click on \"MCP\" in the left sidebar\n   - Click on \"Add Global MCP Server\"\n   - Add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"mysql-mcp-server\",\n      \"args\": [\"mysql://user:password@localhost:3306/mydb\"]\n    }\n  }\n}\n```\n\n3. Save the configuration:\n   - Click \"Save\" or press `Cmd + S`\n   - Restart Cursor for the changes to take effect\n\n### How to use it on Cursor?\nOpen the Agentic chat on Cursor and start asking questions related to our DB and it will have all the context\n![ezgif-3bb869e455a41b](https://github.com/user-attachments/assets/2b765170-52e7-472d-bb47-fb5ebf0acd7b)\n\n### Security Best Practices\n\n1. Use environment variables for sensitive credentials:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"mysql\": {\n         \"command\": \"mysql-mcp-server\",\n         \"args\": [\"mysql://${MYSQL_USER}:${MYSQL_PASSWORD}@host:3306/mydb\"]\n       }\n     }\n   }\n   ```\n\n2. Ensure the MySQL user has minimal required permissions (READ-ONLY access)\n3. Use strong passwords and follow security best practices\n4. Avoid committing configuration files with credentials to version control\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "singlestore-labs--mcp-server-singlestore": {
      "owner": "singlestore-labs",
      "name": "mcp-server-singlestore",
      "url": "https://github.com/singlestore-labs/mcp-server-singlestore",
      "imageUrl": "/freedevtools/mcp/pfp/singlestore-labs.webp",
      "description": "Integrate natural language processing with SingleStore to manage data and execute SQL queries via a user-friendly interface. It simplifies workflows by providing seamless access to database resources.",
      "stars": 27,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:18Z",
      "readme_content": "# SingleStore MCP Server\n\n[![MIT Licence](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/singlestore-labs/mcp-server-singlestore/blob/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/singlestore-mcp-server)](https://pypi.org/project/singlestore-mcp-server/) [![Downloads](https://static.pepy.tech/badge/singlestore-mcp-server)](https://pepy.tech/project/singlestore-mcp-server)\n\n[Model Context Protocol]((https://modelcontextprotocol.io/introduction)) (MCP) is a standardized protocol designed to manage context between large language models (LLMs) and external systems. This repository provides an installer and an MCP Server for Singlestore, enabling seamless integration.\n\nWith MCP, you can use Claude Desktop, Claude Code, Cursor, or any compatible MCP client to interact with SingleStore using natural language, making it easier to perform complex operations effortlessly.\n\n💡 **Pro Tip**: Not sure what the MCP server can do? Just call the `/help` prompt in your chat!\n\n## Requirements\n\n- Python >= v3.10.0\n- [uvx](https://docs.astral.sh/uv/guides/tools/) installed on your python environment\n- VS Code, Cursor, Windsurf, Claude Desktop, Claude Code, Goose or any other MCP client\n\n## Getting started\n\n## Getting started\n\nFirst, install the SingleStore MCP server with your client.\n\n**Standard config** works in most of the tools:\n\n```json\n{\n  \"mcpServers\": {\n    \"singlestore-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"singlestore-mcp-server\",\n        \"start\"\n      ]\n    }\n  }\n}\n```\n\n**No API keys, tokens, or environment variables required!** The server automatically handles authentication via browser OAuth when started.\n\n<details>\n<summary>Claude Desktop</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=claude-desktop\n```\n\n**Manual setup:**\nFollow the MCP install [guide](https://modelcontextprotocol.io/quickstart/user), use the standard config above.\n\n</details>\n\n<details>\n<summary>Claude Code</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=claude-code\n```\nThis will automatically run the Claude CLI command for you.\n\n**Manual setup:**\n```bash\nclaude mcp add singlestore-mcp-server uvx singlestore-mcp-server start\n```\n\n</details>\n\n<details>\n<summary>Cursor</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=cursor\n```\n\n**Manual setup:**\nGo to `Cursor Settings` -> `MCP` -> `Add new MCP Server`. Name to your liking, use `command` type with the command `uvx singlestore-mcp-server start`. You can also verify config or add command line arguments via clicking `Edit`.\n\n</details>\n\n<details>\n<summary>VS Code</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=vscode\n```\n\n**Manual setup:**\nFollow the MCP install [guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server), use the standard config above. You can also install using the VS Code CLI:\n\n```bash\ncode --add-mcp '{\"name\":\"singlestore-mcp-server\",\"command\":\"uvx\",\"args\":[\"singlestore-mcp-server\",\"start\"]}'\n```\n\nAfter installation, the SingleStore MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\n</details>\n\n<details>\n<summary>Windsurf</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=windsurf\n```\n\n**Manual setup:**\nFollow Windsurf MCP [documentation](https://docs.windsurf.com/windsurf/cascade/mcp). Use the standard config above.\n\n</details>\n\n<details>\n<summary>Gemini CLI</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=gemini\n```\n\n**Manual setup:**\nFollow the MCP install [guide](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#configure-the-mcp-server-in-settingsjson), use the standard config above.\n\n</details>\n\n<details>\n<summary>LM Studio</summary>\n\n**Automatic setup:**\n```bash\nuvx singlestore-mcp-server init --client=lm-studio\n```\n\n**Manual setup:**\nGo to `Program` in the right sidebar -> `Install` -> `Edit mcp.json`. Use the standard config above.\n\n</details>\n\n<details>\n<summary>Goose</summary>\n\n**Manual setup only:**\nGo to `Advanced settings` -> `Extensions` -> `Add custom extension`. Name to your liking, use type `STDIO`, and set the `command` to `uvx singlestore-mcp-server start`. Click \"Add Extension\".\n\n</details>\n\n<details>\n<summary>Qodo Gen</summary>\n\n**Manual setup only:**\nOpen [Qodo Gen](https://docs.qodo.ai/qodo-documentation/qodo-gen) chat panel in VSCode or IntelliJ → Connect more tools → + Add new MCP → Paste the standard config above.\n\nClick <code>Save</code>.\n\n</details>\n\n### Using Docker\n\n**NOTE:** An API key is required when using Docker because the OAuth flow isn't supported for servers running in Docker containers.\n\n```json\n{\n  \"mcpServers\": {\n    \"singlestore-mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\", \"--init\", \"--pull=always\",\n        \"-e\", \"MCP_API_KEY=your_api_key_here\",\n        \"singlestore/mcp-server-singlestore\"\n      ]\n    }\n  }\n}\n```\n\nYou can build the Docker image yourself:\n\n```bash\ndocker build -t singlestore/mcp-server-singlestore .\n```\n\nFor better security, we recommend using Docker Desktop to configure the SingleStore MCP server—see [this blog post](https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/) for details on Docker's new MCP Catalog.\n\n## Components\n\n### Tools\n\nThe server implements the following tools:\n\n- **get_user_info**: Retrieve details about the current user\n  - No arguments required\n  - Returns user information and details\n\n- **organization_info**: Retrieve details about the user's current organization\n  - No arguments required\n  - Returns details of the organization\n\n- **choose_organization**: Choose from available organizations (only available when API key environment variable is not set)\n  - No arguments required\n  - Returns a list of available organizations to choose from\n\n- **set_organization**: Set the active organization (only available when API key environment variable is not set)\n  - Arguments: `organization_id` (string)\n  - Sets the specified organization as active\n\n- **workspace_groups_info**: Retrieve details about the workspace groups accessible to the user\n  - No arguments required\n  - Returns details of the workspace groups\n\n- **workspaces_info**: Retrieve details about the workspaces in a specific workspace group\n  - Arguments: `workspace_group_id` (string)\n  - Returns details of the workspaces\n\n- **resume_workspace**: Resume a suspended workspace\n  - Arguments: `workspace_id` (string)\n  - Resumes the specified workspace\n\n- **list_starter_workspaces**: List all starter workspaces accessible to the user\n  - No arguments required\n  - Returns details of available starter workspaces\n\n- **create_starter_workspace**: Create a new starter workspace\n  - Arguments: workspace configuration parameters\n  - Returns details of the created starter workspace\n\n- **terminate_starter_workspace**: Terminate an existing starter workspace\n  - Arguments: `workspace_id` (string)\n  - Terminates the specified starter workspace\n\n- **list_regions**: Retrieve a list of all regions that support workspaces\n  - No arguments required\n  - Returns a list of available regions\n\n- **list_sharedtier_regions**: Retrieve a list of shared tier regions\n  - No arguments required\n  - Returns a list of shared tier regions\n\n- **run_sql**: Execute SQL operations on a connected workspace\n  - Arguments: `workspace_id`, `database`, `sql_query`, and connection parameters\n  - Returns the results of the SQL query in a structured format\n\n- **create_notebook_file**: Create a new notebook file in SingleStore Spaces\n  - Arguments: `notebook_name`, `content` (optional)\n  - Returns details of the created notebook\n\n- **upload_notebook_file**: Upload a notebook file to SingleStore Spaces\n  - Arguments: `file_path`, `notebook_name`\n  - Returns details of the uploaded notebook\n\n- **create_job_from_notebook**: Create a scheduled job from a notebook\n  - Arguments: job configuration including `notebook_path`, `schedule_mode`, etc.\n  - Returns details of the created job\n\n- **get_job**: Retrieve details of an existing job\n  - Arguments: `job_id` (string)\n  - Returns details of the specified job\n\n- **delete_job**: Delete an existing job\n  - Arguments: `job_id` (string)\n  - Deletes the specified job\n\n**Note**: Organization management tools (`choose_organization` and `set_organization`) are only available when the API key environment variable is not set, allowing for interactive organization selection during OAuth authentication.\n\n## Development\n\n### Prerequisites\n\n- Python >= 3.11\n- [uv](https://docs.astral.sh/uv/) for dependency management\n\n### Setup\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/singlestore-labs/mcp-server-singlestore.git\ncd mcp-server-singlestore\n```\n\n1. Install dependencies:\n\n```bash\nuv sync --dev\n```\n\n1. Set up pre-commit hooks (optional but recommended):\n\n```bash\nuv run pre-commit install\n```\n\n### Development Workflow\n\n```bash\n# Quick quality checks (fast feedback)\n./scripts/check.sh\n\n# Run tests independently\n./scripts/test.sh\n\n# Comprehensive validation (before PRs)\n./scripts/check-all.sh\n\n# Create and publish releases\n./scripts/release.sh\n```\n\n### Running Tests\n\n```bash\n# Run test suite with coverage\n./scripts/test.sh\n\n# Or use pytest directly\nuv run pytest\nuv run pytest --cov=src --cov-report=html\n```\n\n### Code Quality\n\nWe use [Ruff](https://docs.astral.sh/ruff/) for both linting and formatting:\n\n```bash\n# Format code\nuv run ruff format src/ tests/\n\n# Lint code\nuv run ruff check src/ tests/\n\n# Lint and fix issues automatically\nuv run ruff check --fix src/ tests/\n```\n\n### Release Process\n\nReleases are managed through git tags and automated PyPI publication:\n\n1. **Create release**: `./scripts/release.sh` (interactive tool)\n2. **Automatic publication**: Triggered by pushing version tags\n3. **No manual PyPI uploads** - fully automated pipeline\n\nSee [`scripts/dev-workflow.md`](scripts/dev-workflow.md) for detailed workflow documentation.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "singlestore",
        "database",
        "server singlestore",
        "singlestore integrate",
        "processing singlestore"
      ],
      "category": "databases"
    },
    "sirmews--mcp-pinecone": {
      "owner": "sirmews",
      "name": "mcp-pinecone",
      "url": "https://github.com/sirmews/mcp-pinecone",
      "imageUrl": "/freedevtools/mcp/pfp/sirmews.webp",
      "description": "Read and write to a Pinecone index from a compatible MCP client like Claude Desktop, facilitating data interaction and retrieval.",
      "stars": 146,
      "forks": 31,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T06:30:25Z",
      "readme_content": "# Pinecone Model Context Protocol Server for Claude Desktop.\n\n[![smithery badge](https://smithery.ai/badge/mcp-pinecone)](https://smithery.ai/server/mcp-pinecone)\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dd/mcp-pinecone?style=flat)](https://pypi.org/project/mcp-pinecone/)\n\nRead and write to a Pinecone index.\n\n\n## Components\n\n```mermaid\nflowchart TB\n    subgraph Client[\"MCP Client (e.g., Claude Desktop)\"]\n        UI[User Interface]\n    end\n\n    subgraph MCPServer[\"MCP Server (pinecone-mcp)\"]\n        Server[Server Class]\n        \n        subgraph Handlers[\"Request Handlers\"]\n            ListRes[list_resources]\n            ReadRes[read_resource]\n            ListTools[list_tools]\n            CallTool[call_tool]\n            GetPrompt[get_prompt]\n            ListPrompts[list_prompts]\n        end\n        \n        subgraph Tools[\"Implemented Tools\"]\n            SemSearch[semantic-search]\n            ReadDoc[read-document]\n            ListDocs[list-documents]\n            PineconeStats[pinecone-stats]\n            ProcessDoc[process-document]\n        end\n    end\n\n    subgraph PineconeService[\"Pinecone Service\"]\n        PC[Pinecone Client]\n        subgraph PineconeFunctions[\"Pinecone Operations\"]\n            Search[search_records]\n            Upsert[upsert_records]\n            Fetch[fetch_records]\n            List[list_records]\n            Embed[generate_embeddings]\n        end\n        Index[(Pinecone Index)]\n    end\n\n    %% Connections\n    UI --> Server\n    Server --> Handlers\n    \n    ListTools --> Tools\n    CallTool --> Tools\n    \n    Tools --> PC\n    PC --> PineconeFunctions\n    PineconeFunctions --> Index\n    \n    %% Data flow for semantic search\n    SemSearch --> Search\n    Search --> Embed\n    Embed --> Index\n    \n    %% Data flow for document operations\n    UpsertDoc --> Upsert\n    ReadDoc --> Fetch\n    ListRes --> List\n\n    classDef primary fill:#2563eb,stroke:#1d4ed8,color:white\n    classDef secondary fill:#4b5563,stroke:#374151,color:white\n    classDef storage fill:#059669,stroke:#047857,color:white\n    \n    class Server,PC primary\n    class Tools,Handlers secondary\n    class Index storage\n```\n\n### Resources\n\nThe server implements the ability to read and write to a Pinecone index.\n\n### Tools\n\n- `semantic-search`: Search for records in the Pinecone index.\n- `read-document`: Read a document from the Pinecone index.\n- `list-documents`: List all documents in the Pinecone index.\n- `pinecone-stats`: Get stats about the Pinecone index, including the number of records, dimensions, and namespaces.\n- `process-document`: Process a document into chunks and upsert them into the Pinecone index. This performs the overall steps of chunking, embedding, and upserting.\n\nNote: embeddings are generated via Pinecone's inference API and chunking is done with a token-based chunker. Written by copying a lot from langchain and debugging with Claude.\n## Quickstart\n\n### Installing via Smithery\n\nTo install Pinecone MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-pinecone):\n\n```bash\nnpx -y @smithery/cli install mcp-pinecone --client claude\n```\n\n### Install the server\n\nRecommend using [uv](https://docs.astral.sh/uv/getting-started/installation/) to install the server locally for Claude.\n\n```\nuvx install mcp-pinecone\n```\nOR\n```\nuv pip install mcp-pinecone\n```\n\nAdd your config as described below.\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nNote: You might need to use the direct path to `uv`. Use `which uv` to find the path.\n\n\n__Development/Unpublished Servers Configuration__\n  \n```json\n\"mcpServers\": {\n  \"mcp-pinecone\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"{project_dir}\",\n      \"run\",\n      \"mcp-pinecone\"\n    ]\n  }\n}\n```\n\n\n__Published Servers Configuration__\n  \n```json\n\"mcpServers\": {\n  \"mcp-pinecone\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"--index-name\",\n      \"{your-index-name}\",\n      \"--api-key\",\n      \"{your-secret-api-key}\",\n      \"mcp-pinecone\"\n    ]\n  }\n}\n```\n\n#### Sign up to Pinecone\n\nYou can sign up for a Pinecone account [here](https://www.pinecone.io/).\n\n#### Get an API key\n\nCreate a new index in Pinecone, replacing `{your-index-name}` and get an API key from the Pinecone dashboard, replacing `{your-secret-api-key}` in the config.\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory {project_dir} run mcp-pinecone\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Source Code\n\nThe source code is available on [GitHub](https://github.com/sirmews/mcp-pinecone).\n\n## Contributing\n\nSend your ideas and feedback to me on [Bluesky](https://bsky.app/profile/perfectlycromulent.bsky.social) or by opening an issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "security",
        "databases secure",
        "secure database",
        "enables querying"
      ],
      "category": "databases"
    },
    "spences10--mcp-memory-libsql": {
      "owner": "spences10",
      "name": "mcp-memory-libsql",
      "url": "https://github.com/spences10/mcp-memory-libsql",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "High-performance vector search and persistent memory system using libSQL, offering efficient knowledge storage and semantic search capabilities. Supports knowledge graph management and secure token-based authentication for accessing local and remote databases.",
      "stars": 71,
      "forks": 13,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T04:11:59Z",
      "readme_content": "# mcp-memory-libsql\n\nA high-performance, persistent memory system for the Model Context\nProtocol (MCP) powered by libSQL. This server provides vector search\ncapabilities and efficient knowledge storage using libSQL as the\nbacking store.\n\n<a href=\"https://glama.ai/mcp/servers/22lg4lq768\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/22lg4lq768/badge\" alt=\"Glama badge\" />\n</a>\n\n## Features\n\n- 🚀 High-performance vector search using libSQL\n- 💾 Persistent storage of entities and relations\n- 🔍 Semantic search capabilities\n- 🔄 Knowledge graph management\n- 🌐 Compatible with local and remote libSQL databases\n- 🔒 Secure token-based authentication for remote databases\n\n## Configuration\n\nThis server is designed to be used as part of an MCP configuration.\nHere are examples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-memory-libsql\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-memory-libsql\"],\n\t\t\t\"env\": {\n\t\t\t\t\"LIBSQL_URL\": \"file:/path/to/your/database.db\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor a detailed guide on setting up this server with Claude Desktop in\nWSL, see\n[Getting MCP Server Working with Claude Desktop in WSL](https://scottspence.com/posts/getting-mcp-server-working-with-claude-desktop-in-wsl).\n\nAdd this to your Claude Desktop configuration for WSL environments:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-memory-libsql\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && LIBSQL_URL=file:/path/to/database.db /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-memory-libsql\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Database Configuration\n\nThe server supports both local SQLite and remote libSQL databases\nthrough the LIBSQL_URL environment variable:\n\nFor local SQLite databases:\n\n```json\n{\n\t\"env\": {\n\t\t\"LIBSQL_URL\": \"file:/path/to/database.db\"\n\t}\n}\n```\n\nFor remote libSQL databases (e.g., Turso):\n\n```json\n{\n\t\"env\": {\n\t\t\"LIBSQL_URL\": \"libsql://your-database.turso.io\",\n\t\t\"LIBSQL_AUTH_TOKEN\": \"your-auth-token\"\n\t}\n}\n```\n\nNote: When using WSL, ensure the database path uses the Linux\nfilesystem format (e.g., `/home/username/...`) rather than Windows\nformat.\n\nBy default, if no URL is provided, it will use `file:/memory-tool.db`\nin the current directory.\n\n## API\n\nThe server implements the standard MCP memory interface with\nadditional vector search capabilities:\n\n- Entity Management\n  - Create/Update entities with embeddings\n  - Delete entities\n  - Search entities by similarity\n- Relation Management\n  - Create relations between entities\n  - Delete relations\n  - Query related entities\n\n## Architecture\n\nThe server uses a libSQL database with the following schema:\n\n- Entities table: Stores entity information and embeddings\n- Relations table: Stores relationships between entities\n- Vector search capabilities implemented using libSQL's built-in\n  vector operations\n\n## Development\n\n### Publishing\n\nDue to npm 2FA requirements, publishing needs to be done manually:\n\n1. Create a changeset (documents your changes):\n\n```bash\npnpm changeset\n```\n\n2. Version the package (updates version and CHANGELOG):\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm (will prompt for 2FA code):\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please read our contributing guidelines\nbefore submitting pull requests.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [libSQL](https://github.com/tursodatabase/libsql)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "libsql",
        "database",
        "secure database",
        "memory libsql",
        "databases secure"
      ],
      "category": "databases"
    },
    "stefanraath3--mcp-supabase": {
      "owner": "stefanraath3",
      "name": "mcp-supabase",
      "url": "https://github.com/stefanraath3/mcp-supabase",
      "imageUrl": "/freedevtools/mcp/pfp/stefanraath3.webp",
      "description": "Connects to a Supabase PostgreSQL database, exposing table schemas as resources and enabling data analysis through read-only SQL query tools and predefined prompts.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-06-25T16:31:13Z",
      "readme_content": "# Supabase MCP Server\n\nAn MCP server that connects to a Supabase PostgreSQL database, exposing table schemas as resources and providing tools for data analysis.\n\n## Features\n\n- Connection to Supabase PostgreSQL database\n- Table schemas exposed as resources\n- Read-only SQL query tools\n- Prompts for common data analysis tasks\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Copy `.env.example` to `.env` and update with your Supabase credentials:\n   ```\n   cp .env.example .env\n   ```\n4. Edit the `.env` file with your actual Supabase connection details\n\n## Running the Server\n\n### Using stdio (command line integration)\n\n```\nnpm start\n```\n\n### Using HTTP with SSE (for web integration)\n\n```\nnpm run start:http\n```\n\n## Using with MCP Clients\n\nThis server can be used with any MCP-compatible client, including Claude.app and the MCP Inspector for testing.\n\n### Available Resources\n\n- `schema://tables` - Lists all tables in the database\n- `schema://table/{tableName}` - Shows schema for a specific table\n\n### Available Tools\n\n- `query` - Runs a read-only SQL query against the database\n- `analyze-table` - Gets basic statistics about a table\n- `find-related-tables` - Discovers tables related to a given table\n\n### Available Prompts\n\n- `table-exploration` - Guides exploration of a specific table\n- `data-summary` - Creates a summary of data in a table\n- `relationship-analysis` - Analyzes relationships between tables\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase postgresql",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "stinkgen--trino_mcp": {
      "owner": "stinkgen",
      "name": "trino_mcp",
      "url": "https://github.com/stinkgen/trino_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/stinkgen.webp",
      "description": "Provides structured access to Trino's distributed SQL query engine, enabling AI models to query and analyze data efficiently. Supports API initialization through Docker and Python, allowing integration with large language models for data insights.",
      "stars": 10,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-19T04:29:23Z",
      "readme_content": "# Trino MCP Server\n\nModel Context Protocol server for Trino, providing AI models with structured access to Trino's distributed SQL query engine.\n\n⚠️ **BETA RELEASE (v0.1.2)** ⚠️  \nThis project is stabilizing with core features working and tested. Feel free to fork and contribute!\n\n## Features\n\n- ✅ Fixed Docker container API initialization issue! (reliable server initalization)\n- ✅ Exposes Trino resources through MCP protocol\n- ✅ Enables AI tools to query and analyze data in Trino\n- ✅ Provides transport options (STDIO transport works reliably; SSE transport has issues)\n- ✅ Fixed catalog handling for proper Trino query execution\n- ✅ Both Docker container API and standalone Python API server options\n\n## Quick Start\n\n```bash\n# Start the server with docker-compose\ndocker-compose up -d\n\n# Verify the API is working\ncurl -X POST \"http://localhost:9097/api/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"SELECT 1 AS test\"}'\n```\n\nNeed a non-containerized version? Run the standalone API:\n\n```bash\n# Run the standalone API server on port 8008\npython llm_trino_api.py\n```\n\n## LLM Integration\n\nWant to give an LLM direct access to query your Trino instance? We've created simple tools for that!\n\n### Command-Line LLM Interface\n\nThe simplest way to let an LLM query Trino is through our command-line tool:\n\n```bash\n# Simple direct query (perfect for LLMs)\npython llm_query_trino.py \"SELECT * FROM memory.bullshit.real_bullshit_data LIMIT 5\"\n\n# Specify a different catalog or schema\npython llm_query_trino.py \"SELECT * FROM information_schema.tables\" memory information_schema\n```\n\n### REST API for LLMs\n\nWe offer two API options for integration with LLM applications:\n\n#### 1. Docker Container API (Port 9097)\n\nThe Docker container exposes a REST API on port 9097:\n\n```bash\n# Execute a query against the Docker container API\ncurl -X POST \"http://localhost:9097/api/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"SELECT 1 AS test\"}'\n```\n\n#### 2. Standalone Python API (Port 8008)\n\nFor more flexible deployments, run the standalone API server:\n\n```bash\n# Start the API server on port 8008\npython llm_trino_api.py\n```\n\nThis creates endpoints at:\n- `GET http://localhost:8008/` - API usage info\n- `POST http://localhost:8008/query` - Execute SQL queries\n\nYou can then have your LLM make HTTP requests to this endpoint:\n\n```python\n# Example code an LLM might generate\nimport requests\n\ndef query_trino(sql_query):\n    response = requests.post(\n        \"http://localhost:8008/query\",\n        json={\"query\": sql_query}\n    )\n    return response.json()\n\n# LLM-generated query\nresults = query_trino(\"SELECT job_title, AVG(salary) FROM memory.bullshit.real_bullshit_data GROUP BY job_title ORDER BY AVG(salary) DESC LIMIT 5\")\nprint(results[\"formatted_results\"])\n```\n\nThis approach allows LLMs to focus on generating SQL, while our tools handle all the MCP protocol complexity!\n\n## Demo and Validation Scripts 🚀\n\nWe've created some badass demo scripts that show how AI models can use the MCP protocol to run complex queries against Trino:\n\n### 1. Bullshit Data Generation and Loading\n\nThe `tools/create_bullshit_data.py` script generates a dataset of 10,000 employees with ridiculous job titles, inflated salaries, and a \"bullshit factor\" rating (1-10):\n\n```bash\n# Generate the bullshit data\npython tools/create_bullshit_data.py\n\n# Load the bullshit data into Trino's memory catalog\npython load_bullshit_data.py\n```\n\n### 2. Running Complex Queries through MCP\n\nThe `test_bullshit_query.py` script demonstrates end-to-end MCP interaction:\n- Connects to the MCP server using STDIO transport\n- Initializes the protocol following the MCP spec\n- Runs a complex SQL query with WHERE, GROUP BY, HAVING, ORDER BY\n- Processes and formats the results\n\n```bash\n# Run a complex query against the bullshit data through MCP\npython test_bullshit_query.py\n```\n\nExample output showing top BS jobs with high salaries:\n```\n🏆 TOP 10 BULLSHIT JOBS (high salary, high BS factor):\n----------------------------------------------------------------------------------------------------\nJOB_TITLE             | COUNT                | AVG_SALARY           | MAX_SALARY           | AVG_BS_FACTOR        \n----------------------------------------------------------------------------------------------------\nAdvanced Innovation Jedi | 2                    |            241178.50 |            243458.00 |                 7.50\nVP of Digital Officer | 1                    |            235384.00 |            235384.00 |                 7.00\nInnovation Technical Architect | 1                    |            235210.00 |            235210.00 |                 9.00\n...and more!\n```\n\n### 3. API Testing\n\nThe `test_llm_api.py` script validates the API functionality:\n\n```bash\n# Test the Docker container API \npython test_llm_api.py\n```\n\nThis performs a comprehensive check of:\n- API endpoint discovery\n- Documentation availability\n- Valid query execution\n- Error handling for invalid queries\n\n## Usage\n\n```bash\n# Start the server with docker-compose\ndocker-compose up -d\n```\n\nThe server will be available at:\n- Trino: http://localhost:9095\n- MCP server: http://localhost:9096\n- API server: http://localhost:9097\n\n## Client Connection\n\n✅ **IMPORTANT**: The client scripts run on your local machine (OUTSIDE Docker) and connect TO the Docker containers. The scripts automatically handle this by using docker exec commands. You don't need to be inside the container to use MCP!\n\nRunning tests from your local machine:\n\n```bash\n# Generate and load data into Trino\npython tools/create_bullshit_data.py  # Generates data locally\npython load_bullshit_data.py          # Loads data to Trino in Docker\n\n# Run MCP query through Docker\npython test_bullshit_query.py         # Queries using MCP in Docker\n```\n\n## Transport Options\n\nThis server supports two transport methods, but only STDIO is currently reliable:\n\n### STDIO Transport (Recommended and Working)\n\nSTDIO transport works reliably and is currently the only recommended method for testing and development:\n\n```bash\n# Run with STDIO transport inside the container\ndocker exec -i trino_mcp_trino-mcp_1 python -m trino_mcp.server --transport stdio --debug --trino-host trino --trino-port 8080 --trino-user trino --trino-catalog memory\n```\n\n### SSE Transport (NOT RECOMMENDED - Has Critical Issues)\n\nSSE is the default transport in MCP but has serious issues with the current MCP 1.3.0 version, causing server crashes on client disconnections. **Not recommended for use until these issues are resolved**:\n\n```bash\n# NOT RECOMMENDED: Run with SSE transport (crashes on disconnection)\ndocker exec trino_mcp_trino-mcp_1 python -m trino_mcp.server --transport sse --host 0.0.0.0 --port 8000 --debug\n```\n\n## Known Issues and Fixes\n\n### Fixed: Docker Container API Initialization\n\n✅ **FIXED**: We've resolved an issue where the API in the Docker container returned 503 Service Unavailable responses. The problem was with the `app_lifespan` function not properly initializing the `app_context_global` and Trino client connection. The fix ensures that:\n\n1. The Trino client explicitly connects during startup\n2. The AppContext global variable is properly initialized\n3. Health checks now work correctly\n\nIf you encounter 503 errors, check that your container has been rebuilt with the latest code:\n\n```bash\n# Rebuild and restart the container with the fix\ndocker-compose stop trino-mcp\ndocker-compose rm -f trino-mcp\ndocker-compose up -d trino-mcp\n```\n\n### MCP 1.3.0 SSE Transport Crashes\n\nThere's a critical issue with MCP 1.3.0's SSE transport that causes server crashes when clients disconnect. Until a newer MCP version is integrated, use STDIO transport exclusively. The error manifests as:\n\n```\nRuntimeError: generator didn't stop after athrow()\nanyio.BrokenResourceError\n```\n\n### Trino Catalog Handling\n\nWe fixed an issue with catalog handling in the Trino client. The original implementation attempted to use `USE catalog` statements, which don't work reliably. The fix directly sets the catalog in the connection parameters.\n\n## Project Structure\n\nThis project is organized as follows:\n\n- `src/` - Main source code for the Trino MCP server\n- `examples/` - Simple examples showing how to use the server\n- `scripts/` - Useful diagnostic and testing scripts\n- `tools/` - Utility scripts for data creation and setup\n- `tests/` - Automated tests\n\nKey files:\n- `llm_trino_api.py` - Standalone API server for LLM integration\n- `test_llm_api.py` - Test script for the API server\n- `test_mcp_stdio.py` - Main test script using STDIO transport (recommended)\n- `test_bullshit_query.py` - Complex query example with bullshit data\n- `load_bullshit_data.py` - Script to load generated data into Trino\n- `tools/create_bullshit_data.py` - Script to generate hilarious test data\n- `run_tests.sh` - Script to run automated tests\n- `examples/simple_mcp_query.py` - Simple example to query data using MCP\n\n## Development\n\n**IMPORTANT**: All scripts can be run from your local machine - they'll automatically communicate with the Docker containers via docker exec commands!\n\n```bash\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run automated tests \n./run_tests.sh\n\n# Test MCP with STDIO transport (recommended)\npython test_mcp_stdio.py\n\n# Simple example query\npython examples/simple_mcp_query.py \"SELECT 'Hello World' AS message\"\n```\n\n## Testing\n\nTo test that Trino queries are working correctly, use the STDIO transport test script:\n\n```bash\n# Recommended test method (STDIO transport)\npython test_mcp_stdio.py\n```\n\nFor more complex testing with the bullshit data:\n```bash\n# Load and query the bullshit data (shows the full power of Trino MCP!)\npython load_bullshit_data.py\npython test_bullshit_query.py\n```\n\nFor testing the LLM API endpoint:\n```bash\n# Test the Docker container API\npython test_llm_api.py \n\n# Test the standalone API (make sure it's running first)\npython llm_trino_api.py\ncurl -X POST \"http://localhost:8008/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"SELECT 1 AS test\"}'\n```\n\n## How LLMs Can Use This\n\nLLMs can use the Trino MCP server to:\n\n1. **Get Database Schema Information**:\n   ```python\n   # Example prompt to LLM: \"What schemas are available in the memory catalog?\"\n   # LLM can generate code to query:\n   query = \"SHOW SCHEMAS FROM memory\"\n   ```\n\n2. **Run Complex Analytical Queries**:\n   ```python\n   # Example prompt: \"Find the top 5 job titles with highest average salaries\"\n   # LLM can generate complex SQL:\n   query = \"\"\"\n   SELECT \n     job_title, \n     AVG(salary) as avg_salary\n   FROM \n     memory.bullshit.real_bullshit_data\n   GROUP BY \n     job_title\n   ORDER BY \n     avg_salary DESC\n   LIMIT 5\n   \"\"\"\n   ```\n\n3. **Perform Data Analysis and Present Results**:\n   ```python\n   # LLM can parse the response, extract insights and present to user:\n   \"The highest paying job title is 'Advanced Innovation Jedi' with an average salary of $241,178.50\"\n   ```\n\n### Real LLM Analysis Example: Bullshit Jobs by Company\n\nHere's a real example of what an LLM could produce when asked to \"Identify the companies with the most employees in bullshit jobs and create a Mermaid chart\":\n\n#### Step 1: LLM generates and runs the query\n\n```sql\nSELECT \n  company, \n  COUNT(*) as employee_count, \n  AVG(bullshit_factor) as avg_bs_factor \nFROM \n  memory.bullshit.real_bullshit_data \nWHERE \n  bullshit_factor > 7 \nGROUP BY \n  company \nORDER BY \n  employee_count DESC, \n  avg_bs_factor DESC \nLIMIT 10\n```\n\n#### Step 2: LLM gets and analyzes the results\n\n```\nCOMPANY | EMPLOYEE_COUNT | AVG_BS_FACTOR\n----------------------------------------\nUnknown Co | 2 | 9.0\nBitEdge | 1 | 10.0\nCyberWare | 1 | 10.0\nBitLink | 1 | 10.0\nAlgoMatrix | 1 | 10.0\nCryptoHub | 1 | 10.0\nBitGrid | 1 | 10.0\nMLStream | 1 | 10.0\nCloudCube | 1 | 10.0\nUltraEdge | 1 | 10.0\n```\n\n#### Step 3: LLM generates a Mermaid chart visualization\n\n```mermaid\n%%{init: {'theme': 'forest'}}%%\ngraph LR\n    title[Companies with Most Bullshit Jobs]\n    style title fill:#333,stroke:#333,stroke-width:1px,color:white,font-weight:bold,font-size:18px\n\n    Companies --> UnknownCo[Unknown Co]\n    Companies --> BitEdge[BitEdge]\n    Companies --> CyberWare[CyberWare]\n    Companies --> BitLink[BitLink]\n    Companies --> AlgoMatrix[AlgoMatrix]\n    Companies --> CryptoHub[CryptoHub]\n    Companies --> BitGrid[BitGrid]\n    Companies --> MLStream[MLStream]\n    Companies --> CloudCube[CloudCube]\n    Companies --> UltraEdge[UltraEdge]\n    \n    UnknownCo --- Count2[2 employees]\n    BitEdge --- Count1a[1 employee]\n    CyberWare --- Count1b[1 employee]\n    BitLink --- Count1c[1 employee]\n    AlgoMatrix --- Count1d[1 employee]\n    CryptoHub --- Count1e[1 employee]\n    BitGrid --- Count1f[1 employee]\n    MLStream --- Count1g[1 employee]\n    CloudCube --- Count1h[1 employee]\n    UltraEdge --- Count1i[1 employee]\n    \n    classDef company fill:#ff5733,stroke:#333,stroke-width:1px,color:white,font-weight:bold;\n    classDef count fill:#006100,stroke:#333,stroke-width:1px,color:white,font-weight:bold;\n    \n    class UnknownCo,BitEdge,CyberWare,BitLink,AlgoMatrix,CryptoHub,BitGrid,MLStream,CloudCube,UltraEdge company;\n    class Count2,Count1a,Count1b,Count1c,Count1d,Count1e,Count1f,Count1g,Count1h,Count1i count;\n```\n\n**Alternative Bar Chart:**\n\n```mermaid\n%%{init: {'theme': 'default'}}%%\npie showData\n    title Companies with Bullshit Jobs\n    \"Unknown Co (BS: 9.0)\" : 2\n    \"BitEdge (BS: 10.0)\" : 1\n    \"CyberWare (BS: 10.0)\" : 1\n    \"BitLink (BS: 10.0)\" : 1\n    \"AlgoMatrix (BS: 10.0)\" : 1\n    \"CryptoHub (BS: 10.0)\" : 1\n    \"BitGrid (BS: 10.0)\" : 1\n    \"MLStream (BS: 10.0)\" : 1\n    \"CloudCube (BS: 10.0)\" : 1\n    \"UltraEdge (BS: 10.0)\" : 1\n```\n\n#### Step 4: LLM provides key insights\n\nThe LLM can analyze the data and provide insights:\n\n- \"Unknown Co\" has the most employees in bullshit roles (2), while all others have just one\n- Most companies have achieved a perfect 10.0 bullshit factor score\n- Tech-focused companies (BitEdge, CyberWare, etc.) seem to create particularly meaningless roles\n- Bullshit roles appear concentrated at executive or specialized position levels\n\nThis example demonstrates how an LLM can:\n1. Generate appropriate SQL queries based on natural language questions\n2. Process and interpret the results from Trino\n3. Create visual representations of the data\n4. Provide meaningful insights and analysis\n\n## Accessing the API\n\nThe Trino MCP server now includes two API options for accessing data:\n\n### 1. Docker Container API (Port 9097)\n\n```python\nimport requests\nimport json\n\n# API endpoint (default port 9097 in Docker setup)\napi_url = \"http://localhost:9097/api/query\"\n\n# Define your SQL query\nquery_data = {\n    \"query\": \"SELECT * FROM memory.bullshit.real_bullshit_data LIMIT 5\",\n    \"catalog\": \"memory\",\n    \"schema\": \"bullshit\"\n}\n\n# Send the request\nresponse = requests.post(api_url, json=query_data)\nresults = response.json()\n\n# Process the results\nif results[\"success\"]:\n    print(f\"Query returned {results['results']['row_count']} rows\")\n    for row in results['results']['rows']:\n        print(row)\nelse:\n    print(f\"Query failed: {results['message']}\")\n```\n\n### 2. Standalone Python API (Port 8008)\n\n```python\n# Same code as above, but with different port\napi_url = \"http://localhost:8008/query\"\n```\n\nBoth APIs offer the following endpoints:\n- `GET /api` - API documentation and usage examples\n- `POST /api/query` - Execute SQL queries against Trino\n\nThese APIs eliminate the need for wrapper scripts and let LLMs query Trino directly using REST calls, making it much simpler to integrate with services like Claude, GPT, and other AI systems.\n\n## Troubleshooting\n\n### API Returns 503 Service Unavailable\n\nIf the Docker container API returns 503 errors:\n\n1. Make sure you've rebuilt the container with the latest code:\n   ```bash\n   docker-compose stop trino-mcp\n   docker-compose rm -f trino-mcp\n   docker-compose up -d trino-mcp\n   ```\n\n2. Check the container logs for errors:\n   ```bash\n   docker logs trino_mcp_trino-mcp_1\n   ```\n\n3. Verify that Trino is running properly:\n   ```bash\n   curl -s http://localhost:9095/v1/info | jq\n   ```\n\n### Port Conflicts with Standalone API\n\nThe standalone API defaults to port 8008 to avoid conflicts. If you see an \"address already in use\" error:\n\n1. Edit `llm_trino_api.py` and change the port number in the last line:\n   ```python\n   uvicorn.run(app, host=\"127.0.0.1\", port=8008) \n   ```\n\n2. Run with a custom port via command line:\n   ```bash\n   python -c \"import llm_trino_api; import uvicorn; uvicorn.run(llm_trino_api.app, host='127.0.0.1', port=8009)\"\n   ```\n\n## Future Work\n\nThis is now in beta with these improvements planned:\n\n- [ ] Integrate with newer MCP versions when available to fix SSE transport issues\n- [ ] Add/Validate support for Hive, JDBC, and other connectors\n- [ ] Add more comprehensive query validation across different types and complexities\n- [ ] Implement support for more data types and advanced Trino features\n- [ ] Improve error handling and recovery mechanisms\n- [ ] Add user authentication and permission controls\n- [ ] Create more comprehensive examples and documentation\n- [ ] Develop admin monitoring and management interfaces\n- [ ] Add performance metrics and query optimization hints\n- [ ] Implement support for long-running queries and result streaming\n\n---\n\n*Developed by Stink Labs, 2025*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "querying",
        "enables querying",
        "query engine",
        "database access"
      ],
      "category": "databases"
    },
    "stuzero--pg-mcp": {
      "owner": "stuzero",
      "name": "pg-mcp",
      "url": "https://github.com/stuzero/pg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/stuzero.webp",
      "description": "Connect to and query PostgreSQL databases, enabling schema discovery, query analysis, and contextual extensions for enhanced AI interactions. Facilitates seamless database integration through a robust API.",
      "stars": 24,
      "forks": 0,
      "license": "No License",
      "language": "HTML",
      "updated_at": "2025-09-03T20:52:44Z",
      "readme_content": "# pg-mcp\n\nThis repository hosts the **static documentation site** for [`pg-mcp-server`](https://github.com/stuzero/pg-mcp-server), available live at:\n\n## 👉 **https://stuzero.github.io/pg-mcp/**\n\nIf you're looking for the source code for `pg-mcp-server`, it's been moved to:\n\n🔗 **https://github.com/stuzero/pg-mcp-server**\n\n`pg-mcp-client` can be found here:\n\n🔗 **https://github.com/stuzero/pg-mcp-client**",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "postgresql databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "subnetmarco--pgmcp": {
      "owner": "subnetmarco",
      "name": "pgmcp",
      "url": "https://github.com/subnetmarco/pgmcp",
      "imageUrl": "",
      "description": "Natural language PostgreSQL queries with automatic streaming, read-only safety, and universal database compatibility.",
      "stars": 476,
      "forks": 48,
      "license": "Other",
      "language": "Go",
      "updated_at": "2025-10-03T16:19:26Z",
      "readme_content": "[![ci](https://github.com/subnetmarco/pgmcp/actions/workflows/ci.yml/badge.svg)](https://github.com/subnetmarco/pgmcp/actions/workflows/ci.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/subnetmarco/pgmcp)](https://goreportcard.com/report/github.com/subnetmarco/pgmcp)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n# PGMCP - PostgreSQL Model Context Protocol Server\n\nPGMCP connects AI assistants to **any PostgreSQL database** through natural language queries. Ask questions in plain English and get structured SQL results with automatic streaming and robust error handling.\n\n**Works with**: Cursor, Claude Desktop, VS Code extensions, and any [MCP-compatible client](https://modelcontextprotocol.io/)\n\n## Quick Start\n\nPGMCP connects to **your existing PostgreSQL database** and makes it accessible to AI assistants through natural language queries.\n\n### Prerequisites\n- PostgreSQL database (existing database with your schema)\n- OpenAI API key (optional, for AI-powered SQL generation)\n\n### Basic Usage\n\n```bash\n# Set up environment variables\nexport DATABASE_URL=\"postgres://user:password@localhost:5432/your-existing-db\"\nexport OPENAI_API_KEY=\"your-api-key\"  # Optional\n\n# Run server (using pre-compiled binary)\n./pgmcp-server\n\n# Test with client in another terminal\n./pgmcp-client -ask \"What tables do I have?\" -format table\n./pgmcp-client -ask \"Who is the customer that has placed the most orders?\" -format table\n./pgmcp-client -search \"john\" -format table\n```\n\nHere is how it works:\n\n```\n👤 User / AI Assistant\n         │\n         │ \"Who are the top customers?\"\n         ▼\n┌─────────────────────────────────────────────────────────────┐\n│                    Any MCP Client                           │\n│                                                             │\n│  PGMCP CLI  │  Cursor  │  Claude Desktop  │  VS Code  │ ... │\n│  JSON/CSV   │  Chat    │  AI Assistant    │  Editor   │     │\n└─────────────────────────────────────────────────────────────┘\n         │\n         │ Streamable HTTP / MCP Protocol\n         ▼\n┌─────────────────────────────────────────────────────────────┐\n│                    PGMCP Server                             │\n│                                                             │\n│  🔒 Security    🧠 AI Engine      🌊 Streaming               │\n│  • Input Valid  • Schema Cache    • Auto-Pagination         │\n│  • Audit Log    • OpenAI API      • Memory Management       │\n│  • SQL Guard    • Error Recovery  • Connection Pool         │\n└─────────────────────────────────────────────────────────────┘\n         │\n         │ Read-Only SQL Queries\n         ▼\n┌─────────────────────────────────────────────────────────────┐\n│                Your PostgreSQL Database                     │\n│                                                             │\n│  Any Schema: E-commerce, Analytics, CRM, etc.               │\n│  Tables • Views • Indexes • Functions                       │\n└─────────────────────────────────────────────────────────────┘\n\nExternal AI Services:\nOpenAI API • Anthropic • Local LLMs (Ollama, etc.)\n\nKey Benefits:\n✅ Works with ANY PostgreSQL database (no assumptions about schema)\n✅ No schema modifications required  \n✅ Read-only access (100% safe)\n✅ Automatic streaming for large results\n✅ Intelligent query understanding (singular vs plural)\n✅ Robust error handling (graceful AI failure recovery)\n✅ PostgreSQL case sensitivity support (mixed-case tables)\n✅ Production-ready security and performance\n✅ Universal database compatibility\n✅ Multiple output formats (table, JSON, CSV)\n✅ Free-text search across all columns\n✅ Authentication support\n✅ Comprehensive testing suite\n```\n\n## Features\n\n- **Natural Language to SQL**: Ask questions in plain English\n- **Automatic Streaming**: Handles large result sets automatically  \n- **Safe Read-Only Access**: Prevents any write operations\n- **Text Search**: Search across all text columns\n- **Multiple Output Formats**: Table, JSON, and CSV\n- **PostgreSQL Case Sensitivity**: Handles mixed-case table names correctly\n- **Universal Compatibility**: Works with any PostgreSQL database\n\n### Environment Variables\n\n**Required:**\n- `DATABASE_URL`: PostgreSQL connection string to your existing database\n\n**Optional:**\n- `OPENAI_API_KEY`: OpenAI API key for AI-powered SQL generation\n- `OPENAI_MODEL`: Model to use (default: \"gpt-4o-mini\")\n- `HTTP_ADDR`: Server address (default: \":8080\")\n- `HTTP_PATH`: MCP endpoint path (default: \"/mcp\")\n- `AUTH_BEARER`: Bearer token for authentication\n\n## Installation\n\n### Download Pre-compiled Binaries\n\n1. Go to [GitHub Releases](https://github.com/subnetmarco/pgmcp/releases)\n2. Download the binary for your platform (Linux, macOS, Windows)\n3. Extract and run:\n\n```bash\n# Example for macOS/Linux\ntar xzf pgmcp_*.tar.gz\ncd pgmcp_*\n./pgmcp-server\n```\n\n### Alternative Options\n\n```bash\n# Homebrew (macOS/Linux) - Available after first release\nbrew tap subnetmarco/homebrew-tap\nbrew install pgmcp\n\n# Build from source\ngo build -o pgmcp-server ./server\ngo build -o pgmcp-client ./client\n```\n\n### Docker/Kubernetes\n\n```bash\n# Docker\ndocker run -e DATABASE_URL=\"postgres://user:pass@host:5432/db\" \\\n  -p 8080:8080 ghcr.io/subnetmarco/pgmcp:latest\n\n# Kubernetes (see examples/ directory for full manifests)\nkubectl create secret generic pgmcp-secret \\\n  --from-literal=database-url=\"postgres://user:pass@host:5432/db\"\nkubectl apply -f examples/k8s/\n```\n\n#### Quick Start\n\n```bash\n# Set up database (optional - works with any existing PostgreSQL database)\nexport DATABASE_URL=\"postgres://user:password@localhost:5432/mydb\"\npsql $DATABASE_URL < schema.sql\n\n# Run server\nexport OPENAI_API_KEY=\"your-api-key\"\n./pgmcp-server\n\n# Test with client\n./pgmcp-client -ask \"Who is the user that places the most orders?\" -format table\n./pgmcp-client -ask \"Show me the top 40 most reviewed items in the marketplace\" -format table\n```\n\n### Environment Variables\n\n**Required:**\n- `DATABASE_URL`: PostgreSQL connection string\n\n**Optional:**\n- `OPENAI_API_KEY`: OpenAI API key for SQL generation\n- `OPENAI_MODEL`: Model to use (default: \"gpt-4o-mini\")\n- `HTTP_ADDR`: Server address (default: \":8080\")\n- `HTTP_PATH`: MCP endpoint path (default: \"/mcp\")\n- `AUTH_BEARER`: Bearer token for authentication\n\n## Usage Examples\n\n```bash\n# Ask questions in natural language\n./pgmcp-client -ask \"What are the top 5 customers?\" -format table\n./pgmcp-client -ask \"How many orders were placed today?\" -format json\n\n# Search across all text fields\n./pgmcp-client -search \"john\" -format table\n\n# Multiple questions at once\n./pgmcp-client -ask \"Show tables\" -ask \"Count users\" -format table\n\n# Different output formats\n./pgmcp-client -ask \"Export all data\" -format csv -max-rows 1000\n```\n\n## Example Database\n\nThe project includes two schemas:\n- **`schema.sql`**: Full Amazon-like marketplace with 5,000+ records\n- **`schema_minimal.sql`**: Minimal test schema with mixed-case `\"Categories\"` table\n\n**Key features:**\n- **Mixed-case table names** (`\"Categories\"`) for testing case sensitivity\n- **Composite primary keys** (`order_items`) for testing AI assumptions\n- **Realistic relationships** and data types\n\nUse your own database:\n```bash\nexport DATABASE_URL=\"postgres://user:pass@host:5432/your_db\"\n./pgmcp-server\n./pgmcp-client -ask \"What tables do I have?\"\n```\n\n## AI Error Handling\n\nWhen AI generates incorrect SQL, PGMCP handles it gracefully:\n\n```json\n{\n  \"error\": \"Column not found in generated query\",\n  \"suggestion\": \"Try rephrasing your question or ask about specific tables\",\n  \"original_sql\": \"SELECT non_existent_column FROM table...\"\n}\n```\n\nInstead of crashing, the system provides helpful feedback and continues operating.\n\n## MCP Integration\n\n### Cursor Integration\n\n```bash\n# Start server\nexport DATABASE_URL=\"postgres://user:pass@localhost:5432/your_db\"\n./pgmcp-server\n```\n\nAdd to Cursor settings:\n```json\n{\n  \"mcp.servers\": {\n    \"pgmcp\": {\n      \"transport\": {\n        \"type\": \"http\",\n        \"url\": \"http://localhost:8080/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Claude Desktop Integration\n\nEdit `~/.config/claude-desktop/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"pgmcp\": {\n      \"transport\": {\n        \"type\": \"http\",\n        \"url\": \"http://localhost:8080/mcp\"\n      }\n    }\n  }\n}\n```\n\n## API Tools\n\n- **`ask`**: Natural language questions → SQL queries with automatic streaming\n- **`search`**: Free-text search across all database text columns  \n- **`stream`**: Advanced streaming for very large result sets with pagination\n\n## Safety Features\n\n- **Read-Only Enforcement**: Blocks write operations (INSERT, UPDATE, DELETE, etc.)\n- **Query Timeouts**: Prevents long-running queries\n- **Input Validation**: Sanitizes and validates all user input\n- **Transaction Isolation**: All queries run in read-only transactions\n\n## Testing\n\n```bash\n# Unit tests\ngo test ./server -v\n\n# Integration tests (requires PostgreSQL)\ngo test ./server -tags=integration -v\n```\n\n## License\n\nApache 2.0 - See LICENSE file for details.\n\n## Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io/) - The underlying protocol specification\n- [MCP Go SDK](https://github.com/modelcontextprotocol/go-sdk) - Go implementation of MCP\n\n---\n\nPGMCP makes your PostgreSQL database accessible to AI assistants through natural language while maintaining security through read-only access controls.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "secure database",
        "databases secure",
        "postgresql queries"
      ],
      "category": "databases"
    },
    "supabase-community--supabase-mcp": {
      "owner": "supabase-community",
      "name": "supabase-mcp",
      "url": "https://github.com/supabase-community/supabase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/supabase-community.webp",
      "description": "Connect Supabase projects with AI assistants for managing tables, fetching configurations, and querying data efficiently. Enables seamless integration of AI capabilities directly into application workflows.",
      "stars": 2142,
      "forks": 236,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-04T09:37:46Z",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "access supabase",
        "supabase projects",
        "supabase community"
      ],
      "category": "databases"
    },
    "sussa3007--mysql-mcp": {
      "owner": "sussa3007",
      "name": "mysql-mcp",
      "url": "https://github.com/sussa3007/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sussa3007.webp",
      "description": "Connect and interact with MySQL databases, execute SQL queries, manage database connections, and retrieve data directly through AI models.",
      "stars": 17,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-18T21:51:53Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@sussa3007/mysql-mcp)](https://smithery.ai/server/@sussa3007/mysql-mcp)\n\n\n<a href=\"https://glama.ai/mcp/servers/@sussa3007/mysql-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@sussa3007/mysql-mcp/badge\" />\n</a>\n\n# MySQL MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Model Context Protocol (MCP) server for MySQL databases that enables AI models to interact with MySQL databases through a structured interface.\n\n## Overview\n\nThe MySQL MCP Server provides a bridge between AI models and MySQL databases, allowing AI agents to query and analyze MySQL data. This implementation follows the Model Context Protocol specification and offers both web server and CLI modes of operation.\n\n## Features\n\n- MySQL database connection management\n- SQL query execution\n- Table listing and structure inspection\n- Database listing and selection\n- Real-time status monitoring via SSE (Server-Sent Events)\n- Web interface for testing MCP tools\n- Support for both stdio and SSE transport methods\n- Docker deployment ready\n\n## Installation\n\n```bash\n# Global installation\nnpm install -g mysql-mcp\n\n# Local installation\nnpm install mysql-mcp\n```\n\n## Using with AI Assistants\n\n### Using the Published Server on Smithery.ai\n\nThe MySQL MCP Server is published on Smithery.ai and can be easily used with various AI assistants:\n\n1. **Access the server**: Visit [https://smithery.ai/server/@sussa3007/mysql-mcp](https://smithery.ai/server/@sussa3007/mysql-mcp)\n\n2. **Configure the server**:\n\n   - Set your MySQL database connection details:\n     - MYSQL_HOST\n     - MYSQL_PORT\n     - MYSQL_USER\n     - MYSQL_PASSWORD\n     - MYSQL_DATABASE\n     - MYSQL_READONLY (optional, set to True for read-only access)\n\n3. **Connect with supported AI platforms**:\n\n   - Anthropic Claude\n   - Cursor AI\n   - Windsurf\n   - Cline\n   - Witsy\n   - Enconvo\n   - Goose\n\n4. **Authentication**: Login to Smithery.ai to save your configuration and generate authentication tokens.\n\n5. **Use in AI prompts**: Once connected, you can utilize MySQL tools in your AI conversations by asking the assistant to perform database operations.\n\n### Using After Local Installation\n\nTo use a locally developed version:\n\n1. Run `npm link` in your project directory\n2. Configure your settings file as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"mysql-mcp\"],\n      \"env\": {\n        \"MYSQL_HOST\": \"localhost\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASSWORD\": \"your_password\",\n        \"MYSQL_DATABASE\": \"your_database\",\n        \"MYSQL_READONLY\": \"true\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\n### status\n\nCheck the current database connection status.\n\n- **Inputs**: No parameters required\n- **Returns**: Connection status information, including host, port, database, and username if connected.\n\n### connect\n\nConnect to a MySQL database.\n\n- **Inputs**:\n  - host (optional string): Database server hostname or IP address\n  - port (optional string): Database server port\n  - user (optional string): Database username\n  - password (optional string): Database password\n  - database (optional string): Database name to connect to\n- **Returns**: Connection success message or error details.\n\n### disconnect\n\nClose the current MySQL database connection.\n\n- **Inputs**: No parameters required\n- **Returns**: Disconnection success message or error details.\n\n### query\n\nExecute an SQL query on the connected database.\n\n- **Inputs**:\n  - sql (string): SQL query to execute\n  - params (optional array): Parameters for prepared statements\n- **Returns**: Query results as JSON or error message.\n\n### list_tables\n\nGet a list of tables in the current database.\n\n- **Inputs**: No parameters required\n- **Returns**: List of table names in the current database.\n\n### describe_table\n\nGet the structure of a specific table.\n\n- **Inputs**:\n  - table (string): Name of the table to describe\n- **Returns**: Table structure details including columns, types, keys, and other attributes.\n\n### list_databases\n\nGet a list of all accessible databases on the server.\n\n- **Inputs**: No parameters required\n- **Returns**: List of database names available on the server.\n\n### use_database\n\nSwitch to a different database.\n\n- **Inputs**:\n  - database (string): Name of the database to switch to\n- **Returns**: Confirmation message or error details.\n\n## Keywords\n\nmysql, mcp, database, ai, model context protocol\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "syahiidkamil--mcp-postgres-full-access": {
      "owner": "syahiidkamil",
      "name": "mcp-postgres-full-access",
      "url": "https://github.com/syahiidkamil/mcp-postgres-full-access",
      "imageUrl": "/freedevtools/mcp/pfp/syahiidkamil.webp",
      "description": "Interact with PostgreSQL databases by performing safe read and write operations with enhanced schema metadata and transaction management. Leverage the advanced capabilities of PostgreSQL while maintaining strict safety controls.",
      "stars": 17,
      "forks": 7,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T08:54:53Z",
      "readme_content": "# PostgreSQL Full Access MCP Server\n\n[![Model Context Protocol](https://img.shields.io/badge/MCP-Compatible-blue.svg)](https://modelcontextprotocol.io)\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n\nA powerful Model Context Protocol server providing **full read-write access** to PostgreSQL databases. Unlike the read-only official MCP PostgreSQL server, this enhanced implementation allows Large Language Models (LLMs) to both query and modify database content with proper transaction management and safety controls.\n\n## Table of Contents\n\n- [Features](#features)\n  - [Full Read-Write Access](#full-read-write-access)\n  - [Rich Schema Information](#rich-schema-information)\n  - [Advanced Safety Controls](#advanced-safety-controls)\n- [Tools](#tools)\n  - [execute_query](#execute_query)\n  - [execute_dml_ddl_dcl_tcl](#execute_dml_ddl_dcl_tcl)\n  - [execute_maintenance](#execute_maintenance)\n  - [execute_commit](#execute_commit)\n  - [execute_rollback](#execute_rollback)\n  - [list_tables](#list_tables)\n  - [describe_table](#describe_table)\n- [Resources](#resources)\n- [Using with Claude Desktop](#using-with-claude-desktop)\n  - [Claude Desktop Integration](#claude-desktop-integration)\n  - [Important: Using \"Allow Once\" for Safety](#important-using-allow-once-for-safety)\n- [Environment Variables](#environment-variables)\n- [Using Full Database Access with Claude](#using-full-database-access-with-claude)\n- [Security Considerations](#security-considerations)\n  - [Database User Permissions](#database-user-permissions)\n  - [Best Practices for Safe Usage](#best-practices-for-safe-usage)\n- [Docker](#docker)\n- [License](#license)\n- [Comparison with Official PostgreSQL MCP Server](#comparison-with-official-postgresql-mcp-server)\n\n## 🌟 Features\n\n### Full Read-Write Access\n\n- Safely execute DML operations (INSERT, UPDATE, DELETE)\n- Create, alter, and manage database objects with DDL\n- Transaction management with explicit commit\n- Safety timeouts and automatic rollback protection\n\n### Rich Schema Information\n\n- Detailed column metadata (data types, descriptions, max length, nullability)\n- Primary key identification\n- Foreign key relationships\n- Index information with type and uniqueness flags\n- Table row count estimates\n- Table and column descriptions (when available)\n\n### Advanced Safety Controls\n\n- SQL query classification (DQL, DML, DDL, DCL, TCL)\n- Enforced read-only execution for safe queries\n- All operations run in isolated transactions\n- Automatic transaction timeout monitoring\n- Configurable safety limits\n- Two-step transaction commit process with explicit user confirmation\n\n## 🔧 Tools\n\n- **execute_query**\n\n  - Execute read-only SQL queries (SELECT statements)\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n  - Results include execution time metrics and field information\n\n- **execute_dml_ddl_dcl_tcl**\n\n  - Execute data modification operations (INSERT, UPDATE, DELETE) or schema changes (CREATE, ALTER, DROP)\n  - Input: `sql` (string): The SQL statement to execute\n  - Automatically wrapped in a transaction with configurable timeout\n  - Returns a transaction ID for explicit commit\n  - **Important safety feature**: The conversation will end after execution, allowing the user to review the results before deciding to commit or rollback\n\n- **execute_maintenance**\n\n  - Execute maintenance commands like VACUUM, ANALYZE, or CREATE DATABASE outside of transactions\n  - Input: `sql` (string): The SQL statement to execute - must be VACUUM, ANALYZE, or CREATE DATABASE\n  - Returns a result object with execution time metrics\n\n- **execute_commit**\n\n  - Explicitly commit a transaction by its ID\n  - Input: `transaction_id` (string): ID of the transaction to commit\n  - Safely handles cleanup after commit or rollback\n  - Permanently applies changes to the database\n\n- **execute_rollback**\n\n  - Explicitly rollback a transaction by its ID\n  - Input: `transaction_id` (string): ID of the transaction to rollback\n  - Safely discards all changes and cleans up resources\n  - Useful when reviewing changes and deciding not to apply them\n\n- **list_tables**\n\n  - Get a comprehensive list of all tables in the database\n  - Includes column count and table descriptions\n  - No input parameters required\n\n- **describe_table**\n  - Get detailed information about a specific table structure\n  - Input: `table_name` (string): Name of the table to describe\n  - Returns complete schema information including primary keys, foreign keys, indexes, and column details\n\n## 📊 Resources\n\nThe server provides enhanced schema information for database tables:\n\n- **Table Schemas** (`postgres://<host>/<table>/schema`)\n  - Detailed JSON schema information for each table\n  - Includes complete column metadata, primary keys, and constraints\n  - Automatically discovered from database metadata\n\n## 🚀 Using with Claude Desktop\n\n### Claude Desktop Integration\n\nTo use this server with Claude Desktop, follow these steps:\n\n1. First, ensure you have Node.js installed on your system\n2. Install the package using npx or add it to your project\n\n3. Configure Claude Desktop by editing `claude_desktop_config.json` (typically found at `~/Library/Application Support/Claude/` on macOS):\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres-full\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-postgres-full-access\",\n        \"postgresql://username:password@localhost:5432/database\"\n      ],\n      \"env\": {\n        \"TRANSACTION_TIMEOUT_MS\": \"60000\",\n        \"MAX_CONCURRENT_TRANSACTIONS\": \"5\",\n        \"PG_STATEMENT_TIMEOUT_MS\": \"30000\"\n      }\n    }\n  }\n}\n```\n\n4. Replace the database connection string with your actual PostgreSQL connection details\n5. Restart Claude Desktop completely\n\n### Important: Using \"Allow Once\" for Safety\n\nWhen Claude attempts to commit changes to your database, Claude Desktop will prompt you for approval:\n\n\n\n**Always review the SQL changes carefully before approving them!**\n\nBest practices for safety:\n\n- Always click \"Allow once\" (not \"Always allow\") for commit operations\n- Review the transaction SQL carefully before approving\n- Consider using a database user with limited permissions\n- Use a testing database if possible when first trying this server\n\nThis \"Allow once\" approach gives you full control to prevent unwanted changes to your database while still enabling Claude to help with data management tasks when needed.\n\n## ⚙️ Environment Variables\n\nYou can customize the server behavior with environment variables in your Claude Desktop config:\n\n```json\n\"env\": {\n  \"TRANSACTION_TIMEOUT_MS\": \"60000\",\n  \"MAX_CONCURRENT_TRANSACTIONS\": \"5\"\n}\n```\n\nKey environment variables:\n\n- `TRANSACTION_TIMEOUT_MS`: Transaction timeout in milliseconds (default: 15000)\n\n  - Increase this if your transactions need more time\n  - Transactions exceeding this time will be automatically rolled back for safety\n\n- `MAX_CONCURRENT_TRANSACTIONS`: Maximum concurrent transactions (default: 10)\n\n  - Lower this number for more conservative operation\n  - Higher values allow more simultaneous write operations\n\n- `ENABLE_TRANSACTION_MONITOR`: Enable/disable transaction monitor (\"true\" or \"false\", default: \"true\")\n\n  - Monitors and automatically rolls back abandoned transactions\n  - Rarely needs to be disabled\n\n- `PG_STATEMENT_TIMEOUT_MS`: SQL query execution timeout in ms (default: 30000)\n\n  - Limits how long any single SQL statement can run\n  - Important safety feature to prevent runaway queries\n\n- `PG_MAX_CONNECTIONS`: Maximum PostgreSQL connections (default: 20)\n\n  - Important to stay within your database's connection limits\n\n- `MONITOR_INTERVAL_MS`: How often to check for stuck transactions (default: 5000)\n  - Usually doesn't need adjustment\n\n## 🔄 Using Full Database Access with Claude\n\nThis server enables Claude to both read from and write to your PostgreSQL database with your approval. Here are some example conversation flows:\n\n### Example: Creating a New Table and Adding Data\n\nYou: \"I need a new products table with columns for id, name, price, and inventory\"\n\nClaude: _Analyzes your database and creates a query_\n\n```sql\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    price DECIMAL(10,2) NOT NULL,\n    inventory INTEGER DEFAULT 0\n);\n```\n\n_Claude Desktop will prompt you to approve this operation_\n\nYou: _Review and click \"Allow once\"_\n\nClaude: \"I've created the products table. Would you like me to add some sample data?\"\n\nYou: \"Yes, please add 5 sample products\"\n\nClaude: _Creates INSERT statements and prompts for approval_\n_You review and approve with \"Allow once\"_\n\n### Example: Data Analysis with Safe Queries\n\nYou: \"What are my top 3 products by price?\"\n\nClaude: _Executes a read-only query automatically_\n_Shows you the results_\n\n### Safety Workflow\n\nThe key safety feature is the two-step approach for any operation that modifies your database:\n\n1. Claude analyzes your request and prepares SQL\n2. For read-only operations (SELECT), Claude executes automatically\n3. For write operations (INSERT, UPDATE, DELETE, CREATE, etc.):\n   - Claude executes the SQL in a transaction and ends the conversation\n   - You review the results\n   - In a new conversation, you respond with \"Yes\" to commit or \"No\" to rollback\n   - Claude Desktop shows you exactly what will be changed and asks for permission\n   - You click \"Allow once\" to permit the specific operation\n   - Claude executes the operation and returns results\n\nThis gives you multiple opportunities to verify changes before they're permanently applied to the database.\n\n## ⚠️ Security Considerations\n\nWhen connecting Claude to your database with write access:\n\n### Database User Permissions\n\n**IMPORTANT:** Create a dedicated database user with appropriate permissions:\n\n```sql\n-- Example of creating a restricted user (adjust as needed)\nCREATE USER claude_user WITH PASSWORD 'secure_password';\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO claude_user;\nGRANT INSERT, UPDATE, DELETE ON TABLE table1, table2 TO claude_user;\n-- Only grant specific permissions as needed\n```\n\n### Best Practices for Safe Usage\n\n1. **Always use \"Allow once\"** to review each write operation\n\n   - Never select \"Always allow\" for database modifications\n   - Take time to review the SQL carefully\n\n2. **Connect to a testing database** when first exploring this tool\n\n   - Consider using a database copy/backup for initial testing\n\n3. **Limit database user permissions** to only what's necessary\n\n   - Avoid using a superuser or admin account\n   - Grant table-specific permissions when possible\n\n4. **Implement database backups** before extensive use\n\n5. **Never share sensitive data** that shouldn't be exposed to LLMs\n\n6. **Verify all SQL operations** before approving them\n   - Check table names\n   - Verify column names and data\n   - Confirm WHERE clauses are appropriate\n   - Look for proper transaction handling\n\n### Docker\n\nThe server can be easily run in a Docker container:\n\n```bash\n# Build the Docker image\ndocker build -t mcp-postgres-full-access .\n\n# Run the container\ndocker run -i --rm mcp-postgres-full-access \"postgresql://username:password@host:5432/database\"\n```\n\nFor Docker on macOS, use host.docker.internal to connect to the host network:\n\n```bash\ndocker run -i --rm mcp-postgres-full-access \"postgresql://username:password@host.docker.internal:5432/database\"\n```\n\n## 📄 License\n\nThis MCP server is licensed under the MIT License.\n\n## 💡 Comparison with Official PostgreSQL MCP Server\n\n| Feature             | This Server            | Official MCP PostgreSQL Server |\n| ------------------- | ---------------------- | ------------------------------ |\n| Read Access         | ✅                     | ✅                             |\n| Write Access        | ✅                     | ❌                             |\n| Schema Details      | Enhanced               | Basic                          |\n| Transaction Support | Explicit with timeouts | Read-only                      |\n| Index Information   | ✅                     | ❌                             |\n| Foreign Key Details | ✅                     | ❌                             |\n| Row Count Estimates | ✅                     | ❌                             |\n| Table Descriptions  | ✅                     | ❌                             |\n\n## Author\n\nCreated by Syahiid Nur Kamil ([@syahiidkamil](https://github.com/syahiidkamil))\n\n---\n\nCopyright © 2024 Syahiid Nur Kamil. All rights reserved.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "postgres",
        "databases",
        "postgresql databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "t3ta--sql-mcp-server": {
      "owner": "t3ta",
      "name": "sql-mcp-server",
      "url": "https://github.com/t3ta/sql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/t3ta.webp",
      "description": "Enables interaction with PostgreSQL databases through secure SSH tunnels, facilitating efficient querying and maintaining data integrity. Designed for local, containerized, or AI-driven applications with support for AWS RDS in read-only mode.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-07T02:51:52Z",
      "readme_content": "# SQL MCP Server (TypeScript)\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-4.x-blue?logo=typescript)](https://www.typescriptlang.org/)\n[](https://github.com/your-org/sql-mcp-server/actions/workflows/ci.yml)\n[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n[![Platform](https://img.shields.io/badge/platform-CLI-lightgrey.svg)]()\n\nThis project provides a **TypeScript implementation** of a Model Context Protocol (MCP) server that enables language models and other MCP-compatible clients to query PostgreSQL databases—via SSH bastion tunnels when required.\n\nBuilt for flexibility and secure database access, it supports AWS RDS with read-only transactions and uses stdin/stdout-based communication, making it suitable for local, containerized, or AI-driven use cases.\n\n## Features\n\n- 🔒 SSH bastion support for secure access to private RDS instances via SSH tunnels\n- 🐘 PostgreSQL read-only query engine using the `pg` library\n- 📡 STDIO-based MCP protocol transport\n- 🧠 Compatible with [memory-bank-mcp-server](https://github.com/memcloud-ai/memory-bank-mcp-server)\n- ⚙️ Easily configurable via `.env` or environment variables\n- 🧪 Fully testable with Jest and mocks\n\n---\n\n## Installation\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/your-org/sql-mcp-server.git\ncd sql-mcp-server\nnpm install\nnpm run build\n\nConfiguration (Optional .env file)\n\nCreate a .env file in the project root:\n\nDB_USER=postgres\nDB_PASS=yourpassword\nDB_HOST=localhost\nDB_PORT=5432\nDB_NAME=mydatabase\nUSE_SSH_TUNNEL=true\nSSH_BASTION_HOST=bastion.example.com\nSSH_BASTION_USER=ec2-user\nSSH_PRIVATE_KEY_PATH=~/.ssh/id_rsa\n\nUsage\n\nRun the server using npx:\n\nnpx -y @modelcontextprotocol/server-postgres postgresql://<user>:<pass>@localhost:5433/<dbname>\n\nFor direct connection without SSH tunneling, set the appropriate environment variables:\n\nDB_HOST=rds-host.amazonaws.com DB_PORT=5432 npx -y @modelcontextprotocol/server-postgres postgresql://<user>:<pass>@rds-host.amazonaws.com/<dbname>\n\nSample Input\n\n{\n  \"type\": \"call_tool\",\n  \"params\": {\n    \"name\": \"query\",\n    \"arguments\": {\n      \"sql\": \"SELECT * FROM users LIMIT 10\"\n    }\n  }\n}\n\nSample Output\n\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"[{\\\"id\\\":1,\\\"name\\\":\\\"Alice\\\"}]\"\n    }\n  ],\n  \"isError\": false\n}\n\n\n\n⸻\n\nRelated Docs\n\t•\tArchitecture\n\t•\tDomain Models\n\t•\tGlossary\n\t•\tCoding Standards\n\t•\tTech Stack\n\t•\tUser Guide\n\n⸻\n\nLicense\n\nMIT\n\n⸻\n\nContribution Guide\n\nWe welcome community contributions! Please see CONTRIBUTING.md for details.\n\n⸻\n\nCompatibility\n\nThis implementation follows the Model Context Protocol (MCP) and has been tested for compatibility with:\n\t•\tmemory-bank-mcp-server\n\t•\tClaude Desktop (via STDIO)\n\t•\tCursor IDE\n\t•\tSupabase + MCP integration\n\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "t3ta",
        "database",
        "databases secure",
        "secure database",
        "t3ta sql"
      ],
      "category": "databases"
    },
    "takuya0206--bigquery-mcp-server": {
      "owner": "takuya0206",
      "name": "bigquery-mcp-server",
      "url": "https://github.com/takuya0206/bigquery-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/takuya0206.webp",
      "description": "Access Google BigQuery datasets and execute SQL queries using a simple interface that incorporates Large Language Models. The server manages authentication, handles connection settings, and provides tools for querying and listing datasets and tables.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-12T01:03:02Z",
      "readme_content": "# BigQuery MCP Server\n\nA Model Context Protocol (MCP) server for accessing Google BigQuery. This server enables Large Language Models (LLMs) to understand BigQuery dataset structures and execute SQL queries.\n\n## Features\n\n### Authentication and Connection Management\n- Supports Application Default Credentials (ADC) or service account key files\n- Configurable project ID and location settings\n- Authentication verification on startup\n\n### Tools\n\n1. **query**\n   - Execute read-only (SELECT) BigQuery SQL queries\n   - Configurable maximum results and bytes billed\n   - Security checks to prevent non-SELECT queries\n\n2. **list_all_datasets**\n   - List all datasets in the project\n   - Returns an array of dataset IDs\n\n3. **list_all_tables_with_dataset**\n   - List all tables in a specific dataset with their schemas\n   - Requires a datasetId parameter\n   - Returns table IDs, schemas, time partitioning information, and descriptions\n\n4. **get_table_information**\n   - Get table schema and sample data (up to 20 rows)\n   - Support for partitioned tables with partition filters\n   - Warnings for queries on partitioned tables without filters\n\n5. **dry_run_query**\n   - Check query validity and estimate cost without execution\n   - Returns processing size and estimated cost\n\n## Security Features\n- Only SELECT queries are allowed (read-only access)\n- Default limit of 500GB for query processing to prevent excessive costs\n- Partition filter recommendations for partitioned tables\n- Secure handling of authentication credentials\n\n## Installation\n\n### Local Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/bigquery-mcp-server.git\ncd bigquery-mcp-server\n\n# Install dependencies\nbun install\n\n# Build the server\nbun run build\n\n# Install command to your own path.\ncp dist/bigquery-mcp-server /path/to/your_place\n```\n\n### Docker Installation\n\nYou can also run the server in a Docker container:\n\n```bash\n# Build the Docker image\ndocker build -t bigquery-mcp-server .\n\n# Run the container\ndocker run -it --rm \\\n  bigquery-mcp-server \\\n  --project-id=your-project-id\n```\n\nOr using Docker Compose:\n\n```bash\n# Edit docker-compose.yml to set your project ID and other options\n# Then run:\ndocker-compose up\n```\n\n## MCP Configuration\n\nTo use this server with an MCP-enabled LLM, add it to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"BigQuery\": {\n      \"command\": \"/path/to/dist/bigquery-mcp-server\",\n      \"args\": [\n        \"--project-id\",\n        \"your-project-id\",\n        \"--location\",\n        \"asia-northeast1\",\n        \"--max-results\",\n        \"1000\",\n        \"--max-bytes-billed\",\n        \"500000000000\"\n      ],\n      \"env\": {\n        \"GOOGLE_APPLICATION_CREDENTIALS\": \"/path/to/service-account-key.json\"\n      }\n    }\n  }\n}\n```\n\nYou can also use Application Default Credentials instead of a service account key file:\n\n```json\n{\n  \"mcpServers\": {\n    \"BigQuery\": {\n      \"command\": \"/path/to/dist/bigquery-mcp-server\",\n      \"args\": [\n        \"--project-id\",\n        \"your-project-id\",\n        \"--location\",\n        \"asia-northeast1\",\n        \"--max-results\",\n        \"1000\",\n        \"--max-bytes-billed\",\n        \"500000000000\"\n      ]\n    }\n  }\n}\n```\n\n### Setting up Application Default Credentials\n\nTo authenticate using Application Default Credentials:\n\n1. Install the Google Cloud SDK if you haven't already:\n   ```bash\n   # For macOS\n   brew install --cask google-cloud-sdk\n   \n   # For other platforms, see: https://cloud.google.com/sdk/docs/install\n   ```\n\n2. Run the authentication command:\n   ```bash\n   gcloud auth application-default login\n   ```\n\n3. Follow the prompts to log in with your Google account that has access to the BigQuery project.\n\n4. The credentials will be saved to your local machine and automatically used by the BigQuery MCP server.\n\n## Testing\n\nYou can use [inspector](https://github.com/modelcontextprotocol/inspector) for testing and debugging.\n\n```\nnpx @modelcontextprotocol/inspector dist/bigquery-mcp-server --project-id={{your_own_project}}\n```\n\n## Usage\n\n### Using the Helper Script\n\nThe included `run-server.sh` script makes it easy to start the server with common configurations:\n\n```bash\n# Make the script executable\nchmod +x run-server.sh\n\n# Run with Application Default Credentials\n./run-server.sh --project-id=your-project-id\n\n# Run with a service account key file\n./run-server.sh \\\n  --project-id=your-project-id \\\n  --location=asia-northeast1 \\\n  --key-file=/path/to/service-account-key.json \\\n  --max-results=1000 \\\n  --max-bytes-billed=500000000000\n```\n\n### Manual Execution\n\nYou can also run the compiled binary directly:\n\n```bash\n# Run with Application Default Credentials\n./dist/bigquery-mcp-server --project-id=your-project-id\n\n# Run with a service account key file\n./dist/bigquery-mcp-server \\\n  --project-id=your-project-id \\\n  --location=asia-northeast1 \\\n  --key-file=/path/to/service-account-key.json \\\n  --max-results=1000 \\\n  --max-bytes-billed=500000000000\n```\n\n### Example Client\n\nAn example Node.js client is included in the `examples` directory:\n\n```bash\n# Make the example executable\nchmod +x examples/sample-query.js\n\n# Edit the example to set your project ID\n# Then run it\ncd examples\n./sample-query.js\n```\n\n### Command Line Options\n\n- `--project-id`: Google Cloud project ID (required)\n- `--location`: BigQuery location (default: asia-northeast1)\n- `--key-file`: Path to service account key file (optional)\n- `--max-results`: Maximum rows to return (default: 1000)\n- `--max-bytes-billed`: Maximum bytes to process (default: 500000000000, 500GB)\n\n## Required Permissions\n\nThe service account or user credentials should have one of the following:\n\n- `roles/bigquery.user` (recommended)\n\nOr both of these:\n- `roles/bigquery.dataViewer` (for reading table data)\n- `roles/bigquery.jobUser` (for executing queries)\n\n## Example Usage\n\n### Query Tool\n\n```json\n{\n  \"query\": \"SELECT * FROM `project.dataset.table` LIMIT 10\",\n  \"maxResults\": 100\n}\n```\n\n### List All Datasets Tool\n\n```json\n// No parameters required\n```\n\n### List All Tables With Dataset Tool\n\n```json\n{\n  \"datasetId\": \"your_dataset\"\n}\n```\n\n### Get Table Information Tool\n\n```json\n{\n  \"datasetId\": \"your_dataset\",\n  \"tableId\": \"your_table\",\n  \"partition\": \"20250101\"\n}\n```\n\n### Dry Run Query Tool\n\n```json\n{\n  \"query\": \"SELECT * FROM `project.dataset.table` WHERE date = '2025-01-01'\"\n}\n```\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Authentication failures\n- Permission issues\n- Invalid queries\n- Missing partition filters\n- Excessive data processing requests\n\n## Code Structure\n\nThe server is organized into the following structure:\n\n```\nsrc/\n├── index.ts              # Entry point\n├── server.ts             # BigQueryMcpServer class\n├── types.ts              # Type definitions\n├── tools/                # Tool implementations\n│   ├── query.ts          # query tool\n│   ├── list-datasets.ts  # list_all_datasets tool\n│   ├── list-tables.ts    # list_all_tables_with_dataset tool\n│   ├── table-info.ts     # get_table_information tool\n│   └── dry-run.ts        # dry_run_query tool\n└── utils/                # Utility functions\n    ├── args-parser.ts    # Command line argument parser\n    └── query-utils.ts    # Query validation and response formatting\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bigquery",
        "databases",
        "database",
        "bigquery datasets",
        "google bigquery",
        "secure database"
      ],
      "category": "databases"
    },
    "thochi--mssql-mcp-server": {
      "owner": "thochi",
      "name": "mssql-mcp-server",
      "url": "https://github.com/thochi/mssql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/thochi.webp",
      "description": "Connects to Microsoft SQL Server databases to execute SQL queries and manage database connections.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-23T02:01:15Z",
      "readme_content": "# MSSQL MCP Server\n\nA Model Context Protocol (MCP) server for connecting to Microsoft SQL Server databases. This server provides tools for executing SQL queries and managing database connections.\n\n## Installation\n\n```bash\nnpm install mssql-mcp-server\n```\n\n## Usage\n\nAdd the server to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"mssql\": {\n      \"command\": \"mssql-mcp-server\",\n      \"env\": {\n        \"MSSQL_CONNECTION_STRING\": \"Server=localhost;Database=master;User Id=sa;Password=yourpassword;\",\n        // Or individual connection parameters:\n        \"MSSQL_HOST\": \"localhost\",\n        \"MSSQL_PORT\": \"1433\",\n        \"MSSQL_DATABASE\": \"master\",\n        \"MSSQL_USER\": \"sa\",\n        \"MSSQL_PASSWORD\": \"yourpassword\",\n        \"MSSQL_ENCRYPT\": \"false\",\n        \"MSSQL_TRUST_SERVER_CERTIFICATE\": \"true\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\n### query\n\nExecute a SQL query on a MSSQL database.\n\n#### Parameters\n\n- `connectionString` (string, optional): Full connection string (alternative to individual parameters)\n- `host` (string, optional): Database server hostname\n- `port` (number, optional): Database server port (default: 1433)\n- `database` (string, optional): Database name (default: master)\n- `username` (string, optional): Database username\n- `password` (string, optional): Database password\n- `query` (string, required): SQL query to execute\n- `encrypt` (boolean, optional): Enable encryption (default: false)\n- `trustServerCertificate` (boolean, optional): Trust server certificate (default: true)\n\nEither `connectionString` OR (`host` + `username` + `password`) must be provided.\n\n#### Example\n\n```typescript\nconst result = await use_mcp_tool({\n  server_name: 'mssql',\n  tool_name: 'query',\n  arguments: {\n    host: 'localhost',\n    username: 'sa',\n    password: 'yourpassword',\n    query: 'SELECT * FROM Users',\n  },\n});\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Run linter\nnpm run lint\n\n# Format code\nnpm run format\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mssql",
        "databases",
        "database",
        "mssql mcp",
        "thochi mssql",
        "databases secure"
      ],
      "category": "databases"
    },
    "timeplus-io--mcp-timeplus": {
      "owner": "timeplus-io",
      "name": "mcp-timeplus",
      "url": "https://github.com/timeplus-io/mcp-timeplus",
      "imageUrl": "/freedevtools/mcp/pfp/timeplus-io.webp",
      "description": "Execute SQL queries and manage databases, including interaction with Kafka topics and Iceberg tables. Offers a user-friendly interface and robust backend capabilities for efficient data workflows.",
      "stars": 10,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-29T07:04:38Z",
      "readme_content": "# Timeplus MCP Server\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-timeplus)](https://pypi.org/project/mcp-timeplus)\n\nAn MCP server for Timeplus.\n\n<a href=\"https://glama.ai/mcp/servers/9aleefsq9s\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/9aleefsq9s/badge\" alt=\"mcp-timeplus MCP server\" /></a>\n\n## Features\n\n### Prompts\n\n* `generate_sql` to give LLM more knowledge about how to query Timeplus via SQL\n\n### Tools\n\n* `run_sql`\n  - Execute SQL queries on your Timeplus cluster.\n  - Input: `sql` (string): The SQL query to execute.\n  - By default, all Timeplus queries are run with `readonly = 1` to ensure they are safe. If you want to run DDL or DML queries, you can set the environment variable `TIMEPLUS_READ_ONLY` to `false`.\n\n* `list_databases`\n  - List all databases on your Timeplus cluster.\n\n* `list_tables`\n  - List all tables in a database.\n  - Input: `database` (string): The name of the database.\n\n* `list_kafka_topics`\n  - List all topics in a Kafka cluster\n\n* `explore_kafka_topic`\n  - Show some messages in the Kafka topic\n  - Input: `topic` (string): The name of the topic. `message_count` (int): The number of messages to show, default to 1.\n\n* `create_kafka_stream`\n  - Setup a streaming ETL in Timeplus to save the Kafka messages locally\n  - Input: `topic` (string): The name of the topic.\n\n* `connect_to_apache_iceberg`\n  - Connect to a database based on Apache Iceberg. Currently this is only available via Timeplus Enterprise and it's planned to make it available for Timeplus Proton soon.\n  - Input: `iceberg_db` (string): The name of the Iceberg database. `aws_account_id` (int): The AWS account ID (12 digits). `s3_bucket` (string): The S3 bucket name. `aws_region` (string): The AWS region, default to \"us-west-2\". `is_s3_table_bucket` (bool): Whether the S3 bucket is a S3 table bucket, default to False.\n\n## Configuration\n\nFirst, ensure you have the `uv` executable installed. If not, you can install it by following the instructions [here](https://docs.astral.sh/uv/).\n\n1. Open the Claude Desktop configuration file located at:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-timeplus\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-timeplus\"],\n      \"env\": {\n        \"TIMEPLUS_HOST\": \"<timeplus-host>\",\n        \"TIMEPLUS_PORT\": \"<timeplus-port>\",\n        \"TIMEPLUS_USER\": \"<timeplus-user>\",\n        \"TIMEPLUS_PASSWORD\": \"<timeplus-password>\",\n        \"TIMEPLUS_SECURE\": \"false\",\n        \"TIMEPLUS_VERIFY\": \"true\",\n        \"TIMEPLUS_CONNECT_TIMEOUT\": \"30\",\n        \"TIMEPLUS_SEND_RECEIVE_TIMEOUT\": \"30\",\n        \"TIMEPLUS_READ_ONLY\": \"false\",\n        \"TIMEPLUS_KAFKA_CONFIG\": \"{\\\"bootstrap.servers\\\":\\\"a.aivencloud.com:28864\\\", \\\"sasl.mechanism\\\":\\\"SCRAM-SHA-256\\\",\\\"sasl.username\\\":\\\"avnadmin\\\", \\\"sasl.password\\\":\\\"thePassword\\\",\\\"security.protocol\\\":\\\"SASL_SSL\\\",\\\"enable.ssl.certificate.verification\\\":\\\"false\\\"}\"\n      }\n    }\n  }\n}\n```\n\nUpdate the environment variables to point to your own Timeplus service.\n\n3. Restart Claude Desktop to apply the changes.\n\nYou can also try this MCP server with other MCP clients, such as [5ire](https://github.com/nanbingxyz/5ire).\n\n## Development\n\n1. In `test-services` directory run `docker compose up -d` to start a Timeplus Proton server. You can also download it via `curl https://install.timeplus.com/oss | sh`, then start with `./proton server`.\n\n2. Add the following variables to a `.env` file in the root of the repository.\n\n```\nTIMEPLUS_HOST=localhost\nTIMEPLUS_PORT=8123\nTIMEPLUS_USER=default\nTIMEPLUS_PASSWORD=\nTIMEPLUS_SECURE=false\nTIMEPLUS_VERIFY=true\nTIMEPLUS_CONNECT_TIMEOUT=30\nTIMEPLUS_SEND_RECEIVE_TIMEOUT=30\nTIMEPLUS_READ_ONLY=false\nTIMEPLUS_KAFKA_CONFIG={\"bootstrap.servers\":\"a.aivencloud.com:28864\", \"sasl.mechanism\":\"SCRAM-SHA-256\",\"sasl.username\":\"avnadmin\", \"sasl.password\":\"thePassword\",\"security.protocol\":\"SASL_SSL\",\"enable.ssl.certificate.verification\":\"false\"}\n```\n\n3. Run `uv sync` to install the dependencies. Then do `source .venv/bin/activate`.\n\n4. For easy testing, you can run `mcp dev mcp_timeplus/mcp_server.py` to start the MCP server. Click the \"Connect\" button to connect the UI with the MCP server, then switch to the \"Tools\" tab to run the available tools.\n\n5. To build the Docker image, run `docker build -t mcp_timeplus .`.\n\n### Environment Variables\n\nThe following environment variables are used to configure the Timeplus connection:\n\n#### Required Variables\n* `TIMEPLUS_HOST`: The hostname of your Timeplus server\n* `TIMEPLUS_USER`: The username for authentication\n* `TIMEPLUS_PASSWORD`: The password for authentication\n\n#### Optional Variables\n* `TIMEPLUS_PORT`: The port number of your Timeplus server\n  - Default: `8443` if HTTPS is enabled, `8123` if disabled\n  - Usually doesn't need to be set unless using a non-standard port\n* `TIMEPLUS_SECURE`: Enable/disable HTTPS connection\n  - Default: `\"false\"`\n  - Set to `\"true\"` for secure connections\n* `TIMEPLUS_VERIFY`: Enable/disable SSL certificate verification\n  - Default: `\"true\"`\n  - Set to `\"false\"` to disable certificate verification (not recommended for production)\n* `TIMEPLUS_CONNECT_TIMEOUT`: Connection timeout in seconds\n  - Default: `\"30\"`\n  - Increase this value if you experience connection timeouts\n* `TIMEPLUS_SEND_RECEIVE_TIMEOUT`: Send/receive timeout in seconds\n  - Default: `\"300\"`\n  - Increase this value for long-running queries\n* `TIMEPLUS_DATABASE`: Default database to use\n  - Default: None (uses server default)\n  - Set this to automatically connect to a specific database\n* `TIMEPLUS_READ_ONLY`: Enable/disable read-only mode\n  - Default: `\"true\"`\n  - Set to `\"false\"` to enable DDL/DML\n* `TIMEPLUS_KAFKA_CONFIG`: A JSON string for the Kafka configuration. Please refer to [librdkafka configuration](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) or take the above example as a reference.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "timeplus",
        "database",
        "access timeplus",
        "timeplus io",
        "databases secure"
      ],
      "category": "databases"
    },
    "tjwells47--supabase-mcp-server": {
      "owner": "tjwells47",
      "name": "supabase-mcp-server",
      "url": "https://github.com/tjwells47/supabase-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tjwells47.webp",
      "description": "Control and manage Supabase databases using natural language commands. Safely execute SQL queries, manage schema changes, and interact with the Supabase Management API with built-in safety features.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-07T20:19:59Z",
      "readme_content": "# Query | MCP server for Supabase\n<p align=\"center\">\n  <a href=\"https://thequery.dev\"></a>\n</p>\n\n<p align=\"center\">\n  <strong>Query MCP is an open-source MCP server that lets your IDE safely run SQL, manage schema changes, call the Supabase Management API, and use Auth Admin SDK — all with built-in safety controls.</strong>\n</p>\n\n<p align=\"center\">\n  ⚡ Free & open-source forever.  \n  💎 Premium features coming soon.\n  🧪 Early Access is live at <a href=\"https://thequery.dev\">thequery.dev</a>.\n  📢 Share your feedback on GitHub issues or at feedback@thequery.dev.\n</p>\n<p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/supabase-mcp-server/\"><img src=\"https://img.shields.io/pypi/v/supabase-mcp-server.svg\" alt=\"PyPI version\" /></a>\n  <a href=\"https://github.com/alexander-zuev/supabase-mcp-server/actions\"><img src=\"https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg\" alt=\"CI Status\" /></a>\n  <a href=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server\"><img src=\"https://codecov.io/gh/alexander-zuev/supabase-mcp-server/branch/main/graph/badge.svg\" alt=\"Code Coverage\" /></a>\n  <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.12%2B-blue.svg\" alt=\"Python 3.12+\" /></a>\n  <a href=\"https://github.com/astral-sh/uv\"><img src=\"https://img.shields.io/badge/uv-package%20manager-blueviolet\" alt=\"uv package manager\" /></a>\n  <a href=\"https://pepy.tech/project/supabase-mcp-server\"><img src=\"https://static.pepy.tech/badge/supabase-mcp-server\" alt=\"PyPI Downloads\" /></a>\n  <a href=\"https://smithery.ai/server/@alexander-zuev/supabase-mcp-server\"><img src=\"https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server\" alt=\"Smithery.ai Downloads\" /></a>\n  <a href=\"https://modelcontextprotocol.io/introduction\"><img src=\"https://img.shields.io/badge/MCP-Server-orange\" alt=\"MCP Server\" /></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n</p>\n\n\n## Table of contents\n<p align=\"center\">\n  <a href=\"#getting-started\">Getting started</a> •\n  <a href=\"#feature-overview\">Feature overview</a> •\n  <a href=\"#troubleshooting\">Troubleshooting</a> •\n  <a href=\"#changelog\">Changelog</a>\n</p>\n\n## ✨ Key features\n- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol\n- 🔐 Control read-only and read-write modes of SQL query execution\n- 🔍 Runtime SQL query validation with risk level assessment\n- 🛡️ Three-tier safety system for SQL operations: safe, write, and destructive\n- 🔄 Robust transaction handling for both direct and pooled database connections\n- 📝 Automatic versioning of database schema changes\n- 💻 Manage your Supabase projects with Supabase Management API\n- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK\n- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively\n- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)\n\n\n## Getting Started\n\n### Prerequisites\nInstalling the server requires the following on your system:\n- Python 3.12+\n\nIf you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).\n\n### PostgreSQL Installation\nPostgreSQL installation is no longer required for the MCP server itself, as it now uses asyncpg which doesn't depend on PostgreSQL development libraries.\n\nHowever, you'll still need PostgreSQL if you're running a local Supabase instance:\n\n**MacOS**\n```bash\nbrew install postgresql@16\n```\n\n**Windows**\n  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/\n  - Ensure \"PostgreSQL Server\" and \"Command Line Tools\" are selected during installation\n\n### Step 1. Installation\n\nSince v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:\n\n```bash\n# if pipx is installed (recommended)\npipx install supabase-mcp-server\n\n# if uv is installed\nuv pip install supabase-mcp-server\n```\n\n`pipx` is recommended because it creates isolated environments for each package.\n\nYou can also install the server manually by cloning the repository and running `pipx install -e .` from the root directory.\n\n#### Installing from source\nIf you would like to install from source, for example for local development:\n```bash\nuv venv\n# On Mac\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n# Install package in editable mode\nuv pip install -e .\n```\n\n#### Installing via Smithery.ai\n\nYou can find the full instructions on how to use Smithery.ai to connect to this MCP server [here](https://smithery.ai/server/@alexander-zuev/supabase-mcp-server).\n\n\n### Step 2. Configuration\n\nThe Supabase MCP server requires configuration to connect to your Supabase database, access the Management API, and use the Auth Admin SDK. This section explains all available configuration options and how to set them up.\n\n> 🔑 **Important**: Since v0.4 MCP server requires an API key which you can get for free at [thequery.dev](https://thequery.dev) to use this MCP server.\n\n#### Environment Variables\n\nThe server uses the following environment variables:\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `SUPABASE_PROJECT_REF` | Yes | `127.0.0.1:54322` | Your Supabase project reference ID (or local host:port) |\n| `SUPABASE_DB_PASSWORD` | Yes | `postgres` | Your database password |\n| `SUPABASE_REGION` | Yes* | `us-east-1` | AWS region where your Supabase project is hosted |\n| `SUPABASE_ACCESS_TOKEN` | No | None | Personal access token for Supabase Management API |\n| `SUPABASE_SERVICE_ROLE_KEY` | No | None | Service role key for Auth Admin SDK |\n| `QUERY_API_KEY` | Yes | None | API key from thequery.dev (required for all operations) |\n\n> **Note**: The default values are configured for local Supabase development. For remote Supabase projects, you must provide your own values for `SUPABASE_PROJECT_REF` and `SUPABASE_DB_PASSWORD`.\n\n> 🚨 **CRITICAL CONFIGURATION NOTE**: For remote Supabase projects, you MUST specify the correct region where your project is hosted using `SUPABASE_REGION`. If you encounter a \"Tenant or user not found\" error, this is almost certainly because your region setting doesn't match your project's actual region. You can find your project's region in the Supabase dashboard under Project Settings.\n\n#### Connection Types\n\n##### Database Connection\n- The server connects to your Supabase PostgreSQL database using the transaction pooler endpoint\n- Local development uses a direct connection to `127.0.0.1:54322`\n- Remote projects use the format: `postgresql://postgres.[project_ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres`\n\n> ⚠️ **Important**: Session pooling connections are not supported. The server exclusively uses transaction pooling for better compatibility with the MCP server architecture.\n\n##### Management API Connection\n- Requires `SUPABASE_ACCESS_TOKEN` to be set\n- Connects to the Supabase Management API at `https://api.supabase.com`\n- Only works with remote Supabase projects (not local development)\n\n##### Auth Admin SDK Connection\n- Requires `SUPABASE_SERVICE_ROLE_KEY` to be set\n- For local development, connects to `http://127.0.0.1:54321`\n- For remote projects, connects to `https://[project_ref].supabase.co`\n\n#### Configuration Methods\n\nThe server looks for configuration in this order (highest to lowest priority):\n\n1. **Environment Variables**: Values set directly in your environment\n2. **Local `.env` File**: A `.env` file in your current working directory (only works when running from source)\n3. **Global Config File**:\n   - Windows: `%APPDATA%\\supabase-mcp\\.env`\n   - macOS/Linux: `~/.config/supabase-mcp/.env`\n4. **Default Settings**: Local development defaults (if no other config is found)\n\n> ⚠️ **Important**: When using the package installed via pipx or uv, local `.env` files in your project directory are **not** detected. You must use either environment variables or the global config file.\n\n#### Setting Up Configuration\n\n##### Option 1: Client-Specific Configuration (Recommended)\n\nSet environment variables directly in your MCP client configuration (see client-specific setup instructions in Step 3). Most MCP clients support this approach, which keeps your configuration with your client settings.\n\n##### Option 2: Global Configuration\n\nCreate a global `.env` configuration file that will be used for all MCP server instances:\n\n```bash\n# Create config directory\n# On macOS/Linux\nmkdir -p ~/.config/supabase-mcp\n# On Windows (PowerShell)\nmkdir -Force \"$env:APPDATA\\supabase-mcp\"\n\n# Create and edit .env file\n# On macOS/Linux\nnano ~/.config/supabase-mcp/.env\n# On Windows (PowerShell)\nnotepad \"$env:APPDATA\\supabase-mcp\\.env\"\n```\n\nAdd your configuration values to the file:\n\n```\nQUERY_API_KEY=your-api-key\nSUPABASE_PROJECT_REF=your-project-ref\nSUPABASE_DB_PASSWORD=your-db-password\nSUPABASE_REGION=us-east-1\nSUPABASE_ACCESS_TOKEN=your-access-token\nSUPABASE_SERVICE_ROLE_KEY=your-service-role-key\n```\n\n##### Option 3: Project-Specific Configuration (Source Installation Only)\n\nIf you're running the server from source (not via package), you can create a `.env` file in your project directory with the same format as above.\n\n#### Finding Your Supabase Project Information\n\n- **Project Reference**: Found in your Supabase project URL: `https://supabase.com/dashboard/project/<project-ref>`\n- **Database Password**: Set during project creation or found in Project Settings → Database\n- **Access Token**: Generate at https://supabase.com/dashboard/account/tokens\n- **Service Role Key**: Found in Project Settings → API → Project API keys\n\n#### Supported Regions\n\nThe server supports all Supabase regions:\n\n- `us-west-1` - West US (North California)\n- `us-east-1` - East US (North Virginia) - default\n- `us-east-2` - East US (Ohio)\n- `ca-central-1` - Canada (Central)\n- `eu-west-1` - West EU (Ireland)\n- `eu-west-2` - West Europe (London)\n- `eu-west-3` - West EU (Paris)\n- `eu-central-1` - Central EU (Frankfurt)\n- `eu-central-2` - Central Europe (Zurich)\n- `eu-north-1` - North EU (Stockholm)\n- `ap-south-1` - South Asia (Mumbai)\n- `ap-southeast-1` - Southeast Asia (Singapore)\n- `ap-northeast-1` - Northeast Asia (Tokyo)\n- `ap-northeast-2` - Northeast Asia (Seoul)\n- `ap-southeast-2` - Oceania (Sydney)\n- `sa-east-1` - South America (São Paulo)\n\n#### Limitations\n\n- **No Self-Hosted Support**: The server only supports official Supabase.com hosted projects and local development\n- **No Connection String Support**: Custom connection strings are not supported\n- **No Session Pooling**: Only transaction pooling is supported for database connections\n- **API and SDK Features**: Management API and Auth Admin SDK features only work with remote Supabase projects, not local development\n\n### Step 3. Usage\n\nIn general, any MCP client that supports `stdio` protocol should work with this MCP server. This server was explicitly tested to work with:\n- Cursor\n- Windsurf\n- Cline\n- Claude Desktop\n\nAdditionally, you can also use smithery.ai to install this server a number of clients, including the ones above.\n\nFollow the guides below to install this MCP server in your client.\n\n#### Cursor\nGo to Settings -> Features -> MCP Servers and add a new server with this configuration:\n```bash\n# can be set to any name\nname: supabase\ntype: command\n# if you installed with pipx\ncommand: supabase-mcp-server\n# if you installed with uv\ncommand: uv run supabase-mcp-server\n# if the above doesn't work, use the full path (recommended)\ncommand: /full/path/to/supabase-mcp-server  # Find with 'which supabase-mcp-server' (macOS/Linux) or 'where supabase-mcp-server' (Windows)\n```\n\nIf configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.\n![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)\n\n#### Windsurf\nGo to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:\n```json\n{\n    \"mcpServers\": {\n      \"supabase\": {\n        \"command\": \"/Users/username/.local/bin/supabase-mcp-server\",  // update path\n        \"env\": {\n          \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n          \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n          \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n          \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n          \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n          \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n        }\n      }\n    }\n}\n```\nIf configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)\n\n#### Claude Desktop\nClaude Desktop also supports MCP servers through a JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Claude Desktop:\n   - Open Claude Desktop\n   - Go to Settings → Developer -> Edit Config MCP Servers\n   - Add a new configuration with the following JSON:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\n> ⚠️ **Important**: Unlike Windsurf and Cursor, Claude Desktop requires the **full absolute path** to the executable. Using just the command name (`supabase-mcp-server`) will result in a \"spawn ENOENT\" error.\n\nIf configuration is correct, you should see the Supabase MCP server listed as available in Claude Desktop.\n\n![How successful Windsurf config looks like](https://github.com/user-attachments/assets/500bcd40-6245-40a7-b23b-189827ed2923)\n\n#### Cline\nCline also supports MCP servers through a similar JSON configuration. Follow these steps to set up the Supabase MCP server:\n\n1. **Find the full path to the executable** (this step is critical):\n   ```bash\n   # On macOS/Linux\n   which supabase-mcp-server\n\n   # On Windows\n   where supabase-mcp-server\n   ```\n   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).\n\n2. **Configure the MCP server** in Cline:\n   - Open Cline in VS Code\n   - Click on the \"MCP Servers\" tab in the Cline sidebar\n   - Click \"Configure MCP Servers\"\n   - This will open the `cline_mcp_settings.json` file\n   - Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"supabase\": {\n         \"command\": \"/full/path/to/supabase-mcp-server\",  // Replace with the actual path from step 1\n         \"env\": {\n           \"QUERY_API_KEY\": \"your-api-key\",  // Required - get your API key at thequery.dev\n           \"SUPABASE_PROJECT_REF\": \"your-project-ref\",\n           \"SUPABASE_DB_PASSWORD\": \"your-db-password\",\n           \"SUPABASE_REGION\": \"us-east-1\",  // optional, defaults to us-east-1\n           \"SUPABASE_ACCESS_TOKEN\": \"your-access-token\",  // optional, for management API\n           \"SUPABASE_SERVICE_ROLE_KEY\": \"your-service-role-key\"  // optional, for Auth Admin SDK\n         }\n       }\n     }\n   }\n   ```\n\nIf configuration is correct, you should see a green indicator next to the Supabase MCP server in the Cline MCP Servers list, and a message confirming \"supabase MCP server connected\" at the bottom of the panel.\n\n![How successful configuration in Cline looks like](https://github.com/user-attachments/assets/6c4446ad-7a58-44c6-bf12-6c82222bbe59)\n\n### Troubleshooting\n\nHere are some tips & tricks that might help you:\n- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.\n- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.\n- **\"No tools found\" error** - If you see \"Client closed - no tools available\" in Cursor despite the package being installed:\n  - Find the full path to the executable by running `which supabase-mcp-server` (macOS/Linux) or `where supabase-mcp-server` (Windows)\n  - Use the full path in your MCP server configuration instead of just `supabase-mcp-server`\n  - For example: `/Users/username/.local/bin/supabase-mcp-server` or `C:\\Users\\username\\.local\\bin\\supabase-mcp-server.exe`\n- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\\supabase-mcp\\.env` on Windows).\n- **Accessing logs** - The MCP server writes detailed logs to a file:\n  - Log file location:\n    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`\n    - Windows: `%USERPROFILE%\\.local\\share\\supabase-mcp\\mcp_server.log`\n  - Logs include connection status, configuration details, and operation results\n  - View logs using any text editor or terminal commands:\n    ```bash\n    # On macOS/Linux\n    cat ~/.local/share/supabase-mcp/mcp_server.log\n\n    # On Windows (PowerShell)\n    Get-Content \"$env:USERPROFILE\\.local\\share\\supabase-mcp\\mcp_server.log\"\n    ```\n\nIf you are stuck or any of the instructions above are incorrect, please raise an issue.\n\n### MCP Inspector\nA super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.\n> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the coming release.\n\n## Feature Overview\n\n### Database query tools\n\nSince v0.3+ server provides comprehensive database management capabilities with built-in safety controls:\n\n- **SQL Query Execution**: Execute PostgreSQL queries with risk assessment\n  - **Three-tier safety system**:\n    - `safe`: Read-only operations (SELECT) - always allowed\n    - `write`: Data modifications (INSERT, UPDATE, DELETE) - require unsafe mode\n    - `destructive`: Schema changes (DROP, CREATE) - require unsafe mode + confirmation\n\n- **SQL Parsing and Validation**:\n  - Uses PostgreSQL's parser (pglast) for accurate analysis and provides clear feedback on safety requirements\n\n- **Automatic Migration Versioning**:\n  - Database-altering operations operations are automatically versioned\n  - Generates descriptive names based on operation type and target\n\n\n- **Safety Controls**:\n  - Default SAFE mode allows only read-only operations\n  - All statements run in transaction mode via `asyncpg`\n  - 2-step confirmation for high-risk operations\n\n- **Available Tools**:\n  - `get_schemas`: Lists schemas with sizes and table counts\n  - `get_tables`: Lists tables, foreign tables, and views with metadata\n  - `get_table_schema`: Gets detailed table structure (columns, keys, relationships)\n  - `execute_postgresql`: Executes SQL statements against your database\n  - `confirm_destructive_operation`: Executes high-risk operations after confirmation\n  - `retrieve_migrations`: Gets migrations with filtering and pagination options\n  - `live_dangerously`: Toggles between safe and unsafe modes\n\n### Management API tools\n\nSince v0.3.0 server provides secure access to the Supabase Management API with built-in safety controls:\n\n- **Available Tools**:\n  - `send_management_api_request`: Sends arbitrary requests to Supabase Management API with auto-injection of project ref\n  - `get_management_api_spec`: Gets the enriched API specification with safety information\n    - Supports multiple query modes: by domain, by specific path/method, or all paths\n    - Includes risk assessment information for each endpoint\n    - Provides detailed parameter requirements and response formats\n    - Helps LLMs understand the full capabilities of the Supabase Management API\n  - `get_management_api_safety_rules`: Gets all safety rules with human-readable explanations\n  - `live_dangerously`: Toggles between safe and unsafe operation modes\n\n- **Safety Controls**:\n  - Uses the same safety manager as database operations for consistent risk management\n  - Operations categorized by risk level:\n    - `safe`: Read-only operations (GET) - always allowed\n    - `unsafe`: State-changing operations (POST, PUT, PATCH, DELETE) - require unsafe mode\n    - `blocked`: Destructive operations (delete project, etc.) - never allowed\n  - Default safe mode prevents accidental state changes\n  - Path-based pattern matching for precise safety rules\n\n**Note**: Management API tools only work with remote Supabase instances and are not compatible with local Supabase development setups.\n\n### Auth Admin tools\n\nI was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.\n\nSince v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:\n  - Includes the following tools:\n    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods\n    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling\n  - Supported methods:\n    - `get_user_by_id`: Retrieve a user by their ID\n    - `list_users`: List all users with pagination\n    - `create_user`: Create a new user\n    - `delete_user`: Delete a user by their ID\n    - `invite_user_by_email`: Send an invite link to a user's email\n    - `generate_link`: Generate an email link for various authentication purposes\n    - `update_user_by_id`: Update user attributes by ID\n    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)\n\n#### Why use Auth Admin SDK instead of raw SQL queries?\n\nThe Auth Admin SDK provides several key advantages over direct SQL manipulation:\n- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)\n- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas\n- **Simplicity**: Offers clear methods with proper validation and error handling\n\n  - Response format:\n    - All methods return structured Python objects instead of raw dictionaries\n    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user[\"id\"]`)\n  - Edge cases and limitations:\n    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors\n    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project\n    - Link types: When generating links, different link types have different requirements:\n      - `signup` links don't require the user to exist\n      - `magiclink` and `recovery` links require the user to already exist in the system\n    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface\n    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK\n\n### Logs & Analytics\n\nThe server provides access to Supabase logs and analytics data, making it easier to monitor and troubleshoot your applications:\n\n- **Available Tool**: `retrieve_logs` - Access logs from any Supabase service\n\n- **Log Collections**:\n  - `postgres`: Database server logs\n  - `api_gateway`: API gateway requests\n  - `auth`: Authentication events\n  - `postgrest`: RESTful API service logs\n  - `pooler`: Connection pooling logs\n  - `storage`: Object storage operations\n  - `realtime`: WebSocket subscription logs\n  - `edge_functions`: Serverless function executions\n  - `cron`: Scheduled job logs\n  - `pgbouncer`: Connection pooler logs\n\n- **Features**: Filter by time, search text, apply field filters, or use custom SQL queries\n\nSimplifies debugging across your Supabase stack without switching between interfaces or writing complex queries.\n\n### Automatic Versioning of Database Changes\n\n\"With great power comes great responsibility.\" While `execute_postgresql` tool coupled with aptly named `live_dangerously` tool provide a powerful and simple way to manage your Supabase database, it also means that dropping a table or modifying one is one chat message away. In order to reduce the risk of irreversible changes, since v0.3.8 the server supports:\n- automatic creation of migration scripts for all write & destructive sql operations executed on the database\n- improved safety mode of query execution, in which all queries are categorized in:\n  - `safe` type: always allowed. Includes all read-only ops.\n  - `write`type: requires `write` mode to be enabled by the user.\n  - `destructive` type: requires `write` mode to be enabled by the user AND a 2-step confirmation of query execution for clients that do not execute tools automatically.\n\n### Universal Safety Mode\nSince v0.3.8 Safety Mode has been standardized across all services (database, API, SDK) using a universal safety manager. This provides consistent risk management and a unified interface for controlling safety settings across the entire MCP server.\n\nAll operations (SQL queries, API requests, SDK methods) are categorized into risk levels:\n- `Low` risk: Read-only operations that don't modify data or structure (SELECT queries, GET API requests)\n- `Medium` risk: Write operations that modify data but not structure (INSERT/UPDATE/DELETE, most POST/PUT API requests)\n- `High` risk: Destructive operations that modify database structure or could cause data loss (DROP/TRUNCATE, DELETE API endpoints)\n- `Extreme` risk: Operations with severe consequences that are blocked entirely (deleting projects)\n\nSafety controls are applied based on risk level:\n- Low risk operations are always allowed\n- Medium risk operations require unsafe mode to be enabled\n- High risk operations require unsafe mode AND explicit confirmation\n- Extreme risk operations are never allowed\n\n#### How confirmation flow works\n\nAny high-risk operations (be it a postgresql or api request) will be blocked even in `unsafe` mode.\n![Every high-risk operation is blocked](https://github.com/user-attachments/assets/c0df79c2-a879-4b1f-a39d-250f9965c36a)\nYou will have to confirm and approve every high-risk operation explicitly in order for it to be executed.\n![Explicit approval is always required](https://github.com/user-attachments/assets/5cd7a308-ec2a-414e-abe2-ff2f3836dd8b)\n\n\n## Changelog\n\n- 📦 Simplified installation via package manager - ✅ (v0.2.0)\n- 🌎 Support for different Supabase regions - ✅ (v0.2.2)\n- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)\n- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)\n- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)\n- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)\n- 🔍 Stronger SQL query validation ✅ (v0.3.8)\n- 📝 Automatic versioning of database changes ✅ (v0.3.8)\n- 📖 Radically improved knowledge and tools of api spec ✅ (v0.3.8)\n- ✍️ Improved consistency of migration-related tools for a more organized database vcs ✅ (v0.3.10)\n- 🥳 Query MCP is released (v0.4.0)\n\n\nFor a more detailed roadmap, please see this [discussion](https://github.com/alexander-zuev/supabase-mcp-server/discussions/46) on GitHub.\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)\n\n---\n\nEnjoy! ☺️",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "databases",
        "database",
        "supabase databases",
        "supabase management",
        "manage supabase"
      ],
      "category": "databases"
    },
    "tuannvm--mcp-trino": {
      "owner": "tuannvm",
      "name": "mcp-trino",
      "url": "https://github.com/tuannvm/mcp-trino",
      "imageUrl": "/freedevtools/mcp/pfp/tuannvm.webp",
      "description": "Enables interaction with Trino's distributed SQL query engine for executing complex SQL queries and discovering data catalogs. Facilitates data analytics capabilities through a high-performance MCP server built in Go.",
      "stars": 73,
      "forks": 24,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-10-02T03:50:12Z",
      "readme_content": "# Trino MCP Server in Go\n\nA high-performance Model Context Protocol (MCP) server for Trino implemented in Go. This project enables AI assistants to seamlessly interact with Trino's distributed SQL query engine through standardized MCP tools.\n\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/tuannvm/mcp-trino/build.yml?branch=main&label=CI%2FCD&logo=github)](https://github.com/tuannvm/mcp-trino/actions/workflows/build.yml)\n[![Go Version](https://img.shields.io/github/go-mod/go-version/tuannvm/mcp-trino?logo=go)](https://github.com/tuannvm/mcp-trino/blob/main/go.mod)\n[![Trivy Scan](https://img.shields.io/github/actions/workflow/status/tuannvm/mcp-trino/build.yml?branch=main&label=Trivy%20Security%20Scan&logo=aquasec)](https://github.com/tuannvm/mcp-trino/actions/workflows/build.yml)\n[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)\n[![Go Report Card](https://goreportcard.com/badge/github.com/tuannvm/mcp-trino)](https://goreportcard.com/report/github.com/tuannvm/mcp-trino)\n[![Go Reference](https://pkg.go.dev/badge/github.com/tuannvm/mcp-trino.svg)](https://pkg.go.dev/github.com/tuannvm/mcp-trino)\n[![Docker Image](https://img.shields.io/github/v/release/tuannvm/mcp-trino?sort=semver&label=GHCR&logo=docker)](https://github.com/tuannvm/mcp-trino/pkgs/container/mcp-trino)\n[![GitHub Release](https://img.shields.io/github/v/release/tuannvm/mcp-trino?sort=semver)](https://github.com/tuannvm/mcp-trino/releases/latest)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/tuannvm/mcp-trino)](https://archestra.ai/mcp-catalog/tuannvm__mcp-trino)\n\n## Overview\n\nThis project implements a Model Context Protocol (MCP) server for Trino in Go. It enables AI assistants to access Trino's distributed SQL query engine through standardized MCP tools.\n\nTrino (formerly PrestoSQL) is a powerful distributed SQL query engine designed for fast analytics on large datasets.\n\n## Architecture\n\n```mermaid\ngraph TB\n    subgraph \"AI Clients\"\n        CC[Claude Code]\n        CD[Claude Desktop]\n        CR[Cursor]\n        WS[Windsurf]\n        CW[ChatWise]\n    end\n    \n    subgraph \"Authentication (Optional)\"\n        OP[OAuth Provider<br/>Okta/Google/Azure AD]\n        JWT[JWT Tokens]\n    end\n    \n    subgraph \"MCP Server (mcp-trino)\"\n        HTTP[HTTP Transport<br/>/mcp endpoint]\n        STDIO[STDIO Transport]\n        AUTH[OAuth Middleware]\n        TOOLS[MCP Tools<br/>• execute_query<br/>• list_catalogs<br/>• list_schemas<br/>• list_tables<br/>• get_table_schema<br/>• explain_query]\n    end\n    \n    subgraph \"Data Layer\"\n        TRINO[Trino Cluster<br/>Distributed SQL Engine]\n        CATALOGS[Data Sources<br/>• PostgreSQL<br/>• MySQL<br/>• S3/Hive<br/>• BigQuery<br/>• MongoDB]\n    end\n    \n    %% Connections\n    CC -.->|OAuth Flow| OP\n    OP -.->|JWT Token| JWT\n    \n    CC -->|HTTP + JWT| HTTP\n    CD -->|STDIO| STDIO\n    CR -->|HTTP + JWT| HTTP\n    WS -->|STDIO| STDIO\n    CW -->|HTTP + JWT| HTTP\n    \n    HTTP --> AUTH\n    AUTH -->|Validated| TOOLS\n    STDIO --> TOOLS\n    \n    TOOLS -->|SQL Queries| TRINO\n    TRINO --> CATALOGS\n    \n    %% Styling\n    classDef client fill:#e1f5fe\n    classDef auth fill:#f3e5f5\n    classDef server fill:#e8f5e8\n    classDef data fill:#fff3e0\n    \n    class CC,CD,CR,WS,CW client\n    class OP,JWT auth\n    class HTTP,STDIO,AUTH,TOOLS server\n    class TRINO,CATALOGS data\n```\n\n**Key Components:**\n\n- **AI Clients**: Various MCP-compatible applications\n- **Authentication**: Optional OAuth 2.0 with OIDC providers\n- **MCP Server**: Go-based server with dual transport support\n- **Data Layer**: Trino cluster connecting to multiple data sources\n\n## Features\n\n- ✅ MCP server implementation in Go\n- ✅ Trino SQL query execution through MCP tools\n- ✅ Catalog, schema, and table discovery\n- ✅ Docker container support\n- ✅ Supports both STDIO and HTTP transports\n- ✅ OAuth 2.0 authentication with OIDC provider support (Okta, Google, Azure AD)\n  - **Native mode**: Direct OAuth with zero server-side secrets\n  - **Proxy mode**: Centralized OAuth with fixed/allowlist redirect URIs\n  - HMAC-SHA256 state signing for multi-pod deployments\n  - PKCE support for enhanced security\n  - Defense-in-depth security model with four independent validation layers\n- ✅ StreamableHTTP support with JWT authentication (upgraded from SSE)\n- ✅ Backward compatibility with SSE endpoints\n- ✅ Compatible with Cursor, Claude Desktop, Windsurf, ChatWise, and any MCP-compatible clients.\n\n## Installation & Quick Start\n\n**Install:**\n\n```bash\n# Homebrew\nbrew install tuannvm/mcp/mcp-trino\n\n# Or one-liner (macOS/Linux)\ncurl -fsSL https://raw.githubusercontent.com/tuannvm/mcp-trino/main/install.sh | bash\n```\n\n**Run (Local Development):**\n\n```bash\nexport TRINO_HOST=localhost TRINO_USER=trino\nmcp-trino\n```\n\nFor production deployment with OAuth, see [Deployment Guide](docs/deployment.md) and [OAuth Architecture](docs/oauth.md).\n\n## Usage\n\n**Supported Clients:** Claude Desktop, Claude Code, Cursor, Windsurf, ChatWise\n\n**Available Tools:** `execute_query`, `list_catalogs`, `list_schemas`, `list_tables`, `get_table_schema`, `explain_query`\n\nFor client integration and tool documentation, see [Integration Guide](docs/integrations.md) and [Tools Reference](docs/tools.md).\n\n## Configuration\n\n**Key Variables:** `TRINO_HOST`, `TRINO_USER`, `TRINO_SCHEME`, `MCP_TRANSPORT`, `OAUTH_PROVIDER`\n\n**OAuth Configuration:**\n\n```bash\n# Native mode (most secure - zero server-side secrets)\nexport OAUTH_ENABLED=true OAUTH_MODE=native OAUTH_PROVIDER=okta\nexport OIDC_ISSUER=https://company.okta.com OIDC_AUDIENCE=https://mcp-server.com\n\n# Proxy mode (centralized credential management)\nexport OAUTH_MODE=proxy OIDC_CLIENT_ID=app-id OIDC_CLIENT_SECRET=secret\nexport OAUTH_REDIRECT_URI=https://mcp-server.com/oauth/callback  # Fixed mode (localhost-only)\nexport OAUTH_REDIRECT_URI=https://app1.com/cb,https://app2.com/cb  # Allowlist mode\nexport JWT_SECRET=$(openssl rand -hex 32)  # Required for multi-pod deployments\n```\n\n**Performance Optimization:**\n\n```bash\n# Focus AI on specific schemas only (10-20x performance improvement)\nexport TRINO_ALLOWED_SCHEMAS=\"hive.analytics,hive.marts,hive.reporting\"\n```\n\nFor complete configuration, see [Deployment Guide](docs/deployment.md), [OAuth Architecture](docs/oauth.md), and [Allowlists Guide](docs/allowlists.md).\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## CI/CD and Releases\n\nThis project uses GitHub Actions for continuous integration and GoReleaser for automated releases.\n\n### Continuous Integration Checks\n\nOur CI pipeline performs the following checks on all PRs and commits to the main branch:\n\n#### Code Quality\n\n- **Linting**: Using golangci-lint to check for common code issues and style violations\n- **Go Module Verification**: Ensuring go.mod and go.sum are properly maintained\n- **Formatting**: Verifying code is properly formatted with gofmt\n\n#### Security\n\n- **Vulnerability Scanning**: Using govulncheck to check for known vulnerabilities in dependencies\n- **Dependency Scanning**: Using Trivy to scan for vulnerabilities in dependencies (CRITICAL, HIGH, and MEDIUM)\n- **SBOM Generation**: Creating a Software Bill of Materials for dependency tracking\n- **SLSA Provenance**: Creating verifiable build provenance for supply chain security\n\n#### Testing\n\n- **Unit Tests**: Running tests with race detection and code coverage reporting\n- **Build Verification**: Ensuring the codebase builds successfully\n\n#### CI/CD Security\n\n- **Least Privilege**: Workflows run with minimum required permissions\n- **Pinned Versions**: All GitHub Actions use specific versions to prevent supply chain attacks\n- **Dependency Updates**: Automated dependency updates via Dependabot\n\n### Release Process\n\nWhen changes are merged to the main branch:\n\n1. CI checks are run to validate code quality and security\n2. If successful, a new release is automatically created with:\n   - Semantic versioning based on commit messages\n   - Binary builds for multiple platforms\n   - Docker image publishing to GitHub Container Registry\n   - SBOM and provenance attestation\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "trino",
        "database",
        "enables querying",
        "access tuannvm",
        "mcp trino"
      ],
      "category": "databases"
    },
    "tylerstoltz--mcp-odbc": {
      "owner": "tylerstoltz",
      "name": "mcp-odbc",
      "url": "https://github.com/tylerstoltz/mcp-odbc",
      "imageUrl": "/freedevtools/mcp/pfp/tylerstoltz.webp",
      "description": "Connects to ODBC-compatible databases for querying, analyzing, and generating insights from data while ensuring read-only access to prevent modifications.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-26T15:12:14Z",
      "readme_content": "# ODBC MCP Server\n\nAn MCP (Model Context Protocol) server that enables LLM tools like Claude Desktop to query databases via ODBC connections. This server allows Claude and other MCP clients to access, analyze, and generate insights from database data while maintaining security and read-only safeguards.\n\n## Features\n\n- Connect to any ODBC-compatible database\n- Support for multiple database connections\n- Flexible configuration through config files or Claude Desktop settings\n- Read-only safeguards to prevent data modification\n- Easy installation with UV package manager\n- Detailed error reporting and logging\n\n## Prerequisites\n\n- Python 3.10 or higher\n- UV package manager\n- ODBC drivers for your database(s) installed on your system\n- For Sage 100 Advanced: ProvideX ODBC driver\n\n## Installation\n\n```bash\ngit clone https://github.com/tylerstoltz/mcp-odbc.git\ncd mcp-odbc\nuv venv\n.venv\\Scripts\\activate # On Mac / Linux: source .venv/bin/activate (untested)\nuv pip install -e .\n```\n\n## Configuration\n\nThe server can be configured through:\n\n1. A dedicated config file\n2. Environment variables\n3. Claude Desktop configuration\n\n### General Configuration Setup\n\nCreate a configuration file (`.ini`) with your database connection details:\n\n```ini\n[SERVER]\ndefault_connection = my_database\nmax_rows = 1000\ntimeout = 30\n\n[my_database]\ndsn = MyDatabaseDSN\nusername = your_username\npassword = your_password\nreadonly = true\n```\n\n### SQLite Configuration\n\nFor SQLite databases with ODBC:\n\n```ini\n[SERVER]\ndefault_connection = sqlite_db\nmax_rows = 1000\ntimeout = 30\n\n[sqlite_db]\ndsn = SQLite_DSN_Name\nreadonly = true\n```\n\n### Sage 100 ProvideX Configuration\n\nProvideX requires special configuration for compatibility. Use this minimal configuration for best results:\n\n```ini\n[SERVER]\ndefault_connection = sage100\nmax_rows = 1000\ntimeout = 60\n\n[sage100]\ndsn = YOUR_PROVIDEX_DSN\nusername = your_username\npassword = your_password\ncompany = YOUR_COMPANY_CODE\nreadonly = true\n```\n\n**Important notes for ProvideX:**\n- Use a minimal configuration - adding extra parameters may cause connection issues\n- Always set `readonly = true` for safety\n- The `company` parameter is required for Sage 100 connections\n- Avoid changing connection attributes after connection is established\n\n### Claude Desktop Integration\n\nTo configure the server in Claude Desktop:\n\n1. Open or create `claude_desktop_config.json`:\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n2. Add MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"odbc\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\path\\\\to\\\\mcp-odbc\",\n        \"run\",\n        \"odbc-mcp-server\",\n        \"--config\", \n        \"C:\\\\path\\\\to\\\\mcp-odbc\\\\config\\\\your_config.ini\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\n### Starting the Server Manually\n\n```bash\n# Start with default configuration\nodbc-mcp-server\n\n# Start with a specific config file\nodbc-mcp-server --config path/to/config.ini\n```\n\n### Using with Claude Desktop\n\n1. Configure the server in Claude Desktop's config file as shown above\n2. Restart Claude Desktop\n3. The ODBC tools will automatically appear in the MCP tools list\n\n### Available MCP Tools\n\nThe ODBC MCP server provides these tools:\n\n1. **list-connections**: Lists all configured database connections\n2. **list-available-dsns**: Lists all available DSNs on the system\n3. **test-connection**: Tests a database connection and returns information\n4. **list-tables**: Lists all tables in the database\n5. **get-table-schema**: Gets schema information for a table\n6. **execute-query**: Executes an SQL query and returns results\n\n## Example Queries\n\nTry these prompts in Claude Desktop after connecting the server:\n\n- \"Show me all the tables in the database\"\n- \"What's the schema of the Customer table?\"\n- \"Run a query to get the first 10 customers\"\n- \"Find all orders placed in the last 30 days\"\n- \"Analyze the sales data by region and provide insights\"\n\n## Troubleshooting\n\n### Connection Issues\n\nIf you encounter connection problems:\n\n1. Verify your ODBC drivers are installed correctly\n2. Test your DSN using the ODBC Data Source Administrator\n3. Check connection parameters in your config file\n4. Look for detailed error messages in Claude Desktop logs\n\n### ProvideX-Specific Issues\n\nFor Sage 100/ProvideX:\n1. Use minimal connection configuration (DSN, username, password, company)\n2. Make sure the Company parameter is correct\n3. Use the special ProvideX configuration template\n4. If you encounter `Driver not capable` errors, check that autocommit is being set at connection time\n\n### Missing Tables\n\nIf tables aren't showing up:\n\n1. Verify user permissions for the database account\n2. Check if the company code is correct (for Sage 100)\n3. Try using fully qualified table names (schema.table)\n\n## License\n\nMIT License - Copyright (c) 2024",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "odbc",
        "databases",
        "database",
        "odbc compatible",
        "connects odbc",
        "odbc connects"
      ],
      "category": "databases"
    },
    "ujjalcal--mcp": {
      "owner": "ujjalcal",
      "name": "mcp",
      "url": "https://github.com/ujjalcal/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ujjalcal.webp",
      "description": "Facilitates the creation and connection of LLM applications by providing a standardized method to expose tools, data, and prompts for interactions. It supports data management and relationship mappings within a graph database environment.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-11T02:10:36Z",
      "readme_content": "# MCP Python SDK\n\n    which python\n    python3 -m venv myenv\n    source myenv/Scripts/activate\n    pip install -r requirements.txt\n    python fast_mcp_server.py\n\n\n# Usage\n## to run the client\n    \n    add\n        .env\n        OPENAI_API_KEY=\n\n## to run the server\n    \n    uvicorn ne04j_mcp_server:app --host 0.0.0.0 --port 8000\n\n# Data Prep\n\n## Insert Node:\n    CREATE (p1:Person {name: \"Tom Hanks\", birthYear: 1956})\n    CREATE (p2:Person {name: \"Kevin Bacon\", birthYear: 1958})\n    CREATE (m1:Movie {title: \"Forrest Gump\", releaseYear: 1994})\n    CREATE (m2:Movie {title: \"Apollo 13\", releaseYear: 1995})\n\n## Insert Relationship\n    MATCH (p:Person {name: \"Tom Hanks\"}), (m:Movie {title: \"Forrest Gump\"})\n    CREATE (p)-[:ACTED_IN]->(m)\n\n    MATCH (p:Person {name: \"Tom Hanks\"}), (m:Movie {title: \"Apollo 13\"})\n    CREATE (p)-[:ACTED_IN]->(m)\n\n    MATCH (p1:Person {name: \"Tom Hanks\"}), (p2:Person {name: \"Kevin Bacon\"})\n    CREATE (p1)-[:FRIENDS_WITH]->(p2)\n\n## Insert properties\n    MATCH (p:Person {name: \"Tom Hanks\"})\n    SET p.oscarsWon = 2\n\n    MATCH (m:Movie {title: \"Forrest Gump\"})\n    SET m.genre = \"Drama\"\n\n    MATCH (p:Person {name: \"Tom Hanks\"})-[r:ACTED_IN]->(m:Movie {title: \"Forrest Gump\"})\n    SET r.role = \"Forrest Gump\"\n\n## Insert Complex Structure\n    // Create a community and then match persons to add them to the community\n    CREATE (c1:Community {name: \"Hollywood Stars\"})\n    WITH c1\n    MATCH (p:Person)\n    WHERE p.name IN [\"Tom Hanks\", \"Kevin Bacon\"]\n    CREATE (p)-[:MEMBER_OF]->(c1)\n   \n# other things - boiler plate - no clue - ignore for now.\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Specification][spec-badge]][spec-url]\n[![GitHub Discussions][discussions-badge]][discussions-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Quickstart](#quickstart)\n- [What is MCP?](#what-is-mcp)\n- [Core Concepts](#core-concepts)\n  - [Server](#server)\n  - [Resources](#resources)\n  - [Tools](#tools)\n  - [Prompts](#prompts)\n  - [Images](#images)\n  - [Context](#context)\n- [Running Your Server](#running-your-server)\n  - [Development Mode](#development-mode)\n  - [Claude Desktop Integration](#claude-desktop-integration)\n  - [Direct Execution](#direct-execution)\n- [Examples](#examples)\n  - [Echo Server](#echo-server)\n  - [SQLite Explorer](#sqlite-explorer)\n- [Advanced Usage](#advanced-usage)\n  - [Low-Level Server](#low-level-server)\n  - [Writing MCP Clients](#writing-mcp-clients)\n  - [MCP Primitives](#mcp-primitives)\n  - [Server Capabilities](#server-capabilities)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [License](#license)\n\n[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg\n[pypi-url]: https://pypi.org/project/mcp/\n[mit-badge]: https://img.shields.io/pypi/l/mcp.svg\n[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE\n[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg\n[python-url]: https://www.python.org/downloads/\n[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg\n[docs-url]: https://modelcontextprotocol.io\n[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg\n[spec-url]: https://spec.modelcontextprotocol.io\n[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk\n[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions\n\n## Overview\n\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\n\n- Build MCP clients that can connect to any MCP server\n- Create MCP servers that expose resources, prompts and tools\n- Use standard transports like stdio and SSE\n- Handle all MCP protocol messages and lifecycle events\n\n## Installation\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:\n\n```bash\nuv add \"mcp[cli]\"\n```\n\nAlternatively:\n```bash\npip install mcp\n```\n\n## Quickstart\n\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\n```python\n# server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n```\n\nYou can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:\n```bash\nmcp install server.py\n```\n\nAlternatively, you can test it with the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## What is MCP?\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\n\n- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)\n- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\n- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)\n- And more!\n\n## Core Concepts\n\n### Server\n\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\n```python\n# Add lifespan support for startup/shutdown with strong typing\nfrom dataclasses import dataclass\nfrom typing import AsyncIterator\nfrom mcp.server.fastmcp import FastMCP\n\n# Create a named server\nmcp = FastMCP(\"My App\")\n\n# Specify dependencies for deployment and development\nmcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n\n@dataclass\nclass AppContext:\n    db: Database  # Replace with your actual DB type\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n    try:\n        # Initialize on startup\n        await db.connect()\n        yield AppContext(db=db)\n    finally:\n        # Cleanup on shutdown\n        await db.disconnect()\n\n# Pass lifespan to server\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n# Access type-safe lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context) -> str:\n    \"\"\"Tool that uses initialized resources\"\"\"\n    db = ctx.request_context.lifespan_context[\"db\"]\n    return db.query()\n```\n\n### Resources\n\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\n\n```python\n@mcp.resource(\"config://app\")\ndef get_config() -> str:\n    \"\"\"Static configuration data\"\"\"\n    return \"App configuration here\"\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -> str:\n    \"\"\"Dynamic user data\"\"\"\n    return f\"Profile data for user {user_id}\"\n```\n\n### Tools\n\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n\n```python\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n    return weight_kg / (height_m ** 2)\n\n@mcp.tool()\nasync def fetch_weather(city: str) -> str:\n    \"\"\"Fetch current weather for a city\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        return response.text\n```\n\n### Prompts\n\nPrompts are reusable templates that help LLMs interact with your server effectively:\n\n```python\n@mcp.prompt()\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n@mcp.prompt()\ndef debug_error(error: str) -> list[Message]:\n    return [\n        UserMessage(\"I'm seeing this error:\"),\n        UserMessage(error),\n        AssistantMessage(\"I'll help debug that. What have you tried so far?\")\n    ]\n```\n\n### Images\n\nFastMCP provides an `Image` class that automatically handles image data:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Image\nfrom PIL import Image as PILImage\n\n@mcp.tool()\ndef create_thumbnail(image_path: str) -> Image:\n    \"\"\"Create a thumbnail from an image\"\"\"\n    img = PILImage.open(image_path)\n    img.thumbnail((100, 100))\n    return Image(data=img.tobytes(), format=\"png\")\n```\n\n### Context\n\nThe Context object gives your tools and resources access to MCP capabilities:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\n@mcp.tool()\nasync def long_task(files: list[str], ctx: Context) -> str:\n    \"\"\"Process multiple files with progress tracking\"\"\"\n    for i, file in enumerate(files):\n        ctx.info(f\"Processing {file}\")\n        await ctx.report_progress(i, len(files))\n        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n    return \"Processing complete\"\n```\n\n## Running Your Server\n\n### Development Mode\n\nThe fastest way to test and debug your server is with the MCP Inspector:\n\n```bash\nmcp dev server.py\n\n# Add dependencies\nmcp dev server.py --with pandas --with numpy\n\n# Mount local code\nmcp dev server.py --with-editable .\n```\n\n### Claude Desktop Integration\n\nOnce your server is ready, install it in Claude Desktop:\n\n```bash\nmcp install server.py\n\n# Custom name\nmcp install server.py --name \"My Analytics Server\"\n\n# Environment variables\nmcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nmcp install server.py -f .env\n```\n\n### Direct Execution\n\nFor advanced scenarios like custom deployments:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\nRun it with:\n```bash\npython server.py\n# or\nmcp run server.py\n```\n\n## Examples\n\n### Echo Server\n\nA simple server demonstrating resources, tools, and prompts:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo\")\n\n@mcp.resource(\"echo://{message}\")\ndef echo_resource(message: str) -> str:\n    \"\"\"Echo a message as a resource\"\"\"\n    return f\"Resource echo: {message}\"\n\n@mcp.tool()\ndef echo_tool(message: str) -> str:\n    \"\"\"Echo a message as a tool\"\"\"\n    return f\"Tool echo: {message}\"\n\n@mcp.prompt()\ndef echo_prompt(message: str) -> str:\n    \"\"\"Create an echo prompt\"\"\"\n    return f\"Please process this message: {message}\"\n```\n\n### SQLite Explorer\n\nA more complex example showing database integration:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nimport sqlite3\n\nmcp = FastMCP(\"SQLite Explorer\")\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -> str:\n    \"\"\"Provide the database schema as a resource\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    schema = conn.execute(\n        \"SELECT sql FROM sqlite_master WHERE type='table'\"\n    ).fetchall()\n    return \"\\n\".join(sql[0] for sql in schema if sql[0])\n\n@mcp.tool()\ndef query_data(sql: str) -> str:\n    \"\"\"Execute SQL queries safely\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    try:\n        result = conn.execute(sql).fetchall()\n        return \"\\n\".join(str(row) for row in result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n## Advanced Usage\n\n### Low-Level Server\n\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator\n\n@asynccontextmanager\nasync def server_lifespan(server: Server) -> AsyncIterator[dict]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    try:\n        # Initialize resources on startup\n        await db.connect()\n        yield {\"db\": db}\n    finally:\n        # Clean up on shutdown\n        await db.disconnect()\n\n# Pass lifespan to server\nserver = Server(\"example-server\", lifespan=server_lifespan)\n\n# Access lifespan context in handlers\n@server.call_tool()\nasync def query_db(name: str, arguments: dict) -> list:\n    ctx = server.request_context\n    db = ctx.lifespan_context[\"db\"]\n    return await db.query(arguments[\"query\"])\n```\n\nThe lifespan API provides:\n- A way to initialize resources when the server starts and clean them up when it stops\n- Access to initialized resources through the request context in handlers\n- Type-safe context passing between lifespan and request handlers\n\n```python\nfrom mcp.server.lowlevel import Server, NotificationOptions\nfrom mcp.server.models import InitializationOptions\nimport mcp.server.stdio\nimport mcp.types as types\n\n# Create a server instance\nserver = Server(\"example-server\")\n\n@server.list_prompts()\nasync def handle_list_prompts() -> list[types.Prompt]:\n    return [\n        types.Prompt(\n            name=\"example-prompt\",\n            description=\"An example prompt template\",\n            arguments=[\n                types.PromptArgument(\n                    name=\"arg1\",\n                    description=\"Example argument\",\n                    required=True\n                )\n            ]\n        )\n    ]\n\n@server.get_prompt()\nasync def handle_get_prompt(\n    name: str,\n    arguments: dict[str, str] | None\n) -> types.GetPromptResult:\n    if name != \"example-prompt\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    return types.GetPromptResult(\n        description=\"Example prompt\",\n        messages=[\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(\n                    type=\"text\",\n                    text=\"Example prompt text\"\n                )\n            )\n        ]\n    )\n\nasync def run():\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                )\n            )\n        )\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(run())\n```\n\n### Writing MCP Clients\n\nThe SDK provides a high-level client interface for connecting to MCP servers:\n\n```python\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"python\", # Executable\n    args=[\"example_server.py\"], # Optional command line arguments\n    env=None # Optional environment variables\n)\n\n# Optional: create a sampling callback\nasync def handle_sampling_message(message: types.CreateMessageRequestParams) -> types.CreateMessageResult:\n    return types.CreateMessageResult(\n        role=\"assistant\",\n        content=types.TextContent(\n            type=\"text\",\n            text=\"Hello, world! from model\",\n        ),\n        model=\"gpt-3.5-turbo\",\n        stopReason=\"endTurn\",\n    )\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=handle_sampling_message) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n\n            # Get a prompt\n            prompt = await session.get_prompt(\"example-prompt\", arguments={\"arg1\": \"value\"})\n\n            # List available resources\n            resources = await session.list_resources()\n\n            # List available tools\n            tools = await session.list_tools()\n\n            # Read a resource\n            content, mime_type = await session.read_resource(\"file://some/path\")\n\n            # Call a tool\n            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(run())\n```\n\n### MCP Primitives\n\nThe MCP protocol defines three core primitives that servers can implement:\n\n| Primitive | Control               | Description                                         | Example Use                  |\n|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n\n### Server Capabilities\n\nMCP servers declare capabilities during initialization:\n\n| Capability  | Feature Flag                 | Description                        |\n|-------------|------------------------------|------------------------------------|\n| `prompts`   | `listChanged`                | Prompt template management         |\n| `resources` | `subscribe`<br/>`listChanged`| Resource exposure and updates      |\n| `tools`     | `listChanged`                | Tool discovery and execution       |\n| `logging`   | -                            | Server logging configuration       |\n| `completion`| -                            | Argument completion suggestions    |\n\n## Documentation\n\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Model Context Protocol specification](https://spec.modelcontextprotocol.io)\n- [Officially supported servers](https://github.com/modelcontextprotocol/servers)\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "schema",
        "secure database",
        "databases secure",
        "data management"
      ],
      "category": "databases"
    },
    "vinhphamai23--iotdb-mcp-server": {
      "owner": "vinhphamai23",
      "name": "iotdb-mcp-server",
      "url": "https://github.com/vinhphamai23/iotdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Enable secure interaction with IoTDB databases by executing SQL queries and exploring database schemas. Functionality includes listing tables, describing table schemas, and executing SELECT or metadata queries.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "iotdb",
        "databases",
        "database",
        "iotdb databases",
        "secure database",
        "databases secure"
      ],
      "category": "databases"
    },
    "vinsidious--mcp-pg-schema": {
      "owner": "vinsidious",
      "name": "mcp-pg-schema",
      "url": "https://github.com/vinsidious/mcp-pg-schema",
      "imageUrl": "/freedevtools/mcp/pfp/vinsidious.webp",
      "description": "Provides read-only access to PostgreSQL databases, enabling inspection of database schemas and execution of read-only SQL queries within a read-only transaction.",
      "stars": 4,
      "forks": 6,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-07T00:36:10Z",
      "readme_content": "# PostgreSQL\n\nA Model Context Protocol server that provides read-only access to PostgreSQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Components\n\n### Tools\n\n- **query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n\n### Resources\n\nThe server provides schema information for each table in the database:\n\n- **Table Schemas** (`postgres://<host>/<table>/schema`)\n  - JSON schema information for each table\n  - Includes column names and data types\n  - Automatically discovered from database metadata\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n### Docker\n\n* when running docker on macos, use host.docker.internal if the server is running on the host network (eg localhost)\n* username/password can be added to the postgresql url with `postgresql://user:password@host:port/db-name`\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"mcp/postgres\", \n        \"postgresql://host.docker.internal:5432/mydb\"]\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-postgres\",\n        \"postgresql://localhost/mydb\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/mydb` with your database name.\n\n## Building\n\nDocker:\n\n```sh\ndocker build -t mcp/postgres -f src/postgres/Dockerfile . \n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgresql",
        "databases",
        "database",
        "postgresql databases",
        "databases secure",
        "pg schema"
      ],
      "category": "databases"
    },
    "vitalyDV--mysql-mcp": {
      "owner": "vitalyDV",
      "name": "mysql-mcp",
      "url": "https://github.com/vitalyDV/mysql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vitalyDV.webp",
      "description": "Interact with a MySQL database by executing SQL queries, retrieving table structures, and accessing data seamlessly. Integrate database capabilities into applications and workflows effectively.",
      "stars": 4,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T03:13:43Z",
      "readme_content": "# MySQL MCP Server\n\nThis project implements an MCP (Model Context Protocol) server for working with MySQL database.\n\n## Repository\n\nThis project is available on GitHub:\nhttps://github.com/vitalyDV/mysql-mcp\n\n### Clone the repository\n\n```bash\ngit clone https://github.com/vitalyDV/mysql-mcp.git\ncd mysql-mcp\nnpm install\n```\n\n## add config to mcp.json\n```json\n{\n  \"mcpServers\": {\n    \"mysql_mcp_readonly\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"./mysql-mcp/index.js\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db\",\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n- `MYSQL_HOST` - MySQL server host\n- `MYSQL_PORT` - MySQL server port\n- `MYSQL_USER` - MySQL username\n- `MYSQL_PASS` - MySQL password\n- `MYSQL_DB` - MySQL database name\n\n## Available MCP tools\n\n- `query` - execute SQL queries (only SELECT, SHOW, EXPLAIN, DESCRIBE)\n- `table-schema` - get table structure\n- `list-tables` - get list of all tables in the database\n\n## Available MCP resources\n\n- `table://{name}` - get data from the specified table (up to 100 rows)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "vivek1612--mongodb-mcp": {
      "owner": "vivek1612",
      "name": "mongodb-mcp",
      "url": "https://github.com/vivek1612/mongodb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vivek1612.webp",
      "description": "Enable interaction with MongoDB databases to query collections, inspect schemas, and manage data through natural language commands, facilitating database operations.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-19T05:43:25Z",
      "readme_content": "# 🗄️ MongoDB MCP Server for LLMS\n\n[![Node.js 18+](https://img.shields.io/badge/node-18%2B-blue.svg)](https://nodejs.org/en/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/mongo-mcp)](https://smithery.ai/server/mongo-mcp)\n\nA Model Context Protocol (MCP) server that enables LLMs to interact directly with MongoDB databases. Query collections, inspect schemas, and manage data seamlessly through natural language.\n\n## ✨ Features\n\n- 🔍 Collection schema inspection\n- 📊 Document querying and filtering\n- 📈 Index management\n- 📝 Document operations (insert, update, delete)\n\n## Demo Video\n\n\nhttps://github.com/user-attachments/assets/2389bf23-a10d-49f9-bca9-2b39a1ebe654\n\n\n\n\n## 🚀 Quick Start\n\nTo get started, find your mongodb connection url and add this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mongo-mcp\",\n        \"mongodb://<username>:<password>@<host>:<port>/<database>?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install MongoDB MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mongo-mcp):\n\n```bash\nnpx -y @smithery/cli install mongo-mcp --client claude\n```\n\n### Prerequisites\n\n- Node.js 18+\n- npx\n- Docker and Docker Compose (for local sandbox testing only)\n- MCP Client (Claude Desktop App for example)\n\n### Test Sandbox Setup\n\nIf you don't have a mongo db server to connect to and want to create a sample sandbox, follow these steps\n\n1. Start MongoDB using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\n2. Seed the database with test data:\n\n```bash\nnpm run seed\n```\n\n### Configure Claude Desktop\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Local Development Mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"mongodb\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"dist/index.js\",\n        \"mongodb://root:example@localhost:27017/test?authSource=admin\"\n      ]\n    }\n  }\n}\n```\n\n### Test Sandbox Data Structure\n\nThe seed script creates three collections with sample data:\n\n#### Users\n\n- Personal info (name, email, age)\n- Nested address with coordinates\n- Arrays of interests\n- Membership dates\n\n#### Products\n\n- Product details (name, SKU, category)\n- Nested specifications\n- Price and inventory info\n- Tags and ratings\n\n#### Orders\n\n- Order details with items\n- User references\n- Shipping and payment info\n- Status tracking\n\n## 🎯 Example Prompts\n\nTry these prompts with Claude to explore the functionality:\n\n### Basic Operations\n\n```plaintext\n\"What collections are available in the database?\"\n\"Show me the schema for the users collection\"\n\"Find all users in San Francisco\"\n```\n\n### Advanced Queries\n\n```plaintext\n\"Find all electronics products that are in stock and cost less than $1000\"\n\"Show me all orders from the user john@example.com\"\n\"List the products with ratings above 4.5\"\n```\n\n### Index Management\n\n```plaintext\n\"What indexes exist on the users collection?\"\n\"Create an index on the products collection for the 'category' field\"\n\"List all indexes across all collections\"\n```\n\n### Document Operations\n\n```plaintext\n\"Insert a new product with name 'Gaming Laptop' in the products collection\"\n\"Update the status of order with ID X to 'shipped'\"\n\"Find and delete all products that are out of stock\"\n```\n\n## 📝 Available Tools\n\nThe server provides these tools for database interaction:\n\n### Query Tools\n\n- `find`: Query documents with filtering and projection\n- `listCollections`: List available collections\n- `insertOne`: Insert a single document\n- `updateOne`: Update a single document\n- `deleteOne`: Delete a single document\n\n### Index Tools\n\n- `createIndex`: Create a new index\n- `dropIndex`: Remove an index\n- `indexes`: List indexes for a collection\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "databases",
        "database",
        "vivek1612 mongodb",
        "mongodb databases",
        "secure database"
      ],
      "category": "databases"
    },
    "vurtnec--mcp-LanceDB-node": {
      "owner": "vurtnec",
      "name": "mcp-LanceDB-node",
      "url": "https://github.com/vurtnec/mcp-LanceDB-node",
      "imageUrl": "/freedevtools/mcp/pfp/vurtnec.webp",
      "description": "Connects to a LanceDB database to perform vector similarity searches using custom embedding functions, facilitating enhanced search capabilities and result processing.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-23T15:36:02Z",
      "readme_content": "# LanceDB Node.js Vector Search\n\nA Node.js implementation for vector search using LanceDB and Ollama's embedding model.\n\n## Overview\n\nThis project demonstrates how to:\n- Connect to a LanceDB database\n- Create custom embedding functions using Ollama\n- Perform vector similarity search against stored documents\n- Process and display search results\n\n## Prerequisites\n\n- Node.js (v14 or later)\n- Ollama running locally with the `nomic-embed-text` model\n- LanceDB storage location with read/write permissions\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n## Dependencies\n\n- `@lancedb/lancedb`: LanceDB client for Node.js\n- `apache-arrow`: For handling columnar data\n- `node-fetch`: For making API calls to Ollama\n\n## Usage\n\nRun the vector search test script:\n\n```bash\npnpm test-vector-search\n```\n\nOr directly execute:\n\n```bash\nnode test-vector-search.js\n```\n\n## Configuration\n\nThe script connects to:\n- LanceDB at the configured path\n- Ollama API at `http://localhost:11434/api/embeddings`\n\n## MCP Configuration\n\nTo integrate with Claude Desktop as an MCP service, add the following to your MCP configuration JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"lanceDB\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/lancedb-node/dist/index.js\",\n        \"--db-path\",\n        \"/path/to/your/lancedb/storage\"\n      ]\n    }\n  }\n}\n```\n\nReplace the paths with your actual installation paths:\n- `/path/to/lancedb-node/dist/index.js` - Path to the compiled index.js file\n- `/path/to/your/lancedb/storage` - Path to your LanceDB storage directory\n\n## Custom Embedding Function\n\nThe project includes a custom `OllamaEmbeddingFunction` that:\n- Sends text to the Ollama API\n- Receives embeddings with 768 dimensions\n- Formats them for use with LanceDB\n\n## Vector Search Example\n\nThe example searches for \"how to define success criteria\" in the \"ai-rag\" table, displaying results with their similarity scores.\n\n## License\n\n[MIT License](LICENSE)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "lancedb",
        "lancedb database",
        "lancedb node",
        "secure database"
      ],
      "category": "databases"
    },
    "wanattichia-skeelo--airtable-mcp-server": {
      "owner": "wanattichia-skeelo",
      "name": "airtable-mcp-server",
      "url": "https://github.com/wanattichia-skeelo/airtable-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/wanattichia-skeelo.webp",
      "description": "Connect to Airtable databases to read and write records, inspect database schemas, and manage data efficiently.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-29T04:26:50Z",
      "readme_content": "# airtable-mcp-server\n\n[![smithery badge](https://smithery.ai/badge/airtable-mcp-server)](https://smithery.ai/server/airtable-mcp-server)\n\nA Model Context Protocol server that provides read and write access to Airtable databases. This server enables LLMs to inspect database schemas, then read and write records.\n\nhttps://github.com/user-attachments/assets/c8285e76-d0ed-4018-94c7-20535db6c944\n\n## Usage\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"airtable\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"airtable-mcp-server\"\n      ],\n      \"env\": {\n        \"AIRTABLE_API_KEY\": \"pat123.abc123\"\n      }\n    }\n  }\n}\n```\n\nReplace `pat123.abc123` with your [Airtable personal access token](https://airtable.com/create/tokens). Your token should have at least `schema.bases:read` and `data.records:read`, and optionally the corresponding write permissions.\n\n## Components\n\n### Tools\n\n- **list_records**\n  - Lists records from a specified Airtable table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to query\n    - `maxRecords` (number, optional): Maximum number of records to return. Defaults to 100.\n    - `filterByFormula` (string, optional): Airtable formula to filter records\n\n- **search_records**\n  - Search for records containing specific text\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to query\n    - `searchTerm` (string, required): Text to search for in records\n    - `fieldIds` (array, optional): Specific field IDs to search in. If not provided, searches all text-based fields.\n    - `maxRecords` (number, optional): Maximum number of records to return. Defaults to 100.\n\n- **list_bases**\n  - Lists all accessible Airtable bases\n  - No input parameters required\n  - Returns base ID, name, and permission level\n\n- **list_tables**\n  - Lists all tables in a specific base\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `detailLevel` (string, optional): The amount of detail to get about the tables (`tableIdentifiersOnly`, `identifiersOnly`, or `full`)\n  - Returns table ID, name, description, fields, and views (to the given `detailLevel`)\n\n- **describe_table**\n  - Gets detailed information about a specific table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table to describe\n    - `detailLevel` (string, optional): The amount of detail to get about the table (`tableIdentifiersOnly`, `identifiersOnly`, or `full`)\n  - Returns the same format as list_tables but for a single table\n  - Useful for getting details about a specific table without fetching information about all tables in the base\n\n- **get_record**\n  - Gets a specific record by ID\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `recordId` (string, required): The ID of the record to retrieve\n\n- **create_record**\n  - Creates a new record in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `fields` (object, required): The fields and values for the new record\n\n- **update_records**\n  - Updates one or more records in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `records` (array, required): Array of objects containing record ID and fields to update\n\n- **delete_records**\n  - Deletes one or more records from a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `recordIds` (array, required): Array of record IDs to delete\n\n- **create_table**\n  - Creates a new table in a base\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `name` (string, required): Name of the new table\n    - `description` (string, optional): Description of the table\n    - `fields` (array, required): Array of field definitions (name, type, description, options)\n\n- **update_table**\n  - Updates a table's name or description\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `name` (string, optional): New name for the table\n    - `description` (string, optional): New description for the table\n\n- **create_field**\n  - Creates a new field in a table\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `name` (string, required): Name of the new field\n    - `type` (string, required): Type of the field\n    - `description` (string, optional): Description of the field\n    - `options` (object, optional): Field-specific options\n\n- **update_field**\n  - Updates a field's name or description\n  - Input parameters:\n    - `baseId` (string, required): The ID of the Airtable base\n    - `tableId` (string, required): The ID of the table\n    - `fieldId` (string, required): The ID of the field\n    - `name` (string, optional): New name for the field\n    - `description` (string, optional): New description for the field\n\n### Resources\n\nThe server provides schema information for Airtable bases and tables:\n\n- **Table Schemas** (`airtable://<baseId>/<tableId>/schema`)\n  - JSON schema information for each table\n  - Includes:\n    - Base id and table id\n    - Table name and description\n    - Primary field ID\n    - Field definitions (ID, name, type, description, options)\n    - View definitions (ID, name, type)\n  - Automatically discovered from Airtable's metadata API\n\n## Contributing\n\nPull requests are welcomed on GitHub! To get started:\n\n1. Install Git and Node.js\n2. Clone the repository\n3. Install dependencies with `npm install`\n4. Run `npm run test` to run tests\n5. Build with `npm run build`\n  - You can use `npm run build:watch` to automatically build after editing [`src/index.ts`](./src/index.ts). This means you can hit save, reload Claude Desktop (with Ctrl/Cmd+R), and the changes apply.\n\n## Releases\n\nVersions follow the [semantic versioning spec](https://semver.org/).\n\nTo release:\n\n1. Use `npm version <major | minor | patch>` to bump the version\n2. Run `git push --follow-tags` to push with tags\n3. Wait for GitHub Actions to publish to the NPM registry.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "airtable",
        "airtable databases",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "weaviate--mcp-server-weaviate": {
      "owner": "weaviate",
      "name": "mcp-server-weaviate",
      "url": "https://github.com/weaviate/mcp-server-weaviate",
      "imageUrl": "",
      "description": "An MCP Server to connect to your Weaviate collections as a knowledge base as well as using Weaviate as a chat memory store.",
      "stars": 152,
      "forks": 40,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-09-13T13:17:17Z",
      "readme_content": "# Weaviate MCP Server\n\n## Instructions\n\nBuild the server:\n```\nmake build\n```\n\nRun the test client\n```\nmake run-client\n```\n\n## Tools\n\n### Insert One\nInsert an object into weaviate.\n\n**Request body:**\n```json\n{}\n```\n\n**Response body**\n```json\n{}\n```\n\n### Query\nRetrieve objects from weaviate with hybrid search.\n\n**Request body:**\n```json\n{}\n```\n\n**Response body**\n```json\n{}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "weaviate",
        "databases",
        "database",
        "server weaviate",
        "access weaviate",
        "using weaviate"
      ],
      "category": "databases"
    },
    "wenb1n-dev--mysql_mcp_server_pro": {
      "owner": "wenb1n-dev",
      "name": "mysql_mcp_server_pro",
      "url": "https://github.com/wenb1n-dev/mysql_mcp_server_pro",
      "imageUrl": "/freedevtools/mcp/pfp/wenb1n-dev.webp",
      "description": "Execute SQL commands, manage MySQL databases, analyze execution plans, and convert Chinese fields to pinyin. Perform database anomaly analysis and provide robust permission control.",
      "stars": 262,
      "forks": 32,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T06:56:04Z",
      "readme_content": "[![简体中文](https://img.shields.io/badge/简体中文-点击查看-orange)](README-zh.md)\n[![English](https://img.shields.io/badge/English-Click-yellow)](README.md)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/wenb1n-dev-mysql-mcp-server-pro)\n[![MCPHub](https://img.shields.io/badge/mcphub-audited-blue)](https://mcphub.com/mcp-servers/wenb1n-dev/mysql_mcp_server_pro)\n\n\n# mcp_mysql_server_pro\n\n## Introduction\nmcp_mysql_server_pro is not just about MySQL CRUD operations, but also includes database anomaly analysis capabilities and makes it easy for developers to extend with custom tools.\n\n- Supports all Model Context Protocol (MCP) transfer modes (STDIO, SSE, Streamable Http)\n- Supports OAuth2.0\n- Supports multiple SQL execution, separated by \";\"\n- Supports querying database table names and fields based on table comments\n- Supports SQL execution plan analysis\n- Supports Chinese field to pinyin conversion\n- Supports table lock analysis\n- Supports database health status analysis\n- Supports permission control with three roles: readonly, writer, and admin\n    ```\n    \"readonly\": [\"SELECT\", \"SHOW\", \"DESCRIBE\", \"EXPLAIN\"],  # Read-only permissions\n    \"writer\": [\"SELECT\", \"SHOW\", \"DESCRIBE\", \"EXPLAIN\", \"INSERT\", \"UPDATE\", \"DELETE\"],  # Read-write permissions\n    \"admin\": [\"SELECT\", \"SHOW\", \"DESCRIBE\", \"EXPLAIN\", \"INSERT\", \"UPDATE\", \"DELETE\", \n             \"CREATE\", \"ALTER\", \"DROP\", \"TRUNCATE\"]  # Administrator permissions\n    ```\n- Supports prompt template invocation\n\n\n## Tool List\n| Tool Name                  | Description                                                                                                                                                                                                              |\n|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| \n| execute_sql                | SQL execution tool that can execute [\"SELECT\", \"SHOW\", \"DESCRIBE\", \"EXPLAIN\", \"INSERT\", \"UPDATE\", \"DELETE\", \"CREATE\", \"ALTER\", \"DROP\", \"TRUNCATE\"] commands based on permission configuration                            |\n| get_chinese_initials       | Convert Chinese field names to pinyin initials                                                                                                                                                                           |\n| get_db_health_running      | Analyze MySQL health status (connection status, transaction status, running status, lock status detection)                                                                                                               |\n| get_table_desc             | Search for table structures in the database based on table names, supporting multi-table queries                                                                                                                         |\n| get_table_index            | Search for table indexes in the database based on table names, supporting multi-table queries                                                                                                                            |\n| get_table_lock             | Check if there are row-level locks or table-level locks in the current MySQL server                                                                                                                                      |\n| get_table_name             | Search for table names in the database based on table comments and descriptions                                                                                                                                          |\n| get_db_health_index_usage  | Get the index usage of the currently connected mysql database, including redundant index situations, poorly performing index situations, and the top 5 unused index situations with query times greater than 30 seconds  | \n| optimize_sql               | Professional SQL performance optimization tool, providing expert optimization suggestions based on MySQL execution plans, table structure information, table data volume, and table indexes.                            |\n| use_prompt_queryTableData | Use built-in prompts to let the model construct a chain call of tools in mcp (not a commonly used fixed tool, you need to modify the code to enable it, see this class for details) |\n\n## Prompt List\n| Prompt Name                | Description                                                                                                                           |\n|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------| \n| analyzing-mysql-prompt    | This is a prompt for analyzing MySQL-related issues                                                                                    |\n| query-table-data-prompt   | This is a prompt for querying table data using tools. If description is empty, it will be initialized as a MySQL database query assistant |\n\n## Usage Instructions\n\n### Installation and Configuration\n1. Install Package\n```bash\npip install mysql_mcp_server_pro\n```\n\n2. Configure Environment Variables\nCreate a `.env` file with the following content:\n```bash\n# MySQL Database Configuration\nMYSQL_HOST=localhost\nMYSQL_PORT=3306\nMYSQL_USER=your_username\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=your_database\n# Optional, default is 'readonly'. Available values: readonly, writer, admin\nMYSQL_ROLE=readonly\n```\n\n3. Run Service\n```bash\n# SSE mode\nmysql_mcp_server_pro --mode sse --envfile /path/to/.env\n\n## Streamable Http mode (default)\nmysql_mcp_server_pro --envfile /path/to/.env\n\n# Streamable Http  oauth Authentication\nmysql_mcp_server_pro --oauth true\n\n```\n\n4. mcp client\n\ngo to see see \"Use uv to start the service\"\n^_^\n\n\nNote:\n- The `.env` file should be placed in the directory where you run the command or use --envfile parameter to specify the path\n- You can also set these variables directly in your environment\n- Make sure the database configuration is correct and can connect\n\n### Run with uvx, Client Configuration\n- This method can be used directly in MCP-supported clients, no need to download the source code. For example, Tongyi Qianwen plugin, trae editor, etc.\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mysql\": {\n\t\t\t\"command\": \"uvx\",\n\t\t\t\"args\": [\n\t\t\t\t\"--from\",\n\t\t\t\t\"mysql_mcp_server_pro\",\n\t\t\t\t\"mysql_mcp_server_pro\",\n\t\t\t\t\"--mode\",\n\t\t\t\t\"stdio\"\n\t\t\t],\n\t\t\t\"env\": {\n\t\t\t\t\"MYSQL_HOST\": \"192.168.x.xxx\",\n\t\t\t\t\"MYSQL_PORT\": \"3306\",\n\t\t\t\t\"MYSQL_USER\": \"root\",\n\t\t\t\t\"MYSQL_PASSWORD\": \"root\",\n\t\t\t\t\"MYSQL_DATABASE\": \"a_llm\",\n\t\t\t\t\"MYSQL_ROLE\": \"admin\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Local Development with Streamable Http mode\n\n- Use uv to start the service\n\nAdd the following content to your mcp client tools, such as cursor, cline, etc.\n\nmcp json as follows:\n```\n{\n  \"mcpServers\": {\n    \"mysql_mcp_server_pro\": {\n      \"name\": \"mysql_mcp_server_pro\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"\",\n      \"isActive\": true,\n      \"url\": \"http://localhost:3000/mcp/\"\n    }\n  }\n}\n```\n\nModify the .env file content to update the database connection information with your database details:\n```\n# MySQL Database Configuration\nMYSQL_HOST=192.168.xxx.xxx\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=root\nMYSQL_DATABASE=a_llm\nMYSQL_ROLE=admin\n```\n\nStart commands:\n```\n# Download dependencies\nuv sync\n\n# Start\nuv run -m mysql_mcp_server_pro.server\n\n# Custom env file location\nuv run -m mysql_mcp_server_pro.server --envfile /path/to/.env\n\n# oauth Authentication\nuv run -m mysql_mcp_server_pro.server --oauth true\n```\n\n### Local Development with SSE Mode\n\n- Use uv to start the service\n\nAdd the following content to your mcp client tools, such as cursor, cline, etc.\n\nmcp json as follows:\n```\n{\n  \"mcpServers\": {\n    \"mysql_mcp_server_pro\": {\n      \"name\": \"mysql_mcp_server_pro\",\n      \"description\": \"\",\n      \"isActive\": true,\n      \"url\": \"http://localhost:9000/sse\"\n    }\n  }\n}\n```\n\nModify the .env file content to update the database connection information with your database details:\n```\n# MySQL Database Configuration\nMYSQL_HOST=192.168.xxx.xxx\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=root\nMYSQL_DATABASE=a_llm\nMYSQL_ROLE=admin\n```\n\nStart commands:\n```\n# Download dependencies\nuv sync\n\n# Start\nuv run -m mysql_mcp_server_pro.server --mode sse\n\n# Custom env file location\nuv run -m mysql_mcp_server_pro.server --mode sse --envfile /path/to/.env\n```\n\n### Local Development with STDIO Mode\n\nAdd the following content to your mcp client tools, such as cursor, cline, etc.\n\nmcp json as follows:\n```\n{\n  \"mcpServers\": {\n      \"operateMysql\": {\n        \"isActive\": true,\n        \"name\": \"operateMysql\",\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"/Volumes/mysql_mcp_server_pro/src/mysql_mcp_server_pro\",    # Replace this with your project path\n          \"run\",\n          \"-m\",\n          \"mysql_mcp_server_pro.server\",\n          \"--mode\",\n          \"stdio\"\n        ],\n        \"env\": {\n          \"MYSQL_HOST\": \"localhost\",\n          \"MYSQL_PORT\": \"3306\",\n          \"MYSQL_USER\": \"root\", \n          \"MYSQL_PASSWORD\": \"123456\",\n          \"MYSQL_DATABASE\": \"a_llm\",\n          \"MYSQL_ROLE\": \"admin\"\n       }\n    }\n  }\n} \n```\n\n## Custom Tool Extensions\n1. Add a new tool class in the handles package, inherit from BaseHandler, and implement get_tool_description and run_tool methods\n\n2. Import the new tool in __init__.py to make it available in the server\n\n## OAuth2.0 Authentication\n1. Start the authentication service. By default, it uses the built-in OAuth 2.0 password mode authentication. You can modify your own authentication service address in the env file.\n```aiignore\nuv run -m mysql_mcp_server_pro.server --oauth true\n```\n\n2. Visit the authentication service at http://localhost:3000/login. Default username and password are configured in the env file.\n   ![image](https://github.com/user-attachments/assets/ec8a629e-62f9-4b93-b3cc-442b3d2dc46f)\n\n\n3. Copy the token and add it to the request headers, for example:\n   ![image](https://github.com/user-attachments/assets/a5451e35-bddd-4e49-8aa9-a4178d30ec88)\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql_mcp_server_pro\": {\n      \"name\": \"mysql_mcp_server_pro\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"\",\n      \"isActive\": true,\n      \"url\": \"http://localhost:3000/mcp/\",\n      \"headers\": {\n        \"authorization\": \"bearer TOKEN_VALUE\"\n      }\n    }\n  }\n}\n```\n\n## Examples\n1. Create a new table and insert data, prompt format as follows:\n```\n# Task\n   Create an organizational structure table with the following structure: department name, department number, parent department, is valid.\n# Requirements\n - Table name: department\n - Common fields need indexes\n - Each field needs comments, table needs comment\n - Generate 5 real data records after creation\n```\n![image](https://github.com/user-attachments/assets/34118993-2a4c-4804-92f8-7fba9df89190)\n![image](https://github.com/user-attachments/assets/f8299f01-c321-4dbf-b5de-13ba06885cc1)\n\n\n2. Query data based on table comments, prompt as follows:\n```\nSearch for data with Department name 'Executive Office' in Department organizational structure table\n```\n![image](https://github.com/user-attachments/assets/dcf96603-548e-42d9-9217-78e569be7a8d)\n\n\n3. Analyze slow SQL, prompt as follows:\n```\nselect * from t_jcsjzx_hjkq_cd_xsz_sk xsz\nleft join t_jcsjzx_hjkq_jcd jcd on jcd.cddm = xsz.cddm \nBased on current index situation, review execution plan and provide optimization suggestions in markdown format, including table index status, execution details, and optimization recommendations\n```\n\n4. Analyze SQL deadlock issues, prompt as follows:\n```\nupdate t_admin_rms_zzjg set sfyx = '0' where xh = '1' is stuck, please analyze the cause\n```\n![image](https://github.com/user-attachments/assets/25bca1cd-854c-4591-ac6e-32d464b12066)\n\n\n5. Analyze the health status prompt as follows\n```\nCheck the current health status of MySQL\n```\n![image](https://github.com/user-attachments/assets/1f221ab8-59bf-402c-a15a-ec3eba1eea59)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "wirdes--db-mcp-tool": {
      "owner": "wirdes",
      "name": "db-mcp-tool",
      "url": "https://github.com/wirdes/db-mcp-tool",
      "imageUrl": "/freedevtools/mcp/pfp/wirdes.webp",
      "description": "Manage and interact with various databases including PostgreSQL, MySQL, and Firestore. Execute queries, list tables, and export data seamlessly to enhance database management tasks.",
      "stars": 5,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-14T17:53:23Z",
      "readme_content": "# Database Explorer MCP Tool\n\n[![smithery badge](https://smithery.ai/badge/@wirdes/db-mcp-tool)](https://smithery.ai/server/@wirdes/db-mcp-tool)\n\nA powerful Model Context Protocol (MCP) tool for exploring and managing different types of databases including PostgreSQL, MySQL, and Firestore.\n\n## Features\n\n- **Multiple Database Support**\n\n  - PostgreSQL\n  - MySQL\n  - Firestore\n\n- **Database Operations**\n  - Connect to databases\n  - List tables\n  - View triggers\n  - List functions\n  - Execute SQL queries (PostgreSQL and MySQL)\n  - Export table schemas\n  - Export table data\n\n## Integration with Cursor\n\nBefore adding the tool to Cursor, you need to build the project:\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\nTo add the tool to Cursor:\n\n1. Open Cursor settings\n2. Navigate to \"Model Context Protocol (MCP)\" section\n3. Click \"Add New Tool\"\n4. Fill in the following details:\n   ```json\n   {\n     \"name\": \"database-explorer\",\n     \"command\": \"node /path/to/project/dist/index.js\",\n     \"description\": \"Database Explorer MCP Tool\"\n   }\n   ```\n   Note: Replace `/path/to/project` with the actual path to your project directory.\n5. Save the settings\n6. Restart Cursor\n\nUsing the Tool:\n\n1. After setup, you can connect to your database using one of these commands:\n\n   - For PostgreSQL: Use `!pg` with connection details\n   - For MySQL: Use `!mysql` with connection details\n   - For Firestore: Use `!firestore` with connection details\n\n2. Once connected, you can use various database operations:\n   - `!tables` to list all tables\n   - `!triggers` to view triggers\n   - `!functions` to list functions\n   - `!query` to execute SQL queries\n   - `!export-db` to export table schemas\n   - `!export-data` to export table data\n\nSee the Commands section below for detailed usage examples.\n\n## Commands\n\n### Connection Commands\n\n- `!pg` - Connect to PostgreSQL database\n\n  ```json\n  {\n    \"connection\": {\n      \"host\": \"hostname\",\n      \"port\": 5432,\n      \"database\": \"dbname\",\n      \"user\": \"username\",\n      \"password\": \"password\"\n    }\n  }\n  ```\n\n- `!mysql` - Connect to MySQL database\n\n  ```json\n  {\n    \"connection\": {\n      \"host\": \"hostname\",\n      \"port\": 3306,\n      \"database\": \"dbname\",\n      \"user\": \"username\",\n      \"password\": \"password\"\n    }\n  }\n  ```\n\n- `!firestore` - Connect to Firestore database\n  ```json\n  {\n    \"connection\": {\n      \"projectId\": \"your-project-id\",\n      \"keyFilename\": \"path/to/keyfile.json\"\n    }\n  }\n  ```\n\n### Database Operation Commands\n\n- `!tables` - List all tables in the connected database\n- `!triggers` - List all triggers in the connected database\n- `!functions` - List all functions in the connected database\n- `!query` - Execute SQL query (PostgreSQL and MySQL only)\n  ```json\n  {\n    \"query\": \"SELECT * FROM table_name\"\n  }\n  ```\n- `!export-db` - Export table schema\n  ```json\n  {\n    \"table\": \"table_name\"\n  }\n  ```\n- `!export-data` - Export table data as INSERT statements\n  ```json\n  {\n    \"table\": \"table_name\"\n  }\n  ```\n\n## Requirements\n\n- Node.js\n- Required database drivers:\n  - `pg` for PostgreSQL\n  - `mysql2` for MySQL\n  - `@google-cloud/firestore` for Firestore\n\n## Usage\n\n1. Make sure you have the necessary database credentials\n2. Connect to your database using the appropriate connection command\n3. Use the available commands to explore and manage your database\n\n## Error Handling\n\n- The tool includes comprehensive error handling for:\n  - Connection failures\n  - Query execution errors\n  - Schema and data export issues\n  - Invalid database operations\n\n## Notes\n\n- Firestore support is limited to basic operations due to its NoSQL nature\n- SQL operations are only available for PostgreSQL and MySQL\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "db",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "wiseman--osm-mcp": {
      "owner": "wiseman",
      "name": "osm-mcp",
      "url": "https://github.com/wiseman/osm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wiseman.webp",
      "description": "Query and visualize OpenStreetMap data through a web-based interface, utilizing PostgreSQL/PostGIS for backend data management. Features include dynamic map interactions such as adding markers and polygons as well as adjustable view settings.",
      "stars": 30,
      "forks": 8,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T03:38:17Z",
      "readme_content": "# MCP-OSM: OpenStreetMap Integration for MCP\n\nThis package provides OpenStreetMap integration for MCP, allowing users to query\nand visualize map data through an MCP interface.\n\n[](osm-mcp.webp)\n\n## Features\n\n- Web-based map viewer using Leaflet and OpenStreetMap\n- Server-to-client communication via Server-Sent Events (SSE)\n- MCP tools for map control (adding markers, polygons, setting view, getting view)\n- PostgreSQL/PostGIS query interface for OpenStreetMap data\n\n## Installation\n\nThis is my `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"OSM PostgreSQL Server\": {\n      \"command\": \"/Users/wiseman/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--env-file\",\n        \".env\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"psycopg2\",\n        \"--with-editable\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"--directory\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"mcp\",\n        \"run\",\n        \"mcp.py\"\n      ]\n    }\n  }\n}\n```\n\nWhen the MCP server starts it also starts a web server at http://localhost:8889/\nthat has the map interface.\n\n### Environment Variables\n\nThe following environment variables can be used to configure the MCP:\n\n- `FLASK_HOST` - Host for the Flask server (default: 127.0.0.1)\n- `FLASK_PORT` - Port for the Flask server (default: 8889)\n- `PGHOST` - PostgreSQL host (default: localhost)\n- `PGPORT` - PostgreSQL port (default: 5432)\n- `PGDB` - PostgreSQL database name (default: osm)\n- `PGUSER` - PostgreSQL username (default: postgres)\n- `PGPASSWORD` - PostgreSQL password (default: postgres)\n\n### MCP Tools\n\nThe following MCP tools are available:\n\n- `get_map_view` - Get the current map view\n- `set_map_view` - Set the map view to specific coordinates or bounds\n- `set_map_title` - Set the title displayed at the bottom right of the map\n- `add_map_marker` - Add a marker at specific coordinates\n- `add_map_line` - Add a line defined by a set of coordinates\n- `add_map_polygon` - Add a polygon defined by a set of coordinates\n- `query_osm_postgres` - Execute a SQL query against the OpenStreetMap database",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "osm",
        "databases",
        "openstreetmap",
        "openstreetmap data",
        "visualize openstreetmap",
        "wiseman osm"
      ],
      "category": "databases"
    },
    "xexr--mcp-libsql": {
      "owner": "xexr",
      "name": "mcp-libsql",
      "url": "https://github.com/Xexr/mcp-libsql",
      "imageUrl": "",
      "description": "Production-ready MCP server for libSQL databases with comprehensive security and management tools.",
      "stars": 14,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-22T12:40:54Z",
      "readme_content": "# MCP libSQL by xexr\n\nA Model Context Protocol (MCP) server for libSQL database operations, providing secure database access through Claude Desktop, Claude Code, Cursor, and other MCP-compatible clients.\n\nRuns on Node, written in TypeScript\n\n## 🔧 **Quick Start**\n\n1. **Install:**\n   ```bash\n   pnpm install -g @xexr/mcp-libsql\n   ```\n\n2. **Test locally:**\n   ```bash\n   mcp-libsql --url file:///tmp/test.db --log-mode console\n   ```\n\n3. **Configure Claude Desktop** with your Node.js path and database URL (see configuration examples below)\n\n## 🚀 **Status**\n\n✅ **Complete database management capabilities** - All 6 core tools implemented and tested  \n✅ **Comprehensive security validation** - 67 security tests covering all injection vectors  \n✅ **Extensive test coverage** - 244 total tests (177 unit + 67 security) with 100% pass rate  \n✅ **Production deployment verified** - Successfully working with MCP clients  \n✅ **Robust error handling** - Connection retry, graceful degradation, and audit logging  \n\n## 🛠️ **Features**\n\n### **Available Tools**\n- **read-query**: Execute SELECT queries with comprehensive security validation\n- **write-query**: INSERT/UPDATE/DELETE operations with transaction support\n- **create-table**: DDL operations for table creation with security measures\n- **alter-table**: Table structure modifications (ADD/RENAME/DROP operations)\n- **list-tables**: Database metadata browsing with filtering options\n- **describe-table**: Table schema inspection with multiple output formats\n\n### **Security & Reliability**\n- **Multi-layer SQL injection prevention** with comprehensive security validation\n- **Connection pooling** with health monitoring and automatic retry logic  \n- **Transaction support** with automatic rollback on errors\n- **Comprehensive audit logging** for security compliance\n\n> 🔐 **Security details:** See [docs/SECURITY.md](docs/SECURITY.md) for comprehensive security features and testing.\n\n### **Developer Experience**\n- **Beautiful table formatting** with proper alignment and NULL handling\n- **Performance metrics** displayed for all operations\n- **Clear error messages** with actionable context\n- **Parameterized query support** for safe data handling\n- **Development mode** with enhanced logging and hot reload\n\n## 📋 **Prerequisites**\n\n- **Node.js** 20+ \n- **pnpm** (or npm) package manager\n- **libSQL database** (file-based or remote)\n- **Claude Desktop** (for MCP integration)\n\n### **Platform Requirements**\n- **macOS**: Native Node.js installation\n- **Linux**: Native Node.js installation  \n- **Windows**: Native Node.js installation or WSL2 with Node.js installation\n\n## 🔧 **Installation**\n\n```bash\n# Use your package manager of choice, e.g. npm, pnpm, bun etc\n\n# Install globally\npnpm install -g @xexr/mcp-libsql\nmcp-libsql -v # check version\n\n# ...or build from the repository\ngit clone https://github.com/Xexr/mcp-libsql.git\ncd mcp-libsql\npnpm install # Install dependencies\npnpm build # Build the project\nnode dist/index.js -v  # check version\n```\n\n## 🚀 **Usage**\n\n### **Local Testing**\n\nGlobal installation assumed below, replace \"mcp-libsql\" with \"node dist/index.js\" if using local build\n\n```bash\n# Test with file database (default: file-only logging)\nmcp-libsql --url file:///tmp/test.db\n\n# Test with HTTP database\nmcp-libsql --url http://127.0.0.1:8080\n\n# Test with Turso database (environment variable, alternatively export the env var)\nLIBSQL_AUTH_TOKEN=\"your-token\" mcp-libsql --url \"libsql://your-db.turso.io\"\n\n# Test with Turso database (CLI parameter)\nmcp-libsql --url \"libsql://your-db.turso.io\" --auth-token \"your-token\"\n\n# Development mode with console logging\nmcp-libsql --dev --log-mode console --url file:///tmp/test.db\n\n# Test with different logging modes\nmcp-libsql --url --log-mode both file:///tmp/test.db\n```\n\n### **Claude Desktop Integration**\n\nConfigure the MCP server in Claude Desktop based on your operating system:\n\n#### **macOS Configuration**\n\n1. **Create configuration file** at `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n**Global install**\n```json\n\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"mcp-libsql\",\n      \"args\": [\n        \"--url\",\n        \"file:///Users/username/database.db\"\n      ]\n    }\n  }\n}\n```\n\n**Alternative configuration for local build installation:**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/Users/username/projects/mcp-libsql/dist/index.js\",\n        \"--url\", \n        \"file:///Users/username/database.db\"\n      ],\n    }\n  }\n}\n```\n\n**Alternative configuration for global install using nvm lts for node**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"zsh\",\n      \"args\": [\n        \"-c\",\n        \"source ~/.nvm/nvm.sh && nvm use --lts > /dev/null && mcp-libsql --url file:///Users/username/database.db\",\n      ],\n    }\n  }\n}\n```\n\n**Important**: The global installation method is recommended as it handles PATH automatically.\n\n#### **Linux Configuration**\n\n1. **Create configuration file** at `~/.config/Claude/claude_desktop_config.json`:\n\n**Global install**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"mcp-libsql\",\n      \"args\": [\n        \"--url\",\n        \"file:///home/username/database.db\"\n      ]\n    }\n  }\n}\n```\n\n**Alternative configuration for local build installation:**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/home/username/projects/mcp-libsql/dist/index.js\",\n        \"--url\",\n        \"file:///home/username/database.db\"\n      ],\n    }\n  }\n}\n```\n\n#### **Windows (WSL2) Configuration**\n\n1. **Create configuration file** at `%APPDATA%\\Claude\\claude_desktop_config.json`:\n\n**Global install**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"wsl.exe\",\n      \"args\": [\n        \"-e\",\n        \"bash\",\n        \"-c\",\n        \"mcp-libsql --url file:///home/username/database.db\",\n      ]\n    }\n  }\n}\n```\n\n**Alternative configuration for local build installation:**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"wsl.exe\",\n      \"args\": [\n        \"-e\",\n        \"bash\",\n        \"-c\",\n        \"/home/username/projects/mcp-libsql/dist/index.js --url file:///home/username/database.db\",\n      ]\n    }\n  }\n}\n```\n\n**Alternative configuration for global install using nvm for node**\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"wsl.exe\",\n      \"args\": [\n        \"-e\",\n        \"bash\",\n        \"-c\",\n        \"source ~/.nvm/nvm.sh && mcp-libsql --url file:///home/username/database.db\",\n      ]\n    }\n  }\n}\n```\n\n**Important**: Use `wsl.exe -e` (not just `wsl.exe`) to ensure proper command handling and avoid issues with server command reception on Windows.\n\n### **Database Authentication**\n\nFor Turso (and other credentialed) databases, you'll need an authentication token. There are two secure ways to provide it:\n\n_Global installation shown below, adjust accordingly for your setup_\n\n#### **Method 1: Environment Variable (Recommended)**\n\n**Configure Claude Desktop with environment variable** (macOS/Linux example):\n```bash\nexport LIBSQL_AUTH_TOKEN=\"your-turso-auth-token-here\"\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"mcp-libsql\",\n      \"args\": [\n        \"--url\",\n        \"libsql://your-database.turso.io\"\n      ]\n    }\n  }\n}\n```\n\n#### **Method 2: CLI Parameter**\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-libsql\": {\n      \"command\": \"mcp-libsql\",\n      \"args\": [\n        \"--url\",\n        \"libsql://your-database.turso.io\",\n        \"--auth-token\",\n        \"your-turso-auth-token-here\"\n      ]\n    }\n  }\n}\n```\n\n#### **Getting Your Turso Auth Token**\n\n1. **Install Turso CLI:**\n   ```bash\n   curl -sSfL https://get.tur.so/install.sh | bash\n   ```\n\n2. **Login to Turso:**\n   ```bash\n   turso auth login\n   ```\n\n3. **Create an auth token:**\n   ```bash\n   turso auth token create --name \"mcp-libsql\"\n   ```\n\n4. **Get your database URL:**\n   ```bash\n   turso db show your-database-name --url\n   ```\n\n#### **Security Best Practices**\n\n- **Environment variables are safer** than CLI parameters (tokens won't appear in process lists)\n- **MCP config files may contain tokens** - ensure they're not committed to version control\n- **Consider using external secret management** for production environments\n- **Use scoped tokens** with minimal required permissions\n- **Rotate tokens regularly** for enhanced security\n- **Monitor token usage** through Turso dashboard\n\n#### **Example: Complete Turso Setup**\n\n1. **Create and configure database:**\n   ```bash\n   # Create database\n   turso db create my-app-db\n   \n   # Get database URL\n   turso db show my-app-db --url\n   # Output: libsql://my-app-db-username.turso.io\n   \n   # Create auth token\n   turso auth token create --name \"mcp-libsql-token\"\n   # Output: your-long-auth-token-string\n   ```\n\n2. **Configure Claude Desktop:**\n    ```bash\n    export LIBSQL_AUTH_TOKEN=\"your-turso-auth-token-here\"\n    ```\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"mcp-libsql\": {\n          \"command\": \"mcp-libsql\",\n          \"args\": [\n            \"--url\",\n            \"libsql://my-app-db-username.turso.io\"\n          ]\n        }\n      }\n    }\n    ```\n\n3. **Test the connection:**\n   ```bash\n   # Test locally first\n   mcp-libsql --url \"libsql://my-app-db-username.turso.io\" --log-mode console\n   ```\n\n#### **Configuration Notes**\n\n- **File paths**: Use absolute paths to avoid path resolution issues\n- **Database URLs**: \n  - File databases: `file:///absolute/path/to/database.db`\n  - HTTP databases: `http://hostname:port`\n  - libSQL/Turso: `libsql://your-database.turso.io`\n- **Node.js path**: Use `which node` to find your Node.js installation path\n- **Working directory**: Set `cwd` to ensure relative paths work correctly\n- **Authentication**: For Turso databases, use environment variables for secure token handling\n- **Logging modes**: \n  - Default `file` mode prevents JSON parsing errors in MCP protocol\n  - Use `--log-mode console` for development debugging\n  - Use `--log-mode both` for comprehensive logging\n  - Use `--log-mode none` to disable all logging\n\n2. **Restart Claude Desktop** completely after updating the configuration\n\n3. **Test the integration** by asking Claude to run SQL queries:\n   ```\n   Can you run this SQL query: SELECT 1 as test\n   ```\n\n\n\n## 📋 **Available Tools**\n\n- **read-query** - Execute SELECT queries with security validation\n- **write-query** - INSERT/UPDATE/DELETE with transaction support  \n- **create-table** - CREATE TABLE with DDL security\n- **alter-table** - Modify table structure (ADD/RENAME/DROP)\n- **list-tables** - Browse database metadata and objects\n- **describe-table** - Inspect table schema and structure\n\n> 📖 **Detailed API documentation:** See [docs/API.md](docs/API.md) for complete input/output examples and parameters.\n\n## 🧪 **Testing**\n\n```bash\n# Run all tests\npnpm test\n\n# Run tests in watch mode\npnpm test:watch\n\n# Run tests with coverage\npnpm test:coverage\n\n# Run specific test file\npnpm test security-verification\n\n# Lint code\npnpm lint\n\n# Fix linting issues\npnpm lint:fix\n\n# Type check\npnpm typecheck\n```\n\n**Test Coverage**: 403 tests covering all functionality including edge cases, error scenarios, CLI arguments, authentication, and comprehensive security validation.\n\n## ⚠️ **Common Issues**\n\n### **1. Build Failures**\n```bash\n# Clean and rebuild\nrm -rf dist node_modules\npnpm install && pnpm build\n```\n\n### **2. Node.js Version Issues (macOS)**\n```\nSyntaxError: Unexpected token '??='\n```\n**Problem**: Claude Desktop may default to using an older Node.js version on your system which doesn't support the required feature set.\n\n**Solution**: Use global installation and nvm node selection method shown above.\n\n### **3. Server Won't Start**\n- For global installation: `pnpm install -g @xexr/mcp-libsql`\n- For local installation: Ensure `pnpm build` was run and `dist/index.js` exists\n- Test locally: `mcp-libsql --url file:///tmp/test.db`\n- Restart Claude Desktop after config changes\n\n### **4. Tools Not Available**\n- Verify database URL is accessible\n- Check Claude Desktop logs for connection errors\n- Test with simple file database: `file:///tmp/test.db`\n\n### **5. JSON Parsing Errors (Resolved)**\n```\nExpected ',' or ']' after array element in JSON\n```\n**Resolved**: This issue is caused by stdout console logging. The `--log-mode` option now defaults to `file` mode which prevents this issue. If you see these errors, ensure you're using the default `--log-mode file` or not specifying `--log-mode` at all. Note, the error is harmless, and the tool will still work with it if you wish to have console logging.\n\n### **6. Database Connection Issues**\n```bash\n# Test database connectivity\nsqlite3 /tmp/test.db \"SELECT 1\"\n\n# Fix permissions\nchmod 644 /path/to/database.db\n```\n\n> 🔧 **Full troubleshooting guide:** See [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for detailed solutions to all issues.\n\n## 🏗️ **Architecture**\n\nBuilt with TypeScript and modern Node.js patterns:\n- **Connection pooling** with health monitoring and retry logic\n- **Tool-based architecture** with consistent validation and error handling\n- **Security-first design** with multi-layer input validation\n- **Comprehensive testing** with 244 tests covering all scenarios\n\n## 🤝 **Contributing**\n\n1. Follow TypeScript strict mode and existing code patterns\n2. Write tests for new features  \n3. Maintain security measures\n4. Update documentation\n\n**Development:** `pnpm dev` • **Build:** `pnpm build` • **Test:** `pnpm test`\n\n## 📄 **License**\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## 🔗 **Links**\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [libSQL Documentation](https://docs.libsql.org/)\n- [Claude Desktop](https://claude.ai/desktop)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "libsql",
        "databases",
        "database",
        "libsql databases",
        "mcp libsql",
        "secure database"
      ],
      "category": "databases"
    },
    "xiangma9712--mysql-mcp-server": {
      "owner": "xiangma9712",
      "name": "mysql-mcp-server",
      "url": "https://github.com/xiangma9712/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/xiangma9712.webp",
      "description": "Manage MySQL databases with functionality for executing read-only queries and safely testing write queries that are rolled back after execution.",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-19T07:48:29Z",
      "readme_content": "# MySQL MCP Server\n\nAn MCP server for interacting with MySQL databases.\n\nThis server supports executing read-only queries (query) and write queries that are ultimately rolled back (test_execute).\n\n<a href=\"https://glama.ai/mcp/servers/kucglstegf\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kucglstegf/badge\" alt=\"MySQL Server MCP server\" />\n</a>\n\n## Setup\n\n### Environment Variables\n\nAdd the following environment variables to `~/.mcp/.env`:\n\n```\nMYSQL_HOST=host.docker.internal  # Hostname to access host services from Docker container\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=your_password\n```\n\n> **Note**: `host.docker.internal` is a special DNS name for accessing host machine services from Docker containers.\n> Use this setting when connecting to a MySQL server running on your host machine.\n> If connecting to a different MySQL server, change to the appropriate hostname.\n\n### mcp.json Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--add-host=host.docker.internal:host-gateway\",\n        \"--env-file\",\n        \"/Users/username/.mcp/.env\",\n        \"ghcr.io/xiangma9712/mcp/mysql\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\n### Starting the Server\n\n```sh\ndocker run -i --rm --add-host=host.docker.internal:host-gateway --env-file ~/.mcp/.env ghcr.io/xiangma9712/mcp/mysql\n```\n\n> **Note**: If you're using OrbStack, `host.docker.internal` is automatically supported, so the `--add-host` option can be omitted.\n> While Docker Desktop also typically supports this automatically, adding the `--add-host` option is recommended for better reliability.\n\n### Available Commands\n\n#### 1. Execute Read-only Query\n\n```json\n{\n  \"type\": \"query\",\n  \"payload\": {\n    \"sql\": \"SELECT * FROM your_table\"\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"id\": 1,\n      \"name\": \"example\"\n    }\n  ]\n}\n```\n\n#### 2. Test Query Execution\n\n```json\n{\n  \"type\": \"test_execute\",\n  \"payload\": {\n    \"sql\": \"UPDATE your_table SET name = 'updated' WHERE id = 1\"\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"success\": true,\n  \"data\": \"The UPDATE SQL query can be executed.\"\n}\n```\n\n#### 3. List Tables\n\n```json\n{\n  \"type\": \"list_tables\"\n}\n```\n\nResponse:\n```json\n{\n  \"success\": true,\n  \"data\": [\"table1\", \"table2\", \"table3\"]\n}\n```\n\n#### 4. Describe Table\n\n```json\n{\n  \"type\": \"describe_table\",\n  \"payload\": {\n    \"table\": \"your_table\"\n  }\n}\n```\n\nResponse:\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"Field\": \"id\",\n      \"Type\": \"int(11)\",\n      \"Null\": \"NO\",\n      \"Key\": \"PRI\",\n      \"Default\": null,\n      \"Extra\": \"\"\n    },\n    {\n      \"Field\": \"name\",\n      \"Type\": \"varchar(255)\",\n      \"Null\": \"YES\",\n      \"Key\": \"\",\n      \"Default\": null,\n      \"Extra\": \"\"\n    }\n  ]\n}\n```\n\n## Implementation Details\n\n- Implemented in TypeScript\n- Uses mysql2 package\n- Runs as a Docker container\n- Accepts JSON commands through standard input\n- Returns JSON responses through standard output\n- Uses `host.docker.internal` to connect to host MySQL (compatible with both OrbStack and Docker Desktop)\n\n## Security Considerations\n\n- Uses environment variables for sensitive information management\n- SQL injection prevention is the implementer's responsibility\n- Proper network configuration required for production use\n- Appropriate firewall settings needed when connecting to host machine services",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "xptotech--mcp2": {
      "owner": "xptotech",
      "name": "mcp2",
      "url": "https://github.com/xptotech/mcp2",
      "imageUrl": "/freedevtools/mcp/pfp/xptotech.webp",
      "description": "Connect and interact with MySQL databases to execute SQL queries and manage database connections. Analyze data through a structured interface tailored for AI model integration.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-09T19:30:35Z",
      "readme_content": "\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "database access",
        "databases secure"
      ],
      "category": "databases"
    },
    "xytangme--neodb-mcp": {
      "owner": "xytangme",
      "name": "neodb-mcp",
      "url": "https://github.com/xytangme/neodb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/xytangme.webp",
      "description": "Interact with a social book cataloging service to fetch user information, search for books, and retrieve detailed book information through its API.",
      "stars": 1,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:33Z",
      "readme_content": "# NeoDB MCP Server\n\nA Message Control Protocol (MCP) server implementation for interacting with [NeoDB](https://neodb.social/), a social book cataloging service. This server provides tools to fetch user information, search books, and retrieve detailed book information through NeoDB's API.\n\n<a href=\"https://glama.ai/mcp/servers/1any3eeaza\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/1any3eeaza/badge\" alt=\"NeoDB Server MCP server\" /></a>\n\n## Setup\n\n### Install UV\nFirst, install UV package installer:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### Create Virtual Environment\nCreate and activate a Python virtual environment using UV:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate     # On Windows\n```\n\n### Install Dependencies\nInstall project dependencies using UV:\n\n```bash\nuv pip install .\n```\n\n## Available Tools\n\nThe server provides the following tools:\n\n1. **get-user-info**\n   - Gets current user's basic information\n   - No parameters required\n\n2. **search-books**\n   - Searches items in the catalog\n   - Parameters:\n     - `query` (string): Search query for books\n\n3. **get-book**\n   - Gets detailed information about a specific book\n   - Parameters:\n     - `book_id` (string): The ID of the book to retrieve\n\n## Usage with Claude Desktop\n\n### Get Access Token\n\nThere are two ways to get your access token:\n\n1. Using the official guide: Follow the [official documentation](https://neodb.net/api/) to obtain your access token.\n\n2. Using automated script: You can use the [neodb-get-access-token](https://github.com/xytangme/neodb-get-access-token) script which provides a simplified way to get your access token.\n\n### Update Config `claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"neodb\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<PATH_TO_PROJECT_DIR>\",\n        \"run\",\n        \"<PATH_TO_SCRIPT>\",\n        \"<API_BASE> e.g. https://neodb.social\",\n        \"<ACCESS_TOKEN>\"\n      ]\n    }\n  }\n}\n```\n\nWhere:\n- `<API_BASE>`: The base URL for the NeoDB API\n- `<ACCESS_TOKEN>`: Your NeoDB API access token\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neodb",
        "databases",
        "database",
        "xytangme neodb",
        "databases secure",
        "secure database"
      ],
      "category": "databases"
    },
    "yannbrrd--simple_snowflake_mcp": {
      "owner": "yannbrrd",
      "name": "simple_snowflake_mcp",
      "url": "https://github.com/YannBrrd/simple_snowflake_mcp",
      "imageUrl": "",
      "description": "Simple Snowflake MCP server that works behind a corporate proxy. Read and write (optional) operations",
      "stars": 6,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T12:58:30Z",
      "readme_content": "# Simple Snowflake MCP server\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/YannBrrd/simple_snowflake_mcp)](https://archestra.ai/mcp-catalog/yannbrrd__simple_snowflake_mcp)\n\n**Enhanced Snowflake MCP Server with comprehensive configuration system and full MCP protocol compliance.**\n\nA production-ready MCP server that provides seamless Snowflake integration with advanced features including configurable logging, resource subscriptions, and comprehensive error handling. Designed to work seamlessly behind corporate proxies.\n\n### Tools\n\nThe server exposes comprehensive MCP tools to interact with Snowflake:\n\n**Core Database Operations:**\n- **execute-snowflake-sql**: Executes a SQL query on Snowflake and returns the result (list of dictionaries)\n- **execute-query**: Executes a SQL query in read-only mode (SELECT, SHOW, DESCRIBE, EXPLAIN, WITH) or not (if `read_only` is false), result in markdown format\n- **query-view**: Queries a view with an optional row limit (markdown result)\n\n**Discovery and Metadata:**\n- **list-snowflake-warehouses**: Lists available Data Warehouses (DWH) on Snowflake\n- **list-databases**: Lists all accessible Snowflake databases\n- **list-schemas**: Lists all schemas in a specified database\n- **list-tables**: Lists all tables in a database and schema\n- **list-views**: Lists all views in a database and schema\n- **describe-table**: Gives details of a table (columns, types, constraints)\n- **describe-view**: Gives details of a view (columns, SQL)\n\n**Advanced Operations:**\n- **get-table-sample**: Gets sample data from a table\n- **explain-query**: Explains the execution plan of a SQL query\n- **show-query-history**: Shows recent query history\n- **get-warehouse-status**: Gets current warehouse status and usage\n- **validate-sql**: Validates SQL syntax without execution\n\n## 🆕 Configuration System (v0.2.0)\n\nThe server now includes a comprehensive YAML-based configuration system that allows you to customize all aspects of the server behavior.\n\n### Configuration File Structure\n\nCreate a `config.yaml` file in your project root:\n\n```yaml\n# Logging Configuration\nlogging:\n  level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file_logging: false  # Set to true to enable file logging\n  log_file: \"mcp_server.log\"  # Log file path (when file_logging is true)\n\n# Server Configuration\nserver:\n  name: \"simple_snowflake_mcp\"\n  version: \"0.2.0\"\n  description: \"Enhanced Snowflake MCP Server with full protocol compliance\"\n  connection_timeout: 30\n  read_only: true  # Set to false to allow write operations\n\n# Snowflake Configuration\nsnowflake:\n  read_only: true\n  default_query_limit: 1000\n  max_query_limit: 50000\n\n# MCP Protocol Settings\nmcp:\n  experimental_features:\n    resource_subscriptions: true  # Enable resource change notifications\n    completion_support: false    # Set to true when MCP version supports it\n  \n  notifications:\n    resources_changed: true\n    tools_changed: true\n  \n  limits:\n    max_prompt_length: 10000\n    max_resource_size: 1048576  # 1MB\n```\n\n### Using Custom Configuration\n\nYou can specify a custom configuration file using the `CONFIG_FILE` environment variable:\n\n**Windows:**\n```cmd\nset CONFIG_FILE=config_debug.yaml\npython -m simple_snowflake_mcp\n```\n\n**Linux/macOS:**\n```bash\nCONFIG_FILE=config_production.yaml python -m simple_snowflake_mcp\n```\n\n### Configuration Override Priority\n\nConfiguration values are resolved in this order (highest to lowest priority):\n1. Environment variables (e.g., `LOG_LEVEL`, `MCP_READ_ONLY`)\n2. Custom configuration file (via `CONFIG_FILE`)\n3. Default `config.yaml` file\n4. Built-in defaults\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n\n\n  ```\n  \"mcpServers\": {\n    \"simple_snowflake_mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \".\", // Use current directory for GitHub\n        \"run\",\n        \"simple_snowflake_mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n\n  ```\n  \"mcpServers\": {\n    \"simple_snowflake_mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"simple_snowflake_mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Docker Setup\n\n### Prerequisites\n\n- Docker and Docker Compose installed on your system\n- Your Snowflake credentials\n\n### Quick Start with Docker\n\n1. **Clone the repository**\n   ```bash\n   git clone <your-repo>\n   cd simple_snowflake_mcp\n   ```\n\n2. **Set up environment variables**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your Snowflake credentials\n   ```\n\n3. **Build and run with Docker Compose**\n   ```bash\n   # Build the Docker image\n   docker-compose build\n   \n   # Start the service\n   docker-compose up -d\n   \n   # View logs\n   docker-compose logs -f\n   ```\n\n### Docker Commands\n\nUsing Docker Compose directly:\n```bash\n# Build the image\ndocker-compose build\n\n# Start in production mode\ndocker-compose up -d\n\n# Start in development mode (with volume mounts for live code changes)\ndocker-compose --profile dev up simple-snowflake-mcp-dev -d\n\n# View logs\ndocker-compose logs -f\n\n# Stop the service\ndocker-compose down\n\n# Clean up (remove containers, images, and volumes)\ndocker-compose down --rmi all --volumes --remove-orphans\n```\n\nUsing the provided Makefile (Windows users can use `make` with WSL or install make for Windows):\n```bash\n# See all available commands\nmake help\n\n# Build and start\nmake build\nmake up\n\n# Development mode\nmake dev-up\n\n# View logs\nmake logs\n\n# Clean up\nmake clean\n```\n\n### Docker Configuration\n\nThe Docker setup includes:\n\n- **Dockerfile**: Multi-stage build with Python 3.11 slim base image\n- **docker-compose.yml**: Service definition with environment variable support\n- **.dockerignore**: Optimized build context\n- **Makefile**: Convenient commands for Docker operations\n\n#### Environment Variables\n\nAll Snowflake configuration can be set via environment variables:\n\n**Required:**\n- `SNOWFLAKE_USER`: Your Snowflake username\n- `SNOWFLAKE_PASSWORD`: Your Snowflake password\n- `SNOWFLAKE_ACCOUNT`: Your Snowflake account identifier\n\n**Optional:**\n- `SNOWFLAKE_WAREHOUSE`: Warehouse name\n- `SNOWFLAKE_DATABASE`: Default database\n- `SNOWFLAKE_SCHEMA`: Default schema\n- `MCP_READ_ONLY`: Set to \"TRUE\" for read-only mode (default: TRUE)\n\n**Configuration System (v0.2.0):**\n- `CONFIG_FILE`: Path to custom configuration file (default: config.yaml)\n- `LOG_LEVEL`: Override logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n\n#### Development Mode\n\nFor development, use the development profile which mounts your source code:\n\n```bash\ndocker-compose --profile dev up simple-snowflake-mcp-dev -d\n```\n\nThis allows you to make changes to the code without rebuilding the Docker image.\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory . run simple-snowflake-mcp\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n\n## New Feature: Snowflake SQL Execution\n\nThe server exposes an MCP tool `execute-snowflake-sql` to execute a SQL query on Snowflake and return the result.\n\n### Usage\n\nCall the MCP tool `execute-snowflake-sql` with a `sql` argument containing the SQL query to execute. The result will be returned as a list of dictionaries (one per row).\n\nExample:\n```json\n{\n  \"name\": \"execute-snowflake-sql\",\n  \"arguments\": { \"sql\": \"SELECT CURRENT_TIMESTAMP;\" }\n}\n```\n\nThe result will be returned in the MCP response.\n\n## Installation and configuration in VS Code\n\n1. **Clone the project and install dependencies**\n   ```sh\n   git clone <your-repo>\n   cd simple_snowflake_mcp\n   python -m venv .venv\n   .venv/Scripts/activate  # Windows\n   pip install -r requirements.txt  # or `uv sync --dev --all-extras` if available\n   ```\n\n2. **Configure Snowflake access**\n   - Copy `.env.example` to `.env` (or create `.env` at the root) and fill in your credentials:\n     ```env\n     SNOWFLAKE_USER=...\n     SNOWFLAKE_PASSWORD=...\n     SNOWFLAKE_ACCOUNT=...\n     # SNOWFLAKE_WAREHOUSE   Optional: Snowflake warehouse name\n     # SNOWFLAKE_DATABASE    Optional: default database name\n     # SNOWFLAKE_SCHEMA      Optional: default schema name\n     # MCP_READ_ONLY=true|false   Optional: true/false to force read-only mode\n     ```\n\n3. **Configure the server (v0.2.0)**\n   - The server will automatically create a default `config.yaml` file on first run\n   - Customize logging, limits, and MCP features by editing `config.yaml`\n   - Use `CONFIG_FILE=custom_config.yaml` to specify a different configuration file\n\n4. **Configure VS Code for MCP debugging**\n   - The `.vscode/mcp.json` file is already present:\n     ```json\n     {\n       \"servers\": {\n         \"simple-snowflake-mcp\": {\n           \"type\": \"stdio\",\n           \"command\": \".venv/Scripts/python.exe\",\n           \"args\": [\"-m\", \"simple_snowflake_mcp\"]\n         }\n       }\n     }\n     ```\n   - Open the command palette (Ctrl+Shift+P), type `MCP: Start Server` and select `simple-snowflake-mcp`.\n\n5. **Usage**\n   - The exposed MCP tools allow you to query Snowflake (list-databases, list-views, describe-view, query-view, execute-query, etc.).\n   - For more examples, see the MCP protocol documentation: https://github.com/modelcontextprotocol/create-python-server\n\n## Enhanced MCP Features (v0.2.0)\n\n### Advanced MCP Protocol Support\n\nThis server now implements comprehensive MCP protocol features:\n\n**🔔 Resource Subscriptions**\n- Real-time notifications when Snowflake resources change\n- Automatic updates for database schema changes\n- Tool availability notifications\n\n**📋 Enhanced Resource Management**\n- Dynamic resource discovery and listing\n- Detailed resource metadata and descriptions  \n- Support for resource templates and prompts\n\n**⚡ Performance & Reliability**\n- Configurable query limits and timeouts\n- Comprehensive error handling with detailed error codes\n- Connection pooling and retry mechanisms\n\n**🔧 Development Features**\n- Multiple output formats (JSON, Markdown, CSV)\n- SQL syntax validation without execution\n- Query execution plan analysis\n- Comprehensive logging with configurable levels\n\n### MCP Capabilities Advertised\n\nThe server advertises these MCP capabilities:\n- ✅ **Tools**: Full tool execution with comprehensive schemas\n- ✅ **Resources**: Dynamic resource discovery and subscriptions  \n- ✅ **Prompts**: Enhanced prompts with resource integration\n- ✅ **Notifications**: Real-time change notifications\n- 🚧 **Completion**: Ready for future MCP versions (configurable)\n\n## Supported MCP Functions\n\nThe server exposes comprehensive MCP tools to interact with Snowflake:\n\n**Core Database Operations:**\n- **execute-snowflake-sql**: Executes a SQL query and returns structured results\n- **execute-query**: Advanced query execution with multiple output formats\n- **query-view**: Optimized view querying with result limiting\n- **validate-sql**: SQL syntax validation without execution\n\n**Discovery and Metadata:**\n- **list-snowflake-warehouses**: Lists available Data Warehouses with status\n- **list-databases**: Lists all accessible databases with metadata  \n- **list-schemas**: Lists all schemas in a specified database\n- **list-tables**: Lists all tables with column information\n- **list-views**: Lists all views with definitions\n- **describe-table**: Detailed table schema and constraints\n- **describe-view**: View definition and column details\n\n**Advanced Analytics:**\n- **get-table-sample**: Sample data extraction with configurable limits\n- **explain-query**: Query execution plan analysis\n- **show-query-history**: Recent query history with performance metrics\n- **get-warehouse-status**: Real-time warehouse status and usage\n- **get-account-usage**: Account-level usage statistics\n\nFor detailed usage examples and parameter schemas, see the MCP protocol documentation.\n\n## 🚀 Getting Started Examples\n\n### Basic Usage\n```python\n# Execute a simple query\n{\n  \"name\": \"execute-query\",\n  \"arguments\": {\n    \"query\": \"SELECT CURRENT_TIMESTAMP;\",\n    \"format\": \"markdown\"\n  }\n}\n\n# List all databases\n{\n  \"name\": \"list-databases\",\n  \"arguments\": {}\n}\n```\n\n### Advanced Configuration\n```yaml\n# config_production.yaml\nlogging:\n  level: WARNING\n  file_logging: true\n  log_file: \"/var/log/mcp_server.log\"\n\nserver:\n  read_only: false  # Allow write operations\n  \nsnowflake:\n  default_query_limit: 5000\n  max_query_limit: 100000\n\nmcp:\n  experimental_features:\n    resource_subscriptions: true\n```\n\n### Debugging and Troubleshooting\n\n**Enable Debug Logging:**\n```bash\n# Method 1: Environment variable\nexport LOG_LEVEL=DEBUG\npython -m simple_snowflake_mcp\n\n# Method 2: Custom config file\nexport CONFIG_FILE=config_debug.yaml\npython -m simple_snowflake_mcp\n```\n\n**Common Issues:**\n- **Connection errors**: Check your Snowflake credentials and network connectivity\n- **Permission errors**: Ensure your user has appropriate Snowflake privileges\n- **Query limits**: Adjust `default_query_limit` in config.yaml for large result sets\n- **MCP compatibility**: Update to latest MCP client version for full feature support\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "security",
        "databases secure",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "yaoxiaolinglong--mcp-mongodb-mysql-server": {
      "owner": "yaoxiaolinglong",
      "name": "mcp-mongodb-mysql-server",
      "url": "https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server",
      "imageUrl": "/freedevtools/mcp/pfp/yaoxiaolinglong.webp",
      "description": "Enables interaction with MySQL and MongoDB databases via a standardized interface, streamlining database operations with secure connections and robust error handling. Supports both SQL and NoSQL functionalities for seamless application development.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-06-16T15:49:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yaoxiaolinglong-mcp-mongodb-mysql-server-badge.png)](https://mseep.ai/app/yaoxiaolinglong-mcp-mongodb-mysql-server)\n<a href=\"https://glama.ai/mcp/servers/@yaoxiaolinglong/mcp-mongodb-mysql-server\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@yaoxiaolinglong/mcp-mongodb-mysql-server/badge\" />\n</a>\n\n# MCP-MongoDB-MySQL-Server\n\n[![GitHub stars](https://img.shields.io/github/stars/yaoxiaolinglong/mcp-mongodb-mysql-server?style=social)](https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/yaoxiaolinglong/mcp-mongodb-mysql-server?style=social)](https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server/network/members)\n[![GitHub license](https://img.shields.io/github/license/yaoxiaolinglong/mcp-mongodb-mysql-server)](https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server/blob/main/LICENSE)\n[![smithery badge](https://smithery.ai/badge/@yaoxiaolinglong/mcp-mongodb-mysql-server)](https://smithery.ai/server/@yaoxiaolinglong/mcp-mongodb-mysql-server)\n\n> 这是一个基于 [enemyrr/mcp-mysql-server](https://github.com/enemyrr/mcp-mysql-server) 项目的二次开发版本，添加了MongoDB支持。\n> \n> This is a fork of [enemyrr/mcp-mysql-server](https://github.com/enemyrr/mcp-mysql-server) with added MongoDB support.\n\n## 项目简介 | Introduction\n\n这是一个Model Context Protocol服务器，提供MySQL和MongoDB数据库操作功能。该服务器使AI模型能够通过标准化接口与MySQL和MongoDB数据库交互。\n\nA Model Context Protocol server that provides MySQL and MongoDB database operations. This server enables AI models to interact with MySQL and MongoDB databases through a standardized interface.\n\n## 二次开发说明 | About This Fork\n\n**作者 | Author**: yaoxiaolinglong\n\n**二次开发原因 | Reason for Fork**: 原项目只支持MySQL数据库，但在实际应用中经常需要使用MongoDB。由于找不到现成的MongoDB MCP工具，因此在原项目基础上添加了MongoDB支持，使其成为一个同时支持MySQL和MongoDB的数据库服务器。\n\nThe original project only supports MySQL database, but MongoDB is often needed in practical applications. Due to the lack of ready-made MongoDB MCP tools, MongoDB support was added to the original project, making it a database server that supports both MySQL and MongoDB.\n\n## 安装与设置 | Installation & Setup for Cursor IDE\n\n### 通过Smithery安装 | Installing via Smithery\n\n通过[Smithery](https://smithery.ai/server/@yaoxiaolinglong/mcp-mongodb-mysql-server)为Claude Desktop自动安装MySQL/MongoDB数据库服务器：\n\nTo install MySQL/MongoDB Database Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yaoxiaolinglong/mcp-mongodb-mysql-server):\n\n```bash\nnpx -y @smithery/cli install @yaoxiaolinglong/mcp-mongodb-mysql-server --client claude\n```\n\n### 手动安装 | Installing Manually\n\n1. 克隆并构建项目 | Clone and build the project:\n```bash\ngit clone https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server.git\ncd mcp-mongodb-mysql-server\nnpm install\nnpm run build\n```\n\n2. 在Cursor IDE设置中添加服务器 | Add the server in Cursor IDE settings:\n   - 打开命令面板(Cmd/Ctrl + Shift + P) | Open Command Palette (Cmd/Ctrl + Shift + P)\n   - 搜索\"MCP: Add Server\" | Search for \"MCP: Add Server\"\n   - 填写以下字段 | Fill in the fields:\n     - 名称 | Name: `mysql-mongodb`\n     - 类型 | Type: `command`\n     - 命令 | Command: `node /absolute/path/to/mcp-mongodb-mysql-server/build/index.js`\n\n> **注意 | Note**: 将`/absolute/path/to/`替换为您克隆并构建项目的实际路径。\n> \n> Replace `/absolute/path/to/` with the actual path where you cloned and built the project.\n\n## 数据库配置 | Database Configuration\n\n### MySQL配置 | MySQL Configuration\n\n您可以通过以下三种方式配置MySQL数据库连接：\n\nYou can configure the MySQL database connection in three ways:\n\n1. **.env文件中的数据库URL（推荐）| Database URL in .env (Recommended)**:\n```env\nDATABASE_URL=mysql://user:password@host:3306/database\n```\n\n2. **.env文件中的单独参数 | Individual Parameters in .env**:\n```env\nDB_HOST=localhost\nDB_USER=your_user\nDB_PASSWORD=your_password\nDB_DATABASE=your_database\n```\n\n3. **通过工具直接连接 | Direct Connection via Tool**:\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"connect_db\",\n  arguments: {\n    url: \"mysql://user:password@host:3306/database\"\n    // 或者 | OR\n    workspace: \"/path/to/your/project\" // 将使用项目的.env文件 | Will use project's .env\n    // 或者 | OR\n    host: \"localhost\",\n    user: \"your_user\",\n    password: \"your_password\",\n    database: \"your_database\"\n  }\n});\n```\n\n### MongoDB配置 | MongoDB Configuration\n\n您可以通过以下三种方式配置MongoDB数据库连接：\n\nYou can configure the MongoDB database connection in three ways:\n\n1. **.env文件中的MongoDB URL（推荐）| MongoDB URL in .env (Recommended)**:\n```env\nMONGODB_URI=mongodb://user:password@host:27017/database\nMONGODB_DATABASE=your_database\n```\n\n2. **通过工具直接连接 | Direct Connection via Tool**:\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"connect_mongodb\",\n  arguments: {\n    url: \"mongodb://user:password@host:27017/database\"\n    // 或者 | OR\n    workspace: \"/path/to/your/project\" // 将使用项目的.env文件 | Will use project's .env\n    // 或者 | OR\n    database: \"your_database\" // 将使用默认连接URI | Will use default connection URI\n  }\n});\n```\n\n## 可用工具 | Available Tools\n\n### MySQL工具 | MySQL Tools\n\n#### 1. connect_db\n连接到MySQL数据库，使用URL、工作区路径或直接凭据。\n\nConnect to MySQL database using URL, workspace path, or direct credentials.\n\n#### 2. query\n执行SELECT查询，支持可选的预处理语句参数。\n\nExecute SELECT queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"query\",\n  arguments: {\n    sql: \"SELECT * FROM users WHERE id = ?\",\n    params: [1]\n  }\n});\n```\n\n#### 3. execute\n执行INSERT、UPDATE或DELETE查询，支持可选的预处理语句参数。\n\nExecute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"execute\",\n  arguments: {\n    sql: \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    params: [\"John Doe\", \"john@example.com\"]\n  }\n});\n```\n\n#### 4. list_tables\n列出连接的数据库中的所有表。\n\nList all tables in the connected database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"list_tables\"\n});\n```\n\n#### 5. describe_table\n获取特定表的结构。\n\nGet the structure of a specific table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"describe_table\",\n  arguments: {\n    table: \"users\"\n  }\n});\n```\n\n#### 6. create_table\n创建一个新表，指定字段和索引。\n\nCreate a new table with specified fields and indexes.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"create_table\",\n  arguments: {\n    table: \"users\",\n    fields: [\n      {\n        name: \"id\",\n        type: \"int\",\n        autoIncrement: true,\n        primary: true\n      },\n      {\n        name: \"email\",\n        type: \"varchar\",\n        length: 255,\n        nullable: false\n      }\n    ],\n    indexes: [\n      {\n        name: \"email_idx\",\n        columns: [\"email\"],\n        unique: true\n      }\n    ]\n  }\n});\n```\n\n#### 7. add_column\n向现有表添加新列。\n\nAdd a new column to an existing table.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"add_column\",\n  arguments: {\n    table: \"users\",\n    field: {\n      name: \"phone\",\n      type: \"varchar\",\n      length: 20,\n      nullable: true\n    }\n  }\n});\n```\n\n### MongoDB工具 | MongoDB Tools\n\n#### 1. connect_mongodb\n连接到MongoDB数据库，使用URL、工作区路径或数据库名称。\n\nConnect to MongoDB database using URL, workspace path, or database name.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"connect_mongodb\",\n  arguments: {\n    url: \"mongodb://user:password@host:27017/database\"\n  }\n});\n```\n\n#### 2. mongodb_list_collections\n列出连接的MongoDB数据库中的所有集合。\n\nList all collections in the connected MongoDB database.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_list_collections\"\n});\n```\n\n#### 3. mongodb_find\n在MongoDB集合中查找文档，支持可选的过滤器、限制、跳过和排序。\n\nFind documents in a MongoDB collection with optional filter, limit, skip, and sort.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_find\",\n  arguments: {\n    collection: \"users\",\n    filter: { age: { $gt: 18 } },\n    limit: 10,\n    skip: 0,\n    sort: { name: 1 }\n  }\n});\n```\n\n#### 4. mongodb_insert\n向MongoDB集合中插入文档。\n\nInsert documents into a MongoDB collection.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_insert\",\n  arguments: {\n    collection: \"users\",\n    documents: [\n      { name: \"John Doe\", email: \"john@example.com\", age: 30 },\n      { name: \"Jane Smith\", email: \"jane@example.com\", age: 25 }\n    ]\n  }\n});\n```\n\n#### 5. mongodb_update\n更新MongoDB集合中的文档。\n\nUpdate documents in a MongoDB collection.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_update\",\n  arguments: {\n    collection: \"users\",\n    filter: { name: \"John Doe\" },\n    update: { $set: { age: 31 } },\n    many: false // 只更新一个文档（默认）| Update only one document (default)\n  }\n});\n```\n\n#### 6. mongodb_delete\n从MongoDB集合中删除文档。\n\nDelete documents from a MongoDB collection.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_delete\",\n  arguments: {\n    collection: \"users\",\n    filter: { name: \"John Doe\" },\n    many: false // 只删除一个文档（默认）| Delete only one document (default)\n  }\n});\n```\n\n#### 7. mongodb_create_collection\n在MongoDB中创建新集合。\n\nCreate a new collection in MongoDB.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"mysql-mongodb\",\n  tool_name: \"mongodb_create_collection\",\n  arguments: {\n    collection: \"new_collection\",\n    options: { capped: true, size: 1000000 }\n  }\n});\n```\n\n## 功能特点 | Features\n\n- 多种连接方法（URL、工作区、直接参数）| Multiple connection methods (URL, workspace, direct)\n- 同时支持MySQL和MongoDB数据库 | Support for both MySQL and MongoDB databases\n- 安全的连接处理和自动清理 | Secure connection handling with automatic cleanup\n- MySQL查询参数的预处理语句支持 | Prepared statement support for MySQL query parameters\n- 两种数据库的架构管理工具 | Schema management tools for both databases\n- 全面的错误处理和验证 | Comprehensive error handling and validation\n- TypeScript支持 | TypeScript support\n- 自动工作区检测 | Automatic workspace detection\n\n## 安全性 | Security\n\n- 在MySQL中使用预处理语句防止SQL注入 | Uses prepared statements to prevent SQL injection in MySQL\n- 通过环境变量支持安全密码处理 | Supports secure password handling through environment variables\n- 执行前验证查询和操作 | Validates queries and operations before execution\n- 自动关闭连接 | Automatically closes connections when done\n\n## 错误处理 | Error Handling\n\n服务器提供以下详细错误消息：| The server provides detailed error messages for:\n- 连接失败 | Connection failures\n- 无效的查询或参数 | Invalid queries or parameters\n- 缺少配置 | Missing configuration\n- 数据库错误 | Database errors\n- 架构验证错误 | Schema validation errors\n\n## 贡献 | Contributing\n\n欢迎贡献！请随时提交Pull Request到 https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/yaoxiaolinglong/mcp-mongodb-mysql-server\n\n## 致谢 | Acknowledgements\n\n本项目基于 [enemyrr/mcp-mysql-server](https://github.com/enemyrr/mcp-mysql-server) 开发，感谢原作者的贡献。\n\nThis project is based on [enemyrr/mcp-mysql-server](https://github.com/enemyrr/mcp-mysql-server). Thanks to the original author for their contribution.\n\n## 许可证 | License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mongodb",
        "nosql",
        "databases",
        "secure database",
        "mongodb databases",
        "databases secure"
      ],
      "category": "databases"
    },
    "ydb--ydb-mcp": {
      "owner": "ydb",
      "name": "ydb-mcp",
      "url": "https://github.com/ydb-platform/ydb-mcp",
      "imageUrl": "",
      "description": "MCP server for interacting with [YDB](https://ydb.tech) databases",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "ydb",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "ydb-platform--ydb-mcp": {
      "owner": "ydb-platform",
      "name": "ydb-mcp",
      "url": "https://github.com/ydb-platform/ydb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ydb-platform.webp",
      "description": "Enable natural language interactions and manage YDB databases through intuitive commands. Run SQL queries, list directories, and control database paths seamlessly with any LLM that supports the Model Context Protocol.",
      "stars": 22,
      "forks": 6,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-29T15:16:15Z",
      "readme_content": "# YDB MCP\n---\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ydb-platform/ydb-mcp/blob/main/LICENSE)\n[![PyPI version](https://badge.fury.io/py/ydb-mcp.svg)](https://badge.fury.io/py/ydb-mcp)\n\n[Model Context Protocol server](https://modelcontextprotocol.io/) for [YDB](https://ydb.tech). It allows to work with YDB databases from any [LLM](https://en.wikipedia.org/wiki/Large_language_model) that supports MCP. This integration enables AI-powered database operations and natural language interactions with your YDB instances.\n\n<a href=\"https://glama.ai/mcp/servers/@ydb-platform/ydb-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ydb-platform/ydb-mcp/badge\" alt=\"YDB MCP server\" />\n</a>\n\n## Usage\n\n### Via uvx\n\n[uvx](https://docs.astral.sh/uv/concepts/tools/), which is an allias for `uv run tool`, allows you to run various python applications without explicitly installing them. Below are examples of how to configure YDB MCP using `uvx`.\n\n#### Example: Using Anonymous Authentication\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ydb-mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\"\n      ]\n    }\n  }\n}\n```\n\n### Via pipx\n\n[pipx](https://pipx.pypa.io/stable/) allows you to run various applications from PyPI without explicitly installing each one. However, it must be [installed](https://pipx.pypa.io/stable/#install-pipx) first. Below are examples of how to configure YDB MCP using `pipx`.\n\n#### Example: Using Anonymous Authentication\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"pipx\",\n      \"args\": [\n        \"run\", \"ydb-mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\"\n      ]\n    }\n  }\n}\n```\n\n### Via pip\n\nYDB MCP can be installed using `pip`, [Python's package installer](https://pypi.org/project/pip/). The package is [available on PyPI](https://pypi.org/project/ydb-mcp/) and includes all necessary dependencies.\n\n```bash\npip install ydb-mcp\n```\n\nTo get started with YDB MCP, you'll need to configure your MCP client to communicate with the YDB instance. Below are example configuration files that you can customize according to your setup and then put into MCP client's settings. Path to the Python interpreter might also need to be adjusted to the correct virtual environment that has the `ydb-mcp` package installed.\n\n#### Example: Using Anonymous Authentication\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\", \"ydb_mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\"\n      ]\n    }\n  }\n}\n```\n\n### Authentication\n\nRegardless of the usage method (`uvx`, `pipx` or `pip`), you can configure authentication for your YDB installation. To do this, pass special command line arguments.\n\n#### Using Login/Password Authentication\n\nTo use login/password authentication, specify the `--ydb-auth-mode`, `--ydb-login`, and `--ydb-password` arguments:\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ydb-mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\",\n        \"--ydb-auth-mode\", \"login-password\",\n        \"--ydb-login\", \"<your-username>\",\n        \"--ydb-password\", \"<your-password>\"\n      ]\n    }\n  }\n}\n```\n\n#### Using Access Token Authentication\n\nTo use access token authentication, specify the `--ydb-auth-mode` and `--ydb-access-token` arguments:\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ydb-mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\",\n        \"--ydb-auth-mode\", \"access-token\",\n        \"--ydb-access-token\", \"qwerty123\"\n      ]\n    }\n  }\n}\n```\n\n#### Using Service Account Authentication\n\nTo use service account authentication, specify the `--ydb-auth-mode` and `--ydb-sa-key-file` arguments:\n\n```json\n{\n  \"mcpServers\": {\n    \"ydb\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ydb-mcp\",\n        \"--ydb-endpoint\", \"grpc://localhost:2136\",\n        \"--ydb-database\", \"/local\",\n        \"--ydb-auth-mode\", \"service-account\",\n        \"--ydb-sa-key-file\", \"~/sa_key.json\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nYDB MCP provides the following tools for interacting with YDB databases:\n\n- `ydb_query`: Run a SQL query against a YDB database\n  - Parameters:\n    - `sql`: SQL query string to execute\n\n- `ydb_query_with_params`: Run a parameterized SQL query with JSON parameters\n  - Parameters:\n    - `sql`: SQL query string with parameter placeholders\n    - `params`: JSON string containing parameter values\n\n- `ydb_list_directory`: List directory contents in YDB\n  - Parameters:\n    - `path`: YDB directory path to list\n\n- `ydb_describe_path`: Get detailed information about a YDB path (table, directory, etc.)\n  - Parameters:\n    - `path`: YDB path to describe\n\n- `ydb_status`: Get the current status of the YDB connection\n\n## Development\n\nThe project uses [Make](https://www.gnu.org/software/make/) as its primary development tool, providing a consistent interface for common development tasks.\n\n### Available Make Commands\n\nThe project includes a comprehensive Makefile with various commands for development tasks. Each command is designed to streamline the development workflow and ensure code quality:\n\n- `make all`: Run clean, lint, and test in sequence (default target)\n- `make clean`: Remove all build artifacts and temporary files\n- `make test`: Run all tests using pytest\n  - Can be configured with environment variables:\n    - `LOG_LEVEL` (default: WARNING) - Control test output verbosity (DEBUG, INFO, WARNING, ERROR)\n- `make unit-tests`: Run only unit tests with verbose output\n  - Can be configured with environment variables:\n    - `LOG_LEVEL` (default: WARNING) - Control test output verbosity (DEBUG, INFO, WARNING, ERROR)\n- `make integration-tests`: Run only integration tests with verbose output\n  - Can be configured with environment variables:\n    - `YDB_ENDPOINT` (default: grpc://localhost:2136)\n    - `YDB_DATABASE` (default: /local)\n    - `MCP_HOST` (default: 127.0.0.1)\n    - `MCP_PORT` (default: 8989)\n    - `LOG_LEVEL` (default: WARNING) - Control test output verbosity (DEBUG, INFO, WARNING, ERROR)\n- `make run-server`: Start the YDB MCP server\n  - Can be configured with environment variables:\n    - `YDB_ENDPOINT` (default: grpc://localhost:2136)\n    - `YDB_DATABASE` (default: /local)\n  - Additional arguments can be passed using `ARGS=\"your args\"`\n- `make lint`: Run all linting checks (flake8, mypy, black, isort)\n- `make format`: Format code using black and isort\n- `make install`: Install the package in development mode\n- `make dev`: Install the package in development mode with all development dependencies\n\n### Test Verbosity Control\n\nBy default, tests run with minimal output (WARNING level) to keep the output clean. You can control the verbosity of test output using the `LOG_LEVEL` environment variable:\n\n```bash\n# Run all tests with debug output\nmake test LOG_LEVEL=DEBUG\n\n# Run integration tests with info output\nmake integration-tests LOG_LEVEL=INFO\n\n# Run unit tests with warning output (default)\nmake unit-tests LOG_LEVEL=WARNING\n```\n\nAvailable log levels:\n- `DEBUG`: Show all debug messages, useful for detailed test flow\n- `INFO`: Show informational messages and above\n- `WARNING`: Show only warnings and errors (default)\n- `ERROR`: Show only error messages",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "ydb",
        "ydb databases",
        "access ydb",
        "manage ydb"
      ],
      "category": "databases"
    },
    "yincongcyincong--VictoriaMetrics-mcp-server": {
      "owner": "yincongcyincong",
      "name": "VictoriaMetrics-mcp-server",
      "url": "https://github.com/yincongcyincong/VictoriaMetrics-mcp-server",
      "imageUrl": "",
      "description": "An MCP server for interacting with VictoriaMetrics database.",
      "stars": 7,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-08T03:06:36Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yincongcyincong-victoriametrics-mcp-server-badge.png)](https://mseep.ai/app/yincongcyincong-victoriametrics-mcp-server)\n\n# VictoriaMetrics MCP Server\n[![smithery badge](https://smithery.ai/badge/@yincongcyincong/victoriametrics-mcp-server)](https://smithery.ai/server/@yincongcyincong/victoriametrics-mcp-server)\n\n\nMCP Server for the VictoriaMetrics.\n\n### Installing via Smithery\n\nTo install VictoriaMetrics Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yincongcyincong/victoriametrics-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @yincongcyincong/victoriametrics-mcp-server --client claude\n```\n\n## Debug\n```\nnpx @modelcontextprotocol/inspector -e VM_URL=http://127.0.0.1:8428  node src/index.js\n\n```\n\n### NPX\n\n```json\n{\n    \"mcpServers\": {\n        \"victoriametrics\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@yincongcyincong/victoriametrics-mcp-server\"\n            ],\n            \"env\": {\n                \"VM_URL\": \"\",\n                \"VM_SELECT_URL\": \"\",\n                \"VM_INSERT_URL\": \"\"\n            }\n        }\n    }\n}\n```\n\n### 📊 VictoriaMetrics Tools API Documentation\n\n## 1. `vm_data_write`\n\n**Description**: Write data to the VictoriaMetrics database.\n\n**Input Parameters**:\n\n| Parameter     | Type        | Description                                | Required |\n|---------------|-------------|--------------------------------------------|----------|\n| `metric`      | `object`    | Tags of the metric                         | ✅        |\n| `values`      | `number[]`  | Array of metric values                     | ✅        |\n| `timestamps`  | `number[]`  | Array of timestamps in Unix seconds        | ✅        |\n\n---\n\n## 2. `vm_prometheus_write`\n\n**Description**: Import Prometheus exposition format data into VictoriaMetrics.\n\n**Input Parameters**:\n\n| Parameter | Type     | Description                                     | Required |\n|-----------|----------|-------------------------------------------------|----------|\n| `data`    | `string` | Metrics in Prometheus exposition format         | ✅        |\n\n---\n\n## 3. `vm_query_range`\n\n**Description**: Query time series data over a specific time range.\n\n**Input Parameters**:\n\n| Parameter | Type     | Description                                     | Required |\n|-----------|----------|-------------------------------------------------|----------|\n| `query`   | `string` | PromQL expression                               | ✅        |\n| `start`   | `number` | Start timestamp in Unix seconds                 | ⛔️        |\n| `end`     | `number` | End timestamp in Unix seconds                   | ⛔️        |\n| `step`    | `string` | Query resolution step width (e.g., `10s`, `1m`) | ⛔️        |\n\n> Only `query` is required; the other fields are optional.\n\n---\n\n## 4. `vm_query`\n\n**Description**: Query the current value of a time series.\n\n**Input Parameters**:\n\n| Parameter | Type     | Description                             | Required |\n|-----------|----------|-----------------------------------------|----------|\n| `query`   | `string` | PromQL expression to evaluate           | ✅        |\n| `time`    | `number` | Evaluation timestamp in Unix seconds    | ⛔️        |\n\n---\n\n## 5. `vm_labels`\n\n**Description**: Get all unique label names.\n\n**Input Parameters**: None\n\n---\n\n## 6. `vm_label_values`\n\n**Description**: Get all unique values for a specific label.\n\n**Input Parameters**:\n\n| Parameter | Type     | Description                  | Required |\n|-----------|----------|------------------------------|----------|\n| `label`   | `string` | Label name to get values for | ✅        |\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "security",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "yourtechtribe--mcp-odoo-for-finance": {
      "owner": "yourtechtribe",
      "name": "mcp-odoo-for-finance",
      "url": "https://github.com/yourtechtribe/mcp-odoo-for-finance",
      "imageUrl": "/freedevtools/mcp/pfp/yourtechtribe.webp",
      "description": "Enables access and manipulation of Odoo ERP data through a standardized interface, facilitating tasks such as viewing partner and accounting information, and performing financial reconciliations. It supports secure authentication and integration with Odoo instances for AI agents.",
      "stars": 10,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-09T11:37:42Z",
      "readme_content": "# MCP-Odoo\n\nModel Context Protocol server for Odoo integration, allowing AI agents to access and manipulate Odoo data through a standardized interface.\n\n## Overview\n\nMCP-Odoo provides a bridge between Odoo ERP systems and AI agents using the Model Context Protocol (MCP). This enables AI systems to:\n\n- Access partner information\n- View and analyze accounting data including invoices and payments\n- Perform reconciliation of financial records\n- Query vendor bills and customer invoices\n\n## Features\n\n- 🔌 Easy integration with Odoo instances\n- 🤖 Standard MCP interface for AI agent compatibility\n- 📊 Rich accounting data access\n- 🔒 Secure authentication with Odoo\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourtechtribe/model-context-protocol-mcp-odoo.git\ncd model-context-protocol-mcp-odoo\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## Configuration\n\nCreate a `.env` file in the project root with the following variables:\n\n```\nODOO_URL=https://your-odoo-instance.com\nODOO_DB=your_database\nODOO_USERNAME=your_username\nODOO_PASSWORD=your_password\nHOST=0.0.0.0\nPORT=8080\n```\n\n## Usage\n\nStart the MCP server:\n\n```bash\n# Using the SSE transport (default)\npython -m mcp_odoo_public\n\n# Using stdio for local agent integration\npython -m mcp_odoo_public --transport stdio\n```\n\n## Documentation\n\nComprehensive documentation is available in the `docs/` directory:\n\n- [Documentation Home](docs/index.md) - Start here for an overview of all documentation\n- [Implementation Guide](docs/implementation_guide.md) - Detailed architecture and implementation details\n- [Accounting Functionality](docs/accounting_guide.md) - In-depth guide to accounting features\n- [Troubleshooting](docs/troubleshooting.md) - Solutions for common issues\n- [Usage Examples](docs/examples/basic_usage.md) - Practical examples to get started\n\n## Development\n\n### Project Structure\n\n- `mcp_odoo_public/`: Main package\n  - `odoo/`: Odoo client and related modules\n  - `resources/`: MCP resources definitions (tools and schemas)\n  - `server.py`: MCP server implementation\n  - `config.py`: Configuration management\n  - `mcp_instance.py`: FastMCP instance definition\n\n### Adding New Resources\n\nResources define the capabilities exposed to AI agents through MCP. To add a new resource:\n\n1. Create a new file in the `resources/` directory\n2. Define your resource using the `@mcp.tool()` decorator\n3. Import your resource in `resources/__init__.py`\n\nFor detailed instructions, see the [Implementation Guide](docs/implementation_guide.md).\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Author\n\nAlbert Gil López  \n- Email: albert.gil@yourtechtribe.com\n- LinkedIn: https://www.linkedin.com/in/albertgilopez/\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "odoo",
        "databases",
        "database",
        "odoo finance",
        "odoo instances",
        "mcp odoo"
      ],
      "category": "databases"
    },
    "yuangjay--db_manager": {
      "owner": "yuangjay",
      "name": "db_manager",
      "url": "https://github.com/yuangjay/db_manager",
      "imageUrl": "/freedevtools/mcp/pfp/yuangjay.webp",
      "description": "Streamline data operations by providing tools for querying, updating, and maintaining database systems efficiently.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-22T05:59:24Z",
      "readme_content": "# db_manager",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "db_manager",
        "secure database",
        "databases secure",
        "yuangjay db_manager"
      ],
      "category": "databases"
    },
    "yuanoOo--oceanbase_mcp_server": {
      "owner": "yuanoOo",
      "name": "oceanbase_mcp_server",
      "url": "https://github.com/yuanoOo/oceanbase_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/yuanoOo.webp",
      "description": "Enables secure interaction with OceanBase databases for listing tables, reading data, and executing SQL queries through a structured interface. Provides proper error handling and secure access via environment variables.",
      "stars": 3,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-04-27T21:56:08Z",
      "readme_content": "# OceanBase MCP Server\n\nA Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases. \nThis server allows AI assistants to list tables, read data, and execute SQL queries through a controlled interface, making database exploration and analysis safer and more structured.\n\n<a href=\"https://glama.ai/mcp/servers/@yuanoOo/oceanbase_mcp_server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@yuanoOo/oceanbase_mcp_server/badge\" alt=\"OceanBase Server MCP server\" />\n</a>\n\n## Features\n\n- List available OceanBase tables as resources\n- Read table contents\n- Execute SQL queries with proper error handling\n- Secure database access through environment variables\n- Comprehensive logging\n\n## Installation\n\n```bash\npip install oceanbase-mcp-server\n```\n\n## Configuration\n\nSet the following environment variables:\n\n```bash\nOB_HOST=localhost     # Database host\nOB_PORT=2881         # Optional: Database port (defaults to 2881 if not specified)\nOB_USER=your_username\nOB_PASSWORD=your_password\nOB_DATABASE=your_database\n```\n\n## Usage\n\n### With Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"oceanbase\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"path/to/oceanbase_mcp_server\",\n        \"run\",\n        \"oceanbase_mcp_server\"\n      ],\n      \"env\": {\n        \"OB_HOST\": \"localhost\",\n        \"OB_PORT\": \"2881\",\n        \"OB_USER\": \"your_username\",\n        \"OB_PASSWORD\": \"your_password\",\n        \"OB_DATABASE\": \"your_database\"\n      }\n    }\n  }\n}\n```\n\n### As a standalone server\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Run the server\npython -m oceanbase_mcp_server\n```\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/oceanbase_mcp_server.git\ncd oceanbase_mcp_server\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install development dependencies\npip install -r requirements-dev.txt\n\n# Run tests\npytest\n```\n\n## Security Considerations\n\n- Never commit environment variables or credentials\n- Use a database user with minimal required permissions\n- Consider implementing query whitelisting for production use\n- Monitor and log all database operations\n\n## Security Best Practices\n\nThis MCP server requires database access to function. For security:\n\n1. **Create a dedicated OceanBase user** with minimal permissions\n2. **Never use root credentials** or administrative accounts\n3. **Restrict database access** to only necessary operations\n4. **Enable logging** for audit purposes\n5. **Regular security reviews** of database access\n\nSee [OceanBase Security Configuration Guide](./SECURITY.md) for detailed instructions on:\n- Creating a restricted OceanBase user\n- Setting appropriate permissions\n- Monitoring database access\n- Security best practices\n\n⚠️ IMPORTANT: Always follow the principle of least privilege when configuring database access.\n\n## License\n\nApache License - see LICENSE file for details.\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oceanbase_mcp_server",
        "oceanbase",
        "databases",
        "oceanbase databases",
        "yuanooo oceanbase_mcp_server",
        "secure database"
      ],
      "category": "databases"
    },
    "yuki777--mysql-mcp-server": {
      "owner": "yuki777",
      "name": "mysql-mcp-server",
      "url": "https://github.com/yuki777/mysql-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/yuki777.webp",
      "description": "Connects local MySQL databases for executing SQL queries and retrieving information using large language models through standard input/output communication, without the need for port binding. Supports management of multiple connection profiles and stores connection settings for efficient reuse.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-05-10T10:01:15Z",
      "readme_content": "# MySQL MCP Server\n\nMySQL Model Context Protocol（MCP）サーバーは、ローカル環境のMySQLデータベースに接続し、大規模言語モデル（LLM）がSQLクエリを実行できるようにするツールです。\n\n## 要件\n\n- **Node.js**: 20.0.0以上\n- **MySQL**: 5.7以上のMySQLまたはMariaDBサーバー\n\n## 機能\n\n- **MySQLクエリの実行**: LLMからSQLクエリを直接実行\n- **データベース情報の取得**: データベース一覧、テーブル一覧、テーブル構造の確認\n- **MCP準拠**: Model Context Protocol に対応し、LLMと統合可能\n- **stdio通信**: 標準入出力を使用してLLMと通信、ポートバインドなし\n- **接続プロファイル管理**: 複数の接続設定をプロファイル名で管理し切り替え\n- **接続情報の保存**: データベース接続情報をローカルに保存し再利用\n\n## インストールと使用方法\n\n### NPXでの一時実行\n\n```bash\n npx -y https://github.com/yuki777/mysql-mcp-server --host 127.0.0.1 --port 13306 --user root\n```\n\n### オプション\n\n| オプション | 説明 | デフォルト値 |\n|----------|------|-------------|\n| `-h, --host <host>` | MySQLホスト | localhost |\n| `-p, --port <port>` | MySQLポート | 13306 |\n| `-u, --user <user>` | MySQLユーザー | root |\n| `--password <password>` | MySQLパスワード | (空文字) |\n| `-d, --database <database>` | デフォルトデータベース | (オプション) |\n| `-c, --config <path>` | 設定ファイルパス | (オプション) |\n| `--auto-connect` | サーバー起動時に自動的にデータベースに接続 | false |\n| `--server-port <port>` | MCPサーバーポート（stdioモードでは使用されません） | 3000 |\n| `--server-host <host>` | MCPサーバーホスト（stdioモードでは使用されません） | localhost |\n| `--query-timeout <ms>` | クエリタイムアウト(ミリ秒) | 30000 |\n| `--max-results <count>` | 最大結果行数 | 1000 |\n| `--debug` | デバッグモード | false |\n\n### 接続情報の保存と再利用\n\nMySQL MCP Serverは、正常に接続したデータベースの情報を名前付きプロファイルとしてローカルに保存します。これにより、次回の起動時に接続情報を名前で指定して再利用できます。保存された接続情報は、ユーザーのホームディレクトリにある `.mysql-mcp-connections.json` ファイルに保存されます。\n\n各接続プロファイルには以下が含まれます：\n- プロファイル名\n- ホスト名\n- ポート番号\n- ユーザー名\n- パスワード\n- データベース名（設定されている場合）\n\n複数のデータベース接続をプロファイル名で管理し、簡単に切り替えることが可能です。\n\n### 設定ファイルの使用\n\n設定ファイル（JSON形式）を使用して接続情報を設定することもできます：\n\n```json\n{\n  \"server\": {\n    \"port\": 3000,\n    \"host\": \"localhost\"\n  },\n  \"mysql\": {\n    \"host\": \"localhost\",\n    \"port\": 13306,\n    \"user\": \"root\",\n    \"password\": \"yourpassword\",\n    \"database\": \"mydb\"\n  },\n  \"debug\": false,\n  \"queryTimeout\": 30000,\n  \"maxResultSize\": 1000\n}\n```\n\n設定ファイルを使用する場合:\n\n```bash\nnpx -y https://github.com/yuki777/mysql-mcp-server -c ./mysql-mcp-config.json\n```\n\n## 通信方式\n\nMySQL MCP ServerはMCP (Model Context Protocol) に準拠した「stdio」モードで動作します。これにより特定のポートにバインドせず、標準入出力を介して通信します。これには次のような利点があります：\n\n1. **ポート競合の回避**: 特定のポートを使用しないため、ポート競合の問題が発生しません\n2. **セキュリティ向上**: ネットワーク通信を使用しないため、ネットワークレベルの攻撃リスクが軽減\n3. **簡素なプロセス間通信**: LLMとの通信がシンプル化\n\n### 注意点\n\n- stdioモードでは、JSON形式のメッセージをやり取りします\n- 一行に一つのJSONメッセージを送信する必要があります\n- エラー情報と接続ログは標準エラー（stderr）に出力されます\n\n## 提供されるMCPツール\n\n### データベース接続管理\n\n| ツール名 | 説明 | 必須パラメータ |\n|---------|------|-------------|\n| connect_database | データベースに接続します | host, port, user |\n| connect_by_profile | 保存済みプロファイル名で接続します | profileName |\n| disconnect_database | 現在のデータベース接続を切断します | なし |\n| get_connection_status | データベース接続の状態を取得します | なし |\n\n### 接続プロファイル管理\n\n| ツール名 | 説明 | 必須パラメータ |\n|---------|------|-------------|\n| list_profiles | 保存済みのプロファイル一覧を取得します | なし |\n| get_profile | プロファイルの詳細を取得します | profileName |\n| add_profile | 新しいプロファイルを追加します | profileName, host, port, user |\n| remove_profile | プロファイルを削除します | profileName |\n\n### SQLクエリ操作\n\n| ツール名 | 説明 | 必須パラメータ |\n|---------|------|-------------|\n| execute_query | MySQLクエリを実行します | query: SQL文 |\n| get_databases | 利用可能なデータベースの一覧を取得します | なし |\n| get_tables | 指定したデータベース内のテーブル一覧を取得します | database (オプション) |\n| describe_table | 指定したテーブルの構造を取得します | table |\n\n## 接続管理機能\n\nMySQL MCP Serverでは、サーバーの起動とデータベース接続を分離することができます。このアプローチにより以下のメリットがあります：\n\n1. **接続情報なしでの起動**: サーバーはデータベース接続情報がなくても起動可能\n2. **複数データベースへの接続**: サーバー起動後に異なるデータベースへ接続の切り替えが可能\n3. **シンプルなインストール**: `npx -y https://github.com/yuki777/mysql-mcp-server` だけで実行可能\n\n### 接続管理の使用方法\n\n1. **自動接続なしでサーバーを起動**:\n   ```bash\n   npx -y https://github.com/yuki777/mysql-mcp-server\n   ```\n\n2. **接続ツールを使用してデータベースに接続（プロファイル名を指定して保存）**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_1\",\n     \"tool\": \"connect_database\",\n     \"arguments\": {\n       \"host\": \"localhost\",\n       \"port\": 3306,\n       \"user\": \"root\",\n       \"password\": \"your_password\",\n       \"database\": \"your_db\",\n       \"profileName\": \"my-db\"\n     }\n   }\n   ```\n\n3. **プロファイル一覧の取得**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_2\",\n     \"tool\": \"list_profiles\",\n     \"arguments\": {}\n   }\n   ```\n\n4. **プロファイル名で接続**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_3\",\n     \"tool\": \"connect_by_profile\",\n     \"arguments\": {\n       \"profileName\": \"my-db\"\n     }\n   }\n   ```\n   \n5. **新しいプロファイルの追加（接続は行わない）**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_4\",\n     \"tool\": \"add_profile\",\n     \"arguments\": {\n       \"profileName\": \"production-db\",\n       \"host\": \"prod.example.com\",\n       \"port\": 3306,\n       \"user\": \"prod_user\",\n       \"password\": \"prod_password\",\n       \"database\": \"production\"\n     }\n   }\n   ```\n\n6. **接続状態の確認**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_5\",\n     \"tool\": \"get_connection_status\",\n     \"arguments\": {}\n   }\n   ```\n\n7. **接続の切断**:\n   ```json\n   {\n     \"type\": \"tool_call\",\n     \"request_id\": \"req_6\",\n     \"tool\": \"disconnect_database\",\n     \"arguments\": {}\n   }\n   ```\n\n### テスト用スクリプト\n\nリポジトリには `test-connection-management.js` というテストスクリプトが含まれています。このスクリプトを使用して、接続管理機能をテストできます：\n\n```bash\nnode test-connection-management.js\n```\n\n## 開発者向け情報\n\n### 開発環境のセットアップ\n\n```bash\n# リポジトリのクローン\ngit clone [repository-url]\ncd mysql-mcp-server\n\n# 依存関係のインストール\nnpm install\n\n# 開発モードでの実行\nnpm run dev\n```\n\n### ビルド\n\n```bash\nnpm run build\n```\n\n## ライセンス\n\nISC\n\n## 貢献\n\nバグレポートや機能リクエスト、プルリクエストを歓迎します。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "mysql",
        "database",
        "mysql databases",
        "secure database",
        "database access"
      ],
      "category": "databases"
    },
    "yuru-sha--mcp-server-mysql": {
      "owner": "yuru-sha",
      "name": "mcp-server-mysql",
      "url": "https://github.com/yuru-sha/mcp-server-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/yuru-sha.webp",
      "description": "Enables inspection of MySQL database schemas and execution of read-only queries. Provides safe query execution within READ ONLY transactions and supports Docker.",
      "stars": 9,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T07:28:38Z",
      "readme_content": "# mcp-server-mysql\n[![CI Status](https://github.com/yuru-sha/mcp-server-mysql/actions/workflows/ci.yml/badge.svg)](https://github.com/yuru-sha/mcp-server-mysql/actions)\n[![smithery badge](https://smithery.ai/badge/@yuru-sha/mcp-server-mysql)](https://smithery.ai/server/@yuru-sha/mcp-server-mysql)\n\nModel Context Protocol Server for MySQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Features\n\n- Read-only access to MySQL databases\n- Schema inspection capabilities\n- Safe query execution within READ ONLY transactions\n- Docker support\n- NPM package available\n\n## Installation\n\n### Using Docker\n\n```bash\n# Build the Docker image\nmake docker\n\n# Run with Docker\ndocker run -i --rm mcp/mysql mysql://host:port/dbname\n```\n\n### Installing via Smithery\n\nTo install MySQL Database Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yuru-sha/mcp-server-mysql):\n\n```bash\nnpx -y @smithery/cli install @yuru-sha/mcp-server-mysql --client claude\n```\n\n## Usage\n\n### With Claude Desktop\n\nAdd the following configuration to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"mcp/mysql\",\n        \"mysql://host:port/dbname\"\n      ]\n    }\n  }\n}\n```\n\nNote: When using Docker on macOS, use `host.docker.internal` if the MySQL server is running on the host network.\n\n### Connection URL Format\n\n```\nmysql://[user][:password]@host[:port]/database\n```\n\nReplace `/database` with your database name.\n\n## Development\n\n```bash\n# Initial setup\nmake setup\n\n# Build the project\nmake build\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n```\n\n## License\n\nThis project is released under the [MIT License](LICENSE).\n\n## Security\n\nThis server enforces read-only access to protect your database. All queries are executed within READ ONLY transactions.\n\nFor enhanced security, we recommend creating a read-only user.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "yuru-sha--mcp-server-postgres": {
      "owner": "yuru-sha",
      "name": "mcp-server-postgres",
      "url": "https://github.com/yuru-sha/mcp-server-postgres",
      "imageUrl": "/freedevtools/mcp/pfp/yuru-sha.webp",
      "description": "Interact with PostgreSQL databases using read-only transactions for safe query execution and schema inspection. Execute queries and retrieve insights without modifying the underlying data.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-07T08:26:29Z",
      "readme_content": "# mcp-server-postgres\n[![CI Status](https://github.com/yuru-sha/mcp-server-postgres/actions/workflows/ci.yml/badge.svg)](https://github.com/yuru-sha/mcp-server-postgres/actions)\n[![smithery badge](https://smithery.ai/badge/@yuru-sha/mcp-server-postgres)](https://smithery.ai/server/@yuru-sha/mcp-server-postgres)\n\nModel Context Protocol Server for PostgreSQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Features\n\n- Read-only access to PostgreSQL databases\n- Schema inspection capabilities\n- Safe query execution within READ ONLY transactions\n- Docker support\n- NPM package available\n\n## Installation\n\n### Using Docker\n\n```bash\n# Build the Docker image\nmake docker\n\n# Run with Docker\ndocker run -i --rm mcp/postgres postgresql://host:port/dbname\n```\n\n### Installing via Smithery\n\nTo install PostgreSQL Database Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yuru-sha/mcp-server-postgres):\n\n```bash\nnpx -y @smithery/cli install @yuru-sha/mcp-server-postgres --client claude\n```\n\n## Usage\n\n### With Claude Desktop\n\nAdd the following configuration to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"mcp/postgres\",\n        \"postgresql://host:port/dbname\"\n      ]\n    }\n  }\n}\n```\n\nNote: When using Docker on macOS, use `host.docker.internal` if the PostgreSQL server is running on the host network.\n\n### Connection URL Format\n\n```\npostgresql://[user][:password]@host[:port]/database\n```\n\nReplace `/database` with your database name.\n\n## Development\n\n```bash\n# Initial setup\nmake setup\n\n# Build the project\nmake build\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n```\n\n## License\n\nThis project is released under the [MIT License](LICENSE).\n\n## Security\n\nThis server enforces read-only access to protect your database. All queries are executed within READ ONLY transactions.\n\nFor enhanced security, we recommend creating a read-only user.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postgres",
        "databases",
        "postgresql",
        "databases secure",
        "secure database",
        "postgresql databases"
      ],
      "category": "databases"
    },
    "zajTools--zaj-MySQL-MCP": {
      "owner": "zajTools",
      "name": "zaj-MySQL-MCP",
      "url": "https://github.com/zajTools/zaj-MySQL-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/zajTools.webp",
      "description": "Execute SQL queries, manage database tables, analyze schema, and generate business insights from a MySQL database.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-03T08:44:43Z",
      "readme_content": "# zaj-MySQL-MCP\n\nA Model Context Protocol (MCP) server implementation that provides database interaction and business intelligence capabilities through MySQL. This server enables running SQL queries, analyzing business data, and automatically generating business insight memos.\n\n## Features\n\n- Execute SQL queries against a MySQL database\n- Create and manage database tables\n- Analyze database schema\n- Generate and store business insights\n- Access a continuously updated business insights memo\n\n## Tools\n\n### Query Tools\n\n- **read_query**: Execute SELECT queries to read data from the database\n  - Input: `query` (string) - The SELECT SQL query to execute\n  - Returns: Query results as array of objects\n\n- **write_query**: Execute INSERT, UPDATE, or DELETE queries\n  - Input: `query` (string) - The SQL modification query\n  - Returns: `{ affected_rows: number }`\n\n- **create_table**: Create new tables in the database\n  - Input: `query` (string) - CREATE TABLE SQL statement\n  - Returns: Confirmation of table creation\n\n### Schema Tools\n\n- **list_tables**: Get a list of all tables in the database\n  - No input required\n  - Returns: Array of table names\n\n- **describe_table**: View schema information for a specific table\n  - Input: `table_name` (string) - Name of table to describe\n  - Returns: Array of column definitions with names and types\n\n### Analysis Tools\n\n- **append_insight**: Add new business insights to the memo resource\n  - Input: `insight` (string) - Business insight discovered from data analysis\n  - Returns: Confirmation of insight addition\n  - Triggers update of memo://insights resource\n\n## Resources\n\nThe server exposes a single resource:\n\n- **memo://insights**: A continuously updated business insights memo that aggregates discovered insights during analysis\n  - Auto-updates as new insights are discovered via the append-insight tool\n\n## Implementation Details\n\nThis MCP server implements the Model Context Protocol directly, without relying on external SDK dependencies. It uses:\n\n- **mysql2**: For MySQL database connectivity\n- **yargs**: For command-line argument parsing\n- **readline**: For handling stdin/stdout communication\n\nThe server follows the JSON-RPC 2.0 protocol for communication with Claude, handling requests for tool listings, resource listings, and tool execution.\n\n## Configuration\n\nThe MySQL MCP server uses environment variables for configuration. Create a `.env` file in the root directory with the following variables:\n\n```\n# Database Connection\nDB_CONNECTION=mysql\n\n# Database Host\nDB_HOST=localhost\n\n# Database Port\nDB_PORT=3306\n\n# Database Username (required)\nDB_USER=your_mysql_username\n\n# Database Password (required)\nDB_PASSWORD=your_mysql_password\n\n# Database Name (required)\nDB_NAME=your_database_name\n```\n\nA `.env.example` file is provided as a template. Copy it to `.env` and update the values:\n\n```bash\ncp .env.example .env\n# Then edit .env with your database credentials\n```\n\n## Usage with Claude Desktop\n\nAdd the server to your `cline_mcp_settings.json`:\n\n```json\n\"mcpServers\": {\n  \"mysql\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"/path/to/zaj_MySQL_MCP/build/index.js\"\n    ],\n    \"disabled\": false,\n    \"autoApprove\": []\n  }\n}\n```\n\nNote that database credentials are now configured through the `.env` file, not through command line arguments.\n\n## Building and Running\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/zajTools/zaj-MySQL-MCP.git\ncd zaj-MySQL-MCP\n```\n\n2. Create and configure your .env file:\n```bash\ncp .env.example .env\n# Edit .env with your database credentials\n```\n\n3. Install dependencies:\n```bash\nnpm install\n```\n\n4. Build the server:\n```bash\nnpm run build\n```\n\n5. Run the server:\n```bash\nnode build/index.js\n```\n\n## Demo and Examples\n\nWe've included example materials to help you get started with the MySQL MCP server:\n\n- **Sample Database**: A complete e-commerce database schema with customers, products, orders, and sales data\n- **Example Queries**: Pre-written queries demonstrating various capabilities of the MCP server\n- **Usage Scenarios**: Examples of how Claude can interact with your MySQL database\n\nTo try the demo:\n\n1. Check out the [Demo Guide](examples/DEMO.md) for step-by-step instructions\n2. Run the [Setup SQL Script](examples/sql/setup.sql) to create the sample database\n3. Configure the MCP server to connect to the demo database\n4. Start asking Claude questions about your data!\n\nThese examples are designed to show the potential of using Claude with MySQL databases, but the MCP server works with any MySQL database you have access to.\n\n## License\n\nThis MCP server is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "zaj mysql"
      ],
      "category": "databases"
    },
    "zhaoxin34--mcp-server-mysql": {
      "owner": "zhaoxin34",
      "name": "mcp-server-mysql",
      "url": "https://github.com/zhaoxin34/mcp-server-mysql",
      "imageUrl": "/freedevtools/mcp/pfp/zhaoxin34.webp",
      "description": "Access and query MySQL databases to execute read-only SQL queries and inspect database schemas. Facilitate data interactions for LLM applications with secure and optimized access.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-08T04:32:46Z",
      "readme_content": "# MCP Server for MySQL based on NodeJS\n[![smithery badge](https://smithery.ai/badge/@benborla29/mcp-server-mysql)](https://smithery.ai/server/@benborla29/mcp-server-mysql)\n\n\n\nA Model Context Protocol server that provides read-only access to MySQL databases. This server enables LLMs to inspect database schemas and execute read-only queries.\n\n## Installation\n\n### Using Smithery\nThe easiest way to install and configure this MCP server is through [Smithery](https://smithery.ai/server/@benborla29/mcp-server-mysql):\n\n```bash\n# Install the MCP server\nnpx -y @smithery/cli@latest install @benborla29/mcp-server-mysql --client claude\n```\n\nDuring configuration, you'll be prompted to enter your MySQL connection details. Smithery will automatically:\n- Set up the correct environment variables\n- Configure your LLM application to use the MCP server\n- Test the connection to your MySQL database\n- Provide helpful troubleshooting if needed\n\n### Using MCP Get\nYou can also install this package using [MCP Get](https://mcp-get.com/packages/%40benborla29%2Fmcp-server-mysql):\n\n```bash\nnpx @michaellatman/mcp-get@latest install @benborla29/mcp-server-mysql\n```\n\nMCP Get provides a centralized registry of MCP servers and simplifies the installation process.\n\n### Using NPM/PNPM\nFor manual installation:\n\n```bash\n# Using npm\nnpm install -g @benborla29/mcp-server-mysql\n\n# Using pnpm\npnpm add -g @benborla29/mcp-server-mysql\n```\n\nAfter manual installation, you'll need to configure your LLM application to use the MCP server (see Configuration section below).\n\n## Components\n\n### Tools\n\n- **mysql_query**\n  - Execute read-only SQL queries against the connected database\n  - Input: `sql` (string): The SQL query to execute\n  - All queries are executed within a READ ONLY transaction\n  - Supports prepared statements for secure parameter handling\n  - Configurable query timeouts and result pagination\n  - Built-in query execution statistics\n\n### Resources\n\nThe server provides comprehensive database information:\n\n- **Table Schemas**\n  - JSON schema information for each table\n  - Column names and data types\n  - Index information and constraints\n  - Foreign key relationships\n  - Table statistics and metrics\n  - Automatically discovered from database metadata\n\n### Security Features\n\n- SQL injection prevention through prepared statements\n- Query whitelisting/blacklisting capabilities\n- Rate limiting for query execution\n- Query complexity analysis\n- Configurable connection encryption\n- Read-only transaction enforcement\n\n### Performance Optimizations\n\n- Optimized connection pooling\n- Query result caching\n- Large result set streaming\n- Query execution plan analysis\n- Configurable query timeouts\n\n### Monitoring and Debugging\n\n- Comprehensive query logging\n- Performance metrics collection\n- Error tracking and reporting\n- Health check endpoints\n- Query execution statistics\n\n## Configuration\n\n### Automatic Configuration with Smithery\nIf you installed using Smithery, your configuration is already set up. You can view or modify it with:\n\n```bash\nsmithery configure @benborla29/mcp-server-mysql\n```\n\n### Manual Configuration for Claude Desktop App\nTo manually configure the MCP server for Claude Desktop App, add the following to your `claude_desktop_config.json` file (typically located in your user directory):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ],\n      \"env\": {\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\"\n      }\n    }\n  }\n}\n```\n\nReplace `db_name` with your database name or leave it blank to access all databases.\n\n### Advanced Configuration Options\nFor more control over the MCP server's behavior, you can use these advanced configuration options:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_server_mysql\": {\n      \"command\": \"/path/to/npx/binary/npx\",\n      \"args\": [\n        \"-y\",\n        \"@benborla29/mcp-server-mysql\"\n      ],\n      \"env\": {\n        // Basic connection settings\n        \"MYSQL_HOST\": \"127.0.0.1\",\n        \"MYSQL_PORT\": \"3306\",\n        \"MYSQL_USER\": \"root\",\n        \"MYSQL_PASS\": \"\",\n        \"MYSQL_DB\": \"db_name\",\n        \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\",\n        \n        // Performance settings\n        \"MYSQL_POOL_SIZE\": \"10\",\n        \"MYSQL_QUERY_TIMEOUT\": \"30000\",\n        \"MYSQL_CACHE_TTL\": \"60000\",\n        \n        // Security settings\n        \"MYSQL_RATE_LIMIT\": \"100\",\n        \"MYSQL_MAX_QUERY_COMPLEXITY\": \"1000\",\n        \"MYSQL_SSL\": \"true\",\n        \n        // Monitoring settings\n        \"MYSQL_ENABLE_LOGGING\": \"true\",\n        \"MYSQL_LOG_LEVEL\": \"info\",\n        \"MYSQL_METRICS_ENABLED\": \"true\"\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n### Basic Connection\n- `MYSQL_HOST`: MySQL server host (default: \"127.0.0.1\")\n- `MYSQL_PORT`: MySQL server port (default: \"3306\")\n- `MYSQL_USER`: MySQL username (default: \"root\")\n- `MYSQL_PASS`: MySQL password\n- `MYSQL_DB`: Target database name\n\n### Performance Configuration\n- `MYSQL_POOL_SIZE`: Connection pool size (default: \"10\")\n- `MYSQL_QUERY_TIMEOUT`: Query timeout in milliseconds (default: \"30000\")\n- `MYSQL_CACHE_TTL`: Cache time-to-live in milliseconds (default: \"60000\")\n\n### Security Configuration\n- `MYSQL_RATE_LIMIT`: Maximum queries per minute (default: \"100\")\n- `MYSQL_MAX_QUERY_COMPLEXITY`: Maximum query complexity score (default: \"1000\")\n- `MYSQL_SSL`: Enable SSL/TLS encryption (default: \"false\")\n\n### Monitoring Configuration\n- `MYSQL_ENABLE_LOGGING`: Enable query logging (default: \"false\")\n- `MYSQL_LOG_LEVEL`: Logging level (default: \"info\")\n- `MYSQL_METRICS_ENABLED`: Enable performance metrics (default: \"false\")\n\n## Testing\n\n### Database Setup\n\nBefore running tests, you need to set up the test database and seed it with test data:\n\n1. **Create Test Database and User**\n   ```sql\n   -- Connect as root and create test database\n   CREATE DATABASE IF NOT EXISTS mcp_test;\n   \n   -- Create test user with appropriate permissions\n   CREATE USER IF NOT EXISTS 'mcp_test'@'localhost' IDENTIFIED BY 'mcp_test_password';\n   GRANT ALL PRIVILEGES ON mcp_test.* TO 'mcp_test'@'localhost';\n   FLUSH PRIVILEGES;\n   ```\n\n2. **Run Database Setup Script**\n   ```bash\n   # Run the database setup script\n   pnpm run setup:test:db\n   ```\n\n   This will create the necessary tables and seed data. The script is located in `scripts/setup-test-db.ts`\n\n3. **Configure Test Environment**\n   Create a `.env.test` file in the project root:\n   ```env\n   MYSQL_HOST=127.0.0.1\n   MYSQL_PORT=3306\n   MYSQL_USER=mcp_test\n   MYSQL_PASS=mcp_test_password\n   MYSQL_DB=mcp_test\n   ```\n\n4. **Update package.json Scripts**\n   Add these scripts to your package.json:\n   ```json\n   {\n     \"scripts\": {\n       \"setup:test:db\": \"ts-node scripts/setup-test-db.ts\",\n       \"pretest\": \"pnpm run setup:test:db\",\n       \"test\": \"vitest run\",\n       \"test:watch\": \"vitest\",\n       \"test:coverage\": \"vitest run --coverage\"\n     }\n   }\n   ```\n\n### Running Tests\n\nThe project includes a comprehensive test suite to ensure functionality and reliability:\n\n```bash\n# First-time setup\npnpm run setup:test:db\n\n# Run all tests\npnpm test\n```\n\n## Troubleshooting\n\n### Using Smithery for Troubleshooting\nIf you installed with Smithery, you can use its built-in diagnostics:\n\n```bash\n# Check the status of your MCP server\nsmithery status @benborla29/mcp-server-mysql\n\n# Run diagnostics\nsmithery diagnose @benborla29/mcp-server-mysql\n\n# View logs\nsmithery logs @benborla29/mcp-server-mysql\n```\n\n### Using MCP Get for Troubleshooting\nIf you installed with MCP Get:\n\n```bash\n# Check the status\nmcp-get status @benborla29/mcp-server-mysql\n\n# View logs\nmcp-get logs @benborla29/mcp-server-mysql\n```\n\n### Common Issues\n\n1. **Connection Issues**\n   - Verify MySQL server is running and accessible\n   - Check credentials and permissions\n   - Ensure SSL/TLS configuration is correct if enabled\n   - Try connecting with a MySQL client to confirm access\n\n2. **Performance Issues**\n   - Adjust connection pool size\n   - Configure query timeout values\n   - Enable query caching if needed\n   - Check query complexity settings\n   - Monitor server resource usage\n\n3. **Security Restrictions**\n   - Review rate limiting configuration\n   - Check query whitelist/blacklist settings\n   - Verify SSL/TLS settings\n   - Ensure the user has appropriate MySQL permissions\n\n4. **Path Resolution**\nIf you encounter an error \"Could not connect to MCP server mcp-server-mysql\", explicitly set the path of all required binaries:\n```json\n{\n  \"env\": {\n    \"PATH\": \"/path/to/node/bin:/usr/bin:/bin\"\n  }\n}\n```\n\n5. **Authentication Issues**\n   - For MySQL 8.0+, ensure the server supports the `caching_sha2_password` authentication plugin\n   - Check if your MySQL user is configured with the correct authentication method\n   - Try creating a user with legacy authentication if needed:\n     ```sql\n     CREATE USER 'user'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to \nhttps://github.com/benborla/mcp-server-mysql\n\n### Development Setup\n\n1. Clone the repository\n2. Install dependencies: `pnpm install`\n3. Build the project: `pnpm run build`\n4. Run tests: `pnpm test`\n\n### Project Roadmap\n\nWe're actively working on enhancing this MCP server. Check our [CHANGELOG.md](./CHANGELOG.md) for details on planned features, including:\n\n- Enhanced query capabilities with prepared statements\n- Advanced security features\n- Performance optimizations\n- Comprehensive monitoring\n- Expanded schema information\n\nIf you'd like to contribute to any of these areas, please check the issues on GitHub or open a new one to discuss your ideas.\n\n### Submitting Changes\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/your-feature-name`\n3. Commit your changes: `git commit -am 'Add some feature'`\n4. Push to the branch: `git push origin feature/your-feature-name`\n5. Submit a pull request\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    },
    "zilliztech--mcp-server-milvus": {
      "owner": "zilliztech",
      "name": "mcp-server-milvus",
      "url": "https://github.com/zilliztech/mcp-server-milvus",
      "imageUrl": "/freedevtools/mcp/pfp/zilliztech.webp",
      "description": "Connects LLM applications to the Milvus vector database, enabling effective search and data management for AI workflows. Provides a standardized protocol for accessing vector data and integrating tools.",
      "stars": 184,
      "forks": 49,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T08:33:42Z",
      "readme_content": "# MCP Server for Milvus\n\n> The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis repository contains a MCP server that provides access to [Milvus](https://milvus.io/) vector database functionality.\n\n\n\n## Prerequisites\n\nBefore using this MCP server, ensure you have:\n\n- Python 3.10 or higher\n- A running [Milvus](https://milvus.io/) instance (local or remote)\n- [uv](https://github.com/astral-sh/uv) installed (recommended for running the server)\n\n## Usage\n\nThe recommended way to use this MCP server is to run it directly with `uv` without installation. This is how both Claude Desktop and Cursor are configured to use it in the examples below.\n\nIf you want to clone the repository:\n\n```bash\ngit clone https://github.com/zilliztech/mcp-server-milvus.git\ncd mcp-server-milvus\n```\n\nThen you can run the server directly:\n\n```bash\nuv run src/mcp_server_milvus/server.py --milvus-uri http://localhost:19530\n```\n\nAlternatively you can change the .env file in the `src/mcp_server_milvus/` directory to set the environment variables and run the server with the following command:\n\n```bash\nuv run src/mcp_server_milvus/server.py\n```\n\n### Important: the .env file will have higher priority than the command line arguments.\n\n### Running Modes\n\nThe server supports two running modes: **stdio** (default) and **SSE** (Server-Sent Events).\n\n### Stdio Mode (Default)\n\n- **Description**: Communicates with the client via standard input/output. This is the default mode if no mode is specified.\n\n- Usage:\n\n  ```bash\n  uv run src/mcp_server_milvus/server.py --milvus-uri http://localhost:19530\n  ```\n\n### SSE Mode\n\n- **Description**: Uses HTTP Server-Sent Events for communication. This mode allows multiple clients to connect via HTTP and is suitable for web-based applications.\n\n- **Usage:**\n\n  ```bash\n  uv run src/mcp_server_milvus/server.py --sse --milvus-uri http://localhost:19530 --port 8000\n  ```\n\n  - `--sse`: Enables SSE mode.\n  - `--port`: Specifies the port for the SSE server (default: 8000).\n\n- **Debugging in SSE Mode:**\n\n  If you want to debug in SSE mode, after starting the SSE service, enter the following command:\n\n  ```bash\n  mcp dev src/mcp_server_milvus/server.py\n  ```\n\n  The output will be similar to:\n\n  ```plaintext\n  % mcp dev src/mcp_server_milvus/merged_server.py\n  Starting MCP inspector...\n  ⚙️ Proxy server listening on port 6277\n  🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n  ```\n\n  You can then access the MCP Inspector at `http://127.0.0.1:6274` for testing.\n\n## Supported Applications\n\nThis MCP server can be used with various LLM applications that support the Model Context Protocol:\n\n- **Claude Desktop**: Anthropic's desktop application for Claude\n- **Cursor**: AI-powered code editor with MCP support\n- **Custom MCP clients**: Any application implementing the MCP client specification\n\n## Usage with Claude Desktop\n\n### Configuration for Different Modes\n\n#### SSE Mode Configuration\n\nFollow these steps to configure Claude Desktop for SSE mode:\n\n1. Install Claude Desktop from https://claude.ai/download.\n2. Open your Claude Desktop configuration file:\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n3. Add the following configuration for SSE mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"milvus-sse\": {\n      \"url\": \"http://your_sse_host:port/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Restart Claude Desktop to apply the changes.\n\n#### Stdio Mode Configuration\n\nFor stdio mode, follow these steps:\n\n1. Install Claude Desktop from https://claude.ai/download.\n2. Open your Claude Desktop configuration file:\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n3. Add the following configuration for stdio mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"milvus\": {\n      \"command\": \"/PATH/TO/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/mcp-server-milvus/src/mcp_server_milvus\",\n        \"run\",\n        \"server.py\",\n        \"--milvus-uri\",\n        \"http://localhost:19530\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude Desktop to apply the changes.\n\n## Usage with Cursor\n\n[Cursor also supports MCP](https://docs.cursor.com/context/model-context-protocol) tools. You can integrate your Milvus MCP server with Cursor by following these steps:\n\n### Integration Steps\n\n1. Open `Cursor Settings` > `MCP`\n2. Click on `Add new global MCP server`\n3. After clicking, it will automatically redirect you to the `mcp.json` file, which will be created if it doesn’t exist\n\n### Configuring the `mcp.json` File\n\n#### For Stdio Mode:\n\nOverwrite the `mcp.json` file with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"milvus\": {\n      \"command\": \"/PATH/TO/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/mcp-server-milvus/src/mcp_server_milvus\",\n        \"run\",\n        \"server.py\",\n        \"--milvus-uri\",\n        \"http://127.0.0.1:19530\"\n      ]\n    }\n  }\n}\n```\n\n#### For SSE Mode:\n\n1. Start the service by running the following command:\n\n   ```bash\n   uv run src/mcp_server_milvus/server.py --sse --milvus-uri http://your_sse_host --port port\n   ```\n\n   > **Note**: Replace `http://your_sse_host` with your actual SSE host address and `port` with the specific port number you’re using.\n\n2. Once the service is up and running, overwrite the `mcp.json` file with the following content:\n\n   ```json\n   {\n       \"mcpServers\": {\n         \"milvus-sse\": {\n           \"url\": \"http://your_sse_host:port/sse\",\n           \"disabled\": false,\n           \"autoApprove\": []\n         }\n       }\n   }\n   ```\n\n### Completing the Integration\n\nAfter completing the above steps, restart Cursor or reload the window to ensure the configuration takes effect.\n\n## Verifying the Integration\n\nTo verify that Cursor has successfully integrated with your Milvus MCP server:\n\n1. Open `Cursor Settings` > `MCP`\n2. Check if \"milvus\" or \"milvus-sse\" appear in the list（depending on the mode you have chosen）\n3. Confirm that the relevant tools are listed (e.g., milvus_list_collections, milvus_vector_search, etc.)\n4. If the server is enabled but shows an error, check the Troubleshooting section below\n\n## Available Tools\n\nThe server provides the following tools:\n\n### Search and Query Operations\n\n- `milvus_text_search`: Search for documents using full text search\n\n  - Parameters:\n    - `collection_name`: Name of collection to search\n    - `query_text`: Text to search for\n    - `limit`: The maximum number of results to return (default: 5)\n    - `output_fields`: Fields to include in results\n    - `drop_ratio`: Proportion of low-frequency terms to ignore (0.0-1.0)\n- `milvus_vector_search`: Perform vector similarity search on a collection\n  - Parameters:\n    - `collection_name`: Name of collection to search\n    - `vector`: Query vector\n    - `vector_field`: Field name for vector search (default: \"vector\")\n    - `limit`: The maximum number of results to return (default: 5)\n    - `output_fields`: Fields to include in results\n    - `filter_expr`: Filter expression\n    - `metric_type`: Distance metric (COSINE, L2, IP) (default: \"COSINE\")\n- `milvus_hybrid_search`: Perform hybrid search on a collection\n  - Parameters:\n    - `collection_name`: Name of collection to search\n    - `query_text`: Text query for search\n    - `text_field`: Field name for text search\n    - `vector`: Vector of the text query\n    - `vector_field`: Field name for vector search\n    - `limit`: The maximum number of results to return\n    - `output_fields`: Fields to include in results\n    - `filter_expr`: Filter expression\n- `milvus_query`: Query collection using filter expressions\n  - Parameters:\n    - `collection_name`: Name of collection to query\n    - `filter_expr`: Filter expression (e.g. 'age > 20')\n    - `output_fields`: Fields to include in results\n    - `limit`: The maximum number of results to return (default: 10)\n\n### Collection Management\n\n- `milvus_list_collections`: List all collections in the database\n\n- `milvus_create_collection`: Create a new collection with specified schema\n\n  - Parameters:\n    - `collection_name`: Name for the new collection\n    - `collection_schema`: Collection schema definition\n    - `index_params`: Optional index parameters\n\n- `milvus_load_collection`: Load a collection into memory for search and query\n\n  - Parameters:\n    - `collection_name`: Name of collection to load\n    - `replica_number`: Number of replicas (default: 1)\n\n- `milvus_release_collection`: Release a collection from memory\n  - Parameters:\n    - `collection_name`: Name of collection to release\n\n- `milvus_get_collection_info`: Lists detailed information like schema, properties, collection ID, and other metadata of a specific collection.\n  - Parameters:\n    - `collection_name`:  Name of the collection to get detailed information about\n\n### Data Operations\n\n- `milvus_insert_data`: Insert data into a collection\n\n  - Parameters:\n    - `collection_name`: Name of collection\n    - `data`: Dictionary mapping field names to lists of values\n\n- `milvus_delete_entities`: Delete entities from a collection based on filter expression\n  - Parameters:\n    - `collection_name`: Name of collection\n    - `filter_expr`: Filter expression to select entities to delete\n\n## Environment Variables\n\n- `MILVUS_URI`: Milvus server URI (can be set instead of --milvus-uri)\n- `MILVUS_TOKEN`: Optional authentication token\n- `MILVUS_DB`: Database name (defaults to \"default\")\n\n## Development\n\nTo run the server directly:\n\n```bash\nuv run server.py --milvus-uri http://localhost:19530\n```\n\n## Examples\n\n### Using Claude Desktop\n\n#### Example 1: Listing Collections\n\n```\nWhat are the collections I have in my Milvus DB?\n```\n\nClaude will then use MCP to check this information on your Milvus DB.\n\n```\nI'll check what collections are available in your Milvus database.\n\nHere are the collections in your Milvus database:\n\n1. rag_demo\n2. test\n3. chat_messages\n4. text_collection\n5. image_collection\n6. customized_setup\n7. streaming_rag_demo\n```\n\n#### Example 2: Searching for Documents\n\n```\nFind documents in my text_collection that mention \"machine learning\"\n```\n\nClaude will use the full-text search capabilities of Milvus to find relevant documents:\n\n```\nI'll search for documents about machine learning in your text_collection.\n\n> View result from milvus-text-search from milvus (local)\n\nHere are the documents I found that mention machine learning:\n[Results will appear here based on your actual data]\n```\n\n### Using Cursor\n\n#### Example: Creating a Collection\n\nIn Cursor, you can ask:\n\n```\nCreate a new collection called 'articles' in Milvus with fields for title (string), content (string), and a vector field (128 dimensions)\n```\n\nCursor will use the MCP server to execute this operation:\n\n```\nI'll create a new collection called 'articles' with the specified fields.\n\nCollection 'articles' has been created successfully with the following schema:\n- title: string\n- content: string\n- vector: float vector[128]\n```\n\n## Troubleshooting\n\n### Common Issues\n\n#### Connection Errors\n\nIf you see errors like \"Failed to connect to Milvus server\":\n\n1. Verify your Milvus instance is running: `docker ps` (if using Docker)\n2. Check the URI is correct in your configuration\n3. Ensure there are no firewall rules blocking the connection\n4. Try using `127.0.0.1` instead of `localhost` in the URI\n\n#### Authentication Issues\n\nIf you see authentication errors:\n\n1. Verify your `MILVUS_TOKEN` is correct\n2. Check if your Milvus instance requires authentication\n3. Ensure you have the correct permissions for the operations you're trying to perform\n\n#### Tool Not Found\n\nIf the MCP tools don't appear in Claude Desktop or Cursor:\n\n1. Restart the application\n2. Check the server logs for any errors\n3. Verify the MCP server is running correctly\n4. Press the refresh button in the MCP settings (for Cursor)\n\n### Getting Help\n\nIf you continue to experience issues:\n\n1. Check the [GitHub Issues](https://github.com/zilliztech/mcp-server-milvus/issues) for similar problems\n2. Join the [Zilliz Community Discord](https://discord.gg/zilliz) for support\n3. File a new issue with detailed information about your problem",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "milvus",
        "secure database",
        "databases secure",
        "server milvus"
      ],
      "category": "databases"
    },
    "zxfgds--mcp-toolkit": {
      "owner": "zxfgds",
      "name": "mcp-toolkit",
      "url": "https://github.com/zxfgds/mcp-toolkit",
      "imageUrl": "/freedevtools/mcp/pfp/zxfgds.webp",
      "description": "Interact with local systems, databases, and external services through file operations, database management, and GitHub integration. Perform secure and efficient commands and transactions to enhance AI applications with real-world capabilities.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-05T00:45:07Z",
      "readme_content": "# MCP Toolkit\n\nEnglish | [中文](README_zh.md)\n\n## Overview\n\nMCP Toolkit is a comprehensive Model Context Protocol (MCP) server implementation that provides a rich set of tools for AI assistants to interact with the local system, files, databases, and external services. It's designed to extend AI capabilities with real-world interactions while maintaining security and control.\n\n## Features\n\n- **File System Operations**\n  - Read and write files\n  - Create and delete directories\n  - Search files with regex patterns\n  - List directory contents\n  - Extract code definitions\n\n- **Database Integration**\n  - MySQL support\n  - PostgreSQL support\n  - Redis support\n  - Transaction management\n  - Parameterized queries\n\n- **GitHub Integration**\n  - Repository management\n  - Code search\n  - File operations\n  - Tree structure analysis\n\n- **Web Capabilities**\n  - Web page content extraction\n  - Brave search integration\n  - HTTP request handling\n  - Proxy support\n\n- **System Tools**\n  - Command execution\n  - Everything search integration\n  - Stack trace analysis\n  - Logging system\n\n## Installation\n\n```bash\nnpm install mcp-toolkit\n```\n\n## Configuration\n\nCreate a `config.json` file in your project:\n\n```json\n{\n  \"workspace\": {\n    \"rootPath\": \"/path/to/workspace\",\n    \"allowedPaths\": [\"/allowed/path1\", \"/allowed/path2\"]\n  },\n  \"network\": {\n    \"proxy\": \"http://proxy-server:port\"  // Optional\n  },\n  \"database\": {\n    \"mysql\": {\n      \"host\": \"localhost\",\n      \"port\": 3306,\n      \"user\": \"user\",\n      \"password\": \"password\",\n      \"database\": \"dbname\"\n    }\n    // Similar configuration for PostgreSQL and Redis\n  }\n}\n```\n\n## Usage\n\n```typescript\nimport { Server } from 'mcp-toolkit';\n\nconst server = new Server({\n  configPath: './config.json'\n});\n\nserver.start();\n```\n\n## Tool Details\n\n### File Operation Tools\n- `read_file`: Read file contents, supports text and binary files\n- `write_to_file`: Write or create files\n- `apply_diff`: Apply differential modifications to files\n- `insert_content`: Insert content at specified positions\n- `search_and_replace`: Search and replace file contents\n- `list_files`: List directory contents\n- `search_files`: Search files using regex\n- `list_code_definition_names`: Extract code definitions\n\n### Database Tools\n- `db_connect`: Connect to databases (MySQL/PostgreSQL/Redis)\n- `db_query`: Execute database queries\n- `db_begin_transaction`: Start transaction\n- `db_commit_transaction`: Commit transaction\n- `db_rollback_transaction`: Rollback transaction\n- `db_close`: Close database connection\n\n### GitHub Tools\n- `github_ls`: List repository contents\n- `github_tree`: Display repository tree structure\n- `github_search_repo`: Search repositories\n- `github_search_code`: Search code\n- `github_cat`: View file contents\n- `github_list_repos`: List user repositories\n- `github_create_repo`: Create repository\n- `github_update_repo`: Update repository settings\n- `github_delete_repo`: Delete repository\n\n### Web Tools\n- `read_webpage`: Extract webpage content\n- `brave_search`: Use Brave search\n- `http_request`: Send HTTP requests\n\n### System Tools\n- `execute_command`: Execute system commands\n- `everything_search`: Local file search\n- `logger`: Logging\n- `get_stack_trace`: Stack trace analysis\n\n## Security\n\n- Configurable workspace restrictions\n- Command execution controls\n- Database access management\n- Token-based authentication for external services\n\n## Contributing\n\nContributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "zxfgds",
        "database",
        "secure database",
        "databases secure",
        "access zxfgds"
      ],
      "category": "databases"
    },
    "zyy07--MCP": {
      "owner": "zyy07",
      "name": "MCP",
      "url": "https://github.com/zyy07/MCP",
      "imageUrl": "/freedevtools/mcp/pfp/zyy07.webp",
      "description": "Facilitates secure interaction with MySQL databases through natural language queries, enabling users to list tables, read data, and execute SQL commands efficiently. Enhances AI capabilities with structured database exploration and analysis.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-13T11:12:05Z",
      "readme_content": "# MCP",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "databases",
        "database",
        "mysql",
        "secure database",
        "databases secure",
        "database access"
      ],
      "category": "databases"
    }
  }
}