{
  "category": "image-and-video-generation",
  "categoryDisplay": "Image and Video Generation",
  "description": "",
  "totalRepositories": 106,
  "repositories": {
    "13rac1--videocapture-mcp": {
      "owner": "13rac1",
      "name": "videocapture-mcp",
      "url": "https://github.com/13rac1/videocapture-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/13rac1.webp",
      "description": "The Video Still Capture MCP server allows AI models to access and control webcams to take still images and adjust camera settings using OpenCV, without streaming video.",
      "stars": 11,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-20T23:19:39Z",
      "readme_content": "# Video Still Capture MCP\n\n**A Model Context Protocol server for accessing and controlling webcams via OpenCV**\n\n## Overview\n\nVideo Still Capture MCP is a Python implementation of the Model Context Protocol (MCP) that provides AI assistants with the ability to access and control webcams and video sources through OpenCV. This server exposes a set of tools that allow language models to capture images, manipulate camera settings, and manage video connections. There is no video capture.\n\n## Examples\n\nHere are some examples of the Video Still Capture  MCP server in action:\n\n### Orange Example\nLeft: Claude's view of the image | Right: Actual webcam capture\n:-------------------------:|:-------------------------:\n![Claude's view of orange](images/orange-claude.png) | ![Webcam capture of orange](images/orange-webcam.jpg)\n\n### Magnet Example\nLeft: Claude's view of the image | Right: Actual webcam capture\n:-------------------------:|:-------------------------:\n![Claude's view of magnet](images/magnet-claude.png) | ![Webcam capture of magnet](images/magnet-webcam.jpg)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10+\n- [OpenCV](https://opencv.org/) (`opencv-python`)\n- [MCP Python SDK](https://modelcontextprotocol.io/docs/)\n- [UV](https://astral.sh/uv/) (optional)\n\n### Installation from source\n\n```bash\ngit clone https://github.com/13rac1/videocapture-mcp.git\ncd videocapture-mcp\npip install -e .\n```\n\nRun the MCP server:\n\n```bash\nmcp dev videocapture_mcp.py\n```\n\n## Integrating with Claude for Desktop\n\n### macOS/Linux\n\nEdit your Claude Desktop configuration:\n\n```bash\n# Mac\nnano ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n# Linux\nnano ~/.config/Claude/claude_desktop_config.json \n```\n\nAdd this MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"VideoCapture \": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"numpy\",\n        \"--with\",\n        \"opencv-python\",\n        \"mcp\",\n        \"run\",\n        \"/ABSOLUTE_PATH/videocapture_mcp.py\"\n      ]\n    }\n  }\n}\n```\n\nEnsure you replace `/ABSOLUTE_PATH/videocapture-mcp` with the project's absolute path.\n\n### Windows\n\nEdit your Claude Desktop configuration:\n\n```powershell\nnano $env:AppData\\Claude\\claude_desktop_config.json\n```\n\nAdd this MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"VideoCapture\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"numpy\",\n        \"--with\",\n        \"opencv-python\",\n        \"mcp\",\n        \"run\",\n        \"C:\\ABSOLUTE_PATH\\videocapture-mcp\\videocapture_mcp.py\"\n      ]\n    }\n  }\n}\n```\n\nEnsure you replace `C:\\ABSOLUTE_PATH\\videocapture-mcp` with the project's absolute path.\n\n### Using the Installation Command\n\nAlternatively, you can use the `mcp` CLI to install the server:\n\n```bash\nmcp install videocapture_mcp.py\n```\n\nThis will automatically configure Claude Desktop to use your videocapture MCP server.\n\nOnce integrated, Claude will be able to access your webcam or video source when requested. Simply ask Claude to take a photo or perform any webcam-related task.\n\n## Features\n\n- **Quick Image Capture**: Capture a single image from a webcam without managing connections\n- **Connection Management**: Open, manage, and close camera connections\n- **Video Properties**: Read and adjust camera settings like brightness, contrast, and resolution\n- **Image Processing**: Basic image transformations like horizontal flipping\n\n## Tools Reference\n\n### `quick_capture`\n\nQuickly open a camera, capture a single frame, and close it.\n\n```python\nquick_capture(device_index: int = 0, flip: bool = False) -> Image\n```\n\n- **device_index**: Camera index (0 is usually the default webcam)\n- **flip**: Whether to horizontally flip the image\n- **Returns**: The captured frame as an Image object\n\n### `open_camera`\n\nOpen a connection to a camera device.\n\n```python\nopen_camera(device_index: int = 0, name: Optional[str] = None) -> str\n```\n\n- **device_index**: Camera index (0 is usually the default webcam)\n- **name**: Optional name to identify this camera connection\n- **Returns**: Connection ID for the opened camera\n\n### `capture_frame`\n\nCapture a single frame from the specified video source.\n\n```python\ncapture_frame(connection_id: str, flip: bool = False) -> Image\n```\n\n- **connection_id**: ID of the previously opened video connection\n- **flip**: Whether to horizontally flip the image\n- **Returns**: The captured frame as an Image object\n\n### `get_video_properties`\n\nGet properties of the video source.\n\n```python\nget_video_properties(connection_id: str) -> dict\n```\n\n- **connection_id**: ID of the previously opened video connection\n- **Returns**: Dictionary of video properties (width, height, fps, etc.)\n\n### `set_video_property`\n\nSet a property of the video source.\n\n```python\nset_video_property(connection_id: str, property_name: str, value: float) -> bool\n```\n\n- **connection_id**: ID of the previously opened video connection\n- **property_name**: Name of the property to set (width, height, brightness, etc.)\n- **value**: Value to set\n- **Returns**: True if successful, False otherwise\n\n### `close_connection`\n\nClose a video connection and release resources.\n\n```python\nclose_connection(connection_id: str) -> bool\n```\n\n- **connection_id**: ID of the connection to close\n- **Returns**: True if successful\n\n### `list_active_connections`\n\nList all active video connections.\n\n```python\nlist_active_connections() -> list\n```\n\n- **Returns**: List of active connection IDs\n\n## Example Usage\n\nHere's how an AI assistant might use the Webcam MCP server:\n\n1. **Take a quick photo**:\n   ```\n   I'll take a photo using your webcam.\n   ```\n   (The AI would call `quick_capture()` behind the scenes)\n\n2. **Open a persistent connection**:\n   ```\n   I'll open a connection to your webcam so we can take multiple photos.\n   ```\n   (The AI would call `open_camera()` and store the connection ID)\n\n3. **Adjust camera settings**:\n   ```\n   Let me increase the brightness of the webcam feed.\n   ```\n   (The AI would call `set_video_property()` with the appropriate parameters)\n\n## Advanced Usage\n\n### Resource Management\n\nThe server automatically manages camera resources, ensuring all connections are properly released when the server shuts down. For long-running applications, it's good practice to explicitly close connections when they're no longer needed.\n\n### Multiple Cameras\n\nIf your system has multiple cameras, you can specify the device index when opening a connection:\n\n```python\n# Open the second webcam (index 1)\nconnection_id = open_camera(device_index=1)\n```\n\n## Troubleshooting\n\n- **Camera Not Found**: Ensure your webcam is properly connected and not in use by another application\n- **Permission Issues**: Some systems require explicit permission to access the camera\n- **OpenCV Installation**: If you encounter issues with OpenCV, refer to the [official installation guide](https://docs.opencv.org/master/d5/de5/tutorial_py_setup_in_windows.html)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "videocapture",
        "mcp",
        "webcams",
        "videocapture mcp",
        "mcp video",
        "13rac1 videocapture"
      ],
      "category": "image-and-video-generation"
    },
    "396001000--ComfyUI_StoryDiffusion": {
      "owner": "396001000",
      "name": "ComfyUI_StoryDiffusion",
      "url": "https://github.com/396001000/ComfyUI_StoryDiffusion",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "ComfyUI_StoryDiffusion allows users to create visually enhanced stories by integrating advanced image generation features into the ComfyUI platform. It utilizes the StoryDiffusion and MS-Diffusion models for creative storytelling through visuals.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "comfyui_storydiffusion",
        "storydiffusion",
        "comfyui",
        "comfyui_storydiffusion allows",
        "comfyui_storydiffusion comfyui_storydiffusion",
        "storytelling visuals"
      ],
      "category": "image-and-video-generation"
    },
    "8bitsats--Grok-MCP": {
      "owner": "8bitsats",
      "name": "Grok-MCP",
      "url": "https://github.com/8bitsats/Grok-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/8bitsats.webp",
      "description": "MCP server for generating images using Grok's AI image generation capabilities, accepting text prompts and returning images as URLs or base64-encoded data. Supports multiple image generation requests and error handling, with configuration options for API keys.",
      "stars": 7,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:23Z",
      "readme_content": "Grok AI Image Generation MCP Server \n\n\nAI Image Generation MCP Server\n\nA server that connects to the xAI/Grok image generation API\nImplemented proper error handling with lazy API key initialization\nAdded support for multiple image generation (up to 10 images)\nAdded support for different response formats (URL or base64 JSON)\nDocker Support:\n\nAdded a Dockerfile to containerize the MCP server\nConfigured the Dockerfile with a dummy API key that can be overridden at runtime\nSet up proper layer caching for efficient builds\nMCP Tools Available:\n\ngenerate_image: Generate images using the Grok-2-image model\nset_api_key: Set the xAI API key at runtime if not provided via environment variable\nHow to Use\nYou can now generate images with prompts like:\n\n\"Generate an image of a cat in a space suit\"\n\"Create a picture of a futuristic city at night\"\nThe MCP server has been configured in your Claude desktop app, and the implementation handles API key management gracefully, allowing the server to start even without an API key initially set.\n\nIf you want to run the server in Docker, you can build and run it with:\n\ncd /Users/8bit/Documents/Cline/MCP/ai-image-generator\ndocker build -t grokart .\ndocker run -e XAI_API_KEY=your-api-key -p 8080:8080 grokart\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "grok",
        "mcp",
        "base64",
        "grok mcp",
        "using grok",
        "8bitsats grok"
      ],
      "category": "image-and-video-generation"
    },
    "Antipas--4oimage-mcp": {
      "owner": "Antipas",
      "name": "4oimage-mcp",
      "url": "https://github.com/Antipas/4oimage-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Antipas.webp",
      "description": "Generate and edit high-quality images using text prompts. Transform existing images or create new visuals and 3D characters with real-time updates and automatic viewing in the browser.",
      "stars": 4,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-26T22:44:07Z",
      "readme_content": "# 4o-image MCP Server\n\nAn MCP server implementation that integrates with 4o-image API, enabling LLMs and other AI systems to generate and edit images through a standardized protocol. Create high-quality art, 3D characters, and custom images using simple text prompts.\n\n<a href=\"https://glama.ai/mcp/servers/@Antipas/4oimage-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Antipas/4oimage-mcp/badge\" alt=\"mcp-4o-Image-Generator MCP server\" />\n</a>\n\n[![npm version](https://img.shields.io/npm/v/4oimage-mcp.svg)](https://www.npmjs.com/package/4oimage-mcp)\n[![Node.js Version](https://img.shields.io/node/v/4oimage-mcp.svg)](https://nodejs.org)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Features\n\n* **Text-to-Image Generation**: Create images from text descriptions with AI\n* **Image Editing**: Transform existing images using text prompts\n* **Real-time Progress Updates**: Get feedback on generation status\n* **Browser Integration**: Automatically open generated images in your default browser\n\n\n## Tools\n\n* **generateImage**\n  * Generate images based on text prompts with optional image editing\n  * Inputs:\n    * `prompt` (string, required): Text description of the desired image\n    * `imageBase64` (string, optional): Base64-encoded image for editing or style transfer\n\n## Configuration\n\n### Getting an API Key\n\n1. Register for an account at [4o-image.app](https://4o-image.app/dashboard/)\n2. Obtain your API key from the user dashboard\n3. Set the API key as an environment variable when running the server\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"4o-image\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"4oimage-mcp\"\n      ],\n      \"env\": {\n        \"API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n## Example Usage\n\nHere's an example of using this MCP server with Claude:\n\n```\nGenerate an image of a dog running on the beach at sunset\n```\n\nClaude will use the MCP server to generate the image, which will automatically open in your default browser. You'll also get a direct link to the image in Claude's response.\n\nFor image editing, you can include a base image and prompt Claude to modify it:\n\n```\nEdit this image to make the sky more dramatic with storm clouds\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. You are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "antipas",
        "images",
        "4oimage",
        "antipas 4oimage",
        "4oimage mcp",
        "generation antipas"
      ],
      "category": "image-and-video-generation"
    },
    "CLOUDWERX-DEV--DiffuGen": {
      "owner": "CLOUDWERX-DEV",
      "name": "DiffuGen",
      "url": "https://github.com/CLOUDWERX-DEV/DiffuGen",
      "imageUrl": "/freedevtools/mcp/pfp/CLOUDWERX-DEV.webp",
      "description": "Seamlessly generate AI images directly within development environments by leveraging local Stable Diffusion models and precise control over parameters. Integrate with MCP-compatible IDEs to facilitate creative development without disruption.",
      "stars": 15,
      "forks": 6,
      "license": "MIT License",
      "language": "Shell",
      "updated_at": "2025-08-25T15:46:42Z",
      "readme_content": "# DiffuGen - Advanced Local Image Generator with MCP Integration\n\n<p align=\"center\">\n  <img src=\"diffugen.png\" alt=\"DiffuGen Logo\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n  <em>Your AI art studio embedded directly in code. Generate, iterate, and perfect visual concepts through this powerful MCP server for Cursor, Windsurf, and other compatible IDEs, utilizing cutting-edge Flux and Stable Diffusion models without disrupting your development process.</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/CLOUDWERX-DEV/diffugen/stargazers\"><img src=\"https://img.shields.io/github/stars/CLOUDWERX-DEV/diffugen\" alt=\"Stars Badge\"/></a>\n  <a href=\"https://github.com/CLOUDWERX-DEV/diffugen/network/members\"><img src=\"https://img.shields.io/github/forks/CLOUDWERX-DEV/diffugen\" alt=\"Forks Badge\"/></a>\n  <a href=\"https://github.com/CLOUDWERX-DEV/diffugen/issues\"><img src=\"https://img.shields.io/github/issues/CLOUDWERX-DEV/diffugen\" alt=\"Issues Badge\"/></a>\n  <a href=\"https://github.com/CLOUDWERX-DEV/diffugen/blob/master/LICENSE\"><img src=\"https://img.shields.io/github/license/CLOUDWERX-DEV/diffugen\" alt=\"License Badge\"/></a>\n</p>\n\n> ⭐ **New**: Now includes OpenAPI server support and OpenWebUI OpenAPI Tools (OWUI Version 0.60.0 Required) integration for seamless image generation and display in chat interfaces! The OpenAPI is seperate from the MCP server and allowss for initigrations into your own projects!\n\n## 📃 Table of Contents\n\n- [Introduction](#-introduction)\n- [Understanding MCP and DiffuGen](#-understanding-mcp-and-diffugen)\n- [Features](#-features)\n- [System Requirements](#-system-requirements)\n- [Installation](#-installation)\n- [IDE Setup Instructions](#-ide-setup-instructions)\n- [Usage](#-usage)\n  - [OpenAPI Server Usage](#openapi-server-usage)\n  - [Default Parameters by Model](#default-parameters-by-model)\n  - [Asking a LLM to Generate Images](#asking-a-llm-to-generate-images)\n  - [Parameter Reference](#parameter-reference)\n  - [Model-Specific Parameter Recommendations](#model-specific-parameter-recommendations)\n  - [Default Parameter Changes](#default-parameter-changes)\n  - [Command Line Usage Notes](#command-line-usage-notes)\n- [Configuration](#️-configuration)\n  - [Configuration Approach](#configuration-approach)\n  - [Environment Variable Overrides](#environment-variable-overrides)\n  - [Setting IDE-Specific Configurations](#setting-ide-specific-configurations)\n  - [Key Configuration Elements](#key-configuration-elements)\n  - [IDE-Specific Options](#ide-specific-options)\n  - [Customizing Default Parameters](#customizing-default-parameters)\n  - [Updating Configuration Files](#updating-configuration-files)\n- [Advanced Usage](#-advanced-usage)\n  - [Using the OpenAPI Server](#using-the-openapi-server)\n- [License](#-license)\n- [Acknowledgments](#-acknowledgments)\n- [Contact](#-contact)\n\n## 🚀 Introduction\n\nDiffuGen is a powerful MCP-based image generation system that brings cutting-edge AI models directly into your development workflow. It seamlessly integrates both Flux models (Flux Schnell, Flux Dev) and Stable Diffusion variants (SDXL, SD3, SD1.5) into a unified interface, allowing you to leverage the unique strengths of each model family without switching tools. With comprehensive parameter control and multi-GPU support, DiffuGen scales from rapid concept sketches on modest hardware to production-quality visuals on high-performance systems.\n\nBuilt on top of the highly optimized [stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp) implementation, DiffuGen offers exceptional performance even on modest hardware while maintaining high-quality output.\n\n## 🧠 Understanding MCP and DiffuGen\n\n### What is MCP?\n\nMCP (Model Context Protocol) is a protocol that enables LLMs (Large Language Models) to access custom tools and services. In simple terms, an MCP client (like Cursor, Windsurf, Roo Code, or Cline) can make requests to MCP servers to access tools that they provide.\n\n### DiffuGen as an MCP Server\n\nDiffuGen functions as an MCP server that provides text-to-image generation capabilities. It implements the MCP protocol to allow compatible IDEs to send generation requests and receive generated images.\n\nThe server exposes two main tools:\n1. `generate_stable_diffusion_image`: Generate with Stable Diffusion models\n2. `generate_flux_image`: Generate with Flux models\n\n### Technical Architecture\n\nDiffuGen consists of several key components:\n\n- **setup-diffugen.sh**: The complete install utility and model downloader and manager\n- **diffugen.py**: The core Python script that implements the MCP server functionality and defines the generation tools\n- **diffugen.sh**: A shell script launcher that sets up the environment and launches the Python server\n- **diffugen.json**: Template configuration file for MCP integration with various IDEs (to be copied into IDE's MCP configuration)\n- **stable-diffusion.cpp**: The optimized C++ implementation of Stable Diffusion used for actual image generation\n\nThe system works by:\n1. Receiving prompt and parameter data from an MCP client\n2. Processing the request through the Python server\n3. Calling the stable-diffusion.cpp binary with appropriate parameters\n4. Saving the generated image to a configured output directory\n5. Returning the path and metadata of the generated image to the client\n\n### About stable-diffusion.cpp\n\n[stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp) is a highly optimized C++ implementation of the Stable Diffusion algorithm. Compared to the Python reference implementation, it offers:\n\n- Significantly faster inference speed (up to 3-4x faster)\n- Lower memory usage (works on GPUs with as little as 4GB VRAM)\n- Optimized CUDA kernels for NVIDIA GPUs\n- Support for various sampling methods and model formats\n- Support for model quantization for better performance\n- No Python dependencies for the core generation process\n\nThis allows DiffuGen to provide high-quality image generation with exceptional performance, even on modest hardware setups.\n\n## ✨ Features\n\n- **Multiple Model Support**: Generate images using various models including Flux Schnell, Flux Dev, SDXL, SD3, and SD1.5\n- **MCP Integration**: Seamlessly integrates with IDEs that support MCP (Cursor, Windsurf, Roo Code, Cline, etc.)\n- **OpenAPI Server**: Additional REST API interface for direct HTTP access to image generation capabilities\n- **Cross-Platform**: Works on Linux, macOS, and Windows (via native or WSL)\n- **Parameter Control**: Fine-tune your generations with controls for:\n  - Image dimensions (width/height)\n  - Sampling steps\n  - CFG scale\n  - Seed values\n  - Negative prompts (for SD models only, Flux does not support negative prompts.)\n  - Sampling methods\n- **CUDA Acceleration**: Utilizes GPU acceleration for faster image generation\n- **Natural Language Interface**: Generate images using simple natural language commands\n- **Smart Error Recovery**: Robust error handling with operation-aware recovery procedures\n- **User-Friendly Setup**: Interactive setup script with improved interrupt handling\n- **Resource Tracking**: Session-aware resource management for efficient cleanup\n- **Customizable Interface**: Support for custom ANSI art logos and visual enhancements\n\n## 💻 System Requirements\n\n### Minimum Requirements:\n\n- **CPU**: 4-core processor (Intel i5/AMD Ryzen 5 or equivalent)\n- **RAM**: 8GB system memory\n- **Storage**: 5GB free disk space (SSD preferred for faster model loading)\n- **Python**: 3.8 or newer\n- **GPU**: Integrated graphics or entry-level dedicated GPU (optional)\n- **Network**: Broadband connection for model downloads (5+ Mbps)\n\n### Recommended Requirements:\n\n- **CPU**: 8+ core processor (Intel i7/i9 or AMD Ryzen 7/9)\n- **RAM**: 16GB+ system memory\n- **GPU**: NVIDIA GPU with 6GB+ VRAM (RTX 2060 or better for optimal performance)\n- **Storage**: 20GB+ free SSD space\n- **Python**: 3.10 or newer (3.11 offers best performance)\n- **Network**: High-speed connection (20+ Mbps) for efficient model downloads\n\n## 📥 Installation\n\n### Automatic Installation (Recommended)\n\nThe easiest way to install DiffuGen is using the provided setup script:\n\n```bash\ngit clone https://github.com/CLOUDWERX-DEV/diffugen.git\ncd DiffuGen\nchmod +x diffugen.sh\nchmod +x setup_diffugen.sh\n./setup_diffugen.sh\n```\n\nFollow the interactive prompts to complete the installation.\n\nThe setup script will:\n- Install necessary dependencies\n- Clone and build stable-diffusion.cpp\n- Set up a Python virtual environment\n- Download selected models (Note: Some models require Clip\\VAE Models as well)\n- Configure file paths for your system\n\n### Manual Installation\n\nIf you prefer to install manually, follow these steps:\n\n1. Clone the repositories:\n\n```bash\ngit clone https://github.com/CLOUDWERX-DEV/diffugen.git\ncd DiffuGen\ngit clone --recursive https://github.com/leejet/stable-diffusion.cpp\n```\n\n2. Build stable-diffusion.cpp:\n\n```bash\ncd stable-diffusion.cpp\nmkdir -p build && cd build\n```\n\nWith CUDA:\n```bash\ncmake .. -DCMAKE_BUILD_TYPE=Release -DSD_CUDA=ON\nmake -j$(nproc)\ncd ../..\n```\n\nWithout CUDA:\n```bash\ncmake .. -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\ncd ../..\n```\n\n3. Create and activate a Python virtual environment:\n\n```bash\npython3 -m venv diffugen_env\nsource diffugen_env/bin/activate  # On Windows: diffugen_env\\Scripts\\activate\npip install -r requirements.txt\n```\n\n4. Download required models (structure shown below):\n\n```\nstable-diffusion.cpp/models/\n├── ae.sft                           # VAE model\n├── clip_l.safetensors               # CLIP model\n├── flux/\n│   ├── flux1-schnell-q8_0.gguf     # Flux Schnell model (default)\n│   └── flux1-dev-q8_0.gguf          # Flux Dev model\n├── sd3-medium.safetensors           # SD3 model\n├── sdxl-1.0-base.safetensors        # SDXL model\n├── sdxl_vae-fp16-fix.safetensors    # SDXL VAE\n├── t5xxl_fp16.safetensors           # T5 model\n└── v1-5-pruned-emaonly.safetensors  # SD1.5 model\n```\n\nYou can download the models from the following sources:\n\n```bash\n# Create model directories\nmkdir -p stable-diffusion.cpp/models/flux\n\n# Flux models\n# Flux Schnell - Fast generation model (Q8 Quantized,requires t5xxl, clip-l, vae)\ncurl -L https://huggingface.co/leejet/FLUX.1-schnell-gguf/resolve/main/flux1-schnell-q8_0.gguf -o stable-diffusion.cpp/models/flux/flux1-schnell-q8_0.gguf\n\n# Flux Dev - Development model with better quality (Q8 QUantized, requires t5xxl, clip-l, vae)\ncurl -L https://huggingface.co/leejet/FLUX.1-dev-gguf/resolve/main/flux1-dev-q8_0.gguf -o stable-diffusion.cpp/models/flux/flux1-dev-q8_0.gguf\n\n# Required models for Flux\n# T5XXL Text Encoder\ncurl -L https://huggingface.co/Sanami/flux1-dev-gguf/resolve/main/t5xxl_fp16.safetensors -o stable-diffusion.cpp/models/t5xxl_fp16.safetensors\n\n# CLIP-L Text Encoder\ncurl -L https://huggingface.co/Sanami/flux1-dev-gguf/resolve/main/clip_l.safetensors -o stable-diffusion.cpp/models/clip_l.safetensors\n\n# VAE for image decoding\ncurl -L https://huggingface.co/pretentioushorsefly/flux-models/resolve/main/models/vae/ae.safetensors -o stable-diffusion.cpp/models/ae.sft\n\n# Stable Diffusion models\n# SDXL 1.0 Base Model (requires sdxl-vae)\ncurl -L https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors -o stable-diffusion.cpp/models/sd_xl_base_1.0.safetensors\n\n# SDXL VAE (required for SDXL)\ncurl -L https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl_vae-fp16-fix.safetensors -o stable-diffusion.cpp/models/sdxl_vae-fp16-fix.safetensors\n\n# Stable Diffusion 1.5 (standalone)\ncurl -L https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors -o stable-diffusion.cpp/models/v1-5-pruned-emaonly.safetensors\n\n# Stable Diffusion 3 Medium (standalone)\ncurl -L https://huggingface.co/leo009/stable-diffusion-3-medium/resolve/main/sd3_medium_incl_clips_t5xxlfp16.safetensors -o stable-diffusion.cpp/models/sd3_medium_incl_clips_t5xxlfp16.safetensors\n```\n\nNote: Model download may take a long time depending on your internet connection. The SDXL model is approximately 6GB, SD3 is about 13GB, SD1.5 is around 4GB, and Flux models are 8-13GB each.\n\n5. Update file paths in configuration:\n\nSet shell script as Executable\n\n```\nchmod +x diffugen.sh\n```\n\n**Configuration Approach**:\nDiffuGen uses a single configuration file (`diffugen.json`) as the source of truth for all settings. The workflow is:\n\n1. Edit `diffugen.json` in the DiffuGen root directory with your desired settings\n2. Run option 5 in `setup_diffugen.sh` to automatically update paths in this file\n3. Copy the content of `diffugen.json` to your IDE's MCP configuration file\n\nThe file contains all necessary settings:\n- File paths (command, SD_CPP_PATH, models_dir, output_dir)\n- Default model parameters (steps, cfg_scale, sampling_method)\n- VRAM usage settings\n- Metadata for IDE integration\n\n```json\n{\n  \"mcpServers\": {\n    \"diffugen\": {\n      \"command\": \"/home/cloudwerxlab/Desktop/Servers/MCP/Tools/DiffuGen/diffugen.sh\",\n      \"args\": [],\n      \"env\": {\n        \"CUDA_VISIBLE_DEVICES\": \"0\",\n        \"SD_CPP_PATH\": \"path/to/stable-diffusion.cpp\",\n        \"default_model\": \"flux-schnell\"\n      },\n      \"resources\": {\n        \"models_dir\": \"path/to/stable-diffusion.cpp/models\",\n        \"output_dir\": \"path/to/outputs\",\n        \"vram_usage\": \"adaptive\"\n      },\n      \"metadata\": {\n        \"name\": \"DiffuGen\",\n        \"version\": \"1.0\",\n        \"description\": \"Your AI art studio embedded directly in code. Generate, iterate, and perfect visual concepts through this powerful MCP server for Cursor, Windsurf, and other compatible IDEs, utilizing cutting-edge Flux and Stable Diffusion models without disrupting your development process.\",\n        \"author\": \"CLOUDWERX LAB\",\n        \"homepage\": \"https://github.com/CLOUDWERX-DEV/diffugen\",\n        \"usage\": \"Generate images using two primary methods:\\n1. Standard generation: 'generate an image of [description]' with optional parameters:\\n   - model: Choose from flux-schnell (default), flux-dev, sdxl, sd3, sd15\\n   - dimensions: width and height (default: 512x512)\\n   - steps: Number of diffusion steps (default: 20, lower for faster generation)\\n   - cfg_scale: Guidance scale (default: 7.0, lower for more creative freedom)\\n   - seed: For reproducible results (-1 for random)\\n   - sampling_method: euler, euler_a (default), heun, dpm2, dpm++2s_a, dpm++2m, dpm++2mv2, lcm\\n   - negative_prompt: Specify elements to avoid in the image\\n2. Quick Flux generation: 'generate a flux image of [description]' for faster results with fewer steps (default: 4)\"\n      },\n      \"cursorOptions\": {\n        \"autoApprove\": true,\n        \"category\": \"Image Generation\",\n        \"icon\": \"🖼️\",\n        \"displayName\": \"DiffuGen\"\n      },\n      \"windsurfOptions\": {\n        \"displayName\": \"DiffuGen\",\n        \"icon\": \"🖼️\",\n        \"category\": \"Creative Tools\"\n      },\n      \"default_params\": {\n        \"steps\": {\n          \"flux-schnell\": 8,\n          \"flux-dev\": 20,\n          \"sdxl\": 20,\n          \"sd3\": 20,\n          \"sd15\": 20\n        },\n        \"cfg_scale\": {\n          \"flux-schnell\": 1.0,\n          \"flux-dev\": 1.0,\n          \"sdxl\": 7.0,\n          \"sd3\": 7.0, \n          \"sd15\": 7.0\n        },\n        \"sampling_method\": {\n          \"flux-schnell\": \"euler\",\n          \"flux-dev\": \"euler\",\n          \"sdxl\": \"euler\",\n          \"sd3\": \"euler\",\n          \"sd15\": \"euler\"\n        }\n      }\n    }\n  }\n}\n```\n\n## 🔧 IDE Setup Instructions\n\n### Setting up with Cursor\n\n1. Download and install [Cursor](https://cursor.sh)\n2. Go to Cursor Settings > MCP and click \"Add new global MCP server\"\n3. **Copy the contents of your DiffuGen's `diffugen.json` file** and paste it into `~/.cursor/mcp.json`\n4. Refresh MCP Servers in Settings > MCP\n5. Use DiffuGen by opening the AI chat panel (Ctrl+K or Cmd+K) and requesting image generation\n\n### Setting up with Windsurf\n\n1. Download and install [Windsurf](https://codeium.com/windsurf)\n2. Navigate to Windsurf > Settings > Advanced Settings or Command Palette > Open Windsurf Settings Page\n3. Scroll down to the Cascade section and click \"Add Server\" > \"Add custom server +\"\n4. **Copy the contents of your DiffuGen's `diffugen.json` file** and paste into `~/.codeium/windsurf/mcp_config.json`\n5. Use DiffuGen through the Cascade chat interface\n\n### Setting up with Roo Code\n\n1. Download and install [Roo Code](https://roo.ai)\n2. Locate the MCP configuration file for Roo Code\n3. **Copy the contents of your DiffuGen's `diffugen.json` file** into Roo Code's MCP configuration\n4. Use DiffuGen through the AI assistant feature\n\n### Setting up with Cline\n\n1. Download and install [Cline](https://cline.live)\n2. **Copy the contents of your DiffuGen's `diffugen.json` file** into Cline's MCP settings\n3. Use DiffuGen through the AI chat or command interface\n\n### Setting up with Claude in Anthropic Console\n\nClaude can use DiffuGen if you've set it up as an MCP server on your system. When asking Claude to generate images, be specific about using DiffuGen and provide the parameters you want to use.\n\n## 🎮 Usage\n\nTo start the DiffuGen server manually:\n\n```bash\ncd /path/to/diffugen\n./diffugen.sh\n```\n\nOr using Python directly:\n\n```bash\ncd /path/to/diffugen\npython -m diffugen\n```\n\nYou should see: `DiffuGen ready` when the server is successfully started.\n\n### OpenAPI Server Usage\n\nThe OpenAPI server provides a REST API interface for direct HTTP access to DiffuGen's image generation capabilities. This is in addition to the MCP integration and can be useful for:\n- Direct HTTP API access\n- Integration with other tools that don't support MCP\n- Custom applications that need programmatic access\n\nFor detailed setup instructions and advanced configuration options, see the [OpenAPI Integration Guide](OPENAPI_SETUP.md).\n\nTo start the OpenAPI server:\n```bash\npython diffugen_openapi.py\n```\n\nThe server can be configured to use a different host or port if needed. By default, it runs on:\n- Host: 0.0.0.0\n- Port: 8080\n\nThe server will be available at http://0.0.0.0:8080 with interactive documentation at http://0.0.0.0:8080/docs.\n\nGenerated images are saved to the `/output` directory by default. If this directory is not accessible, the server will automatically create an `output` directory in the current working directory. Images are served through the `/images` endpoint.\n\n#### OpenWebUI Integration\n\n1. Open OpenWebUI Settings (gear icon)\n2. Navigate to the \"Tools\" section\n3. Click the \"+\" button to add a new tool server\n4. Enter the following details:\n   - URL: http://0.0.0.0:5199\n   - API Key: (leave empty)\n5. Click \"Save\"\n\nOnce added, DiffuGen will appear in the available tools list when clicking the tools icon in the chat interface. The following endpoints will be available:\n- `generate_stable_image_generate_stable_post`: Generate with Stable Diffusion\n- `generate_flux_image_endpoint_generate_flux_post`: Generate with Flux Models\n- `list_models_models_get`: List Available Models\n\nExample using curl:\n```bash\ncurl -X POST \"http://0.0.0.0:5199/generate/flux\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"prompt\": \"A beautiful sunset\", \"model\": \"flux-schnell\"}'\n```\n\nExample using Python requests:\n```python\nimport requests\n\nresponse = requests.post(\n    \"http://0.0.0.0:5199/generate/flux\",\n    json={\n        \"prompt\": \"A beautiful sunset\",\n        \"model\": \"flux-schnell\"\n    }\n)\nresult = response.json()\n```\n\n### Default Parameters by Model\n\nEach model has specific default parameters optimized for best results:\n\n| Model | Default Steps | Default CFG Scale | Best For |\n|-------|--------------|-----------------|----------|\n| flux-schnell | 8 | 1.0 | Fast drafts, conceptual images |\n| flux-dev | 20 | 1.0 | Better quality flux generations |\n| sdxl | 20 | 7.0 | High-quality detailed images |\n| sd3 | 20 | 7.0 | Latest generation with good quality |\n| sd15 | 20 | 7.0 | Classic baseline model |\n\nThese default parameters can be customized by adding a `default_params` section to your IDE's MCP configuration file:\n\n```json\n\"default_params\": {\n  \"steps\": {\n    \"flux-schnell\": 12,  // Customize steps for better quality\n    \"sdxl\": 30           // Increase steps for more detailed SDXL images\n  },\n  \"cfg_scale\": {\n    \"sd15\": 9.0          // Higher cfg_scale for stronger prompt adherence\n  }\n}\n```\n\nYou only need to specify the parameters you want to override - any unspecified values will use the built-in defaults.\n\n> **Note**: For model-specific command line examples and recommendations, see [Model-Specific Parameter Recommendations](#model-specific-parameter-recommendations) section.\n\n### Asking a LLM to Generate Images\n\nHere are examples of how to ask an AI assistant to generate images with DiffuGen:\n\n#### Basic Requests:\n\n```\nGenerate an image of a cat playing with yarn\n```\n\n```\nCreate a picture of a futuristic cityscape with flying cars\n```\n\n#### With Model Specification:\n\n```\nGenerate an image of a medieval castle using the sdxl model\n```\n\n```\nCreate a flux image of a sunset over mountains\n```\n\n#### With Advanced Parameters:\n\n```\nGenerate an image of a cyberpunk street scene, model=flux-dev, width=768, height=512, steps=25, cfg_scale=1.0, seed=42\n```\n\n```\nCreate an illustration of a fantasy character with model=sd15, width=512, height=768, steps=30, cfg_scale=7.5, sampling_method=dpm++2m, negative_prompt=blurry, low quality, distorted\n```\n\n### Parameter Reference\n\nDiffuGen can be used from the command line with the following basic syntax:\n\n```bash\n./diffugen.sh \"Your prompt here\" [options]\n```\n\nExample:\n```bash\n./diffugen.sh \"A futuristic cityscape with flying cars\"\n```\n\nThis command generates an image using default parameters (flux-schnell model, 512x512 resolution, etc.) and saves it to the configured output directory.\n\nBelow are the parameters that can be used with DiffuGen (applicable to both MCP interface and command line):\n\n| Parameter | Description | Default | Valid Values | Command Line Flag |\n|-----------|-------------|---------|-------------|-------------------|\n| model | The model to use for generation | flux-schnell/sd15 | flux-schnell, flux-dev, sdxl, sd3, sd15 | --model |\n| width | Image width in pixels | 512 | 256-2048 | --width |\n| height | Image height in pixels | 512 | 256-2048 | --height |\n| steps | Number of diffusion steps | model-specific | 1-100 | --steps |\n| cfg_scale | Classifier-free guidance scale | model-specific | 0.1-30.0 | --cfg-scale |\n| seed | Random seed for reproducibility | -1 (random) | -1 or any integer | --seed |\n| sampling_method | Diffusion sampling method | euler | euler, euler_a, heun, dpm2, dpm++2s_a, dpm++2m, dpm++2mv2, lcm | --sampling-method |\n| negative_prompt | Elements to avoid in the image | \"\" (empty) | Any text string | --negative-prompt |\n| output_dir | Directory to save images | Config-defined | Valid path | --output-dir |\n\nThese parameters can be specified when asking an AI assistant to generate images or when using the command line interface. Parameters are passed in different formats depending on the interface:\n\n- **In MCP/AI Assistant**: `parameter=value` (e.g., `model=sdxl, width=768, height=512`)\n- **In Command Line**: `--parameter value` (e.g., `--model sdxl --width 768 --height 512`)\n\nThe default values are chosen to provide good results out-of-the-box with minimal waiting time. For higher quality images, consider increasing steps or switching to models like sdxl.\n\n### Model-Specific Parameter Recommendations\n\n> **Note**: These recommendations build on the [Default Parameters by Model](#default-parameters-by-model) section and provide practical examples.\n\nFor best results when using specific models via command line:\n\n#### Flux Models (flux-schnell, flux-dev)\n```bash\n# Flux-Schnell (fastest)\n./diffugen.sh \"Vibrant colorful abstract painting\" \\\n  --model flux-schnell \\\n  --cfg-scale 1.0 \\\n  --sampling-method euler \\\n  --steps 8\n\n# Flux-Dev (better quality)\n./diffugen.sh \"Detailed fantasy landscape with mountains and castles\" \\\n  --model flux-dev \\\n  --cfg-scale 1.0 \\\n  --sampling-method euler \\\n  --steps 20\n```\n\n#### Standard SD Models (sdxl, sd3, sd15)\n```bash\n# SDXL (highest quality)\n./diffugen.sh \"Hyperrealistic portrait of a Celtic warrior\" \\\n  --model sdxl \\\n  --cfg-scale 7.0 \\\n  --sampling-method dpm++2m \\\n  --steps 30\n\n# SD15 (classic model)\n./diffugen.sh \"Photorealistic landscape at sunset\" \\\n  --model sd15 \\\n  --cfg-scale 7.0 \\\n  --sampling-method euler_a \\\n  --steps 20\n```\n\n### Default Parameter Changes\n\nThe command-line interface of DiffuGen uses the following defaults if not otherwise specified in configuration:\n\n- Default Model: If not specified, function-appropriate models are used (flux-schnell for Flux functions, sd15 for SD functions)\n- Default Sampling Method: `euler` (best for Flux models)\n- Default CFG Scale: `1.0` for Flux models, `7.0` for standard SD models\n- Default Steps: `8` for flux-schnell, `20` for other models\n- Default Dimensions: 512x512 pixels\n\nWhen using the command line, you don't need to specify these parameters unless you want to override the defaults. If you frequently use specific parameters, consider adding them to your configuration file rather than specifying them on each command line.\n\n### Command Line Usage Notes\n\n- Generated images are saved to the configured output directory with filenames based on timestamp and parameters\n- You can generate multiple images in sequence by running the command multiple times\n- For batch processing, consider creating a shell script that calls DiffuGen with different parameters\n- To see all available command-line options, run `./diffugen.sh --help`\n- The same engine powers both the MCP interface and command-line tool, so quality and capabilities are identical\n\n## ⚙️ Configuration\n\n### Configuration Approach\n\nDiffuGen uses a single configuration approach centered around the `diffugen.json` file:\n\n1. **Primary Configuration File**: `diffugen.json` in the DiffuGen root directory is the single source of truth for all settings\n2. **IDE Integration**: Copy the contents of `diffugen.json` to your IDE's MCP configuration file\n3. **Environment Variables**: For advanced usage, you can override settings with environment variables\n\n### Environment Variable Overrides\n\nFor advanced usage, you can override settings using environment variables:\n\n- `SD_CPP_PATH`: Override the path to stable-diffusion.cpp\n- `DIFFUGEN_OUTPUT_DIR`: Override the output directory\n- `DIFFUGEN_DEFAULT_MODEL`: Override the default model\n- `DIFFUGEN_VRAM_USAGE`: Override VRAM usage settings\n- `CUDA_VISIBLE_DEVICES`: Control which GPUs are used for generation\n\n### Setting IDE-Specific Configurations\n\nDiffuGen allows you to have different configurations for different IDEs by using environment variables in each IDE's MCP configuration. This lets you maintain a single base `diffugen.json` while customizing parameters per IDE.\n\nThe configuration priority works as follows:\n1. Environment variables (highest priority)\n2. Settings from local `diffugen.json` file (base configuration)\n\n**Example: Different Output Directories for Different IDEs**\n\nFor Cursor (in `~/.cursor/mcp.json`):\n```json\n\"env\": {\n  \"CUDA_VISIBLE_DEVICES\": \"0\",\n  \"SD_CPP_PATH\": \"/path/to/stable-diffusion.cpp\",\n  \"DIFFUGEN_OUTPUT_DIR\": \"/cursor/specific/output/directory\",\n  \"default_model\": \"flux-schnell\"\n}\n```\n\nFor Windsurf (in `~/.codeium/windsurf/mcp_config.json`):\n```json\n\"env\": {\n  \"CUDA_VISIBLE_DEVICES\": \"0\",\n  \"SD_CPP_PATH\": \"/path/to/stable-diffusion.cpp\",\n  \"DIFFUGEN_OUTPUT_DIR\": \"/windsurf/specific/output/directory\",\n  \"default_model\": \"sdxl\"\n}\n```\n\n**Example: Different Default Models and VRAM Settings**\n\n```json\n\"env\": {\n  \"CUDA_VISIBLE_DEVICES\": \"0\",\n  \"SD_CPP_PATH\": \"/path/to/stable-diffusion.cpp\",\n  \"default_model\": \"flux-dev\", \n  \"DIFFUGEN_VRAM_USAGE\": \"maximum\"\n}\n```\n\nThis approach lets you customize DiffuGen's behavior per IDE while still using the same underlying installation.\n\n### Key Configuration Elements\n\n#### Command and Arguments\n\n- **command**: Full path to the `diffugen.sh` script (must be absolute path)\n- **args**: Additional command-line arguments to pass to the script (usually left empty)\n\n#### Environment Variables\n\n- **CUDA_VISIBLE_DEVICES**: Controls which GPUs are used for generation\n  - `\"0\"`: Use only the first GPU\n  - `\"1\"`: Use only the second GPU\n  - `\"0,1\"`: Use both first and second GPUs\n  - `\"-1\"`: Disable CUDA and use CPU only\n\n- **SD_CPP_PATH**: Path to the stable-diffusion.cpp installation directory\n  - This is used to locate the stable-diffusion.cpp binary and models\n\n- **default_model**: The default model to use when none is specified\n\n#### Resource Configuration\n\n- **models_dir**: Directory containing the model files\n  - Should point to the `models` directory inside your stable-diffusion.cpp installation\n\n- **output_dir**: Directory where generated images will be saved\n  - Must be writable by the user running DiffuGen\n\n- **vram_usage**: Controls VRAM usage strategy\n  - `\"adaptive\"`: Automatically adjust memory usage based on available VRAM\n  - `\"minimal\"`: Use minimal VRAM at the cost of speed\n  - `\"balanced\"`: Balance memory usage and speed (default)\n  - `\"maximum\"`: Use maximum available VRAM for best performance\n\n### IDE-Specific Options\n\nEach IDE has specific options you can customize in the `diffugen.json` file:\n\n#### Cursor Options\n\n```json\n\"cursorOptions\": {\n  \"autoApprove\": true,\n  \"category\": \"Image Generation\",\n  \"icon\": \"🖼️\",\n  \"displayName\": \"DiffuGen\"\n}\n```\n\n#### Windsurf Options\n\n```json\n\"windsurfOptions\": {\n  \"displayName\": \"DiffuGen\",\n  \"icon\": \"🖼️\",\n  \"category\": \"Creative Tools\"\n}\n```\n\n### Customizing Default Parameters\n\nYou can customize default parameters for each model in the `default_params` section:\n\n```json\n\"default_params\": {\n  \"steps\": {\n    \"flux-schnell\": 12,\n    \"sdxl\": 30\n  },\n  \"cfg_scale\": {\n    \"sd15\": 9.0\n  },\n  \"sampling_method\": {\n    \"flux-schnell\": \"euler\",\n    \"sdxl\": \"dpm++2m\"\n  }\n}\n```\n\n### Updating Configuration Files\n\nWhen using the automatic setup script, a properly configured `diffugen.json` file is created with the correct paths for your system when you run option 5. To integrate DiffuGen with your IDE:\n\n1. Run option 5 in `setup_diffugen.sh` to update paths in `diffugen.json`\n2. Copy the entire contents of the generated `diffugen.json` file\n3. Paste it into your IDE's MCP configuration file (e.g., `~/.cursor/mcp.json`)\n4. Restart your IDE to apply changes\n\nThe key advantage of this approach is a single source of truth for configuration, making it easier to maintain and update your DiffuGen setup.\n\n## 📃 Advanced Usage\n\nThe DiffuGen Python module can be imported and used programmatically in your own Python scripts:\n\n```python\nfrom diffugen import generate_image\n\n# Generate an image programmatically\nresult = generate_image(\n    prompt=\"A starry night over a quiet village\",\n    model=\"sdxl\",\n    width=1024,\n    height=768,\n    steps=30,\n    cfg_scale=7.0,\n    seed=42,\n    sampling_method=\"dpm++2m\",\n    negative_prompt=\"blurry, low quality\"\n)\n\nprint(f\"Image saved to: {result['file_path']}\")\n```\n\n### Using the OpenAPI Server\n\nYou can also use the OpenAPI server programmatically in your applications:\n\n```python\nimport requests\n\ndef generate_image_via_api(prompt, model=\"flux-schnell\", width=512, height=512):\n    response = requests.post(\n        \"http://0.0.0.0:5199/generate/flux\",\n        json={\n            \"prompt\": prompt,\n            \"model\": model,\n            \"width\": width,\n            \"height\": height\n        }\n    )\n    return response.json()\n\n# Example usage\nresult = generate_image_via_api(\n    prompt=\"A magical forest at night\",\n    model=\"flux-schnell\",\n    width=768,\n    height=512\n)\nprint(f\"Generated image: {result['file_path']}\")\n```\n\n## 🔍 Troubleshooting\n\n### Common Issues and Solutions\n\n1. **Missing models or incorrect paths**\n   - Ensure all model files are downloaded and placed in the correct directories\n   - Check that paths in the configuration file are correctly set\n   - Verify file permissions allow read access to model files\n\n2. **CUDA/GPU issues**\n   - Make sure your NVIDIA drivers are up-to-date\n   - Set `CUDA_VISIBLE_DEVICES` to target a specific GPU\n   - If running out of VRAM, try using a smaller model or reducing dimensions\n\n3. **Image quality issues**\n   - Increase steps for better quality (at the cost of generation time)\n   - Adjust CFG scale: higher for more prompt adherence, lower for creativity\n   - Try different sampling methods (dpm++2m often provides good results)\n   - Use more detailed prompts with specific style descriptions\n\n4. **File permission errors**\n   - Ensure the output directory is writable by the user running DiffuGen\n   - Check that all scripts have execution permissions (`chmod +x diffugen.sh`)\n\n### Getting Help\n\nIf you encounter issues not covered here, you can:\n- Check the GitHub repository for issues and solutions\n- Run with debug logging enabled: `DEBUG=1 ./diffugen.sh \"your prompt\"`\n- Contact the developers via GitHub issues\n\n## 🌟 Contributing\n\nContributions to DiffuGen are welcome! To contribute:\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\nPlease ensure your code follows the project's coding standards and includes appropriate tests.\n\n## 📄 License\n\nThis project is licensed under the Apache License - see the LICENSE file for details.\n\n* All models are licensed under their respective distribution and are not in any way licensed or provided by CLOUDWERX.DEV\n* HuggingFace.co is used to download models and is not affiliated in any way with CLOUDWERX.DEV\n\n## 🙏 Acknowledgments\n\n- [stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp) for the optimized C++ implementation\n- [Stability AI](https://stability.ai/) for Stable Diffusion models\n- [Black Forest Labs](https://blackforestlabs.ai/) for their Flux Models\n- [Hugging Face](https://huggingface.co/) for the download links\n- All contributors to the MCP protocol\n\n## 📬 Contact\n\n- GitHub: [CLOUDWERX-DEV](https://github.com/CLOUDWERX-DEV)\n- Website: [cloudwerx.dev](http://cloudwerx.dev)\n- Mail: [sysop@cloudwerx.dev](mailto:sysop@cloudwerx.dev)\n- Discord: [Join our server](https://discord.gg/SvZFuufNTQ)\n\n```\n                   ______   __   ___   ___         _______              \n                  |   _  \\ |__|.'  _|.'  _|.--.--.|   _   |.-----.-----.\n                  |.  |   \\|  ||   _||   _||  |  ||.  |___||  -__|     |\n                  |.  |    \\__||__|  |__|  |_____||.  |   ||_____|__|__|\n                  |:  1    /                      |:  1   |             \n                  |::.. . /                       |::.. . |             \n                  `------'                        `-------'             \n```\n\n<p align=\"center\">\n  Made with ❤️ by CLOUDWERX LAB\n</p> \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "cloudwerx",
        "dev",
        "generate ai",
        "ai images",
        "generation cloudwerx"
      ],
      "category": "image-and-video-generation"
    },
    "CLOUDWERX-DEV--gpt-image-1-mcp": {
      "owner": "CLOUDWERX-DEV",
      "name": "gpt-image-1-mcp",
      "url": "https://github.com/CLOUDWERX-DEV/gpt-image-1-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/CLOUDWERX-DEV.webp",
      "description": "Enables AI assistants to generate and edit images from text prompts, supporting both creation and modification of images using specified masks. Integrates with various MCP clients and provides flexible workflows for image handling, including automatic file saving and comprehensive error reporting.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-15T19:40:52Z",
      "readme_content": "<p align=\"center\">\n  <img src=\"logo.png\" alt=\"GPT Image 1 MCP Logo\" width=\"200\"/>\n</p>\n\n<h1 align=\"center\">@cloudwerxlab/gpt-image-1-mcp</h1>\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@cloudwerxlab/gpt-image-1-mcp\"><img src=\"https://img.shields.io/npm/v/@cloudwerxlab/gpt-image-1-mcp.svg\" alt=\"npm version\"></a>\n  <a href=\"https://www.npmjs.com/package/@cloudwerxlab/gpt-image-1-mcp\"><img src=\"https://img.shields.io/npm/dm/@cloudwerxlab/gpt-image-1-mcp.svg\" alt=\"npm downloads\"></a>\n  <a href=\"https://github.com/CLOUDWERX-DEV/gpt-image-1-mcp/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/CLOUDWERX-DEV/gpt-image-1-mcp.svg\" alt=\"license\"></a>\n  <a href=\"https://nodejs.org/\"><img src=\"https://img.shields.io/node/v/@cloudwerxlab/gpt-image-1-mcp.svg\" alt=\"node version\"></a>\n  <a href=\"https://cloudwerx.dev\"><img src=\"https://img.shields.io/badge/website-cloudwerx.dev-blue\" alt=\"Website\"></a>\n</p>\n\n<p align=\"center\">\n  A Model Context Protocol (MCP) server for generating and editing images using the OpenAI <code>gpt-image-1</code> model.\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/OpenAI-GPT--Image--1-6E46AE\" alt=\"OpenAI GPT-Image-1\">\n  <img src=\"https://img.shields.io/badge/MCP-Compatible-00A3E0\" alt=\"MCP Compatible\">\n</p>\n\n## 🚀 Quick Start\n\n<div align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@cloudwerxlab/gpt-image-1-mcp\"><img src=\"https://img.shields.io/badge/NPX-Ready-red.svg\" alt=\"NPX Ready\"></a>\n</div>\n\n<p align=\"center\">Run this MCP server directly using NPX without installing it. <a href=\"https://www.npmjs.com/package/@cloudwerxlab/gpt-image-1-mcp\">View on npm</a>.</p>\n\n```bash\nnpx -y @cloudwerxlab/gpt-image-1-mcp\n```\n\n<p align=\"center\">The <code>-y</code> flag automatically answers \"yes\" to any prompts that might appear during the installation process.</p>\n\n### 📋 Prerequisites\n\n<table>\n  <tr>\n    <td width=\"50%\" align=\"center\">\n      <img src=\"https://img.shields.io/badge/Node.js-v14+-339933?logo=node.js&logoColor=white\" alt=\"Node.js v14+\">\n      <p>Node.js (v14 or higher)</p>\n    </td>\n    <td width=\"50%\" align=\"center\">\n      <img src=\"https://img.shields.io/badge/OpenAI-API_Key-412991?logo=openai&logoColor=white\" alt=\"OpenAI API Key\">\n      <p>OpenAI API key with access to gpt-image-1</p>\n    </td>\n  </tr>\n</table>\n\n### 🔑 Environment Variables\n\n<table>\n  <tr>\n    <th>Variable</th>\n    <th>Required</th>\n    <th>Description</th>\n  </tr>\n  <tr>\n    <td><code>OPENAI_API_KEY</code></td>\n    <td>✅ Yes</td>\n    <td>Your OpenAI API key with access to the gpt-image-1 model</td>\n  </tr>\n  <tr>\n    <td><code>GPT_IMAGE_OUTPUT_DIR</code></td>\n    <td>❌ No</td>\n    <td>Custom directory for saving generated images (defaults to user's Pictures folder under <code>gpt-image-1</code> subfolder)</td>\n  </tr>\n</table>\n\n### 💻 Example Usage with NPX\n\n<table>\n  <tr>\n    <th>Operating System</th>\n    <th>Command Line Example</th>\n  </tr>\n  <tr>\n    <td><strong>Linux/macOS</strong></td>\n    <td>\n\n```bash\n# Set your OpenAI API key\nexport OPENAI_API_KEY=sk-your-openai-api-key\n\n# Optional: Set custom output directory\nexport GPT_IMAGE_OUTPUT_DIR=/home/username/Pictures/ai-generated-images\n\n# Run the server with NPX\nnpx -y @cloudwerxlab/gpt-image-1-mcp\n```\n  </tr>\n  <tr>\n    <td><strong>Windows (PowerShell)</strong></td>\n    <td>\n\n```powershell\n# Set your OpenAI API key\n$env:OPENAI_API_KEY = \"sk-your-openai-api-key\"\n\n# Optional: Set custom output directory\n$env:GPT_IMAGE_OUTPUT_DIR = \"C:\\Users\\username\\Pictures\\ai-generated-images\"\n\n# Run the server with NPX\nnpx -y @cloudwerxlab/gpt-image-1-mcp\n```\n  </tr>\n  <tr>\n    <td><strong>Windows (Command Prompt)</strong></td>\n    <td>\n\n```cmd\n:: Set your OpenAI API key\nset OPENAI_API_KEY=sk-your-openai-api-key\n\n:: Optional: Set custom output directory\nset GPT_IMAGE_OUTPUT_DIR=C:\\Users\\username\\Pictures\\ai-generated-images\n\n:: Run the server with NPX\nnpx -y @cloudwerxlab/gpt-image-1-mcp\n```\n  </tr>\n</table>\n\n## 🔌 Integration with MCP Clients\n\n<div align=\"center\">\n  <img src=\"https://img.shields.io/badge/VS_Code-MCP_Extension-007ACC?logo=visual-studio-code&logoColor=white\" alt=\"VS Code MCP Extension\">\n  <img src=\"https://img.shields.io/badge/Roo-Compatible-FF6B6B\" alt=\"Roo Compatible\">\n  <img src=\"https://img.shields.io/badge/Cursor-Compatible-4C2889\" alt=\"Cursor Compatible\">\n  <img src=\"https://img.shields.io/badge/Augment-Compatible-6464FF\" alt=\"Augment Compatible\">\n  <img src=\"https://img.shields.io/badge/Windsurf-Compatible-00B4D8\" alt=\"Windsurf Compatible\">\n</div>\n\n### 🛠️ Setting Up in an MCP Client\n\n<table>\n  <tr>\n    <td>\n      <h4>Step 1: Locate Settings File</h4>\n      <ul>\n        <li>For <strong>Roo</strong>: <code>c:\\Users\\&lt;username&gt;\\AppData\\Roaming\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\mcp_settings.json</code></li>\n        <li>For <strong>VS Code MCP Extension</strong>: Check your extension documentation for the settings file location</li>\n        <li>For <strong>Cursor</strong>: <code>~/.config/cursor/mcp_settings.json</code> (Linux/macOS) or <code>%APPDATA%\\Cursor\\mcp_settings.json</code> (Windows)</li>\n        <li>For <strong>Augment</strong>: <code>~/.config/augment/mcp_settings.json</code> (Linux/macOS) or <code>%APPDATA%\\Augment\\mcp_settings.json</code> (Windows)</li>\n        <li>For <strong>Windsurf</strong>: <code>~/.config/windsurf/mcp_settings.json</code> (Linux/macOS) or <code>%APPDATA%\\Windsurf\\mcp_settings.json</code> (Windows)</li>\n      </ul>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <h4>Step 2: Add Configuration</h4>\n      <p>Add the following configuration to the <code>mcpServers</code> object:</p>\n    </td>\n  </tr>\n</table>\n\n```json\n{\n  \"mcpServers\": {\n    \"gpt-image-1\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@cloudwerxlab/gpt-image-1-mcp\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"PASTE YOUR OPEN-AI KEY HERE\",\n        \"GPT_IMAGE_OUTPUT_DIR\": \"OPTIONAL: PATH TO SAVE GENERATED IMAGES\"\n      }\n    }\n  }\n}\n```\n\n#### Example Configurations for Different Operating Systems\n\n<table>\n  <tr>\n    <th>Operating System</th>\n    <th>Example Configuration</th>\n  </tr>\n  <tr>\n    <td><strong>Windows</strong></td>\n    <td>\n\n```json\n{\n  \"mcpServers\": {\n    \"gpt-image-1\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@cloudwerxlab/gpt-image-1-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"sk-your-openai-api-key\",\n        \"GPT_IMAGE_OUTPUT_DIR\": \"C:\\\\Users\\\\username\\\\Pictures\\\\ai-generated-images\"\n      }\n    }\n  }\n}\n```\n  </tr>\n  <tr>\n    <td><strong>Linux/macOS</strong></td>\n    <td>\n\n```json\n{\n  \"mcpServers\": {\n    \"gpt-image-1\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@cloudwerxlab/gpt-image-1-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"sk-your-openai-api-key\",\n        \"GPT_IMAGE_OUTPUT_DIR\": \"/home/username/Pictures/ai-generated-images\"\n      }\n    }\n  }\n}\n```\n  </tr>\n</table>\n\n> **Note**: For Windows paths, use double backslashes (`\\\\`) to escape the backslash character in JSON. For Linux/macOS, use forward slashes (`/`).\n\n## ✨ Features\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <h3>🎨 Core Tools</h3>\n        <ul>\n          <li><code>create_image</code>: Generate new images from text prompts</li>\n          <li><code>create_image_edit</code>: Edit existing images with text prompts and masks</li>\n        </ul>\n      </td>\n      <td align=\"center\">\n        <h3>🚀 Key Benefits</h3>\n        <ul>\n          <li>Simple integration with MCP clients</li>\n          <li>Full access to OpenAI's gpt-image-1 capabilities</li>\n          <li>Streamlined workflow for AI image generation</li>\n        </ul>\n      </td>\n    </tr>\n  </table>\n</div>\n\n### 💡 Enhanced Capabilities\n\n<table>\n  <tr>\n    <td>\n      <h4>📊 Output & Formatting</h4>\n      <ul>\n        <li>✅ <strong>Beautifully Formatted Output</strong>: Responses include emojis and detailed information</li>\n        <li>✅ <strong>Automatic Image Saving</strong>: All generated images saved to disk for easy access</li>\n        <li>✅ <strong>Detailed Token Usage</strong>: View token consumption for each request</li>\n      </ul>\n    </td>\n    <td>\n      <h4>⚙️ Configuration & Handling</h4>\n      <ul>\n        <li>✅ <strong>Configurable Output Directory</strong>: Customize where images are saved</li>\n        <li>✅ <strong>File Path Support</strong>: Edit images using file paths instead of base64 encoding</li>\n        <li>✅ <strong>Comprehensive Error Handling</strong>: Detailed error reporting with specific error codes, descriptions, and troubleshooting suggestions</li>\n      </ul>\n    </td>\n  </tr>\n</table>\n\n## 🔄 How It Works\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th align=\"center\">🖼️ Image Generation</th>\n      <th align=\"center\">✏️ Image Editing</th>\n    </tr>\n    <tr>\n      <td>\n        <ol>\n          <li>Server receives prompt and parameters</li>\n          <li>Calls OpenAI API using gpt-image-1 model</li>\n          <li>API returns base64-encoded images</li>\n          <li>Server saves images to configured directory</li>\n          <li>Returns formatted response with paths and metadata</li>\n        </ol>\n      </td>\n      <td>\n        <ol>\n          <li>Server receives image, prompt, and optional mask</li>\n          <li>For file paths, reads and prepares files for API</li>\n          <li>Uses direct curl command for proper MIME handling</li>\n          <li>API returns base64-encoded edited images</li>\n          <li>Server saves images to configured directory</li>\n          <li>Returns formatted response with paths and metadata</li>\n        </ol>\n      </td>\n    </tr>\n  </table>\n</div>\n\n### 📁 Output Directory Behavior\n\n<table>\n  <tr>\n    <td width=\"50%\">\n      <h4>📂 Storage Location</h4>\n      <ul>\n        <li>🔹 <strong>Default Location</strong>: User's Pictures folder under <code>gpt-image-1</code> subfolder (e.g., <code>C:\\Users\\username\\Pictures\\gpt-image-1</code> on Windows)</li>\n        <li>🔹 <strong>Custom Location</strong>: Set via <code>GPT_IMAGE_OUTPUT_DIR</code> environment variable</li>\n        <li>🔹 <strong>Fallback Location</strong>: <code>./generated-images</code> (if Pictures folder can't be determined)</li>\n      </ul>\n    </td>\n    <td width=\"50%\">\n      <h4>🗂️ File Management</h4>\n      <ul>\n        <li>🔹 <strong>Directory Creation</strong>: Automatically creates output directory if it doesn't exist</li>\n        <li>🔹 <strong>File Naming</strong>: Images saved with timestamped filenames (e.g., <code>image-2023-05-05T12-34-56-789Z.png</code>)</li>\n        <li>🔹 <strong>Cross-Platform</strong>: Works on Windows, macOS, and Linux with appropriate Pictures folder detection</li>\n      </ul>\n    </td>\n  </tr>\n</table>\n\n## Installation & Usage\n\n### NPM Package\n\nThis package is available on npm: [@cloudwerxlab/gpt-image-1-mcp](https://www.npmjs.com/package/@cloudwerxlab/gpt-image-1-mcp)\n\nYou can install it globally:\n\n```bash\nnpm install -g @cloudwerxlab/gpt-image-1-mcp\n```\n\nOr run it directly with npx as shown in the Quick Start section.\n\n### Tool: `create_image`\n\nGenerates a new image based on a text prompt.\n\n#### Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `prompt` | string | Yes | The text description of the image to generate (max 32,000 chars) |\n| `size` | string | No | Image size: \"1024x1024\" (default), \"1536x1024\", or \"1024x1536\" |\n| `quality` | string | No | Image quality: \"high\" (default), \"medium\", or \"low\" |\n| `n` | integer | No | Number of images to generate (1-10, default: 1) |\n| `background` | string | No | Background style: \"transparent\", \"opaque\", or \"auto\" (default) |\n| `output_format` | string | No | Output format: \"png\" (default), \"jpeg\", or \"webp\" |\n| `output_compression` | integer | No | Compression level (0-100, default: 0) |\n| `user` | string | No | User identifier for OpenAI usage tracking |\n| `moderation` | string | No | Moderation level: \"low\" or \"auto\" (default) |\n\n#### Example\n\n```xml\n<use_mcp_tool>\n<server_name>gpt-image-1</server_name>\n<tool_name>create_image</tool_name>\n<arguments>\n{\n  \"prompt\": \"A futuristic city skyline at sunset, digital art\",\n  \"size\": \"1024x1024\",\n  \"quality\": \"high\",\n  \"n\": 1,\n  \"background\": \"auto\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### Response\n\nThe tool returns:\n- A formatted text message with details about the generated image(s)\n- The image(s) as base64-encoded data\n- Metadata including token usage and file paths\n\n### Tool: `create_image_edit`\n\nEdits an existing image based on a text prompt and optional mask.\n\n#### Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `image` | string, object, or array | Yes | The image(s) to edit (base64 string or file path object) |\n| `prompt` | string | Yes | The text description of the desired edit (max 32,000 chars) |\n| `mask` | string or object | No | The mask that defines areas to edit (base64 string or file path object) |\n| `size` | string | No | Image size: \"1024x1024\" (default), \"1536x1024\", or \"1024x1536\" |\n| `quality` | string | No | Image quality: \"high\" (default), \"medium\", or \"low\" |\n| `n` | integer | No | Number of images to generate (1-10, default: 1) |\n| `background` | string | No | Background style: \"transparent\", \"opaque\", or \"auto\" (default) |\n| `user` | string | No | User identifier for OpenAI usage tracking |\n\n#### Example with Base64 Encoded Image\n\n```xml\n<use_mcp_tool>\n<server_name>gpt-image-1</server_name>\n<tool_name>create_image_edit</tool_name>\n<arguments>\n{\n  \"image\": \"BASE64_ENCODED_IMAGE_STRING\",\n  \"prompt\": \"Add a small robot in the corner\",\n  \"mask\": \"BASE64_ENCODED_MASK_STRING\",\n  \"quality\": \"high\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### Example with File Path\n\n```xml\n<use_mcp_tool>\n<server_name>gpt-image-1</server_name>\n<tool_name>create_image_edit</tool_name>\n<arguments>\n{\n  \"image\": {\n    \"filePath\": \"C:/path/to/your/image.png\"\n  },\n  \"prompt\": \"Add a small robot in the corner\",\n  \"mask\": {\n    \"filePath\": \"C:/path/to/your/mask.png\"\n  },\n  \"quality\": \"high\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### Response\n\nThe tool returns:\n- A formatted text message with details about the edited image(s)\n- The edited image(s) as base64-encoded data\n- Metadata including token usage and file paths\n\n## 🔧 Troubleshooting\n\n<div align=\"center\">\n  <img src=\"https://img.shields.io/badge/Support-Available-brightgreen\" alt=\"Support Available\">\n</div>\n\n### 🚨 Common Issues\n\n<table>\n  <tr>\n    <th align=\"center\">Issue</th>\n    <th align=\"center\">Solution</th>\n  </tr>\n  <tr>\n    <td>\n      <h4>🖼️ MIME Type Errors</h4>\n      <p>Errors related to image format or MIME type handling</p>\n    </td>\n    <td>\n      <p>Ensure image files have the correct extension (.png, .jpg, etc.) that matches their actual format. The server uses file extensions to determine MIME types.</p>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <h4>🔑 API Key Issues</h4>\n      <p>Authentication errors with OpenAI API</p>\n    </td>\n    <td>\n      <p>Verify your OpenAI API key is correct and has access to the gpt-image-1 model. Check for any spaces or special characters that might have been accidentally included.</p>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <h4>🛠️ Build Errors</h4>\n      <p>Issues when building from source</p>\n    </td>\n    <td>\n      <p>Ensure you have the correct TypeScript version installed (v5.3.3 or compatible) and that your <code>tsconfig.json</code> is properly configured. Run <code>npm install</code> to ensure all dependencies are installed.</p>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <h4>📁 Output Directory Issues</h4>\n      <p>Problems with saving generated images</p>\n    </td>\n    <td>\n      <p>Check if the process has write permissions to the configured output directory. Try using an absolute path for <code>GPT_IMAGE_OUTPUT_DIR</code> if relative paths aren't working.</p>\n    </td>\n  </tr>\n</table>\n\n### 🔍 Error Handling and Reporting\n\nThe MCP server includes comprehensive error handling that provides detailed information when something goes wrong. When an error occurs:\n\n1. **Error Format**: All errors are returned with:\n   - A clear error message describing what went wrong\n   - The specific error code or type\n   - Additional context about the error when available\n\n2. **AI Assistant Behavior**: When using this MCP server with AI assistants:\n   - The AI will always report the full error message to help with troubleshooting\n   - The AI will explain the likely cause of the error in plain language\n   - The AI will suggest specific steps to resolve the issue\n\n## 📄 License\n\n<div align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"MIT License\"></a>\n</div>\n\n<p align=\"center\">\n  This project is licensed under the MIT License - see the <a href=\"LICENSE\">LICENSE</a> file for details.\n</p>\n\n<details>\n  <summary>License Summary</summary>\n\n  <p>The MIT License is a permissive license that is short and to the point. It lets people do anything with your code with proper attribution and without warranty.</p>\n\n  <p><strong>You are free to:</strong></p>\n  <ul>\n    <li>Use the software commercially</li>\n    <li>Modify the software</li>\n    <li>Distribute the software</li>\n    <li>Use and modify the software privately</li>\n  </ul>\n\n  <p><strong>Under the following terms:</strong></p>\n  <ul>\n    <li>Include the original copyright notice and the license notice in all copies or substantial uses of the work</li>\n  </ul>\n\n  <p><strong>Limitations:</strong></p>\n  <ul>\n    <li>The authors provide no warranty with the software and are not liable for any damages</li>\n  </ul>\n</details>\n\n## 🙏 Acknowledgments\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://openai.com/\">\n          <img src=\"https://img.shields.io/badge/OpenAI-412991?logo=openai&logoColor=white\" alt=\"OpenAI\">\n          <p>For providing the gpt-image-1 model</p>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/model-context-protocol/mcp\">\n          <img src=\"https://img.shields.io/badge/MCP-Protocol-00A3E0\" alt=\"MCP Protocol\">\n          <p>For the protocol specification</p>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/CLOUDWERX-DEV/gpt-image-1-mcp/issues\">Report Bug</a> •\n    <a href=\"https://github.com/CLOUDWERX-DEV/gpt-image-1-mcp/issues\">Request Feature</a> •\n    <a href=\"https://cloudwerx.dev\">Visit Our Website</a>\n  </p>\n</div>\n\n<div align=\"center\">\n  <p>\n    Developed with ❤️ by <a href=\"https://cloudwerx.dev\">CLOUDWERX</a>\n  </p>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cloudwerx",
        "mcp",
        "images",
        "image mcp",
        "gpt image",
        "generation cloudwerx"
      ],
      "category": "image-and-video-generation"
    },
    "CaullenOmdahl--pexels-mcp-server": {
      "owner": "CaullenOmdahl",
      "name": "pexels-mcp-server",
      "url": "https://github.com/CaullenOmdahl/pexels-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/CaullenOmdahl.webp",
      "description": "Access and retrieve photos, videos, and collections from Pexels using a standardized protocol. Supports search by various criteria and provides detailed information about media content.",
      "stars": 4,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-30T15:07:46Z",
      "readme_content": "# Pexels MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@CaullenOmdahl/pexels-mcp-server)](https://smithery.ai/server/@CaullenOmdahl/pexels-mcp-server)\n\nA Model Context Protocol (MCP) server that provides access to the Pexels API, allowing AI models to search for and retrieve photos, videos, and collections from Pexels.\n\n## Features\n\n- Search for photos and videos by query, orientation, size, and color\n- Access curated and popular content from Pexels\n- Browse Pexels collections\n- Get detailed information about specific photos and videos\n- Access content via tools or direct URI resources\n\n## Requirements\n\n- Node.js 18 or higher\n- A Pexels API key (get one at [https://www.pexels.com/api/](https://www.pexels.com/api/))\n\n## Local Development\n\n1. Clone the repository\n2. Install dependencies\n   ```bash\n   pnpm install\n   ```\n3. Build the project\n   ```bash\n   pnpm build\n   ```\n4. Run in development mode\n   ```bash\n   PEXELS_API_KEY=your_api_key pnpm dev\n   ```\n\n## Deploying to Smithery\n\nThis MCP server is ready to be deployed to Smithery. Follow these steps:\n\n1. Add the server to Smithery or claim an existing server\n2. Go to the Deployments tab (only visible to authenticated owners)\n3. Deploy the server\n4. When configuring the deployment, provide your Pexels API key in the configuration settings\n\n## API Usage\n\nThe server provides the following tools:\n\n### Photo Tools\n\n- `searchPhotos`: Search for photos by query (use descriptive keywords for relevant results, e.g., 'Thai hotel reception', 'red sports car driving', not just 'hotel' or 'car'; combine with parameters like `orientation`, `size`, `color`, and `locale` for refined results), with optional filters for orientation, size, color, locale (e.g., 'en-US', 'es-ES'), page, and results per page. Returns metadata including photo IDs and URLs, plus current API rate limit status.\n- `downloadPhoto`: Fetches a specific photo by its ID and desired size (optional, defaults to 'original'). Available sizes: 'original', 'large2x', 'large', 'medium', 'small', 'portrait', 'landscape', 'tiny'. Returns a direct download link for the requested image size, suggested filename (including size), and attribution information. The AI client should use its available local tools (like `curl` or PowerShell's `Invoke-WebRequest`) to download the photo using the provided link.\n- `getCuratedPhotos`: Retrieve a curated set of photos from Pexels, optionally paginated.\n- `getPhoto`: Retrieve detailed information about a specific photo by its ID.\n\n### Video Tools\n\n- `searchVideos`: Search for videos by query (use descriptive keywords for relevant results, e.g., 'drone footage beach sunset', 'time lapse city traffic', not just 'beach' or 'city'; combine with parameters like `orientation` and `size` for refined results), with optional filters for orientation, size, locale (e.g., 'en-US', 'es-ES'), page, and results per page. Returns metadata including video IDs and URLs, plus current API rate limit status.\n- `getPopularVideos`: Retrieve a list of popular videos from Pexels, with optional filters for dimensions, duration, page, and results per page.\n- `getVideo`: Retrieve detailed information about a specific video by its ID.\n- `downloadVideo`: Fetches a specific video by its ID and preferred quality (hd/sd). Returns a direct download link, suggested filename, and attribution information. The AI client should use its available local tools (like `curl` or PowerShell's `Invoke-WebRequest`) to download the video using the provided link.\n\n### Collection Tools\n\n- `getFeaturedCollections`: Retrieve a list of featured collections from Pexels, optionally paginated.\n- ~~`getMyCollections`~~: (Commented out in code) Requires OAuth 2.0 authentication, not supported by this server.\n- `getCollectionMedia`: Retrieve media items (photos or videos) from a specific collection by collection ID, with optional filters for type, sort order, page, and results per page.\n\n### Resources\n\nThe server provides the following URI-addressable resources:\n\n- `pexels-photo://{id}`: Access a specific photo by ID\n- `pexels-video://{id}`: Access a specific video by ID\n- `pexels-collection://{id}`: Access a specific collection by ID\n\n## Error Handling\n\nThe server attempts to provide informative error messages for common issues like invalid API keys, rate limits, or missing resources. Successful responses also include the current Pexels API rate limit status (remaining requests, reset time) in the output.\n\n## Attribution Requirements\n\nWhen using the Pexels API, you must follow their attribution requirements:\n\n- Always show a prominent link to Pexels (e.g., \"Photos provided by Pexels\")\n- Always credit photographers (e.g., \"Photo by John Doe on Pexels\")\n\n## License\n\nISC",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pexels",
        "photos",
        "videos",
        "pexels mcp",
        "pexels using",
        "caullenomdahl pexels"
      ],
      "category": "image-and-video-generation"
    },
    "Emmanuel97423--video_maker": {
      "owner": "Emmanuel97423",
      "name": "video_maker",
      "url": "https://github.com/Emmanuel97423/video_maker",
      "imageUrl": "/freedevtools/mcp/pfp/Emmanuel97423.webp",
      "description": "Create and manage video projects using an intuitive interface built with Next.js, facilitating video content creation and project management through streamlined workflows and powerful features.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-01T18:44:57Z",
      "readme_content": "pmThis is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "video_maker",
        "video",
        "projects",
        "video generation",
        "video_maker create",
        "video projects"
      ],
      "category": "image-and-video-generation"
    },
    "GMKR--mcp-imagegen": {
      "owner": "GMKR",
      "name": "mcp-imagegen",
      "url": "https://github.com/GMKR/mcp-imagegen",
      "imageUrl": "/freedevtools/mcp/pfp/GMKR.webp",
      "description": "Generate images from text prompts using advanced AI models. Supports both local and SSE endpoint configurations with specific provider requirements.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-18T01:58:09Z",
      "readme_content": "# MCP Image Generator\n\nA Model Context Protocol (MCP) server for generating images using Together AI's image generation models. This MCP Server can be run locally or using an SSE endpoint. \nThe MCP Image Generator required a provider, only \"Replicate\" and \"Together\" are supported currently. You need to set the `TOGETHER_API_KEY` or `REPLICATE_API_TOKEN` environment variables. and set the `PROVIDER` environment variable to \"replicate\" or \"together\"/\n\n## SSE Endpoint (Docker environment)\n\n### Clone the repository\n\n```bash\ngit clone https://github.com/gmkr/mcp-imagegen.git\ncd mcp-imagegen\n```\n\n### Build and run Docker container\n\n```bash\ndocker build -f Dockerfile.server -t mcp-imagegen .\ndocker run -p 3000:3000 mcp-imagegen\n```\n\n### Configuring with MCP Client\n```\n{\n  \"mcpServers\": {\n    \"imagegenerator\": {\n      \"url\": \"http://localhost:3000/sse\",\n      \"env\": {\n        \"PROVIDER\": \"replicate\",\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\"\n      }\n    }\n  }\n}\n```\nAdjust the `url` to the endpoint of the MCP server you want to use.  `provider` can be \"replicate\" or \"together\".\n\n## Running locally using stdio\n\n### Prerequisites\n\n- Node.js\n- Together AI API key or Replicate API token\n\n### Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/gmkr/mcp-imagegen.git\n   cd mcp-imagegen\n   ```\n\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n### Configuration\nCreate a configuration file for your MCP client. Here's an example configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"imagegenerator\": {\n      \"command\": \"pnpx\",\n      \"args\": [\n        \"-y\",\n        \"tsx\",\n        \"/path/to/mcp-imagegen/src/index.ts\"\n      ],\n      \"env\": {\n        \"PROVIDER\": \"replicate\",\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\"\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/mcp-imagegen` with the absolute path to your cloned repository and `your-replicate-api-token` with your actual Replicate API token.\n\n## Usage\n\nThe MCP Image Generator provides a tool called `generate_image` that can be used to generate images based on text prompts.\n\n### Tool: generate_image\n\nGenerates an image based on the provided prompt.\n\n**Parameters:**\n- `prompt` (string): The text prompt to generate an image for\n- `width` (number, optional): The width of the image to generate (default: 512)\n- `height` (number, optional): The height of the image to generate (default: 512)\n- `numberOfImages` (number, optional): The number of images to generate (default: 1)\n\n## Environment Variables\n- `PROVIDER`: The provider to use for image generation (default: \"replicate\")\n- `REPLICATE_API_TOKEN`: Your Replicate API token\n- `TOGETHER_API_KEY`: Your Together AI API key\n- `MODEL_NAME`: The model to use for image generation (default: \"black-forest-labs/flux-schnell\")\n\n## License\n\nMIT \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "imagegen",
        "mcp",
        "images",
        "mcp imagegen",
        "generate images",
        "imagegen generate"
      ],
      "category": "image-and-video-generation"
    },
    "Garoth--dalle-mcp": {
      "owner": "Garoth",
      "name": "dalle-mcp",
      "url": "https://github.com/Garoth/dalle-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Garoth.webp",
      "description": "Generate images from text prompts using OpenAI's DALL-E API. Edit existing images and create variations of them while ensuring API key validation for secure access.",
      "stars": 10,
      "forks": 8,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-14T03:51:46Z",
      "readme_content": "# DALL-E MCP Server\n\n<img src=\"assets/dall-e-logo.png\" alt=\"DALL-E MCP Logo\" width=\"256\" height=\"256\">\n\nAn MCP (Model Context Protocol) server for generating images using OpenAI's DALL-E API.\n\n## Features\n\n- Generate images using DALL-E 2 or DALL-E 3\n- Edit existing images (DALL-E 2 only)\n- Create variations of existing images (DALL-E 2 only)\n- Validate OpenAI API key\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Garoth/dalle-mcp.git\ncd dalle-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Important Note for Cline Users\n\nWhen using this DALL-E MCP server with Cline, it's recommended to save generated images in your current workspace directory by setting the `saveDir` parameter to match your current working directory. This ensures Cline can properly locate and display the generated images in your conversation.\n\nExample usage with Cline:\n```json\n{\n  \"prompt\": \"A tropical beach at sunset\",\n  \"saveDir\": \"/path/to/current/workspace\"\n}\n```\n\n\n## Usage\n\n### Running the Server\n\n```bash\n# Run the server\nnode build/index.js\n```\n\n### Configuration for Cline\n\nAdd the dall-e server to your Cline MCP settings file inside VSCode's settings (ex. ~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"dalle-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/dalle-mcp-server/build/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"SAVE_DIR\": \"/path/to/save/directory\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nMake sure to:\n1. Replace `/path/to/dalle-mcp-server/build/index.js` with the actual path to the built index.js file\n2. Replace `your-api-key-here` with your OpenAI API key\n\n### Available Tools\n\n#### generate_image\n\nGenerate an image using DALL-E based on a text prompt.\n\n```json\n{\n  \"prompt\": \"A futuristic city with flying cars and neon lights\",\n  \"model\": \"dall-e-3\",\n  \"size\": \"1024x1024\",\n  \"quality\": \"standard\",\n  \"style\": \"vivid\",\n  \"n\": 1,\n  \"saveDir\": \"/path/to/save/directory\",\n  \"fileName\": \"futuristic-city\"\n}\n```\n\nParameters:\n- `prompt` (required): Text description of the desired image\n- `model` (optional): DALL-E model to use (\"dall-e-2\" or \"dall-e-3\", default: \"dall-e-3\")\n- `size` (optional): Size of the generated image (default: \"1024x1024\")\n  - DALL-E 3: \"1024x1024\", \"1792x1024\", or \"1024x1792\"\n  - DALL-E 2: \"256x256\", \"512x512\", or \"1024x1024\"\n- `quality` (optional): Quality of the generated image, DALL-E 3 only (\"standard\" or \"hd\", default: \"standard\")\n- `style` (optional): Style of the generated image, DALL-E 3 only (\"vivid\" or \"natural\", default: \"vivid\")\n- `n` (optional): Number of images to generate (1-10, default: 1)\n- `saveDir` (optional): Directory to save the generated images (default: current directory or SAVE_DIR from .env). **For Cline users:** Setting this to your current workspace directory is recommended for proper image display.\n- `fileName` (optional): Base filename for the generated images without extension (default: \"dalle-{timestamp}\")\n\n#### edit_image\n\nEdit an existing image using DALL-E based on a text prompt.\n\n> **⚠️ Known Issue (March 18, 2025):** The DALL-E 2 image edit API currently has a bug where it sometimes ignores the prompt and returns the original image without any edits, even when using proper RGBA format images and masks. This issue has been reported in the [OpenAI community forum](https://community.openai.com/t/dall-e-2-image-edit-issue/668376/7). If you experience this issue, try using the `create_variation` tool instead, which seems to work more reliably.\n\n```json\n{\n  \"prompt\": \"Add a red hat\",\n  \"imagePath\": \"/path/to/image.png\",\n  \"mask\": \"/path/to/mask.png\",\n  \"model\": \"dall-e-2\",\n  \"size\": \"1024x1024\",\n  \"n\": 1,\n  \"saveDir\": \"/path/to/save/directory\",\n  \"fileName\": \"edited-image\"\n}\n```\n\nParameters:\n- `prompt` (required): Text description of the desired edits\n- `imagePath` (required): Path to the image to edit\n- `mask` (optional): Path to the mask image (white areas will be edited, black areas preserved)\n- `model` (optional): DALL-E model to use (currently only \"dall-e-2\" supports editing, default: \"dall-e-2\")\n- `size` (optional): Size of the generated image (default: \"1024x1024\")\n- `n` (optional): Number of images to generate (1-10, default: 1)\n- `saveDir` (optional): Directory to save the edited images (default: current directory or SAVE_DIR from .env). **For Cline users:** Setting this to your current workspace directory is recommended for proper image display.\n- `fileName` (optional): Base filename for the edited images without extension (default: \"dalle-edit-{timestamp}\")\n\n#### create_variation\n\nCreate variations of an existing image using DALL-E.\n\n```json\n{\n  \"imagePath\": \"/path/to/image.png\",\n  \"model\": \"dall-e-2\",\n  \"size\": \"1024x1024\",\n  \"n\": 4,\n  \"saveDir\": \"/path/to/save/directory\",\n  \"fileName\": \"image-variation\"\n}\n```\n\nParameters:\n- `imagePath` (required): Path to the image to create variations of\n- `model` (optional): DALL-E model to use (currently only \"dall-e-2\" supports variations, default: \"dall-e-2\")\n- `size` (optional): Size of the generated image (default: \"1024x1024\")\n- `n` (optional): Number of variations to generate (1-10, default: 1)\n- `saveDir` (optional): Directory to save the variation images (default: current directory or SAVE_DIR from .env). **For Cline users:** Setting this to your current workspace directory is recommended for proper image display.\n- `fileName` (optional): Base filename for the variation images without extension (default: \"dalle-variation-{timestamp}\")\n\n#### validate_key\n\nValidate the OpenAI API key.\n\n```json\n{}\n```\n\nNo parameters required.\n\n## Development\n\n## Testing Configuration\n\n**Note: The following .env configuration is ONLY needed for running tests, not for normal operation.**\n\nIf you're developing or running tests for this project, create a `.env` file in the root directory with your OpenAI API key:\n\n```\n# Required for TESTS ONLY: OpenAI API Key\nOPENAI_API_KEY=your-api-key-here\n\n# Optional: Default save directory for test images\n# If not specified, images will be saved to the current directory\n# SAVE_DIR=/path/to/save/directory\n```\n\nFor normal operation with Cline, configure your API key in the MCP settings JSON as described in the \"Adding to MCP Settings\" section above.\n\nYou can get your API key from [OpenAI's API Keys page](https://platform.openai.com/api-keys).\n\n### Running Tests\n\n```bash\n# Run basic tests\nnpm test\n\n# Run all tests including edit and variation tests\nnpm run test:all\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Run specific test by name\nnpm run test:name \"should validate API key\"\n```\n\nNote: Tests use real API calls and may incur charges on your OpenAI account.\n\n### Generating Test Images\n\nThe project includes a script to generate test images for development and testing:\n\n```bash\n# Generate a test image in the assets directory\nnpm run generate-test-image\n  ```\n\nThis will create a simple test image in the `assets` directory that can be used for testing the edit and variation features.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "images",
        "garoth",
        "generate images",
        "mcp generate",
        "images create"
      ],
      "category": "image-and-video-generation"
    },
    "GongRzhe--Image-Generation-MCP-Server": {
      "owner": "GongRzhe",
      "name": "Image-Generation-MCP-Server",
      "url": "https://github.com/GongRzhe/Image-Generation-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/GongRzhe.webp",
      "description": "Generate images from text prompts using the Replicate Flux model, enabling the creation of unique visuals tailored to specific specifications.",
      "stars": 40,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T20:18:52Z",
      "readme_content": "# Image Generation MCP Server\n![](https://badge.mcpx.dev?type=server 'MCP Server')\n\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Image-Generation-MCP-Server)](https://smithery.ai/server/@GongRzhe/Image-Generation-MCP-Server)\n\nThis MCP server provides image generation capabilities using the Replicate Flux model.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Image Generation MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Image-Generation-MCP-Server):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Image-Generation-MCP-Server --client claude\n```\n\n### Option 1: NPX Method (No Local Setup Required)\nYou can use the package directly from npm without installing it locally:\n\n```bash\n# No installation needed - npx will handle it\n```\n\n### Option 2: Local Installation\nIf you prefer a local installation:\n\n```bash\n# Global installation\nnpm install -g @gongrzhe/image-gen-server\n\n# Or local installation\nnpm install @gongrzhe/image-gen-server\n```\n\n## Setup\n\n### Configure Claude Desktop\n\nEdit your Claude Desktop configuration file:\n\n- On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n#### Option 1: NPX Configuration (Recommended)\nThis method runs the server directly from npm without needing local files:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-gen\": {\n      \"command\": \"npx\",\n      \"args\": [\"@gongrzhe/image-gen-server\"],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\",\n        \"MODEL\": \"alternative-model-name\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n#### Option 2: Local Installation Configuration\nIf you installed the package locally:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-gen\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/image-gen-server/build/index.js\"],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\",\n        \"MODEL\": \"alternative-model-name\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Get Your Replicate API Token\n\n1. Sign up/login at https://replicate.com\n2. Go to https://replicate.com/account/api-tokens\n3. Create a new API token\n4. Copy the token and replace `your-replicate-api-token` in the MCP settings\n\n![image](https://github.com/user-attachments/assets/583afa78-1a08-4eb5-9a37-decb95bd50c4)\n\n### Environment Variables\n\n- `REPLICATE_API_TOKEN` (required): Your Replicate API token for authentication\n- `MODEL` (optional): The Replicate model to use for image generation. Defaults to \"black-forest-labs/flux-schnell\"\n\n### Configuration Parameters\n\n- `disabled`: Controls whether the server is enabled (`false`) or disabled (`true`)\n- `autoApprove`: Array of tool names that can be executed without user confirmation. Empty array means all tool calls require confirmation.\n\n## Available Tools\n\n### generate_image\n\nGenerates images using the Flux model based on text prompts.\n\n![image](https://github.com/user-attachments/assets/766921ce-ca8e-4d68-866d-8c7b55b2e09d)\n\n![out-0 (1)](https://github.com/user-attachments/assets/83549b2e-525a-4ff9-825c-83ba74459575)\n\n#### Parameters\n\n- `prompt` (required): Text description of the image to generate\n- `seed` (optional): Random seed for reproducible generation\n- `aspect_ratio` (optional): Image aspect ratio (default: \"1:1\")\n- `output_format` (optional): Output format - \"webp\", \"jpg\", or \"png\" (default: \"webp\")\n- `num_outputs` (optional): Number of images to generate (1-4, default: 1)\n\n#### Example Usage\n\n```typescript\nconst result = await use_mcp_tool({\n  server_name: \"image-gen\",\n  tool_name: \"generate_image\",\n  arguments: {\n    prompt: \"A beautiful sunset over mountains\",\n    aspect_ratio: \"16:9\",\n    output_format: \"png\",\n    num_outputs: 1\n  }\n});\n```\n\nThe tool returns an array of URLs to the generated images.\n\n## 📜 License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "generate",
        "mcp",
        "replicate",
        "generate images",
        "image generation",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "Hzzy2O--flux-cloudfare-mcp": {
      "owner": "Hzzy2O",
      "name": "flux-cloudfare-mcp",
      "url": "https://github.com/Hzzy2O/flux-cloudfare-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Hzzy2O.webp",
      "description": "Provides high-quality image generation via the Flux model through a Cloudflare Worker API, enabling seamless integration into applications with customizable parameters for image output.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-17T01:14:19Z",
      "readme_content": "# Flux Cloudflare MCP\n\n![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![TypeScript](https://img.shields.io/badge/TypeScript-4.9+-blue)\n![Model Context Protocol](https://img.shields.io/badge/MCP-Enabled-purple)\n\nA powerful Model Context Protocol (MCP) server that provides AI assistants with the ability to generate images using [Black Forest Labs' Flux model](https://developer.cloudflare.com/ai-gateway/models/flux-1/) via a Cloudflare Worker API.\n\n[Installation](#installation) • [Features](#features) • [Usage](#usage) • [Documentation](#documentation) • [Contributing](#contributing)\n\n---\n\n## 🌟 Features\n\n- **🖼️ High-Quality Image Generation**: Access to Flux, a state-of-the-art image generation model\n- **🤖 Seamless AI Integration**: Enable AI assistants like Claude to generate images directly\n- **🎛️ Customizable Parameters**: Control aspect ratio, inference steps, and more\n- **🔌 MCP Compatible**: Works with any MCP client (Cursor, Claude Desktop, Cline, Zed, etc.)\n- **🔒 Local Processing**: All requests are processed securely through the Cloudflare Worker\n- **💬 Chat Completions**: Get text completions using the same API\n\n## 📦 Installation\n\n### Direct Usage with NPX\n\n```bash\nFLUX_API_TOKEN=your_token FLUX_API_URL=your_api_url npx -y flux-cloudflare-mcp\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/Hzzy2O/flux-cloudflare-mcp.git\ncd flux-cloudflare-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## 🚀 Setting Up Your Flux API\n\nThis MCP server requires a Flux API endpoint to function. You have two options for setting up the API:\n\n### Option 1: Deploy using snakeying/flux-api-worker (Recommended)\n\n[snakeying/flux-api-worker](https://github.com/snakeying/flux-api-worker) provides a simple and efficient Cloudflare Worker for accessing the Flux model:\n\n1. Fork the [flux-api-worker repository](https://github.com/snakeying/flux-api-worker)\n2. Deploy it to Cloudflare Workers:\n   - Create a new Worker in your Cloudflare dashboard\n   - Connect it to your forked repository\n   - Set up the required environment variables:\n     - `API_KEY`: Your chosen API key for authentication\n     - `CF_ACCOUNT_ID`: Your Cloudflare account ID\n     - `CF_API_TOKEN`: Your Cloudflare API token with Workers AI access\n     - `FLUX_MODEL`: The Flux model to use (default: \"@cf/black-forest-labs/flux-1-schnell\")\n3. Once deployed, your API will be available at `https://your-worker-name.your-subdomain.workers.dev`\n4. Use this URL as your `FLUX_API_URL` and your chosen API key as `FLUX_API_TOKEN`\n\n### Option 2: Deploy using aigem/cf-flux-remix\n\nFor a more feature-rich implementation with a web UI, you can use [aigem/cf-flux-remix](https://github.com/aigem/cf-flux-remix):\n\n1. Follow the installation instructions in the [cf-flux-remix repository](https://github.com/aigem/cf-flux-remix)\n2. Once deployed, your API will be available at your deployed URL\n3. Use this URL as your `FLUX_API_URL` and your configured API key as `FLUX_API_TOKEN`\n\n## 📚 Documentation\n\n### Available Tools\n\n#### `generate_image`\n\nGenerates an image based on a text prompt using the Flux model.\n\n```typescript\n{\n  prompt: string;                // Required: Text description of the image to generate\n  num_inference_steps?: number;  // Optional: Number of denoising steps (1-4) (default: 4)\n  aspect_ratio?: string;         // Optional: Aspect ratio (e.g., \"16:9\", \"4:3\") (default: \"1:1\")\n}\n```\n\n## 🔧 Usage\n\n### Cursor Integration\n\n#### Method 1: Using mcp.json\n\n1. Create or edit the `.cursor/mcp.json` file in your project directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"flux-cloudflare-mcp\": {\n      \"command\": \"env FLUX_API_TOKEN=YOUR_TOKEN FLUX_API_URL=YOUR_API_URL npx\",\n      \"args\": [\"-y\", \"flux-cloudflare-mcp\"]\n    }\n  }\n}\n```\n\n2. Replace `YOUR_TOKEN` with your actual Flux API token and `YOUR_API_URL` with your API URL\n3. Restart Cursor to apply the changes\n\n#### Method 2: Using Cursor MCP Settings\n\n1. Open Cursor and go to Settings\n2. Navigate to the \"MCP\" or \"Model Context Protocol\" section\n3. Click \"Add Server\" or equivalent\n4. Enter the following command in the appropriate field:\n\n```\nenv FLUX_API_TOKEN=YOUR_TOKEN FLUX_API_URL=YOUR_API_URL npx -y flux-cloudflare-mcp\n```\n\n5. Replace `YOUR_TOKEN` with your actual Flux API token and `YOUR_API_URL` with your API URL\n6. Save the settings and restart Cursor if necessary\n\n### Claude Desktop Integration\nenv FLUX_API_TOKEN=YOUR_TOKEN FLUX_API_URL=YOUR_API_URL npx -y flux-cloudflare-mcp\n\n```json\n{\n  \"mcpServers\": {\n    \"flux-cloudflare-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"flux-cloudflare-mcp\"],\n      \"env\": {\n        \"FLUX_API_TOKEN\": \"YOUR_TOKEN\",\n        \"FLUX_API_URL\": \"YOUR_API_URL\"\n      }\n    }\n  }\n}\n```\n\n## 💻 Local Development\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/Hzzy2O/flux-cloudflare-mcp.git\ncd flux-cloudflare-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n## 🛠 Technical Stack\n\n* Model Context Protocol SDK - Core MCP functionality\n* Cloudflare Workers - Serverless API for image generation\n* TypeScript - Type safety and modern JavaScript features\n* Zod - Runtime type validation\n\n## ⚙️ Configuration\n\nThe server requires the following environment variables:\n\n- `FLUX_API_TOKEN`: Your API token for authentication with the Flux API\n- `FLUX_API_URL`: The URL of your deployed Flux API (from snakeying/flux-api-worker or aigem/cf-flux-remix)\n\n## 🔍 Troubleshooting\n\n### Common Issues\n\n#### Authentication Error\n- Ensure your `FLUX_API_TOKEN` is correctly set in the environment\n- Verify your token is valid by testing it with the Flux API directly\n\n#### API Connection Issues\n- Check that your Flux API (Cloudflare Worker) is running and accessible\n- Ensure your network allows connections to Cloudflare Workers\n\n#### Safety Filter Triggered\n- The model has a built-in safety filter that may block certain prompts\n- Try modifying your prompt to avoid potentially problematic content\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🔗 Resources\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io)\n- [Cloudflare Workers Documentation](https://developers.cloudflare.com/workers/)\n- [Flux Model Documentation](https://developer.cloudflare.com/ai-gateway/models/flux-1/)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [snakeying/flux-api-worker](https://github.com/snakeying/flux-api-worker) - Simple Flux API implementation\n- [aigem/cf-flux-remix](https://github.com/aigem/cf-flux-remix) - Feature-rich Flux API with web UI\n\n[![smithery badge](https://smithery.ai/badge/@Hzzy2O/flux-cloudfare-mcp)](https://smithery.ai/server/@Hzzy2O/flux-cloudfare-mcp)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cloudflare",
        "flux",
        "cloudfare",
        "flux cloudfare",
        "cloudfare mcp",
        "model cloudflare"
      ],
      "category": "image-and-video-generation"
    },
    "IA-Programming--mcp-images": {
      "owner": "IA-Programming",
      "name": "mcp-images",
      "url": "https://github.com/IA-Programming/mcp-images",
      "imageUrl": "/freedevtools/mcp/pfp/IA-Programming.webp",
      "description": "Fetch and process images from URLs and local file paths, handling automatic compression and MIME type retrieval. Images are returned as base64-encoded strings to facilitate integration and support parallel processing with robust error handling.",
      "stars": 13,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T05:36:12Z",
      "readme_content": "# MCP Server - Image\nA Model Context Protocol (MCP) server that provides tools for fetching and processing images from URLs, local file paths, and numpy arrays. The server includes a tool called fetch_images that returns images as base64-encoded strings along with their MIME types.\n\n## Support Us\n\nIf you find this project helpful and would like to support future projects, consider buying us a coffee! Your support helps us continue building innovative AI solutions.\n\n<a href=\"https://www.buymeacoffee.com/blazzmocompany\"><img src=\"https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=blazzmocompany&button_colour=40DCA5&font_colour=ffffff&font_family=Cookie&outline_colour=000000&coffee_colour=FFDD00\"></a>\n\nYour contributions go a long way in fueling our passion for creating intelligent and user-friendly applications.\n\n## Table of Contents\n- [Features](#features)\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Running the Server](#running-the-server)\n  - [Direct Method](#1-direct-method)\n  - [Configure for Windsurf/Cursor](#2-configure-for-windsurfcursor)\n- [Available Tools](#available-tools)\n  - [Usage Examples](#usage-examples)\n- [Debugging](#debugging)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Features\n- Fetch images from URLs (http/https)\n- Load images from local file paths\n- Specialized handling for large local images\n- Automatic image compression for large images (>1MB)\n- Parallel processing of multiple images\n- Proper MIME type mapping for different file extensions\n- Comprehensive error handling and logging\n## Prerequisites\n- Python 3.10+\n- uv package manager (recommended)\n## Installation\n1. Clone this repository\n2. Create and activate a virtual environment using uv:\n```bash\nuv venv\n# On Windows:\n.venv\\Scripts\\activate\n# On Unix/MacOS:\nsource .venv/bin/activate\n```\n3. Install dependencies using uv:\n```bash\nuv pip install -r requirements.txt\n```\n## Running the Server\nThere are two ways to run the MCP server:\n\n### 1. Direct Method\nTo start the MCP server directly:\n\n```bash\nuv run python mcp_image.py\n```\n### 2. Configure for Windsurf/Cursor\n#### Windsurf\nTo add this MCP server to Windsurf:\n\n1. Edit the configuration file at ~/.codeium/windsurf/mcp_config.json\n2. Add the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"image\": {\n      \"command\": \"uv\",\n        \"args\": [\"--directory\", \"/path/to/mcp-image\", \"run\", \"mcp_image.py\"]\n    }\n  }\n}\n```\n#### Cursor\nTo add this MCP server to Cursor:\n\n1. Open Cursor and go to *Settings* (Navbar → Cursor Settings)\n2. Navigate to *Features* → *MCP Servers*\n3. Click on + Add New MCP Server\n4. Enter the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"image\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/mcp-image\", \"run\", \"mcp_image.py\"]\n    }\n  }\n}\n```\n\n## Available Tools\nThe server provides the following tools:\n\n[fetch_images](mcp_image.py#L318): Fetch and process images from URLs or local file paths\nParameters:\nimage_sources: List of URLs or file paths to images\nReturns:\nList of processed images with base64 encoding and MIME types\n\n### Usage Examples\nYou can now use commands like:\n\n- \"Fetch these images: [list of URLs or file paths]\"\n- \"Load and process this local image: [file_path]\"\n\n#### Examples\n```\n# URL-only test\n[\n  \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Chocolate_%28blue_background%29.jpg/400px-Chocolate_%28blue_background%29.jpg\",\n  \"https://imgs.search.brave.com/Sz7BdlhBoOmU4wZjnUkvgestdwmzOzrfc3GsiMr27Ik/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9pbWdj/ZG4uc3RhYmxlZGlm/ZnVzaW9ud2ViLmNv/bS8yMDI0LzEwLzE4/LzJmOTY3NTViLTM0/YmQtNDczNi1iNDRh/LWJlMTVmNGM5MDBm/My5qcGc\",\n  \"https://shigacare.fukushi.shiga.jp/mumeixxx/img/main.png\"\n]\n\n# Mixed URL and local file test\n[\n  \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Chocolate_%28blue_background%29.jpg/400px-Chocolate_%28blue_background%29.jpg\",\n  \"C:\\\\Users\\\\username\\\\Pictures\\\\image1.jpg\",\n  \"https://imgs.search.brave.com/Sz7BdlhBoOmU4wZjnUkvgestdwmzOzrfc3GsiMr27Ik/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9pbWdj/ZG4uc3RhYmxlZGlm/ZnVzaW9ud2ViLmNv/bS8yMDI0LzEwLzE4/LzJmOTY3NTViLTM0/YmQtNDczNi1iNDRh/LWJlMTVmNGM5MDBm/My5qcGc\",\n  \"C:\\\\Users\\\\username\\\\Pictures\\\\image2.jpg\"\n]\n```\n\n## Debugging\nIf you encounter any issues:\n\n1. Check that all dependencies are installed correctly\n2. Verify that the server is running and listening for connections\n3. For local image loading issues, ensure the file paths are correct and accessible\n4. For \"Unsupported image type\" errors, verify the content type handling\n5. Look for any error messages in the server output\n## Contributing\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "base64",
        "images",
        "mcp",
        "mcp images",
        "process images",
        "programming mcp"
      ],
      "category": "image-and-video-generation"
    },
    "IncomeStreamSurfer--chatgpt-native-image-gen-mcp": {
      "owner": "IncomeStreamSurfer",
      "name": "chatgpt-native-image-gen-mcp",
      "url": "https://github.com/IncomeStreamSurfer/chatgpt-native-image-gen-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/IncomeStreamSurfer.webp",
      "description": "Generates and edits images using OpenAI's advanced image generation model based on text prompts. Supports image inpainting and variations, with customizable filenames for automated saving.",
      "stars": 15,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T22:33:09Z",
      "readme_content": "# OpenAI Image Generation MCP Server\n\nThis project implements an MCP (Model Context Protocol) server that provides tools for generating and editing images using OpenAI's `gpt-image-1` model via the official Python SDK.\n\n## Features\n\nThis MCP server provides the following tools:\n\n*   **`generate_image`**: Generates an image using OpenAI's `gpt-image-1` model based on a text prompt and saves it.\n    *   **Input Schema:**\n        ```json\n        {\n          \"type\": \"object\",\n          \"properties\": {\n            \"prompt\": { \"type\": \"string\", \"description\": \"The text description of the desired image(s).\" },\n            \"model\": { \"type\": \"string\", \"default\": \"gpt-image-1\", \"description\": \"The model to use (currently 'gpt-image-1').\" },\n            \"n\": { \"type\": [\"integer\", \"null\"], \"default\": 1, \"description\": \"The number of images to generate (Default: 1).\" },\n            \"size\": { \"type\": [\"string\", \"null\"], \"enum\": [\"1024x1024\", \"1536x1024\", \"1024x1536\", \"auto\"], \"default\": \"auto\", \"description\": \"Image dimensions ('1024x1024', '1536x1024', '1024x1536', 'auto'). Default: 'auto'.\" },\n            \"quality\": { \"type\": [\"string\", \"null\"], \"enum\": [\"low\", \"medium\", \"high\", \"auto\"], \"default\": \"auto\", \"description\": \"Rendering quality ('low', 'medium', 'high', 'auto'). Default: 'auto'.\" },\n            \"user\": { \"type\": [\"string\", \"null\"], \"default\": null, \"description\": \"An optional unique identifier representing your end-user.\" },\n            \"save_filename\": { \"type\": [\"string\", \"null\"], \"default\": null, \"description\": \"Optional filename (without extension). If None, a default name based on the prompt and timestamp is used.\" }\n          },\n          \"required\": [\"prompt\"]\n        }\n        ```\n    *   **Output:** `{\"status\": \"success\", \"saved_path\": \"path/to/image.png\"}` or error dictionary.\n\n*   **`edit_image`**: Edits an image or creates variations using OpenAI's `gpt-image-1` model and saves it. Can use multiple input images as reference or perform inpainting with a mask.\n    *   **Input Schema:**\n        ```json\n        {\n          \"type\": \"object\",\n          \"properties\": {\n            \"prompt\": { \"type\": \"string\", \"description\": \"The text description of the desired final image or edit.\" },\n            \"image_paths\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"description\": \"A list of file paths to the input image(s). Must be PNG. < 25MB.\" },\n            \"mask_path\": { \"type\": [\"string\", \"null\"], \"default\": null, \"description\": \"Optional file path to the mask image (PNG with alpha channel) for inpainting. Must be same size as input image(s). < 25MB.\" },\n            \"model\": { \"type\": \"string\", \"default\": \"gpt-image-1\", \"description\": \"The model to use (currently 'gpt-image-1').\" },\n            \"n\": { \"type\": [\"integer\", \"null\"], \"default\": 1, \"description\": \"The number of images to generate (Default: 1).\" },\n            \"size\": { \"type\": [\"string\", \"null\"], \"enum\": [\"1024x1024\", \"1536x1024\", \"1024x1536\", \"auto\"], \"default\": \"auto\", \"description\": \"Image dimensions ('1024x1024', '1536x1024', '1024x1536', 'auto'). Default: 'auto'.\" },\n            \"quality\": { \"type\": [\"string\", \"null\"], \"enum\": [\"low\", \"medium\", \"high\", \"auto\"], \"default\": \"auto\", \"description\": \"Rendering quality ('low', 'medium', 'high', 'auto'). Default: 'auto'.\" },\n            \"user\": { \"type\": [\"string\", \"null\"], \"default\": null, \"description\": \"An optional unique identifier representing your end-user.\" },\n            \"save_filename\": { \"type\": [\"string\", \"null\"], \"default\": null, \"description\": \"Optional filename (without extension). If None, a default name based on the prompt and timestamp is used.\" }\n          },\n          \"required\": [\"prompt\", \"image_paths\"]\n        }\n        ```\n    *   **Output:** `{\"status\": \"success\", \"saved_path\": \"path/to/image.png\"}` or error dictionary.\n\n## Prerequisites\n\n*   Python (3.8 or later recommended)\n*   pip (Python package installer)\n*   An OpenAI API Key (set directly in the script or via the `OPENAI_API_KEY` environment variable - **using environment variables is strongly recommended for security**).\n*   An MCP client environment (like the one used by Cline) capable of managing and launching MCP servers.\n\n## Installation\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/IncomeStreamSurfer/chatgpt-native-image-gen-mcp.git\n    cd chatgpt-native-image-gen-mcp\n    ```\n2.  **Set up a virtual environment (Recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n4.  **(Optional but Recommended) Set Environment Variable:**\n    Set the `OPENAI_API_KEY` environment variable with your OpenAI key instead of hardcoding it in the script. How you set this depends on your operating system.\n\n## Configuration (for Cline MCP Client)\n\nTo make this server available to your AI assistant (like Cline), add its configuration to your MCP settings file (e.g., `cline_mcp_settings.json`).\n\nFind the `mcpServers` object in your settings file and add the following entry:\n\n```json\n{\n  \"mcpServers\": {\n    // ... other server configurations ...\n\n    \"openai-image-gen-mcp\": {\n      \"autoApprove\": [\n        \"generate_image\",\n        \"edit_image\"\n      ],\n      \"disabled\": false,\n      \"timeout\": 180, // Increased timeout for potentially long image generation\n      \"command\": \"python\", // Or path to python executable if not in PATH\n      \"args\": [\n        // IMPORTANT: Replace this path with the actual absolute path\n        // to the openai_image_mcp.py file on your system\n        \"C:/path/to/your/cloned/repo/chatgpt-native-image-gen-mcp/openai_image_mcp.py\"\n      ],\n      \"env\": {\n        // If using environment variables for the API key:\n        // \"OPENAI_API_KEY\": \"YOUR_API_KEY_HERE\"\n      },\n      \"transportType\": \"stdio\"\n    }\n\n    // ... other server configurations ...\n  }\n}\n```\n\n**Important:** Replace `C:/path/to/your/cloned/repo/` with the correct absolute path to where you cloned this repository on your machine. Ensure the path separator is correct for your operating system (e.g., use backslashes `\\` on Windows). If you set the API key via environment variable, you can remove it from the script and potentially add it to the `env` section here if your MCP client supports it.\n\n## Running the Server\n\nYou don't typically need to run the server manually. The MCP client (like Cline) will automatically start the server using the `command` and `args` specified in the configuration file when one of its tools is called for the first time.\n\nIf you want to test it manually (ensure dependencies are installed and API key is available):\n```bash\npython openai_image_mcp.py\n```\n\n## Usage\n\nThe AI assistant interacts with the server using the `generate_image` and `edit_image` tools. Images are saved within an `ai-images` subdirectory created where the `openai_image_mcp.py` script is located. The tools return the absolute path to the saved image upon success.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "images",
        "image",
        "image generation",
        "image gen",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "InhiblabCore--mcp-image-compression": {
      "owner": "InhiblabCore",
      "name": "mcp-image-compression",
      "url": "https://github.com/InhiblabCore/mcp-image-compression",
      "imageUrl": "/freedevtools/mcp/pfp/InhiblabCore.webp",
      "description": "Optimizes images by compressing various formats for faster loading and improved user experience, while offering features like offline usage and batch processing. Supports smart compression to balance file size and visual quality based on image content.",
      "stars": 26,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T03:13:54Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/inhiblabcore-mcp-image-compression-badge.png)](https://mseep.ai/app/inhiblabcore-mcp-image-compression)\n\n# mcp-image-compression\n\n## Project Overview\n\nmcp-image-compression is a high-performance image compression microservice based on MCP (Modal Context Protocol) architecture. This service focuses on providing fast and high-quality image compression capabilities to help developers optimize image resources for websites and applications, improving loading speed and user experience.\n\n## Features\n\n- **Multi-format support**: Compress mainstream image formats including JPEG, PNG, WebP, AVIF\n- **Offline Usage**: No need to connect to the internet to use\n- **Smart compression**: Automatically select optimal compression parameters based on image content\n- **Batch processing**: Support parallel compression of multiple images for improved efficiency\n- **Quality control**: Customizable compression quality to balance file size and visual quality\n\n## TOOLS\n\n1. `image_compression`\n   - Image compression\n   - Inputs:\n     - `urls` (strings): URLs of images to compress\n     - `quality` (int): Quality of compression (0-100)\n     - `format` (string): Format of compressed image (e.g. \"jpeg\", \"png\", \"webp\", \"avif\")\n   - Returns: Compressed images url\n\n## Setup\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"Image compression\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@inhiblab-core/mcp-image-compression\"\n      ],\n      \"env\": {\n        \"IMAGE_COMPRESSION_DOWNLOAD_DIR\": \"<YOUR_DIR>\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Build\n\n```bash\ndocker build -t mcp-image-compression .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "compression",
        "compressing",
        "inhiblabcore",
        "image compression",
        "images compressing",
        "smart compression"
      ],
      "category": "image-and-video-generation"
    },
    "Kira-Pgr--PromptShopMCP": {
      "owner": "Kira-Pgr",
      "name": "PromptShopMCP",
      "url": "https://github.com/Kira-Pgr/PromptShopMCP",
      "imageUrl": "/freedevtools/mcp/pfp/Kira-Pgr.webp",
      "description": "Transforms images based on natural language commands, enabling users to edit photos by describing desired changes such as adding accessories or modifying backgrounds.",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T20:38:19Z",
      "readme_content": "# PromptShopMCP\n\n![](https://badge.mcpx.dev?type=server 'MCP Server')  \n\nEnglish | [中文](README_ZH.md)   \n\n\nA powerful MCP (Model Context Protocol) server that transforms images using simple text commands. Edit photos like a professional designer - just describe what you want in natural language!\n## Demo\nOriginal Image  \n<img src=\"https://github.com/user-attachments/assets/a987b4c4-3bba-4a52-a2a8-9f088868d857\" width=\"300\"/>  \n\nPrompt: **add a coat to the dog**  \n<img src=\"https://github.com/user-attachments/assets/6de3cdd1-a3b9-422b-95dd-12e2172f6f1d\" width=\"300\"/>  \n\nPrompt: **Add a hat to it**  \n<img src=\"https://github.com/user-attachments/assets/047289ca-f3d0-4d16-acf7-09d5af641c68\" width=\"300\"/>  \n \n\n##  Features\n\n- **Image Generation**: Create images from text prompts using Google's Gemini models\n- **Image Modification**: Transform existing images based on text instructions\n- **Background Removal**: Remove backgrounds from images using the remove.bg API\n- **Image Hosting**: Share generated images via FreeImage.host\n- **Resource Management**: Track and manage generated and uploaded images\n\n## Requirements\n\n- Python 3.11 or higher\n- Required API keys:\n  - Google Gemini API key [Get key](https://aistudio.google.com/apikey)\n  - FreeImage.host API key [Get key](https://freeimage.host/page/api)\n  - Remove.bg API key [Get key](https://www.remove.bg/dashboard#api-key)\n\n##  Installation\n\n1. Clone this repository:\n   ```sh\n   git https://github.com/Kira-Pgr/Image-Toolkit-MCP-Server.git\n   cd Image-Toolkit-MCP-Server\n   ```\n\n2. Install UV (if not already installed):\n   ```sh\n   # On macOS and Linux.\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   # On Windows.\n   powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n   # With pip.\n   pip install uv\n   ```\n\n3. Install dependencies using UV:\n   ```sh\n   uv venv --python=python3.11\n   source .venv/bin/activate #or .venv/Scripts/activate on Windows\n   uv pip install -r requirements.txt\n   ```\n\n##  Usage\n\n1. **Claude Desktop Integration**: Add the following configuration to your `claude_desktop_config.json` file to run the server directly from Claude Desktop:\n   ```json\n   \"PromptShopMCP\": {\n     \"command\": \"uv\",\n     \"args\": [\n       \"--directory\",\n       \"/project/dir/\",\n       \"run\",\n       \"mcp\",\n       \"run\",\n       \"/project/dir/server.py\"\n     ],\n     \"env\": {\n       \"GEMINI_API_KEY\": \"key\",\n       \"FREEIMAGE_API_KEY\": \"key\",\n       \"REMOVEBG_API_KEY\": \"key\"\n     }\n   }\n   ```\n   Note: Replace the placeholder `\"key\"` values with your actual API keys.\n2. **Cursor Integration**:    \n   **Linux/macOS**:\n  Modify the `cursor.sh` file to set your API keys and project directory.   \n  * In cursor settings, go to the \"MCP\" tab, click on `Add new MCP server`,   \n  * Name the server whatever you want, and set the command to `sh /absolute/path/to/cursor.sh`.   \n  * Wait for the server to start, and you can see the server and available tools.   \n  * Then when you use the agent, it would automatically detect whether use the tools.   \n  <img width=\"1240\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b41016fe-a0f8-4029-8f5d-82f25c606a65\" />\n  \n  **Windows**: \n  Modify the `cursor.bat` file to set your API keys and project directory.   \n  * In cursor settings, go to the \"MCP\" tab, click on `Add new MCP server`,   \n  * Name the server whatever you want, and set the command to `cmd /c C:\\absolute\\path\\to\\cursor.bat`.   \n  * Wait for the server to start, and you can see the server and available tools.   \n  * Then when you use the agent, it would automatically detect whether use the tools.   \n\n\n\n\n## Acknowledgements\n\n- [Google Gemini](https://aistudio.google.com/): For the image generation capabilities\n- [Remove.bg](https://www.remove.bg/): For background removal services\n- [FreeImage.host](https://freeimage.host/): For image hosting services\n- [MCP](https://modelcontextprotocol.io/introduction): For the Model Context Protocol\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "promptshopmcp",
        "images",
        "pgr",
        "pgr promptshopmcp",
        "promptshopmcp transforms",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "Letz-AI--letzai-mcp": {
      "owner": "Letz-AI",
      "name": "letzai-mcp",
      "url": "https://github.com/Letz-AI/letzai-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Letz-AI.webp",
      "description": "Create and upscale images based on prompts using the LetzAI MCP. This server integrates with the Claude Desktop App for seamless image generation.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-25T17:46:55Z",
      "readme_content": "# LetzAI MCP Setup Guide\n\nThis guide will walk you through the process of setting up and using the LetzAI MCP (Model Context Protocol) for image generation.\n\n## Prerequisites\n\nBefore you begin, ensure that you have the following:\n\n- **Node.js** installed on your system. You can download it from [Node.js official site](https://nodejs.org/).\n- **Claude Desktop App** installed. If you don't have it, download it from [Claude Desktop App](https://claude.app).\n- **LetzAI API Key**. You can obtain it by visiting [LetzAI API](https://letz.ai/docs/api).\n\n## Setup Steps\n\n### 1. Download the Git Folder\n\nDownload the repository containing the LetzAI MCP project and place it in a location outside of your Downloads folder. For example:\n\n```\nC:\\\\Users\\\\username\\\\desktop\n```\n\nAlternatively, you can use `git clone` to clone the repository:\n\n```bash\ngit clone <repository-url> C:\\\\Users\\\\username\\\\desktop\n```\n\n### 2. Install Dependencies\n\nNavigate to the project folder using your terminal or command prompt:\n\n```bash\ncd C:\\\\Users\\\\username\\\\desktop\n```\n\nRun the following command to install all required dependencies:\n\n```bash\nnpm install\n```\n\n### 3. Compile the Project\n\nAfter installing the dependencies, compile the TypeScript files into JavaScript using the following command:\n\n```bash\nnpx tsc\n```\n\nThis will generate the compiled JavaScript files in the `build` folder.\n\n### 4. Restart Claude App\n\nAfter running `npx tsc`, you must **restart** the Claude Desktop App for it to recognize the updated MCP configuration and compiled files.\n\n### 5. Set Up MCP Configuration in Claude Desktop App\n\n![open settings](settingsOpen.png)\n\n1. **Open the Claude Desktop App**.\n2. **Click on the Menu Icon** in the top-left corner.\n3. From the dropdown, select **File**.\n4. Navigate to **Settings**.\n5. Under the **Developer** section, you will see an option for **Edit Config**.\n   ![view developer settings](developerSettings.png)\n6. Click on **Edit Config** — this will open the configuration folder.\n7. Locate the file `claude_desktop_config.json` and edit it as needed.\n\n#### Windows Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"letzai\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\letzai-mcp\\\\build\\\\index.js\"\n      ],\n      \"env\": {\n        \"LETZAI_API_KEY\": \"<Your LetzAI API Key>\"\n      }\n    }\n  }\n}\n```\n\n#### Ubuntu Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"letzai\": {\n      \"command\": \"node\",\n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/letzai-mcp/build/index.js\"],\n      \"env\": {\n        \"LETZAI_API_KEY\": \"<Your LetzAI API Key>\"\n      }\n    }\n  }\n}\n```\n\n#### macOS Configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"letzai\": {\n      \"command\": \"node\",\n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/letzai-mcp/build/index.js\"],\n      \"env\": {\n        \"LETZAI_API_KEY\": \"<Your LetzAI API Key>\"\n      }\n    }\n  }\n}\n```\n\n### Configuration Explanation\n\n- **command**: The command to run the application. We use `node` to run the JavaScript file generated by TypeScript.\n- **args**: This is the path to the compiled `index.js` file. Make sure the path is correct according to where your files are located after compilation. If you've placed the folder at `C:\\\\Users\\\\username\\\\desktop\\\\letzai-mcp`, the path will be:\n\n```\n\nC:\\\\Users\\\\username\\\\desktop\\\\letzai-mcp\\\\build\\\\index.js\n\n```\n\n### 6. Run the MCP Server\n\nNow that everything is set up, you can start using the LetzAI MCP in the Claude Desktop App. The server should be ready for image generation tasks once the app is running with the correct API key in the environment.\n\n**Important:** After making changes to the configuration, you **must restart Claude** for the changes to take effect.\n\n### 7. Testing the New MCP in Claude\n\n![claude prompt ui after installtion](claudeInterface.png)\nClick on the hammer icon to view the installed MCP tools.\n![claude mcp tools](mcpToolsInfo.png)\n\nOnce you've set up the MCP in the Claude Desktop App, you can test it by running the following prompt:\n\n- **Create image with LetzAI using prompt: \"photo of @mischstrotz drinking a beer, dressed as a knight\"**\n\nThis will create the image based on the provided prompt, using the model @mischstrotz from LetzAI. Claude will open the image in your preferred browser.\n\n- **Upscale this image with strength 1: [https://letz.ai/image/d6a67077-f156-46d7-a1a2-1dc49e83dd91](https://images.letz.ai/5ed74083-f9d1-4897-b8e3-c8f1596af767/d6a67077-f156-46d7-a1a2-1dc49e83dd91/high_quality_photo_of_mischstrotz_holding_a_beer_s20250322080513.jpg)**\n\nThis will upscale the image using the strength parameter 1. You can pass entire URLs, or just the LetzAI Image IDs e.g. d6a67077-f156-46d7-a1a2-1dc49e83dd91\n\n## Troubleshooting\n\n- **Node.js not found**: Ensure that Node.js is installed and added to your system's PATH environment variable.\n- **Invalid API Key**: Double-check that you have correctly added your API key under the `LETZAI_API_KEY` variable in the Claude Desktop App settings.\n- **File Path Issues**: Make sure that the path to the `index.js` file is correct. If you're unsure about the path, use the absolute path to the file.\n\nFor more detailed documentation and support, visit [LetzAI Docs](https://letz.ai/docs/api).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "upscale",
        "letzai",
        "images",
        "image generation",
        "upscale images",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "Lucker631--mcp-templateio": {
      "owner": "Lucker631",
      "name": "mcp-templateio",
      "url": "https://github.com/Lucker631/mcp-templateio",
      "imageUrl": "/freedevtools/mcp/pfp/Lucker631.webp",
      "description": "Generates customized visuals by creating images based on templates using the Templated.io API. Supports dynamic graphics creation through user-provided text and image URLs.",
      "stars": 0,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-10T15:36:42Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/lucker631-mcp-templateio-badge.png)](https://mseep.ai/app/lucker631-mcp-templateio)\n\n# MCP TemplateIO - Image Generation Tool\n\nA Model Context Protocol (MCP) server built with mcp-framework that provides an image generation tool using Templated.io.\n\n## Overview\n\nThis template provides a starting point for building MCP servers with custom tools. It includes an example tool and instructions on how to add more tools, develop them, and publish them to npm. This README will guide you through the process of setting up, developing, and deploying your own MCP server.\n\n## Quick Start\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Project Structure\n\n```\nmcp-templateio/\n├── src/\n│   ├── tools/        # MCP Tools\n│   │   ├── ExampleTool.ts\n│   │   └── TemplatedImageTool.ts # Image generation tool\n│   └── index.ts      # Server entry point\n├── package.json\n└── tsconfig.json\n```\n\n## Available Tools\n\n### Templated Image Generator\n\nThis tool generates an image based on a template, given text and image URLs, using the Templated.io API.\n\n**Input Parameters:**\n\n- `templateId`: ID of the Templated.io template to use\n- `photoBgImageUrl`: URL for the image to place in the \"photo-bg\" layer.\n- `bgYellowImageUrl`: URL for the image to place in the \"bg-yellow\" layer.\n- `buildText`: Text content for the \"build\" text layer.\n\n## Tool Development\n\nExample tool structure:\n\n```typescript\nimport { MCPTool } from \"mcp-framework\";\nimport { z } from \"zod\";\n\ninterface MyToolInput {\n  message: string;\n}\n\nclass MyTool extends MCPTool<MyToolInput> {\n  name = \"my_tool\";\n  description = \"Describes what your tool does\";\n\n  schema = {\n    message: {\n      type: z.string(),\n      description: \"Description of this input parameter\",\n    },\n  };\n\n  async execute(input: MyToolInput) {\n    // Your tool logic here\n    return `Processed: ${input.message}`;\n  }\n}\n\nexport default MyTool;\n```\n\n## Adding Components\n\nThe project comes with an example tool in `src/tools/ExampleTool.ts` and the `TemplatedImageTool.ts`. You can add more tools using the CLI:\n\n```bash\n# Add a new tool\nmcp add tool my-tool\n\n# Example tools you might create:\nmcp add tool data-processor\nmcp add tool api-client\nmcp add tool file-handler\n```\n\n## Publishing to npm\n\n1. Update your package.json:\n\n   - Ensure `name` is unique and follows npm naming conventions\n   - Set appropriate `version`\n   - Add `description`, `author`, `license`, etc.\n   - Check `bin` points to the correct entry file\n\n2. Build and test locally:\n\n   ```bash\n   npm run build\n   npm link\n   mcp-templateio  # Test your CLI locally\n   ```\n\n3. Login to npm (create account if necessary):\n\n   ```bash\n   npm login\n   ```\n\n4. Publish your package:\n   ```bash\n   npm publish\n   ```\n\nAfter publishing, users can add it to their claude desktop client (read below) or run it with npx\n\n## Using with Claude Desktop\n\n### Local Development\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-templateio\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-templateio/dist/index.js\"]\n    }\n  }\n}\n```\n\n### After Publishing\n\nGET YOUR API KEY HERE: https://app.templated.io/api-integration?template=4ae9a86b-4ecd-44ee-aebd-7c5a49c16969\n\nAdd this configuration to your Claude Desktop config file:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-templateio\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\Users\\\\alex0\\\\Documents\\\\AA_CodeAndScripts\\\\modelcontextprotocol\\\\mcp-templateio\\\\dist\\\\index.js\"\n      ],\n      \"env\": {\"TEMPLATED_API_KEY\":\"YOUR-API-KEY-HERE\"}\n    },\n  }\n}\n```\n\n## Building and Testing\n\n1. Make changes to your tools\n2. Run `npm run build` to compile\n3. The server will automatically load your tools on startup\n\n## Learn More\n\n- [MCP Framework Github](https://github.com/QuantGeekDev/mcp-framework)\n- [MCP Framework Docs](https://mcp-framework.com)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "templateio",
        "templated",
        "templates",
        "mcp templateio",
        "templateio generates",
        "graphics creation"
      ],
      "category": "image-and-video-generation"
    },
    "MichaelYangjson--mcp-ghibli-video": {
      "owner": "MichaelYangjson",
      "name": "mcp-ghibli-video",
      "url": "https://github.com/MichaelYangjson/mcp-ghibli-video",
      "imageUrl": "/freedevtools/mcp/pfp/MichaelYangjson.webp",
      "description": "Transforms static images into animated videos using AI technology. Users can manage video generation tasks and check API credits through a straightforward interface.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T13:57:06Z",
      "readme_content": "# mcp-server-ghibli MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@MichaelYangjson/mcp-ghibli-video)](https://smithery.ai/server/@MichaelYangjson/mcp-ghibli-video)\n\nA TypeScript-based MCP server that provides AI image and video generation capabilities through a simple interface.\n\n> **Note**: This server requires an API key from [GPT4O Image Generator](https://www.gpt4oimg.com/). Please visit the website to obtain your API key before using this service.\n\n## Features\n\n### Tools\n\n#### 1. Image to Video Conversion\n\n- `image_to_video` - Convert static images into animated videos\n  - Required parameters:\n    - `image`: Base64 encoded image or image URL\n    - `api_key`: Authentication key\n  - Optional parameters:\n    - `prompt`: Text prompt to guide video generation (default: \"in the style of ghibli\")\n    - `aspect_ratio`: Output video aspect ratio (default: \"9:16\")\n    - `negative_prompt`: Negative prompt to guide generation (default: \"bad prompt\")\n\n#### 2. Points Management\n\n- `get_points` - Check remaining API credits\n  - Required parameters:\n    - `api_key`: Authentication key\n\n#### 3. Task Management\n\n- `get_task_result` - Check the status of a video generation task\n  - Required parameters:\n    - `taskId`: Task ID returned from image_to_video\n    - `api_key`: Authentication key\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-ghibli-video\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@openmcprouter/mcp-server-ghibli-video\"],\n      \"env\": {\n        \"Ghibli_API_URL\": \"https://www.gpt4oimg.com\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install mcp-server-ghibli MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MichaelYangjson/mcp-ghibli-video):\n\n```bash\nnpx -y @smithery/cli install @MichaelYangjson/mcp-ghibli-video --client claude\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "animated",
        "ghibli",
        "videos",
        "animated videos",
        "video generation",
        "ghibli video"
      ],
      "category": "image-and-video-generation"
    },
    "MiniMax-AI--MiniMax-MCP-JS": {
      "owner": "MiniMax-AI",
      "name": "MiniMax-MCP-JS",
      "url": "https://github.com/MiniMax-AI/MiniMax-MCP-JS",
      "imageUrl": "/freedevtools/mcp/pfp/MiniMax-AI.webp",
      "description": "Integrates with MiniMax's AI capabilities to facilitate interaction with multimedia generation tools, including image generation, video generation, text-to-speech, and voice cloning. Supports a flexible and configurable JavaScript/TypeScript framework for versatile deployment scenarios.",
      "stars": 84,
      "forks": 29,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T15:24:03Z",
      "readme_content": "![export](https://github.com/MiniMax-AI/MiniMax-01/raw/main/figures/MiniMaxLogo-Light.png)\n\n<div align=\"center\">\n\n# MiniMax MCP JS\n\nJavaScript/TypeScript implementation of MiniMax MCP, providing image generation, video generation, text-to-speech, and more.\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://www.minimax.io\" target=\"_blank\" style=\"margin: 2px; color: var(--fgColor-default);\">\n    <img alt=\"Homepage\" src=\"https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2501.08313\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Paper\" src=\"https://img.shields.io/badge/📖_Paper-MiniMax--01-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.minimax.io/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.minimax.io/platform\" style=\"margin: 2px;\">\n    <img alt=\"API\" src=\"https://img.shields.io/badge/⚡_API-Platform-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://huggingface.co/MiniMaxAI\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/🤗_Hugging_Face-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"WeChat\" src=\"https://img.shields.io/badge/_WeChat-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://www.modelscope.cn/organization/MiniMax\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"ModelScope\" src=\"https://img.shields.io/badge/_ModelScope-MiniMax-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div style=\"line-height: 1.5;\">\n  <a href=\"https://github.com/MiniMax-AI/MiniMax-MCP-JS/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/_Code_License-MIT-FF4040?style=flat-square&labelColor=2C3E50\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@MiniMax-AI/MiniMax-MCP-JS\"></a>\n</div>\n\n</div>\n\n## Documentation\n\n- [中文文档](README.zh-CN.md)\n- [Python Version](https://github.com/MiniMax-AI/MiniMax-MCP) - Official Python implementation of MiniMax MCP\n\n## Release Notes\n\n### July 22, 2025\n\n#### 🔧 Fixes & Improvements\n- **TTS Tool Fixes**: Fixed parameter handling for `languageBoost` and `subtitleEnable` in the `text_to_audio` tool\n- **API Response Enhancement**: TTS API can return both audio file and subtitle file, providing a more complete speech-to-text experience\n\n### July 7, 2025\n\n#### 🆕 What's New\n- **Voice Design**: New `voice_design` tool - create custom voices from descriptive prompts with preview audio\n- **Video Enhancement**: Added `MiniMax-Hailuo-02` model with ultra-clear quality and duration/resolution controls  \n- **Music Generation**: Enhanced `music_generation` tool powered by `music-1.5` model\n\n#### 📈 Enhanced Tools\n- `voice_design` - Generate personalized voices from text descriptions\n- `generate_video` - Now supports MiniMax-Hailuo-02 with 6s/10s duration and 768P/1080P resolution options\n- `music_generation` - High-quality music creation with music-1.5 model\n\n## Features\n\n- Text-to-Speech (TTS)\n- Image Generation\n- Video Generation\n- Voice Cloning\n- Music Generation\n- Voice Design\n- Dynamic configuration (supports both environment variables and request parameters)\n- Compatible with MCP platform hosting (ModelScope and other MCP platforms)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MiniMax MCP JS for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@MiniMax-AI/MiniMax-MCP-JS):\n\n```bash\nnpx -y @smithery/cli install @MiniMax-AI/MiniMax-MCP-JS --client claude\n```\n\n### Installing manually\n```bash\n# Install with pnpm (recommended)\npnpm add minimax-mcp-js\n```\n\n## Quick Start\n\nMiniMax MCP JS implements the [Model Context Protocol (MCP)](https://github.com/anthropics/model-context-protocol) specification and can be used as a server to interact with MCP-compatible clients (such as Claude AI).\n\n### Quickstart with MCP Client\n\n1. Get your API key from [MiniMax International Platform](https://www.minimax.io/platform/user-center/basic-information/interface-key).\n2. Make sure that you already installed [Node.js and npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n3. **Important: API HOST&KEY are different in different region**, they must match, otherwise you will receive an `Invalid API key` error.\n\n|Region| Global  | Mainland  |\n|:--|:-----|:-----|\n|MINIMAX_API_KEY| go get from [MiniMax Global](https://www.minimax.io/platform/user-center/basic-information/interface-key) | go get from [MiniMax](https://platform.minimaxi.com/user-center/basic-information/interface-key) |\n|MINIMAX_API_HOST| ​https://api.minimaxi.chat (note the extra **\"i\"**) | ​https://api.minimax.chat |\n\n\n### Using with MCP Clients (Recommended)\n\nConfigure your MCP client:\n\n#### Claude Desktop\n\nGo to `Claude > Settings > Developer > Edit Config > claude_desktop_config.json` to include:\n\n```json\n{\n  \"mcpServers\": {\n    \"minimax-mcp-js\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"minimax-mcp-js\"\n      ],\n      \"env\": {\n        \"MINIMAX_API_HOST\": \"<https://api.minimaxi.chat|https://api.minimax.chat>\",\n        \"MINIMAX_API_KEY\": \"<your-api-key-here>\",\n        \"MINIMAX_MCP_BASE_PATH\": \"<local-output-dir-path, such as /User/xxx/Desktop>\",\n        \"MINIMAX_RESOURCE_MODE\": \"<optional, [url|local], url is default, audio/image/video are downloaded locally or provided in URL format>\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nGo to `Cursor → Preferences → Cursor Settings → MCP → Add new global MCP Server` to add the above config.\n\n⚠️ **Note**: If you encounter a \"No tools found\" error when using MiniMax MCP JS with Cursor, please update your Cursor to the latest version. For more information, see this [discussion thread](https://forum.cursor.com/t/mcp-servers-no-tools-found/49094/23).\n\nThat's it. Your MCP client can now interact with MiniMax through these tools.\n\n**For local development**: \nWhen developing locally, you can use `npm link` to test your changes:\n```bash\n# In your project directory\nnpm link\n```\n\nThen configure Claude Desktop or Cursor to use npx as shown above. This will automatically use your linked version.\n\n⚠️ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimaxi.chat`\n\n## Transport Modes\n\nMiniMax MCP JS supports three transport modes:\n\n| Feature | stdio (default) | REST | SSE |\n|:-----|:-----|:-----|:-----|\n| Environment | Local only | Local or cloud deployment | Local or cloud deployment |\n| Communication | Via `standard I/O` | Via `HTTP requests` | Via `server-sent events` |\n| Use Cases | Local MCP client integration | API services, cross-language calls | Applications requiring server push |\n| Input Restrictions | Supports `local files` or `URL` resources | When deployed in cloud, `URL` input recommended | When deployed in cloud, `URL` input recommended |\n\n## Configuration\n\nMiniMax-MCP-JS provides multiple flexible configuration methods to adapt to different use cases. The configuration priority from highest to lowest is as follows:\n\n### 1. Request Parameter Configuration (Highest Priority)\n\nIn platform hosting environments (like ModelScope or other MCP platforms), you can provide an independent configuration for each request via the `meta.auth` object in the request parameters:\n\n```json\n{\n  \"params\": {\n    \"meta\": {\n      \"auth\": {\n        \"api_key\": \"your_api_key_here\",\n        \"api_host\": \"<https://api.minimaxi.chat|https://api.minimaxi.chat>\",\n        \"base_path\": \"/path/to/output\",\n        \"resource_mode\": \"url\"\n      }\n    }\n  }\n}\n```\n\nThis method enables multi-tenant usage, where each request can use different API keys and configurations.\n\n### 2. API Configuration\n\nWhen used as a module in other projects, you can pass configuration through the `startMiniMaxMCP` function:\n\n```javascript\nimport { startMiniMaxMCP } from 'minimax-mcp-js';\n\nawait startMiniMaxMCP({\n  apiKey: 'your_api_key_here',\n  apiHost: 'https://api.minimaxi.chat', // Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat\n  basePath: '/path/to/output',\n  resourceMode: 'url'\n});\n```\n\n### 3. Command Line Arguments\n\n1. Install the CLI tool globally:\n```bash\n# Install globally\npnpm install -g minimax-mcp-js\n```\n\n2. When used as a CLI tool, you can provide configuration via command line arguments:\n\n```bash\nminimax-mcp-js --api-key your_api_key_here --api-host https://api.minimaxi.chat --base-path /path/to/output --resource-mode url\n```\n\n### 4. Environment Variables (Lowest Priority)\n\nThe most basic configuration method is through environment variables:\n\n```bash\n# MiniMax API Key (required)\nMINIMAX_API_KEY=your_api_key_here\n\n# Base path for output files (optional, defaults to user's desktop)\nMINIMAX_MCP_BASE_PATH=~/Desktop\n\n# MiniMax API Host (optional, defaults to https://api.minimaxi.chat, Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat)\nMINIMAX_API_HOST=https://api.minimaxi.chat\n\n# Resource mode (optional, defaults to 'url')\n# Options: 'url' (return URLs), 'local' (save files locally)\nMINIMAX_RESOURCE_MODE=url\n```\n\n### Configuration Priority\n\nWhen multiple configuration methods are used, the following priority order applies (from highest to lowest):\n\n1. **Request-level configuration** (via `meta.auth` in each API request)\n2. **Command line arguments**\n3. **Environment variables**\n4. **Configuration file**\n5. **Default values**\n\nThis prioritization ensures flexibility across different deployment scenarios while maintaining per-request configuration capabilities for multi-tenant environments.\n\n### Configuration Parameters\n\n| Parameter | Description | Default Value |\n|-----------|-------------|---------------|\n| apiKey | MiniMax API Key | None (Required) |\n| apiHost | MiniMax API Host | Global Host - https://api.minimaxi.chat, Mainland Host - https://api.minimax.chat |\n| basePath | Base path for output files | User's desktop |\n| resourceMode | Resource handling mode, 'url' or 'local' | url |\n\n⚠️ **Note**: The API key needs to match the host address. Different hosts are used for global and mainland China versions:\n- Global Host: `https://api.minimaxi.chat` (note the extra \"i\")\n- Mainland China Host: `https://api.minimax.chat`\n\n## Example usage\n\n⚠️ Warning: Using these tools may incur costs.\n\n### 1. broadcast a segment of the evening news\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_20-07-53.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 2. clone a voice\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-45-13.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 3. generate a video\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-58-52.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/Snipaste_2025-04-09_19-59-43.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 4. generate images\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/gen_image1.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle; \"/>\n\n### 5. generate music\n<img src=\"https://filecdn.minimax.chat/public/5675b3dc-6789-4ceb-9505-8ef39ae4224f.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n### 6. voice design\n<img src=\"https://filecdn.minimax.chat/public/5654f5df-0642-477f-9c5d-b853d185b8b0.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Available Tools\n\n### Text to Audio\n\nConvert text to speech audio file.\n\nTool Name: `text_to_audio`\n\nParameters:\n- `text`: Text to convert (required)\n- `model`: Model version, options are 'speech-02-hd', 'speech-02-turbo', 'speech-01-hd', 'speech-01-turbo', 'speech-01-240228', 'speech-01-turbo-240228', default is 'speech-02-hd'\n- `voiceId`: Voice ID, default is 'male-qn-qingse'\n- `speed`: Speech speed, range 0.5-2.0, default is 1.0\n- `vol`: Volume, range 0.1-10.0, default is 1.0\n- `pitch`: Pitch, range -12 to 12, default is 0\n- `emotion`: Emotion, options are 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised', 'neutral', default is 'happy'. Note: This parameter only works with 'speech-02-hd', 'speech-02-turbo', 'speech-01-turbo', 'speech-01-hd' models\n- `format`: Audio format, options are 'mp3', 'pcm', 'flac', 'wav', default is 'mp3'\n- `sampleRate`: Sample rate (Hz), options are 8000, 16000, 22050, 24000, 32000, 44100, default is 32000\n- `bitrate`: Bitrate (bps), options are 64000, 96000, 128000, 160000, 192000, 224000, 256000, 320000, default is 128000\n- `channel`: Audio channels, options are 1 or 2, default is 1\n- `languageBoost`: Enhance the ability to recognize specified languages and dialects.\nSupported values include:\n'Chinese', 'Chinese,Yue', 'English', 'Arabic', 'Russian', 'Spanish', 'French', 'Portuguese', 'German', 'Turkish', 'Dutch', 'Ukrainian', 'Vietnamese', 'Indonesian', 'Japanese', 'Italian', 'Korean', 'Thai', 'Polish', 'Romanian', 'Greek', 'Czech', 'Finnish', 'Hindi', 'auto', default is 'auto'\n- `stream`: Enable streaming output\n- `subtitleEnable`: The parameter controls whether the subtitle service is enabled. The model must be 'speech-01-turbo' or 'speech-01-hd'. If this parameter is not provided, the default value is false\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n\n### Play Audio\n\nPlay an audio file. Supports WAV and MP3 formats. Does not support video.\n\nTool Name: `play_audio`\n\nParameters:\n- `inputFilePath`: Path to the audio file to play (required)\n- `isUrl`: Whether the audio file is a URL, default is false\n\n### Voice Clone\n\nClone a voice from an audio file.\n\nTool Name: `voice_clone`\n\nParameters:\n- `audioFile`: Path to audio file (required)\n- `voiceId`: Voice ID (required)\n- `text`: Text for demo audio (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Text to Image\n\nGenerate images based on text prompts.\n\nTool Name: `text_to_image`\n\nParameters:\n- `prompt`: Image description (required)\n- `model`: Model version, default is 'image-01'\n- `aspectRatio`: Aspect ratio, default is '1:1', options are '1:1', '16:9','4:3', '3:2', '2:3', '3:4', '9:16', '21:9'\n- `n`: Number of images to generate, range 1-9, default is 1\n- `promptOptimizer`: Whether to optimize the prompt, default is true\n- `subjectReference`: Path to local image file or public URL for character reference (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Generate Video\n\nGenerate videos based on text prompts.\n\nTool Name: `generate_video`\n\nParameters:\n- `prompt`: Video description (required)\n- `model`: Model version, options are 'T2V-01', 'T2V-01-Director', 'I2V-01', 'I2V-01-Director', 'I2V-01-live', 'S2V-01', 'MiniMax-Hailuo-02', default is 'MiniMax-Hailuo-02'\n- `firstFrameImage`: Path to first frame image (optional)\n- `duration`: The duration of the video. The model must be \"MiniMax-Hailuo-02\". Values can be 6 and 10. (optional)\n- `resolution`: The resolution of the video. The model must be \"MiniMax-Hailuo-02\". Values range [\"768P\", \"1080P\"]. (optional)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n- `outputFile`: Path to save the output file (optional, auto-generated if not provided)\n- `asyncMode`: Whether to use async mode. Defaults to False. If True, the video generation task will be submitted asynchronously and the response will return a task_id. Should use `query_video_generation` tool to check the status of the task and get the result. (optional)\n\n### Query Video Generation Status\n\nQuery the status of a video generation task.\n\nTool Name: `query_video_generation`\n\nParameters:\n- `taskId`: The Task ID to query. Should be the task_id returned by `generate_video` tool if `async_mode` is True. (required)\n- `outputDirectory`: Directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n### Generate Music\n\nGenerate music from prompt and lyrics.\n\nTool Name: `music_generation`\n\nParameters:\n- `prompt`: Music creation inspiration describing style, mood, scene, etc. Example: \"Pop music, sad, suitable for rainy nights\". Character range: [10, 300]. (required)\n- `lyrics`: Song lyrics for music generation. Use newline (\\\\n) to separate each line of lyrics. Supports lyric structure tags [Intro] [Verse] [Chorus] [Bridge] [Outro] to enhance musicality. Character range: [10, 600] (each Chinese character, punctuation, and letter counts as 1 character). (required)\n- `sampleRate`: Sample rate of generated music. Values: [16000, 24000, 32000, 44100], default is 32000. (optional)\n- `bitrate`: Bitrate of generated music. Values: [32000, 64000, 128000, 256000], default is 128000. (optional)\n- `format`: Format of generated music. Values: [\"mp3\", \"wav\", \"pcm\"], default is 'mp3'. (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n\n### Voice Design\n\nGenerate a voice based on description prompts.\n\nTool Name: `voice_design`\n\nParameters:\n- `prompt`: The prompt to generate the voice from. (required)\n- `previewText`: The text to preview the voice. (required)\n- `voiceId`: The id of the voice to use. For example, \"male-qn-qingse\"/\"audiobook_female_1\"/\"cute_boy\"/\"Charming_Lady\"... (optional)\n- `outputDirectory`: The directory to save the output file. `outputDirectory` is relative to `MINIMAX_MCP_BASE_PATH` (or `basePath` in config). The final save path is `${basePath}/${outputDirectory}`. For example, if `MINIMAX_MCP_BASE_PATH=~/Desktop` and `outputDirectory=workspace`, the output will be saved to `~/Desktop/workspace/`. (optional)\n\n## FAQ\n\n### 1. How to use `generate_video` in async-mode\nDefine completion rules before starting:\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_rule2.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\nAlternatively, these rules can be configured in your IDE settings (e.g., Cursor):\n<img src=\"https://public-cdn-video-data-algeng.oss-cn-wulanchabu.aliyuncs.com/cursor_video_rule.png?x-oss-process=image/resize,p_50/format,webp\" style=\"display: inline-block; vertical-align: middle;\"/>\n\n## Development\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/MiniMax-AI/MiniMax-MCP-JS.git\ncd minimax-mcp-js\n\n# Install dependencies\npnpm install\n```\n\n### Build\n\n```bash\n# Build the project\npnpm run build\n```\n\n### Run\n\n```bash\n# Run the MCP server\npnpm start\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minimax",
        "multimedia",
        "javascript",
        "minimax ai",
        "ai minimax",
        "mcp js"
      ],
      "category": "image-and-video-generation"
    },
    "Momo707577045--tinypng-script-with-cache": {
      "owner": "Momo707577045",
      "name": "tinypng-script-with-cache",
      "url": "https://github.com/Momo707577045/tinypng-script-with-cache",
      "imageUrl": "/freedevtools/mcp/pfp/Momo707577045.webp",
      "description": "Compress images without dependencies, automatically skip already compressed images, and replace source files, while maintaining quality. The server utilizes multiple API keys for compression and generates compression reports while ensuring no redundant files are created during the process.",
      "stars": 23,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-24T13:15:05Z",
      "readme_content": "# 无依赖的 tinypng node 脚本\n## 特点\n- 【无依赖，纯脚本】\n  - 下载脚本代码，直接使用 node 命令即可运行。\n  - 将使用门槛降到最低。\n- 【过滤重复压缩】\n  - 自动记录已被压缩过的图片，跳过压缩，加快进度。\n  - 记录图片压缩后的 md5 值，再次运行压缩脚本时，跳过压缩。\n  - 通过 md5 值比较文件变更，即使「文件迁移」也能自动过滤。\n  - 通过 md5 值比较文件变更，即使「使用同名文件替换」也能自动识别，并压缩，没有漏网之鱼。\n- 【替换源文件】\n  - 压缩成功，直接替换源文件，不生成冗余文件，不需要复制粘贴，移动图片。\n  - 静默压缩，对项目无感知，无任何影响。\n- 【自动切换 api key】\n  - tinypng 申请的 [api key](https://tinypng.com/developers) 每月只有 500 次免费压缩额度。\n  - 可设置多个 api key，当某 key 超过使用次数时，自动切换下一个 key 进行压缩。\n- 【压缩报告】\n  - 记录每个图片的压缩数据，并生成汇总信息。\n- 【压缩安全边界】\n  - 压缩安全线，当压缩比例低于该百分比值时，保持源文件，避免过分压缩，损伤图片质量。\n- 【源码携带详细备注，自带测试图片】\n  - 降低源码阅读门槛，降低测试门槛，减低使用门槛。\n  - 推荐阅读源码，打破恐惧，便于定制个性化需求。\n\n\n## 专为小型项目定制\n- 纯脚本，不依赖 gulp，不依赖 webpack，无需搭建脚手架环境\n- 小型项目，或者只有几个静态页面，搭建脚手架的成本过高。本脚解决的即是脚手架依赖的问题。\n- 当然，中大型项目也可以用，只是其「无依赖」的特点在里面没那么突出。中大型项目推荐使用其 [gulp 版本](https://segmentfault.com/a/1190000023895556)，实现更灵活的配置。\n\n\n## 单文件使用方式\n- 第一步，点击[下载源码](http://upyun.luckly-mjw.cn/lib/mtp.js)\n- 第二步，在脚本文件头部添加 tinypng 的 [api key](https://tinypng.com/developers)\n  ```\n  global.tinypngConf = {\n    apiKeyList: [\n      // 'XgNgkoyWbdIZd8OizINMjX2TpxAd_Gp3', // 无效 key\n      // 'IAl6s3ekmONUVMEqWZdIp1nV2ItJL1PC', // 无效 key\n      'IAl6s3ekmONUVMEqWZdIp1nV2ItJLyPC', // 有效 key\n    ]\n  }\n  ```\n  ![配置图](http://upyun.luckly-mjw.cn/Assets/tinypng/004.png)\n- 第三步，赋予脚本文件「可执行」权限，```chmod +x ./mtp.js```\n- 第四步，将脚本文件放置到项目所在目录\n  ![运行效果](http://upyun.luckly-mjw.cn/Assets/tinypng/007.jpeg)\n- 第五步，在项目所在目录运行脚本```node ./mtp.js```\n  ![运行效果](http://upyun.luckly-mjw.cn/Assets/tinypng/006.jpeg)\n- 后续使用，仅需最后两步「第四步」「第五步」\n\n\n## 全局配置使用方式\n- 第一步，全局安装```npm install -g tinypng-script-with-cache```\n- 第二步，全局配置 api key\n  ```mtp setKey XgNgkoyWbdIZd8OizINMjX2TpxAd_Gp3,IAl6s3ekmONUVMEqWZdIp1nV2ItJLyPC```\n- 第三步，在项目所在目录运行脚本```mtp```\n- 后续使用，无需配置，直接在目标目录运行```mtp```\n\n  ![运行效果](http://upyun.luckly-mjw.cn/Assets/tinypng/008.png)\n\n## 参数传递方式\n#### 默认配置\n- 默认压缩「运行命令所在文件夹」下的图片\n- 「命令传参」优先级高于「修改源文件设置」\n\n\n#### 修改源文件设置\n- 在源文件头部，写入全局参数，程序运行时自动获取\n- 全部参考配置如下\n  ```\n  global.tinypngConf = {\n     basePath: '/Users/mjw/Desktop/git/tinypng-script-with-cache/test-img', // 压缩路径\n     createMd5FormOrigin: false, // 不进行压缩操作，只生成现有图片的 md5 信息，并作为缓存。用于「初次项目接入」及手动清理冗余的「图片md5信息」\n     apiKeyList: [ // tiny png 的 api key 数组，当其中一个不可用或超过使用次数时，自动切换下一个 key 调用\n       'IAl6s3ekmONUVMEqWZdIp1nV2ItJLyPC', // 有效 key\n     ]\n   }\n  ```\n  ![配置图](http://upyun.luckly-mjw.cn/Assets/tinypng/004.png)\n\n#### 命令传参\n- 参数通过空格区分\n- 参数一：压缩路径\n- 参数二：是否不进行压缩操作，只生成现有图片的 md5 信息。除空字符串```''```外，其余值均为 true\n- 参数三：apiKeyList，以逗号区分```,```\n- 传参参考\n  ```\n  node ./mtp.js /Users/mjw/Desktop/git/tinypng-script-with-cache/test-img '' IAl6s3ekmONUVMEqWZdIp1nV2ItJLyPC\n  ```\n  ![运行效果](http://upyun.luckly-mjw.cn/Assets/tinypng/005.jpeg)\n\n#### 配置合并优先级源码\n```\nconst vfs = require('vinyl-fs');\nlet tinypng = require('./tinypng-with-cache')\n\nlet apiKeyList = [] // 接口 key 默认为空\nlet basePath = process.cwd() // 默认运行脚本所在目录\nlet createMd5FormOrigin = false // 不进行压缩操作，只生成现有图片的 md5 信息，并作为缓存。用于「初次项目接入」及手动清理冗余的「图片md5信息」\n\n// 如果有全局传值\nif (global.tinypngConf) {\n  basePath = tinypngConf.basePath || basePath\n  apiKeyList = tinypngConf.apiKeyList || apiKeyList\n  createMd5FormOrigin = tinypngConf.createMd5FormOrigin || createMd5FormOrigin\n}\n\n// 动态参数传值\nbasePath = process.argv[2] || basePath\ncreateMd5FormOrigin = process.argv[3] || createMd5FormOrigin\napiKeyList = process.argv[4] ? process.argv[4].split(',') : apiKeyList\n\nlet fileFilter = tinypngConf.fileFilter || [\n  basePath + '/**/*.png',\n  basePath + '/**/*.jpg',\n  basePath + '/**/*.jpeg',\n  `!${basePath}/**/node_modules/**`, // 忽略无需遍历的文件，路径匹配语法参考：https://www.gulpjs.com.cn/docs/getting-started/explaining-globs/\n  `!${basePath}/**/dist/**`,\n]\n\nconsole.log({\n  basePath,\n  apiKeyList,\n  fileFilter,\n  createMd5FormOrigin,\n})\n\nif (!apiKeyList.length) {\n  return console.error('tinypng-script-with-cache', 'tinypny key 列表不能为空!')\n}\n\nvfs.src(fileFilter, {\n  base: './', // 对文件使用相路径，为了后面覆盖源文件\n  nodir: true, // 忽略文件夹\n})\n.pipe(tinypng({\n  apiKeyList,\n  reportFilePath: basePath + '/tinypngReport.json', // 不设置，则不进行日志记录\n  md5RecordFilePath: basePath + '/tinypngMd5Record.json', // 不设置，则不进行缓存过滤\n  minCompressPercentLimit: 10, // 默认值为零，最小压缩百分比限制，为保证图片质量，当压缩比例低于该值时，保持源文件，避免过分压缩，损伤图片质量\n  createMd5FormOrigin, // 不进行压缩操作，只生成现有图片的 md5 信息，并作为缓存。用于「初次项目接入」及手动清理冗余的「图片md5信息」\n}))\n.pipe(vfs.dest('./', { overwrite: true })) // 覆写原文件\n```\n\n## [项目地址](https://github.com/Momo707577045/tinypng-script-with-cache)\n\n## 二次开发，生成自定义脚本\n- git clone 下载项目\n- npm install 安装依赖\n- 修改「tinypng-mjw.js」与「tinypng-with-cache.js」源文件\n- 执行```npx webpack --config webpack.config.js```命令，进行打包\n- 生成目标文件```dist/mtp.js```\n\n\n## 测试资源\n- test-img：图片压缩测试目录\n- test-img-origin：测试图片备份目录，用于恢复测试\n\n\n## 运行效果\n![运行效果](http://upyun.luckly-mjw.cn/Assets/tinypng/006.jpeg)\n\n## 压缩报告\n![压缩报告](http://upyun.luckly-mjw.cn/Assets/tinypng/002.png)\n\n## md5 记录\n![md5 记录](http://upyun.luckly-mjw.cn/Assets/tinypng/003.png)\n\n## gulp 版本请参考[这里](https://segmentfault.com/a/1190000023895556)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tinypng",
        "compression",
        "compressed",
        "momo707577045 tinypng",
        "compress images",
        "compressed images"
      ],
      "category": "image-and-video-generation"
    },
    "NightTrek--moondream-mcp": {
      "owner": "NightTrek",
      "name": "moondream-mcp",
      "url": "https://github.com/NightTrek/moondream-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/NightTrek.webp",
      "description": "Advanced image analysis capabilities including captioning, object detection, and visual question answering for applications requiring sophisticated computer vision tasks.",
      "stars": 18,
      "forks": 9,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-08-09T18:23:23Z",
      "readme_content": "# 🌙 Moondream MCP Server\n\nA powerful Model Context Protocol (MCP) server that brings advanced image analysis capabilities to your applications using the Moondream vision model. This server seamlessly integrates with Claude and Cline, providing a bridge between AI assistants and sophisticated computer vision tasks.\n\nThis IS NOT an offical Moondream package. All credit to [moondream.ai](https://github.com/vikhyat/moondream) for making the best open source vision model that you can run on consumer hardware.\n\n<div align=\"center\" style=\"height: 150px; overflow: hidden; display: flex; align-items: center; margin: 20px 0;\">\n  <img src=\"https://github.com/user-attachments/assets/e999ada0-9dfa-4f3d-a489-e4ce58434ecb\" alt=\"Moondream MCP Banner\" style=\"width: 100%; object-fit: cover;\">\n</div>\n\n\n## ✨ Features\n\n- 🖼️ **Image Captioning**: Generate natural language descriptions of images\n- 🔍 **Object Detection**: Identify and locate specific objects within images\n- 💭 **Visual Question Answering**: Ask questions about image content and receive intelligent responses\n- 🚀 **High Performance**: Uses quantized 8-bit models for efficient inference\n- 🔄 **Automatic Setup**: Handles model downloading and environment setup\n- 🛠️ **MCP Integration**: Standardized protocol for seamless tool usage\n\n## 🎯 Use Cases\n\n- **Content Analysis**: Automatically generate descriptions for image content\n- **Accessibility**: Create alt text for visually impaired users\n- **Data Extraction**: Extract specific information from images through targeted questions\n- **Object Verification**: Confirm the presence of specific objects in images\n- **Scene Understanding**: Analyze complex scenes and their components\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Node.js v18 or higher\n- Python 3.8+\n- UV package manager (automatically installed if not present)\n\n### Installation\n\n1. **Clone and Setup**\n```bash\ngit clone <repository-url>\ncd moondream-server\npnpm install\n```\n\n2. **Build the Server**\n```bash\npnpm run build\n```\n\nThe server handles the rest automatically:\n- Creates Python virtual environment\n- Installs UV if not present\n- Downloads and sets up the Moondream model\n- Manages the model server process\n\n### Integration with Claude/Cline\n\nAdd to your MCP settings file (`claude_desktop_config.json` or `cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"moondream\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/moondream-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## 🛠️ Available Tools\n\n### analyze_image\n\nPowerful image analysis tool with multiple modes:\n\n```typescript\n{\n  \"name\": \"analyze_image\",\n  \"arguments\": {\n    \"image_path\": string,  // Path to image file\n    \"prompt\": string       // Analysis command\n  }\n}\n```\n\n**Prompt Types:**\n- `\"generate caption\"` - Creates natural language description\n- `\"detect: [object]\"` - Finds specific objects (e.g., \"detect: car\")\n- `\"[question]\"` - Answers questions about the image\n\n**Examples:**\n\n```javascript\n// Image Captioning\n{\n  \"image_path\": \"photo.jpg\",\n  \"prompt\": \"generate caption\"\n}\n\n// Object Detection\n{\n  \"image_path\": \"scene.jpg\",\n  \"prompt\": \"detect: person\"\n}\n\n// Visual Q&A\n{\n  \"image_path\": \"painting.jpg\",\n  \"prompt\": \"What colors are used in this painting?\"\n}\n```\n\n## 🔧 Technical Details\n\n### Architecture\n\nThe server operates as a dual-component system:\n\n1. **MCP Interface Layer**\n   - Handles protocol communication\n   - Manages tool interfaces\n   - Processes requests/responses\n\n2. **Moondream Model Server**\n   - Runs the vision model\n   - Processes image analysis\n   - Provides HTTP API endpoints\n\n### Model Information\n\nUses the Moondream quantized model:\n- Default: `moondream-2b-int8.mf.gz`\n- Efficient 8-bit quantization\n- Automatic download from Hugging Face\n- ~500MB model size\n\n### Performance\n\n- Fast startup with automatic caching\n- Efficient memory usage through quantization\n- Responsive API endpoints\n- Concurrent request handling\n\n## 🔍 Debugging\n\nCommon issues and solutions:\n\n1. **Model Download Issues**\n   ```bash\n   # Manual model download\n   wget https://huggingface.co/vikhyatk/moondream2/resolve/main/moondream-0_5b-int4.mf.gz\n   ```\n\n2. **Server Port Conflicts**\n   - Default port: 3475\n   - Check for process using: `lsof -i :3475`\n\n3. **Python Environment**\n   - UV manages dependencies\n   - Check logs in temp directory\n   - Virtual env in system temp folder\n\n## 🤝 Contributing\n\nContributions welcome! Areas of interest:\n\n- Additional model support\n- Performance optimizations\n- New analysis capabilities\n- Documentation improvements\n\n## 📄 License\n\n[Add your license information here]\n\n## 🙏 Acknowledgments\n\n- [Moondream Model Team](https://github.com/vikhyat/moondream)\n- Model Context Protocol (MCP) Community\n- Contributors and maintainers\n\n---\n\n<p align=\"center\">\nMade with ❤️ by Nighttrek\n</p>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nighttrek",
        "vision",
        "captioning",
        "nighttrek moondream",
        "moondream mcp",
        "advanced image"
      ],
      "category": "image-and-video-generation"
    },
    "PawNzZi--image-server": {
      "owner": "PawNzZi",
      "name": "image-server",
      "url": "https://github.com/PawNzZi/image-server",
      "imageUrl": "/freedevtools/mcp/pfp/PawNzZi.webp",
      "description": "Transform text prompts into images using advanced AI techniques, creating unique visuals tailored to user descriptions.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-17T12:56:27Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@PawNzZi/image-server)](https://smithery.ai/server/@PawNzZi/image-server)\n\n### Installing via Smithery\n\nTo install text2image for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PawNzZi/image-server):\n\n```bash\nnpx -y @smithery/cli install @PawNzZi/image-server --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "text",
        "prompts",
        "prompts images",
        "pawnzzi image",
        "advanced ai"
      ],
      "category": "image-and-video-generation"
    },
    "Sheshiyer--jina-ai-mcp-multimodal-search": {
      "owner": "Sheshiyer",
      "name": "jina-ai-mcp-multimodal-search",
      "url": "https://github.com/Sheshiyer/jina-ai-mcp-multimodal-search",
      "imageUrl": "/freedevtools/mcp/pfp/Sheshiyer.webp",
      "description": "Seamless integration with Jina AI's neural search capabilities enables semantic, image, and cross-modal searches through a simple interface. Perform searches based on natural language queries, visual similarities, and text-to-image or image-to-text conversions.",
      "stars": 4,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-19T08:35:51Z",
      "readme_content": "# Jina AI MCP Server\n\nA Model Context Protocol (MCP) server that provides seamless integration with Jina AI's neural search capabilities. This server enables semantic search, image search, and cross-modal search functionalities through a simple interface.\n\n## 🚀 Features\n\n- **Semantic Search**: Find semantically similar documents using natural language queries\n- **Image Search**: Search for visually similar images using image URLs\n- **Cross-Modal Search**: Perform text-to-image or image-to-text searches\n\n## 📋 Prerequisites\n\n- Node.js 16 or higher\n- A Jina AI account and API key ([Get one here](https://cloud.jina.ai/))\n- MCP-compatible environment (e.g., Cline)\n\n## 🛠️ Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd jina-ai-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create a `.env` file with your Jina AI API key:\n```bash\nJINA_API_KEY=your_api_key_here\n```\n\n4. Build the server:\n```bash\nnpm run build\n```\n\n## ⚙️ Configuration\n\nAdd the following configuration to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"jina-ai\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/jina-ai-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"JINA_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## 🔍 Available Tools\n\n### 1. Semantic Search\nPerform semantic/neural search on text documents.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"jina-ai\",\n  tool_name: \"semantic_search\",\n  arguments: {\n    query: \"search query text\",\n    collection: \"your-collection-name\",\n    limit: 10 // optional, defaults to 10\n  }\n})\n```\n\n### 2. Image Search\nSearch for similar images using an image URL.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"jina-ai\",\n  tool_name: \"image_search\",\n  arguments: {\n    imageUrl: \"https://example.com/image.jpg\",\n    collection: \"your-collection-name\",\n    limit: 10 // optional, defaults to 10\n  }\n})\n```\n\n### 3. Cross-Modal Search\nPerform text-to-image or image-to-text search.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"jina-ai\",\n  tool_name: \"cross_modal_search\",\n  arguments: {\n    query: \"a beautiful sunset\", // or image URL for image2text\n    mode: \"text2image\", // or \"image2text\"\n    collection: \"your-collection-name\",\n    limit: 10 // optional, defaults to 10\n  }\n})\n```\n\n## 📝 Response Format\n\nAll search tools return results in the following format:\n\n```typescript\n{\n  content: [\n    {\n      type: \"text\",\n      text: JSON.stringify({\n        results: [\n          {\n            id: string,\n            score: number,\n            data: Record<string, any>\n          }\n        ]\n      }, null, 2)\n    }\n  ]\n}\n```\n\n## 🔐 Error Handling\n\nThe server handles various error cases:\n- Invalid API key\n- Missing or invalid parameters\n- API rate limits\n- Network errors\n- Invalid collection names\n\nAll errors are properly formatted and returned with appropriate error codes and messages.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Jina AI](https://jina.ai/) for their excellent neural search platform\n- [Model Context Protocol](https://github.com/modelcontextprotocol/protocol) for the MCP specification\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "multimodal",
        "search",
        "searches",
        "multimodal search",
        "mcp multimodal",
        "modal searches"
      ],
      "category": "image-and-video-generation"
    },
    "Siddhant-K-code--memory-journal-mcp-server": {
      "owner": "Siddhant-K-code",
      "name": "memory-journal-mcp-server",
      "url": "https://github.com/Siddhant-K-code/memory-journal-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Siddhant-K-code.webp",
      "description": "Search and analyze photos in a library using various intuitive tools, including location-based searches to easily find images taken in specific places.",
      "stars": 21,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-28T18:59:40Z",
      "readme_content": "# 📸 Smart Photo Journal MCP Server\n\n**Smart Photo Journal** is an MCP server designed to help you search and analyze your photo library with powerful, intuitive tools. Whether you're reminiscing about family moments or looking for a specific photo with friends, this server has got you covered! 🎉\n\n> **Inspired by:** [burningion/video-editing-mcp](https://github.com/burningion/video-editing-mcp)\n> A huge shoutout to [@burningion](https://x.com/burningion) for the innovative idea of using MCP for creative media management!\n\n<a href=\"https://glama.ai/mcp/servers/51jiworg5k\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/51jiworg5k/badge\" alt=\"Smart Photo Journal Server MCP server\" /></a>\n\n## 🎯 Features\n\n- **Location Search:** Find photos from specific places with ease. 🌍\n- **Label Search:** Search photos by keywords or labels like \"Birthday,\" \"Beach,\" or \"Vacation.\" 🎉\n- **People Search:** Quickly locate photos featuring specific people. 👥\n- **Photo Analysis:** Discover fun insights like the most popular times and days for your photo shoots. 🕰️\n- **Fuzzy Matching:** Not sure of the exact name? Don't worry! The server supports fuzzy matching for flexibility. 🔍\n\n## 🚀 Getting started\n\n### Prerequisites\n\n1. Ensure you have macOS with a Photos library.\n2. Install [uv](https://docs.astral.sh/uv/) to manage dependencies and run the server.\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/Siddhant-K-code/memory-journal-mcp-server.git\n   cd memory-journal-mcp-server\n   ```\n\n2. Install dependencies using `uv`:\n\n   ```bash\n   uv sync\n   ```\n\n3. Configure the MCP server. Update your `claude_desktop_config.json` with the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"smart-photo-journal\": {\n         \"command\": \"/Users/<YOUR_DEVICE_USERNAME>/.local/bin/uv\",\n         \"args\": [\n           \"--directory\",\n           \"/Users/<PATH_TO_CLONED_DIR>/memory-journal-mcp-server\",\n           \"run\",\n           \"server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Start the server with following command or just open Claude Desktop:\n   ```bash\n   uv run server.py\n   ```\n\n> **Note:** Replace `<YOUR_DEVICE_USERNAME>` and `<PATH_TO_CLONED_DIR>` with your actual device username and the path to the cloned directory.\n> You will get a popup to authorize the server to access your photos. It will be in local only, and no data will be shared with anyone except Claude services.\n\n### MCP Server Initialization\n\nWhen the server starts, you'll see:\n\n```\nStarting Smart Photo Journal MCP server.\n```\n\nIt's now ready to process your photo queries! 🎉\n\n---\n\n## 🛠️ Usage\n\n### Available Tools\n\n1. **Location Search**\n\n   - Description: Find photos taken in a specific location.\n   - Input Example:\n     ```json\n     {\n       \"location\": \"Udaipur\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Found 5 photos from Udaipur:\n     📷 IMG_1234.jpg\n     ...\n     ```\n\n2. **Label Search**\n\n   - Description: Search for photos by labels or keywords.\n   - Input Example:\n     ```json\n     {\n       \"label\": \"Birthday\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Photos labeled as 'Birthday' (3 found):\n     📷 IMG_5678.jpg\n     ...\n     ```\n\n3. **People Search**\n\n   - Description: Find photos containing specific people.\n   - Input Example:\n     ```json\n     {\n       \"person\": \"Maa\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Photos with Maa (10 found):\n     📷 IMG_9101.jpg\n     ...\n     ```\n\n4. **Photo Analysis**\n   - Description: Analyze patterns in your photo library, such as the most common times or days for photo shoots.\n   - Input Example:\n     ```json\n     {}\n     ```\n   - Expected Output:\n     ```\n     📸 Photo Taking Patterns:\n     Total Photos: 200\n     ...\n     ```\n\n---\n\n## 📚 Example Use-Cases\n\n### 1. **Family & Friends Album Organizer**\n\nWant to gather all your family moments in one place? Use the `people-search` tool with names like \"Papa\" or \"Mom\" or \"Any Friend\" to find photos with specific people.\n\n### 2. **Vacation Highlights**\n\nSearch for photos from your vacation destination using the `location-search` tool.\n\n### 3. **Throwback Fun**\n\nCurious about your past birthday photos? Use `label-search` with \"Birthday\" and relive the fun!\n\n### 4. **Understand Your Photography Habits**\n\nUse the `photo-analysis` tool to understand when and where you take most of your photos. Plan your next shoot accordingly!\n\n---\n\n## ⚡ Tips for Best Results\n\n- Ensure your Photos library is loaded in macOS.\n- Be as specific as possible with search queries for more accurate results.\n- Use fuzzy matching for flexibility when you're unsure of the exact name.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "photos",
        "images",
        "searches",
        "photos library",
        "analyze photos",
        "easily images"
      ],
      "category": "image-and-video-generation"
    },
    "Sunwood-ai-labs--ideagram-mcp-server": {
      "owner": "Sunwood-ai-labs",
      "name": "ideagram-mcp-server",
      "url": "https://github.com/Sunwood-ai-labs/ideagram-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Sunwood-ai-labs.webp",
      "description": "Generate images based on prompts with customizable parameters like aspect ratio and style using the Ideogram API.",
      "stars": 5,
      "forks": 7,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-20T21:40:42Z",
      "readme_content": "<div align=\"center\">\n\n![](docs/ideogram-image_2025-05-18T06-31-45-777Z.png)\n\n  <h1>🎨 Ideogram MCP Server</h1>\n\n  <p>\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/sunwood-ai-labs/ideagram-mcp-server\">\n    <img alt=\"GitHub package.json version\" src=\"https://img.shields.io/github/package-json/v/sunwood-ai-labs/ideagram-mcp-server\">\n    <img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/sunwood-ai-labs/ideagram-mcp-server\">\n    <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/sunwood-ai-labs/ideagram-mcp-server\">\n    <img alt=\"npm\" src=\"https://img.shields.io/npm/v/@sunwood-ai-labs/ideagram-mcp-server\">\n    <img alt=\"npm\" src=\"https://img.shields.io/npm/dt/@sunwood-ai-labs/ideagram-mcp-server\">\n  </p>\n\n  <p>\n    Ideogram APIを使って画像生成を提供するModel Context Protocol (MCP) サーバーだよ！<br>\n    <b>Ideogram 3.0</b>対応で、Claude DesktopやMCPクライアントから爆速連携できるのが神✨\n  </p>\n</div>\n\n---\n\n## 📦 プロジェクト概要\n\n- Ideogram API (v3.0) をMCPサーバー経由で使えるTypeScript製ツール\n- 画像生成・スタイル参照・マジックプロンプト・アスペクト比・モデル選択など多機能\n- Claude Desktopや他MCPクライアントから即利用OK\n\n---\n\n\n## ⚡️ クイックスタート\n\nClaude Desktopや他MCPクライアントで爆速連携したいなら、  \n下記JSONスニペットを設定ファイルにコピペでOK！✨\n\n```json\n{\n  \"mcpServers\": {\n    \"ideogram\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@sunwood-ai-labs/ideagram-mcp-server\"\n      ],\n      \"env\": {\n        \"IDEOGRAM_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n\n---\n\n## 🛠️ MCPツール仕様\n\n### generate_image\n\n#### パラメータ一覧（最新版）\n\n| パラメータ         | 型         | 説明                                                                                 | 必須/任意 | 備考                      |\n|--------------------|------------|--------------------------------------------------------------------------------------|-----------|---------------------------|\n| prompt             | string     | 画像生成プロンプト（英語推奨）                                                        | 必須      |                           |\n| aspect_ratio       | string     | アスペクト比（例: \"1x1\", \"16x9\", \"4x3\" など）                                        | 任意      | 15種類                    |\n| resolution         | string     | 解像度（公式ドキュメント参照、全69種）                                               | 任意      |                           |\n| seed               | integer    | 乱数シード（再現性担保用）                                                            | 任意      | 0～2147483647             |\n| magic_prompt       | string     | マジックプロンプト（\"AUTO\"|\"ON\"|\"OFF\"）                                               | 任意      | デフォルト\"AUTO\"          |\n| rendering_speed    | string     | v3用レンダリング速度（\"TURBO\"|\"DEFAULT\"|\"QUALITY\"）                                  | 任意      |                           |\n| style_codes        | string[]   | 8文字のスタイルコード配列                                                             | 任意      |                           |\n| style_type         | string     | スタイルタイプ（\"AUTO\"|\"GENERAL\"|\"REALISTIC\"|\"DESIGN\"）                              | 任意      |                           |\n| negative_prompt    | string     | 除外要素（英語推奨）                                                                  | 任意      |                           |\n| num_images         | number     | 生成画像数（1～8）                                                                    | 任意      |                           |\n| style_reference    | object     | スタイル参照（Ideogram 3.0新機能）                                                   | 任意      | 下記詳細                   |\n| └ urls             | string[]   | 参照画像URL配列（最大3つ）                                                            | 任意      |                           |\n| └ style_code       | string     | スタイルコード                                                                        | 任意      |                           |\n| └ random_style     | boolean    | ランダムスタイル使用                                                                  | 任意      |                           |\n| output_dir         | string     | 画像保存ディレクトリ（デフォルト: \"docs\"）                                            | 任意      |                           |\n| base_filename      | string     | 保存ファイル名のベース（デフォルト: \"ideogram-image\"）                                | 任意      | タイムスタンプ・ID付与     |\n| blur_mask          | boolean    | 画像の縁をぼかす（trueでマスク合成）                                                  | 任意      | デフォルト: false          |\n\n#### 📝 使用例\n\n```typescript\nconst result = await use_mcp_tool({\n  server_name: \"ideagram-mcp-server\",\n  tool_name: \"generate_image\",\n  arguments: {\n    prompt: \"A beautiful sunset over mountains\",\n    aspect_ratio: \"16x9\",\n    rendering_speed: \"QUALITY\",\n    num_images: 2,\n    style_reference: {\n      urls: [\n        \"https://example.com/ref1.jpg\",\n        \"https://example.com/ref2.jpg\"\n      ],\n      random_style: false\n    },\n    blur_mask: true\n  }\n});\n```\n\n---\n\n## 🧑‍💻 開発・ビルド・テスト\n\n- `npm run build` ... TypeScriptビルド\n- `npm run watch` ... 開発モード（自動ビルド）\n- `npm run lint` ... コードリント\n- `npm test` ... テスト実行\n\n---\n\n## 🗂️ ディレクトリ構成\n\n```bash\nideagram-mcp-server/\n├── assets/\n├── docs/\n│   └── ideogram-image_2025-05-18T06-31-45-777Z.png\n├── src/\n│   ├── tools/\n│   ├── types/\n│   ├── utils/\n│   ├── ideogram-client.ts\n│   ├── index.ts\n│   ├── server.ts\n│   └── test.ts\n├── .env.example\n├── package.json\n├── tsconfig.json\n├── README.md\n└── ...（省略）\n```\n\n---\n\n## 📝 コントリビューション\n\n1. このリポジトリをフォーク\n2. 新ブランチ作成 (`git checkout -b feature/awesome`)\n3. 変更コミット（コミットメッセージは日本語＋絵文字推奨！）\n4. プッシュ＆プルリク作成\n\n---\n\n## 🚀 デプロイ & リリース\n\n- GitHub Actionsで自動npm公開\n- バージョン更新→タグpushで自動デプロイ\n\n```bash\nnpm version patch|minor|major\ngit push --follow-tags\n```\n\n詳細は [docs/npm-deploy.md](docs/npm-deploy.md) を参照！\n\n---\n\n## 📄 ライセンス\n\nMIT\n\n---\n\n<div align=\"center\">\n\n![](assets/header-animation.svg)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ideagram",
        "ideogram",
        "images",
        "generate images",
        "ideagram mcp",
        "ideogram api"
      ],
      "category": "image-and-video-generation"
    },
    "SureScaleAI--openai-gpt-image-mcp": {
      "owner": "SureScaleAI",
      "name": "openai-gpt-image-mcp",
      "url": "https://github.com/SureScaleAI/openai-gpt-image-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SureScaleAI.webp",
      "description": "Generate and edit images using the latest OpenAI GPT-4o and gpt-image-1 models with advanced prompt control. Outputs can be saved to disk or received in base64 format for integration with MCP-compatible clients.",
      "stars": 74,
      "forks": 23,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:32:34Z",
      "readme_content": "# openai-gpt-image-mcp\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@modelcontextprotocol/sdk\"><img src=\"https://img.shields.io/npm/v/@modelcontextprotocol/sdk?label=MCP%20SDK&color=blue\" alt=\"MCP SDK\"></a>\n  <a href=\"https://www.npmjs.com/package/openai\"><img src=\"https://img.shields.io/npm/v/openai?label=OpenAI%20SDK&color=blueviolet\" alt=\"OpenAI SDK\"></a>\n  <a href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/SureScaleAI/openai-gpt-image-mcp?color=brightgreen\" alt=\"License\"></a>\n  <a href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/stargazers\"><img src=\"https://img.shields.io/github/stars/SureScaleAI/openai-gpt-image-mcp?style=social\" alt=\"GitHub stars\"></a>\n  <a href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/actions\"><img src=\"https://img.shields.io/github/actions/workflow/status/SureScaleAI/openai-gpt-image-mcp/main.yml?label=build&logo=github\" alt=\"Build Status\"></a>\n</p>\n\n---\n\nA Model Context Protocol (MCP) tool server for OpenAI's GPT-4o/gpt-image-1 image generation and editing APIs.\n\n- **Generate images** from text prompts using OpenAI's latest models.\n- **Edit images** (inpainting, outpainting, compositing) with advanced prompt control.\n- **Supports**: Claude Desktop, Cursor, VSCode, Windsurf, and any MCP-compatible client.\n\n---\n\n## ✨ Features\n\n- **create-image**: Generate images from a prompt, with advanced options (size, quality, background, etc).\n- **edit-image**: Edit or extend images using a prompt and optional mask, supporting both file paths and base64 input.\n- **File output**: Save generated images directly to disk, or receive as base64.\n\n---\n\n## 🚀 Installation\n\n```sh\ngit clone https://github.com/SureScaleAI/openai-gpt-image-mcp.git\ncd openai-gpt-image-mcp\nyarn install\nyarn build\n```\n\n---\n\n## 🔑 Configuration\n\nAdd to Claude Desktop or VSCode (including Cursor/Windsurf) config:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\"],\n      \"env\": { \"OPENAI_API_KEY\": \"sk-...\" }\n    }\n  }\n}\n```\n\nAlso supports Azure deployments:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\"],\n      \"env\": { \n        \"AZURE_OPENAI_API_KEY\": \"sk-...\",\n        \"AZURE_OPENAI_ENDPOINT\": \"my.endpoint.com\",\n        \"OPENAI_API_VERSION\": \"2024-12-01-preview\"\n      }\n    }\n  }\n}\n```\n\nAlso supports supplying an environment files:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\", \"--env-file\", \"./deployment/.env\"]\n    }\n  }\n}\n```\n\n---\n\n## ⚡ Advanced\n\n- For `create-image`, set `n` to generate up to 10 images at once.\n- For `edit-image`, provide a mask image (file path or base64) to control where edits are applied.\n- Provide an environment file with `--env-file path/to/file/.env`\n- See `src/index.ts` for all options.\n\n---\n\n## 🧑‍💻 Development\n\n- TypeScript source: `src/index.ts`\n- Build: `yarn build`\n- Run: `node dist/index.js`\n\n---\n\n## 📝 License\n\nMIT\n\n---\n\n## 🩺 Troubleshooting\n\n- Make sure your `OPENAI_API_KEY` is valid and has image API access.\n- You must have a [verified OpenAI organization](https://platform.openai.com/account/organization). After verifying, it can take 15–20 minutes for image API access to activate.\n- File paths must be absolute.\n  - **Unix/macOS/Linux**: Starting with `/` (e.g., `/path/to/image.png`)\n  - **Windows**: Drive letter followed by `:` (e.g., `C:/path/to/image.png` or `C:\\path\\to\\image.png`)\n- For file output, ensure the directory is writable.\n- If you see errors about file types, check your image file extensions and formats.\n\n---\n\n## ⚠️ Limitations & Large File Handling\n\n- **1MB Payload Limit:** MCP clients (including Claude Desktop) have a hard 1MB limit for tool responses. Large images (especially high-res or multiple images) can easily exceed this limit if returned as base64.\n- **Auto-Switch to File Output:** If the total image size exceeds 1MB, the tool will automatically save images to disk and return the file path(s) instead of base64. This ensures compatibility and prevents errors like `result exceeds maximum length of 1048576`.\n- **Default File Location:** If you do not specify a `file_output` path, images will be saved to `/tmp` (or the directory set by the `MCP_HF_WORK_DIR` environment variable) with a unique filename.\n- **Environment Variable:**\n  - `MCP_HF_WORK_DIR`: Set this to control where large images and file outputs are saved. Example: `export MCP_HF_WORK_DIR=/your/desired/dir`\n- **Best Practice:** For large or production images, always use file output and ensure your client is configured to handle file paths.\n\n---\n\n## 📚 References\n\n- [OpenAI Images API Documentation](https://platform.openai.com/docs/api-reference/images)\n\n---\n\n## 🙏 Credits\n\n- Built with [@modelcontextprotocol/sdk](https://www.npmjs.com/package/@modelcontextprotocol/sdk)\n- Uses [openai](https://www.npmjs.com/package/openai) Node.js SDK \n- Built by [SureScale.ai](https://surescale.ai)\n- Contributions from [Axle Research and Technology](https://axleinfo.com/)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "gpt",
        "mcp",
        "openai gpt",
        "gpt image",
        "image mcp"
      ],
      "category": "image-and-video-generation"
    },
    "agan2023416--workers": {
      "owner": "agan2023416",
      "name": "workers",
      "url": "https://github.com/agan2023416/workers",
      "imageUrl": "/freedevtools/mcp/pfp/agan2023416.webp",
      "description": "An MCP server for image generation that interfaces with a Cloudflare Worker to enable asynchronous image creation, real-time status updates, and error handling. It supports type-safe API calls for generating images based on given prompts.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T00:28:04Z",
      "readme_content": "# Cloudflare Workers Collection\n\nThis repository contains a collection of specialized Cloudflare Workers and related tools, each designed to provide specific functionality and services.\n\n## Available Projects\n\n### [replicate-2-r2](./replicate-2-r2)\nA worker that integrates Replicate's AI image generation with Cloudflare R2 storage. This worker:\n- Generates images using Replicate's API\n- Stores generated images in Cloudflare R2\n- Provides immediate URL generation\n- Supports webhook notifications\n- Includes MCP server integration for seamless AI tooling\n\n👉 [Learn more about replicate-2-r2](./replicate-2-r2)\n\n### [generate-image](./mcps/generate-image)\nA Model Context Protocol (MCP) server that provides a simple interface to the replicate-2-r2 worker. This server:\n- Interfaces with replicate-2-r2 worker\n- Provides type-safe API calls\n- Handles asynchronous image generation\n- Supports real-time status updates\n\n👉 [Learn more about generate-image](./mcps/generate-image)\n\n### [n8n-image-generator](./mcps/n8n-image-generator) ⭐ NEW\nA specialized MCP server designed specifically for n8n integration with SSE protocol support. This server:\n- **Perfect n8n Integration**: Works seamlessly with n8n's MCP Client Tool\n- **SSE Protocol Support**: Real-time communication using Server-Sent Events\n- **Multi-Model Support**: Supports Flux, Stable Diffusion, and more AI models\n- **Production Ready**: Built with error handling and monitoring\n- **Zero Worker Changes**: Uses existing replicate-2-r2 worker without modifications\n\n👉 [Learn more about n8n-image-generator](./mcps/n8n-image-generator)\n\n## 🚀 Quick Start for n8n Users\n\nIf you want to use AI image generation in n8n workflows, follow these steps:\n\n### 1. Deploy the Worker\n```bash\ncd replicate-2-r2\nnpm install\nnpm run deploy\n```\n\n### 2. Set up the MCP Server\n```bash\ncd mcps/n8n-image-generator\nnpm install\nnpm run build\n```\n\n### 3. Configure Environment\n```bash\nexport CLOUDFLARE_WORKERS_URL=https://your-worker.workers.dev\nexport WORKER_API_TOKEN=your-api-token\nnpm start\n```\n\n### 4. Add to n8n\nIn your n8n workflow:\n1. Add **MCP Client Tool** node\n2. Configure connection to your MCP server\n3. Use `generate_image` tool with your prompt\n\n## Architecture Overview\n\n```mermaid\ngraph TB\n    A[n8n Workflow] -->|SSE/MCP| B[n8n-image-generator MCP Server]\n    B -->|HTTP API| C[replicate-2-r2 Worker]\n    C -->|AI Generation| D[Replicate API]\n    C -->|Storage| E[Cloudflare R2]\n    C -->|Webhook| C\n    \n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n```\n\n## Getting Started\n\nEach project is contained in its own directory with its own documentation. To get started:\n\n1. Choose the project you want to use\n2. Navigate to its directory\n3. Follow the setup instructions in its README.md\n\n## Repository Structure\n\n```\ncloudflare-workers/\n├── README.md\n├── replicate-2-r2/         # Replicate integration worker\n│   ├── README.md           # Worker-specific documentation\n│   ├── src/                # Source code\n│   └── ...                 # Other worker files\n└── mcps/                   # MCP servers\n    ├── generate-image/     # Original MCP server\n    │   ├── README.md       # Server documentation\n    │   ├── src/           # Source code\n    │   └── ...            # Other server files\n    └── n8n-image-generator/ # ⭐ NEW: n8n-specific MCP server\n        ├── README.md       # Detailed setup guide\n        ├── src/           # TypeScript source\n        └── ...            # Configuration files\n```\n\n## 🔄 Migration from generate-image to n8n-image-generator\n\nIf you're currently using the original `generate-image` MCP server, consider migrating to `n8n-image-generator` for better n8n integration:\n\n### Benefits of n8n-image-generator:\n- ✅ **Better SSE Support**: Designed specifically for n8n's MCP Client Tool\n- ✅ **Enhanced Error Handling**: More robust error messages and logging\n- ✅ **Improved Performance**: Optimized for n8n workflow patterns\n- ✅ **Better Documentation**: Comprehensive setup and usage guides\n- ✅ **Active Development**: Focused on n8n use cases\n\n### Migration Steps:\n1. Install the new MCP server: `cd mcps/n8n-image-generator && npm install`\n2. Update your n8n MCP Client Tool configuration\n3. Test your workflows with the new server\n4. Enjoy improved reliability and performance!\n\nMore projects will be added to this collection in the future. Stay tuned for updates!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cloudflare",
        "mcp",
        "images",
        "server image",
        "generating images",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "aiyogg--tinypng-mcp-server": {
      "owner": "aiyogg",
      "name": "tinypng-mcp-server",
      "url": "https://github.com/aiyogg/tinypng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/aiyogg.webp",
      "description": "Compress images using the TinyPNG API to reduce file size while maintaining quality. Integrate image optimization into various projects seamlessly.",
      "stars": 4,
      "forks": 4,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-04-28T10:03:48Z",
      "readme_content": "## MCP server for TinyPNG\n[![smithery badge](https://smithery.ai/badge/@aiyogg/tinypng-mcp-server)](https://smithery.ai/server/@aiyogg/tinypng-mcp-server) [![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/aiyogg-tinypng-mcp-server)\n\n### Usage\n\n### Use `bun` or `node` to run the server\n\n1. Install dependencies and build\n\n```bash\npnpm i\npnpm build\n```\n\n2. Edit the `mcp.json` file\n\n```json\n{\n  \"mcpServers\": {\n    \"tinypng\": {\n      \"command\": \"bun\", // or \"node\"\n      \"args\": [\"/path/to/tinypng-mcp-server/src/index.ts\"], // or \"dist/index.js\"\n      \"env\": {\n        \"TINYPNG_API_KEY\": \"your-tinypng-api-key\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install TinyPNG MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@aiyogg/tinypng-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @aiyogg/tinypng-mcp-server --client claude\n```\n\n### Tools\n\n1. Compress local image\n\n```js\n{\n  name: 'compress_local_image',\n  description: 'Compress a local image file',\n  inputSchema: {\n    type: 'object',\n    properties: {\n      imagePath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to the image file to compress',\n        example: '/Users/user/Downloads/image.jpg',\n      },\n      outputPath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to save the compressed image file',\n        example: '/Users/user/Downloads/image_compressed.jpg',\n      },\n      outputFormat: {\n        type: 'string',\n        description: 'The format to save the compressed image file',\n        enum: SUPPORTED_IMAGE_TYPES,\n        example: 'image/jpeg',\n      },\n    },\n    required: ['imagePath'],\n  },\n}\n```\n\n2. Compress remote image\n\n```js\n{\n  name: 'compress_remote_image',\n  description: 'Compress a remote image file by giving the URL of the image',\n  inputSchema: {\n    type: 'object',\n    properties: {\n      imageUrl: {\n        type: 'string',\n        description: 'The URL of the image file to compress',\n        example: 'https://example.com/image.jpg',\n      },\n      outputPath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to save the compressed image file',\n        example: '/Users/user/Downloads/image_compressed.jpg',\n      },\n      outputFormat: {\n        type: 'string',\n        description: 'The format to save the compressed image file',\n        enum: SUPPORTED_IMAGE_TYPES,\n        example: 'image/jpeg',\n      },\n    },\n    required: ['imageUrl'],\n  },\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tinypng",
        "compress",
        "images",
        "tinypng mcp",
        "compress images",
        "using tinypng"
      ],
      "category": "image-and-video-generation"
    },
    "apinetwork--piapi-mcp-server": {
      "owner": "apinetwork",
      "name": "piapi-mcp-server",
      "url": "https://github.com/apinetwork/piapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/apinetwork.webp",
      "description": "Integrates with PiAPI's API to facilitate media content generation using various services like Midjourney, Flux, and more. It connects AI models with tools for seamless content creation directly from applications that support the Model Context Protocol.",
      "stars": 61,
      "forks": 22,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-27T21:10:22Z",
      "readme_content": "# piapi-mcp-server\n\n[![Website](https://img.shields.io/badge/Website-piapi.ai-blue?style=flat-square&logo=internet-explorer)](https://piapi.ai)\n[![Documentation](https://img.shields.io/badge/Documentation-docs-green?style=flat-square&logo=bookstack)](https://piapi.ai/docs)\n[![Discord](https://img.shields.io/badge/Discord-Join%20chat-7289da?style=flat-square&logo=discord)](https://discord.gg/qRRvcGa7Wb)\n\n[![smithery badge](https://smithery.ai/badge/piapi-mcp-server)](https://smithery.ai/server/piapi-mcp-server)\n\nA TypeScript implementation of a Model Context Protocol (MCP) server that integrates with PiAPI's API. PiAPI makes user able to generate media content with Midjourney/Flux/Kling/LumaLabs/Udio/Chrip/Trellis directly from Claude or any other MCP-compatible apps.\n\n<a href=\"https://glama.ai/mcp/servers/ywvke8xruo\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ywvke8xruo/badge\" alt=\"PiAPI-Server MCP server\" /></a>\n\n## Features (more coming soon)\n\nNote: Time-consuming tools like video generation may not complete due to Claude's timeout limitations\n\n- [x] Base Image toolkit\n- [x] Base Video toolkit\n- [x] Flux Image generation from text/image prompt\n- [x] Hunyuan Video generation from text/image prompt\n- [x] Skyreels Video generation from image prompt\n- [x] Wan Video generation from text/image prompt\n- [x] MMAudio Music generation from video\n- [x] TTS Zero-Shot voice generation\n- [ ] Midjourney Image generation\n  - [x] imagine\n  - [ ] other\n- [x] Kling Video and Effects generation\n- [x] Luma Dream Machine video generation\n- [x] Suno Music generation\n- [ ] Suno Lyrics generation\n- [ ] Udio Music and Lyrics generation\n- [x] Trellis 3D model generation from image\n- [ ] Workflow planning inside LLMs\n\n## Working with Claude Desktop Example\n\n![image](./assets/Claude-desktop.png)\n\n## Prerequisites\n\n- Node.js 16.x or higher\n- npm or yarn\n- A PiAPI API key (get one at [piapi.ai](https://piapi.ai/workspace/key))\n\n## Installation\n\n### Installing via Smithery\n\nTo install PiAPI MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/piapi-mcp-server):\n\n```bash\nnpx -y @smithery/cli install piapi-mcp-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/apinetwork/piapi-mcp-server\ncd piapi-mcp-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\nAfter building, a `dist/index.js` file will be generated. You can then configure this file with Claude Desktop and other applications. For detailed configuration instructions, please refer to the Usage section.\n\n4. (Optional) Test server with MCP Inspector:\n\nFirst, create a `.env` file in the project root directory with your API key:\n\n```bash\nPIAPI_API_KEY=your_api_key_here\n```\n\nThen run the following command to start the MCP Inspector:\n\n```bash\nnpm run inspect\n```\n\nAfter running the command, MCP Inspector will be available at http://localhost:5173 (default port: 5173). Open this URL in your browser to start testing. The default timeout for inspector operations is 10000ms (10 seconds), which may not be sufficient for image generation tasks. It's recommended to increase the timeout when testing image generation or other time-consuming operations. You can adjust the timeout by adding a timeout parameter to the URL, for example: http://localhost:5173?timeout=60000 (sets timeout to 60 seconds)\n\nThe MCP Inspector is a powerful development tool that helps you test and debug your MCP server implementation. Key features include:\n\n- **Interactive Testing**: Test your server's functions directly through a web interface\n- **Real-time Feedback**: See immediate results of your function calls and any errors that occur\n- **Request/Response Inspection**: View detailed information about requests and responses\n- **Function Documentation**: Browse available functions and their parameters\n- **Custom Parameters**: Set custom timeout values and other configuration options\n- **History Tracking**: Keep track of your previous function calls and their results\n\nFor detailed information about using the MCP Inspector and its features, visit the [official MCP documentation](https://modelcontextprotocol.io/docs/tools/inspector).\n\n## Usage\n\n### Connecting to Claude Desktop\n\nAdd this to your Claude Desktop configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"piapi\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/piapi-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"PIAPI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nAfter updating your configuration file, you need to restart Claude for Desktop. Upon restarting, you should see a hammer icon in the bottom right corner of the input box.\nFor more detailed information, visit the [official MCP documentation](https://modelcontextprotocol.io/quickstart/user)\n\n### Connecting to Cursor\n\nNote: Following guide is based on Cursor 0.47.5. Features and behaviors may vary in different versions.\n\nTo configure the MCP server:\n\n1. Navigate to: File > Preferences > Cursor Settings, or use the shortcut key `Ctrl+Shift+J`\n2. Select \"MCP\" tab on the left panel\n3. Click \"Add new global MCP server\" button in the top right\n4. Add your configuration in the opened mcp.json file\n\n```json\n{\n  \"mcpServers\": {\n    \"piapi\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/piapi-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"PIAPI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n5. After configuration, you'll see a \"piapi\" entry in MCP Servers page\n6. Click the Refresh button on the entry or restart Cursor to connect to the piapi server\n\nTo test the piapi image generation:\n\n1. Open and select \"Agent mode\" in Cursor Chat, or use the shortcut key `Ctrl+I`\n2. Enter a test prompt, for example: \"generate image of a dog\"\n3. The image will be generated based on your prompt using piapi server\n\nTo disable the piapi server:\n\n1. Navigate to the MCP Servers page in Cursor Settings\n2. Find the \"piapi\" entry in the server list\n3. Click the \"Enabled\" toggle button to switch it to \"Disabled\"\n\n## Development\n\n### Project Structure\n\n```\npiapi-mcp-server/\n├── assets/\n├── src/\n│   ├── index.ts        # Main server entry point\n├── package.json\n├── tsconfig.json\n└── .env.example\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "apinetwork",
        "piapi",
        "api",
        "piapi api",
        "apinetwork piapi",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "awkoy--replicate-flux-mcp": {
      "owner": "awkoy",
      "name": "replicate-flux-mcp",
      "url": "https://github.com/awkoy/replicate-flux-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/awkoy.webp",
      "description": "Generate images from text prompts using advanced AI models. Customize parameters for tailored outputs with secure and local processing.",
      "stars": 50,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T19:36:15Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/awkoy-replicate-flux-mcp-badge.png)](https://mseep.ai/app/awkoy-replicate-flux-mcp)\n\n# Replicate Flux MCP\n\n![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![TypeScript](https://img.shields.io/badge/TypeScript-4.9+-blue)\n![Model Context Protocol](https://img.shields.io/badge/MCP-Enabled-purple)\n[![smithery badge](https://smithery.ai/badge/@awkoy/replicate-flux-mcp)](https://smithery.ai/server/@awkoy/replicate-flux-mcp)\n![NPM Downloads](https://img.shields.io/npm/dw/replicate-flux-mcp)\n![Stars](https://img.shields.io/github/stars/awkoy/replicate-flux-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/ss8n1knen8\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ss8n1knen8/badge\" />\n</a>\n\n**Replicate Flux MCP** is an advanced Model Context Protocol (MCP) server that empowers AI assistants to generate high-quality images and vector graphics. Leveraging [Black Forest Labs' Flux Schnell model](https://replicate.com/black-forest-labs/flux-schnell) for raster images and [Recraft's V3 SVG model](https://replicate.com/recraft-ai/recraft-v3-svg) for vector graphics via the Replicate API.\n\n## 📑 Table of Contents\n\n- [Getting Started & Integration](#-getting-started--integration)\n  - [Setup Process](#setup-process)\n  - [Cursor Integration](#cursor-integration)\n  - [Claude Desktop Integration](#claude-desktop-integration)\n  - [Smithery Integration](#smithery-integration)\n  - [Glama.ai Integration](#glamaai-integration)\n- [Features](#-features)\n- [Documentation](#-documentation)\n  - [Available Tools](#available-tools)\n  - [Available Resources](#available-resources)\n- [Development](#-development)\n- [Technical Details](#-technical-details)\n- [Troubleshooting](#-troubleshooting)\n- [Contributing](#-contributing)\n- [License](#-license)\n- [Resources](#-resources)\n- [Examples](#-examples)\n\n## 🚀 Getting Started & Integration\n\n### Setup Process\n\n1. **Obtain a Replicate API Token**\n   - Sign up at [Replicate](https://replicate.com/)\n   - Create an API token in your account settings\n\n2. **Choose Your Integration Method**\n   - Follow one of the integration options below based on your preferred MCP client\n\n3. **Ask Your AI Assistant to Generate an Image**\n   - Simply ask naturally: \"Can you generate an image of a serene mountain landscape at sunset?\"\n   - Or be more specific: \"Please create an image showing a peaceful mountain scene with a lake reflecting the sunset colors in the foreground\"\n\n4. **Explore Advanced Features**\n   - Try different parameter settings for customized results\n   - Experiment with SVG generation using `generate_svg`\n   - Use batch image generation or variant generation features\n\n### Cursor Integration\n\n#### Method 1: Using mcp.json\n\n1. Create or edit the `.cursor/mcp.json` file in your project directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"replicate-flux-mcp\": {\n      \"command\": \"env REPLICATE_API_TOKEN=YOUR_TOKEN npx\",\n      \"args\": [\"-y\", \"replicate-flux-mcp\"]\n    }\n  }\n}\n```\n\n2. Replace `YOUR_TOKEN` with your actual Replicate API token\n3. Restart Cursor to apply the changes\n\n#### Method 2: Manual Mode\n\n1. Open Cursor and go to Settings\n2. Navigate to the \"MCP\" or \"Model Context Protocol\" section\n3. Click \"Add Server\" or equivalent\n4. Enter the following command in the appropriate field:\n\n```\nenv REPLICATE_API_TOKEN=YOUR_TOKEN npx -y replicate-flux-mcp\n```\n\n5. Replace `YOUR_TOKEN` with your actual Replicate API token\n6. Save the settings and restart Cursor if necessary\n\n### Claude Desktop Integration\n\n1. Create or edit the `mcp.json` file in your configuration directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"replicate-flux-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"replicate-flux-mcp\"],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"YOUR TOKEN\"\n      }\n    }\n  }\n}\n```\n\n2. Replace `YOUR_TOKEN` with your actual Replicate API token\n3. Restart Claude Desktop to apply the changes\n\n### Smithery Integration\n\nThis MCP server is available as a hosted service on Smithery, allowing you to use it without setting up your own server.\n\n1. Visit [Smithery](https://smithery.ai/) and create an account if you don't have one\n2. Navigate to the [Replicate Flux MCP server page](https://smithery.ai/server/@awkoy/replicate-flux-mcp)\n3. Click \"Add to Workspace\" to add the server to your Smithery workspace\n4. Configure your MCP client (Cursor, Claude Desktop, etc.) to use your Smithery workspace URL\n\nFor more information on using Smithery with your MCP clients, visit the [Smithery documentation](https://smithery.ai/docs).\n\n### Glama.ai Integration\n\nThis MCP server is also available as a hosted service on Glama.ai, providing another option to use it without local setup.\n\n1. Visit [Glama.ai](https://glama.ai/) and create an account if you don't have one\n2. Go to the [Replicate Flux MCP server page](https://glama.ai/mcp/servers/ss8n1knen8)\n3. Click \"Install Server\" to add the server to your workspace\n4. Configure your MCP client to use your Glama.ai workspace\n\nFor more information, visit the [Glama.ai MCP servers documentation](https://glama.ai/mcp/servers).\n\n## 🌟 Features\n\n- **🖼️ High-Quality Image Generation** - Create stunning images using Flux Schnell, a state-of-the-art AI model\n- **🎨 Vector Graphics Support** - Generate professional SVG vector graphics with Recraft V3 SVG model\n- **🤖 AI Assistant Integration** - Seamlessly enable AI assistants like Claude to generate visual content\n- **🎛️ Advanced Customization** - Fine-tune generation with controls for aspect ratio, quality, resolution, and more\n- **🔌 Universal MCP Compatibility** - Works with all MCP clients including Cursor, Claude Desktop, Cline, and Zed\n- **🔒 Secure Local Processing** - All requests are processed locally for enhanced privacy and security\n- **🔍 Comprehensive History Management** - Track, view, and retrieve your complete generation history\n- **📊 Batch Processing** - Generate multiple images from different prompts in a single request\n- **🔄 Variant Exploration** - Create and compare multiple interpretations of the same concept\n- **✏️ Prompt Engineering** - Fine-tune image variations with specialized prompt modifications\n\n## 📚 Documentation\n\n### Available Tools\n\n#### `generate_image`\n\nGenerates an image based on a text prompt using the Flux Schnell model.\n\n```typescript\n{\n  prompt: string;                // Required: Text description of the image to generate\n  seed?: number;                 // Optional: Random seed for reproducible generation\n  go_fast?: boolean;             // Optional: Run faster predictions with optimized model (default: true)\n  megapixels?: \"1\" | \"0.25\";     // Optional: Image resolution (default: \"1\")\n  num_outputs?: number;          // Optional: Number of images to generate (1-4) (default: 1)\n  aspect_ratio?: string;         // Optional: Aspect ratio (e.g., \"16:9\", \"4:3\") (default: \"1:1\")\n  output_format?: string;        // Optional: Output format (\"webp\", \"jpg\", \"png\") (default: \"webp\")\n  output_quality?: number;       // Optional: Image quality (0-100) (default: 80)\n  num_inference_steps?: number;  // Optional: Number of denoising steps (1-4) (default: 4)\n  disable_safety_checker?: boolean; // Optional: Disable safety filter (default: false)\n}\n```\n\n#### `generate_multiple_images`\n\nGenerates multiple images based on an array of prompts using the Flux Schnell model.\n\n```typescript\n{\n  prompts: string[];             // Required: Array of text descriptions for images to generate (1-10 prompts)\n  seed?: number;                 // Optional: Random seed for reproducible generation\n  go_fast?: boolean;             // Optional: Run faster predictions with optimized model (default: true)\n  megapixels?: \"1\" | \"0.25\";     // Optional: Image resolution (default: \"1\")\n  aspect_ratio?: string;         // Optional: Aspect ratio (e.g., \"16:9\", \"4:3\") (default: \"1:1\")\n  output_format?: string;        // Optional: Output format (\"webp\", \"jpg\", \"png\") (default: \"webp\")\n  output_quality?: number;       // Optional: Image quality (0-100) (default: 80)\n  num_inference_steps?: number;  // Optional: Number of denoising steps (1-4) (default: 4)\n  disable_safety_checker?: boolean; // Optional: Disable safety filter (default: false)\n}\n```\n\n#### `generate_image_variants`\n\nGenerates multiple variants of the same image from a single prompt.\n\n```typescript\n{\n  prompt: string;                // Required: Text description for the image to generate variants of\n  num_variants: number;          // Required: Number of image variants to generate (2-10, default: 4)\n  prompt_variations?: string[];  // Optional: List of prompt modifiers to apply to variants (e.g., [\"in watercolor style\", \"in oil painting style\"])\n  variation_mode?: \"append\" | \"replace\"; // Optional: How to apply variations - 'append' adds to base prompt, 'replace' uses variations directly (default: \"append\")\n  seed?: number;                 // Optional: Base random seed. Each variant will use seed+variant_index\n  go_fast?: boolean;             // Optional: Run faster predictions with optimized model (default: true)\n  megapixels?: \"1\" | \"0.25\";     // Optional: Image resolution (default: \"1\")\n  aspect_ratio?: string;         // Optional: Aspect ratio (e.g., \"16:9\", \"4:3\") (default: \"1:1\")\n  output_format?: string;        // Optional: Output format (\"webp\", \"jpg\", \"png\") (default: \"webp\")\n  output_quality?: number;       // Optional: Image quality (0-100) (default: 80)\n  num_inference_steps?: number;  // Optional: Number of denoising steps (1-4) (default: 4)\n  disable_safety_checker?: boolean; // Optional: Disable safety filter (default: false)\n}\n```\n\n#### `generate_svg`\n\nGenerates an SVG vector image based on a text prompt using the Recraft V3 SVG model.\n\n```typescript\n{\n  prompt: string;                // Required: Text description of the SVG to generate\n  size?: string;                 // Optional: Size of the generated SVG (default: \"1024x1024\")\n  style?: string;                // Optional: Style of the generated image (default: \"any\")\n                                // Options: \"any\", \"engraving\", \"line_art\", \"line_circuit\", \"linocut\"\n}\n```\n\n#### `prediction_list`\n\nRetrieves a list of your recent predictions from Replicate.\n\n```typescript\n{\n  limit?: number;  // Optional: Maximum number of predictions to return (1-100) (default: 50)\n}\n```\n\n#### `get_prediction`\n\nGets detailed information about a specific prediction.\n\n```typescript\n{\n  predictionId: string;  // Required: ID of the prediction to retrieve\n}\n```\n\n### Available Resources\n\n#### `imagelist`\n\nBrowse your history of generated images created with the Flux Schnell model.\n\n#### `svglist`\n\nBrowse your history of generated SVG images created with the Recraft V3 SVG model.\n\n#### `predictionlist`\n\nBrowse all your Replicate predictions history.\n\n## 💻 Development\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/awkoy/replicate-flux-mcp.git\ncd replicate-flux-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Start development mode:\n\n```bash\nnpm run dev\n```\n\n4. Build the project:\n\n```bash\nnpm run build\n```\n\n5. Connect to Client:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-generation-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"/Users/{USERNAME}/{PATH_TO}/replicate-flux-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"YOUR REPLICATE API TOKEN\"\n      }\n    }\n  }\n}\n```\n\n## ⚙️ Technical Details\n\n### Stack\n\n- **Model Context Protocol SDK** - Core MCP functionality for tool and resource management\n- **Replicate API** - Provides access to state-of-the-art AI image generation models\n- **TypeScript** - Ensures type safety and leverages modern JavaScript features\n- **Zod** - Implements runtime type validation for robust API interactions\n\n### Configuration\n\nThe server can be configured by modifying the `CONFIG` object in `src/config/index.ts`:\n\n```javascript\nconst CONFIG = {\n  serverName: \"replicate-flux-mcp\",\n  serverVersion: \"0.1.2\",\n  imageModelId: \"black-forest-labs/flux-schnell\",\n  svgModelId: \"recraft-ai/recraft-v3-svg\",\n  pollingAttempts: 25,\n  pollingInterval: 2000, // ms\n};\n```\n\n## 🔍 Troubleshooting\n\n### Common Issues\n\n#### Authentication Error\n- Ensure your `REPLICATE_API_TOKEN` is correctly set in the environment\n- Verify your token is valid by testing it with the Replicate API directly\n\n#### Safety Filter Triggered\n- The model has a built-in safety filter that may block certain prompts\n- Try modifying your prompt to avoid potentially problematic content\n\n#### Timeout Error\n- For larger images or busy servers, you might need to increase `pollingAttempts` or `pollingInterval` in the configuration\n- Default settings should work for most use cases\n\n## 🤝 Contributing\n\nContributions are welcome! Please follow these steps to contribute:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nFor feature requests or bug reports, please create a GitHub issue. If you like this project, consider starring the repository!\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🔗 Resources\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io)\n- [Replicate API Documentation](https://replicate.com/docs)\n- [Flux Schnell Model](https://replicate.com/black-forest-labs/flux-schnell)\n- [Recraft V3 SVG Model](https://replicate.com/recraft-ai/recraft-v3-svg)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [Smithery Documentation](https://smithery.ai/docs)\n- [Glama.ai MCP Servers](https://glama.ai/mcp/servers)\n\n## 🎨 Examples\n\n![Demo](https://github.com/user-attachments/assets/ad6db606-ae3a-48db-a1cc-e1f88847769e)\n\n| Multiple Prompts | Prompt Variants |\n|-----------------|-----------------|\n| ![Multiple prompts example: \"A serene mountain lake at sunset\", \"A bustling city street at night\", \"A peaceful garden in spring\"](https://github.com/user-attachments/assets/e5ac56d2-bfbb-4f33-938c-a3d7bffeee60) | ![Variants example: Base prompt \"A majestic castle\" with modifiers \"in watercolor style\", \"as an oil painting\", \"with gothic architecture\"](https://github.com/user-attachments/assets/8ebe5992-4803-4bf3-a82a-251135b0698a) |\n\nHere are some examples of how to use the tools:\n\n### Batch Image Generation with `generate_multiple_images`\n\nCreate multiple distinct images at once with different prompts:\n\n```json\n{\n  \"prompts\": [\n    \"A red sports car on a mountain road\", \n    \"A blue sports car on a beach\", \n    \"A vintage sports car in a city street\"\n  ]\n}\n```\n\n### Image Variants with `generate_image_variants`\n\nCreate different interpretations of the same concept using seeds:\n\n```json\n{\n  \"prompt\": \"A futuristic city skyline at night\",\n  \"num_variants\": 4,\n  \"seed\": 42\n}\n```\n\nOr explore style variations with prompt modifiers:\n\n```json\n{\n  \"prompt\": \"A character portrait\",\n  \"prompt_variations\": [\n    \"in anime style\", \n    \"in watercolor style\", \n    \"in oil painting style\", \n    \"as a 3D render\"\n  ]\n}\n```\n\n---\n\nMade with ❤️ by Yaroslav Boiko\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "mcp",
        "prompts",
        "mcp generate",
        "generate images",
        "awkoy replicate"
      ],
      "category": "image-and-video-generation"
    },
    "bendusy--pollinations-mcp": {
      "owner": "bendusy",
      "name": "pollinations-mcp",
      "url": "https://github.com/bendusy/pollinations-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bendusy.webp",
      "description": "Connects AI models to Pollinations.ai's services for generating images and text via the MCP protocol. Facilitates seamless interaction with Pollinations.ai's API for image generation, downloading images, and text generation.",
      "stars": 8,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-04T11:40:45Z",
      "readme_content": "# Pollinations MCP 服务器\n\n<div align=\"center\">\n  <img src=\"./icon.png\" alt=\"Pollinations MCP 服务器图标\" width=\"200\">\n</div>\n\n这是一个基于[Model Context Protocol (MCP)](https://github.com/microsoft/modelcontextprotocol)的服务器实现，用于连接[Pollinations.ai](https://pollinations.ai)服务的API接口。该服务器允许AI模型通过MCP协议调用Pollinations.ai的图像和文本生成功能。\n\n## 功能特点\n\n- 支持通过MCP协议与Pollinations.ai服务交互\n- 提供三个主要工具：\n  - `generate_image`: 使用Pollinations.ai生成图像并返回URL（默认无水印）\n  - `download_image`: 下载生成的图像到本地文件\n  - `generate_text`: 使用Pollinations.ai生成文本\n- 基于TypeScript实现，支持类型安全\n- 使用stdio传输机制，便于与AI模型集成\n\n## 安装\n\n1. 克隆仓库：\n\n```bash\ngit clone https://github.com/bendusy/pollinations-mcp.git\ncd pollinations-mcp\n```\n\n2. 安装依赖：\n\n```bash\nnpm install\n```\n\n3. 构建项目：\n\n```bash\nnpm run build\n```\n\n## 使用方法\n\n### 作为MCP服务器运行\n\n```bash\nnpm start\n```\n\n服务器将通过标准输入/输出(stdio)启动，等待MCP客户端连接。\n\n### 在Cursor中使用（当前可能无法正常工作）\n\n**注意：** 目前在Cursor中配置此服务器可能不会成功。如果您需要使用此功能，建议使用Cline（见下文）。\n\n### 在Cline中使用（推荐）\n\n[Cline](https://cline.app)是一个支持MCP协议的AI终端，可以成功使用本服务器提供的图像生成功能。设置步骤如下：\n\n1. 安装并启动Cline\n2. 打开Cline的设置文件，通常位于：\n   - Windows: `%APPDATA%\\Cline\\config.json`\n   - Mac: `~/Library/Application Support/Cline/config.json`\n   - Linux: `~/.config/Cline/config.json`\n\n3. 在配置文件中找到或添加`mcpServers`部分，然后添加以下配置：\n\n```json\n\"mcpServers\": {\n  \"pollinations-mcp\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"完整路径/到您的/pollinations-mcp/dist/index.js\"\n    ],\n    \"disabled\": false,\n    \"autoApprove\": [\n      \"download_image\",\n      \"generate_image\",\n      \"generate_text\"\n    ]\n  }\n}\n```\n\n例如，Windows系统上的完整配置可能如下：\n\n```json\n\"mcpServers\": {\n  \"pollinations-mcp\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"C:\\\\Users\\\\用户名\\\\路径\\\\到\\\\pollinations-mcp\\\\dist\\\\index.js\"\n    ],\n    \"disabled\": false,\n    \"autoApprove\": [\n      \"download_image\",\n      \"generate_image\",\n      \"generate_text\"\n    ]\n  }\n}\n```\n\n4. 保存配置文件并重启Cline\n5. 现在您可以在Cline中使用Pollinations图像生成功能了，例如：\n\n```\n使用Pollinations生成图像：beautiful sunset over ocean with palm trees\n```\n\n### 与AI模型集成\n\n本服务器设计用于与支持MCP协议的AI模型集成，使其能够生成图像。\n\n### 支持的工具\n\n#### generate_image\n\n使用Pollinations.ai生成图像并返回URL。\n\n参数：\n- `prompt` (必需): 图像描述提示词\n- `width` (可选): 图像宽度（像素），默认为1024\n- `height` (可选): 图像高度（像素），默认为1024\n- `seed` (可选): 随机种子值（用于生成一致的图像）\n- `model` (可选): 要使用的模型，默认为'flux'\n- `nologo` (可选): 设置为true可去除水印，默认为true\n- `enhance` (可选): 提高图像质量（应用增强滤镜），默认为false\n- `safe` (可选): 启用安全过滤（过滤不适内容），默认为false\n- `private` (可选): 设置为true可使图像私有（不在公共feed中显示），默认为false\n\n**提示词最佳实践：**\n- 尽量使用英文编写提示词，Pollinations.ai对英文的理解更好\n- 保持提示词简短精确，避免过长或模糊的描述\n- 使用具体的形容词和名词，而非抽象概念\n- 例如：\"beautiful sunset over ocean with palm trees\"比\"一张日落的图片\"效果更好\n\n#### download_image\n\n下载Pollinations.ai生成的图像到本地文件。\n\n参数：\n- `url` (必需): 要下载的图像URL\n- `output_path` (可选): 保存图像的路径（包括文件名），默认为'image.jpg'\n\n#### generate_text\n\n使用Pollinations.ai生成文本。\n\n参数：\n- `prompt` (必需): 文本提示词\n- `model` (可选): 要使用的模型（如openai、mistral等），默认为'openai'\n- `seed` (可选): 随机种子值（用于生成一致的结果）\n- `system` (可选): 系统提示词（设置AI行为）\n- `json` (可选): 是否返回JSON格式的响应，默认为false\n- `private` (可选): 设置为true可使响应私有，默认为false\n\n## API参考\n\n本项目使用Pollinations.ai的官方API。完整的API文档请参考：[Pollinations API文档](https://github.com/pollinations/pollinations/blob/master/APIDOCS.md)\n\n### 图像生成API\n\n基本格式：`https://image.pollinations.ai/prompt/{prompt}?{参数}`\n\n示例：\n```\nhttps://image.pollinations.ai/prompt/beautiful%20sunset?width=1024&height=1024&nologo=true\n```\n\n### 可用的图像模型\n\n- `flux` (默认): 主流文生图模型，功能全面\n- `variation`: 图像变体生成\n- `dreamshaper`: 梦幻风格\n- `anything`: 动漫风格图像\n- `pixart`: 高质量插图风格\n\n### 文本生成API\n\n基本格式：`https://text.pollinations.ai/{prompt}?{参数}`\n\n示例：\n```\nhttps://text.pollinations.ai/Tell%20me%20about%20artificial%20intelligence?model=openai\n```\n\n### 可用的文本模型\n\n- `openai` (默认): OpenAI模型\n- `mistral`: Mistral模型\n- `gemini`: Google Gemini模型\n\n## 开发\n\n### 项目结构\n\n- `src/index.ts`: 主服务器实现\n- `dist/`: 编译后的JavaScript文件\n- `package.json`: 项目配置和依赖\n\n### 依赖\n\n- `@modelcontextprotocol/sdk`: MCP协议SDK\n- `axios`: HTTP客户端，用于下载图像\n- `typescript`: TypeScript编译器\n\n## 许可\n\n本项目采用ISC许可证。详情请参阅[LICENSE](LICENSE)文件。\n\n## 相关链接\n\n- [Pollinations.ai](https://pollinations.ai)\n- [Model Context Protocol](https://github.com/microsoft/modelcontextprotocol)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "pollinations",
        "images",
        "pollinations ai",
        "generating images",
        "ai api"
      ],
      "category": "image-and-video-generation"
    },
    "beordle--tinypng-mcp-server": {
      "owner": "beordle",
      "name": "tinypng-mcp-server",
      "url": "https://github.com/beordle/tinypng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/beordle.webp",
      "description": "Compress images efficiently using the TinyPNG API. Supports both local and remote image compression with minimal setup required.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-03-31T16:01:05Z",
      "readme_content": "## MCP server for TinyPNG\n\n### Usage\n\n### Use `bun` or `node` to run the server\n\n1. Install dependencies and build\n\n```bash\npnpm i\npnpm build\n```\n\n2. Edit the `mcp.json` file\n\n```json\n{\n  \"mcpServers\": {\n    \"tinypng\": {\n      \"command\": \"bun\", // or \"node\"\n      \"args\": [\"/path/to/tinypng-mcp-server/src/index.ts\"], // or \"dist/index.js\"\n      \"env\": {\n        \"TINYPNG_API_KEY\": \"your-tinypng-api-key\"\n      }\n    }\n  }\n}\n```\n\n### Tools\n\n1. Compress local image\n\n```js\n{\n  name: 'compress_local_image',\n  description: 'Compress a local image file',\n  inputSchema: {\n    type: 'object',\n    properties: {\n      imagePath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to the image file to compress',\n        example: '/Users/user/Downloads/image.jpg',\n      },\n      outputPath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to save the compressed image file',\n        example: '/Users/user/Downloads/image_compressed.jpg',\n      },\n      outputFormat: {\n        type: 'string',\n        description: 'The format to save the compressed image file',\n        enum: SUPPORTED_IMAGE_TYPES,\n        example: 'image/jpeg',\n      },\n    },\n    required: ['imagePath'],\n  },\n}\n```\n\n2. Compress remote image\n\n```js\n{\n  name: 'compress_remote_image',\n  description: 'Compress a remote image file by giving the URL of the image',\n  inputSchema: {\n    type: 'object',\n    properties: {\n      imageUrl: {\n        type: 'string',\n        description: 'The URL of the image file to compress',\n        example: 'https://example.com/image.jpg',\n      },\n      outputPath: {\n        type: 'string',\n        description: 'The ABSOLUTE path to save the compressed image file',\n        example: '/Users/user/Downloads/image_compressed.jpg',\n      },\n      outputFormat: {\n        type: 'string',\n        description: 'The format to save the compressed image file',\n        enum: SUPPORTED_IMAGE_TYPES,\n        example: 'image/jpeg',\n      },\n    },\n    required: ['imageUrl'],\n  },\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tinypng",
        "compression",
        "compress",
        "compress images",
        "tinypng mcp",
        "beordle tinypng"
      ],
      "category": "image-and-video-generation"
    },
    "bitscorp-mcp--mcp-ffmpeg": {
      "owner": "bitscorp-mcp",
      "name": "mcp-ffmpeg",
      "url": "https://github.com/bitscorp-mcp/mcp-ffmpeg",
      "imageUrl": "/freedevtools/mcp/pfp/bitscorp-mcp.webp",
      "description": "Manipulate video files by resizing them to various resolutions and extracting audio in multiple formats. Interact with video processing capabilities using natural language requests via API calls.",
      "stars": 35,
      "forks": 13,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T04:42:34Z",
      "readme_content": "# MCP FFmpeg Video Processor\n[![smithery badge](https://smithery.ai/badge/@bitscorp-mcp/mcp-ffmpeg)](https://smithery.ai/server/@bitscorp-mcp/mcp-ffmpeg)\n\nA Node.js server that uses FFmpeg to manipulate video files. This server provides APIs to:\n\n- Resize videos to different resolutions (360p, 480p, 720p, 1080p)\n- Extract audio from videos in various formats (MP3, AAC, WAV, OGG)\n\n## Prerequisites\n\nBefore running this application, you need to have the following installed:\n\n1. **Node.js** (v14 or higher)\n2. **FFmpeg** - This is required for video processing\n\n### Installing FFmpeg\n\n#### On macOS:\n```bash\nbrew install ffmpeg\n```\n\n#### On Ubuntu/Debian:\n```bash\nsudo apt update\nsudo apt install ffmpeg\n```\n\n#### On Windows:\n1. Download FFmpeg from the [official website](https://ffmpeg.org/download.html)\n2. Extract the files to a folder (e.g., `C:\\ffmpeg`)\n3. Add the `bin` folder to your PATH environment variable\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/bitscorp-mcp/mcp-ffmpeg.git\ncd mcp-ffmpeg\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n### Installing via Smithery\n\nTo install mcp-ffmpeg for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@bitscorp-mcp/mcp-ffmpeg):\n\n```bash\nnpx -y @smithery/cli install @bitscorp-mcp/mcp-ffmpeg --client claude\n```\n\n## Running the Server\n\nStart the server with:\n\n```bash\nnpm start\n```\n\nFor development with auto-restart on file changes:\n\n```bash\nnpm run dev\n```\n\n### Installing via Smithery\n\nTo install mcp-ffmpeg for Claude Desktop automatically via [Smithery](https://smithery.ai/server/bitscorp-mcp/mcp-ffmpeg):\n\n```bash\nnpx -y @smithery/cli install @bitscorp-mcp/mcp-ffmpeg --client claude\n```\n\nTo install mcp-ffmpeg for Cursor, go to Settings -> Cursor Settings -> Features -> MCP Servers -> + Add\n\nSelect Type: command and paste the below, using your API key from Adjust\n```\nnpx -y @smithery/cli@latest run @bitscorp/mcp-ffmpeg\n```\n\n## Using with Claude Desktop\n\nThis MCP FFmpeg server can be integrated with Claude Desktop to process videos through natural language requests.\n\n### Running with npx\n\nYou can run the server directly with npx:\n\n```bash\nnpx /path/to/mcp-ffmpeg\n```\n\nOr if you've published the package to npm:\n\n```bash\nnpx mcp-ffmpeg\n```\n\n### Configuring Claude Desktop\n\nTo add this server to Claude Desktop, update your Claude Desktop configuration file:\n\n1. Locate your Claude Desktop config file:\n   - macOS: `~/.config/claude-desktop/config.json` or `~/Library/Application Support/Claude Desktop/config.json`\n   - Windows: `%APPDATA%\\Claude Desktop\\config.json`\n   - Linux: `~/.config/claude-desktop/config.json`\n\n2. Add the FFmpeg MCP server to the `mcpServers` section:\n\n```json\n{\n    \"mcpServers\": {\n        \"ffmpeg\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"--yes\",\n                \"/absolute/path/to/mcp-ffmpeg\"\n            ]\n        }\n    }\n}\n```\n\nIf you've published the package to npm:\n\n```json\n{\n    \"mcpServers\": {\n        \"ffmpeg\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"--yes\",\n                \"mcp-ffmpeg\"\n            ]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop for the changes to take effect.\n\n### Example Prompts for Claude\n\nOnce configured, you can use prompts like:\n\n```\nUsing the ffmpeg MCP server, please resize the video at /path/to/video.mp4 to 720p resolution.\n```\n\n## Notes\n\n- Uploaded videos are stored temporarily in the `uploads` directory\n- Processed videos and audio files are stored in the `output` directory\n- The server has a file size limit of 500MB for uploads\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ffmpeg",
        "audio",
        "formats",
        "mcp ffmpeg",
        "video generation",
        "video processing"
      ],
      "category": "image-and-video-generation"
    },
    "bobtista--luma-ai-mcp-server": {
      "owner": "bobtista",
      "name": "luma-ai-mcp-server",
      "url": "https://github.com/bobtista/luma-ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/bobtista.webp",
      "description": "Integrates with Luma AI's Dream Machine API to facilitate the generation and manipulation of AI-generated videos and images. Offers tools for text-to-video generation, image processing, and audio integration to enhance creative projects.",
      "stars": 3,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-23T00:09:33Z",
      "readme_content": "# Luma AI MCP Server 🎥\n\nA Model Context Protocol server for Luma AI's Dream Machine API.\n\n## Overview\n\nThis MCP server integrates with Luma AI's Dream Machine API (v1) to provide tools for generating, managing, and manipulating AI-generated videos and images via Large Language Models. It implements the Model Context Protocol (MCP) to enable seamless interaction between AI assistants and Luma's creative tools.\n\n## Features ✨\n\n- Text-to-video generation\n- Advanced video generation with keyframes\n- Image-to-video conversion\n- Video extension and interpolation\n- Image generation with reference images\n- Audio addition to videos\n- Video upscaling\n- Credit management\n- Generation tracking and status checking\n\n## Tools 🛠️\n\n1. `ping`\n\n   - Check if the Luma API is running\n   - No parameters required\n\n2. `create_generation`\n\n   - Creates a new video generation\n   - Input:\n     - `prompt` (string, required): Text description of the video to generate\n     - `model` (string, optional): Model to use (default: \"ray-2\")\n       - Available models: \"ray-1-6\", \"ray-2\", \"ray-flash-2\"\n     - `resolution` (string, optional): Video resolution (choices: \"540p\", \"720p\", \"1080p\", \"4k\")\n     - `duration` (string, optional): Video duration (only \"5s\" and \"9s\" are currently supported)\n     - `aspect_ratio` (string, optional): Video aspect ratio (e.g., \"16:9\", \"1:1\", \"9:16\", \"4:3\", \"3:4\", \"21:9\", \"9:21\")\n     - `loop` (boolean, optional): Whether to make the video loop\n     - `keyframes` (object, optional): Start and end frames for advanced video generation:\n       - `frame0` and/or `frame1` with either:\n         - `{\"type\": \"image\", \"url\": \"image_url\"}` for image keyframes\n         - `{\"type\": \"generation\", \"id\": \"generation_id\"}` for video keyframes\n\n3. `get_generation`\n\n   - Gets the status of a generation\n   - Input:\n     - `generation_id` (string, required): ID of the generation to check\n   - Output includes:\n     - Generation ID\n     - State (queued, dreaming, completed, failed)\n     - Failure reason (if failed)\n     - Video URL (if completed)\n\n4. `list_generations`\n\n   - Lists all generations\n   - Input:\n     - `limit` (number, optional): Maximum number of generations to return (default: 10)\n     - `offset` (number, optional): Number of generations to skip\n\n5. `delete_generation`\n\n   - Deletes a generation\n   - Input:\n     - `generation_id` (string, required): ID of the generation to delete\n\n6. `upscale_generation`\n\n   - Upscales a video generation to higher resolution\n   - Input:\n     - `generation_id` (string, required): ID of the generation to upscale\n     - `resolution` (string, required): Target resolution for the upscaled video (one of \"540p\", \"720p\", \"1080p\", or \"4k\")\n   - Note:\n     - The generation must be in a completed state to be upscaled\n     - The target resolution must be higher than the original generation's resolution\n     - Each generation can only be upscaled once\n\n7. `add_audio`\n\n   - Adds AI-generated audio to a video generation\n   - Input:\n     - `generation_id` (required): The ID of the generation to add audio to\n     - `prompt` (required): The prompt for the audio generation\n     - `negative_prompt` (optional): The negative prompt for the audio generation\n     - `callback_url` (optional): URL to notify when the audio processing is complete\n\n8. `generate_image`\n\n   - Generates an image from a text prompt with optional reference images\n   - Input:\n     - `prompt` (string, required): Text description of the image to generate\n     - `model` (string, optional): Model to use for image generation (default: \"photon-1\")\n       - Available models: \"photon-1\", \"photon-flash-1\"\n     - `aspect_ratio` (string, optional): Image aspect ratio (same options as video)\n     - `image_ref` (array, optional): Reference images to guide generation\n       - Each ref: `{\"url\": \"image_url\", \"weight\": optional_float}`\n     - `style_ref` (array, optional): Style reference images\n       - Each ref: `{\"url\": \"image_url\", \"weight\": optional_float}`\n     - `character_ref` (object, optional): Character reference images\n       - Format: `{\"identity_name\": {\"images\": [\"url1\", \"url2\", ...]}}`\n     - `modify_image_ref` (object, optional): Image to modify\n       - Format: `{\"url\": \"image_url\", \"weight\": optional_float}`\n\n9. `get_credits`\n\n   - Gets credit information for the current user\n   - No parameters required\n   - Returns available credit balance in USD cents\n\n10. `get_camera_motions`\n    - Gets all supported camera motions\n    - No parameters required\n    - Returns: List of available camera motion strings\n\n## Setup for Claude Desktop 🖥️\n\n1. Get your Luma API key from [Luma AI](https://lumalabs.ai) (sign up or log in to get your API key)\n\n2. Add this to your Claude Desktop configuration file:\n\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"luma\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--project\",\n           \"/path/to/your/luma-ai-mcp-server\",\n           \"-m\",\n           \"luma_ai_mcp_server\"\n         ],\n         \"env\": {\n           \"LUMA_API_KEY\": \"your-luma-api-key-here\"\n         }\n       }\n     }\n   }\n   ```\n\n   Replace:\n\n   - `/path/to/your/luma-ai-mcp-server` with the actual path to your server directory\n   - `your-luma-api-key-here` with your actual Luma API key\n\n3. Restart Claude Desktop\n\n4. That's it! You can now use Luma AI tools directly in Claude Desktop conversations.\n\n## Quick Troubleshooting 🛠️\n\nIf you're having issues:\n\n1. Check your API key is correct\n2. Make sure the path to the server is correct\n3. View logs with: `tail -n 20 -f ~/Library/Logs/Claude/mcp*.log`\n\n## Advanced Video Generation Types 🎬\n\nThe Luma API supports various types of advanced video generation through keyframes:\n\n1. **Starting from an image**: Provide `frame0` with `type: \"image\"` and an image URL\n2. **Ending with an image**: Provide `frame1` with `type: \"image\"` and an image URL\n3. **Extending a video**: Provide `frame0` with `type: \"generation\"` and a generation ID\n4. **Reverse extending a video**: Provide `frame1` with `type: \"generation\"` and a generation ID\n5. **Interpolating between videos**: Provide both `frame0` and `frame1` with `type: \"generation\"` and generation IDs\n\n## API Limitations and Notes 📝\n\n- **Duration**: Currently, the API only supports durations of \"5s\" or \"9s\"\n- **Resolution**: Valid values are \"540p\", \"720p\", \"1080p\", and \"4k\"\n- **Models**:\n  - Video generation:\n    - \"ray-2\" (default) - Best quality, slower\n    - \"ray-flash-2\" - Faster generation\n    - \"ray-1-6\" - Legacy model\n  - Image generation:\n    - \"photon-1\" (default) - Best quality, slower\n    - \"photon-flash-1\" - Faster generation\n- **Generation types**: Video, image, and advanced (with keyframes)\n- **Aspect Ratios**: \"1:1\" (square), \"16:9\" (landscape), \"9:16\" (portrait), \"4:3\" (standard), \"3:4\" (standard portrait), \"21:9\" (ultrawide), \"9:21\" (ultrawide portrait)\n- **States**: \"queued\", \"dreaming\", \"completed\", \"failed\"\n- **Upscaling**:\n  - Video generations can only be upscaled when they're in a \"complete\" state\n  - Target resolution must be higher than the original generation's resolution\n  - Each generation can only be upscaled once\n- **API Key**: Required in environment variables\n- **API Version**: Uses Dream Machine API v1\n\n## License 📄\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "luma",
        "ai",
        "bobtista",
        "ai dream",
        "dream machine",
        "luma ai"
      ],
      "category": "image-and-video-generation"
    },
    "burningion--video-editing-mcp": {
      "owner": "burningion",
      "name": "video-editing-mcp",
      "url": "https://github.com/burningion/video-editing-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/burningion.webp",
      "description": "Upload, edit, search, and generate videos using large language models and Video Jungle's tools. The server enables interaction with videos through a custom URI scheme for managing individual videos and projects.",
      "stars": 214,
      "forks": 29,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T07:41:40Z",
      "readme_content": "# Video Editor MCP server\n\n[![Video Jungle MCP Server](./assets/create-edit.png)](https://www.video-jungle.com)\n\nSee a demo here: [https://www.youtube.com/watch?v=KG6TMLD8GmA](https://www.youtube.com/watch?v=KG6TMLD8GmA)\n\nUpload, edit, search, and generate videos from everyone's favorite LLM and [Video Jungle](https://www.video-jungle.com/).\n\nYou'll need to sign up for an account at [Video Jungle](https://app.video-jungle.com/register) in order to use this tool, and add your API key.\n\n[![PyPI version](https://badge.fury.io/py/video-editor-mcp.svg)](https://badge.fury.io/py/video-editor-mcp)\n\n## Components\n\n### Resources\n\nThe server implements an interface to upload, generate, and edit videos with:\n- Custom vj:// URI scheme for accessing individual videos and projects\n- Each project resource has a name, description\n- Search results are returned with metadata about what is in the video, and when, allowing for edit generation directly\n\n### Prompts\n\nComing soon.\n\n### Tools\n\nThe server implements a few tools:\n- add-video\n  - Add a Video File for analysis from a URL. Returns an vj:// URI to reference the Video file\n- create-videojungle-project\n  - Creates a Video Jungle project to contain generative scripts, analyzed videos, and images for video edit generation\n- edit-locally\n  - Creates an OpenTimelineIO project and downloads it to your machine to open in a Davinci Resolve Studio instance (Resolve Studio _must_ already be running before calling this tool.) \n- generate-edit-from-videos\n  - Generates a rendered video edit from a set of video files\n- generate-edit-from-single-video\n  - Generate an edit from a single input video file\n- get-project-assets\n  - Get assets within a project for video edit generation.\n- search-videos\n  - Returns video matches based upon embeddings and keywords\n- update-video-edit\n  - Live update a video edit's information. If Video Jungle is open, edit will be updated in real time.\n\n### Using Tools in Practice\n\nIn order to use the tools, you'll need to sign up for Video Jungle and add your API key.\n\n**add-video**\n\nHere's an example prompt to invoke the `add-video` tool:\n\n```\ncan you download the video at https://www.youtube.com/shorts/RumgYaH5XYw and name it fly traps?\n```\n\nThis will download a video from a URL, add it to your library, and analyze it for retrieval later. Analysis is multi-modal, so both audio and visual components can be queried against.\n\n**search-videos**\n\nOnce you've got a video downloaded and analyzed, you can then do queries on it using the `search-videos` tool:\n\n```\ncan you search my videos for fly traps?\n```\n\nSearch results contain relevant metadata for generating a video edit according to details discovered in the initial analysis.\n\n**search-local-videos**\n\nYou must set the environment variable `LOAD_PHOTOS_DB=1` in order to use this tool, as it will make Claude prompt to access your files on your local machine.\n\nOnce that's done, you can search through your Photos app for videos that exist on your phone, using Apple's tags.\n\nIn my case, when I search for \"Skateboard\", I get 1903 video files.\n\n```\ncan you search my local video files for Skateboard?\n```\n\n**generate-edit-from-videos**\n\nFinally, you can use these search results to generate an edit:\n\n```\ncan you create an edit of all the times the video says \"fly trap\"?\n```\n\n(Currently), the video edits tool relies on the context within the current chat. \n\n**generate-edit-from-single-video**\n\nFinally, you can cut down an edit from a single, existing video:\n\n```\ncan you create an edit of all the times this video says the word \"fly trap\"?\n```\n\n## Configuration\n\nYou must login to [Video Jungle settings](https://app.video-jungle.com/profile/settings), and get your [API key](https://app.video-jungle.com/profile/settings). Then, use this to start Video Jungle MCP:\n\n```bash\n$ uv run video-editor-mcp YOURAPIKEY\n```\n\nTo allow this MCP server to search your Photos app on MacOS:\n\n```\n$ LOAD_PHOTOS_DB=1 uv run video-editor-mcp YOURAPIKEY\n```\n## Quickstart\n\n### Install\n\n#### Installing via Smithery\n\nTo install Video Editor for Claude Desktop automatically via [Smithery](https://smithery.ai/server/video-editor-mcp):\n\n```bash\nnpx -y @smithery/cli install video-editor-mcp --client claude\n```\n\n#### Claude Desktop\n\nYou'll need to adjust your `claude_desktop_config.json` manually:\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n<details>\n  <summary>Published Server Configuration</summary>\n  \n ```json\n  \"mcpServers\": {\n    \"video-editor-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"video-editor-mcp\",\n        \"YOURAPIKEY\"\n      ]\n    }\n  }\n  ```\n</details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  \n ```json\n  \"mcpServers\": {\n    \"video-editor-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/YOURDIRECTORY/video-editor-mcp\",\n        \"run\",\n        \"video-editor-mcp\",\n        \"YOURAPIKEY\"\n      ]\n    }\n  }\n  ```\n\n  With local Photos app access enabled (search your Photos app):\n\n  ```json\n    \"video-jungle-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/<PATH_TO>/video-jungle-mcp\",\n        \"run\",\n        \"video-editor-mcp\",\n        \"<YOURAPIKEY>\"\n      ],\n     \"env\": {\n\t      \"LOAD_PHOTOS_DB\": \"1\"\n      }\n    },\n  ```\n\n</details>\n\nBe sure to replace the directories with the directories you've placed the repository in on **your** computer.\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### MCP Server Registry\n\n```\nmcp-name: io.github.burningion/video-editing-mcp\n```\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n(Be sure to replace `YOURDIRECTORY` and `YOURAPIKEY` with the directory this repo is in, and your Video Jungle API key, found in the settings page.)\n\n```bash\nnpx @modelcontextprotocol/inspector uv run --directory /Users/YOURDIRECTORY/video-editor-mcp video-editor-mcp YOURAPIKEY\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n\nAdditionally, I've added logging to `app.log` in the project directory. You can add logging to diagnose API calls via a:\n\n```\nlogging.info(\"this is a test log\")\n```\n\nA reasonable way to follow along as you're workin on the project is to open a terminal session and do a:\n\n```bash\n$ tail -n 90 -f app.log\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "videos",
        "editing",
        "upload",
        "video generation",
        "generate videos",
        "video editing"
      ],
      "category": "image-and-video-generation"
    },
    "bytefer--mcp-flux-schnell": {
      "owner": "bytefer",
      "name": "mcp-flux-schnell",
      "url": "https://github.com/bytefer/mcp-flux-schnell",
      "imageUrl": "/freedevtools/mcp/pfp/bytefer.webp",
      "description": "Generate images from text descriptions using the Flux Schnell model through an MCP interface. This server connects with Cloudflare's Flux Schnell worker API to deliver image generation capabilities.",
      "stars": 5,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-20T04:32:23Z",
      "readme_content": "# mcp-flux-schnell MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@bytefer/mcp-flux-schnell)](https://smithery.ai/server/@bytefer/mcp-flux-schnell)\n\nA TypeScript-based MCP server that implements a text-to-image generation tool using the Flux Schnell model. This server integrates with Cloudflare's Flux Schnell worker API to provide image generation capabilities through MCP.\n\n- [Creating your own Flux Schnell MCP Server is so easy! — Part 1](https://medium.com/@bytefer/creating-your-own-flux-schnell-mcp-server-is-so-easy-part-1-4b9a5b3fb14f)\n- [Creating your own Flux Schnell MCP Server is so easy! — Part 2](https://medium.com/@bytefer/creating-your-own-flux-schnell-mcp-server-is-so-easy-part-2-bd711836a493)\n\n## Features\n\n### Tools\n- `generate_image` - Generate images from text descriptions\n  - Takes a text prompt as input (1-2048 characters)\n  - Returns the path to the generated image file\n\n## Environment Variables\n\nThe following environment variables must be configured:\n\n- `FLUX_API_URL` - The URL of the Flux Schnell API endpoint\n- `FLUX_API_TOKEN` - Your authentication token for the Flux Schnell API\n- `WORKING_DIR` (optional) - Directory where generated images will be saved (defaults to current working directory)\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n# or\npnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n# or\npnpm build\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install Flux Schnell Image Generator for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@bytefer/mcp-flux-schnell):\n\n```bash\nnpx -y @smithery/cli install @bytefer/mcp-flux-schnell --client claude\n```\n\n### Cursor Configuration\n\nThere are two ways to configure the MCP server in Cursor:\n\n#### Project Configuration\n\nFor tools specific to a project, create a `.cursor/mcp.json` file in your project directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-flux-schnell\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-flux-schnell/build/index.js\"],\n      \"env\": {\n        \"FLUX_API_URL\": \"your flux api url\",\n        \"FLUX_API_TOKEN\": \"your flux api token\",\n        \"WORKING_DIR\": \"your working directory\"\n      }\n    }\n  }\n}\n```\n\nThis configuration will only be available within the specific project.\n\n#### Global Configuration\n\nFor tools that you want to use across all projects, create a `~/.cursor/mcp.json` file in your home directory with the same configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-flux-schnell\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-flux-schnell/build/index.js\"],\n      \"env\": {\n        \"FLUX_API_URL\": \"your flux api url\",\n        \"FLUX_API_TOKEN\": \"your flux api token\",\n        \"WORKING_DIR\": \"your working directory\"\n      }\n    }\n  }\n}\n```\n\nThis makes the MCP server available in all your Cursor workspaces.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flux",
        "images",
        "bytefer",
        "flux schnell",
        "mcp flux",
        "image generation"
      ],
      "category": "image-and-video-generation"
    },
    "c-rick--jimeng-mcp": {
      "owner": "c-rick",
      "name": "jimeng-mcp",
      "url": "https://github.com/c-rick/jimeng-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/c-rick.webp",
      "description": "Integrates with the Jimeng AI service to generate images from text prompts. Supports customization of image parameters such as size, quality, and negative prompts without the need for third-party APIs.",
      "stars": 40,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-18T10:20:11Z",
      "readme_content": "# Jimeng MCP 服务器\n\n\n使用TypeScript实现的Model Context Protocol (MCP) 服务器项目，集成了即梦AI图像生成服务，通过逆向工程直接调用即梦官方API。\n\n\n## 功能\n\n- 基于TypeScript构建\n- 使用tsup作为构建工具\n- 实现了MCP协议，支持标准的stdio通信\n- 直接调用即梦AI图像生成服务，无需第三方API\n- 提供多种即梦模型的图像生成工具\n- 支持多种图像参数调整，如尺寸、精细度、负面提示词等\n- 支持图片混合/参考图生成（通过filePath参数，支持本地图片和网络图片）\n- 支持视频生成，支持添加参考图片（首尾帧通过filePath参数设置）\n\n## 安装\n\n### 通过Smithery安装\n\n要通过 [Smithery](https://smithery.ai/server/@c-rick/jimeng-mcp) 自动为Claude Desktop安装jimeng-mcp，请执行以下命令：\n\n```bash\nnpx -y @smithery/cli install @c-rick/jimeng-mcp --client claude\n```\n\n### 手动安装\n```bash\n# 使用yarn安装依赖\nyarn install\n\n# 或使用npm安装依赖\nnpm install\n```\n\n## 环境配置\n\n在MCP客户端配置（如Claude Desktop）中设置以下环境变量：\n\n进入[Smithery托管项目](https://smithery.ai/server/@c-rick/jimeng-mcp)，点击json, 填入JIMENG_API_TOKEN， 点击connect, 生成下面mcpServers config json\n\n```json\n{\n  \"mcpServers\": {\n    \"jimeng-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@c-rick/jimeng-mcp\",\n        \"--key\",\n        \"[Smithery生成]\",\n        \"--profile\",\n        \"[Smithery生成]\"\n      ]\n    }\n  }\n}\n```\n\n### 获取JIMENG_API_TOKEN\n\n1. 访问 [即梦AI官网](https://jimeng.jianying.com) 并登录账号\n2. 按F12打开浏览器开发者工具\n3. 在Application > Cookies中找到`sessionid`的值\n4. 将找到的sessionid值配置为JIMENG_API_TOKEN环境变量\n\n## 开发\n\n```bash\n# 开发模式运行\nyarn dev\n\n# 使用nodemon开发并自动重启\nyarn start:dev\n```\n\n## 构建\n\n```bash\n# 构建项目\nyarn build\n```\n\n## 运行\n\n```bash\n# 启动服务器\nyarn start\n\n# 测试MCP服务器\nyarn test\n```\n\n## Claude Desktop 配置示例\n\n以下是在Claude Desktop中配置此MCP服务器的完整示例:\n\n```json\n{\n  \"mcpServers\": {\n    \"jimeng\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/jimeng-mcp/lib/index.js\"],\n      \"env\": {\n        \"JIMENG_API_TOKEN\": \"your_jimeng_session_id_here\"\n      }\n    }\n  }\n}\n```\n\n## 即梦AI图像生成\n\n本MCP服务器直接调用即梦AI图像生成API，提供图像生成工具：\n\n`generateImage` - 提交图像生成请求并返回图像URL列表\n- 参数：\n  - `prompt`：生成图像的文本描述（必填）\n  - `filePath`：本地图片路径或图片URL（可选，若填写则为图片混合/参考图生成功能）\n  - `model`：模型名称，可选值: jimeng-3.0, jimeng-2.1, jimeng-2.0-pro, jimeng-2.0, jimeng-1.4, jimeng-xl-pro（可选，默认为jimeng-2.1，图片混合时自动切换为jimeng-2.0-pro）\n  - `width`：图像宽度，默认值：1024（可选）\n  - `height`：图像高度，默认值：1024（可选）\n  - `sample_strength`：精细度，默认值：0.5，范围0-1（可选）\n  - `negative_prompt`：反向提示词，告诉模型不要生成什么内容（可选）\n\n> **注意：**\n> - `filePath` 支持本地绝对/相对路径和图片URL。\n> - 若指定 `filePath`，将自动进入图片混合/参考图生成模式，底层模型自动切换为 `jimeng-2.0-pro`。\n> - 网络图片需保证可公开访问。\n\n### 图片混合/参考图生成功能\n\n如需基于图片进行混合生成，只需传入`filePath`参数（支持本地路径或图片URL），即可实现图片风格融合、参考图生成等高级玩法。\n\n#### 示例：\n\n```javascript\n// 参考图片混合生成\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"梵高风格的猫\",\n    filePath: \"./test.png\", // 本地图片路径\n    sample_strength: 0.6\n  }\n});\n```\n\n或\n\n```javascript\n// 使用网络图片作为参考\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"未来城市\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n### 支持的模型\n\n服务器支持以下即梦AI模型：\n\n- 图片模型\n- `jimeng-3.1`：即梦第三代模型，丰富的美学多样性，画面更鲜明生动 （默认）\n- `jimeng-3.0`：即梦第三代模型，效果更好，支持更强的图像生成能力\n- `jimeng-2.1`：即梦2.1版本模型，默认模型\n- `jimeng-2.0-pro`：即梦2.0 Pro版本\n- `jimeng-2.0`：即梦2.0标准版本\n- `jimeng-1.4`：即梦1.4版本\n- `jimeng-xl-pro`：即梦XL Pro特殊版本\n- 视频模型\n- `jimeng-video-3.0-pro`：即梦视频3.0 Pro模型，适合高质量视频生成\n- `jimeng-video-3.0`：即梦视频3.0标准模型，主力视频生成模型（默认）\n- `jimeng-video-2.0-pro`：即梦视频2.0 Pro模型，兼容性好，适合多场景\n- `jimeng-video-2.0`：即梦视频2.0标准模型，适合基础视频生成\n\n### 技术实现\n\n- 直接调用即梦官方API，无需第三方服务\n- 逆向工程API调用流程，实现完整的图像生成过程\n- 支持积分自动领取和使用\n- 基于面向对象设计，将API实现封装为类\n- 返回高质量图像URL列表\n- 支持图片上传，自动处理本地/网络图片，自动切换混合模型\n- 图片混合时自动上传图片到即梦云端，流程全自动\n\n### 使用示例\n\n通过MCP协议调用图像生成功能：\n\n```javascript\n// 生成图像（文本生成）\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"一只可爱的猫咪在草地上\",\n    model: \"jimeng-3.0\",\n    width: 1024,\n    height: 1024,\n    sample_strength: 0.7,\n    negative_prompt: \"模糊，扭曲，低质量\"\n  }\n});\n\n// 生成图像（图片混合/参考图生成）\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"未来城市\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n## 响应格式\n\nAPI将返回生成的图像URL数组，可以直接在各类客户端中显示：\n\n```javascript\n[\n  \"https://example.com/generated-image-1.jpg\",\n  \"https://example.com/generated-image-2.jpg\",\n  \"https://example.com/generated-image-3.jpg\",\n  \"https://example.com/generated-image-4.jpg\"\n]\n```\n\n## 资源\n\n服务器还提供了以下信息资源：\n\n- `greeting://{name}` - 提供个性化问候\n- `info://server` - 提供服务器基本信息\n- `jimeng-ai://info` - 提供即梦AI图像生成服务的使用说明\n\n## Cursor或Claude使用提示\n\n在Cursor或Claude中，你可以这样使用Jimeng图像生成服务：\n\n1. 确保已经配置了MCP服务器\n2. 提示Claude/Cursor生成图像，例如：\n   ```\n   请生成一张写实风格的日落下的山脉图片\n   ```\n3. Claude/Cursor会调用Jimeng MCP服务器生成图像并显示\n\n## 常见问题\n\n1. **图像生成失败**\n   - 检查JIMENG_API_TOKEN是否正确配置\n   - 登录即梦官网检查账号积分是否充足\n   - 尝试更换提示词，避免敏感内容\n   - 若为图片混合，检查filePath路径/URL是否有效、图片是否可访问\n   - 网络图片建议使用https直链，避免防盗链/权限问题\n\n2. **服务器无法启动**\n   - 确保已安装所有依赖\n   - 确保环境变量正确设置\n   - 检查Node.js版本是否为14.0或更高\n\n## 许可证\n\nMIT \n\n## 即梦AI视频生成\n\n本MCP服务器集成了即梦AI视频生成API，提供视频生成工具：\n\n`generateVideo` - 提交视频生成请求并返回视频URL\n- 参数：\n  - `prompt`：生成视频的文本描述（必填）\n  - `filePath`：首帧和尾帧图片路径，支持数组，最多2个元素，分别为首帧和尾帧（可选）\n  - `model`：模型名称，默认jimeng-video-3.0（可选）\n  - `resolution`：分辨率，可选720p或1080p，默认720p（可选）\n  - `width`：视频宽度，默认值：1024（可选）\n  - `height`：视频高度，默认值：1024（可选）\n  - `refresh_token`：即梦API令牌（可选，通常从环境变量读取）\n  - `req_key`：自定义参数，兼容旧接口（可选）\n\n> **注意：**\n> - `filePath` 支持本地绝对/相对路径和图片URL。\n> - 若指定 `filePath`，可实现首帧/尾帧定制的视频生成。\n> - 网络图片需保证可公开访问。\n\n### 使用示例\n\n通过MCP协议调用视频生成功能：\n\n```javascript\n// 生成视频（文本生成）\nclient.callTool({\n  name: \"generateVideo\",\n  arguments: {\n    prompt: \"一只小狗在草地上奔跑，阳光明媚，高清\",\n    model: \"jimeng-video-3.0\",\n    resolution: \"720p\",\n    width: 1024,\n    height: 1024\n  }\n});\n\n// 生成视频（首帧/尾帧定制）\nclient.callTool({\n  name: \"generateVideo\",\n  arguments: {\n    prompt: \"城市夜景延时摄影\",\n    filePath: [\"./first.png\", \"./last.png\"],\n    resolution: \"1080p\"\n  }\n});\n```\n\n## 视频响应格式\n\nAPI将返回生成的视频URL字符串，可以直接在各类客户端中播放：\n\n```javascript\n\"https://example.com/generated-video.mp4\"\n``` \n\n\n## 支持api服务启动\n\n如需以API服务方式启动（适合HTTP接口调用）：\n\n```bash\ncp .env.example .env   # 复制环境变量模板\n# 根据需要编辑.env，填写JIMENG_API_TOKEN等配置\n\n# 启动API服务\nyarn start:api\n```\n\nAPI服务启动后将监听配置端口，支持通过HTTP接口调用即梦AI图像和视频生成功能。 \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jimeng",
        "mcp",
        "images",
        "generate images",
        "jimeng mcp",
        "jimeng ai"
      ],
      "category": "image-and-video-generation"
    },
    "catalystneuro--mcp_read_images": {
      "owner": "catalystneuro",
      "name": "mcp_read_images",
      "url": "https://github.com/catalystneuro/mcp_read_images",
      "imageUrl": "/freedevtools/mcp/pfp/catalystneuro.webp",
      "description": "Analyze images using OpenRouter vision models like Claude-3.5-sonnet and Claude-3-opus through a simple API interface.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-14T12:27:13Z",
      "readme_content": "# MCP Read Images\n\nAn MCP server for analyzing images using OpenRouter vision models. This server provides a simple interface to analyze images using various vision models like Claude-3.5-sonnet and Claude-3-opus through the OpenRouter API.\n\n## Installation\n\n```bash\nnpm install @catalystneuro/mcp_read_images\n```\n\n## Configuration\n\nThe server requires an OpenRouter API key. You can get one from [OpenRouter](https://openrouter.ai/keys).\n\nAdd the server to your MCP settings file (usually located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json` for VSCode):\n\n```json\n{\n  \"mcpServers\": {\n    \"read_images\": {\n      \"command\": \"read_images\",\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\",\n        \"OPENROUTER_MODEL\": \"anthropic/claude-3.5-sonnet\"  // optional, defaults to claude-3.5-sonnet\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool `analyze_image` that can be used to analyze images:\n\n```typescript\n// Basic usage with default model\nuse_mcp_tool({\n  server_name: \"read_images\",\n  tool_name: \"analyze_image\",\n  arguments: {\n    image_path: \"/path/to/image.jpg\",\n    question: \"What do you see in this image?\"  // optional\n  }\n});\n\n// Using a specific model for this call\nuse_mcp_tool({\n  server_name: \"read_images\",\n  tool_name: \"analyze_image\",\n  arguments: {\n    image_path: \"/path/to/image.jpg\",\n    question: \"What do you see in this image?\",\n    model: \"anthropic/claude-3-opus-20240229\"  // overrides default and settings\n  }\n});\n```\n\n### Model Selection\n\nThe model is selected in the following order of precedence:\n1. Model specified in the tool call (`model` argument)\n2. Model specified in MCP settings (`OPENROUTER_MODEL` environment variable)\n3. Default model (anthropic/claude-3.5-sonnet)\n\n### Supported Models\n\nThe following OpenRouter models have been tested:\n- anthropic/claude-3.5-sonnet\n- anthropic/claude-3-opus-20240229\n\n## Features\n\n- Automatic image resizing and optimization\n- Configurable model selection\n- Support for custom questions about images\n- Detailed error messages\n- Automatic JPEG conversion and quality optimization\n\n## Error Handling\n\nThe server handles various error cases:\n- Invalid image paths\n- Missing API keys\n- Network errors\n- Invalid model selections\n- Image processing errors\n\nEach error will return a descriptive message to help diagnose the issue.\n\n## Development\n\nTo build from source:\n\n```bash\ngit clone https://github.com/catalystneuro/mcp_read_images.git\ncd mcp_read_images\nnpm install\nnpm run build\n```\n\n## License\n\nMIT License. See [LICENSE](LICENSE) for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_read_images",
        "catalystneuro",
        "vision",
        "catalystneuro mcp_read_images",
        "mcp_read_images analyze",
        "analyze images"
      ],
      "category": "image-and-video-generation"
    },
    "champierre--image-mcp-server": {
      "owner": "champierre",
      "name": "image-mcp-server",
      "url": "https://github.com/champierre/image-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/champierre.webp",
      "description": "Analyzes images by accepting URLs or local file paths, providing detailed insights through advanced image recognition powered by the GPT-4o-mini model. Validates image URLs and supports loading images from local files and Base64 encoding.",
      "stars": 6,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-09T18:23:14Z",
      "readme_content": "# image-mcp-server\n\n[日本語の README](README.ja.md)\n\n<a href=\"https://glama.ai/mcp/servers/@champierre/image-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@champierre/image-mcp-server/badge\" alt=\"Image Analysis MCP Server\" />\n</a>\n\n[![smithery badge](https://smithery.ai/badge/@champierre/image-mcp-server)](https://smithery.ai/server/@champierre/image-mcp-server)\nAn MCP server that receives image URLs or local file paths and analyzes image content using the GPT-4o-mini model.\n\n## Features\n\n- Receives image URLs or local file paths as input and provides detailed analysis of the image content\n- High-precision image recognition and description using the GPT-4o-mini model\n- Image URL validity checking\n- Image loading from local files and Base64 encoding\n\n## Installation\n\n### Installing via Smithery\n\nTo install Image Analysis Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@champierre/image-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @champierre/image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/champierre/image-mcp-server.git # or your forked repository\ncd image-mcp-server\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript\nnpm run build\n```\n\n## Configuration\n\nTo use this server, you need an OpenAI API key. Set the following environment variable:\n\n```\nOPENAI_API_KEY=your_openai_api_key\n```\n\n## MCP Server Configuration\n\nTo use with tools like Cline, add the following settings to your MCP server configuration file:\n\n### For Cline\n\nAdd the following to `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-analysis\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/image-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key\"\n      }\n    }\n  }\n}\n```\n\n### For Claude Desktop App\n\nAdd the following to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-analysis\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/image-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nOnce the MCP server is configured, the following tools become available:\n\n- `analyze_image`: Receives an image URL and analyzes its content.\n- `analyze_image_from_path`: Receives a local file path and analyzes its content.\n\n### Usage Examples\n\n**Analyzing from URL:**\n\n```\nPlease analyze this image URL: https://example.com/image.jpg\n```\n\n**Analyzing from local file path:**\n\n```\nPlease analyze this image: /path/to/your/image.jpg\n```\n\n### Note: Specifying Local File Paths\n\nWhen using the `analyze_image_from_path` tool, the AI assistant (client) must specify a **valid file path in the environment where this server is running**.\n\n- **If the server is running on WSL:**\n  - If the AI assistant has a Windows path (e.g., `C:\\...`), it needs to convert it to a WSL path (e.g., `/mnt/c/...`) before passing it to the tool.\n  - If the AI assistant has a WSL path, it can pass it as is.\n- **If the server is running on Windows:**\n  - If the AI assistant has a WSL path (e.g., `/home/user/...`), it needs to convert it to a UNC path (e.g., `\\\\wsl$\\Distro\\...`) before passing it to the tool.\n  - If the AI assistant has a Windows path, it can pass it as is.\n\n**Path conversion is the responsibility of the AI assistant (or its execution environment).** The server will try to interpret the received path as is.\n\n### Note: Type Errors During Build\n\nWhen running `npm run build`, you may see an error (TS7016) about missing TypeScript type definitions for the `mime-types` module.\n\n```\nsrc/index.ts:16:23 - error TS7016: Could not find a declaration file for module 'mime-types'. ...\n```\n\nThis is a type checking error, and since the JavaScript compilation itself succeeds, it **does not affect the server's execution**. If you want to resolve this error, install the type definition file as a development dependency.\n\n```bash\nnpm install --save-dev @types/mime-types\n# or\nyarn add --dev @types/mime-types\n```\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "image",
        "encoding",
        "image mcp",
        "image urls",
        "analyzes images"
      ],
      "category": "image-and-video-generation"
    },
    "chenyeju295--mcp_generate_images": {
      "owner": "chenyeju295",
      "name": "mcp_generate_images",
      "url": "https://github.com/chenyeju295/mcp_generate_images",
      "imageUrl": "/freedevtools/mcp/pfp/chenyeju295.webp",
      "description": "An image generation service that integrates with Cursor IDE, offering features such as customizable image aspect ratios, high-quality image generation, and batch processing capabilities.",
      "stars": 21,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-29T07:27:23Z",
      "readme_content": "# AI 图像生成服务\n\n基于火山引擎（抖音豆包）的图像生成服务，专门设计用于与 Cursor MCP 服务集成。支持自定义图片宽高比、保存路径等功能，提供高质量图像生成能力。\n\n## 功能特点\n\n- 支持高质量图像生成\n- 多种常见宽高比支持（1:1、4:3、16:9、3:4、9:16）\n- 火山引擎豆包模型（doubao-seedream-3-0-t2i-250415）\n- 自动重试和详细错误处理\n- 完整的路径和权限验证\n- 详细的错误提示和日志\n- 异步处理支持\n\n## 环境准备\n\n### 1. Python 环境\n\n- Python 3.10+\n- 下载地址： <https://www.python.org/downloads/>\n\n- 推荐使用 pyenv 管理 Python 版本：\n\n```bash\n# macOS 安装 pyenv\nbrew install pyenv\n\n# 安装 Python\npyenv install 3.13.2\npyenv global 3.13.2\n```\n\n### 2. Nodejs 环境\n\n- 下载地址： <https://nodejs.org/zh-cn>  \n\n### 3. uv 包管理工具\n\nuv 是一个快速的 Python 包管理器，需要先安装：\n\n```bash\n# macOS 安装 uv\nbrew install uv\n\n# 或者使用 pip 安装\npip install uv\n```\n\n### 4. 火山引擎 API 密钥\n\n1. 访问 [火山引擎方舟大模型服务](https://console.volcengine.com/ark)\n2. 注册/登录账号\n3. 创建新的 API 密钥\n4. 复制密钥并保存，格式如：`YOUR_API_KEY`\n\n### 5. Cursor\n\n- 下载并安装 [Cursor IDE](https://cursor.sh/)\n- 确保 Cursor 已正确配置 Python 环境\n\n## 安装配置\n\n### 1. 克隆项目\n\n```bash\ngit clone https://github.com/chenyeju295/mcp_generate_images.git\ncd mcp_generate_images\n```\n\n### 2. 安装依赖(cd 到mcp_generate_images 安装)\n \n```bash\npython3 -m pip install fastmcp requests 'volcengine-python-sdk[ark]'\n```\n\n或者使用requirements.txt文件：\n\n```bash\npip install -r requirements.txt\n```\n\n出现证书问题可以使用：\n\n```bash\npython3 -m pip install fastmcp requests 'volcengine-python-sdk[ark]' --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade --force-reinstall --no-cache-dir\n```\n\ntips: 需确保安装成功，否则配置MCP 服务会报红。\n\n### 3. 配置 API 密钥\n\n设置环境变量（推荐方式）：\n\n```bash\nexport ARK_API_KEY=your_api_key_here\n```\n\n或者在 ~/.bashrc 或 ~/.zshrc 中添加：\n\n```bash\necho 'export ARK_API_KEY=your_api_key_here' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n验证环境变量已设置：\n\n```bash\necho $ARK_API_KEY\n```\n\n### 4. 配置服务\n\n在 `mcp_server.py` 中可以修改以下配置：\n\n```python\nCONFIG = {\n    \"api\": {\n        \"base_url\": \"https://ark.cn-beijing.volces.com/api/v3\",\n        \"model\": \"doubao-seedream-3-0-t2i-250415\",\n        \"timeout\": 120,\n        \"max_retries\": 3,\n        \"retry_delay\": 5\n    },\n    \"image\": {\n        \"max_width\": 1024,   \n        \"max_height\": 1024, \n        \"default_width\": 1024,\n        \"default_height\": 1024,\n        \"max_batch_size\": 1\n    },\n    \"output\": {\n        \"base_folder\": \"你的默认保存路径\",\n        \"allowed_extensions\": [\".png\", \".jpg\", \".jpeg\"],\n        \"default_extension\": \".png\"\n    }\n}\n```\n\n## 运行服务\n\n开发模式运行（带调试界面）：\n\n```bash\nuv run --with fastmcp fastmcp dev /Users/username/Documents/mcp_generate_images/mcp_server.py\n```\n\n## 在 Cursor 中使用\n \n### 1. 在 Cursor 中引入 MCP 服务\n\n在 Cursor 的 MCP 配置中添加：\n\n```json\n{\n  \"mcpServers\": {\n    \"generate_images\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"fastmcp\",\n        \"run\",\n        \"/Users/chenyeju/Documents/github/mcp_generate_images/mcp_server.py\"\n      ]\n    } \n  }\n}\n```\n\n### 3. 服务运行成功示例\n\n![image.png](./images/image.png)\n\n### 4. 在 Cursor Composer 的 agent 模式下使用\n\n![image.png](./images/image_2.png)\n\n## 参数说明\n\n图像生成工具支持以下参数：\n\n| 参数名 | 类型 | 必填 | 说明 |\n|-------|------|------|------|\n| prompt | 字符串 | 是 | 图片生成提示词，建议不超过500字符 |\n| file_name | 字符串 | 是 | 保存的文件名(不含路径，如果没有后缀则默认使用.png) |\n| save_folder | 字符串 | 是 | 保存目录的绝对路径 |\n| aspect_ratio | 字符串 | 否 | 图片的宽高比，支持 '1:1', '4:3', '16:9', '3:4', '9:16'。默认为'1:1' |\n\n\n## 使用示例\n\n```\n生成一张宽高比为16:9的风景图片：\n\ngenerate_image(\n  prompt=\"A beautiful mountain landscape with sunset\", \n  file_name=\"landscape.png\", \n  save_folder=\"/Users/username/Documents/images\", \n  aspect_ratio=\"16:9\"\n)\n```\n\n## 使用注意事项\n\n1. **模型**：使用火山引擎豆包模型（doubao-seedream-3-0-t2i-250415），支持最大1024x1024的尺寸。\n2. **长宽比**：建议使用1:1的宽高比（正方形图片），例如512x512或1024x1024，以获得最佳效果和生成速度。\n3. **提示词**：简洁明了的提示词通常能获得更好的结果，尽量不超过500字符。支持中文提示词。\n4. **超时问题**：对于复杂提示词或非正方形图片，生成可能需要更长时间，有时会导致超时错误。\n5. **API限制**：火山引擎API每次只生成一张图片，相比之前的批量生成有所不同。\n\n## 错误排查\n\n如果遇到问题，请检查：\n\n1. 服务是否正常运行\n2. 保存路径是否正确（必须是绝对路径）\n3. 目录权限是否正确\n4. 网络连接是否正常\n5. API 密钥是否有效\n6. Python 环境是否正确配置\n7. uv 是否正确安装\n8. 依赖包是否完整安装\n\n## 常见错误及解决方案\n\n| 错误信息 | 可能原因 | 解决方案 |\n|---------|---------|---------|\n| \"未能生成图片: API 请求超时\" | 网络问题或请求耗时过长 | 使用更简单的提示词，检查网络连接 |\n| \"未能生成图片: API 调用频率受限\" | 火山引擎API频率限制 | 等待几分钟后再试 |\n| \"未能生成图片: API 认证失败\" | API密钥无效 | 检查并更新火山引擎API密钥 |\n| \"没有权限保存图片到...\" | 目录权限问题 | 确保目录存在且有写入权限 |\n| \"不支持的宽高比\" | 使用了不支持的宽高比 | 使用支持的宽高比：'1:1', '4:3', '16:9', '3:4', '9:16' |\n| \"Failed to download generated images\" | 图片下载失败 | 检查网络连接，确保能访问火山引擎的图片URL | \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_generate_images",
        "image",
        "cursor",
        "chenyeju295 mcp_generate_images",
        "mcp_generate_images image",
        "image generation"
      ],
      "category": "image-and-video-generation"
    },
    "ckz--flux-img-mcp": {
      "owner": "ckz",
      "name": "flux-img-mcp",
      "url": "https://github.com/ckz/flux-img-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ckz.webp",
      "description": "Utilize advanced AI models to generate images from textual prompts, enabling users to convert their ideas into visual art. This server facilitates effortless image creation through simple command inputs.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-12T05:08:24Z",
      "readme_content": "# Flux Image MCP Server\n\nThis MCP server provides image generation capabilities using the Flux Schnell model on Replicate.\n\n## Installation\n\n0. Install the MCP SDK globally:\n```bash\nnpm install -g @modelcontextprotocol/sdk@latest\n```\n\n1. Clone this repository to your MCP servers directory:\n```bash\ncd ~/Documents/Cline/MCP\ngit clone https://github.com/yourusername/flux-img-mcp.git\ncd flux-img-mcp\nnpm install\n```\n\n\n\n2. Build the server:\n```bash\nnpm run build\n```\n\n3. Add the server configuration to your MCP settings file (either global or workspace):\n\n```json\n{\n  \"mcpServers\": {\n    \"flux-img\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/flux-img-mcp/build/index.js\"],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n## Configuration\n\nThe server requires the following environment variable:\n\n- `REPLICATE_API_TOKEN`: Your Replicate API token. You can get this from your [Replicate account settings](https://replicate.com/account).\n\n## Usage\n\nOnce installed and configured, the server provides the following tool:\n\n### generate_image\n\nGenerates an image using the Flux Schnell model based on a text prompt.\n\nParameters:\n- `prompt` (string, required): Text description of the desired image\n\nExample usage:\n```typescript\n<use_mcp_tool>\n<server_name>flux-img</server_name>\n<tool_name>generate_image</tool_name>\n<arguments>\n{\n  \"prompt\": \"A beautiful sunset over mountains\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe tool will return a JSON response containing:\n- `status`: The status of the generation request\n- `output`: The URL of the generated image (if successful)\n- `error`: Any error message (if failed)\n\n## Development\n\nTo make changes to the server:\n\n1. Modify the source code in `src/index.ts`\n2. Rebuild the server: `npm run build`\n3. Restart the MCP server for changes to take effect\n\n## Error Handling\n\nThe server includes comprehensive error handling for:\n- Missing API token\n- Invalid parameters\n- API request failures\n- Network issues\n\n## Security\n\n- Never commit your Replicate API token to version control\n- Always provide the token through environment variables\n- The server validates all input parameters before making API requests\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "images",
        "img",
        "generate images",
        "image creation",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "ckz--flux-schnell-mcp": {
      "owner": "ckz",
      "name": "flux-schnell-mcp",
      "url": "https://github.com/ckz/flux-schnell-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ckz.webp",
      "description": "Generate images from text prompts using the Replicate API, enabling users to create customized visuals based on detailed descriptions. The server manages the communication with the API and handles errors effectively.",
      "stars": 3,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-12T09:44:48Z",
      "readme_content": "# Flux Schnell MCP Server\n\n一个基于MCP（Model Context Protocol）的服务器，用于通过Replicate API调用Flux Schnell模型生成图片。\n\n## 功能特点\n\n- 提供`generate_image`工具用于生成图片\n- 支持自定义文本提示词\n- 自动处理与Replicate API的通信\n- 完整的错误处理和响应\n\n## 前置要求\n\n1. Node.js (v14或更高版本)\n2. Replicate API Token\n3. MCP兼容的环境（如Claude Desktop）\n\n## 获取Replicate API Token\n\n1. 访问 [Replicate官网](https://replicate.com/) 并注册账号\n2. 登录后访问 [API Tokens页面](https://replicate.com/account/api-tokens)\n3. 点击\"Create API token\"创建新的token\n4. 复制生成的token（格式如：r8_xxxxxx）\n\n## 安装\n\n1. 克隆项目并安装依赖：\n```bash\ngit clone [repository-url]\ncd flux-schnell-mcp\nnpm install\n```\n\n2. 构建服务器：\n```bash\nnpm run build\n```\n\n## 配置\n\n### Claude Desktop配置\n\n1. 打开配置文件：\n   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. 添加服务器配置：\n```json\n{\n  \"mcpServers\": {\n    \"flux-schnell\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/flux-schnell-mcp/build/index.js\"],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"your-replicate-api-token\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n### VSCode Roo配置\n\n1. 打开配置文件：\n   - Linux: `~/.vscode-remote/data/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`\n   - MacOS: `~/Library/Application Support/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`\n   - Windows: `%APPDATA%/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`\n\n2. 添加与上述相同的服务器配置。\n\n## 使用方法\n\n服务器提供了一个名为`generate_image`的工具，可以通过MCP调用：\n\n```typescript\n<use_mcp_tool>\n<server_name>flux-schnell</server_name>\n<tool_name>generate_image</tool_name>\n<arguments>\n{\n  \"prompt\": \"a beautiful sunset over the ocean, digital art style\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### 参数说明\n\n- `prompt`: 用于生成图片的文本描述（必填）\n  - 建议使用详细的描述来获得更好的生成结果\n  - 可以包含风格、场景、细节等信息\n\n### 响应格式\n\n服务器将返回Replicate API的完整响应，包含生成的图片URL和其他元数据。\n\n## 调试\n\n由于MCP服务器通过stdio通信，调试可能比较困难。推荐使用[MCP Inspector](https://github.com/modelcontextprotocol/inspector)：\n\n```bash\nnpm run inspector\n```\n\nInspector将提供一个URL，可以在浏览器中访问调试工具。\n\n## 注意事项\n\n1. 请妥善保管您的Replicate API Token，不要将其分享给他人\n2. 确保在配置文件中使用正确的文件路径\n3. 生成图片可能需要一些时间，请耐心等待响应\n4. 如遇到错误，请检查API Token是否正确，以及网络连接是否正常\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "replicate",
        "generate",
        "visuals",
        "generate images",
        "video generation",
        "ckz flux"
      ],
      "category": "image-and-video-generation"
    },
    "coderjun--shaka-packager-mcp-server": {
      "owner": "coderjun",
      "name": "shaka-packager-mcp-server",
      "url": "https://github.com/coderjun/shaka-packager-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/coderjun.webp",
      "description": "Supports advanced video transcoding, packaging, and analysis using Shaka Packager. Facilitates format conversion, DRM application, and content preparation for streaming, featuring intelligent path handling and error management.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-19T18:32:59Z",
      "readme_content": "# Shaka Packager MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)\n[![Status: Alpha](https://img.shields.io/badge/Status-Alpha%20%7C%20Experimental-red)](https://github.com/coderjun/shaka-packager-mcp)\n\n> **⚠️ EXPERIMENTAL STATUS DISCLAIMER**\n> \n> This project is in early alpha stage and is highly experimental. It is not recommended for production use. It is also likely **MESSY!**\n> \n> **Current limitations:**\n> - You may run into inconsistent behavior\n> - Advanced features (packaging, conversion, etc.) are still under active development\n> - Path translation between Docker and host environments may require manual configuration\n> - Expect frequent breaking changes and potential instability\n>\n> Please report any issues you encounter to help improve the project.\n\nAn MCP (Model Context Protocol) server that integrates [Shaka Packager](https://shaka-project.github.io/shaka-packager/) with Claude AI applications for video transcoding, packaging, and analysis.\n\nThis server works with the [Filesystem MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) to enable Claude Desktop to access and process video files on your computer, turning Claude into a powerful assistant for media processing tasks.\n\n## Features\n\n- **Video Analysis**: Analyze video files to extract detailed stream information, codecs, bitrates, and more\n- **Media Packaging**: Convert videos for streaming in HLS and DASH formats with support for VOD and live streaming\n- **Advanced Options**: \n  - Apply DRM encryption (Widevine, PlayReady, FairPlay)\n  - Configure ad insertion markers\n  - Convert between formats (MP4, TS, etc.)\n- **Intelligent Path Handling**: Automatically translates paths between Docker and host environments\n- **Robust Error Management**: Provides meaningful error analysis with suggestions for resolution\n- **Command Assistance**: Helps correctly format Shaka Packager commands for optimal results\n- **Interactive Documentation**: Built-in help and examples to guide users through complex operations\n- **Detailed Outputs**: Comprehensive summaries and execution details for all operations\n\n## Prerequisites\n\n- Python 3.10 or higher\n- Shaka Packager installed and available in your PATH\n  - [Download from GitHub](https://github.com/shaka-project/shaka-packager/releases)\n  - Or build from source following [these instructions](https://shaka-project.github.io/shaka-packager/html/build_instructions.html)\n- An MCP-compatible client (like Claude Desktop)\n\n## Installation\n\n### Using pip or uv (coming soon)\n\nInstall the package with pip:\n\n```bash\npip install shaka-packager-mcp\n```\n\nOr with uv:\n\n```bash\nuv pip install shaka-packager-mcp\n```\n\n### From source (recommended)\n\n```bash\ngit clone https://github.com/coderjun/shaka-packager-mcp.git\ncd shaka-packager-mcp\npip install -e .\n```\n\nOr with uv:\n\n```bash\ngit clone https://github.com/coderjun/shaka-packager-mcp.git\ncd shaka-packager-mcp\nuv pip install -e .\n```\n\n## Claude Desktop Integration\n\nSince Claude Desktop doesn't directly support uploading video files, we'll use a two-server approach:\n1. A simplified **filesystem MCP server** to access video files on your computer\n2. The **Shaka Packager MCP server** to analyze and process those videos\n\n### Step 1: Set Up the MCP Filesystem Server\n\nUse the official MCP filesystem server to allow Claude to access your video files:\n\n1. Install the official filesystem server with Docker:\n   ```bash\n   docker pull mcp/filesystem\n   ```\n\n2. Alternatively, you can build it from source following the instructions in the [Filesystem MCP Server repository](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem)\n\n### Step 2: Find the Configuration File\n\nLocate your Claude Desktop configuration file:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nIf the file doesn't exist, create it.\n\n### Step 3: Add Both Servers to the Configuration\n\nAdd the following configuration, making sure to use absolute paths:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--mount\", \"type=bind,src=/PATH/TO/VIDEOS/DIRECTORY,dst=/projects/video-drop\",\n        \"mcp/filesystem\",\n        \"/projects\"\n      ]\n    },\n    \"shaka-packager\": {\n      \"command\": \"/ABSOLUTE/PATH/TO/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/ABSOLUTE/PATH/TO/shaka_packager_mcp.py\"\n      ],\n      \"env\": {\n        \"VIDEO_PATH\": \"/PATH/TO/VIDEOS/DIRECTORY\",\n        \"SHAKA_PACKAGER_PATH\": \"/PATH/TO/PACKAGER\"\n      }\n    }\n  }\n}\n```\n\nReplace:\n- `/PATH/TO/VIDEOS/DIRECTORY` with the path to the directory containing your video files\n- `/ABSOLUTE/PATH/TO/uv` with the full path to your uv executable\n- `/ABSOLUTE/PATH/TO/shaka_packager_mcp.py` with the full path to the script file\n- `/PATH/TO/PACKAGER` with the full path to your Shaka Packager executable\n\nFor example:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--mount\", \"type=bind,src=/Users/username/Videos,dst=/projects/video-drop\",\n        \"mcp/filesystem\",\n        \"/projects\"\n      ]\n    },\n    \"shaka-packager\": {\n      \"command\": \"/Users/username/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/Users/username/Development/shaka-packager-mcp/shaka_packager_mcp.py\"\n      ],\n      \"env\": {\n        \"VIDEO_PATH\": \"/Users/username/Videos\",\n        \"SHAKA_PACKAGER_PATH\": \"/Users/username/.shaka/packager\"\n      }\n    }\n  }\n}\n```\n\n### Step 4: Restart Claude Desktop\n\nAfter editing the configuration file, restart Claude Desktop to apply the changes.\n\n### How to Use the Two-Server Approach\n\n1. First, browse your video files using the simplified filesystem server:\n   - Ask Claude to \"List the files in my video directory\"\n   - Navigate to the video file you want to analyze or process\n\n2. Once you've found your video file, use its path with the Shaka Packager tools:\n   - For analysis: \"Please analyze this video: /Users/username/Videos/example.mp4\"\n   - For processing: \"Please package this video for HLS: /Users/username/Videos/example.mp4\"\n\n### Troubleshooting\n\nIf you encounter any issues:\n\n1. Make sure both servers are properly configured with absolute paths\n2. Verify that Shaka Packager is installed and accessible\n3. Ensure the directory specified for the filesystem server exists and contains videos\n4. Check Claude Desktop logs for errors at:\n   - macOS: `~/Library/Logs/Claude/mcp*.log`\n   - Windows: `%APPDATA%\\Claude\\logs\\mcp*.log`\n\n## Usage\n\nOnce both the Filesystem MCP server and the Shaka Packager MCP server are running in Claude Desktop:\n\n1. **Access your video files**:\n   ```\n   Please show me the files in my Videos directory\n   ```\n\n2. **Navigate to your video file**:\n   ```\n   Please show me the files in the Movies subdirectory\n   ```\n\n3. **Copy the file:// URI path of the video** you want to process\n\n4. **Use the Shaka Packager tools with the file path**:\n   ```\n   Please analyze this video: file:///Users/username/Videos/my_video.mp4\n   ```\n   or\n   ```\n   Please package this video for HLS and DASH streaming: file:///Users/username/Videos/my_video.mp4\n   ```\n\n5. The server will execute the appropriate Shaka Packager command and provide a detailed summary of the results\n\nYou can also use direct file paths if you know the exact location of your video files:\n```\nPlease analyze this video: /Users/username/Videos/my_video.mp4\n```\n\n## Tools\n\nThe server provides these tools:\n\n1. **analyze_video**: Examines a video file and provides detailed stream information with intelligent error handling\n2. **run_shaka_packager**: Executes any Shaka Packager command with custom arguments and proper path handling\n3. **get_shaka_options**: Retrieves available command options and version information\n4. **get_shaka_documentation**: Provides comprehensive documentation and examples for using Shaka Packager\n\n## Prompts\n\nThe server includes these prompt templates:\n\n- MP4 to TS conversion\n- VOD packaging in HLS and DASH\n- Live streaming packaging\n- Content encryption\n- Ad insertion preparation\n- Video analysis\n- Command format reminder\n- Error interpretation guidance\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `SHAKA_PACKAGER_PATH`: Path to the Shaka Packager executable (highly recommended for Claude Desktop)\n- `VIDEO_PATH`: Path to your local video directory (used for translating paths between Docker and host)\n- `DOCKER_PATH`: Docker container mount path (default: \"/projects/video-drop\")\n- `TEMP_DIR`: Custom temporary directory for file uploads\n- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n- `COMMAND_TIMEOUT`: Timeout in seconds for Shaka Packager commands (default: 300)\n\nYou can set these in:\n1. Your Claude Desktop configuration file (preferred for `SHAKA_PACKAGER_PATH` and `VIDEO_PATH`)\n2. Your environment variables\n3. A `.env` file in the same directory as the script\n\nExample `.env` file:\n```\nSHAKA_PACKAGER_PATH=/usr/local/bin/packager\nVIDEO_PATH=/Users/yourusername/Videos\nLOG_LEVEL=DEBUG\n```\n\n## Development\n\n### Setting up a development environment\n\n```bash\n# Clone the repository\ngit clone https://github.com/coderjun/shaka-packager-mcp.git\ncd shaka-packager-mcp\n\n# Install development dependencies with pip\npip install -e \".[dev]\"\n\n# Or with uv\nuv pip install -e \".[dev]\"\n```\n\n### Running tests\n\n```bash\npytest\n```\n\n### Code formatting\n\n```bash\nblack .\nisort .\n```\n\n### Understanding the Code Structure\n\nThe main components of the Shaka Packager MCP server are:\n\n- `shaka_packager_mcp.py`: Main server implementation with MCP tools and prompts\n- `tests/`: Test suite for verifying functionality\n\nThis server is designed to work with the official MCP filesystem server for accessing video files.\n\n### Key Features in the Implementation\n\n- **Robust path handling**: Automatically translates paths between Docker and host environments\n- **Smart error handling**: Provides meaningful error messages and suggestions\n- **Command syntax assistance**: Helps correctly format Shaka Packager commands\n- **Documentation integration**: Provides comprehensive documentation and examples\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Getting Help\n\nFeel free to use an AI code copilot, the author does.\n\nIf you encounter any issues or have questions:\n\n1. Check the troubleshooting section in this README\n2. Review the [Shaka Packager documentation](https://shaka-project.github.io/shaka-packager/html/index.html)\n3. Use the `get_shaka_documentation` tool for interactive help within Claude\n4. [Open an issue](https://github.com/coderjun/shaka-packager-mcp/issues) on GitHub\n\n## Acknowledgements\n\n- [Shaka Packager](https://github.com/shaka-project/shaka-packager) for the powerful video processing capabilities\n- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) for the communication framework\n- [Claude](https://claude.ai) for the AI assistant capabilities\n- [Anthropic](https://www.anthropic.com/) for developing Claude and the MCP standard",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "packager",
        "coderjun",
        "streaming",
        "shaka packager",
        "video transcoding",
        "packager mcp"
      ],
      "category": "image-and-video-generation"
    },
    "dasheck0--face-generator": {
      "owner": "dasheck0",
      "name": "face-generator",
      "url": "https://github.com/dasheck0/face-generator",
      "imageUrl": "/freedevtools/mcp/pfp/dasheck0.webp",
      "description": "Generate realistic human face images with customizable shapes, sizes, and backgrounds. Supports batch generation for multiple images and offers transparent backgrounds for non-square outputs.",
      "stars": 5,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:05Z",
      "readme_content": "# Face Generator MCP Server: Generate Human Faces with Ease\n\n[![Smithery badge](https://smithery.ai/badge/@dasheck0/face-generator)](https://smithery.ai/server/@dasheck0/face-generator)\n\n<a href=\"https://glama.ai/mcp/servers/0v6oomxing\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0v6oomxing/badge\" alt=\"Face Generator Server MCP server\" />\n</a>\n\n## Features\nThis project provides a Model Context Protocol (MCP) server for generating human face images using https://thispersondoesnotexist.com. Think of it as a tool that lets other applications, like Cline, generate realistic-looking faces on demand.\n\nThis guide is designed for beginners, so we'll walk through everything step-by-step. We'll cover:\n\n1.  **Prerequisites:** What you need before you start.\n2.  **Installation and Setup:** Getting everything up and running.\n3.  **Running the Server:** Starting the server.\n4.  **Integrating with Cline:** Connecting this server to the Cline VS Code extension.\n5.  **Troubleshooting:** Common problems and solutions.\n6.  **Tool Parameters:** A list of the parameters you can use with the `generate_face` tool.\n\n## 1. Prerequisites\n\nBefore you begin, you'll need a few things:\n\n*   **Node.js and npm:** Node.js is a JavaScript runtime that lets you run JavaScript code outside of a web browser. npm (Node Package Manager) is included with Node.js and is used to install packages (libraries of code).\n    *   [Download Node.js](https://nodejs.org/en/download/). **Choose the LTS (Long Term Support) version.** This is the most stable version. Follow the installation instructions for your operating system. Make sure to include npm in the installation (it's usually included by default).\n    *   **Verify Installation:** After installing Node.js, open a new terminal (command prompt on Windows, Terminal on macOS/Linux) and type:\n        ```bash\n        node -v\n        npm -v\n        ```\n        You should see version numbers for both Node.js and npm. If you see an error, Node.js might not be installed correctly, or it might not be in your system's PATH. (See Troubleshooting below).\n\n## 2. Installation and Setup\n\nLet's get the project code and set it up:\n\n1.  **Clone the Repository:**\n    *   **Using Git (command line):**\n        1.  Open a terminal (command prompt or Terminal).\n        2.  Navigate to the directory where you want to store the project. For example, to put it on your Desktop:\n            ```bash\n            cd Desktop\n            ```\n        3.  Clone the repository:\n            ```bash\n            git clone https://github.com/Moe/mcp-face-generator\n            ```\n        4.  Change into the project directory:\n            ```bash\n            cd mcp-face-generator\n            ```\n    *   **Using GitHub Desktop:**\n        1.  Open GitHub Desktop.\n        2.  Click \"File\" -> \"Clone Repository...\".\n        3.  In the \"URL\" tab, paste the repository URL.\n        4.  Choose a local path (where you want to save the project on your computer).\n        5.  Click \"Clone\".\n\n2.  **Install Dependencies:** This downloads all the necessary libraries the project needs. In the terminal, inside the project directory, run:\n    ```bash\n    npm install\n    ```\n    This might take a few minutes.\n\n3.  **Build the Project:** This compiles the code into an executable format.\n    ```bash\n    npm run build\n    ```\n\n## 3. Running the Server\n\nYou can run the server in two main ways:\n\n*   **Standalone Mode:** This runs the server directly, and it will output messages to the terminal.\n*   **Development/Debug Mode:** This runs the server with the MCP Inspector. You can open the URL that it outputs in your browser and start playing around.\n\n### 3.1 Standalone Mode\n\nTo run the server in standalone mode, use the following command in the terminal (from the project directory):\n\n```bash\nnpm run start\n```\n\nYou should see messages in the terminal indicating that the server is running. It will listen for connections from MCP clients. The server will keep running until you stop it (usually with Ctrl+C).\n\n### 3.2 Development/Debug Mode (with Inspector)\n\nThis mode is useful for debugging.\n\n1.  **Start the server in debug mode:**\n    ```bash\n    npm run dev\n    ```\n    This will start the server and output a message like: `🔍 MCP Inspector is up and running at http://localhost:5173 🚀`. This is the URL you'll use to open the MCP inspector in your Browser.\n\n## 4. Integrating with Cline\n\nCline is a VS Code extension that uses MCP servers to provide language support. Here's how to connect this face generator server to Cline:\n\n1.  **Install Cline:** If you haven't already, install the \"Cline\" extension in VS Code.\n\n2.  **Open Cline Settings:**\n    *   Open the VS Code settings (File -> Preferences -> Settings, or Ctrl+,).\n    *   Search for \"Cline MCP Settings\".\n    *   Click \"Edit in settings.json\". This will open the `cline_mcp_settings.json` file.\n\n3.  **Add the Server Configuration:** You'll need to add an entry to the `servers` array in the `cline_mcp_settings.json` file. Here's an example:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"face-generator\": {\n          \"command\": \"node\",\n          \"args\": [\n            \"C:/PATH_TO/mcp-face-generator/build/index.js\"\n          ],\n          \"disabled\": false,\n          \"autoApprove\": []\n        }\n      }\n    }\n    ```\n    *   Replace `\"C:/PATH_TO/mcp-face-generator/build/index.js\"` with the actual path to the `index.js` file in your project directory.  Use forward slashes (/) or double backslashes (\\\\\\\\) for the path on Windows.\n\n4.  **Test the Connection:**\n    *   Cline should automatically connect to the server. You will see the Server appear in the \"MCP Servers\" Panel (in the Cline extension, you'll find different buttons on the top.)\n    *   Ask Cline to generate a face and it should mention the MCP Server and should try to use the corresponding tools\n\n## 5. Troubleshooting\n\n*   **`node -v` or `npm -v` gives an error:**\n    *   Make sure Node.js is installed correctly. Try reinstalling it.\n    *   Ensure that the Node.js installation directory is in your system's PATH environment variable. On Windows, you can edit environment variables through the System Properties (search for \"environment variables\" in the Start Menu).\n*   **`npm install` fails:**\n    *   Make sure you have an internet connection.\n    *   Try deleting the `node_modules` folder and running `npm install` again.\n    *   If you're behind a proxy, you might need to configure npm to use the proxy. Search online for \"npm proxy settings\".\n*   **Cline doesn't connect to the server:**\n    *   Double-check the settings in `cline_mcp_settings.json`. It *must* be the correct path to the `index.js` file.\n    *   Make sure the server is running (use `npm run start` to check).\n    *   Restart VS Code.\n\n## 6. Tool Parameters\n\nThe `generate_face` tool accepts the following parameters:\n\n*   `outputDir`: (required) Directory to save the images\n*   `fileName`: Optional file name (defaults to timestamp)\n*   `count`: Number of images to generate (default: 1)\n*   `width`: Image width in pixels (default: 256)\n*   `height`: Image height in pixels (default: 256)\n*   `shape`: Image shape (square|circle|rounded, default: square)\n*   `borderRadius`: Border radius for rounded shape (default: 32)\n*   `returnImageContent`: Return image as base64 encoded content instead of file path (default: false)\n\n## Example\n\n```json\n{\n  \"outputDir\": \"./output\",\n  \"count\": 3,\n  \"width\": 512,\n  \"height\": 512,\n  \"shape\": \"circle\",\n  \"returnImageContent\": true\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "generate",
        "dasheck0",
        "backgrounds",
        "face generator",
        "face images",
        "dasheck0 face"
      ],
      "category": "image-and-video-generation"
    },
    "douglarek--unsplash-mcp-server": {
      "owner": "douglarek",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/douglarek/unsplash-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/douglarek.webp",
      "description": "Access a vast library of high-quality images from Unsplash through a simplified API integration. Fetch stunning images on demand to enhance visual content in applications.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-08-25T02:26:41Z",
      "readme_content": "# Unsplash MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@douglarek/unsplash-mcp-server)](https://smithery.ai/server/@douglarek/unsplash-mcp-server)\n\nA rewrite of the [Unsplash MCP Server](https://github.com/hellokaton/unsplash-mcp-server) using the [mark3labs/mcp-go](https://github.com/mark3labs/mcp-go) library.\n\n## Usage\n\nBefore building, you must install go 1.24+ first.\n\n```bash\ngit clone https://github.com/douglarek/unsplash-mcp-server.git\ncd unsplash-mcp-server\nmake build\n```\n\n### Cursor Editor Integration\n\nTo use this server in Cursor, you can add the following to your `mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"<source_dir>/cmd/server/unsplash-mcp-server\",\n      \"args\": [],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"<your_unsplash_access_key>\"\n      }\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "unsplash",
        "images",
        "mcp",
        "stunning images",
        "images unsplash",
        "unsplash mcp"
      ],
      "category": "image-and-video-generation"
    },
    "drumnation--unsplash-smart-mcp-server": {
      "owner": "drumnation",
      "name": "unsplash-smart-mcp-server",
      "url": "https://github.com/drumnation/unsplash-smart-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/drumnation.webp",
      "description": "Connects AI models to Unsplash for searching and delivering stock photos with context-aware selection and automatic attribution management.",
      "stars": 47,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T20:15:22Z",
      "readme_content": "# 🖼️ Unsplash Smart MCP Server\n\n> **Empower your AI agents with stunning visuals, zero hassle.**\n\nA powerful FastMCP server that enables AI agents to seamlessly search, recommend, and deliver professional stock photos from Unsplash with intelligent context awareness and automated attribution management.\n\n![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)\n![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.x-brightgreen)\n![TypeScript Ready](https://img.shields.io/badge/TypeScript-Ready-blue)\n[![smithery badge](https://smithery.ai/badge/@drumnation/unsplash-smart-mcp-server)](https://smithery.ai/server/@drumnation/unsplash-smart-mcp-server)\n[![npm version](https://img.shields.io/npm/v/@drumnation/unsplash-smart-mcp-server.svg)](https://www.npmjs.com/package/@drumnation/unsplash-smart-mcp-server)\n\n## 🚀 Why Choose This Unsplash Integration\n\nIn the landscape of visual content integration, our Unsplash Smart MCP Server stands out as the **definitive solution** for AI-powered image acquisition:\n\n- **🧠 AI-Agent Optimized**: Purpose-built for AI agents like Claude in Cursor, streamlining image requests with natural language\n- **🔍 Context-Aware Image Selection**: Interprets vague requests intelligently, delivering relevant images even from abstract prompts\n- **⚡ Single Tool Efficiency**: Eliminates tool spam with a unified `stock_photo` tool that handles the entire image workflow\n- **📊 Resource Optimization**: URL-first approach conserves bandwidth and storage while maintaining flexibility\n- **✅ Automatic Attribution**: Built-in compliance with Unsplash's Terms of Service with zero developer effort\n- **📁 Project-Aware Organization**: Intelligently organizes images based on your project structure (Next.js, React, Vue, etc.)\n- **🧩 Seamless Integration**: Designed for minimal setup and maximum compatibility with your existing workflow\n\n## ✨ Features Beyond Comparison\n\n### For AI Agent Developers\n\n- **Smart Contextual Search**: Find the perfect image through natural language requests\n- **Automatic Subject Selection**: AI determines optimal image subjects from your purpose description\n- **Intent-Driven Results**: Get images that match not just keywords, but the underlying intent\n- **Seamless Agent Integration**: Works out-of-the-box with Claude in Cursor and other MCP-compatible agents\n\n### For Project Efficiency\n\n- **Two-Step Workflow**: Get URLs for controlled downloads, avoiding permission issues and unnecessary storage\n- **Project-Aware File Management**: Auto-organizes images based on framework conventions\n- **Intelligent Directory Creation**: Creates appropriate folder structures based on your project type\n- **Progressive Enhancement**: Works with any project size, from quick prototypes to enterprise applications\n\n### For Compliance Peace of Mind\n\n- **Complete Attribution Management**:\n  - Local attribution database tracks all image usage\n  - Automatic embedding of photographer metadata in images (EXIF, IPTC, XMP)\n  - One-click generation of attribution pages in multiple formats\n  - Comprehensive API for attribution data\n\n## 🛠️ Installation\n\n### Prerequisites\n\n- Node.js 18.x or higher\n- An Unsplash API access key ([get one here](https://unsplash.com/developers))\n\n### Local Installation (Recommended)\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/drumnation/unsplash-smart-mcp-server.git\ncd unsplash-smart-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Configure your Cursor MCP settings:\n   - macOS: Edit `~/.cursor/mcp.json`\n   - Windows: Edit `%USERPROFILE%\\.cursor\\mcp.json`\n   - Linux: Edit `~/.cursor/mcp.json`\n\n4. Add the following configuration:\n```json\n{\n  \"servers\": {\n    \"unsplash\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"src/server.ts\"],\n      \"cwd\": \"/absolute/path/to/unsplash-smart-mcp-server\",\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n5. Replace:\n   - `/absolute/path/to/unsplash-smart-mcp-server` with the actual path where you cloned the repo\n   - `your_api_key_here` with your Unsplash API key\n\n6. Save the file and restart Cursor.\n\n> **Important:** Unlike many MCP servers, this server requires direct process piping and cannot be accessed via TCP ports or through npm directly due to how it handles FastMCP's I/O interactions. The local installation method is the most reliable approach.\n\n### Cursor CLI Alternative\n\nIf you prefer using Cursor's CLI:\n\n```bash\nclaude mcp add unsplash npx tsx /path/to/unsplash-smart-mcp-server/src/server.ts --cwd /path/to/unsplash-smart-mcp-server\nclaude mcp config set unsplash UNSPLASH_ACCESS_KEY=your_api_key_here\n```\n\nReplace the paths and API key with your actual values.\n\n### Via Docker (Most Reliable Method)\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/drumnation/unsplash-smart-mcp-server.git\ncd unsplash-smart-mcp-server\n```\n\n2. Create a `docker-compose.yml` file:\n```yaml\nservices:\n  unsplash-mcp:\n    build: .\n    image: unsplash-mcp-server\n    restart: always\n    stdin_open: true\n    tty: true\n    environment:\n      - UNSPLASH_ACCESS_KEY=your_api_key_here\n```\n\n3. Build and start the container:\n```bash\ndocker-compose up -d\n```\n\n4. Configure your Cursor MCP settings:\n   - macOS: Edit `~/.cursor/mcp.json`\n   - Windows: Edit `%USERPROFILE%\\.cursor\\mcp.json`\n   - Linux: Edit `~/.cursor/mcp.json`\n\n5. Add the following configuration:\n```json\n{\n  \"servers\": {\n    \"unsplash\": {\n      \"command\": \"docker\",\n      \"args\": [\"exec\", \"-i\", \"unsplash-mcp-unsplash-mcp-1\", \"tsx\", \"src/server.ts\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n6. Save the file and restart Cursor.\n\nThis setup will:\n- Start the server automatically when Docker starts\n- Restart the server if it crashes\n- Run in the background without terminal windows\n- Provide a reliable connection to Cursor\n\n### Via Smithery (Cloud Deployment)\n\nIf you prefer cloud deployment, you can use Smithery:\n\n1. Install the server in Cursor via Smithery:\n\n```bash\nnpx @smithery/cli install @drumnation/unsplash-smart-mcp-server --client cursor --key your_api_key_here\n```\n\n2. Alternatively, you can log in to [Smithery.ai](https://smithery.ai) and deploy it through their web interface.\n\n> **Note for Windows users:** Smithery deployment includes special handling for Windows compatibility.\n\nFor detailed instructions and troubleshooting, see the [Smithery Deployment Guide](./docs/smithery-deployment.md).\n\n## 🧩 Integration with AI Agents\n\n### Step-by-Step Guide for Claude in Cursor\n\nOur Unsplash Smart MCP Server is designed to make image acquisition through AI agents effortless and intuitive:\n\n1. **Initiate a request**: Simply ask Claude for an image in natural language\n2. **AI interpretation**: Claude understands your needs and calls the `stock_photo` tool with optimized parameters\n3. **Smart image selection**: The server interprets context and finds the most relevant images\n4. **Presentation of options**: Claude presents you with the best matches and download commands\n5. **Seamless download**: Execute the suggested commands to place images exactly where you need them\n6. **Automatic attribution**: All attribution data is stored and can be accessed whenever needed\n\nThis process eliminates the traditional workflow of:\n1. ~~Searching Unsplash manually~~\n2. ~~Scrolling through hundreds of results~~\n3. ~~Downloading images to random locations~~\n4. ~~Moving files to the correct project folders~~\n5. ~~Manually tracking attribution data~~\n6. ~~Creating attribution pages~~\n\n### Example Prompts for AI Agents\n\nAsk Claude in Cursor for images using natural language prompts like these:\n\n```\n\"Find a professional image for a tech startup landing page hero section\"\n```\n\n## 🪟 Windows Compatibility\n\nIf you're using Windows and experiencing the \"Client closed\" error when running the MCP server in Cursor, follow these special configuration steps:\n\n### Windows-specific MCP Configuration\n\nCreate a file named `mcp.json` in your `.cursor` directory (typically at `%USERPROFILE%\\.cursor\\mcp.json`) with one of these configurations:\n\n#### Option 1: Direct Node Execution (Recommended)\n\n```json\n{\n  \"mcpServers\": {\n    \"stock_photo\": {\n      \"command\": \"node\",\n      \"args\": [\"./node_modules/.bin/tsx\", \"path/to/unsplash-mcp/src/server.ts\"],\n      \"disabled\": false,\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"your_api_key_here\"\n      },\n      \"shell\": false\n    }\n  }\n}\n```\n\n#### Option 2: PowerShell Approach\n\n```json\n{\n  \"mcpServers\": {\n    \"stock_photo\": {\n      \"command\": \"powershell\",\n      \"args\": [\"-Command\", \"npx tsx path/to/unsplash-mcp/src/server.ts\"],\n      \"disabled\": false,\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nFor complete documentation on Windows compatibility, see [Windows Compatibility Guide](./docs/windows-compatibility.md).\n\n## 🛠️ API Reference\n\n### URL-First Approach: The Smart Choice\n\nOur architecture uses a URL-first approach rather than direct image embedding for several critical reasons:\n\n1. **Storage Efficiency**: Prevents AI agents from unnecessarily storing large binary data in their context\n2. **Bandwidth Conservation**: Reduces data transfer between services, improving response times\n3. **Placement Flexibility**: Allows developers to download images exactly where they're needed\n4. **Permission Management**: Avoids filesystem permission issues in restricted environments\n5. **Workflow Integration**: Seamlessly integrates with existing development pipelines\n\nThis strategy enables AI agents to intelligently suggest the optimal download location based on project context, without being constrained by their own environment limitations.\n\n### Minimizing Tool Spam and API Calls\n\nUnlike other solutions that require multiple tool calls for searching, filtering, downloading, and attributing images, our server:\n\n- **Unifies the entire image workflow** into a single `stock_photo` tool\n- **Optimizes result retrieval** by requesting more images upfront to enable better filtering\n- **Eliminates ping-pong interactions** between the agent and services\n- **Reduces agent token usage** by streamlining request and response formats\n\nThis design significantly reduces the number of API calls and tool invocations, leading to faster results and lower operational costs.\n\n## 🔄 Automatic Attribution and Compliance\n\n### Unsplash Terms of Service: Effortless Compliance\n\nUsing images from Unsplash requires adherence to their [Terms of Service](https://unsplash.com/license). Our server handles this automatically:\n\n1. **Attribution Data Capture**: Every image download automatically stores photographer information\n2. **Metadata Embedding**: Photographer details are embedded directly into image files\n3. **Attribution Database**: A local database maintains a record of all image usage\n4. **Attribution Generators**: Built-in tools create HTML and React attribution components\n5. **API Access**: Simple endpoints to retrieve attribution data for any project\n\nBy using our Unsplash Smart MCP Server, you are automatically compliant with Unsplash's requirements without any additional effort.\n\n### Attribution Management System\n\nThe server includes a comprehensive attribution management system:\n\n```javascript\n// Retrieve attribution data for your project\nconst attributions = await fetch('http://localhost:3000/api/unsplash', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    method: 'get_attributions',\n    params: {\n      format: 'json',  // Options: json, html, react\n      projectPath: '/path/to/your/project'\n    }\n  })\n}).then(res => res.json());\n\n// attributions contains complete data about every image used\n```\n\nThe API can generate three types of attribution files:\n\n1. **JSON**: Structured data for custom implementations\n2. **HTML**: Ready-to-use HTML page for website footer or credits section\n3. **React**: Drop-in React component for modern web applications\n\n## 💼 Developer Workflow Integration\n\n### Real-World Use Cases\n\nOur Unsplash Smart MCP Server seamlessly integrates into your development workflow:\n\n#### UI Development\n- Instantly populate mockups with relevant placeholder images\n- Maintain consistent image dimensions across components\n- Organize images logically within your project structure\n\n#### Documentation\n- Enhance technical documentation with explanatory visuals\n- Create visually appealing tutorials and guides\n- Maintain proper attribution for all visual assets\n\n#### Content Creation\n- Quickly find images for blog posts and articles\n- Generate visuals for social media content\n- Access consistent imagery for product marketing\n\n#### Application Development\n- Populate e-commerce sites with product imagery\n- Create visually rich user experiences\n- Maintain separate image collections for different sections\n\n### Framework-Specific Organization\n\nImages are automatically organized based on your project type:\n\n| Framework | Default Image Path | Alternate Paths |\n|-----------|-------------------|----------------|\n| Next.js   | `/public/images/` | `/public/assets/images/` |\n| React     | `/src/assets/images/` | `/assets/images/` |\n| Vue       | `/src/assets/images/` | `/public/images/` |\n| Angular   | `/src/assets/images/` | `/assets/images/` |\n| Generic   | `/assets/images/` | `~/Downloads/stock-photos/` |\n\n## 🥇 Competitive Differentiation\n\n### Why Choose Our Unsplash Integration?\n\n| Feature | Unsplash Smart MCP Server | Alternatives |\n|---------|--------------|--------------|\n| **AI Agent Integration** | ✅ Purpose-built for AI agent workflow | ❌ Typically requires manual parameter setting |\n| **Context Awareness** | ✅ Interprets vague requests intelligently | ❌ Relies on exact keyword matching |\n| **Tool Efficiency** | ✅ Single tool handles entire workflow | ❌ Often requires multiple separate tools |\n| **Attribution Management** | ✅ Comprehensive system with multiple formats | ❌ Manual tracking or basic text output |\n| **Project Organization** | ✅ Framework-aware folder structures | ❌ Generic downloads to a single location |\n| **Installation Complexity** | ✅ Simple one-line command | ❌ Often requires multiple configuration steps |\n| **Response Format** | ✅ AI-optimized with relevant context | ❌ Generic JSON requiring further processing |\n| **Download Flexibility** | ✅ URL-first with intelligent suggestions | ❌ Either direct downloads or just URLs |\n\n## ⚙️ Configuration\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `UNSPLASH_ACCESS_KEY` | Your Unsplash API access key | - |\n| `PORT` | Port for the server to listen on | `3000` |\n| `HOST` | Host for the server | `localhost` |\n| `ATTRIBUTION_DB_PATH` | Path to store attribution database | `~/.unsplash-mcp` |\n\n### Tool Parameters\n\n#### stock_photo\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `query` | string | What to search for (AI will choose if not specified) | - |\n| `purpose` | string | Where the image will be used (e.g., hero, background) | - |\n| `count` | number | Number of images to return | `1` |\n| `orientation` | string | Preferred orientation (any, landscape, portrait, square) | `any` |\n| `width` | number | Target width in pixels | - |\n| `height` | number | Target height in pixels | - |\n| `minWidth` | number | Minimum width for filtering results | - |\n| `minHeight` | number | Minimum height for filtering results | - |\n| `outputDir` | string | Directory to save photos | `~/Downloads/stock-photos` |\n| `projectType` | string | Project type for folder structure (next, react, vue, angular) | - |\n| `category` | string | Category for organizing images (e.g., heroes, backgrounds) | - |\n| `downloadMode` | string | Whether to download images or return URLs | `urls_only` |\n\n#### get_attributions\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `format` | string | Output format (json, html, react) | `json` |\n| `projectPath` | string | Filter attributions to a specific project path | - |\n| `outputPath` | string | Where to save attribution files | - |\n\n## 🔧 Troubleshooting\n\n### Common Issues and Solutions\n\n| Issue | Solution |\n|-------|----------|\n| **Connection Refused** | Ensure the server is running on the configured port |\n| **Authentication Error** | Verify your Unsplash API key is correctly set |\n| **No Images Found** | Try broader search terms or check your search query |\n| **Download Permission Issues** | Use `downloadMode: 'urls_only'` and manual download commands |\n| **Docker Container Exits Prematurely** | Ensure you're using `CMD [\"npm\", \"start\"]` in your Dockerfile instead of directly running the TypeScript file with tsx. This ensures the server stays running in a Docker environment. |\n| **Timeout Errors** | The default MCP timeout is 60 seconds, which may be insufficient for downloading larger images or processing multiple images. For image-heavy operations: 1) Process fewer images per request, 2) Use smaller image dimensions, 3) Consider using `urls_only` mode instead of auto-download, 4) Check network connectivity |\n| **Attribution Not Found** | Verify the image was downloaded through the MCP server |\n| **Unhandled MCP Errors** | If you see `\"McpError: MCP error -32001: Request timed out\"` errors, your request is likely taking too long. Break it into smaller operations or use the URLs-only approach |\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n### Development Workflow\n\n1. Clone the repository\n2. Install dependencies with `npm install`\n3. Create a `.env` file with your Unsplash API key\n4. Run in development mode with `npm run dev`\n5. Run tests with `npm test`\n\n## 🗺️ Roadmap\n\nHere's what we're planning for future releases:\n\n- **Image Editing Capabilities**: Basic resizing, cropping, and adjustment tools\n- **Advanced Search Filters**: More granular control over image selection\n- **Batch Processing**: Handle multiple image requests efficiently\n- **Custom Collections**: Save and manage groups of images for projects\n- **Team Collaboration**: Share attribution and image collections\n- **Usage Analytics**: Track image usage across projects\n- **Additional Image Sources**: Integration with other stock photo providers\n- **Improved Timeout Handling**: Enhanced timeout configuration and recovery mechanisms\n\n## 📄 License\n\nMIT License\n\n## 📚 Attribution Requirements\n\nWhen using images from Unsplash, you must comply with the [Unsplash License](https://unsplash.com/license):\n\n- Attribution is not required but appreciated\n- You cannot sell unaltered copies of the photos\n- You cannot compile photos from Unsplash to create a competing service\n\nOur server's attribution system makes it easy to provide proper credit to photographers.\n\n## 📞 Contact\n\nFor issues or questions, please [open an issue](https://github.com/drumnation/unsplash-smart-mcp-server/issues) on GitHub.\n\n## 🧰 Development and Testing\n\n### Running the Server Locally\n\n```bash\n# Clone the repository\ngit clone https://github.com/drumnation/unsplash-smart-mcp-server.git\ncd unsplash-smart-mcp-server\n\n# Install dependencies\nnpm install\n\n# Set up your environment variables\ncp .env.example .env\n# Edit .env to add your UNSPLASH_ACCESS_KEY\n\n# Start the development server\nnpm run dev\n```\n\n### Testing\n\nThe package includes a comprehensive test suite:\n\n```bash\n# Run core tests\nnpm test\n\n# Run all tests and get a summary report\nnpm run test:all\n```\n\nThe test suite includes:\n- Unit and integration tests\n- Manual tool testing\n- Docker container tests\n- Smithery.ai integration tests\n\nFor detailed information about testing, see [docs/testing.md](docs/testing.md).\n\n---\n\n<p align=\"center\">\n  <strong>Empower your AI agents with the perfect images, every time.</strong><br>\n  Built with ❤️ for developers and AI enthusiasts.\n</p>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "mcp",
        "attribution",
        "smart mcp",
        "automatic attribution",
        "photos context"
      ],
      "category": "image-and-video-generation"
    },
    "dvejsada--mcp_media_generator": {
      "owner": "dvejsada",
      "name": "mcp_media_generator",
      "url": "https://github.com/dvejsada/mcp_media_generator",
      "imageUrl": "/freedevtools/mcp/pfp/dvejsada.webp",
      "description": "Create images using the Amazon Nova Canvas model and videos using the Amazon Nova Reel model. Connects to existing tools for media generation and storage.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-16T11:26:11Z",
      "readme_content": "# What is it?\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) Server running over SSE\n\n# What it offers?\n\nTools to create images using Amazon Nova Canvas model and videos using Amazon Nova Reel model.\n\n# What do I need?\n\n- Amazon Bedrock account with access to Amazon Nova Canvas and Amazon Nova Reel models.\n- Amazon S3 bucket to store the video\n- MCP Client, such is Claude Desktop or [LibreChat](https://github.com/danny-avila/LibreChat)\n\n# How to run this?\n\nUsing Docker with precompiled image as per docker-compose.yml. App is listening on port 8961.\n\n## How to add to LibreChat\n\nIn your librechat.yaml file, add the following section:\n\n```yaml\nmcpServers:\n  media-creator:\n    type: sse # type can optionally be omitted\n    url: URL of your docker container # e.g. http://localhost:8961/sse\n```\n\n## How to use in LibreChat\n\nAfter the server is added to LibreChat as per above, restart LibreChat to connect to MCP server and discover tools. Then, create an agent and add the respective tools to agent.\n\nWhen the agent is created, you may ask the agent to create image or video which should invoke the provided tools.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_media_generator",
        "dvejsada",
        "videos",
        "dvejsada mcp_media_generator",
        "video generation",
        "generation dvejsada"
      ],
      "category": "image-and-video-generation"
    },
    "el-el-san--vidu-mcp-server": {
      "owner": "el-el-san",
      "name": "vidu-mcp-server",
      "url": "https://github.com/el-el-san/vidu-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/el-el-san.webp",
      "description": "Generate videos from static images using advanced AI models, while monitoring the status of video generation tasks and uploading images for processing.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-09T11:11:26Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/el-el-san-vidu-mcp-server-badge.png)](https://mseep.ai/app/el-el-san-vidu-mcp-server)\n\n# Vidu MCP Server\n[![smithery badge](https://smithery.ai/badge/@el-el-san/vidu-mcp-server)](https://smithery.ai/server/@el-el-san/vidu-mcp-server)\n\nVidu動画生成APIと連携するためのModel Context Protocol (MCP) サーバーです。Viduの強力なAIモデルを使用して、画像から動画を生成するツールを提供します。\n\n## 機能\n\n- **画像から動画への変換**: カスタマイズ可能な設定で静止画から動画を生成\n  - 複数モデル対応: viduq1、vidu1.5、vidu2.0\n  - モデル固有の時間・解像度制約\n  - 4秒動画向けのBGM対応\n  - 非同期通知用のコールバックURL対応\n- **生成状況の確認**: クレジット使用量情報付きで動画生成タスクの進捗を監視\n- **画像アップロード**: Vidu APIで使用する画像を簡単にアップロード（最大10MB）\n\n## 前提条件\n\n- Node.js (v14以上)\n- Vidu APIキー（[Viduウェブサイト](https://vidu.com)から取得可能）\n- TypeScript（開発用）\n\n## インストール\n\n### Smithery経由でのインストール\n\n[Smithery](https://smithery.ai/server/@el-el-san/vidu-mcp-server)を使用してClaude Desktop用のVidu Video Generation Serverを自動インストール:\n\n```bash\nnpx -y @smithery/cli install @el-el-san/vidu-mcp-server --client claude\n```\n\n### Gemini CLI設定\n\nGemini CLIで使用するには、`~/.gemini/settings.json`にサーバー設定を追加してください:\n\n```json\n{\n  \"mcpServers\": {\n    \"vidu\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"your_path/vidu-mcp-server/build/index.js\"\n      ],\n      \"env\": {\n        \"VIDU_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n**注意**: `your_path`を実際のインストールディレクトリのパスに、`your_api_key_here`をあなたのVidu APIキーに置き換えてください。\n\n### 手動インストール\n1. このリポジトリをクローン:\n```bash\ngit clone https://github.com/el-el-san/vidu-mcp-server.git\ncd vidu-mcp-server\n```\n\n2. 依存関係をインストール:\n```bash\nnpm install\n```\n\n3. `.env.template`を基に`.env`ファイルを作成し、Vidu APIキーを追加:\n```\nVIDU_API_KEY=your_api_key_here\n```\n\n## 使用方法\n\n### Gemini CLI用\n\n1. TypeScriptコードをビルド:\n```bash\nnpm run build\n```\n\n2. Gemini CLI設定で設定（上記のGemini CLI設定セクションを参照）\n\n3. Gemini CLIを再起動してMCPを読み込み\n\n## ツール\n\n### 1. 画像から動画への変換\n\nカスタマイズ可能なパラメータで静止画を動画に変換します。\n\nパラメータ:\n- `image_url` (必須): 動画に変換する画像のURL\n- `prompt` (オプション): 動画生成用のテキストプロンプト（最大1500文字）\n- `duration` (オプション): 出力動画の時間（秒）（モデル固有）\n  - **viduq1**: 5秒のみ\n  - **vidu1.5/vidu2.0**: 4秒または8秒（デフォルト4秒）\n- `model` (オプション): 生成用モデル名（\"viduq1\", \"vidu1.5\", \"vidu2.0\", デフォルト \"vidu2.0\"）\n- `resolution` (オプション): 出力動画の解像度（モデル/時間固有）\n  - **viduq1 (5s)**: 1080pのみ\n  - **vidu1.5/vidu2.0 (4s)**: \"360p\", \"720p\", \"1080p\"（デフォルト \"360p\"）\n  - **vidu1.5/vidu2.0 (8s)**: \"720p\"のみ\n- `movement_amplitude` (オプション): フレーム内オブジェクトの動きの振幅（\"auto\", \"small\", \"medium\", \"large\", デフォルト \"auto\"）\n- `seed` (オプション): 再現性のためのランダムシード\n- `bgm` (オプション): 動画にBGMを追加（boolean, デフォルト false, 4秒動画のみ）\n- `callback_url` (オプション): 生成状況変更時の非同期通知用URL\n\nリクエスト例:\n```json\n{\n  \"image_url\": \"https://example.com/image.jpg\",\n  \"prompt\": \"山を背景にした静かな湖\",\n  \"duration\": 8,\n  \"model\": \"vidu2.0\",\n  \"resolution\": \"720p\",\n  \"movement_amplitude\": \"medium\",\n  \"seed\": 12345,\n  \"bgm\": false\n}\n```\n\n### 2. 生成状況の確認\n\n実行中の動画生成タスクの状況を確認します。\n\nパラメータ:\n- `task_id` (必須): 画像から動画への変換ツールで返されたタスクID\n\nリクエスト例:\n```json\n{\n  \"task_id\": \"12345abcde\"\n}\n```\n\n### 3. 画像アップロード\n\nVidu APIで使用する画像をアップロードします。\n\nパラメータ:\n- `image_path` (必須): 画像ファイルのローカルパス\n- `image_type` (必須): 画像ファイルタイプ（\"png\", \"webp\", \"jpeg\", \"jpg\"）\n\nリクエスト例:\n```json\n{\n  \"image_path\": \"/path/to/your/image.jpg\",\n  \"image_type\": \"jpg\"\n}\n```\n\n## トラブルシューティング\n\n- **APIキーの問題**: Vidu APIキーが`.env`ファイル（手動設定の場合）またはGemini CLI設定（Gemini CLI設定の場合）で正しく設定されていることを確認してください\n- **ファイルアップロードエラー**: 画像ファイルが有効で、サイズ制限内（upload-imageツールは10MB、直接URL画像は最大50MB）であることを確認してください\n- **接続問題**: インターネットアクセスがあり、Vidu APIサーバーに到達できることを確認してください\n- **Gemini CLIの問題**: \n  - Gemini CLIで設定する前にサーバーがビルドされている（`npm run build`）ことを確認してください\n  - settings.jsonのパスが正しい`build/index.js`ファイルを指していることを確認してください\n  - 設定変更後にGemini CLIを再起動してください\n  - サーバー設定で`\"disabled\": false`に設定してください\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "videos",
        "vidu",
        "generate videos",
        "video generation",
        "image video"
      ],
      "category": "image-and-video-generation"
    },
    "evalstate--mcp-hfspace": {
      "owner": "evalstate",
      "name": "mcp-hfspace",
      "url": "https://github.com/evalstate/mcp-hfspace",
      "imageUrl": "/freedevtools/mcp/pfp/evalstate.webp",
      "description": "Connects to Hugging Face Spaces to access various AI models for tasks including image generation, text-to-speech, speech-to-text, and chat functionalities, requiring minimal setup.",
      "stars": 360,
      "forks": 57,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:31:58Z",
      "readme_content": "# mcp-hfspace MCP Server 🤗\n\n> [!TIP]\n>\n> You can access and configure Hugging Face MCP services directly at https://hf.co/mcp, including Gradio spaces.\n>\n> This project has been superceded by the official [Hugging Face MCP Server](https://github.com/evalstate/hf-mcp-server) and [Gradio MCP Endpoints](https://huggingface.co/blog/gradio-mcp).\n> \n> Alternatively you can run hf-mcp-server locally as a STDIO Server, or with robust support for SSE, Streaming HTTP and Streaming HTTP JSON Mode. This also runs a local UI for selecting tools and endpoints and supports `ToolListChangedNotifications` too.\n\n## hf.co/mcp\n\n![image](https://github.com/user-attachments/assets/9cbf407b-2330-4330-8274-e47305a555b9)\n\n## mcp-hfspace\n\nRead the introduction here [llmindset.co.uk/resources/mcp-hfspace/](https://llmindset.co.uk/resources/mcp-hfspace/)\n\nConnect to [Hugging Face Spaces](https://huggingface.co/spaces) with minimal setup needed - simply add your spaces and go!\n\nBy default, it connects to `black-forest-labs/FLUX.1-schnell` providing Image Generation capabilities to Claude Desktop.\n\n\n\n\n![Default Setup](./images/2024-12-09-flower.png)\n\n\n## Gradio MCP Support\n\n> [!TIP]\n> Gradio 5.28 now has integrated MCP Support via SSE: https://huggingface.co/blog/gradio-mcp. Check out whether your target Space is MCP Enabled!\n\n## Installation\n\nNPM Package is `@llmindset/mcp-hfspace`.\n\nInstall a recent version of [NodeJS](https://nodejs.org/en/download) for your platform, then add the following to the `mcpServers` section of your `claude_desktop_config.json` file:\n\n```json\n    \"mcp-hfspace\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\"\n      ]\n    }\n```\n\nPlease make sure you are using Claude Desktop 0.78 or greater.\n\nThis will get you started with an Image Generator.\n\n### Basic setup\n\nSupply a list of HuggingFace spaces in the arguments. mcp-hfspace will find the most appropriate endpoint and automatically configure it for usage. An example `claude_desktop_config.json` is supplied [below](#installation).\n\nBy default the current working directory is used for file upload/download. On Windows this is a read/write folder at `\\users\\<username>\\AppData\\Roaming\\Claude\\<version.number\\`, and on MacOS it is the is the read-only root: `/`.\n\nIt is recommended to override this and set a Working Directory for handling the upload and download of images and other file-based content. Specify either the `--work-dir=/your_directory` argument or `MCP_HF_WORK_DIR` environment variable.\n\nAn example configuration for using a modern image generator, vision model and text to speech, with a working directory set is below:\n\n```json\n    \"mcp-hfspace\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\",\n        \"--work-dir=/Users/evalstate/mcp-store\",\n        \"shuttleai/shuttle-jaguar\",\n        \"styletts2/styletts2\",\n        \"Qwen/QVQ-72B-preview\"\n      ]\n    }\n```\n\nTo use private spaces, supply your Hugging Face Token with either the `--hf-token=hf_...` argument or `HF_TOKEN` environment variable.\n\nIt's possible to run multiple server instances to use different working directories and tokens if needed.\n\n## File Handling and Claude Desktop Mode\n\nBy default, the Server operates in _Claude Desktop Mode_. In this mode, Images are returned in the tool responses, while other files are saved in the working folder, their file path is returned as a message. This will usually give the best experience if using Claude Desktop as the client.\n\nURLs can also be supplied as inputs: the content gets passed to the Space.\n\nThere is an \"Available Resources\" prompt that gives Claude the available files and mime types from your working directory. This is currently the best way to manage files.\n\n### Example 1 - Image Generation (Download Image / Claude Vision)\n\nWe'll use Claude to compare images created by `shuttleai/shuttle-3.1-aesthetic` and `FLUX.1-schnell`. The images gets saved to the Work Directory, as well as included in Claude's context window - so Claude can use its vision capabilities.\n\n![Image Generation Comparison](./images/2024-12-05-flux-shuttle.png)\n\n### Example 2 - Vision Model (Upload Image)\n\nWe'll use `merve/paligemma2-vqav2` [space link](https://huggingface.co/spaces/merve/paligemma2-vqav2) to query an image. In this case, we specify the filename which is available in the Working Directory: we don't want to upload the Image directly to Claude's context window. So, we can prompt Claude:\n\n`use paligemma to find out who is in \"test_gemma.jpg\"` -> `Text Output: david bowie`\n![Vision - File Upload](./images/2024-12-09-bowie.png)\n\n_If you are uploading something to Claude's context use the Paperclip Attachment button, otherwise specify the filename for the Server to send directly._\n\nWe can also supply a URL. For example : `use paligemma to detect humans in https://e3.365dm.com/24/12/1600x900/skynews-taylor-swift-eras-tour_6771083.jpg?20241209000914` -> `One person is detected in the image - Taylor Swift on stage.`\n\n### Example 3 - Text-to-Speech (Download Audio)\n\nIn _Claude Desktop Mode_, the audio file is saved in the WORK_DIR, and Claude is notified of the creation. If not in desktop mode, the file is returned as a base64 encoded resource to the Client (useful if it supports embedded Audio attachments).\n\n![Voice Production](./images/2024-12-08-mcp-parler.png)\n\n### Example 4 - Speech-to-Text (Upload Audio)\n\nHere, we use `hf-audio/whisper-large-v3-turbo` to transcribe some audio, and make it available to Claude.\n\n![Audio Transcribe](./images/2024-12-09-transcribe.png)\n\n### Example 5 - Image-to-Image\n\nIn this example, we specify the filename for `microsoft/OmniParser` to use, and get returned an annotated Image and 2 separate pieces of text: descriptions and coordinates. The prompt used was `use omniparser to analyse ./screenshot.png` and `use the analysis to produce an artifact that reproduces that screen`. `DawnC/Pawmatch` is also good at this.\n\n![Omniparser and Artifact](./images/2024-12-08-mcp-omni-artifact.png)\n\n### Example 6 - Chat\n\nIn this example, Claude sets a number of reasoning puzzles for Qwen, and asks follow-up questions for clarification.\n\n![Qwen Reasoning Test](./images/2024-12-09-qwen-reason.png)\n\n### Specifying API Endpoint\n\nIf you need, you can specify a specific API Endpoint by adding it to the spacename. So rather than passing in `Qwen/Qwen2.5-72B-Instruct` you would use `Qwen/Qwen2.5-72B-Instruct/model_chat`.\n\n### Claude Desktop Mode\n\nThis can be disabled with the option --desktop-mode=false or the environment variable CLAUDE_DESKTOP_MODE=false. In this case, content as returned as an embedded Base64 encoded Resource.\n\n## Recommended Spaces\n\nSome recommended spaces to try:\n\n### Image Generation\n\n- shuttleai/shuttle-3.1-aesthetic\n- black-forest-labs/FLUX.1-schnell\n- yanze/PuLID-FLUX\n- gokaygokay/Inspyrenet-Rembg (Background Removal)\n- diyism/Datou1111-shou_xin - [Beautiful Pencil Drawings](https://x.com/ClementDelangue/status/1867318931502895358)\n\n### Chat\n\n- Qwen/Qwen2.5-72B-Instruct\n- prithivMLmods/Mistral-7B-Instruct-v0.3\n\n### Text-to-speech / Audio Generation\n\n- fantaxy/Sound-AI-SFX\n- parler-tts/parler_tts\n\n### Speech-to-text\n\n- hf-audio/whisper-large-v3-turbo\n- (the openai models use unnamed parameters so will not work)\n\n### Text-to-music\n\n- haoheliu/audioldm2-text2audio-text2music\n\n### Vision Tasks\n\n- microsoft/OmniParser\n- merve/paligemma2-vqav2\n- merve/paligemma-doc\n- DawnC/PawMatchAI\n- DawnC/PawMatchAI/on_find_match_click - for interactive dog recommendations\n\n## Other Features\n\n### Prompts\n\nPrompts for each Space are generated, and provide an opportunity to input. Bear in mind that often Spaces aren't configured with particularly helpful labels etc. Claude is actually very good at figuring this out, and the Tool description is quite rich (but not visible in Claude Desktop).\n\n### Resources\n\nA list of files in the WORK_DIR is returned, and as a convenience returns the name as \"Use the file...\" text. If you want to add something to Claude's context, use the paperclip - otherwise specify the filename for the MCP Server. Claude does not support transmitting resources from within Context.\n\n### Private Spaces\n\nPrivate Spaces are supported with a HuggingFace token. The Token is used to download and save generated content.\n\n### Using Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hfspace\": {\n      \"command\": \"npx\"\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\",\n        \"--work-dir=~/mcp-files/ or x:/temp/mcp-files/\",\n        \"--HF_TOKEN=HF_{optional token}\"\n        \"Qwen/Qwen2-72B-Instruct\",\n        \"black-forest-labs/FLUX.1-schnell\",\n        \"space/example/specific-endpint\"\n        (... and so on)\n        ]\n    }\n  }\n}\n```\n\n## Known Issues and Limitations\n\n### mcp-hfspace\n\n- Endpoints with unnamed parameters are unsupported for the moment.\n- Full translation from some complex Python types to suitable MCP formats.\n\n### Claude Desktop\n\n- Claude Desktop 0.75 doesn't seem to respond to errors from the MCP Server, timing out instead. For persistent issues, use the MCP Inspector to get a better look at diagnosing what's going wrong. If something suddenly stops working, it's probably due to exhausting your HuggingFace ZeroGPU quota - try again after a short period, or set up your own Space for hosting.\n- Claude Desktop seems to use a hard timeout value of 60s, and doesn't appear to use Progress Notifications to manage UX or keep-alive. If you are using ZeroGPU spaces, large/heavy jobs may timeout. Check the WORK_DIR for results though; the MCP Server will still capture and save the result if it was produced.\n- Claude Desktops reporting of Server Status, logging etc. isn't great - use [@modelcontextprotocol/inspector](https://github.com/modelcontextprotocol/inspector) to help diagnose issues.\n\n### HuggingFace Spaces\n\n- If ZeroGPU quotas or queues are too long, try duplicating the space. If your job takes less than sixty seconds, you can usually change the function decorator `@spaces.GPU(duration=20)` in `app.py` to request less quota when running the job.\n- Passing HF_TOKEN will make ZeroGPU quotas apply to your (Pro) HF account\n- If you have a private space, and dedicated hardware your HF_TOKEN will give you direct access to that - no quota's apply. I recommend this if you are using for any kind of Production task.\n\n## Third Party MCP Services\n\n<a href=\"https://glama.ai/mcp/servers/s57c80wvgq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/s57c80wvgq/badge\" alt=\"mcp-hfspace MCP server\" /></a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hfspace",
        "ai",
        "mcp",
        "mcp hfspace",
        "face spaces",
        "image generation"
      ],
      "category": "image-and-video-generation"
    },
    "evalstate--mcp-webcam": {
      "owner": "evalstate",
      "name": "mcp-webcam",
      "url": "https://github.com/evalstate/mcp-webcam",
      "imageUrl": "/freedevtools/mcp/pfp/evalstate.webp",
      "description": "Streams live images from a webcam to an MCP Client, supporting both capturing frames and taking screenshots.",
      "stars": 90,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T23:37:10Z",
      "readme_content": "# ⭐⭐ mcp-webcam 0.2.0 - the 50 Star Update ⭐⭐ \n\nIn celebration of getting 52 GitHub stars, `mcp-webcam 0.2.0` is here! Now supports streamable-http!! No installation required! - try it now at [`https://webcam.fast-agent.ai/`](https://webcam.fast-agent.ai/). You can specify your own UserID by adding `?user=<YOUR_USER_ID>` after the URL. Note this shared instance is for fun, not security - see below for instructions how to run your own copy locally.\n\nIn streamable-http mode multiple clients can connect simultaneously, and you can choose which is used for Sampling.\n\n![mcp_webcam_020_thumb](https://github.com/user-attachments/assets/041e3091-71e5-4aa1-9170-ee20177485ef)\n\nIf we get to 100 stars I'll add another feature 😊.\n\n## Multi-user Mode\n\nWhen run in Streaming mode, if you set an MCP_HOST environment variable the host name is used as a prefix in URL construction, and 5 character UserIDs are automatically generated when the User lands on the webpage. \n\n![image](https://github.com/user-attachments/assets/30d06cc2-59b6-485b-989d-7030b39c287d)\n\n\n## mcp-webcam\n\nMCP Server that provides access to your WebCam. Provides `capture` and `screenshot` tools to take an image from the Webcam, or take a screenshot. The current image is also available as a Resource.\n\n### MCP Sampling\n\n`mcp-webcam` supports \"sampling\"! Press the \"Sample\" button to send a sampling request to the Client along with your entered message. \n\n> [!TIP]\n> Claude Desktop does not currently support Sampling. If you want a Client that can handle multi-modal sampling request, try https://github.com/evalstate/fast-agent/ or VSCode (more details below).\n\n## Installation and Running\n\n### NPX\n\nInstall a recent version of [NodeJS](https://nodejs.org/en/download) for your platform. The NPM package is `@llmindset/mcp-webcam`. \n\nTo start in **STDIO** mode: `npx @llmindset/mcp-webcam`. This starts the `mcp-webcam` UI on port 3333. Point your browser at `http://localhost:3333` to get started.\n\nTo change the port: `npx @llmindset/mcp-webcam 9999`. This starts `mcp-webcam` the UI on port 9999.\n\nFor **Streaming HTTP** mode: `npx @llmindset/mcp-webcam --streaming`. This will make the UI available at `http://localhost:3333` and the MCP Server available at `http://localhost:3333/mcp`.\n\n### Docker\n\nYou can run `mcp-webcam` using Docker. By default, it starts in **streaming mode**:\n\n```bash\ndocker run -p 3333:3333 ghcr.io/evalstate/mcp-webcam:latest\n```\n\n#### Environment Variables\n\n- `MCP_TRANSPORT_MODE` - Set to `stdio` for STDIO mode, defaults to `streaming`\n- `PORT` - The port to run on (default: `3333`)\n- `BIND_HOST` - Network interface to bind the server to (default: `localhost`)\n- `MCP_HOST` - Public-facing URL for user instructions and MCP client connections (default: `http://localhost:3333`)\n\n#### Examples\n\n```bash\n# STDIO mode\ndocker run -p 3333:3333 -e MCP_TRANSPORT_MODE=stdio ghcr.io/evalstate/mcp-webcam:latest\n\n# Custom port\ndocker run -p 8080:8080 -e PORT=8080 ghcr.io/evalstate/mcp-webcam:latest\n\n# For cloud deployments with custom domain (e.g., Hugging Face Spaces)\ndocker run -p 3333:3333 -e MCP_HOST=https://evalstate-mcp-webcam.hf.space ghcr.io/evalstate/mcp-webcam:latest\n\n# Complete cloud deployment example\ndocker run -p 3333:3333 -e MCP_HOST=https://your-domain.com ghcr.io/evalstate/mcp-webcam:latest\n```\n\n## Clients\n\nIf you want a Client that supports sampling try:\n\n### fast-agent\n\nStart the `mcp-webcam` in streaming mode, install [`uv`](https://docs.astral.sh/uv/) and connect with:\n\n`uvx fast-agent-mcp go --url http://localhost:3333/mcp`\n\n`fast-agent` currently uses Haiku as its default model, so set an `ANTHROPIC_API_KEY`. If you want to use a different model, you can add `--model` on the command line. More instructions for installation and configuration are available here: https://fast-agent.ai/models/.\n\nTo start the server in STDIO mode, add the following to your `fastagent.config.yaml`\n\n```yaml\nwebcam_local:\n   command: \"npx\"\n   args: [\"@llmindset/mcp-webcam\"]\n```\n\n### VSCode\n\nVSCode versions 1.101.0 and above support MCP Sampling. Simply start `mcp-webcam` in streaming mode, and add `http://localhost:3333/mcp` as an MCP Server to get started.\n\n### Claude Desktop\n\nClaude Desktop does **NOT** support Sampling. To run `mcp-webcam` from Claude Desktop, add the following to the `mcpServers` section of your `claude_desktop_config.json` file:\n\n```json\n    \"webcam\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-webcam\"\n      ]\n    }\n```\n\nStart Claude Desktop, and connect to `http://localhost:3333`. You can then ask Claude to `get the latest picture from my webcam`, or `Claude, take a look at what I'm holding` or `what colour top am i wearing?`. You can \"freeze\" the current image and that will be returned to Claude rather than a live capture. \n\nYou can ask for Screenshots - navigate to the browser so that you can guide the capture area when the request comes in. Screenshots are automatically resized to be manageable for Claude (useful if you have a 4K Screen). The button is there to allow testing of your platform specific Screenshot UX - it doesn't do anything other than prepare you for a Claude intiated request. NB this does not **not** work on Safari as it requires human initiation.\n\n## Other notes\n\nThat's it really. \n\nThis MCP Server was built to demonstrate exposing a User Interface on an MCP Server, and serving live resources back to Claude Desktop.\n\nThis project might prove useful if you want to build a local, interactive MCP Server.\n\nThanks to  https://github.com/tadasant for help with testing and setup. \n\nPlease read the article at [https://llmindset.co.uk/posts/2025/01/resouce-handling-mcp](https://llmindset.co.uk/posts/2025/01/mcp-files-resources-part1/) for more details about handling files and resources in LLM / MCP Chat Applications, and why you might want to do this.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcam",
        "mcp",
        "capturing",
        "mcp webcam",
        "webcam mcp",
        "images webcam"
      ],
      "category": "image-and-video-generation"
    },
    "falahgs--MCP-Storybook-Image-Generator": {
      "owner": "falahgs",
      "name": "MCP-Storybook-Image-Generator",
      "url": "https://github.com/falahgs/MCP-Storybook-Image-Generator",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Generates high-quality storybook images and matching children's stories using Google's Gemini AI, offering multiple art styles such as 3D cartoon, watercolor, and pixel art. It allows instant previewing of creations and saves them locally in an organized manner.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-12T03:16:03Z",
      "readme_content": "# MCP Storybook Image Generator\n\nA professional-grade server that generates beautiful storybook images with matching children's stories using Google's Gemini AI.\n\n## 🎬 Demo\n\n![Storybook Generator Demo](video/1.gif)\n\n## 🌟 Features\n\n- **Storybook Image Generation**: Creates high-quality images in various art styles for children's stories\n- **Automatic Story Creation**: Generates engaging children's stories to match the images\n- **Multiple Art Styles**: Choose from 3D cartoon, watercolor, pixel art, hand drawn, or claymation styles\n- **Instant Preview**: Automatically opens generated images and stories in your browser\n- **Local Storage**: Saves images and stories in an organized output directory\n\n## 🛠️ Technical Stack\n\n- **Core Framework**: Model Context Protocol (MCP) SDK\n- **AI Integration**: Google Generative AI (Gemini)\n- **Runtime**: Node.js v14+\n- **Language**: TypeScript\n- **Package Manager**: npm\n\n## 📋 Prerequisites\n\n- Node.js (v14 or higher)\n- Google Gemini API key\n- TypeScript\n\n## ⚙️ Installation\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n3. Configure environment:\nCreate a `.env` file in the root directory:\n```env\nGEMINI_API_KEY=your_api_key_here\n```\n\n4. Build the project:\n```bash\nnpm run build\n```\n\n## 🚀 Using the CLI\n\nYou can use the storybook generator directly from the command line:\n\n```bash\n# Using npx (after publishing to npm)\nnpx mcp-storybook-image-generator --api-key your_api_key_here --save-to-desktop\n\n# Or run locally\nnode build/cli.js --api-key your_api_key_here --save-to-desktop\n```\n\n### Command Line Options\n\n| Option | Description |\n|--------|-------------|\n| `--api-key <key>` | Set your Gemini API key |\n| `--save-to-desktop` | Save generated files to desktop |\n| `--debug` | Enable debug logging |\n| `--help` | Show help information |\n\n## 🔧 Configuring Claude Desktop with MCP Server\n\nTo integrate this server with Claude Desktop:\n\n1. Locate the Claude Desktop Configuration File:\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"storybook-generator\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-storybook-image-generator@latest\",\n        \"--api-key\",\n        \"your_gemini_api_key_here\"\n      ],\n      \"env\": {\n        \"SAVE_TO_DESKTOP\": \"true\",\n        \"DEBUG\": \"false\"\n      }\n    }\n  }\n}\n```\n\n## 🚀 Available Tool\n\n### Storybook Image Generator Tool\n\n```json\n{\n  \"name\": \"generate_storybook_image\",\n  \"description\": \"Generates a 3D style cartoon image with a children's story based on the given prompt\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"prompt\": {\n        \"type\": \"string\",\n        \"description\": \"The prompt describing the storybook scene to generate\"\n      },\n      \"fileName\": {\n        \"type\": \"string\",\n        \"description\": \"Base name for the output files (without extension)\"\n      },\n      \"artStyle\": {\n        \"type\": \"string\",\n        \"description\": \"The art style for the image (default: '3d cartoon')\",\n        \"enum\": [\"3d cartoon\", \"watercolor\", \"pixel art\", \"hand drawn\", \"claymation\"]\n      }\n    },\n    \"required\": [\"prompt\", \"fileName\"]\n  }\n}\n```\n\n## 📄 Example Usage\n\n### Storybook Generation Examples\n\n```javascript\n// Generate a storybook with a 3D cartoon style\n{\n  \"name\": \"generate_storybook_image\",\n  \"arguments\": {\n    \"prompt\": \"A friendly dragon teaching kids how to fly\",\n    \"fileName\": \"dragon_flight_lesson\",\n    \"artStyle\": \"3d cartoon\"\n  }\n}\n\n// Generate a storybook with a watercolor style\n{\n  \"name\": \"generate_storybook_image\",\n  \"arguments\": {\n    \"prompt\": \"A rabbit and turtle having a tea party in the forest\",\n    \"fileName\": \"forest_tea_party\",\n    \"artStyle\": \"watercolor\"\n  }\n}\n\n// Generate a storybook with pixel art style\n{\n  \"name\": \"generate_storybook_image\",\n  \"arguments\": {\n    \"prompt\": \"A space adventure with a kid astronaut meeting friendly aliens\",\n    \"fileName\": \"space_adventure\",\n    \"artStyle\": \"pixel art\"\n  }\n}\n```\n\n## ⚙️ Configuration Options\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `GEMINI_API_KEY` | Google Gemini API key for AI generation | (Required) |\n| `SAVE_TO_DESKTOP` | Force saving to desktop directory | false |\n| `DEBUG` | Enable verbose debug logging | false |\n\n## 📝 Output Files\n\nFor each storybook generation request, the server produces:\n\n1. **PNG Image**: The generated illustration matching your prompt in the requested art style\n2. **Text File**: The matching children's story in plain text format\n3. **HTML Preview**: A combined view showing both the image and story together\n\nThese files are saved to either:\n- Your desktop in a folder called \"storybook-images\" (if `SAVE_TO_DESKTOP=true`)\n- The server's directory in a folder called \"storybook-images\"\n\n## 🤝 Contributing\n\nContributions, issues, and feature requests are welcome! Feel free to check issues page.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "storybook",
        "images",
        "image",
        "storybook images",
        "storybook image",
        "image generator"
      ],
      "category": "image-and-video-generation"
    },
    "falahgs--flux-imagegen-mcp-server": {
      "owner": "falahgs",
      "name": "flux-imagegen-mcp-server",
      "url": "https://github.com/falahgs/flux-imagegen-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Generates and manipulates images using advanced AI models, offering functionalities such as image URL generation, direct image creation from text prompts, and management of multiple image generation models.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-18T19:58:34Z",
      "readme_content": "# Flux ImageGen MCP Server\r\n\r\nA specialized Model Context Protocol (MCP) server for image generation and manipulation, powered by Pollinations AI.\r\n\r\n## Developer\r\n- **Author**: Falah.G.Salieh\r\n- **Copyright**: © 2025 All rights reserved\r\n\r\n## Overview\r\n\r\nImageGen MCP Server is a streamlined server implementation that provides powerful image generation capabilities through the Model Context Protocol (MCP). This server specializes in three core functionalities:\r\n\r\n1. Image URL Generation\r\n2. Direct Image Generation\r\n3. Model Listing and Management\r\n\r\n## Features\r\n\r\n- 🖼️ **Image Generation**: Create stunning images from text prompts\r\n- 🎨 **Multiple Models**: Support for various image generation models\r\n- 🔧 **Flexible Configuration**: Easy to set up and customize\r\n- 🚀 **High Performance**: Optimized for quick response times\r\n- 🔄 **MCP Compatible**: Fully compliant with Model Context Protocol\r\n\r\n## Installation\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/yourusername/flux-imagegen-mcp-server.git\r\n\r\n# Install dependencies\r\nnpm install\r\n```\r\n\r\n## Configuration\r\n\r\n### Claude Desktop Configuration\r\n\r\nTo use this server with Claude Desktop, update your configuration file at:\r\n`C:\\Users\\[YourUsername]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"mcpollinations\": {\r\n      \"command\": \"cmd\",\r\n      \"args\": [\r\n        \"/c\",\r\n        \"node\",\r\n        \"PATH_TO_YOUR_SERVER\\\\server.js\"\r\n      ],\r\n      \"tools\": [\r\n        \"generateImageUrl\",\r\n        \"generateImage\",\r\n        \"listImageModels\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nReplace `PATH_TO_YOUR_SERVER` with your actual server path.\r\n\r\n## Available Tools\r\n\r\n### 1. Generate Image URL (`generateImageUrl`)\r\nGenerates a URL for an image based on a text prompt.\r\n\r\n```javascript\r\n{\r\n  \"prompt\": \"A beautiful sunset over mountains\",\r\n  \"model\": \"flux\",  // optional, defaults to 'flux'\r\n  \"width\": 1024,    // optional\r\n  \"height\": 1024,   // optional\r\n  \"enhance\": true,  // optional\r\n  \"safe\": false     // optional\r\n}\r\n```\r\n\r\n### 2. Generate Image (`generateImage`)\r\nGenerates and saves an image directly from a text prompt.\r\n\r\n```javascript\r\n{\r\n  \"prompt\": \"A serene lake reflecting mountains\",\r\n  \"model\": \"flux\",\r\n  \"width\": 1024,\r\n  \"height\": 1024,\r\n  \"enhance\": true,\r\n  \"safe\": false,\r\n  \"outputPath\": \"./output\",\r\n  \"fileName\": \"mountain_lake\",\r\n  \"format\": \"png\"\r\n}\r\n```\r\n\r\n### 3. List Image Models (`listImageModels`)\r\nReturns a list of available image generation models.\r\n\r\n```javascript\r\n// Example response:\r\n{\r\n  \"models\": [\r\n    {\r\n      \"id\": \"flux\",\r\n      \"name\": \"Flux\",\r\n      \"description\": \"Default image generation model\"\r\n    },\r\n    // ... other models\r\n  ]\r\n}\r\n```\r\n## Running the Server\r\n\r\n```bash\r\n# Start the server\r\nnode server.js\r\n```\r\n\r\n## Environment Requirements\r\n\r\n- Node.js >= 16.0.0\r\n- NPM >= 7.0.0\r\n- Windows/Linux/MacOS compatible\r\n\r\n## Development\r\n\r\nTo contribute or modify the server:\r\n\r\n1. Fork the repository\r\n2. Create your feature branch\r\n3. Make your changes\r\n4. Submit a pull request\r\n\r\n## Error Handling\r\n\r\nThe server provides detailed error messages for common issues:\r\n\r\n```javascript\r\n{\r\n  \"error\": {\r\n    \"code\": \"ERROR_CODE\",\r\n    \"message\": \"Human-readable error message\",\r\n    \"details\": { /* Additional error details */ }\r\n  }\r\n}\r\n```\r\n\r\n## Examples\r\n\r\n### Basic Image Generation\r\n```javascript\r\n// Generate an image URL\r\nconst response = await generateImageUrl({\r\n  prompt: \"A futuristic city at night\",\r\n  model: \"flux\",\r\n  width: 1024,\r\n  height: 1024\r\n});\r\n\r\n// Generate and save an image\r\nconst image = await generateImage({\r\n  prompt: \"A peaceful garden with butterflies\",\r\n  outputPath: \"./images\",\r\n  fileName: \"garden_scene\"\r\n});\r\n```\r\n\r\n### Download Image Example\r\n```javascript\r\n// Download an image from URL\r\nconst downloadResult = await downloadImage({\r\n  imageUrl: \"https://example.com/image.jpg\",\r\n  fileName: \"downloaded-image\",\r\n  format: \"png\"\r\n});\r\n```\r\n\r\n## Support\r\n\r\nFor issues and feature requests, please create an issue in the repository or contact the developer:\r\n- Email: [Your contact email]\r\n- GitHub: [Your GitHub profile]\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the LICENSE file for details.\r\n\r\n---\r\nMade with ❤️ by Falah.G.Salieh\r\n© 2025 All rights reserved\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "imagegen",
        "images",
        "ai",
        "image generation",
        "image creation",
        "imagegen mcp"
      ],
      "category": "image-and-video-generation"
    },
    "falahgs--imagen-3.0-generate-google-mcp-server": {
      "owner": "falahgs",
      "name": "imagen-3.0-generate-google-mcp-server",
      "url": "https://github.com/falahgs/imagen-3.0-generate-google-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Generates high-quality images using Google's Imagen 3.0 model via the Gemini API, manages image files with intelligent naming, and creates HTML previews for local viewing. Integrates seamlessly with MCP-compatible hosts for enhanced AI capabilities.",
      "stars": 3,
      "forks": 8,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-10T13:44:15Z",
      "readme_content": "# Gemini Imagen 3.0 MCP Server\r\n\r\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\r\n![Node](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)\r\n![TypeScript](https://img.shields.io/badge/typescript-%5E5.3.3-blue)\r\n\r\nA professional Model Context Protocol (MCP) server implementation that harnesses Google's Imagen 3.0 model through the Gemini API for high-quality image generation. Built with TypeScript and designed for seamless integration with Claude Desktop and other MCP-compatible hosts.\r\n\r\n## 🌟 Features\r\n\r\n- Leverage Google's state-of-the-art Imagen 3.0 model via Gemini API\r\n- Generate up to 4 high-quality images per request\r\n- Automatic file management with intelligent naming\r\n- HTML preview generation with file:// protocol support\r\n- Built on MCP protocol for AI agent compatibility\r\n- TypeScript implementation with robust error handling\r\n\r\n## 🚀 Quick Start\r\n\r\n### Prerequisites\r\n\r\n- Node.js 18 or higher\r\n- Google Gemini API key\r\n- Claude Desktop or another MCP-compatible host\r\n\r\n### Installation\r\n\r\n1. Clone the repository:\r\n```bash\r\ngit clone https://github.com/yourusername/gemini-imagen-mcp-server.git\r\ncd gemini-imagen-mcp-server\r\n```\r\n\r\n2. Install dependencies:\r\n```bash\r\nnpm install\r\n```\r\n\r\n3. Build the TypeScript code:\r\n```bash\r\nnpm run build\r\n```\r\n\r\n## ⚙️ Configuration\r\n\r\n1. Configure Claude Desktop by adding to `claude_desktop_config.json`:\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"gemini-image-gen\": {\r\n      \"command\": \"node\",\r\n      \"args\": [\"./build/index.js\"],\r\n      \"cwd\": \"<path-to-project-directory>\",\r\n      \"env\": {\r\n        \"GEMINI_API_KEY\": \"your-gemini-api-key\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n2. Replace placeholders:\r\n   - `<path-to-project-directory>`: Your project path\r\n   - `your-gemini-api-key`: Your Gemini API key\r\n\r\n## 🛠️ Available Tools\r\n\r\n### 1. generate_images\r\nGenerates images using Google's Imagen 3.0 model.\r\n\r\nParameters:\r\n- `prompt` (required): Text description of the image to generate\r\n- `numberOfImages` (optional): Number of images (1-4, default: 1)\r\n\r\nFile Management:\r\n- Images are automatically saved in `G:\\image-gen3-google-mcp-server\\images`\r\n- Filenames follow the pattern: `{sanitized-prompt}-{timestamp}-{index}.png`\r\n- Timestamps ensure unique filenames\r\n- Prompts are sanitized for safe filesystem usage\r\n\r\nExample:\r\n```\r\nGenerate an image of a futuristic city at night\r\n```\r\n\r\n### 2. create_image_html\r\nCreates HTML preview tags for generated images.\r\n\r\nParameters:\r\n- `imagePaths` (required): Array of image file paths\r\n- `width` (optional): Image width in pixels (default: 512)\r\n- `height` (optional): Image height in pixels (default: 512)\r\n\r\nReturns HTML tags with absolute file:// URLs for local viewing.\r\n\r\nExample:\r\n```\r\nCreate HTML tags for the generated images with width=400\r\n```\r\n\r\n## 🔧 Development\r\n\r\n```bash\r\n# Install dependencies\r\nnpm install\r\n\r\n# Build TypeScript\r\nnpm run build\r\n\r\n# Run tests (when available)\r\nnpm test\r\n```\r\n\r\n## 🤝 Contributing\r\n\r\nContributions are welcome! Please feel free to submit a Pull Request. For major changes:\r\n\r\n1. Fork the repository\r\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\r\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\r\n4. Push to the branch (`git push origin feature/AmazingFeature`)\r\n5. Open a Pull Request\r\n\r\n## 📝 Error Handling\r\n\r\nThe server implements two main error codes:\r\n- `tool_not_found` (1): When the requested tool is not available\r\n- `execution_error` (2): When image generation or HTML creation fails\r\n\r\n## 📄 License\r\n\r\nMIT License - see the [LICENSE](LICENSE) file for details.\r\n\r\n## ✨ Author\r\n\r\n**Falah G. Salieh**\r\n- Copyright © 2025\r\n- GitHub: [@yourgithubhandle](https://github.com/yourgithubhandle)\r\n- Email: [your.email@example.com](mailto:your.email@example.com)\r\n\r\n## 🙏 Acknowledgments\r\n\r\n- Google Gemini API and Imagen 3.0 model\r\n- Model Context Protocol (MCP) by Anthropic\r\n- Claude Desktop team for MCP host implementation\r\n\r\n## 📌 Tags\r\n\r\n`#MCP` `#Gemini` `#Imagen3` `#AI` `#ImageGeneration` `#TypeScript` `#NodeJS` `#GoogleAI` `#ClaudeDesktop`\r\n\r\n---\r\nMade with ❤️ by Falah G. Salieh ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "imagen",
        "images",
        "mcp",
        "imagen generate",
        "google imagen",
        "imagen model"
      ],
      "category": "image-and-video-generation"
    },
    "falahgs--mcp-3d-style-cartoon-gen-server": {
      "owner": "falahgs",
      "name": "mcp-3d-style-cartoon-gen-server",
      "url": "https://github.com/falahgs/mcp-3d-style-cartoon-gen-server",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Generates high-quality 3D-style cartoon images from text prompts using Google's Gemini AI, with child-friendly designs for engaging visuals. Offers secure file system operations for managing files, including reading and writing capabilities.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-11T21:40:56Z",
      "readme_content": "# MCP Combined Server: 3D Cartoon Generator & File System Tools\n\nA professional-grade server that provides two major capabilities: \n1. High-quality 3D-style cartoon image generation using Google's Gemini AI\n2. Secure file system operations for reading, writing, and managing files\n\n![3D Cartoon Generator Demo](./video/mcp-3d-style-server.gif)\n\n## 🌟 Features\n\n### Image Generation\n- **3D Cartoon Generation**: Creates high-quality 3D-style cartoon images\n- **Child-Friendly Design**: Focuses on colorful, playful, and engaging visuals\n- **Instant Preview**: Automatically opens generated images in your default browser\n- **Local Storage**: Saves images and previews in an organized output directory\n\n### File System Operations\n- **Secure File Access**: Path validation and security checks\n- **Read/Write Files**: Read and write text file contents\n- **Directory Operations**: List, create, and navigate directories\n- **File Search**: Find files matching patterns\n\n### System Features\n- **Professional Configuration**: Robust error handling and controlled logging\n- **Cross-Platform Support**: Intelligent file path handling for Windows, macOS, and Linux\n- **Smart OS Detection**: Automatically finds the best save location for each operating system\n- **Security Controls**: Restricted directory access through configuration\n\n## 🛠️ Technical Stack\n\n- **Core Framework**: Model Context Protocol (MCP) SDK\n- **AI Integration**: Google Generative AI (Gemini)\n- **Runtime**: Node.js v14+\n- **Language**: TypeScript\n- **Package Manager**: npm\n\n## 📋 Prerequisites\n\n- Node.js (v14 or higher)\n- Google Gemini API key\n- TypeScript\n\n## ⚙️ Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/falahgs/mcp-3d-style-cartoon-gen-server.git\ncd mcp-3d-style-cartoon-gen-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Configure environment:\nCreate a `.env` file in the root directory:\n```env\nGEMINI_API_KEY=your_api_key_here\nALLOWED_DIRECTORIES=/path/to/allowed/dir1,/path/to/allowed/dir2\n```\n\n4. Build the project:\n```bash\nnpm run build\n```\n\n## 🔧 Configuring Claude Desktop with MCP Server\n\nTo integrate this combined server with Claude Desktop:\n\n1. Locate the Configuration File:\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-3d-cartoon-generator\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"path/to/your/build/index.js\"\n      ],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_gemini_api_key_here\",\n        \"IS_REMOTE\": \"true\",\n        \"SAVE_TO_DESKTOP\": \"true\",\n        \"DETECT_OS_PATHS\": \"true\",\n        \"ALLOWED_DIRECTORIES\": \"C:\\\\Users\\\\YourUsername\\\\Desktop,C:\\\\Users\\\\YourUsername\\\\Documents\",\n        \"DEBUG\": \"false\"\n      }\n    }\n  }\n}\n```\n\n### Windows PowerShell Helper Script\n\nFor Windows users, you can use the included `fix_claude_config.ps1` script to automatically configure Claude Desktop:\n\n1. Edit the script to update the path to your server build and your Gemini API key\n2. Run the script in PowerShell:\n```powershell\npowershell -ExecutionPolicy Bypass -File .\\fix_claude_config.ps1\n```\n\nThis will create or update the configuration file with proper encoding and settings.\n\n## 🚀 Available Tools\n\n### 1. Image Generation Tool\n\n```json\n{\n  \"name\": \"generate_3d_cartoon\",\n  \"description\": \"Generates a 3D style cartoon image for kids based on the given prompt\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"prompt\": {\n        \"type\": \"string\",\n        \"description\": \"The prompt describing the 3D cartoon image to generate\"\n      },\n      \"fileName\": {\n        \"type\": \"string\",\n        \"description\": \"The name of the output file (without extension)\"\n      }\n    },\n    \"required\": [\"prompt\", \"fileName\"]\n  }\n}\n```\n\n### 2. File System Tools\n\n#### Read File\n```json\n{\n  \"name\": \"read_file\",\n  \"description\": \"Read the contents of a file\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\n        \"type\": \"string\",\n        \"description\": \"Path to the file to read\"\n      }\n    },\n    \"required\": [\"path\"]\n  }\n}\n```\n\n#### Write File\n```json\n{\n  \"name\": \"write_file\",\n  \"description\": \"Write content to a file\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\n        \"type\": \"string\",\n        \"description\": \"Path to the file to write\"\n      },\n      \"content\": {\n        \"type\": \"string\",\n        \"description\": \"Content to write to the file\"\n      }\n    },\n    \"required\": [\"path\", \"content\"]\n  }\n}\n```\n\n#### List Directory\n```json\n{\n  \"name\": \"list_directory\",\n  \"description\": \"List the contents of a directory\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\n        \"type\": \"string\",\n        \"description\": \"Path to the directory to list\"\n      }\n    },\n    \"required\": [\"path\"]\n  }\n}\n```\n\n#### Create Directory\n```json\n{\n  \"name\": \"create_directory\",\n  \"description\": \"Create a new directory\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\n        \"type\": \"string\",\n        \"description\": \"Path to the directory to create\"\n      }\n    },\n    \"required\": [\"path\"]\n  }\n}\n```\n\n#### Search Files\n```json\n{\n  \"name\": \"search_files\",\n  \"description\": \"Search for files matching a pattern\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\n        \"type\": \"string\",\n        \"description\": \"Base directory to search from\"\n      },\n      \"pattern\": {\n        \"type\": \"string\",\n        \"description\": \"Search pattern (glob format)\"\n      },\n      \"excludePatterns\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\"\n        },\n        \"description\": \"Patterns to exclude from search (glob format)\"\n      }\n    },\n    \"required\": [\"path\", \"pattern\"]\n  }\n}\n```\n\n## 📄 Example Usage\n\n### Image Generation Examples\n\n```javascript\n// Generate a 3D cartoon\n{\n  \"name\": \"generate_3d_cartoon\",\n  \"arguments\": {\n    \"prompt\": \"A friendly robot playing with a cat\",\n    \"fileName\": \"robot_cat_play\"\n  }\n}\n```\n\n### File System Examples\n\n```javascript\n// Read a file\n{\n  \"name\": \"read_file\",\n  \"arguments\": {\n    \"path\": \"C:/Users/YourUsername/Documents/example.txt\"\n  }\n}\n\n// Write a file\n{\n  \"name\": \"write_file\",\n  \"arguments\": {\n    \"path\": \"C:/Users/YourUsername/Documents/new-file.txt\",\n    \"content\": \"This is the content of the file.\"\n  }\n}\n\n// List directory contents\n{\n  \"name\": \"list_directory\",\n  \"arguments\": {\n    \"path\": \"C:/Users/YourUsername/Documents\"\n  }\n}\n\n// Create a directory\n{\n  \"name\": \"create_directory\",\n  \"arguments\": {\n    \"path\": \"C:/Users/YourUsername/Documents/new-folder\"\n  }\n}\n\n// Search for files\n{\n  \"name\": \"search_files\",\n  \"arguments\": {\n    \"path\": \"C:/Users/YourUsername/Documents\",\n    \"pattern\": \"*.txt\",\n    \"excludePatterns\": [\"temp*\", \"*.tmp\"]\n  }\n}\n```\n\n## 🔒 Security Features\n\nThe server implements several security measures:\n\n1. **Path Validation**: All file paths are validated to ensure they are within allowed directories.\n2. **Allowed Directories**: Only directories explicitly set in the `ALLOWED_DIRECTORIES` environment variable can be accessed.\n3. **Symlink Protection**: Prevents access to directories outside the allowed scope via symlinks.\n4. **Controlled Logging**: Debug logs are disabled by default to prevent information leakage.\n\n## ⚙️ Configuration Options\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `GEMINI_API_KEY` | Google Gemini API key for image generation | (Required) |\n| `ALLOWED_DIRECTORIES` | Comma-separated list of allowed file system paths | User's home dir, current dir |\n| `IS_REMOTE` | Run in remote mode without browser opening | false |\n| `SAVE_TO_DESKTOP` | Force saving to desktop directory | false |\n| `DETECT_OS_PATHS` | Enable OS-specific path detection | true |\n| `DEBUG` | Enable verbose debug logging | false |\n\n## 🛠️ Troubleshooting\n\n### Common Issues:\n\n1. **JSON Parsing Errors in Claude**:\n   - Ensure `DEBUG` is set to \"false\" to prevent logs from interfering with JSON communication\n   - Check for proper JSON formatting in the Claude configuration\n\n2. **File Access Denied**:\n   - Verify that the paths you're trying to access are included in `ALLOWED_DIRECTORIES`\n   - Check file permissions on the target files/directories\n\n3. **Images Not Saving**:\n   - Set `SAVE_TO_DESKTOP` to \"true\" to ensure images save to the desktop\n   - Check desktop path detection in the server logs (enable DEBUG temporarily)\n\n## 📄 License\n\n[MIT License](LICENSE)\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "images",
        "cartoon",
        "mcp 3d",
        "cartoon images",
        "cartoon gen"
      ],
      "category": "image-and-video-generation"
    },
    "felores--placid-mcp-server": {
      "owner": "felores",
      "name": "placid-mcp-server",
      "url": "https://github.com/felores/placid-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/felores.webp",
      "description": "Integrates with Placid.app to list available templates and generate images and videos using dynamic content. Provides secure API token management and robust error handling.",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-02T13:59:52Z",
      "readme_content": "# Placid.app MCP Server\n[![smithery badge](https://smithery.ai/badge/@felores/placid-mcp-server)](https://smithery.ai/server/@felores/placid-mcp-server)\n\nAn MCP server implementation for integrating with Placid.app's API. This server provides tools for listing templates and generating images and videos through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/xeklsydon0\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xeklsydon0/badge\" />\n</a>\n\n## Features\n\n- List available Placid templates with filtering options\n- Generate images and videos using templates and dynamic content\n- Secure API token management\n- Error handling and validation\n- Type-safe implementation\n\n## Requirements: Node.js\n\n1. Install Node.js (version 18 or higher) and npm from [nodejs.org](https://nodejs.org/)\n2. Verify installation:\n   ```bash\n   node --version\n   npm --version\n   ```\n\n## Installation\n\n### Quick Start (Recommended)\n\nThe easiest way to get started is using Smithery, which will automatically configure everything for you:\n\n```bash\nnpx -y @smithery/cli install @felores/placid-mcp-server --client claude\n```\n\n### Manual Configuration\n\nIf you prefer to configure manually, add this to your Claude Desktop or Cline settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"placid\": {\n      \"command\": \"npx\",\n      \"args\": [\"@felores/placid-mcp-server\"],\n      \"env\": {\n        \"PLACID_API_TOKEN\": \"your-api-token\"\n      }\n    }\n  }\n}\n```\n\n## Getting Your Placid API Token\n\n1. Log in to your [Placid.app](https://placid.app/) account\n2. Go to Settings > API\n3. Click on \"Create API Token\"\n4. Give your token a name (e.g., \"MCP Server\")\n5. Copy the generated token\n6. Add the token to your configuration as shown above\n\n## Development\n\n```bash\n# Run in development mode with hot reload\nnpm run dev\n\n# Run tests\nnpm test\n```\n\n## Tools\n\n### placid_list_templates\nLists available Placid templates with filtering options. Each template includes its title, ID, preview image URL, available layers, and tags.\n\n#### Parameters\n- `collection_id` (optional): Filter templates by collection ID\n- `custom_data` (optional): Filter by custom reference data\n- `tags` (optional): Array of tags to filter templates by\n\n#### Response\nReturns an array of templates, each containing:\n- `uuid`: Unique identifier for the template\n- `title`: Template name\n- `thumbnail`: Preview image URL (if available)\n- `layers`: Array of available layers with their names and types\n- `tags`: Array of template tags\n\n### placid_generate_video\nGenerate videos by combining Placid templates with dynamic content like videos, images, and text. For longer videos (>60 seconds processing time), you'll receive a job ID to check status in your Placid dashboard.\n\n#### Parameters\n- `template_id` (required): UUID of the template to use\n- `layers` (required): Object containing dynamic content for template layers\n  - For video layers: `{ \"layerName\": { \"video\": \"https://video-url.com\" } }`\n  - For image layers: `{ \"layerName\": { \"image\": \"https://image-url.com\" } }`\n  - For text layers: `{ \"layerName\": { \"text\": \"Your content\" } }`\n- `audio` (optional): URL to an mp3 audio file\n- `audio_duration` (optional): Set to 'auto' to trim audio to video length\n- `audio_trim_start` (optional): Timestamp of trim start point (e.g. '00:00:45' or '00:00:45.25')\n- `audio_trim_end` (optional): Timestamp of trim end point (e.g. '00:00:55' or '00:00:55.25')\n\n#### Response\nReturns an object containing:\n- `status`: Current status (\"finished\", \"queued\", or \"error\")\n- `video_url`: URL to download the generated video (when status is \"finished\")\n- `job_id`: ID for checking status in Placid dashboard (for longer videos)\n\n#### Example Usage for LLM models\n```json\n{\n  \"template_id\": \"template-uuid\",\n  \"layers\": {\n    \"MEDIA\": { \"video\": \"https://example.com/video.mp4\" },\n    \"PHOTO\": { \"image\": \"https://example.com/photo.jpg\" },\n    \"LOGO\": { \"image\": \"https://example.com/logo.png\" },\n    \"HEADLINE\": { \"text\": \"My Video Title\" }\n  },\n  \"audio\": \"https://example.com/background.mp3\",\n  \"audio_duration\": \"auto\"\n}\n```\n\n### placid_generate_image\nGenerate static images by combining Placid templates with dynamic content like text and images.\n\n#### Parameters\n- `template_id` (required): UUID of the template to use\n- `layers` (required): Object containing dynamic content for template layers\n  - For text layers: `{ \"layerName\": { \"text\": \"Your content\" } }`\n  - For image layers: `{ \"layerName\": { \"image\": \"https://image-url.com\" } }`\n\n#### Response\nReturns an object containing:\n- `status`: \"finished\" when complete\n- `image_url`: URL to download the generated image\n\n#### Example Usage for LLM models\n```json\n{\n  \"template_id\": \"template-uuid\",\n  \"layers\": {\n    \"headline\": { \"text\": \"Welcome to My App\" },\n    \"background\": { \"image\": \"https://example.com/bg.jpg\" }\n  }\n}\n```\n\n## Documentation\n\nFor more detailed information about the Placid API, visit the [Placid API Documentation](https://placid.app/docs/api/).\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "placid",
        "mcp",
        "server",
        "placid app",
        "placid mcp",
        "mcp server"
      ],
      "category": "image-and-video-generation"
    },
    "fengin--image-gen-server": {
      "owner": "fengin",
      "name": "image-gen-server",
      "url": "https://github.com/fengin/image-gen-server",
      "imageUrl": "/freedevtools/mcp/pfp/fengin.webp",
      "description": "Generate images from text descriptions and save them, seamlessly integrated with Cursor IDE. Users can create multiple image outputs simultaneously and specify custom save paths.",
      "stars": 205,
      "forks": 25,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T14:14:49Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/fengin-image-gen-server-badge.png)](https://mseep.ai/app/fengin-image-gen-server)\n\n# Image-Gen-Server\n\n<div align=\"center\">\n  <img src=\"images/logo_0.png\" alt=\"Image-Gen-Server Logo\" width=\"100%\">\n</div>\n\n[![smithery badge](https://smithery.ai/badge/@fengin/image-gen-server)](https://smithery.ai/server/@fengin/image-gen-server)\n\n基于即梦AI的图像生成服务，专门设计用于与Cursor IDE集成。它接收来自Cursor的文本描述，生成相应的图像，并提供图片下载和保存功能。\n\n此插件的开发过程可以看我的网站：[开发一个MCP Server与Cursor集成，给Cursor插上翅膀！](https://aibook.ren/archives/mcp-server-for-cursor)\n\n更多AI知识，见AI全书(https://aibook.ren)\n\n<div align=\"center\">\n  <img src=\"images/example.png\" alt=\"Image-Gen-Server Logo\" width=\"100%\">\n</div>\n\n## 特性\n\n- 与Cursor IDE完美集成\n- 支持文本到图像的生成\n- 自动保存生成的图像\n- 支持自定义保存路径\n- 一次生成四张图，供更多选择\n\n## 安装\n\n### Installing via Smithery\n\nTo install Image-Gen-Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@fengin/image-gen-server):\n\n```bash\nnpx -y @smithery/cli install @fengin/image-gen-server --client claude\n```\n\n1. 环境准备，MCP比较新的东西，依赖环境版本都比较新\n- python 3.10+\n\n- 安装npm\n\n- 安装nodejs（实测v15 v16都不行，开发环境验证v20可以，其他未验证）\n\n- 安装 pip install uv\n\n- 如果要调试，还需要安装这个：npm install -g @modelcontextprotocol/inspector@0.4.0\n2. 克隆项目\n   \n   ```bash\n   git clone https://github.com/fengin/image-gen-server.git\n   cd image-gen-server\n   ```\n\n3. 安装依赖\n   \n   ```bash\n   pip install -r requirements.txt\n   pip install uv\n   ```\n\n4. 设置即梦Token和图片默认保存地址\n   修改server.py文件里面这两个配置\n   \n   ```bash\n   # API配置\n   JIMENG_API_TOKEN = \"057f7addf85dxxxxxxxxxxxxx\" # 你登录即梦获得的session_id，支持多个，在后面用逗号分隔   \n   IMG_SAVA_FOLDER = \"D:/code/image-gen-server/images\" # 图片默认保存路径\n   ```\n\n    \n\n## Cursor集成\n\n<div align=\"center\">\n  <img src=\"images/cursor_config.png\" alt=\"Image-Gen-Server Logo\" width=\"100%\">\n</div>\n\n1. 打开Cursor设置\n   \n   - 点击左下角的设置图标\n   - 选择 Features > MCP Servers\n   - 点击 \"Add new MCP server\"\n\n2. 填写服务器配置\n   \n   - Name: `image-gen-server`（或其他你喜欢的名称）\n   \n   - Type: `command`\n   \n   - Command: \n     \n     ```bash\n     uv run --with fastmcp fastmcp run D:\\code\\image-gen-service\\server.py\n     ```\n     \n     注意：将路径替换为你的实际项目路径\n     \n     - Windows示例: ` uv run --with fastmcp fastmcp run D:/code/image-gen-service/server.py`\n     - macOS/Linux示例: ` uv run --with fastmcp fastmcp run /Users/username/code/image-gen-server/server.py`\n     \n     windows路径问题比较多，D:/code/image-gen-server/server.py 各种斜杠都试下\n     \n     填写完后，会弹出一个黑窗口，然后你就可以叫Cursor给你生成需要的图片了，目前黑窗口会一直运行，目前还没办法解决弹出这个的问题\n\n## 使用方法\n\n在Cursor中，你要让cursor生成图片，在agent模式下，你提示它了解下图片工具使用方法，然后直接提你要生成的图片要求，保存位置就行了\n\n## 获取即梦Token\n\n1. 访问 [即梦](https://jimeng.jianying.com/)\n2. 登录账号\n3. 按F12打开开发者工具\n4. 在Application > Cookies中找到`sessionid`\n5. 将找到的sessionid设置到server.py的JIMENG_API_TOKEN中\n\n## 工具函数说明\n\n### generate_image\n\n```python\nasync def generate_image(prompt: str, file_name: str, save_folder: str = None, sample_strength: float = 0.5, width: int = 1024, height: int = 1024) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    \"\"\"根据文本描述生成图片\n\n    Args:\n        prompt: 图片的文本prompt描述\n        file_name: 生成图片的文件名(不含路径，如果没有后缀则默认使用.jpg)\n        save_folder: 图片保存绝对地址目录(可选,默认使用IMG_SAVA_FOLDER)\n        sample_strength: 生成图片的精细度(可选,范围0-1,默认0.5)\n        width: 生成图片的宽度(可选,默认1024)\n        height: 生成图片的高度(可选,默认1024)\n\n    Returns:\n        List: 包含生成结果的JSON字符串\n    \"\"\"\n```\n\n### 技术实现\n\n1. server.py采用了fastmcp实现了mcp sever的能力，提供给cursor/claude使用\n\n   2.sever.py调用了proxy.jimeng模块逆向与即梦AI进行交互。\nproxy.jimeng逆向模块也可以单独install使用，主要提供了以下主要功能：\n\n- 图像生成（generate_images）\n- 同步对话补全（create_completion）\n- 流式对话补全（create_completion_stream）\n- 多账号token支持\n- 完整的错误处理\n\n更多详细信息请参考`proxy/jimeng/README.md`。\n\n### 使用示例\n\n```cmd\n# cursor agent模式下\n#例子一\n根据提供过你的项目需求，帮我生成一张产品logo，放在项目目录images下面\n\n#例子二\n根据项目需求，帮我制作网站的首页，头部需要有banner图片。\n```\n\n## 许可证\n\nMIT License \n作者：凌封\n\n## 故障排除\n\n1.配置完后跳出黑窗口，很快消失，工具状态变成No tools found\n\n  原因：没有正常启动，一般有以下原因\n\n- 配置命令不对，检查命令是否正确，一般是server.py路径不对，或者路径中包含中文，或者正反斜杠不对\n- 依赖的环境没准备好\n- 依赖运行的终端不对，像我windows的，终端有git bash，cmd，powershell，wsl等，这些终端都试下，cursor配置我这默认终端是cmd，如果你在这对应终端运行报错，一般是环境没装好，安装环境就可以\n\n2.正常运行后，想看调用日志，或者调试怎么弄\n\n  命令改成以下：\n\n```\nuv run --with fastmcp fastmcp dev D:/code/image-gen-service/server.py\n```\n\n\n  即把最后一个run 改成 dev。\n\n  或者找个终端运行以下命令进入调试模式：\n\n```\nfastmcp dev D:/code/image-gen-service/server.py\n```\n\n会有一个调试地址输出：http://localhost:5173/，你可以浏览器打开这地址MCP Inspector进行调试，具体MCP Inspector怎么使用，可以看官方文档\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fengin",
        "generate",
        "images",
        "generate images",
        "image gen",
        "fengin image"
      ],
      "category": "image-and-video-generation"
    },
    "hamflx--imagen3-mcp": {
      "owner": "hamflx",
      "name": "imagen3-mcp",
      "url": "https://github.com/hamflx/imagen3-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hamflx.webp",
      "description": "Generate high-quality images using Google's Imagen 3.0 model through an MCP interface, facilitating integration with tools like Cherry Studio or Cursor. Supports configurable deployment options using a Google Gemini API key.",
      "stars": 43,
      "forks": 7,
      "license": "No License",
      "language": "Rust",
      "updated_at": "2025-09-20T19:36:59Z",
      "readme_content": "# Imagen3-MCP\n\n[English Version](#imagen3-mcp-english)\n\n基于 Google 的 Imagen 3.0 的图像生成工具，通过 MCP（Model Control Protocol）提供服务。\n\n## 效果\n\n画一只奔跑的杰克罗素犬，长焦镜头，阳光透过狗狗的毛发，照片级画质\n\n![奔跑的杰克罗素犬](./docs/Snipaste_2025-04-26_15-18-15.png)\n\n画一个科技感十足的苹果\n\n![科技感十足的苹果](./docs/Snipaste_2025-04-26_15-18-02.png)\n\n## 安装要求\n\n- 有效的 [Google Gemini API 密钥](https://aistudio.google.com/apikey)\n\n## 安装步骤——Cherry Studio\n\n1. 从 [GitHub Releases](https://github.com/hamflx/imagen3-mcp/releases) 下载最新版本的可执行文件\n2. 将下载的可执行文件放置在系统中的任意位置，例如 `C:\\bin\\imagen3-mcp.exe`\n3. 在 Cherry Studio 中配置：\n   - Command 字段填写可执行文件路径，例如 `C:\\bin\\imagen3-mcp.exe`\n   - 环境变量 `GEMINI_API_KEY` 中填写你的 Gemini API 密钥\n   - [可选] 环境变量 `BASE_URL` 中填写代理地址，例如 `https://lingxi-proxy.hamflx.dev/api/provider/google`（这个地址可以解决 GFW 的问题，但是解决不了 Google 对 IP 的限制问题，因此还是得挂梯子）。\n   - [可选] 环境变量 `SERVER_LISTEN_ADDR`：设置服务器监听的 IP 地址（默认为 `127.0.0.1`）。\n   - [可选] 环境变量 `SERVER_PORT`：设置服务器监听的端口和图片 URL 使用的端口（默认为 `9981`）。\n   - [可选] 环境变量 `IMAGE_RESOURCE_SERVER_ADDR`：设置图片 URL 中使用的服务器地址（默认为 `127.0.0.1`）。这在服务器运行在容器或远程机器上时很有用。\n\n![配置](./docs/config.png)\n\n## 安装步骤——Cursor\n\n```json\n{\n  \"mcpServers\": {\n    \"imagen3\": {\n      \"command\": \"C:\\\\bin\\\\imagen3-mcp.exe\",\n      \"env\": {\n        \"GEMINI_API_KEY\": \"<GEMINI_API_KEY>\"\n        // Optional environment variables:\n        // \"BASE_URL\": \"<PROXY_URL>\",\n        // \"SERVER_LISTEN_ADDR\": \"0.0.0.0\", // Example: Listen on all interfaces\n        // \"SERVER_PORT\": \"9981\",\n        // \"IMAGE_RESOURCE_SERVER_ADDR\": \"your.domain.com\" // Example: Use a domain name for image URLs\n      }\n    }\n  }\n}\n```\n\n## 许可证\n\nMIT\n\n---\n\n# Imagen3-MCP (English)\n\nAn image generation tool based on Google's Imagen 3.0, providing services through MCP (Model Control Protocol).\n\n## Examples\n\nA running Jack Russell Terrier, telephoto lens, sunlight filtering through the dog's fur, photorealistic quality\n\n![Running Jack Russell Terrier](./docs/Snipaste_2025-04-26_15-18-15.png)\n\nA high-tech apple\n\n![High-tech apple](./docs/Snipaste_2025-04-26_15-18-02.png)\n\n## Requirements\n\n- Valid [Google Gemini API key](https://aistudio.google.com/apikey)\n\n## Installation Steps—Cherry Studio\n\n1. Download the latest executable from [GitHub Releases](https://github.com/hamflx/imagen3-mcp/releases)\n2. Place the downloaded executable anywhere in your system, e.g., `C:\\bin\\imagen3-mcp.exe`\n3. Configure in Cherry Studio:\n   - Fill in the Command field with the executable path, e.g., `C:\\bin\\imagen3-mcp.exe`\n   - Enter your Gemini API key in the `GEMINI_API_KEY` environment variable\n   - [Optional] Enter a proxy URL in the `BASE_URL` environment variable, e.g., `https://your-proxy.com`.\n   - [Optional] Set the `SERVER_LISTEN_ADDR` environment variable: The IP address the server listens on (defaults to `127.0.0.1`).\n   - [Optional] Set the `SERVER_PORT` environment variable: The port the server listens on and uses for image URLs (defaults to `9981`).\n   - [Optional] Set the `IMAGE_RESOURCE_SERVER_ADDR` environment variable: The server address used in the image URLs (defaults to `127.0.0.1`). Useful if the server runs in a container or remote machine.\n\n![Configuration](./docs/config.png)\n\n## Installation Steps—Cursor\n\n```json\n{\n  \"mcpServers\": {\n    \"imagen3\": {\n      \"command\": \"C:\\\\bin\\\\imagen3-mcp.exe\",\n      \"env\": {\n        \"GEMINI_API_KEY\": \"<GEMINI_API_KEY>\"\n        // Optional environment variables:\n        // \"BASE_URL\": \"<PROXY_URL>\",\n        // \"SERVER_LISTEN_ADDR\": \"0.0.0.0\", // Example: Listen on all interfaces\n        // \"SERVER_PORT\": \"9981\",\n        // \"IMAGE_RESOURCE_SERVER_ADDR\": \"your.domain.com\" // Example: Use a domain name for image URLs\n      }\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "imagen3",
        "imagen",
        "hamflx",
        "imagen3 mcp",
        "hamflx imagen3",
        "google imagen"
      ],
      "category": "image-and-video-generation"
    },
    "htessaro--mcp-test-deploy-2": {
      "owner": "htessaro",
      "name": "mcp-test-deploy-2",
      "url": "https://github.com/htessaro/mcp-test-deploy-2",
      "imageUrl": "/freedevtools/mcp/pfp/htessaro.webp",
      "description": "Access a wide array of cat images and detailed breed information through a TypeScript SDK, enabling image uploads, retrieval of breed data, and user interactions such as favoriting or voting on images.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-28T17:24:43Z",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "htessaro",
        "breed",
        "images",
        "htessaro mcp",
        "cat images",
        "breed information"
      ],
      "category": "image-and-video-generation"
    },
    "huangmiuXyz--jimeng-mcp": {
      "owner": "huangmiuXyz",
      "name": "jimeng-mcp",
      "url": "https://github.com/huangmiuXyz/jimeng-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/huangmiuXyz.webp",
      "description": "Integrate AI-powered image generation capabilities into applications using the Jimeng AI model. Generate high-quality images through a simple MCP interface for advanced AI image synthesis.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-20T02:13:23Z",
      "readme_content": "{\n  \"mcpServers\": {\n    \"jimeng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"jimeng\",\n        \"-y\"\n      ],\n      \"env\": {\n        \"VOLCENGINE_ACCESS_KEY\": \"your_access_key_here\",\n        \"VOLCENGINE_SECRET_KEY\": \"your_secret_key_here\"\n      }\n    }\n  }\n}\n\nhttps://console.volcengine.com/iam/keymanage/ 获取Access Key ID和Secret Access Key\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "jimeng",
        "generate",
        "image generation",
        "image synthesis",
        "ai image"
      ],
      "category": "image-and-video-generation"
    },
    "husniadil--mcp-image-placeholder": {
      "owner": "husniadil",
      "name": "mcp-image-placeholder",
      "url": "https://github.com/husniadil/mcp-image-placeholder",
      "imageUrl": "/freedevtools/mcp/pfp/husniadil.webp",
      "description": "Generates placeholder images from multiple providers, supporting both simple and real images as placeholders. Validates input parameters and returns image URLs for immediate use.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-06-17T08:32:20Z",
      "readme_content": "# MCP Image Placeholder Server\n\nThis is a Model Context Protocol (MCP) server that provides a tool for generating placeholder images from different providers.\n\n## Features\n\n- Generates placeholder images from supported providers\n- Supports two image providers:\n  - [`placehold`](https://placehold.co/): Provides simple placeholder images\n  - [`lorem-picsum`](https://picsum.photos/): Provides real images as placeholder images\n- Validates input parameters\n- Returns image URLs for immediate use\n\n## Requirements\n\n- Python 3.9+\n- `uv` package manager\n\n## Installation\n\n1. Clone this repository\n2. [Set up the configuration for MCP server](#configuration)\n\n## Usage\n\nThe server exposes one tool:\n\n### `image_placeholder`\n\nGenerate a placeholder image URL based on specified parameters.\n\n**Parameters:**\n- `provider`: The image provider to use (`placehold` or `lorem-picsum`)\n- `width`: The width of the image (1-10000)\n- `height`: The height of the image (1-10000)\n\n**Returns:**\n- URL string of the generated image\n\n**Example Usage:**\n```python\n# Generate a 300x200 placeholder image\nurl = image_placeholder(provider=\"placehold\", width=300, height=200)\n\n# Generate a 500px square lorem-picsum image\nurl = image_placeholder(provider=\"lorem-picsum\", width=500)\n```\n\n## Configuration\n\n### To connect this server to Claude for Desktop:\n\n1. Add the following to your `claude_desktop_config.json`:\n   ```json\n   {\n       \"mcpServers\": {\n           \"image-placeholder\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"/ABSOLUTE/PATH/TO/PROJECT\",\n                   \"run\",\n                   \"main.py\"\n               ]\n           }\n       }\n   }\n   ```\n2. Restart Claude for Desktop\n\n### To connect this server to Cursor:\n\n1. Open Cursor Settings\n2. Head to the `Features` section\n3. Scroll down to the `MCP Servers` section\n4. Click on the `Add new MCP server` button\n5. Enter the following information:\n   - Name: `image-placeholder`\n   - Type: `command`\n   - Server URL: `uv --directory /ABSOLUTE/PATH/TO/PROJECT run main.py`\n6. Click on the `Add ↵` button\n\n\n## Troubleshooting\n\nIf the tool is not detected, use absolute path of the `uv` command, e.g.\n```\n/ABSOLUTE/PATH/TO/uv --directory /ABSOLUTE/PATH/TO/PROJECT run main.py\n```\n\n## Example Usage and Output (Cursor)\n\nPrompt:\n```\nCreate a new directory named \"example\" and a file named output.html.\n\nThen create a single modern looking page using tailwindcss: https://unpkg.com/@tailwindcss/browser@4\n\nShow a nice header, content, and footer, showing a photo gallery.\n\nSave this into output.html\n```\n\n![Screenshot of Cursor Agent](example/cursor-agent.png)\n\nOutput:\n[Example Output (Cursor)](example/output.html)\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "placeholder",
        "images",
        "mcp",
        "placeholder images",
        "image placeholder",
        "mcp image"
      ],
      "category": "image-and-video-generation"
    },
    "ifmelate--mcp-image-extractor": {
      "owner": "ifmelate",
      "name": "mcp-image-extractor",
      "url": "https://github.com/ifmelate/mcp-image-extractor",
      "imageUrl": "/freedevtools/mcp/pfp/ifmelate.webp",
      "description": "Extracts images from local files and URLs, processing them into base64 format for analysis by large language models (LLMs). Suitable for analyzing image-based data, such as screenshots from tests.",
      "stars": 14,
      "forks": 4,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-09-21T00:31:03Z",
      "readme_content": "# MCP Image Extractor\n\nMCP server for extracting and converting images to base64 for LLM analysis.\n\nThis MCP server provides tools for AI assistants to:\n- Extract images from local files\n- Extract images from URLs\n- Process base64-encoded images\n\n<a href=\"https://glama.ai/mcp/servers/@ifmelate/mcp-image-extractor\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ifmelate/mcp-image-extractor/badge\" alt=\"Image Extractor MCP server\" />\n</a>\n\nHow it looks in Cursor:\n\n<img width=\"687\" alt=\"image\" src=\"https://github.com/user-attachments/assets/8954dbbd-7e7a-4f27-82a7-b251bd3c5af2\" />\n\nSuitable cases:\n- analyze playwright test results: screenshots\n\n## Installation\n\n### Recommended: Using npx in mcp.json (Easiest)\n\nThe recommended way to install this MCP server is using npx directly in your `.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-extractor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-image-extractor\"\n      ]\n    }\n  }\n}\n```\n\nThis approach:\n- Automatically installs the latest version\n- Does not require global installation\n- Works reliably across different environments\n\n### Alternative: Local Path Installation\n\nIf you prefer to use a local installation of the package, you can clone the repository and point to the built files:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-extractor\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/mcp-image-extractor/dist/index.js\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n### Manual Installation\n\n```bash\n# Clone and install \ngit clone https://github.com/ifmelate/mcp-image-extractor.git\ncd mcp-image-extractor\nnpm install\nnpm run build\nnpm link\n```\n\nThis will make the `mcp-image-extractor` command available globally.\n\nThen configure in `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-extractor\": {\n      \"command\": \"mcp-image-extractor\",\n      \"disabled\": false\n    }\n  }\n}\n```\n\n> **Troubleshooting for Cursor Users**: If you see \"Failed to create client\" error, try the local path installation method above or ensure you're using the correct path to the executable.\n\n## Available Tools\n\n### extract_image_from_file\n\nExtracts an image from a local file and converts it to base64.\n\nParameters:\n- `file_path` (required): Path to the local image file\n\n**Note:** All images are automatically resized to optimal dimensions (max 512x512) for LLM analysis to limit the size of the base64 output and optimize context window usage.\n\n### extract_image_from_url\n\nExtracts an image from a URL and converts it to base64.\n\nParameters:\n- `url` (required): URL of the image to extract\n\n**Note:** All images are automatically resized to optimal dimensions (max 512x512) for LLM analysis to limit the size of the base64 output and optimize context window usage.\n\n### extract_image_from_base64\n\nProcesses a base64-encoded image for LLM analysis.\n\nParameters:\n- `base64` (required): Base64-encoded image data\n- `mime_type` (optional, default: \"image/png\"): MIME type of the image\n\n**Note:** All images are automatically resized to optimal dimensions (max 512x512) for LLM analysis to limit the size of the base64 output and optimize context window usage.\n\n## Example Usage\n\nHere's an example of how to use the tools from Claude:\n\n```\nPlease extract the image from this local file: images/photo.jpg\n```\n\nClaude will automatically use the `extract_image_from_file` tool to load and analyze the image content.\n\n```\nPlease extract the image from this URL: https://example.com/image.jpg\n```\n\nClaude will automatically use the `extract_image_from_url` tool to fetch and analyze the image content.\n\n## Docker\n\nBuild and run with Docker:\n\n```bash\ndocker build -t mcp-image-extractor .\ndocker run -p 8000:8000 mcp-image-extractor\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "base64",
        "image",
        "image extractor",
        "mcp image",
        "extracts images"
      ],
      "category": "image-and-video-generation"
    },
    "jaokuohsuan--draw-things-mcp-cursor": {
      "owner": "jaokuohsuan",
      "name": "draw-things-mcp-cursor",
      "url": "https://github.com/jaokuohsuan/draw-things-mcp-cursor",
      "imageUrl": "/freedevtools/mcp/pfp/jaokuohsuan.webp",
      "description": "Generates images based on text prompts using AI, integrating seamlessly within workflows. The server utilizes the Draw Things API to transform user-defined prompts into visual creations.",
      "stars": 12,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-16T06:00:20Z",
      "readme_content": "# Draw Things MCP\n\nDraw Things API integration for Cursor using Model Context Protocol (MCP).\n\n## Prerequisites\n\n- Node.js >= 14.0.0\n- Draw Things API running on http://127.0.0.1:7888\n\n## Installation\n\n```bash\n# Install globally\nnpm install -g draw-things-mcp-cursor\n\n# Or run directly\nnpx draw-things-mcp-cursor\n```\n\n## Cursor Integration\n\nTo set up this tool in Cursor, see the detailed guide in [cursor-setup.md](./cursor-setup.md).\n\nQuick setup:\n\n1. Create or edit `~/.cursor/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"draw-things\": {\n      \"command\": \"draw-things-mcp-cursor\",\n      \"args\": []\n    }\n  }\n}\n```\n\n2. Restart Cursor\n3. Use in Cursor: `generateImage({\"prompt\": \"a cute cat\"})`\n\n## CLI Usage\n\n### Generate Image\n\n```bash\necho '{\"prompt\": \"your prompt here\"}' | npx draw-things-mcp-cursor\n```\n\n### Parameters\n\n- `prompt`: The text prompt for image generation (required)\n- `negative_prompt`: The negative prompt for image generation\n- `width`: Image width (default: 360)\n- `height`: Image height (default: 360)\n- `steps`: Number of steps for generation (default: 8)\n- `model`: Model to use for generation (default: \"flux_1_schnell_q5p.ckpt\")\n- `sampler`: Sampling method (default: \"DPM++ 2M AYS\")\n\nExample:\n\n```bash\necho '{\n  \"prompt\": \"a happy smiling dog, professional photography\",\n  \"negative_prompt\": \"ugly, deformed, blurry\",\n  \"width\": 360,\n  \"height\": 360,\n  \"steps\": 4\n}' | npx draw-things-mcp-cursor\n```\n\n### MCP Tool Integration\n\nWhen used as an MCP tool in Cursor, the tool will be registered as `generateImage` with the following parameters:\n\n```typescript\n{\n  prompt: string;       // Required - The prompt to generate the image from\n  negative_prompt?: string;  // Optional - The negative prompt\n  width?: number;       // Optional - Image width (default: 360)\n  height?: number;      // Optional - Image height (default: 360)\n  model?: string;       // Optional - Model name\n  steps?: number;       // Optional - Number of steps (default: 8)\n}\n```\n\nThe generated images will be saved in the `images` directory with a filename format of:\n`<sanitized_prompt>_<timestamp>.png`\n\n## Response Format\n\nSuccess:\n```json\n{\n  \"type\": \"success\",\n  \"content\": [{\n    \"type\": \"image\",\n    \"data\": \"base64 encoded image data\",\n    \"mimeType\": \"image/png\"\n  }],\n  \"metadata\": {\n    \"parameters\": { ... }\n  }\n}\n```\n\nError:\n```json\n{\n  \"type\": \"error\",\n  \"error\": \"error message\",\n  \"code\": 500\n}\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n\n- Ensure Draw Things API is running at http://127.0.0.1:7888\n- Check log files in `~/.cursor/logs` if using with Cursor\n- Make sure src/index.js has execution permissions: `chmod +x src/index.js`\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "visual",
        "cursor",
        "draw",
        "visual creations",
        "generates images",
        "utilizes draw"
      ],
      "category": "image-and-video-generation"
    },
    "jbrower95--mcp-asset-gen": {
      "owner": "jbrower95",
      "name": "mcp-asset-gen",
      "url": "https://github.com/jbrower95/mcp-asset-gen",
      "imageUrl": "/freedevtools/mcp/pfp/jbrower95.webp",
      "description": "Generate high-quality image assets for game or web development by providing descriptive prompts. Streamline asset creation workflows with automated image generation through AI.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-05T20:41:33Z",
      "readme_content": "# mcp-asset-gen\n[![npm version](https://badge.fury.io/js/mcp-asset-gen.svg)](https://badge.fury.io/js/mcp-asset-gen)\n\n\nThis tool allows Claude to speak to OpenAI, and use `gpt-image-1` to generate image assets. This can be pretty useful for game or web development, when you need to print individual assets.\n\n# Requirements\n\n- Node \n- An OpenAI [API Key](https://platform.openai.com/settings/organization/api-keys)\n    - Note that this requires an organization, with ID verification at the moment.\n\n# Installation\n\n1. Get an API KEY from the [OpenAI website](https://platform.openai.com/settings/organization/api-keys).\n\n2. Export it in your shell as API_KEY.\n\n```bash\nexport API_KEY=sk-proj-....\n```\n\n3. Then, paste this into that same shell:\n\n```bash\njq --arg apiKey \"$API_KEY\" \\\n'.mcpServers = (.mcpServers // {}) | \n .mcpServers[\"mcp-asset-gen\"] = {\n   type: \"stdio\",\n   command: \"npx\",\n   args: [\"mcp-asset-gen\"],\n   env: { API_KEY: $apiKey }\n }' ~/.claude.json > ~/.claude.tmp && mv ~/.claude.tmp ~/.claude.json\n```\n\n4. You're done! Ask Claude to generate you an image for something, and you'll see\n\n```\n> generate a small logo image that i could use for my arcade game\n\n I'll generate a logo for your arcade game. Please provide a more specific description of what you'd like to see in the logo (colors, style,\n  game theme, etc.) for better results.\n\n> It should look like an old school pacman machine (liek a wooden arcade machine)\n\n⏺ mcp-asset-gen:generateImag(outputPathAbsolute: \"/Users/jbrower/projects/mcp-asset-gen/arcade-logo.png\", prompt: \"An old school wooden arcade…\n   (MCP)                    machine logo in the style of Pacman, with retro colors and pixelated elements, top-down view of the cabinet,\n                            nostalgic arcade gaming feel, vibrant colors against dark background\", quality: \"high\", size: \"1024x1024\")\n  ⎿  The image is now available at /Users/jbrower/projects/mcp-asset-gen/arcade-logo.png.\n\n⏺ Generated arcade logo saved to /Users/jbrower/projects/mcp-asset-gen/arcade-logo.png\n```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "generate",
        "image",
        "image generation",
        "video generation",
        "image assets"
      ],
      "category": "image-and-video-generation"
    },
    "jezweb--openai-mcp": {
      "owner": "jezweb",
      "name": "openai-mcp",
      "url": "https://github.com/jezweb/openai-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jezweb.webp",
      "description": "Connect to OpenAI's DALL-E API for image generation with support for various options, enabling seamless integration into MCP-compatible AI assistants like Roo Code.",
      "stars": 1,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-24T08:28:31Z",
      "readme_content": "# OpenAI MCP - DALL-E API Integration for Roo Code\n\nThis project provides a Model Context Protocol (MCP) server for connecting to OpenAI's DALL-E API for image generation with full support for all available options. It's specifically designed to work with Roo Code and other MCP-compatible AI assistants.\n\n## Overview\n\nThis MCP server provides a tool for DALL-E image generation with comprehensive support for all DALL-E API options. It allows AI assistants like Roo Code to generate images through the Model Context Protocol (MCP) with fine-grained control over the generation process.\n\n## Project Structure\n\n- `src/` - Source code for the MCP server\n  - `dalle.ts` - Implementation of the DALL-E API integration with all options\n  - `index.ts` - Main server file with the DALL-E tool and input schema\n  - `install.ts` - Installation script for Roo Code and Claude Desktop\n- `build/` - Compiled JavaScript files\n- `dalle-test.html` - HTML page to display the generated image and document available options\n- `test-dalle.js` - Direct test script for the DALL-E API with examples of different options\n\n## Setup Instructions for Roo Code\n\n### Installation\n\n1. Install the package globally:\n   ```\n   npm install -g openai-mcp\n   ```\n\n2. Run the setup command to configure Roo Code:\n   ```\n   openai-mcp install\n   ```\n\n3. Set your OpenAI API key in Roo Code settings:\n   - Open Roo Code\n   - Go to Settings\n   - Add the following environment variable to the MCP server configuration:\n     ```json\n     \"openai-mcp\": {\n       \"env\": {\n         \"OPENAI_API_KEY\": \"your-openai-api-key\"\n       }\n     }\n     ```\n\n4. Restart Roo Code",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "mcp",
        "ai",
        "openai mcp",
        "image generation",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "jhacksman--OpenSCAD-MCP-Server": {
      "owner": "jhacksman",
      "name": "OpenSCAD-MCP-Server",
      "url": "https://github.com/jhacksman/OpenSCAD-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/jhacksman.webp",
      "description": "Generates 3D models from text descriptions or images, focusing on parametric model creation through multi-view reconstruction and integration with OpenSCAD. Facilitates remote processing and includes an image approval workflow for model generation.",
      "stars": 84,
      "forks": 17,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T04:21:59Z",
      "readme_content": "# OpenSCAD MCP Server\n\nA Model Context Protocol (MCP) server that enables users to generate 3D models from text descriptions or images, with a focus on creating parametric 3D models using multi-view reconstruction and OpenSCAD.\n\n## Features\n\n- **AI Image Generation**: Generate images from text descriptions using Google Gemini or Venice.ai APIs\n- **Multi-View Image Generation**: Create multiple views of the same 3D object for reconstruction\n- **Image Approval Workflow**: Review and approve/deny generated images before reconstruction\n- **3D Reconstruction**: Convert approved multi-view images into 3D models using CUDA Multi-View Stereo\n- **Remote Processing**: Process computationally intensive tasks on remote servers within your LAN\n- **OpenSCAD Integration**: Generate parametric 3D models using OpenSCAD\n- **Parametric Export**: Export models in formats that preserve parametric properties (CSG, AMF, 3MF, SCAD)\n- **3D Printer Discovery**: Optional network printer discovery and direct printing\n\n## Architecture\n\nThe server is built using the Python MCP SDK and follows a modular architecture:\n\n```\nopenscad-mcp-server/\n├── src/\n│   ├── main.py                  # Main application\n│   ├── main_remote.py           # Remote CUDA MVS server\n│   ├── ai/                      # AI integrations\n│   │   ├── gemini_api.py        # Google Gemini API for image generation\n│   │   └── venice_api.py        # Venice.ai API for image generation (optional)\n│   ├── models/                  # 3D model generation\n│   │   ├── cuda_mvs.py          # CUDA Multi-View Stereo integration\n│   │   └── code_generator.py    # OpenSCAD code generation\n│   ├── workflow/                # Workflow components\n│   │   ├── image_approval.py    # Image approval mechanism\n│   │   └── multi_view_to_model_pipeline.py  # Complete pipeline\n│   ├── remote/                  # Remote processing\n│   │   ├── cuda_mvs_client.py   # Client for remote CUDA MVS processing\n│   │   ├── cuda_mvs_server.py   # Server for remote CUDA MVS processing\n│   │   ├── connection_manager.py # Remote connection management\n│   │   └── error_handling.py    # Error handling for remote processing\n│   ├── openscad_wrapper/        # OpenSCAD CLI wrapper\n│   ├── visualization/           # Preview generation and web interface\n│   ├── utils/                   # Utility functions\n│   └── printer_discovery/       # 3D printer discovery\n├── scad/                        # Generated OpenSCAD files\n├── output/                      # Output files (models, previews)\n│   ├── images/                  # Generated images\n│   ├── multi_view/              # Multi-view images\n│   ├── approved_images/         # Approved images for reconstruction\n│   └── models/                  # Generated 3D models\n├── templates/                   # Web interface templates\n└── static/                      # Static files for web interface\n```\n\n## Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/jhacksman/OpenSCAD-MCP-Server.git\n   cd OpenSCAD-MCP-Server\n   ```\n\n2. Create a virtual environment:\n   ```\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n4. Install OpenSCAD:\n   - Ubuntu/Debian: `sudo apt-get install openscad`\n   - macOS: `brew install openscad`\n   - Windows: Download from [openscad.org](https://openscad.org/downloads.html)\n\n5. Install CUDA Multi-View Stereo:\n   ```\n   git clone https://github.com/fixstars/cuda-multi-view-stereo.git\n   cd cuda-multi-view-stereo\n   mkdir build && cd build\n   cmake ..\n   make\n   ```\n\n6. Set up API keys:\n   - Create a `.env` file in the root directory\n   - Add your API keys:\n     ```\n     GEMINI_API_KEY=your-gemini-api-key\n     VENICE_API_KEY=your-venice-api-key  # Optional\n     REMOTE_CUDA_MVS_API_KEY=your-remote-api-key  # For remote processing\n     ```\n\n## Remote Processing Setup\n\nThe server supports remote processing of computationally intensive tasks, particularly CUDA Multi-View Stereo reconstruction. This allows you to offload processing to more powerful machines within your LAN.\n\n### Server Setup (on the machine with CUDA GPU)\n\n1. Install CUDA Multi-View Stereo on the server machine:\n   ```\n   git clone https://github.com/fixstars/cuda-multi-view-stereo.git\n   cd cuda-multi-view-stereo\n   mkdir build && cd build\n   cmake ..\n   make\n   ```\n\n2. Start the remote CUDA MVS server:\n   ```\n   python src/main_remote.py\n   ```\n\n3. The server will automatically advertise itself on the local network using Zeroconf.\n\n### Client Configuration\n\n1. Configure remote processing in your `.env` file:\n   ```\n   REMOTE_CUDA_MVS_ENABLED=True\n   REMOTE_CUDA_MVS_USE_LAN_DISCOVERY=True\n   REMOTE_CUDA_MVS_API_KEY=your-shared-secret-key\n   ```\n\n2. Alternatively, you can specify a server URL directly:\n   ```\n   REMOTE_CUDA_MVS_ENABLED=True\n   REMOTE_CUDA_MVS_USE_LAN_DISCOVERY=False\n   REMOTE_CUDA_MVS_SERVER_URL=http://server-ip:8765\n   REMOTE_CUDA_MVS_API_KEY=your-shared-secret-key\n   ```\n\n### Remote Processing Features\n\n- **Automatic Server Discovery**: Find CUDA MVS servers on your local network\n- **Job Management**: Upload images, track job status, and download results\n- **Fault Tolerance**: Automatic retries, circuit breaker pattern, and error tracking\n- **Authentication**: Secure API key authentication for all remote operations\n- **Health Monitoring**: Continuous server health checks and status reporting\n\n## Usage\n\n1. Start the server:\n   ```\n   python src/main.py\n   ```\n\n2. The server will start on http://localhost:8000\n\n3. Use the MCP tools to interact with the server:\n\n   - **generate_image_gemini**: Generate an image using Google Gemini API\n     ```json\n     {\n       \"prompt\": \"A low-poly rabbit with black background\",\n       \"model\": \"gemini-2.0-flash-exp-image-generation\"\n     }\n     ```\n\n   - **generate_multi_view_images**: Generate multiple views of the same 3D object\n     ```json\n     {\n       \"prompt\": \"A low-poly rabbit\",\n       \"num_views\": 4\n     }\n     ```\n\n   - **create_3d_model_from_images**: Create a 3D model from approved multi-view images\n     ```json\n     {\n       \"image_ids\": [\"view_1\", \"view_2\", \"view_3\", \"view_4\"],\n       \"output_name\": \"rabbit_model\"\n     }\n     ```\n\n   - **create_3d_model_from_text**: Complete pipeline from text to 3D model\n     ```json\n     {\n       \"prompt\": \"A low-poly rabbit\",\n       \"num_views\": 4\n     }\n     ```\n\n   - **export_model**: Export a model to a specific format\n     ```json\n     {\n       \"model_id\": \"your-model-id\",\n       \"format\": \"obj\"  // or \"stl\", \"ply\", \"scad\", etc.\n     }\n     ```\n\n   - **discover_remote_cuda_mvs_servers**: Find CUDA MVS servers on your network\n     ```json\n     {\n       \"timeout\": 5\n     }\n     ```\n\n   - **get_remote_job_status**: Check the status of a remote processing job\n     ```json\n     {\n       \"server_id\": \"server-id\",\n       \"job_id\": \"job-id\"\n     }\n     ```\n\n   - **download_remote_model_result**: Download a completed model from a remote server\n     ```json\n     {\n       \"server_id\": \"server-id\",\n       \"job_id\": \"job-id\",\n       \"output_name\": \"model-name\"\n     }\n     ```\n\n   - **discover_printers**: Discover 3D printers on the network\n     ```json\n     {}\n     ```\n\n   - **print_model**: Print a model on a connected printer\n     ```json\n     {\n       \"model_id\": \"your-model-id\",\n       \"printer_id\": \"your-printer-id\"\n     }\n     ```\n\n## Image Generation Options\n\nThe server supports multiple image generation options:\n\n1. **Google Gemini API** (Default): Uses the Gemini 2.0 Flash Experimental model for high-quality image generation\n   - Supports multi-view generation with consistent style\n   - Requires a Google Gemini API key\n\n2. **Venice.ai API** (Optional): Alternative image generation service\n   - Supports various models including flux-dev and fluently-xl\n   - Requires a Venice.ai API key\n\n3. **User-Provided Images**: Skip image generation and use your own images\n   - Upload images directly to the server\n   - Useful for working with existing photographs or renders\n\n## Multi-View Workflow\n\nThe server implements a multi-view workflow for 3D reconstruction:\n\n1. **Image Generation**: Generate multiple views of the same 3D object\n2. **Image Approval**: Review and approve/deny each generated image\n3. **3D Reconstruction**: Convert approved images into a 3D model using CUDA MVS\n   - Can be processed locally or on a remote server within your LAN\n4. **Model Refinement**: Optionally refine the model using OpenSCAD\n\n## Remote Processing Workflow\n\nThe remote processing workflow allows you to offload computationally intensive tasks to more powerful machines:\n\n1. **Server Discovery**: Automatically discover CUDA MVS servers on your network\n2. **Image Upload**: Upload approved multi-view images to the remote server\n3. **Job Processing**: Process the images on the remote server using CUDA MVS\n4. **Status Tracking**: Monitor the job status and progress\n5. **Result Download**: Download the completed 3D model when processing is finished\n\n## Supported Export Formats\n\nThe server supports exporting models in various formats:\n\n- **OBJ**: Wavefront OBJ format (standard 3D model format)\n- **STL**: Standard Triangle Language (for 3D printing)\n- **PLY**: Polygon File Format (for point clouds and meshes)\n- **SCAD**: OpenSCAD source code (for parametric models)\n- **CSG**: OpenSCAD CSG format (preserves all parametric properties)\n- **AMF**: Additive Manufacturing File Format (preserves some metadata)\n- **3MF**: 3D Manufacturing Format (modern replacement for STL with metadata)\n\n## Web Interface\n\nThe server provides a web interface for:\n\n- Generating and approving multi-view images\n- Previewing 3D models from different angles\n- Downloading models in various formats\n\nAccess the interface at http://localhost:8000/ui/\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openscad",
        "3d",
        "mcp",
        "openscad mcp",
        "openscad facilitates",
        "jhacksman openscad"
      ],
      "category": "image-and-video-generation"
    },
    "jmanhype--mcp-flux-studio": {
      "owner": "jmanhype",
      "name": "mcp-flux-studio",
      "url": "https://github.com/jmanhype/mcp-flux-studio",
      "imageUrl": "/freedevtools/mcp/pfp/jmanhype.webp",
      "description": "Integrates advanced image generation capabilities from Flux into AI coding assistants, enabling seamless text-to-image generation and manipulation within development environments.",
      "stars": 20,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-11T12:07:21Z",
      "readme_content": "# MCP Flux Studio\n\n[![smithery badge](https://smithery.ai/badge/@jmanhype/mcp-flux-studio)](https://smithery.ai/server/@jmanhype/mcp-flux-studio)\n\nA powerful Model Context Protocol (MCP) server that brings Flux's advanced image generation capabilities to your AI coding assistants. This server enables direct integration of Flux's image generation, manipulation, and control features into Cursor and Windsurf (Codeium) IDEs.\n\n## Overview\n\nMCP Flux Studio bridges the gap between AI coding assistants and Flux's powerful image generation API, allowing seamless integration of image generation capabilities directly into your development workflow.\n\n### Features\n\n- **Image Generation**\n  - Text-to-image generation with precise control\n  - Multiple model support (flux.1.1-pro, flux.1-pro, flux.1-dev, flux.1.1-ultra)\n  - Customizable aspect ratios and dimensions\n\n- **Image Manipulation**\n  - Image-to-image transformation\n  - Inpainting with customizable masks\n  - Resolution upscaling and enhancement\n\n- **Advanced Controls**\n  - Edge-based generation (canny)\n  - Depth-aware generation\n  - Pose-guided generation\n\n- **IDE Integration**\n  - Full support for Cursor (v0.45.7+)\n  - Compatible with Windsurf/Codeium Cascade (Wave 3+)\n  - Seamless tool invocation through AI assistants\n\n## Quick Start\n\n1. **Prerequisites**\n   - Node.js 18+\n   - Python 3.12+\n   - Flux API key\n   - Compatible IDE (Cursor or Windsurf)\n\n2. **Installation**\n\n### Installing via Smithery\n\nTo install Flux Studio for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jmanhype/mcp-flux-studio):\n\n```bash\nnpx -y @smithery/cli install @jmanhype/mcp-flux-studio --client claude\n```\n\n### Manual Installation\n   ```bash\n   git clone https://github.com/jmanhype/mcp-flux-studio.git\n   cd mcp-flux-studio\n   npm install\n   npm run build\n   ```\n\n3. **Basic Configuration**\n   ```env\n   BFL_API_KEY=your_flux_api_key\n   FLUX_PATH=/path/to/flux/installation\n   ```\n\nFor detailed setup instructions, including IDE-specific configuration and troubleshooting, see our [Installation Guide](docs/INSTALLATION.md).\n\n## Documentation\n\n- [Installation Guide](docs/INSTALLATION.md) - Comprehensive setup instructions\n- [API Documentation](docs/API.md) - Detailed tool documentation\n- [Example Usage](examples/tool-examples.md) - Real-world usage examples\n- [Contributing Guidelines](docs/CONTRIBUTING.md) - How to contribute\n\n## IDE Integration\n\n### Cursor (v0.45.7+)\n\nMCP Flux Studio integrates seamlessly with Cursor's AI assistant:\n\n1. **Configuration**\n   - Configure via Settings > Features > MCP\n   - Supports both stdio and SSE connections\n   - Environment variables can be set via wrapper scripts\n\n2. **Usage**\n   - Tools automatically available to Cursor's AI assistant\n   - Tool invocations require user approval\n   - Real-time feedback on generation progress\n\n### Windsurf/Codeium (Wave 3+)\n\nIntegration with Windsurf's Cascade AI:\n\n1. **Configuration**\n   - Edit `~/.codeium/windsurf/mcp_config.json`\n   - Supports process-based tool execution\n   - Environment variables configured in JSON\n\n2. **Usage**\n   - Access tools through Cascade's MCP toolbar\n   - Automatic tool discovery and loading\n   - Integrated with Cascade's AI capabilities\n\nFor detailed IDE-specific setup instructions, see the [Installation Guide](docs/INSTALLATION.md).\n\n## Usage\n\nThe server provides the following tools:\n\n### generate\nGenerate an image from a text prompt.\n```json\n{\n  \"prompt\": \"A photorealistic cat\",\n  \"model\": \"flux.1.1-pro\",\n  \"aspect_ratio\": \"1:1\",\n  \"output\": \"generated.jpg\"\n}\n```\n\n### img2img\nGenerate an image using another image as reference.\n```json\n{\n  \"image\": \"input.jpg\",\n  \"prompt\": \"Convert to oil painting\",\n  \"model\": \"flux.1.1-pro\",\n  \"strength\": 0.85,\n  \"output\": \"output.jpg\",\n  \"name\": \"oil_painting\"\n}\n```\n\n### inpaint\nInpaint an image using a mask.\n```json\n{\n  \"image\": \"input.jpg\",\n  \"prompt\": \"Add flowers\",\n  \"mask_shape\": \"circle\",\n  \"position\": \"center\",\n  \"output\": \"inpainted.jpg\"\n}\n```\n\n### control\nGenerate an image using structural control.\n```json\n{\n  \"type\": \"canny\",\n  \"image\": \"control.jpg\",\n  \"prompt\": \"A realistic photo\",\n  \"output\": \"controlled.jpg\"\n}\n```\n\n## Development\n\n### Project Structure\n\n```\nflux-mcp-server/\n├── src/\n│   ├── index.ts          # Main server implementation\n│   └── types.ts          # TypeScript type definitions\n├── tests/\n│   └── server.test.ts    # Server tests\n├── docs/\n│   ├── API.md           # API documentation\n│   └── CONTRIBUTING.md  # Contribution guidelines\n├── examples/\n│   ├── generate.json    # Example tool usage\n│   └── config.json      # Example configuration\n├── package.json\n├── tsconfig.json\n└── README.md\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](docs/CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol/mcp) - The protocol specification\n- [Flux API](https://flux.ai) - The underlying image generation API\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "flux",
        "studio",
        "image generation",
        "flux studio",
        "flux ai"
      ],
      "category": "image-and-video-generation"
    },
    "joshmouch--mcp-image-generator": {
      "owner": "joshmouch",
      "name": "mcp-image-generator",
      "url": "https://github.com/joshmouch/mcp-image-generator",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Generate, edit, and create variations of images using OpenAI's DALL-E API, supporting multiple DALL-E models with customizable parameters. Validate OpenAI API keys for seamless operation.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "images",
        "dall",
        "image generator",
        "mcp image",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "jyjune--mcp_vms": {
      "owner": "jyjune",
      "name": "mcp_vms",
      "url": "https://github.com/jyjune/mcp_vms",
      "imageUrl": "/freedevtools/mcp/pfp/jyjune.webp",
      "description": "Connects to CCTV recording software to retrieve live and recorded video streams, manage video channel information, and control VMS features like PTZ camera presets and playback dialogs.",
      "stars": 10,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T17:24:47Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jyjune-mcp-vms-badge.png)](https://mseep.ai/app/jyjune-mcp-vms)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/jyjune/mcp_vms)](https://archestra.ai/mcp-catalog/jyjune__mcp_vms)\n\n# MCP Server - VMS Integration\n\nA Model Context Protocol (MCP) server designed to connect to a CCTV recording program (VMS) to retrieve recorded and live video streams. It also provides tools to control the VMS software, such as showing live or playback dialogs for specific channels at specified times.\n\n![diagram](https://github.com/jyjune/mcp_vms/blob/main/mcp_vms_diagram.png?raw=true)\n\n## Features\n\n- Retrieve video channel information, including connection and recording status.\n- Fetch recording dates and times for specific channels.\n- Fetch live or recorded images from video channels.\n- Show live video streams or playback dialogs for specific channels and timestamps.\n- Control PTZ (Pan-Tilt-Zoom) cameras by moving them to preset positions.\n- Comprehensive error handling and logging.\n\n## Prerequisites\n\n- Python 3.12+\n- `vmspy` library (for VMS integration)\n- `Pillow` library (for image processing)\n\n## MCP-server Configuration\n\nIf you want to use `mcp-vms` with Claude desktop, you need to set up the `claude_desktop_config.json` file as follows:\n\n```json\n{\n  \"mcpServers\": {\n\t\"vms\": {\n\t  \"command\": \"uv\",\n\t  \"args\": [\n\t\t\"--directory\",\n\t\t\"X:\\\\path\\\\to\\\\mcp-vms\",\n\t\t\"run\",\n\t\t\"mcp_vms.py\"\n\t  ]\n\t}\n  }\n}\n```\n\n## VMS Connection Configuration\n\nThe server uses the following default configuration for connecting to the VMS:\n- mcp_vms_config.py\n```python\nvms_config = {\n    'img_width': 320,\n    'img_height': 240,\n    'pixel_format': 'RGB',\n    'url': '127.0.0.1',\n    'port': 3300,\n    'access_id': 'admin',\n    'access_pw': 'admin',\n}\n```\n\n## Installation\n\n### 1. Install UV Package Manager\nRun the following command in PowerShell to install `UV`:\n\n```shell\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nFor alternative installation methods, see the [official UV documentation](https://docs.astral.sh/uv/getting-started/installation/).\n\n### 2.Install VMS Server\n   Download and install the VMS server from:  \n   [http://surveillance-logic.com/en/download.html](http://surveillance-logic.com/en/download.html)\n   (Required before using this MCP server)\n\n### 3.Install Python Dependencies\n   Download the vmspy library:  \n   [vmspy1.4-python3.12-x64.zip](https://sourceforge.net/projects/security-vms/files/vmspy1.4-python3.12-x64.zip/download)\n   Extract the contents into your `mcp_vms` directory\n\nThe mcp-vms directory should look like this:\n\n```shell\nmcp-vms/\n├── .gitignore\n├── .python-version\n├── LICENSE\n├── README.md\n├── pyproject.toml\n├── uv.lock\n├── mcp_vms.py            # Main server implementation\n├── mcp_vms_config.py     # VMS connection configuration\n├── vmspy.pyd             # VMS Python library\n├── avcodec-61.dll        # FFmpeg libraries\n├── avutil-59.dll\n├── swresample-5.dll\n├── swscale-8.dll\n```\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/7027c4cd-a9c1-43dd-9e74-771fc7cc42da)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_vms",
        "cctv",
        "vms",
        "jyjune mcp_vms",
        "cctv recording",
        "connects cctv"
      ],
      "category": "image-and-video-generation"
    },
    "kshern--image-tools-mcp": {
      "owner": "kshern",
      "name": "image-tools-mcp",
      "url": "https://github.com/kshern/image-tools-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kshern.webp",
      "description": "Retrieve image dimensions, compress images, and convert images to various formats using local files or URLs. Supports image processing with detailed output on dimensions, types, and compression information.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:19Z",
      "readme_content": "# Image Tools MCP\n\n[![smithery badge](https://smithery.ai/badge/@kshern/image-tools-mcp)](https://smithery.ai/server/@kshern/image-tools-mcp)\n\nA Model Context Protocol (MCP) service for retrieving image dimensions and compressing images, supporting both URL and local file sources.\n\n_[中文文档](./README_zh.md)_\n\n## Features\n\n- Retrieve image dimensions from URLs\n- Get image dimensions from local files\n- Compress images from URLs using TinyPNG API\n- Compress local images using TinyPNG API\n- Convert images to different formats (webp, jpeg/jpg, png)\n- Returns width, height, type, MIME type, and compression information\n\n### Example Results\n\n![Example Result 1](./public/image_gemini_1.jpg)\n![Example Result 2](./public/image_gemini_2.jpg)\n\n![Example Result 1](./public/image_1.png)\n![Example Result 2](./public/image_2.png)\n\ndownload from figma url and compress\n![Example Result 3](./public/image_figma_url.png)\n\n## Usage\n\n### Using as an MCP Service\n\nThis service provides five tool functions:\n\n1. `get_image_size` - Get dimensions of remote images\n2. `get_local_image_size` - Get dimensions of local images\n3. `compress_image_from_url` - Compress remote images using TinyPNG API\n4. `compress_local_image` - Compress local images using TinyPNG API\n5. `figma` - Fetch image links from Figma API and compress them using TinyPNG API\n\n### Client Integration\n\nTo use this MCP service, you need to connect to it from an MCP client. Here are examples of how to integrate with different clients:\n\n#### Usage\n\n```json\n{\n  \"mcpServers\": {\n    \"image-tools\": {\n      \"command\": \"npx\",\n      \"args\": [\"image-tools-mcp\"],\n      \"env\": {\n        \"TINIFY_API_KEY\": \"<YOUR_TINIFY_API_KEY>\",\n        \"FIGMA_API_TOKEN\": \"<YOUR_FIGMA_API_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Using with MCP Client Library\n\n````typescript\nimport { McpClient } from \"@modelcontextprotocol/client\";\n\n// Initialize the client\nconst client = new McpClient({\n  transport: \"stdio\" // or other transport options\n});\n\n// Connect to the server\nawait client.connect();\n\n// Get image dimensions from URL\nconst urlResult = await client.callTool(\"get_image_size\", {\n  options: {\n    imageUrl: \"https://example.com/image.jpg\"\n  }\n});\nconsole.log(JSON.parse(urlResult.content[0].text));\n// Output: { width: 800, height: 600, type: \"jpg\", mime: \"image/jpeg\" }\n\n// Get image dimensions from local file\nconst localResult = await client.callTool(\"get_local_image_size\", {\n  options: {\n    imagePath: \"D:/path/to/image.png\"\n  }\n});\nconsole.log(JSON.parse(localResult.content[0].text));\n// Output: { width: 1024, height: 768, type: \"png\", mime: \"image/png\", path: \"D:/path/to/image.png\" }\n\n// Compress image from URL\nconst compressUrlResult = await client.callTool(\"compress_image_from_url\", {\n  options: {\n    imageUrl: \"https://example.com/image.jpg\",\n    outputFormat: \"webp\" // Optional: convert to webp, jpeg/jpg, or png\n  }\n});\nconsole.log(JSON.parse(compressUrlResult.content[0].text));\n// Output: { originalSize: 102400, compressedSize: 51200, compressionRatio: \"50.00%\", tempFilePath: \"/tmp/compressed_1615456789.webp\", format: \"webp\" }\n\n// Compress local image\nconst compressLocalResult = await client.callTool(\"compress_local_image\", {\n  options: {\n    imagePath: \"D:/path/to/image.png\",\n    outputPath: \"D:/path/to/compressed.webp\", // Optional\n    outputFormat: \"image/webp\" // Optional: convert to image/webp, image/jpeg, or image/png\n  }\n});\nconsole.log(JSON.parse(compressLocalResult.content[0].text));\n// Output: { originalSize: 102400, compressedSize: 51200, compressionRatio: \"50.00%\", outputPath: \"D:/path/to/compressed.webp\", format: \"webp\" }\n\n// Fetch image links from Figma API\n\nconst figmaResult = await client.callTool(\"figma\", {\n  options: {\n    figmaUrl: \"https://www.figma.com/file/XXXXXXX\"\n  }\n});\nconsole.log(JSON.parse(figmaResult.content[0].text));\n// Output: { imageLinks: [\"https://example.com/image1.jpg\", \"https://example.com/image2.jpg\"] }\n\n### Tool Schemas\n\n#### get_image_size\n\n```typescript\n{\n  options: {\n    imageUrl: string // URL of the image to retrieve dimensions for\n  }\n}\n````\n\n#### get_local_image_size\n\n```typescript\n{\n  options: {\n    imagePath: string; // Absolute path to the local image file\n  }\n}\n```\n\n#### compress_image_from_url\n\n```typescript\n{\n  options: {\n    imageUrl: string // URL of the image to compress\n    outputFormat?: \"image/webp\" | \"image/jpeg\" | \"image/jpg\" | \"image/png\" // Optional output format\n  }\n}\n```\n\n#### compress_local_image\n\n```typescript\n{\n  options: {\n    imagePath: string // Absolute path to the local image file\n    outputPath?: string // Optional absolute path for the compressed output image\n    outputFormat?: \"image/webp\" | \"image/jpeg\" | \"image/jpg\" | \"image/png\" // Optional output format\n  }\n}\n```\n\n#### figma\n\n```typescript\n{\n  options: {\n    figmaUrl: string; // URL of the Figma file to fetch image links from\n  }\n}\n```\n\n## Changelog\n\n- **2025-05-12:** Updated Figma API to support additional parameters, including 2x image scaling.\n\n## Technical Implementation\n\nThis project is built on the following libraries:\n\n- [probe-image-size](https://github.com/nodeca/probe-image-size) - For image dimension detection\n- [tinify](https://github.com/tinify/tinify-nodejs) - For image compression via the TinyPNG API\n- [figma-api](https://github.com/figma/api) - For fetching image links from Figma API\n\n## Environment Variables\n\n- `TINIFY_API_KEY` - Required for image compression functionality. Get your API key from [TinyPNG](https://tinypng.com/developers)\n  - When not provided, the compression tools (`compress_image_from_url` and `compress_local_image`) will not be registered\n- `FIGMA_API_TOKEN` - Required for fetching image links from Figma API. Get your API token from [Figma](https://www.figma.com/developers)\n  - When not provided, the Figma tool (`figma`) will not be registered\n\nNote: The basic image dimension tools (`get_image_size` and `get_local_image_size`) are always available regardless of API keys.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "mcp",
        "compression",
        "image tools",
        "convert images",
        "images convert"
      ],
      "category": "image-and-video-generation"
    },
    "lalanikarim--comfy-mcp-server": {
      "owner": "lalanikarim",
      "name": "comfy-mcp-server",
      "url": "https://github.com/lalanikarim/comfy-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/lalanikarim.webp",
      "description": "Generates images based on user prompts by interacting with a remote Comfy server. Utilizes the FastMCP framework to manage image generation workflows.",
      "stars": 31,
      "forks": 12,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T17:04:48Z",
      "readme_content": "# Comfy MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@lalanikarim/comfy-mcp-server)](https://smithery.ai/server/@lalanikarim/comfy-mcp-server)\n\n> A server using FastMCP framework to generate images based on prompts via a remote Comfy server.\n\n## Overview\n\nThis script sets up a server using the FastMCP framework to generate images based on prompts using a specified workflow. It interacts with a remote Comfy server to submit prompts and retrieve generated images.\n\n## Prerequisites\n\n- [uv](https://docs.astral.sh/uv/) package and project manager for Python.\n- Workflow file exported from Comfy UI. This code includes a sample `Flux-Dev-ComfyUI-Workflow.json` which is only used here as reference. You will need to export from your workflow and set the environment variables accordingly.\n\nYou can install the required packages for local development:\n\n```bash\nuvx mcp[cli]\n```\n\n## Configuration\n\nSet the following environment variables:\n\n- `COMFY_URL` to point to your Comfy server URL.\n- `COMFY_WORKFLOW_JSON_FILE` to point to the absolute path of the API export json file for the comfyui workflow.\n- `PROMPT_NODE_ID` to the id of the text prompt node.\n- `OUTPUT_NODE_ID` to the id of the output node with the final image.\n- `OUTPUT_MODE` to either `url` or `file` to select desired output.\n\nOptionally, if you have an [Ollama](https://ollama.com) server running, you can connect to it for prompt generation.\n\n- `OLLAMA_API_BASE` to the url where ollama is running.\n- `PROMPT_LLM` to the name of the model hosted on ollama for prompt generation.\n\nExample:\n\n```bash\nexport COMFY_URL=http://your-comfy-server-url:port\nexport COMFY_WORKFLOW_JSON_FILE=/path/to/the/comfyui_workflow_export.json\nexport PROMPT_NODE_ID=6 # use the correct node id here\nexport OUTPUT_NODE_ID=9 # use the correct node id here\nexport OUTPUT_MODE=file\n```\n\n## Usage\n\nComfy MCP Server can be launched by the following command:\n\n```bash\nuvx comfy-mcp-server\n```\n\n### Example Claude Desktop Config\n\n```json\n{\n  \"mcpServers\": {\n    \"Comfy MCP Server\": {\n      \"command\": \"/path/to/uvx\",\n      \"args\": [\n        \"comfy-mcp-server\"\n      ],\n      \"env\": {\n        \"COMFY_URL\": \"http://your-comfy-server-url:port\",\n        \"COMFY_WORKFLOW_JSON_FILE\": \"/path/to/the/comfyui_workflow_export.json\",\n        \"PROMPT_NODE_ID\": \"6\",\n        \"OUTPUT_NODE_ID\": \"9\",\n        \"OUTPUT_MODE\": \"file\",\n      }\n    }\n  }\n}\n\n```\n\n## Functionality\n\n### `generate_image(prompt: str, ctx: Context) -> Image | str`\n\nThis function generates an image using a specified prompt. It follows these steps:\n\n1. Checks if all the environment variable are set.\n2. Loads a prompt template from a JSON file.\n3. Submits the prompt to the Comfy server.\n4. Polls the server for the status of the prompt processing.\n5. Retrieves and returns the generated image once it's ready.\n\n### `generate_prompt(topic: str, ctx: Context) -> str`\n\nThis function generates a comprehensive image generation prompt from specified topic.\n\n## Dependencies\n\n- `mcp`: For setting up the FastMCP server.\n- `json`: For handling JSON data.\n- `urllib`: For making HTTP requests.\n- `time`: For adding delays in polling.\n- `os`: For accessing environment variables.\n- `langchain`: For creating simple LLM Prompt chain to generate image generation prompt from topic.\n- `langchain-ollama`: For ollama specific modules for LangChain.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/lalanikarim/comfy-mcp-server/blob/main/LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fastmcp",
        "mcp",
        "lalanikarim",
        "utilizes fastmcp",
        "mcp server",
        "fastmcp framework"
      ],
      "category": "image-and-video-generation"
    },
    "laosu888--tupianyasuo": {
      "owner": "laosu888",
      "name": "tupianyasuo",
      "url": "https://github.com/laosu888/tupianyasuo",
      "imageUrl": "/freedevtools/mcp/pfp/laosu888.webp",
      "description": "A front-end image compression tool supporting various formats like PNG and JPG, enabling users to customize compression ratios and preview results in real-time. The application allows users to download optimized images with comparisons of file sizes before and after compression.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2024-12-25T19:15:50Z",
      "readme_content": "# 图片压缩工具\n\n一个简单易用的在线图片压缩工具，具有精美的苹果风格界面设计。\n\n## 功能特点\n\n- 支持PNG、JPG等格式图片上传\n- 支持自定义压缩比例\n- 实时预览压缩前后的图片效果\n- 显示压缩前后文件大小对比\n- 支持压缩后图片下载\n- 纯前端实现，无需后端服务\n\n## 项目结构\n\n```\n├── index.html          # 主页面\n├── css/               \n│   └── style.css      # 样式文件\n├── js/\n│   └── main.js        # 主要功能实现\n└── assets/\n    └── icons/         # SVG图标\n```\n\n## 技术栈\n\n- HTML5\n- CSS3 (Flexbox & Grid)\n- Vanilla JavaScript\n- 浏览器原生图片压缩API ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "compression",
        "png",
        "images",
        "image compression",
        "compression tool",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "luojunhui1--ImageOnC": {
      "owner": "luojunhui1",
      "name": "ImageOnC",
      "url": "https://github.com/luojunhui1/ImageOnC",
      "imageUrl": "/freedevtools/mcp/pfp/luojunhui1.webp",
      "description": "Implement vehicle license plate recognition using C/C++ on FPGA, utilizing OpenCV for image display and Eigen for optimized matrix operations. The project includes code for training neural networks and processing license plate images.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "C++",
      "updated_at": "2024-05-17T18:19:28Z",
      "readme_content": "# ImageOnC\n## 1.介绍\n本仓库为实现在FPGA上的车牌识别而创建，但本仓库只保有C/C++部分的代码，并未保存使用HLS工具后的代码。要特别说明的是，本项目中的代码均用C实现，其中出现的C++主要为便于OpenCV进行图像显示或者Eigen库加速矩阵运算，但均可删除或改成C中的数组而不影响其正常功能。\n## 2.文件组成\n```\n.\n├── build\n├── cmake-build-debug\n├── CMakeLists.txt\n├── database\n├── Fit.cpp\n├── Han\n├── include\n├── Letters\n├── main.cpp\n├── paramLetters.txt\n├── param.txt\n├── README.md\n└── Train.cpp\n```\n\n其中build和cmake-build-debug文件均为编译执行过程产生的文件；CMakeLists.txt用于指导编译方式；database为车牌图片文件夹；Fit.cpp原作测试网络准确性，但其内容在测试后被整合到main.cpp中，故该文件无实际意义；Han文件夹保存了用于训练汉字识别的图像；Letters中则保存了用于训练字母和数字识别的代码；main.cpp为执行的识别车牌的主函数；Train.cpp用于训练神经网络；param.txt及paramLetters.txt则保存了网络参数；include文件中保存了一写自定义的功能函数，其文件树如下：\n```\n.\n├── Config.h\n├── Eigen\n├── FileProcess.h\n├── ModelTrans.h\n├── Net.h\n├── Process.h\n├── SaveLoad.h\n└── unsupported\n```\n**Config.h**: 用于约定网络参数和一些全局变量，便于项目代码组织\n\n**Eigen**: Eigen库代码\n\n**unsupported**: Eigen库代码,原为使用Tensor类表示高维矩阵，但Tensor使用不便，实际未使用\n\n**FileProcess**: 用于系统文件操作，主要是查询文件夹下的所有文件并遍历\n\n**ModelTrans**: 用于从图像的数据矩阵中读取BGR图像并将其分割、保存\n\n**Net.h**: 神经网络的定义、训练及使用部分\n\n**SaveLoad**: 用于从图像路径读取bmp图像并分通道保存图像数据部分\n\n## 3. 实际效果\n数据集比较简单，能做到100%。\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opencv",
        "recognition",
        "imageonc",
        "opencv image",
        "plate recognition",
        "license plate"
      ],
      "category": "image-and-video-generation"
    },
    "luoshui-coder--image-generator-mcp-server": {
      "owner": "luoshui-coder",
      "name": "image-generator-mcp-server",
      "url": "https://github.com/luoshui-coder/image-generator-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/luoshui-coder.webp",
      "description": "Generates images based on prompts using OpenAI's DALL-E model, saving them in a specified directory on the user's desktop.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-14T04:57:37Z",
      "readme_content": "# image-generator MCP Server\n\nAn mcp server that generates images based on image prompts\n\nThis is a TypeScript-based MCP server that implements image generation using **OPENAI**'s `dall-e-3` image generation model.\n\n## Features\n\n### Tools\n- `generate_image` - Generate an image for given prompt\n  - Takes `prompt` as a required parameter\n  - Takes `imageName` as a required parameter to save the generated image in a `generated-images` directory on your desktop\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"command\": \"image-generator\",\n      \"env\": {\n        \"OPENAI_API_KEY\": \"<your-openai-api-key>\"\n    }\n  }\n}\n```\nMake sure to replace `<your-openai-api-key>` with your actual **OPENAI** Api Key.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "generates",
        "images",
        "generates images",
        "image generator",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "m-mcp--flux-schnell-server": {
      "owner": "m-mcp",
      "name": "flux-schnell-server",
      "url": "https://github.com/m-mcp/flux-schnell-server",
      "imageUrl": "/freedevtools/mcp/pfp/m-mcp.webp",
      "description": "Provides an MCP protocol-based API for generating images from text prompts with customizable dimensions and reproducible results using a specified random seed. Supports asynchronous streaming responses and integration with Hugging Face model services.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-29T09:05:08Z",
      "readme_content": "# Flux Schnell Server\n\n[![smithery badge](https://smithery.ai/badge/@m-mcp/flux-schnell-server)](https://smithery.ai/server/@m-mcp/flux-schnell-server)\n\n基于[Flux Schnell](https://huggingface.co/spaces/black-forest-labs/flux-1-schnell)模型的MCP图像生成服务器。\n\n## 功能特点\n\n- 提供基于MCP协议的图像生成API\n- 支持自定义图片尺寸（宽度和高度）\n- 支持设置随机种子以复现特定生成结果\n- 支持异步流式响应\n- 提供HTTP接口调用Hugging Face的模型服务\n\n## 安装要求\n\n- Python >= 3.10\n- 依赖包：\n  - httpx >= 0.28.1\n  - mcp[cli] >= 1.3.0\n\n## 使用方法\n### 开发环境设置\n\n1. 创建并激活 Python 虚拟环境\n```bash\nuv venv && source .venv/bin/activate  # Unix/macOS\n# 或\n.venv\\Scripts\\activate  # Windows\n```\n\n2. 安装开发依赖\n```bash\nuv sync  # 以可编辑模式安装项目\n```\n\n### 调试方法\n\n1. 启用调试\n```bash\nmcp dev main.py\n或者\nnpx -y @modelcontextprotocol/inspector uv run main.py\n```\n\n2. 调用图像生成工具：\n```python\n# 示例代码\nasync def test_main():\n    img_url = await image_generation(\n        prompt=\"your prompt here\",\n        image_width=512,  # 可选，默认512\n        image_height=512, # 可选，默认512\n        seed=3           # 可选，默认3\n    )\n    print(img_url)\n```\n\n## API参数说明\n\n- `prompt` (str): 图像生成提示词\n- `image_width` (int, optional): 生成图片宽度，默认512\n- `image_height` (int, optional): 生成图片高度，默认512\n- `seed` (int, optional): 随机种子，默认3\n\n## 示例\n\n### 春天的生机\n\n![春天的生机](https://black-forest-labs-flux-1-schnell.hf.space/file=/tmp/gradio/45d6489d73142fa77851d8985bb1010572433d6a/image.webp)\n\n> 春天来了，大地苏醒，万物复苏。花儿竞相开放，嫩绿的叶子在微风中轻轻摇曳。空气中弥漫着泥土的芬芳和花儿的香气。小鸟在枝头欢快地歌唱，蝴蝶在花丛中翩翩起舞。阳光洒在大地上，温暖而明亮。春天的生机勃勃，让人心旷神怡。\n\n这个示例展示了使用服务生成的图片效果。您可以在demo目录中找到完整的网页展示代码。\n\n生成的图片URL可以直接用于：\n1. 网页图片展示\n2. 社交媒体分享\n3. 应用程序界面\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "images",
        "image",
        "generating images",
        "provides mcp",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "madhusudan-kulkarni--mcp-fal-ai-image": {
      "owner": "madhusudan-kulkarni",
      "name": "mcp-fal-ai-image",
      "url": "https://github.com/madhusudan-kulkarni/mcp-fal-ai-image",
      "imageUrl": "/freedevtools/mcp/pfp/madhusudan-kulkarni.webp",
      "description": "Generate images from text prompts using various fal.ai models through the Model Context Protocol (MCP), enabling seamless integration with AI IDEs. Save generated images locally with customizable output paths and robust error handling.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-25T04:25:20Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/madhusudan-kulkarni-mcp-fal-ai-image-badge.png)](https://mseep.ai/app/madhusudan-kulkarni-mcp-fal-ai-image)\n\n[![npm version](https://img.shields.io/npm/v/mcp-fal-ai-image.svg)](https://www.npmjs.com/package/mcp-fal-ai-image) [![Node.js Version](https://img.shields.io/node/v/mcp-fal-ai-image)](https://nodejs.org/) [![TypeScript](https://img.shields.io/badge/TypeScript-5.8-blue.svg)](https://www.typescriptlang.org/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n\n# MCP fal.ai Image Server\n\nEffortlessly generate images from text prompts using [fal.ai](https://fal.ai) and the Model Context Protocol (MCP). Integrates directly with AI IDEs like Cursor and Windsurf.\n\n## When and Why to Use\n\nThis tool is designed for:\n- Developers and designers who want to generate images from text prompts without leaving their IDE.\n- Rapid prototyping of UI concepts, marketing assets, or creative ideas.\n- Content creators needing unique visuals for blogs, presentations, or social media.\n- AI researchers and tinkerers experimenting with the latest fal.ai models.\n- Automating workflows that require programmatic image generation via MCP.\n\nKey features:\n- Supports any valid fal.ai model and all major image parameters.\n- Works out of the box with Node.js and a fal.ai API key.\n- Saves images locally with accessible file paths.\n- Simple configuration and robust error handling.\n\n## Quick Start\n\n1. **Requirements:** Node.js 18+, [fal.ai API key](https://fal.ai)\n2. **Configure MCP:**\n   ```json\n   {\n     \"mcpServers\": {\n       \"fal-ai-image\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"mcp-fal-ai-image\"],\n         \"env\": { \"FAL_KEY\": \"YOUR-FAL-AI-API-KEY\" }\n       }\n     }\n   }\n   ```\n3. **Run:** Use the `generate-image` tool from your IDE.\n\n> **💡 Typical Workflow:**\n> Describe the image you want (e.g., “generate a landscape with flying cars using model fal-ai/kolors, 2 images, landscape_16_9”) and get instant results in your IDE.\n\n### 🗨️ Example Prompts\n\n- `generate an image of a red apple`\n- `generate an image of a red apple using model fal-ai/kolors`\n- `generate 3 images of a glowing red apple in a futuristic city using model fal-ai/recraft-v3, square_hd, 40 inference steps, guidance scale 4.0, safety checker on`\n\n**Supported parameters:** prompt, model ID (any fal.ai model), number of images, image size, inference steps, guidance scale, safety checker.\n\nImages are saved locally; file paths are shown in the response. For model IDs, see [fal.ai/models](https://fal.ai/models).\n\n## Troubleshooting\n\n- `FAL_KEY environment variable is not set`: Set your fal.ai API key as above.\n- `npx` not found: Install Node.js 18+ and npm.\n\n<details>\n<summary>Advanced: Example MCP Request/Response</summary>\n\n```json\n{\n  \"tool\": \"generate-image\",\n  \"args\": {\n    \"prompt\": \"A futuristic cityscape at sunset\",\n    \"model\": \"fal-ai/kolors\"\n  }\n}\n\n// Example response\n{\n  \"images\": [\n    { \"url\": \"file:///path/to/generated_image1.png\" },\n    { \"url\": \"file:///path/to/generated_image2.png\" }\n  ]\n}\n```\n\n</details>\n\n## 📁 Image Output Directory\n\nGenerated images are saved to your local system:\n\n- **By default:** `~/Downloads/fal_ai` (on Linux/macOS; uses XDG standard if available)\n- **Custom location:** Set the environment variable `FAL_IMAGES_OUTPUT_DIR` to your desired folder. Images will be saved in `<your-folder>/fal_ai`.\n\nThe full file path for each image is included in the tool's response.\n\n## ⚠️ Error Handling & Troubleshooting\n\n- If you specify a model ID that is not supported by fal.ai, you will receive an error from the backend. Double-check for typos or visit [fal.ai/models](https://fal.ai/models) to confirm the model ID.\n- For the latest list of models and their capabilities, refer to the [fal.ai model catalog](https://fal.ai/models) or [API docs](https://fal.ai/docs/api).\n- For other errors, consult your MCP client logs or open an issue on GitHub.\n\n## 🤝 Contributing\n\nContributions and suggestions are welcome! Please open issues or pull requests on [GitHub](https://github.com/madhusudan-kulkarni/mcp-fal-ai-image).\n\n## 🔒 Security\n\n- Your API key is only used locally to authenticate with fal.ai.\n- No user data is stored or transmitted except as required by fal.ai API.\n\n## 🔗 Links\n\n- [NPM](https://www.npmjs.com/package/mcp-fal-ai-image)\n- [GitHub](https://github.com/madhusudan-kulkarni/mcp-fal-ai-image)\n- [fal.ai](https://fal.ai)\n\n## 🛡 License\n\nMIT License © 2025 Madhusudan Kulkarni\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "image",
        "ai",
        "image generate",
        "generate images",
        "generated images"
      ],
      "category": "image-and-video-generation"
    },
    "manascb1344--together-mcp-server": {
      "owner": "manascb1344",
      "name": "together-mcp-server",
      "url": "https://github.com/manascb1344/together-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/manascb1344.webp",
      "description": "Generate high-quality images using the Flux.1 Schnell model by specifying customizable parameters such as width and height, while ensuring clear error handling for prompt validation and API interactions.",
      "stars": 9,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-30T19:28:08Z",
      "readme_content": "# Image Generation MCP Server\n\nA Model Context Protocol (MCP) server that enables seamless generation of high-quality images using the Flux.1 Schnell model via Together AI. This server provides a standardized interface to specify image generation parameters.\n<div align=\"center\">\n  \n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/manascb1344/together-mcp-server)\n\n</div>\n\n<div align=\"center\">\n\n<a href=\"https://glama.ai/mcp/servers/y6qfizhsja\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/y6qfizhsja/badge\" alt=\"Image Generation Server MCP server\" />\n</a>\n</div>\n\n## Features\n\n- High-quality image generation powered by the Flux.1 Schnell model\n- Support for customizable dimensions (width and height)\n- Clear error handling for prompt validation and API issues\n- Easy integration with MCP-compatible clients\n- Optional image saving to disk in PNG format\n\n## Installation\n\n```bash\nnpm install together-mcp\n```\n\nOr run directly:\n\n```bash\nnpx together-mcp@latest\n```\n\n### Configuration\n\nAdd to your MCP server configuration:\n\n<summary>Configuration Example</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"together-image-gen\": {\n      \"command\": \"npx\",\n      \"args\": [\"together-mcp@latest -y\"],\n      \"env\": {\n        \"TOGETHER_API_KEY\": \"<API KEY>\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides one tool: `generate_image`\n\n### Using generate_image\n\nThis tool has only one required parameter - the prompt. All other parameters are optional and use sensible defaults if not provided.\n\n#### Parameters\n\n```typescript\n{\n  // Required\n  prompt: string;          // Text description of the image to generate\n\n  // Optional with defaults\n  model?: string;          // Default: \"black-forest-labs/FLUX.1-schnell-Free\"\n  width?: number;          // Default: 1024 (min: 128, max: 2048)\n  height?: number;         // Default: 768 (min: 128, max: 2048)\n  steps?: number;          // Default: 1 (min: 1, max: 100)\n  n?: number;             // Default: 1 (max: 4)\n  response_format?: string; // Default: \"b64_json\" (options: [\"b64_json\", \"url\"])\n  image_path?: string;     // Optional: Path to save the generated image as PNG\n}\n```\n\n#### Minimal Request Example\n\nOnly the prompt is required:\n\n```json\n{\n  \"name\": \"generate_image\",\n  \"arguments\": {\n    \"prompt\": \"A serene mountain landscape at sunset\"\n  }\n}\n```\n\n#### Full Request Example with Image Saving\n\nOverride any defaults and specify a path to save the image:\n\n```json\n{\n  \"name\": \"generate_image\",\n  \"arguments\": {\n    \"prompt\": \"A serene mountain landscape at sunset\",\n    \"width\": 1024,\n    \"height\": 768,\n    \"steps\": 20,\n    \"n\": 1,\n    \"response_format\": \"b64_json\",\n    \"model\": \"black-forest-labs/FLUX.1-schnell-Free\",\n    \"image_path\": \"/path/to/save/image.png\"\n  }\n}\n```\n\n#### Response Format\n\nThe response will be a JSON object containing:\n\n```json\n{\n  \"id\": string,        // Generation ID\n  \"model\": string,     // Model used\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"timings\": {\n        \"inference\": number  // Time taken for inference\n      },\n      \"index\": number,      // Image index\n      \"b64_json\": string    // Base64 encoded image data (if response_format is \"b64_json\")\n      // OR\n      \"url\": string        // URL to generated image (if response_format is \"url\")\n    }\n  ]\n}\n```\n\nIf image_path was provided and the save was successful, the response will include confirmation of the save location.\n\n### Default Values\n\nIf not specified in the request, these defaults are used:\n\n- model: \"black-forest-labs/FLUX.1-schnell-Free\"\n- width: 1024\n- height: 768\n- steps: 1\n- n: 1\n- response_format: \"b64_json\"\n\n### Important Notes\n\n1. Only the `prompt` parameter is required\n2. All optional parameters use defaults if not provided\n3. When provided, parameters must meet their constraints (e.g., width/height ranges)\n4. Base64 responses can be large - use URL format for larger images\n5. When saving images, ensure the specified directory exists and is writable\n\n## Prerequisites\n\n- Node.js >= 16\n- Together AI API key\n  1. Sign in at [api.together.xyz](https://api.together.xyz/)\n  2. Navigate to [API Keys settings](https://api.together.xyz/settings/api-keys)\n  3. Click \"Create\" to generate a new API key\n  4. Copy the generated key for use in your MCP configuration\n\n## Dependencies\n\n```json\n{\n  \"@modelcontextprotocol/sdk\": \"0.6.0\",\n  \"axios\": \"^1.6.7\"\n}\n```\n\n## Development\n\nClone and build the project:\n\n```bash\ngit clone https://github.com/manascb1344/together-mcp-server\ncd together-mcp-server\nnpm install\nnpm run build\n```\n\n### Available Scripts\n\n- `npm run build` - Build the TypeScript project\n- `npm run watch` - Watch for changes and rebuild\n- `npm run inspector` - Run MCP inspector\n\n## Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository\n2. Create a new branch (`feature/my-new-feature`)\n3. Commit your changes\n4. Push the branch to your fork\n5. Open a Pull Request\n\nFeature requests and bug reports can be submitted via GitHub Issues. Please check existing issues before creating a new one.\n\nFor significant changes, please open an issue first to discuss your proposed changes.\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "images",
        "flux",
        "video generation",
        "quality images",
        "manascb1344 mcp"
      ],
      "category": "image-and-video-generation"
    },
    "maoxiaoke--mcp-media-processor": {
      "owner": "maoxiaoke",
      "name": "mcp-media-processor",
      "url": "https://github.com/maoxiaoke/mcp-media-processor",
      "imageUrl": "/freedevtools/mcp/pfp/maoxiaoke.webp",
      "description": "A Node.js server for executing various media processing tasks, including video and image manipulation. It supports operations like video conversion, image effects, and media compression.",
      "stars": 24,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-16T09:41:08Z",
      "readme_content": "# MCP Media Processing Server\n\n[![smithery badge](https://smithery.ai/badge/@maoxiaoke/mcp-media-processor)](https://smithery.ai/server/@maoxiaoke/mcp-media-processor)\n\nA Node.js server implementing Model Context Protocol (MCP) for media processing operations, providing powerful video and image manipulation capabilities.\n\n## Features\n\n* Video processing and conversion\n* Image processing and manipulation\n* Media compression\n* Video trimming and editing\n* Image effects and watermarking\n\n## Prerequisites\n\nBefore using this server, make sure you have the following dependencies installed on your system:\n\n* **FFmpeg**: Required for video processing operations\n  * macOS: `brew install ffmpeg`\n  * Ubuntu/Debian: `sudo apt-get install ffmpeg`\n  * Windows: Download from [FFmpeg official website](https://ffmpeg.org/download.html)\n\n* **ImageMagick**: Required for image processing operations\n  * macOS: `brew install imagemagick`\n  * Ubuntu/Debian: `sudo apt-get install imagemagick`\n  * Windows: Download from [ImageMagick official website](https://imagemagick.org/script/download.php)\n\n## How to use\n\nAdd this to your `claude_desktop_config.json`:\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"mediaProcessor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-media-processor@latest\"\n      ]\n    }\n  }\n}\n```\n\n## API\n\n### Tools\n\n#### Video Operations\n\n* **execute-ffmpeg**\n  * Execute any FFmpeg command with custom options\n  * Inputs:\n    * `inputPath` (string): Absolute path to input video file\n    * `options` (string[]): Array of FFmpeg command options\n    * `outputPath` (string, optional): Absolute path for output file\n    * `outputFilename` (string, optional): Output filename\n\n* **convert-video**\n  * Convert video to different format\n  * Inputs:\n    * `inputPath` (string): Absolute path to input video file\n    * `outputFormat` (string): Desired output format (e.g., mp4, mkv, avi)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **compress-video**\n  * Compress video file\n  * Inputs:\n    * `inputPath` (string): Absolute path to input video file\n    * `quality` (number, optional): Compression quality (1-51, lower is better quality)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **trim-video**\n  * Trim video to specified duration\n  * Inputs:\n    * `inputPath` (string): Absolute path to input video file\n    * `startTime` (string): Start time in format HH:MM:SS\n    * `duration` (string): Duration in format HH:MM:SS\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n#### Image Operations\n\n* **compress-image**\n  * Compress PNG image using ImageMagick\n  * Inputs:\n    * `inputPath` (string): Absolute path to input PNG image\n    * `quality` (number, optional): Compression quality (1-100)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **convert-image**\n  * Convert image to different format\n  * Inputs:\n    * `inputPath` (string): Absolute path to input image file\n    * `outputFormat` (string): Desired output format (e.g., jpg, png, webp, gif)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **resize-image**\n  * Resize image to specified dimensions\n  * Inputs:\n    * `inputPath` (string): Absolute path to input image file\n    * `width` (number, optional): Target width in pixels\n    * `height` (number, optional): Target height in pixels\n    * `maintainAspectRatio` (boolean, optional): Whether to maintain aspect ratio\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **rotate-image**\n  * Rotate image by specified degrees\n  * Inputs:\n    * `inputPath` (string): Absolute path to input image file\n    * `degrees` (number): Rotation angle in degrees\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **add-watermark**\n  * Add watermark to image\n  * Inputs:\n    * `inputPath` (string): Absolute path to input image file\n    * `watermarkPath` (string): Absolute path to watermark image file\n    * `position` (string, optional): Position of watermark (default: \"southeast\")\n    * `opacity` (number, optional): Watermark opacity (0-100)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n* **apply-effect**\n  * Apply visual effect to image\n  * Inputs:\n    * `inputPath` (string): Absolute path to input image file\n    * `effect` (string): Effect to apply (blur, sharpen, edge, emboss, grayscale, sepia, negate)\n    * `intensity` (number, optional): Effect intensity (0-100)\n    * `outputPath` (string, optional): Custom output path\n    * `outputFilename` (string, optional): Custom output filename\n\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "processor",
        "node",
        "mcp",
        "media processor",
        "media processing",
        "mcp media"
      ],
      "category": "image-and-video-generation"
    },
    "mario-andreschak--mcp-image-recognition": {
      "owner": "mario-andreschak",
      "name": "mcp-image-recognition",
      "url": "https://github.com/mario-andreschak/mcp-image-recognition",
      "imageUrl": "/freedevtools/mcp/pfp/mario-andreschak.webp",
      "description": "Leverages image recognition capabilities to analyze and describe images using advanced vision APIs. Supports multiple formats and allows for optional text extraction from images.",
      "stars": 26,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:16Z",
      "readme_content": "# MCP Image Recognition Server\n\nAn MCP server that provides image recognition capabilities using Anthropic and OpenAI vision APIs. Version 0.1.2.\n\n## Features\n\n- Image description using Anthropic Claude Vision or OpenAI GPT-4 Vision\n- Support for multiple image formats (JPEG, PNG, GIF, WebP)\n- Configurable primary and fallback providers\n- Base64 and file-based image input support\n- Optional text extraction using Tesseract OCR\n\n## Requirements\n\n- Python 3.8 or higher\n- Tesseract OCR (optional) - Required for text extraction feature\n  - Windows: Download and install from [UB-Mannheim/tesseract](https://github.com/UB-Mannheim/tesseract/wiki)\n  - Linux: `sudo apt-get install tesseract-ocr`\n  - macOS: `brew install tesseract`\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/mario-andreschak/mcp-image-recognition.git\ncd mcp-image-recognition\n```\n\n2. Create and configure your environment file:\n```bash\ncp .env.example .env\n# Edit .env with your API keys and preferences\n```\n\n3. Build the project:\n```bash\nbuild.bat\n```\n\n## Usage\n\n### Running the Server\nSpawn the server using python:\n```bash\npython -m image_recognition_server.server\n```\n\nStart the server using batch instead:\n```bash\nrun.bat server\n```\n\nStart the server in development mode with the MCP Inspector:\n```bash\nrun.bat debug\n```\n\n### Available Tools\n\n1. `describe_image`\n   - Input: Base64-encoded image data and MIME type\n   - Output: Detailed description of the image\n\n2. `describe_image_from_file`\n   - Input: Path to an image file\n   - Output: Detailed description of the image\n\n### Environment Configuration\n\n- `ANTHROPIC_API_KEY`: Your Anthropic API key.\n- `OPENAI_API_KEY`: Your OpenAI API key.\n- `VISION_PROVIDER`: Primary vision provider (`anthropic` or `openai`).\n- `FALLBACK_PROVIDER`: Optional fallback provider.\n- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR).\n- `ENABLE_OCR`: Enable Tesseract OCR text extraction (`true` or `false`).\n- `TESSERACT_CMD`: Optional custom path to Tesseract executable.\n- `OPENAI_MODEL`: OpenAI Model (default: `gpt-4o-mini`). Can use OpenRouter format for other models (e.g., `anthropic/claude-3.5-sonnet:beta`).\n- `OPENAI_BASE_URL`: Optional custom base URL for the OpenAI API.  Set to `https://openrouter.ai/api/v1` for OpenRouter.\n- `OPENAI_TIMEOUT`: Optional custom timeout (in seconds) for the OpenAI API.\n\n### Using OpenRouter\n\nOpenRouter allows you to access various models using the OpenAI API format. To use OpenRouter, follow these steps:\n\n1.  Obtain an OpenAI API key from OpenRouter.\n2.  Set `OPENAI_API_KEY` in your `.env` file to your OpenRouter API key.\n3.  Set `OPENAI_BASE_URL` to `https://openrouter.ai/api/v1`.\n4.  Set `OPENAI_MODEL` to the desired model using the OpenRouter format (e.g., `anthropic/claude-3.5-sonnet:beta`).\n5. Set `VISION_PROVIDER` to `openai`.\n\n### Default Models\n\n- Anthropic: `claude-3.5-sonnet-beta`\n- OpenAI: `gpt-4o-mini`\n- OpenRouter: Use the `anthropic/claude-3.5-sonnet:beta` format in `OPENAI_MODEL`.\n\n## Development\n\n### Running Tests\n\nRun all tests:\n```bash\nrun.bat test\n```\n\nRun specific test suite:\n```bash\nrun.bat test server\nrun.bat test anthropic\nrun.bat test openai\n```\n\n### Docker Support\n\nBuild the Docker image:\n```bash\ndocker build -t mcp-image-recognition .\n```\n\nRun the container:\n```bash\ndocker run -it --env-file .env mcp-image-recognition\n```\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Release History\n\n- **0.1.2** (2025-02-20): Improved OCR error handling and added comprehensive test coverage for OCR functionality\n- **0.1.1** (2025-02-19): Added Tesseract OCR support for text extraction from images (optional feature)\n- **0.1.0** (2025-02-19): Initial release with Anthropic and OpenAI vision support\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recognition",
        "images",
        "vision",
        "extraction images",
        "image recognition",
        "analyze images"
      ],
      "category": "image-and-video-generation"
    },
    "mario-andreschak--mcp-veo2": {
      "owner": "mario-andreschak",
      "name": "mcp-veo2",
      "url": "https://github.com/mario-andreschak/mcp-veo2",
      "imageUrl": "/freedevtools/mcp/pfp/mario-andreschak.webp",
      "description": "Generates high-quality videos from text prompts or images using Google's Veo2 model and provides access to these generated videos through MCP resources.",
      "stars": 30,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:32:31Z",
      "readme_content": "# MCP Video Generation with Veo2\n\n[![smithery badge](https://smithery.ai/badge/@mario-andreschak/mcp-video-generation-veo2)](https://smithery.ai/server/@mario-andreschak/mcp-video-generation-veo2)\n\nThis project implements a Model Context Protocol (MCP) server that exposes Google's Veo2 video generation capabilities. It allows clients to generate videos from text prompts or images, and access the generated videos through MCP resources.\n\n<a href=\"https://glama.ai/mcp/servers/@mario-andreschak/mcp-veo2\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mario-andreschak/mcp-veo2/badge\" alt=\"Video Generation with Veo2 MCP server\" />\n</a>\n\n## Features\n\n- Generate **videos from text** prompts\n- Generate **videos from images**\n- Access generated videos through MCP resources\n- Example video generation templates\n- Support for both stdio and SSE transports\n\n## Example Images\n![1dec9c71-07dc-4a6e-9e17-8da355d72ba1](https://github.com/user-attachments/assets/ba987d14-dd46-49ac-9b31-1ce398e86c6f)\n\n\n## Example Image to Video\n[Image to Video - from Grok generated puppy](https://github.com/mario-andreschak/mcp-veo2/raw/refs/heads/main/example-files/2a6a0807-d323-4424-a48a-e40a82b883bb.mp4)\n\n[Image to Video - from real cat](https://github.com/mario-andreschak/mcp-veo2/raw/refs/heads/main/example-files/55b9f28b-61a6-423e-bb86-f3791c639177.mp4)\n\n\n## Prerequisites\n\n- Node.js 18 or higher\n- Google API key with access to Gemini API and Veo2 model (= You need to set up a credit card with your API key! -> Go to aistudio.google.com )\n\n## Installation\n\n### Installing in [FLUJO](https://github.com/mario-andreschak/FLUJO/)\n1. Click Add Server\n2. Copy & Paste Github URL into FLUJO\n3. Click Parse, Clone, Install, Build and Save.\n\n### Installing via Smithery\n\nTo install mcp-video-generation-veo2 for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mario-andreschak/mcp-veo2):\n\n```bash\nnpx -y @smithery/cli install @mario-andreschak/mcp-veo2 --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/yourusername/mcp-video-generation-veo2.git\n   cd mcp-video-generation-veo2\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file with your Google API key:\n   ```bash\n   cp .env.example .env\n   # Edit .env and add your Google API key\n   ```\n\n   The `.env` file supports the following variables:\n   - `GOOGLE_API_KEY`: Your Google API key (required)\n   - `PORT`: Server port (default: 3000)\n   - `STORAGE_DIR`: Directory for storing generated videos (default: ./generated-videos)\n   - `LOG_LEVEL`: Logging level (default: fatal)\n     - Available levels: verbose, debug, info, warn, error, fatal, none\n     - For development, set to `debug` or `info` for more detailed logs\n     - For production, keep as `fatal` to minimize console output\n\n4. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Usage\n\n### Starting the Server\n\nYou can start the server with either stdio or SSE transport:\n\n#### stdio Transport (Default)\n\n```bash\nnpm start\n# or\nnpm start stdio\n```\n\n#### SSE Transport\n\n```bash\nnpm start sse\n```\n\nThis will start the server on port 3000 (or the port specified in your `.env` file).\n\n### MCP Tools\n\nThe server exposes the following MCP tools:\n\n#### generateVideoFromText\n\nGenerates a video from a text prompt.\n\nParameters:\n- `prompt` (string): The text prompt for video generation\n- `config` (object, optional): Configuration options\n  - `aspectRatio` (string, optional): \"16:9\" or \"9:16\"\n  - `personGeneration` (string, optional): \"dont_allow\" or \"allow_adult\"\n  - `numberOfVideos` (number, optional): 1 or 2\n  - `durationSeconds` (number, optional): Between 5 and 8\n  - `enhancePrompt` (boolean, optional): Whether to enhance the prompt\n  - `negativePrompt` (string, optional): Text describing what not to generate\n\nExample:\n```json\n{\n  \"prompt\": \"Panning wide shot of a serene forest with sunlight filtering through the trees, cinematic quality\",\n  \"config\": {\n    \"aspectRatio\": \"16:9\",\n    \"personGeneration\": \"dont_allow\",\n    \"durationSeconds\": 8\n  }\n}\n```\n\n#### generateVideoFromImage\n\nGenerates a video from an image.\n\nParameters:\n- `image` (string): Base64-encoded image data\n- `prompt` (string, optional): Text prompt to guide the video generation\n- `config` (object, optional): Configuration options (same as above, but personGeneration only supports \"dont_allow\")\n\n#### listGeneratedVideos\n\nLists all generated videos.\n\n### MCP Resources\n\nThe server exposes the following MCP resources:\n\n#### videos://{id}\n\nAccess a generated video by its ID.\n\n#### videos://templates\n\nAccess example video generation templates.\n\n## Development\n\n### Project Structure\n\n- `src/`: Source code\n  - `index.ts`: Main entry point\n  - `server.ts`: MCP server configuration\n  - `config.ts`: Configuration handling\n  - `tools/`: MCP tool implementations\n  - `resources/`: MCP resource implementations\n  - `services/`: External service integrations\n  - `utils/`: Utility functions\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Development Mode\n\n```bash\nnpm run dev\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "veo2",
        "videos",
        "mcp",
        "mcp veo2",
        "google veo2",
        "generated videos"
      ],
      "category": "image-and-video-generation"
    },
    "mikeyny--ai-image-gen-mcp": {
      "owner": "mikeyny",
      "name": "ai-image-gen-mcp",
      "url": "https://github.com/mikeyny/ai-image-gen-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikeyny.webp",
      "description": "Generate images from text prompts using Replicate's flux-schnell model, with configurable image parameters and the ability to save generated images to a specified directory.",
      "stars": 126,
      "forks": 15,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T02:43:44Z",
      "readme_content": "# Image Generation MCP Server\n\nAn [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) server implementation for generating images using Replicate's [`black-forest-labs/flux-schnell`](https://replicate.com/black-forest-labs/flux-schnell) model.\n\nIdeally to be used with Cursor's MCP feature, but can be used with any MCP client.\n\n## Features\n\n- Generate images from text prompts\n- Configurable image parameters (resolution, aspect ratio, quality)\n- Save generated images to specified directory\n- Full MCP protocol compliance\n- Error handling and validation\n\n## Prerequisites\n\n- Node.js 16+\n- Replicate API token\n- TypeScript SDK for MCP\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Add your Replicate API token directly in the code at `src/imageService.ts` by updating the `apiToken` constant:\n   ```bash\n   // No environment variables are used since they can't be easily set in cursor\n   const apiToken = \"your-replicate-api-token-here\";\n   ```\n\n   > **Note:** If using with Claude, you can create a `.env` file in the root directory and set your API token there:\n   ```bash\n   REPLICATE_API_TOKEN=your-replicate-api-token-here\n   ```\n\n   Then build the project:\n   ```bash\n   npm run build\n   ```\n\n## Usage\n\nTo use with cursor:\n1. Go to Settings\n2. Select Features\n3. Scroll down to \"MCP Servers\"\n4. Click \"Add new MCP Server\"\n5. Set Type to \"Command\"\n6. Set Command to: `node ./path/to/dist/server.js`\n\n## API Parameters\n\n| Parameter           | Type    | Required | Default | Description                                     |\n|--------------------|---------|----------|---------|------------------------------------------------|\n| `prompt`           | string  | Yes      | -       | Text prompt for image generation               |\n| `output_dir`       | string  | Yes      | -       | Server directory path to save generated images |\n| `go_fast`          | boolean | No       | false   | Enable faster generation mode                  |\n| `megapixels`       | string  | No       | \"1\"     | Resolution quality (\"1\", \"2\", \"4\")            |\n| `num_outputs`      | number  | No       | 1       | Number of images to generate (1-4)            |\n| `aspect_ratio`     | string  | No       | \"1:1\"   | Aspect ratio (\"1:1\", \"4:3\", \"16:9\")          |\n| `output_format`    | string  | No       | \"webp\"  | Image format (\"webp\", \"png\", \"jpeg\")         |\n| `output_quality`   | number  | No       | 80      | Compression quality (1-100)                   |\n| `num_inference_steps`| number| No       | 4       | Number of denoising steps (4-20)             |\n\n## Example Request\n\n```json\n{\n  \"prompt\": \"black forest gateau cake spelling out 'FLUX SCHNELL'\",\n  \"output_dir\": \"/var/output/images\",\n  \"filename\": \"black_forest_cake\",\n  \"output_format\": \"webp\"\n  \"go_fast\": true,\n  \"megapixels\": \"1\",\n  \"num_outputs\": 2,\n  \"aspect_ratio\": \"1:1\"\n}\n```\n\n## Example Response\n\n```json\n{\n  \"image_paths\": [\n    \"/var/output/images/output_0.webp\",\n    \"/var/output/images/output_1.webp\"\n  ],\n  \"metadata\": {\n    \"model\": \"black-forest-labs/flux-schnell\",\n    \"inference_time_ms\": 2847\n  }\n}\n```\n\n## Error Handling\n\nThe server handles the following error types:\n\n- Validation errors (invalid parameters)\n- API errors (Replicate API issues)\n- Server errors (filesystem, permissions)\n- Unknown errors (unexpected issues)\n\nEach error response includes:\n- Error code\n- Human-readable message\n- Detailed error information\n\n## License\n\nISC ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "generate",
        "images",
        "mcp",
        "generate images",
        "generated images",
        "image gen"
      ],
      "category": "image-and-video-generation"
    },
    "murataskin--image-mcp-server-gemini": {
      "owner": "murataskin",
      "name": "image-mcp-server-gemini",
      "url": "https://github.com/murataskin/image-mcp-server-gemini",
      "imageUrl": "/freedevtools/mcp/pfp/murataskin.webp",
      "description": "Analyzes images and videos by providing URLs or local file paths, allowing for detailed insights and descriptions of the content. Uses the Gemini 2.0 Flash model for high-precision recognition and can evaluate relationships between multiple visual inputs.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-10T03:32:38Z",
      "readme_content": "\n# image-mcp-server-gemini\n\n\n\n\n[![smithery badge](https://smithery.ai/badge/@Rentapad/image-mcp-server-gemini)](https://smithery.ai/server/@Rentapad/image-mcp-server-gemini)\nAn MCP server that receives image/video URLs or local file paths and analyzes their content using the Gemini 2.0 Flash model.(forked from github.com/champierre/image-mcp-server)\n\n## Features\n\n- Analyzes content from one or more image/video URLs or local file paths.\n- Analyzes videos directly from YouTube URLs.\n- Can analyze relationships between multiple images or videos provided together.\n- Supports optional text prompts to guide the analysis.\n- High-precision recognition and description using the Gemini 2.0 Flash model.\n- URL validity checking and local file loading with Base64 encoding.\n- Basic security checks for local file paths.\n- Handles various image and video MIME types (see Usage section for details).\n\n## Installation\n\n### Installing via Smithery\n\nTo install Image Analysis Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Rentapad/image-mcp-server-gemini):\n\n```bash\nnpx -y @smithery/cli install @Rentapad/image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Rentapad/image-mcp-server-gemini.git \ncd image-mcp-server-gemini\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript\nnpm run build\n```\n\n## Configuration\n\nTo use this server, you need a Gemini API key. Set the following environment variable:\n\n```\nGEMINI_API_KEY=your_gemini_api_key\n```\n\n## MCP Server Configuration\n\nTo use with tools like Cline, add the following settings to your MCP server configuration file:\n\n### For Cline\n\nAdd the following to `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-video-analysis\": { // Consider renaming for clarity\n      \"command\": \"node\",\n      \"args\": [\"/path/to/image-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_gemini_api_key\"\n      }\n    }\n  }\n}\n```\n\n### For Claude Desktop App\n\nAdd the following to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"image-video-analysis\": { // Consider renaming for clarity\n      \"command\": \"node\",\n      \"args\": [\"/path/to/image-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_gemini_api_key\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nOnce the MCP server is configured, the following tools become available:\n\n- `analyze_image`: Receives one or more image URLs and analyzes their content.\n  - Arguments: `imageUrls` (array of strings, required), `prompt` (string, optional).\n- `analyze_image_from_path`: Receives one or more local image file paths and analyzes their content.\n  - Arguments: `imagePaths` (array of strings, required), `prompt` (string, optional).\n- `analyze_video`: Receives one or more video URLs and analyzes their content. Best for smaller videos (see Video Notes).\n  - Arguments: `videoUrls` (array of strings, required), `prompt` (string, optional).\n- `analyze_video_from_path`: Receives one or more local video file paths and analyzes their content. Best for smaller videos (see Video Notes).\n  - Arguments: `videoPaths` (array of strings, required), `prompt` (string, optional).\n- `analyze_youtube_video`: Receives a single YouTube video URL and analyzes its content.\n  - Arguments: `youtubeUrl` (string, required), `prompt` (string, optional).\n\n### Usage Examples\n\n**Analyzing a single image from URL:**\n```\nPlease analyze this image: https://example.com/image.jpg\n```\n\n**Analyzing multiple images from local paths and comparing them:**\n```\nAnalyze these images: /path/to/your/image1.png, /path/to/your/image2.jpeg. Which one contains a cat?\n```\n*(The client would call `analyze_image_from_path` with `imagePaths: [\"/path/to/your/image1.png\", \"/path/to/your/image2.jpeg\"]` and `prompt: \"Which one contains a cat?\"`)*\n\n**Analyzing a video from URL with a specific prompt:**\n```\nSummarize the content of this video: https://example.com/video.mp4\n```\n*(The client would call `analyze_video` with `videoUrls: [\"https://example.com/video.mp4\"]` and `prompt: \"Summarize the content of this video\"`)*\n\n**Analyzing a YouTube video:**\n```\nWhat is the main topic of this YouTube video? https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n*(The client would call `analyze_youtube_video` with `youtubeUrl: \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"` and `prompt: \"What is the main topic of this YouTube video?\"`)*\n\n### Video Notes\n\n- **Size Limit:** For videos provided via URL (`analyze_video`) or path (`analyze_video_from_path`), Gemini currently has limitations on the size of video data that can be processed directly (typically around 20MB after Base64 encoding). Larger videos may fail. YouTube analysis does not have this same client-side download limit.\n- **Supported MIME Types:** The server attempts to map and use MIME types supported by Gemini for video. Officially supported types include: `video/mp4`, `video/mpeg`, `video/mov`, `video/avi`, `video/x-flv`, `video/mpg`, `video/webm`, `video/wmv`, `video/3gpp`. Files with other MIME types might be skipped. YouTube videos are handled separately.\n\n### Note: Specifying Local File Paths\n\nWhen using the `..._from_path` tools, the AI assistant (client) must specify **valid file paths in the environment where this server is running**.\n\n- **If the server is running on WSL:**\n  - If the AI assistant has a Windows path (e.g., `C:\\...`), it needs to convert it to a WSL path (e.g., `/mnt/c/...`) before passing it to the tool.\n  - If the AI assistant has a WSL path, it can pass it as is.\n- **If the server is running on Windows:**\n  - If the AI assistant has a WSL path (e.g., `/home/user/...`), it needs to convert it to a UNC path (e.g., `\\\\wsl$\\Distro\\...`) before passing it to the tool.\n  - If the AI assistant has a Windows path, it can pass it as is.\n\n**Path conversion is the responsibility of the AI assistant (or its execution environment).** The server will try to interpret the received path as is, applying basic security checks.\n\n### Note: Type Errors During Build\n\nWhen running `npm run build`, you may see an error (TS7016) about missing TypeScript type definitions for the `mime-types` module.\n\n```\nsrc/index.ts:16:23 - error TS7016: Could not find a declaration file for module 'mime-types'. ...\n```\n\nThis is a type checking error, and since the JavaScript compilation itself succeeds, it **does not affect the server's execution**. If you want to resolve this error, install the type definition file as a development dependency.\n\n```bash\nnpm install --save-dev @types/mime-types\n# or\nyarn add --dev @types/mime-types\n```\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev\n```\n\n## License\n\nMIT\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flash",
        "images",
        "gemini",
        "gemini flash",
        "server gemini",
        "image mcp"
      ],
      "category": "image-and-video-generation"
    },
    "nickbaumann98--everart-forge-mcp": {
      "owner": "nickbaumann98",
      "name": "everart-forge-mcp",
      "url": "https://github.com/nickbaumann98/everart-forge-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nickbaumann98.webp",
      "description": "Generates and converts vector and raster images using advanced AI models with support for multiple formats. Provides flexible storage options and automatic formatting for efficient image processing.",
      "stars": 10,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-10T04:55:11Z",
      "readme_content": "# EverArt Forge MCP for Cline\n\n![EverArt Forge MCP](icon.svg)\n\nAn advanced Model Context Protocol (MCP) server for [Cline](https://github.com/cline/cline) that integrates with EverArt's AI models to generate both vector and raster images. This server provides powerful image generation capabilities with flexible storage options and format conversion.\n\n## Features\n\n- **Vector Graphics Generation**\n  - Create SVG vector graphics using Recraft-Vector model\n  - Automatic SVG optimization\n  - Perfect for logos, icons, and scalable graphics\n\n- **Raster Image Generation**\n  - Support for PNG, JPEG, and WebP formats\n  - Multiple AI models for different styles\n  - High-quality image processing\n\n- **Flexible Storage**\n  - Custom output paths and filenames\n  - Automatic directory creation\n  - Format validation and extension handling\n  - Web project integration\n\n## Available Models\n\n- **5000:FLUX1.1**: Standard quality, general-purpose image generation\n- **9000:FLUX1.1-ultra**: Ultra high quality for detailed images\n- **6000:SD3.5**: Stable Diffusion 3.5 for diverse styles\n- **7000:Recraft-Real**: Photorealistic style\n- **8000:Recraft-Vector**: Vector art style (SVG output)\n\n## Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/nickbaumann98/everart-forge-mcp.git\n   cd everart-forge-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n4. Get your EverArt API key:\n   - Sign up at [EverArt](https://everart.ai/) \n   - Navigate to your account settings\n   - Create or copy your API key\n\n5. Add the server to your Cline MCP settings file:\n\n   **For VS Code Extension**:  \n   Edit `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"everart-forge\": {\n         \"command\": \"node\",\n         \"args\": [\"/absolute/path/to/everart-forge-mcp/build/index.js\"],\n         \"env\": {\n           \"EVERART_API_KEY\": \"your_api_key_here\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   **For Claude Desktop App**:  \n   Edit `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or appropriate location for your OS\n\n6. Restart Cline to load the new MCP server\n\n## Usage Examples\n\nOnce configured, you can use Cline to generate images with prompts like:\n\n- \"Generate a minimalist tech logo in SVG format using the Recraft-Vector model\"\n- \"Create a photorealistic landscape image with the FLUX1.1-ultra model\"\n- \"Make me a vector icon for my project that represents artificial intelligence\"\n- \"Generate a professional company logo as an SVG file and save it to my desktop\"\n\n### Tool Capabilities\n\nThe server provides these tools:\n\n#### generate_image\n\nGenerate images with extensive customization options:\n\n```\nParameters:\n- prompt (required): Text description of desired image\n- model: Model ID (5000:FLUX1.1, 9000:FLUX1.1-ultra, 6000:SD3.5, 7000:Recraft-Real, 8000:Recraft-Vector)\n- format: Output format (svg, png, jpg, webp)\n- output_path: Custom output path for the image\n- web_project_path: Path to web project root for proper asset organization\n- project_type: Web project type (react, vue, html, next, etc.)\n- asset_path: Subdirectory within the web project assets\n- image_count: Number of images to generate (1-10)\n```\n\nNotes:\n- SVG format is only available with Recraft-Vector (8000) model\n- Default format is \"svg\" for model 8000, \"png\" for others\n- You can specify combined model IDs (e.g., \"8000:Recraft-Vector\")\n\n#### list_images\n\nList all previously generated images stored by the server.\n\n#### view_image\n\nOpen a specific image in the default image viewer:\n\n```\nParameters:\n- filename: Name of the image file to view\n```\n\n## Troubleshooting\n\n- **Error: Invalid model ID**: Make sure you're using one of the supported model IDs (5000, 6000, 7000, 8000, 9000)\n- **Format not compatible with model**: SVG format is only available with Recraft-Vector (8000) model\n- **Image not found**: Use the list_images tool to see available images\n- **API authentication failed**: Check your EverArt API key\n- **Images not appearing**: Check file permissions and paths\n\n## License\n\nMIT License - see LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "raster",
        "ai",
        "video generation",
        "raster images",
        "image processing"
      ],
      "category": "image-and-video-generation"
    },
    "noeltg77--Replicate-Designer": {
      "owner": "noeltg77",
      "name": "Replicate-Designer",
      "url": "https://github.com/noeltg77/Replicate-Designer",
      "imageUrl": "/freedevtools/mcp/pfp/noeltg77.webp",
      "description": "Generate images from text descriptions using Replicate's Flux 1.1 Pro model. Designed for seamless integration in projects requiring high-quality image generation capabilities.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-31T20:07:00Z",
      "readme_content": "# Replicate Designer MCP\n\nAn MCP server for generating images using Replicate's Flux 1.1 Pro model.\n\n## Installation\n\n### Using Directly from GitHub\n\nYou can use the MCP server directly from GitHub in several ways:\n\n#### Option 1: Install directly with pip\n\n```bash\npip install git+https://github.com/yourusername/replicate-designer.git\n```\n\nThen run it with:\n```bash\nmcp-replicate-designer\n```\n\n#### Option 2: Use npx with GitHub repository\n\nCreate a configuration file (e.g., `mcps.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"replicateDesigner\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\", \n        \"github:yourusername/replicate-designer\"\n      ],\n      \"env\": {\n        \"REPLICATE_API_TOKEN\": \"your_replicate_api_token_here\"\n      }\n    }\n  }\n}\n```\n\nThen use it with Claude or another assistant:\n```bash\nnpx @anthropic-ai/assistant --mcps-json mcps.json\n```\n\nThis method allows you to include your Replicate API token directly in the configuration file, which is more convenient than setting environment variables separately.\n\n#### Option 3: Local Installation\n\nClone the repository and install from the local directory:\n\n```bash\ngit clone https://github.com/yourusername/replicate-designer.git\ncd replicate-designer\npip install -e .\n```\n\n### Publishing and Using via npm\n\nTo make your MCP available via npm (for easier distribution):\n\n1. Package and publish your MCP:\n```bash\n# Build a wheel\npip install build\npython -m build\n\n# Publish to npm (after setting up an npm account)\nnpm init\nnpm publish\n```\n\n2. Then users can install and use it directly:\n```bash\nnpx -y mcp-replicate-designer\n```\n\n## Usage\n\n### Setting the API Token\n\nThere are several ways to provide your Replicate API token:\n\n1. **Environment variable** (for command line usage):\n   ```bash\n   export REPLICATE_API_TOKEN=your_api_token_here\n   ```\n\n2. **In the MCP configuration file** (as shown in Option 2 above):\n   ```json\n   {\n     \"mcpServers\": {\n       \"replicateDesigner\": {\n         \"command\": \"...\",\n         \"args\": [\"...\"],\n         \"env\": {\n           \"REPLICATE_API_TOKEN\": \"your_replicate_api_token_here\"\n         }\n       }\n     }\n   }\n   ```\n\n3. **Using a .env file** in your project directory:\n   ```\n   REPLICATE_API_TOKEN=your_api_token_here\n   ```\n   \n   Then, install the python-dotenv package:\n   ```bash\n   pip install python-dotenv\n   ```\n\n> **Security Note**: Be careful with your API tokens. Never commit them to public repositories, and use environment variables or secure secret management when possible.\n\n### Running the MCP server\n\n```bash\nmcp-replicate-designer\n```\n\nBy default, it runs in stdio mode which is compatible with npx use. You can also run it in SSE mode:\n\n```bash\nmcp-replicate-designer --transport sse --port 8000\n```\n\n## Using with npx\n\nThis MCP can be used with an AI agent using npx in two ways:\n\n### Direct command line\n\n```bash\nnpx @anthropic-ai/assistant --mcp mcp-replicate-designer\n```\n\n### As a configuration object\n\nIn your configuration JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"replicateDesigner\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-replicate-designer\"\n      ]\n    }\n  }\n}\n```\n\nThen use it with:\n\n```bash\nnpx @anthropic-ai/assistant --mcps-json /path/to/your/config.json\n```\n\n## Tool\n\nThis MCP exposes a single tool:\n\n### generate_image\n\nGenerates an image using Replicate's Flux 1.1 Pro model.\n\n**Parameters:**\n\n- `prompt` (string, required): Text description of the image to generate\n- `aspect_ratio` (string, optional, default: \"1:1\"): Aspect ratio for the generated image\n- `output_format` (string, optional, default: \"webp\"): Format of the output image\n- `output_quality` (integer, optional, default: 80): Quality of the output image (1-100)\n- `safety_tolerance` (integer, optional, default: 2): Safety tolerance level (0-3)\n- `prompt_upsampling` (boolean, optional, default: true): Whether to use prompt upsampling\n\n**Example:**\n\n```json\n{\n  \"prompt\": \"A photograph of an humanoid AI agent looking sad and in disrepair, the agent is sat at a workbench getting fixed by a human male\",\n  \"aspect_ratio\": \"1:1\",\n  \"output_format\": \"webp\"\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "replicate",
        "generate",
        "images",
        "image generation",
        "generate images",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "peng-shawn--mermaid-mcp-server": {
      "owner": "peng-shawn",
      "name": "mermaid-mcp-server",
      "url": "https://github.com/peng-shawn/mermaid-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/peng-shawn.webp",
      "description": "Converts Mermaid diagram descriptions into high-quality PNG images using the Mermaid markdown syntax. Supports customizable themes and backgrounds for visual representations of data and processes.",
      "stars": 185,
      "forks": 21,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:19Z",
      "readme_content": "# Mermaid MCP Server\n\nA Model Context Protocol (MCP) server that converts Mermaid diagrams to PNG images or SVG files. This server allows AI assistants and other applications to generate visual diagrams from textual descriptions using the Mermaid markdown syntax.\n\n## Features\n\n- Converts Mermaid diagram code to PNG images or SVG files\n- Supports multiple diagram themes (default, forest, dark, neutral)\n- Customizable background colors\n- Uses Puppeteer for high-quality headless browser rendering\n- Implements the MCP protocol for seamless integration with AI assistants\n- Flexible output options: return images/SVG directly or save to disk\n- Error handling with detailed error messages\n\n## How It Works\n\nThe server uses Puppeteer to launch a headless browser, render the Mermaid diagram to SVG, and optionally capture a screenshot of the rendered diagram. The process involves:\n\n1. Launching a headless browser instance\n2. Creating an HTML template with the Mermaid code\n3. Loading the Mermaid.js library\n4. Rendering the diagram to SVG\n5. Either saving the SVG directly or taking a screenshot as PNG\n6. Either returning the image/SVG directly or saving it to disk\n\n## Build\n\n```bash\nnpx tsc\n```\n\n## Usage\n\n### Use with Claude desktop\n\n```json\n{\n  \"mcpServers\": {\n    \"mermaid\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@peng-shawn/mermaid-mcp-server\"]\n    }\n  }\n}\n```\n\n### Use with Cursor and Cline\n\n```bash\nenv CONTENT_IMAGE_SUPPORTED=false npx -y @peng-shawn/mermaid-mcp-server\n```\n\nYou can find a list of mermaid diagrams under `./diagrams`, they are created using Cursor agent with prompt: \"generate mermaid diagrams and save them in a separate diagrams folder explaining how renderMermaidPng work\"\n\n### Run with inspector\n\nRun the server with inspector for testing and debugging:\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nThe server will start and listen on stdio for MCP protocol messages.\n\nLearn more about inspector [here](https://modelcontextprotocol.io/docs/tools/inspector).\n\n### Installing via Smithery\n\nTo install Mermaid Diagram Generator for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@peng-shawn/mermaid-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @peng-shawn/mermaid-mcp-server --client claude\n```\n\n### Docker and Smithery Environments\n\nWhen running in Docker containers (including via Smithery), you may need to handle Chrome dependencies:\n\n1. The server now attempts to use Puppeteer's bundled browser by default\n2. If you encounter browser-related errors, you have two options:\n\n   **Option 1: During Docker image build:**\n\n   - Set `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true` when installing Puppeteer\n   - Install Chrome/Chromium in your Docker container\n   - Set `PUPPETEER_EXECUTABLE_PATH` at runtime to point to the Chrome installation\n\n   **Option 2: Use Puppeteer's bundled Chrome:**\n\n   - Ensure your Docker container has the necessary dependencies for Chrome\n   - No need to set `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD`\n   - The code will use the bundled browser automatically\n\nFor Smithery users, the latest version should work without additional configuration.\n\n## API\n\nThe server exposes a single tool:\n\n- `generate`: Converts Mermaid diagram code to a PNG image or SVG file\n  - Parameters:\n    - `code`: The Mermaid diagram code to render\n    - `theme`: (optional) Theme for the diagram. Options: \"default\", \"forest\", \"dark\", \"neutral\"\n    - `backgroundColor`: (optional) Background color for the diagram, e.g. 'white', 'transparent', '#F0F0F0'\n    - `outputFormat`: (optional) Output format for the diagram. Options: \"png\", \"svg\" (defaults to \"png\")\n    - `name`: Name for the generated file (required when CONTENT_IMAGE_SUPPORTED=false)\n    - `folder`: Absolute path to save the image/SVG to (required when CONTENT_IMAGE_SUPPORTED=false)\n\nThe behavior of the `generate` tool depends on the `CONTENT_IMAGE_SUPPORTED` environment variable:\n\n- When `CONTENT_IMAGE_SUPPORTED=true` (default): The tool returns the image/SVG directly in the response\n- When `CONTENT_IMAGE_SUPPORTED=false`: The tool saves the image/SVG to the specified folder and returns the file path\n\n## Environment Variables\n\n- `CONTENT_IMAGE_SUPPORTED`: Controls whether images are returned directly in the response or saved to disk\n  - `true` (default): Images are returned directly in the response\n  - `false`: Images are saved to disk, requiring `name` and `folder` parameters\n\n## Examples\n\n### Basic Usage\n\n```javascript\n// Generate a flowchart with default settings\n{\n  \"code\": \"flowchart TD\\n    A[Start] --> B{Is it?}\\n    B -->|Yes| C[OK]\\n    B -->|No| D[End]\"\n}\n```\n\n### With Theme and Background Color\n\n```javascript\n// Generate a sequence diagram with forest theme and light gray background\n{\n  \"code\": \"sequenceDiagram\\n    Alice->>John: Hello John, how are you?\\n    John-->>Alice: Great!\",\n  \"theme\": \"forest\",\n  \"backgroundColor\": \"#F0F0F0\"\n}\n```\n\n### Saving to Disk (when CONTENT_IMAGE_SUPPORTED=false)\n\n```javascript\n// Generate a class diagram and save it to disk as PNG\n{\n  \"code\": \"classDiagram\\n    Class01 <|-- AveryLongClass\\n    Class03 *-- Class04\\n    Class05 o-- Class06\",\n  \"theme\": \"dark\",\n  \"name\": \"class_diagram\",\n  \"folder\": \"/path/to/diagrams\"\n}\n```\n\n### Generating SVG Output\n\n```javascript\n// Generate a state diagram as SVG\n{\n  \"code\": \"stateDiagram-v2\\n    [*] --> Still\\n    Still --> [*]\\n    Still --> Moving\\n    Moving --> Still\\n    Moving --> Crash\\n    Crash --> [*]\",\n  \"outputFormat\": \"svg\",\n  \"name\": \"state_diagram\",\n  \"folder\": \"/path/to/diagrams\"\n}\n```\n\n## FAQ\n\n### Doesn't Claude desktop already support mermaid via canvas?\n\nYes, but it doesn't support the `theme` and `backgroundColor` options. Plus, having a dedicated server makes it easier to create mermaid diagrams with different MCP clients.\n\n### Why do I need to specify CONTENT_IMAGE_SUPPORTED=false when using with Cursor?\n\nCursor doesn't support inline images in responses yet.\n\n## Publishing\n\nThis project uses GitHub Actions to automate the publishing process to npm.\n\n### Method 1: Using the Release Script (Recommended)\n\n1. Make sure all your changes are committed and pushed\n2. Run the release script with either a specific version number or a semantic version increment:\n\n   ```bash\n   # Using a specific version number\n   npm run release 0.1.4\n\n   # Using semantic version increments\n   npm run release patch  # Increments the patch version (e.g., 0.1.3 → 0.1.4)\n   npm run release minor  # Increments the minor version (e.g., 0.1.3 → 0.2.0)\n   npm run release major  # Increments the major version (e.g., 0.1.3 → 1.0.0)\n   ```\n\n3. The script will:\n   - Validate the version format or semantic increment\n   - Check if you're on the main branch\n   - Detect and warn about version mismatches between files\n   - Update all version references consistently (package.json, package-lock.json, and index.ts)\n   - Create a single commit with all version changes\n   - Create and push a git tag\n   - The GitHub workflow will then automatically build and publish to npm\n\n### Method 2: Manual Process\n\n1. Update your code and commit the changes\n2. Create and push a new tag with the version number:\n   ```bash\n   git tag v0.1.4  # Use the appropriate version number\n   git push origin v0.1.4\n   ```\n3. The GitHub workflow will automatically:\n   - Build the project\n   - Publish to npm with the version from the tag\n\nNote: You need to set up the `NPM_TOKEN` secret in your GitHub repository settings. To do this:\n\n1. Generate an npm access token with publish permissions\n2. Go to your GitHub repository → Settings → Secrets and variables → Actions\n3. Create a new repository secret named `NPM_TOKEN` with your npm token as the value\n\n## Badges\n\n[![smithery badge](https://smithery.ai/badge/@peng-shawn/mermaid-mcp-server)](https://smithery.ai/server/@peng-shawn/mermaid-mcp-server)\n\n<a href=\"https://glama.ai/mcp/servers/lzjlbitkzr\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lzjlbitkzr/badge\" alt=\"mermaid-mcp-server MCP server\" />\n</a>\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mermaid",
        "png",
        "images",
        "mermaid diagram",
        "mermaid markdown",
        "mermaid mcp"
      ],
      "category": "image-and-video-generation"
    },
    "philipp-eisen--modal-mcp-toolbox": {
      "owner": "philipp-eisen",
      "name": "modal-mcp-toolbox",
      "url": "https://github.com/philipp-eisen/modal-mcp-toolbox",
      "imageUrl": "/freedevtools/mcp/pfp/philipp-eisen.webp",
      "description": "A collection of tools that provides a sandboxed environment for executing Python code and generating images using the FLUX model.",
      "stars": 22,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-12T21:44:25Z",
      "readme_content": "# Modal MCP Toolbox 🛠️\n\n[![smithery badge](https://smithery.ai/badge/@philipp-eisen/modal-mcp-toolbox)](https://smithery.ai/server/@philipp-eisen/modal-mcp-toolbox)\n\nA collection of Model Context Protocol (MCP) tools that run on Modal.\nThis let's you extend the capabilities of your LLM in tools such as [Goose](https://block.github.io/goose/) or the [Claude Desktop App](https://claude.ai/download).\n\n<a href=\"https://glama.ai/mcp/servers/ai78w0p5mc\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ai78w0p5mc/badge\" alt=\"Modal Toolbox MCP server\" /></a>\n\n## Tools\n\n- `run_python_code_in_sandbox`: Let's you run python code in a sandboxed environment.\n- `generate_flux_image`: Generate an image using the FLUX model.\n\n## Demo\n\n### Flux Image Generation\n\n![🎬Flux Image Generation](./assets/flux.gif)\n\n### Python Code Execution\n\n![🎬Python Code Execution](./assets/python-sandbox.gif)\n\n## Prerequisites\n\n- A [modal account](https://modal.com/signup) and a configured modal CLI.\n- [UV](https://github.com/astral-sh/uv?tab=readme-ov-file#installation)\n- A client that supports MCP. Such as the [Claude Desktop App](https://claude.ai/download) or [Goose](https://block.github.io/goose/)\n\nThis runs against your modal account, so you will need to have a modal account and be logged in.\n\n## Installation\n\nInstallation depends on the client that uses the MCP. Here is instructions for Claude and Goose.\n\n### Claude\n\nGot to `Settings > Developer` in the Claude Desktop App. And click on Edit Config.\n![🖼️Claude Settings](./assets/claude-settings.png)\n\nAdd the config for the mcp server. My config looks like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"modal-toolbox\": {\n      \"command\": \"uvx\",\n      \"args\": [\"modal-mcp-toolbox\"]\n    }\n  }\n}\n```\n\n### Goose\n\nGo to `Settings` and Click on Add.\n\n![🖼️Goose Settings](./assets/goose-settings-1.png)\n\nThen add an extension like in the screenshot below.\nThe important part is to set command to:\n\n```\nuvx modal-mcp-toolbox\n```\n\nThe rest you can fill in as you like.\n\n![🖼️Goose MCP Settings](./assets/goose-settings-2.png)\n\n### Installing via Smithery (not working currently)\n\nTo install Modal MCP Toolbox for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@philipp-eisen/modal-mcp-toolbox):\n\n```bash\nnpx -y @smithery/cli install @philipp-eisen/modal-mcp-toolbox --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "toolbox",
        "modal",
        "modal mcp",
        "generating images",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "pinkpixel-dev--MCPollinations": {
      "owner": "pinkpixel-dev",
      "name": "MCPollinations",
      "url": "https://github.com/pinkpixel-dev/MCPollinations",
      "imageUrl": "/freedevtools/mcp/pfp/pinkpixel-dev.webp",
      "description": "Generates images, text, and audio from prompts using the Pollinations APIs. It supports returning images as base64-encoded data and allows listing available models for image and text generation.",
      "stars": 34,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T03:37:33Z",
      "readme_content": "# MCPollinations Multimodal MCP Server\nA Model Context Protocol (MCP) server that enables AI assistants to generate images, text, and audio through the Pollinations APIs\n\n[![smithery badge](https://smithery.ai/badge/@pinkpixel-dev/mcpollinations)](https://smithery.ai/server/@pinkpixel-dev/mcpollinations) [![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/8448e4ec-c863-476a-8adb-aed3cf16ea2b)\n\n## Features\n\n- Generate image URLs from text prompts\n- Generate images and return them as base64-encoded data AND save as png, jpeg, jpg, or webp (default: png)\n- Generate text responses from text prompts\n- Generate audio responses from text prompts\n- List available image and text generation models\n- No authentication required\n- Simple and lightweight\n- Compatible with the Model Context Protocol (MCP)\n\n## System Requirements\n\n- **Node.js**: Version 14.0.0 or higher\n  - For best performance, we recommend Node.js 16.0.0 or higher\n  - Node.js versions below 16 use an AbortController polyfill\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install mcpollinations for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/mcpollinations):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/mcpollinations --client claude\n```\n\nThe easiest way to use the MCP server:\n\n```bash\n# Run directly with npx (no installation required)\nnpx @pinkpixel/mcpollinations\n```\n\nIf you prefer to install it globally:\n\n```bash\n# Install globally\nnpm install -g @pinkpixel/mcpollinations\n\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n\n```\n\nOr clone the repository:\n\n```bash\n# Clone the git repository\ngit clone https://github.com/pinkpixel-dev/mcpollinations.git\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n# or run directly\nnode /path/to/MCPollinations/pollinations-mcp-server.js\n\n```\n\n## MCP Integration\n\nTo integrate the server with applications that support the Model Context Protocol (MCP):\n\n1. Generate an MCP configuration file:\n\n```bash\n# If installed globally\nnpx @pinkpixel/mcpollinations generate-config\n\n# Or run directly\nnode /path/to/MCPollinations/generate-mcp-config.js\n```\n\n### Quick MCP Config (env)\nIf you prefer to skip the generator, copy this into your MCP client config:\n\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"YOUR_TOKEN_OPTIONAL\",\n      \"referrer\": \"your-app-or-domain-optional\",\n      \"IMAGE_MODEL\": \"flux\",\n      \"IMAGE_WIDTH\": \"1024\",\n      \"IMAGE_HEIGHT\": \"1024\",\n      \"IMAGE_ENHANCE\": \"true\",\n      \"IMAGE_SAFE\": \"false\",\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"\",\n      \"AUDIO_VOICE\": \"alloy\",\n      \"OUTPUT_DIR\": \"./mcpollinations-output\"\n    }\n  }\n}\n```\n\n2. Follow the prompts to customize your configuration or use the defaults.\n   - Set an output directory (relative paths recommended for portability)\n     - **Windows users**: Consider using absolute paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations`) for more reliable file saving\n   - Configure optional authentication (token, referrer) under `env`\n   - Configure default parameters for image generation (with a list of available models, dimensions, etc.)\n   - Configure default parameters for text generation (with a list of available models)\n   - Configure default parameters for audio generation (voice)\n\n\n3. Copy the generated `mcp.json` file to your application's MCP settings .json file.\n4. Restart your application.\n\nAfter integration, you can use commands like:\n\n\"Generate an image of a sunset over the ocean using MCPollinations\"\n\n## Authentication (Optional)\n\nMCPollinations supports optional authentication to provide access to more models and better rate limits. The server works perfectly without authentication (free tier), but users with API tokens can get enhanced access.\n\n### Configuration Methods\n\n**Method 1: Environment Variables (Recommended for security)**\n```bash\n# Set environment variables before running the server\nexport POLLINATIONS_TOKEN=\"your-api-token\"\nexport POLLINATIONS_REFERRER=\"https://your-domain.com\"\n\n# Then run the server\nnpx @pinkpixel/mcpollinations\n```\n\n**Method 2: MCP Configuration File (env)**\nWhen generating your MCP configuration, place auth inside `env` so your MCP client passes them as environment variables to the server process:\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"your-api-token\",\n      \"referrer\": \"your-app-or-domain\"\n    }\n  }\n}\n```\n\nYou can also provide `POLLINATIONS_TOKEN` and `POLLINATIONS_REFERRER` instead; the server recognizes both forms. Using `token` and `referrer` inside `env` is recommended for MCP configs.\n\n### Authentication Parameters\n\n- **`token`** (optional): Your Pollinations API token for enhanced access\n- **`referrer`** (optional): Your domain/application referrer URL\n\nBoth parameters are completely optional. Leave them empty or unset to use the free tier.\n\n## Using Your Configuration Settings\n\nMCPollinations respects your MCP configuration settings placed in `env` as defaults. When you ask an AI assistant to generate content:\n\n- **Your configured models, output directories, and parameters are used automatically**\n- **To override**: Specifically instruct the AI to use different settings\n  - \"Generate an image using the kontext model\"\n  - \"Save this image to my Desktop folder\"\n  - \"Use a temperature of 1.2 for this text generation\"\n\n**Example Instructions:**\n- ✅ \"Generate a sunset image\" → Uses your configured model and output directory\n- ✅ \"Generate a sunset image with the flux model\" → Overrides model only\n- ✅ \"Generate a sunset image and save it to C:\\Pictures\" → Overrides output path only\n\nThis ensures your preferences are always respected unless you specifically want different settings for a particular request.\n\n## Troubleshooting\n\n### \"AbortController is not defined\" Error\n\nIf you encounter this error when running the MCP server:\n\n```\nReferenceError: AbortController is not defined\n```\n\nThis is usually caused by running on an older version of Node.js (below version 16.0.0). Try one of these solutions:\n\n1. **Update Node.js** (recommended):\n   - Update to Node.js 16.0.0 or newer\n\n2. **Use Global Installation**\n   - Update to the latest version of the package:\n   ```bash\n   npm install -g @pinkpixel/mcpollinations\n   # Run with npx\n   npx @pinkpixel/mcpollinations\n   ```\n\n3. **Install AbortController manually**:\n   - If for some reason the polyfill doesn't work:\n   ```bash\n   npm install node-abort-controller\n   ```\n\n### Check Your Node.js Version\n\nTo check your current Node.js version:\n\n```bash\nnode --version\n```\n\nIf it shows a version lower than 16.0.0, consider upgrading for best compatibility.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### **Image Generation Tools**\n1. `generateImageUrl` - Generates an image URL from a text prompt\n2. `generateImage` - Generates an image, returns it as base64-encoded data, and saves it to a file by default (PNG format)\n3. `editImage` - **NEW!** Edit or modify existing images based on text prompts\n4. `generateImageFromReference` - **NEW!** Generate new images using existing images as reference\n5. `listImageModels` - Lists available models for image generation\n\n### **Text & Audio Tools**\n6. `respondText` - Responds with text to a prompt using text models (customizable parameters)\n7. `respondAudio` - Generates an audio response to a text prompt (customizable voice parameter)\n8. `listTextModels` - Lists available models for text generation\n9. `listAudioVoices` - Lists all available voices for audio generation\n\n## Text Generation Details\n\n### Available Parameters\n\nThe `respondText` tool supports several parameters for fine-tuning text generation:\n\n- **`model`**: Choose from available text models (use `listTextModels` to see current options)\n- **`temperature`** (0.0-2.0): Controls randomness in the output\n  - Lower values (0.1-0.7) = more focused and deterministic\n  - Higher values (0.8-2.0) = more creative and random\n- **`top_p`** (0.0-1.0): Controls diversity via nucleus sampling\n  - Lower values = more focused on likely tokens\n  - Higher values = considers more token possibilities\n- **`system`**: System prompt to guide the model's behavior and personality\n\n### Customizing Text Generation\n\n```javascript\n// Example options for respondText\nconst options = {\n  model: \"openai\",           // Model selection\n  temperature: 0.7,          // Balanced creativity\n  top_p: 0.9,               // High diversity\n  system: \"You are a helpful assistant that explains things clearly and concisely.\"\n};\n```\n\n### Configuration Examples\n\nIn your MCP configuration, set defaults under `env` so the server uses them automatically:\n\n```json\n{\n  \"mcpollinations\": {\n    \"env\": {\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"You are a helpful coding assistant.\"\n    }\n  }\n}\n```\n\n## Image-to-Image Generation (NEW!)\n\nMCPollinations now supports powerful image-to-image generation with two specialized tools:\n\n### **editImage Tool**\nPerfect for modifying existing images:\n- **Remove objects**: \"remove the cat from this image\"\n- **Add elements**: \"add a dog to this scene\"\n- **Change backgrounds**: \"replace the background with mountains\"\n- **Style modifications**: \"make the lighting more dramatic\"\n\n### **generateImageFromReference Tool**\nPerfect for creating variations and new styles:\n- **Style transfer**: \"make this photo look like a painting\"\n- **Format changes**: \"convert this to a cartoon style\"\n- **Creative variations**: \"create a futuristic version of this\"\n- **Artistic interpretations**: \"make this look like a sketch\"\n\n### **Supported Models**\n- **`kontext`**: Specialized model optimized for image-to-image tasks\n- **`nanobanana`**: New Google model supporting both text-to-image and image-to-image generation\n- **`seedream`**: New ByteDance model supporting both text-to-image and image-to-image generation\n\nMulti-reference images: `editImage` and `generateImageFromReference` accept `imageUrl` as a single URL or an array of URLs. The server encodes arrays as the comma-separated `image` parameter used by the API. Ordering matters; kontext uses only the first image, nanobanana is safe up to ~4 refs, and seedream supports up to 10.\n\nImportant: URLs only. The image-to-image tools require publicly accessible HTTP(S) URLs. Local file paths, file uploads, and base64/data URLs are not supported by this MCP server (it does not upload files). If you need to work from a local image, host it somewhere accessible (e.g., a temporary file host, object storage, or a raw link in a repo) and pass the URL.\n\n### **Example Usage**\n```javascript\n// Edit an existing image\nconst editResult = await editImage(\n  \"change the background to a sunset beach\",\n  \"https://example.com/photo.jpg\",\n  \"nanobanana\"  // or \"kontext\", \"seedream\"\n);\n\n// Generate from reference\nconst referenceResult = await generateImageFromReference(\n  \"make this into a watercolor painting\",\n  \"https://example.com/photo.jpg\",\n  \"seedream\"  // or \"kontext\", \"nanobanana\"\n);\n```\n\n## Image Generation Details\n\n### Default Behavior\n\nWhen using the `generateImage` tool:\n\n- Images are saved to disk by default as PNG files\n- The default save location is the current working directory where the MCP server is running\n- The 'flux' model is used by default\n- A random seed is generated by default for each image (ensuring variety)\n- Base64-encoded image data is always returned, regardless of whether the image is saved to a file\n\n### Customizing Image Generation\n\n```javascript\n// Example options for generateImage\nconst options = {\n  // Model selection (defaults to 'flux')\n  // Available models: \"flux\", \"turbo\", \"kontext\", \"nanobanana\", \"seedream\"\n  model: \"flux\",\n\n  // Image dimensions\n  width: 1024,\n  height: 1024,\n\n  // Generation options\n  seed: 12345,  // Specific seed for reproducibility (defaults to random)\n  enhance: true,  // Enhance the prompt using an LLM before generating (defaults to true)\n  safe: false,  // Content filtering (defaults to false)\n\n  // File saving options\n  saveToFile: true,  // Set to false to skip saving to disk\n  outputPath: \"/path/to/save/directory\",  // Custom save location\n  fileName: \"my_custom_name\",  // Without extension\n  format: \"png\"  // png, jpeg, jpg, or webp\n};\n```\n\n### Where Images Are Saved\n\nWhen using Claude or another application with the MCP server:\n\n1. **Images are saved in the current working directory of where the MCP server is running**, not where Claude or the client application is installed.\n\n2. If you start the MCP server manually from a specific directory, images will be saved there by default.\n\n3. If Claude Desktop launches the MCP server automatically, images will be saved in Claude Desktop's working directory (typically in an application data folder).\n\n**💡 Windows Users**: For reliable file saving on Windows, use absolute paths in your MCP configuration instead of relative paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations` instead of `./mcpollinations-output`). Relative paths may not resolve as expected depending on the working directory context.\n\n### Finding Your Generated Images\n\n- The response from Claude after generating an image includes the full file path where the image was saved\n- You can specify a familiar location using the `outputPath` parameter\n- Best practice: Ask Claude to save images to an easily accessible folder like your Pictures or Downloads directory\n\n### Unique Filenames\n\nThe MCP server ensures that generated images always have unique filenames and will never overwrite existing files:\n\n1. **Default filenames** include:\n   - A sanitized version of the prompt (first 20 characters)\n   - A timestamp\n   - A random suffix\n\n2. **Custom filenames** are also protected:\n   - If you specify a filename and a file with that name already exists, a numeric suffix will be added automatically\n   - For example: `sunset.png`, `sunset_1.png`, `sunset_2.png`, etc.\n\nThis means you can safely generate multiple images with the same prompt or filename without worrying about overwriting previous images.\n\n### Accessing Base64 Data\n\nEven when saving to a file, the base64-encoded image data is always returned and can be used for:\n\n- Embedding in web pages (`<img src=\"data:image/png;base64,...\" />`)\n- Passing to other services or APIs\n- Processing in memory without filesystem operations\n- Displaying in applications that support data URIs\n\n## For Developers\n\nIf you want to use the package in your own projects:\n\n```bash\n# Install as a dependency\nnpm install @pinkpixel/mcpollinations\n\n# Import in your code\nimport { generateImageUrl, generateImage, repsondText, respondAudio, listTextModels, listImageModels, listAudioVoices } from '@pinkpixel/mcpollinations';\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pinkpixel",
        "images",
        "image",
        "pinkpixel dev",
        "generation pinkpixel",
        "generates images"
      ],
      "category": "image-and-video-generation"
    },
    "qhdrl12--mcp-server-gemini-image-generator": {
      "owner": "qhdrl12",
      "name": "mcp-server-gemini-image-generator",
      "url": "https://github.com/qhdrl12/mcp-server-gemini-image-generator",
      "imageUrl": "/freedevtools/mcp/pfp/qhdrl12.webp",
      "description": "Generate high-quality images from text prompts using the Gemini AI model, manage local image storage, and facilitate creative modifications of existing images.",
      "stars": 23,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T16:39:30Z",
      "readme_content": "[![MseeP Badge](https://mseep.net/pr/qhdrl12-mcp-server-gemini-image-generator-badge.jpg)](https://mseep.ai/app/qhdrl12-mcp-server-gemini-image-generator)\n[![smithery badge](https://smithery.ai/badge/@qhdrl12/mcp-server-gemini-image-gen)](https://smithery.ai/server/@qhdrl12/mcp-server-gemini-image-gen)\n\n<a href=\"https://glama.ai/mcp/servers/@qhdrl12/mcp-server-gemini-image-generator\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@qhdrl12/mcp-server-gemini-image-generator/badge\" alt=\"Gemini Image Generator Server MCP server\" />\n</a>\n\n# Gemini Image Generator MCP Server\n\nGenerate high-quality images from text prompts using Google's Gemini model through the MCP protocol.\n\n## Overview\n\nThis MCP server allows any AI assistant to generate images using Google's Gemini AI model. The server handles prompt engineering, text-to-image conversion, filename generation, and local image storage, making it easy to create and manage AI-generated images through any MCP client.\n\n## Features\n\n- Text-to-image generation using Gemini 2.0 Flash\n- Image-to-image transformation based on text prompts\n- Support for both file-based and base64-encoded images\n- Automatic intelligent filename generation based on prompts\n- Automatic translation of non-English prompts\n- Local image storage with configurable output path\n- Strict text exclusion from generated images\n- High-resolution image output\n- Direct access to both image data and file path\n\n## Available MCP Tools\n\nThe server provides the following MCP tools for AI assistants:\n\n### 1. `generate_image_from_text`\n\nCreates a new image from a text prompt description.\n\n```\ngenerate_image_from_text(prompt: str) -> Tuple[bytes, str]\n```\n\n**Parameters:**\n- `prompt`: Text description of the image you want to generate\n\n**Returns:**\n- A tuple containing:\n  - Raw image data (bytes)\n  - Path to the saved image file (str)\n\nThis dual return format allows AI assistants to either work with the image data directly or reference the saved file path.\n\n**Examples:**\n- \"Generate an image of a sunset over mountains\"\n- \"Create a photorealistic flying pig in a sci-fi city\"\n\n#### Example Output\n\nThis image was generated using the prompt:\n\n```\n\"Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\"\n```\n\n![Flying pig over sci-fi city](examples/flying_pig_scifi_city.png)\n\n*A 3D rendered pig with wings and a top hat flying over a futuristic sci-fi city filled with greenery*\n\n### Known Issues\n\nWhen using this MCP server with Claude Desktop Host:\n\n1. **Performance Issues**: Using `transform_image_from_encoded` may take significantly longer to process compared to other methods. This is due to the overhead of transferring large base64-encoded image data through the MCP protocol.\n\n2. **Path Resolution Problems**: There may be issues with correctly resolving image paths when using Claude Desktop Host. The host application might not properly interpret the returned file paths, making it difficult to access the generated images.\n\nFor the best experience, consider using alternative MCP clients or the `transform_image_from_file` method when possible. \n\n### 2. `transform_image_from_encoded`\n\nTransforms an existing image based on a text prompt using base64-encoded image data.\n\n```\ntransform_image_from_encoded(encoded_image: str, prompt: str) -> Tuple[bytes, str]\n```\n\n**Parameters:**\n- `encoded_image`: Base64 encoded image data with format header (must be in format: \"data:image/[format];base64,[data]\")\n- `prompt`: Text description of how you want to transform the image\n\n**Returns:**\n- A tuple containing:\n  - Raw transformed image data (bytes)\n  - Path to the saved transformed image file (str)\n\n**Example:**\n- \"Add snow to this landscape\"\n- \"Change the background to a beach\"\n\n### 3. `transform_image_from_file`\n\nTransforms an existing image file based on a text prompt.\n\n```\ntransform_image_from_file(image_file_path: str, prompt: str) -> Tuple[bytes, str]\n```\n\n**Parameters:**\n- `image_file_path`: Path to the image file to be transformed\n- `prompt`: Text description of how you want to transform the image\n\n**Returns:**\n- A tuple containing:\n  - Raw transformed image data (bytes)\n  - Path to the saved transformed image file (str)\n\n**Examples:**\n- \"Add a llama next to the person in this image\"\n- \"Make this daytime scene look like night time\"\n\n#### Example Transformation\n\nUsing the flying pig image created above, we applied a transformation with the following prompt:\n\n```\n\"Add a cute baby whale flying alongside the pig\"\n```\n\n**Before:**\n![Flying pig over sci-fi city](examples/flying_pig_scifi_city.png)\n\n**After:**\n![Flying pig with baby whale](examples/pig_cute_baby_whale.png)\n\n*The original flying pig image with a cute baby whale added flying alongside it*\n\n## Setup\n\n### Prerequisites\n\n- Python 3.11+\n- Google AI API key (Gemini)\n- MCP host application (Claude Desktop App, Cursor, or other MCP-compatible clients)\n\n### Getting a Gemini API Key\n\n1. Visit [Google AI Studio API Keys page](https://aistudio.google.com/apikey)\n2. Sign in with your Google account\n3. Click \"Create API Key\"\n4. Copy your new API key for use in the configuration\n5. Note: The API key provides a certain quota of free usage per month. You can check your usage in the Google AI Studio\n\n### Installation\n\n### Installing via Smithery\n\nTo install Gemini Image Generator MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@qhdrl12/mcp-server-gemini-image-gen):\n\n```bash\nnpx -y @smithery/cli install @qhdrl12/mcp-server-gemini-image-gen --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n```bash\ngit clone https://github.com/your-username/mcp-server-gemini-image-generator.git\ncd mcp-server-gemini-image-generator\n```\n\n2. Create a virtual environment and install dependencies:\n```bash\n# Using uv (recommended)\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n\n# Or using regular venv\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e .\n```\n\n3. Set up environment variables (choose one method):\n\n**Method A: Using .env file (optional)**\n```bash\n# Create .env file in the project root\ncat > .env << 'EOF'\nGEMINI_API_KEY=your-gemini-api-key-here\nOUTPUT_IMAGE_PATH=/path/to/save/images\nEOF\n```\n\n**Method B: Set directly in Claude Desktop config (recommended)**\n- Set environment variables directly in the `claude_desktop_config.json` (shown in configuration section below)\n\n### Configure Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n    \"mcpServers\": {\n        \"gemini-image-generator\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/absolute/path/to/mcp-server-gemini-image-generator\",\n                \"run\",\n                \"mcp-server-gemini-image-generator\"\n            ],\n            \"env\": {\n                \"GEMINI_API_KEY\": \"your-actual-gemini-api-key-here\",\n                \"OUTPUT_IMAGE_PATH\": \"/absolute/path/to/your/images/directory\"\n            }\n        }\n    }\n}\n```\n\n**Important Configuration Notes:**\n\n1. **Replace paths with your actual paths:**\n   - Change `/absolute/path/to/mcp-server-gemini-image-generator` to the actual location where you cloned this repository\n   - Change `/absolute/path/to/your/images/directory` to where you want generated images to be saved\n\n2. **Environment Variables:**\n   - Replace `your-actual-gemini-api-key-here` with your real Gemini API key from Google AI Studio\n   - Use absolute paths for `OUTPUT_IMAGE_PATH` to ensure images are saved correctly\n\n3. **Example with real paths:**\n```json\n{\n    \"mcpServers\": {\n        \"gemini-image-generator\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/Users/username/Projects/mcp-server-gemini-image-generator\",\n                \"run\",\n                \"mcp-server-gemini-image-generator\"\n            ],\n            \"env\": {\n                \"GEMINI_API_KEY\": \"GEMINI_API_KEY\",\n                \"OUTPUT_IMAGE_PATH\": \"OUTPUT_IMAGE_PATH\"\n            }\n        }\n    }\n}\n```\n\n## Usage\n\nOnce installed and configured, you can ask Claude to generate or transform images using prompts like:\n\n### Generating New Images\n- \"Generate an image of a sunset over mountains\"\n- \"Create an illustration of a futuristic cityscape\"\n- \"Make a picture of a cat wearing sunglasses\"\n\n### Transforming Existing Images\n- \"Transform this image by adding snow to the scene\"\n- \"Edit this photo to make it look like it was taken at night\"\n- \"Add a dragon flying in the background of this picture\"\n\nThe generated/transformed images will be saved to your configured output path and displayed in Claude. With the updated return types, AI assistants can also work directly with the image data without needing to access the saved files.\n\n## Testing\n\nYou can test the application by running the FastMCP development server:\n\n```\nfastmcp dev server.py\n```\n\nThis command starts a local development server and makes the MCP Inspector available at http://localhost:5173/. \nThe MCP Inspector provides a convenient web interface where you can directly test the image generation tool without needing to use Claude or another MCP client. \nYou can enter text prompts, execute the tool, and see the results immediately, which is helpful for development and debugging.\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "image",
        "qhdrl12",
        "image generator",
        "gemini image",
        "video generation"
      ],
      "category": "image-and-video-generation"
    },
    "qpd-v--mcp-image-downloader": {
      "owner": "qpd-v",
      "name": "mcp-image-downloader",
      "url": "https://github.com/qpd-v/mcp-image-downloader",
      "imageUrl": "/freedevtools/mcp/pfp/qpd-v.webp",
      "description": "Provides tools for downloading images from URLs and performing basic image optimization tasks such as resizing, quality adjustment, and format conversion.",
      "stars": 11,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:03Z",
      "readme_content": "# MCP Image Downloader\n\nAn MCP server that provides tools for downloading and optimizing images. Built using the Model Context Protocol (MCP), this server enables AI assistants to download images from URLs and perform basic image optimization tasks.\n\n## Features\n\n- Download images from URLs with proper error handling\n- Optimize images with options for:\n  - Resizing (maintaining aspect ratio)\n  - Quality adjustment (JPEG/WebP)\n  - Format conversion\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/qpd-v/mcp-image-downloader.git\ncd mcp-image-downloader\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### As an MCP Server\n\nAdd the server to your MCP configuration (e.g., in Claude Desktop's config):\n\n```json\n{\n  \"mcpServers\": {\n    \"image-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-image-downloader/build/index.js\"]\n    }\n  }\n}\n```\n\n### Available Tools\n\n#### download_image\nDownloads an image from a URL to a specified path.\n\nParameters:\n- `url`: URL of the image to download\n- `outputPath`: Path where to save the image\n\n#### optimize_image\nCreates an optimized version of an image.\n\nParameters:\n- `inputPath`: Path to the input image\n- `outputPath`: Path where to save the optimized image\n- `width` (optional): Target width (maintains aspect ratio if only width is specified)\n- `height` (optional): Target height (maintains aspect ratio if only height is specified)\n- `quality` (optional): JPEG/WebP quality (1-100)\n\n## Development\n\n```bash\n# Run in development mode\nnpm run start\n\n# Build the project\nnpm run build\n```\n\n## Requirements\n\n- Node.js 16 or higher\n- NPM or compatible package manager\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\nqpd-v\n\n## Version\n\n0.1.0 - Initial release",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "qpd",
        "mcp",
        "downloader",
        "image downloader",
        "mcp image",
        "downloading images"
      ],
      "category": "image-and-video-generation"
    },
    "rmcendarfer2017--MCP-image-gen": {
      "owner": "rmcendarfer2017",
      "name": "MCP-image-gen",
      "url": "https://github.com/rmcendarfer2017/MCP-image-gen",
      "imageUrl": "/freedevtools/mcp/pfp/rmcendarfer2017.webp",
      "description": "Generate stunning images using advanced AI models with a built-in storage system for managing and accessing creations. Users can customize image styles and utilize a prompt-based interface for generating images.",
      "stars": 0,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-08T16:13:41Z",
      "readme_content": "# Image Generator MCP Server\n\nAn MCP server that uses Replicate to generate images and allows users to save them.\n\n## Components\n\n### Resources\n\nThe server implements an image storage system with:\n- Custom image:// URI scheme for accessing individual generated images\n- Each image resource has a name based on its prompt, description with creation date, and image/png mimetype\n\n### Prompts\n\nThe server provides a single prompt:\n- generate-image: Creates prompts for generating images using Stable Diffusion\n  - Optional \"style\" argument to control the image style (realistic/artistic/abstract)\n  - Generates a prompt template with style-specific guidance\n\n### Tools\n\nThe server implements three tools:\n- generate-image: Generates an image using Replicate's Stable Diffusion model\n  - Takes \"prompt\" as a required string argument\n  - Optional parameters include \"negative_prompt\", \"width\", \"height\", \"num_inference_steps\", and \"guidance_scale\"\n  - Returns the generated image and its URL\n- save-image: Saves a generated image to the local filesystem\n  - Takes \"image_url\" and \"prompt\" as required string arguments\n  - Generates a unique ID for the image and saves it to the \"generated_images\" directory\n- list-saved-images: Lists all saved images\n  - Returns a list of all saved images with their metadata and thumbnails\n\n## Configuration\n\n### Replicate API Token\n\nTo use this image generator, you need a Replicate API token:\n\n1. Create an account at [Replicate](https://replicate.com/)\n2. Get your API token from [https://replicate.com/account](https://replicate.com/account)\n3. Create a `.env` file based on the provided `.env.example` template:\n\n```\nREPLICATE_API_TOKEN=your_replicate_api_token_here\n```\n\n> **Important:** The `.env` file is excluded from version control via `.gitignore` to prevent accidentally exposing your API token. Never commit sensitive information to your repository.\n\n### Environment Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/image-generator.git\ncd image-generator\n```\n\n2. Create and activate a virtual environment:\n```bash\n# Using venv\npython -m venv .venv\n# On Windows\n.venv\\Scripts\\activate\n# On macOS/Linux\nsource .venv/bin/activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Set up your `.env` file as described above\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"image-generator\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"B:\\NEWTEST\\image-generator\",\n        \"run\",\n        \"image-generator\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"image-generator\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"image-generator\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Usage\n\nOnce the server is running, you can:\n\n1. Generate an image by using the \"generate-image\" tool with a descriptive prompt\n2. Save the generated image using the \"save-image\" tool with the image URL and prompt\n3. View all saved images using the \"list-saved-images\" tool\n4. Access saved images through the resource list\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory B:\\NEWTEST\\image-generator run image-generator\n```\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "generate",
        "images",
        "mcp",
        "generating images",
        "mcp image",
        "image gen"
      ],
      "category": "image-and-video-generation"
    },
    "sammyl720--image-generator-mcp-server": {
      "owner": "sammyl720",
      "name": "image-generator-mcp-server",
      "url": "https://github.com/sammyl720/image-generator-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/sammyl720.webp",
      "description": "Connects to OpenAI's DALL-E 3 model to generate images based on user prompts, saving the results to a specified directory on the user's desktop.",
      "stars": 10,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-24T09:22:57Z",
      "readme_content": "# image-generator MCP Server\n\nAn mcp server that generates images based on image prompts\n\nThis is a TypeScript-based MCP server that implements image generation using **OPENAI**'s `dall-e-3` image generation model.\n\n## Features\n\n### Tools\n- `generate_image` - Generate an image for given prompt\n  - Takes `prompt` as a required parameter\n  - Takes `imageName` as a required parameter to save the generated image in a `generated-images` directory on your desktop\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"command\": \"image-generator\",\n      \"env\": {\n        \"OPENAI_API_KEY\": \"<your-openai-api-key>\"\n    }\n  }\n}\n```\nMake sure to replace `<your-openai-api-key>` with your actual **OPENAI** Api Key.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "openai",
        "generate",
        "generate images",
        "image generator",
        "mcp server"
      ],
      "category": "image-and-video-generation"
    },
    "sarthakkimtani--mcp-image-gen": {
      "owner": "sarthakkimtani",
      "name": "mcp-image-gen",
      "url": "https://github.com/sarthakkimtani/mcp-image-gen",
      "imageUrl": "/freedevtools/mcp/pfp/sarthakkimtani.webp",
      "description": "Generates high-quality images from textual prompts with customizable dimensions using the Flux.1 Schnell model. Provides standardized interfaces for specifying image generation parameters and includes error handling features.",
      "stars": 15,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-05T13:37:04Z",
      "readme_content": "# Image Generation MCP Server\n\nA Model Context Protocol (MCP) server that enables seamless generation of high-quality images via Together AI. This server provides a standardized interface to specify image generation parameters.\n\n<a href=\"https://glama.ai/mcp/servers/o0137xiz62\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/o0137xiz62/badge\" alt=\"Image Generation Server MCP server\" />\n</a>\n\n## Features\n\n- High-quality image generation powered by the Flux.1 Schnell model\n- Support for customizable dimensions (width and height)\n- Clear error handling for prompt validation and API issues\n- Easy integration with MCP-compatible clients\n\n## Installation\n\n#### Claude Desktop\n\n- On MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<summary>Development/Unpublished Servers Configuration</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"image-gen\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/ABSOLUTE/PATH/TO/image-gen/\", \"run\", \"image-gen\"],\n      \"env\": {\n        \"TOGETHER_AI_API_KEY\": \"<API KEY>\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server implements one tool:\n\n### generate_image\n\nGenerates an image based on the given textual prompt and optional dimensions.\n\n**Input Schema:**\n\n```json\n{\n  \"prompt\": {\n    \"type\": \"string\",\n    \"description\": \"A descriptive prompt for generating the image (e.g., 'a futuristic cityscape at sunset')\"\n  },\n  \"width\": {\n    \"type\": \"integer\",\n    \"description\": \"Width of the generated image in pixels (optional)\"\n  },\n  \"height\": {\n    \"type\": \"integer\",\n    \"description\": \"Height of the generated image in pixels (optional)\"\n  },\n  \"model\": {\n    \"type\": \"string\",\n    \"description\": \"The exact model name as it appears in Together AI. If incorrect, it will fallback to the default model (black-forest-labs/FLUX.1-schnell).\"\n  }\n}\n```\n\n## Prerequisites\n\n- Python 3.12 or higher\n- httpx\n- mcp\n\n## Contributing\n\nContributions are welcome! Please follow these steps to contribute:\n\n1. Fork the repository\n2. Create a new branch (`feature/my-new-feature`)\n3. Commit your changes\n4. Push the branch to your fork\n5. Open a Pull Request\n\nFor significant changes, please open an issue first to discuss your proposed changes.\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "generates",
        "images",
        "mcp",
        "image generation",
        "mcp image",
        "image gen"
      ],
      "category": "image-and-video-generation"
    },
    "sshtunnelvision--MCP-LOGO-GEN": {
      "owner": "sshtunnelvision",
      "name": "MCP-LOGO-GEN",
      "url": "https://github.com/sshtunnelvision/MCP-LOGO-GEN",
      "imageUrl": "/freedevtools/mcp/pfp/sshtunnelvision.webp",
      "description": "Logo generation using AI tools, including features for image creation, background removal, and automatic scaling for high-quality outputs in various sizes.",
      "stars": 171,
      "forks": 17,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-08-18T06:34:36Z",
      "readme_content": "# MCP Tool Server for Logo Generation\n\nThis server provides logo generation capabilities using FAL AI, with tools for image generation, background removal, and automatic scaling.\n\n## Demo\n\n[![MCP Tool Server Demo](https://img.youtube.com/vi/Miemu1xEZng/0.jpg)](https://www.youtube.com/watch?v=Miemu1xEZng)\n\n## Installation\n\n1. Install `uv` (Universal Virtualenv):\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Create and activate a virtual environment:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate     # On Windows\n```\n\n3. Install dependencies:\n\n```bash\nuv pip install -r requirements.txt\n```\n\n4. Set up your environment variables:\n   - Create a `.env` file in the root directory\n   - Add your FAL AI API key:\n\n```bash\nFAL_KEY=your_fal_ai_key_here\n```\n\n## Running the Server\n\nStart the server with:\n\n```bash\npython run_server.py\n```\n\nThe server will be available at `http://127.0.0.1:7777`\n\n### Troubleshooting\n\nIf you encounter a `FileNotFoundError` on Windows when running the server, make sure you're running the command from the root directory of the project. If the issue persists, try updating to the latest version of the repository which includes fixes for Windows compatibility.\n\nFor Windows users specifically:\n\n1. Make sure you've activated your virtual environment with `.venv\\Scripts\\activate`\n2. Run the server from the root directory of the project with `python run_server.py`\n3. If you see any path-related errors, please report them in the issues section of the repository\n\n## Cursor IDE Configuration\n\n1. Open Cursor Settings\n2. Navigate to the MCP section\n3. Add the following configuration:\n   - URL: `http://127.0.0.1:7777/sse`\n   - Connection Type: `SSE`\n   - Enable the connection\n\n## Notes\n\n- Always reference `@logo-creation.mdc` in your Cursor Composer for consistent results\n- Steps are defined in `@logo-creation.mdc` but tools can be used independently\n- All generated logos will be saved in the `downloads` directory\n- Each logo is automatically generated in three sizes:\n  - Original size\n  - 32x32 pixels\n  - 128x128 pixels\n- All logos maintain transparency in their final PNG format\n- Prompts created by agent are informed by examples and prompt structure seen in server.py. You can customize the prompt structure by editing the server.py file.\n- You can use the generate_image tool to generate any image you want, not just logos\n\n## Requirements\n\n- Python 3.8+\n- FAL AI API key (required for image generation)\n- Active internet connection\n\n## References\n\n- [Cursor MCP Documentation](https://docs.cursor.com/context/model-context-protocol)\n- [Model Context Protocol Introduction](https://modelcontextprotocol.io/introduction)\n- [FAL AI Dashboard](https://fal.ai/dashboard)\n\n---\n\nIf you find this tool helpful, you can [buy me a coffee](https://buymeacoffee.com/sshtunnelvision) ☕️ to support development!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logo",
        "mcp",
        "sshtunnelvision",
        "logo generation",
        "mcp logo",
        "logo gen"
      ],
      "category": "image-and-video-generation"
    },
    "surferdot--mcp-svg-converter": {
      "owner": "surferdot",
      "name": "mcp-svg-converter",
      "url": "https://github.com/surferdot/mcp-svg-converter",
      "imageUrl": "/freedevtools/mcp/pfp/surferdot.webp",
      "description": "Converts SVG code to high-quality PNG and JPG images while providing options for transparency and image quality customization. Enhances image processing by allowing transformation of vector graphics into raster formats with detailed settings.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T13:58:17Z",
      "readme_content": "# MCP SVG Converter\n\n[![npm version](https://img.shields.io/npm/v/mcp-svg-converter.svg)](https://www.npmjs.com/package/mcp-svg-converter)\n[![Downloads](https://img.shields.io/npm/dt/mcp-svg-converter.svg)](https://www.npmjs.com/package/mcp-svg-converter)\n[![License](https://img.shields.io/npm/l/mcp-svg-converter.svg)](https://github.com/surferdot/mcp-svg-converter/blob/main/LICENSE)\n\n[English](#english) | [中文](#中文)\n\n<a name=\"english\"></a>\n## English\n\nA Model Context Protocol (MCP) server that provides tools for converting SVG code to high-quality PNG and JPG images with detailed customization options.\n\n### Features\n\n- Convert SVG code to high-quality PNG images with transparency support\n- Convert SVG code to high-quality JPG images with customizable quality settings\n- Automatic dimension detection and preservation from original SVG\n- Support for scaling to higher resolutions\n- Background color customization\n- Intelligent path handling with automatic redirection to allowed directories\n- Secure file system access with configurable permissions\n\n### Installation\n\n#### Quick Install with npx\n\n```bash\nnpx mcp-svg-converter /path/to/allowed/directory\n```\n\n#### Global Installation\n\n```bash\nnpm install -g mcp-svg-converter\nmcp-svg-converter /path/to/allowed/directory\n```\n\n#### From Source\n\n##### Prerequisites\n\n- Node.js 16 or higher\n- npm or yarn\n\n##### Installation Steps\n\n1. Clone this repository\n   ```bash\n   git clone https://github.com/surferdot/mcp-svg-converter.git\n   cd mcp-svg-converter\n   ```\n\n2. Install dependencies\n   ```bash\n   npm install\n   ```\n\n3. Build the project\n   ```bash\n   npm run build\n   ```\n\n### Usage\n\n#### As a standalone server\n\nRun the server by specifying one or more allowed directories where the converted images can be saved:\n\n```bash\nnode build/index.js /path/to/allowed/directory1 /path/to/allowed/directory2\n```\n\n#### With Claude Desktop\n\n1. Download and install [Claude Desktop](https://claude.ai/download)\n2. Create or confirm you have access to an output directory:\n   ```bash\n   # macOS/Linux\n   mkdir -p ~/Desktop/svg-output\n   \n   # Windows\n   mkdir \"%USERPROFILE%\\Desktop\\svg-output\"\n   ```\n\n3. Configure Claude Desktop by editing the configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n4. Open the Claude app, click on the Claude menu in your system menu bar and select \"Settings...\"\n5. Click on \"Developer\" in the left sidebar\n6. Click \"Edit Config\" to open the configuration file\n\n7. Add this server configuration:\n\n##### Using npm package with npx (recommended)\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-svg-converter\",\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n##### Using global installation\n\nIf you've installed the package globally:\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"mcp-svg-converter\",\n      \"args\": [\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n##### Using local build\n\nIf you've built from source:\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mcp-svg-converter/build/index.js\",\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n8. Save the file and restart Claude Desktop\n\n#### Verifying the Setup\n\nWhen Claude Desktop restarts, if configured correctly:\n\n1. You should see a hammer icon <img src=\"https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg\" style=\"display: inline; height: 1.3em; vertical-align: middle\"> at the bottom right of the input box indicating MCP tools are available.\n2. Clicking the hammer icon should show the `svg-to-png` and `svg-to-jpg` tools.\n\n### Examples in Claude Desktop\n\n#### Example 1: Converting a Simple SVG to PNG\n\nIn Claude Desktop, send a message like:\n\n```\nPlease convert this SVG to PNG and save it to my output directory:\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 200 100\" width=\"200\" height=\"100\">\n  <rect x=\"10\" y=\"10\" width=\"80\" height=\"80\" fill=\"#4285f4\" />\n  <circle cx=\"140\" cy=\"50\" r=\"40\" fill=\"#ea4335\" />\n  <path d=\"M10 50 L90 50 L50 90 Z\" fill=\"#fbbc05\" />\n  <text x=\"100\" y=\"20\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\" fill=\"#34a853\">SVG Example</text>\n</svg>\n```\n\n#### Example 2: High-Quality JPG Conversion\n\n```\nPlease convert this SVG to a JPG with 95% quality and 2x scaling:\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 200 200\">\n  <rect width=\"200\" height=\"200\" fill=\"#f0f0f0\" />\n  <circle cx=\"100\" cy=\"100\" r=\"80\" fill=\"#ff6b6b\" />\n  <path d=\"M100 50 L130 150 L70 150 Z\" fill=\"white\" />\n</svg>\n```\n\n### Tools\n\n#### svg-to-png\n\nConverts SVG code to a high-quality PNG image with transparency support.\n\n**Parameters:**\n- `svgCode` (string, required): The SVG code to convert\n- `outputPath` (string, required): Path where the PNG file should be saved\n- `backgroundColor` (string, optional): Background color (default: transparent)\n- `scale` (number, optional): Scale factor for higher resolution (default: 1)\n\n#### svg-to-jpg\n\nConverts SVG code to a high-quality JPG image.\n\n**Parameters:**\n- `svgCode` (string, required): The SVG code to convert\n- `outputPath` (string, required): Path where the JPG file should be saved\n- `backgroundColor` (string, optional): Background color (default: white)\n- `quality` (number, optional): JPEG quality from 1-100 (default: 90)\n- `scale` (number, optional): Scale factor for higher resolution (default: 1)\n\n### Advanced Usage Tips\n\n#### Specifying Multiple Output Directories\n\nYou can specify multiple allowed output directories for more flexible file saving:\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-svg-converter\",\n        \"/Users/yourusername/Desktop/svg-output\",\n        \"/Users/yourusername/Documents/svg-images\",\n        \"/Users/yourusername/Downloads\"\n      ]\n    }\n  }\n}\n```\n\n#### Using Custom Output Filenames\n\nSpecify detailed file paths in your request:\n\n```\nPlease convert this SVG to PNG, and save it as \"colorful_shapes.png\" in my output directory.\n\n<svg>...</svg>\n```\n\n#### Automatic Path Redirection\n\nIf you request saving to a non-allowed directory, the converter automatically redirects to an allowed directory and informs you of the actual save location.\n\n### Troubleshooting\n\n#### Claude Doesn't Show MCP Tools Icon\n1. Verify the configuration file has correct JSON syntax\n2. Ensure all paths are absolute paths\n3. Make sure output directories exist and are writable\n4. Completely exit and restart Claude Desktop\n5. Check Claude logs:\n   - macOS: `~/Library/Logs/Claude/mcp*.log`\n   - Windows: `%APPDATA%\\Claude\\logs\\mcp*.log`\n\n#### Tool Execution Fails\n1. Ensure `mcp-svg-converter` is correctly installed\n2. Check output directory permissions\n3. Verify the SVG code is valid\n4. Check Claude logs for detailed error messages\n\n#### \"Command Not Found\" Error\n1. Ensure `mcp-svg-converter` is globally installed or correctly reference `npx`\n2. Confirm npm's global bin directory is in your PATH\n3. Try using full paths in configuration\n\n### Debugging\n\nYou can use the MCP Inspector to debug and test the server directly:\n\n```bash\nnpx @modelcontextprotocol/inspector npx mcp-svg-converter /path/to/allowed/directory\n```\n\nThis opens an interactive interface where you can test all available tools without going through Claude Desktop.\n\n### Security Considerations\n\n- The server will only write files to the directories specified when starting the server\n- If a user attempts to save to a non-allowed directory, the file will be automatically redirected to an allowed directory\n- Path traversal attacks are prevented by proper path validation\n\n### License\n\nMIT\n\n---\n\n<a name=\"中文\"></a>\n## 中文\n\nMCP SVG 转换器是一个基于模型上下文协议 (MCP) 的服务器，提供将 SVG 代码转换为高质量 PNG 和 JPG 图像的工具，支持详细的自定义选项。\n\n### 特点\n\n- 将 SVG 代码转换为支持透明度的高质量 PNG 图像\n- 将 SVG 代码转换为可定制质量设置的高质量 JPG 图像\n- 自动检测并保留原始 SVG 的尺寸\n- 支持缩放到更高分辨率\n- 可自定义背景颜色\n- 智能路径处理，自动重定向到允许的目录\n- 可配置权限的安全文件系统访问\n\n### 安装\n\n#### 使用 npx 快速安装\n\n```bash\nnpx mcp-svg-converter /path/to/allowed/directory\n```\n\n#### 全局安装\n\n```bash\nnpm install -g mcp-svg-converter\nmcp-svg-converter /path/to/allowed/directory\n```\n\n#### 从源代码安装\n\n##### 前提条件\n\n- Node.js 16 或更高版本\n- npm 或 yarn\n\n##### 安装步骤\n\n1. 克隆此仓库\n   ```bash\n   git clone https://github.com/surferdot/mcp-svg-converter.git\n   cd mcp-svg-converter\n   ```\n\n2. 安装依赖\n   ```bash\n   npm install\n   ```\n\n3. 构建项目\n   ```bash\n   npm run build\n   ```\n\n### 使用方法\n\n#### 作为独立服务器运行\n\n通过指定一个或多个允许存储转换后图像的目录来运行服务器：\n\n```bash\nnode build/index.js /path/to/allowed/directory1 /path/to/allowed/directory2\n```\n\n#### 与 Claude Desktop 一起使用\n\n1. 下载并安装 [Claude Desktop](https://claude.ai/download)\n2. 创建或确认你有权限访问的输出目录：\n   ```bash\n   # macOS/Linux\n   mkdir -p ~/Desktop/svg-output\n   \n   # Windows\n   mkdir \"%USERPROFILE%\\Desktop\\svg-output\"\n   ```\n\n3. 配置 Claude Desktop：\n   - 打开 Claude 应用程序\n   - 点击系统菜单栏中的 Claude 图标\n   - 选择\"Settings...\"（设置）\n   - 在左侧菜单中选择\"Developer\"（开发者）\n   - 点击\"Edit Config\"（编辑配置）按钮\n\n4. 编辑配置文件：\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n5. 添加服务器配置：\n\n##### 使用 npm 包与 npx（推荐）\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-svg-converter\",\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n##### 使用全局安装\n\n如果你已全局安装了此包：\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"mcp-svg-converter\",\n      \"args\": [\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n##### 使用本地构建\n\n如果你从源代码构建：\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mcp-svg-converter/build/index.js\",\n        \"/absolute/path/to/output/directory\"\n      ]\n    }\n  }\n}\n```\n\n6. 保存文件并重启 Claude Desktop\n\n#### 验证设置\n\n当 Claude Desktop 重启后，如果配置正确：\n\n1. 你应该在输入框右下角看到一个锤子图标 <img src=\"https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg\" style=\"display: inline; height: 1.3em; vertical-align: middle\">，表示 MCP 工具可用。\n2. 点击锤子图标应显示 `svg-to-png` 和 `svg-to-jpg` 工具。\n\n### Claude Desktop 中的使用示例\n\n#### 示例 1：将简单 SVG 转换为 PNG\n\n在 Claude Desktop 中发送以下消息：\n\n```\n请将这个 SVG 转换为 PNG 并保存到我的输出目录：\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 200 100\" width=\"200\" height=\"100\">\n  <rect x=\"10\" y=\"10\" width=\"80\" height=\"80\" fill=\"#4285f4\" />\n  <circle cx=\"140\" cy=\"50\" r=\"40\" fill=\"#ea4335\" />\n  <path d=\"M10 50 L90 50 L50 90 Z\" fill=\"#fbbc05\" />\n  <text x=\"100\" y=\"20\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\" fill=\"#34a853\">SVG 示例</text>\n</svg>\n```\n\n#### 示例 2：高质量 JPG 转换\n\n```\n请将这个 SVG 转换为 95% 质量和 2 倍缩放的 JPG 图像：\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 200 200\">\n  <rect width=\"200\" height=\"200\" fill=\"#f0f0f0\" />\n  <circle cx=\"100\" cy=\"100\" r=\"80\" fill=\"#ff6b6b\" />\n  <path d=\"M100 50 L130 150 L70 150 Z\" fill=\"white\" />\n</svg>\n```\n\n### 工具\n\n#### svg-to-png\n\n将 SVG 代码转换为支持透明度的高质量 PNG 图像。\n\n**参数：**\n- `svgCode` (字符串，必需)：要转换的 SVG 代码\n- `outputPath` (字符串，必需)：PNG 文件的保存路径\n- `backgroundColor` (字符串，可选)：背景颜色 (默认：透明)\n- `scale` (数字，可选)：更高分辨率的缩放因子 (默认：1)\n\n#### svg-to-jpg\n\n将 SVG 代码转换为高质量 JPG 图像。\n\n**参数：**\n- `svgCode` (字符串，必需)：要转换的 SVG 代码\n- `outputPath` (字符串，必需)：JPG 文件的保存路径\n- `backgroundColor` (字符串，可选)：背景颜色 (默认：白色)\n- `quality` (数字，可选)：JPEG 质量，范围从 1 到 100 (默认：90)\n- `scale` (数字，可选)：更高分辨率的缩放因子 (默认：1)\n\n### 高级使用技巧\n\n#### 指定多个输出目录\n\n你可以指定多个允许的输出目录，以提供更灵活的文件保存选项：\n\n```json\n{\n  \"mcpServers\": {\n    \"svg-converter\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-svg-converter\",\n        \"/Users/yourusername/Desktop/svg-output\",\n        \"/Users/yourusername/Documents/svg-images\",\n        \"/Users/yourusername/Downloads\"\n      ]\n    }\n  }\n}\n```\n\n#### 使用自定义输出文件名\n\n在请求中指定详细的文件路径：\n\n```\n请将这个 SVG 转换为 PNG，并以文件名 \"colorful_shapes.png\" 保存到输出目录。\n\n<svg>...</svg>\n```\n\n#### 自动路径重定向\n\n如果你请求保存到一个不允许的目录，转换器会自动将文件重定向到允许的目录，并在响应中告知你实际的保存位置。\n\n### 故障排除\n\n#### Claude 没有显示 MCP 工具图标\n1. 确认配置文件格式正确（JSON 语法）\n2. 检查所有路径是否为绝对路径\n3. 确保输出目录存在且可写\n4. 完全退出并重启 Claude Desktop\n5. 检查 Claude 日志：\n   - macOS: `~/Library/Logs/Claude/mcp*.log`\n   - Windows: `%APPDATA%\\Claude\\logs\\mcp*.log`\n\n#### 工具执行失败\n1. 确保已正确安装 `mcp-svg-converter`\n2. 检查输出目录的权限\n3. 验证 SVG 代码是否有效\n4. 检查 Claude 日志了解详细错误信息\n\n#### \"command not found\" 错误\n1. 确保已全局安装 `mcp-svg-converter` 或正确引用 `npx`\n2. 确认 npm 的全局 bin 目录在系统 PATH 中\n3. 尝试在配置中使用完整路径\n\n### 调试\n\n您可以使用 MCP Inspector 直接调试和测试服务器：\n\n```bash\nnpx @modelcontextprotocol/inspector npx mcp-svg-converter /path/to/allowed/directory\n```\n\n这将打开一个交互式界面，你可以在其中测试所有可用工具，而无需通过 Claude Desktop。\n\n### 安全考虑\n\n- 服务器只会将文件写入启动服务器时指定的目录\n- 如果用户尝试保存到非允许目录，文件将自动重定向到允许的目录\n- 通过适当的路径验证防止路径遍历攻击\n\n### 许可证\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "svg",
        "png",
        "surferdot",
        "svg converter",
        "converts svg",
        "mcp svg"
      ],
      "category": "image-and-video-generation"
    },
    "tzafrir--mcp-server-replicate": {
      "owner": "tzafrir",
      "name": "mcp-server-replicate",
      "url": "https://github.com/tzafrir/mcp-server-replicate",
      "imageUrl": "/freedevtools/mcp/pfp/tzafrir.webp",
      "description": "Access various AI models hosted on Replicate through a standardized interface for image generation with customizable parameters, enabling output resizing and optimization. Future enhancements will include text and video generation features.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-05T06:38:38Z",
      "readme_content": "# MCP Server for Replicate\n\nA FastMCP server implementation for interfacing with Replicate's API. This server provides tools for accessing various AI models hosted on Replicate through a standardized interface.\n\n## Current Status: Early Alpha\n\nThis project is in early alpha development. Features and APIs may change significantly.\n\n### Currently Supported\n- Image generation models with:\n  - Model schema inspection\n  - Image generation with customizable parameters\n  - Output resizing and optimization\n\n## Roadmap\n\n### Planned Features\n1. Text Generation\n   - Support for text completion models\n   - Chat model integration\n   - Streaming support for real-time responses\n\n2. Video Generation\n   - Support for video generation models\n   - Video output handling and optimization\n   - Progress tracking for long-running generations\n\n3. Additional Features\n   - Model version management\n   - Better error handling and retries\n   - Caching for frequently used models\n   - Rate limiting and queue management\n\n## Setup\n\n1. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n2. Set up your Replicate API token in `.env`:\n```\nREPLICATE_API_TOKEN=your_token_here\n```\n\n3. Run the server:\n```bash\nfastmcp dev server.py\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "replicate",
        "ai",
        "mcp",
        "video generation",
        "server replicate",
        "image generation"
      ],
      "category": "image-and-video-generation"
    },
    "vishwa684--unet": {
      "owner": "vishwa684",
      "name": "unet",
      "url": "https://github.com/vishwa684/unet",
      "imageUrl": "/freedevtools/mcp/pfp/vishwa684.webp",
      "description": "Train and deploy U-Net models for biomedical image segmentation using the Medical Decathlon dataset, with support for both 2D and 3D U-Net scripts. Visualize predictions and assess model performance through comprehensive demos and visual outputs.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-05-11T00:13:12Z",
      "readme_content": "# Deep Learning Medical Decathlon Demos for Python*\n### U-Net Biomedical Image Segmentation with Medical Decathlon Dataset.\n\nThis repository contains [2D](https://github.com/IntelAI/unet/tree/master/2D) and [3D](https://github.com/IntelAI/unet/tree/master/3D) U-Net scripts for training models using the [Medical Decathlon](http://medicaldecathlon.com/) dataset (http://medicaldecathlon.com/).\n\n![pred152_3D](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_152_img3D.gif\n\"BRATS image #152:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\").  ![pred195](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif \"BRATS image #195:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\")\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "segmentation",
        "biomedical",
        "dataset",
        "biomedical image",
        "models biomedical",
        "decathlon dataset"
      ],
      "category": "image-and-video-generation"
    },
    "wheattoast11--mcp-video-gen": {
      "owner": "wheattoast11",
      "name": "mcp-video-gen",
      "url": "https://github.com/wheattoast11/mcp-video-gen",
      "imageUrl": "/freedevtools/mcp/pfp/wheattoast11.webp",
      "description": "Generate videos and images from text prompts or existing images using advanced AI models, with capabilities for audio addition, content upscaling, and prompt enhancement. Manage and refine AI-generated content through API interactions with RunwayML and Luma AI.",
      "stars": 11,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T02:58:53Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/wheattoast11-mcp-video-gen-badge.png)](https://mseep.ai/app/wheattoast11-mcp-video-gen)\n\n# RunwayML + Luma AI MCP Server\n\nThis MCP server provides tools to interact with the RunwayML and Luma AI APIs for video and image generation tasks.\n\n## Features\n\n*   Generate videos from text prompts (RunwayML or Luma AI).\n*   Generate videos from images (RunwayML or Luma AI).\n*   Generate images from text prompts (Luma AI).\n*   Manage Luma AI generations (list, get, delete).\n*   Add audio to Luma AI generations.\n*   Upscale Luma AI generations.\n*   Enhance prompts using OpenRouter LLMs before generation.\n\n## Prerequisites\n\n*   Node.js (v18 LTS or later recommended)\n*   npm (usually included with Node.js)\n*   API Keys:\n    *   RunwayML API Secret\n    *   Luma AI API Key\n    *   OpenRouter API Key (for the `enhance_prompt` tool)\n\n## Installation\n\n1.  **Clone or Download:** Obtain the server code.\n2.  **Navigate to Directory:** Open a terminal in the server's root directory (`runwayml-mcp-server`).\n3.  **Install Dependencies:**\n    ```bash\n    npm install\n    ```\n\n## Configuration\n\n1.  **Create `.env` file:** In the server's root directory, create a file named `.env`.\n2.  **Add API Keys:** Add your API keys to the `.env` file:\n    ```dotenv\n    RUNWAYML_API_SECRET=your_runwayml_api_secret_here\n    LUMAAI_API_KEY=your_luma_api_key_here\n    OPENROUTER_API_KEY=your_openrouter_api_key_here\n    ```\n    Replace the placeholder values with your actual keys.\n\n## Running the Server\n\n1.  **Build the Server:** Compile the TypeScript code:\n    ```bash\n    npm run build\n    ```\n2.  **Start the Server:**\n    ```bash\n    npm start\n    ```\n    You should see a message like `RunwayML MCP server running on stdio` in your terminal's error output (stderr).\n\n## MCP Client Setup (e.g., Claude Desktop App, Cline)\n\nConfigure your MCP client to connect to this server. The exact steps depend on the client, but you'll typically need to provide:\n\n*   **Name:** A descriptive name (e.g., `runway-luma-server`)\n*   **Command:** `node`\n*   **Arguments:** The full path to the compiled server index file (e.g., `/path/to/your/runwayml-mcp-server/build/server-index.js`)\n*   **Environment Variables:**\n    *   `RUNWAYML_API_SECRET`: Your RunwayML API Secret\n    *   `LUMAAI_API_KEY`: Your Luma AI API Key\n    *   `OPENROUTER_API_KEY`: Your OpenRouter API Key\n\n**Example Configuration (Conceptual):**\n\n```json\n{\n  \"mcpServers\": {\n    \"runway-luma-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/runwayml-mcp-server/build/server-index.js\"],\n      \"env\": {\n        \"RUNWAYML_API_SECRET\": \"your_runwayml_api_secret_here\",\n        \"LUMAAI_API_KEY\": \"your_luma_api_key_here\",\n        \"OPENROUTER_API_KEY\": \"your_openrouter_api_key_here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n*(Remember to replace `/full/path/to/` with the actual path on your system)*\n\n## Available Tools\n\n*   **`generate_text_to_video`**: Generates video from text.\n    *   `provider`: (Optional) `runwayml` (default) or `lumaai`.\n    *   `promptText`: (Required) The text prompt.\n    *   `runway_model`: (Optional) Runway model (e.g., \"gen-2\").\n    *   `runway_resolution`: (Optional) Runway resolution (`1280:768` or `768:1280`).\n    *   `runway_watermark`: (Optional) Boolean, default `false`.\n    *   `luma_model`: (Optional) Luma model (`ray-flash-2`, `ray-2` (default), `ray-1-6`).\n    *   `luma_aspect_ratio`: (Optional) Luma aspect ratio (e.g., `16:9` (default), `1:1`).\n    *   `luma_loop`: (Optional) Boolean.\n    *   `duration`: (Optional) Video duration in seconds (number).\n    *   `seed`: (Optional) Generation seed (number).\n*   **`generate_image_to_video`**: Generates video from an image.\n    *   `provider`: (Optional) `runwayml` (default) or `lumaai`.\n    *   `promptImage`: (Required) URL of the input image, or for Runway, an array `[{uri: \"url\", position: \"first\" | \"last\"}]`.\n    *   `promptText`: (Optional) Text prompt to accompany the image.\n    *   `runway_model`: (Optional) Runway model (`gen3a_turbo` (default)).\n    *   `runway_duration`: (Optional) Runway duration (`5` (default) or `10`).\n    *   `runway_ratio`: (Optional) Runway resolution (`1280:768` or `768:1280`).\n    *   `runway_watermark`: (Optional) Boolean, default `false`.\n    *   `luma_model`: (Optional) Luma model (`ray-flash-2`, `ray-2` (default), `ray-1-6`).\n    *   `luma_aspect_ratio`: (Optional) Luma aspect ratio (e.g., `16:9` (default)).\n    *   `luma_loop`: (Optional) Boolean.\n    *   `seed`: (Optional) Generation seed (number).\n*   **`enhance_prompt`**: Refines a prompt using OpenRouter.\n    *   `original_prompt`: (Required) The prompt to enhance.\n    *   `model`: (Optional) OpenRouter model name (defaults to a capable model like `anthropic/claude-3.5-sonnet`).\n    *   `instructions`: (Optional) Specific instructions for the enhancement.\n*   **`luma_generate_image`**: Generates an image using Luma AI.\n    *   `prompt`: (Required) Text prompt.\n    *   `aspect_ratio`: (Optional) Luma aspect ratio (`16:9` (default)).\n    *   `model`: (Optional) Luma image model (`photon-1` (default), `photon-flash-1`).\n    *   `image_ref`: (Optional) Array of image reference objects (`{url: string, weight?: number}`). Max 4.\n    *   `style_ref`: (Optional) Array of style reference objects (`{url: string, weight?: number}`). Max 1.\n    *   `character_ref`: (Optional) Character reference object (`{ identity0: { images: [url1, ...] } }`).\n    *   `modify_image_ref`: (Optional) Modify image reference object (`{url: string, weight?: number}`).\n*   **`luma_list_generations`**: Lists previous Luma AI generations.\n    *   `limit`: (Optional) Number of results (default 10).\n    *   `offset`: (Optional) Offset for pagination (default 0).\n*   **`luma_get_generation`**: Gets details for a specific Luma AI generation.\n    *   `generation_id`: (Required) UUID of the generation.\n*   **`luma_delete_generation`**: Deletes a specific Luma AI generation.\n    *   `generation_id`: (Required) UUID of the generation.\n*   **`luma_get_camera_motions`**: Lists supported camera motions for Luma AI prompts. (No parameters).\n*   **`luma_add_audio`**: Adds audio to a Luma generation.\n    *   `generation_id`: (Required) UUID of the generation.\n    *   `prompt`: (Required) Prompt for the audio.\n    *   `negative_prompt`: (Optional) Negative prompt for audio.\n*   **`luma_upscale`**: Upscales a Luma generation.\n    *   `generation_id`: (Required) UUID of the generation.\n    *   `resolution`: (Optional) Target resolution (`1080p` (default) or `4k`).\n\n*(Note: For tools involving generation (`generate_*`, `luma_upscale`), the server initiates the task and returns immediately. Progress updates and the final result URL will be sent via MCP progress notifications.)*\n\n## Example Workflows\n\nHere are examples of how to combine the server's tools for common use cases:\n\n### 1. Music Video Snippet (Cyberpunk Noir)\n\n**Goal:** Create a 5-second cyberpunk noir video clip for the lyric \"Neon rivers flowing through a city of chrome\".\n\n**Steps:**\n\n1.  **Generate Base Image (Luma):**\n    ```json\n    {\n      \"tool_name\": \"luma_generate_image\",\n      \"arguments\": {\n        \"prompt\": \"Overhead shot of a dark, rainy cyberpunk city street at night. Bright neon signs reflect on wet pavement, resembling rivers of light flowing between towering chrome skyscrapers. Film noir aesthetic, photorealistic.\",\n        \"aspect_ratio\": \"16:9\"\n      }\n    }\n    ```\n    *(Wait for image generation to complete and get the image URL)*\n\n2.  **Animate Image (Luma):**\n    ```json\n    {\n      \"tool_name\": \"generate_image_to_video\",\n      \"arguments\": {\n        \"provider\": \"lumaai\",\n        \"promptImage\": \"{IMAGE_URL_FROM_STEP_1}\",\n        \"promptText\": \"Slow pan left across the rainy cyberpunk cityscape, neon lights flickering subtly.\",\n        \"luma_aspect_ratio\": \"16:9\",\n        \"duration\": 5\n      }\n    }\n    ```\n    *(Wait for video generation to complete)*\n\n### 2. Product Ad Concept (Floating Earbud)\n\n**Goal:** Create a 5-second video showing a futuristic earbud floating in a minimalist environment.\n\n**Steps:**\n\n1.  **Generate Scene with Product Reference (Luma):**\n    ```json\n    {\n      \"tool_name\": \"luma_generate_image\",\n      \"arguments\": {\n        \"prompt\": \"A single, sleek futuristic wireless earbud floats weightlessly in the center of a bright, minimalist white room with soft, diffused ambient light. Zero gravity effect.\",\n        \"aspect_ratio\": \"1:1\",\n        \"image_ref\": [{ \"url\": \"{PRODUCT_IMAGE_URL}\", \"weight\": 0.8 }]\n      }\n    }\n    ```\n    *(Wait for image generation to complete and get the image URL)*\n\n2.  **Animate Scene (Luma):**\n    ```json\n    {\n      \"tool_name\": \"generate_image_to_video\",\n      \"arguments\": {\n        \"provider\": \"lumaai\",\n        \"promptImage\": \"{IMAGE_URL_FROM_STEP_1}\",\n        \"promptText\": \"The earbud slowly rotates and drifts gently in zero gravity.\",\n        \"luma_aspect_ratio\": \"1:1\",\n        \"duration\": 5\n      }\n    }\n    ```\n    *(Wait for video generation to complete)*\n\n### 3. Image Animation (RunwayML Gen3a)\n\n**Goal:** Animate an existing image using RunwayML's Gen3a model.\n\n**Steps:**\n\n1.  **(Optional) Generate Base Image (Luma):** Use `luma_generate_image` if you don't have an image.\n2.  **Animate Image (RunwayML):**\n    ```json\n    {\n      \"tool_name\": \"generate_image_to_video\",\n      \"arguments\": {\n        \"provider\": \"runwayml\",\n        \"promptImage\": \"{YOUR_IMAGE_URL}\",\n        \"promptText\": \"Subtle zoom in, cinematic lighting.\",\n        \"runway_model\": \"gen3a_turbo\",\n        \"runway_duration\": \"5\",\n        \"runway_ratio\": \"1280:768\" // Or \"768:1280\"\n      }\n    }\n    ```\n    *(Wait for video generation to complete)*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "videos",
        "runwayml",
        "video generation",
        "generate videos",
        "ai generated"
      ],
      "category": "image-and-video-generation"
    },
    "xenoailimited--mcp-mavae": {
      "owner": "xenoailimited",
      "name": "mcp-mavae",
      "url": "https://github.com/xenoailimited/mcp-mavae",
      "imageUrl": "/freedevtools/mcp/pfp/xenoailimited.webp",
      "description": "A Model Context Protocol (MCP) server for interacting with image media tools, providing capabilities for image generation, editing, and management of collections and models.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-24T16:15:59Z",
      "readme_content": "# MAVAE - IMAGE TOOLBOX\nA powerful creative and editing toolkit designed for AI Agents.\n\n[![smithery badge](https://smithery.ai/badge/@xenoailimited/mavae-image-toolbox)](https://smithery.ai/server/@xenoailimited/mavae-image-toolbox)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Node.js](https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white)](https://nodejs.org/)\n[![MCP](https://img.shields.io/badge/MCP-Model_Context_Protocol-blue?style=for-the-badge)](https://github.com/anthropics/model-context-protocol)\n[![Docker](https://img.shields.io/badge/Docker-2CA5E0?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)\n\nMAVAE is a Model Context Protocol (MCP) server for interacting with image media tools. It provides a standardized interface for AI Agents to generate and manipulate images.\n\n## 🚀 Features\n\n- **Image Generation**: Generate images using both raw configurations and predefined collections\n- **Image Editing**: Compress, crop, and resize images with proportional or fixed dimensions\n- **Collection Management**: Create, manage, and share configurations for consistent image generation\n- **Model & Lora Management**: List and utilize available models and Loras\n- **API Token Management**: Handle authentication for secure interaction with Mavae services\n\n## 📋 Prerequisites\n\n- Node.js (v16 or higher)\n- MAVAE API Key (set as environment variable, [Apply here](https://mcp.mavae.ai/))\n\n## 🛠️ Installation\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Start the server\nnpm start\n```\n\n## MCP Json\n```json\n{\n  \"mcpServers\": {\n      \"mavae\": {\n          \"command\": \"node\",\n          \"args\": [\n              \"***/dist/index.js\"\n          ],\n          \"env\": {\n              \"MAVAE_API_KEY\": MAVAE_API_KEY\n          }\n      }\n  }\n}\n```\nWhen using MAVAE MCP locally, this path is an absolute path 👉🏻 \"***/dist/index.js\"\n\n## 🐳 Docker Support\n\n```bash\n# Build Docker image\ndocker build -t mavae-mcp-server .\n\n# Run Docker container\ndocker run -e MAVAE_API_KEY=your_api_key mavae-mcp-server\n```\n\n## 📁 Project Structure\n\n```\nmavae/\n├── src/                  # Source code\n│   ├── actions/          # API endpoint implementation handlers\n│   │   ├── aigc.ts       # Image generation operations\n│   │   ├── collection.ts # Collection management operations\n│   │   ├── edit.ts       # Image editing operations\n│   │   └── token.ts      # API token operations\n│   ├── tools/            # MCP tool definitions\n│   │   ├── aigc.ts       # Image generation tool definitions\n│   │   ├── collection.ts # Collection management tool definitions\n│   │   └── edit.ts       # Image editing tool definitions\n│   ├── types/            # TypeScript type definitions\n│   │   ├── aigc.ts       # Image generation types\n│   │   ├── collection.ts # Collection types\n│   │   ├── edit.ts       # Image editing types\n│   │   └── response.ts   # API response types\n│   ├── utils/            # Utility functions\n│   │   └── constants.ts  # Constant values\n│   └── index.ts          # Server entry point\n├── dist/                 # Compiled JavaScript files\n├── package.json          # Project dependencies and scripts\n└── tsconfig.json         # TypeScript configuration\n```\n\n## 🛍️ Available Tools\n\n### Image Generation\n- `image_raw_generate` - Generate an image using raw AIGC configuration\n- `image_collection_generate` - Generate an image using a collection's AIGC configuration\n- `image_retry_generate` - Retry a failed image generation\n- `image_state` - Get the details of an owned image\n- `generate_task_state` - Get the generation state of an image by task id\n\n### Collection Management\n- `collection_create` - Create a new collection\n- `collection_delete` - Delete a collection\n- `collection_toggle_public` - Toggle the public status of a collection\n- `collection_list` - Get the list of owned collections\n- `collection_state` - Get the details of an owned collection\n\n### Image Editing\n- `compress_image` - Lossless compression of images\n- `crop_image` - Crop images with local path and URL support\n- `resize_image` - Resize images with proportional or fixed dimensions\n\n### Model & Resources\n- `list_images` - Get the list of owned images\n- `list_loras` - Get the list of available loras\n- `list_models` - Get the list of available models\n\n### Authentication\n- `token_state` - Get the x-api-token state\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "mavae",
        "protocol",
        "mcp mavae",
        "xenoailimited mcp",
        "mcp server"
      ],
      "category": "image-and-video-generation"
    },
    "xiyuefox--mcp-hfspace": {
      "owner": "xiyuefox",
      "name": "mcp-hfspace",
      "url": "https://github.com/xiyuefox/mcp-hfspace",
      "imageUrl": "/freedevtools/mcp/pfp/xiyuefox.webp",
      "description": "Connect to Hugging Face Spaces to access various AI models for tasks like image generation and text-to-speech with minimal setup. Leverage the default model for seamless integration into applications.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-10T03:52:55Z",
      "readme_content": "# mcp-hfspace MCP Server 🤗\n\nRead the introduction here [llmindset.co.uk/resources/mcp-hfspace/](https://llmindset.co.uk/resources/mcp-hfspace/)\n\nConnect to [Hugging Face Spaces](https://huggingface.co/spaces)  with minimal setup needed - simply add your spaces and go!\n\nBy default, it connects to `evalstate/FLUX.1-schnell` providing Image Generation capabilities to Claude Desktop.\n\n![Default Setup](./images/2024-12-09-flower.png)\n\n## Installation\n\nNPM Package is `@llmindset/mcp-hfspsace`.\n\nInstall a recent version of [NodeJS](https://nodejs.org/en/download) for your platform, then add the following to the `mcpServers` section of your `claude_desktop_config.json` file:\n\n```json\n    \"mcp=hfspace\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\"\n      ]\n    }\n```\n\nPlease make sure you are using Claude Desktop 0.78 or greater.\n\nThis will get you started with an Image Generator.\n\n### Basic setup\n\nSupply a list of HuggingFace spaces in the arguments. mcp-hfspace will find the most appropriate endpoint and automatically configure it for usage. An example `claude_desktop_config.json` is supplied [below](#installation).\n\nBy default the current working directory is used for file upload/download. On Windows this is a read/write folder at `\\users\\<username>\\AppData\\Roaming\\Claude\\<version.number\\`, and on MacOS it is the is the read-only root: `/`.\n\nIt is recommended to override this and set a Working Directory for handling the upload and download of images and other file-based content. Specify either the `--work-dir=/your_directory` argument or `MCP_HF_WORK_DIR` environment variable.\n\nAn example configuration for using a modern image generator, vision model and text to speech is below with a working directory set is below:\n\n```json\n    \"mcp-hfspace\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\",\n        \"--work-dir=/Users/evalstate/mcp-store\",\n        \"shuttleai/shuttle-jaguar\",\n        \"styletts2/styletts2\",\n        \"Qwen/QVQ-72B-preview\"\n      ]\n    }\n```\n\n\nTo use private spaces, supply your Hugging Face Token with either the  `--hf-token=hf_...` argument or `HF_TOKEN` environment variable.\n\nIt's possible to run multiple server instances to use different working directories and tokens if needed.\n\n## File Handling and Claude Desktop Mode\n\nBy default, the Server operates in _Claude Desktop Mode_. In this mode, Images are returned in the tool responses, while other files are saved in the working folder, their file path is returned as a message. This will usually give the best experience if using Claude Desktop as the client.\n\nURLs can also be supplied as inputs: the content gets passed to the Space.\n\nThere is an \"Available Resources\" prompt that gives Claude the available files and mime types from your working directory. This is currently the best way to manage files.\n\n### Example 1 - Image Generation (Download Image / Claude Vision)\n\nWe'll use Claude to compare images created by `shuttleai/shuttle-3.1-aesthetic` and `FLUX.1-schnell`. The images gets saved to the Work Directory, as well as included in Claude's context window - so Claude can use its vision capabilities.\n\n![Image Generation Comparison](./images/2024-12-05-flux-shuttle.png)\n\n### Example 2 - Vision Model (Upload Image)\n\nWe'll use `merve/paligemma2-vqav2` [space link](https://huggingface.co/spaces/merve/paligemma2-vqav2) to query an image. In this case, we specify the filename which is available in the Working Directory: we  don't want to upload the Image directly to Claude's context window. So, we can prompt Claude:\n\n`use paligemma to find out who is in \"test_gemma.jpg\"` -> `Text Output: david bowie`\n![Vision - File Upload](./images/2024-12-09-bowie.png)\n\n_If you are uploading something to Claude's context use the Paperclip Attachment button, otherwise specify the filename for the Server to send directly._\n\nWe can also supply a URL. For example : `use paligemma to detect humans in https://e3.365dm.com/24/12/1600x900/skynews-taylor-swift-eras-tour_6771083.jpg?20241209000914` -> `One person is detected in the image - Taylor Swift on stage.`\n\n### Example 3 - Text-to-Speech (Download Audio)\n\nIn _Claude Desktop Mode_, the audio file is saved in the WORK_DIR, and Claude is notified of the creation. If not in desktop mode, the file is returned as a base64 encoded resource to the Client (useful if it supports embedded Audio attachments).\n\n![Voice Production](./images/2024-12-08-mcp-parler.png)\n\n### Example 4 - Speech-to-Text (Upload Audio)\n\nHere, we use `hf-audio/whisper-large-v3-turbo` to transcribe some audio, and make it available to Claude.\n\n![Audio Transcribe](./images/2024-12-09-transcribe.png)\n\n### Example 5 - Image-to-Image\n\nIn this example, we specify the filename for `microsoft/OmniParser` to use, and get returned an annotated Image and 2 separate pieces of text: descriptions and coordinates. The prompt used was `use omniparser to analyse ./screenshot.png` and `use the analysis to produce an artifact that reproduces that screen`. `DawnC/Pawmatch` is also good at this.\n\n![Omniparser and Artifact](./images/2024-12-08-mcp-omni-artifact.png)\n\n### Example 6 - Chat\n\nIn this example, Claude sets a number of reasoning puzzles for Qwen, and asks follow-up questions for clarification.\n\n![Qwen Reasoning Test](./images/2024-12-09-qwen-reason.png)\n\n### Specifying API Endpoint\n\nIf you need, you can specify a specific API Endpoint by adding it to the spacename. So rather than passing in `Qwen/Qwen2.5-72B-Instruct` you would use `Qwen/Qwen2.5-72B-Instruct/model_chat`.\n\n### Claude Desktop Mode\n\nThis can be disabled with the option --desktop-mode=false or the environment variable CLAUDE_DESKTOP_MODE=false. In this case, content as returned as an embedded Base64 encoded Resource.\n\n## Recommended Spaces\n\nSome recommended spaces to try:\n\n### Image Generation\n\n- shuttleai/shuttle-3.1-aesthetic\n- black-forest-labs/FLUX.1-schnell\n- yanze/PuLID-FLUX\n- Inspyrenet-Rembg (Background Removal)\n- diyism/Datou1111-shou_xin - [Beautiful Pencil Drawings](https://x.com/ClementDelangue/status/1867318931502895358) \n\n### Chat\n\n- Qwen/Qwen2.5-72B-Instruct\n- prithivMLmods/Mistral-7B-Instruct-v0.3\n\n### Text-to-speech / Audio Generation\n\n- fantaxy/Sound-AI-SFX\n- parler-tts/parler_tts\n\n### Speech-to-text\n\n- hf-audio/whisper-large-v3-turbo\n- (the openai models use unnamed parameters so will not work)\n\n### Text-to-music\n\n- haoheliu/audioldm2-text2audio-text2music\n\n### Vision Tasks\n\n- microsoft/OmniParser\n- merve/paligemma2-vqav2\n- merve/paligemma-doc\n- DawnC/PawMatchAI \n- DawnC/PawMatchAI/on_find_match_click - for interactive dog recommendations\n\n## Other Features\n\n### Prompts\n\nPrompts for each Space are generated, and provide an opportunity to input. Bear in mind that often Spaces aren't configured with particularly helpful labels etc. Claude is actually very good at figuring this out, and the Tool description is quite rich (but not visible in Claude Desktop).\n\n### Resources\n\nA list of files in the WORK_DIR is returned, and as a convenience returns the name as \"Use the file...\" text. If you want to add something to Claude's context, use the paperclip - otherwise specify the filename for the MCP Server. Claude does not support transmitting resources from within Context.\n\n### Private Spaces\n\nPrivate Spaces are supported with a HuggingFace token. The Token is used to download and save generated content.\n\n### Using Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hfspace\": {\n      \"command\": \"npx\"\n      \"args:\" [\n        \"-y\",\n        \"@llmindset/mcp-hfspace\",\n        \"--work-dir=~/mcp-files/ or x:/temp/mcp-files/\",\n        \"--HF_TOKEN=HF_{optional token}\"\n        \"Qwen/Qwen2-72B-Instruct\",\n        \"black-forest-labs/FLUX.1-schnell\",\n        \"space/example/specific-endpint\"\n        (... and so on)\n        ]\n    }\n  }\n}\n```\n\n## Known Issues and Limitations\n\n### mcp-hfspace\n\n- Endpoints with unnamed parameters are unsupported for the moment.\n- Full translation from some complex Python types to suitable MCP formats.\n\n### Claude Desktop\n\n- Claude Desktop 0.75 doesn't seem to respond to errors from the MCP Server, timing out instead. For persistent issues, use the MCP Inspector to get a better look at diagnosing what's going wrong. If something suddenly stops working, it's probably due to exhausting your HuggingFace ZeroGPU quota - try again after a short period, or set up your own Space for hosting.\n- Claude Desktop seems to use a hard timeout value of 60s, and doesn't appear  to use Progress Notifications to manage UX or keep-alive. If you are using ZeroGPU spaces, large/heavy jobs may timeout. Check the WORK_DIR for results though; the MCP Server will still capture and save the result if it was produced.\n- Claude Desktops reporting of Server Status, logging etc. isn't great - use [@modelcontextprotocol/inspector](https://github.com/modelcontextprotocol/inspector) to help diagnose issues.\n\n### HuggingFace Spaces\n\n- If ZeroGPU quotas or queues are too long, try duplicating the space. If your job takes less than sixty seconds, you can usually change the function decorator `@spaces.GPU(duration=20)` in `app.py` to request less quota when running the job.\n- If you have a HuggingFace Pro account, please note that The Gradio API does not your additional quote for ZeroGPU jobs - you will need to set an `X-IP-Token` header to achieve that.\n- If you have a private space, and dedicated hardware your HF_TOKEN will give you direct access to that - no quota's apply. I recommend this if you are using for any kind of Production task.\n\n## Third Party MCP Services\n\n<a href=\"https://glama.ai/mcp/servers/s57c80wvgq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/s57c80wvgq/badge\" alt=\"mcp-hfspace MCP server\" /></a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hfspace",
        "xiyuefox",
        "ai",
        "xiyuefox mcp",
        "mcp hfspace",
        "image generation"
      ],
      "category": "image-and-video-generation"
    },
    "xoy8n--webp-converter": {
      "owner": "xoy8n",
      "name": "webp-converter",
      "url": "https://github.com/xoy8n/webp-converter",
      "imageUrl": "/freedevtools/mcp/pfp/xoy8n.webp",
      "description": "A server that converts image files such as PNG, JPG, and JPEG to WebP format, supporting both single and batch conversions. It allows configuration of quality settings and provides detailed conversion reports.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-19T01:34:07Z",
      "readme_content": "# WebP Conversion MCP Server\n\nThis project is a Model Context Protocol (MCP) server that converts image files to WebP format.\n\n## Features\n\n- Convert PNG, JPG, and JPEG files to WebP\n- Support for single image or batch image conversion\n- Option to configure quality and lossless compression\n- Option to keep original files\n- Provides a detailed report of the conversion result\n\n### Installation & Execution\n\n```bash\nnpx -y @xoy8n/webp-converter@latest\n```\n\n### Cursor mcp.json Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"webp-converter\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@xoy8n/webp-converter@latest\"]\n    }\n  }\n}\n```\n\n## MCP Tool List\n\n### 1. convert_to_webp\n\nConverts a single image file to WebP format.\n\n**Parameters:**\n\n- `image_path`: Path to the image file to convert\n- `base_path`: Base directory path\n- `quality`: WebP quality setting (default: 95)\n- `lossless`: Whether to use lossless compression (default: false)\n- `keep_original`: Whether to retain the original file (default: false)\n\n**Returns:**\n\n- Conversion success status\n- Input/output file paths\n- File size before/after conversion\n- Applied quality and compression settings\n\n### 2. batch_convert_to_webp\n\nConverts multiple image files to WebP format in one go.\n\n**Parameters:**\n\n- `image_paths`: Array of paths to image files to convert\n- `base_path`: Base directory path (optional)\n- `quality`: WebP quality setting (default: 95)\n- `lossless`: Whether to use lossless compression (default: false)\n- `keep_original`: Whether to retain the original files (default: false)\n\n**Returns:**\n\n- Array of conversion results for each image file\n\n## How to Use\n\n1. Select the image files you want to convert.\n2. Run the `convert_to_webp` or `batch_convert_to_webp` command via the MCP tools.\n3. Check the conversion results.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webp",
        "jpeg",
        "converter",
        "webp converter",
        "jpeg webp",
        "xoy8n webp"
      ],
      "category": "image-and-video-generation"
    },
    "yanjunz--mcp_search_images": {
      "owner": "yanjunz",
      "name": "mcp_search_images",
      "url": "https://github.com/yanjunz/mcp_search_images",
      "imageUrl": "/freedevtools/mcp/pfp/yanjunz.webp",
      "description": "Search for high-quality images from sources like Unsplash, Pexels, and Pixabay, and generate custom icons based on text descriptions, facilitating visual enhancements for projects.",
      "stars": 10,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T13:46:13Z",
      "readme_content": "# MCP 图像搜索与图标生成服务\n\n基于多个图片API的搜索服务和图标生成功能，专门设计用于与 Cursor MCP 服务集成。支持图片搜索、下载和AI生成图标。\n\n![MCP图像搜索工具示例](examples/mcp_search_example.png)\n\n## 工作原理\n\n本工具通过MCP (Model Control Protocol) 为Cursor IDE提供图像搜索和图标生成功能：\n\n1. **搜索图片**: 连接Unsplash、Pexels和Pixabay等图片源，根据关键词搜索高质量图片\n2. **下载图片**: 将搜索到的图片下载到指定位置，方便直接在项目中使用\n3. **生成图标**: 基于文本描述生成自定义图标，满足项目UI需求\n\n### 系统工作流程\n\n```\n用户 (在Cursor中) → 向Claude/大模型提问 → 大模型调用MCP工具 → 工具处理请求 → 返回结果 → 大模型展示结果\n```\n\n比如，你可以在Cursor中向Claude询问\"帮我找5张关于太空的图片\"，Claude会通过MCP工具搜索并展示图片，然后你可以进一步要求下载或生成特定图标。\n\n## 功能特点\n\n* 支持多个图片源搜索 (Unsplash, Pexels, Pixabay)\n* 高质量图标生成 (基于Together AI)\n* 简单易用的API\n* 完整的错误处理\n* 自定义保存路径和文件名\n* 可调整图片尺寸\n\n## 环境准备\n\n### 1. Python 环境\n\n* Python 3.10+\n* 下载地址： https://www.python.org/downloads/\n* 推荐使用 pyenv 管理 Python 版本：\n\n```bash\n# macOS 安装 pyenv\nbrew install pyenv\n\n# 安装 Python\npyenv install 3.13.2\npyenv global 3.13.2\n```\n\n### 2. uv 包管理工具\n\nuv 是一个快速的 Python 包管理器，需要先安装：\n\n```bash\n# macOS 安装 uv\nbrew install uv\n\n# 或者使用 pip 安装\npip install uv\n```\n\n### 3. 图片API密钥\n\n#### Unsplash API 密钥\n1. 访问 [Unsplash Developers](https://unsplash.com/developers)\n2. 注册/登录账号\n3. 创建新的应用程序\n4. 获取 Access Key\n\n#### Pexels API 密钥\n1. 访问 [Pexels API](https://www.pexels.com/api/)\n2. 注册/登录账号\n3. 请求API密钥\n\n#### Pixabay API 密钥\n1. 访问 [Pixabay API](https://pixabay.com/api/docs/)\n2. 注册/登录账号\n3. 获取API密钥\n\n#### Together AI API 密钥\n1. 访问 [Together AI API Keys](https://api.together.xyz/keys)\n2. 注册/登录账号\n3. 创建新的 API 密钥\n\n### 4. Cursor\n\n* 下载并安装 [Cursor IDE](https://cursor.sh/)\n* 确保 Cursor 已正确配置 Python 环境\n\n## 安装配置\n\n1. 克隆项目：\n\n```bash\ngit clone https://github.com/yanjunz/mcp_search_images.git\n```\n\n2. 安装依赖：\n\n```bash\npython3 -m pip install fastmcp requests\n```\n\n出现证书问题可以使用：\n\n```bash\npython3 -m pip install fastmcp requests --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade --force-reinstall --no-cache-dir\n```\n\n3. 配置 API 密钥：\n\n从模板创建配置文件：\n\n```bash\n# 复制模板文件作为配置文件\ncp config.json.template config.json\n\n# 编辑配置文件，设置API密钥\nnano config.json  # 或使用其他编辑器\n```\n\n在 `config.json` 中修改以下配置：\n\n```json\n{\n    \"api\": {\n        \"unsplash_access_key\": \"你的Unsplash访问密钥\",\n        \"pexels_api_key\": \"你的Pexels API密钥\",\n        \"pixabay_api_key\": \"你的Pixabay API密钥\",\n        \"together_api_key\": \"你的Together API密钥\",\n        \"timeout\": 30,\n        \"max_retries\": 3,\n        \"retry_delay\": 5\n    },\n    // ...其他配置...\n}\n```\n\n> **注意**：请确保不要将包含API密钥的配置文件提交到版本控制系统中。\n> 项目中的 `.gitignore` 文件已配置为忽略 `config.json`，但保留 `config.json.template`。\n\n## 运行服务\n\n### 方法一：直接使用Python运行\n\n这是最简单的方式，直接使用Python运行服务：\n\n```bash\npython3.11 main.py\n```\n\n服务启动后会显示以下信息:\n```\n启动图片搜索服务 - 端口: 5173\n提供的工具: search_images, download_image, generate_icon\nINFO:     Started server process [xxxxx]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:5173 (Press CTRL+C to quit)\n```\n\n### 方法二：使用fastmcp命令运行\n\n如果您安装了fastmcp包，也可以使用fastmcp命令运行：\n\n1. 开发模式运行（带调试界面）：\n\n```bash\nfastmcp dev main.py\n```\n\n2. 生产模式运行：\n\n```bash\nfastmcp run main.py\n```\n\n3. 如果端口被占用，可以指定其他端口：\n\n```bash\nPORT=5174 fastmcp dev main.py\n```\n\n### 方法三：使用uv运行\n\n如果您使用uv作为包管理器：\n\n```bash\nuv run --with fastmcp fastmcp run main.py\n```\n\n或者在开发模式下：\n\n```bash\nuv run --with fastmcp fastmcp dev main.py\n```\n\n### Cursor与MCP的工作原理\n\n为了更好地理解和解决连接问题，以下是Cursor与MCP服务交互的基本工作原理：\n\n1. **MCP服务启动流程**：\n   * 当运行`python3.11 main.py`时，服务初始化并创建SSE（Server-Sent Events）应用\n   * 服务在指定端口（默认5173）开始监听请求\n   * 服务注册工具函数（search_images, download_image, generate_icon）\n   * 对于使用ServerLink方式的连接，服务需要在`/sse`路径上正确处理SSE请求\n\n2. **Cursor连接流程**：\n   * 当在Cursor设置中添加MCP工具时，Cursor尝试与提供的URL建立连接\n   * Cursor发送初始化请求，检查服务是否正常响应\n   * 服务需要返回正确的MCP协议响应，包括可用工具列表\n   * 连接成功后，Cursor会将该工具添加到可用工具列表\n   \n3. **诊断连接问题**：\n   * 检查服务是否在运行：`lsof -i :5173`\n   * 检查网络连接：`curl http://localhost:5173`\n   * 检查服务是否正确实现MCP协议：服务启动日志应显示注册的工具\n   * 检查防火墙和网络权限：本地服务有时可能被防火墙阻止\n   \n4. **完整的测试流程**：\n   ```bash\n   # 1. 停止任何可能正在运行的服务\n   pkill -f \"python.*main.py\"\n   \n   # 2. 启动服务（在前台运行以查看日志）\n   python3.11 main.py\n   \n   # 3. 在新的终端窗口中，测试连接\n   curl http://localhost:5173\n   \n   # 4. 测试SSE端点（用于ServerLink方式）\n   curl http://localhost:5173/sse\n   \n   # 5. 在Cursor中添加MCP工具并测试\n   ```\n\n如果按照以上步骤操作后仍然无法连接，可能需要检查Python版本兼容性或依赖包是否正确安装。有时重新安装依赖包也有帮助：\n\n```bash\npython3.11 -m pip uninstall fastmcp mcp uvicorn starlette -y\npython3.11 -m pip install fastmcp mcp uvicorn starlette\n```\n\n## 使用说明\n\n### 在 Cursor IDE 中使用\n\n1. 确保服务正在运行\n   ```bash\n   # 直接运行Python脚本\n   python3.11 main.py\n   ```\n   服务启动后会显示以下信息:\n   ```\n   启动图片搜索服务 - 端口: 5173\n   提供的工具: search_images, download_image, generate_icon\n   INFO:     Started server process [xxxxx]\n   INFO:     Waiting for application startup.\n   INFO:     Application startup complete.\n   INFO:     Uvicorn running on http://0.0.0.0:5173 (Press CTRL+C to quit)\n   ```\n\n2. 在Cursor中添加MCP服务:\n   * 打开Cursor IDE\n   * 点击左下角的齿轮图标，打开设置\n   * 选择\"AI & Copilot\"设置\n   * 在\"MCP工具\"部分点击\"添加MCP工具\"\n   * 填写以下信息:\n     - 名称: 图片搜索服务\n     - 类型: SSE (Server-Sent Events)\n     - URL: http://localhost:5173\n     - 点击\"保存\"\n     \n   **备选配置方法**:\n   * 某些版本的Cursor可能需要使用ServerLink配置:\n     - 名称: 图片搜索服务\n     - 类型: sse\n     - ServerLink: http://localhost:5173/sse\n     - 点击\"保存\"\n\n   > **注意**: 如果出现\"Fail to create client\"错误，请检查以下几点:\n   > 1. 确认服务正在运行 (通过`lsof -i :5173`检查端口是否被监听)\n   > 2. 尝试在浏览器中访问`http://localhost:5173`测试连接性\n   > 3. 确保URL没有多余的斜杠或空格\n   > 4. 对于ServerLink方式，确保使用正确的端点路径`/sse`\n   > 5. 重启服务后再次尝试添加\n   > 6. 有时需要重启Cursor IDE以清除之前的连接缓存\n\n3. 开始使用MCP工具:\n   * 在Cursor中打开包含Claude或其他支持工具调用的大模型对话窗口\n   * 当服务正在运行时，大模型可以自动发现并使用该工具\n   * 如果大模型未自动发现工具，可以提示它:\"请使用图片搜索服务来查找图片\"\n\n4. 在开发过程中随时使用:\n   * 编写代码时需要图标素材，可以直接向大模型描述需求\n   * 例如:\"帮我找一些适合作为登录按钮的图标\"\n   * 大模型会调用MCP工具搜索图片并展示结果\n   * 你可以进一步要求下载或生成自定义图标\n\n5. 查看图标保存位置:\n   * 默认情况下，图标会保存在项目根目录下的`icons`文件夹中\n   * 可以通过以下命令查看已保存的图标:\n     ```bash\n     ls -la icons\n     ```\n\n### 功能使用示例\n\n#### 搜索图片\n\n可以直接向大模型描述需求:\n```\n搜索关键词为\"technology\"的图片\n```\n或更具体的描述:\n```\n请在Unsplash上搜索5张关于\"artificial intelligence\"的图片\n```\n\n#### 下载图片\n\n当大模型显示搜索结果后，你可以要求下载特定图片:\n```\n下载第2张图片并保存为tech-icon.png\n```\n或者指定保存路径:\n```\n将第3张图片下载到/Users/username/Desktop/，文件名为ai-image.jpg\n```\n\n#### 生成图标\n\n可以提供详细的描述来生成符合需求的图标:\n```\n生成一个蓝色科技风格的图标，保存为blue-tech.png\n```\n或者更详细的描述:\n```\n请创建一个扁平化设计的邮件图标，红色轮廓，白色背景，图标尺寸为256x256，保存为email-icon.png\n```\n\n### 实际对话示例\n\n查看[示例对话](examples/dialog_example.md)了解如何在实际使用中与Claude/大模型交互来搜索和生成图标。\n\n### 集成到项目工作流\n\n1. 在项目初始阶段批量生成图标:\n   * 创建设计系统时，可以一次性生成多个相关图标\n   * 例如:\"帮我生成一套包含主页、设置、用户、消息通知的应用图标\"\n\n2. 开发过程中按需搜索:\n   * 在编写代码时随时查找所需图片资源\n   * 例如:\"我正在开发一个天气应用，需要几个天气相关的图标\"\n\n3. 项目完善阶段定制图标:\n   * 根据应用风格统一优化图标\n   * 例如:\"生成一组与我当前应用风格一致的社交媒体分享图标\"\n\n### 最佳实践\n\n1. **使用明确的关键词**: 搜索时使用具体、明确的关键词获得更精确的结果\n2. **指定图片源**: 根据需求选择合适的图片源（Unsplash适合自然风光，Pixabay适合商业图片等）\n3. **保存结构化命名**: 为图标使用结构化命名，如`category-name-size.png`\n4. **批量操作**: 一次性请求多个相关图标而不是逐个请求\n5. **与代码结合**: 在实际开发中提及代码上下文，大模型可以更准确地理解你的需求\n\n## 错误排查\n\n### Cursor MCP连接错误\n\n如果在Cursor中添加MCP服务时遇到\"Fail to create client\"错误，请尝试以下解决方法：\n\n1. **检查服务状态**：\n   ```bash\n   # 检查服务是否正在运行\n   lsof -i :5173\n   # 如果没有输出，表示服务未运行，请启动服务\n   python3.11 main.py\n   ```\n\n2. **测试连接**：\n   ```bash\n   # 使用curl测试API连接\n   curl -v http://localhost:5173\n   ```\n\n3. **修改连接设置**：\n   * 确保选择了正确的连接类型：SSE\n   * 尝试使用IP地址代替localhost：`http://127.0.0.1:5173`\n   * 确保URL不含额外斜杠：使用`http://localhost:5173`而非`http://localhost:5173/`\n   * 尝试使用ServerLink方式配置：\n     - 类型: sse\n     - ServerLink: http://localhost:5173/sse\n   * 有些版本的Cursor可能对URL格式有特定要求，两种方式都值得尝试\n\n4. **重启组件**：\n   * 停止并重启MCP服务\n   * 重启Cursor IDE\n   * 如果使用macOS，检查防火墙设置是否阻止了连接\n\n5. **检查日志**：\n   * 观察服务启动时的日志输出\n   * 当尝试从Cursor连接时，查看服务端有无新的日志输出\n\n6. **尝试其他端口**：\n   * 修改代码中的端口（如改为5174）并重启服务：\n   ```python\n   uvicorn.run(sse_app, host=\"0.0.0.0\", port=5174)\n   ```\n\n### 其他常见问题\n\n如果遇到问题，请检查：\n\n1. 服务是否正常运行\n2. 保存路径是否正确\n3. 目录权限是否正确\n4. 网络连接是否正常\n5. API 密钥是否有效\n6. Python 环境是否正确配置\n7. uv 是否正确安装\n8. 依赖包是否完整安装\n\n## 贡献\n\n欢迎提交问题和拉取请求来改进项目。\n\n## 许可\n\n[MIT License](LICENSE) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_search_images",
        "images",
        "icons",
        "yanjunz mcp_search_images",
        "mcp_search_images search",
        "quality images"
      ],
      "category": "image-and-video-generation"
    },
    "yunwoong7--aws-nova-canvas-mcp": {
      "owner": "yunwoong7",
      "name": "aws-nova-canvas-mcp",
      "url": "https://github.com/yunwoong7/aws-nova-canvas-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/yunwoong7.webp",
      "description": "Generate and edit images with advanced features such as text-to-image generation, image inpainting, and background removal, using the Nova Canvas model from Amazon Bedrock.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-22T12:28:20Z",
      "readme_content": "<h2 align=\"center\">\nAWS Nova Canvas MCP Server\n</h2>\n\n<div align=\"center\">\n  <img src=\"https://img.shields.io/badge/Python-3.12-3776AB?logo=python\"/>\n  <img src=\"https://img.shields.io/badge/Amazon-Bedrock-FF9900?logo=amazon&logoColor=white\"/>\n</div>\n\nAn MCP server that allows you to generate and edit images using the Nova Canvas model of Amazon Bedrock.\n\n## Features\n\n- Text to Image\n- Image Inpainting\n- Image Outpainting\n- Image Variation\n- Image Conditioning\n- Color Guided Generation\n- Background Removal\n- Show Image Thumbnails\n\n## Installation\n\n### Claude Desktop Setup\n\n1. Configure Claude Desktop\n   * Click on **Claude > Settings** from the Claude Desktop menu.\n   * When the popup appears, select **Developer** from the left menu, and click the **Edit Settings** button.\n   * This will open a folder containing the settings file. The name of this settings file is:\n   * `claude_desktop_config.json`\n\n<div align=\"center\">\n<img src=\"https://blog.kakaocdn.net/dn/bIl5q9/btsM3U5Vjw5/aGruWqP3wNmWZ1sKrnhbPk/img.png\" width=\"70%\">\n</div>\n\n3. Add the following content to the settings file (Python version):\n\n   - python version\n\n     ```json\n     \"nova-canvas\": {\n       \"command\": \"uvx\",\n       \"args\": [\n         \"aws-nova-canvas-mcp\"\n       ],\n       \"env\": {\n         \"AWS_PROFILE\": \"YOUR_AWS_PROFILE\"\n       }\n     }\n     ```\n\n     > ✅ Only AWS_PROFILE is required. Other variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, and PORT are optional and not necessary if your AWS profile is set correctly.\n     >\n     > ​\t⚙️ If the setup is completed successfully, you can see that the \"nova-canvas\" item has been added in **Claude > Settings > Developer tab**.\n     > ⚠️ **Important:** MCP settings only work on the **Claude desktop app, not the Claude web browser version**\n\n## Image Save Location\n\nBy default, all generated or edited images will be saved in the following directory:\n\n* **macOS / Linux**:  `~/Desktop/aws-nova-canvas`\n* **Windows**:  `C:\\Users\\YourUsername\\Desktop\\aws-nova-canvas`\n\n> 📁 If no image save path is specified, the application will automatically create and use the folder above.\n\n<div align=\"center\">\n<img src=\"https://blog.kakaocdn.net/dn/bpUWLj/btsM4kJZC6v/HHQfQctKsevWnK6LCKEkv0/img.png\" width=\"70%\">\n</div>\n\n## Usage Example\n\n<div align=\"center\">\n<img src=\"https://blog.kakaocdn.net/dn/uNi8L/btsM4pEjswV/hSfxo1gHzPvpXPsEEyuijk/img.gif\" width=\"70%\">\n</div>\n\n## Limitations\n\n- Prompt text supports up to 1024 characters\n- Image generation allows up to 3 images at a time\n- Image variation requires 1-5 reference images\n- Color guide supports 1-10 color codes\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "canvas",
        "bedrock",
        "images",
        "nova canvas",
        "canvas mcp",
        "generation image"
      ],
      "category": "image-and-video-generation"
    },
    "zjf2671--hh-mcp-comfyui": {
      "owner": "zjf2671",
      "name": "hh-mcp-comfyui",
      "url": "https://github.com/zjf2671/hh-mcp-comfyui",
      "imageUrl": "/freedevtools/mcp/pfp/zjf2671.webp",
      "description": "Integrates with local ComfyUI instances via API calls to enable natural language-driven image generation. Supports dynamic parameter replacement in workflows and automatic loading of workflow files as resources.",
      "stars": 16,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T16:25:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zjf2671-hh-mcp-comfyui-badge.png)](https://mseep.ai/app/zjf2671-hh-mcp-comfyui)\n\n# ComfyUI MCP 服务\n\n[![English](https://img.shields.io/badge/English-Click-yellow)](docs/README.EN.md)\n[![简体中文](https://img.shields.io/badge/简体中文-点击查看-orange)](README.md)\n![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](LiCENSE)\n[![smithery badge](https://smithery.ai/badge/@zjf2671/hh-mcp-comfyui)](https://smithery.ai/server/@zjf2671/hh-mcp-comfyui)\n\n这是一个基于Model Context Protocol (MCP)的ComfyUI图像生成服务，通过API调用本地ComfyUI实例生成图片。\n\n## 功能特性\n\n- 通过MCP协议提供图像生成服务，实现自然语言生图自由\n- 支持动态替换工作流中的提示词和尺寸等参数\n- 自动加载workflows目录下的工作流文件作为资源\n\n## 新增功能记录\n- [2025-06-29] 支持kontext图片编辑工作流\n![edit-image-85457440acc11a9f386f8ef284fd62f2.jpg](https://image.harryzhang.site/2025/07/edit-image-85457440acc11a9f386f8ef284fd62f2.jpg)\n- [2025-05-11] 支持工作流文件目录动态配置\n- [2025-05-09] 增加docker构建方式,支持Python 3.12+\n- [2025-05-07] 增加pip构建方式\n- [2025-05-06] 把项目目录src/hh修改成src/hh_mcp_comfyui,增加uvx构建方式\n- [2025-04-26] 增加图生图和移除背景样例工作流及支持图生图工具\n- [2025-04-20] 加入文生图生成工具\n \n## 效果\n\n- **Cherry Studio中使用效果**\n![image-b8f946109d63fe1ccb5e2d63933e3f9e.png](https://image.harryzhang.site/2025/07/image-b8f946109d63fe1ccb5e2d63933e3f9e.png)\n\n- **Cline中使用效果**\n![cline_gen_image-48d8515e0b59cd313879c62a1546162d.png](https://image.harryzhang.site/2025/07/cline_gen_image-48d8515e0b59cd313879c62a1546162d.png)\n![ComfyUI_00020_-d9171f87fc9e67fcc1966cdbfb952a0c.png](https://image.harryzhang.site/2025/07/ComfyUI_00020_-d9171f87fc9e67fcc1966cdbfb952a0c.png)\n\n## 安装依赖\n\n**1. 确保已安装Python 3.12+**\n\n**2. 使用uv管理Python环境：**\n- 安装uv:\n  ```bash\n  # On macOS and Linux.\n  $ curl -LsSf https://astral.sh/uv/install.sh | sh\n\n  # On Windows.\n  $ powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n  # 更新uv(非必要操作):\n  $ uv self update\n  ```\n\n## 测试运行服务\n\n- **uvx方式**\n  ```bash\n  $ uvx hh-mcp-comfyui\n\n  INFO:hh_mcp_comfyui.server:Scanning for workflows in: C:\\Users\\tianw\\AppData\\Local\\uv\\cache\\archive-v0\\dp4MTo0f1qL0DdYF_BYCL\\Lib\\site-packages\\hh_mcp_comfyui\\workflows\n  INFO:hh_mcp_comfyui.server:Starting ComfyUI MCP Server...\n  ```\n- **pip方式**\n  ```bash\n  $ pip install hh_mcp_comfyui\n  \n  $ python -m hh_mcp_comfyui\n\n  INFO:hh_mcp_comfyui.server:Scanning for workflows in: F:\\Python\\Python313\\Lib\\site-packages\\hh_mcp_comfyui\\workflows\n  INFO:hh_mcp_comfyui.server:Starting ComfyUI MCP Server...\n  ```\n**出现上面的信息表示服务启动成功**\n\n## 使用方法\n> **必须确保本地ComfyUI实例正在运行(默认地址: http://127.0.0.1:8188) [ComfyUI安装地址](https://github.com/comfyanonymous/ComfyUI.git)**\n\n### Cherry Studio、Cline、Cursor等客户端的使用方式\n\n<details>\n  <summary>uvx MCP服务配置</summary>\n\n  ```bash\n  {\n    \"mcpServers\": {\n      \"hh-mcp-comfyui\": {\n        \"command\": \"uvx\",\n        \"args\": [\n          \"hh-mcp-comfyui@latest\"\n        ],\n        \"env\": {\n          \"COMFYUI_API_BASE\": \"http://127.0.0.1:8188\",\n          \"COMFYUI_WORKFLOWS_DIR\": \"/path/hh-mcp-comfyui/workflows\"\n        }\n      }\n    }\n  }\n  ```\n  </details>\n\n<details>\n  <summary>pip MCP服务配置</summary>\n\n  **需要先执行命令窗口先执行：pip install hh_mcp_comfyui**\n\n  ```bash\n  {\n    \"mcpServers\": {\n      \"hh-mcp-comfyui\": {\n        \"command\": \"python\",\n        \"args\": [\n          \"-m\",\n          \"hh_mcp_comfyui\"\n        ],\n        \"env\": {\n          \"COMFYUI_API_BASE\": \"http://127.0.0.1:8188\",\n          \"COMFYUI_WORKFLOWS_DIR\": \"/path/hh-mcp-comfyui/workflows\"\n        }\n      }\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>docker MCP服务配置</summary>\n\n  **前提是已安装docker**\n\n  ```bash\n  {\n    \"mcpServers\": {\n      \"hh-mcp-comfyui\": {\n        \"command\": \"docker\",\n        \"args\": [\n            \"run\",\n            \"--net=host\",\n            \"-v\",\n            \"/path/hh-mcp-comfyui/workflows:/app/workflows\",\n            \"-i\",\n            \"--rm\",\n            \"zjf2671/hh-mcp-comfyui:latest\"\n        ],\n        \"env\": {\n          \"COMFYUI_API_BASE\": \"http://127.0.0.1:8188\"\n        }\n      }\n    }\n  }\n  ```\n</details>\n\n## 样例工作流copy到指定工作流目录：\n\n  （**注意**：使用下面uvx或pip方式找到你的安装工作流目录的位置把样例工作流添加进去，然后重启你的MCP服务）\n- **uvx**\n  ```bash\n  $ uvx hh-mcp-comfyui\n  ```\n  ![image-2-f89caf964efbccdad7b6fa2672d1cac0.png](https://image.harryzhang.site/2025/07/image-2-f89caf964efbccdad7b6fa2672d1cac0.png)\n- **pip**\n  \n   ```bash\n  #首先安装依赖\n  $ pip install hh_mcp_comfyui\n  $ python -m hh_mcp_comfyui\n  ```\n  ![image-3-03a069f40492fea9947a351b8707aa3f.png](https://image.harryzhang.site/2025/07/image-3-03a069f40492fea9947a351b8707aa3f.png)\n\n## 测试\n\n> **使用MCP Inspector测试服务端工具**\n\n- **uvx方式**\n  ```bash\n  $ npx @modelcontextprotocol/inspector uvx hh-mcp-comfyui\n  ``` \n- **pip方式**\n  ```bash\n  $ pip install hh_mcp_comfyui\n  $ npx @modelcontextprotocol/inspector python -m hh_mcp_comfyui\n  ``` \n - **docker方式**\n    ```bash\n    $ npx @modelcontextprotocol/inspector docker run --net=host -i --rm zjf2671/hh-mcp-comfyui\n    ``` \n然后点击连接如图即可调试：\n![image-1-44c6a003ee317093afe5a61cfe028720.png](https://image.harryzhang.site/2025/07/image-1-44c6a003ee317093afe5a61cfe028720.png)\n\n## 使用注意事项（针对没有用过comfyui的特别注意）\n\n- 默认工作流为`t2image_bizyair_flux`\n- 图片尺寸默认为1024x1024\n- 服务启动时会自动加载workflows目录下的所有JSON工作流文件\n- 如果你使用的是本项目中的**样例工作流**需要在comfyui中下载个插件，详细操作请查看：[样例工作流插件安装教程](https://ziitefe2yxn.feishu.cn/wiki/PlSmwBbBWiA0iDkc07scb4EEnHc)\n- 如果使用你本地的comfyui工作流的话，先要保证你的工作流能在comfyui正常运行，然后需要导出(API)的JSON格式，并放入到你本地的`/path/hh_mcp_comfyui/workflows`目录中\n\n## 添加新工作流\n\n1. 将工作流JSON文件放入`/path/hh_mcp_comfyui/workflows`目录中\n  \n    如果是uvx和pip启动方式请看上面 《**样例工作流copy到指定工作流目录**》 的使用方式\n\n2. 重启服务自动加载新工作流\n\n## 开发\n\n\n### 项目结构\n\n```\n.\n├── .gitignore\n├── .python-version\n├── pyproject.toml\n├── README.md\n├── uv.lock\n├── example/              # 示例工作流目录\n│   └── workflows/\n│       ├── i2image_bizyair_sdxl.json\n│       ├── t2image_bizyair_flux.json\n│       ├── i2image_cogview4.json\n│       └── t2image_sd1.5.json\n├── src/                  # 源代码目录\n│   └── hh_mcp_comfyui/\n│       ├── comfyui_client.py    # ComfyUI客户端实现\n│       ├── server.py            # MCP服务主文件\n│       └── workflows/           # 工作流文件目录\n```\n\n\n ### 初始化项目开发环境：  \n\n  ```bash\n  # Clone the repository.\n  $ git clone https://github.com/zjf2671/hh-mcp-comfyui.git\n\n  $ cd hh-mcp-comfyui\n\n  # Initialized venv\n  $ uv venv\n\n  # Activate the virtual environment.\n  $ .venv\\Scripts\\activate\n\n  # Install dependencies.\n  $ uv lock\n  Resolved 30 packages in 1ms\n\n  # sync dependencies.\n  $ uv sync\n  Resolved 30 packages in 2.54s\n  Audited 29 package in 0.02ms\n  ```\n\n### 检查服务是否正常\n\n  ```bash\n  $ uv --directory 你本地安装目录/hh-mcp-comfyui run hh-mcp-comfyui\n\n  INFO:__main__:Scanning for workflows in: D:\\cygitproject\\hh-mcp-comfyui\\src\\hh_mcp_comfyui\\workflows\n  INFO:__main__:Registered resource: workflow://t2image_bizyair_flux -> t2image_bizyair_flux.json\n  INFO:__main__:Starting ComfyUI MCP Server...\n  ```\n### 使用MCP Inspector测试服务端工具\n  \n  ```bash\n  $ npx @modelcontextprotocol/inspector uv --directory 你本地安装目录/hh-mcp-comfyui run hh-mcp-comfyui\n  ```\n\n### MCP配置\n\n  ```bash\n  {\n    \"mcpServers\": {\n      \"hh-mcp-comfyui\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"项目绝对路径（例如：D:/hh-mcp-comfyui）\",\n          \"run\",\n          \"hh-mcp-comfyui\"\n        ],\n        \"env\": {\n          \"COMFYUI_API_BASE\": \"http://127.0.0.1:8188\",\n          \"COMFYUI_WORKFLOWS_DIR\": \"/path/hh-mcp-comfyui/workflows\"\n        }\n      }\n    }\n  }\n  ```\n\n## 贡献\n\n1. Fork项目\n2. 创建特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 打开Pull Request\n\n---\n## 如有问题可以到公众号中联系我：\n\n*<center>![公众号二维码](https://image.harryzhang.site/2025/04/image-1-5ac2e62b072e6f1d6eb4e3638634094c.png)</center>*\n\n<center><u>👆 扫码关注，发现更多好玩的！</u></center>\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "comfyui",
        "image",
        "dynamic",
        "comfyui instances",
        "comfyui integrates",
        "mcp comfyui"
      ],
      "category": "image-and-video-generation"
    },
    "zxkane--mcp-server-amazon-bedrock": {
      "owner": "zxkane",
      "name": "mcp-server-amazon-bedrock",
      "url": "https://github.com/zxkane/mcp-server-amazon-bedrock",
      "imageUrl": "/freedevtools/mcp/pfp/zxkane.webp",
      "description": "Integrates with Amazon Bedrock's Nova Canvas model to generate high-quality images based on text descriptions. Provides advanced features for refining image composition through negative prompts and allows control over image dimensions and quality.",
      "stars": 21,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T09:31:13Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zxkane-mcp-server-amazon-bedrock-badge.png)](https://mseep.ai/app/zxkane-mcp-server-amazon-bedrock)\n\n# Amazon Bedrock MCP Server\n\nA Model Control Protocol (MCP) server that integrates with Amazon Bedrock's Nova Canvas model for AI image generation.\n\n<a href=\"https://glama.ai/mcp/servers/9qw7dwpvj9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/9qw7dwpvj9/badge\" alt=\"Amazon Bedrock Server MCP server\" /></a>\n\n## Features\n\n- High-quality image generation from text descriptions using Amazon's Nova Canvas model\n- Advanced control through negative prompts to refine image composition\n- Flexible configuration options for image dimensions and quality\n- Deterministic image generation with seed control\n- Robust input validation and error handling\n\n## Prerequisites\n\n1. Active AWS account with Amazon Bedrock and Nova Canvas model access\n2. Properly configured AWS credentials with required permissions\n3. Node.js version 18 or later\n\n## Installation\n\n### AWS Credentials Configuration\n\nThe server requires AWS credentials with appropriate Amazon Bedrock permissions. Configure these using one of the following methods:\n\n1. Environment variables:\n   ```bash\n   export AWS_ACCESS_KEY_ID=your_access_key\n   export AWS_SECRET_ACCESS_KEY=your_secret_key\n   export AWS_REGION=us-east-1  # or your preferred region\n   ```\n\n2. AWS credentials file (`~/.aws/credentials`):\n   ```ini\n   [the_profile_name]\n   aws_access_key_id = your_access_key\n   aws_secret_access_key = your_secret_key\n   ```\n   Environment variable for active profile:\n   ```bash\n   export AWS_PROFILE=the_profile_name\n   ```\n\n3. IAM role (when deployed on AWS infrastructure)\n\n### Claude Desktop Integration\n\nTo integrate with Claude Desktop, add the following configuration to your settings file:\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"amazon-bedrock\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@zxkane/mcp-server-amazon-bedrock\"\n      ],\n      \"env\": {\n        \"AWS_PROFILE\": \"your_profile_name\",         // Optional, only if you want to use a specific profile\n        \"AWS_ACCESS_KEY_ID\": \"your_access_key\",     // Optional if using AWS credentials file or IAM role\n        \"AWS_SECRET_ACCESS_KEY\": \"your_secret_key\", // Optional if using AWS credentials file or IAM role\n        \"AWS_REGION\": \"us-east-1\"                   // Optional, defaults to 'us-east-1'\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### generate_image\n\nCreates images from text descriptions using Amazon Bedrock's Nova Canvas model.\n\n#### Parameters\n\n- `prompt` (required): Descriptive text for the desired image (1-1024 characters)\n- `negativePrompt` (optional): Elements to exclude from the image (1-1024 characters)\n- `width` (optional): Image width in pixels (default: 1024)\n- `height` (optional): Image height in pixels (default: 1024)\n- `quality` (optional): Image quality level - \"standard\" or \"premium\" (default: \"standard\")\n- `cfg_scale` (optional): Prompt adherence strength (1.1-10, default: 6.5)\n- `seed` (optional): Generation seed for reproducibility (0-858993459, default: 12)\n- `numberOfImages` (optional): Batch size for generation (1-5, default: 1)\n\n#### Example Implementation\n\n```typescript\nconst result = await callTool('generate_image', {\n  prompt: \"A serene mountain landscape at sunset\",\n  negativePrompt: \"people, buildings, vehicles\",\n  quality: \"premium\",\n  cfg_scale: 8,\n  numberOfImages: 2\n});\n```\n\n#### Prompt Guidelines\n\nFor optimal results, avoid negative phrasing (\"no\", \"not\", \"without\") in the main prompt. Instead, move these elements to the `negativePrompt` parameter. For example, rather than using \"a landscape without buildings\" in the prompt, use \"buildings\" in the `negativePrompt`.\n\nFor detailed usage guidelines, refer to the [Nova Canvas documentation][nova-canvas-doc].\n\n## Development\n\nTo set up and run the server in a local environment:\n\n```bash\ngit clone https://github.com/zxkane/mcp-server-amazon-bedrock.git\ncd mcp-server-amazon-bedrock\nnpm install\nnpm run build\n```\n\n### Performance Considerations\n\nGeneration time is influenced by resolution (`width` and `height`), `numberOfImages`, and `quality` settings. When using higher values, be mindful of potential timeout implications in your implementation.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n[nova-canvas-doc]: https://docs.aws.amazon.com/nova/latest/userguide/image-gen-access.html\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bedrock",
        "canvas",
        "images",
        "amazon bedrock",
        "bedrock nova",
        "image composition"
      ],
      "category": "image-and-video-generation"
    },
    "zym9863--pixabay-mcp": {
      "owner": "zym9863",
      "name": "pixabay-mcp",
      "url": "https://github.com/zym9863/pixabay-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zym9863.webp",
      "description": "Connect to the Pixabay API to search for images and retrieve formatted results that include image URLs and metadata. Handle errors seamlessly during API interactions for reliable performance.",
      "stars": 4,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T07:41:49Z",
      "readme_content": "# pixabay-mcp MCP Server\n\n[中文版](README_zh.md)\n\nA Model Context Protocol (MCP) server for Pixabay image and video search with structured results & runtime validation.\n\n<a href=\"https://glama.ai/mcp/servers/@zym9863/pixabay-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@zym9863/pixabay-mcp/badge\" alt=\"Pixabay Server MCP server\" />\n</a>\n\nThis TypeScript MCP server exposes Pixabay search tools over stdio so AI assistants / agents can retrieve media safely and reliably.\n\nHighlights:\n- Image & video search tools (Pixabay official API)\n- Runtime argument validation (enums, ranges, semantic checks)\n- Consistent error logging without leaking sensitive keys\n- Planned structured JSON payloads for easier downstream automation (see Roadmap)\n\n## Features\n\n### Tools\n`search_pixabay_images`\n  - Required: `query` (string)\n  - Optional: `image_type` (all|photo|illustration|vector), `orientation` (all|horizontal|vertical), `per_page` (3-200)\n  - Returns: human-readable text block (current) + (planned) structured JSON array of hits\n\n`search_pixabay_videos`\n  - Required: `query`\n  - Optional: `video_type` (all|film|animation), `orientation`, `per_page` (3-200), `min_duration`, `max_duration`\n  - Returns: human-readable text block + (planned) structured JSON with duration & URLs\n\n### Configuration\nEnvironment variables:\n| Name | Required | Default | Description |\n| ---- | -------- | ------- | ----------- |\n| `PIXABAY_API_KEY` | Yes | - | Your Pixabay API key (images & videos) |\n| `PIXABAY_TIMEOUT_MS` | No | 10000 (planned) | Request timeout once feature lands |\n| `PIXABAY_RETRY` | No | 0 (planned) | Number of retry attempts for transient network errors |\n\nNotes:\n- Safe search is enabled by default.\n- Keys are never echoed back in structured errors or logs.\n\n## Usage Examples\n\nCurrent (text only response excerpt):\n```\nFound 120 images for \"cat\":\n- cat, pet, animal (User: Alice): https://.../medium1.jpg\n- kitten, cute (User: Bob): https://.../medium2.jpg\n```\n\nPlanned structured result (Roadmap v0.4+):\n```jsonc\n{\n  \"content\": [\n    { \"type\": \"text\", \"text\": \"Found 120 images for \\\"cat\\\":\\n- ...\" },\n    {\n      \"type\": \"json\",\n      \"data\": {\n        \"query\": \"cat\",\n        \"totalHits\": 120,\n        \"page\": 1,\n        \"perPage\": 20,\n        \"hits\": [\n          { \"id\": 123, \"tags\": [\"cat\",\"animal\"], \"user\": \"Alice\", \"previewURL\": \"...\", \"webformatURL\": \"...\", \"largeImageURL\": \"...\" }\n        ]\n      }\n    }\n  ]\n}\n```\n\nError response (planned shape):\n```json\n{\n  \"content\": [{ \"type\": \"text\", \"text\": \"Pixabay API error: 400 ...\" }],\n  \"isError\": true,\n  \"metadata\": { \"status\": 400, \"code\": \"UPSTREAM_BAD_REQUEST\", \"hint\": \"Check API key or parameters\" }\n}\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nWatch mode:\n```bash\nnpm run watch\n```\n\n## Installation\n\n### Option 1: Using npx (Recommended)\n\nAdd this to your Claude Desktop configuration:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"pixabay-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"pixabay-mcp@latest\"],\n      \"env\": {\n        \"PIXABAY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Local Installation\n\n1. Clone and build the project:\n\n```bash\ngit clone https://github.com/zym9863/pixabay-mcp.git\ncd pixabay-mcp\nnpm install\nnpm run build\n```\n\n2. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"pixabay-mcp\": {\n      \"command\": \"/path/to/pixabay-mcp/build/index.js\",\n      \"env\": {\n        \"PIXABAY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### API Key Setup\n\nGet your Pixabay API key from [https://pixabay.com/api/docs/](https://pixabay.com/api/docs/) and set it in the configuration above. The same key grants access to both image and video endpoints.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Roadmap (Condensed)\n| Version | Focus | Key Items |\n| ------- | ----- | --------- |\n| v0.4 | Structured & Reliability | JSON payload, timeout, structured errors |\n| v0.5 | UX & Pagination | page/order params, limited retry, modular refactor, tests |\n| v0.6 | Multi-source Exploration | Evaluate integrating Unsplash/Pexels abstraction |\n\nSee `product.md` for full backlog & prioritization.\n\n## Contributing\nPlanned contributions welcome once tests & module split land (v0.5 target). Feel free to open issues for API shape / schema suggestions.\n\n## License\nMIT\n\n## Disclaimer\nThis project is not affiliated with Pixabay. Respect Pixabay's Terms of Service and rate limits.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pixabay",
        "images",
        "mcp",
        "pixabay api",
        "pixabay mcp",
        "connect pixabay"
      ],
      "category": "image-and-video-generation"
    },
    "zym9863--together-ai-image-server": {
      "owner": "zym9863",
      "name": "together-ai-image-server",
      "url": "https://github.com/zym9863/together-ai-image-server",
      "imageUrl": "/freedevtools/mcp/pfp/zym9863.webp",
      "description": "Generates images from text prompts using Together AI's image generation models via the MCP protocol. It supports optional parameters for fine-tuning the image generation process.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-21T08:41:48Z",
      "readme_content": "# Together AI Image Server\n\nEnglish | [简体中文](README_zh.md)\n\nA TypeScript-based MCP (Model Context Protocol) server for generating images using Together AI API.\n\n<a href=\"https://glama.ai/mcp/servers/p1ctvg1l87\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/p1ctvg1l87/badge\" alt=\"Together AI Image Server MCP server\" />\n</a>\n\n## Overview\n\nThis server provides a simple interface to generate images using Together AI's image generation models through the MCP protocol. It allows Claude and other MCP-compatible assistants to generate images based on text prompts.\n\n## Features\n\n### Tools\n\n- `generate_image` - Generate images from text prompts\n  - Takes a text prompt as required parameter\n  - Optional parameters for controlling generation steps and number of images\n  - Returns URLs and local paths to generated images\n\n## Prerequisites\n\n- Node.js (v14 or later recommended)\n- Together AI API key\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/zym9863/together-ai-image-server.git\ncd together-ai-image-server\n\n# Install dependencies\nnpm install\n```\n\n## Configuration\n\nSet your Together AI API key as an environment variable:\n\n```bash\n# On Linux/macOS\nexport TOGETHER_API_KEY=\"your-api-key-here\"\n\n# On Windows (Command Prompt)\nset TOGETHER_API_KEY=your-api-key-here\n\n# On Windows (PowerShell)\n$env:TOGETHER_API_KEY=\"your-api-key-here\"\n```\n\nAlternatively, you can create a `.env` file in the project root:\n\n```\nTOGETHER_API_KEY=your-api-key-here\n```\n\n## Development\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Usage with Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"Together AI Image Server\": {\n      \"command\": \"/path/to/together-ai-image-server/build/index.js\"\n    }\n  }\n}\n```\n\nReplace `/path/to/together-ai-image-server` with the actual path to your installation.\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## API Reference\n\n### generate_image\n\nGenerates images based on a text prompt using Together AI's image generation API.\n\n**Parameters:**\n\n- `prompt` (string, required): Text prompt for image generation\n- `steps` (number, optional, default: 4): Number of diffusion steps (1-4)\n- `n` (number, optional, default: 1): Number of images to generate (1-4)\n\n**Returns:**\n\nJSON object containing:\n- `image_urls`: Array of URLs to the generated images\n- `local_paths`: Array of paths to locally cached images\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "images",
        "ai",
        "image",
        "ai image",
        "generates images",
        "image generation"
      ],
      "category": "image-and-video-generation"
    }
  }
}